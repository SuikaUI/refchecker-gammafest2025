Supervised learning with quantum enhanced feature spaces
Vojtech Havlicek1,∗Antonio D. C´orcoles1, Kristan Temme1, Aram W. Harrow2,
Abhinav Kandala1, Jerry M. Chow1, and Jay M. Gambetta1
1IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA and
2Center for Theoretical Physics, Massachusetts Institute of Technology, USA
 
Machine learning and quantum computing are
two technologies each with the potential for altering how computation is performed to address
previously untenable problems.
Kernel methods for machine learning are ubiquitous for pattern recognition, with support vector machines
(SVMs) being the most well-known method for
classiﬁcation problems. However, there are limitations to the successful solution to such problems when the feature space becomes large, and
the kernel functions become computationally expensive to estimate. A core element to computational speed-ups aﬀorded by quantum algorithms
is the exploitation of an exponentially large quantum state space through controllable entanglement and interference.
Here, we propose and experimentally implement two novel methods on a superconducting
processor.
Both methods represent the feature
space of a classiﬁcation problem by a quantum
state, taking advantage of the large dimensionality of quantum Hilbert space to obtain an
enhanced solution.
One method, the quantum
variational classiﬁer builds on and operates
variational
to classify a training set in direct analogy to
conventional SVMs.
In the second, a quantum
kernel estimator, we estimate the kernel function and optimize the classiﬁer directly.
two methods present a new class of tools for
exploring the applications of noisy intermediate
scale quantum computers to machine learning.
The intersection between machine learning and quantum computing has been dubbed quantum machine
learning, and has attracted considerable attention in recent years . This has led to a number of recently proposed quantum algorithms . Here, we present
a quantum algorithm that has the potential to run on
near-term quantum devices.
A natural class of algorithms for such noisy devices are short-depth circuits,
which are amenable to error-mitigation techniques that
reduce the eﬀect of decoherence .
convincing arguments that indicate that even very sim-
∗On leave from Quantum Group, Department of Computer Science, University of Oxford, Wolfson Building, Parks Road, Oxford OX1 3QD, UK
ple circuits are hard to simulate by classical computational means . The algorithm we propose takes
on the original problem of supervised learning: the construction of a classiﬁer. For this problem, we are given
data from a training set T and a test set S of a subset Ω⊂Rd. Both are assumed to be labeled by a map
m : T ∪S →{+1, −1} unknown to the algorithm. The
training algorithm only receives the labels of the training
data T. The goal is to infer an approximate map on the
test set ˜m : S →{+1, −1} such that it agrees with high
probability with the true map m(⃗s) = ˜m(⃗s) on the test
data ⃗s ∈S. For such a learning task to be meaningful it
is assumed that there is a correlation between the labels
given for training and the true map. A classical approach
to constructing an approximate labeling function uses socalled support vector machines (SVMs) . The data
gets mapped non-linearly to a high dimensional space,
the feature space, where a hyperplane is constructed to
separate the labeled samples. A quantum version of this
approach has already been proposed in , where an
exponential improvement can be achieved if data is provided in a coherent superposition. However, when data
is provided in the conventional way, i.e. from a classical
computer, then the methods of cannot be applied.
Here, we propose two SVM type classiﬁers that process
data provided purely classically and use the quantum
state space as the feature space to still obtain a quantum advantage. This is done by mapping the data nonlinearly to a quantum state Φ : ⃗x ∈Ω→| Φ(⃗x)⟩⟨Φ(⃗x) |,
1(a). We implement both classiﬁers on a superconducting quantum processor. In the ﬁrst approach
we use a variational circuit as given in that
generates a separating hyperplane in the quantum feature space. In the second approach we use the quantum
computer to estimate the kernel function of the quantum feature space directly and implement a conventional
SVM. A necessary condition to obtain a quantum advantage, in either of the two approaches, is that the kernel
cannot be estimated classically. This is true, even when
complex variational quantum circuits are used as classiﬁers.
In the experiment, we want to disentangle the
question of whether the classiﬁer can be implemented in
hardware, from the problem of choosing a suitable feature map for a practical data set. The data that is classi-
ﬁed here is chosen so that it can be classiﬁed with 100%
success to verify the method. We demonstrate that this
success ratio is subsequently achieved in experiment.
Our experimental device consists of ﬁve coupled superconducting transmons, only two of which are used in this
 
Quantum Kernel Functions: (a) Feature map representation for a single qubit. A classical dataset in the interval
Ω= (0, 2π] with binary labels (a, right) can be mapped onto the Bloch sphere (red / blue - lines) by using the non-linear
feature map described in (b). For a single qubit UΦ(x) = Zx is a phase-gate of angle x ∈Ω. The mapped data can be separated
by the hyperplane given by normal ⃗w. States with a positive expectation value of ⃗w receive a [+1] (red) label, while negative
values are labeld [−1](blue). (b) For the general circuit UΦ(⃗x) is formed by products of single- and two-qubit unitaries that
are diagonal in the computational basis. In our experiments, both the training and testing data is artiﬁcially generated to be
perfectly classiﬁable with the aforementioned feature map. The circuit family depends non-linearly on the data through the
coeﬃcients φS(⃗x) with |S| ≤2. (c) Experimental implementation of the parameterized diagonal two-qubit operations using
CNOTs and Z−gates.
work, as shown in Fig. 2(a). Two co-planar waveguide
(CPW) resonators, acting as quantum buses, provide the
device connectivity. Each qubit has one additional CPW
resonator for control and readout. Entanglement in our
system is achieved via CNOT gates, which use crossresonance as well as single qubit gates as primitives.
The quantum processor is thermally anchored to the mixing chamber plate of a dilution refrigerator.
Quantum feature map: Before discussing the two methods of classiﬁcation, we discuss the feature map. Training
and classiﬁcation with conventional support vector machines is eﬃcient when inner products between feature
vectors can be evaluated eﬃciently . We will
see that classiﬁers based on quantum circuits, such as
the one presented in Fig 2(c) cannot provide a quantum
advantage over a conventional support vector machine if
the feature vector kernel K(⃗x,⃗z) = | ⟨Φ(⃗x) | Φ(⃗z) ⟩|2 is
too simple. For example, a classiﬁer that uses a feature
map that only generates product states can immediately
be implement classically. To obtain an advantage over
classical approaches we need to implement a map based
on circuits that are hard to simulate classically. Since
quantum computers are not expected to be classically
simulable, there exists a long list of (universal) circuit
families one can choose from. Here, we propose to use
a circuit that works well in our experiments and is not
too deep. We deﬁne a feature map on n-qubits generated
by the unitary UΦ(⃗x) = UΦ(⃗x)H⊗nUΦ(⃗x)H⊗n, where H
denotes the conventional Hadamard gate and
UΦ(⃗x) = exp
is a diagonal gate in the Pauli Z - basis, c.f. Fig 1 (b).
This circuit will act on | 0⟩n as initial state. We use the
coeﬃcients φS(⃗x) ∈R, to encode the data ⃗x ∈Ω. In
general any diagonal unitary UΦ(⃗x) can be used if it can
be implemented eﬃciently. This is for instance the case
when only weight |S| ≤2 interactions are considered.
The exact evaluation of the inner-product between two
states generated from a similar circuit with only a single
diagonal layer UΦ(⃗x) is #P - hard .
Nonetheless,
in the experimentally relevant context of additive error
approximation, simulation of a single layer preparation
circuit can be achieved eﬃciently classically by uniform
sampling .
We conjecture that the evaluation of
inner products generated from circuits with two basis
changes and diagonal gates up to additive error to be
hard, c.f. supplementary material for a discussion.
To test our two methods, we generate
artiﬁcial data that can be fully separated by our feature
map. We use the map for n = d = 2 - qubits in Fig. 1(b)
with φ{i}(⃗x) = xi and φ{1,2}(⃗x) = (π −x1)(π −x2). We
generate the labels for data vectors ⃗x ∈T ∪S ⊂(0, 2π]2,
by ﬁrst choosing f = Z1Z2 as the parity function and
a random unitary V ∈SU(4). We assign m(⃗x) = +1,
when ⟨Φ(⃗x) |V †fV | Φ(⃗x)⟩≥∆and m(⃗x) = −1 when
⟨Φ(⃗x) |V †fV | Φ(⃗x)⟩≤−∆, c.f. Fig 3(b). The data has
been separated by a gap of ∆= 0.3. Both the training
sets and the classiﬁcation sets consist of 20 data points
per label.
We show one of such classiﬁcation sets as
circle symbols in Fig. 3(b).
Quantum variational classiﬁcation:
The ﬁrst classi-
ﬁcation protocol follows four steps.
First, the data
⃗x ∈Ωis mapped to a quantum state by applying
the feature map circuit UΦ(⃗x) in Fig. 1(b) to a reference state | 0⟩n.
Second, a short depth quantum circuit W(⃗θ), described in Fig
2(b) is applied to the
feature state. A circuit with l - layers is parametrized
by ⃗θ ∈R2n(l+1) that will be optimized during training.
Third, for a two label classiﬁcation y ∈{+1, −1}
problem, a binary measurement {My} is applied to the
state W(⃗θ)UΦ(⃗x)| 0⟩n. This measurement is implemented
by measurements in the Z - basis and feeding the output bit-string z ∈{0, 1}n to a chosen boolean function
f : {0, 1}n →{+1, −1}. The measurement operator is
given by My = 2−1(1 + yf), where we have deﬁned f =
z∈{0,1}n f(z)| z⟩⟨z |. The probability of obtaining outcome y is py(⃗x) = ⟨Φ(⃗x) |W †(⃗θ)MyW(⃗θ)| Φ(⃗x)⟩. Fourth,
for the decision rule we perform R repeated measurement
shots to obtain the empirical distribution ˆpy(⃗x). We assign the label ˜m(⃗x) = y, whenever ˆpy(⃗x) > ˆp−y(⃗x) −yb,
where we have introduced an additional bias parameter
b ∈[−1, 1] that can be optimized during training.
The feature map circuit UΦ(⃗x) as well as the boolean
function f are ﬁxed choices. During the training of the
classiﬁer we optimize the parameters (⃗θ, b). For the optimization, we need to deﬁne a cost-function. We deﬁne
the empirical risk Remp(⃗θ) given by the error probability
Pr ( ˜m(⃗x) ̸= m(⃗x)) of assigning the incorrect label averaged over the samples in the training set T,
Remp(⃗θ) = 1
Pr ( ˜m(⃗x) ̸= m(⃗x)) .
For the binary problem, the error probability of assigning the wrong label is given by the binomial cumulative density function (CDF) of the empirical distribution
ˆpy(⃗x), c.f. supplementary material for a derivation. The
binomial CDF can be approximated for a large number of samples (shots) R ≫1 by a sigmoid function
sig(x) = (1 + e−x)−1.
The probability that the label
m(⃗x) = y is assigned incorrectly is approximated by
Pr ( ˜m(⃗x) ̸= m(⃗x)) ≈sig
ˆpy(⃗x) −yb
2(1 −ˆpy(⃗x))ˆpy(⃗x)
The experiment itself is split in to two phases; First, we
train the classiﬁer and optimize (⃗θ, b). We have found
that Spall’s SPSA stochastic gradient decent
algorithm performs well in the noisy experimental
We can use the circuit as a classiﬁer after
the parameters have converged to (⃗θ∗, b∗).
Second, in
the classiﬁcation phase, the classiﬁer assigns labels to
unlabeled data ⃗s ∈S according to the decision rule ˜m(⃗s).
We implement the quantum variational classiﬁer W(⃗θ)
over 5 diﬀerent depths (l = 0 through l = 4), c.f.
Fig 2(b), in our superconducting quantum processor.
We expect a higher classiﬁcation success for increased
repeat l - times
Experimental implementations (a) Schematic
of the 5-qubit quantum processor. The experiment was performed on qubits Q0 and Q1, highlighted in the image. (b)
Variational circuit used for our optimization method. Here
we choose a rather common Ansatz for the variational unitary
W(⃗θ) = U (l)
loc(θl) Uent . . . U (2)
loc (θ2) Uent U (1)
loc (θ1) . We
alternate layers of entangling gates Uent = Q
(i,j)∈E CZ(i, j)
with full layers single qubit rotations U (t)
loc(θt) = ⊗n
i=1U(θi,t)
with U(θi,t) ∈SU(2). For the entangling step we use controlled phase gates CZ(i, j) along the edges (i, j) ∈E present
in the connectivity of the superconducting chip. (c) Circuit to
directly estimate the ﬁdelity between a pair of feature vectors
for data ⃗x and ⃗y as used for our second method.
The binary measurement is obtained from the
parity function f = Z1Z2. For each depth we train three
diﬀerent data sets, using training sets consisting of 20
data points per label. One of these data sets is shown in
Fig. 3 (b), along with the training set used for this particular data set. Fig. 3(a) shows the optimization of the
empirical risk Remp(⃗θ) for two diﬀerent training sets and
depths. In all experiments throughout this work we implemented an error mitigation technique which relies on
zero-noise extrapolation to ﬁrst order . To obtain
a zero-noise estimate, a copy of the circuit was run on
a time scale slowed down by a factor of 1.5, c.f. supplemental material. This technique is implemented at each
trial step, and it is the mitigated cost function that is fed
to the classical optimizer. We observe that the empirical risk in Fig. 3(a) converges to a lower value for depth
l = 4 than for l = 0, albeit with more optimization steps.
Whereas error mitigation does not appreciably improve
the results for depth 0 - the noise in our system is not the
limiting factor in that case-, it does help substantially for
larger depths. Although Pr ( ˜m(⃗x) ̸= m(⃗x)) explicitly includes the number of experimental shots taken, we ﬁxed
R = 200 to avoid gradient problems, even though we
took 2000 shots in the actual experiment.
After each training is completed, we use the trained
Convergence of the method and classiﬁcation results: (a) Convergence of the cost function Remp(⃗θ) after
250 iterations of Spall’s SPSA algorithm. Red (black) curves correspond to l = 4 (l = 0). The value of the cost function,
obtained from estimates of ˆpk after zero-noise extrapolation (solid lines), is compared with values obtained from experimental
measurements of ˆpk with standard single and two-qubit gate times (dashed). We train three sets of data per depth and perform
20 classiﬁcations per trained set.
The results of these classiﬁcations are shown in (c) as black dots (amounting to 60 per
depth), with mean values at each depth represented by red dots. The error bar is the standard error of the mean. The inset
shows histograms as a function of the probability of measuring label +1 for a test set of 20 points per label obtained with a
l = 4 classiﬁer circuit, depicting classiﬁcation with 100% success.. The dashed blue lines show the results of our direct kernel
estimation method for comparison, with Sets I and II yielding 100% success and Set III yielding 94.75% success. (b) Example
data set used for both methods in this work. The data labels (red for +1 label and blue for −1 label) are generated with a
separation gap of magnitude 0.3 between them (white areas). The 20 points per label training set is shown as white and black
circles. For our quantum kernel estimation method we show the obtained support vectors (green circles) and a classiﬁed test
set (white and black squares). Three of the test sets points are misclasiﬁed, labeled as A, B, and C. For each of the test data
points ⃗xj we plot at the bottom of (b) the amount P
i K(⃗xi, ⃗xj) over all support vectors ⃗xi, where yi ∈{+1, −1} are the
labels of the support vectors. Points A, B, and C, all belonging to label +1, give P
i K(⃗xi, ⃗xj) = -1.033, -0.367 and -1.082,
respectively.
set of parameters (⃗θ∗, b∗= 0) to classify 20 diﬀerent
test sets -randomly drawn each time- per data set. We
run these classiﬁcation experiments at 10,000 shots,
versus the 2,000 used for training. The classiﬁcation of
each data point is error-mitigated and repeated twice,
averaging the success ratios obtained in each of the two
classiﬁcations. Fig. 3 (c) shows the classiﬁcation results
for our quantum variational approach.
We clearly see
an increase in classiﬁcation success with increasing
circuit depth, c.f. Fig. 3(c), reaching values very close
to 100% for depths larger than 1.
This classiﬁcation
success remarkably remains up to depth 4, despite the
decoherence associated with 8 CNOTs in the training
and classiﬁcation circuits, for l = 4.
A path to quantum advantage: Such variational circuit
classiﬁers are directly related to conventional SVMs
To see why a quantum advantage can only
be obtained for feature maps with a classically hard
to estimate kernel, we point out the following:
decision rule py(⃗x) > p−y(⃗x) −yb can be restated
as ˜m(⃗x) = sign(⟨Φ(⃗x) |W †(⃗θ)fW(⃗θ)| Φ(⃗x)⟩+ b).
variational circuit W followed by a binary measurement can be understood as a separating hyperplane
orthogonal,
hermitian,
α = 1, . . . , 4n with tr
= 2nδα,β such as the
Pauli-group on n-qubits.
Expand both the quantum
state | Φ(⃗x)⟩⟨Φ(⃗x) | and the measurement W †(⃗θ)fW(⃗θ)
in this matrix basis.
Both the expectation value of
the binary measurement and the decision rule can be
expressed in terms of wα(⃗θ) = tr
W †(⃗θ)fW(⃗θ)Pα
Φα(⃗x) = ⟨Φ(⃗x) |Pα| Φ(⃗x)⟩. For any variational unitary
the classiﬁcation rule can be restated in the familiar
SVM form ˜m(x) = sign
α wα(⃗θ)Φα(⃗x) + b
classiﬁer can only be improved when the constraint is
lifted that the wα come from a variational circuit. The
optimal wα can alternatively be found by employing
kernel methods and considering the standard Wolfe dual of the SVM . Moreover, this decomposition indicates that one should think of the feature space as the
quantum state space with feature vectors | Φ(⃗x)⟩⟨Φ(⃗x) |
and inner products K(⃗x,⃗z) = | ⟨Φ(⃗x) | Φ(⃗z) ⟩|2. Indeed,
the direct use of the Hilbert space H = (C2)⊗n as a
feature space would lead to a conceptual problem, since
a vector | Φ(⃗x)⟩∈H is only physically deﬁned up to a
global phase.
Quantum kernel estimation: The second classiﬁcation
protocol uses this connection to implement the SVM directly. Rather than using a variational quantum circuit
to generate the separating hyperplane, we use a classical
SVM for classiﬁcation. The quantum computer is used
twice in this protocol. First, the kernel K(⃗xi, ⃗xj) is estimated on a quantum computer for all pairs of training
data ⃗xi, ⃗xj ∈T, c.f. Fig. 2(c). Here it will be convenient to write T = {⃗x1, . . . , ⃗xt} with t = |T|; also let
yi = m(⃗xi) be the corresponding label. The optimization problem for the optimal SVM can be formulated in
terms of a dual quadratic program that only uses access
to the kernel. We maximize
yiyjαiαjK(⃗xi, ⃗xj),
subject to Pt
i=1 αiyi = 0 and αi ≥0 for each i. This
problem is concave whenever K(⃗xi, ⃗xj) is a positive definite matrix. The solution to this problem will be given
by a nonnegative vector ⃗α = (α1, . . . , αt). The quantum
computer is used a second time to estimate the kernel
for a new datum ⃗s ∈S with all the support vectors. The
optimal solution ⃗α∗is used to construct the classiﬁer
˜m(⃗s) = sign
i K(⃗xi,⃗s) + b
Due to complementary slackness, we expect that many
of the αi will be zero. This can make the evaluation of
˜m(⃗s) cheaper, since K(⃗xi,⃗s) only needs to be estimated
i > 0. The bias b in ˜m(⃗s) can calculated from the
weights α∗
i by choosing any i with α∗
i > 0 and solving
jK(⃗xj, ⃗xi) + b = yi for b.
Let us discuss how the quantum computer is used to
estimate the kernel. The kernel entries are the ﬁdelities
between diﬀerent feature vectors.
Various methods
 exist, such as the swap test, to estimate the
ﬁdelity between general quantum states. However, since
the states in the feature space are not arbitrary, the
overlap can be estimated directly from the transition
| ⟨Φ(⃗x) | Φ(⃗z ⟩|2
Φ(⃗x)UΦ(⃗z)| 0n⟩|2.
First, we apply the circuit Fig. 2(c), a composition
of two consecutive feature map circuits, to the initial
reference state | 0n⟩.
Second, we measure the ﬁnal
state in the Z-basis R - times and record the number
of all zero strings 0n.
The frequency of this string is
the estimate of the transition probability.
The kernel
entry is obtained to an additive sampling error of ˜ϵ
when O(˜ϵ−2) shots are used.
In the training phase a
total of O(|T|2) amplitudes have to be estimated. An
estimator ˆK for the kernel matrix that deviates with
high probability in operator norm from the exact kernel
K by at most ∥K −ˆK∥≤ϵ can be obtained with a
total of R = O(ϵ−2|T|4) shots.
The sampling error
can compromise the positive semi-deﬁniteness of the
Although not applied in this work, this can
Kernels for Set III. (a) Experimental (left) and
ideal (right) kernel matrices containing the inner products of
all data points used for training Set III (round symbols in Fig.
3 (b)). The maximal deviation from the ideal kernel |K −ˆK|
occurs at element K8,15. A cut through row 8 (indicated by
red arrow in (a)) is shown in (b), where the experimental
(ideal) results are shown as red (blue) bars. Note that entries
that are close zero in the kernel can become negative (c.f.
K8,30 (b)) when the error-mitigation technique is applied.
be remedied by employing an adaption of the scheme
presented in . The direct connection to conventional
SVMs enables us to use the conventional bounds on the
V C-dimension that ensure convergence and guide the
structural risk minimization.
For the experimental implementation of estimating
the kernel matrix K, c.f.
circuit Fig. 2(c), we again
apply the error-mitigation protocol to ﬁrst
The kernel entries are obtained by running a
time-stretched copy of the circuit and reporting the
mitigated entry. We use 50, 000 shots per matrix entry.
Using this protocol, we obtain support vectors αi that
are very similar to the noise-free case.
We run the
training stage on three diﬀerent data sets, which we
will label as Set I, Set II and Set III. Set III is shown
in Fig. 3(b).
Note that the training data used to
obtain the kernel and the support vectors is the same
data used in training of our variational classiﬁer. The
support vectors (green circles in (b)) are then used to
classify 10 diﬀerent test sets randomly drawn from each
entire set. Set I and Set II yield 100% success over the
classiﬁcation of all 10 diﬀerent test sets each, whereas
Set III averages a success of 94.75%. For more details
see the supplementary information. These classiﬁcation
results are given in Fig. 3(c) as dashed blue lines to
compare with the results of our variational method.
In Fig. 4(a) we show the ideal and the experimentally
obtained kernel matrices, K and ˆK, for Set III. The
maximum diﬀerence across the matrices between K
and ˆK is found at row (or column) 8.
This is shown
in Fig. 4(b). All support vectors for the three sets and
equivalent plots are given in the supplementary material.
Conclusions: We have experimentally demonstrated
a classiﬁer that exploits a quantum feature space. The
kernel of this feature space has been conjectured to be
hard to estimate classically. In the experiment we ﬁnd
that even in the presence of noise, we are capable of
achieving success rates up to 100%.
In the future it
becomes intriguing to ﬁnd suitable feature maps for
this technique with provable quantum advantages while
providing signiﬁcant improvement on real world data
sets. With the ubiquity of kernel methods in machine
learning, we are optimistic that our technique will ﬁnd
application beyond binary classiﬁcation.
During composition of this manuscript we became
aware of the independent theoretical work by Schuld et
al. .
Supplementary Information is available in the
online version of the paper.
Acknowledgments We thank Sergey Bravyi for
insightful discussions.
A.W.H. acknowledges funding
from the MIT-IBM Watson AI Lab under the project
Machine Learning in Hilbert space.
The research was
supported by the IBM Research Frontiers Institute.
We acknowledge support from IARPA under contract
W911NF-10-1-0324 for device fabrication.
Author contributions The work on the classiﬁer
theory was led by V.H. and K.T. The experiment was
designed by A.D.C, J.M.G and K.T. and implemented
by A.D.C. All authors contributed to the manuscript.
Author information The authors declare no competing ﬁnancial interests. Correspondence and requests
for materials should be addressed to A.D.C. and K.T.
 Mitarai, K., Negoro, M., Kitagawa, M. & Fujii, K. Quantum circuit learning.
arXiv preprint arXiv:1803.00745
 Farhi, E. & Neven, H. Classiﬁcation with quantum neural networks on near term processors.
arXiv preprint
 
 Preskill, J. Quantum computing in the nisq era and beyond. arXiv preprint arXiv:1801.00862 .
 Arunachalam, S. & de Wolf, R. Guest column: a survey
of quantum learning theory. j-SIGACT 48, 41–67 .
 Ciliberto, C. et al. Quantum machine learning: a classical
perspective. Proc. R. Soc. A 474, 20170551 .
 Dunjko, V. & Briegel, H. J. Machine learning & artiﬁcial
intelligence in the quantum domain: a review of recent
progress. Reports on Progress in Physics .
 Biamonte, J. et al. Quantum machine learning. Nature
549, 195 .
 Romero, J., Olson, J. P. & Aspuru-Guzik, A. Quantum
autoencoders for eﬃcient compression of quantum data.
Quantum Science and Technology 2, 045001 .
 Wan, K. H., Dahlsten, O., Kristj´ansson, H., Gardner, R.
& Kim, M. Quantum generalisation of feedforward neural
networks. arXiv preprint arXiv:1612.01045 .
 Temme, K., Bravyi, S. & Gambetta, J. M. Error mitigation for short-depth quantum circuits. Physical review
letters 119, 180509 .
 Li, Y. & Benjamin, S. C. Eﬃcient variational quantum
simulator incorporating active error minimization. Physical Review X 7, 021050 .
 Terhal, B. M. & DiVincenzo, D. P.
Adaptive quantum computation, constant depth quantum circuits and
arthur-merlin games.
Quant. Inf. Comp. 4, 134–145
 Bremner,
Montanaro,
computations.
//doi.org/10.22331/q-2017-04-25-8.
 Vapnik, V.
The nature of statistical learning theory
 .
 Rebentrost, P., Mohseni, M. & Lloyd, S. Quantum support vector machine for big data classiﬁcation. Physical
review letters 113, 130503 .
 Kandala, A. et al. Hardware-eﬃcient variational quantum eigensolver for small molecules and quantum magnets. Nature 549, 242 .
 Farhi, E., Goldstone, J., Gutmann, S. & Neven, H.
Quantum algorithms for ﬁxed qubit architectures. arXiv
 
 Rigetti, C. & Devoret, M. Fully microwave-tunable universal gates in superconducting qubits with linear couplings and ﬁxed transition frequencies.
Phys. Rev. B
81, 134507 .
URL 
10.1103/PhysRevB.81.134507.
 Burges, C. J. A tutorial on support vector machines for
pattern recognition. Data mining and knowledge discovery 2, 121–167 .
 Boser, B. E., Guyon, I. M. & Vapnik, V. N. A training
algorithm for optimal margin classiﬁers. In Proceedings
of the ﬁfth annual workshop on Computational learning
theory, 144–152 .
 Goldberg, L. A. & Guo, H. The complexity of approximating complex-valued ising and tutte partition functions. computational complexity 26, 765–833 .
 Demarie, T. F., Ouyang, Y. & Fitzsimons, J. F. Classical veriﬁcation of quantum circuits containing few basis
changes. Physical Review A 97, 042319 .
 Spall, J. C.
A one-measurement form of simultaneous
perturbation stochastic approximation. Automatica 33,
109 .
 Spall, J. C. Adaptive stochastic approximation by the
simultaneous perturbation method.
IEEE Transaction
on Automatic Control 45, 1839 .
 Kandala, A. et al. Extending the computational reach
of a noisy superconducting quantum processor.
 
 Buhrman, H., Cleve, R., Watrous, J. & De Wolf, R.
Quantum ﬁngerprinting.
Physical Review Letters 87,
167902 .
 Cincio, L., Suba¸sı, Y., Sornborger, A. T. & Coles, P. J.
Learning the quantum algorithm for state overlap. arXiv
 
 Smolin, J. A., Gambetta, J. M. & Smith, G. Eﬃcient
method for computing the maximum-likelihood quantum
state from measurements with additive gaussian noise.
Physical review letters 108, 070502 .
 Schuld, M. & Killoran, N. Quantum machine learning in
feature hilbert spaces. arXiv preprint arXiv:1803.07128
 Schuld, M., Bocharov, A., Svore, K. & Wiebe, N.
Circuit-centric
classiﬁers.
arXiv preprint
 
 Stoudenmire, E. & Schwab, D. J. Supervised learning
with tensor networks. In Advances in Neural Information
Processing Systems, 4799–4807 .
 Van Dam, W., Hallgren, S. & Ip, L. Quantum algorithms
for some hidden shift problems. SIAM Journal on Computing 36, 763–778 .
 R¨otteler, M. Quantum algorithms for highly non-linear
boolean functions.
In Proceedings of the twenty-ﬁrst
annual ACM-SIAM symposium on Discrete algorithms,
448–457 .
 Bremner, M. J., Montanaro, A. & Shepherd, D. J.
Average-case complexity versus approximate simulation
of commuting quantum computations.
Physical review
letters 117, 080501 .
 d’Alessandro, D.
Introduction to quantum control and
dynamics .
 Tropp, J. A. et al. An introduction to matrix concentration inequalities. Foundations and Trends
⃝in Machine
Learning 8, 1–230 .
 Bergeal, N. et al. Analog information processing at the
quantum limit with a josephson ring modulator. Nature
Physics 6, 296 EP – . URL 
10.1038/nphys1516. Article.
 McKay, D. C., Wood, C. J., Sheldon, S., Chow, J. M. &
Gambetta, J. M. Eﬃcient z gates for quantum computing. Physical Review A 96, 022330 .
 Gambetta, J. M. et al. Characterization of addressability
by simultaneous randomized benchmarking. Phys. Rev.
Lett. 109, 240504 . URL 
doi/10.1103/PhysRevLett.109.240504.
 C´orcoles, A. D. et al. Process veriﬁcation of two-qubit
quantum gates by randomized benchmarking. Phys. Rev.
A 87, 030301 . URL 
10.1103/PhysRevA.87.030301.
 Sheldon, S., Magesan, E., Chow, J. M. & Gambetta,
J. M. Procedure for systematically tuning up cross-talk
in the cross-resonance gate.
Phys. Rev. A 93, 060302
SUPPLEMENTARY INFORMATION:
SUPERVISED LEARNING WITH QUANTUM ENHANCED FEATURE SPACES
Classiﬁcation problems
Consider a classiﬁcation task on a set C = {1, 2 . . . c} of c classes (labels) in a supervised learning scenario. In such
settings, we are given a training set T and a test set S, both are assumed to be labeled by a map m : T ∪S →C
unknown to the programmer. Both sets S and T are provided to the programmer, but the programmer only receives
the labels of the training set. So, formally, the programmer has only access to a restriction m|T of the indexing map
m|T : T →C, s.t.: m|T (⃗t) = m(⃗t), ∀⃗t ∈T.
It is the programmer’s goal to use the knowledge of m|T to infer an indexing map ˜m : S →C over the set S, such
that m(⃗s) = ˜m(⃗s) with high probability for any ⃗s ∈S. The accuracy of the approximation to the map is quantiﬁed
by a classiﬁcation success rate, proportional to the number of collisions of m and ˜m:
νsucc. = |{⃗s ∈S|m(⃗s) = ˜m(⃗s)}|
For such a learning task to be meaningful it is assumed that there is a correlation in output of the indexing
map m over the sets S and T.
For that reason, we assume that both sets could in principle be constructed
by drawing the S and T sample sets from a family of d-dimensional distributions
pc : Ω⊂Rd →R
labeling the outputs according to the distribution.
It is assumed that the hypothetical classiﬁcation function m
to be learned is constructed this way.
The programmer, however, does not have access to these distributions of
the labeling function directly. She is only provided with a large, but ﬁnite number of samples and the matching labels.
The conventional approach to this problem is to construct a family of classically computable function ˜m : ⟨⃗θ, S⟩→C,
indexed by a set of parameters ⃗θ. These weights are then inferred from m|T by a optimization procedure on a classical
cost function. We consider a scenario where the whole, or parts of the classiﬁcation protocol m, are generated on a
quantum computer.
Description of the Algorithm
We consider two diﬀerent learning schemes. The ﬁrst is referred to as “Quantum variational classiﬁcation”, the
second is referred to “Quantum kernel estimation”. Both schemes construct a separating hyperplane in the state
space of n qubits. The classical data is mapped to this space with dim = 4n using a unitary circuit family starting
from the reference state | 0⟩⟨0 |n.
Quantum variational classiﬁcation
For our ﬁrst classiﬁcation approach we design a variational algorithm which exploits the large dimensional Hilbert
space of our quantum processor to ﬁnd an optimal cutting hyperplane in a similar vein as Support Vector Machines
(SVM) do. The algorithm consists of two main parts: a training stage and a classiﬁcation stage. For the training
stage, a set of labeled data points are provided, on which the algorithm is performed. For the classiﬁcation stage, we
take a diﬀerent set of data points and run the optimized classifying circuit on them without any label input. Then
we compare the label of each data point to the output of the classiﬁer to obtain a success ratio for the data set. For
both the training and the classiﬁcation stages, the quantum circuit that implements the algorithm comprises three
main parts: the encoding of the feature map, the variational optimization and the measurement, Fig S1. The training
phase consists of these steps.
Quantum variational classiﬁcation: The circuit takes a references state, | 0⟩n, applies the unitary UΦ(vecx) followed by the variational unitary W(⃗θ) and applies a measurement in the Z-basis.
The resulting bit string z ∈{0, 1}n
is then mapped to a label in C.
This circuit is re - run multiple times and sampled to estimate the expectation value
py = ⟨Φ(⃗x) |W †(⃗θ)MyW(⃗θ)| Φ(⃗x)⟩for the labels y ∈C. In the experiment we consider C = {+1, −1}.
Algorithm 1 Quantum variational classiﬁcation: the training phase
1: Input Labeled training samples T = {⃗x ∈Ω⊂Rn} × {y ∈C}, Optimization routine,
2: Parameters Number of measurement shots R, and initial parameter ⃗θ0.
3: Calibrate the quantum Hardware to generate short depth trial circuits.
4: Set initial values of the variational parameters ⃗θ = ⃗θ0 for the short-depth circuit W(⃗θ)
5: while Optimization (e.g. SPSA) of Remp(⃗θ) has not converged do
for i = 1 to |T|
Set the counter ry = 0 for every y ∈C.
for shot = 1 to R
Use UΦ(⃗xi) to prepare initial feature-map state | Φ(⃗xi)⟩⟨Φ(⃗xi) |
Apply discriminator circuit W(⃗θ) to the initial feature-map state .
Apply |C| - outcome measurement {My}y∈C
Record measurement outcome label y by setting ry →ry + 1
Construct empirical distribution ˆpy(⃗xi) = ryR−1.
Evaluate Pr ( ˜m(⃗xi) ̸= yi|m(⃗x) = yi) with ˆpy(⃗xi) and yi
Add contribution Pr ( ˜m(⃗xi) ̸= yi|m(⃗x) = yi) to cost function Remp(⃗θ).
Use optimization routine to propose new ⃗θ with information from Remp(⃗θ)
19: end while
20: return the ﬁnal parameter ⃗θ∗and value of the cost function Remp(θ∗)
The classiﬁcation can be applied when the training phase is complete. The optimal parameters are used to decide
the correct label for new input data. Again, the same circuit is applied as in Fig S1, however, this time the parameters
are ﬁxed and and the outcomes are combined to determine the label which is reported as output of the classiﬁer.
Algorithm 2 Quantum variational classiﬁcation: the classiﬁcation phase
1: Input An unlabeled sample from the test set ⃗s ∈S, optimal parameters ⃗θ∗for the discriminator circuit.
2: Parameters Number of measurement shots R
3: Calibrate the quantum Hardware to generate short depth trial circuits.
4: Set the counter ry = 0 for every y ∈C.
5: for shot = 1 to R
Use UΦ(⃗s) to prepare initial feature-map state | Φ(⃗s)⟩⟨Φ(⃗s) |
Apply optimal discriminator circuit W( ⃗θ∗) to the initial feature-map state .
Apply |C| - outcome measurement {My}y∈C
Record measurement outcome label y by setting ry →ry + 1
10: end for
11: Construct empirical distribution ˆpy(⃗s) = ryR−1.
12: Set label = argmaxy{ˆpy(⃗s)}
13: return label
Quantum kernel estimation
For the second classiﬁcation protocol, we restrict ourselves to the binary label case, with C = {+1, −1}. In this
protocol we only use the quantum computer to estimate the |T| × |T| kernel matrix K(⃗xi, ⃗xj) = | ⟨Φ(⃗xi) | Φ(⃗xj) ⟩|2.
For all pairs of points ⃗xi, ⃗xj ∈T in the the training data, we sample the overlap to obtain the matrix entry in the
kernel. This output probability can be estimated from the circuit depicted in Fig.
S5.b. by sampling the output
distribution with R shots and only taking the 0n count.
After the kernel matrix for the full training data has
been constructed we use the conventional (classical) support vector machine classiﬁer. The optimal hyperplane is
constructed by solving the dual problem LD in eqn. (6), which is completely speciﬁed after we have been given the
labels yi and have estimated the kernel K(⃗xi, ⃗xj). The solution of the optimization problem is given in terms of the
support vectors NS for which αi > 0.
In the classiﬁcation phase, we want to assign a label to a new datum ⃗s ∈S of the test set. For this, the inner
product K(⃗xi,⃗s) between all support vectors ⃗xi ∈T with i ∈NS and the new datum ⃗s has to be estimated on the
quantum computer. The new label ˜m(⃗s) for the datum is assigned according to eqn. (14). Since all support vectors
are known from the training phase and we have obtained access to the kernel K(⃗xi,⃗s) from the quantum hardware,
the label can be directly computed.
The Relationship of variational quantum classiﬁers to support vector machines
The references provide a detailed introduction to the construction of support vector machines for pattern
recognition. Support vector machines are an important tool to construct classiﬁers for tasks in supervised learning. We
will show that the variational circuit classiﬁer bears many similarities to a classical non-linear support vector machine.
Support vector machines (SVM):
First, let us brieﬂy review the training task of classical, linear support vector machines for data where C = {+1, −1},
so that (⃗xi, yi)i=1,...,t with ⃗xi ∈T ⊂Rd, yi ∈{+1, −1} that is linearly separable. Linear separability asks that the
set of points can be split in two regions by a hyperplane (w, b), parametrized by a normal vector w ∈Rd and a bias
b ∈R. The points ⃗x ∈Rd that lie directly on the hyperplane satisfy the equation
w ◦⃗x + b = 0
expressed in terms of the inner product ◦for vectors in Rd. The perpendicular distance of the hyperplane to the
origin in Rn is given by b∥w∥−1. The data set {xi, yi} is linearly separable by margin 2||w||−1 in Rd if there exists a
vector w and a b, such that:
yi (w ◦⃗xi + b) ≥1.
∀i = 1, . . . , t
The classiﬁcation function ˜m(⃗x, (w, b)) that is constructed from such a hyperplane for any new data point ⃗x ∈Rn
assigns the label according to which side of the hyperplane the new data-point lies by setting
˜m(⃗x, (w, b)) = sign (w ◦⃗x + b) .
The task in constructing a linear support vector machine (SVM) in this scenario is the following. One is looking
for a hyperplane that separates the data, with the largest possible distance between the two separated sets. The
perpendicular distance between the plane and two points with diﬀerent labels is called a margin and such points are
referred to as ‘support vectors’. This means that we want to maximize the margin by minimizing ||w||, or equivalently
||w||2 subject to the constraints as given in eqn. (2), for all data points in the training set T. The corresponding cost
function can be written as:
αiyi(w ◦⃗xi + b) +
where αi ≥0 are Lagrange multipliers chosen to ensure the constraints are satisﬁed.
For non-separable datasets, it is possible to introduce non-negative slack variables {ξi}i=1,...,t ∈R+
0 which can be
used to soften the constraints for linear separability of (⃗xi, yi) to
yi (w ◦⃗xi + b) ≥(1 −ξi),
These slack variables are then used to modify the objective function by 1/2∥w∥2 →1/2∥w∥2 + C(P
i ξi)r + P
When we choose r ≥1 the optimization problem remains convex and a dual can be constructed. In particular, for
r = 1, neither the ξi or their Lagrange multipliers µi appear in the dual Lagrangian.
It is very helpful to consider the dual of the original primal problem LP in eqn. (4). The primal problem is a
convex, quadratic programming problem, for which the Wolfe dual cost function LD for the Lagrange multipliers can
be readily derived by variation with respect to w and b. The dual optimization problem is
αiαjyiyj⃗xi ◦⃗xj,
subject to constraints:
The variables of the primal are given in terms of the dual variables by
αiyi⃗xi = w
and the bias b can be computed from the Karush-Kuhn-Tucker (KKT) conditions when the corresponding Lagrange
multiplier does not vanish. The optimal variables satisfy the KKT conditions and play an important role in the
understanding of the SVM. They are given for primal as
∂wβLP = wβ −
αiyi⃗xiβ = 0
β = 1, . . . , d.
yi (⃗x ◦w + b) −1 ≥0
αi (yi(w ◦⃗xi + b) −1) = 0.
Note that the condition eqn. (12) ensures that either the optimal αi = 0 or the corresponding constraint eqn. (12) is
tight. This is a property referred to as complementary slackness, and indicates that only the vectors for which the
constraint is tight give rise to non-zero αi > 0. These vectors are referred to as the support vectors and we will write
NS for their index set. The classiﬁer in the dual picture is given by substituting w from eqn. (7) and b into the
classiﬁer eqn. (3). The bias b is obtained for any i ∈NS from the equality in eqn. (11).
The method can be generalized to the case when the decision function does depend non-linearly on the data by
using a trick from and introducing a high-dimensional, non-linear feature map. The data is mapped via
from a low dimensional space non-linearly in to a high dimensional Hilbert-space H. This space is commonly referred
to as the feature space. If a suitable feature map has been chosen, it is then possible to apply the SVM classiﬁer for
the mapped data in H, rather than in Rd.
It is important to note that it is in fact not necessary to construct the mapped data Φ(⃗xi) in H explicitly. Both the
training data, as well as the new data to be classiﬁed enters only through inner products, in both the optimization
problem for training, c.f. eqn. (6), as well as in the classiﬁer, eqn. (3). Hence, we can construct the SVM for
arbitrarily high dimensional feature maps (Φ, H), if we can eﬃciently evaluate the inner products Φ(⃗xi) ◦Φ(⃗xj) and
Φ(⃗xi) ◦Φ(⃗s), for ⃗xi ∈T and ⃗s ∈S. In particular, if we can ﬁnd a kernel K(⃗x, ⃗y) = Φ(⃗x) ◦Φ(⃗y) that satisﬁes Mercer’s
condition (which ensures that the kernel is positive semi-deﬁnite and can be interpreted as matrix of inner products)
 , we can construct a classiﬁer by setting
˜m(⃗s) = sign
αiyiK(⃗xi,⃗s) + b
Here we only need to sum over all support vectors i ∈NS for which αi > 0. Moreover, one can replace the inner
product in the optimization problem eqn. (6) by the kernel. Examples of such kernels that are frequently considered
in the classical literature are for instance the polynomial kernel K(⃗x, ⃗y) = (⃗x ◦⃗y + 1)d or even the inﬁnite dimensional
Gaussian kernel K(⃗x, ⃗y) = exp(−1/2∥⃗x −⃗y∥2).
If the feature map is suﬃciently powerful, increasingly complex
distributions can be classiﬁed.
In this paper, the feature map is a classical to quantum mapping by a tunable
quantum circuit family, that maps Φ : Rd →S(H⊗n
2 ) in to the state space, or space of density matrices, of n qubits
= 4n. The example of the Gaussian kernel indicates, that the sheer dimension of the Hilbert
space on a quantum computer by itself does not provide an advantage, since classically even inﬁnite dimensional
spaces are available by for instance using the Gaussian kernel. However, this hints towards a potential source of
quantum advantage as we may construct states in feature space with hard-to-estimate overlaps.
Variational circuit classiﬁers:
Let us now turn to the case of binary classiﬁcation based on variational quantum circuits. Recall that in our setting,
we ﬁrst take the data ⃗x ∈Rd and map it to a quantum state | Φ(⃗x)⟩⟨Φ(⃗x) | ∈S(H⊗n
2 ) on n-qubits, c.f. eqn. (22).
Then we apply a variational circuit W(⃗θ) to the initial state that depends on some variational parameters ⃗θ, c.f. eqn.
(31). Lastly, for a binary classiﬁcation task, we measure the resulting state in the canonical Z-basis and assign the
resulting bit-string z ∈{0, 1}n to a label based on a predetermined boolean function f : {0, 1}n →{+1, −1}. Hence
the probability of measuring either label y ∈{+1, −1} is given by:
 1 + y⟨Φ(⃗x) |W †(θ) f W(θ)| Φ(⃗x)⟩
where we have deﬁned the diagonal operator
f(z)| z⟩⟨z |.
In classiﬁcation tasks we assign c.f. eqn. (34), the label with the highest empirical weight of the distribution py. We
ask whether the outcome +1 is more likely than −1, or vice versa. That is, we ask, whether p+1 > p−1 −b or whether
the converse is true. This of course depends on the sign of the expectation value ⟨Φ(⃗x) |W †(θ, ϕ) f W(θ, ϕ)| Φ(⃗x)⟩for
the data point ⃗x.
To understand how this relates to the SVM in greater detail, we need to choose an orthonormal operator basis,
such as for example the Pauli group
Pn = ⟨Xi, Yi, Zi⟩i=1,...,n .
Note that when ﬁxing the phase to +1 every element Pα ∈Pn , with α = 1, . . . , 4n of the Pauli-group is an orthogonal
reﬂection P 2
α = 1. Furthermore, Pauli matrices are mutually orthogonal in terms of the trace inner product
tr [PαPβ] = δα,β2n.
This means that both the measurement operator W †(θ) f W(θ) in the W-rotated frame as well as the state
| Φ(⃗x)⟩⟨Φ(⃗x) | can be expanded in terms of the operator basis with only real coeﬃcients as
W †(θ, ϕ) f W(θ, ϕ) = 1
wα(θ, ϕ)Pα
wα(θ, ϕ) = tr
W †(θ, ϕ) f W(θ, ϕ)Pα
| Φ(⃗x)⟩⟨Φ(⃗x) | = 1
Φα(⃗x) = tr [| Φ(⃗x)⟩⟨Φ(⃗x) |Pα] .
Note, that the values wα(θ, ϕ) as well as Φα(⃗x) are constrained due to the fact that they originate from a rotated
projector and from a pure state. Since f 2 = 1, we have that
h W †(θ, ϕ) f W(θ, ϕ)
= 2n. Furthermore, the projector squares to itself so that
| Φ(⃗x)⟩⟨Φ(⃗x) |2
= 1. In particular, this means that the norms of both vectors satisfy
α(⃗x) = 2n as well as P
α(θ, ϕ) = 4n.
Since the expectation value of the measured observable is
⟨Φ(⃗x) |W †(θ, ϕ)fW(θ, ϕ)| Φ(⃗x)⟩= tr
| Φ(⃗x)⟩⟨Φ(⃗x) |W †(θ, ϕ) f W(θ, ϕ)
, it can be expressed in terms of the inner
⟨Φ(⃗x) |W †(θ) f W(θ)| Φ(⃗x)⟩= 1
wα(θ)Φα(⃗x).
Observe that f only has eigenvalues +1, −1, and we have that ⟨Φ(⃗x) |W †(θ) f W(θ)| Φ(⃗x)⟩∈[−1, +1]. Let us now
consider a decision rule, where we assign the label y ∈{+1, −1} over the label −y with some ﬁxed bias b ∈[−1, +1].
In that case we demand that py > p−y −yb. If we substitute eqn. (15) and use the expansion in eqn. (20) we have
that the corresponding label is given by the decision function y = ˜m(⃗x), where
˜m(⃗x) = sign
wα(θ)Φα(⃗x) + b
This expression is identical to the conventional SVM classiﬁer, c.f. eqn. (3), after the feature map has been applied.
However, in the experiment we only have access to the probabilities py through estimation. Furthermore, the wα(θ)
are constrained to stem from the observable f measured in the rotated frame.
This means, that the correct feature space, where a linearly separating hyperplane is constructed is in fact the
quantum state space of density matrices, and not the Hilbert space Hn
2 itself. This is reasonable, since the physical
states in Hn
2 are only deﬁned up to a global phase | ψ⟩∼eiη| ψ⟩. The equivalence of states up to a global phase would
make it impossible to ﬁnd a separating hyperplane, since both | ψ⟩and −| ψ⟩give rise to the same physical state but
can lie on either side of a separating plane.
Encoding of the data using a suitable feature map
In the quantum setting, the feature map is an injective encoding of classical information ⃗x ∈Rd into a quantum
state | Φ⟩⟨Φ | on an n-qubit register. Here H2 = C2 is a single qubit Hilbert space, and S
denotes the cone of
positive semideﬁnite density matrices ρ ≥0 with unit trace tr [ρ] = 1. This cone is a subset of the 4n dimensional
Hilbert space of M2n×2n(C) of complex matrices when ﬁtted with the inner product tr
for A, B ∈M2n×2n(C).
The feature map acts as
Φ : Ω⊂Rd →S
Φ : ⃗x 7→| Φ(⃗x)⟩⟨Φ(⃗x) |.
The action of the map can be understood by a unitary circuit family denoted by UΦ(⃗x) that is applied to some
reference state, e.g. | 0⟩n. The resulting state is given by | Φ(⃗x)⟩= UΦ(⃗x)| 0⟩n. The state in the feature space should
depend non-linearly on the data. Let us discuss proposals for possible feature maps
Product state feature maps
There are many choices for the feature map Φ. Let us ﬁrst discuss what would happen
if we were to choose a feature map that corresponds to a product input state. We assume a feature map, comprised of
single qubit rotations U(ϕ) ∈SU(2) on every qubit on the quantum circuit. The angles for every qubit can be chosen
as a non-linear function ϕ : ⃗x →(0, 2π]2 × [0, π] into the space of Euler angles for the individual qubits, so that the
full feature map can be implemented as:
⃗x 7→| φi(⃗x)⟩= U(ϕi(⃗x))| 0⟩,
for an individual qubit, so that
Φ : ⃗x 7→| Φ(⃗x)⟩⟨Φ(⃗x) | =
| φi(x)⟩⟨φi(x) |
for the full qubit state.
One example for such an implementation is the unitary implementation of the feature map used in the context of
the classical classiﬁers by Stoudenmire and Schwab based on tensor networks. There each qubit encodes a single
component xi of ⃗x ∈ n so that n qubits are used. The resulting state that that is prepared is then
| φi(x)⟩⟨φi(x) | = 1
j (θj(⃗x)) Pαj
when expanded in terms of the Pauli-matrix basis where Φα
i (θi(⃗x)) = ⟨φi(x) |Pαi| φi(x)⟩for all i = 1, . . . n. and
Pαi ∈{1, Xi, Zi, Yi}. The corresponding decision function can be constructed as in eqn. (14), where the kernel
K(⃗x, ⃗y) = Qn
i=1 | ⟨φi(⃗x) | φi(⃗y) ⟩|2 is replaced by the inner product between the resulting product states. These can
be evaluated with resources scaling linearly in the number of qubits, so that no quantum advantage can be expected
in this setting.
Non-trivial feature map with entanglement
There are many choices of feature maps, that do not suﬀer from the malaise of the aforementioned product state feature maps.
To obtain an quantum advantage we would like these maps to give rise to a kernel
K(⃗x, ⃗y) = | ⟨Φ(⃗x) | Φ(⃗y) ⟩|2 that is computationally hard to estimate up to an additive polynomially small error
by classical means. Otherwise the map is immediately amenable to classical analysis and we are guaranteed to have
lost any conceivable quantum advantage.
Let us therefore turn to a family of feature maps, c.f. Fig S2 for which we conjecture that it is hard to estimate the
overlap | ⟨Φ(⃗x) | Φ(⃗y) ⟩|2 on a classical computer. We deﬁne the family of feature map circuit as follows
| Φ(⃗x)⟩= UΦ(⃗x) H⊗n UΦ(⃗x)H⊗n| 0⟩⊗n
UΦ(⃗x) = exp
where now the 2n possible coeﬃcients φS(⃗x) ∈R are non-linear functions of the input data ⃗x ∈Rn. It is convenient to
use maps with low-degree expansions, i.e. |S| ≤d. Any such map can be eﬃciently implemented. In the experiments
reported in this paper we have restricted to d = 2.
So we only consider Ising type interactions in the unitaries
UΦ(⃗x). In particular we choose these interactions as the ones that are present in the actual connectivity graph of the
superconducting chip G = (E, V ). This ensures that the feature map can be generated from a short depth circuit.
The resulting unitary can then be generated from one- and two- qubit gates of the form
Uφ{l,m}(⃗x) = exp
 iφ{k,l}(⃗x)ZkZl
Uφ{k}(⃗x) = exp
 iφ{k}(⃗x)Zk
which leaves |V | + |E|, real parameters to encode the data. In particular, we know that we have at least |V | = n real
numbers to encode the data. Furthermore, depending on the connectivity of the interactions, we have |E| ≤n(n−1)/2
further parameters that can be used to encode more data or nonlinear relations of the initial data points.
A circuit representation of the feature map family we consider here. We ﬁrst apply a series of Hadamard gates before
applying the diagonal phase gate component. Then we apply a second layer of Hadamard gates, followed by the same diagonal
phase gate. This encodes both the actual function value of the phase Φ⃗x(z) as well as the value of the ZN
2 Fourier transform
ˆΦ⃗x(S) for every basis element.
This feature map encodes both the actual function Φ⃗x(z) of the diagonal phases, as well as the corresponding
Fourier-Walsh transform ˆΦ⃗x(p) at z, p ∈{0, 1}n
Φ⃗x(z) = exp
ˆΦ⃗x(p) = 1
Φ⃗x(z)(−1)p◦z,
for every basis element respectively. The resulting state, after the datum ⃗x ∈Rd has been mapped to the feature
space, is given by
Φ⃗x(p)ˆΦ⃗x(p)| p⟩.
We conjecture that it is hard to estimate this kernel up to an additive polynomially small error by classical means.
The intuition for the conjecture stems from a connection of the feature map to a particular circuit family for the
hidden shift problem for boolean functions . The feature map is similar to a quantum algorithm for estimating
the hidden shift in boolean bent functions, c.f. Ref. , algorithm A1 in Theorem 6. The circuit UΦ we consider is
indeed very similar to the one discussed in . Let us make a minor modiﬁcation, and ask the two diagonal layers in
˜UΦ = UΦ2H⊗nUΦ1H⊗n to diﬀer. We choose the phase gate layers so that UΦ1 and UΦ2 encode a shifted bent-function
and the function’s dual, then the circuit A1 is given by H⊗n ˜UΦ. If the interaction graph G is bi-partite, it is possible
to encode Maiorana-McFarland bent-functions for d = 2 by choosing the corresponding φ{k,l}(⃗x), φk(⃗x) to be either
π or 0. In , the diagonal layer in A1, are queries to an oracle encoding the the shifted bent-function and it’s dual.
It can be shown that with respect to this oracle there is an exponential separation in query complexity over classical
query algorithms. The ﬁnal circuit we implement to estimate the overlap | ⟨Φ(⃗x) | Φ(⃗y) ⟩|2 is still larger. This circuit
has four layers of Hadamard gates and 3 diagonal layers. If the conjecture could be proven, it would establish a
valuable step in rigorously establishing a quantum advantage on near-term devices. If, however, it ultimately turns
out that this family can also be evaluated classically, we would need to improve the complexity of this circuit family.
A good example of a circuit family that entails a hardness result, yet does not provide an advantage in our setting is
the following: One could consider the case, where only a single layer of Hadamard gates and a single diagonal unitary
is applied. Such a feature map is directly related to the circuit family introduced in . Indeed the resulting kernel
K(⃗x, ⃗y) = |⟨0n |H⊗nUΦ(⃗y)UΦ(⃗x)H⊗n| 0n⟩|2 corresponds to the output probability of the considered IQP circuits. In fact
it is known that if general real values φS(⃗x) are allowed that d = 2 suﬃces to encode #P-hard problems in the output
probability . This hardness result, only applies to the case where the output probability can be approximated up to
a multiplicative error. A noisy quantum computer, however, is also not able to provide a multiplicative error estimate
to this quantity. Nevertheless, Bremner et al. show, contingent on additional complexity theoretic conjectures, that
it is hard for a classical sampler to produce samples from the output distribution of the IQP circuit. Does this result
imply some form of hardness for the feature map version of this circuit? The answer is unfortunately no. The kernel
can be estimated up to an additive error of ϵ by drawing R = ϵ−2 samples from the uniform distribution over n
classical bits and averaging ˜Φ⃗x−⃗y(z) = Φ⃗x(z)Φ⃗y(z), c.f. eqn. (28)
|⟨0n |H⊗nUΦ(⃗y)UΦ(⃗x)H⊗n| 0n⟩|2 =
˜Φ⃗x−⃗y(z)
Since we have that the variance of the random variable is bounded by 1 since |Φ⃗x(z)|2 = 1 , we get an additive error
that scales as O(ϵ). This means for a single layer, the kernel can be estimated classically.
Quantum variational classiﬁcation
Following the structure of the feature map circuit, we construct the classiﬁer part of the variational algorithm by
appending layers of single-qubit unitaries and entangling gates Fig. S3.a. Each subsequent layer, or depth, contains
an additional set of entanglers across all the qubits used for the algorithm. We use a coherently controllable quantum
mechanical system, such as for example the superconducting chip with n transmon qubits to prepare a short depth
quantum circuit W(⃗θ). In the experiment here, comprising n = 2 qubits, one controlled-phase gate is added per
depth. The single-qubit unitaries used in the classiﬁer are limited to Y and Z rotations to simplify the number of
parameters to be handled by the classical optimizer. Our use of controlled-phase, rather than CNOT, gates for the
entanglers is justiﬁed by our aim at increased generality in our software. Using controlled-phase gates does not require
to particularize this part of the algorithm for diﬀerent systems topologies. A speciﬁc entangling map for a given device
can then be used by our compiler to translate each controlled-phase gate into the CNOTs available in our system.
The general circuit is comprised of the following sequence of single qubit and multi-qubit gates:
W(⃗θ) = U (l)
loc(θl) Uent . . . U (2)
loc (θ2) Uent U (1)
We apply a circuit of l repeated entanglers as depicted in Fig S3.b and interleave them with layers comprised of local
single qubit rotations:
loc(θt) = ⊗n
m=1U(θm,t)
U(θm,t) = ei 1
parametrized by θt ∈R2n and θi,t ∈R2. In principle, there exist multiple choices for the entangling unitaries Uent
 . For the feature map that we consider, however, we use the entangler that is comprised of products of control
phase gates CZ(i, j) between qubits i and j. The entangling interactions follow the interaction graph that G = (E, V )
that was used to generate the feature map in eqn. (27).
This short-depth circuit can generate any unitary if suﬃciently many layers d are applied. The circuit Fig. S3.a
(a) Circuit representation of short depth quantum circuit to deﬁne the separating hyperplane. The single qubit
rotations U(θi,t) ∈SU(2) are depicted by single line boxes parametrized by the angles θi, while the entangling operation Uent
is determined by the interaction graph of the superconducting chip. (b) Depiction of entangling gate as a product of CZi,i+1
for i = 1, . . . , 5 gates following the interaction graph of a circle G = C5.
can be understood as a bang-bang controlled evolution of an Ising model Hamiltonian H0 = P
(ij)∈E JijZiZj,c.f.
Fig. S3.b, interspersed with single qubit control pulses in SU(2) on every qubit. It is known that this set of drift
steps together with all single control pulses are universal, so we have that any state can be prepared this way with
suﬃcient circuit depth . For a general unitary gate sequence the entangling unitary has to be eﬀectively generated
from cross resonance gates, by applying single qubit control pulses.
Choosing the cost-function for the circuit optimization
The central goal is to ﬁnd an optimal classifying circuit W(⃗θ), c..f. eqn. (31) that separates the data sets with
diﬀerent labels. Since we can re-run the same classifying multiple times (R shots), we may consider a ‘winner takes
all’ scenario, where we assign the label according to the outcome with the largest probability. We choose a cost
function, so that the optimization procedure minimizes the probability of assigning the wrong label after having
constructed the distribution after R shots.
There are multiple ways of performing a multi-label classiﬁcation. We only need to modify the ﬁnal measurement
M, to correspond to multiple partitions. This can be achieved by multiple strategies. For example one could choose
to measure again in the computational basis, i.e. the basis in which Pauli Z are diagonal and then constructing
classical labels form the measured samples, such as a labeling the outcome z ∈{0, 1}n according to a function
f : {0, 1}n →{1, . . . , c}. The resulting {My}y=1,...,c is therefore diagonal in the computational basis. Alternatively
one could construct a commuting measurement akin to the syndrome check measurement for quantum stabilizers. For
this approach we choose a set {gi}i=1...⌈log2(c)⌉of Pauli matrices gi ∈PN that are commuting [gi, gj] = 0. The resulting
measurement that would need to be performed is similar to that of an error correcting scheme. The measurement
operators are given by My = 2−1 
1 −Q⌈log2(c)⌉
. Here yi denotes the i’th bit in the binary representation of y.
In either case, the decision rule that assigns the labels can be written as
˜m|T (⃗x) = arg max
c′ ⟨Φ(⃗x) |W(⃗θ)†Mc′W(⃗θ)| Φ(⃗x)⟩.
This corresponds to taking R shots in order to estimate the largest outcome probability from the outcome statistics
of the measurement My for y = 1, . . . , c. Labelling Tc the subset of samples T labelled with c, the overall expected
misclassiﬁcation rate is given by:
 ˜m|T (s) ̸= c|s ∈Tc
The error probability of misclassifying an individual datum is given by Pr
 ˜m|T (s) ̸= c|s ∈Tc
. This error probability
is now used to deﬁne the empirical risk function Remp(⃗θ) = Perr. We now discuss of how to ﬁnd suitable ways of
evaluating the cost function for this classiﬁcation scheme.
Binary label classiﬁcation
Assume the programmer classiﬁes into labels y ∈{−1, 1} by taking R shots for a single datapoint. She obtains an
empirical estimates of probability of the datum being labeled by a label y
After R = ry + r−y shots and a prior bias b, she misclassiﬁes into a label y if
ˆpy < ˆp−y + yb →ry < r−y + ybR →ry <
The probability of her misclassifying a y sample according to the argmax rule is hence estimated by
 ˜m|T (s) ̸= y|s ∈Ty
= Pr (ry < r−y + yb) =
y(1 −py)R−j .
Assuming large R, computing this exactly may be diﬃcult. Setting Rpy = a, Rpy(1 −py) = β2 and ⌈
we can approximate the binomial CDF as an error function:
 ˜m|T (s) ̸= y|s ∈Ty
(1 −pc)j ≈
dz e−z2 = 1
2(1 −py)py
See Fig. S4. The error function can be consequently approximated with a sigmoid
Single shot to multi-shot decision rule for classiﬁcation. The contribution to the cost-function interpolates from
linear to logistic-normal CDF (approximately sigmoid). In the experiment the data was sampled with R = O(103), although
the cost-function was evaluated with only ˜R = O(102) to provide a smoother function to the optimization routine.
1 + exp(−x) ≈1
2 (erf(x) + 1) ,
which gives
 ˜m|T (s) ̸= y|s ∈Ty
2(1 −py)py
as an estimate for misclassifying a sample s. The cost function to optimize is then given by using this in eqn. (35).
Multi label classiﬁcation
For multiple labels, one tries to optimize
 ˜m|T (s) ̸= c|s ∈Tc
 ˜m|T (s) ̸= c|s ∈Tc
 {nc′}c′/c
We consider the case of three labels. For R samples with frequencies {n0, n1, n2}, drawn independently from the
output probability distribution, the probability of misclassifying a sample s ∈T0 by argmax is given by
 ˜m|T (s) ̸= 0|s ∈T0
n0 < max(n1, n2)
N + |n1 −n2|
where the last inequality is derived as follows
2n0 < 2 max(n1, n2) = |n1 −n2| + n1 + n2 = |n1 −n2| + N −n0 .
Hence setting γ = N+|n1−n2|
, it follows that
 ˜m|T (s) ̸= 0|s ∈T0
0(1 −p0)N−k ≈sig
2N(1 −p0)p0
This however still depends on n1, n2, which can’t be simply eliminated. Additionally, for a general k-label case, there
is no simple analytic solution for γ. For this reason, we therefore try to estimate the above probability by simply
taking γ = maxc′  {nc′}c′/c
. So for k-label case, the cost function terms are given by
 ˜m|T (s) ̸= c|s ∈Tc
Rmaxc′  {nc′}c′/c
2(N −nc)nc
Quantum kernel estimation
For the second classiﬁcation method we only use the quantum computer to estimate the kernel K(⃗xi, ⃗xj) =
| ⟨Φ(⃗xi) | Φ(⃗xj) ⟩|2 for all the labeled training data ⃗xj ∈T.
Then we use the classical optimization problem as
outlined again in eqn. (6) to obtain the optimal Lagrange multipliers αi and support vectors NS can be obtained.
From this the classiﬁer can be constructed, c.f .eqn. (14). To apply the classiﬁer to a new datum ⃗s ∈S the kernel
K(⃗xi,⃗s) between ⃗s and the support vectors in i ∈NS has to be estimated. We discuss two methods to estimate this
overlap for our setting.
a) Estimating the expectation value of the SWAP- matrix as derived in . This circuit diagonalizes the SWAP gate, when it is treated as a hermitian observables with eigenvalues ±1. Averaging the eigenvalues, c.f. eqn. (36), with samples
from the output distribution constructs an estimator for | ⟨Φ(⃗x) | Φ(⃗y) ⟩|2. b) The ciruit estimates the ﬁdelity between two
states in feature space directly by ﬁrst applying the unitary UΦ(⃗x) followed by the inverse U†
Φ(⃗y) and measuring all bits at the
output of the circuit. The frequency ν0,...,0 = | ⟨Φ(⃗x) | Φ(⃗y) ⟩|2 of the all the zero outcome precisely corresponds to the desired
state overlap.
The usual method of estimating the ﬁdelity between two states is by using the swap test . This circuit, however,
is not a short depth circuit on a quantum computing architecture with geometrically local gates. It would require
a sequence of controlled SWAP, also known as Fredkin gates, all conditioned on the state of the same ancilla qubit.
A very nice protocol was recently developed in .
The authors have learned multiple ways of optimizing the
conventional swap test.
If only the value of the ﬁdelity is needed, as is the case for our algorithm, the authors
propose, c.f. section III.C, a circuit that is constant depth if pairs of CNOT gates can be executed in parallel.
This circuit, c.f. Fig S5.a, evaluates the expectation value ⟨ψ |⟨φ |SWAP| ψ⟩| φ⟩directly. The action of the algorithm
can be understood as follows:
The SWAP gate is both a unitary gate and a hermitian observable SWAP† = SWAP with eigenvalues ±1. The
expectation value on two product states as we said given by
⟨ψ |⟨φ |SWAP| ψ⟩| φ⟩= | ⟨φ | ψ ⟩|2.
Now the gate can be decomposed in to a product of two qubit swap gates
k=1 SWAPsktk that act all in parallel.
To evaluate the expectation value one has to diagonalize the
full gate.
This is achieved by diagonalizing the individual two-qubit swap gates by observing that SWAPij =
CNOTi→jCNOTj→iCNOTi→j. Furthermore using the circuit identity CNOTj→i = HjCZjiHj, we see that SWAPij is
diagonalized by CNOTj→iHj and has eigenvalue (−1)xixj. For the full circuit Fig S5.a one ﬁrst applies a transversal
set of CNOT gates across both registers followed by a single layer of Hadamard gates H on the top register. Then the
output is sampled and the average of the boolean function
f(s, t) = (−1)(s1t1+...sntn)
is reported. The output bits on the top register are labeled by s ∈{0, 1}n, while t ∈{0, 1}n is the output string on
the lower register.
| ψ⟩, | φ⟩.
structured
are all generated by the unitary eqn.
(26) as shown in Fig
Writing out the kernel explicitly
K(⃗y, ⃗x) = | ⟨Φ(⃗y) | Φ(⃗x) ⟩|2 = |⟨0n |U†
Φ(⃗y)UΦ(⃗x)| 0n⟩|2 gives the indication of how to measure it.
Simply apply
the circuit U†
Φ(⃗y)UΦ(⃗x) to the state | 0n⟩, c.f. Fig S5.b. Then sample R times the resulting state U†
Φ(⃗y)UΦ(⃗x)| 0n⟩in the
Z basis. Record the number of all observed zero (0, . . . , 0) bit-strings and divide by the total number of shots R. The
frequency ν(0,...,0) = #{(0, . . . , 0)}R−1 then gives an estimator for K(⃗y, ⃗x) up to a sampling error ˜ϵ = O(R−1/2). A
rough estimate for the operator norm ∥·∥of the deviation of the resulting estimator ˆK from the true kernel matrix K
is given by ∥K −ˆK∥≤∥K −ˆK∥F . Here ∥A∥F =
ij |Aij|2 denotes the Frobenius norm of the matrix A. A crude
bound can then be obtained from the largest sampling error ˜ϵ of all matrix entries and setting ∥K −ˆK∥F ≤˜ϵ|T|,
since both matrices of the training set T are of size |T| × |T|. Hence to ensure a maximum deviation of ϵ with high
probability a number of R = O(ϵ−2|T|2) shots have to be drawn for each matrix entry. Due to the symmetry of the
K matrix and the trivial diagonals, |T|(|T| −1)2−1 matrix entries have to be estimated. Thus the full sampling
complexity is expected to scale as O(ϵ−2|T|4). A more careful analysis of the statistical error could be carried out by
using one of the matrix-concentration results .
Note that the optimization problem, eqn. (6) is only concave, when the matrix K ≥0 is positive semi-deﬁnite. It
can happen, that the shot noise and other errors in the experiment lead to a ˆK that is no longer positive semi-deﬁnite.
We have indeed observed this multiple times in the experiment. A possible way of dealing with this problem is a
method developed in , where an optimization problem is solved to ﬁnd the closest positive semi-deﬁnite K-matrix
in trace norm to ˆK consistent with the constraint. In our experiments however, we have found this not to be necessary
and the performance has been almost optimal without performing this method.
Device parameters
Our device is fabricated on a 720-µm-thick Si substrate. A single optical lithography step is used to deﬁne all CPW
structures and the qubit capacitors with Nb. The Josephson junctions are patterned via electron beam lithography
and made by double-angle deposition of Al.
The dispersive readout signals are ampliﬁed by Josephson Parametric Converters (JPC). Both the quantum
processor and the JPC ampliﬁers are thermally anchored to the mixing chamber plate of a dilution refrigerator.
The two qubit fundamental transition frequencies are ωi/2π = {5.2760(4), 5.2122(3)} GHz, with anharmonicities ∆i/2π = {−330.3, −331.9} MHz, where i ∈{0, 1}.
The readout resonator frequencies used are ωRi/2π =
{6.530553, 6.481651} GHz, while the CPW bus resonator connecting Q0 and Q1 was unmeasured and designed to be
7.0 GHz. The dispersive shifts and line widths of the readout resonators are measured to be 2χi/2π = {−1.06, −1.02}
MHz and κi/2π = {661, 681} kHz, respectively.
The two qubit lifetimes and coherences were measured intermittently throughout our experiments. The observed
mean values were T1(i) = {55, 38}, T ∗
2(i) = {16, 17}, T echo
2(i) = {43, 46} µs with i ∈{0, 1}
Gate characterization
Our experiments use calibrated X−rotations (Xπ and Xπ/2) as single-qubit unitary primitives. Y −rotations are
attained by appropiate adjustment of the pulse phases, whereas Z−rotations are implemented via frame changes
in software . A time buﬀer is added at the end of each physical pulse to mitigate eﬀects from reﬂections and
evanescent waves in the cryogenic control lines and components.
We use two sets of gate times in order to perform the Richardson extrapolation of the noise in our system .
For the ﬁrst set of gate times we use 83 ns for single-qubit gates and 333 ns for each cross-resonance pulse. The
buﬀer after each physical pulse is 6.5 ns. The single-qubit gates are gaussian shaped pulses with σ = 20.75 ns. The
cross-resonance gates are ﬂat with turn-on and -oﬀgaussian shapes of σ = 10 ns. Our implementation of a CNOT
has a duration of two single-qubit pulses and two cross-resonance pulses, giving a total of 858 ns for the ﬁrst set
of gate times, including buﬀers. For our second set of gate times we use the times of the ﬁrst set but stretched by
a factor of 1.5, including the pulses σs and the buﬀers. This gives a total CNOT time of 1.287 µs and single-qubit
gates of ∼125 ns.
We experimentally veriﬁed our single- and two-qubit unitaries by Randomized Benchmarking (RB) . The
following table shows the RB results for all single-qubit gates used in our experiments, including individual and
simultaneous RB.
Q0 (83 ns)
Q1 (83 ns)
Q0 (125 ns)
Q1 (125 ns)
0.715 ± 0.005
1.244 ± 0.010
1.319 ± 0.017
1.410 ± 0.010
1.367 ± 0.011 0.763 ± 0.005 1.484 ± 0.014 1.271 ± 0.010
RB of our single-qubit gates. Qubit labels indicate which qubit was bencharmked on each case, with label 11
indicating simultaneous RB.
Our two-qubit unitarias are CNOTs constructed from echo cross-resonance sequences . Each of the two
cross-resonance pulses in a CNOT has durations of 333 and 500 ns for the two diﬀerent gate lengths used in our
experiments. For our two-qubit RB we obtain a CNOT error of 0.0373 ± .0015 (0.0636 ± .0021) for the 333 (500) ns
cross-resonance pulse.
Readout correction
Our readout assigned ﬁdelity was ∼95% for both qubits.
For each experiment, we run 4 (22) calibration sequences preparing our two qubits in their joint computational
states. We gather statistics of these calibrations and create a measurement matrix Aij = P(|i⟩||j⟩) where P(|n⟩||m⟩)
is the probability of measuring state |m⟩having prepared state |n⟩. We then correct the observed outcomes of our
experiments by inverting this matrix and multiplying our output probability distributions by this inverse.
Support vectors
Here we show the support vectors and αi as calculated for each of the three datasets studied from their K matrices.
Error mitigation for kernel estimation
The experimental estimation of the kernel matrices shown in Fig. S7 and in Fig. 4 in the main text involves running
the experiments at diﬀerent gate lengths and extrapolating the expectation value of the observable of interest to its
zero-noise value. While this technique can be extremely powerful in scenarios where the noise is invariant under time
rescaling, it is particularly sensitive to measurement sampling noise. In many cases it is the experimental readout
assignment ﬁdelity that determines the bound on how precisely the observable can be estimated.
[5.3407,2.9531] 2.4185 +1
[5.1522,4.6496] 2.6248 +1
[0.6912,0.7540] 3.1379 +1
[6.0947,5.7177] 1.2891 +1
[6.0947,4.1469] 1.3604 +1
[2.6389,4.8381] 1.7163 +1
[4.1469,2.0735] 1.3242 -1
[0.2513,3.2044] 0.9757 -1
[2.3248,3.9584] 1.4953 -1
[4.1469,6.2204] 2.4536 -1
[2.6389,1.6965] 0.5293 -1
[5.4664,4.6496] 4.0213 -1
[4.1469,3.0788] 1.7477 -1
bias = -0.1090
[0.1885,3.3301] 2.1871 +1
[5.4035,1.7593]
[5.2150,3.7071] 2.0330 +1
[4.71240,4.6496] 0.7559 +1
[0.4398,2.388]
[6.2204,5.7177] 2.3090 +1
[5.9062,4.2726] 1.6297 -1
[1.8221,2.1362] 2.9281 -1
[0.3770,3.8327] 3.0627 -1
[0.31416,1.6965] 1.3793 -1
[1.6336,3.2672] 0.2759 -1
[2.5761,3.2673] 0.4544 -1
bias = 0.2102
[5.4664,3.7071] 6.2649 +1
[1.5080,5.2779] 0.7777 +1
[5.9690,3.8956] 1.5619 +1
[5.3407,3.5186] 0.6482 +1
[5.7177,4.5239] 1.6387 +1
[0.0628,3.5186] 0.9379 +1
[6.1575,1.445] 0.3882 -1
[5.9690,0.9425] 0.8346 -1
[0.2513,1.3823] 2.5561 -1
[5.2779,2.0735] 4.8076 -1
[6.0947,2.0106] 3.2428 -1
bias = -0.1865
Suport vectors for all three data sets used for our kernel estimation method, as shown as green circles in Figs. 3
(b) and S6.
Sets I (a) and II (b), including training data points (white and black circles) and support vectors (green circles).
Even though for our Sets I and II we attain 100 % classiﬁcation success over 10 randomly drawn test sets in each
case, we can quantify how close our experimentally determined separating hyperplane is to the ideal.
The optimal hyperplane for a given training set can be expressed as the linear combination P
i αiyi⃗xi = w (eqn.
7), where w is a vector orthogonal to the optimal separating hyperplane and ⃗xi are the support vectors. We can
therefore quantify the distance between the experimentally obtained hyperplane and the ideal hyperplane by calculating the inner product ⟨w, wideal⟩= P
i αj|⟨⃗xi⃗xj⟩|2/||w||||wideal|| where NS and N ′
S are the sets
of experimentally obtained and ideal support vectors, respectively.
In Fig. S8 we show the inner products between the ideal and all experimental hyperplanes, including the two sets
Experimentally estimated and ideal kernel matrices for Sets I (a, b) and II (c, d). For both datasets we show a cut
(b, d) through the row at which the maximum of | ˆK −K| occurs, similarly as in Fig. 4 in the main text.
of gate times used throughout our experiments, c1 and c1.5, as well as the error-mitigated hyperplanes.
Inner products between experimentally obtained hyperplanes and the ideal for each training set. The x−axis shows
the results for the two diﬀerent time stretches used in our experiments, c1 and c1.5, corresponding to the faster and slower
gates, respectively. We also show the error-mitigated hyperplanes inner products.
For Sets I and II, which classify at 100 % success, it is clear that error mitigation improves our results very
signiﬁcantly. This is not the case for Set III, which classiﬁes at 94.75 % success. In fact, for Set III we see that
error-mitigation worsens the hyperplane, as the results are closer to the ideal for the unmitigated experiments than
both Sets I and II.
A look at the calibration data taken along the direct kernel estimation experiments for each set, we see that the
readout assignment ﬁdelities of Q0(Q1) are 96.56% (96.31%) for Set I, 95.90% (96.36%) for Set II, and 93.99% (95.47%)
for Set III. The slightly worse readout ﬁdelities for Set III could partially explain the worse classiﬁcation results for
this set, but other aspects of the protocol might also contribute to this, like for example some gates operating on a
somewhat non-linear regime after the calibrations in this set.
Another symptom for the degree of classiﬁcation success in a given set can be observed by looking at the combined
weight of negative eigenvalues in the kernel matrix, P
ki<0 |ki|, with ki the eigenvalues of the kernel. We obtain 1.40,
1.27 and 2.41 for Sets I, II, and III, respectively.