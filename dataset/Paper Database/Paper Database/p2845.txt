HAL Id: hal-01418216
 
Submitted on 16 Dec 2016
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Hollywood in Homes: Crowdsourcing Data Collection
for Activity Understanding
Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev,
Abhinav Gupta
To cite this version:
Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, et al.. Hollywood in
Homes: Crowdsourcing Data Collection for Activity Understanding. Computer Vision – ECCV 2016,
Oct 2016, Amsterdam, Netherlands. pp.510 - 526, ￿10.1007/978-3-319-46448-0_31￿. ￿hal-01418216￿
Hollywood in Homes: Crowdsourcing Data
Collection for Activity Understanding
Gunnar A. Sigurdsson1, G¨ul Varol2, Xiaolong Wang1,
Ali Farhadi3,4, Ivan Laptev2, and Abhinav Gupta1,4
1Carnegie Mellon University
3University of Washington
4The Allen Institute for AI
 
Abstract. Computer vision has a great potential to help our daily lives
by searching for lost keys, watering ﬂowers or reminding us to take a pill.
To succeed with such tasks, computer vision methods need to be trained
from real and diverse examples of our daily dynamic scenes. While most
of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect
suﬃciently many diverse but boring samples representing our lives? We
propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing
and crowdsourcing the whole process of video creation from script writing
to video recording and annotation. Following this procedure we collect a
new dataset, Charades, with hundreds of people recording videos in their
own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 seconds,
showing activities of 267 people from three continents. Each video is
annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847
video descriptions, 66,500 temporally localized intervals for 157 action
classes and 41,104 labels for 46 object classes. Using this rich data, we
evaluate and provide baseline results for several tasks including action
recognition and automatic description generation. We believe that the
realism, diversity, and casual nature of this dataset will present unique
challenges and new opportunities for computer vision community.
Introduction
Large scale visual learning fueled by huge datasets has changed the computer
vision landscape . Given the source of this data, it’s not surprising that most
of our current success is biased towards static scenes and objects in Internet
images. As we move forward into the era of AI and robotics, however, new
questions arise. How do we learn about diﬀerent states of objects (e.g., cut vs.
whole)? How do common activities aﬀect changes of object states? In fact, it is
not even yet clear if the success of the Internet pre-trained recognition models
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
will transfer to real-world settings where robots equipped with our computer
vision models should operate.
Shifting the bias from Internet images to real scenes will most likely require collection of new large-scale datasets representing activities of our boring
everyday life: getting up, getting dressed, putting groceries in fridge, cutting
vegetables and so on. Such datasets will allow us to develop new representations
and to learn models with the right biases. But more importantly, such datasets
representing people interacting with objects and performing natural action sequences in typical environments will ﬁnally allow us to learn common sense and
contextual knowledge necessary for high-level reasoning and modeling.
But how do we ﬁnd these boring videos of our daily lives? If we search
common activities such as “drinking from a cup”, “riding a bike” on video sharing
websites such as YouTube, we observe a highly-biased sample of results (see
Figure 1). These results are biased towards entertainment—boring videos have
no viewership and hence no reason to be uploaded on YouTube!
In this paper, we propose a novel Hollywood in Homes approach to collect
a large-scale dataset of boring videos of daily activities. Standard approaches in
the past have used videos downloaded from the Internet gathered
from movies or recorded in controlled environments .
Instead, as the name suggests: we take the Hollywood ﬁlming process to the
homes of hundreds of people on Amazon Mechanical Turk (AMT). AMT workers
follow the three steps of ﬁlming process: (1) script generation; (2) video direction
and acting based on scripts; and (3) video veriﬁcation to create one of the largest
and most diverse video dataset of daily activities.
There are threefold advantages of using the Hollywood in Homes approach
for dataset collection: (a) Unlike datasets shot in controlled environments (e.g.,
MPII ), crowdsourcing brings in diversity which is essential for generalization.
In fact, our approach even allows the same script to be enacted by multiple
people; (b) crowdsourcing the script writing enhances the coverage in terms of
scenarios and reduces the bias introduced by generating scripts in labs; and
(c) most importantly, unlike for web videos, this approach allows us to control
the composition and the length of video scenes by proposing the vocabulary of
scenes, objects and actions during script generation.
The Charades v1.0 Dataset
Charades is our large-scale dataset with a focus on common household activities
collected using the Hollywood in Homes approach. The name comes from of a
popular American word guessing game where one player acts out a phrase and the
other players guess what phrase it is. In a similar spirit, we recruited hundreds of
people from Amazon Mechanical Turk to act out a paragraph that we presented
to them. The workers additionally provide action classiﬁcation, localization, and
video description annotations. The ﬁrst publicly released version of our Charades
dataset will contain 9,848 videos of daily activities 30.1 seconds long on average
(7,985 training and 1,863 test). The dataset is collected in 15 types of indoor
scenes, involves interactions with 46 object classes and has a vocabulary of 30
verbs leading to 157 action classes. It has 66,500 temporally localized actions,
Hollywood in Homes: Crowdsourcing Data Collection
The Charades Dataset
Fig. 1. Comparison of actions in the Charades dataset and on YouTube: Reading a
book, Opening a refrigerator, Drinking from a cup. YouTube returns entertaining and
often atypical videos, while Charades contains typical everyday videos.
12.8 seconds long on average, recorded by 267 people in three continents, and
over 15% of the videos have more than one person. We believe this dataset will
provide a crucial stepping stone in developing action representations, learning
object states, human object interactions, modeling context, object detection in
videos, video captioning and many more. The dataset is publicly available at
 
Contributions The contributions of our work are three-fold: (1) We introduce
the Hollywood in Homes approach to data collection, (2) we collect and release
the ﬁrst crowdsourced large-scale dataset of boring household activities, and (3)
we provide extensive baseline evaluations.
The KTH action dataset paved the way for algorithms that recognized
human actions. However, the dataset was limited in terms of number of categories and enacted in the same background. In order to scale up the learning
and the complexity of the data, recent approaches have instead tried collecting
video datasets by downloading videos from Internet. Therefore, datasets such as
UCF101 , Sports1M and others appeared and presented more challenges including background clutter, and scale. However, since it is impossible to
ﬁnd boring daily activities on Internet, the vocabulary of actions became biased
towards more sports-like actions which are easy to ﬁnd and download.
There have been several eﬀorts in order to remove the bias towards sporting actions. One such commendable eﬀort is to use movies as the source of
data . Recent papers have also used movies to focus on the video description problem leading to several datasets such as MSVD , M-VAD ,
and MPII-MD . Movies however are still exciting (and a source of entertainment) and do not capture the scenes, objects or actions of daily living. Other
eﬀorts have been to collect in-house datasets for capturing human-object interactions or human-human interactions . Some relevant big-scale eﬀorts in
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
Table 1. Comparison of Charades with other video datasets.
per video Classes
localization
Charades v1.0
Daily Activities
ActivityNet 
Human Activities Yes
UCF101 
HMDB51 
YouTube/Movies
THUMOS’15 
Sports 1M 
MPII-Cooking 46
30 In-house actors Cooking
20 Volunteers
Ego-centric
MPII-MD 
Captions Captions 68K
this direction include MPII Cooking , TUM Breakfast , and the TACoS
Multi-Level datasets. These datasets focus on a narrow domain by collecting the data in-house with a ﬁxed background, and therefore focus back on the
activities themselves. This allows for careful control of the data distribution, but
has limitations in terms of generalizability, and scalability. In contrast, PhotoCity used the crowd to take pictures of landmarks, suggesting that the same
could be done for other content at scale.
Another relevant eﬀort in collection of data corresponding to daily activities
and objects is in the domain of ego-centric cameras. For example, the Activities
of Daily Living dataset recorded 20 people performing unscripted, everyday
activities in their homes in ﬁrst person, and another extended that idea to animals . These datasets provide a challenging task but fail to provide diversity
which is crucial for generalizability. It should however be noted that these kinds
of datasets could be crowdsourced similarly to our work.
The most related dataset is the recently released ActivityNet dataset .
It includes actions of daily living downloaded from YouTube. We believe the
ActivityNet eﬀort is complementary to ours since their dataset is uncontrolled,
slightly biased towards non-boring actions and biased in the way the videos
are professionally edited. On the other hand, our approach focuses more on
action sequences (generated from scripts) involving interactions with objects.
Our dataset, while diverse, is controlled in terms of vocabulary of objects and
actions being used to generate scripts. In terms of the approach, Hollywood in
Homes is also related to . However, only generates synthetic data. A
comparison with other video datasets is presented in Table 1. To the best of our
knowledge, our approach is the ﬁrst to demonstrate that workers can be used to
collect a vision dataset by ﬁlming themselves at such a large scale.
Hollywood in Homes
We now describe the approach and the process involved in a large-scale video
collection eﬀort via AMT. Similar to ﬁlming, we have a three-step process for
generating a video. The ﬁrst step is generating the script of the indoor video.
Hollywood in Homes: Crowdsourcing Data Collection
The key here is to allow workers to generate diverse scripts yet ensure that we
have enough data for each category. The second step in the process is to use the
script and ask workers to record a video of that sentence being acted out. In
the ﬁnal step, we ask the workers to verify if the recorded video corresponds to
script, followed by an annotation procedure.
Generating Scripts
In this work we focus on indoor scenes, hence, we group together rooms in
residential homes (Living Room, Home Oﬃce, etc.). We found 15 types of rooms
to cover most of typical homes, these rooms form the scenes in the dataset. In
order to generate the scripts (a text given to workers to act out in a video),
we use a vocabulary of objects and actions to guide the process. To understand
what objects and actions to include in this vocabulary, we analyzed 549 movie
scripts from popular movies in the past few decades. Using both term-frequency
(TF) and TF-IDF we analyzed which nouns and verbs occur in those rooms
in these movies. From those we curated a list of 40 objects and 30 actions to be
used as seeds for script generation, where objects and actions were chosen to be
generic for diﬀerent scenes.
To harness the creativity of people, and understand their bias towards activities, we crowdsourced the script generation as follows. In the AMT interface, a
single scene, 5 randomly selected objects, and 5 randomly selected actions were
presented to workers. Workers were asked to use two objects and two actions
to compose a short paragraph about activities of one or two people performing
realistic and commonplace activities in their home. We found this to be a good
compromise between controlling what kind of words were used and allowing the
users to impose their own human bias on the generation. Some examples of generated scripts are shown in Figure 2. (see the website for more examples). The
distribution of the words in the dataset is presented in Figure 3.
Generating Videos
Once we have scripts, our next step is to collect videos. To maximize the diversity of scenes, objects, clothing and behaviour of people, we ask the workers
themselves to record the 30 second videos by following collected scripts.
AMT is a place where people commonly do quick tasks in the convenience of
their homes or during downtime at their work. AMT has been used for annotation
and editing but can we do content creation via AMT? During a pilot study we
asked workers to record the videos, and until we paid up to $3 per video, no
worker picked up our task. (For comparison, to annotate a video : 3 workers
× 157 questions × 1 second per question × $8/h salary = $1.) To reduce the base
cost to a more manageable $1 per video, we have used the following strategies:
Worker Recruitment. To overcome the inconvenience threshold, worker recruitment was increased through sign-up bonuses (211% increased new worker
rate) where we awarded a $5 bonus for the ﬁrst submission. This increased the
total cost by 17%. In addition, “recruit a friend” bonuses ($5 if a friend submits
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
refrigerator
"A person opens a refrigerator,
and begins drinking out of a
jug of milk before closing it."
"A person is washing their
refrigerator. Then, opening it,
the person begins putting
away their groceries."
"A person stands in the kitchen
and cleans the fridge. Then start
to put groceries away from a bag"
"person drinks milk from a fridge,
they then walk out of the room."
Sampled Words
Recorded Videos
Annotations
Opening a refrigerator
Closing a refrigerator
Putting groceries somewhere
Opening a refrigerator
Drinking from cup/bottle
Fig. 2. An overview of the three Amazon Mechanical Turk (AMT) crowdsourcing
stages in the Hollywood in Homes approach.
15 videos) were introduced, and were claimed by 4% of the workforce, generating indeterminate outreach to the community. US, Canada, UK, and, for a time,
India were included in this study. The ﬁrst three accounted for estimated 73%
of the videos, and 59% of the peak collection rate.
Worker Retention. Worker retention was mitigated through performance bonuses
every 15th video, and while only accounting for a 33% increase in base cost,
signiﬁcantly increased retention (34% increase in come-back workers), and performance (109% increase in output per worker).
Each submission in this phase was manually veriﬁed by other workers to
enforce quality control, where a worker was required to select the corresponding
sentence from a line-up after watching the video. The rate of collection peaked at
1225 per day from 72 workers. The ﬁnal cost distribution was: 65% base cost per
video, 21% performance bonuses, 11% recruitment bonuses, and 3% veriﬁcation.
The code and interfaces will be made publicly available along with the dataset.
Annotations
Using the generated scripts, all (verb,proposition,noun) triplets were analyzed,
and the most frequent grouped into 157 action classes (e.g., pouring into cup,
running, folding towel, etc.). The distribution of those is presented in Figure 3.
For each recorded video we have asked other workers to watch the video and
describe what they have observed with a sentence (this will be referred to as a
description in contrast to the previous script used to generate the video). We
use the original script and video descriptions to automatically generate a list of
interacted objects for each video. Such lists were veriﬁed by the workers. Given
the list of (veriﬁed) objects, for each video we have made a short list of 4-5
actions (out of 157) involving corresponding object interactions and asked the
workers to verify the presence of these actions in the video.
In addition, to minimize the missing labels, we expanded the annotations
by exhaustively annotating all actions in the video using state-of-the-art crowdsourcing practices , where we focused particularly on the test set.
Finally, for all the chosen action classes in each video, another set of workers
was asked to label the starting and ending point of the activity in the video, resulting in a temporal interval of each action. A visualization of the data collection
Hollywood in Homes: Crowdsourcing Data Collection
process is illustrated in Figure 2. On the website we show numerous additional
examples from the dataset with annotated action classes.
Charades v1.0 Analysis
Charades is built up by combining 40 objects and 30 actions in 15 scenes. This
relatively small vocabulary, combined with open-ended writing, creates a dataset
that has substantial coverage of a useful domain. Furthermore, these combinations naturally form action classes that allow for standard benchmarking. In Figure 3 the distributions of action classes, and most common nouns/verbs/scenes
in the dataset are presented. The natural world generally follows a long-tailed
distribution , but we can see that the distribution of words in the dataset
is relatively even. In Figure 3 we also present a visualization of what scenes,
objects, and actions occur together. By embedding the words based on their cooccurance with other words using T-SNE , we can get an idea of what words
group together in the videos of the dataset, and it is clear that the dataset possesses real-world intuition. For example, food, and cooking are close to Kitchen,
but note that except for Kitchen, Home Oﬃce, and Bathroom, the scene is not
highly discriminative of the action, which reﬂects common daily activities.
Since we have control over the data acquisition process, instead of using
Internet search, there are on average 6.8 relevant actions in each video. We hope
that this may inspire new and interesting algorithms that try to capture this
kind of context in the domain of action recognition. Some of the most common
pairs of actions measured in terms of normalized pointwise mutual information
(NPMI), are also presented in Figure 3. These actions occur in various orders
and context, similar to our daily lives. For example, in Figure 4 we can see that
among these ﬁve videos, there are multiple actions occurring, and some are in
common. We further explore this in Figure 5, where for a few actions, we visualize
the most probable actions to precede, and most probable actions to follow that
action. As the scripts for the videos are generated by people imagining a boring
realistic scenario, we ﬁnd that these statistics reﬂect human behaviour.
Applications
We run several state-of-the-art algorithms on Charades to provide the community with a benchmark for recognizing human activities in realistic home
environments. Furthermore, the performance and failures of tested algorithms
provide insights into the dataset and its properties.
Train/test set. For evaluating algorithms we split the dataset into train and
test sets by considering several constraints: (a) the same worker should not
appear in both training and test; (b) the distribution of categories over the test
set should be similar to the one over the training set; (c) there should be at least
6 test videos and 25 training videos in each category; (d) the test set should
not be dominated by a single worker. We randomly split the workers into two
groups (80% in training) such that these constraints were satisﬁed. The resulting
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
-5 -4 -3 -2
Close box / Open box
Hold picture / Put picture
Hold phone / Play with phone
Hold pillow / Put pillow
Hold shoes / Take shoes
Hold shoes / Put shoes
Hold clothes / Take clothes
Hold box / Take box
Hold pillow / Take pillow
Hold broom / Take broom
Hold book / Read book
Hold broom / Put broom
Hold box / Put box
Hold phone / Take picture
Close closet / Open a closet
Hold picture / Look at picture
Eat sandwich / Eating
Hold broom / Tidy with broom
Close fridge / Open fridge
Dining Room
Living Room
Recreation Room
Walk-in Closet
PantryGarage
Laundry Room
refrigerator
television
Recreation room
Walk-in Closet
Laundry room
Dining room
Home Office
Living room
broom / Tidy on floor
Co-occurring pairs
Throwing a broom
Washing a window
Fixing a door
Washing a cup/glass
Washing some clothes
Taking a laptop from
Closing a box
Throwing a pillow
Tidying up a blanket/s
Lying on the floor
Snuggling with a pillow
Putting shoes somewhere
Putting a box somewhere
Holding a vacuum
Putting a blanket
Holding a shoe/shoes
Holding some medicine
Holding a laptop
Holding a box
Holding a broom
Someone is undressing
Holding a blanket
Putting some food
Someone is laughing
Playing with a phone
Holding a dish
Holding a phone/camera
Sitting in a chair
Fig. 3. Statistics for actions (gray, every ﬁfth label shown), verbs (green), nouns (blue),
scenes (red), and most co-occurring pairs of actions (cyan). Co-occurrence is measured
with normalized pointwise mutual information. In addition, a T-SNE embedding of
the co-occurrence matrix is presented. We can see that while there are some words
that strongly associate with each other (e.g., lying and bed), many of the objects and
actions co-occur with many of the scenes. (Action names are abbreviated as necessary
to ﬁt space constraints.)
Hollywood in Homes: Crowdsourcing Data Collection
Holding a laptop
Closing a laptop
Put down laptop
Taking a dish
Taking a dish
Watching TV
Watching laptop
Closing a laptop
Taking phone
Playing on phone
Playing on phone
Watching TV
Sitting at table
Standing up
Sitting at table
Watching TV
Fixing hair
Walk in doorway
Taking box
Tidying shelf
Fig. 4. Keyframes from ﬁve videos in Charades. We see that actions occur together in
many diﬀerent conﬁgurations. (Shared actions are highlighed in color).
training and test sets contain 7,985 and 1,863 videos, respectively. The number
of annotated action intervals are 49,809 and 16,691 for training and test.
Action Classiﬁcation
Given a video, we would like to identify whether it contains one or several actions out of our 157 action classes. We evaluate the classiﬁcation performance
for several baseline methods. Action classiﬁcation performance is evaluated with
the standard mean average precision (mAP) measure. A single video is assigned
to multiple classes and the distribution of classes over the test set is not uniform.
Smiling at a book
Reading a book 13%
Smiling 9%
Holding a book 9%
Taking a book 7%
Laughing 5%
Smiling 12%
Laughing 5%
Closing a book 9%
Put book somewhere 6%
Reading a book 5%
Snuggling with a blanket
Holding a blanket 8%
Taking a blanket 7%
Sitting down 5%
Sitting in a chair 3%
Walk in doorway 3%
Standing up 7%
Put blanket 4%
Throw blanket 4%
Awakening 3%
Smiling 3%
Taking a picture
Play with camera 38%
Hold camera 26%
Take camera 17%
Walk in doorway 6%
Sit in chair 5%
Put camera 9%
Smiling 7%
Playing with camera 5%
Take camera 4%
Hold camera 3%
Opening a window
Walk in doorway 22%
Look out window 16%
Standing up 13%
Smiling 10%
Running 9%
Look out window 16%
Close window 5%
Drink from cup 3%
Holding cup 3%
Sneezing 3%
Fig. 5. Selected actions from the dataset, along with the top ﬁve most probable actions
before, and after the action. For example, when Opening a window, it is likely that
someone was Standing up before that, and after opening, Looking out the window.
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
Table 2. mAP (%) for action classiﬁcation with various baselines.
Two-Stream-B
Two-Stream
Table 3. Action classiﬁcation evaluation with the state-of-the-art approach on Charades. We study diﬀerent parameters for improved trajectories, by reporting for diﬀerent local descriptor sets and diﬀerent number of GMM clusters. Overall performance
improves by combining all descriptors and using a larger descriptor vocabulary.
HOG+HOF+MBH
The label precision for the data is 95.6%, measured using an additional veriﬁcation step, as well as comparing against a ground truth made from 19 iterations
of annotations on a subset of 50 videos. We now describe the baselines.
Improved trajectories. We compute improved dense trajectory features (IDT)
 capturing local shape and motion information with MBH, HOG and HOF
video descriptors. We reduce the dimensionality of each descriptor by half with
PCA, and learn a separate feature vocabulary for each descriptor with GMMs
of 256 components. Finally, we encode the distribution of local descriptors over
the video with Fisher vectors . A one-versus-rest linear SVM is used for
classiﬁcation. Training on untrimmed intervals gave the best performance.
Static CNN features. In order to utilize information about objects in the
scene, we make use of deep neural networks pretrained on a large collection of
object images. We experiment with VGG-16 and AlexNet to compute
fc6 features over 30 equidistant frames in the video. These features are averaged
across frames, L2-normalized and classiﬁed with a one-versus-rest linear SVM.
Training on untrimmed intervals gave the best performance.
Two-stream networks. We use the VGG-16 model architecture for both
networks and follow the training procedure introduced in Simonyan et al. ,
with small modiﬁcations. For the spatial network, we applied ﬁnetuning on ImageNet pre-trained networks with diﬀerent dropout rates. The best performance
was with 0.5 dropout rate and ﬁnetuning on all fully connected layers. The temporal network was ﬁrst pre-trained on the UCF101 dataset and then similarly
ﬁnetuned on conv4, conv5, and fc layers. Training on trimmed intervals gave the
best performance.
Balanced two-stream networks. We adapt the previous baseline to handle
class imbalance. We balanced the number of training samples through sampling,
and ensured each minibatch of 256 had at least 50 unique classes (each selected
uniformly at random). Training on trimmed intervals gave the best performance.
Hollywood in Homes: Crowdsourcing Data Collection
Action classes sorted by size in descending order
Throwing box
Throwing broom
Throwing bag
Laughing at picture
Holding mirror
Put picture somewhere
Throwing food
Closing laptop
Throwing book
Washing mirror
Grabbing picture
Fixing doorknob
Taking laptop
Standing on chair
Throwing shoes
Tidying with broom
Closing fridge
Drinking from cup
Tidying floor
Lying on bed
Opening fridge
Opening door
Sitting at table
Sitting in chair
Sitting down
Standing up
Wash window
Walking in doorway
Walking in doorway
Wash window
Standing up
Sitting down
Sitting in chair
Lying on bed
Opening fridge
Tidying with broom
Tidying floor
Sitting at table
Sitting in chair
Drinking from cup
Fig. 6. On the left classiﬁcation accuracy for the 15 highest and lowest actions is
presented for Combined. On the right, the classes are sorted by their size. The top
actions on the left are annotated on the right. We can see that while there is a slight
trend for smaller classes to have lower accuracy, many classes do not follow that trend.
C3D features. Following the recent approach from , we extract fc6 features
from a 3D convnet pretrained on the Sports-1M video dataset . These features
capture complex hierarchies of spatio-temporal patterns given an RGB clip of 16
frames. Similar to , we compute features on chunks of 16 frames by sliding 8
frames, average across chunks, and use a one-versus-rest linear SVM. Training
on untrimmed intervals gave the best performance.
Action classiﬁcation results are presented in Table 2, where we additionally consider Combined which combines all the other methods with late fusion.
Notably, the accuracy of the tested state-of-the-art baselines is much lower
than in most currently available benchmarks. Consistently with several other
datasets, IDT features outperform other methods by obtaining 17.2% mAP.
To analyze these results, Figure 6(left) illustrates the results for subsets of best
and worst recognized action classes. We can see that while the mAP is low, there
are certain classes that have reasonable performance, for example Washing a
window has 62.1% AP. To understand the source of diﬀerence in performance
for diﬀerent classes, Figure 6(right) illustrates AP for each action, sorted by
the number of examples, together with names for the best performing classes.
The number of actions in a class is primarily decided by the universality of the
action (can it happen in any scene), and if it is common in typical households
(writer bias). It is interesting to notice, that while there is a trend for actions
with higher number of examples to have higher AP, it is not true in general, and
actions such as Sitting in chair, and Washing windows have top-15 performance.
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
Probability (%)
True Class
Predicted Class
Fig. 7. Confusion matrix for the Combined baseline on the classiﬁcation task. Actions
are grouped by the object being interacted with. Most of the confusion is with other
actions involving the same object (squares on the diagonal), and we highlight some
prominent objects. Note: (A) High confusion between actions using Blanket, Clothes,
and Towel; (B) High confusion between actions using Couch and Bed; (C) Little confusion among actions with no speciﬁc object of interaction (e.g. standing up, sneezing).
Delving even further, we investigate the confusion matrix for the Combined
baseline in Figure 7, where we convert the predictor scores to probabilities and
accumulate them for each class. For clearer analysis, the classes are sorted by
the object being interacted with. The ﬁrst aspect to notice is the squares on
the diagonal, which imply that the majority of the confusion is among actions
that interact with the same object (e.g., Putting on clothes, or Taking clothes
from somewhere), and moreover, there is confusion among objects with similar
functional properties. The most prominent squares are annotated with the object being shared among those actions. The ﬁgure caption contains additional
observations. While there are some categories that show no clear trend, we can
observe less confusion for many actions that have no speciﬁc object of interaction. Evaluation of action recognition on this subset results in 38.9% mAP, which
is signiﬁcantly higher than average. Recognition of ﬁne-grained actions involving
interactions with the same object class appears particularly diﬃcult even for the
best methods available today. We hope our dataset will encourage new methods
addressing activity recognition for complex person-object interactions.
Sentence Prediction
Our ﬁnal, and arguably most challenging task, concerns prediction of free-from
sentences describing the video. Notably, our dataset contains sentences that have
been used to create the video (scripts), as well as multiple video descriptions
obtained manually for recorded videos. The scripts used to create videos are
biased by the vocabulary, and due to the writer’s imagination, generally describe
Hollywood in Homes: Crowdsourcing Data Collection
Table 4. Sentence Prediction. In the script task one sentence is used as ground truth,
and in the description task 2.4 sentences are used as ground truth on average. We ﬁnd
that S2VT is the strongest baseline.
Description
RW Random NN S2VT Human
RW Random NN S2VT Human
diﬀerent aspects of the video than descriptions. The description of the video by
other people is generally simpler and to the point. Captions are evaluated using
the CIDEr, BLEU, ROUGE, and METEOR metrics, as implemented in the
COCO Caption Dataset . These metrics are common for comparing machine
translations to ground truth, and have varying degrees of similarity with human
judgement. For comparison, human performance is presented along with the
baselines where workers were similarly asked to watch the video and describe
what they observed. We now describe the sentence prediction baselines in detail:
GT: A person opens a
closet and picks up a
pink toy laptop oﬀ of
the shelf. They close
the closet, turn oﬀ the
light, and exit the
GT: A person sweeps
the ﬂoor and places
the dirt into a trash
GT: A person is sitting
in a chair while
watching something
on a laptop. The
person then begins to
GT: A person is
cooking on a stove
they are mixing the
food in the pot they go
to the cabinet and take
out a spice they put
the spice in the pot
GT: Person is
standing in the
doorway drinking
coﬀee before grabbing
a towel from the closet
and tossing it out the
GT: A person wakes
up and turns a light
on and oﬀ before
going back to sleep
A person is walking
into a room and then
picks up a broom and
puts it on the ﬂoor
Person is standing in
front of a mirror ,
opens a cabinet and
takes out out of a
A person is lying on a
bed with a blanket .
the person then gets
up and walks to the
room and sits down
A person is standing
in the kitchen
cooking on a stove .
they then take a
drink from a glass
and drink it
A person is standing
in the doorway
holding a pillow . the
person then takes a
drink from a glass
and drinks it
A person is lying on a
bed with a blanket .
the person then gets
up and walks to the
door and sits down
Fig. 8. Three generated captions that scored low on the CIDEr metric (red), and three
that scored high (green) from the strongest baseline (S2VT). We can see that while
the captions are fairly coherent, the captions lack suﬃcient relevance.
Random Words (RW): Random words from the training set.
Random Sentence (Random): Random sentence from the training set.
Nearest Neighbor (NN): Inspired by Devlin et al. we simply use a 1-
Nearest Neighbor baseline computed using AlexNet fc7 outputs averaged over
frames, and use the caption from that nearest neighbor in the training set.
Sigurdsson, Varol, Wang, Farhadi, Laptev, Gupta
S2VT: We use the S2VT method from Venugopalan et al. , which is a
combination of a CNN, and a LSTM.
Table 4 presents the performance of multiple baselines on the caption generation task. We both evaluate on predicting the script, as well as predicting
the description. As expected, we can observe that descriptions made by people
after watching the video are more similar to other descriptions, rather than the
scripts used to generate the video. Table 4 also provides insight into the diﬀerent
evaluation metrics, and it is clear that CIDEr oﬀers the highest resolution, and
most similarity with human judgement on this task. In Figure 8 few examples
are presented for the highest scoring baseline (S2VT). We can see that while
the language model is accurate (the sentences are coherent), the model struggles with providing relevant captions, and tends to slightly overﬁt to frequent
patterns in the data (e.g., drinking from a glass/cup).
Conclusions
We proposed a new approach for building datasets. Our Hollywood in Homes
approach allows not only the labeling, but the data gathering process to be
crowdsourced. In addition, Charades oﬀers a novel large-scale dataset with diversity and relevance to the real world. We hope that Charades and Hollywood
in Homes will have the following beneﬁts for our community:
(1) Training data: Charades provides a large-scale set of 66,500 annotations of
actions with unique realism.
(2) A benchmark: Our publicly available dataset and provided baselines enable
benchmarking future algorithms.
(3) Object-action interactions: The dataset contains signiﬁcant and intricate
object-action relationships which we hope will inspire the development of novel
computer vision techniques targeting these settings.
(4) A framework to explore novel domains: We hope that many novel datasets
in new domains can be collected using the Hollywood in Homes approach.
(5) Understanding daily activities: Charades provides data from a unique humangenerated angle, and has unique attributes, such as complex co-occurrences of
activities. This kind of realistic bias, may provide new insights that aid robots
equipped with our computer vision models operating in the real world.
Acknowledgements
This work was partly supported by ONR MURI N00014-16-1-2007, ONR N00014-
13-1-0720, NSF IIS-1338054, ERC award ACTIVIA, Allen Distinguished Investigator Award, gifts from Google, and the Allen Institute for Artiﬁcial Intelligence.
The authors would like to thank: Nick Rhinehart and the anonymous reviewers
for helpful feedback on the manuscript; Ishan Misra for helping in the initial experiments; and Olga Russakovsky, Mikel Rodriguez, and Rahul Sukhantakar for
invaluable suggestions and advice. Finally, the authors want to extend thanks
to all the workers at Amazon Mechanical Turk.
Hollywood in Homes: Crowdsourcing Data Collection