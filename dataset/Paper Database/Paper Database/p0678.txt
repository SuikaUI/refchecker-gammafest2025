Self-supervised Visual Feature Learning with
Deep Neural Networks: A Survey
Longlong Jing and Yingli Tian∗, Fellow, IEEE
Abstract—Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual
feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating
large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general
image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive
review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation,
general pipeline, and terminologies of this ﬁeld are described. Then the common deep neural network architectures that used for
self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed
followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally,
quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image
and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual
feature learning.
Index Terms—Self-supervised Learning, Unsupervised Learning, Convolutional Neural Network, Transfer Learning, Deep Learning.
INTRODUCTION
Motivation
UE to the powerful ability to learn different levels of
general visual features, deep neural networks have
been used as the basic structure to many computer vision
applications such as object detection , , , semantic
segmentation , , , image captioning , etc. The models trained from large-scale image datasets like ImageNet
are widely used as the pre-trained models and ﬁne-tuned
for other tasks for two main reasons: (1) the parameters
learned from large-scale diverse datasets provide a good
starting point, therefore, networks training on other tasks
can converge faster, (2) the network trained on large-scale
datasets already learned the hierarchy features which can
help to reduce over-ﬁtting problem during the training of
other tasks, especially when datasets of other tasks are small
or training labels are scarce.
The performance of deep convolutional neural networks
(ConvNets) greatly depends on their capability and the
amount of training data. Different kinds of network architectures were developed to increase the capacity of network models, and larger and larger datasets were collected
these days. Various networks including AlexNet , VGG
 , GoogLeNet , ResNet , and DenseNet and
L. Jing is with the Department of Computer Science, The Graduate
University
 
Y. Tian is with the Department of Electrical Engineering, The City
College, and the Department of Computer Science, the Graduate
University
 
∗Corresponding author
This material is based upon work supported by the National Science Foundation under award number IIS-1400802.
large scale datasets such as ImageNet , OpenImage 
have been proposed to train very deep ConvNets. With
the sophisticated architectures and large-scale datasets, the
performance of ConvNets keeps breaking the state-of-thearts for many computer vision tasks , , , , .
collection
annotation
large-scale
datasets are time-consuming and expensive. As one of the
most widely used datasets for pre-training very deep 2D
convolutional neural networks (2DConvNets), ImageNet
 contains about 1.3 million labeled images covering
1, 000 classes while each image is labeled by human workers
with one class label. Compared to image datasets, collection
and annotation of video datasets are more expensive due
to the temporal dimension. The Kinetics dataset , which
is mainly used to train ConvNets for video human action
recognition, consists of 500, 000 videos belonging to 600
categories and each video lasts around 10 seconds. It took
many Amazon Turk workers a lot of time to collect and
annotate a dataset at such a large scale.
To avoid time-consuming and expensive data annotations, many self-supervised methods were proposed to
learn visual features from large-scale unlabeled images or
videos without using any human annotations. To learn
visual features from unlabeled data, a popular solution is to
propose various pretext tasks for networks to solve, while
the networks can be trained by learning objective functions
of the pretext tasks and the features are learned through this
process. Various pretext tasks have been proposed for selfsupervised learning including colorizing grayscale images
 , image inpainting , image jigsaw puzzle , etc.
The pretext tasks share two common properties: (1) visual
features of images or videos need to be captured by ConvNets to solve the pretext tasks, (2) pseudo labels for the
pretext task can be automatically generated based on the
attributes of images or videos.
 
Self-supervised Pretext Task Training
Supervised Downstream Task Training
Unlabeled Dataset
Labeled Dataset
Downstream
Knowledge Transfer
Fig. 1. The general pipeline of self-supervised learning. The visual
feature is learned through the process of training ConvNets to solve
a pre-deﬁned pretext task. After self-supervised pretext task training
ﬁnished, the learned parameters serve as a pre-trained model and
are transferred to other downstream computer vision tasks by ﬁnetuning. The performance on these downstream tasks is used to evaluate
the quality of the learned features. During the knowledge transfer for
downstream tasks, the general features from only the ﬁrst several layers
are unusually transferred to downstream tasks.
The general pipeline of self-supervised learning is shown
in Fig. 1. During the self-supervised training phase, a predeﬁned pretext task is designed for ConvNets to solve, and
the pseudo labels for the pretext task are automatically generated based on some attributes of data. Then the ConvNet
is trained to learn object functions of the pretext task. After the self-supervised training ﬁnished, the learned visual
features can be further transferred to downstream tasks
(especially when only relatively small data available) as pretrained models to improve performance and overcome over-
ﬁtting. Generally, shallow layers capture general low-level
features like edges, corners, and textures while deeper layers
capture task related high-level features. Therefore, visual
features from only the ﬁrst several layers are transferred
during the supervised downstream task training phase.
Term Deﬁnition
To make this survey easy to read, we ﬁrst deﬁne the terms
used in the remaining sections.
Human-annotated label:
Human-annotated labels
refer to labels of data that are manually annotated by
human workers.
Pseudo label: Pseudo labels are automatically generated labels based on data attributes for pretext tasks.
Pretext Task: Pretext tasks are pre-designed tasks for
networks to solve, and visual features are learned by
learning objective functions of pretext tasks.
Downstream Task: Downstream tasks are computer
vision applications that are used to evaluate the quality of features learned by self-supervised learning.
These applications can greatly beneﬁt from the pretrained models when training data are scarce. In general, human-annotated labels are needed to solve the
downstream tasks. However, in some applications,
the downstream task can be the same as the pretext
task without using any human-annotated labels.
Supervised
Learning: Supervised learning indicates learning methods using data with ﬁne-grained
human-annotated labels to train networks.
Semi-supervised Learning: Semi-supervised learning refers to learning methods using a small amount
of labeled data in conjunction with a large amount of
unlabeled data.
Weakly-supervised Learning:
Weakly supervised
learning refers to learning methods to learn with
coarse-grained labels or inaccurate labels. The cost
of obtaining weak supervision labels is generally
much cheaper than ﬁne-grained labels for supervised
Unsupervised
Unsupervised
human-annotated labels.
Self-supervised Learning: Self-supervised learning
is a subset of unsupervised learning methods. Selfsupervised learning refers to learning methods in
which ConvNets are explicitly trained with automatically generated labels. This review only focuses on
self-supervised learning methods for visual feature
learning with ConvNets in which the features can
be transferred to multiple different computer vision
Since no human annotations are needed to generate
pseudo labels during self-supervised training, very largescale datasets can be used for self-supervised training.
Trained with these pseudo labels, self-supervised methods
achieved promising results and the gap with supervised
methods in performance on downstream tasks becomes
smaller. This paper provides a comprehensive survey of
deep ConvNets-based self-supervised visual feature learning methods. The key contributions of this paper are as
To the best of our knowledge, this is the ﬁrst comprehensive survey about self-supervised visual feature
learning with deep ConvNets which will be helpful
for researchers in this ﬁeld.
An in-depth review of recently developed selfsupervised learning methods and datasets.
Quantitative performance analysis and comparison
of the existing methods are provided.
A set of possible future directions for self-supervised
learning is pointed out.
FORMULATION
Based on the training labels, visual feature learning methods
can be grouped into the following four categories: supervised, semi-supervised, weakly supervised, and unsupervised. In this section, the four types of learning methods are
compared and key terminologies are deﬁned.
Supervised Learning Formulation
For supervised learning, given a dataset X, for each data
Xi in X, there is a corresponding human-annotated label
Yi. For a set of N labeled training data D = {Xi}N
training loss function is deﬁned as:
loss(D) = min
loss(Xi, Yi).
Trained with accurate human-annotated labels, the supervised learning methods obtained break-through results
on different computer vision applications , , , .
However, data collection and annotation usually are expensive and may require special skills. Therefore, semisupervised, weakly supervised, and unsupervised learning
methods were proposed to reduce the cost.
Semi-Supervised Learning Formulation
For semi-supervised visual feature learning, given a small
labeled dataset X and a large unlabeled dataset Z, for each
data Xi in X, there is a corresponding human-annotated
label Yi. For a set of N labeled training data D1 = {Xi}N
and M unlabeled training data D2 = {Zi}M
i=0, the training
loss function is deﬁned as:
loss(D1, D2) = min
loss(Xi, Yi)+ 1
loss(Zi, R(Zi, X)),
where the R(Zi, X) is a task-speciﬁc function to represent
the relation between each unlabeled training data Zi with
the labeled dataset X.
Weakly Supervised Learning Formulation
For weakly supervised visual feature learning, given a
dataset X, for each data Xi in X, there is a corresponding coarse-grained label Ci. For a set of N training data
i=0, the training loss function is deﬁned as:
loss(D) = min
loss(Xi, Ci).
Since the cost of weak supervision is much lower than
the ﬁne-grained label for supervised methods, large-scale
datasets are relatively easier to obtain. Recently, several
papers proposed to learn image features from web collected
images using hashtags as category labels , , and
obtained very good performance .
Unsupervised Learning Formulation
Unsupervised learning refers to learning methods that do
not need any human-annotated labels. This type of methods
including fully unsupervised learning methods in which
the methods do not need any labels at all, as well as selfsupervised learning methods in which networks are explicitly trained with automatically generated pseudo labels
without involving any human annotation.
Self-supervised Learning
Recently, many self-supervised learning methods for visual
feature learning have been developed without using any
human-annotated labels , , , , , , ,
 , , , , , , . Some papers refer to
this type of learning methods as unsupervised learning ,
 , , , , , , , , , , , .
Compared to supervised learning methods which require a
data pair Xi and Yi while Yi is annotated by human labors,
self-supervised learning also trained with data Xi along
with its pseudo label Pi while Pi is automatically generated for a pre-deﬁned pretext task without involving any
human annotation. The pseudo label Pi can be generated by
using attributes of images or videos such as the context of
images , , , , or by traditional hand-designed
methods , , .
Given a set of N training data D = {Pi}N
i=0, the training
loss function is deﬁned as:
loss(D) = min
loss(Xi, Pi).
As long as the pseudo labels P are automatically generated without involving human annotations, then the
methods belong to self-supervised learning. Recently, selfsupervised learning methods have achieved great progress.
This paper focuses on the self-supervised learning methods
that mainly designed for visual feature learning, while the
features have the ability to be transferred to multiple visual
tasks and to perform new tasks by learning from limited
labeled data. This paper summarizes these self-supervised
feature learning methods from different perspectives including network architectures, commonly used pretext tasks,
datasets, and applications, etc.
COMMON DEEP NETWORK ARCHITECTURES
No matter the categories of learning methods, they share
similar network architectures. This section reviews common
architectures for learning both image and video features.
Architectures for Learning Image Features
Various 2DConvNets have been designed for image feature
learning. Here, ﬁve milestone architectures for image feature
learning including AlexNet , VGG , GoogLeNet ,
ResNet , and DenseNet are reviewed.
AlexNet obtained a big improvement in the performance of
image classiﬁcation on ImageNet dataset compared to the
previous state-of-the-art methods . With the support of
powerful GPUs, AlexNet which has 62.4 million parameters
were trained on ImageNet with 1.3 million images. As
shown in Fig. 2, the architecture of AlexNet has 8 layers in
which 5 are convolutional layers and 3 are fully connected
layers. The ReLU is applied after each convolutional layers.
94% of the network parameters come from the fully connected layers. With this scale of parameters, the network
can easily be over-ﬁtting. Therefore, different kinds of techniques are applied to avoid over-ﬁtting problem including
data augmentation, dropout, and normalization.
384 384 384 4096 4096 1000
Fig. 2. The architecture of AlexNet . The numbers indicate the number
of channels of each feature map. Figure is reproduced based on AlexNet
VGG is proposed by Simonyan and Zisserman and won
the ﬁrst place for ILSVRC 2013 competition . Simonyan
and Zisserman proposed various depth of networks, while
the 16-layer VGG is the most widely used one due to its
moderate model size and its superior performance. The
architecture of VGG-16 is shown in Fig. 3. It has 16 convolutional layers belong to ﬁve convolution blocks. The main
difference between VGG and AlexNet is that AlexNet has
large convolution stride and large kernel size while all the
convolution kernels in VGG have same small size (3×3) and
small convolution stride (1×1). The large kernel size leads to
too many parameters and large model size, while the large
convolution stride may cause the network to miss some
ﬁne features in the lower layers. The smaller kernel size
makes the training of very deep convolution neural network
feasible while still reserving the ﬁne-grained information in
the network.
VGG demonstrated that deeper networks are possible to
obtain better performance. However, deeper networks are
more difﬁcult to train due to two problems: gradient vanishing and gradient explosion. ResNet is proposed by He
et al. to use the skip connection in convolution blocks by
sending the previous feature map to the next convolution
Max pooling
Convolution + Relu
Fully connect + Relu
Fig. 3. The architecture of VGG . Figure is reproduced based on VGG
block to overcome the gradient vanishing and gradient
explosion . The details of the skip connection are shown
in Fig. 4. With the skip connection, training of very deep
neural networks on GPUs becomes feasible.
weight layer
weight layer
Fig. 4. The architecture of Residual block . The identity mapping
can effectively reduce gradient vanishing and explosion which make the
training of very deep network feasible. Figure is reproduced based on
ResNet .
In ResNet , He et al. also evaluated networks with
different depths for image classiﬁcation. Due to its smaller
model size and superior performance, ResNet is often used
as the base network for other computer vision tasks. The
convolution blocks with skip connection also widely used
as the basic building blocks.
GoogLeNet, a 22-layer deep network, is proposed by
Szegedy et al. which won ILSVRC-2014 challenge with a top-
5 test accuracy of 93.3% . Compared to previous work
that to build a deeper network, Szegedy et al. explored to
build a wider network in which each layer has multiple
parallel convolution layers. The basic block of GoogLeNet
is inception block which consists of 4 parallel convolution
layers with different kernel sizes and followed by 1 × 1 convolution for dimension reduction purpose. The architecture
for the inception block of GoogLeNet is shown in Fig. 5.
With a carefully crafted design, they increased the depth
and width of the network while keeping the computational
cost constant.
1x1convolution
1x1 convolution
3x3 convolution
5x5 convolution
3x3 convolution
1x1convolution
1x1 convolution
Previous Layer
Concatenation
Fig. 5. The architecture of Inception block . Figure is reproduced
based on GoogLeNet .
Fig. 6. The architecture of the Dense Block proposed in DenseNet .
Figure is reproduced based on .
Most of the networks including AlexNet, VGG, and
ResNet follow a hierarchy architecture. The images are fed
to the network and features are extracted by different layers.
The shallow layers extract low-level general features, while
the deep layers extract high-level task-speciﬁc features .
However, when a network goes deeper, the deeper layers
may suffer from memorizing the low-level features needed
by the network to accomplish the task.
To alleviate this problem, Huang et al. proposed the
dense connection to send all the features before a convolution block as the input to the next convolution block in the
neural network . As shown in Fig. 6, the output features
of all the previous convolution blocks serve as the input to
the current block. In this way, the shallower blocks focus on
the low-level general features while the deeper blocks can
focus on the high-level task-speciﬁc features.
Architectures for Learning Video Features
To extract both spatial and temporal information from
videos, several architectures have been designed for video
feature learning including 2DConvNet-based methods ,
3DConvNet-based methods , and LSTM-based methods
 . The 2DConvNet-based methods apply 2DConvNet
on every single frame and the image features of multiple
frames are fused as video features. The 3DConvNet-based
methods employ 3D convolution operation to simultaneously extract both spatial and temporal features from multiple frames. The LSTM-based methods employ LSTM to
model long term dynamics within a video. This section
brieﬂy summarizes these three types of architectures of
video feature learning.
Two-Stream Network
Spatial Stream ContNet
Temporal Stream ContNet
Prediction
Eye Makeup
Optical Flow
Fig. 7. The general architecture of the two-stream network which including one spatial stream and one temporal stream. Figure is reproduced
based on .
Videos generally are composed of various numbers of
frames. To recognize actions in a video, networks are required to capture appearance features as well as temporal dynamics from frame sequences. As shown in Fig. 7,
a two-stream 2DConvNet-based network is proposed by
Simonyan and Zisserman for human action recognition,
while using a 2DConvNet to capture spatial features from
RGB stream and another 2DConvNet to capture temporal
features from optical ﬂow stream . Optical ﬂow encodes
boundary of moving objects, therefore, the temporal stream
ConvNet is relatively easier to capture the motion information within the frames.
Experiments showed that the fusion of the two streams
can signiﬁcantly improve action recognition accuracy. Later,
this work has been extended to multi-stream network ,
 , , , to fuse features from different types of
inputs such as dynamic images and difference of frames
Spatiotemporal Convolutional Neural Network
3D convolution operation was ﬁrst proposed in 3DNet
 for human action recognition. Compared to 2DConvNets which individually extract the spatial information of
each frame and then fuse them together as video features,
3DConvNets are able to simultaneously extract both spatial
and temporal features from multiple frames.
C3D is a VGG-like 11-layer 3DConvNet designed for
human action recognition. The network contains 8 convolutional layers, and 3 fully connected layers. All the kernels
have the size of 3 × 3 × 3, the convolution stride is ﬁxed
to 1 pixel. Due to its powerful ability of simultaneously
extracting both spatial and temporal features from multiple
frames, the network achieved state-of-the-art on several
video analysis tasks including human action recognition
 , action similarity labeling , scene classiﬁcation ,
and object recognition in videos .
The input of C3D is 16 consecutive RGB frames where
the appearance and temporal cues from 16-frame clips are
extracted. However, the paper of long-term temporal convolutions (LTC) argues that, for the long-lasting actions, 16
frames are insufﬁcient to represent whole actions which last
longer. Therefore, larger numbers of frames are employed
to train 3DConvNets and achieved better performance than
C3D , .
With the success of applying 3D convolution on video
analysis tasks, various 3DConvNet architectures have been
proposed , , . Hara et al. proposed 3DResNet
by replacing all the 2D convolution layers in ResNet with
3D convolution layers and showed comparable performance
with the state-of-the-art performance on action recognition
task on several datasets .
Recurrent Neural Network
Golf Swing
Average Fusion
Fig. 8. The architecture of long-term recurrent convolutional networks
(LRCN) . LSTM is employed to model the long term temporal information within a frame sequence. Figure is reproduced based on .
Due to the ability to model the temporal dynamics
within a sequence, recurrent neural networks (RNN) are
often applied to videos as ordered frame sequences. Compared to standard RNN , long short term memory
(LSTM) uses memory cells to store, modify, and access
internal states, to better model the long-term temporal relationships within video frames .
Based on the advantage of the LSTM, Donahue et al. proposed long-term recurrent convolutional networks (LRCN)
for human action recognition . The framework of the
LRCN is shown in Fig. 8. The LSTM is sequentially applied
to the features extracted by ConvNets to model the temporal
dynamics in the frame sequence. With the LSTM to model a
video as frame sequences, this model is able to explicitly
model the long-term temporal dynamics within a video.
Later on, this model is extended to a deeper LSTM for action
recognition , , video captioning , and gesture
recognition tasks .
Summary of ConvNet Architectures
Deep ConvNets have demonstrated great potential in various computer vision tasks. And the visualization of the
image and video features has shown that these networks
truly learned meaningful features that required by the corresponding tasks , , , . However, one common
drawback is that these networks can be easily over-ﬁt when
training data are scarce since there are over millions of
parameters in each network.
Take 3DResNet for an example, the performance of an
18-layer 3DResNet on UCF101 action recognition dataset
 is 42% when trained from scratch. However, with
a supervised pre-trained model on the large-scale Kinetics dataset (500, 000 videos of 600 classes) with humanannotated class labels and then ﬁne-tuned on UCF101
dataset, the performance can increase to 84%. Pre-trained
models on large-scale datasets can speed up the training
process and improve the performance on relatively small
datasets. However, the cost of collecting and annotating
large-scale datasets is very expensive and time-consuming.
In order to obtain pre-trained models from large-scale
datasets without expensive human annotations, many selfsupervised learning methods were proposed to learn image
and video features from pre-designed pretext tasks. The next
section describes the general pipeline of the self-supervised
image and video feature learning.
COMMONLY USED PRETEXT AND DOWNSTREAM
Self-supervised Pretext Task Training
Unlabeled Dataset
Objective Function
&'(((P", %")
&'(((P#, %#)
&'(((P$, %$)
Fig. 9. Self-supervised visual feature learning schema. The ConvNet is
trained by minimizing errors between pseudo labels P and predictions
O of the ConvNet. Since the pseudo labels are automatically generated,
no human annotations are involved during the whole process.
Most existing self-supervised learning approaches follow the schema shown in Fig 9. Generally, a pretext task
is deﬁned for ConvNets to solve and visual features can be
learned through the process of accomplishing this pretext
task. The pseudo labels P for pretext task can be automatically generated without human annotations. ConvNet is
optimized by minimizing the error between the prediction
of ConvNet O and the pseudo labels P. After the training
on the pretext task is ﬁnished, ConvNet models that can
capture visual features for images or videos are obtained.
Learning Visual Features from Pretext Tasks
To relieve the burden of large-scale dataset annotation, a
pretext task is generally designed for networks to solve
while pseudo labels for the pretext task are automatically
generated based on data attributes. Many pretext tasks have
been designed and applied for self-supervised learning such
as foreground object segmentation , image inpainting
 , clustering , image colorization , temporal order
veriﬁcation , visual audio correspondence veriﬁcation
Free Semantic Label-Based Methods
Moving Object
Segmentation
Relative Depth
Prediction
Contour Detection
Depth Estimation
Semantic Label
Surface Normal
Prediction
Context Similarity
Clustering
Context-Based Methods
Spatial Context Structure
Temporal Context Structure
Image Jigsaw
Transformation
Frame Order
Veriﬁcation
Frame Order
Recognition
Generation-Based Methods
Image Generation
Video Generation
Inpainting
Colorization
Video Generation
Video Colorization
Video Future
Prediction
Image Generation
Super-Resolution
Segmentation
Cross Modal-Based Methods
Flow-RGB Correspondence Visual-Audio Correspondence
Optical Flow
Estimation
Correspondence
Veriﬁcation
Audio Visual
Correspondence
Ego-motion
Ego-motion
Fig. 10. Categories of pretext tasks for self-supervised visual feature learning: generation-based, context-based, free semantic label-based, and
cross modal-based.
 , and so on. Effective pretext tasks ensure that semantic
features are learned through the process of accomplishing
the pretext tasks.
Take image colorization as an example, image colorization is a task to colorize gray-scale images into colorful
images. To generate realistic colorful images, networks are
required to learn the structure and context information of
images. In this pretext task, the data X is the gray-scale
images which can be generated by performing a linear
transformation in RGB images, while the pseudo label P
is the RGB image itself. The training pair Xi and Pi can be
generated in real time with negligible cost. Self-Supervised
learning with other pretext tasks follow a similar pipeline.
Commonly Used Pretext Tasks
According to the data attributes used to design pretext
tasks, as shown in Fig. 10, we summarize the pretext tasks
into four categories: generation-based, context-based, free
semantic label-based, and cross modal-based.
Generation-based Methods: This type of methods learn
visual features by solving pretext tasks that involve image
or video generation.
Generation:
through the process of image generation tasks. This
type of methods includes image colorization ,
image super resolution , image inpainting ,
image generation with Generative Adversarial Networks (GANs) , .
Generation:
through the process of video generation tasks. This
type of methods includes video generation with
GANs , and video prediction .
Context-based pretext tasks: The design of contextbased pretext tasks mainly employ the context features of
images or videos such as context similarity, spatial structure,
temporal structure, etc.
Context Similarity: Pretext tasks are designed based
on the context similarity between image patches.
This type of methods includes image clusteringbased methods , , and graph constraint-based
methods .
Spatial Context Structure: Pretext tasks are used to
train ConvNets are based on the spatial relations
among image patches. This type of methods includes
image jigsaw puzzle , , , , context
prediction , and geometric transformation recognition , , etc.
Temporal Context Structure: The temporal order
from videos is used as supervision signal. The ConvNet is trained to verify whether the input frame
sequence in correct order , or to recognize
the order of the frame sequence .
Free Semantic Label-based Methods: This type of pretext tasks train networks with automatically generated semantic labels. The labels are generated by traditional hardcode algorithms , or by game engines . The
pretext tasks include moving object segmentation , ,
contour detection , , relative depth prediction ,
Cross Modal-based Methods: This type of pretext tasks
train ConvNets to verify whether two different channels
of input data are corresponding to each other. This type
of methods includes Visual-Audio Correspondence Veriﬁcation , , RGB-Flow Correspondence Veriﬁcation ,
and egomotion , .
Commonly Used Downstream Tasks for Evaluation
To evaluate the quality of the learned image or video features by self-supervised methods, the learned parameters by
self-supervised learning are employed as pre-trained models and then ﬁne-tuned on downstream tasks such as image classiﬁcation, semantic segmentation, object detection,
and action recognition etc. The performance of the transfer
learning on these high-level vision tasks demonstrates the
generalization ability of the learned features. If ConvNets
of self-supervised learning can learn general features, then
the pre-trained models can be used as a good starting point
for other vision tasks that require capturing similar features
from images or videos.
Image classiﬁcation, semantic segmentation, and object
detection usually are used as the tasks to evaluate the
generalization ability of the learned image features by selfsupervised learning methods, while human action recognition in videos is used to evaluate the quality of video
features obtained from self-supervised learning methods.
Below are brief introductions of the commonly used highlevel tasks for visual feature evaluation.
Semantic Segmentation
Semantic segmentation, the task of assigning semantic labels
to each pixel in images, is of great importance in many
applications such as autonomous driving, human-machine
interaction, and robotics. The community has recently made
promising progress and various networks have been proposed such as Fully Convolutional Network (FCN) ,
DeepLab , PSPNet and datasets such as PASCAL VOC
 , CityScape , ADE20K .
Among all these methods, FCN is a milestone work
for semantic segmentation since it started the era of applying fully convolution network (FCN) to solve this task. The
architecture of FCN is shown in Fig. 11. 2DConvNet such
as AlexNet, VGG, ResNet is used as the base network for
feature extraction while the fully connected layer is replaced
by transposed convolution layer to obtain the dense prediction. The network is trained end-to-end with pixel-wise
annotations.
When using semantic segmentation as downstream task
to evaluate the quality of image features learned by selfsupervised learning methods, the FCN is initialized with the
parameters trained with the pretext task and ﬁne-tuned on
the semantic segmentation dataset, then the performance on
the semantic segmentation task is evaluated and compared
with that of other self-supervised methods.
384 384 384 4096 4096 21
Pixelwise prediction
Input Image
Fig. 11. The framework of the Fully Convolutional Neural Network proposed for semantic segmentation . Figure is reproduced based on
Object Detection
Object Detection, a task of localizing the position of objects
in images and recognizing the category of the objects, is also
very import for many computer vision applications such
as autonomous driving, robotics, scene text detection and
so on. Recently, many datasets such as MSCOCO and
OpenImage have been proposed for object detection
and many ConvNet-based models , , , , ,
 , , have been proposed and obtained great
performance.
Fast-RCNN is a two-stage network for object detection. The framework of Fast-RCNN is shown in Fig. 12.
Object proposals are generated based on feature maps produced by a convolution neural network, then these proposals are fed to several fully connected layers to generate the
bounding box of objects and the categories of these objects.
Deep ConvNet
ROI Projection
Feature Map
ROI feature vector
For each RoI
Fig. 12. The pipeline of the Fast-RCNN for object detection. Figure is
reproduced based on .
When using object detection as downstream task to evaluate the quality of the self-supervised image features, networks that trained with the pretext task on unlabeled large
data are served as the pre-trained model for the Fast-RCNN
 and then ﬁne-tuned on object detection datasets, then
the performance on the object detection task is evaluated
to demonstrate the generalization ability of self-supervised
learned features.
Image Classiﬁcation
Image Classiﬁcation is a task of recognizing the category of
objects in each image. Many networks have been designed
for this task such as AlexNet , VGG , ResNet ,
GoogLeNet , DenseNet , etc. Usually, only one class
label is available for each image although the image may
contains different classes of objects.
When choosing image classiﬁcation as a downstream
task to evaluate the quality of image features learned
from self-supervised learning methods, the self-supervised
learned model is applied on each image to extract features
which then are used to train a classiﬁer such as Support Vector Machine (SVM) . The classiﬁcation performance on
testing data is compared with other self-supervised models
to evaluate the quality of the learned features.
Human Action Recognition
Human action recognition is a task of identifying what people doing in videos for a list of pre-deﬁned action classes.
Generally, videos in human action recognition datasets contain only one action in each video , , . Both the
spatial and temporal features are needed to accomplish this
The action recognition task is often used to evaluate the
quality of video features learned by self-supervised learning
methods. The network is ﬁrst trained on unlabeled video
data with pretext tasks, then it is ﬁne-tuned on action recognition datasets with human annotations to recognize the
actions. The testing performance on action recognition task
is compared with other self-supervised learning methods to
evaluate the quality of the learned features.
Qualitative Evaluation
In addition to these quantitative evaluations of the learned
features, there are also some qualitative visualization methods to evaluate the quality of self-supervised learning features. Three methods are often used for this purpose: kernel
visualization, feature map visualization, and image retrieval
visualization , , , .
Kernel Visualization: Qualitatively visualize the kernels
of the ﬁrst convolution layer learned with the pretext tasks
and compare the kernels from supervised models. The
similarity of the kernels learned by supervised and selfsupervised models are compared to indicate the effectiveness of self-supervised methods , .
Feature Map Visualization: Feature maps are visualized to show the attention of networks. Larger activation
represents the neural network pays more attention to the
corresponding region in the image. Feature maps are usually
qualitatively visualized and compared with that of supervised models , .
Nearest Neighbor Retrieval: In general, images with
similar appearance usually are closer in the feature space.
The nearest neighbor method is used to ﬁnd the top K
nearest neighbors from the feature space of the features
learned by the self-supervised learned model , , .
This section summarizes the commonly used image and
video datasets for training and evaluating of self-supervised
visual feature learning methods. Self-supervised learning
methods can be trained with images or videos by discarding human-annotated labels, therefore, any datasets
that collected for supervised learning can be used for selfsupervised visual feature learning without using humanannotated labels. The evaluation of the quality of learned
features is normally conducted by ﬁne-tuned on high-level
vision tasks with relatively small datasets (normally with
accurate labels) such as video action recognition, object
detection, semantic segmentation, etc. It is worth noting
that networks use these synthetic datasets for visual feature
learning are considered as self-supervised learning in this
paper since labels of synthetic datasets are automatically
generated by game engines and no human annotations are
involved. Table 1 summarizes the commonly used image
and video datasets.
Image Datasets
ImageNet: The ImageNet dataset contains 1.3
million images uniformly distributed into 1, 000
classes and is organized according to the WordNet
hierarchy. Each image is assigned with only one class
label. ImageNet is the most widely used dataset for
self-supervised image feature learning.
Places: The Places dataset is proposed for scene
recognition and contains more than 2.5 million images covering more than 205 scene categories with
more than 5, 000 images per category.
Places365: The Places365 is the 2nd generation of the
Places database which is built for high-level visual
understanding tasks, such as scene context, object
recognition, action and event prediction, and theoryof-mind inference . There are more than 10
million images covering more than 400 classes and
5, 000 to 30, 000 training images per class.
SUNCG: The SUNCG dataset is a large synthetic
3D scene repository for indoor scenes which consists of over 45, 000 different scenes with manually
created realistic room and furniture layouts .
The synthetic depth, object level semantic labels, and
volumetric ground truth are available.
MNIST: The MNIST is a dataset of handwritten digits consisting of 70, 000 images while 60, 000 images
belong to training set and the rest 10, 000 images are
for testing . All digits have been size-normalized
and centered in ﬁxed-size images.
SVHN is a dataset for recognizing digits
and numbers in natural scene images which obtained from house numbers from Google Street View
images . The dataset consists of over 600, 000
images and all digits have been resized to a ﬁxed
resolution of 32 × 32 pixels.
CIFAR10: The CIFAR10 dataset is a collection of
tiny images for image classiﬁcation task . It
consists of 60, 000 images of size 32 × 32 that covers
10 different classes. The 10 classes include airplane,
automobile, bird, cat, deer, dog, frog, horse, ship, and
truck. The dataset is balanced and there are 6, 000
images of each class.
STL-10: The STL-10 dataset is speciﬁcally designed
for developing unsupervised feature learning .
It consists of 500 labeled training images, 800 testing
images, and 100, 000 unlabeled images covering 10
Summary of commonly used image and video datasets. Note that image datasets can be used to learn image features, while video datasets can
be used to learn both image and video features.
ImageNet 
1.3 million images
Object category label
Places 
2.5 million images
scene categories label
Places365 
10 million images
scene categories label
SUNCG 
150, 000 images
depth, volumetric data
MNIST 
70, 000 images
Digit class label
SVHN 
600, 000 Images
Digit class label
CIFAR10 
60, 000 Images
Object category label
STL-10 
101, 300 Images
Object category label
PASCAL VOC 
2, 913 images
Category label, bounding box, segmentation mask
YFCC100M 
Image/Video
100 million media data
SceneNet RGB-D 
5 million images
Depth, Instance Segmentation, Optical Flow
Moment-in-Time 
1 million 3-second videos
Video category class
Kinetics 
0.5 million 10-second videos
Human action class
AudioSet 
2 million 10-second videos
Audio event class
KITTI 
Data captured by various sensors are available
UCF101 
10, 031 videos
Human action class
HMDB51 
6, 766 videos
Human action class
classes which include airplane, bird, car, cat, deer,
dog, horse, monkey, ship, and truck.
PASCAL Visual Object Classes (VOC): The VOC
2,012 dataset contains 20 object categories including vehicles, household, animals, and other:
aeroplane, bicycle, boat, bus, car, motorbike, train,
TV/monitor, bird, cat, cow, dog, horse, sheep, and
person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations,
and object class annotations. This dataset has been
widely used as a benchmark for object detection,
semantic segmentation, and classiﬁcation tasks. The
PASCAL VOC dataset is split into three subsets:
1, 464 images for training, 1, 449 images for validation and a private testing . All the self-supervised
image representation learning methods are evaluated
on this dataset with the three tasks.
Video Datasets
YFCC100M: The Yahoo Flickr Creative Commons
100 Million Dataset (YFCC100M) is a large public
multimedia collection from Flickr, consisting of 100
million media data, of which around 99.2 million are
images and 0.8 million are videos . The statistics
on hashtags used in the YFCC100M dataset show
that the data distribution is severely unbalanced
SceneNet RGB-D: The SceneNet RGB-D dataset is a
large indoor synthetic video dataset which consists of
5 million rendered RGB-D images from over 15K trajectories in synthetic layouts with random but physically simulated object poses . It provides pixellevel annotations for scene understanding problems
such as semantic segmentation, instance segmentation, and object detection, and also for geometric
computer vision problems such as optical ﬂow, depth
estimation, camera pose estimation, and 3D reconstruction .
Moment in Time: The Moment-in-Time dataset is a
large balanced and diverse dataset for video understanding . The dataset consists of 1 million video
clips that cover 339 classes, and each video lasts
around 3 seconds. The average number of video clips
for each class is 1, 757 with a median of 2, 775. The
video in this dataset contains videos that capturing
visual and/or audible actions, produced by humans,
animals, objects or nature .
Kinetics: The Kinetics dataset is a large-scale, highquality dataset for human action recognition in
videos . The dataset consists of around 500, 000
video clips covering 600 human action classes with at
least 600 video clips for each action class. Each video
clip lasts around 10 seconds and is labeled with a
single action class.
of 2, 084, 320
human-labeled 10-second sound clips drawn from
YouTube videos covers ontology of 632 audio event
classes . The event classes cover a wide range
of human and animal sounds, musical instruments
and genres, and common everyday environmental
sounds. This dataset is mainly used for the selfsupervised learning from video and audio consistence .
KITTI: The KITTI dataset is collected from driving
a car around a city which equipped with various
sensors including high-resolution RGB camera, grayscale stereo camera, a 3D laser scanner, and highprecision GPS measurements and IMU accelerations
from a combined GPS/IMU system . Videos
with various modalities captured by these sensors
are available in this dataset.
UCF101: The UCF101 is a widely used video dataset
for human action recognition . The dataset consists of 13, 370 video clips with more than 27 hours
belonging to 101 categories in this dataset. The
videos in this dataset have a spatial resolution of
320 × 240 pixels and 25 FPS frame rate. This dataset
has been widely used for evaluating the performance
of human action recognition. In the self-supervised
sensorial, the self-supervised models are ﬁne-tuned
on the dataset and the accuracy of the action recog-
Summary of self-supervised image feature learning methods based on the category of pretext tasks. Multi-task means the method explicitly or
implicitly uses multiple pretext tasks for image feature learning.
Contribution
Generation
Forerunner of GAN
DCGAN 
Generation
Deep convolutional GAN for image generation
WGAN 
Generation
Proposed WGAN which makes the training of GAN more stable
BiGAN 
Generation
Bidirectional GAN to project data into latent space
SelfGAN 
Use rotation recognition and GAN for self-supervised learning
ColorfulColorization 
Generation
Posing image colorization as a classiﬁcation task
Colorization 
Generation
Using image colorization as the pretext task
AutoColor 
Generation
Training ConvNet to predict per-pixel color histograms
Split-Brain 
Generation
Using split-brain auto-encoder as the pretext task
Context Encoder 
Generation
Employing ConvNet to solve image inpainting
CompletNet 
Generation
Employing two discriminators to guarantee local and global consistent
SRGAN 
Generation
Employing GAN for single image super-resolution
SpotArtifacts 
Generation
Learning by recognizing synthetic artifacts in images
ImproveContext 
Techniques to improve context based self-supervised learning methods
Context Prediction 
Learning by predicting the relative position of two patches from an image
Jigsaw 
Image patch Jigsaw puzzle as the pretext task for self-supervised learning
Damaged Jigsaw 
Learning by solving jigsaw puzzle, inpainting, and colorization together
Arbitrary Jigsaw 
Learning with jigsaw puzzles with arbitrary grid size and dimension
DeepPermNet 
A new method to solve image patch jigsaw puzzle
RotNet 
Learning by recognizing rotations of images
Boosting 
Using clustering to boost the self-supervised learning methods
JointCluster 
Jointly learning of deep representations and image clusters
DeepCluster 
Using clustering as the pretext
ClusterEmbegging 
Deep embedded clustering for self-supervised learning
GraphConstraint 
Learning with image pairs mined with Fisher Vector
Ranking 
Learning by ranking video frames with a triplet loss
PredictNoise 
Learning by mapping images to a uniform distribution over a manifold
MultiTask 
Using multiple pretext tasks for self-supervised feature learning
Learning2Count 
Learning by counting visual primitive
Watching Move 
Free Semantic Label
Learning by grouping pixels of moving objects in videos
Edge Detection 
Free Semantic Label
Learning by detecting edges
Cross Domain 
Free Semantic Label
Utilizing synthetic data and its labels rendered by game engines
nition are reported to evaluate the quality of the
HMDB51: Compared to other datasets, the HMDB51
dataset is a smaller video dataset for human action
recognition. There are around 7, 000 video clips in
this dataset belong to 51 human action categories
 . The videos in HMDB51 dataset have 320×240
pixels spatial resolution and 30 FPS frame rate. In the
self-supervised sensorial, the self-supervised models
are ﬁne-tuned on the dataset to evaluate the quality
of the learned video features.
IMAGE FEATURE LEARNING
In this section, three groups of self-supervised image feature
learning methods are reviewed including generation-based
methods, context-based methods, and free semantic labelbased methods. A list of the image feature self-supervised
learning methods can be found in Table 2. Since the cross
modal-based methods mainly learn features from videos
and most methods of this type can be used for both image
and video feature learning, so cross modal-based methods
are reviewed in the video feature learning section.
Generation-based Image Feature Learning
Generation-based self-supervised methods for learning image features involve the process of generating images including image generation with GAN (to generate fake images), super-resolution (to generate high-resolution images),
image inpainting (to predict missing image regions), and image colorization (to colorize gray-scale images into colorful
images). For these tasks, pseudo training labels P usually
are the images themselves and no human-annotated labels
are needed during training, therefore, these methods belong
to self-supervised learning methods.
The pioneer work about the image generation-based
methods is the Autoencoder which learns to compress
an image into a low-dimension vector which then is uncompressed into the image that closes to the original image
with a bunch of layers. With an auto-encoder, networks can
reduce the dimension of an image into a lower dimension
vector that contains the main information of the original
image. The current image generation-based methods follow
a similar idea but with different pipelines to learn visual
features through the process of image generation.
Image Generation with GAN
Generative Adversarial Network (GAN) is a type of deep
generative model that was proposed by Goodfellow et al.
 . A GAN model generally consists of two kinds of
networks: a generator which is to generate images from
latent vectors and a discriminator which is to distinguish
whether the input image is generated by the generator. By
playing the two-player game, the discriminator forces the
generator to generate realistic images, while the generator
forces the discriminator to improve its differentiation ability.
During the training, the two networks are competing against
with each other and make each other stronger.
The common architecture for the image generation from
a latent variable task is shown in Fig. 13. The generator is
trained to map any latent vector sampled from latent space
into an image, while the discriminator is forced to distinguish whether the image from the real data distribution or
generated data distribution. Therefore, the discriminator is
required to capture the semantic features from images to
accomplish the task. The parameters of the discriminator
can server as the pre-trained model for other computer
vision tasks.
Discriminator
Real World Images
Fig. 13. The pipeline of Generative Adversarial Networks . By playing
the two-player game, the discriminator forces the generator to generate
realistic images, while the generator forces the discriminator to improve
its differentiation ability.
Mathematically, the generator G is trained to learn a
distribution pz of real word image data to generate realist
data that undistinguished from the real data, while the discriminator D is trained to distinguish the distribution of the
real data pdata and of the data distribution pz generated by
the generator G. The min-max game between the generator
G and the discriminator D is formulated as:
Ex∼pdata(x)[logD(x)] + Ez∼pz(z)[log(1−D(G(z)))], (5)
where x is the real data, G(z) is the generated data.
The discriminator D is trained to maximize the probability for the real data x (that is, Ex∼pdata(x)[logD(x)]) and
minimize the probability for the generated data G(z) (that
is, Ex∼pdata(x)[logD(x)]). The generator is trained to generate
data that close to real data x, so as the output of the
discriminator is maximized Ex∼pdata(x)[logD(G(z))].
Most of the methods for image generation from random
variables do not need any human-annotated labels. However, the main purpose of this type of task is to generate
realistic images instead of obtaining better performance on
downstream applications. Generally, the inception scores of
the generated images are used to evaluate the quality of
the generated images , . And only a few methods
evaluated the quality of the feature learned by the discriminator on the high-level tasks and compared with others
 , , .
The adversarial training can help the network to capture
the real distribution of the real data and generate realists
data, and it has been widely used in computer vision tasks
such as image generation , , video generation
 , , super-resolution , image translation , and
image inpainting , . When there is no humanannotated label involves, the method falls into the selfsupervised learning.
Image Generation with Inpainting
(a) Input Context
(b) Human Artist
(c) Network Prediction
Fig. 14. Qualitative illustration of image inpainting task. Given an image
with a missing region (a), a human artist has no trouble inpainting it
(b). Automatic inpainting using context encoder proposed in trained
with L2 reconstruction loss and adversarial loss is shown in (c). Figure
is reproduced based on .
Image inpainting is a task of predicting arbitrary missing regions based on the rest of an image. A qualitative
illustration of the image inpainting task is shown in Fig. 14.
The Fig. 14(a) is an image with a missing region, while the
Fig 14(c) is the prediction of networks. To correctly predict
missing regions, networks are required to learn the common
knowledge including the color and structure of the common
objects. Only by knowing this knowledge, networks are able
to infer missing regions based on the rest part of the image.
By analogy with auto-encoders, Pathak et al. made the
ﬁrst step to train a ConvNet to generate the contents of
an arbitrary image region based on the rest of the image
 . Their contributions are in two folds: using a ConvNet to tackle image inpainting problem, and using the
adversarial loss to help the network generate a realistic
hypothesis. Most of the recent methods follow a similar
pipeline . Usually, there are two kinds of networks: a
generator network is to generate the missing region with the
pixel-wise reconstruction loss and a discriminator network
is to distinguish whether the input image is real with an
adversarial loss. With the adversarial loss, the network is
able to generate sharper and realistic hypothesis for the
missing image region. Both the two kinds of networks are
able to learn the semantic features from images and can
be transferred to other computer vision tasks. However,
only Pathak et al. studied the performance of transfer
learning for the learned parameters of the generator from
the image inpainting task.
The generator network which is a fully convolutional
network has two parts: encoder and decoder. The input of
the encoder is the image that needs to be inpainted and the
context encoder learns the semantic feature of the image.
The context decoder is to predict the missing region based
on this feature. The generator is required to understand the
content of the image in order to generate a plausible hypothesis. The discriminator is trained to distinguish whether the
input image is the output of the generator. To accomplish
the image inpainting task, both networks are required to
learn semantic features of images.
Image Generation with Super Resolution
Image super-resolution (SR) is a task of enhancing the
resolution of images. With the help of fully convolutional
networks, ﬁner and realistic high-resolution images can be
generated from low-resolution images. SRGAN is a generative adversarial network for single image super-resolution
proposed by Ledig et al. . The insight of this approach is
to take advantage of the perceptual loss which consists of an
adversarial loss and a content loss. With the perceptron loss,
the SRGAN is able to recover photo-realistic textures from
heavily downsampled images and show signiﬁcant gains in
perceptual quality.
There are two networks: one is generator which is to
enhance the resolution of the input low-resolution image
and the other is the discriminator which is to distinguish
whether the input image is the output of the generator. The
loss function for the generator is the pixel-wise L2 loss plus
the content loss which is the similarity of the feature of
the predicted high-resolution image and the high-resolution
original image, while the loss for the discriminator is the binary classiﬁcation loss. Compared to the network that only
minimizing the Mean Squared Error (MSE) which generally
leads to high peak signal-to-noise ratios but lacking highfrequency details, the SRGAN is able to recover the ﬁne
details of the high-resolution image since the adversarial
loss pushes the output to the natural image manifold by the
discriminator network.
The networks for image super-resolution task are able
to learn the semantic features of images. Similar to other
GANs, the parameters of the discriminator network can be
transferred to other downstream tasks. However, no one
tested the performance of the transferred learning on other
tasks yet. The quality of the enhanced image is mainly
compared to evaluate the performance of the network.
Image Generation with Colorization
Fig. 15. The architecture of image colorization proposed in . The
ﬁgure is from with author’s permission.
Image colorization is a task of predicting a plausible
color version of the photograph given a gray-scale photograph as input. A qualitative illustration of the image
colorization task is shown in Fig. 15. To correctly colorize
each pixel, networks need to recognize objects and to group
pixels of the same part together. Therefore, visual features
can be learned in the process of accomplishing this task.
Many deep learning-based colorization methods have
been proposed in recent years , , . A straightforward idea would be to employ a fully convolution neural
network which consists of an encoder for feature extraction
and a decoder for the color hallucination to colorization.
The network can be optimized with L2 loss between the
predicted color and its original color. Zhang et al. proposed
to handle the uncertainty by posting the task as a classiﬁcation task and used class-rebalancing to increase the
diversity of predicted colors . The framework for image
colorization proposed by Zhang et al. is shown in Fig. 15.
Trained in large-scale image collections, the method shows
great results and fools human on 32% of the trials during
the colorization test.
Some work speciﬁcally employs the image colorization
task as the pretext for self-supervised image representation
learning , , , . After the image colorization
training is ﬁnished, the features learned through the colorization process are speciﬁcally evaluated on other downstream high-level tasks with transfer learning.
Context-Based Image Feature Learning
The context-based pretext tasks mainly employ the context features of images including context similarity, spatial
structure, and temporal structure as the supervision signal.
Features are learned by ConvNet through the process of
solving the pretext tasks designed based on attributes of the
context of images.
Learning with Context Similarity
Fig. 16. The architecture of DeepClustering . The features of images are iteratively clustered and the cluster assignments are used as
pseudo-labels to learn the parameters of the ConvNet. The ﬁgure is from
 with author’s permission.
Clustering is a method of grouping sets of similar data
in the same clusters. Due to its powerful ability of grouping
data by using the attributes of the data, it is widely used
in many ﬁelds such as machine learning, image processing,
computer graphics, etc. Many classical clustering algorithms
have been proposed for various applications .
In the self-supervised scenario, the clustering methods
mainly employed as a tool to cluster image data. A naive
method would be to cluster the image data based on the
hand-designed feature such as HOG , SIFT , or
Fisher Vector . After the clustering, several clusters are
obtained while the image within one cluster has a smaller
distance in feature space and images from different clusters
have a larger distance in feature space. The smaller the
distance in feature space, the more similar the image in
the appearance in the RGB space. Then a ConvNet can be
trained to classify the data by using the cluster assignment
as the pseudo class label. To accomplish this task, the ConvNet needs to learn the invariance within one class and the
variance among different classes. Therefore, the ConvNet is
able to learn semantic meaning of images.
The existing methods about using the clustering variants
as the pretext task follow these principals , , ,
 , . Firstly, the image is clustered into different clusters which the images from the same cluster have smaller
distance and images from different clusters have larger
distance. Then a ConvNet is trained to recognize the cluster
assignment , or to recognize whether two imaged
are from same cluster . The pipeline of DeepCluster, a
clustering based methods, is shown in Fig. 16. DeepCluster
iteratively clusters images with Kmeans and use the subsequent assignments as supervision to update the weights
of the network. And it is the current state-of-the-art for the
self-supervised image representation learning.
Learning with Spatial Context Structure
Images contain rich spatial context information such as the
relative positions among different patches from an image
which can be used to design the pretext task for selfsupervised learning. The pretext task can be to predict the
relative positions of two patches from same image , or
to recognize the order of the shufﬂed a sequence of patches
from same image , , . The context of full images
can also be used as a supervision signal to design pretext
tasks such as to recognize the rotating angles of the whole
images . To accomplish these pretext tasks, ConvNets
need to learn spatial context information such as the shape
of the objects and the relative positions of different parts of
an object.
Fig. 17. The visualization of the Jigsaw Image Puzzle . (a) is an
image with 9 sampled image patches, (b) is an example of shufﬂed
image patches, and (c) shows the correct order of the sampled 9
patches. Figure is reproduced based on .
The method proposed by Doersch et al. is one of the pioneer work of using spatial context cues for self-supervised
visual feature learning . Random pairs of image patches
are extracted from each image, then a ConvNet is trained
to recognize the relative positions of the two image patches.
To solve this puzzle, ConvNets need to recognize objects in
images and learn the relationships among different parts of
objects. To avoid the network learns trivial solutions such as
simply using edges in patches to accomplish the task, heavy
data augmentation is applied during the training phase.
Following this idea, more methods are proposed to learn
image features by solving more difﬁcult spatial puzzles ,
 , , , . As illustrated in Fig. 17, one typical work
proposed by Noroozi et al. attempted to solve an image
Jigsaw puzzle with ConvNet . Fig. 17(a) is an image with
9 sampled image patches, Fig 17(b) is an example of shufﬂed
image patches, and Fig 17(c) shows the correct order of the
sampled 9 patches. The shufﬂed image patches are fed to
the network which trained to recognize the correct spatial
locations of the input patches by learning spatial context
structures of images such as object color, structure, and highlevel semantic information.
Given 9 image patches from an image, there are 362, 880
(9!) possible permutations and a network is very unlikely to
recognize all of them because of the ambiguity of the task.
To limit the number of permutations, usually, hamming
distance is employed to choose only a subset of permutations among all the permutations that with relative large
hamming distance. Only the selected permutations are used
to train ConvNet to recognize the permutation of shufﬂed
image patches , , , .
The main principle of designing puzzle tasks is to ﬁnd a
suitable task which is not too difﬁcult and not too easy for
a network to solve. If it is too difﬁcult, the network may not
converge due to the ambiguity of the task or can easily learn
trivial solutions if it is too easy. Therefore, a reduction in the
search space is usually employed to reduce the difﬁculty of
Free Semantic Label-based Image Feature Learning
The free semantic label refers to labels with semantic meanings that obtained without involving any human annotations. Generally, the free semantic labels such as segmentation masks, depth images, optic ﬂows, and surface normal
images can be rendered by game engine or generated by
hard-code methods. Since these semantic labels are automatically generated, the methods using the synthetic datasets
or using them in conjunction with a large unlabeled image
or video datasets are considered as self-supervised learning
Learning with Labels Generated by Game Engines
Given models of various objects and layouts of environments, game engines are able to render realistic images
and provide accurate pixel-level labels. Since game engines
can generate large-scale datasets with negligible cost, various game engines such as Airsim and Carla 
have been used to generate large-scale synthetic datasets
with high-level semantic labels including depth, contours,
surface normal, segmentation mask, and optical ﬂow for
training deep networks. An example of an RGB image with
its generated accurate labels is shown in Fig. 18.
Instance Segmentation
Synthetic Image
Optical Flow
Fig. 18. An example of an indoor scene generated by a game engine
 . For each synthetic image, the corresponding depth, instance
segmentation, and optical ﬂow can be automatically generated by the
Game engines can generate realistic images with accurate pixel-level labels with very low cost. However, due to
the domain gap between synthetic and real-world images,
the ConvNet purely trained on synthetic images cannot be
directly applied to real-world images. To utilize synthetic
datasets for self-supervised feature learning, the domain
gap needs to be explicitly bridged. In this way, the ConvNet
trained with the semantic labels of the synthetic dataset can
be effectively applied to real-world images.
To overcome the problem, Ren and Lee proposed an unsupervised feature space domain adaptation method based
on adversarial learning . As shown in Fig. 19, the network predicts surface normal, depth, and instance contour
for the synthetic images and a discriminator network D
is employed to minimize the difference of feature space
domains between real-world and synthetic data. Helped
with adversarial training and accurate semantic labels of
synthetic images, the network is able to capture visual
features for real-world images.
Synthetic Data
Real World Data
Real / Synthetics
Parameters
Fig. 19. The architecture for utilizing synthetic and real-world images
for self-supervised feature learning . Figure is reproduced based on
Compared to other pretext tasks in which the pretext
tasks implicitly force ConvNets to learn semantic features,
this type of methods are trained with accurate semantic
labels which explicitly force ConvNets to learn features that
highly related to the objects in images.
Learning with Labels Generated by Hard-code programs
Applying hard-code programs is another way to automatically generate semantic labels such as salience, foreground
masks, contours, depth for images and videos. With these
methods, very large-scale datasets with generated semantic
labels can be used for self-supervised feature learning. This
type of methods generally has two steps: (1) label generation
by employing hard-code programs on images or videos to
obtain labels, (2) train ConvNets with the generated labels.
Various hard-code programs have been applied to generate labels for self-supervised learning methods include
methods for foreground object segmentation , edge detection , and relative depth prediction . Pathak et al.
proposed to learn features by training a ConvNet to segment
foreground objects in each frame of a video while the label is
the mask of moving objects in videos . Li et al. proposed
to learn features by training a ConvNet for edge prediction
while labels are motion edges obtained from ﬂow ﬁelds
from videos . Jing et al. proposed to learn features by
training a ConvNet to predict relative scene depths while
the labels are generated from optical ﬂow .
No matter what kind of labels used to train ConvNets,
the general idea of this type of methods is to distill knowledge from hard-code detector. The hard-code detector can
be edge detector, salience detector, relative detector, etc.
As long as no human-annotations are involved through
the design of detectors, then the detectors can be used to
generate labels for self-supervised training.
Compared to other self-supervised learning methods,
the supervision signal in these pretext tasks is semantic labels which can directly drive the ConvNet to learn semantic
features. However, one drawback is that the semantic labels
generated by hard-code detector usually are very noisy
which need to speciﬁcally cope with.
VIDEO FEATURE LEARNING
This section reviews the self-supervised methods for learning video features, as listed in Table 3, they can be categorized into four classes: generation-based methods, contextbased methods, free semantic label-based methods, and
cross modal-based methods.
Since video features can be obtained by various kinds
of networks including 2DConvNet, 3DConvNet, and LSTM
combined with 2DConvNet or 3DConvNet. When 2DConvNet is employed for video self-supervised feature learning,
then the 2DConvNet is able to extract both image and
video features after the self-supervised pretext task training
Generation-based Video Feature Learning
Learning from video generation refers to the methods that
visual features are learned through the process of video generation while without using any human-annotated labels.
This type of methods includes video generation with GAN
 , video colorization and video prediction . For
these pretext tasks, the pseudo training label P usually is
the video itself and no human-annotated labels are needed
during training, therefore, these methods belong to selfsupervised learning.
Learning from Video Generation
Background Stream
2D convolutions
Foreground Stream
3D convolutions
Foreground
Background
Replicate over Time
m ⊙f + (1 −m) ⊙b
Generated Video
Space-Time Cuboid
Fig. 20. The architecture of the generator in VideoGan for video generation with GAN proposed in . The ﬁgure is from with author’s
permission.
Summary of self-supervised video feature learning methods based on the category of pretext tasks.
SubCategory
Contribution
VideoGAN 
Generation
Forerunner of video generation with GAN
MocoGAN 
Generation
Decomposing motion and content for video generation with GAN
TemporalGAN 
Generation
Decomposing temporal and image generator for video generation
Video Colorization 
Generation
Employing video colorization as the pretext task
Un-LSTM 
Generation
Forerunner of video prediction with LSTM
ConvLSTM 
Generation
Employing Convolutional LSTM for video prediction
MCNet 
Generation
Disentangling motion and content for video prediction
LSTMDynamics 
Generation
Learning by predicting long-term temporal dynamic in videos
Video Jigsaw 
Learning by jointly reasoning about spatial and temporal context
Transitive 
Learning inter and intra instance variations with a Triplet loss
3DRotNet 
Learning by recognizing rotations of video clips
CubicPuzzles 
Learning by solving video cubic puzzles
ShufﬂeLearn 
Employing temporal order veriﬁcation as the pretext task
LSTMPermute 
Learning by temporal order veriﬁcation with LSTM
Using frame sequence order recognition as the pretext task
Learning by identifying odd video sequences
ArrowTime 
Learning by recognizing the arrow of time in videos
TemporalCoherence 
Learning with the temporal coherence of features of frame sequence
FlowNet 
Cross Modal
Forerunner of optical ﬂow estimation with ConvNet
FlowNet2 
Cross Modal
Better architecture and better performance on optical ﬂow estimation
UnFlow 
Cross Modal
An unsupervised loss for optical ﬂow estimation
CrossPixel 
Cross Modal
Learning by predicting motion from a single image as the pretext task
CrossModel 
Cross Modal
Optical ﬂow and RGB correspondence veriﬁcation as pretext task
Cross Modal
Visual and Audio correspondence veriﬁcation as pretext task
AudioVisual 
Cross Modal
Jointly modeling visual and audio as fused multisensory representation
LookListenLearn 
Cross Modal
Forerunner of Audio-Visual Correspondence for self-supervised learning
AmbientSound 
Cross Modal
Predicting a statistical summary of the sound from a video frame
EgoMotion 
Cross Modal
Learning by predicting camera motion and the scene structure from videos
LearnByMove 
Cross Modal
Learning by predicting the camera transformation from a pairs of images
TiedEgoMotion 
Cross Modal
Learning from ego-motor signals and video sequence
GoNet 
Cross Modal
Jointly learning monocular depth, optical ﬂow and ego-motion estimation from videos
DepthFlow 
Cross Modal
Depth and optical ﬂow learning using cross-task consistency from videos
VisualOdometry 
Cross Modal
An unsupervised paradigm for deep visual odometry learning
ActivesStereoNet 
Cross Modal
End-to-end self-supervised learning of depth from active stereo systems
After GAN-based methods obtained breakthrough results in image generation, researchers employed GAN to
generate videos , , . One pioneer work of video
generation with GAN is VideoGAN , and the architecture of the generator network is shown in Fig. 20. To model
the motion of objects in videos, a two-stream network is
proposed for video generation while one stream is to model
the static regions in in videos as background and another
stream is to model moving object in videos as foreground
 . Videos are generated by the combination of the foreground and background streams. The underline assumption
is that each random variable in the latent space represents
one video clip. This method is able to generate videos with
dynamic contents. However, Tulyakov et al. argues that this
assumption increases difﬁculties of the generation, instead,
they proposed MocoGAN to use the combination of two
subspace to represent a video by disentangling the context
and motions in videos . One space is context space which
each variable from this space represents one identity, and
another space is motion space while the trajectory in this
space represents the motion of the identity. With the two
sub-spaces, the network is able to generate videos with
higher inception score.
The generator learns to map latent vectors from latent
space into videos, while discriminator learns to distinguish
the real world videos with generated videos. Therefore, the
discriminator needs to capture the semantic features from
videos to accomplish this task. When no human-annotated
labels are used in these frameworks, they belong to the
self-supervised learning methods. After the video generation training on large-scale unlabeled dataset ﬁnished, the
parameters of discriminator can be transferred to other
downstream tasks .
Learning from Video Colorization
Temporal coherence in videos refers to that consecutive
frames within a short time have similar coherent appearance. The coherence of color can be used to design pretext
tasks for self-supervised learning. One way to utilize color
coherence is to use video colorization as a pretext task for
self-supervised video feature learning.
Video colorization is a task to colorize gray-scale frames
into colorful frames. Vondrick et al. proposed to constrain
colorization models to solve video colorization by learning
to copy colors from a reference frame . Given the
reference RGB frame and a gray-scale image, the network
needs to learn the internal connection between the reference
RGB frame and gray-scale image to colorize it.
Another perspective is to tackle video colorization by
employing a fully convolution neural network. Tran et al.
proposed an U-shape convolution neural network for video
colorization . The network is an encoder-decoder based
3DConvNet. The input of the network is a clip of grayscale
video clip, while the output if a colorful video clip. The
encoder is a bunch of 3D convolution layers to extract
features while the decoder is a bunch of 3D deconvolution
layers to generate colorful video clips from the extracted
The color coherence in videos is a strong supervision
signal. However, only a few work studied to employ it for
self-supervised video feature learning . More work can
be done by studying using color coherence as a supervision
signal for self-supervised video feature learning.
Learning from Video Prediction
Motion Encoder
Content Encoder
Fig. 21. The architecture for video prediction task proposed by .
Figure is reproduced based on .
Video prediction is a task of predicting future frame
sequences based on a limited number of frames of a video.
To predict future frames, network must learn the change in
appearance within a given frame sequence. The pioneer of
applying deep learning for video prediction is Un-LSTM
 . Due to the powerful ability of modeling long-term
dynamic in videos, LSTM is used in both the encoder and
decoder .
Many methods have been proposed for video prediction
 , , , , , , . Since its superior
ability to model temporal dynamics, most of them use LSTM
or LSTM variant to encode temporal dynamics in videos
or to infer the future frames , , , , .
These methods can be employed for self-supervised feature
learning without using human-annotations.
Most of the frameworks follow the encoder-decoder
pipeline in which the encoder to model spatial and temporal features from the given video clips and the decoder
to generate future frames based on feature extracted by
encoder. Fig. 21 shows a pipeline of MCnet proposed by
Villegas et al. in . McNet is built on Encoder-Decoder
Convolutional Neural Network and Convolutional LSTM
for video prediction. It has two encoders, one is Content
Encoder to capture the spatial layout of an image, and
the other is Motion Encoder to model temporal dynamics
within video clips. The spatial features and temporal features are concatenated to feed to the decoder to generate the
next frame. By separately modeling temporal and spatial
features, this model can effectively generate future frames
recursively.
Video prediction is a self-supervised learning task and
the learned features can be transferred to other tasks. However, no work has been done to study the generalization
ability of features learned by video prediction. Generally,
The Structural Similarity Index (SSIM) and Peak Signal to
Noise Ratio (PSNR) are employed to evaluate the difference
between the generated frame sequence and the ground truth
frame sequence.
Temporal Context-based Learning
Prediction
Correct Order
Incorrect Order
share parameter
Fig. 22. The pipeline of Shufﬂe and Learn . The network is trained
to verify whether the input frames are in correct temporal order. Figure
is reproduced based on .
Videos consist of various lengths of frames which have
rich spatial and temporal information. The inherent temporal information within videos can be used as supervision
signal for self-supervised feature learning. Various pretext
tasks have been proposed by utilizing temporal context
relations including temporal order veriﬁcation , ,
 and temporal order recognition , . Temporal
order veriﬁcation is to verify whether a sequence of input
frames is in correct temporal order, while temporal order
recognition is to recognize the order of a sequence of input
As shown in Fig. 22, Misra et al. proposed to use the
temporal order veriﬁcation as the pretext task to learn image
features from videos with 2DConvNet which has two
main steps: (1) The frames with signiﬁcant motions are
sampled from videos according to the magnitude of optical
ﬂow, (2) The sampled frames are shufﬂed and fed to the
network which is trained to verify whether the input data
is in correct order. To successfully verify the order of the
input frames, the network is required to capture the subtle
difference between the frames such as the movement of the
person. Therefore, semantic features can be learned through
the process of accomplishing this task. The temporal order
recognition tasks use networks of similar architecture.
However, the methods usually suffer from a massive
dataset preparation step. The frame sequences that used
to train the network are selected based on the magnitude
of the optical ﬂow, and the computation process of optical
ﬂow is expensive and slow. Therefore, more straightforward
and time-efﬁciency methods are needed for self-supervised
video feature learning.
Cross Modal-based Learning
Cross modal-based learning methods usually learn video
features from the correspondence of multiple data streams
including RGB frame sequence, optical ﬂow sequence, audio
data, and camera pose.
In addition to rich temporal and spatial information in
videos, optical ﬂow sequence can be generated to speciﬁcally indicate the motion in videos, and the difference of
frames can be computed with negligible time and spacetime complexity to indicate the boundary of the moving
objects. Similarly, audio data also provide a useful hint
about the content of videos. Based on the type of data used,
these methods fall into three groups: (1) methods that learn
features by using the RGB and optical ﬂow correspondence
 , , (2) methods that learn features by utilizing the
video and audio correspondence , , (3) ego-motion
that learn by utilizing the correspondence between egocentric video and ego-motor sensor signals , . Usually,
the network is trained to recognize if the two kinds of input
data are corresponding to each other , , or is trained
to learn the transformation between different modalities
Learning from RGB-Flow Correspondence
Optical ﬂow encodes object motions between adjacent
frames, while RGB frames contain appearance information.
The correspondence of the two types of data can be used
to learn general features , , , . This type of
pretext tasks include optical ﬂow estimation , and
RGB and optical ﬂow correspondence veriﬁcation .
Sayed et al. proposed to learn video features by verifying
whether the input RGB frames and the optical ﬂow corresponding to each other. Two networks are employed while
one is for extracting features from RGB input and another
is for extracting features from optical ﬂow input . To
verify whether two input data correspond to each other,
the network needs to capture mutual information between
the two modalities. The mutual information across different
modalities usually has higher semantic meaning compared
to information which is modality speciﬁc. Through this pretext task, the mutual information that invariant to speciﬁc
modality can be captured by ConvNet.
Optical ﬂow estimation is another type of pretext tasks
that can be used for self-supervised video feature learning.
Fischer et al. proposed FlowNet which is an end-to-end
convolution neural network for optical ﬂow estimation from
two consecutive frames , . To correctly estimate
optical ﬂow from two frames, the ConvNet needs to capture
appearance changes of two frames. Optical ﬂow estimation
can be used for self-supervised feature learning because
it can be automatically generated by simulators such as
game engines or by hard-code programs without human
annotation.
Learning from Visual-Audio Correspondence
Recently, some researchers proposed to use the correspondence between visual and audio streams to design Visual-
Audio Correspondence learning task , , , .
The general framework of this type of pretext tasks is
shown in Fig. 23. There are two subnetworks: the vision
Visual Stream ConvNet
Audio Stream ConvNet
Correspond?
Visual Audio Correspondence Network
Fig. 23. The architecture of video and audio correspondence veriﬁcation
task .
subnetwork and the audio subnetwork. The input of vision
subnetwork is a single frame or a stack of image frames and
the vision subnetwork learns to capture visual features of
the input data. The audio network is a 2DConvNet and the
input is the Fast Fourier Transform (FFT) of the audio from
the video. Positive data are sampled by extracting video
frames and audio from the same time of one video, while
negative training data are generated by extracting video
frames and audio from different videos or from different
times of one video. Therefore, the networks are trained to
discover the correlation of video data and audio data to
accomplish this task.
Since the inputs of the ConvNets are two kinds of data,
the networks are able to learn the two kinds of information
jointly by solving the pretext task. The performance of
the two networks obtained very good performance on the
downstream applications .
Ego-motion
With the self-driving car which usually equipped with
various sensors, the large-scale egocentric video along with
ego-motor signal can be easily collected with very low cost
by driving the car in the street. Recently, some researchers
proposed to use the correspondence between visual signal
and motor signal for self-supervised feature learning ,
 , .
Camera Pose
Transformation
Share Parameters
Fig. 24. The architecture of camera pose transformation estimation from
egocentric videos .
The underline intuition of this type of methods is that
a self-driving car can be treated as a camera moving in a
scene and thus the egomotion of the visual data captured
by the camera is as same as that of the car. Therefore, the
correspondence between visual data and egomotion can
be utilized for self-supervised feature learning. A typical
network of using ego-motor signal is shown in Fig. 24
proposed by Agrawal et al. for self-supervised image feature
learning . The inputs to the network are two frames
sampled from an egocentric video within a short time. The
labels for the network indicate the rotation and translation
relation between the two sampled images which can be
derived from the odometry data of the dataset. With this
task, the ConvNet is forced to identify visual elements that
are present in both sampled images.
The ego-motor signal is a type of accurate supervision
signal. In addition to directly applying it for self-supervised
feature learning, it has also been used for unsupervised
learning of depth and ego-motion . All these networks
can be used for self-supervised feature learning and transferred for downstream tasks.
PERFORMANCE COMPARISON
This section compares the performance of image and video
feature self-supervised learning methods on public datasets.
For image feature self-supervised learning, the performance
on downstream tasks including image classiﬁcation, semantic segmentation, and object detection are compared. For
video feature self-supervised learning, the performance on
a downstream task which is human action recognition in
videos is reported.
Performance of Image Feature Learning
As described in Section 4.3, the quality of features learned by
self-supervised learned models is evaluated by ﬁne-tuning
them on downstream tasks such as semantic segmentation,
object detection, and image classiﬁcation. This section summarizes the performance of the existing image feature selfsupervised learning methods.
Table 4 lists the performance of image classiﬁcation
performance on ImageNet and Places datasets.
During self-supervised pretext tasks training, most of the
methods are trained on ImageNet dataset with AlexNet as
based network without using the category labels. After pretext task self-supervised training ﬁnished, a linear classiﬁer
is trained on top of different frozen convolutional layers of
the ConvNet on the training split of ImageNet and Places
datasets. The classiﬁcation performances on the two datasets
are used to demonstrate the quality of the learned features.
As shown in Table 4, the overall performance of the
self-supervised models is lower than that of models trained
either with ImageNet labels or with Places labels. Among all
the self-supervised methods, the DeepCluster achieved
the best performance on the two dataset. Three conclusions
can be drawn based on the performance from the Table: (1)
The features from different layers are always beneﬁted from
the self-supervised pretext task training. The performance of
self-supervised learning methods is always better than the
performance of the model trained from scratch. (2) All of
the self-supervised methods perform well with the features
from conv3 and conv4 layers while performing worse with
the features from conv1, conv2, and conv5 layers. This
is probably because shallow layers capture general lowlevel features, while deep layers capture pretext task-related
features. (3) When there is a domain gap between dataset for
pretext task training and the dataset of downstream task, the
self-supervised learning method is able to reach comparable
performance with the model trained with ImageNet labels.
In addition to image classiﬁcation, object detection and
semantic segmentation are also used as the downstream
tasks to evaluate the quality of the features learned by
self-supervised learning. Usually, ImageNet is used for selfsupervised pretext task pre-training by discarding category
labels, while the AlexNet is used as the base network and
ﬁne-tuned on the three tasks. Table 5 lists the performance of
image classiﬁcation, object detection, and semantic segmentation tasks on the PASCAL VOC dataset. The performance
of classiﬁcation and detection is obtained by testing the
model on the test split of PASCAL VOC 2007 dataset, while
the performance of semantic segmentation is obtained by
testing the model on the validation split of PASCAL VOC
2012 dataset.
As shown in Table 5, the performance of the selfsupervised models on segmentation and detection dataset
are very close to that of the supervised method which is
trained with ImageNet labels during pre-training. Specifically, the margins of the performance differences on the
object detection and semantic segmentation tasks are less
than 3% which indicate that the learned features by selfsupervised learning have a good generalization ability.
Among all the self-supervised learning methods, the Deep-
Clustering obtained the best performance on all the
Performance of Video Feature Learning
For self-supervised video feature learning methods, human
action recognition task is used to evaluate the quality of
learned features. Various video datasets have been used
for self-supervised pre-training, and different network architectures have been used as the base network. Usually
after the pretext task pre-training ﬁnished, networks are
ﬁne-tuned and tested on the commonly used UCF101 and
HMDB51 datasets for human action recognition task. Table 6 compares the performance of existing self-supervised
video feature learning methods on UCF101 and HMDB51
As shown in Table 6, the best performance of the ﬁnetune results on UCF101 is less than 66%. However, the supervised model which trained with Kinetics labels can easily
obtain an accuracy of more than 84%. The performance
of the self-supervised model is still much lower than the
performance of the supervised model. More effective selfsupervised video feature learning methods are desired.
Based on the results, conclusions can be drawn about
the performance and reproducibility of the self-supervised
learning methods.
Performance: For image feature self-supervised learning, due to the well-designed pretext tasks, the performance of self-supervised methods are comparable to the
supervised methods on some downstream tasks, especially
for the object detection and semantic segmentation tasks.
The margins of the performance differences on the object
detection and semantic segmentation tasks are less than 3%
which indicate that the learned features by self-supervised
learning have a good generalization ability. However, the
Linear classiﬁcation on ImageNet and Places datasets using activations from the convolutional layers of an AlexNet as features. ”Convn” means
the linear classiﬁer is trained based on the n-th convolution layer of AlexNet. ”Places Labels” and ”ImageNet Labels” indicate using supervised
model trained with human-annotated labels as the pre-trained model.
Pretext Tasks
Places labels 
ImageNet labels 
Random(Scratch) 
ColorfulColorization 
Generation
BiGAN 
Generation
SplitBrain 
Generation
ContextEncoder 
ContextPrediction 
Jigsaw 
Learning2Count 
DeepClustering 
Comparison of the self-supervised image feature learning methods on classiﬁcation, detection, and segmentation on PASCAL VOC dataset.
”ImageNet Labels” indicates using supervised model trained with human-annotated labels as the pre-trained model.
Pretext Tasks
Classiﬁcation
Segmentation
ImageNet Labels 
Random(Scratch) 
ContextEncoder 
Generation
BiGAN 
Generation
ColorfulColorization 
Generation
SplitBrain 
Generation
RankVideo 
PredictNoise 
JigsawPuzzle 
ContextPrediction 
Learning2Count 
DeepClustering 
WatchingVideo 
Free Semantic Label
CrossDomain 
Free Semantic Label
AmbientSound 
Cross Modal
TiedToEgoMotion 
Cross Modal
EgoMotion 
Cross Modal
Comparison of the existing self-supervised methods for action
recognition on the UCF101 and HMDB51 datasets. * indicates the
average accuracy over three splits. ”Kinetics Labels” indicates using
supervised model trained with human-annotated labels as the
pre-trained model.
Pretext Task
Kinetics Labels* 
VideoGAN 
Generation
VideoRank 
ShufﬂeLearn 
3DRotNet 
CubicPuzzle* 
RGB-Flow 
Cross Modal
PoseAction 
Cross Modal
performance of video feature self-supervised learning methods is still much lower than that of the supervised models on
downstream tasks. The best performance of the 3DConvNetbased methods on UCF101 dataset is more than 18% lower
than that of the supervised model . The poor performance of 3DCovnNet self-supervised learning methods
probably because 3DConvNets usually have more parameters which lead to easily over-ﬁtting and the complexity of
video feature learning due to the temporal dimension of the
Reproducibility: As we can observe, for the image feature self-supervised learning methods, most of the networks
use AlexNet as a base network to pre-train on ImageNet
dataset and then evaluate on same downstream tasks for
quality evaluation. Also, the code of most methods are
released which is a great help for reproducing results.
However, for the video self-supervised learning, various
datasets and networks have been used for self-supervised
pre-training, therefore, it is unfair to directly compare different methods. Furthermore, some methods use UCF101
as self-supervised pre-training dataset which is a relatively
small video dataset. With this size of the dataset, the power
of a more powerful model such as 3DCovnNet may not be
fully discovered and may suffer from server over-ﬁtting.
Therefore, larger datasets for video feature self-supervised
pre-training should be used.
Evaluation Metrics: Another fact is that more evaluation
metrics are needed to evaluate the quality of the learned
features in different levels. The current solution is to use the
performance on downstream tasks to indicate the quality
of the features. However, this evaluation metric does not
give insight what the network learned through the selfsupervsied pre-training. More evaluation metrics such as
network dissection should be employed to analysis the
interpretability of the self-supervised learned features.
FUTURE DIRECTIONS
Self-supervised learning methods have been achieving great
success and obtaining good performance that close to supervised models on some computer vision tasks. Here, some
future directions of self-supervised learning are discussed.
Learning Features from Synthetic Data: A rising trend
of self-supervised learning is to train networks with synthetic data which can be easily rendered by game engines
with very limited human involvement. With the help of
game engines, millions of synthetic images and videos with
accuracy pixel-level annotations can be easily generated.
With accurate and detailed annotations, various pretext
tasks can be designed to learn features from synthetic data.
One problem needed to solve is how to bridge the domain
gap between synthetic data and real-world data. Only a
few work explored self-supervised learning from synthetic
data by using GAN to bridge the domain gap , .
With more available large-scale synthetic data, more selfsupervised learning methods will be proposed.
Learning from Web Data: Another rising trend is to train
networks with web collected data , , based
on their existing associated tags. With the search engine,
millions of images and videos can be downloaded from
websites like Flickr and YouTube with negligible cost. In
addition to its raw data, the title, keywords, and reviews
can also be available as part of the data which can be
used as extra information to train networks. With carefully
curated queries, the web data retrieved by reliable search
engines can be relatively clean. With large-scale web data
and their associated metadata, the performance of selfsupervised methods may be boosted up. One open problem
about learning from web data is how to handle the noise in
web data and their associated metadata.
Learning Spatiotemporal Features from Videos: Selfsupervised image feature learning has been well studied
and the margin of the performance between supervised
models and self-supervised models are very small on some
downstream tasks such as semantic segmentation and object
detection. However, self-supervised video spatiotemporal
feature learning with 3DConvNet is not well addressed yet.
More effective pretext tasks that speciﬁcally designed to
learn spatiotemporal features from videos are needed.
Learning with Data from Different Sensors: Most existing self-supervised visual feature learning methods focused
on only images or videos. However, if other types of data
from different sensors are available, the constraint between
different types of data can be used as additional sources
to train networks to learn features . The self-driving
cars usually are equipped with various sensors including
RGB cameras, gray-scale cameras, 3D laser scanners, and
high-precision GPS measurements and IMU accelerations.
Very large-scale datasets can be easily obtained through
the driving, and the correspondence of data captured by
different devices can be used as a supervision signal for
self-supervised feature learning.
Learning with Multiple Pretext Tasks: Most existing
self-supervised visual feature learning methods learn features by training ConvNet to solve one pretext tasks. Different pretext tasks provide different supervision signals
which can help the network learn more representative features. Only a few work explored the multiple pretext tasks
learning for self-supervised feature learning , . More
work can be done by studying the multiple pretext task selfsupervised feature learning.
CONCLUSION
Self-supervised image feature learning with deep convolution neural network has obtained great success and the margin between the performance of self-supervised methods
and that of supervised methods on some downstream tasks
becomes very small. This paper has extensively reviewed
recently deep convolution neural network-based methods
for self-supervised image and video feature learning from
all perspectives including common network architectures,
pretext tasks, algorithms, datasets, performance comparison, discussions, and future directions etc. The comparative
summary of the methods, datasets, and performance in
tabular forms clearly demonstrate their properties which
will beneﬁt researchers in the computer vision community.