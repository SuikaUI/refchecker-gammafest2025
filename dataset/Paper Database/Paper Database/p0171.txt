Choosing the Sample Size of
a Computer Experiment:
A Practical Guide
Jason L. Loeppky, Jerome Sacks
and William Welch
Technical Report Number 170
February 2008
National Institute of Statistical Sciences
19 T. W. Alexander Drive
PO Box 14006
Research Triangle Park, NC 27709-4006
www.niss.org
Choosing the Sample Size of a Computer Experiment: A
Practical Guide
Jason L. Loeppky
Jerome Sacks
Mathematics, Statistics, and Physics
National Institute of Statistical Sciences
University of British Columbia, Okanagan
Research Triangle Park, NC, 27709
Kelowna, BC V1V 1V7, CANADA
( )
( )
William J. Welch
Department of Statistics
University of British Columbia
Vancouver, BC V6T 1Z2, CANADA
( )
February 19, 2008
We produce reasons and evidence supporting the informal rule that the number of
runs for an eﬀective initial computer experiment should be about 10 times the input
dimension. Our arguments quantify two key characteristics of computer codes that
aﬀect the sample size required for a desired level of accuracy when approximating
the code via a Gaussian process (GP). The ﬁrst characteristic is the total sensitivity
of a code output variable to all input variables. The second corresponds to the
way this total sensitivity is distributed across the input variables, speciﬁcally the
possible presence of a few prominent input factors and many impotent ones (eﬀect
sparsity).
Both measures relate directly to the correlation structure in the GP
approximation of the code. In this way, the article moves towards a more formal
treatment of sample size for a computer experiment. The evidence supporting these
arguments stems primarily from a simulation study and via speciﬁc codes modeling
climate and ligand activation of G-protein.
KEYWORDS: Computer experiment, Gaussian process, Random function, Latin
hypercube design, Sample size.
Introduction
Choosing the sample size of any experiment is an important issue in the design of experiments, yet there is a lack of formal guidance. The reasons range from inadequate
prior information about the process under study to inadequate results (and inability) for
making necessary calculations. In standard regression settings, ﬁnding a sample size to
produce satisfactory predictive accuracy depends on the design points of the data collection and the error variance, but both the form of the regression model and the error
variance are typically unknown a priori. Bayesian strategies can be deployed with some
Deterministic computer experiments present a wholly diﬀerent set of challenges, primarily because concepts such as randomization and replication play no role and predictive
accuracy of the model is aﬀected solely by bias. Because physical experimentation is absent, the constraints on experimental size are typically caused by the time it takes to
make runs of the code. Such constraints are often vague and ﬂexible. Where budget
issues prevail (“you get this much computer time to make your runs”) the choice of sample size, n, is taken out of our hands. Nevertheless, it is useful to have some practical
guidance in choosing n and to know if the selected n is adequate to achieve stated goals.
In addition to guiding an experimenter in the choice of n for a speciﬁc experiment, we
will consider more general questions. In particular, what is the role of dimensionality of
the input space? If the curse of dimensionality applies, high-dimensional problems might
require huge, even intractable, sample sizes for good prediction accuracy. On the other
hand, if the total sensitivity of the function to all input variables is kept ﬁxed, with this
sensitivity just spread over more input variables, dimensionality might conceivably have
a limited eﬀect on accuracy, as in Monte Carlo integration. In this article, how total
sensitivity grows with dimension and how this sensitivity is spread across the dimensions
are key to understanding prediction accuracy, and hence sample size. Indeed, the article
is really about deﬁning the properties of functions that arise in practice, from which
simple rules about sample size follow for that class of problems.
Little has been written on this topic. Among the few exceptions, Chapman et al.
 and Jones et al. used the often quoted rule of selecting a sample size
that is 10 times the number of inputs. Although this rule has proved useful in practice
it lacks theoretical underpinning. One theoretical exploration by Chen showed
that, for a single varying input to the computer code whose output is under study, the
order of the prediction error is n−n for very smooth output functions and for an equally
spaced design. In higher dimensions, Chen produced results on rates for product
designs. Though these rates are instructive, product designs are impractical and more
precise understanding of prediction error is needed for choosing a sample size in practical
settings. The key conclusion we arrive at is that the empirically based recommendation
of n = 10d is a good path to follow for a large class of problems.
An Example
Yi et al. studied a computer model of ligand activation of G-protein in yeast
where the computer code takes four inputs and solves a system of ordinary diﬀerential
equations (details are in Section 3). Following the path taken in the literature since
1989 , approximate the computer output using a
Gaussian Process (GP) constructed from a set of code runs. The question that concerns
us here is: How many runs are needed to obtain adequate prediction accuracy at untried
test points? In the G-protein example, the code is relatively quick to run and we are able
to investigate the eﬀect of n on the prediction error by making runs for various values of
n. For each value of n the code was evaluated at inputs from an n-point maximin Latin
hypercube design (LHD) in 4 dimensions .
The plot in Figure 1 shows the square root of the integrated mean squared error (RIMSE)
for predictions using the GP model, for various choice of n. (RIMSE is computed for
both a set of 120 hold-out points and by leave-one-out cross-validation.)
improvement in RIMSE for sample sizes greater than 40 = 10d is a feature of many
In general, characterization of the factors aﬀecting approximation accuracy, and hence
sample size, requires precise formulation of the goals of the experiment. Such a formulation is often elusive, however. We restrict attention to the experimental objective of
approximating the code on the basis of sample runs. Even here, the choice of measures
of accuracy is open to subjective judgment. Those we use are given in (5) and (6) below. Issues such as optimization of a target criterion could bring other considerations,
especially that of fully sequential experimentation.
We have obscured the role of the design of the location of inputs in this process. Con-
sample size
Figure 1: Root integrated mean squared error (RIMSE) of prediction against n for the
G-protein example. The solid line shows RIMSE computed for a hold-out sample; the
dashed line shows RIMSE from leave one out cross-validation.
siderable experience built up over a number of applications leads us to restrict attention
to designs that are space-ﬁlling and, for the problems we address this is well managed
by maximin LHDs (which we used in the G-protein example), but simpler to construct
zero-correlation LHDs could also be deployed.
While there are many issues that can be addressed in determining sample size we
focus on these:
• Is n = 10d a good rule? What are the limitations of such a rule?
• How does accuracy increase with n? When are feasible sample sizes available?
• What impact do criteria have on assessing accuracy?
• What should be done when a criterion for accuracy is not met?
We will partially answer these questions, enough to provide useful practical advice for the
choice of n. Our approach to this problem investigates properties of the GP and the eﬀect
of n on prediction by ﬁrst ﬁnding connections between the design and the complexity of
the problem and then conducting a simulation study. The simulations focus on deciding
if n = 10d is a reasonable rule and characterizing the complexity of problems that can
be dealt with using n = 10d.
The paper is organized as follows. Section 2 reviews the GP model and gives speciﬁc
formulations of the measures of accuracy we use. Section 3 explores the G-protein example in more detail. Section 4 investigates the relationships among dimension, sample
size and complexity of the problem that guide the simulation study in Section 5 and 6.
Section 7 discusses strategies for a follow-up experiment to augment an initial design and
the implications for several examples. Finally, in Sections 8 and 9 we comment on open
and future issues and summarize our conclusions.
The Gaussian Process Model
A complex computer code mathematically describes the relationship between several
input variables and one or more (possibly functional) output variables.
Usually, the
computer model of interest is computationally demanding, and scientiﬁc objectives like
optimization would require too many evaluations if the code is used directly. As a consequence, strategies relying on computationally eﬃcient statistical approximation (emulation) of the code have been developed and have proved eﬀective. Following the path
taken in the literature since 1989 , we place a homogeneous Gaussian process prior on the possible output functions,
which leads to an approximator given by the posterior mean conditional on the data
from the computer experiment. Although the output from the computer model is often
multivariate, we will restrict our attention to scalar output. The results for scalar output
can be carried over by using principal component analysis or wavelet decompositions of
functional output as in Higdon et al. and Bayarri et al. .
The computer code output is denoted by y(x), where the code’s vector-valued input,
x = (x1, . . . , xd), is assumed to be a point in a d-dimensional unit cube. As long as the
input space is rectangular, there is no loss of generality here because any rectangle can
be transformed simply to the unit cube with only trivial implications for the analysis
method to be described.
The GP model places a prior on the class of possible y(x) functions. Let Y (x) denote
the random function whose distribution is determined by the prior. Speciﬁcally, we take
Y (x) = µ + Z(x),
where µ is a mean parameter and Z(x) is a Gaussian stochastic process with mean zero
and constant variance σ2. In this model, the correlation structure is crucial to prediction.
At two input vectors, x and x′, we take the correlation between Y (x) and Y (x′) as
R(x, x′) = exp (−h(x, x′)) ,
h(x, x′) =
is a measure of distance between x and x′ with weights θj ≥0 and distance-metric
parameters 1 ≤pj ≤2.
Experience in a variety of circumstances 
suggests that very smooth, even analytic, output is typical, especially in engineering
As such it is often the case that pj is ﬁxed at 2 for all j, leading to the
Gaussian correlation function. We adopt this special case for most of the article, but
return to the issue of pj < 2 in Sections 7 and 8. With pj = 2, it is easily shown that
Hence, the weight θj may be interpreted as a measure of the “sensitivity” of Y (x) to xj.
Characterizing the distribution of the distances in (2) across design points as a function
of the values of the sensitivity measures, θ1, . . ., θd, (Section 4) leads to an understanding
of the the factors aﬀecting prediction accuracy and hence sample size.
Suppose we make n runs of the code at a design D of input vectors x(1), . . . , x(n) in
 d, leading to the data y = (y(x(1)), . . . , y(x(n)))T. The predictor ˆY (x) of Y (x) is the
posterior mean of Y (x) given the data and θ = (θ1, . . ., θd):
ˆY (x) = E (Y (x)|y, θ) = ˆµ + rT(x)R−1(y −ˆ
where r(x) = (R(x, x(1)), . . ., R(x, x(n)))T is an n × 1 vector, R is an n × n matrix with
element i, j given by R(x(i), x(j)), and ˆµ is an estimate of µ, often from the method of
maximum likelihood. The mean squared error (MSE) of ˆY (x), taking account of the
uncertainty from estimating µ by maximum likelihood, is given by
MSE( ˆY (x)) = E
ˆY (x) −Y (x)
1 −rT(x)R−1r(x) + (1 −1TR−1r(x))2
where 1 is an n × 1 vector with all elements equal to 1. In practice, σ2 and θ also have
to be estimated, again often by maximum likelihood .
MSE in (4) can be directly computed given an experimental design and θ, and is
used in Section 4 for theoretical arguments. However, for our empirical studies we take a
diﬀerent path to deﬁne prediction accuracy by using leave-one-out cross-validation(CV)
 as follows.
Given a design D with sample size n and code runs y, denote the cross-validated
prediction of y(x(i)) by ˆY−i(x(i)), which is the predictor (3) from the n−1 runs excluding
run i. Then the cross-validated error of prediction is ˆY−i(x(i)) −y(x(i)) for i = 1, . . . , n.
Average and maximum measures of error based on cross-validation are given by
ˆY−i(x(i)) −y(x(i))
x(1),...,x(n)
ˆY−i(x(i)) −y(x(i))
We also normalize for the scale of the function by dividing by the range of the values of
y in the data, leading to the following inaccuracy summaries:
ˆY−i(x(i)) −y(x(i))
range of y(x(1)), . . . , y(x(n))
x(1),...,x(n)
ˆY−i(x(i)) −y(x(i))
range of y(x(1)), . . ., y(x(n)) .
The tolerable level of inaccuracy will be application-speciﬁc, but we will typically take
eavg < 0.1 as the target for a “useful” approximation of the code.
For questions relating to the sample size of an initial design, we do not have data
available, at least not code data. But we can simulate data using the GP model with
given θ. We will distinguish e depending on whether the data are from code runs or from
simulations by eavg|code and eavg respectively.
Before code runs are made, we can perform replicate simulations of the random function and obtain a collection of eavg values, which will then provide an empirical distribution of (5). The average of the simulations will then be an estimate of expected
inaccuracy as formulated in (5). Similarly we can get an estimate of expected accuracy
as formulated in (6).
Why proceed with simulations rather than attempt direct computation of expected
values, for example? There are four reasons:
1. The ratios in (5) and (6) are appealing measures of accuracy. Producing expected
values or other quantities of these measures are hopeless without simulation; they
are readily estimated via simulation.
2. As described in Section 7, after the sample size is selected and the computer experiment is run we can evaluate e·|code and, with the information from the simulations,
especially their empirical distribution, we can gauge whether the GP model and
sample size are well matched to the actual code.
3. Even if we take expectation and remove all randomness in the data, there are
other sources of randomness in practice. Most experimental designs are isomorphic
with respect to various symmetries such as interchanging the columns. Diﬀerent
versions of the design within the equivalence class would lead to diﬀerent measures
of prediction error, even after taking expectation with respect to the data.
G-protein Computer Code
The ligand activation of G-protein in yeast is described by Yi et al. . The computer
code solves a system of ordinary diﬀerential equations (ODEs) with nine parameters that
can vary. The system dynamics, the diﬀerential equations, are given by:
−u1η1x + u2η2 −u3η1 + u5
u1η1x −u2η2 −u4η2
−u6η2η3 + u8(Gtot −η3 −η4)(Gtot −η3)
u6η2η3 −u7η4
(Gtot −η3)/Gtot,
where η1, . . . , η4 are concentrations of four chemical species, ˙ηi ≡∂ηi
∂t ; x is the concentration of the ligand; u1, . . ., u8 is a vector of 8 kinetic parameters; Gtot is the (ﬁxed)
total concentration of G-protein complex after 30 seconds; and y is the normalized concentration of a relevant part of the complex. In one study , ﬁve of
these kinetic parameters are ﬁxed (only allowing x, u1, u6, and u7 to vary). The GP
model is used to construct an approximation as a function of the transformed variables
log(x), log(u1), log(u6), log(u7) each of which is further transformed to .
The ODE solver is quick to run and enables us to evaluate the aﬀect of n on the
criterion in (5) using a real model.
The design points at which the code is run are
selected by using maximin LHDs. These space-ﬁlling designs have proved to be highly
eﬀective in the study and application of computer experiments.
The values of n we use are multiples (7, 10, 15, 20) of the dimension, 4, and also
include 33, the number of runs made in the Feeley et al. study. For each choice of
n, we run the ODE solver to obtain data {y(x(i)); x(i) ∈D}. The data are modeled as if
they were the realizations of a GP, and maximum likelihood estimates of ˆµ, ˆσ and ˆθ are
obtained for the parameters of the GP (see Section 2) for each choice of n. For each value
of n, we use the code runs to calculate eavg|code from (5), except no normalization for the
range is made here. (In any case, the normalization factor is close to 1 at about 0.8 and
makes little diﬀerence.) We also compare this measure with the analog from a set of new
test points. We generate an additional independent 120-point maximin LHD, D0, and
evaluate the ODE solver to obtain data for the out-of-sample test points. The same 120
test runs are used for all evaluations. Using the test sample, the analogous version of (5)
is computed by replacing the average in the numerator by
ˆY (x) −y(x)
The plot in Figure 1 shows how the two unnormalized eavg|code measures behave as n
changes. A major point is that eavg|code changes little as n increases past 40, nor is there
any substantial diﬀerence between using cross validation instead of a new test sample.
That cross validation leads to larger errors is not surprising, since leaving out one point
can produce a big gap making it hard to predict the omitted point. This is relevant
because the use of new test data is a luxury, only enjoyed if the code can be run quickly,
and so we rely on cross validation for measuring accuracy.
Judging the quality of prediction over a wide range of scenarios is simply not possible
through runs of the computer code unless the code is very quick to run. Therefore, we
will rely on simulated data generated by using a GP. Because a GP model often has
similar properties to those of the computer codes we expect to encounter in practice, we
are at least close to mimicking reality. Before simulating, however, we need to know the
important factors in function complexity, so that an eﬃcient and insightful simulation
study may be conducted.
Eﬀect of d, θ, and n on prediction accuracy
Intuitively, we know that when we predict Y (x) at some x, the design-point neighbors
of x will tend to be closer as n becomes larger, improving accuracy.
If θ has many
large values, however, the correlation between Y (x) and Y for the neighbors will be
low, even for nearby points, leading to poorer prediction accuracy. Here, we develop this
intuition into some quantitative rules relating d, θ, and n to distances and the correlation
structure, shedding some light on how prediction accuracy depends on these quantities.
First, we consider how the theoretical mean squared error, MSE( ˆY (x)) in (4), depends
on d, θ, and n. Recall that the empirical deﬁnitions of prediction accuracy in (5) and (6)
are normalized for scale.
Similarly, without loss of generality, we can ignore σ2 in a
normalized version of mean squared prediction error:
MSEnorm( ˆY (x)) = 1 −rT(x)R−1r(x) + (1 −1TR−1r(x))2
We see that MSEnorm( ˆY (x)) is determined by R and r(x) only. Thus, it is a function
of n—as R and r(x) are an n × n matrix and an n × 1 vector, respectively—and the
correlations in R and r(x). Dimensionality, d, aﬀects MSEnorm( ˆY (x)) only indirectly via
these correlations.
For simplicity, we will explore the factors aﬀecting MSEnorm( ˆY (x)) for completely
random Latin hypercube designs (where the columns are permuted independently). We
consider the case where n is ﬁxed and look at the eﬀect of d and θ1, . . ., θd on the
correlation structure and on MSEnorm( ˆY (x)) averaged over x. Our argument establishes
two main results:
1. For moderately large d and a random LHD, the distribution of inter-point squared
distance (weighted by θ1, . . . , θd) in (2) can be approximated by a normal distribution, with mean and variance given by simple functions of θ1, . . . , θd. The approximate distribution of (oﬀ-diagonal) correlations in R follows from the transformation
2. Under the same conditions, the distribution of correlations in r(x) for x drawn
randomly from d is similar to the distribution of correlations in R.
We recognize that the matrix inverse in (7) makes MSEnorm much more complicated
than can be explained by these distributions of correlations. Nonetheless, the simulations in Section 5 and 6 show that the factors aﬀecting the correlation distribution explain
much of the eﬀect of d and θ1, . . . , θd on our empirical accuracy measures. Indeed, understanding these factors leads to simulation studies with straightforward interpretation.
Take two points, x and x′, at random from a random LHD. An LHD is deﬁned here
to have ﬁxed grid points {0, 1/(n −1), . . . , 1} for each variable xj. Let
hj = |xj −x′
be the unweighted distance in dimension j appearing in h in (2). The ﬁrst two moments
j are given by Lemma 1.
Lemma 1: Let hj be the distance between two randomly chosen points xj and x′
dimension j for a random LHD. Then
P(hj = i/(n −1)) = n −i
for i = 1, . . ., n −1,
j) ≡m1(n) = 1
j) ≡m2(n) =
n(n −2)(n + 1)(7n + 9)
The proof of Lemma 1 can be found in Appendix A. Note that the two moments converge
to 1/6 and 7/180 as n →∞.
If d = 1 than the probability distribution P(hj = i/(n −1)) in Lemma 1 exactly
describes the distribution of all possible distances between distinct points x and x′. That
is, since the design points are on the grid, every possible distance i/(n −1) occurs n −i
If d > 1, however, not all of the possible distances over all dimensions will be observed in any one design, and we rely on the moments given in Lemma 1 to describe
behavior. Speciﬁcally, for two randomly chosen points, the squared distance in (2) across
all dimensions which arises if pj = 2 has expectation
E(h) = m1(n)
For a completely random LHD, which has independently permuted columns,
Var(h) = m2(n)
Furthermore, as d increases, the central limit theorem applies unless there are a few
θj weights that dominate, so that h is approximately normal with mean and variance
given by (8) and (9). Hence, the correlation in (1) is approximately log-normal with
these moments (after a change of sign). Figure 2 compares the empirical distributions
for a single random LHD with the approximations, for d = 10, n = 100, and θ =
(2.71, 2.17, 1.69, 1.27, 0.91, 0.61, 0.37, 0.19, 0.07, 0.01) the approximations are seen to be
good. The values chosen for θj comprise a canonical conﬁguration, to be explained in
Section 5.
correlation
Figure 2: Distribution of squared distance (left panel) and correlation (right panel) for
a randomly chosen pair of points from a random Latin hypercube design with d = 10,
n = 100, and θ = (2.71, 2.17, 1.69, 1.27, 0.91, 0.61, 0.37, 0.19, 0.07, 0.01)
Similarly, Figure 3 shows the analogous distributions for the vector r(x). The empirical distribution of the distance and the correlation are taken over a single random
LHD, D, and the distance between x and all n points in D is computed for 50 randomly
chosen test point x ∈ d. The same normal or log-normal approximations established
above for inter-point distance or correlation are transferred to test-point to design-point
distance or correlation. It is seen that the correlations in r(x) behave like those in R.
Note that there is a negative impact on prediction accuracy when the mean distance
correlation
Figure 3: Distribution of squared distance (left panel) and correlation (right panel) for a
randomly chosen test point and a random Latin hypercube design with d = 10, n = 100,
and θ = (2.71, 2.17, 1.69, 1.27, 0.91, 0.61, 0.37, 0.19, 0.07, 0.01)
in (8) increases or when the variance in (9) decreases. When the variance decreases, small
squared distances (high correlations) are less likely, whereas high-correlation neighboring
points lead to good prediction accuracy. Indeed, if we want to predict Y (x), just one
close design neighbor, x(i), of x, close in the sense of correlation, may by itself give good
prediction accuracy. Let ρ = exp(−h(x, x(i))) be the correlation for these two points.
Predicting using only Y (x(i)) provides an upper bound on MSEnorm obtained from (7).
As R is the scalar 1, and we have
MSEnorm( ˆY (x)) < 1 −ρ2 + (1 −ρ)2 = 2(1 −ρ).
If ρ is larger than about 0.95, or equivalently h is less than about 0.05, MSEnorm( ˆY (x)) <
0.1, the target we have set.
The magnitudes of the correlations in R and r(x), which lead to MSEnorm in (7),
depend on τ = Pd
j=1 θj and ψ = Pd
j to a good approximation.
There are two
practical consequences. First, these two quantities are used to plan the simulations in
Section 5 and 6. We ﬁnd that the behavior of the empirical analog, eavg, of MSEnorm
is largely dependent on τ and ψ. Secondly, the distributions of the correlations in r(x)
(for random test points) and in R (between design points) are similar. The implication
is that accuracy estimates will be similar from cross validation based on leaving out one
design point at a time versus random test points.
There are many possible θ1, . . . , θd conﬁgurations, and we examine three special cases,
ordered from worst to best behavior in terms of the eﬀect of dimensionality.
1. Suppose θ1 = · · · = θd = θ, i.e., as dimensionality increases, further equally active
variables are added. Then, τ = dθ and ψ = dθ2. Thus, the mean of the distribution
of h increases linearly with d, the standard deviation of the distribution increases as
d, and the h distribution becomes stochastically larger with d. For large enough
d, prediction accuracy will be poor, even if θ is small.
2. Suppose τ is kept constant, i.e., a ﬁxed amount of total sensitivity is spread across
all dimensions. Clearly, ψ takes its minimum value of ψ = τ 2/d when θ1 = · · · =
θd = τ/d. Thus, equally active factors are worst for prediction accuracy. Moreover,
as ψ = τ 2/d decreases with d, this eﬀect becomes worse as d increases. For large
enough d, the h distribution will become concentrated at its mean, m1(n)τ, and
the limiting accuracy depends on τ. In this sense, if the total amount of sensitivity
is kept constant, the worst-case eﬀect of dimensionality is small.
3. Alternatively, suppose we keep τ and ψ constant as d increases. Write
(θj −¯θ)2 + 1
Because the second term on the right decreases with d, Pd
j=1(θj −¯θ)2 must increase
to keep ψ constant. Another way of looking at the fact that θ1, . . . , θd must become
more variable with d to maintain prediction accuracy is that some dimensions are
more active than others, or there is eﬀect sparsity.
The argument that accuracy decreases as τ = Pd
j=1 θj increases or as ψ = Pd
decreases is borne out by the simulations in Section 5 and 6.
The quantitative eﬀect of n on accuracy is less obvious, however. The mean and
variance of the squared distance distribution in (8) and (9) do not depend on n in
the limit. Thus, R and r(x) in (7) have elements that depend only weakly on n in this
statistical sense. Rather, MSEnorm depends on n because R and r(x) have more elements.
The closest neighbor in the bound (10) will tend to become closer with larger n, thus
driving the bound down. A full analysis of the impact of using all n design points is
complicated by the inverse of R in (7). All that we can say is that harder problems
(larger τ and smaller ψ) will require larger sample sizes, regardless of dimensionality
to a large extent. This insight greatly facilitates quantiﬁcation of the impact of n by
simulation in Section 5 and 6.
If p1 = · · · = pd, but the common value is less than 2, m1(n) and m2(n) in Lemma 1
will change. The mean and variance of h in (8) and in (9) will still depend on τ and ψ,
Simulation Results for Average Error
The arguments in Section 4 suggest that the eﬀect of the correlation parameters on eavg
is through τ and ψ, thereby diminishing the role of d. To investigate this further we will
engage a simulation study that changes dimension but keeps τ, ψ ﬁxed.
But before doing so we must decide on the conﬁgurations of the θ vectors to be
Past experience has indicated that for well-behaved outputs there may be
a few large components of θ, a few moderately sized, and the remainder small.
example, for the G-protein model and the 33-run experiment, ˆθ = (1.71, 0.29, 0.27, 0.25)
has one moderate value and the other three are small. From this point of view, we will
adopt a two-parameter class of canonical conﬁgurations of θ, deﬁned by
for j = 1, . . ., d, and b ≥1, τ > 0.
Here θj decreases in j and Pd
j=1 θj = τ.
The generated θ vector tends to have the
characteristics we expect, especially as d gets large. Examples of θ conﬁgurations for
d = 10 and τ = 1 are given in Table 1.
When τ ̸= 1, the value of θ is found by
multiplying each θj in the table by τ.
Table 1: Conﬁgurations of θ for d = 10
Data for the simulation study are generated as follows.
Given d and n, select a
maximin LHD D of n points in d. Fix values of µ = 0, σ2 = 1, p = 2 and select
a canonical θ (as speciﬁed above) for the parameters of the GP given in (2). Generate 50 independent realizations of the GP resulting in 50 diﬀerent sets of observations
{y(x(i)); x(i) ∈D}. Since, the measure of accuracy in (5) or (6) is standardized by the
range, the particular value of σ2 = 1 is largely irrelevant.
For each data set form a predictor using (3) with the value of θ the same as that used
to generate the simulated data. Alternatively, for each data set, we could estimate θ
and construct a predictor with ˆθ. We found that there is no essential diﬀerence between
predictors based on θ and ˆθ in terms of our summary measures of prediction accuracy,
and using the ﬁxed θ takes much less time in our extensive study. The predictor leads
to a value of eavg in (5) for each data set.
We start with d = 5 and b = 1 in (12), which results in θj = τ/5, j = 1, . . ., 5. As
argued in Section 4, this choice of θ minimizes ψ for a ﬁxed τ and represent a “worst
case” starting point. For a given τ value, ψ = τ 2/5 when d = 5. If τ and ψ are kept
constant as d changes, then the canonical θ vector must satisfy P θ2
j = τ 2/5. For d = 10,
15, and 20, this means that b = 3.445, 5.507, and 7.55, respectively, in (12). Values of
τ = 3, 10, 20, and 40 are chosen to cover problems from “easy” to “very hard”.
The arguments in Section 4 suggest that similar accuracy should be obtained in any
dimension for ﬁxed values of n, τ and ψ, but intuitively we expect that n must increase
with d. Our results for the ﬁrst two moments may not fully explain the behavior of
the tails of the distribution of h, and small distances in particular play a prominent role.
Thus, we allow n to increase modestly with d, speciﬁcally linearly. We also allow diﬀerent
rates, i.e., n = kd, where k = 7, 10, 15, or 20.
The four panels in Figure 4 correspond to τ = 3, 10, 20, and 40, respectively. In each
panel, four curves are plotted, for d = 5, 10, 15, and 20, respectively. A curve shows
the mean of eavg computed from the 50 realizations of the GP, which we denote by ¯eavg,
plotted against k (recall n = kd). Several features of these plots are worth singling out:
• All curves lie below 0.20 suggesting that even in very hard problems (τ = 40) the
average error does not get extremely large.
• The case of τ = 3 represents an “easy” problem owing to the small components of
• When ψ is ﬁxed, the curves for d = 5, 10, 15, and 20 are all quite close.
• The choice of n = 10d leads to predictions that on average are accurate to within
10% of the range of the data providing that τ ≤10; reliable ﬁts are barely obtainable
or not obtainable for τ ≥20.
• The improvement in ﬁt for sample sizes greater than n = 10d is marginal.
Suppose ¯eavg decreases with n approximately at the convergence rate n−c. The rate
c can be estimated from the points shown in Figure 4 from the slope of the least squares
ﬁt of log(¯eavg) regressed on log(k). The estimated rates are in Table 2.
There are a few interesting things to notice in Table 2. For easy problems (τ = 3)
convergence rates close to 1 are achievable for dimensions as large as d = 20 so that
doubling sample size can reduce eavg by half.
On the other hand, in hard problems
the rates of convergence can be very small. For example, when d = 15 and τ = 20,
it takes about 8 times as many runs to reduce eavg by half. When τ = 40 it appears
hopeless to reduce eavg substantially without enormous sample sizes. In such situations,
the computer experiment may have to be reformulated and restricted.
Figure 4: The four panels correspond to four values of τ, and ψ = τ 2/5. In each panel,
¯eavg is plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)
and d = 20 (dot-dashed line).
Table 2: Estimated convergence rates for ¯eavg
The arguments in Section 4 suggest that for ﬁxed total sensitivity τ, dividing τ equally
across the d input variables is the worst case for prediction accuracy, i.e., ψ = τ 2/d.
Figure 5 explores worst-case problems by plotting eavg against τ. There is a separate
plot for d = 5, 10, 15, 20, and n = 10d throughout. Fifty simulated realizations are made
for each value of τ.
The lines in Figure 5 drawn through the averages of eavg show
Figure 5: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each
panel, eavg (squares) from 50 realizations and ¯eavg (solid line) are plotted against τ. The
horizontal line indicates accuracy to within 10% of the range of the data.
little diﬀerence as d increases, as predicted in Section 4 for the worst case studied here.
There is a small dimensionality eﬀect (and n = 10d is increasing with d), but the total
sensitivity, τ, is the important factor. For τ ≥20, eavg is above the target of 0.1 for
all d studied, though it tends to remain below about 0.2 to 0.25. This latter fact is a
somewhat surprising feature and, in fact, holds for τ as large as 100 (not shown).
To investigate the more realistic situation where the problem has some degree of
sparsity we allow ψ to vary. We ﬁx d = 10 and n = 100. As suggested by Figure 4, for
ﬁxed values of τ and ψ, results for other values of d (with n = 10d) are similar. For each
ﬁxed value of τ we increase the value of ψ so that the sparsity is increased, and the total
sensitivity of the function is shifted to fewer and fewer dimensions.
Figure 6: The four panels correspond to four values of τ. In each panel, eavg (squares)
from 50 realizations and ¯eavg (solid line) are plotted against ψ.
The horizontal line
indicates accuracy to within 10% of the range of the data.
Even a moderate degree of sparsity can result in drastic reduction of error.
τ = 40 panel in Figure 6 is interesting, since even in such a complex problem reasonable
accuracy can be obtained when there is a degree of sparsity. In particular the last few
values of ψ represent situations where the 10-dimensional problem contains ﬁve or fewer
active dimensions.
Simulation Results for Maximum Error
In Section 5 results were presented corresponding to ¯eavg; in this section we discuss the
similarities and diﬀerences that arise in using ¯emax, i.e. the average of emax in (6) across
simulations. The four panels in Figure 7 correspond to τ = 3, 10, 20, and 40, respectively.
In each panel, curves for ¯emax (averaged across 50 simulations) are plotted for d = 5, 10,
15, and 20. Clearly, one would not expect to see the same level of accuracy as obtained
using ¯eavg so we set a threshold of 0.2 as opposed to the threshold of 0.1 used for eavg.
Comparing this to the plots in Figure 4 there a few things to notice:
Figure 7: The four panels correspond to four values of τ and ψ = τ 2/5. In each panel
¯emax is plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)
and d = 20 (dot-dashed line).
• The case of τ = 3 again represents an “easy” problem owing to the small components of θ.
• When τ is ﬁxed, the curves for d = 5, 10, 15, and 20 are all quite close.
• The choice of n = 10d leads to predictions that on average are accurate to within
20% to 30% of the range of the data providing that τ ≤10; reliable ﬁts are barely
obtainable or not obtainable for τ ≥20. When τ = 10 thresholds for emax are
somewhat harder to reach compared to those using eavg.
• The improvement in ﬁt for sample sizes greater than n = 10d is marginal.
The analysis leading to Table 2 can be duplicated for ¯emax; the result is Table 3. The
convergence rates, as expected, are lower than for ¯eavg and are essentially 0 when τ = 40.
Table 3 can be used in the same way as Table 2 is used in Section 7 to derive sample
sizes needed to reduce ¯emax.
Table 3: Estimated convergence rates for ¯emax
Figure 8 explores worst-case problems (equal θj) by plotting emax against τ. There is
a separate plot for d = 5, 10, 15, 20, and n = 10d throughout. Fifty simulated realizations
are made for each value of τ. The lines in Figure 8 drawn through the averages of emax
show little diﬀerence as d increases. Comparing this to the analogous plot in Figure 5
we see the same general trend using emax as opposed to eavg. There is again a small
dimensionality eﬀect, but the total sensitivity, τ, is the important factor. For τ ≥20,
emax is above the target of 0.2 for all d.
The Figure 9 shows the eﬀect of sparsity when using emax and corresponds to the plot
in Figure 6 for eavg. Even a moderate degree of sparsity can result in drastic reduction of
error. The τ = 40 panel in Figure 9 is interesting, since even in such a complex problem
reasonable accuracy can be obtained when there is a degree of sparsity. In particular the
last few values of ψ represent situations where the 10-dimensional problem contains ﬁve
or fewer active dimensions. This is similar to what was found in Figure 6.
Figure 8: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each
panel, emax (squares) from 50 realizations and ¯emax (solid line) are plotted against τ. The
horizontal line indicates accuracy to within 20% of the range of the data.
Follow-up Experiments
Suppose an initial experiment of given sample size has been conducted. We now have
real data from running the code to ﬁt a GP model following the methodology described
in Section 2. Estimates of the correlation parameters, θ and p, are available, as well as
values of eavg|code and emax|code computed from (5) and (6).
What should be done to augment the initial design, if anything? Set eA as a threshold
value for acceptable eavg (for example, eA = 0.1). If eavg|code < eA then nothing more
needs to be done to increase accuracy. If eavg|code > eA then we propose the following
follow-up strategy:
1. Do a simulation study using the estimated correlation parameters, ˆθ and ˆp, and
the initial sample size. Compute eavg (and emax) for each realized data set.
Figure 9: The four panels correspond to four values of τ. In each panel, emax (squares)
from 50 realizations and ¯emax (solid line) are plotted against ψ.
The horizontal line
indicates accuracy to within 20% of the range of the data.
2. If the distribution of the eavg values from the simulations suggests a non-trivial
probability of exceeding eA, it is plausible that the initial sample size is inadequate
and we could go to Step 4. Otherwise, continue with Step 3.
3. If the distribution of the eavg values from the simulations is inconsistent with
eavg|code, the appropriateness of the GP model is in question; see Section 8. Of
course, it would be foolish to worry about small inconsistencies, and some subjectivity is inevitable in assessing what is substantial.
4. To decide on a follow-up sample size, explore plausible choices of n through a
simulation study using the estimated correlation parameters, ˆθ and ˆp.
In order to illustrate the points above, we revisit the G-protein example in Section 3.
The initial sample size of n = 33 led to eavg|code = 0.0221 (see Figure 1). For most
examples this would be considered small, and there would be nothing further to do.
On the other hand, if we wanted to reduce the error rate by a half, a simulation could
be performed using the outline above. Since ˆp = 2, we could also consult Table 2 to
calculate the desired sample size. From the analysis of the n = 33 runs, ˆτ = 2.52 and
ˆψ = 3.15. For d = 5 and τ = 3, Table 2 gives a rate of convergence of 1.34, which is very
likely to be conservative. Thus, a sample size of at least n = 33(21/1.34) = 55 is required.
From Figure 1, eavg|code at n = 60 is 0.0137, which is just slightly larger than half of the
observed value when n = 33.
Gough and Welch considered a model for ocean circulation with d = 7 inputs
and seven diﬀerent outputs, y1, . . . , y7. From an initial experiment yielding 36 good runs,
a GP was ﬁt separately for each output. In each case p = 2 for each output y1, . . . , y7.
Estimated values of θ are provided in Table 4. The ˆτ and ˆψ rows in Table 4 summarize
Table 4: Estimates of θ for the ocean-circulation model.
the GP ﬁts from the 36 runs.
If eA = 0.1, the values for eavg|code in Table 5 show that a reasonable approximation
has been obtained. Moreover, for all output variables, eavg|code is within ¯eavg ± 2 bsd(eavg),
i.e., eavg|code lies within the support of the empirical distribution of simulated eavg. The
Table 5: Actual and simulated accuracy measures for the ocean-circulation model
accuracy for y7 is lower than for the other output variables, however, and we consider
reducing eavg|code from 0.078 to, say, 0.05. In this case ˆp = 2, and again we can use Table
2 for guidance in choosing an appropriate sample size.
Interpolating the convergence rates in the table suggest that ¯eavg should be decreasing
at rate roughly 1/n, and reducing the sample size to 0.05 would require a sample size
of approximately 56 runs. Alternatively, cutting ¯eavg by half would require doubling the
run size. As no further code runs are available we investigate this strategy by simulating
what would have happened if n = 70 runs had been performed: for y7, ¯eavg is reduced by
just over a factor of 2, as predicted.
Chapman et al. analyzed a computer code describing the seasonal growth and
decline of Arctic sea ice. The code had d = 13 input variables and four outputs, y1, . . . , y4.
From an initial design of n1 = 69 runs, GPs were ﬁt separately for each output. Every
ﬁtted GP had at least one input variables with ˆpj < 2. Estimated values of θj and
αj = 2 −pj for n1 = 69 runs are provided in Table 6.
Estimated vales of θj and αj = 2 −pj for n = 157 runs are provided in Table 7.
The values of eavg|code are in Table 8; each is below 0.1, and if eA = 0.1 we would be
tempted to stop.
The eavg|code values for y3 and y4 are below 0.1 and within 2 bsd(eavg) of ¯eavg but the
emax|code values of about 0.5 are of concern. The simulated emax distributions are inconsistent with the observed emax|code values, as evidenced by ¯emax and bsd(emax) in Table 8,
Table 6: Estimates of θj and αj = 2 −pj for the sea-ice code using n1 = 69 runs.
although for y3 the range of the simulated emax values covers emax|code. The sea-ice code
failed to converge for 12 of 81 attempted runs (hence the 69 good runs), a suggestion of
erratic behavior of the code and a possible explanation of the diﬀerence between actual
and simulated error in some regions of the input space.
Faced by similar concerns about the approximation accuracy from the initial experiment, Chapman et al. opted to make additional runs and ended up with a total
of 157 good code runs. As these are the only follow-up runs available, we restrict our
analysis to seeing whether we can predict by simulation the impact of such a follow-up
experiment.
The accuracy measures for the n = 157 runs conducted and from simulation are
Table 7: Estimates of θj and αj = 2 −pj for the sea-ice code using n = 157 runs.
compared in Table 8. Relative to n = 69, simulation suggests only modest reduction in
eavg. For y4, even this modest reduction is not realized by eavg|code. With n = 157 runs, the
simulated values of emax are again inconsistent with emax|code for the troublesome y3 and
y4. Although the magnitude of the maximum error is underestimated, the simulations
correctly predict that there will be little impact on emax|code from the further runs. Thus,
the simulation study leads to the same conclusion that Chapman et al. reached
after the follow-up experiment: Taking more runs is not eﬀective. Alternative ways of
proceeding are discussed in Section 8.
Average error
Maximum Error
Table 8: Actual and simulated accuracy measures for the sea-ice code
Comments and Open Issues
There are several open issues, concerned mainly with follow-up once an initial set of code
runs has been collected and analyzed.
Eﬀective dimensionality
The ocean-circulation model had an initial sample size of
n = 36, about half the recommended value of n = 10d. Even so, a good ﬁt was obtained.
A closer look at this application shows that ˆθ has elements that are near zero for three
of the input variables. Thus, the input space is eﬀectively reduced to d = 4 dimensions,
leading to a recommendation of n = 40. If there are good a priori reasons to expect that
the number of active dimensions, d0, is less than d then choosing n = 10d0 could be a
useful complement to the recommended strategy, especially if there are serious budget
constraints.
The GP model is a poor ﬁt
Good general strategies to cope with lack of ﬁt of the GP model are not readily
available. There is interesting work by Gramacy and Lee which could be useful
when runs are plentiful. The approach used extensively by Aslett et al. and by
Gramacy and Lee , of narrowing the space of inputs, better enables approximation
of code output by a homogeneous GP; the assumption of homogeneity is less sustainable
when the input space is too large. But how to do this in a measured way is not clear and
needs further research.
Canonical conﬁgurations of θ
For p = 2, we chose a simple two-parameter family in our analyses in Section 5 and 6.
Other sets of values for θ can be explored, but we ﬁnd little incentive to do so for the
purpose of settling on initial sample size. We have found that even if θ is not a canonical
conﬁguration there is little to no diﬀerence in distributions of eavg or emax relative to a
canonical θ provided τ and ψ are the same.
Treating a GP with p ̸= 2 (as in the sea-ice example)
We have not discussed the relevance, nor the use, of τ and ψ when p ̸= 2. The
interpretation of τ and ψ values need to be reexamined.
In the case of the exponential correlation function (all pj = 1), the implied prior
distribution is on a much larger class of functions and achieving good accuracy is more
diﬃcult. It is easy to work out the mean and variance of h1
j as in Lemma 1, and again
we ﬁnd that τ and ψ should be important. The exact values are given in Lemma 2.
Lemma 2: Let hj be the distance between two randomly chosen points for variable xj
in a random LHD. Then
Var(hj) = 1
(n −2)(n + 1)
The proof of Lemma 2 can be found in Appendix A. Note that the two moments converge
to 1/3 and 1/18 as n →∞, i.e., they do not depend on n in the limit. The mean of
j, is now approximately twice that for the case pj = 2, indicating that larger samples
could be needed to achieve desired accuracy. How this all plays out in analogues of the
analyses in Section 5 and 6, to enable follow-up recommendations has yet to be explored.
When 1 < pj < 2, exact calculations of the mean and variance of h
are not available. Approximations are obtainable as follows, however. Assume that xj and x′
approximately independent and uniform on , and again let hj = |xj −x′
j|. We ﬁnd
(pj + 1)(pj + 2)
(pj + 1)(2pj + 1).
For pj = 2 this produces E(h2
j) = 1/6 and Var(h2
j) = 7/180, which are the asymptotic
values found in Lemma 1.
For the general case, with pj varying with xj, assume the design is a completely
random LHD. Asymptotically, h(x, x′) = Pd
j=1 θj|xj −x′
j|pj in (2) has mean depending
on both θ and p:
(pj + 1)(pj + 2).
Similarly, it has asymptotic variance
(pj + 1)(2pj + 1) −
(pj + 1)2(pj + 2)2
Deﬁning canonical sets of correlation parameters is now much more complicated. Some
preliminary calculations for the sea-ice application suggest that the convergence rates for
p ̸= 2 are diﬀerent from those obtained when p = 2 and thus one must examine rates
for various combinations of both θ and p. This too calls for additional examination.
Discussion
In the introduction we raised a set of issues that should be treated. In the subsequent
sections we have provided evidence that:
• “n = 10d” is a viable and valuable rule-of-thumb for choosing an initial sample size
for a computer experiment.
• Criteria can make a diﬀerence for post-experimental analysis but have less inﬂuence
on initial sample size. The sea-ice example shows that the conﬂict between the eavg
and emax criteria has implications, as spelled out in Section 7. However, as seen in
Section 6 both criteria support the “n = 10d” rule.
• When p = 2 there is good information about rates at which error decreases with
n and when feasible sample sizes are available. These depend on the parameters τ
and ψ, whose values are not known until the post-experimental stage and are then
useful for deciding how to follow-up. When p ̸= 2 much remains to be done.
• In the case that accuracy goals are not met with an initial sample size, a follow-up
strategy is needed, but a full analysis is lacking and is a topic for further inquiry.
APPENDIX A: Proof of Lemma 1
Proof of Lemma 1:
Let D be an n × d random LHD, and let xj and x′
j be any two randomly chosen runs
of the design in dimension j. The construction of the LHD ensures that xj ̸= x′
hence xj and x′
j are dependent random variables.
There are a total of
pairs of points and each pair is equally likely. Clearly, P(xj = i/(n −1)) = 1/n and
j = k/(n −1)|xj = i/(n −1)) = 1/(n −1). Consider any two points that are an
absolute distance of i/(n −1) apart. By a simple counting argument, there are n −i
pairs giving rise to this distance. This establishes
P(hj = i/(n −1)) = (n −i)
i = 1, . . ., n −1.
The expected value of h2
Similarly,
n(n −2)(n + 1)(7n + 9)
Algebra was carried out in Maple. 2
Proof of Lemma 2:
Following Lemma 1, the expected value is:
Similarly, the variance is:
j) −E(hj)2 = 1
(n −2)(n + 1)
Algebra was carried out in Maple. 2
ACKNOWLEDGEMENTS
The research of Loeppky and Welch was supported by grants from the Natural Sciences and Engineering Research Council of Canada.