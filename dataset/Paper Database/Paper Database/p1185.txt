HAL Id: inria-00541788
 
Submitted on 1 Dec 2010
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Guest Editorial Active Learning and Intrinsically
Motivated Exploration in Robots: Advances and
Challenges
Manuel Lopes, Pierre-Yves Oudeyer
To cite this version:
Manuel Lopes, Pierre-Yves Oudeyer.
Guest Editorial Active Learning and Intrinsically Motivated
Exploration in Robots: Advances and Challenges. IEEE Transactions on Autonomous Mental Development, 2010, 2 (2), pp.65–69. ￿inria-00541788￿
IEEE TRANSACTIONS ON AUTONOMOUS MENTAL DEVELOPMENT, VOL. 2, NO. 2, JUNE 2010
Guest Editorial
Active Learning and Intrinsically Motivated
Exploration in Robots: Advances and Challenges
I. TWO CONVERGING APPROACHES TO EXPLORATION
EARNING techniques are increasingly being used in
today’s complex robotic systems. Robots are expected to
deal with a large variety of tasks using their high-dimensional
and complex bodies, to manipulate objects and also, to interact
with humans in an intuitive and friendly way. In this new
setting, not all relevant information is available at design time,
and robots should typically be able to learn, through self-experimentation or through human–robot interaction, how to tune
their innate perceptual-motor skills or to learn, cumulatively,
novel skills that were not preprogrammed initially. In a word,
robots need to have the capacity to develop in an open-ended
manner and in an open-ended environment, in a way that is
analogous to human development which combines genetic
and epigenetic factors. This challenge is at the center of the
developmental robotics ﬁeld , – . Among the various
technical challenges that are raised by these issues, exploration
is paramount. Self-experimentation and learning by interacting
with the physical and social world is essential to acquire new
knowledge and skills.
Exploration in physical robotic agents poses challenging
problems due to the high-dimensional and complex dynamics
of the body–environment system (especially when other agents
are part of the environment), and to the open-ended nature of
the sensory-motor spaces of real environments. Typically, in
those spaces, the lack of adequate constraints in exploration
strategies can result in at best very slow learning, and most
often in no consistent learning that can even be dangerous or
destructive.
This special issue1 provides a discussion forum and presents
novel contributions on intentional exploration, i.e., internal
mechanisms and constraints that explicitly foster organized
exploration. Of central interest are mechanisms that push
agents to select actions that allow them to gain maximal
knowledge or maximal control/competence over their bodies
and environments. To this, end different ﬁelds suggested different approaches, ranging from operational heuristics that
indirectly target maximal information gain, to theoretically
Digital Object Identiﬁer 10.1109/TAMD.2010.2052419
1Two IEEE technical committees sponsor this special issue, CIS TC on Autonomous Mental Development and the RAS TC on Robot Learning, to incorporate contributions from a large diversity of approaches.
optimal policies in relation to various criteria. The ﬁeld of
statistical learning theory studied approaches such as optimal
experimental design and active learning – , , ,
and the ﬁeld of developmental and cognitive robotics studied
approaches based on intrinsic motivation , , , .
Active learning was initially developed for classiﬁcation and
regression learning problems where the cost of querying a given
data point for its label or output value is high. Therefore, the goal
is to ﬁnd strategies to minimize the number of queried data to
reach a given level of performance, thus maximizing the usefulness of each new experiment. This goal can be instantiated
in various settings , such as pool-based querying, sequential acquisition, experimental design, among others. A large diversity of criteria can be used to evaluate the utility of given
sampling candidates, such as the maximization of prediction errors , the local density of already queried points , the
maximization of the decrease of global model variance , expected improvement , or maximal uncertainty of the model
 , among others. There have been active-extensions to most
of the existing learning methods, e.g., logistic regression ,
support vector machines , and gaussian processes – .
It has been proved that, under mild conditions, an active learner
outperforms a passive learner , , provided that we have
some information on the structure of the problem, even when we
cannot estimate in advance that improvement . However, the
extension of these guarantees to more general models is still an
open question.
Another approach to exploration came from an initially different problem, that of understanding how robots could achieve
cumulative and open-ended learning. This raised the question
of the task-independent mechanisms that may allow a robot
to get interested in practicing skills that were not speciﬁed at
design time. Two communities of researchers, the ﬁrst one in
reinforcement learning , , , , , the second
one in developmental robotics , , – , formalized,
implemented, and experimented several mechanisms based on
the concept of intrinsic motivation (sometimes called curiositydriven learning) grounded into theories of motivation, spontaneous exploration, free play, and development in humans ,
 , , as well as in recent ﬁndings in the neuroscience of
motivation – .
As argued in , , and , architectures based on
intrinsically motivated learning can be conceptualized as active
learning mechanisms which, in addition to allowing for the
self-organized formation of behavioral and developmental
complexity, can also allow an agent to efﬁcient learn a model
1943-0604/$26.00 © 2010 IEEE
Authorized licensed use limited to: CR Bordeaux. Downloaded on August 03,2010 at 09:28:50 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON AUTONOMOUS MENTAL DEVELOPMENT, VOL. 2, NO. 2, JUNE 2010
of the world by parsimoniously designing its own experiments/queries. Yet, in spite of these similarities between work
in active learning and intrinsic motivation, these two strands
of approaches often differ in their underlying assumptions and
constraints, leading to sometimes very different active learning
algorithms. Many active learning models make combinations
of the following assumptions:
• it is possible to learn a model of the complete world within
the lifetime of the learning agent;
• the world is learnable everywhere;
• the noise is homogeneous.
Given those assumptions, heuristics based on the exploration of
parts of the space where the learned model has maximal uncertainties or where its prediction are maximally wrong are often
very efﬁcient. Yet, these assumptions typically do not hold in
real-world robots in an unconstrained environment: the sensorimotor spaces, including the body dynamics and its interactions
with the external world, are simply much too large to be learned
entirely in a lifetime even with optimal exploration strategies;
there are typically subspaces which are unlearnable due to inadequate learning biases or unobservable variables; noise can
be strongly heterogeneous. Thus, different authors claimed that
typical criteria used in traditional active learning approaches,
such as the search for maximal uncertainty or prediction errors,
might get trapped or become inefﬁcient in situations that are
common in open-ended robotic environments , . To approach those challenges, different criteria were elaborated, such
as the search for maximal reduction in prediction error, maximal compression progress, or maximal competence progress
 , . Only very recently have these approaches been applied
to robotic problems, and even more recently if we consider examples with real robots. Nevertheless, examples that consider
robotic problems already exist for a large variety of problems:
mapping , , reinforcement learning , body schema
learning , imitation , , exploration of objects and
body properties , manipulation , among many others.
This special issue tries to address several topics. How can traditional active learning heuristics be applied to robotics problems such as motor learning, affordance learning or interaction learning? How to select an active strategy? Are there general purpose methods, or are they task dependent? How can
active and intrinsically motivated exploration enable life-long,
task-independent learning and development? Is there a uniﬁed
formalism to both approaches? How precisely do they model
human active learning and exploration and its role in development? Can these approaches be used for social tasks, e.g.,
joint-work and human–robot interaction?
II. CONTENTS OF THE SPECIAL ISSUE
This special issue contains a selection of six papers among 20
papers that were submitted.
The ﬁrst paper attacks the fundamental question of what
are extrinsic and intrinsic motivations and rewards, and how
they relate to each other, by formulating this question within
a broad evolutionary perspective. The authors show that when
internal reward systems are evolved for organisms that have to
solve a given (set of) task(s) in environments that change and
vary, then typically one ends up with reward systems which do
not directly and solely encode task-speciﬁc rewards, but also
encode rewards which push the organism to search explicitly
for novelty. This gives several important theoretical and practical insights. First, it shows that, even in situations where there
is a predeﬁned task, the optimal reward function that should be
given to an agent equipped with machine learning algorithms,
such as those used for reinforcement learning problems, shall
not always be a direct “translation” of the task ﬁtness function.
It might be very beneﬁcial to include task-independent reward
mechanisms such as the search for novelty. This is an important contribution since it provides a ﬁrm theoretical basis to
understand how and when task-independent intrinsically motivated exploration can be helpful to solve particular tasks, as
illustrated in the various existing experiments in developmental
robotics and intrinsically motivated reinforcement learning.
Furthermore, this evolutionary perspective also provides the
tools to understand the relation between work on intrinsic
motivation in developmental robotics with other work recently
presented in evolutionary robotics research , , which
showed that when evolving robot controllers for solving a
predeﬁned tasks in a changing environment, it can be very
useful to use a ﬁtness function which does push the system to
search for novelty explicitly and independently from the task.
Finally, the evolutionary perspective provided in allows us
to understand that there is probably a continuum between what
we call “intrinsic” and “extrinsic” motivation, and that they
might be entangled in a complex manner in real organisms. This
work also provides the practical insight that evolving the reward
function of learning robots, even for task-speciﬁc setups, might
theoretically provide more efﬁcient reward functions than those
that can be built by hand. This practical insight is explored
and conﬁrmed in a follow-up paper , which shows how
evolutionary programming method can allow to ﬁnd in practice
those theoretically near-optimal reward functions.
The third paper shows how to control the motion of a virtual
eye to optimally gather information in the environment . The
problem of detecting desired objects in an image is approached
with an Infomax perspective. Images are searched by relying on
probabilistic models to decide where and how to sample next,
thus avoiding the sampling of the whole image. It is shown
how active models of attentional control can speed up importantly the achievement of tasks such as image recognition or
interpretation.
An example of using active learning in a human–robot interaction scenario is presented by . Some previous approaches
suggested different methods for using active learning in a
learning by demonstration scenario , . The authors go
further in this topic by providing a real-robot implementation,
and a careful validation and comparison of the efﬁciency of
different approaches in terms of learning the task and the
interaction with humans.
The work presented in compares different motivational
approaches such as novelty-seeking, interest, and competenceseeking motivation. It aims at providing a system that is able
to incorporate the different mechanisms in a single formalism,
and thus to compare quantitatively, in given contexts, various
Authorized licensed use limited to: CR Bordeaux. Downloaded on August 03,2010 at 09:28:50 UTC from IEEE Xplore. Restrictions apply.
LOPES AND OUDEYER: ACTIVE LEARNING AND INTRINSICALLY MOTIVATED EXPLORATION IN ROBOTS
technical approaches to the elaboration of intrinsic motivation
systems in robots.
The ﬁnal work addresses the problem of how intrinsic motivation can be used to guide the cumulative learning of a hierarchical structure of skills . Active learning is coupled with
the introduction of incremental option learning, which are two
complementary mechanisms allowing to decrease the time necessary to learn the causal structure of the environment as well
as associated skills.
III. DISCUSSION AND FUTURE PERSPECTIVES
The selection of papers in this issue addresses many important challenges of active learning and intrinsically motivated exploration. In this section, we highlight several open problems
which remain to be further addressed.
A. Systematic Experiments With Real Robots and Scalability
to High-Dimensional Spaces
The use of real robotic platforms is limited in this series of articles as much as in the broader literature on active learning and
intrinsic motivation. An important reason for this is that there
are many practical difﬁculties in setting up experiments with
real robots such that statistically signiﬁcant results can be obtained. In particular, not only the physical experiments should
be repeated to show how stable is the learning and exploration
process, but they should also be performed under different environments and with various exploration strametategies for comparisons. As illustrated in the accepted papers, simulations are
often used to address this issue, and when physical robots are
used, their sensorimotor space is typically tightly constrained to
allow for repeated experiments. Yet, this methodological stance
biases research to avoid a potentially very challenging problem:
how do active learning and intrinsically motivated exploration
scale up in practice in real continuous high-dimensional and
large volume spaces? It is actually surprising to observe that this
question has been very little addressed so far given that among
the initial motivations for elaborating active learning and intrinsic motivation systems, the scalability of exploration in such
spaces was central. Indeed, there are both practical and theoretical challenges ahead of research. One of them is meta-exploration. All active learning and intrinsic motivation systems
are based, in a way or another, on a measure of “interestingness” of particular zones of the sensorimotor/state space, and
are used to push systems to focus on zones of maximal interest. But the computation of this measure of interest for a
given zone of the sensorimotor space requires that this zone be
at least explored/sampled a little bit. In other words, we have
a meta-exploration/meta-exploitation dilemna for achieving ef-
ﬁciently the “smart” exploration process. How this meta-exploration/meta-exploitation dilemna could be solved (or maybe
cannot be solved without particular constraints) in high-dimensional spaces remains to be understood.
B. Comparison, Mixing, and Autonomous Selection of Active
Exploration Criteria
Results have already shown that there is no active learning
criteria that is optimal for all situations. Taking into account the
large variety of exploration criteria, it is important to understand
moreformallywhenandwhycertaincriteriaaremoresuitedthan
others. A related challenge is to devise methods that would allow
an autonomous system, e.g., a robot for example, to dynamically
mix and select those criteria depending on the current situation.
The development of formal guaranties of convergence, and
speed of convergence, of the methods, may provide a rule to
select among them.
C. Integration of Intrinsically Motivated Learning in Complete
Autonomous Creatures
A motivation for introducing the use of intrinsic motivation
systems in developmental robotics was to address aspects of
the problem of open-ended sensorimotor and cognitive development. As argued many times in the literature (e.g., ), ﬂexible
and adaptive behavior and development in autonomous creatures can hardly be built or understood by only focusing on individual components. Development and behavior self-organize
as a result of the complex dynamical interaction between all elements of the cognitive apparatus, of the body, and of the physical and social environment . Yet, most research on active
learning and intrinsic motivation have studied these exploration
mechanisms in an isolated manner. Further work should try to
understand how they might interact with other kinds of motivations systems (e.g., extrinsic motivational systems), within a
complete sensorimotor and cognitive architecture, characterized
by various constraints such as motor primitives or maturation
of the perceptual system, within a body that has peculiar morphological properties and performs morphological computation
 , and within a physical and social environment which also
strongly inﬂuences the organism.
D. Active Learning and Intrinsic Motivation in Multiagent
An aspect to explore further is the use of active learning and
intrinsic motivation in human–robot interaction and other multiagent systems. Results have shown that it is possible to learn
in a multiagent scenario even without knowledge of the other’s
actions and even without knowing that there is another agent
 , . In such settings, exploration is fundamental to ensure convergence. More interesting settings would be based on
an intuitive interaction between robots, or robot and humans,
based on joint–attention and multimodal communication.
E. Evaluation Methodology
As active learning and intrinsic motivation have different origins and motivations, their similarities in terms of technical approaches should be counterbalanced with their dissimilarities in
terms of evaluation methodologies. This means that depending
on the point of view, they might or might not be considered as
identical scientiﬁc topics. Both approaches can be evaluated on
a single task with a standard loss function. Yet, if we consider a
developmental perspective and evaluate the approaches taking
into account many tasks and life-long learning, then intrinsic
motivation can no longer be seen as only a method for building
fast and accurate models of the world, but also a crucial component of a general development program. Under this perspective, exploration guides not only the learning of a single task but
a learning process that simultaneously acquires sensory-motor
Authorized licensed use limited to: CR Bordeaux. Downloaded on August 03,2010 at 09:28:50 UTC from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON AUTONOMOUS MENTAL DEVELOPMENT, VOL. 2, NO. 2, JUNE 2010
representations, skills, and general task solutions. This includes
hierarchical structures of information and control, among many
other complex representations. In that case, a comprehensive
evaluation framework, including both quantitative and qualitative analysis, is still to be elaborated.
F. Theoretical Understanding of the Nature and Origins of
Intrinsic Motivation and Its Relation to Exploration
Intrinsic motivation is a complex concept imported from psychology, and computational modelling can play a role in understanding it better through attempts of formalization and operationalization. Yet, as discussed in , there is wide diversity of
potential formalization of intrinsic motivation, some of which
appearing compliant with for example Berlyne’s deﬁnition ,
but yet not fostering spontaneous exploration. A paradigmatic
example is that of cognitive homeostasis (e.g., and ),
which is a mechanim which explicitly push autonomous systems to search for low prediction errors and in a task-independant manner. Another example is mechanisms that push organism to be “interested” in synchrony or contingency independantly of the particular task or sensorimotor channels at play
(e.g., ). Thus, the articulation between intrinsic motivation
and spontaneous exploration should be further investigated. The
complexity of the concept of intrinsic motivation is also illustrated by , which shows that searching for hard and fast features for discriminating intrinsic motivation from extrinsic motivation might be impossible. At the same time, provides
a very original outline of an hypothesis for understanding the
evolutionary origins of intrinsic motivation in animals, humans
in particular. It would be particularly stimulating to see this hypothesis going back to and being assessed by psychology, neuroscience, and evolutionary biology.
MANUEL LOPES, Guest Editor
School of Computing and Mathematics
University of Plymouth
Plymouth, PL4 8AA U.K.
E-mail: 
PIERRE-YVES OUDEYER, Guest Editor
Talence, 33640 France
E-mail: