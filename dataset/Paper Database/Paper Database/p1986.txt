A rigorous and robust quantum speed-up in supervised machine learning
Yunchao Liu,1, 2, ∗Srinivasan Arunachalam,2, † and Kristan Temme2, ‡
1Department of Electrical Engineering and Computer Sciences,
University of California, Berkeley, CA 94720
2IBM Quantum, T.J. Watson Research Center, Yorktown Heights, NY 10598
 
Over the past few years several quantum machine learning algorithms were proposed that
promise quantum speed-ups over their classical counterparts. Most of these learning algorithms either assume quantum access to data – making it unclear if quantum speed-ups still
exist without making these strong assumptions, or are heuristic in nature with no provable
advantage over classical algorithms. In this paper, we establish a rigorous quantum speed-up
for supervised classiﬁcation using a general-purpose quantum learning algorithm that only
requires classical access to data. Our quantum classiﬁer is a conventional support vector
machine that uses a fault-tolerant quantum computer to estimate a kernel function. Data
samples are mapped to a quantum feature space and the kernel entries can be estimated
as the transition amplitude of a quantum circuit. We construct a family of datasets and
show that no classical learner can classify the data inverse-polynomially better than random
guessing, assuming the widely-believed hardness of the discrete logarithm problem. Meanwhile, the quantum classiﬁer achieves high accuracy and is robust against additive errors in
the kernel entries that arise from ﬁnite sampling statistics.
Finding potential applications for quantum computing which demonstrate quantum speed-ups
is a central goal of the ﬁeld. Much attention has been drawn towards establishing a quantum
advantage in machine learning due to its wide applicability . In this direction there have been
several quantum algorithms for machine learning tasks that promise polynomial and exponential
speed-ups. A family of such quantum algorithms assumes that classical data is encoded in amplitudes of a quantum state, which uses a number of qubits that is only logarithmic in the size of
the dataset. These quantum algorithms are therefore able to achieve exponential speed-ups over
classical approaches . However, it is not known whether data can be eﬃciently provided this
way in practically relevant settings. This raises the question of whether the advantage comes from
the quantum algorithm, or from the way data is provided . Indeed, recent works have shown
that if classical algorithms have an analogous sampling access to data, then some of the proposed
exponential speed-ups do no longer exist .
Consequently a diﬀerent class of quantum algorithms has been developed which only assumes
classical access to data. Most of these algorithms use variational circuits for learning, where a
candidate circuit is selected from a parameterized circuit family via classical optimization . Although friendly to experimental implementation, these algorithms are heuristic in nature
since no formal evidence has been provided which shows that they have a genuine advantage over
classical algorithms. An important challenge is therefore to ﬁnd one example of such a heuristic
quantum machine learning algorithm, which given classical access to data can provably outperform
all classical learners for some learning problem.
In this paper, we answer this in the aﬃrmative. We show that an exponential quantum speedup can be obtained via the use of a quantum-enhanced feature space , where each data
point is mapped non-linearly to a quantum state and then classiﬁed by a linear classiﬁer in the
high dimensional Hilbert space. To eﬃciently learn a linear classiﬁer in feature space from training
data, we use the standard kernel method in support vector machines (SVMs), a well-known family
∗ 
† 
‡ 
 
of supervised classiﬁcation algorithms . We obtain the kernel matrix by measuring the
pairwise inner product of the feature states on a quantum computer, a procedure we refer to as
quantum kernel estimation (QKE). This kernel matrix is then given to a classical optimizer that
eﬃciently ﬁnds the linear classiﬁer that optimally separates the training data in feature space by
running a convex quadratic program.
The advantage of our quantum learner stems from its ability to recognize classically intractable
complex patterns using the feature map. We prove an end-to-end quantum advantage based on this
intuition, where our quantum classiﬁer is guaranteed to achieve high accuracy for a classically hard
classiﬁcation problem. We show that under a suitable quantum feature map, the classical data
points, which are indistinguishable from having random labels by eﬃcient classical algorithms, are
linearly separable with a large margin in high-dimensional Hilbert space. Based on this property,
we then combine ideas from classic results on the generalization of soft margin classiﬁers to
rigorously bound the misclassiﬁcation error of the SVM-QKE algorithm. The optimization for large
margin classiﬁers in the SVM program is crucial in our proof, as it allows us to learn the optimal
separator in the exponentially large feature space, while also making our quantum classiﬁer robust
against additive sampling errors.
Our classiﬁcation problem that shows the exponential quantum speed-up is constructed based on
the discrete logarithm problem (DLP). We prove that no eﬃcient classical algorithm can achieve an
accuracy that is inverse-polynomially better than random guessing, assuming the widely-believed
classical hardness of DLP.
In computational learning theory, the use of one-way functions for
constructing classically hard learning problems is a well-known technique . Rigorous separations
between quantum and classical learnability have been established using this idea in the quantum
oracular and PAC model , as well as in the classical generative setting .
quantum algorithms are constructed speciﬁcally to solve the problems for showing a separation,
and in general are not applicable to other learning problems.
Based on diﬀerent complexitytheoretic assumptions, evidences of an exponential quantum speed-up were shown for a quantum
generative model , where the overall performance is not guaranteed.
A classically intractable learning problem
The task of supervised classiﬁcation is to assign a label y ∈{−1, 1} to a datum x ∈X from data
space X according to some unknown decision rule f (usually referred to as a concept), by learning
from labeled examples S = {(xi, yi)}i=1,...,m that are generated from this concept, yi = f(xi). Given
the training set S, an eﬃcient learner needs to compute a classiﬁer f∗in time that is polynomial
in the size of S, with the goal of achieving high test accuracy,
accf(f∗) = Pr
x∼X [f∗(x) = f(x)] ,
the probability of agreeing with f on unseen examples.
Here we assume that the datum x is
sampled uniformly random from X, in both training and testing, and the size of S is polynomial
in the data dimension.
An important ingredient of machine learning is prior knowledge, i.e., additional information
given to the learning algorithm besides the training set. In standard computational learning theory , this is modeled as a concept class – a (often exponentially large) set of labeling rules,
and the target concept is promised to be chosen from the concept class. A concept class C is
eﬃciently learnable if for every f ∈C, an eﬃcient algorithm can achieve 0.99 test accuracy by
learning from examples labeled according to f with high success probability. See Appendix A for
detailed deﬁnitions.
Our concept class that separates quantum and classical learnability is based on the discrete
logarithm problem (DLP). For a large prime number p and a generator g of Z∗
p = {1, 2, . . . , p −1},
FIG. 1. Learning the concept class C by a quantum feature map. (a) After taking the discrete log of the data
samples, they become separated in log space by the concept s. (b) However, in the original data space, the
data samples look like randomly labeled and cannot be learned by an eﬃcient classical algorithm. (c) Using
the quantum feature map, each x ∈Z∗
p is mapped to a quantum state |φ(x)⟩, which corresponds to a uniform
superposition of an interval in log space starting with logg x. This feature map creates a large margin, as
the +1 labeled example (red interval) has high overlap with a separating hyperplane (green interval), while
the −1 labeled example (blue interval) has zero overlap.
it is a widely-believed conjecture that no classical algorithm can compute logg x on input x ∈Z∗
in time polynomial in n = ⌈log2(p)⌉, the number of bits needed to represent p. Meanwhile, DLP
can be solved by Shor’s quantum algorithm in polynomial time.
Based on DLP, we deﬁne our concept class C = {fs}s∈Z∗p over the data space X = Z∗
p as follows,
if logg x ∈[s, s + p−3
Each concept fs : Z∗
p →{−1, 1} maps half the elements in Z∗
p to +1 and half of them to −1.
To see why the discrete logarithm is important in our deﬁnition, note that if we change logg x
to x in Eq. (2), then learning the concept class C is a trivial problem. Indeed, if we imagine the
elements of Z∗
p as lying on a circle, then each concept fs corresponds to a direction for cutting the
circle in two halves (Fig. 1a). Therefore, the training set of labeled examples can be separated as
two distinct clusters, where one cluster is labeled +1 and the other labeled −1. To classify a new
example, a learning algorithm can simply decide based on which cluster is closer to the example.
This intuition also explains why the original concept class C is learnable by a quantum learner,
since the learner can use Shor’s algorithm to compute the discrete log for every data sample it
receives , and then solve the above trivial learning problem.
On the other hand, due to the classical intractability of DLP, the training samples look like
randomly labeled from the viewpoint of a classical learner (Fig. 1b). In fact, we can prove that the
best a classical learner can do is randomly guess the label for new examples, which achieves 50%
test accuracy. These results are summarized below.
Theorem 1. Assuming the classical hardness of DLP, no eﬃcient classical algorithm can achieve
poly(n) test accuracy for C.
Our proof, c.f. Appendix B, of classical hardness of learning C is based on an average-case
hardness result for discrete log by Blum and Micali . They showed that computing the most
signiﬁcant bit of logg x for 1
poly(n) fraction of x ∈Z∗
p is as hard as solving DLP. We then reduce
our concept class learning problem to DLP using this result, by showing that if an eﬃcient learner
can achieve 1
poly(n) test accuracy for C, then it can be used to construct an eﬃcient classical
algorithm for DLP, which proves Theorem 1.
In addition to establishing a separation between quantum and classical learnability for binary
classiﬁcation, we also note that this separation can be eﬃciently veriﬁed by a classical veriﬁer in an
interactive setting. This follows from a nice property of our concept class. We show that for every
concept f ∈C, we can eﬃciently generate labeled examples (x, f(x)) classically where x ∼Z∗
uniformly distributed, despite f(x) being hard to compute by deﬁnition. To test if a prover can
learn C, a classical veriﬁer can pick a random concept and eﬃciently generate two sets of data
(S, T), where S is a training set of labeled examples and T ⊆Z∗
p is a test set of examples with
labels removed. The veriﬁer then sends (S, T) to the prover and asks for labels for T, and ﬁnally
accepts or rejects based on the accuracy of these labels. As a corollary of Theorem 1, an eﬃcient
quantum learner can pass this challenge, while no eﬃcient classical learner can pass it assuming
the classical hardness of DLP.
Eﬃcient learnability with QKE
We now turn our attention to general-purpose quantum learning algorithms which only requires
classical access to data and in principle can be applied to a wide range of learning problems.
Examples include quantum neural networks , generative models , and kernel methods . The main challenge of proving a quantum advantage for these algorithms is that they may
not be able to utilize the full power of quantum computers, and it is unclear if the set of problems
that they can solve is beyond the capability of classical algorithms. Indeed, previous analysis of
these quantum algorithms only establish evidence that parts of the algorithm cannot be eﬃciently
simulated classically , which does not guarantee that the algorithms can solve classically
hard learning problems.
Recall that we have constructed a learning problem that is as hard as the discrete log problem.
This implies classical intractability assuming the hardness of discrete log, while also assuring that
the problem is within the power of quantum computers due to Shor’s algorithm. This provides
the basis to solving our main challenge – we now show that the concept class C can be eﬃciently
learned by our support vector machine algorithm with quantum kernel estimation (SVM-QKE).
This formally establishes our intuition that quantum feature maps can recognize patterns that are
unrecognizable by classical algorithms, even when the quantum classiﬁer is inherently noisy due
to ﬁnite sampling statistics. We have therefore established an end-to-end quantum advantage for
quantum kernel methods.
The core component in our algorithm that leads to its ability to outperform classical learners
is the quantum feature map. For learning the concept class C, the feature map is constructed prior
to seeing the training samples and has the following form,
x 7→|φ(x)⟩=
which maps a classical data point x ∈Z∗
p to a n-qubit quantum state |φ(x)⟩⟨φ(x)|, and k = n−t log n
for some constant t. This family of states was ﬁrst introduced in to study the complexity of
quantum state generation and statistical zero knowledge, where it is shown that |φ(x)⟩= U(x) |0n⟩
can be eﬃciently prepared on a fault tolerant quantum computer by a circuit U(x) which uses
Shor’s algorithm as a subroutine, c.f. Appendix D 1.
In learning algorithms, feature maps play the role of pattern recognition: the intrinsic labeling
patterns for data, which are hard to recognize in the original space (Fig. 1b), become easy to
Quantum kernel estimation.
A quantum feature map x 7→|φ(x)⟩is represented by a circuit,
|φ(x)⟩= U(x) |0n⟩. Each kernel entry K(xi, xj) is obtained using a quantum computer by running the
circuit U †(xj)U(xi) on input |0n⟩, and then estimate
⟨0n|U †(xj)U(xi)|0n⟩
2 by counting the frequency of
the 0n output.
identify once mapped to the feature space. Our feature map indeed achieves this by mapping low
dimensional data vectors to the Hilbert space with exponentially large dimension. For each concept
fs ∈C, we show that there exists a separating hyperplane ws = |φs⟩⟨φs| in feature space. That
is, for +1 labeled examples, we have Tr[ws |φ(x)⟩⟨φ(x)|] = | ⟨φs|φ(x)⟩|2 = 1/poly(n) , while for −1
labeled examples we have | ⟨φs|φ(x)⟩|2 = 0 with probability 1−1/poly(n) (Fig. 1c). If we think of ws
as the normal vector of a hyperplane in feature space associated with the Hilbert-Schmidt inner
product, then this property suggests that +1/-1 labeled examples are separated by this hyperplane
by a large margin. This large margin property is the fundamental reason that our algorithm can
succeed: it suggests that to correctly classify the data samples, it suﬃces to ﬁnd a good linear
classiﬁer in feature space, while also guaranteeing that such a good linear classiﬁer exists.
The idea of applying a high-dimensional feature map to reduce a complex pattern recognition
problem to linear classiﬁcation is not new, and has been the foundation of a family of supervised
learning algorithms called support vector machines (SVMs) . Consider a general feature
map φ : X →H that maps data to a feature space H associated with an inner product ⟨·, ·⟩. To
ﬁnd a linear classiﬁer in H, we consider the convex quadratic program
s.t. yi · ⟨φ(xi), w⟩≥1 −ξi
where ξi ≥0. Here λ > 0 is a constant, w is a hyperplane in H which deﬁnes a linear classiﬁer
y = sign (⟨φ(x), w⟩), and ξi are slack variables used in the soft margin constraints. Intuitively, this
program optimizes for the hyperplane that maximally separates +1/-1 labeled data. Note that (4)
is eﬃcient in the dimension of H. However, once we map to a high-dimensional feature space, it
takes exponential time to ﬁnd the optimal hyperplane. The main insight which leads to the success
of SVMs is that this problem can be solved by running the dual program of Eq. (4)
αiαjyiyjK(xi, xj) −1
where K(xi, xj) = ⟨φ(xi), φ(xj)⟩is the kernel matrix. This dual program, which returns a linear
classiﬁer deﬁned as y = sign (Pm
i=1 αiyiK(x, xi)), is equivalent to the original program as guaranteed by strong duality. Eﬀectively, this means that we can do optimization in the high-dimensional
feature space eﬃciently, as long as the kernel K(xi, xj) can be eﬃciently computed.
The same insight can be applied to our quantum feature map: to utilize the full power of the
quantum feature space, it suﬃces to compute the inner products | ⟨φ(xi)|φ(xj)⟩|2 between the
feature states. To estimate such an inner product using a quantum computer, we simply apply
U†(xj)U(xi) on input |0n⟩, and measure the probability of the 0n output (see Fig. 2). We call such
a procedure quantum kernel estimation (QKE). The overall procedure for learning with quantum
feature map is now clear. On input a set of m labeled training examples S, run QKE to obtain
the m × m kernel matrix, then run the dual SVM Eq. (5) on a classical computer to obtain the
solution α.
To classify a new example x, run QKE to obtain K(x, xi) for each i = 1, . . . , m,
then return
αiyiK(x, xi)
Throughout the entire SVM-QKE algorithm, QKE is the only subroutine that requires a quantum
computer, while all other optimization steps can be performed classically. See Appendix C for a
detailed description of the algorithm.
Despite the seemingly direct analogy between quantum and classical feature maps, one important aspect of QKE makes the analysis of our quantum algorithm fundamentally diﬀerent from
classical SVMs. Note that estimating the output probability of a quantum computer is inherently
noisy due to ﬁnite sampling statistics, even when the quantum computer is fully error corrected. In
QKE, this ﬁnite sampling error can be modeled as i.i.d. additive errors for each kernel entry, with
mean 0 and variance 1
R, where R is the number of measurement shots for each kernel estimation
circuit. Our main result rigorously establishes the performance guarantee of SVM-QKE, which
remains robust under this noise model.
Theorem 2. The concept class C is eﬃciently learnable by SVM-QKE. More speciﬁcally, for any
concept f ∈
C, the SVM-QKE algorithm returns a classiﬁer with test accuracy at least 0.99 in
polynomial time, with probability at least 2/3 over the choice of random training samples and over
noise in QKE estimation.
The main idea in our proof, c.f. Appendix D and E, is to connect the large margin property to
existing results on the generalization of soft margin classiﬁers . There, it is shown that
if the learning algorithm ﬁnds a hyperplane w that has a large margin on the training set, then
the linear classiﬁer y = sign (⟨φ(x), w⟩) has high accuracy with high probability. To see how we
can apply these results, recall that in the large margin property, we have established that there
exists a hyperplane w∗with a large margin on the training set. Therefore, as the SVM program
optimizes for the hyperplane with largest margin, it is guaranteed to ﬁnd a good hyperplane w,
although not necessarily the same as w∗. Applying the generalization bounds to this w gives us
the desired result.
As discussed above, the missing piece in the proof sketch is to show that the performance
of SVM-QKE remains robust with additive noise in the kernel. In the following we prove noise
robustness by introducing two additional results.
First, we show that the dual SVM program
(Eq. (5)) is robust, i.e., when the kernel used in (5) has a small additive perturbation, then the
solution returned by the program also has a small perturbation. This follows from strong convexity
of (5) and standard perturbation analysis of positive deﬁnite quadratic programs . This result
implies that the hyperplane w′ obtained by the noisy kernel is close to the noiseless solution w with
high probability. Second, we show that when w′ is close to w, the linear classiﬁer obtained by w′
has high accuracy. This seemingly simple statement is not trivial, as the sign function is sensitive
to noise. That is, if ⟨φ(x), w⟩is very close to 0, then a small perturbation in w could change its
sign. We provide a solution to this problem by proving a stronger generalization bound. We show
that if a hyperplane w has a large margin on the training set, then not only does ⟨φ(x), w⟩have
the correct sign, it is also bounded away from 0 with high probability. Therefore, when the noisy
solution w′ is close to w, ⟨φ(x), w′⟩also has the correct sign with high probability. Combining
these two results with the proof sketch, we have the full proof of Theorem 2.
Conclusions and outlook
We show that learning with quantum feature maps provides a way to harness the computational
power of quantum mechanics in machine learning problems. This idea leads to a simple quantum
machine learning algorithm that makes no additional assumptions on data access and has rigorous
and robust performance guarantees.
While the learning problem we have presented here that
demonstrates an exponential quantum speed-up is not practically motivated, our result sets a
positive theoretical foundation for the search of practical quantum advantage in machine learning.
An important future direction is to construct quantum feature maps that can be applied to practical
machine learning problems that are classically challenging. The results we have established here
can be useful for the theoretical analysis of such proposals.
An important advantage of the SVM-QKE algorithm, which only uses quantum computers to
estimate kernel entries, is that error-mitigation techniques can be applied when the feature
map circuit is suﬃciently shallow.
Our robustness analysis gives hope that an error-mitigated
quantum feature map can still maintain its computational power. Finding quantum feature maps
that are suﬃciently powerful and shallow is therefore the stepping stone towards obtaining a
quantum advantage in machine learning on near-term devices.
ACKNOWLEDGMENTS
We thank Sergey Bravyi and Robin Kothari for helpful comments and discussions. Y.L. was supported by Vannevar Bush faculty fellowship N00014-17-1-3025 and DOE QSA grant #FP00010905.
Part of this work was done when Y.L. was a research intern at IBM. S.A. and K.T. acknowledge
support from the MIT-IBM Watson AI Lab under the project Machine Learning in Hilbert Space,
the IBM Research Frontiers Institute and the ARO Grant W911NF-20-1-0014.
 J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and S. Lloyd, Nature 549, 195 .
 S. Arunachalam and R. de Wolf, SIGACT News 48, 41–67 .
 V. Dunjko and H. J. Briegel, Reports on Progress in Physics 81, 074001 .
 C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto, S. Severini, and L. Wossnig, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 20170551 .
 G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto, and L. Zdeborov´a,
Rev. Mod. Phys. 91, 045002 .
 A. W. Harrow, A. Hassidim, and S. Lloyd, Phys. Rev. Lett. 103, 150502 .
 N. Wiebe, D. Braun, and S. Lloyd, Phys. Rev. Lett. 109, 050505 .
 S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum algorithms for supervised and unsupervised machine
learning , arXiv:1307.0411 [quant-ph].
 S. Lloyd, M. Mohseni, and P. Rebentrost, Nature Physics 10, 631 .
 P. Rebentrost, M. Mohseni, and S. Lloyd, Phys. Rev. Lett. 113, 130503 .
 S. Lloyd, S. Garnerone, and P. Zanardi, Quantum algorithms for topological and geometric analysis of
big data , arXiv:1408.3106 [quant-ph].
 I. Cong and L. Duan, New Journal of Physics 18, 073011 .
 I. Kerenidis and A. Prakash, Quantum recommendation systems , arXiv:1603.08675 [quant-ph].
 F. G. S. L. Brand˜ao, A. Kalev, T. Li, C. Y.-Y. Lin, K. M. Svore, and X. Wu, in 46th International Colloquium on Automata, Languages, and Programming , Leibniz International Proceedings
in Informatics (LIPIcs), Vol. 132 pp. 27:1–27:14.
 P. Rebentrost, A. Steﬀens, I. Marvian, and S. Lloyd, Phys. Rev. A 97, 012327 .
 Z. Zhao, J. K. Fitzsimons, and J. F. Fitzsimons, Phys. Rev. A 99, 052331 .
 S. Aaronson, Nature Physics 11, 291 .
 E. Tang, in Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC
 p. 217–228.
 E. Tang, Quantum-inspired classical algorithms for principal component analysis and supervised clustering , arXiv:1811.00414 [cs.DS].
 A. Gily´en, S. Lloyd, and E. Tang, Quantum-inspired low-rank stochastic regression with logarithmic
dependence on the dimension , arXiv:1811.04909 [cs.DS].
 N.-H. Chia, H.-H. Lin, and C. Wang, Quantum-inspired sublinear classical algorithms for solving lowrank linear systems , arXiv:1811.04852 [cs.DS].
Quantum-inspired
 
 N.-H. Chia, A. Gily´en, T. Li, H.-H. Lin, E. Tang, and C. Wang, in Proceedings of the 52nd Annual
ACM SIGACT Symposium on Theory of Computing, STOC p. 387–400.
 K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii, Physical Review A 98, 032309 .
 E. Farhi and H. Neven, arXiv preprint arXiv:1802.06002 .
 E. Grant, M. Benedetti, S. Cao, A. Hallam, J. Lockhart, V. Stojevic, A. G. Green, and S. Severini, npj
Quantum Information 4, 1 .
 M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, Physical Review A 101, 032308 .
 M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, Quantum Science and Technology 4, 043001 .
 V. Havl´ıˇcek, A. D. C´orcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta,
Nature 567, 209 .
 M. Schuld and N. Killoran, Phys. Rev. Lett. 122, 040504 .
 B. E. Boser, I. M. Guyon, and V. N. Vapnik, in Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT p. 144–152.
 V. Vapnik, The nature of statistical learning theory .
 M. Anthony and P. L. Bartlett, Combinatorics, Probability and Computing 9, 213–225 .
 J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anthony, IEEE Transactions on Information
Theory 44, 1926 .
 P. Bartlett and J. Shawe-Taylor, Generalization performance of support vector machines and other
pattern classiﬁers, in Advances in Kernel Methods: Support Vector Learning p. 43–54.
 J. Shawe-Taylor and N. Cristianini, IEEE Transactions on Information Theory 48, 2721 .
 M. J. Kearns, The computational complexity of machine learning .
 R. A. Servedio and S. J. Gortler, SIAM J. Comput. 33, 1067–1092 .
 R. Sweke, J.-P. Seifert, D. Hangleiter, and J. Eisert, On the quantum versus classical learnability of
discrete distributions , arXiv:2007.14451 [quant-ph].
 X. Gao, Z.-Y. Zhang, and L.-M. Duan, Science Advances 4, 10.1126/sciadv.aat9004 .
 M. J. Kearns and U. V. Vazirani, An introduction to computational learning theory .
 P. W. Shor, SIAM Journal on Computing 26, 1484 .
 M. Blum and S. Micali, SIAM J. Comput. 13, 850–864 .
 D. Aharonov and A. Ta-Shma, SIAM Journal on Computing 37, 47 .
 P. L. Bartlett and P. M. Long, Journal of Computer and System Sciences 56, 174 .
 J. W. Daniel, Mathematical Programming 5, 41 .
 K. Temme, S. Bravyi, and J. M. Gambetta, Phys. Rev. Lett. 119, 180509 .
 Y. Li and S. C. Benjamin, Phys. Rev. X 7, 021050 .
 A. Kandala, K. Temme, A. D. C´orcoles, A. Mezzacapo, J. M. Chow, and J. M. Gambetta, Nature 567,
491 .
 M. Mosca and C. Zalka, International Journal of Quantum Information 02, 91 .
 C. J. Burges, Data Mining and Knowledge Discovery 2, 121 .
 A. J. Smola and B. Sch¨olkopf, Statistics and Computing 14, 199 .
 C. Gidney and M. Eker˚a, How to factor 2048 bit rsa integers in 8 hours using 20 million noisy qubits
 , arXiv:1905.09749 [quant-ph].
 Note that here the halfspace learning problem is deﬁned in feature space; our original concept class
is not a halfspace learning problem. Also, in our case the noise is deﬁned in terms of additive error
in the kernel, which is diﬀerent from the well-studied question of learning halfspaces with noise in
computational learning theory.
 A. Grønlund, L. Kamma, and K. G. Larsen, Proceedings of International Conference on Machine
Learning, ICML .
Appendix A: Supervised learning and the discrete log problem
We work in the same setting as in standard computational learning theory . The data
space X ⊆Rd is a ﬁxed subset of d-dimensional Euclidean space. In this paper we will be concerned
with distribution-dependent learning, i.e., we ﬁx our data distribution to be the uniform distribution
over X. A concept class C is a set of functions that maps data vectors to binary labels, i.e., every
f ∈C is a function f : X →{−1, 1}.
A learning algorithm is given a set of training samples S = {(xi, f(xi))}m
i=1, where each xi is
independently drawn from the uniform distribution over X, and f ∈C is an unknown concept
in the concept class. The goal of the learning algorithm is to return a classiﬁer f∗that runs in
polynomial time. The test accuracy of the learned classiﬁer is deﬁned as the probability of agreeing
with the unknown concept,
accf(f∗) = Pr
x∼X [f∗(x) = f(x)] .
Deﬁnition 3 (Eﬃcient learning of C). Let X ⊆Rd. A concept class C ⊆{f : X →{−1, 1}} is
eﬃciently learnable, if there exists a learning algorithm A that satisﬁes the following: for every
f ∈C, algorithm A takes as input poly(d) many training samples S and with probability at least 2/3
(over the choice of random training samples and randomness of the algorithm), outputs a classiﬁer
in time poly(d) that achieves 99% test accuracy.
The concept class we construct for showing our quantum advantage is based on the discrete log
problem (DLP) which we deﬁne ﬁrst:
DLP: given a prime p, a primitive element g of Z∗
p = {1, 2, . . . , p −1}, and y ∈Z∗
p such that gx ≡y (mod p).
For a ﬁxed p, g, we let DLP(p, g) be the discrete log problem with inputs y ∈Z∗
p. The input to the
DLP problem can be described by n = ⌈log2 p⌉bits. It is shown that DLP is reducible to the
following decision problem DLP 1
• Input: prime p, generator g of Z∗
• Output: 1 if logg y ≤p−1
and 0 otherwise.
Lemma 4 ([43, Theorem 3]). For every prime p and generator g, if there exists a polynomial time
algorithm that correctly decides DLP 1
2 (p, g) for at least 1
poly(n) fraction of the inputs y ∈Z∗
then there exists a polynomial time algorithm for DLP(p, g).
Furthermore, showed that DLP can be further reduced to the following promise discrete
logarithm problem DLPc for
poly(n) ≤c ≤1
• Input: prime p, generator g of Z∗
• Promise: logg y ∈[1, c(p −1)] or [p−1
+ c(p −1)]
• Output: −1 if logg y ∈[1, c(p −1)] and +1 if logg y ∈[p−1
+ c(p −1)].
Lemma 5 ( ). For every prime p, generator g and
poly(n) ≤c ≤1
2, if there exists a polynomial
time algorithm for DLPc(p, g), then there exists a polynomial time algorithm for DLP(p, g).
The proof of this fact is implicitly implied by the proof of Lemma 3 and Theorem 3 in .
Appendix B: A concept class reducible to discrete log
In this section we construct a concept class, wherein learning the concept class is as hard as
solving DLP 1
2 (p, g). Therefore, assuming the hardness of DLP(p, g), no classical polynomial time
algorithm can learn this concept class.
On the other hand, the learning problem is a simple
clustering problem in 1D after taking the discrete logarithm, and therefore is easy to learn using a
quantum computer.
We work in the setting introduced in the previous section, where we use standard deﬁnitions (see
Deﬁnition 3) from computational learning theory, and we assume a ﬁxed p, g such that computing
discrete log in Z∗
p is classically hard. Our concept class is deﬁned as follows.
Deﬁnition 6 (Concept class). We deﬁne a concept class over the data space X = Z∗
p ⊆{0, 1}n,
where n = ⌈log2 p⌉. For any s ∈Z∗
p, deﬁne a concept fs : Z∗
p →{−1, 1} as
if logg x ∈[s, s + p−3
Note that in interval [s, s + p−3
2 ], s + i denotes addition within Z∗
p. By deﬁnition, fs maps half the
elements in Z∗
p to +1 and half of them to −1. The concept class is deﬁned as C = {fs}s∈Z∗p.
A target concept in C can be speciﬁed by choosing an element s ∈Z∗
p, which can be understood
as the “secret key” for the concept fs.
We can also eﬃciently generate training samples for
every concept.
Lemma 7. For every concept f ∈C, there exists an eﬃcient classical algorithm that can generate
samples (x, f(x)), where x is uniformly random in Z∗
Proof. To generate a sample (x, fs(x)) for a concept fs, ﬁrst generate a random y ∼Z∗
p, and let
if y ∈[s, s + p−3
Then return (gy, b). Since gy is also uniformly distributed in Z∗
p, this procedure correctly generates
a sample from the data distribution.
Using the quantum algorithm for discrete logarithm problem , the concept class C is
polynomially learnable in BQP (in fact with probability 1). On the other hand, the result of Blum
and Micali implies that no eﬃcient classical algorithm can do better than random guessing.
Theorem 8. The concept class C is eﬃciently learnable in BQP. On the other hand, suppose there
exists an eﬃcient classical algorithm that, for every concept f ∈C, can achieve 1
poly(n) test
accuracy, with probability at least 2/3 over the choice of random training samples and randomness
of the algorithm. Then there exists an eﬃcient classical algorithm for DLP.
Remark 1. In the following we prove a stronger statement for classical hardness. We show that
assuming the classical hardness of DLP, no eﬃcient classical algorithm can achieve 1
poly(n) test
accuracy with probability 2
3 for any concept in the concept class C.
Proof. We ﬁrst show quantum learnability. For every concept fs ∈C, use a quantum computer to
take the discrete logarithm of the classical training data samples. Then, after taking the discrete
logarithm, the training data samples are clustered in two intervals: [s, s + p−3
2 ] with label +1, and
2 , s + p −2] with label −1, for the unknown s ∈Z∗
p (which deﬁnes the unknown concept fs).
For a new data sample x, use a quantum computer to take its discrete log. Then, compute the
average distance d+/d−between logg x and the +1/−1 labeled clusters, respectively. Assign label
to x based on which cluster is closer to logg x. This algorithm can achieve 99% accuracy for any
concept fs, with high probability over random training samples. We omit the detailed proof here.
To show classical hardness as stated in Remark 1, consider an arbitrary concept fs and polynomially many training samples. By Lemma 7 this can be generated classically in polynomial time.
By assumption, an eﬃcient classical algorithm A can learn this concept with 1
poly(n) accuracy
(call A a good classiﬁer if it satisﬁes this), with probability at least 2/3. We use this learned
classiﬁer to solve DLP 1
Algorithm A′ for DLP 1
1. On input y ∈Z∗
p such that logg y ∈[1, p−1
2 ] or [p−1
+ 1, p −1].
2. Send y·gs−1 to the classiﬁer A, decide logg y ∈[1, p−1
2 ] if the classiﬁer returns +1, and decide
logg y ∈[p−1
+ 1, p −1] if the classiﬁer returns −1.
To see that this procedure correctly decides DLP 1
2 (p, g) with a non-trivial bias, for a good classi-
ﬁer A we have
A′ correctly decides DLP 1
2 (p, g) on input y
A correctly classiﬁes y · gs−1 for concept fs
[A correctly classiﬁes y for concept fs]
By Lemma 4, once we have an algorithm that can correctly decide DLP 1
2 (p, g) on 1
poly(n) fraction
of the inputs, it can be used to solve DLP(p, g) with high success probability. Finally by a simple
union bound, we have a polynomial time algorithm that with high probability solves DLP(p, g).
An advantage of our supervised learning task is that it is eﬃciently veriﬁable by a classical
veriﬁer. Consider the following challenge:
1. A classical veriﬁer picks a random concept fs ∼C (it can do so by choosing a uniformly
random s ∼Z∗
p). Then, generate polynomial-sized samples (S, T) where the data labels in T
are removed.
2. The veriﬁer sends (S, T) to a prover, and the prover returns a set of {−1, 1} labels for T.
3. The veriﬁer accepts if more than 99% of the labels returned by the prover are correct.
Say a prover passes the challenge if the veriﬁer accepts with probability at least 2/3.
hardness result implies the following Corollary:
Corollary 9. There exists a BQP prover that can pass the above challenge. Assuming the classical
hardness of DLP, no polynomial-time classical prover can pass the above challenge.
Appendix C: Support vector machine and the quantum kernel estimation algorithm
Support vector machines
We give a brief overview of support vector machine and the quantum kernel estimation algorithm . Along the way, we also establish properties that are useful for the analysis of our
algorithm in the next section. We refer to Ref. for a more detailed introduction to support
vector machines.
A support vector machine (SVM) is a classiﬁcation algorithm that takes as input a set of training
samples S = {(x1, y1), . . . , (xm, ym)} where xi ∈Rd, y ∈{−1, 1} and in time poly(d, m) (assume
that the training set has polynomial size, m = poly(d)) returns a set of parameters (w, b) ∈Rd × R
which deﬁne a linear classiﬁer f∗: X →{−1, 1} as follows
ypred = f∗(x) = sign (⟨w, x⟩+ b) ,
where ⟨w, x⟩= P
i xiwi. For a data vector x with true label y, it is easy to see that the classiﬁer is
correct on this point if and only if y (⟨w, x⟩+ b) > 0. We say a training set S is linearly separable
if there exists (w, b) such that
yi (⟨w, xi⟩+ b) > 0,
for every (xi, yi) ∈S,
and such a (w, b) is called a separating hyperplane for S in Rd. When the training set is linearly
separable, the SVM algorithm can eﬃciently ﬁnd a separating hyperplane by running the so-called
hard margin primal program
yi (⟨xi, w⟩+ b) ≥1,
a convex quadratic program whose optimal solution can be obtained in polynomial time. One
important property of SVM is that it is a maximum margin classiﬁer. For a general unnormalized
hyperplane (w, b), deﬁne its normalized margin on a training data (x, y) as
ˆγ(w,b)(x, y) =
y (⟨w, x⟩+ b)
and let γ(w,b)(x, y) = y (⟨w, x⟩+ b) denote the unnormalized margin. It is easy to see that Eq. (C3)
returns a classiﬁer that maximizes
(x,y)∈S ˆγ(w,b)(x, y),
which is the minimum distance from any training point to the hyperplane. The general intuition
that SVM maximizes the margin is useful for understanding the generalization bounds that we will
prove in the next section.
However, for most “practical” purposes, assuming S is linearly separable is a strong assumption.
Additionally, when the training set S is not linearly separable, Eq. (C3) does not have a feasible
solution. To ﬁnd a good linear classiﬁer with the presence of outliers, we introduce the soft margin
primal program
yi (⟨xi, w⟩+ b) ≥1 −ξi
where ξi are the slack variables introduced to relax the margin constraints, with an additional
penalty term λ
i . For any positive integer p, Eq. (C6) is a convex program and is feasible. In
this work, we focus on choosing p = 2, which becomes a quadratic program. In practice it is also
common to use p = 1.
For the p = 2 case, we further derive the Wolfe dual program of (C6) based on Lagrangian
duality, resulting in the L2 soft margin dual program
αiαjyiyj⟨xi, xj⟩−1
Here the primal Lagrangian is given by
αi (yi (⟨xi, w⟩+ b) −1 + ξi) −
and the primal and dual optimal solutions can be connected via the Karush-Kuhn-Tucker (KKT)
conditions
λξi −αi −µi = 0
αi (yi (⟨xi, w⟩+ b) −1 + ξi) = 0
yi (⟨xi, w⟩+ b) −1 + ξi ≥0.
An immediate corollary of Eq. (C9) is
at the optimal solution. This means that when λ is a constant, the Lagrangian multipliers αi is
proportional to the slack variables ξi. This is a useful property for our analysis later. In addition,
the bias parameter b can be determined by the equality yi (⟨xi, w⟩+ b) −1 + ξi = 0 for any αi ̸= 0.
Finally, it will be convenient for us to work with optimizations without the bias parameter
b ∈R. We show that we can assume this is without loss of generality, as we can add one extra
dimension ˜x = (x, 1)/
2 to the data vectors so that the bias parameter is absorbed into w. In this
case, the L2 soft margin dual program becomes
αiαjyiyj (⟨xi, xj⟩+ 1) −1
Here we used the fact that ⟨˜xi, ˜xj⟩=
2 (⟨xi, xj⟩+ 1), and notice that the equality constraint
i αiyi = 0 is removed. This is because of the new KKT conditions
λξi −αi −µi = 0
αi (yi (⟨xi, w⟩) −1 + ξi) = 0
yi (⟨xi, w⟩) −1 + ξi ≥0,
where αi = λξi still hold at optimality.
Non-linear classiﬁcation
In this section we generalize support vector machines for non-linear classiﬁcation, i.e., we map
the d-dimensional data vectors into a n-dimensional feature space (n ≫d) via a feature map:
φ : X →Rn,
where we assume that φ is normalized, i.e., it maps a unit vector to a unit vector. The feature map
is chosen prior to seeing the training data. Notice that in the dual program, training data is only
accessed via the kernel matrix K ∈Rm×m (recall that m denotes the number of training samples)
K(xi, xj) = ⟨φ(xi), φ(xj)⟩.
Therefore, it’s possible to work with an exponentially large (or even inﬁnite-dimensional) feature
space (i.e., when n is large), as long as the kernel is computable in poly(d) time.
In addition, for any feature map φ0 : X →Rn, we can always use a new feature map φ : X →
Rn+1 such that φ(x) = (φ0(x), 1)/
2, which allows us to remove the bias parameter b. This can
be done via changing the kernel as K(xi, xj) = 1
2 (K0(xi, xj) + 1) as shown in the previous section.
Therefore, with a suitable kernel transformation, we can run the following dual program without
loss of generality:
αiαjyiyjK(xi, xj) −1
Let Q ∈Rm×m be a matrix such that Qij = yiyjK(xi, xj). Then we can write the dual program
in vectorized form
It is easy to see that for every λ > 0, Eq. (C16) is a strongly convex quadratic program and has
a unique optimal solution. After training is ﬁnished (i.e., we obtain training samples, compute
the kernel K and solve Eq. (C16)), when a learner is presented with a new test example x, the
classiﬁer returns
ypred = sign (⟨w, φ(x)⟩) = sign
αiyiK(xi, x)
where we used the KKT condition w = P
i αiyiφ(xi) (which can be derived by simply replacing
xi with φ(xi) in Eq. (C12)). So a classiﬁer needs to evaluate the kernel function on the new test
example and output ypred.
Quantum kernel estimation
Diﬀerent from the above standard approaches, the kernel used in our quantum algorithm is
constructed by a quantum feature map. The main idea in the quantum kernel estimation algorithm is to map classical data vectors into quantum states:
x →|φ(x)⟩⟨φ(x)| ,
where we use the density matrix representation to avoid global phase. Then, the kernel function
is the Hilbert-Schmidt inner product between density matrices,
K(xi, xj) = Tr
|φ(xi)⟩⟨φ(xi)| · |φ(xj)⟩⟨φ(xj)|
= |⟨φ(xi)|φ(xj)⟩|2 .
This quantum feature map is implemented via a quantum circuit parameterized by x,
|φ(x)⟩= U(x) |0n⟩,
where we assume the feature map uses n qubits. Therefore, to obtain the kernel function
K(xi, xj) =
⟨0n| U†(xi)U(xj) |0n⟩
we can run the quantum circuit U†(xi)U(xj) on input |0n⟩, and measure the probability of
the 0n output.
Algorithm 1 Support vector machine with quantum kernel estimation (SVM-QKE training)
Input: a training set S = {(xi, yi)}m
Output: α1, . . . , αm (solution to the L2 soft margin dual program (C16))
1: for i = 1 . . . m do
K0(xi, xi) := 1
3: end for
4: for i = 1 . . . m do
▷quantum kernel estimation
for j = i + 1 . . . m do
Apply U †(xi)U(xj) on input |0n⟩
Measure the output probability of 0n with R shots, denoted as p
K0(xi, xj) = K0(xj, xi) := p
10: end for
11: K := 1
2 (K0 + 1m×m)
▷SVM training
12: Run the dual program (C16), record solution as α
13: Return α
Algorithm 2 Support vector machine with quantum kernel estimation (SVM-QKE testing)
Input: a new example x ∈X, a training set S = {(xi, yi)}m
i=1, training parameters α1, . . . , αm
Output: y ∈{−1, 1}
2: for i = 1 . . . m do
▷quantum kernel estimation
Apply U †(x)U(xi) on input |0n⟩
Measure the output probability of 0n with R shots, denoted as p
t := t + αiyi · p+1
6: end for
7: Return sign(t)
We describe the full support vector machine algorithm with quantum kernel estimation (SVM-
QKE), with training (Algorithm 1) and testing (Algorithm 2) phases. Here, 1 denotes the all-one
matrix, and R denotes the number of measurement shots for each kernel estimation circuit.
The main diﬀerences between QKE and classical kernels are two-fold:
• On the one hand, quantum feature maps are more expressive than classical feature maps.
Therefore, a SVM trained with a quantum kernel may achieve better performance than with
classical kernels.
• On the other hand, a fundamental feature in QKE is that we only have a noisy estimate
of the quantum kernel entries in both training and testing, due to ﬁnite sampling error in
experiment. More speciﬁcally, for each K0(xi, xj) as deﬁned in Algorithm 1, we have access
to a noisy estimator p with mean equals K0(xi, xj) and variance 1
Therefore, noise robustness is an important property for provable quantum advantage with QKE.
We will formally prove this in the next section.
Appendix D: Eﬃcient learnability with quantum kernel estimation
Quantum feature map
We now deﬁne our quantum feature map for learning the concept class C based on the discrete
logarithm problem (recall Deﬁnition 6). This family of states, whose construction is based on the
discrete logarithm problem, was ﬁrst introduced in Ref. to study the complexity of quantum
state generation and statistical zero knowledge.
For a prime p, let n = ⌈log2 p⌉be the number of bits needed to represent {0, 1, . . . , p −1}. For
p, k ∈{1, 2, . . . , n −1}, deﬁne a polynomial-sized classical circuit family {Cy,k} as follows:
Cy,k : {0, 1}k →{0, 1}n,
Cy,k(i) = y · gi
(mod p), ∀i ∈{0, 1}k.
It’s easy to see that Cy,k is injective, i.e., for all i ̸= j, we have Cy,k(i) ̸= Cy,k(j). Furthermore,
Cy,k(i) can be computed using O(n) multiplications within Z∗
p. We now show how to prepare a
uniform superposition over the elements of Cy,k on a quantum computer, which we refer to as the
n-qubit feature state
First construct the reversible extension of Cy,k as
˜Cy,k : {0, 1}2n →{0, 1}2n,
˜Cy,k |i⟩|0n⟩= |i⟩|Cy,k(i)⟩,
where |i⟩uses the least signiﬁcant bits of the n-bit register. Then, construct a quantum circuit Uy
using the quantum algorithm for discrete log which uses ˜O(n3) gates (we use ˜O(·) to
hide polylog factors),
Uy |Cy,k(i)⟩|0⟩= |i⟩|Cy,k(i)⟩.
The overall procedure for preparing |Cy,k⟩is as follows, up to adding/discarding auxiliary qubits:
|i⟩|Cy,k(i)⟩
|Cy,k(i)⟩.
Deﬁnition 10. Deﬁne the family of feature states via the map (y, k) →|Cy,k⟩that takes classical
data y ∈Z∗
p, k ∈{1, 2, . . . , n −1} and maps it to a n-qubit feature state
Such a procedure can be implemented in BQP using ˜O(n3) gates.
In Ref. , it was proven that constructing these feature states is as hard as solving discrete
log, where they show that DLP1/6 can be reduced to estimating the inner product between |Cy,k⟩
with diﬀerent y and k.
In this work, our quantum kernel is constructed via the feature map
with a ﬁxed k, which is chosen prior to running the quantum kernel estimation algorithm. More
speciﬁcally, for diﬀerent training samples y, y′ ∈Z∗
p, the corresponding kernel entry is given by
K0(y, y′) =
We note that these feature states have a special structure: after taking the discrete log for
each basis state, the feature state becomes the superposition of an interval. Therefore, computing
the inner product between feature states is equivalent to computing the intersection between their
corresponding intervals. We provide more details in the following deﬁnition.
Deﬁnition 11 (Interval states). For a ﬁxed g, p, suppose y = gx. The feature state can be written
as |Cy,k⟩=
. This can be understood as “interval states” in the log space, where
the exponent spans an interval [x, . . . , x + 2k −1] of length 2k. As a consequence, since y = gx is a
one-to-one mapping, computing the inner products between feature states is equivalent to computing
intersection of the corresponding intervals.
By deﬁnition, our kernel K0 is constructed using interval states with a ﬁxed length. In order for
our quantum algorithm to solve a classically hard problem, a necessary condition is that the kernel
entries K0(y, y′) cannot be eﬃciently estimated up to additive error.
Otherwise, the quantum
kernel estimation procedure can be eﬃciently simulated classically. Next we show that estimating
the kernel entries is as hard as solving the discrete log problem. Although this is implied by our
main results (Theorem 8 and 22), here we give a direct proof which is a generalization of Ref. .
Lemma 12. For an arbitrary (ﬁxed) prime p and generator g, if there exists a polynomial time
algorithm such that, on input y, y′ ∈Z∗
p, computes K0(y, y′) up to 0.01 additive error, then there
exists a polynomial time algorithm for DLP(p, g).
Proof. We show this lemma by using an algorithm that estimates the kernel entries well to solve
16 (p, g) which in turn (by Lemma 5) implies an eﬃcient algorithm for DLP(p, g).
following we assume k = n −3, but the proof can be generalized to any k = n −t log n for some
constant t.
Consider an input y = gx for the problem DLP 1
16 (p, g), where we are promised that either
x ∈[1, p−1
16 ] or x ∈[p−1
Let y′ = g(p+1)/2 and consider feature states |Cy⟩
. Then for the two cases,
1. If x ∈[1, p−1
16 ], |Cy⟩corresponds to a subinterval of [1, p−1
2 ] and therefore K0(y, y′) = 0.
2. If x ∈[p−1
16 ], the intersection of the corresponding intervals is at least
K0(y, y′) ≥1
Therefore, an algorithm that can approximate K0(y, y′) within 0.01 additive error can decide the
promise problem DLP 1
16 (p, g). Lemma 5 now shows the lemma statement.
Mapping to high dimensional Euclidean space
Now we are ready to apply the feature map in our support vector machine algorithm using
quantum kernel estimation.
We recall the deﬁnition of C (Deﬁnition 6) here for convenience:
C = {fs}s∈Z∗p, where
if logg x ∈[s, s + p−3
Consider the mapping from x ∈Z∗
p to the quantum feature states described in the previous section
(renamed here as |φ(x)⟩),
x →|φ(x)⟩=
where k = n −t log n for some constant t to be speciﬁed later (recall that n = ⌈log2 p⌉). It was
shown in Deﬁnition 10 that |φ(x)⟩can be prepared in BQP. Let ∆= 2k+1
= O(n−t). Then the
feature states span a ∆
2 = O(n−t) fraction of the elements in Z∗
Also deﬁne the halfspace state |φs⟩corresponding to every concept cs ∈C as follows
for every s ∈Z∗
Observe that the halfspace state spans a 1
2-fraction of the full space Z∗
p. The following property
shows that |φs⟩is a separating hyperplane in Hilbert space.
• | ⟨φs|φ(x)⟩|2 = ∆, for 1 −∆fraction of x in {x : fs(x) = +1}.
• | ⟨φs|φ(x)⟩|2 = 0, for 1 −∆fraction of x in {x : fs(x) = −1}.
Notice that |φs⟩has a large margin property: it separates training samples with label +1 from
those with label −1. The probability of having an outlier (a data point that lies inside the margin
or on the wrong side of the hyperplane) is small, which equals ∆= 1/poly(n). Recall that the goal
of an SVM algorithm is to ﬁnd a hyperplane that maximizes the margin on the training set, and
the above property shows that such a good hyperplane exists.
In general, learning a separating hyperplane that separates +1/−1 examples is called a halfspace
learning problem. Rigorously speaking, our data vectors are represented by quantum states with
the Hilbert-Schmidt inner product.
For simplicity, we now show that our learning problem is
equivalent to learning a halfspace in 4n-dimensional Euclidean space. For that, we ﬁrst express a
Hermitian matrix W ∈C2n×2n uniquely in terms of the orthonormal Pauli basis as follows
p∈{0,1,2,3}n
where σp ∈{I, X, Y, Z}⊗n are n-qubit Pauli operators and wp =
2n Tr[σpW] ∈R are the Fourier
coeﬃcients. We can use the 4n dimensional Pauli vector w = (wp) to represent W, as the Hilbert-
Schmidt inner product ⟨W, W ′⟩= ⟨w, w′⟩is equivalent to the Euclidean inner product. Also note
that a pure quantum state in Hilbert space corresponds to a unit Pauli vector in Euclidean space.
The large margin property can be recast in Euclidean space with the Pauli basis representation.
Lemma 13. For any concept fs ∈C, let ws be the Pauli vector of |φs⟩⟨φs|, b = −∆
2 , and ˆx be the
Pauli vector of |φ(x)⟩⟨φ(x)|. Then
• ⟨ws, ˆx⟩+ b = ∆
2 , for 1 −∆fraction of x in {x : fs(x) = +1},
• ⟨ws, ˆx⟩+ b = −∆
2 , for 1 −∆fraction of x in {x : fs(x) = −1},
where ⟨·, ·⟩denotes Euclidean inner product.
Our SVM algorithm that uses the kernel K0(x, x′) = |⟨φ(x)|φ(x′)⟩|2 can be equivalently understood as using the kernel K0(x, x′) = ⟨ˆx, ˆx′⟩based on a feature map that maps x ∈Z∗
p to ˆx, a 4n
dimensional vector in Euclidean space. Finally, recall that we only have access to a noisy estimate
of K0(x, x′) using quantum kernel estimation. The noisy estimator that we obtain from a quantum
computer has mean K0(x, x′) and variance 1
R, where R denotes the number of measurement shots
for each kernel estimation circuit.
To summarize, here we have shown that the original problem of learning the concept class C can
be mapped to a noisy halfspace learning problem in high dimensional Euclidean space with the
following properties. For simplicity, below we do not specify the concept, since everything holds
equivalently for each concept in C. From now on our analysis is restricted to the high dimensional
Euclidean space, and for notation simplicity we overload x to represent the Pauli vector ˆx.
Properties of noisy halfspace learning.
1. Data space: X ⊆R4n with unit length ∥x∥2 = 1 for every x ∈X. Each x is associated with
a label y ∈{−1, 1}.
2. Separability: the data points lie outside a margin of ∆
2 with high probability over the uniform
distribution on X. That is, there exists a hyperplane (w, b) where w ∈R4n, ∥w∥2 = 1, and
b ∈R, such that
y(⟨w, x⟩+ b) ≥∆
3. Bounded distance: all data points are close to the above hyperplane:
|y(⟨w, x⟩+ b)| ≤∆
for every x ∈X.
4. Noisy kernel: instead of having the ideal kernel K0(xi, xj) = ⟨xi, xj⟩, we have access to a
noisy kernel K′
0, where K′
0(xi, xj) = K0(xi, xj) + eij. Here, eij are independent random
variables satisfying
• eij ∈[−1, 1]
• E[eij] = 0
• Var[eij] ≤1
R, where R denotes the number of measurement shots.
These properties are simple corollaries of the deﬁnition of |φ(x)⟩, |φs⟩and Lemma 13.
In order to further simplify our analysis, we perform an additional transform which allows us
to remove the bias parameter b without loss of generality. This will help us simplify our notations
in later proofs. More speciﬁcally, we replace x with (x, 1)/
2 and w with (w, b)/
w2 + b2. This
corresponds to replacing the original kernel K0 with a new kernel K = 1
2 (K0 + 1m×m) where 1
denotes the all-one matrix. These steps are explained in more detail in Section C. The ﬁnal form
of our halfspace learning problem is given below.
Lemma 14. We have mapped the original problem of learning the concept class C into the following
noisy halfspace learning problem. Below we do not specify the concept, as these properties hold for
every concept in C.
1. Data space: X ⊆R4n+1 with unit length ∥x∥2 = 1, ∀x ∈X. Each x is associated with a label
y ∈{−1, 1}.
2. Separability: the data points lie outside a O(∆) margin with high probability over the uniform
distribution. That is, there exists a hyperplane w where w ∈R4n+1, ∥w∥2 = 1, such that
3. Bounded distance: all data points are close to the above hyperplane:
|y⟨w, x⟩| ≤
for every x ∈X.
4. Noisy kernel: let Kij =
2 (1 + K0(xi, xj)). We have access to a noisy kernel K′, where
ij = Kij + eij. Here, eij are independent random variables satisfying
• eij ∈[−1/2, 1/2]
• E[eij] = 0
• Var[eij] ≤1
R, where R denotes the number of measurement shots.
The hyperplane speciﬁed in the above lemma is particularly useful for our analysis later. We
deﬁne its unnormalized version as the “ground truth hyperplane” as follows.
Deﬁnition 15. Consider the halfspace learning problem deﬁned in Lemma 14. Deﬁne w∗∈R4n+1
as the (unnormalized) ground truth hyperplane as given in Lemma 14, that satisﬁes the following properties:
x∼X [y⟨w∗, x⟩≥1] = 1 −∆,
|y⟨w∗, x⟩| ≤1,
for every x ∈X.
Note that the norm of w∗is ∥w∗∥2 = O(∆−1).
Generalization of the noisy classiﬁer
Next, we focus on the noisy halfspace learning problem as given by Lemma 14. We show that the
four properties established in Lemma 14 are suﬃcient for formally proving the eﬃcient learnability
of the concept class C using our quantum algorithm.
Consider the primal optimization problem in the support vector machine used by Algorithm 1:
yi⟨xi, w⟩≥1 −ξi
with the dual form
αiαjyiyjKij −1
The above duality follows from the KKT conditions w = P
i αiyixi and αi = λξi. The kernel
matrix K is a positive semideﬁnite matrix. Let Q be a matrix such that Qij = yiyjKij, which
is also positive semideﬁnite. Then the dual program (D18) is equivalent to the following convex
quadratic program:
Recall that we have small additive perturbations in K, which in turn gives additive perturbations
in Q. One useful property of the dual program (D19) is that it is robust to perturbations in Q.
More speciﬁcally, we use the following lemma from standard perturbation analysis.
Lemma 16 ([46, Theorem 2.1]). Let x0 be the solution to the quadratic program
2xT Kx −cT x
where K is positive deﬁnite with smallest eigenvalue λ > 0. Let K′ be a positive deﬁnite matrix
such that ∥K′ −K∥F ≤ε < λ. Let x′
0 be the solution to (D20) with K replaced by K′. Then
λ −ε∥x0∥2.
Before going into the analysis of robustness against noise, notice that the bound in Lemma 16
is multiplicative. Therefore, it is useful to establish an upper bound on ∥α∥2, the norm of the
solution to the noiseless quadratic program (D18). Recall from the KKT conditions that αi = λξi,
where the dual variables are directly related to the slack variables in the primal program (D17).
The following lemma establishes a useful property for ξ∗
i for the ground truth hyperplane.
Lemma 17. For the ground truth hyperplane w∗as deﬁned in Deﬁnition 15, the corresponding
slack variables ξ∗
i in the primal program (D17) satisfy
where the expectation is taken over the training set.
Proof. We can write the slack variables as
i = max{1 −yi⟨xi, w∗⟩, 0}.
By the properties given in Deﬁnition 15, we have
i = 0] = 1 −∆,
Therefore,
Now we are ready to bound the norm of the dual variables αi. Intuitively, we can do so because
2 is part of the training loss in the primal program (D17). The loss of the solution returned by
the program can only be smaller than the loss of the ground truth hyperplane, which is guaranteed
to be small.
Lemma 18. Let α0 be the solution returned by the dual program (D18). We have
where the expectation is over the training set.
Proof. Let w0 = P
i α0iyixi be the hyperplane which corresponds to α0, and let ξ0 be the corresponding slack variable. Then
2 = λ2∥ξ0∥2
2 + λ∥ξ0∥2
2 + λ∥ξ∗∥2
Here, the ﬁrst line follows from the KKT condition αi = λξi, and the third line is because w0 is
the optimal solution to (D17). Therefore by Lemma 17, E
Remark 2. Recall that we have the freedom to choose ∆= O(n−t) for any constant t. Suppose
we have polynomially many training samples m ≈nc, and let t = c/3. Then the above bound gives
Having established the above lemmas, now we are ready to prove our key result for noise
robustness (Lemma 19). Let Q′ be the noisy kernel measured by a quantum computer, and let
λ ∈(0, 1) be a constant. Here we brieﬂy recall the steps in Algorithm 1 and 2. The classiﬁer
is constructed in two steps. First, use a classical computer to run the dual program (D19) with
Q replaced by the experimental estimate Q′, and let α′ be the solution returned by the program.
Second, given a new data sample x, use a quantum computer to obtain noisy estimates K′(x, xi)
for all i, and output
ypred = sign
iyiK′(x, xi)
Let h(x) = P
i αiyiK(x, xi) and h′(x) = P
iyiK′(x, xi), which corresponds to the value of the
noiseless/noisy classiﬁer before taking the sign. We will prove the following result which establishes
the noise robustness of h.
Lemma 19 (Noise robustness). Suppose we take R = O(m4) measurement shots for each quantum
kernel estimation circuit. Then, with probability at least 0.99 (over the choice of random training
samples and measurement noise), for every x ∈X we have
h(x) −h′(x)
Proof. Consider the (noisy) quadratic program (D18). The Frobenius norm is given by
where eij are independent random variables satisfying E[eij] = 0 and E
Now we invoke Lemma 16 and Lemma 18 (see Remark 2).
Markov’s inequality: with probability at least 0.999 (over the choice of training samples and measurement noise), we have that
Let δi = α′
i −αi. Since λmin
λ is lower bounded by a constant, Lemma 16 gives
Then, let νi = K′(x, xi)−K(x, xi) for i = 1, . . . , m. Similarly, νi are independent random variables
satisfying E[νi] = 0 and E
By Markov’s inequality, we have ∥ν∥2 ≤O
probability at least 0.999. Overall for any x ∈X, the error bound gives
h(x) −h′(x)
(αi + δi)yi(K(x, xi) + νi) −
αiyiK(x, xi)
|αiνi + δiK(x, xi) + δiνi|
≤∥α∥2 · ∥ν∥2 + √m∥δ∥2 + ∥δ∥2 · ∥ν∥2
where the third line uses Cauchy–Schwarz inequality. Therefore, R = O(m4) measurement shots
is suﬃcient for achieving |h(x) −h′(x)| ≤0.01, and by a simple union bound this holds with
probability at least 0.99.
Having established noise robustness, it remains to prove a generalization error bound for the
noisy classiﬁer: if the classiﬁer has small training error/loss, it should also have small test error, which is referred to as generalization error in learning theory. The main idea is a two-step argument:
1. The noiseless classiﬁer y = sign (h(x)) (we have deﬁned h(x) = P
i αiyiK(x, xi) = ⟨w, x⟩)
has small generalization error, which follows from standard generalization bounds for soft
margin classiﬁers.
2. We have established that the noisy classiﬁer is close to the noiseless classiﬁer. Therefore, the
noisy classiﬁer should also have small generalization error.
For the ﬁrst step, we refer to standard results on the generalization of soft margin classiﬁers (see,
for example ). Recall that a hyperplane w correctly classiﬁes a data point (x, y) if and
only if y⟨w, x⟩> 0. Therefore for a speciﬁc concept f ∈C, the test accuracy of f∗(x) = sign (⟨w, x⟩)
is given by
accf(f∗) = Pr
x∼X [f∗(x) = f(x)] = 1 −Pr
x∼X [y⟨w, x⟩< 0] ,
where we have used y = f(x).
Our results will be given in the form of an upper bound on
Prx∼X [y⟨w, x⟩< 0].
The following result gives a generalization bound that coincides with our
L2 training loss up to polylog factors, as indicated by the ˜O notation, and therefore is directly
applicable to the noiseless classiﬁer.
Lemma 20 ([36, Theorem VII.11]). For any hyperplane w satisfying the constraints of the primal
program (D17), with probability 1 −δ over randomly drawn training set S of size m, the generalization error is bounded by
x∼X[y⟨w, x⟩< 0] ≤1
However, although this result establishes step 1, it cannot be directly applied to step 2: our
noise robustness result, which states that h′(x) is close to h(x), does not guarantee that sign(h′(x))
agrees well with sign(h(x)). The above lemma implies that h(x) is on the correct side of the origin
with high probability, but it could still be very close to the origin, which may lead to a bad noisy
classiﬁer sign(h′(x)).
A simple solution to this problem is to show a stronger generalization bound, which in addition
to h(x) being correct, also shows that h(x) is bounded away from the origin. We indeed prove such
a result, by combining ideas from the aforementioned references. Notice that the only diﬀerence
between the following lemma and the previous lemma is that we replaced 0 with 0.1.
Lemma 21. For any hyperplane w satisfying the constraints of the primal program (D17), with
probability 1−δ over randomly drawn training set S of size m, the generalization error is bounded by
x∼X[y⟨w, x⟩< 0.1] ≤1
The proof is presented in Section E. Combining Lemma 19 with Lemma 21, we arrive at our
main theorem for the learnability of C with our quantum algorithm.
Theorem 22. For any concept fs ∈C, the SVM-QKE algorithm returns a classiﬁer with test
accuracy at least 0.99 in polynomial time, with probability at least 2/3 over the choice of random
training samples and over noise.
Proof. Below we do not specify the concept, as the proof works equivalently for every concept
fs ∈C. Let w∗be the ground truth hyperplane as in Deﬁnition 15. Note that ∥w∗∥2 = O(∆−1).
Using Lemma 17, we have that with probability at least 0.99 over the choice of training samples,
the L2 training loss of w∗satisﬁes
Loss(w∗) := 1
Let w0 be the optimal solution of the primal program (D17), and let h(x) = ⟨w0, x⟩. Let h′ be the
noisy classiﬁer obtained by the dual program (D18). By Lemma 19, for any x ∈X we have
|yh′(x) −yh(x)| ≤0.01,
with probability at least 0.99 over the choice of training samples and noise. Therefore, by a simple
union bound, with probability at least 2/3, the test error of the noisy classiﬁer is upper bounded by
x∼X[yh′(x) < 0] ≤Pr
x∼X[yh′(x) < 0.09]
x∼X[y⟨w0, x⟩< 0.1]
˜O(Loss(w0))
˜O(Loss(w∗)) ≤˜O
where in the third line we use Lemma 21, and the fourth line is because w0 is the optimal solution
to (D17). Finally, notice that the above bound holds for arbitrary ∆= O
for constant t. In
order to optimize this bound, we can choose t = c/3 for m = nc (also see Remark 2). This gives
the ﬁnal bound
x∼X[yh′(x) < 0] ≤˜O
Therefore, polynomially many training samples are suﬃcient for learning the concept class C with
high accuracy.
Appendix E: Generalization bound for soft margin SVM
In this section we prove Lemma 21, a generalization bound for the L2 soft margin SVM in
Eq. (D17) (restated below for convenience).
yi⟨xi, w⟩≥1 −ξi
Let w be an unnormalized feasible solution to Eq. (E1). The ﬁrst step is to use a trick developed
by , that converts the soft margin problem to a hard margin problem by mapping to a larger
space. Let S = {(x1, y1), . . . , (xm, ym)} be the training set. Consider the mapping
x 7→˜x = (x, δx)
yδx · ξ(x, y, w)
where δx : X →{0, 1} is a function deﬁned as δx(x′) = 1 if and only if x′ = x and ξ(x, y, w) =
max{0, 1−y⟨w, x⟩} are the slack variables used in Eq. (E1). Denote the enlarged space by LX and
∥· ∥its induced norm. The following useful properties hold for this transform:
1. If (x, y) ∈S, y⟨˜w, ˜x⟩≥1.
2. If (x, y) /∈S, ⟨˜w, ˜x⟩= ⟨w, x⟩.
3. ∥˜w∥2 = ∥w∥2
4. ∥˜x∥2 = 2.
In the following, we can assume that the training data does not appear when testing the classiﬁer,
which is the case with high probability. By property 2, to bound the generalization performance
of the hyperplane w, we only need to bound the generalization performance of ˜w in the enlarged
space, which corresponds to a hard margin problem.
For the generalization error of hard margin classiﬁers, it is well-known that the generalization
error bound is captured by the VC-dimension which characterizes the complexity of the classiﬁer
family. Intuitively, a hard margin classiﬁer corresponds to a “thick” hyperplane in the data space,
which reduces its complexity compared with margin-less hyperplanes. The relevant complexity
measure in our results is the so-called fat-shattering dimension which we deﬁne now.
Deﬁnition 23 ([36, Deﬁnition III.4]). Let F ⊆{f : X →R} be a set of real-valued functions. We
say that a set of points ˆX ⊆X is γ-shattered by F, if there are real numbers rx indexed by x ∈ˆX
such that for all binary vectors bx indexed by x ∈ˆX, there is a function fb ∈F satisfying
if bx = 1,
otherwise.
The γ-fat-shattering dimension of F, denoted fatF(γ), is the size of the largest set ˆX that is γshattered by F, if this is ﬁnite or inﬁnity otherwise.
Let H be a set of linear functions that map from LX to R, such that their norm equals to ∥˜w∥.
Since the data vectors ˜x have bounded norm, the fat-shattering dimension of H was shown to
be bounded by
fatH(γ) ≤O
Next we invoke the following lemma from Ref. (also see ) which uses fat-shattering dimension to understand generalization bounds for learning real-valued concept classes.
Lemma 24 ([33, Corollary 3.3]). Let C, H be sets of functions that map from a set X to . Then
for all η, γ, δ ∈(0, 1), for every f ∈C and for every probability measure D on X, with probability
at least 1 −δ (over the choice of S = {x1, . . . , xm} where xi ∼D), if h ∈H and |h(xi) −f(xi)| ≤η
for 1 ≤i ≤m, then
|h(x) −f(x)| ≥η + γ
To apply this lemma, consider the function h(˜x) =
∥˜w∥⟨˜w, ˜x⟩. Let γ0 =
∥˜w∥and γ = 0.9γ0. Let
y = f(˜x) ∈{−1, 1} be any labeling rule, and S = {(˜x1, y1), . . . , (˜xm, ym)} be a training set. By the
properties of the mapping, for all ˜x ∈S we have yh(˜x) ≥γ0, which means that
|h(˜x) −f(˜x)| = |yh(˜x) −1| ≤1 −γ0.
Applying Lemma 24, we have with probability at least 1 −δ,
|h(˜x) −f(˜x)| ≥1 −0.1γ0
∥˜w∥2 + log 1
Finally, note that yh(˜x) ≤0.1γ0 implies that |h(˜x) −f(˜x)| ≥1 −0.1γ0, therefore
Pr [yh(˜x) ≤0.1γ0] ≤1
∥˜w∥2 + log 1
This concludes the proof of Lemma 21, as
x∼X[y⟨w, x⟩< 0.1] = Pr
x∼X[y⟨˜w, ˜x⟩< 0.1]
x∼X[yh(˜x) < 0.1γ0]
∥˜w∥2 + log 1