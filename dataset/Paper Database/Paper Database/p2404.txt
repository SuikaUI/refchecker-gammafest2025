Early-Stage Malware Prediction Using Recurrent Neural Networks
Matilda Rhodea,∗, Pete Burnapa, Kevin Jonesb
aSchool of Computer Science and Informatics, CardiﬀUniversity
bAirbus Group
Static malware analysis is well-suited to endpoint anti-virus systems as it can be conducted
quickly by examining the features of an executable piece of code and matching it to previously
observed malicious code. However, static code analysis can be vulnerable to code obfuscation
techniques. Behavioural data collected during ﬁle execution is more diﬃcult to obfuscate,
but takes a relatively long time to capture - typically up to 5 minutes, meaning the malicious
payload has likely already been delivered by the time it is detected.
In this paper we investigate the possibility of predicting whether or not an executable
is malicious based on a short snapshot of behavioural data. We ﬁnd that an ensemble of
recurrent neural networks are able to predict whether an executable is malicious or benign
within the ﬁrst 5 seconds of execution with 94% accuracy. This is the ﬁrst time general types
of malicious ﬁle have been predicted to be malicious during execution rather than using a
complete activity log ﬁle post-execution, and enables cyber security endpoint protection to
be advanced to use behavioural data for blocking malicious payloads rather than detecting
them post-execution and having to repair the damage.
malware detection, intrusion detection, recurrent neural networks, machine
learning, deep learning
1. Introduction
Automatic malware detection is necessary to process the rapidly rising rate and volume
of new malware being generated. Virus Total, a free tool which can be used to evaluate
whether ﬁles are malicious, regularly approaches one million new, distinct ﬁles for analysis
each day1 .
Commonly, automatic malware detection used in anti-virus systems compares (features
extracted from) the code of an incoming ﬁle to a known list of malware signatures. However,
this form of ﬁltering using static data is unsuited to detecting completely new (“zero-day”)
∗Corresponding author
Email addresses: (Matilda Rhode), (Pete Burnap),
 (Kevin Jones)
10.935 million on 2nd December 2017
 
malware unless it shares code with previously known strains .
Obfuscating the code,
now common practice among malware authors, can even enable previously seen malware to
escape detection .
Malware detection research has evolved to respond to the inadequacies of static detection.
Behavioural analysis (dynamic analysis) examines a sample ﬁle in a virtual environment
whilst it is being executed. Behavioural analysis approaches assume that malware cannot
avoid leaving a measurable footprint as a result of the actions necessary for it to achieve
its aims. However, executing the malware incurs a time penalty by comparison with static
analysis. Whilst dynamic data can lead to more accurate and resilient detection models than
static data ( , , ), in practice behavioural data is rarely used in commercial endpoint
anti-virus systems due to this time penalty. It is inconvenient and ineﬃcient to wait for
several minutes whilst a single ﬁle is analysed, and ultimately, the malicious payload has
likely been delivered by the end of the analysis window so the opportunity to block malicious
actions has been missed.
To avoid waiting, some approaches monitor “live” activity on the local network or the
machine. These detection systems tend either to look for traits that signify a particular type
of malware (e.g. ransomware) or to ﬂag deviations from a baseline of “normal” behaviour.
These two approaches suﬀer from speciﬁc ﬂaws.
Searching for particular behaviours is
analogous to the traditional methods of comparing incoming ﬁles with known variants, and
may miss detecting new types of malware. Whilst anomaly detection is prone to a high falsepositive rate as any activity that deviates from a “normal” baseline is deemed malicious.
In practice anomalous activity is often investigated by human analysts, making the model
vulnerable to exploitation. An attacker could bring about lots of anomalous behaviour such
that the human analysts are ﬂooded with investigation requests, reducing the chances of the
activity created by the attack itself from being detected.
We propose a behaviour-based model to predict whether or not a ﬁle is malicious using the
ﬁrst few seconds of ﬁle execution with a view to developing a tool that could be incorporated
into an end-point solution. Though general malicious and benign ﬁles comprise a wide range
of software and potential behaviours, our intuition is that malicious activity begins rapidly
once a malicious ﬁle begins execution because this reduces the overall runtime of the ﬁle
and thus the window of opportunity for being disrupted (by a detection system, analyst,
or technical failure). As far as we are aware this is the ﬁrst paper attempting to predict
malicious behaviour for various types of malware based on early stage activity.
We feed a concise feature set of ﬁle machine activity into an ensemble of recurrent neural
networks and ﬁnd that we achieve a 94% accurate detection of benign and malicious ﬁles
5 seconds into execution. Previous dynamic analysis research collects data for around 5
minutes per sample.
The main contributions of this paper are:
1. We propose a recurrent neural network (RNN) model to predict malicious behaviour
using machine activity data and demonstrate its capabilities are superior to other
machine learning solutions that have previously been used for malware detection
2. We conduct a random search of hyperparameter conﬁgurations and provide details
of the conﬁgurations leading to high classiﬁcation accuracy, giving insight into the
methods required for optimising our malware detection model
3. We investigate the capacity of our model to detect malware families and variants which
it has not seen previously - simulating ‘zero day’ and advanced persistent threat (APT)
attacks that are notoriously diﬃcult to detect
4. We conduct a case-study using 3,000 ransomware samples and show that our model
has high detection accuracy (94%) at 1 second into execution without prior exposure
to examples of ransomware, and investigate the combinations of features most relevant
to the model decisions
2. Related Work
Automatic malware detection models typically use either code or behaviour based features to represent malicious and benign samples. Each of these approaches has its beneﬁts
and drawbacks, such that research continues to explore detection methods using both kinds
Hybrid approaches use both static and dynamic data, closer approximating the methods
used by anti-virus engines; why analyse the behaviour of a ﬁle if it matches a known malware
signature? But unless static detection is used purely to ﬁlter out known malwares, any
dependence on static methods in a hybrid approach leaves the model open to the same
weaknesses as a purely static model.
Static data. Static data, derived directly from code, can be collected quickly.
signature-based methods fail to detect obfuscated or entirely new malware, researchers have
extracted other features for static detection. Saxe and Berlin distinguish malware from
benignware using a deep feed-forward neural network with a true-positive rate of 95.2% using
features derived from code. However, the true-positive rate falls to 67.7% when the model
is trained using ﬁles only seen before a given date and tested using those discovered for the
ﬁrst time after that date, indicating the weakness of static methods in detecting completely
new malwares. Damodoran et al. conducted a comparative study of static, behavioural
and hybrid detection models for malware detection and found behavioural data to give the
highest area under the curve (AUC) value, 0.98, using Hidden Markov Models with a dataset
of 785 samples. Additionally, Grosse et al. show that, in the case of Android software,
static data can be obfuscated to cause a classiﬁer previously achieving 97% accuracy to
fall as low as 20% when classifying obfuscated samples. Training using obfuscated samples
allowed a partial recovery of accuracy, but accuracy did not improve beyond random chance.
Dynamic data. Methods using dynamic data assume that malware must enact the behaviours necessary to achieve their aims. Typically, these approaches capture behaviours
such as API calls to the operating system kernel. Tobiyama et al. use RNNs to extract
features from 5 minutes of API call log sequences which are then fed into a convolutional
neural network to obtain 0.96 AUC score with a dataset of 170 samples. Firadusi et al. 
compare machine learning algorithms trained on API calls and achieve an accuracy of 96.8%
using correlation-based feature selection and a J48 decision tree. The 250 benign samples
used for the experiment are all collected from the WindowsXP System32 directory, which
is likely to give a higher degree of homogeneity than benign software encountered in the
wild. Ahmed et al. detect malware using API call streams and associated metadata
with a Naive Bayes classiﬁer, achieving 0.988 AUC, again with the 100 benign samples being WindowsXP 32-bit system ﬁles. Both Tian et al. and Hansen et al. use Random
Forests trained on API calls and associated metadata to achieve 97% accuracy and a 98%
F-Score respectively. Huang and Stokes achieve the highest accuracy in the literature,
99.64%, using System API calls and features derived from those API calls using a shallow
feed-forward neural network. Table 1 outlines the dataset sizes and recording time for the
related literature. The median dataset size for binary classiﬁcation is 1,300 samples. Huang
and Stokes and Pascanu et al. are outliers with much larger datasets, both obtained through access to the corpus of samples held privately by the authors’ companies.
The majority of research does not mention a time-cap on ﬁle execution, in these cases we
may presume that the ﬁles are executed until activity stops. The median data capture time
frame for those reported is 5 minutes (see Table 1).
Reported time collecting dynamic
Binary classiﬁcation
No time cap mentioned - implicit
full execution
No time cap mentioned - implicit
full execution
Fixed time and 5-10 minutes mentioned but overall time cap not explicitly stated
30 seconds
3.33 minutes (200 seconds)
At least 15 steps - exact time unreported
No time cap mentioned - implicit
full execution
Malware family classiﬁcation
No time cap mentioned - implicit
full execution
Table 1: Reported data sample sizes and times collecting dynamic behavioural data per sample
Time-eﬃciency dynamic analysis methods. Existing methods to reduce dynamic data recording time focus on eﬃciency. The core concept is only to record dynamic data if it will improve
accuracy, either by omitting some ﬁles from dynamic data collection or by stopping data
collection early. Shibahara et al. decide when to stop analysis for each sample based
on changes in network communication, reducing the total time taken by 67% compared
with a “conventional” method that analyses samples for 15 minutes each. Neugschwandtner
et al. used static data to determine dissimilarity to known malware variants using a
clustering algorithm. If the sample is suﬃciently unlike any seen before, dynamic analysis
is carried out. This approach demonstrated an improvement in classiﬁcation accuracy by
comparison with randomly selecting which ﬁles to dynamically analyse, or selecting based
on sample diversity. Similarly, Bayer et al. create behavioural proﬁles to try and identify polymorphic variants of known malware, reducing the number of ﬁles undergoing full
dynamic analysis by 25%. Approaches to date still allow some ﬁles to be run for a long
dynamic execution time, whereas here we investigate a blanket cut-oﬀof dynamic analysis
for all samples, with a view to this analysis being run in an endpoint anti-virus engine.
RNNs for malware detection. We propose using a recurrent neural network (RNN) for predicting malicious activity as as they are able to process time-series data, thus capturing
information about change over time as well as the raw input feature values. Kolsnaji et
al sought to detect malware families with deep neural networks, including recurrent
networks, to classify malware into families using API call sequences. By combining a convolutional neural network with long-short-term memory (LSTM) cells, the authors were
able to attain a recall of 89.4%, but do not address the binary classiﬁcation problem of
distinguishing malware from benignware. Pascanu et al. did conduct experiments into
whether ﬁles were malicious or benign using RNNs and Echo State Networks. The authors
found that Echo State Networks performed better with an accuracy of around 95% (error
rate of 5%) but did not attempt to predict malicious behaviour from initial execution.
Ransomware detection. In Section 5.4 we test our model on a corpus of 3,000 ransomware
samples. Early prediction is particularly useful for types of malware from which recovery
is diﬃcult and/or costly. Ransomware encrypts user ﬁles and withholds the decryption key
until a ransom is paid to the attackers. This type of attack cannot be remedied without
ﬁnancial loss unless a backup of the data exists. Recent work on ransomware detection by
Scaife et al. uses features from ﬁle system data, such as whether the contents appears to
have been encrypted, and number of changes made to the ﬁle type. The authors were able
to detect and block all of the 492 ransomware samples tested with less than 33% of user data
being lost in each instance. Continella et al. propose a self-healing system, which detects
malware using ﬁle system machine activity (such as read/write ﬁle counts), the authors were
able to detect all 305 ransomware samples tested, with a very low false-positive rate. These
two approaches use features selected speciﬁcally for their ability to detect ransomware, but
this requires knowledge of how the malware operates. Our approach seeks to use features
which can be used to detect any malware family, including those which have not been seen
before. That is to say, we will demonstrate the eﬀectiveness of detecting ransomware without
dependence on ransomware-speciﬁc training data. The key purpose of this ﬁnal experiment
is to show that our general model of malware detection is able to detect general types of
malware as well as time-critical samples such as ransomware.
3. Methods
Dynamically collected data is more robust to obfuscation methods than statically collected data ( , ), but dynamic collection takes longer. In order to advance malware
detection to a more predictive model that can respond in seconds we propose a model which
uses only short sequences of the initial dynamic data to investigate whether this is suﬃcient
to judge a ﬁle as malicious with a high degree of accuracy.
We use 10 machine activity data metrics as feature inputs to the model. We take a
snapshot of the metrics every second for 20 seconds whilst the sample executes, starting at
0s, such that at 1s, we have two feature sets or a sequence length of 2. Though API calls
to the operating system kernel are the most popular behavioural features used in dynamic
malware detection, there are several reasons why we have chosen machine activity features as
inputs to the model instead. Firstly, recent work has shown that API calls are vulnerable to
manipulation, causing neural networks to misclassify samples ( , ). As Burnap et al.
 argue“malware cannot avoid leaving a behavioural footprint” of machine activity, future
work will necessarily examine the robustness of machine activity to adversarial crafting, but
this is outside the scope of this paper. A key advantage of continuous data such as machine
activity metrics is that the model is able to infer information from completely unseen input
values; any unseen data values in the test set will still have numerical relevance to the data
from the training set as it will have a relative value that can be mapped onto the learned
model. API calls on the other hand are categorical, such the meaning of unseen API call
cannot be interpolated against existing data. Practically, categorical features require an
input vector with a placeholder for each category to record whether it is present or not.
Hundreds or even thousands ( ) of API calls can be collected, leading to a very large
input vector, which in turn makes the model slower to train. Being categorical, any API
calls not present in the training data will have no placeholder in the input vector at the
classiﬁcation stage even if they appear in later test samples. The machine activity data we
collected are continuous numeric values, allowing for a large number of diﬀerent machine
states to be represented in a small vector of size 10.
As illustrated in Figure 1, to collect our activity data we executed Portable Executable
(PE) samples using Cuckoo Sandbox , a virtualised sandboxing tool. While executing
each sample we extracted machine activity metrics using a custom auxiliary module reliant
on the Python Psutil library . The metrics captured were: system CPU usage, user CPU
use, packets sent, packets received, bytes sent, bytes received, memory use, swap use, the
total number of processes currently running and the maximum process ID assigned.
Figure 1: High-level model overview
As the data are sequential, we chose an algorithm capable of analysing sequential data.
Making use of the time-series data means that the rate and direction of change in features
as well as the raw values themselves are all inputs to the model. Recurrent Neural Networks
(RNNs) and Hidden Markov Models are both able to capture sequential changes, but RNNs
hold the advantage in situations with a large possible universe of states and memory over
an extended chain of events , and are therefore better suited to detecting malware using
machine activity data.
RNNs can create temporal depth in the same way that neural networks are deep when
multiple hidden layers are used. Until the development of the LSTM cell by Hochreiter
and Schmidhuber in 1997, RNNs performed poorly in classifying long sequences, as the
updates required to tune the weights between neurons would tend to vanish or explode .
LSTM cells can hold information back from the network until such a time as it is relevant
or “forget” information, thus mitigating the problems surrounding weight updates. The
success of LSTM has prompted a number of variants, though few of these have signiﬁcantly
improved on the classiﬁcation abilities of the original model . Gated Recurrent Units
(GRUs) , however, have been shown to have comparable classiﬁcation to LSTM cells,
and in some instances can be faster to train , for this potential training speed advantage,
we use GRU units.
An appropriate architecture and learning procedure of a neural network is usually integral
to a successful model. These attributes are captured by hyperparameter settings, which are
often hand-crafted. Due to the rapid evolution of malware, we anticipate that the RNN
should be re-trained regularly with newly discovered samples, thus the architecture may
need to change too. As it needs to be carried out multiple times, this process should be
automated. We chose to conduct a random search of the hyperparameter space as it can
easily be parallelised (unlike a grid search), it is trivial to implement, and has been found
to be more eﬃcient at ﬁnding good conﬁgurations than grid search .
We chose the
conﬁguration which performed best on a 10-fold cross-validation over the training set for
our ﬁnal model conﬁguration, the hyperparameter search space and ﬁnal conﬁguration is
detailed in Table 2 for reproducibiltiy.
Hyperparameter
Possible values
Best conﬁguration
Bidirectional
True, False
Hidden neurons
Dropout rate
0 – 0.5 (0.1 increments)
Weight regularisation
None, l1, l2, l1 and l2
Bias regularisation
None, l1, l2, l1 and l2
Batch size
32, 64, 128, 256
Table 2: Possible hyperparameter values and the hyperparameters of the best-perfoming conﬁguration on
the training set
4. Dataset
4.1. Samples
We initially obtained 1,000 malicious and 600 “trusted” Windows7 executables from
VirusTotal along with 800 trusted samples from the system ﬁles of a fresh Windows7 64bit installation. We then downloaded a further 4,000 Windows 7 applications from popular
free software sources, such as Softonic , PortableApps and SourceForge . We
included the online download ﬁles as they are a better representation the typical workload
of an anti-virus system than Windows system ﬁles.
We used the VirusTotal API as a proxy to label the downloaded software as benign
or malicious. VirusTotal runs ﬁles through around 60 anti-virus engines and reports the
number of engines that detected the ﬁle as malicious. Similar to , for malicious samples,
we omitted any ﬁles that were deemed malicious by less than 5 engines in the VirusTotal
API as the labelling of these ﬁles is contentious. Files not labelled as malicious by any
of the anti-virus engines were deemed ’trusted’ as there is no evidence to suggest they are
malware. We therefore consider these as benign samples. This has the limitation of not
detecting previously unseen malware but our samples are selected from an extended time
period historically so it is likely that it would be reported as malware at some point in this
period if it were actually malicious.
The ﬁnal dataset comprised 2,345 benign and 2,286 malicious samples, which is consistent
with dataset sizes in this ﬁeld of research e.g. ( , , , , , , ). We used a
further 2,876 ransomware samples obtained from the VirusShare online malware repository
 for the ransomware case study in Section 5.4.
We were also able to extract the date that VirusTotal had ﬁrst seen each ﬁle and the
families and variants that each anti-virus engine classiﬁed the malware samples. The dates
that the ﬁles were ﬁrst seen ranged from 2006 to 2017. We split the test and training set
ﬁles according to the date ﬁrst seen to mimic the arrival of completely new software. The
training set only comprised samples ﬁrst seen by VirusTotal before 11:15 on 10th October
2017 and the test set only samples after this date, which produced a test set of 500 samples
(206 trusted and 316 malicious). We choose this date and time as it gave a number of each
malicious and benign samples that is is line with the sample size in the existing literature.
The total instances of the diﬀerent malware families is documented in Table 3. The
“disputed” class represents those malware for which a family could not be determined because the anti-virus engines did not produce a majority vote in favour of one type. We also
found the precise variants where possible, and have listed the numbers of advanced persistent
threat malware (APTs) and ransomware in each category as APTs are notoriously diﬃcult
for static engines to detect and the ransomware case-study in Section 5.4 required removal
of all ransomware from the training set.
Total (apt)(ransomware)
1,382 (0)(76)
407 (20)(56)
180 (0)(51)
123 (7)(0)
Table 3: Number of instances of diﬀerent malware families in dataset
4.2. Input Features
Table 4 outlines the minimum and maximum values of the 10 inputs we collected for
malware and benignware respectively. Though the inter-quartile ranges of values are generally similar (See Figure 2) The benign data sees a far greater number of outliers in RAM use
(memory and swap) and packets being received. The malicious data has a large number of
outliers in total number of processes, but the benign samples have outliers in the maximum
assigned process ID, indicating that malicious ﬁles in this dataset try to carry out lots of
longer processes simultaneously, whereas benign ﬁles will carry out a number of quick actions
in succession.
Data preprocessing. Prior to training and classiﬁcation, we normalise the data to improve
model convergence speed in training. By keeping data between 1 and -1, the model is able to
converge more quickly, as the neurons within the network operate within this numeric range
 . We achieve this by normalising around the zero mean and unit variance of the training
data. For each feature, i , we establish the mean, µi, and variance, σi, of the training data.
These values are stored, after which every feature, xi is scaled:
Total Processes
Max. Process ID
CPU User (%)
CPU System (%)
Memory Use (MB)
Swap Use (MB)
Packets Sent (000s)
Packets Received (000s)
Bytes Received (MB)
Bytes Sent (MB)
Table 4: Minimum and maximum values of each input feature for benign and malicious samples
Figure 2: Frequency distributions of input features for benign and malicious samples
5. Experimental Results
For reproducibility, the code used to implement the following experiments can be found at
 Information on the data supporting the results presented here, including how to access them, can be found in the Cardiﬀ
University data catalogue at 
Keras to implment the RNN experiments, ScikitLearn to implement all other machine lerning algorithms and trained the models using an Nvidia GTX1080 GPU. The Virtual
Machine used 8GB RAM, 25GB storage, and a single CPU core running 64-bit Windows
7. We installed Python 2.7 on the machine along with a free oﬃce software suite (Libre-
Oﬃce), browser (Google Chrome) and PDF reader (Adobe Acrobat). The virutal machine
was restarted between each sample execution to ensure that malicious and benign ﬁles alike
began from the same machine set-up.
5.1. Hyperparameter conﬁguration
Each layer of a neural network learns an abstracted representation of the data fed in
from the previous layer. There must be a suﬃcient number of neurons in each layer and
a suﬃcient number of layers to represent the distinctions between the output classes. The
network can also learn to represent the training data too closely, causing the model to overﬁt.
Choosing hyperparameters is about ﬁnding a nuanced, but generalisable representation of
the data. Table 2 details the search space and ﬁnal hyperparameters selected for the models
in the later experiments. Although there are only 8 parameters to tune, but there are 576
million diﬀerent possible conﬁgurations. As well as the hyperparameters above, we randomly
select the time into execution of data. Although the goal is to ﬁnd the best classiﬁer for
the shortest amount of time, selecting an arbitrary time such as 5 or 10 seconds into ﬁle
execution may only produce models capable of high accuracy at that sequence length. We
do not know whether a model will increase monotonically in accuracy with more data or
peak at a particular time into the ﬁle execution.
Randomising the time into execution
used for training and classiﬁcation reduces the chances of having a blinkered view of model
capabilities.
Without regularisation measures, the representations learned by a neural network can
fail to generalise well. For regularisation, we try using dropout as well as l1 and l2 regularisation on the weight and bias terms in the network in our search space. Dropout 
randomly omits a pre-deﬁned percentage of nodes each training epoch, which commonly
limits overﬁtting. l1 regularisation penalises weights growing to large values whilst l2 regularisation allows a limited number of weights to grow to large values. Our random search
indicated that a dropout rate of 0.1-0.3 produced the best results on the training set, but
weight regularisation was also prevalent in the best-performing conﬁgurations.
Bidirectional RNNs use two layers in every hidden layer, one processing the time series
progressively, and the second processing regressively. Pasacnu et al. found good results
using a bidirectional RNN, as the authors were concerned that the start of a ﬁle’s processes
may be forgotten by a progressive sequence as if the LSTM cell forgets it in favour of new
data, the regressive sequence ensures that the initial data remains prevalent in decisionmaking. We also found that many of the the best-scoring conﬁgurations used a bidirectional
architecture.
A model depth of 2 or 3 gave the best results. The number of hidden neurons was 50
or more in each layer to give any accuracy above 60%. All conﬁgurations used the “Adam”
weight updating rule as it learns to adjust the rate at which weights are updated during
5.2. Predicting malware using early-stage data
Our goal is to predict malware using behavioural analysis quickly enough that user experience would not (signiﬁcantly) suﬀer from the time delay. If the model is accurate within
a short time, this sandbox-based analysis could be integrated into an endpoint antivirus
We tested RNNs against other machine learning algorithms used for behavioural malware
classiﬁcation: Random Forest, J48 Decision Tree, Gradient Boosted Decision Trees, Support
Vector Machine (SVM), Naive Bayes, K-Nearest Neighbour and Multi-Layer Perceptron
algorithms (as in , , , ).
Previous research indicates that Random Forest,
Decision Tree or SVM are likely to perform the best of those considered.
To mimic the challenge of analysing new incoming samples, we have derived a test set
using only the samples that were ﬁrst seen by VirusTotal after 11:15 on 10th October 2017.
This does not account for variants of the same family being present in both the test and
training set, but we explore this question in Section 5.3.
Figure 3 shows the accuracy trend as execution time progresses for the 10-fold cross
validation on the training set and on the test set.
Random Forest achieves the highest
accuracy over the 20 seconds of execution on the training set (see Table 5), but the RNN
achieves the highest accuracy on the unseen test set (see Table 6) and outperforms all
other algorithms on the unseen test set after 1 second of execution (see lower graph in
Figure 3). This could be because the training set is quite homogeneous and so relatively
easy for the Random Forest to learn, but it is unable to generalise as well as the RNN to
the completely new ﬁles in the test set. The RNN cannot usefully learn from 0 seconds
as there is no sequence to analyse so accuracy is equivalent to random guess. Using just
1 snapshot (at 0 seconds) of machine activity data, the SVM performs best on the test
set and is able to classify 80% of unseen samples correctly. But after 1 second the RNN
performs consistently better than all other algorithms. Using 4 seconds of data the RNN
correctly classiﬁes 91% of unseen samples, and achieves 96% accuracy at 19 seconds into
execution, whereas the highest accuracy at any time predicted by any other algorithm is 92%
(see Table 7). The RNN improves in accuracy as the amount of sequential data increases.
Although peak accuracy occurs at 19 seconds, the predictive accuracy gains per second
begin to diminish after 4 seconds . From 0 to 4 seconds accuracy improves by 41 percentage
points (11 percentage points from 1 second to 4 seconds) but only by 5 points from 4 to 19
seconds. Our results indicate that dynamic data from just a few seconds of execution can
be used to predict whether or not a ﬁle is malicious. At 4 seconds we are able to accurately
classify 91% of samples, which constitutes an 8 percentage point loss from the state of the
art dynamic detection accuracy in exchange for a 04:56 minutes time saved from the
typically documented data recording time per sample (see Table 1), making our model a
plausible addition to endpoint anti-virus detection systems.
Figure 3: Classiﬁcation accuracy for diﬀerent machine learning algorithms and a recurrent neural network
as time into ﬁle execution increases
Accuracy (%)
RandomForest
MultiLayerPerceptron
KNearestNeighbors
DecisionTree
NaiveBayes
GradientBoostedDecisionTrees
Table 5: Highest average accuracy over 10-fold cross validation on training set during ﬁrst 20 seconds of
execution with corresponding false positive rate (FP) and false negative rate (FN)
Accuracy (%)
RandomForest
MultiLayerPerceptron
KNearestNeighbors
DecisionTree
NaiveBayes
GradientBoostedDecisionTrees
Table 6: Highest accuracy on unseen test set during ﬁrst 20 seconds of execution with corresponding false
positive rate (FP) and false negative rate (FN)
Table 7: RNN prediction Accuracy (Acc.), false negative rate (FN) and false positive rate (FP) on test set
from 1 to 20 seconds into ﬁle execution time
5.3. Simulation of zero-day malware detection
Dividing the test and training set by date ensures that the two groups are distinct sets
of ﬁles. However, a slight variant on a known strain is technically a new ﬁle. We were able
to extract information about the malware families and variants and want to test how well
the model performs when confronted with a completely new family or variant.
Table 8 gives the numbers in the test set for the families and those variants for which there
were more than 100 instances in the dataset. Dinwod, Eldorado, Zusy and Wisdomeyes are
Trojans; Kazy and Scar are Viruses. We also collected all of those variants listed as advanced
persistent threats (APTs) for as signature based systems struggle to detect these especially if
previously unseen. The APTs and some of the high-level families have less than 100 samples
and as such the results are unlikely to be indicative for the general population of that family
but we test them anyway for comparison.
To avoid contamination from those samples that were disputed, these are removed from
the dataset for the following experiments. For each family in Table 8, we trained a completely
new model without any samples from the family of interest.
The test set is entirely malicious, which means accuracy is an appropriate metric as it is
just the rate of correct detection from the only class of interest. Table 9 gives the predictive
accuracy over time for diﬀerent families and for APTs, and Table 10 gives the predictive
Family/Variant
1,382 (0)(76)
407 (20)(56)
180 (0)(51)
123 (7)(0)
Wisdomeyes
Table 8: Test accuracy diﬀerence between family omitted and included in training set
accuracies for the ﬁve variants for which we collected over 100 samples. Perhaps surprisingly,
we see high classiﬁcation accuracies across these two sets of results. The families are detected
with lower accuracy in general. For the Trojans particularly, during the ﬁrst few seconds,
accuracy is actually worse than random chance.
Because so much of the dataset set is
comprised of Trojans, removing these from training halves the number of malware samples,
so this may account for the particularly poor performance.
The accuracy does increase
signiﬁcantly between 1 and 3 seconds of execution. This is probably because Trojans are
deﬁned by their delivery mechanism, and the model has not been trained on any examples of
this form of malware delivery. The model has, however, seen malicious behaviour from other
families, which may be similar to some of the later behaviours by the Trojans, accounting
for the signiﬁcant rise in accuracy. Though the Worms are actually detected with a 100%
accuracy at each second, there were only 24 Worm samples in the dataset.
Table 9: Classiﬁcation accuracy on diﬀerent malware families with all instances of that family removed from
training set
Wisdomeyes 92.59
Table 10: Classiﬁcation accuracy on diﬀerent malware variants with all instances of that variant removed
from training set
Figure 4: Comparative detection accuracy on various malware families with examples of the family omitted
from the training set
Figure 5: Comparative detection accuracy on various malware variants with examples of the variant omitted
from the training sett
The variants tend to achieve a higher predictive accuracy than the families. Other than
Dinwod, all families score lower at 10 seconds than at 1 second. Each variant is a kind of
Trojan or Virus, but the model was trained on other types of Trojan and Virus. This can help
explain the slight drop in accuracy over the ﬁrst 10 seconds. It is the delivery mechanism
which the variants have in common with samples in the training set, so the period over
which this occurs (the ﬁrst few seconds) gives the best predictive accuracy. Every variant
was detected with over 89% accuracy during the ﬁrst second of execution, despite the model
having no exposure to that variant previously.
If the model is able to score well on a family without ever having seen a sample from
that family, the model may hold a robustness against zero days, and support our hypothesis
that malware do not exhibit wildly diﬀerent behavioural activity from one another as their
goals are not wildly divergent, even if the attack vector mechanisms are.
5.4. Ransomware Case Study
Early prediction that a sample is malicious enables defensive techniques to move from
recovery to prevention. This is particularly desirable for malware such as ransomware, from
which data recovery is only possible by paying a ransom if a backup does not exist. We
obtained an additional 2,788 ransomware samples from the VirusShare website to test
the predictive capability of our model.
Reports in the wake of the high proﬁle ransomware attacks, e.g. WannaCry/WannaDecryptor
worm in May 2017, were reported to be preventable if a patch released two months earlier
had been installed . Endpoint users cannot be relied on to carry out security updates
as the primary defence against new malware. We test our method by removing the 183
ransomware samples and the disputed-family samples from our original dataset and train a
new model on the remaining samples, we then test how well the model is able to detect the
VirusShare samples and the removed 183 samples.
The model is able to detect 94% of samples at 1 second into execution without having
seen any ransomware previously. When we include half of the ransomware samples in the
training set, this rises to 99.86% (see Table 11).
Training Set
Half included
Table 11: Classiﬁcation accuracy on ransomware for one model which has not been trained on ransomware
(omitted), and for one which has (half included)
In Figure 6 there is a clear distinction in the accuracy trend over execution time between
the model which has been trained on some of the relevant family. The model which has
never seen ransomware before starts to drop in accuracy after the initial few seconds. Again
we believe this is because the model is recognising the delivery mechanism at the start of
execution, which will be common to other types of malware in the training set, though
the later malicious behaviour is is less recognisable to the model by comparison with the
later behaviour of the other types of malware it has seen. The model trained with half of
the samples knows how ransomware behaves after a few seconds and so maintains a high
detection accuracy.
It would be interesting to see if the model at 1 second and the model at 5 seconds rely on
diﬀerent input features to reach accurate predictions. It is diﬃcult to penetrate the decision
making process of a neural network; the architecture presented here has 1,344 neurons almost
4 million trainable parameters, but we can turn the input features on and oﬀand see the
eﬀect of combinations of features on classiﬁcation accuracy. By setting the inputs to zero,
which is the normalised mean of the training data, we can turn a feature “oﬀ”. By turning
oﬀall the features and then turning them back on sequentially, we can see which features
are needed to gain a certain level of accuracy.
In Table 12, we can see that with just two features, both the 1 second and the 5 seconds models trained with and without ransomware are able to beat 50% accuracy (random
chance). The model trained using ransomware is able to correctly detect more than 99%
of ransomware samples as malicious using just the number of packets sent and either the
number of packets or number of bytes received. Unlike the model trained with ransomware,
which draws accurate conclusions from packet data and total processes, when no ransomware
is included in the training set, memory usage is also a prominent feature in accurate detection. Comparing to the broader families, in classifying Adware, Trojans and Viruses,
memory and packets a single input feature allowed the model to achieve more than 50%
accuracy, Trojans are the only family for which memory contributes to scoring above 50%
at the one-second model, when combined with packets sent and swap. As Trojans comprise
Figure 6: Classiﬁcation accuracy on ransomware for one model which has not been trained on ransomware
(omitted), and for one which has (half included)
the majority of the dataset it makes sense that the most relevant features for classifying
them help to deﬁne what constitutes malware to the model.
The accuracy in identifying unseen families highlights the presence of shared dynamic
characteristics between diﬀerent malware types. The broad families, which detail the malware infection mechanism particularly help to identify malware early on. Whilst new malware variants are likely to appear, new delivery mechanisms are far less common and help
to distinguish unseen families from benignware.
5.5. Improving prediction accuracy with an ensemble classiﬁer
As well as accuracy, the values of the model predictions increase with time into ﬁle execution. Therefore we now propose an ensemble method, using the top three best performing
conﬁgurations found in the hyperparameter search space during the previous experiments,
to try and improve the classiﬁcation conﬁdence earlier in the ﬁle execution. Accuracy does
not increase monotonically in our ﬁrst conﬁguration, and of the best three conﬁgurations on
Ransomware omitted from training set
Ransomware in training set
# Features
1 second model
5 second model
1 second model
5 second model
rx packets
tx packets
Table 12: Maximum accuracy scores in predicting ransomware with only one and two features turned on for
a model not trained on ransomware and for a model trained on ransomware
the 10-fold cross-validation, no single conﬁguration consistently achieved the highest accuracy at each second, the conﬁguration used in the previous sections was the conﬁguration
that scored the highest accuracy at 1 second.
We take the best-scoring conﬁgurations on the training set across the ﬁrst 5 seconds,
which are 3 distinct hyperparameter sets (one model was the best at 1 and 2 seconds, one
at 3 and 5 seconds) and take the maximum of the predictions of these three RNNs before
thresholding at 0.5 to give a ﬁnal malicious/benign label. The conﬁguration details are in
Table 13, conﬁguration “A” is the same as has been used in the previous experiments.
Hyperparameter
Bidirectional
Hidden neurons
Dropout rate
Weight regularisation
Bias regularisation
Batch size
Table 13: Highest accuracy-scoring conﬁgurations during ﬁrst 5 seconds in 10-fold cross validation on training
To combine the predictions of conﬁgurations A, B and C we take the maximum value
of the three to bias the predictions in favour of detecting malware (labelled as 1) over
benignware (labelled as 0). An ensemble of models does tend to boost accuracy, increasing
detection from 92% to 94% at 5 seconds, and the maximum accuracy from conﬁguration A
alone, 96%, is reached at 9 seconds instead of at 19 seconds (see Table 14). The results in
Table 14 show that the accuracy score improves or matches the highest scoring model of
conﬁgurations A, B and C for 12 of the ﬁrst 20 seconds. Model A, the original conﬁguration,
only bests the ensemble accuracy once. We tested whether the ensemble scores improved
predictive conﬁdence on the individual samples compared with the predictions of the bestscoring model. We can measure predictive conﬁdence by rewarding those correct predictions
closer to 1 or 0 more highly, i.e. a prediction of 0.9 is better than 0.8 when the sample is
malicious. The equation for predictive conﬁdence is as follows:
confidence = 1 −|b −p|
where b is the true label and p is the predicted label.
Highest accuracy
conﬁgurations
A, B and C
Ensemble acc. (%)
Ensemble FP (%)
Ensemble FN (%)
87.52 (A, C)
Table 14: Ensemble accuracy (acc.), false positive rate (FP) and false negative rate (FN) compared with
highest accuracy of conﬁgurations A, B and C. Those marked with a “*” signify predictions that were
statistically signiﬁcantly more conﬁdent by at the conﬁdence level of 0.01
Using a one-sided T-test, we found that the conﬁdence of predictions from the ensemble
method were signiﬁcantly higher (at 0.01 conﬁdence level) for every second after 1 second,
malicious predictions are likely be more conﬁdent as we are taking the maximum value of the
three models, but it is interesting that taking the maximum of the benign samples does not
out weigh the increase in conﬁdence. This indicates that three models are more conﬁdent
about benign samples than malicious ones. A further beneﬁt of the ensemble approach is
the reduction in the false negative rate. The minimum false negative rate for Model A was
4.5%, but here the false positive rate is at least 3 percentage points lower than for model
A during the ﬁrst 7 seconds, and remains lower than Model A’s global minimum for the
remaining 20 seconds.
If the gains in accuracy for the ensemble classiﬁer are due to diﬀerences in the features
learned by the network, this could help to protect against adversarial manipulation of data.
We attempt to interpret what conﬁgurations A, B, and C are using to distinguish malware
and benignware. These preliminary tests seek to gauge whether it is possible to analyse the
decisions made by the trained neural networks.
By setting the test data for a feature (or set of features) to zero, we can approximate
the absence of that information between samples. We assess the overall impact of turning
features “oﬀ” by observing the fall in accuracy and dividing it by the number of features
turned oﬀ. A single feature incurring a 5 percentage point loss attains an impact factor of
-5, but two features creating the same loss would be awarded -2.5 each. Finally, we take the
average across impact scores to assess the importance of each feature when a given number
of features are switched oﬀ.
Figure 7 gives the impact factors for each feature at 4 seconds into ﬁle execution. Intuitively, the more features omitted, the higher the impact factors become. Interestingly, there
are some very small gains in accuracy for conﬁgurations A and B when only one feature is
missing but no more than 0.2 percentage points. For each of the conﬁgurations, CPU use
on the system has the highest impact factor. It is most integral for conﬁguration A, which
is also the best-scoring model. The CPU use in conﬁguration A does not really see an increase in its impact factor as we remove more input features, but for conﬁguration B, all
features attain higher impact factors the more are removed. We can infer that conﬁguration
B has learned a representation of the data which combines the inputs to decide whether the
output is malicious or benign, whereas conﬁguration A appears to have learned at least one
representation of CPU system use as a predictor of malware.
The diﬀerence between the impact scores and their emphasis can help us to see which
features are most predictive at diﬀerent time steps (at 4 seconds this is CPU usage) and to
understand how an ensemble classiﬁer is able to outperform the predictions of its components. As all three models suﬀer the biggest loss from CPU usage, if an adversary knew this
she might be able to manipulate CPU system use to avoid detection. Future work should
examine the decision processes of networks to detect potential weaknesses that could be
exploited to evade detection. The ensemble oﬀers a small increase in accuracy but more
importantly, this analysis can help to understand ways in which the models may be manipulated, by biasing results towards malicious predictions (taking the maximum prediction)
we introduce a form of safety-net against the manipulation of a single model.
6. Limitations and Future work
Our results indicate that behavioural data can provide a good indication of whether or
not a ﬁle is malicious based only on its initial behaviours, even when the model has not
been exposed to a particular malware variant before. Dynamic analysis could reasonably
be incorporated into endpoint antivirus systems if the analysis only takes a few seconds per
ﬁle. Further challenges which must be addressed before this is possible include:
6.1. Other ﬁle types and operating systems
So far we have only examined Windows7 executables. Though Windows7 is the most
prevalent operating system globally and Windows executables are the most commonly
submitted ﬁle to VirusTotal , we should extend these methods to see if the model is
capable of detecting malicious PDFs, URLs and other potential vehicles for malware, as
well as applications which run on other operating systems.
6.2. Robustness to adversarial samples
The robustness of this approach is limited if adversaries know that the ﬁrst 5 seconds
are being used to determine whether a ﬁle will run in the network. By planting long sleeps
or benign behaviour at the start of a malicious ﬁle, adversaries could avoid detection in
the virtual machine. We hypothesised that malicious executables begin attempting their
objectives as soon as possible to mitigate the chances of being interrupted, but this would
be likely to change if malware authors knew that only subsections of activity were the basis of
anti-virus system decisions. We envisage future work examining a sliding-window approach
to behavioral prediction.
The sliding-window approach will take snapshots (of 5 seconds) of data and monitor
machine activity on a per-process basis to try and predict whether or not a ﬁle is malicious.
This would run in the background as the ﬁle is executed in a live environment. The advantage
of this approach is that we eliminate the waiting time before a user is allowed to access the
ﬁle. The challenges in implementing these next steps are recalibration for endpoint machines
(see Section 6.4 below) and suﬃciently quick killing of the malicious process once it has been
detected, i.e. before the malicious payload is executed.
Despite the future worry that executables could be amended to avoid detection by the
model proposed in this paper, this does not invalidate the use of our proposed method.
Whilst some attacks may be altered speciﬁcally to evade an behavioral early-detection system, this would be in response the attacker knowing that the target in question was employing these types of defence. However, there would still be many malwares without benign
behaviour injections at the start of the ﬁle. We continue to use signature-based detection
in antivirus systems despite the use of static obfuscation techniques, because it is still an
invaluable method for quickly detecting previously seen malwares. The model proposed here
indicates that we can quickly detect unseen variants, and we hope that future research will
evaluate the robustness of the sliding window approach using adversarially crafted samples.
6.3. Process blocking
If a live monitoring system is implemented, processes predicted to be malicious will need
to be terminated. Future work should examine the ability of the model to block once the
classiﬁer anticipates malicious activity, and to investigate whether the malicious payload has
been executed.
6.4. Portability to other machines and operating systems
The machine activity metrics are speciﬁc to the context of the virtual machine used in this
experiment. To move towards adoption in an endpoint anti-virus system, the RNN should
be retrained on the input data generated by a set of samples on the target machine. Though
this recalibration will take a few hours at the start of the security system installation, it will
only need to be performed when hardware is upgraded (once per machine for most users)
and opens the possibility of porting the model to other operating systems, including other
versions of Windows.
Though we have not tested the portability of the data between machines, i.e. training
with data recorded on one machine and testing with data recorded on another, it is easy to see
cases in which this will not work. Some metrics, such as CPU usage are relative (measured
as a percentage of total available processing power) and so will change dramatically with
hardware capacities. For example, a ﬁle requiring 100% of CPU capacity on one machine
may use just 30% on another with more cores. However, we see no reason why the model
cannot be re-calibrated to a new machine. There is cause for concern if the hardware means
that the granularity of the data falls below that which is used in this paper. For example a
very small amount of RAM could limit the memory usage such that the useful information
that one sample uses 1.1MB and another 1.2MB are both capped at 1MB, thus appearing
the same to the model. Whilst the experiments in this paper are conducted in a virtual
machine and the memory, storage and processing power can be replicated, we hope that
future work will extend this model to run live in the background on the intended recipient
machine. Since the hardware capacities of a typical modern computer are greater than those
for the virtual machine used here, this may in turn provide more granularity in the data and
possibly allow the model to learn a better representation of the diﬀerence between malicious
and benign software. The diﬀerent results that we would be likely to see on a more powerful
machine oﬀer a potential advantage in training but also necessitate re-calibration on a permachine basis. Since this is a one-oﬀtime cost, it is not a major limitation of the proposed
7. Conclusions
Dynamic malware detection methods are often preferred to static detection as the latter
are particularly susceptible to obfuscation and evasion when attackers manipulate the code
of an executable ﬁle. However, dynamic methods previously incurred a time penalty due
to the need to execute the ﬁle and collect its activity footprint before making a decision
on its malicious status. This meant the malicious payload had likely already been executed
before the attack was detected. We have developed a novel malware prediction model based
on recurrent neural networks (RNNs) that signiﬁcantly reduces dynamic detection time, to
less than 5 seconds per ﬁle, whilst retaining the advantages of a dynamic model. This oﬀers
the new ability to develop methods that can predict and block malicious ﬁles before they
execute their payload completely, preventing attacks rather than having to remedy them.
Through our experimental results we have shown that it is possible to achieve a detection
accuracy of 94% with just 5 seconds of dynamic data using an ensemble of RNNs and an
accuracy of 96% in less than 10 seconds, whilst typical ﬁle execution time for dynamic
analysis is around 5 minutes.
The best RNN network conﬁgurations discovered through random search each employed
bidirectional hidden layers, indicating that making use of the input features progressing as
well as regressing in time aided distinction between malicious and benign behavioural data.
A single RNN was capable of detecting completely unseen malware variants with over
89% accuracy for the 6 diﬀerent variants tested at just 1 second into ﬁle execution. The
accuracy tended to fall a little after the ﬁrst 2 seconds, implying that the model was best
able to recognise the infection mechanism at a family level (e.g. Trojan, Virus) given that
this would be the ﬁrst activity to occur. The RNN was less accurate at detecting malware at
a family level when that family had been omitted from the training data (11% accuracy at
1 second detecting Trojans), further indicating that the model was easily able to detect new
variants, provided it had been exposed to examples of that family of infection mechanisms.
Our ransomware use case experiment supported this theory further, as the RNN was able to
detect ransomware, which shares common infection mechanisms with other types of attack
such as Trojans, with 94% accuracy, without being exposed to any ransomware previously.
However, this accuracy fell as time into ﬁle execution increased, again implying that the
model was easily able to detect a malicious delivery mechanism, better than the activity
itself. After exposure to ransomware, the model accuracy remained above 96% for the ﬁrst
10 seconds.
The RNN models outperformed other machine learning classiﬁers in analysing the unseen
test set, though the other algorithms performed competitively on the training set. This
indicates that the RNN was more robust against overﬁtting to the training set than the
other algorithms and had learnt a more generalisable representation of the diﬀerence between
malicious and benign ﬁles. This is particularly important in malware detection as adversaries
are constantly developing new malware strains and variants in an attempt to evade automatic
detection.
To date this is the ﬁrst analysis of the extent to which general malware executable ﬁles
can be predicted to be malicious during its execution rather than using the complete log
ﬁle post-execution, we anticipate that future work can build on these results to integrate
ﬁle-speciﬁc behavioural detection into endpoint anti-virus systems across diﬀerent operating