Capturing Sensor-Generated Time Series with Quality Guarantees
Iosif Lazaridis
University of California, Irvine
Irvine, CA, USA
 
Sharad Mehrotra
University of California, Irvine
Irvine, CA, USA
 
We are interested in capturing time series generated by
small wireless electronic sensors. Battery-operated sensors
must avoid heavy use of their wireless radio which is a key
cause of energy dissipation. When many sensors transmit,
the resources of the recipient of the data are taxed; hence,
limiting communication will beneÔ¨Åt the recipient as well. In
our paper we show how time series generated by sensors
can be captured and stored in a database system (archive).
Sensors compress time series instead of sending them in raw
form. We propose an optimal on-line algorithm for constructing a piecewise constant approximation (PCA) of a
time series which guarantees that the compressed representation satisÔ¨Åes an error bound on the
distance. In addition to the capture task, we often want to estimate the values of a time series ahead of time, e.g., to answer real-time
queries. To achieve this, sensors may Ô¨Åt predictive models
on observed data, sending parameters of these models to
the archive. We exploit the interplay between prediction and
compression in a uniÔ¨Åed framework that avoids duplicating
effort and leads to reduced communication.
1. Introduction
Data generated by small wireless electronic sensors are
increasingly signiÔ¨Åcant for emerging applications . Sensors are becoming smaller, cheaper and more con-
Ô¨Ågurable . Current and future sensor designs routinely
include a fully programmable CPU, a local memory buffer
and a wireless radio for communication .
Sensors must be treated as equal partners in future distributed
database systems as they can store, manipulate and communicate information.
1.1. The Time Series Capture Task
In our paper we are interested in capturing sensorgenerated time series. Each sensor, or data producer generates a series of values of some measured attribute, e.g., temperature. Sending these raw values to the data archiver (a
database system) uses up the limited communication bandwidth and causes energy drain . If multiple sources of information are involved, bandwidth at the
archiver end may be limited as well . Even if all information can be received, it may be too difÔ¨Åcult to process
if the rate of data generation is high . Obviously,
limiting communication in a system involving sensors will
beneÔ¨Åt all involved parties.
We assume that some loss of precision in the archived
version of the time series can be tolerated if this helps reduce communication. We do not want, however, unbounded
inaccuracy in the stored imprecise series. Besides the capture task, time series values may be needed ahead of time
by real-time applications, e.g., queries. Such applications
and the capture task must gracefully co-exist.
We observe that time series values are not entirely random and can thus be compressed. This implies that some
number of samples must be accumulated, since compression exploits the redundancy of information across many
samples; the sensor must see some samples, compress them
and forward the compressed representation to the archiver.
Propagating messages from the sensor to the archiver
takes time. Hence, any application that requires knowledge
of recent, present or future time series values must wait for
these to arrive. This time will be longer if samples are not
forwarded immediately but are rather compressed. To address this issue, sensors are tasked with Ô¨Åtting parametric
predictive models of the time series, sending parameters of
these models to the archive. Using these, values of the time
series can be estimated ahead of time, reducing the latency
seen by applications.
1.2. Why is Capturing Time Series Important?
Many applications over sensors are primarily motivated
by their ability to monitor the physical world in real-time. In
many situations sensor data is useful not only for its present
utility for some application, but for its potential future util-
ity as well. Therefore, capturing the complete history of a
time series is essential for systems incorporating sensors.
This contrasts somewhat with the emerging paradigm of
rapidly produced data streams whose focus is not primarily on storage .
For example, sensors will often be used in large-scale
scientiÔ¨Åc experiments. Such experiments, often involving
changing behavior (e.g., the diffusion of pollutants in a
water stream), over long periods of time cannot be accurately studied unless one stores the entire history of the phenomenon. In some cases, e.g., major earthquakes, environmental disasters, volcano eruptions, the studied process is
rare and hence the value of data collected about it is significant.
In a second example, consider an intrusion detection system relying on sound and light intensity measuring devices.
A torch-carrying intruder may set off an alarm, but it is conceivable that the real-time application may be misguided
into not raising an alarm. The next day, when the intrusion is detected, e.g., a precious item is missing, it would be
useful to ‚Äúrewind‚Äù the time series produced by the system‚Äôs
sensors, and try to identify traces of the intrusion.
We view time series generated by sensors as a commodity, besides its real-time usefulness. Our work is part of the
Quality-Aware Sensor Architecture (QUASAR) project at
UC Irvine, which aims to create a general architecture for
different sensor-based applications, both on-line and offline, both current and future.
This differs from a commonplace view in which sensor-based applications are built
from scratch with a single objective (e.g., real-time monitoring) without accounting for unforeseen uses of sensorgenerated data.
1.3. Paper Organization
In Section 2 we formulate our problem and sketch the
proposed solution. In Section 3 we consider compression
with quality guarantees. In Section 4 we motivate the need
for prediction, show how it can be performed, and how it
can co-exist with compression. In Section 5 we evaluate
our techniques experimentally. In Section 6 we cover some
related work, and in Section 7 we summarize our work and
present future research directions.
2. Problem Formulation
We will speak of a single data producer (sensor) and data
archiver (database). Keep in mind that in a real system, the
archiver will interact with many producers. Each producerarchiver interaction will use the algorithms presented in our
paper. The archiver may assign different importance to different sensors. The problem of gathering data from multiple
sources to achieve an overall level of quality of the archive
is itself very interesting . Rather than capturing the
time series by probing the sensor, we will do so by receiving messages from it. Wireless devices pay a heavy price
(energy-wise) for listening on the radio channel even if no
data is being transmitted .
2.1. DeÔ¨Ånitions and Assumptions
For simplicity‚Äôs sake, we will assume that the producer‚Äôs
clock is synchronized with the archiver‚Äôs. Time synchronization is an important issue of research in sensor networks
 but goes beyond the scope of our paper. We will assume that time is discrete and will denote the time domain
 . The time quantum, corresponding to
one step, is the sampling period of the sensor. We will also
deal with time series whose value domain is
 , i.e., the real
is the value of an observed real-world process at time position
note the time position of now as
' . The observed series,
is noted as
-! . We use
to note a subseries from time
<! . Hence,
The sensor has a Ô¨Ånite energy supply. This is depleted
during normal sensing operation at some rate. Additional
energy drain is caused when using any of the sensor‚Äôs equipment, including (i) powering its memory, (ii) using its CPU,
(iii) communicating bits, or listening on the wireless channel.
The speciÔ¨Åc rates of energy consumption for these
operations are sensor-speciÔ¨Åc. Modern sensors try to be
power-aware, shutting down components when they are not
used or e.g., changing the frequency/voltage of the processor depending on the workload.
Different sensors are bound to differ in the details of their
architecture and power consumption. We simply observe
that communication is often the major cause of energy drain
 in sensors and hence, in the interest of extending the
sensor‚Äôs life, communication must be curtailed.
2.2. Objectives
We identiÔ¨Åed communication as the main target for optimization. Our goals are the following:
To capture an approximate version
of the series
in the archive.
is conceptually a time series for the
same time positions as
but has a smaller actual representation. We present algorithms to construct such a
compressed representation in Section 3.
To predict values of
ahead of time, i.e., before receiving them at the archive. This can be achieved by
Ô¨Åtting predictive models, using these to estimate values. We examine the problem of prediction in our setting in Section 4.
2.3. Quality Metric
A commonly used metric for comparing time series is
that of Euclidean distance . If
then this is deÔ¨Åned as:
If we were to specify quality as an upper bound on this
distance, then we would make room for large divergence
of individual samples of the time series.
For similarityretrieval types of applications , the main goal is to
identify similarity of overall structure, rather than similarity
of individual values. We do not want to assume the use of
the time series data. Hence, we will use a stronger notion of
quality, namely that the estimation for any individual sample
should not deviate from
more than an upper
bound. We can use the
and state the quality requirement as follows:
(F[Z]\_^a`cb
)( is said to be a within-
d approximation of
)( if the
above holds. Note that this is a ‚Äústronger‚Äù notion of quality
in that it implies a bound on the Euclidean distance1:
Our Ô¨Årst objective can be formalized as capturing and
storing a within-
^a`jb_d version of
at the archive, where
^a`jb_d , is a user-speciÔ¨Åed capture, or compression tolerance.
2.4. Latency and the need for prediction
The use of prediction is motivated by the latency between the producer and the archiver of the time series. This
can be broken down to:
Communication Latency,
.‚Äî This includes the
transmission, propagation and queuing times in both
the wireless and wired links between producer and
m+nporqsUt
also implies a bound on
zV{3nporqs
namely that
z,{3npoqs
}~x , but this is a very loose bound as it
is proportional to
Compression Latency,
b .‚Äî This consists of the
time spent at the sensor processing
( so as to produce
As a result of the overall latency, at time
' , the archiver
will have received not
=( , but rather
¬Ä¬Ç¬Å¬Ñ¬É where
is the number of time positions it is
‚Äúbehind‚Äù the producer.
Any real-time applications (e.g.,
queries) that require the value of the time series for any time
'<¬à , in the future (
'K¬àw¬â¬ä' ), the present (
'K¬àB¬ã' ),
or the recent past (
' ) must wait for that
value to arrive from the producer.
Suppose that the value of the time series at time position
`U¬á is needed. The system will provide an
using any of the following three evaluation
strategies:
Predict.‚Äî Some predictive model
and its parameters
¬ê are stored in the archive. Subsequently,
predicted as
. We will note this as
F is known. This raises the issue: how does one
obtain such a predictive model, and how can one be
guaranteed of the difference between the predicted and
the actual value.
Probe.‚Äî The producer is asked directly for
requires from the producer to maintain all samples (or
at least their approximations) it has not forwarded to
the archive. Additionally, the producer must now listen
on the wireless channel for probes. This is a cause of
energy drain . The producer can tune in to listen
for probes occasionally. This will, however, increase
the latency seen by queries.
Wait.‚Äî The Ô¨Ånal strategy is to simply wait for
arrive from the producer. The quality of
is guaranteed; it is within
We note here that prediction is very attractive, since it
does away with the latency involved with either doing a
probe or waiting for a value to be sent by the sensor. Our
second objective can be formalized as providing, to any interested applications, a within-
\cb¬ïj¬ñM¬ó estimation of time series values, before these values arrive at the archive. In Section 4 we show how this can be achieved. We will also
brieÔ¨Çy discuss the important problem of choosing a predictive model among many, and present the criterion by which
different candidate models can be compared.
2.5. Combining Compression with Prediction
Some of the work done for prediction can be used for the
capture task as well. If the predictive model is somewhat
accurate, then the archive already has an idea of some time
series values before receiving them. The sensor can use this
to limit the effort that must be spent to compress the time
series. In Section 4.4 we will show how this basic intuition
can be used algorithmically.
3. Compression Algorithms
Work in approximating time series has been extensive in
the literature. Time series have been approximated using
wavelets , Fourier transforms , piecewise linear approximations , or polynomials . Since the approximation must be carried out by the sensor, a device of limited abilities, the employed algorithm must be lightweight
in terms of processing and memory utilization.
3.1. The Piecewise Constant Approximation
An attractive type of lossy2 compression is the piecewise
constant approximation3 (PCA) , whereby the time series
is represented as a sequence of
H is the end-point of a segment and
H is a constant
value for times in
, or for times in
the Ô¨Årst segment. In such an approximate representation,
we estimate
This representation is intuitive and easy to index.
terms of compression, we note that a single segment costs
d¬¶b to store, where
¬§¬• is the size of a sample value
d¬¶b is the size of a time position index. If a time series
is approximated with a PCA sequence of length
, then the compression ratio is
¬°¬®¬ßp¬© ¬™¬Ñ¬´<¬© ¬¨¬Æ¬≠¬Ø
. If each segment
corresponds to many samples of the time series (
is signiÔ¨Åcantly less than 1), then high compression ratios can be
)( can be approximated by different
approximations. As we will see, very simple on-line algorithms with
F space requirement can be used to construct a PCA representation that preserves the desired quality guarantee with minimum
3.2. Poor Man‚Äôs Compression
Poor Man‚Äôs Compression (PMC) is a bare-bones form of
compression that can be used to reduce the size of a timeseries representation. It is an on-line algorithm, producing
2Experimentation with lossless methods (gzip, not reported) indicate
very small (
¬± 50%) compression ratios. Lossless compression also does
not exploit the precision-compression tradeoff.
3This was called Adaptive Piecewise Constant Approximation (APCA)
in to distinguish it from a similar approximation (PAA) with equal
segment lengths.
procedure PMC-MR
time series
;6! , tolerance
compressed time series PCA(S) within-
(1) PCA(S)
o .hasMoreSamples()
¬∂¬∏¬∑j¬π¬∫U¬ª¬çsr¬º
} ¬æ¬¶¬ø¬Ü√Ä√Å¬∂5√Ç¬Ç√É+¬∫c√Ñ√Ås¬º¬Ω
} ¬æ¬¶¬ø=√Ö;√Ü_x √á √àc√âU√ä
s}¬®√Ä>√ècu to PCA(S);
¬≥√ê¬∂5√Ç¬Ç√É¬∫c√Ñ¬õsr¬º¬Ω
¬≥0¬∂¬∏¬∑j¬π+¬∫U¬ª¬çsr¬º¬Ω
(13) append
s~}¬®√Ä>√ècu to PCA(S);
Figure 1. PMC-MR Algorithm
segments of the PCA representation as new samples arrive.
It requires only
F space and performs
F computation
per sample. Hence, its overall time complexity for a series
F . This computation is interspersed with the
arrival of samples; the compressed series is ‚Äúready to go‚Äù as
soon as the last sample is processed. Hence the
of Section 2 is minimized.
be some time series. Can this be compressed
in a single segment in a manner that preserves the
guarantee? Lemma 1 supplies the necessary and sufÔ¨Åcient
condition.
Lemma 1 The time series
can be compressed in a
single segment
F with an error tolerance
^a`jb_d iff:
Proof: If for all
d . Hence,
d . We used the proposition
C . Conversely, if
d then the segment
D√ü=√†c√°√¢√§√£ √•_√£f√¶
√™√¢√§√£ √•U√£f√¶
be trivially shown to compress it within-
3.2.1. Poor Man‚Äôs Compression - Midrange. Our Ô¨Årst algorithm (see Figure 1), PMC-MR (for midrange) uses the
converse of Lemma 1. It monitors the range of its input.
While this is less or equal to
\_^a`jb_d it updates the range
if needed (lines 11-12). When the range exceeds
' , then the segment ending at
 with a constant being equal to the midrange of the preceding points is output
(line 7). The algorithm then tries to compress the next set
of samples, starting at time
(lines 8-9).
PMC-MR not only achieves the goal of compression, but
satisÔ¨Åes an even stronger property: that no other PCA representation satisfying the
^a`jb_d constraint, over any input time
series can be a valid compression for that time series if it
has fewer segments. PMC-MR is thus instance optimal not
only for the class of on-line algorithms, but over any algorithm that solves this problem correctly. We state our claim
and its proof formally.
Theorem 1 Let
be an aribitrary time series that must be approximated with a piecewise constant
approximation that satisÔ¨Åes for all
^a`jb_d . If the PMC-MR algorithm (Figure
1) produces a
F representation with
then no valid PCA representation with
Proof: By contradiction. Let
√∞3√±√ë√≤¬∏√≤Y√±√Å√≥
F be a valid
representation of
 . Therefore of the
intervals of
F at least one does not contain an endpoint
√∞3√±√ë√≤¬∏√≤Y√±¬õ√≥
This cannot be the Ô¨Ånal one, since
that must be
' , which is contained in the Ô¨Ånal interval of
be the interval of
that does not contain an endpoint of
√∞3√±√ë√≤¬∏√≤Y√±¬õ√≥
be the interval of
√∞√Å√±¬õ√≤¬∏√≤Y√±¬õ√≥
F that covers
as well (since
 ). But since
is an interval of
produced by the PMC-MR algorithm, and it is not the Ô¨Ånal one, then the range of values in
greater than
d . The range of values in any time interval
is always greater than the range of values in any of its subintervals. Hence, the range of values in
is greater than
^a`jb_d . Therefore there doesn‚Äôt exist a value
such that for all values
d (Lemma 1). Therefore, the segment of
√∞√Å√±√ë√≤¬∏√≤9√±¬õ√≥
F whose endpoint is at time position
violates the
d constraint and
√∞3√±√ë√≤¬∏√≤Y√±¬õ√≥
F is not a
valid representation for
PMC-MR does an optimal job at compression, but it has
two disadvantages. First, the time series it generates cannot be easily incorporated in similarity-retrieval systems,
which usually rely on PCA representations where the constant value for each segment is the mean of the time series
values for that segment . Second, the mean error produced by PMC-MR may sometimes be large, even close to
^a`jb_d , especially if the distribution of values is skewed. This
problem does not conÔ¨Çict with our speciÔ¨Åcation of quality,
but it is an undesirable property of the algorithm.
Consider the time series
suppose that
PMC-MR will approximate
it with one segment
The mean error will be
Poor Man‚Äôs Compression - Mean.
To address these problems, we propose a modiÔ¨Åed algorithm,
called Poor Man‚Äôs Compression-Mean (PMC-MEAN).
PMC-MEAN is identical to PMC-MR except that it uses
the mean of the points in each segment as the constant
of the segment.
Values are sampled until the mean of
the points seen thus far is more than
d away from the
minimum or maximum of the observed points.
segment is output and the algorithm tries to compress the
next set of points starting from the one that caused the
tolerance violation.
As an example, for the series
above, PMC-MEAN
would output two segments
F . Its error
would be zero for these segments, but it will have produced
more segments than the optimal algorithm (PMC-MR).
Choosing between these two algorithms must depend
on the use of the data and their relative performance at
compression. In our experiments of Section 5 we will see
that over many datasets, PMC-MEAN performed only little
worse than PMC-MR. Hence, we consider it as a viable
alternative to PMC-MR.
PCA Segment Transmission.
We observe that
the PMC algorithms produce a sequence of compressed
segments. These can be forwarded either immediately, or
aggregated into packets for network transmission when
either the sensor‚Äôs memory buffer is Ô¨Ålled, or the maximum
packet length is reached. Normally, we would like to Ô¨Åt
as many segments into a packet as possible, since each
packet has some overhead in terms of header information.
However, since packets may be lost, especially over the
unreliable links available to sensors, it might make sense
to limit the maximum packet length for transmission, thus
avoiding the loss of large segments of the time series all at
Note, that there is no guarantee for the time it takes for
a segment to be output. If compression is successful, then
potentially, a single segment could go on forever ‚Äî if all
new points do not cause the violation of the
d condition.
In practice, we might interrupt these algorithms if we want
to receive segments of the time series in a timely manner.
4. Prediction
In Section 2, we motivated the use of prediction from
the need of real-time applications to co-exist with the capture task. We will now address some issues arising when
prediction is performed.
4.1. Who should predict?
There are two fundamental ways in which prediction can
Archive-side.‚Äî The data archive contains at least
. This can be used, via some prediction model,
to provide an estimate
, for time positions
Producer-side.‚Äî The producer sees the entire
Hence, it can also use some prediction model to provide an estimate
. In this case, the parameters of
this model need to be transmitted to the archive.
Archive-side prediction has the obvious advantage of not
requiring communication with the sensor4. A second advantage is that the archive sees the ‚Äúbroad picture‚Äù of the
sensor‚Äôs history. It can thus infer predictive models at a
larger time scale, accounting perhaps for cyclic behavior,
global trends or other aspects not discernible from the sensor‚Äôs limited (time-wise) perspective. Its disadvantages are:
(i) it is based on
and not on the precise
 , (ii) it cannot provide any prediction quality guarantee, as the archive
does not monitor the precise
which can deviate from the
without bound, and (iii) prediction must be accurate
`U¬á steps into the future for it to predict the present
value accurately. As we mentioned in Section 3.2.3,
may be very large.
Producer-side prediction has the disadvantage of requiring communication. Since producers have limited memory,
only the most recent past of the time series can be stored in
it, or perhaps very coarse derivative information about the
more distant past. Hence, long-term effects like cycles cannot be incorporated into the prediction model. However,
the main advantage of producer-side prediction is that it
uses the raw
series, and allows for prediction guarantees.
Producer-side prediction will be used in the following.
4.2. Producer-Side Prediction
The basic form of Producer-Side Prediction (PSP) is
shown in Figure 2. The input of the algorithm is a time
series, a prediction tolerance
\cb¬ïU¬ñM¬ó and a parametric predictive model
. PSP begins by guessing a set of parameters
(line 1). Subsequently, each sample
is checked
against the predicted value based on the last set of parameters
d (line 7). If this is greater than
b¬ïU¬ñM¬ó , then a new set
of parameters is computed (line 8), by updating the old parameters with the samples at time positions greater than the
time when the last prediction parameters were estimated.
The algorithm produces a sequence of
F pairs. These
4In principle, the archive could also send prediction parameters to the
sensor, especially if prediction guarantees are required.
procedure PSP
time series
;i! ,
prediction tolerance
¬â*¬≤ , model
prediction sequence
(6) while S.hasMoreSamples()
updateParameters
s}Ou to PS;
Figure 2. Producer-Side Prediction with Error
predict the time series starting from time
' . This sequence
deÔ¨Ånes a within-
b¬ïU¬ñM¬ó approximate version of
 , which we
may note as
We observe that prediction parameters do not arrive
instantaneously to the archive. Let
be an upper bound on
this time. Clearly, if the time were unbounded, then PSP
would provide no guarantee, as queries can never be certain
whether a parameter refresh is on its way or not.
\¬Ñb¬ïU¬ñM¬ó .
The best value for
depends on the quality requirements of real-time applications.
If values must be predicted frequently at a high quality,
b¬ïU¬ñM¬ó must be set low. This problem was studied in
detail in . In that paper, the (implicit) prediction model
. We can adopt a similar algorithm to
b¬ïj¬ñ¬Ñ¬ó adaptively. The main intuition in is that as
data becomes more variable,
\cb¬ïU¬ñM¬ó is increased, to reduce
the number of messages. On the other hand, when queries
arrive at a high rate with small error tolerances,
decreased to make sure that these queries can be answered
at the server without performing any probes. Setting
adaptively does not conÔ¨Çict with the algorithms presented
in this paper.
Choosing a Prediction Model.
Our use of
prediction does not assume a particular predictive model.
The actual model to be used, must be chosen based on
(i) domain knowledge about the monitored attribute, and
(ii) engineering concerns, i.e., the cost associated with
Ô¨Åtting prediction models and (especially) transmitting
their parameters. Traditionally, prediction performance is
gauged by prediction error. Suppose that
F are competing models with their parameters. If
' , it is the case that:
F is a ‚Äúworse‚Äù predictor than
From a system performance perspective, as long as
F do not produce errors greater than
\Mb¬ïU¬ñM¬ó , (resulting in new transmission of new parameters),
they are equivalent. Consider competing models
denote the size (in bytes) of their parameters. If
J messages are generated by
J is preferred if
, since this leads
to reduced data transmission.5 If the model must be Ô¨Åxed
a priori, then a decision must be made based on the above
criterion, using experimentation, expert opinion or past
experience to choose between competing models.
Adaptive Model Selection.
In many situations, a global model for predicting a time series is not a
valid assumption. It is likely that a time series can best be
approximated by different models for different epochs of
its history. We informally deÔ¨Åne an epoch as a time period
during which a time series‚Äô behavior is well-predicted by a
single model.
Consider for example a moving object in one dimension.
The object‚Äôs position at different times is the time series of
interest. At times, the object may be idle. The best model
¬û . At other times, it is moving at a constant
speed; a good model is then
Sometimes it is accelerating, or decelerating, etc. All these
times are epochs of the object‚Äôs history.
The problem of detecting changes in sequences is complex . A general solution, applicable to different
types of time series and different classes of models cannot
be easily found. The two main problems in epoch detection is to (i) discover the epoch boundaries, and (ii) not be
fooled by temporary abnormal behavior into thinking that a
new epoch has commenced. These matters are part of our
current research. BrieÔ¨Çy, we anticipate two modes of adaptivity:
Producer-side model selection, in which the sensor
chooses from competing models in reaction to the
changing behavior of the time series.
Archiver-side model selection, in which the archive
monitors system performance and ‚Äúuploads‚Äù predic-
5Depending on the network protocols in place, the difference of
 may not be critical. Costs associated with the protocols (e.g., headers)
may dominate the cost of transmitting either
√é , if e.g., the difference
 is only a few bytes. In such a case, the simple test
√é would be preferred.
Queried Time
HISTORY RECENT PAST FUTURE
Figure 3. Estimating Time Series Values
tion code to the sensor, either by expert intervention,
or automatically.
In Section 5 we will perform a simple experiment validating
the need for adaptive model selection.
4.3. Estimating time series values ahead of time
An application
+ arrives at time
' , asking for some estimate
with bounded error:
¬à . The discussion that follows will refer to Figure 3.
`U¬á . The estimate is based on the
captured series:
the quality of the captured series does not sufÔ¨Åce to
provide an estimate with error bounded by
¬à . In such a
case, the system can only provide an estimate
which may violate the
¬à tolerance.
F be the most recent parameters received at the archive.
¬à . We look at the prediction sequence for the parameters
most recent time position (with respect to
¬à ) in which
parameters were refreshed. The answer is
b¬ïU¬ñM¬ó , but
then the application can wait for
to arrive. As
we have mentioned in Section 3.2.3, this time may be
unbounded. We can force compressed segments to be
sent in a timely manner. Finally, if
it is necessary to do a probe which returns the exact6
'<¬à>¬â," . The answer is
. If at time
¬à , the time series
had violated the
\cb¬ïj¬ñM¬ó tolerance, then we would have
received new parameters by ‚Äúnow‚Äù (
' ). As we have
not received new parameters (since
6Samples need to be kept in the sensor‚Äôs buffer if applications are allowed to probe the sensor for exact values.
then, we are guaranteed that
Z$\¬Ñb¬ïj¬ñM¬ó .
\¬Ñb¬ïU¬ñM¬ó , a probe has to be issued for the
" . Potentially,
new parameters have been estimated for time
last update is not guaranteed to be valid until time position
" . Thus, we wait until that time. While we
wait, it may be that
d changes, since parameters
estimated at times both before
'K¬à and after
'<¬à may arrive. As long as they are before
'6¬à , we still have to
wait. Otherwise (new
d of Figure 3), it becomes
¬à , in which case we estimate the value
as described (Case II). Once again, we can choose to
or probe for
depending on the
4.4. Combining Prediction and Compression
In our discussion so far, we have treated compression
and prediction separately. However, both of them estimate
values of the same time series, from its past and its future
respectively. Can we further reduce the communication cost
by combining the two?
Observe, that prediction in itself can be viewed as a form
of compression. Each set of parameters is a within-
approximation of the time series for all times until the next
parameter refresh. If we wanted to capture the time series
d , we could just as well have set
\U^a`jb_d .
This would have sufÔ¨Åced to capture the time series in the
How good would is such a strategy? If the time series
is very predictable, it may work better than any form of
compression that doesn‚Äôt assume a model for the time series. Consider, e.g., the time series
¬Ü√´' . By Ô¨Åtting the
i√∫' , we never need to re-send any parameters
at all. On the other hand, suppose that the model approximates the time series behavior poorly. Then, parameter refreshes would be sent frequently. Compression would work
much better in this case.
Clearly, if
d , no compression is needed. If
may deviate from
by more than
^a`jb_d . We can still make use of
by observing the following result.
Theorem 2 Let
is a within-
\U^a`jb_d approximation of
-B( , then the series
K! is a within-
d approximation of
Proof: By contradiction. Let
=( not be a within-
d approximation of
)( . Then, there exists some
\U^a`jb_d3e
\U^a`jb_d3e
d . This contradicts our hypothesis, because
-2( is a within-
d approximation of
Theorem 2 presents an alternative strategy for compressing
 , if prediction is performed in the system. The sensor
can monitor the time series
of the prediction errors and
compress it within
d . Subsequently, the compressed
can be sent to the archive which can then obtain a within-
d version of
by adding the predicted series
compressed error
When is compressing
preferrable to compressing
This depends on
and the quality of the predictive
model. When
\¬Ñb¬ïU¬ñM¬ó is close to
d then the error series
taking values in the interval
will probably violate the
d tolerance infrequently. Hence, compressing
may be better than compressing
 . Conversely, as
increases, the error is allowed to Ô¨Çuctuate more; so, perhaps
compressing
is preferrable. The quality of the predictive
model is also a major factor. If
has a small range (irrespective of
b¬ïU¬ñM¬ó ) then it may be more compressible than
In Figure 4 we show (Ô¨Årst four plots), a time series
compression
 , its within-
prediction
and its within-
prediction
Subsequently, we show (next four plots)the prediction error
3 , its compression
3 , the prediction error
compression
3 . The compression of
has 14 segments,
while the compression of
3 has only 10 segments, since
the prediction tolerance
0 is close to
For this time series, the compression of
3 has 16 segments
and is thus worse than the compression of
In Section 5 we will see situations where either compressing
is better. The sensor can compress the
series in both ways. When a message is to be transmitted,
it can choose to forward either the compressed series or the
compressed error, depending on which is smaller. This has
a small overhead for adding a marker to identify the used
strategy and a 2-fold increase in processing and memory usage at the sensor. This is reasonable, since it reduces communication.
5. Performance Study
In this section, we perform an evaluation of our ideas in
this paper. The results conÔ¨Årm the good performance of our
algorithms under different situations.
5.1. Compression Experiments
First, we examine the effectiveness of PMC-MR and
PMC-MEAN for synthetic and real-world data. We use synthetic Random Walk data generated as:
Compressed S (ecapt=2)
Predicted S (epred=2.5)
Predicted S (epred=5)
Delta (epred=1.2)
Compressed Delta (epred=2.5)
Delta (epred=5)
Compressed Delta (epred=5)
Figure 4. Combinng Prediction and Compression
We also used time series of environmental variables from an
oceanographic buoy sampled at 10 min intervals . We
used a Sea Surface Temperature, Salinity, and Shortwave
Radiation series. Statistics about all used series are given in
Table 1. We preprocessed the buoy series to remove missing
values. We compress these time series at various
^a`jb_d as follows. We Ô¨Årst determined the range of
each time series and used 1/1000th of that as our baseline
¬ñ . We compressed the time series
d by multiplying
¬ñ with factors of
¬≤ . We thus covered compression
tolerances from
 of the time series value range.
In Figure 5 we show the
ratio achieved by PMC-MR
and PMC-MEAN over these time series for varying
As expected, this ratio drops as
d increases. The performance of PMC-MEAN is very slightly worse than the optimal PMC-MR algorithm. For the central
d value (1% of
range), the
ratio was on average 8.3% for PMC-MR and
9.4% for PMC-MEAN.
In Figure 6 we show the mean absolute error over all time
positions. This is roughly less than half the
^a`jb_d maximum.
PMC-MEAN and PMC-MR were comparable over the un-
>@?BADCFEHGJILKMADN
Random Walk
[-53.55, 148.35]
Sea Surface Temperature
[25.82, 31.87]
[33.41, 35.28]
Shortwave Radiation
[0, 1351.3]
Table 1. Statistics for Time Series used in our
Compression Experiments
Œµcapt tolerance (% Range)
Random Walk
Œµcapt tolerance (% Range)
Sea Surface Temperature
Œµcapt tolerance (% Range)
Œµcapt tolerance (% Range)
Shortwave Radiation
Figure 5. Compression Performance (K/n ratio)
Mean Error (% Range)
Œµcapt tolerance (% Range)
Random Walk
Mean Error (% Range)
Œµcapt tolerance (% Range)
Sea Surface Temperature
Mean Error (% Range)
Œµcapt tolerance (% Range)
Mean Error (% Range)
Œµcapt tolerance (% Range)
Shortwave Radiation
Figure 6. Compression Performance (Mean
Absolute Error)
biased synthetic data, but with real data, PMC-MEAN had
a slight edge. This is due to better approximation by using
the mean, and to the greater number of segments output by
PMC-MEAN for the same
\_^a`jb_d .
Relative Error (% Range)
Œµcapt tolerance (% Range)
Aggregate Queries
AVG with PMC-MR
AVG with PMC-MEAN
MIN with PMC-MR
MIN with PMC-MEAN
Œµcapt tolerance (% Range)
Selection Queries
False pos. PMC-MR
False pos. PMC-MEAN
False neg. with PMC-MR
False neg. PMC-MEAN
Œµcapt tolerance (% Range)
20-NN Queries
False pos. PMC-MR
False pos. PMC-MEAN
False neg. with PMC-MR
False neg. PMC-MEAN
Figure 7. Answering Queries over
Compressed Time Series
Next, we test how query performance is impacted by
using compressed as opposed to precise time series. We
generate 100 series, each with 1,000 time positions from
the random walk model, choosing the
simulate the difference in values reported by different sensors. We compressed these using the
f¬≤PO and
multiples as before, and asked: (i) 100 queries,
for random time positions of the form ‚ÄúWhat is the minimum and average sensor reading?‚Äù (Aggregate Queries),
(ii) 1,000 queries, one per time position of the form ‚ÄúWhich
sensors‚Äô values are above
¬û ?‚Äù (Selection Queries), where
¬û is uniformly chosen from
for each query, and (iii)
100 queries, asking for the 20 nearest neighbors, in terms of
Euclidean distance, of all 100 time series (20-NN Queries).
The results are shown in Figure 7. We measure, for (i)
the relative error deÔ¨Åned as the fraction of the absolute error over the exact answer, and for (ii) and (iii) the average
number of false positives and false negatives, i.e., number
of time series that should not have been retrieved and number of time series that should have been retrieved but were
not. For aggregate queries, relative error is large only for
aggregate, since the PCA representation consistently overestimates the
7. For the selection queries,
the number of both false positives and false negatives was
small compared to the average query selectivity of
The results are equally good for the 20-NN queries. In fact
PMC-MEAN had no false positives/negatives in this case.
5.2. Prediction Experiments
In our Ô¨Årst experiment, we want to motivate experimentally the need for appropriate model selection as hinted in
7A lower bound on the
can be easily found, given that the PCA
representation has an
√á√àc√â_√ä guarantee
Constant Velocity Time Series
Last Known Value Model
Constant Velocity Model
Constant Acceleration Model
Constant Acceleration Time Series
Last Known Value Model
Constant Velocity Model
Constant Acceleration Model
Figure 8. Model Selection
Œµpred/Œµcapt Ratio
Random Walk
Comp. Only
Pred. Only
Pred. and Comp. of Delta
Pred. and Comp. of S
Œµpred/Œµcapt Ratio
Sea Surface Temperature
Comp. Only
Pred. Only
Pred. and Comp. of Delta
Pred. and Comp. of S
Œµpred/Œµcapt Ratio
Comp. Only
Pred. Only
Pred. and Comp. of Delta
Pred. and Comp. of S
Œµpred/Œµcapt Ratio
Shortwave Radiation
Comp. Only
Pred. Only
Pred. and Comp. of Delta
Pred. and Comp. of S
Figure 9. Prediction and Combined Prediction/Compression Experiments
Section 4.2.2. We consider the location of an object moving in one dimension. This can be captured by a sensor,
either on the object (e.g., GPS) or independent of it (e.g.,
radar). The object may move at a constant speed
√∏ for some
length of time or accelerate/decelerate We generated 100
time series of length 500 for each type of motion, choosing
F . We added some measurement error
F on the location and tried to predict the location as: (i) last known location, (ii) Ô¨Årst-order
model (constant speed), (iii) second-order model (constant
acceleration). We Ô¨Åt these models on the 10 most recent
samples at prediction time. In Figure 8 we show the relative
performance (number of parameter refreshes) using these
three models, for varying
\cb¬ïU¬ñM¬ó ranging from 10 to 160 meters. Not surprisingly, the best predictive model in each case
is the one which generates the behavior. As an example, for
the constant velocity series, the last-know-value is ‚Äútoo simple‚Äù failing to capture the change in the object‚Äôs location,
while the constant acceleration model is ‚Äútoo complex‚Äù and,
despite encompassing the constant velocity model as a special case, fails to outperform it. Our example illustrates the
beneÔ¨Åt of pushing some intelligent behavior to the sensor
and the importance of choosing a model carefully.
In our next experiment, we used a simple predictive
model, namely predicting future values of the series as
equal to its value at prediction time. This is optimal if the
series is undergoing an unbiased random walk, since the expected value of the series at every future time is equal to its
value at prediction time. Using this, each parameter update
consists of a value and the prediction time. It thus has the
same size as a segment of the PCA representation.
We use the same time series as before. We set
¬ñ , i.e., the ‚Äúmiddle‚Äù value of our compression experiments. We simulate for
b¬ïU¬ñM¬ó in 1- to 5-fold multiples of
\U^a`jb_d . To conserve space, we combine a number of curves
in the graphs of Figure 9. ‚ÄúCompression Only‚Äù is the number of segments for PMC-MR compression of
tolerance and ‚ÄúPrediction Only‚Äù is the number of parameter
refreshes when
d , i.e., when prediction alone
is used to capture the time series. As we expect, compression works much better because it compresses values already seen optimally, rather than predicting future (uncertain) values. ‚ÄúPrediction and Compression of S‚Äù is the sum
of a within-
^a`jb_d compression of
and the number of parameter refreshes for
b¬ïj¬ñM¬ó ranging from one to Ô¨Åve times
^a`jb_d . For ‚ÄúPrediction and Compression of Delta‚Äù we use of
the result of Section 4.4 and compress the error series rather
 . As mentioned, when
\cb¬ïj¬ñM¬ó is small, compressing
as opposed to
is preferrable. As
\cb¬ïj¬ñ¬Ñ¬ó increases, the
two curves approach each other. In the Ô¨Årst two time series,
compression of
is slightly preferrable, while in the other
two the situation is reversed.
6. Related Work
Olston et al.
 studies the performance/accuracy
tradeoff with approximately replicated data. The motivation is in reducing communication, quantiÔ¨Åed as the number
of exchanged messages between producer and the receiver
of data. Interval-based approximations are stored at the receiver end, supplied as guarantees by the producer. Our
work differs in employing a general model of approximate
replication which considers temporal latency and combines
compression and prediction. proposes an algorithm for
adaptively setting the interval width; this can also be used
to adaptively set
b¬ïU¬ñM¬ó . The adaptation problem was also
studied by Deolasee et al.
for web data.
Sensor databases have recently been the center of much
research in the database community e.g., in the Cougar 
and Telegraph projects. These efforts aim to create
technology that will enable the creation of databases where
sensors can be accommodated, taking into account the novel
performance and semantic issues that distinguish sensors
from traditional data sources.
Time series data has long been an important area of research. Our paper is not focused in introducing algorithms
for extracting information from time series or in similarity
retrieval as e.g., in Keogh et al. , or Agrawal et al. .
Our focus is in capturing sensor-generated series; applications similar to the above can then be applied to such series
in the archive.
Chen et al. propose compression of databases, motivated by the storage and bandwidth limitations of mobile
devices. Unlike our paper, devices are the destinations of
In Chen et al.
 the problem of database compression and querying over compressed databases is studied. The authors motivate their work by the increase in
CPU power, making it attractive to spend CPU time in compressing/decompressing data rather than in doing disk I/O
for them. Our motivation is similar, making using sensors‚Äô
CPU power to limit communication and energy drain.
In our paper, we use prediction as a means of improving
system performance, namely saving communicationand energy drain. This is different from the common use of prediction in which only the predicted values themselves are
of interested. Gao et al. also proposed to use prediction of time series values. In , the goal is to enable
similarity-based pattern queries in batch mode by Ô¨Ånding
nearest neighbors of an incoming time series. By applying
prediction on this time series, the system can generate candidate nearest neighbors ahead of time. When the actual
values of the incoming series arrive, these are Ô¨Åltered and
the actual nearest neighbors are returned.
Chen et al. propose to perform on-line regression
analysis over time series data streams. We also propose to
Ô¨Åt models to time series, but our motivation is to improve
system performance, rather than regression analysis. A useful extension to our work would be to use some of the ideas
in to address correlations between multiple time series
that a single sensor may be monitoring.
Finally, we refer to work in moving object databases
 . In this research Ô¨Åeld, we Ô¨Ånd the idea of approximating the time series of an object‚Äôs location without
continuous updates, in Wolfson et al. , of predicting an
object‚Äôs future location based on its velocity vector in Saltenis et al. , and of using the predictability of motion for
improving performance in Lazaridis et al. .
7. Conclusions
In this paper we motivate the importance of capturing
time series generated by wireless sensors. To achieve this
we task sensors with compressing time series and Ô¨Åtting
predictive models. We propose an optimal online algorithm
for creating the piecewise-constant approximation of a realvalued time series, satisfying a bound on the
and show how prediction and compression can co-exist in a
system to address the needs of both the time series capture
task and real-time applications.
In the future, we plan to (i) evaluate the effectiveness of
our techniques in a real-world setting, especially for motion
time series, (ii) to examine how to evaluate general SQL
queries with answer quality or response deadline tolerances,
(iii) to develop adaptive algorithms for predictive model selection, and (iv) to investigate lateral communication between sensors, exploiting redundancy of information across
many sensors to further improve performance in the time
series capture setting.
Acknowledgements
Our work was supported by the National Science Foundation (Awards IIS-9996140, IIS-0086124, CCR-0220069,
IIS-0083489) and by the United States Air Force .