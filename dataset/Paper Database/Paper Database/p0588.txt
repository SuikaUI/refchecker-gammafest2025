Machine Learning, 25, 117–149 
c⃝1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
The Power of Amnesia: Learning Probabilistic
Automata with Variable Memory Length
 
Laboratory for Computer Science, MIT, Cambridge, MA 02139
YORAM SINGER
 
AT&T Labs, 600 Mountain Avenue, Murray Hill, NJ 07974
NAFTALI TISHBY
 
Institute of Computer Science, Hebrew University, Jerusalem 91904, Israel
Editor: Thomas Hancock
We propose and analyze a distribution learning algorithm for variable memory length Markov
processes.
These processes can be described by a subclass of probabilistic ﬁnite automata which we name
Probabilistic Sufﬁx Automata (PSA). Though hardness results are known for learning distributions generated by
general probabilistic automata, we prove that the algorithm we present can efﬁciently learn distributions generated
by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by
the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with
high conﬁdence in polynomial time and sample complexity. The learning algorithm is motivated by applications
in human-machine interaction. Here we present two applications of the algorithm. In the ﬁrst one we apply the
algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In
the second application we construct a simple stochastic model for E.coli DNA.
Keywords: Learning distributions, probabilistic automata, Markov models, sufﬁx trees, text correction
Introduction
Statistical modeling of complex sequences is a fundamental goal of machine learning due to
its wide variety of natural applications. The most noticeable examples of such applications
are statistical models in human communication such as natural language, handwriting and
speech , and statistical models of biological sequences such as
DNA and proteins .
These kinds of complex sequences clearly do not have any simple underlying statistical
source since they are generated by natural sources. However, they typically exhibit the
following statistical property, which we refer to as the short memory property.
consider the (empirical) probability distribution on the next symbol given the preceding
subsequence of some given length, then there exists a length L (the memory length) such
that the conditional probability distribution does not change substantially if we condition it
on preceding subsequences of length greater than L.
ThisobservationleadShannon, inhisseminalpaper , tosuggestmodeling
such sequences by Markov chains of order L > 1, where the order is the memory length
of the model. Alternatively, such sequences may be modeled by Hidden Markov Models
D. RON, Y. SINGER AND N. TISHBY
(HMMs) which are more complex distribution generators and hence may capture additional
properties of natural sequences. These statistical models deﬁne rich families of sequence
distributions and moreover, they give efﬁcient procedures both for generating sequences
and for computing their probabilities. However, both models have severe drawbacks. The
size of Markov chains grows exponentially with their order, and hence only very low order
Markov chains can be considered in practical applications. Such low order Markov chains
might be very poor approximators of the relevant sequences. In the case of HMMs, there
are known hardness results concerning their learnability which we discuss in Section 1.1.
In this paper we propose a simple stochastic model and describe its learning algorithm.
It has been observed that in many natural sequences, the memory length depends on the
context and is not ﬁxed. The model we suggest is hence a variant of order L Markov chains,
in which the order, or equivalently, the memory, is variable. We describe this model using
a subclass of Probabilistic Finite Automata (PFA), which we name Probabilistic Sufﬁx
Automata (PSA).
Each state in a PSA is labeled by a string over an alphabet Σ. The transition function
between the states is deﬁned based on these string labels, so that a walk on the underlying
graph of the automaton, related to a given sequence, always ends in a state labeled by
a sufﬁx of the sequence. The lengths of the strings labeling the states are bounded by
some upper bound L, but different states may be labeled by strings of different length,
and are viewed as having varying memory length. When a PSA generates a sequence,
the probability distribution on the next symbol generated is completely deﬁned given the
previously generated subsequence of length at most L. Hence, as mentioned above, the
probability distributions these automata generate can be equivalently generated by Markov
chains of order L, but the description using a PSA may be much more succinct. Since the
size of order L markov chains is exponential in L, their estimation requires data length and
time exponential in L.
In our learning model we assume that the learning algorithm is given a sample (consisting
either of several sample sequences or of a single sample sequence) generated by an unknown
target PSA M of some bounded size. The algorithm is required to output a hypothesis
M, which is not necessarily a PSA but which has the following properties. ˆ
be used both to efﬁciently generate a distribution which is similar to the one generated by
M, and given any sequence s, it can efﬁciently compute the probability assigned to s by
this distribution.
Several measures of the quality of a hypothesis can be considered. Since we are mainly
interested in models for statistical classiﬁcation and pattern recognition, the most natural
measure is the Kullback-Leibler (KL) divergence. Our results hold equally well for the
variation (L1) distance and other norms, which are upper bounded by the KL-divergence.
Since the KL-divergence between Markov sources grows linearly with the length of the
sequence, the appropriate measure is the KL-divergence per symbol. Therefore, we deﬁne
an ϵ-good hypothesis to be an hypothesis which has at most ϵ KL-divergence per symbol
to the target source.
In particular, the hypothesis our algorithm outputs, belongs to a class of probabilistic
machines named Probabilistic Sufﬁx Trees (PST). The learning algorithm grows such a
sufﬁx tree starting from a single root node, and adaptively adds nodes (strings) for which
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
there is strong evidence in the sample that they signiﬁcantly affect the prediction properties
of the tree.
We show that every distribution generated by a PSA can equivalently be generated by
a PST which is not much larger. The converse is not true in general. We can however
characterize the family of PSTs for which the converse claim holds, and in general, it is
always the case that for every PST there exists a not much larger PFA that generates an
equivalent distribution. There are some contexts in which PSAs are preferable, and some
in which PSTs are preferable, and therefore we use both representation in the paper. For
example, PSAs are more efﬁcient generators of distributions, and since they are probabilistic
automata, their well deﬁned state space and transition function can be exploited by dynamic
programming algorithms which are used for solving many practical problems. In addition,
there is a natural notion of the stationary distribution on the states of a PSA which PSTs
lack. On the other hand, PSTs sometimes have more succinct representations than the
equivalent PSAs, and there is a natural notion of growing them.
Stated formally, our main theoretical result is the following. If both a bound L, on
the memory length of the target PSA, and a bound n, on the number of states the target
PSA has, are known, then for every given 0 < ϵ < 1 and 0 < δ < 1, our learning
algorithm outputs an ϵ-good hypothesis PST, with conﬁdence 1 −δ, in time polynomial in
L, n, |Σ|, 1
δ . Furthermore, such a hypothesis can be obtained from a single sample
sequence if the sequence length is also polynomial in a parameter related to the rate in
which the target machine converges to its stationary distribution. Despite an intractability
result concerning the learnability of distributions generated by Probabilistic Finite Automata
 , that is described in Section 1.1, our restricted model can be learned
in a PAC-like sense efﬁciently. This has not been shown so far for any of the more popular
sequence modeling algorithms.
We present two applications of the learning algorithm. In the ﬁrst application we apply
the algorithm in order to construct a model of the English language, and use this model to
correct corrupted text. In the second application we construct a simple stochastic model for
E.coli DNA. Combined with a learning algorithm for a different subclass of probabilistic
automata , the algorithm presented here is part of a complete
cursive handwriting recognition system .
Related Work
The most powerful (and perhaps most popular) model used in modeling natural sequences is
the Hidden Markov Model (HMM). A detailed tutorial on the theory of HMMs as well as selected applications in speech recognition is given by Rabiner . A commonly
used procedure for learning an HMM from a given sample is a maximum likelihood parameter estimation procedure that is based on the Baum-Welch method (which is a special case of the EM (Expectation-Maximization) algorithm
 ). However, this algorithm is guaranteed to converge
only to a local maximum, and thus we are not assured that the hypothesis it outputs can
serve as a good approximation for the target distribution. One might hope that the problem
D. RON, Y. SINGER AND N. TISHBY
can be overcome by improving the algorithm used or by ﬁnding a new approach. Unfortunately, there is strong evidence that the problem cannot be solved efﬁciently.
Abe and Warmuth study the problem of training HMMs. The
HMM training problem is the problem of approximating an arbitrary, unknown source
distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. Gillman and Sipser
 study the problem of exactly inferring an (ergodic) HMM over a
binary alphabet when the inference algorithm can query a probability oracle for the longterm probability of any binary string. They prove that inference is hard: any algorithm
for inference must make exponentially many oracle calls. Their method is information
theoretic and does not depend on separation assumptions for any complexity classes.
Natural simpler alternatives, which are often used as well, are order L Markov chains
 , also known as n-gram models. As noted earlier, the size
of an order L Markov chain is exponential in L and hence, if we want to capture more
than very short term memory dependencies in the sequences, of substantial length in the
sequences, then these models are clearly not practical.
H¨offgen studies families of distributions related to the ones studied in
this paper, but his algorithms depend exponentially and not polynomially on the order, or
memory length, of the distributions. Freund et. al. point out that
their result for learning typical deterministic ﬁnite automata from random walks without
membership queries, can be extended to learning typical PFAs. Unfortunately, there is
strong evidence indicating that the problem of learning general PFAs is hard. Kearns et.
al. show that PFAs are not efﬁciently learnable under the assumption
that there is no efﬁcient algorithm for learning noisy parity functions in the PAC model.
The machines used as our hypothesis representation, namely Probabilistic Sufﬁx Trees
(PSTs), were introduced (in a slightly different form) in and have been
used for other tasks such as universal data compression .
Perhaps the
strongest among these results (which has been brought to our attention after the completion
of this work) and which is most tightly related to our result is . This paper describes an efﬁcient sequential procedure for universal data compression for PSTs by using a larger model class. This algorithm can be viewed as a distribution
learning algorithm but the hypothesis it produces is not a PST or a PSA and hence cannot
be used for many applications. Willems et. al. show that their algorithm can be modiﬁed
to give the minimum description length PST. However, in case the source generating the
examples is a PST, they are able to show that this PST convergence only in the limit of
inﬁnite sequence length to that source.
VitterandKrishnan adaptaversionof
the Ziv-Lempel data compression algorithm to get a page prefetching
algorithm, where the sequence of page accesses is assumed to be generated by a PFA. They
show that the page fault rate of their algorithm converges to the page fault rate of the
best algorithm that has full knowledge of the source. This is true for almost all page
access sequences (in the limit of the sequence length). Laird and Saul 
describe a prediction algorithm which is similar in spirit to our algorithm and is based
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
on the Markov tree or Directed Acyclic Word Graph approach which is used for data
compression . They do not analyze the correctnes of the algorithm formally,
but present several applications of the algorithm.
Overview of the Paper
The paper is organized as follows. In Section 2 we give basic deﬁnitions and notation and
describe the families of distributions studied in this paper, namely those generated by PSAs
and those generated by PSTs. In Section 4 we discuss the relation between the above two
families of distributions. In Section 5 the learning algorithm is described. Some of the
proofs regarding the correctness of the learning algorithm are given in Section 6. Finally,
we demonstrate the applicability of the algorithm by two illustrative examples in Section 7.
In the ﬁrst example we use our algorithm to learn the structure of natural English text, and
use the resulting hypothesis for correcting corrupted text. In the second example we use
our algorithm to build a simple stochastic model for E.coli DNA. The detailed proofs of the
claims presented in Section 4 concerning the relation between PSAs and PSTs are provided
in Appendices A and B. The more technical proofs and lemmas regarding the correctness
of the learning algorithm are given in Appendix C.
Preliminaries
Basic Deﬁnitions and Notations
Let Σ be a ﬁnite alphabet. By Σ∗we denote the set of all possible strings over Σ. For any
integer N, ΣN denotes all strings of length N, and Σ≤N denotes the set of all strings with
length at most N. The empty string is denoted by e. For any string s = s1 . . . sl, si ∈Σ,
we use the following notations:
The longest preﬁx of s different from s is denoted by preﬁx(s)
= s1s2 . . . sl−1.
The longest sufﬁx of s different from s is denoted by suﬃx(s)
= s2 . . . sl−1sl.
The set of all sufﬁxes of s is denoted by Suﬃx ∗(s)
= {si . . . sl | 1 ≤i ≤l} ∪{e}.
A string s′ is a proper sufﬁx of s, if it a sufﬁx of s but is not s itself.
Let s1 and s2 be two strings in Σ∗. If s1 is a sufﬁx of s2 then we shall say that s2 is a
sufﬁx extension of s1.
A set of strings S is called a sufﬁx free set if ∀s ∈S, Suﬃx ∗(s) ∩S = {s}.
D. RON, Y. SINGER AND N. TISHBY
Probabilistic Finite Automata and Prediction Sufﬁx Trees
Probabilistic Finite Automata
A Probabilistic Finite Automaton (PFA) M is a 5-tuple (Q, Σ, τ, γ, π), where Q is a ﬁnite
set of states, Σ is a ﬁnite alphabet, τ : Q × Σ →Q is the transition function, γ : Q × Σ →
 is the next symbol probability function, and π : Q → is the initial probability
distribution over the starting states. The functions γ and π must satisfy the following
conditions: for every q ∈Q, P
σ∈Σ γ(q, σ) = 1, and P
q∈Q π(q) = 1. We assume that
the transition function τ is deﬁned on all states q and symbols σ for which γ(q, σ) > 0,
and on no other state-symbol pairs. τ can be extended to be deﬁned on Q × Σ∗as follows:
τ(q, s1s2 . . . sl) = τ(τ(q, s1 . . . sl−1), sl) = τ(τ(q, preﬁx(s)), sl).
A PFA M generates strings of inﬁnite length, but we shall always discuss probability
distributions induced on preﬁxes of these strings which have some speciﬁed ﬁnite length.
If PM is the probability distribution M deﬁnes on inﬁnitely long strings, then P N
M, for any
N ≥0, will denote the probability induced on strings of length N. We shall sometimes
drop the superscript N, assuming that it is understood from the context. The probability
that M generates a string r = r1r2 . . . rN in ΣN is
γ(qi−1, ri) ,
where qi+1 = τ(qi, ri).
Probabilistic Sufﬁx Automata
We are interested in learning a subclass of PFAs which we name Probabilistic Sufﬁx Automata (PSA). These automata have the following property. Each state in a PSA M is
labeled by a string of ﬁnite length in Σ∗. The set of strings labeling the states is sufﬁx free.
For every two states q1, q2 ∈Q and for every symbol σ ∈Σ, if τ(q1, σ) = q2 and q1 is
labeled by a string s1, then q2 is labeled by a string s2 which is a sufﬁx of s1·σ. In order
that τ be well deﬁned on a given set of strings S, not only must the set be sufﬁx free, but it
must also have the following property. For every string s in S labeling some state q, and
every symbol σ for which γ(q, σ) > 0, there exists a string in S which is a sufﬁx of sσ.
For our convenience, from this point on, if q is a state in Q then q will also denote the string
labeling that state.
We assume that the underlying graph of M, deﬁned by Q and τ(·, ·), is strongly connected,
i.e., for every pair of states q and q′ there is a directed path from q to q′. Note that in our
deﬁnition of PFAs we assumed that the probability associated with each transition (edge in
the underlying graph) is non-zero, and hence strong connectivity implies that every state
can be reached from every other state with non-zero probability. For simplicity we assume
M is aperiodic, i.e., that the greatest common divisor of the lengths of the cycles in its
underlying graph is 1. These two assumptions ensure us that M is ergodic. Namely, there
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
exists a distribution ΠM on the states such that for every state we may start at, the probability
distribution on the state reached after time t as t grows to inﬁnity, converges to ΠM. The
probability distribution ΠM is the unique distribution satisfying
q′ s.t. τ(q′,σ)=q
ΠM(q′)γ(q′, σ) ,
and is named the stationary distribution of M. We ask that for every state q in Q, the
initial probability of q, π(q), be the stationary probability of q, ΠM(q). It should be noted
that the assumptions above are needed only when learning from a single sample string and
not when learning from many sample strings. However, for sake of brevity we make these
requirements in both cases.
For any given L ≥0, the subclass of PSAs in which each state is labeled by a string of
length at most L is denoted by L-PSA. An example 2-PSA is depicted in Figure 1. A special
case of these automata is the case in which Q includes all strings in ΣL. An example of
such a 2-PSA is depicted in Figure 1 as well. These automata can be described as Markov
chains of order L. The states of the Markov chain are the symbols of the alphabet Σ, and
the next state transition probability depends on the last L states (symbols) traversed. Since
every L-PSA can be extended to a (possibly much larger) equivalent L-PSA whose states
are labeled by all strings in ΣL, it can always be described as a Markov chain of order L.
Alternatively, since the states of an L-PSA might be labeled by only a small subset of Σ≤L,
and many of the sufﬁxes labeling the states may be much shorter than L, it can be viewed
as a Markov chain with variable order, or variable memory.
Learning Markov chains of order L, i.e., L-PSAs whose states are labeled by all ΣL
strings, is straightforward (though it takes time exponential in L). Since the ‘identity’ of
the states (i.e., the strings labeling the states) is known, and since the transition function
τ is uniquely deﬁned, learning such automata reduces to approximating the next symbol
probability function γ. For the more general case of L-PSAs in which the states are labeled
by strings of variable length, the task of an efﬁcient learning algorithm is much more
involved since it must reveal the identity of the states as well.
Prediction Sufﬁx Trees
Though we are interested in learning PSAs, we choose as our hypothesis class the class
of prediction sufﬁx trees (PST) deﬁned in this section. We later show (Section 4) that for
every PSA there exists an equivalent PST of roughly the same size.
A PST T, over an alphabet Σ, is a tree of degree |Σ|. Each edge in the tree is labeled by
a single symbol in Σ, such that from every internal node there is exactly one edge labeled
by each symbol. The nodes of the tree are labeled by pairs (s, γs) where s is the string
associated with the walk starting from that node and ending in the root of the tree, and
γs : Σ → is the next symbol probability function related with s. We require that for
every string s labeling a node in the tree, P
σ∈Σ γs(σ) = 1.
As in the case of PFAs, a PST T generates strings of inﬁnite length, but we consider the
probability distributions induced on ﬁnite length preﬁxes of these strings. The probability
that T generates a string r = r1r2 . . . rN in ΣN is
D. RON, Y. SINGER AND N. TISHBY
T (r) = ΠN
i=1γsi−1(ri) ,
where s0 = e, and for 1 ≤j ≤N −1, sj is the string labeling the deepest node reached
by taking the walk corresponding to riri−1 . . . r1 starting at the root of T. For example,
using the PST depicted in Figure 1, the probability of generating the string 00101, is
0.5 × 0.5 × 0.25 × 0.5 × 0.75, and the labels of the nodes that are used for the prediction
are s0 = e, s1 = 0, s2 = 00, s3 = 1, s4 = 10. In view of this deﬁnition, the requirement
that every internal node have exactly |Σ| sons may be loosened, by allowing the omission
of nodes labeled by substrings which are generated by the tree with probability 0.
PSTs therefore generate probability distributions in a similar fashion to PSAs. As in the
case of PSAs, symbols are generated sequentially and the probability of generating a symbol
depends only on the previously generated substring of some bounded length. In both cases
there is a simple procedure for determining this substring, as well as for determining the
probability distribution on the next symbol conditioned on the substring. However, there
are two (related) differences between PSAs and PSTs. The ﬁrst is that PSAs generate each
symbol simply by traversing a single edge from the current state to the next state, while for
each symbol generated by a PST, one must walk down from the root of the tree, possibly
traversing L edges. This implies that PSAs are more efﬁcient generators. The second
difference is that while in PSAs for each substring (state) and symbol, the next state is
well deﬁned, in PSTs this property does not necessarily hold. Namely, given the current
generating node of a PST, and the next symbol generated, the next node is not necessarily
uniquely deﬁned, but might depend on previously generated symbols which are not included
in the string associated with the current node. For example, assume we have a tree whose
leaves are: 1,00,010,110 (see Figure B.1 in Appendix B). If 1 is the current generating
leaf and it generates 0, then the next generating leaf is either 010 or 110 depending on the
symbol generated just prior to 1.
PSTs, like PSAs, can always be described as Markov chains of (ﬁxed) ﬁnite order, but as
in the case of PSAs this description might be exponentially large.
We shall sometimes want to discuss only the structure of a PST and ignore its prediction
property. In other words, we will be interested only in the string labels of the nodes and not
in the values of γs(·). We refer to such trees as sufﬁx trees. We now introduce two more
notations. The set of leaves of a sufﬁx tree T is denoted by L(T), and for a given string s
labeling a node v in T, T(s) denotes the subtree rooted at v.
The Learning Model
The learning model described in this paper is motivated by the PAC model for learning boolean concepts from labeled examples and is similar in spirit to that introduced in
 . We start by deﬁning an ϵ-good hypothesis PST with respect to a
given PSA.
Deﬁnition. Let M be a PSA and let T be a PST. Let PM and PT be the two probability
distributions they generate respectively. We say that T is an ϵ-good hypothesis with respect
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
π(00)=0.25
π(10)=0.25
π(00)=0.25
π(10)=0.25
π(11)=0.25
π(01)=0.25
(0.25,0.75)
(0.75,0.25)
Figure 1. Left: A 2-PSA. The strings labeling the states are the sufﬁxes corresponding to them. Bold edges
denote transitions with the symbol ‘1’, and dashed edges denote transitions with ‘0’. The transition probabilities
are depicted on the edges. Middle: A 2-PSA whose states are labeled by all strings in {0, 1}2. The strings
labeling the states are the last two observed symbols before the state was reached, and hence it can be viewed as
a representation of a Markov chain of order 2. Right: A prediction sufﬁx tree. The prediction probabilities of the
symbols ‘0’ and ‘1’, respectively, are depicted beside the nodes, in parentheses. The three models are equivalent
in the sense that they induce the same probability distribution on strings from {0, 1}⋆.
to M, if for every N > 0,
M(r) log P N
is the Kullback-Leibler divergence between the two distributions.
In this deﬁnition we chose the Kullback-Leibler (KL) divergence as a distance measure
between distributions. Similar deﬁnitions can be considered for other distance measures
such as the variation and the quadratic distances. Note that the KL-divergence bounds the
variation distance as follows : DKL[P1||P2] ≥
2||P1 −P2||2
Since the L1 norm bounds the L2 norm, the last bound holds for the quadratic distance
Note that the KL-divergence between distributions, generated by ﬁnite order
markov chains, is proportional to the length of the strings over which the divergence is
computed, when this length is longer than the order of the model. Hence, to obtain a
measure independent of that length it is necessary to divide the KL-divergence by the
length of the strings, N.
A learning algorithm for PSAs is given the maximum length L of the strings labeling
the states of the target PSA M, and an upper bound n on the number of states in M. The
algorithm is also given a conﬁdence (security) parameter 0 < δ < 1 and an approximation
parameter 0 < ϵ < 1. We analyze the following two learning scenarios. In the ﬁrst
scenario the algorithm has access to a source of sample strings of minimal length L + 1,
independently generated by M. In the second scenario it is given only a single (long)
sample string generated by M. In both cases we require that it output a hypothesis PST ˆT,
which with probability at least 1 −δ is an ϵ-good hypothesis with respect to M.
The only drawback to having a PST as our hypothesis instead of a PSA (or more generally
a PFA), is that the prediction procedure using a tree is somewhat less efﬁcient (by at most
D. RON, Y. SINGER AND N. TISHBY
a factor of L). Since no transition function is deﬁned, in order to predict/generate each
symbol, we must walk from the root until a leaf is reached. As mentioned earlier, we show
in Appendix B that every PST can be transformed into an equivalent PFA which is not much
larger. This PFA differs from a PSA only in the way it generates the ﬁrst L symbols. We
also show that if the PST has a certain property (deﬁned in Appendix B), then it can be
transformed into an equivalent PSA.
In order to measure the efﬁciency of the learning algorithm, we separate the case in which
the algorithm is given a sample consisting of independently generated sample strings, from
the case in which it is given a single sample string. In the ﬁrst case we say that the learning
algorithm is efﬁcient if it runs in time polynomial in L, n, |Σ|, 1
δ . In order to deﬁne
efﬁciency in the latter case we need to take into account an additional property of the model
– its mixing or convergence rate. To do this we next discuss another parameter of PSAs
(actually, of PFAs in general).
For a given PSA, M, let RM denote the n × n stochastic transition matrix deﬁned by
τ(·, ·) and γ(·, ·) when ignoring the transition labels. That is, if si and sj are states in M
and the last symbol in sj is σ, then RM(si, sj) is γ(si, σ) if τ(si, σ) = sj, and 0 otherwise.
Hence, RM is the transition matrix of an ergodic Markov chain.
Let ˜RM denote the time reversal of RM. That is,
˜RM(si, sj) = ΠM(sj)RM(sj, si)
where ΠM is the stationary probability vector of RM as deﬁned in Equation (2). Deﬁne
the multiplicative reversiblization UM of M by UM = RM ˜RM. Denote the second largest
eigenvalue of UM by λ2(UM).
If the learning algorithm receives a single sample string, we allow the length of the string
(and hence the running time of the algorithm) to be polynomial not only in L, n, |Σ|, 1
δ , but also in 1/(1 −λ2(UM)). The rationale behind this is roughly the following.
In order to succeed in learning a given PSA, we must observe each state whose stationary
probability is non-negligible enough times so that the algorithm can identify that the state
is signiﬁcant, and so that the algorithm can compute (approximately) the next symbol
probability function. When given several independently generated sample strings, we can
easily bound the size of the sample needed by a polynomial in L, n, |Σ|, 1
Chernoff bounds. When given one sample string, the given string must be long enough so as
to ensure convergence of the probability of visiting a state to the stationary probability. We
show that this convergence rate can be bounded using algebraic properties of UM, namely,
its second largest eigenvalue .
Emulation of PSAs by PSTs
In this section we show that for every PSA there exists an equivalent PST which is not
much larger. This allows us to consider the PST equivalent to our target PSA, whenever it
is convenient.
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
Theorem 1 For every L-PSA, M = (Q, Σ, τ, γ, π), there exists an equivalent PST TM,
of maximal depth L and at most L · |Q| nodes.
Proof: (Sketch) We describe below the construction needed to prove the claim.
complete proof is provided in Appendix A.
Let TM be the tree whose leaves correspond to the strings in Q. For each leaf s, and
for every symbol σ, let γs(σ) = γ(s, σ). This ensures that for every given string s which
is a sufﬁx extension of a leaf in TM, and for every symbol σ, PM(σ|s) = PTM (σ|s). It
remains to deﬁne the next symbol probability functions for the internal nodes of TM. These
functions must be deﬁned so that TM generates all strings related to its nodes with the same
probability as M.
For each node s in the tree, let the weight of s, denoted by ws, be ws
s′∈Q, s∈Suﬃx ∗(s′) π(s′). In other words, the weight of a leaf in TM is the stationary
probability of the corresponding state in M; and the weight of an internal node labeled by
a string s, equals the sum of the stationary probabilities over all states of which s is a sufﬁx
(which also equals the sum of the weights of the leaves in the subtree rooted at the node).
Using the weights of the nodes we assign values to the γs’s of the internal nodes s in the tree
in the following manner. For every symbol σ let γs(σ) = P
s′∈Q, s∈Suﬃx ∗(s′)
ws γ(s′, σ).
The probability γs(σ), of generating a symbol σ following a string s, shorter than any state
in M, is thus a weighted average of γ(s′, σ) taken over all states s′ which correspond to
sufﬁx extensions of s. The weight related with each state in this average, corresponds to
its stationary probability. As an example, the probability distribution over the ﬁrst symbol generated by TM, is P
s∈Q π(s)γ(s, ·). This probability distribution is equivalent, by
deﬁnition, to the probability distribution over the ﬁrst symbol generated by M.
Finally, if for some internal node in TM, its next symbol probability function is equivalent
to the next symbol probability functions of all of its descendants, then we remove all its
descendants from the tree.
An example of the construction described in the proof of Theorem 1 is illustrated in
Figure 1. The PST on the right was constructed based on the PSA on the left, and is
equivalent to it. Note that the next symbol probabilities related with the leaves and the
internal nodes of the tree are as deﬁned in the proof of the theorem.
The Learning Algorithm
We start with an overview of the algorithm. Let M = (Q, Σ, τ, γ, π) be the target L-PSA
we would like to learn, and let |Q| ≤n. According to Theorem 1, there exists a PST T, of
size bounded by L · |Q|, which is equivalent to M. We use the sample statistics to deﬁne
the empirical probability function, ˜P(·), and using ˜P, we construct a sufﬁx tree, ¯T, which
with high probability is a subtree of T. We deﬁne our hypothesis PST, ˆT, based on ¯T and
The construction of ¯T is done as follows. We start with a tree consisting of a single node
(labeled by the empty string e) and add nodes which we have reason to believe should
be in the tree. A node v labeled by a string s is added as a leaf to ¯T if the following
holds. The empirical probability of s, ˜P(s), is non-negligble, and for some symbol σ, the
D. RON, Y. SINGER AND N. TISHBY
empirical probability of observing σ following s, namely ˜P(σ|s), differs substantially from
the empirical probability of observing σ following suﬃx(s), namely ˜P(σ|suﬃx(s)). Note
that suﬃx(s) is the string labeling the parent node of v. Our decision rule for adding v,
is thus dependent on the ratio between ˜P(σ|s) and ˜P(σ|suﬃx(s)). We add a given node
only when this ratio is substantially greater than 1. This sufﬁces for our analysis (due to
properties of the KL-divergence), and we need not add a node if the ratio is smaller than 1.
Thus, we would like to grow the tree level by level, adding the sons of a given leaf in the
tree, only if they exhibit such a behavior in the sample, and stop growing the tree when the
above is not true for any leaf. The problem is that the node might belong to the tree even
though its next symbol probability function is equivalent to that of its parent node. The
leaves of a PST must differ from their parents (or they are redundant) but internal nodes
might not have this property. The PST depicted in Figure 1 illustrates this phenomena. In
this example, γ0(·) ≡γe(·), but both γ00(·) and γ10(·) differ from γ0(·). Therefore, we
must continue testing further potential descendants of the leaves in the tree up to depth L.
As mentioned before, we do not test strings which belong to branches whose empirical count in the sample is small.
This way we avoid exponential grow-up in the
number of strings tested.
A similar type of branch-and-bound technique (with various bounding criteria) is applied in many algorithms which use trees as data structures
 ). The set of strings tested at each step, denoted by ¯S,
can be viewed as a kind of potential frontier of the growing tree ¯T, which is of bounded size.
After the construction of ¯T is completed, we deﬁne ˆT by adding nodes so that all internal
nodes have full degree, and deﬁning the next symbol probability function for each node
based on ˜P. These probability functions are deﬁned so that for every string s in the tree and
for every symbol σ, γs(σ) is bounded from below by γmin which is a parameter that is set
subsequently. This is done by using a conventional smoothing technique. Such a bound on
γs(σ) is needed in order to bound the KL-divergence between the target distribution and
the distribution our hypothesis generates.
The above scheme follows a top-down approach since we start with a tree consisting of a
single root node and a frontier consisting only of its children, and incrementally grow the
sufﬁx tree ¯T and the frontier ¯S. Alternatively, a bottom-up procedure can be devised. In
a bottom-up procedure we start by putting in ¯S all strings of length at most L which have
signiﬁcant counts, and setting ¯T to be the tree whose nodes correspond to the strings in
¯S. We then trim ¯T starting from its leaves and proceeding up the tree by comparing the
prediction probabilities of each node to its parent node as done in the top-down procedure.
The two schemes are equivalent and yield the same prediction sufﬁx tree. However, we ﬁnd
the incremental top-down approach somewhat more intuitive, and simpler to implement.
Moreover, our top-down procedure can be easily adapted to an online setting which is useful
in some practical applications.
Let P denote the probability distribution generated by M. We now formally deﬁne the
empirical probability function ˜P, based on a given sample generated by M. For a given
string s, ˜P(s) is roughly the relative number of times s appears in the sample, and for any
symbol σ, ˜P(σ|s) is roughly the relative number of times σ appears after s. We give a more
precise deﬁnition below.
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
If the sample consists of one sample string r of length m, then for any string s of length
at most L, deﬁne χj(s) to be 1 if rj−|s|+1 . . . rj = s and 0 otherwise. Let
and for any symbol σ, let
j=L χj+1(sσ)
If the sample consists of m′ sample strings r1, . . . , rm′, each of length ℓ≥L + 1, then for
any string s of length at most L, deﬁne χi
j(s) to be 1 if ri
j−|s|+1 . . . ri
j = s, and 0 otherwise.
and for any symbol σ, let
j=L χj+1(sσ)
For simplicity we assume that all the sample strings have the same length and that this
length is polynomial in n, L, and Σ. The case in which the sample strings are of different
lengths can be treated similarly, and if the strings are too long then we can ignore parts of
In the course of the algorithm and in its analysis we refer to several parameters which are
all simple functions of ϵ, n, L and |Σ|, and are set as follows:
2nL log(1/γmin) =
2nL log(48L|Σ|/ϵ) ,
= L log(48L|Σ|/ϵ)|Σ|
The size of the sample is set in the analysis of the algorithm.
A pseudo code describing the learning algorithm is given in Figure 2 and an illustrative
run of the algorithm is depicted in Figure 3.
D. RON, Y. SINGER AND N. TISHBY
Algorithm Learn-PSA
Initialize ¯T and ¯S: let ¯T consist of a single root node (corresponding to e), and let
¯S ←{σ | σ ∈Σ and ˜P(σ) ≥(1 −ϵ1)ϵ0}.
While ¯S ̸= ∅, pick any s ∈¯S and do:
(A) Remove s from ¯S;
(B) If there exists a symbol σ ∈Σ such that
˜P(σ|s) ≥(1 + ϵ2)γmin and ˜P(σ|s)/ ˜P(σ|suﬃx(s)) > 1 + 3ϵ2 ,
then add to ¯T the node corresponding to s and all the nodes on the path from the
deepest node in ¯T that is a sufﬁx of s, to s;
(C) If |s| < L then for every σ′ ∈Σ, if ˜P(σ′·s) ≥(1 −ϵ1)ϵ0, then add σ′·s to ¯S.
Initialize ˆT to be ¯T.
Extend ˆT by adding all missing sons of internal nodes.
For each s labeling a node in ˆT, let
ˆγs(σ) = ˜P(σ|s′)(1 −|Σ|γmin) + γmin ,
where s′ is the longest sufﬁx of s in ¯T.
Figure 2. Algorithm Learn-PSA
Analysis of the Learning Algorithm
In this section we state and prove our main theorem regarding the correctness and efﬁciency
of the learning algorithm Learn-PSA, described in Section 5.
Theorem 2 For every target PSA M, and for every given security parameter 0 < δ < 1,
and approximation parameter 0 < ϵ < 1, Algorithm Learn-PSA outputs a hypothesis PST,
ˆT, such that with probability at least 1 −δ:
1. ˆT is an ϵ-good hypothesis with respect to M.
2. The number of nodes in ˆT is at most |Σ| · L times the number of states in M.
If the algorithm has access to a source of independently generated sample strings, then
its running time is polynomial in L, n, |Σ|, 1
δ . If the algorithm has access to only
one sample string, then its running time is polynomial in the same parameters and in
1/(1 −λ2(UM)).
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
000 (0.8,0.2)
000 (0.8,0.2)
Figure 3. An illustrative run of the learning algorithm. The prediction sufﬁx trees created along the run of the
algorithm are depicted from left to right and top to bottom. At each stage of the run, the nodes from ¯T are plotted
in dark grey while the nodes from ¯S are plotted in light grey. The alphabet is binary and the predictions of the next
bit are depicted in parenthesis beside each node. The ﬁnal tree is plotted on the bottom right part and was built by
adding to ¯T (bottom left) all missing children. Note that the node labeled by 100 was added to the ﬁnal tree but
is not part of any of the intermediate trees. This can happen when the probability of the string 100 is small.
D. RON, Y. SINGER AND N. TISHBY
In order to prove the theorem above we ﬁrst show that with probability1−δ, a large enough
sample generated according to M is typical to M, where typical is deﬁned subsequently.
We then assume that our algorithm in fact receives a typical sample and prove Theorem 2
based on this assumption. Roughly speaking, a sample is typical if for every substring
generated with non-negligible probability by M, the empirical counts of this substring and
of the next symbol given this substring, are not far from the corresponding probabilities
deﬁned by M.
Deﬁnition. A sample generated according to M is typical if for every string s ∈Σ≤L the
following two properties hold:
1. If s ∈Q then | ˜P(s) −π(s)| ≤ϵ1ϵ0;
2. If ˜P(s) ≥(1 −ϵ1)ϵ0 then for every σ ∈Σ, | ˜P(σ|s) −P(σ|s)| ≤ϵ2γmin;
Where ϵ0, ϵ1, ϵ2, and γmin were deﬁned in Section 5.
1. There exists a polynomial m′
0 in L, n, |Σ|, 1
δ , such that the probability that a
sample of m′ ≥m′
0(L, n, |Σ|, 1
δ ) strings each of length at least L + 1 generated
according to M is typical is at least 1 −δ.
2. There exists a polynomial m0 in L, n, |Σ|, 1
δ , and 1/(1 −λ2(UM)), such that
the probability that a single sample string of length m ≥m0(L, n, |Σ|, 1
δ , 1/(1 −
λ2(UM))) generated according to M is typical is at least 1 −δ.
The proof of Lemma 1 is provided in Appendix C.
Let T be the PST equivalent to the target PSA M, as deﬁned in Theorem 1. In the next
lemma we prove two claims. In the ﬁrst claim we show that the prediction properties of
our hypothesis PST ˆT, and of T, are similar. We use this in the proof of the ﬁrst claim in
Theorem 2, when showing that the KL-divergence per symbol between ˆT and M is small.
In the second claim we give a bound on the size of ˆT in terms of T, which implies a similar
relation between ˆT and M (second claim in Theorem 2).
Lemma 2 If Learn-PSA is given a typical sample then:
1. For every string s in T, if P(s) ≥ϵ0 then γs(σ)
ˆγs′(σ) ≤1 + ϵ/2 , where s′ is the longest
sufﬁx of s corresponding to a node in ˆT.
2. | ˆT| ≤(|Σ| −1) · |T|.
Proof: (Sketch, the complete proofs of both claims are provided in Appendix C.)
In order to prove the ﬁrst claim, we argue that if the sample is typical, then there cannot
exist such strings s and s′ which falsify the claim. We prove this by assuming that there
exists such a pair, and reaching contradiction. Based on our setting of the parameters ϵ2
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
and γmin, we show that for such a pair, s and s′, the ratio between γs(σ) and γs′(σ)
must be bounded from below by 1 + ϵ/4. If s = s′, then we have already reached a
contradiction. If s ̸= s′, then we can show that the algorithm must add some longer sufﬁx
of s to ¯T, contradicting the assumption that s′ is the longest sufﬁx of s corresponding
to a node in ˆT.
In order to bound the size of ˆT, we show that ¯T is a subtree of T.
This sufﬁces to prove the second claim, since when transforming ¯T into ˆT, we add at
most all |Σ| −1 siblings of every node in ¯T. We prove that ¯T is a subtree of T, by
arguing that in its construction, we did not add any string which does not correspond to
a node in T. This follows from the decision rule according to which we add nodes to ¯T.
Proof of Theorem 2: According to Lemma 1, with probability at least 1−δ our algorithm
receives a typical sample. Thus according to the second claim in Lemma 2, | ˆT| ≤(|Σ| −
1)·|T| and since |T| ≤L·|Q|, then | ˆT| ≤|Σ|·L·|Q| and the second claim in the theorem
Let r = r1r2 . . . rN, where ri ∈Σ, and for any preﬁx r(i) of r, where r(i) = r1 . . . ri,
let s[r(i)] and ˆs[r(i)] denote the strings corresponding to the deepest nodes reached upon
taking the walk ri . . . r1 on T and ˆT respectively. In particular, s[r(0)] = ˆs[r(0)] = e. Let
ˆP denote the probability distribution generated by ˆT. Then
P(r) log P(r)
P(r) · log
i=1 γs[r(i−1)](ri)
i=1 ˆγˆs[r(i−1)](ri)
log γs[r(i−1)](ri)
ˆγˆs[r(i−1)](ri)
P (s[r(i−1)])<ϵ0
P(r) · log γs[r(i−1)](ri)
ˆγˆs[r(i−1)](ri)
P (s[r(i−1)])≥ϵ0
P(r) · log γs[r(i−1)](ri)
ˆγˆs[r(i−1)](ri)
For every 1 ≤i ≤N, the ﬁrst term in the parenthesis in Equation (8d) can be bounded as
follows. For each string r, the worst possible ratio between γs[r(i−1)](ri) and ˆγˆs[r(i−1)](ri),
is 1/γmin. The total weight of all strings in the ﬁrst term equals the total weight of all
the nodes in T whose weight is at most ϵ0, which is at most nLϵ0. The ﬁrst term is thus
bounded by nLϵ0 log(1/γmin). Based on Lemma 2, the ratio between γs[r(i−1)](ri) and
ˆγˆs[r(i−1)](ri) for every string r in the second term in the parenthesis, is at most 1 + ϵ/2.
Since the total weight of all these strings is bounded by 1, the second term is bounded by
log(1 + ϵ/2). Combining the above with the value of ϵ0 (that was set in Section 5 to be
ϵ/ (2nL log(1/γmin)) ), we get that,
D. RON, Y. SINGER AND N. TISHBY
N DKL[P N|| ˆP N] ≤
N · N [n L ϵ0 log
+ log(1 + ϵ/2)] ≤ϵ .
Using a straightforward implementation of the algorithm, we can get a (very rough) upper
bound on the running time of the algorithm which is of the order of the square of the size
of the sample times L. In this implementation, each time we add a string s to ¯S or to ¯T,
we perform a complete pass over the given sample to count the number of occurrences
of s in the sample and its next symbol statistics. According to Lemma 1, this bound is
polynomial in the relevant parameters, as required in the theorem statement. Using the
following more time-efﬁcient, but less space-efﬁcient implementation, we can bound the
running time of the algorithm by the size of the sample times L. For each string in ¯S, and
each leaf in ¯T we keep a set of pointers to all the occurrences of the string in the sample.
For such a string s, if we want to test which of its extensions, σs should we add to ¯S or
to ¯T, we need only consider all occurrences of s in the sample (and then distribute them
accordingly among the strings added). For each symbol in the sample there is a single
pointer, and each pointer corresponds to a single string of length i for every 1 ≤i ≤L.
Thus the running time of the algorithm is of the order of the size of the sample times L.
Applications
A slightly modiﬁed version of our learning algorithm was applied and tested on various problemssuchas: correctingcorruptedtext, predictingDNAbases ,
and part-of-speech disambiguation resolving . We are still exploring other possible applications of the algorithm. Here we demonstrate how the algorithm
can be used to correct corrupted text and how to build a simple model for DNA strands.
Correcting Corrupted Text
In many machine recognition systems such as speech or handwriting recognizers, the recognition scheme is divided into two almost independent stages. In the ﬁrst stage a low-level
model is used to perform a (stochastic) mapping from the observed data (e.g., the acoustic
signal in speech recognition applications) into a high level alphabet. If the mapping is
accurate then we get a correct sequence over the high level alphabet, which we assume
belongs to a corresponding high level language. However, it is very common that errors
in the mapping occur, and sequences in the high level language are corrupted. Much of
the effort in building recognition systems is devoted to correct the corrupted sequences. In
particular, in many optical and handwriting character recognition systems, the last stage
employs natural-language analysis techniques to correct the corrupted sequences. This can
be done after a good model of the high level language is learned from uncorrupted examples
of sequences in the language. We now show how to use PSAs in order to perform such a
We applied the learning algorithm to the bible. The alphabet was the english letters and
the blank character. We removed Jenesis and it served as a test set. The algorithm was
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
applied to the rest of the books with L = 30, and the accuracy parameters (ϵi) were of
N), where N is the length of the training data. This resulted in a PST having
less than 3000 nodes. This PST was transformed into a PSA in order to apply an efﬁcient
text correction scheme which is described subsequently. The ﬁnal automaton constitutes
both of states that are of length 2, like ‘qu’ and ‘xe’, and of states which are 8 and 9
symbols long, like ‘shall be’ and ‘there was’. This indicates that the algorithm really
captures the notion of variable memory that is needed in order to have accurate predictions.
Building a Markov chain of order L in this case is clearly not practical since it requires
|Σ|L = 279 = 7625597484987 states!
Let ¯r = (r1, r2, . . . , rt) be the observed (corrupted) text. If an estimation of the corrupting noise probability is given, then we can calculate for each state sequence ¯q =
(q0, q1, q2, . . . , qt), qi ∈Q, the probability that ¯r was created by a walk over the PSA
which constitutes of the states ¯q. For 0 ≤i ≤t, let Xi be a random variable over Q, where
Xi = q denotes the event that the ith state passed was q. For 1 ≤i ≤t let Yi be a random
variable over Σ, where Yi = σ denotes the event that the ith symbol observed was σ. For
¯q ∈Qt+1, let ¯X = ¯q denote the joint event that Xi = qi for every 0 ≤i ≤t, and for
¯r ∈Σt, let ¯Y = ¯r denote the joint event that Yi = ri for every 1 ≤i ≤t. If we assume
that the corrupting noise is i.i.d and is independent of the states that constitute the walk,
then the most likely state sequence, ¯qML, is
¯qML = arg max
¡ ¯X = ¯q| ¯Y = ¯r
¡ ¯Y = ¯r| ¯X = ¯q
P( ¯X = ¯q) (10a)
Yi = ri| ¯X = ¯q
P (Xi = qi|Xi−1 = qi−1)
log (P (Yi = ri|Xi = qi) + log(π(q0)) +
log (P (Xi = qi|Xi−1 = qi−1))
where for deriving the last Equality (10c) we used the monotonicity of the log function
and the fact that the corruption noise is independent of the states. Let the string labeling
qi be s1, . . . , sl. Then P(Yi = ri|Xi = qi) is the probability that ri is an uncorrupted
symbol if ri = sl, and is the probability that the noise process ﬂipped sl to be ri otherwise.
Note that the sum (10c) can be computed efﬁciently in a recursive manner. Moreover, the
maximization of Equation (10a) can be performed efﬁciently by using a dynamic programming (DP) scheme . This scheme requires O(|Q| × t) operations. If |Q| is
large, then approximation schemes to the optimal DP, such as the stack decoding algorithm
 can be employed. Using similar methods it is also possible to correct errors
when insertions and deletions of symbols occur as well.
We tested the algorithm by taking a text from Jenesis and corrupting it in two ways. First,
we altered every letter (including blanks) with probability 0.2. In the second test we altered
D. RON, Y. SINGER AND N. TISHBY
every letter with probability 0.1 and we also changed each blank character, in order to test
whether the resulting model is powerful enough to cope with non-uniform noise. The result
of the correction algorithm for both cases as well as the original and corrupted texts are
depicted in Figure 4.
Original Text:
and god called the dry land earth and the gathering together of the waters called he seas and
god saw that it was good and god said let the earth bring forth grass the herb yielding seed
and the fruit tree yielding fruit after his kind
Corrupted text (1):
and god cavsed the drxjland earth ibd shg gathervng together oj the waters cﬂled re seas
aed god saw thctpit was good ann god said let tae earth bring forth gjasb tse hemb yielpinl
peed and thesfruit tree sielxing fzuitnafter his kind
Corrected text (1):
and god caused the dry land earth and she gathering together of the waters called he sees
and god saw that it was good and god said let the earth bring forth grass the memb yielding
peed and the fruit tree ﬁelding fruit after his kind
Corrupted text (2):
andhgodpcilledjthesdryjlandbeasthcandmthelgatceringhlogetherjfytrezaatersoczlled
xherseasaknddgodbsawwthathitqwasoqoohanwzgodcsaidhletdtheuejrthriringmforth
hbgrasstthexherbyieldingzseedmazdctcybfruitttreeayieldinglfruztbafherihiskind
Corrected text (2):
and god called the dry land earth and the gathering together of the altars called he seasaked
god saw that it was took and god said let the earthriring forth grass the herb yielding seed
and thy fruit treescielding fruit after his kind
Figure 4. Correcting corrupted text.
We compared the performance of the PSA we constructed to the performance of Markov
chains of order 0 – 3. The performance is measured by the negative log-likelihood obtained
by the various models on the (uncorrupted) test data, normalized per observation symbol.
The negative log-likelihood measures the amount of ‘statistical surprise’ induced by the
model. The results are summarized in Table 1. The ﬁrst four entries correspond to the
Markov chains of order 0 – 3, and the last entry corresponds to the PSA. The order of
the PSA is deﬁned to be log|Σ|(|Q|). These empirical results imply that using a PSA of
reasonable size, we get a better model of the data than if we had used a much larger full
order Markov chain.
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
Table 1. Comparison of full order Markov chains versus a PSA (a Markov
model with variable memory).
Fixed Order Markov
Model Order
Number of States
Negative Log-Likelihood
Building A Simple Model for E.coli DNA
The DNA alphabet is composed of four nucleotides denoted by: A,C,T,G. DNA strands are
composed of sequences of protein coding genes and ﬁllers between those regions named
intergenic regions.
Locating the coding genes is necessary, prior to any further DNA
analysis. Using manually segmented data of E. coli we built two different
PSAs, one for the coding regions and one for the intergenic regions. We disregarded the
internal (triplet) structure of the coding genes and the existence of start and stop codons
at the beginning and the end of those regions. The models were constructed based on
250 different DNA strands from each type, their lengths ranging from 20 bases to several
thousands. The PSAs built are rather small compared to the HMM model described in
 : the PSA that models the coding regions has 65 states and
the PSA that models the intergenic regions has 81 states.
We tested the performance of the models by calculating the log-likelihood of the two
models obtained on test data drawn from intergenic regions.
In 90% of the cases the
log-likelihood obtained by the PSA trained on intergenic regions was higher than the loglikelihood of the PSA trained on the coding regions. Misclassiﬁcations (when the loglikelihood obtained by the second model was higher) occurred only for sequences shorter
than 100 bases. Moreover, the log-likelihood difference between the models scales linearly
with the sequence length where the slope is close to the KL-divergence between the Markov
models (which can be computed from the parameters of the two PSAs), as depicted in
Figure 5. The main advantage of PSA models is in their simplicity. Moreover, the loglikelihood of a set of substrings of a given strand can be computed in time linear in the
number of substrings. The latter property combined with the results mentioned above
indicate that the PSA model might be used when performing tasks such as DNA gene
locating. However, we should stress that we have done only a preliminary step in this
direction and the results obtained in as part of a complete
parsing system are better.
Acknowledgments
We would like to thank Anders Krogh and David Haussler for letting us use their E. coli
DNA data and for helpful discussions. Special thanks to Kenn Rudd for supplying the E.
coli sequences used in the DNA experiments. We also would like to thank Ronitt Rubinfeld
and Yoav Freund for their helpful comments. Thanks to Lee Giles for providing us with the
software for plotting ﬁnite state machines. This research has been supported in part by a
grant from the Israeli Ministry of Science and Arts and by the Bruno Goldberg endowment
D. RON, Y. SINGER AND N. TISHBY
Log-Likelihood Difference
Sequence Length
Figure 5. The difference between the log-likelihood induced by a PSA trained on data taken from intergenic
regions and a PSA trained on data taken from coding regions. The test data was taken from intergenic regions. In
90% of the cases the likelihood of the ﬁrst PSA was higher.
fund. Dana Ron would like to thank the support of the Eshkol fellowship. Yoram Singer
would like to thank the Clore Foundation for its support.
Appendix A
Proof of Theorem 1
Theorem 1 For every L-PSA M = (Q, Σ, τ, γ, π), there exists an equivalent PST TM, of
maximal depth L and at most L · |Q| nodes.
Let TM be the tree whose leaves correspond to the strings in Q (the states of M).
For each leaf s, and for every symbol σ, let γs(σ) = γ(s, σ). This ensures that for every
string which is a sufﬁx extension of some leaf in TM, both M and TM generate the next
symbol with the same probability. The remainder of this proof is hence dedicated to deﬁning
the next symbol probability functions for the internal nodes of TM. These functions must
be deﬁned so that TM generates all strings related to nodes in TM, with the same probability
For each node s in the tree, let the weight of s, denoted by ws, be deﬁned as follows
s′∈Q s.t. s∈Suﬃx ∗(s′)
In other words, the weight of a leaf in TM is the stationary probability of the corresponding
state in M; and the weight of an internal node labeled by a string s, equals the sum of the
stationary probabilities over all states of which s is a sufﬁx. Note that the weight of any
internal node is the sum of the weights of all the leaves in its subtree, and in particular
we = 1. Using the weights of the nodes we assign values to the γs’s of the internal nodes
s in the tree in the following manner. For every symbol σ let
s′∈Q s.t. s∈Suﬃx ∗(s′)
γ(s′, σ) .
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
According to the deﬁnition of the weights of the nodes, it is clear that for every node s,
γs(·) is in fact a probability function on the next output symbol as required in the deﬁnition
of prediction sufﬁx trees.
What is the probability that M generates a string s which is a node in TM (a sufﬁx of a state
in Q)? By deﬁnition of the transition function of M, for every s0 ∈Q, if s′ = τ(s0, s), then
s′ must be a sufﬁx extension of s. Thus PM(s) is the sum over all such s′ of the probability
of reaching s′, when s0 is chosen according to the initial distribution π(·) on the starting
states. But if the initial distribution is stationary then at any point the probability of being
at state s′ is just π(s′), and
s′∈Q s.t. s∈Suﬃx ∗(s′)
We next prove that PTM (s) equals ws as well. We do this by showing that for every
s = s1 . . . sl in the tree, where |s| ≥1, ws = wpreﬁx(s)γpreﬁx(s)(sl). Since we = 1, it
follows from a simple inductive argument that PTM (s) = ws.
By our deﬁnition of PSAs, π(·) is such that for every s ∈Q, s = s1 . . . sl,
s′ s.t. τ(s′,sl)=s
π(s′)γ(s′, sl) .
Hence, if s is a leaf in TM then
s′∈L(TM) s.t. s∈Suﬃx ∗(s′sl)
ws′γs′(sl)
s′∈L(TM(preﬁx(s)))
ws′γs′(sl)
= wpreﬁx(s)γpreﬁx(s)(sl) ,
where (a) follows by substituting ws′ for π(s′) and γs′(sl) for γ(s′, sl) in Equation (A.4),
and by the deﬁnition of τ(·, ·); (b) follows from our deﬁnition of the structure of prediction
sufﬁx trees; and (c) follows from our deﬁnition of the weights of internal nodes. Hence, if
s is a leaf, ws = wpreﬁx(s)γpreﬁx(s)(sl) as required.
If s is an internal node then using the result above and Equation (A.2) we get that
s′∈L(TM(s))
s′∈L(TM(s))
wpreﬁx(s′)γpreﬁx(s′)(sl)
= wpreﬁx(s)γpreﬁx(s)(sl) .
It is left to show that the resulting tree is not bigger than L times the number of states
in M. The number of leaves in TM equals the number of states in M, i.e. |L(T)| = |Q|.
If every internal node in TM is of full degree (i.e.
the probability TM generates any
D. RON, Y. SINGER AND N. TISHBY
string labeling a leaf in the tree is strictly greater than 0) then the number of internal
nodes is bounded by |Q| and the total number of nodes is at most 2|Q|. In particular,
the above is true when for every state s in M, and every symbol σ, γ(s, σ) > 0. If
this is not the case then we can simply bound the total number of nodes by L · |Q|.
Appendix B
Emulation of PSTs by PFAs
In this section we show that for every PST there exists an equivalent PFA which is not much
larger and which is a slight variant of a PSA. Furthermore, if the PST has a certain property,
deﬁned below and denoted by Property∗, then it can be emulated by a PSA.
Property∗For every string s labeling a node in the tree, T,
Before we state our theorem, we observe that Property∗implies that for every string r,
This is true for the following simple reasoning. If r is a node in T, then Equality (B.1) is
equivalent to Property∗. Otherwise let r = r1r2, where r1 is the longest preﬁx of r which
is a leaf in T.
PT (r) = PT (r1) · PT (r2|r1)
PT (σr1) · PT (r2|r1)
PT (σr1) · PT (r2|σr1)
where Equality (B.2c) follows from the deﬁnition of PST’s.
Theorem 3 For every PST, T, of depth L over Σ there exists an equivalent PFA, MT , with
at most L · |L(T)| states. Furthermore, if Property∗holds for T, then it has an equivalent
In the proof of Theorem 1, we were given a PSA M and we deﬁned the equivalent
sufﬁx tree TM to be the tree whose leaves correspond to the states of the automaton. Thus,
given a sufﬁx tree T, the natural dual procedure would be to construct a PSA MT whose
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
states correspond to the leaves of T. The ﬁrst problem with this construction is that we
might not be able to deﬁne the transition function τ on all pairs of states and symbols. That
is, there might exist a state s and a symbol σ such that there is no state s′ which is a sufﬁx
of sσ. The solution is to extend T to a larger tree T ′ (of which T is a subtree) such that τ is
well deﬁned on the leaves of T ′. It can easily be veriﬁed that the following is an equivalent
requirement on T ′: for each symbol σ, and for every leaf s in T ′, sσ is either a leaf in the
subtree T ′(σ) rooted at σ, or is a sufﬁx extension of a leaf in T ′(σ). In this case we shall
say that T ′ covers each of its children’s subtrees. Viewing this in another way, for every
leaf s, the longest preﬁx of s must be either a leaf or an internal node in T ′. We thus obtain
T ′ by adding nodes to T until the above property holds.
The next symbol probability functions of the nodes in T ′ are deﬁned as follows. For every
node s in T ∩T ′ and for every σ ∈Σ, let γ′
s(σ) = γs(σ). For each new node s′ = s′
1 . . . s′
in T ′ −T, let γ′
s′(σ) = γs(σ), where s is the longest sufﬁx of s′ in T (i.e. the deepest
ancestor of s′ in T). The probability distribution generated by T ′ is hence equivalent to
that generated by T. From Equality (B.1) it directly follows that if Property∗holds for T,
then it holds for T ′ as well.
Based on T ′ we now deﬁne MT = (Q, Σ, τ, γ, π). If Property∗holds for T, then we
deﬁne MT as follows. Let the states of MT be the leaves of T ′ and let the transition function
be deﬁned as usual for PSAs (i.e. for every state s and symbol σ, τ(s, σ) is the unique sufﬁx
of sσ.) Note that the number of states in MT is at most L times the number of leaves in T,
as required. This is true since for each original leaf in the tree T, at most L −1 preﬁxes
might be added to T ′. For each s ∈Q and for every σ ∈Σ, let γ(s, σ) = γ′
s(σ), and let
π(s) = PT (s). It should be noted that MT is not necessarily ergodic. It follows from this
construction that for every string r which is a sufﬁx extension of a leaf in T ′, and every
symbol σ, PMT (σ|r) = PT (σ|r). It remains to show that for every string r which is a node
in T ′, PMT (r) = PT ′(r) (= PT (r)). For a state s ∈Q, let P s
MT (r) denote the probability
that r is generated assuming we start at state s. Then,
π(s)PMT (r|s)
PT ′(s)PT ′(r|s)
= PT ′(r) ,
where Equality (B.3b) follows from the deﬁnition of PSAs, Equality (B.3c) follows from our
deﬁnition of π(·), and Equality (B.3e) follows from a series of applications of Equality (B.1).
If T does not have Property∗, then we may not be able to deﬁne an initial distribution on the
states of the PSA MT such that for every string r which is a node in T ′, PMT (r) = PT ′(r).
We thus deﬁne a slight variant of MT as follows. Let the states of MT be the leaves of T ′
and all their preﬁxes, and let τ(·, ·) be deﬁned as follows: for every state s and symbol σ,
D. RON, Y. SINGER AND N. TISHBY
τ(s, σ) is the longest sufﬁx of sσ. Thus, MT has the structure of a preﬁx tree combined
with a PSA. If we deﬁne γ(·, ·) as above, and let the empty string, e, be the single starting
state (i.e., π(e) = 1), then, by deﬁnition, MT is equivalent to T.
An illustration of the constructions described above is given in Figure B.1.
(5/11,6/11)
(0.25,0.75)
Figure B.1. Left: A Prediction sufﬁx tree. The prediction probabilities of the symbols ‘0’ and ‘1’, respectively,
are depicted beside the nodes, in parentheses. Right: The PFA that is equivalent to the PST on the left. Bold edges
denote transitions with the symbol ‘1’ and dashed edges denote transitions with ‘0’. Since Property∗holds for the
PST, then it actually has an equivalent PSA which is deﬁned by the circled part of the PFA. The initial probability
distribution of this PSA is: π(01) = 3/11, π(00) = 2/11, π(11) = 3/11, π(010) = 3/22, π(110) = 3/22.
Note that states ‘11’ and ‘01’ in the PSA replaced the node ’1’ in the tree.
Appendix C
Proofs of Lemma 1 and Lemma 2
1. There exists a polynomial m′
0 in L, n, |Σ|, 1
δ , such that the probability that a
sample of m′ ≥m′
0(L, n, |Σ|, 1
δ ) strings each of length at least L + 1 generated
according to M is typical is at least 1 −δ.
2. There exists a polynomial m0 in L, n, |Σ|, 1
δ , and 1/(1 −λ2(UM)), such that
the probability that a single sample string of length m ≥m0(L, n, |Σ|, 1
δ , 1/(1 −
λ2(UM))) generated according to M is typical is at least 1 −δ.
Proof: Before proving the lemma we would like to recall that the parameters ϵ0, ϵ1, ϵ2,
and γmin, are all polynomial functions of 1/ϵ, n, L, and |Σ|, and were deﬁned in Section 5.
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
Several sample strings
We start with obtaining a lower bound for m′, so that the ﬁrst
property of a typical sample holds. Since the sample strings are generated independently,
we may view ˜P(s), for a given state s, as the average value of m′ independent random
variables. Each of these variables is in the range and its expected value is π(s). Using
Hoeffding’s inequality we get that if m′ ≥
δ , then with probability at least 1−δ
| ˜P(s) −π(s)| ≤ϵ1ϵ0. The probability that this inequality holds for every state is hence at
least 1 −δ
We would like to point out that since our only assumptions on the sample strings are that
they are generated independently, and that their length is at least L + 1, we use only the
independence between the different strings when bounding our error. We do not assume
anything about the random variables related to ˜P(s) when restricted to any one sample
string, other than that their expected value is π(s). If the strings are known to be longer,
then a more careful analysis can be applied as described subsequently for the case of a
single sample string.
We now show that for an appropriate m′ the second property holds with probability at
least 1 −δ
2 as well. Let s be a string in Σ≤L. In the following lines, when we refer to
appearances of s in the sample we mean in the sense deﬁned by ˜P. That is, we count
only appearances of s which end at the Lth or greater symbol of a sample string. For
the ith appearance of s in the sample and for every symbol σ, let Xi(σ|s) be a random
variable which is 1 if σ appears after the ith appearance of s and 0 otherwise. If s is either
a state or a sufﬁx extension of a state, then for every σ, the random variables {Xi(σ|s)} are
independent 0/1 random variables with expected value P(σ|s). Let Ns be the total number
of times s appears in the sample, and let Nmin =
min ln 4|Σ|n
ϵ0δ . If Ns ≥Nmin, then
with probability at least 1 −δϵ0
2n , for every symbol σ, | ˜P(σ|s) −P(σ|s)| ≤1
2ϵ2γmin. If s
is a sufﬁx of several states s1, . . . , sk, then for every symbol σ,
P(s) P(σ|si) ,
(where P(s) = Pk
i=1 π(si)) and
˜P(σ|si) .
Recall that ϵ1 = (ϵ2γmin)/(8nϵ0). If:
(1) for every state si, | ˜P(si) −π(si)| ≤ϵ1ϵ0;
(2) for each si satisfying π(si) ≥2ϵ1ϵ0, | ˜P(σ|si) −P(σ|si)| ≤1
2ϵ2γmin for every σ;
then | ˜P(σ|s) −P(σ|s)| ≤ϵ2γmin, as required.
If the sample has the ﬁrst property required of a typical sample (i.e., ∀s ∈Q, | ˜P(s) −
P(s)| ≤ϵ1ϵ0), and for every state s such that ˜P(s) ≥ϵ1ϵ0, Ns ≥Nmin, then with
probability at least 1−δ
4 the second property of a typical sample holds for all strings which
are either states or sufﬁxes of states. If for every string s which is a sufﬁx extension a state
such that ˜P(s) ≥(1 −ϵ1)ϵ0, Ns ≥Nmin, then for all such strings the second property
D. RON, Y. SINGER AND N. TISHBY
holds with probability at least 1 −δ
4 as well. Putting together all the bounds above, if
δ + Nmin/(ϵ1ϵ0), then with probability at least 1 −δ the sample is typical.
A single sample string In this case the analysis is somewhat more involved. We view our
sample string generated according to M as a walk on the markov chain described by RM
(deﬁned in Subsection 3). We may assume that the starting state is visible as well since its
contribution to ˜P(·) is negligible. We shall need the following theorem from 
which gives bounds on the convergence rate to the stationary distribution of general ergodic
Markov chains. This theorem is partially based on a work by Mihail , who
gives bounds on the convergence in terms of combinatorial properties of the chain.
Markov Chain Convergence Theorem For any state s0 in the Markov chain
RM, let Rt
M(s0, ·) denote the probability distribution over the states in RM, after taking a
walk of length t starting from state s0. Then
M(s0, s) −π(s)|
≤(λ2(UM))t
First note that by simply applying Markov’s inequality, we get that with probability at
2n, | ˜P(s) −π(s)| ≤ϵ1ϵ0, for each state s such that π(s) < (δϵ1ϵ0)/(2n). It
thus remains to obtain a lower bound on m, so that the same is true for each s such that
π(s) ≥(δϵ1ϵ0)/(2n). We do this by bounding the variance of the random variable related
with ˜P(s), and applying Chebishev’s Inequality.
n3/ 32δ3ϵ5
ln (1/λ2(UM))
We next show that for every s satisfying π(s) ≥(δϵ1ϵ0)/(2n) , |Rt0
M(s, s) −π(s)| ≤
0. By the theorem above and our assumption on π(s),
M(s, s) −π(s)
M(s, s′) −π(s′)|
≤(λ2(UM))t0
(λ2(UM))t0
e−t0 ln(1/λ2(UM))
Therefore, |Rt
M(s, s) −π(s)| ≤
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
Intuitively, this means that for every two integers, t > t0, and i ≤t −t0, the event that s
is the (i + t0)th state passed on a walk of length t, is ‘almost independent’ of the event that
s is the ith state passed on the same walk.
For a given state s, satisfying π(s) ≥(δϵ1ϵ0)/(2n), let Xi be a 0/1 random variable
which is 1 iff s is the ith state on a walk of length t, and Y = Pt
i=1 Xi. By our deﬁnition
of ˜P, in the case of a single sample string, ˜P(s) = Y/t, where t = m −L −1. Clearly
E(Y/t) = π(s), and for every i, V ar(Xi) = π(s) −π2(s). We next bound V ar(Y/t).
E(Xi)E(Xj)
i,j s.t. |i−j|<t0
i,j s.t. |i−j|≥t0
0π(s) −π2(s) .
If we pick t to be greater than (4nt0)/(δϵ2
0), then V ar(Y/t) <
0, and using
Chebishev’s Inequality Pr[|Y/t −π(s)| > ϵ1ϵ0] <
2n. The probability the above holds
for any s is at most δ
2. The analysis of the second property required of a typical sample is
identical to that described in the case of a sample consisting of many strings.
Lemma 2 If Learn-PSA is given a typical sample then:
For every string s in T, if P(s) ≥ϵ0 then γs(σ)
ˆγs′(σ) ≤1 + ϵ/2 , where s′ is the longest
sufﬁx of s corresponding to a node in ˆT.
| ˆT| ≤(|Σ| −1) · |T|.
Assume contrary to the claim that there exists a string labeling a node s in T
such that P(s) ≥ϵ0 and for some σ ∈Σ
ˆγs′(σ) > 1 + ϵ/2,
where s′ is the longest sufﬁx of s in ˆT. For simplicity of the presentation, let us assume
that there is a node labeled by s′ in ¯T. If this is not the case (suﬃx(s′) is an internal node
in ¯T, whose son s′ is missing), the analysis is very similar. If s ≡s′ then we easily show
below that our counter assumption is false. If s′ is a proper sufﬁx of s then we prove the
following. If the counter assumption is true, then we added to ¯T a (not necessarily proper)
D. RON, Y. SINGER AND N. TISHBY
sufﬁx of s which is longer than s′. This contradicts the fact that s′ is the longest sufﬁx of s
We ﬁrst achieve a lower bound on the ratio between the two true next symbol probabilities,
γs(σ) and γs′(σ). According to our deﬁnition of ˆγs′(·),
ˆγs′(σ) ≥(1 −|Σ|γmin) ˜P(σ|s′) .
We analyze separately the case in which γs′(σ) ≥γmin, and the case in which γs′(σ) <
γmin. Recall that γmin = ϵ2/|Σ|. If γs′(σ) ≥γmin, then
ˆγs′(σ) · (1 −ϵ2)(1 −|Σ|γmin)
2)(1 −ϵ2)2 ,
where Inequality (C.8a) follows from our assumption that the sample is typical, Inequality (C.8b) follows from our deﬁnition of ˆγs′(σ), and Inequality (C.8c) follows from the
counter assumption (C.6), and our choice of γmin. Since ϵ2 < ϵ/12, and ϵ < 1 then we get
If γs′(σ) < γmin, then ˆγs′(σ) ≥γs′(σ), since ˆγs′(σ) is deﬁned to be at least γmin.
Therefore,
γs′(σ) ≥γs(σ)
ˆγs′(σ) > 1 + ϵ
as well. If s ≡s′ then the counter assumption (C.6) is evidently false, and we must only
address the case in which s ̸= s′, i.e., s′ is a proper sufﬁx of s.
Let s = s1s2 . . . sl, and let s′ be si . . . sl, for some 2 ≤i ≤l. We now show that if the
counter assumption (C.6) is true, then there exists an index 1 ≤j < i such that sj . . . sl
was added to ¯T. Let 2 ≤r ≤i be the ﬁrst index for which γsr...sl(σ) < (1 + 7ϵ2)γmin.
If there is no such index then let r = i. The reason we need to deal with the prior case is
clariﬁed subsequently. In either case, since ϵ2 < ϵ/48, and ϵ < 1, then
γsr...sl(σ) > 1 + ϵ
In other words
γs2...sl(σ) · γs2...sl(σ)
γs3...sl(σ) · . . . · γsr−1...sl(σ)
γsr...sl(σ)
This last inequality implies that there must exist an index 1 ≤j ≤i −1, for which
LEARNING PROBABILISTIC AUTOMATA WITH VARIABLE MEMORY LENGTH
γsj...sl(σ)
γsj+1...sl(σ) > 1 + ϵ
We next show that Inequality (C.13) implies that sj . . . sl was added to ¯T. We do this by
showing that sj . . . sl was added to ¯S, that we compared ˜P(σ|sj . . . sl) to ˜P(σ|sj+1 . . . sl),
and that the ratio between these two values is at least (1 + 3ϵ2). Since P(s) ≥ϵ0 then
necessarily
˜P(sj . . . sl) ≥(1 −ϵ1)ϵ0 ,
and sj . . . sl must have been added to ¯S. Based on our choice of the index r, and since
γsj...sl(σ) ≥(1 + 7ϵ2)γmin.
Since we assume that the sample is typical,
˜P(σ|sj . . . sl) ≥(1 + 6ϵ2)γmin > (1 + ϵ2)γmin ,
which means that we must have compared ˜P(σ|sj . . . sl) to ˜P(σ|sj+1 . . . sl).
We now separate the case in which γsj+1...sl(σ) < γmin, from the case in which
γsj+1...sl(σ) ≥γmin. If γsj+1...sl(σ) < γmin then
˜P(σ|sj+1 . . . sl) ≤(1 + ϵ2)γmin .
Therefore,
˜P(σ|sj . . . sl)
˜P(σ|sj+1 . . . sl)
≥(1 + 6ϵ2)γmin
(1 + ϵ2)γmin
≥(1 + 3ϵ2) ,
and sj . . . sl would have been added to ¯T. On the other hand, if γsj+1...sl(σ) ≥γmin, the
same would hold since
˜P(σ|sj . . . sl)
˜P(σ|sj+1 . . . sl)
(1 −ϵ2)γsj...sl(σ)
(1 + ϵ2)γsj+1...sl(σ)
> (1 −ϵ2)(1 +
≥(1 −ϵ2)(1 + 6ϵ2)
> 1 + 3ϵ2 ,
where Inequality C.19c follows from our choice of ϵ2 (ϵ2 =
48L). This contradicts our
initial assumption that s′ is the longest sufﬁx of s added to ¯T.
2nd Claim:
We prove below that ¯T is a subtree of T. The claim then follows directly,
since when transforming ¯T into ˆT, we add at most all |Σ| −1 siblings of every node in ¯T.
D. RON, Y. SINGER AND N. TISHBY
Therefore it sufﬁces to show that we did not add to ¯T any node which is not in T. Assume
to the contrary that we add to ¯T a node s which is not in T. According to the algorithm,
the reason we add s to ¯T, is that there exists a symbol σ such that ˜P(σ|s) ≥(1 + ϵ2)γmin,
and ˜P(σ|s)/ ˜P(σ|suﬃx(s)) > 1 + 3ϵ2, while both ˜P(s) and ˜P(suﬃx(s)) are greater than
(1 −ϵ1)ϵ0. If the sample string is typical then
P(σ|s) ≥γmin ,
˜P(σ|s) ≤P(σ|s) + ϵ2γmin ≤(1 + ϵ2)P(σ|s) ,
˜P(σ|suﬃx(s)) ≥P(σ|suﬃx(s)) −ϵ2γmin .
If P(σ|suﬃx(s)) ≥γmin then ˜P(σ|suﬃx(s)) ≥(1 −ϵ2)P(σ|suﬃx(s)), and thus
P(σ|suﬃx(s)) ≥(1 −ϵ2)
(1 + ϵ2) (1 + 3ϵ2) ,
which is greater than 1 since ϵ2 < 1/3. If P(σ|suﬃx(s)) < γmin , since P(σ|s) ≥γmin ,
then P(σ|s)/P(σ|suﬃx(s)) > 1 as well. In both cases this ratio cannot be greater than 1
if s is not in the tree, contradicting our assumption.