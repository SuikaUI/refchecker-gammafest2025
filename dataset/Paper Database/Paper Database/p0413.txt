HAL Id: hal-00650905
 
Submitted on 2 Mar 2013
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Scikit-learn: Machine Learning in Python
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al.
To cite this version:
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, et al..
Scikit-learn:
Machine Learning in Python.
Journal of Machine Learning Research, 2011.
00650905v2￿
Journal of Machine Learning Research 12 2825-2830
Submitted 3/11; Revised 8/11; Published 10/11
Scikit-learn: Machine Learning in Python
Fabian Pedregosa
 
Ga¨el Varoquaux
 
Alexandre Gramfort
 
Vincent Michel
 
Bertrand Thirion
 
Parietal, INRIA Saclay
Neurospin, Bˆat 145, CEA Saclay
91191 Gif sur Yvette – France
Olivier Grisel
 
20 rue Soleillet
75 020 Paris – France
Mathieu Blondel
 
Kobe University
1-1 Rokkodai, Nada
Kobe 657-8501 – Japan
Peter Prettenhofer
 
Bauhaus-Universit¨at Weimar
Bauhausstr. 11
99421 Weimar – Germany
 
Google Inc
76 Ninth Avenue
New York, NY 10011 – USA
Vincent Dubourg
 
Clermont Universit´e, IFMA, EA 3867, LaMI
BP 10448, 63000 Clermont-Ferrand – France
Jake Vanderplas
 
Astronomy Department
University of Washington, Box 351580
Seattle, WA 98195 – USA
Alexandre Passos
 
UMass Amherst
Amherst MA 01002 – USA
David Cournapeau
 
21 J.J. Thompson Avenue
Cambridge, CB3 0FA – UK
c⃝2011 Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David
Cournapeau, Matthieu Brucher, Matthieu Perrot and ´Edouard Duchesnay
Pedregosa, Varoquaux, Gramfort et al.
Matthieu Brucher
 
Total SA, CSTJF
avenue Larribau
64000 Pau – France
Matthieu Perrot
 
´Edouard Duchesnay
 
Neurospin, Bˆat 145, CEA Saclay
91191 Gif sur Yvette – France
Editor: Mikio Braun
Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems.
This package
focuses on bringing machine learning to non-specialists using a general-purpose high-level
language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simpliﬁed BSD license,
encouraging its use in both academic and commercial settings. Source code, binaries, and
documentation can be downloaded from 
Python, supervised learning, unsupervised learning, model selection
1. Introduction
The Python programming language is establishing itself as one of the most popular languages for scientiﬁc computing. Thanks to its high-level interactive nature and its maturing
ecosystem of scientiﬁc libraries, it is an appealing choice for algorithmic development and
exploratory data analysis . Yet, as a generalpurpose language, it is increasingly used not only in academic settings but also in industry.
Scikit-learn harnesses this rich environment to provide state-of-the-art implementations
of many well known machine learning algorithms, while maintaining an easy-to-use interface
tightly integrated with the Python language. This answers the growing need for statistical
data analysis by non-specialists in the software and web industries, as well as in ﬁelds
outside of computer-science, such as biology or physics.
Scikit-learn diﬀers from other
machine learning toolboxes in Python for various reasons: i) it is distributed under the
BSD license ii) it incorporates compiled code for eﬃciency, unlike MDP 
and pybrain , iii) it depends only on numpy and scipy to facilitate easy
distribution, unlike pymvpa that has optional dependencies such as R
and shogun, and iv) it focuses on imperative programming, unlike pybrain which uses a
data-ﬂow framework. While the package is mostly written in Python, it incorporates the
C++ libraries LibSVM and LibLinear that provide
reference implementations of SVMs and generalized linear models with compatible licenses.
Scikit-learn: Machine Learning in Python
Binary packages are available on a rich set of platforms including Windows and any POSIX
platforms.
Furthermore, thanks to its liberal license, it has been widely distributed as
part of major free software distributions such as Ubuntu, Debian, Mandriva, NetBSD and
Macports and in commercial distributions such as the “Enthought Python Distribution”.
2. Project Vision
Code quality. Rather than providing as many features as possible, the project’s goal has
been to provide solid implementations. Code quality is ensured with unit tests—as of release
0.8, test coverage is 81%—and the use of static analysis tools such as pyflakes and pep8.
Finally, we strive to use consistent naming for the functions and parameters used throughout
a strict adherence to the Python coding guidelines and numpy style documentation.
BSD licensing. Most of the Python ecosystem is licensed with non-copyleft licenses. While
such policy is beneﬁcial for adoption of these tools by commercial projects, it does impose
some restrictions: we are unable to use some existing scientiﬁc code, such as the GSL.
Bare-bone design and API. To lower the barrier of entry, we avoid framework code and keep
the number of diﬀerent objects to a minimum, relying on numpy arrays for data containers.
Community-driven development. We base our development on collaborative tools such as
git, github and public mailing lists. External contributions are welcome and encouraged.
Documentation. Scikit-learn provides a ∼300 page user guide including narrative documentation, class references, a tutorial, installation instructions, as well as more than 60
examples, some featuring real-world applications. We try to minimize the use of machinelearning jargon, while maintaining precision with regards to the algorithms employed.
3. Underlying Technologies
Numpy: the base data structure used for data and model parameters. Input data is presented as numpy arrays, thus integrating seamlessly with other scientiﬁc Python libraries.
Numpy’s view-based memory model limits copies, even when binding with compiled code
 . It also provides basic arithmetic operations.
Scipy: eﬃcient algorithms for linear algebra, sparse matrix representation, special functions
and basic statistical functions. Scipy has bindings for many Fortran-based standard numerical packages, such as LAPACK. This is important for ease of installation and portability,
as providing libraries around Fortran code can prove challenging on various platforms.
Cython: a language for combining C in Python. Cython makes it easy to reach the performance of compiled languages with Python-like syntax and high-level operations. It is also
used to bind compiled libraries, eliminating the boilerplate code of Python/C extensions.
4. Code Design
Objects speciﬁed by interface, not by inheritance. To facilitate the use of external objects
with scikit-learn, inheritance is not enforced; instead, code conventions provide a consistent
interface. The central object is an estimator, that implements a fit method, accepting as
arguments an input data array and, optionally, an array of labels for supervised problems.
Supervised estimators, such as SVM classiﬁers, can implement a predict method. Some
Pedregosa, Varoquaux, Gramfort et al.
scikit-learn
Support Vector Classiﬁcation
Lasso (LARS)
Elastic Net
k-Nearest Neighbors
PCA (9 components)
k-Means (9 clusters)
-: Not implemented.
⋆: Does not converge within 1 hour.
Table 1: Time in seconds on the Madelon data set for various machine learning libraries
exposed in Python: MLPy , PyBrain ,
pymvpa , MDP and Shogun . For more benchmarks see 
estimators, that we call transformers, for example, PCA, implement a transform method,
returning modiﬁed input data. Estimators may also provide a score method, which is an
increasing evaluation of goodness of ﬁt: a log-likelihood, or a negated loss function. The
other important object is the cross-validation iterator, which provides pairs of train and test
indices to split input data, for example K-fold, leave one out, or stratiﬁed cross-validation.
Model selection. Scikit-learn can evaluate an estimator’s performance or select parameters
using cross-validation, optionally distributing the computation to several cores. This is accomplished by wrapping an estimator in a GridSearchCV object, where the “CV” stands for
“cross-validated”. During the call to fit, it selects the parameters on a speciﬁed parameter
grid, maximizing a score (the score method of the underlying estimator). predict, score,
or transform are then delegated to the tuned estimator. This object can therefore be used
transparently as any other estimator. Cross validation can be made more eﬃcient for certain
estimators by exploiting speciﬁc properties, such as warm restarts or regularization paths
 . This is supported through special objects, such as the LassoCV.
Finally, a Pipeline object can combine several transformers and an estimator to create
a combined estimator to, for example, apply dimension reduction before ﬁtting. It behaves
as a standard estimator, and GridSearchCV therefore tune the parameters of all steps.
5. High-level yet Eﬃcient: Some Trade Oﬀs
While scikit-learn focuses on ease of use, and is mostly written in a high level language, care
has been taken to maximize computational eﬃciency. In Table 1, we compare computation
time for a few algorithms implemented in the major machine learning toolkits accessible
in Python. We use the Madelon data set , 4400 instances and 500
attributes, The data set is quite large, but small enough for most algorithms to run.
SVM. While all of the packages compared call libsvm in the background, the performance of
scikit-learn can be explained by two factors. First, our bindings avoid memory copies and
have up to 40% less overhead than the original libsvm Python bindings. Second, we patch
libsvm to improve eﬃciency on dense data, use a smaller memory footprint, and better use
memory alignment and pipelining capabilities of modern processors. This patched version
also provides unique features, such as setting weights for individual samples.
Scikit-learn: Machine Learning in Python
LARS. Iteratively reﬁning the residuals instead of recomputing them gives performance
gains of 2–10 times over the reference R implementation . Pymvpa
uses this implementation via the Rpy R bindings and pays a heavy price to memory copies.
Elastic Net. We benchmarked the scikit-learn coordinate descent implementations of Elastic
Net. It achieves the same order of performance as the highly optimized Fortran version
glmnet on medium-scale problems, but performance on very large
problems is limited since we do not use the KKT conditions to deﬁne an active set.
kNN. The k-nearest neighbors classiﬁer implementation constructs a ball tree of the samples, but uses a more eﬃcient brute force search in large dimensions.
PCA. For medium to large data sets, scikit-learn provides an implementation of a truncated
PCA based on random projections .
k-means. scikit-learn’s k-means algorithm is implemented in pure Python. Its performance
is limited by the fact that numpy’s array operations take multiple passes over data.
6. Conclusion
Scikit-learn exposes a wide variety of machine learning algorithms, both supervised and
unsupervised, using a consistent, task-oriented interface, thus enabling easy comparison
of methods for a given application. Since it relies on the scientiﬁc Python ecosystem, it
can easily be integrated into applications outside the traditional range of statistical data
analysis. Importantly, the algorithms, implemented in a high-level language, can be used
as building blocks for approaches speciﬁc to a use case, for example, in medical imaging
 . Future work includes online learning, to scale to large data sets.