Discriminatively Trained Dense
Surface Normal Estimation
L’ubor Ladick´y, Bernhard Zeisl, and Marc Pollefeys
ETH Z¨urich, Switzerland
{lubor.ladicky,bernhard.zeisl,marc.pollefeys}@inf.ethz.ch
Abstract. In this work we propose the method for a rather unexplored
problem of computer vision - discriminatively trained dense surface normal estimation from a single image. Our method combines contextual and
segment-based cues and builds a regressor in a boosting framework by
transforming the problem into the regression of coeﬃcients of a local coding. We apply our method to two challenging data sets containing images
of man-made environments, the indoor NYU2 data set and the outdoor
KITTI data set. Our surface normal predictor achieves results better than
initially expected, signiﬁcantly outperforming state-of-the-art.
Introduction
Recently, single-view reconstruction methods, estimating scene geometry directly
by learning from data, have gained quite some popularity. While resulting 3D
reconstructions of such methods are of debatable quality, coarse information
about the 3D layout of a scene has shown to help boost the performance of
applications such as object detection , semantic reasoning or general scene
understanding .
The principal underlying idea behind these methods is, that particular
structures have a certain real world size, and thus their size in an image gives
rise to the scene depth. We argue that this is a rather weak hypothesis, since
structures are likely to exist at diﬀerent size in reality and perspective projection
distorts them. As a consequence it renders the problem of single image depth
estimation ill-posed in general. However, perspective cues are not harmful, but
actually helpful, because they carry information about the local surface orientation and allow to reason about the scene, for example about the viewpoint of
the camera. We argue that it is beneﬁcial to directly estimate ﬁrst order derivatives of depth, i.e. surface normals, as it can provide more accurate results than
estimation of absolute depth. In addition we do not need to worry about depth
discontinuities, e.g. due to occlusions, which are diﬃcult to detect and harm
single image reconstruction .
While data-driven normal estimation seems to be a more promising approach,
it has not been exploited much so far. We believe this is due to the lack of available ground truth data, which is hard to obtain, as recording requires accurate
capturing devices. With the recent advances in low cost commodity depth sensors such as Kinect, ToF cameras or laser scanners, acquisition was made easier
D. Fleet et al. (Eds.): ECCV 2014, Part V, LNCS 8693, pp. 468–484, 2014.
⃝Springer International Publishing Switzerland 2014
Discriminatively Trained Dense Surface Normal Estimation
and there are multiple data sets available nowadays, which should foster
research in this direction.
The importance of surface normal estimation has been already recognized
long before such data was available. Due to the lack of data, proposed approaches had to rely purely on the knowledge of underlying physics of
light and shading. Thus, resulting methods work only under strong assumptions
about the knowledge of locations of light sources and properties of the material,
such as the assumption of Lambertian surfaces. However, these approaches do
not work in more complex scenarios such as indoor or outdoor scenes, and thus
are not applicable for general problems. The ﬁrst approach, that directly tries to
estimate surface normals from the data was proposed in . The method aims
to extract a set of both visually-discriminative and geometrically-informative
primitives from training data. For test images the learned detector ﬁres at sparse
positions with similar appearance and hypothesizes about the underlying surface
orientations by means of the learned primitives. Hoiem et al. do not directly
estimate normal directions, but formulate the task as a labeling problem with
more abstract surface orientations, such as left- or right-facing, vertical, etc. in
order to estimate the 3D contextual frame of an image. In Gupta et al.
extracted a qualitative physical representation of an outdoor scene by reasoning
about the pairwise depth relations (and thus also not via absolute depth); though
their model is approximated to consist of blocks only. Other authors have simpli-
ﬁed the task to be more robust and incorporated strong orientation priors such
as vanishing points and lines or Manhattan world constrains .
In this work we aim to extract surface normals for each pixel in a single image without any measured knowledge about the underlying 3D scene geometry.
We present a discriminative learning approach to estimate pixel-wise surface
orientation solely from the image appearance. We do not incorporate any kind
of geometric priors; rather we utilize recent work in image labelling as often
used for semantic image segmentation, where context enhanced pixel-based and
segment-based feature representations proved best performances. For the semantic segmentation problem it is reasonable to assume that all pixels within
a detected segment share the same label, i.e. segments correspond to objects.
However, for normal estimation this assumption of label-consistency holds only
for planar regions, such as segments on a wall; for segments related to non-planar
objects, e.g. a cylindrical shaped pot, it is violated.
We account for this property and propose a feature representation, that combines the cues of pixel-wise and segment-based methods. The strength of our
approach stems from the fact that we join both representations and intrinsically
learn, when to use which. It has the desired eﬀect that results tend to follow segment (and by this object) boundaries, but do not necessarily have to. Then we
formulate the surface normal estimation as a regression of coeﬃcients of the local coding, to make the learning problem more discriminative. Finally, we adapt
the standard boosting framework to deal with this speciﬁc learning problem.
The whole pipeline is illustrated in Figure 1. We apply our method to two data
sets from two diﬀerent man-made environments - indoors and outdoors .
L’. Ladick´y, B. Zeisl, and M. Pollefeys
Fig. 1. Workﬂow of the algorithm. In the training stage images are segmented using
multiple unsupervised segmentations, dense features are extracted and discriminative
feature representations combining contextual and segment-based features are built.
Ground truth normals are approximated using the local coding by a weighted sum of
representative normals and the discriminative regressor for these coeﬃcients is trained.
In the test stage the likelihood of each representative normal is predicted by the classiﬁer and the output normal is recovered as a weighted sum of representative normals.
The colours of the half-sphere represent corresponding normal directions.
Our classiﬁer obtains surprisingly good results, successfully recognizing surface
normals for a wide range of diﬀerent scenes.
Our paper is structured as follows: In Section 2 we explain the details of
our feature representation and draw connections to related work. Our learning
procedure is illustrated in Section 3. In Section 4 we describe more implementation details, in Section 5 the acquisition of the ground truth data, and ﬁnally
Section 6 reports our experimental results.
Feature Representation for Surface Normal Estimation
The standard label prediction pipeline for recognition tasks in computer vision consists of dense or sparse feature extraction, the composition of suitable
compact discriminative feature representations, and the application of an appropriate machine learning algorithm, capable of discriminating between diﬀerent
labels corresponding to the diﬀerent possible outcomes for the given task. For
pixel-wise tasks, such as semantic segmentation, the feature representations are
typically built either over pixels, or alternatively over segments (a.k.a. superpixels), obtained using any unsupervised segmentation technique .
Next we elaborate more on both approaches.
Context-Based Pixel-Wise Methods
For pixel-based approaches only a local feature vector of a pixel itself is insuﬃcient to predict the label. Thus, a certain form of context ,
Discriminatively Trained Dense Surface Normal Estimation
combining information from neighbouring pixels capturing a spatial conﬁguration of features, has to be used. The context is captured either using a pixelbased or rectangle-based context representation. In a pixel-based context approach , the contextual information for a given pixel is obtained by concatenating individual feature vectors of neighbouring pixels, placed at a ﬁxed set
of displacements from the reference pixel. In a rectangle-based context
approach , the feature representation for a pixel j is obtained by concatenating bag-of-words representations bow(ri + j) for a ﬁxed set of rectangles ri ∈R, placed relative to the pixel j; i.e. xj
CXT = [bow(r1 + j), bow(r2 +
j), .., bow(r|R| + j)]. For both forms of context, multiple local features can be
used jointly .
In practice, even for a small data set it is impossible to store these huge
feature representations for all training samples in memory. Typically, the feature
representation values are evaluated on the ﬂy during the learning process. For
a pixel-based context, the context is quickly evaluated from individual feature
vector maps stored in memory. For a rectangle-based approach, the contextual
information is obtained eﬃciently using integral images, calculated individually
for each visual word. The response for one individual dimension, corresponding
to the number of certain visual words in a rectangle, placed relative to the given
pixel, is often referred to as a shape-ﬁlter response .
The predictions using context based approaches are typically very noisy, do
not follow object boundaries, and thus require some form of regularization .
The rectangle-based context representation is typically more robust and leads to
better performance quantitatively . On the other hand the pixel-based
approach is much faster and with a suitable learning method it can be evaluated
in real-time during testing. In this work we are more interested in high
performance of our method, and thus we build on the superior rectangle-based
rather than the faster pixel-based context.
Segment-Based Methods
Segment-based methods are built upon the assumption, that predicted labels are consistent over segments obtained by unsupervised segmentation. This assumption plays two signiﬁcant roles. First, the learning and
evaluation over pixels can be reduced to a much smaller problem over segments,
which allows for more complex and slower learning methods to be used, such
as kernel SVMs . Second, it allows us to build robust feature representations by combining features of all pixels in each segment. The most common
segment-based representation is a L1-normalized bag-of-words ,
modelling the distribution of visual words within a segment. Recently, several
other alternatives beyond bag-of-words have been proposed , suitable
for labelling of segments.
All standard segmentation methods have free colour and spatial
range parameters, that can be tuned specially for each individual task or data set,
and are either hand-tuned or chosen based on an appropriate quality measure 
to satisfy the label consistency in segments. However, even choosing the best
L’. Ladick´y, B. Zeisl, and M. Pollefeys
Input Image
Mean-shift 
Normalized cut 
Graph-cut 
The example segmentations obtained by 4 diﬀerent unsupervised methods . The segments largely diﬀer in terms of smoothness, shape consistency
or variances of size. The notion of their quality largely depends on the task they are
applied to. For semantic segmentation similar sized segments have typically more discriminant feature representations, but methods producing segments of diﬀerent scales
are more suitable for enforcing label consistency in segments . For normal estimation a single unsupervised segmentation method can not produce label-consistent
segments in general, e.g. the lamp in an image is not planar at any scale. Optimally, the
learning method should decide by itself, which method – if any – and which features
are more suitable for each speciﬁc task.
unsupervised segmentation method is harder than it seems. Human perception
of the segment quality is very misleading, see Figure 2. Methods producing
segments of large variation in size , capturing information over the right
scale, may look visually more appealing, but the feature representations obtained
using methods producing similar sized segments may be more stable,
and thus more discriminative. Choosing the right parameters is even harder;
to obtain segments that will not contain multiple labels, the parameters of the
unsupervised segmentation method must be chosen to produce a large number
of very small segments. However, at that point the information in each segment
is often not suﬃcient to correctly predict the label. Two kinds of approaches
have been proposed to deal with this problem. In , the feature representation
of segments also includes a feature representation of the union of neighbours to
encode contextual information. Alternatively in multiple segmentations are
combined in the CRF framework by ﬁnding the smooth labelling that agrees
with most of the predictions of individual pixel-wise and segment-wise classiﬁers
and enforces label-consistency of segments as a soft constraint (see also ).
For normal estimation, the assumption of label-consistency is even more
damming. It would imply, that all segments must be planar. It is a very good
assumption for ﬂoor or walls, however, some objects are intrinsically not planar,
such as cylindrical thrash bins or spherical lamp shades.
Joint Context-Based Pixel-Wise and Segment-Method
In our approach we propose a joint feature representation, that can deal with the
weaknesses of individual context-based and segment-based methods. In particular, we overcome the inability of context-based approaches to produce smooth
labellings tightly following boundaries and to capture the information on the
correct object-based level, and the inability of segment-based methods to learn
Discriminatively Trained Dense Surface Normal Estimation
from a suitable-sized context. Unlike in , the contextual and segment cues
are combined directly during in the learning stage.
This can be achieved by a very simple trick. Any learning method deﬁned
over segments, with a loss function weighted by the size of the segment, is
equivalent to a learning method deﬁned over pixels, with the feature vector
SEG = x′s(j)
SEG, where s(j) is the segment the pixel j belongs to, and x′k
is any segment-based feature representation of the segment k. This transformation allows us to trivially combine feature representations over multiple segmentations as xj
MSEG = (xj
SEG2, .., xj
SEGN ), where xj
SEGi is the representation for an i-th segmentation. Learning over such pixel representations
becomes equivalent to learning over intersections of multiple segmentations .
And ﬁnally, we concatenate this representation with contextual information
MSEG). For normal estimation, this representation is powerful
enough to learn the properties, such as wall-like features are more discriminative for segments from a particular segmentation or context-driven features can
determine correct normals for spherical objects. Unlike in , the framework is
able to potentially enforce label inconsistency in segments, which are identiﬁed
to be non-planar.
Learning Normals
Due to a large dimensionality of the problem, we preferred learning algorithms,
that use only a small randomly sampled subset of dimensions in each iteration,
such as random forests or Ada-boost . Direct application of a even a
simple linear regression would not be feasible. In practice Ada-boost typically
performs better in terms of performance , random forests in terms of
speed. Similarly to the choice of contextual representation, we chose better over
faster. Intuitively the most discriminative contextual features will correspond to
the local conﬁguration of corner-like dense features, each one discriminant for a
narrow range of normals. Thus, we make the learning problem simpler by lifting
it to the problem of regressing coeﬃcients of local coding , typically
used in the feature space to increase the discriminative power of linear SVM
classiﬁers. Standard multi-class Ada-boost is designed for classiﬁcation over
a discrete set of labels, not for continuous regression. We adapt the learning
algorithm to deal with a set of continuous ground truth labels, both during
training and evaluation.
Local Coding in the Label Space
We start by performing standard k-means clustering on the set of ground truth
normals nj in the training set. In each iteration we back-project each cluster
mean to the unit (half-)sphere. We refer to the cluster mean as the reference
normal Nk. The Delaunay triangulation is evaluated on the set of reference normals to obtain the set of triangles T , where each triangle ti ∈T is an unordered
L’. Ladick´y, B. Zeisl, and M. Pollefeys
triplet of cluster indexes {t1
i }. For each ground truth normal nj we ﬁnd
the closest triangle t(j) by solving the non-negative least squares problem :
t(j) = arg min
such that 3
i = 1 and αj
i ≥0, ∀p ∈{1, 2, 3}. Each ground truth normal
is approximated by nj ≈
kNk, where 3 potentially non-zero coeﬃcients αj
come from the corresponding problem (1) for the triplet in t(j) and for all other
coeﬃcients αj
k = 0. In general, any reconstruction based local coding can be
Multi-class Boosting with Continuous Ground Truth Labels
A standard boosting algorithm builds a strong classiﬁer H(x, l) for a feature
vector x and a class label l ∈L as a sum of weak classiﬁers h(x, l) as:
h(m)(x, l),
where M is the number of iterations (boosts). The weak classiﬁers h(x, l) are
typically found iteratively as: H(m)(x, l) = H(m−1)(x, l) + h(m)(x, l).
Standard multi-class Ada-boost with discrete labels minimizes the expected exponential loss:
e−zlH(x,l)
where the zl ∈{−1, 1} is the membership label for a class l. A natural extension
to continuous ground truth labels is to minimize the weighted exponential loss
deﬁned as:
αle−H(x,l) + (1 −αl)eH(x,l)
where αl is the coeﬃcient of a cluster mean l of the local coding, in our case
corresponding to a reference normal. This cost function can be optimized using
adaptive Newton steps by following the procedure in . Each weak classiﬁer
h(m)(x, l) is chosen to minimize the second order Taylor expansion approximation of the cost function (4). Replacing expectation by an empirical risk leads
to a minimization of the error :
l e−H(m−1)(xj,l)(1 −h(m)(xj, l))2
l )eH(m−1)(xj,l)(1 + h(m)(xj, l))2).
Discriminatively Trained Dense Surface Normal Estimation
Deﬁning two sets of weights:
l e−H(m−1)(xj,l),
l )eH(m−1)(xj,l),
the minimization problem transforms into:
(1 −h(m)(xj, l))2 + vj,(m−1)
(1 + h(m)(xj, l))2).
The weights are initialized to wj,(0)
l and vj,(0)
l and updated
iteratively as:
= wj,(m−1)
e−h(m)(xj,l),
= vj,(m−1)
eh(m)(xj,l).
The most common weak classiﬁer for multi-class boosting are generalized decision stumps, deﬁned as :
h(m)(x, l) =
a(m)δ(xi(m) > θ(m)) + b(m)
if l ∈L(m)
otherwise,
where xi(m) is one particular dimension of x, L(m) ⊆L is the subset of labels
the decision stump is applied to; and i(m), a(m), b(m), k(m)
and θ(m) parameters
of the weak classiﬁer.
In each iteration the most discriminant weak classiﬁer is found by randomly
sampling dimensions i(m) and thresholds θ(m) and calculating the set of remaining parameters a(m), b(m), k(m)
and L(m) by minimising the cost function (8).
The parameters a(m), b(m) and k(m)
are derived by setting the derivative of (8)
to 0, leading to a close form solution:
j(wj,(m−1)
)δ(xi(m) ≤θ(m))
j(wj,(m−1)
+ vj,(m−1)
)δ(xi(m) ≤θ(m))
j(wj,(m−1)
)δ(xi(m) > θ(m))
j(wj,(m−1)
+ vj,(m−1)
)δ(xi(m) > θ(m))
j(wj,(m−1)
j(wj,(m−1)
+ vj,(m−1)
, ∀l ̸∈L(m).
The subset of labels L(m) is found greedily by iterative inclusion of additional
labels, if they decrease the cost function (8).
L’. Ladick´y, B. Zeisl, and M. Pollefeys
Prediction of the Surface Normal
During test time the responses H(x, l) for each reference normal are evaluated
and the most probable triangle is selected by maximizing:
t(x) = arg max
The non-zero local coding coeﬃcients for each index k of a triangle t(x) are
obtained as:
eH(x,t(x)k)
p=1 eH(x,t(x)p) ,
and the resulting normal nj(x) for a pixel j is recovered by computing the linear
combination projected to the unit sphere:
t(x)pNt(x)p
t(x)pNt(x)p|
corresponding to the expected value under standard probabilistic interpretation
of boosted classiﬁer . Weighted prediction leads to better performance both
qualitatively and quantitatively (see Figure 6).
Implementation Details
The complete work ﬂow of our method is shown in the Figure 1. In our implementation four dense features were extracted for each pixel in each image texton , SIFT , local quantized ternary patters and self-similarity
features . Each feature was clustered into 512 visual words using k-means
clustering, and for each pixel a soft assignment for 8 nearest cluster centres is
calculated using distance-based exponential kernel weighting . The rectanglebased contextual part of the feature representation consists of a concatenation
of soft-weighted bag-of-words representations over 200 rectangles, resulting in
200 × 4 × 512 dimensional feature vector xj
CXT . The Segment-based part xj
consists of soft-weighted bag-of-words representations over 16 unsupervised segmentations obtained by varying kernel parameters of 4 diﬀerent methods, 4 segmentations each - Mean-shift , SLIC , normalized cut and graph-cut
based segmentation . In the boosting process, the same number of dimensions from the contextual and segment part of the feature representation were
sampled in each iteration, to balance diﬀerent dimensionality of these representations. This was achieved by increasing the sampling probability of each
dimension of the segment-part 200
16 times. The strong classiﬁer consisted of 5000
weak classiﬁers. The whole learning procedure has been applied independently
for 5 diﬀerent colour models - RGB, Lab, Luv, Opponent and GreyScale. Each
individual classiﬁer was expected to perform approximately the same, and thus
Discriminatively Trained Dense Surface Normal Estimation
the ﬁnal classiﬁer response was simply averaged over these 5 classiﬁers without
any additional training of weights for each individual colour space. In practice
this averaging procedure has similar eﬀects to the use of multiple decision trees
in the random forest; it leads to smoother results and avoids over-ﬁtting to noise.
Ground Truth Acquisition
Required ground truth measurements about the underlying 3D scene geometry
can be captured with active devices, such as laser scanners, commodity depth
sensors, stereo cameras or from dense 3D reconstructions of image collections.
In all cases the depth measurements are likely to contain noise, which will get
ampliﬁed in their ﬁrst derivatives. Since our aim is to obtain piecewise constant
normal directions – as reﬂected typically in man-made environments – we leverage (second order) Total Generalized Variation (TGV) for denoising. The
optimization is formulated as a primal-dual saddle point problem and solved
via iterative optimization; for more detail we refer the interested reader to .
Normals are then computed on the 3D point cloud for each point in a local
3D spatial neighborhood. Compared to computations on the depth map itself,
this guarantees that measurements of distant structures in 3D which project to
neighboring pixels do not get intermixed. Finally, for point-wise normal estimation we utilize a least squares regression kernel in a RANSAC scheme in order
to preserve surface edges. Given the quality of the depth data, obtained ground
truth normals look visually signiﬁcantly better than the direct ﬁrst derivatives
of the original raw depth data. However, the quality of normals often degrades
in the presence of reﬂective surfaces, near image edges or in regions without
suﬃcient amount of direct depth measurements.
Experiments
We trained our classiﬁer on the indoor NYU2 and on the outdoor KITTI 
data set to demonstrate the ability of our method to predict normals in various
man-made environments.
NYU2 Data Set
The NYU2 data set consists of 795 training and 654 test images of resolution
640×480, containing pixel-wise depth obtained by a Kinect sensor. The data set
covers a wide range of types of indoor scenes, such as oﬃces, bedrooms, living
rooms, kitchens or bathrooms. To train the classiﬁer, the ground truth normals
were clustered into 40 reference normals. The mean angle between neighbouring
reference normals was 18 degrees. The training of the classiﬁer took three weeks
on ﬁve 8-core machines. Thus, the parameters of our method (such as number of
normal clusters, segmentations or boosts) have been chosen based on our expert
knowledge and not tweaked at all. The evaluation took 40 minutes per image on
L’. Ladick´y, B. Zeisl, and M. Pollefeys
Input Image
Our result
Ground Truth
Input Image
Our result
Ground Truth
Fig. 3. Qualitative results on NYU2 data set. The colours, corresponding to diﬀerent
normals, are shown in Figure 1. Our algorithm consistently predicted high quality
surface normals for a single image. Note, the imperfect quality of the ground truth
labelling (see for example the image in bottom-right).
Discriminatively Trained Dense Surface Normal Estimation
Input Image
Our result
3DP result
Input Image
Our result
3DP result
Fig. 4. Qualitative comparison of our method with 3DP on the RMRC data set.
Both methods were trained on the NYU2 training data. Several images of RMRC data
set were taken from unusual viewpoints, not present in the NYU2 data set, causing
troubles to both methods.
Input Image
Our result
Input Image
Our result
Input Image
Our result
Fig. 5. Additional results of our method using the classiﬁer trained on NYU2 data set
L’. Ladick´y, B. Zeisl, and M. Pollefeys
Fig. 6. Quantitative comparison of locally coded and hard assigned version of our
method with 3DP method on the NYU2 data set. The performance is evaluated in term of ratio of pixels within diﬀerent angular error (10, 20, 30, 40 and 50)
and calculated either over the masks, corresponding to the regions with direct depth
measurements; or over the whole image. Our method estimated approximately half of
normals in the masked region within 20 degree error. The numbers do not fully reﬂect
the quality of our results due to the imperfection of the ground truth (see Figure 3).
Input Image
Our result
Input Image
Our result
Fig. 7. Qualitative results of our method on the KITTI data set. The ground truth
colours are the same as for the NYU2 data set. Our classiﬁer essentially learnt the
typical geometrical layout of the scene and the spatial conﬁguration of surface normals
of cars seen from various viewpoints.
Discriminatively Trained Dense Surface Normal Estimation
a single core; however, it can be easily parallelized. The results are signiﬁcantly
better than initially expected. Our classiﬁer consistently managed to successfully
predict normals for various complicated indoor environments. The qualitative results are shown in Figure 3. Quantitative comparisons of our classiﬁer (weighted
by local coding coeﬃcients and hard-assigned to the normal cluster with the
highest response) to the state-of-the-art are shown in Figure 6. The results
are reported for full images (561×427 sub-window) and on the masks (as in ),
that deﬁne the regions containing direct depth measurements. Approximately for
one half of the pixels in the masks the predicted normals were within 20 degrees
angle. The numbers do not reﬂect the quality of our results, because even in a
ﬂat surfaces the normals of the ground truth often vary by 10 or 20 degrees (see
for example the ground truth of the bottom-right image in Figure 3). To get an
idea of the interpretation of the error, the average angular error of the visually
very appealing result on the test image in Figure 1 is 28 degrees.
The success of our method on this data set lead us to further experiments
using the already trained classiﬁer. We applied it to the Reconstruction-Meets-
Recognition depth challenge (RMRC) data set , consisting of 558 images.
Qualitative comparisons with the method are shown in Figure 4. Ground
truth depth images are not publicly available. Our method was able to successfully predict normals for images that looked visually similar to the NYU2 data.
However, for images, that were not taken upright (as in NYU2), our classiﬁer
predicted normals as if they were. We evaluated our classiﬁer also on images
captured by ourselves, see Figure 5.
KITTI Data Set
The KITTI depth data set consists of 194 training images and 195 test outdoor
images, containing sparse disparity maps obtained by a Velodyne laser scanner.
The distribution of normals within an image seemed much more predictable
than for indoor scenes, due to a very typical image layout and lower variety of
normals. Thus, to train a classiﬁer we clustered normals only into 20 clusters.
The training took ﬁve days on ﬁve 8-core machines. The evaluation took 20
minutes per image on a single core. Qualitative results are shown in Figure 7.
The ground truth depths are not publicly available, thus we do not provide any
quantitative results.
Conclusions and Future Work
In this paper we proposed a method for dense normal estimation from RGB
images by combining state-of-the-art context-based and segment-based cues in a
continuous Ada-Boost framework. The results have the potential to be applied
to several other reconstruction problems in computer vision, such as stereo,
single-view or 3D volumetric reconstruction, as a geometric prior for their regularization. In the future we would like to do further research along these lines.
L’. Ladick´y, B. Zeisl, and M. Pollefeys