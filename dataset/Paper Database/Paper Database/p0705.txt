Pre-trained Language Model Representations for Language Generation
Sergey Edunov∗, Alexei Baevski∗, Michael Auli
Facebook AI Research
Menlo Park, CA
Pre-trained language model representations
have been successful in a wide range of language understanding tasks. In this paper, we
examine different strategies to integrate pretrained representations into sequence to sequence models and apply it to neural machine translation and abstractive summarization.
We ﬁnd that pre-trained representations are most effective when added to the encoder network which slows inference by only
14%. Our experiments in machine translation
show gains of up to 5.3 BLEU in a simulated
resource-poor setup. While returns diminish
with more labeled data, we still observe improvements when millions of sentence-pairs
are available. Finally, on abstractive summarization we achieve a new state of the art on
the full text version of CNN-DailyMail. 1
Introduction
Pre-training of language models has been shown
to provide large improvements
for a range
of language understanding tasks .
The key idea is to train a
large generative model on vast corpora and use
the resulting representations on tasks for which
only limited amounts of labeled data is available.
Pre-training of sequence to sequence models has
been previously investigated for text classiﬁcation but not for text generation. In neural machine translation, there has been
work on transferring representations from highresource language pairs to low-resource settings
 .
In this paper, we apply pre-trained representations from language models to language genera-
∗Equal contribution.
pre-trained
 
bi_trans_lm/examples/pretraining
tion tasks that can be modeled by sequence to sequence architectures. Previous work on integrating language models with sequence to sequence
models focused on the decoder network and added
language model representations right before the
output of the decoder . We
extend their study by investigating several other
strategies such as inputting ELMo-style representations or ﬁne-tuning the language model (§2).
Our experiments rely on strong transformerbased language models trained on up to six billion tokens (§3). We present a detailed study of
various strategies in different simulated labeled
training data scenarios and observe the largest improvements in low-resource settings but gains of
over 1 BLEU are still possible when ﬁve million
sentence-pairs are available. The most successful
strategy to integrate pre-trained representations is
as input to the encoder network (§4).
Strategies to add representations
We consider augmenting a standard sequence to
sequence model with pre-trained representations
following an ELMo-style regime (§2.1) as well as
by ﬁne-tuning the language model (§2.2).
ELMo augmentation
The ELMo approach of Peters et al. forms
contextualized word embeddings based on language model representations without adjusting
the actual language model parameters.
Speciﬁcally, the ELMo module contains a set of parameters λ1 . . . λL, γ to form a linear combination of
the L layers of the language model: ELMo =
Z exp(λi)hk where γ is a learned scalar,
Z is a constant to normalize the exp(λi) to sum
to one and hk is the output of the k-th language
model layer; the module also considers the input
word embeddings of the language model. We also
apply layer normalization to each
hk before computing ELMo vectors.
We experiment with an ELMo module to input
contextualized embeddings either to the encoder
(SRC-ELMO) or the decoder (TGT-ELMO).
provides word representations speciﬁc to the current input sentence and these representations have
been trained on much more data than is available
for the text generation task.
Fine-tuning approach
Fine-tuning the pre-trained representations adjusts the language model parameters by the learning signal of the end-task .
We replace learned input
word embeddings in the encoder network with the
output of the language model (SRC-FT). Speciﬁcally, we use the language model representation of
the layer before the softmax and feed it to the encoder. We also add dropout to the language model
output. Tuning separate learning rates for the language model and the sequence to sequence model
may lead to better performance but we leave this
to future work. However, we do tune the number
of encoder blocks N as we found this important to
obtain good accuracy for this setting. We apply the
same strategy to the decoder: we input language
model representations to the decoder network and
ﬁne-tune the language model when training the sequence to sequence model (TGT-FT).
Experimental setup
Pre-training.
We train language models on two
languages: One model is estimated on the German newscrawl distributed by WMT’18 comprising 260M sentences or 6B tokens. Another model
is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint
Byte-Pair-Encoding 
vocabulary of 37K types on the German and English newscrawl and train the language models
with this vocabulary.
Machine translation.
We consider two benchmarks: Most experiments are run on the WMT’18
English-German (en-de) news translation task and
we validate our ﬁndings on the WMT’18 English-
Turkish (en-tr) news task. For WMT’18 English-
German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we
remove sentences longer than 250 tokens as well
as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence
pairs. We tokenize all data with the Moses tokenizer and apply the BPE vocabulary learned on the monolingual corpora.
For WMT’18 English-Turkish, we use all of the
available bitext comprising 208K sentence-pairs
without any ﬁltering. We develop on newstest2017
and test on newstest2018. For en-tr we only experiment with adding representations to the encoder
and therefore apply the language model vocabulary to the source side. For the target vocabulary
we learn a BPE code with 32K merge operations
on the Turkish side of the bitext. Both datasets are
evaluated in terms of case-sensitive de-tokenized
BLEU .2
Summarization.
DailyMail abstractive document summarization
task comprising over 280K news articles paired
with multi-sentence summaries. CNN-DailyMail
is a widely used dataset for abstractive text
summarization.
Following ,
non-anonymized
CNN-DailyMail
entity-anonymized version because the language model
was trained on full text. Articles are truncated to
400 tokens and we use a BPE
vocabulary of 32K types . We
evaluate in terms of F1-Rouge, that is Rouge-1,
Rouge-2 and Rouge-L .3
Language model pre-training
We consider two types of architectures:
a bidirectional language model to augment the sequence to sequence encoder and a uni-directional
model to augment the decoder.
self-attention and the unidirectional model contains N = 12 transformer
blocks, followed by a word classiﬁer to predict
the next word on the right.
The bi-directional
model solves a cloze-style token prediction task
at training time . The model
consists of two towers, the forward tower operates left-to-right and the tower operating rightto-left as backward tower; each tower contains
2sacreBLEU signatures:
BLEU+case.mixed+lang.en-
{de,tr}+numrefs.1+smooth.exp+test.wmt18+tok.13a
+version.1.2.1
parameters
ROUGE-1.5.5.pl: -m -a -n 2
Bitext tokens
BLEU delta wrt baseline
SRC-ELMO+SHDEMB
Figure 1: BLEU difference to a bitext-only baseline when adding pre-trained language model representations
to a neural machine translation model in different simulated bitext settings.
Results are based on averaging
newstest2012-2017 of WMT English-German translation.
12 transformer blocks.
The forward
and backward representations are combined via
a self-attention module and the output of this
module is used to predict the token at position i.
The model has access to the entire input surrounding the current target token.
Models use the standard settings for the Big Transformer . The bi-directional
model contains 353M parameters and the unidirectional model 190M parameters. Both models
were trained for 1M steps using Nesterov’s accelerated gradient with momentum 0.99 following Baevski and Auli .
The learning rate is linearly warmed up from
10−7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single
phase to 0.0001 .
We train on 32 Nvidia V100 SXM2 GPUs and
use the NCCL2 library as well as the torch distributed package for inter-GPU communication.
Training relies on 16-bit ﬂoating point operations and it took six days for the
bi-directional model and four days for the unidirectional model.
Sequence to sequence model
We use the transformer implementation of the
fairseq toolkit . The WMT en-de
and en-tr experiments are based on the Big Transformer sequence to sequence architecture with 6
blocks in the encoder and decoder. For abstractive
summarization we use a base transformer model
 . We tune dropout values of
between 0.1 and 0.4 on the validation set. Models
are optimized with Adam 
using β1 = 0.9, β2 = 0.98, and ǫ = 1e −8
and we use the same learning rate schedule as
Vaswani et al. ; we perform 10K-200K depending on bitext size.
All models use label
smoothing with a uniform prior distribution over
the vocabulary ǫ = 0.1 .
We run experiments on 8
GPUs and generate translations with a beam of
Machine translation
We ﬁrst present a comparison of the various strategies in different simulated parallel corpus size settings. For each experiment, we tune the dropout
applied to the language model representations,
and we reduce the number of optimizer steps for
smaller bitext setups as models converge faster;
all other hyper-parameters are equal between setups. Our baseline is a Big Transformer model
and we also consider a variant where we share token embeddings between the encoder and decoder
 .
Figure 1 shows results averaged over six test
sets relative to the baseline which does not share
source and target embeddings (Appendix A shows
SRC-ELMO+SHDEMB
Table 1: BLEU on newstest2018 of WMT English-
German in three simulated bitext size scenarios.
SRC-ELMO+SHDEMB
Table 2: WMT English-Turkish translation results in
terms of BLEU on newstest2017 (valid) and newstest2018 (test) with ELMo inputs to the encoder.
a detailed breakdown).
SHARED performs very
well with little labeled data but the gains erode to
practically zero in large bitext settings. Pre-trained
language model representations are most effective
in low bitext setups. The best performing strategy
is ELMo embeddings input to the encoder (SRC-
ELMO). This improves the baseline by 3.8 BLEU
in the 160K bitext setting and it still improves the
5.2M setting by over 1 BLEU.
We further improve
SRC-ELMO by sharing
learned word representations
in the decoder
by tying input and output embeddings (SRC-
ELMO+SHDEMB).
This conﬁguration performs
even better than SRC-ELMO with a gain of 5.3
BLEU in the 160K setup. Sharing decoder embeddings is equally applicable to SRC-FT. Language
model representations are much less effective in
the decoder:
TGT-FT improves the 160K bitext
setup but yields no improvements thereafter and
TGT-ELMO performs even worse. We conjecture
that pre-trained representations give much easier
wins in the encoder. Table 1 shows additional results on newstest2018.
Pre-trained representations mostly impacts the
training time of the sequence to sequence model
(see Appendix B): SRC-ELMO slows throughput
during training by about 5.3x and SRC-FT is
even slower because of the need to backpropagate through the LM for ﬁne-tuning (9.2x). However, inference is only 12-14% slower than the
See et al. 
Gehrmann et al. 
SRC-ELMO+SHDEMB
Table 3: Abstractive summarization results on CNN-
DailyMail. ELMo inputs achieve a new state of the art.
baseline when adding pre-trained embeddings to
the encoder (SRC-ELMO, SRC-FT).
This is because the LM computation can be paralelized for
all input tokens. Inference is much slower when
adding representations to the decoder because the
LM needs to be invoked repeatedly. Our current
implementation does not cache LM operations for
the previous state and can be made much faster.
The baseline uses a BPE vocabulary estimated
on the language model corpora (§3). Appendix A
shows that this vocabulary actually leads to sligtly
better performance than a joint BPE code learned
on the bitext as is usual.
Next, we validate our ﬁndings on the WMT’18
English-Turkish task for which the bitext is truly
limited (208K sentence-pairs).
We use the language model vocab for the the English side of the
bitext and a BPE vocabulary learned on the Turkish side. Table 2 shows that ELMo embeddings for
the encoder improve English-Turkish translation.
Abstractive summarization
Following See et al. , we experiment on
the non-anonymized version of CNN-DailyMail.
When generating summaries, we follow standard
practice of tuning the maximum output length and
disallow repeating the same trigram .
For this task we train
language model representations on the combination of newscrawl and the CNN-DailyMail training data.
Table 3 shows that pre-trained embeddings can signiﬁcantly improve on top of a
strong baseline transformer. We also compare to
Gehrmann et al. who use a task-speciﬁc architecture compared to our generic sequence to sequence baseline. Pre-trained representations are
complementary to their method.
Conclusion
We presented an analysis of different strategies to
add pre-trained language model representations to
sequence to sequence models for neural machine
translation and abstractive document summarization. Adding pre-trained representations is very
effective for the encoder network and while returns diminish when more labeled data becomes
available, we still observe improvements when
millions of examples are available. In future research we will investigate ways to improve the decoder with pre-trained representations.