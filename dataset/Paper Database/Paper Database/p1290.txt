Zurich Open Repository and
University of Zurich
University Library
Strickhofstrasse 39
CH-8057 Zurich
www.zora.uzh.ch
Year: 2006
Bayesian auxiliary variable models for binary and multinomial regression
Holmes, C C ; Held, L
DOI: 
Posted at the Zurich Open Repository and Archive, University of Zurich
ZORA URL: 
Journal Article
Originally published at:
Holmes, C C; Held, L . Bayesian auxiliary variable models for binary and multinomial regression. Bayesian
Analysis, 1(1):145-168.
DOI: 
Bayesian Analysis 
1, Number 1, pp. 145–168
Bayesian Auxiliary Variable Models for Binary
and Multinomial Regression
Chris C. Holmes∗
Leonhard Held†
In this paper we discuss auxiliary variable approaches to Bayesian
binary and multinomial regression.
These approaches are ideally suited to automated Markov chain Monte Carlo simulation. In the ﬁrst part we describe a
simple technique using joint updating that improves the performance of the conventional probit regression algorithm.
In the second part we discuss auxiliary
variable methods for inference in Bayesian logistic regression, including covariate
set uncertainty. Finally, we show how the logistic method is easily extended to
multinomial regression models. All of the algorithms are fully automatic with no
user set parameters and no necessary Metropolis-Hastings accept/reject steps.
Keywords: Auxiliary variables, Bayesian binary and multinomial regression, Markov
chain Monte Carlo, Model averaging, Scale mixture of normals, Variable selection
Introduction
Binary and polychotomous (or multinomial) regression using Generalised Linear Models
(GLMs) is a widely used technique in applied statistics and the Bayesian approach to
this subject is well documented ). Inference in
Bayesian GLMs is complicated by the fact that no conjugate prior exists for the parameters in the model other than for normal regression; this makes simulation diﬃcult. In
a seminal paper, Albert & Chib demonstrated an auxiliary variable approach for
binary probit regression models that renders the conditional distributions of the model
parameters equivalent to those under the Bayesian normal linear regression model with
Gaussian noise. In this case, conjugate priors are available to the conditional likelihood
and the block Gibbs sampler can then be used to great eﬀect. In this paper we describe
three extensions to the Albert and Chib approach. First, we highlight a simple technique to improve performance in probit regression simulation by jointly updating the
regression coeﬃcients and the auxiliary variables. Second, we show that the auxiliary
approach is also possible for logistic regression, by using a scale mixture of normals
representation for the noise process. The logistic model is an important extension, as
typically the logit link is the preferred method of choice for most statistical applications.
Although the logit and probit link tend to give similar goodness of ﬁt and qualitative
conclusions, the preference for logistic regression is due to the strong interpretation of
the regression coeﬃcients which then quantify the change to the log-odds of one class
over another for unit change in the associated covariate. Moreover, the logit link is analytic and has relatively heavy tails; this is in contrast to the probit link where high or
low predictive probabilities can induce numerical diﬃculties in calculating likelihoods.
∗University of Oxford, Oxford, U.K., 
†Ludwig-Maximilians-University, Munich, Germany, 
c⃝2006 International Society for Bayesian Analysis
Bayesian auxiliary variable models
Finally, we show that the logistic model is easily generalised to accommodate covariate
set uncertainty and to multinomial response data.
We believe the methods discussed here oﬀer a valuable extension to the current
literature by oﬀering fully automatic multivariate sampling schemes for Bayesian binary and polychotomous regression methods. Chen and Dey described a logistic
regression model based on the scale mixture representation. However, their approach
requires the evaluation of the mixing density, which is only known as an inﬁnite series
expansion; hence, they must resort to approximate numerical techniques. Moreover,
their method requires a Metropolis sampler, which introduces an accept-reject stage
into their algorithm. In contrast, our approach is exact, fully automatic (no acceptreject) and we present extensions to multinomial (multi-class, polychotomous) regression. Alternative strategies for the logistic model include Gamerman who uses a
normal approximation to the posterior density of the regression coeﬃcients found using
iterative-reweighted-least-squares and Dellaportas and Smith who suggest the
use of adaptive-rejection sampling (ARS) from the univariate conditional densities of
the coeﬃcients. The approach of Gamerman requires Metropolis-Hastings updates and
hence data dependent accept-reject steps. Our auxiliary variable method uses direct
sampling from the conditional distributions. The ARS algorithm does not suﬀer from
Metropolis-Hastings updates but does have the weakness of univariate updating of the
coeﬃcients; hence, if there is strong posterior dependence between the coeﬃcients, the
sampler will mix very poorly. In contrast, we provide a joint multivariate update scheme
for the regression parameters.
In Section 2 we present the methods and algorithms. The approach is also well
suited to generalisations of the standard binary regression model; in Section 2.5 we
describe such an application, namely, in covariate set uncertainty.
In Section 3 we
extend our approach to deal with polychotomous data. Finally, in Section 4 we oﬀer a
brief discussion, contrasting the approach to existing methods and pointing to possible
extensions.
An implementation of the various procedures written in pseudo code is
listed in the Appendix. MATLAB code is available from the ﬁrst author on request.
Data augmentation in binary regression models
To begin, consider the Bayesian binary regression model,
Bernoulli (g−1(ηi))
where yi ∈{0, 1}, i = 1, . . . , n is a binary response variable for a collection of n objects
with associated p covariate measurements xi = (xi1, . . . , xip), g(u) is a link function,
ηi denotes the linear predictor and β represents a (p × 1) column vector of regression
coeﬃcients which a priori are from some distribution π(·).
Chris C. Holmes and Leonhard Held
Probit regression using auxiliary variables
For the probit link — g−1(u) = Φ(u), where Φ(u) denotes the cumulative distribution
function of a standard normal random variable — the model in (1) has a well known
representation using auxiliary variables,
where yi is now deterministic conditional on the sign of the stochastic auxiliary variable
zi. Under independence of ǫi, i = 1, . . . , n, the marginal likelihood L(β|y) in model (2)
is the same as in (1).
The advantage of working with representation (2) is that, for judicious choice of
π(β), we can perform eﬃcient simulation using the block Gibbs sampler as reported in
Albert and Chib , hereafter A&C. In particular, in the case of a normal prior on
β, π(β) = N(b, v), the full conditional distribution of β is still normal,
V (v−1b + x′z)
(v−1 + x′x)−1,
where x = (x′
2, . . . , x′
n)′. The full conditional for each element zi is then truncated
zi|β, xi, yi
N(xiβ, 1) I(zi > 0)
N(xiβ, 1) I(zi ≤0)
otherwise,
which is straightforward to sample from, see for example Robert .
The auxiliary variable method oﬀers a convenient framework for Markov chain Monte
Carlo (MCMC) simulation by iteratively sampling from the conditional densities in (3)
and (4). However, a potential problem lurks in that there is strong posterior correlation
between β and z, clearly indicated in the model (2). In the standard A&C iterative
updating, this correlation is likely to cause slow mixing in the chain.
To combat this we suggest a simple approach that reduces autocorrelation and improves mixing in the Markov chain. We propose to update β and z jointly, making use
of the factorisation,
π(β, z|y) = π(z|y)π(β|z),
where the distribution π(β|z) is unchanged from above in (3) but now z is updated
from its marginal distribution having integrated over β. We assume from here on that
the prior for β is a mean zero normal density, N(0, v). From standard matrix algebra
we then obtain,
π(z|y) ∝N(0, In + xvx′)Ind(y, z)
Bayesian auxiliary variable models
where In denotes the n×n identity matrix and Ind(y, z) is an indicator function which
truncates the multivariate normal distribution of z to the appropriate region. Direct
sampling from the multivariate truncated normal is known to be diﬃcult; however, it
is straightforward to Gibbs sample the distribution,
zi|z−i, yi
N(mi, vi) I(zi > 0)
N(mi, vi) I(zi ≤0)
otherwise,
where z−i denotes the auxiliary variables z with the ith variable removed. The means
mi and variances vi, i = 1, . . . , n, are obtained from the leave-one-out marginal predictive densities. Using, for example, Henderson & Searle , we can calculate the
parameters eﬃciently as,
xiB −wi(zi −xiB)
hi/(1 −hi)
where zi is the current value for zi, B is taken from (3) and hi is the ith diagonal
element of the Bayesian hat matrix, hi = (H)ii, H = xV x′, with V deﬁned in (3).
Following an update to each zi we must recalculate the posterior mean B using, for
example, the relationship,
B = Bold + Si(zi −zold
where Bold and zold
denote the values of B and zi prior to the update of zi, and Si
denotes the ith column of S = V x′.
Note that the variance vi in (5) is always greater than one, which is the variance of the
conventional iterative sampler. During simulation, the calculation of S, wi and vi need
only be performed once prior to the MCMC loop. The procedure is best illustrated by
considering the pseudo-code listed in Appendix A1, with notation deﬁned in Appendix
The algorithm carries little increase in computational burden over the conventional
approach (see the comments at the bottom of the pseudo-code in A1). The use of joint
updating should improve mixing and sampling eﬃciency in the Markov chain. In the
next section we present an analysis on four data sets that bears witness to this.
Empirical test on four data sets
To illustrate the relative eﬃciency gains of the joint updating scheme we present results
from the analysis of four binary regression data sets. The ﬁrst data set is the Pima
Indian example used in Ripley ; the other three data sets are the Australian
Credit, Heart, and German Credit data sets used in the STATLOG project .
Chris C. Holmes and Leonhard Held
To test our procedure we simulated a Gibbs sampler for 10,000 iterations, for both
the conventional iterative algorithm and our joint update scheme.
Unless otherwise
stated, we take π(β) = N(0, 100Ip) from here on. To measure eﬃciency we recorded
the total CPU run time and the average Euclidean update distance for β between
iterations, measured as:
||β(i) −β(i+1)||
where β(i) denotes the ith of N MCMC samples. Larger values of Dist. indicate a better
mixing algorithm.
We also calculated the “eﬀective sample size”, ESS ). The
eﬀective sample size for a single coeﬃcient is calculated as:
ESS = M/(1 + 2
where M is the number of post burn in MCMC samples and Pk
j=1 ρ(k) is the sum of
the k monotone sample autocorrelations, estimated by the initial monotone sequence
estimator . The ESS is an estimate of the number of independent samples
needed to obtain a parameter estimate with the same precision as the MCMC estimate
considered based on M dependent samples. It was calculated for each coeﬃcient and
then averaged.
The results, averaged over 10 runs, along with some characteristics of the data sets,
are given in Table 1. The standard deviation around the mean was less than 10−2 times
the mean value for all results. The programs were written in MATLAB 6.5 and run on a
desktop PC. The ﬁnal two columns in Table 1 record the relative eﬃciency of the joint
updating approach over the iterative scheme having standardised for CPU run time;
the penultimate column lists the relative updating distance to β while the ﬁnal column
lists the relative eﬀective sample size.
The improvement of the joint updating scheme is substantial, giving up to a twofold improvement in the parameter distance jumped between updates to β while also
reducing the autocorrelation in the chain, leading to around a 50% improvement in the
eﬀective sample size. These results are obtained for minimal increase in algorithmic
complexity as shown in Appendix A1. The results are perhaps not surprising given the
extent of the posterior correlation in the model (2).
Logistic regression with auxiliary variables
Consider the model in (2).
If we now take ǫi ∼π(ǫi) to be the standard logistic
distribution, then we obtain the logistic regression model.
As it stands we lose the
conditional conjugacy for updating β.
However, we can introduce a further set of
Bayesian auxiliary variable models
Rel. Dist.
Table 1: Table listing performance measures for Section 2.2 on four data sets, comparing the
conventional iterative updating (Itr.) with our joint update scheme. The ﬁnal two columns
present the relative eﬃciency of the schemes having standardised for CPU run time.
measures Dist. and ESS are deﬁned in equations (6) and (7) respectively.
variables, λi, i = 1, . . . , n, and note the additional representation
where ψi, i = 1, . . . , n, are independent random variables following the Kolmogorov-
Smirnov (KS) distribution ). In this case, ǫi has a scale mixture of
normal form with a marginal logistic distribution , so that
the marginal likelihood L(β|y) for models (8) and (1) with logit link are equivalent.
As before, in the case of a normal prior on β, π(β) = N(b, v), the full conditional
distribution of β given z and λ is still normal,
V (v−1b + x′W z)
(v−1 + x′W x)−1,
1 , . . . , λ−1
and the full conditional for zi is still truncated normal, but now with individual variances
zi|β, xi, yi, λi
N(xiβ, λi) I(zi > 0)
N(xiβ, λi) I(zi ≤0)
otherwise.
Finally, the conditional distribution π(λi|zi, β) does not have a standard form. However, it is simple to generate from, which is the only important issue, using rejection
sampling as outlined in Appendix A4.
Chris C. Holmes and Leonhard Held
The above speciﬁcation allows for automatic sampling from the Bayesian logistic
regression model using iterative updates, say, (β|z, λ) followed by (z|β, λ) and then
The sampling scheme will be slower than that for the probit case, as not
only must we sample λ, but also the posterior variance-covariance matrix V in (9) will
change for each update of λ.
As in Section 2.1, we can look to improve matters through joint updating. Interestingly there are two options here. On the one hand, we can follow the procedure outlined
in Section 2.1 and update {z, β} jointly given λ,
π(z, β|y, λ) = π(z|y, λ)π(β|z, λ),
followed by an update to λ|z, β; the pseudo-code for this method is given in Appendix
A2. On the other hand we can update {z, λ} jointly given β,
π(z, λ|β, y) = π(z|β, y)π(λ|z, β)
followed by an update to β|z, λ. In this latter case the marginal densities for the zi’s
are independent truncated logistic distributions,
zi|β, xi, yi
 Logistic(xiβ, 1) I(zi > 0)
Logistic(xiβ, 1) I(zi ≤0)
otherwise,
where Logistic(a, b) denotes the density function of the logistic distribution with mean
a and scale parameter b . An advantage of this latter approach
is that sampling from the truncated logistic distribution can be done eﬃciently by the
inversion method, because both the distribution function and its inverse have simple
analytic forms; the pseudo-code for this approach is given in Appendix A3.
Empirical test of logistic model on four data sets
In Table 2 we repeat the analysis of Section 2.2 but here we compare the two joint
update schemes for the logistic model. We ran the Gibbs sampler 10 times each for
10,000 iterations, 1,000 of which are taken as a burn in.
The averaged results are
presented in Table 2. The standard deviation around the mean was no greater than
10−2 times the mean value for all results. In Table 2, scheme A (columns 2-4) refers
to the results for updating {z, λ} jointly; scheme B (columns 5-7) lists the results for
updating {z, β} jointly. The ﬁnal two columns compare the relative eﬃciency after we
have standardised for CPU run time.
It is interesting to compare Tables 1 and 2. We see, as expected, that the logistic sampler takes considerably longer to run due to the extra computational burden of
sampling the auxiliary mixing variances and having to invert the posterior covariance
matrix for β at each iteration. Having said that, the longest run time for the logistic
sampling was still under 10 minutes for 10,000 samples using a non-compiled language
(MATLAB). Another striking feature is that the average distance jumped between iterations is much larger for the logistic model; this is due to the larger variance π2/3 of
the logistic distribution compared to the standard normal.
Bayesian auxiliary variable models
Rel. Dist.
Table 2: Table listing performance measures of the two joint sampling schemes in Section 2.3,
on the four data sets described in Section 2.2. Scheme A uses a joint update to {z, λ} while
Scheme B uses a joint update to {z, β}. The measures Dist. and ESS are deﬁned in equations
(6) and (7)
Comparing the procedures within Table 2, we see that scheme A is consistently
faster to run than scheme B. This is due to a combination of the simple form for logistic
sampling of z (by inversion) and also that this procedure can be written in vector form
in MATLAB, which proves much faster than looping. It would be interesting to see how
they compare under a compiled language.
The relative eﬃciencies shown in the rightmost two columns of Table 2 are also
interesting. Scheme A is more eﬃcient with regards to the eﬀective sample size measure;
this suggests that the logistic updates to z appear to reduce the autocorrelation in the
β samples. However, the expected jump distance per iteration is greater under scheme
B. Thus, although there is greater autocorrelation in the β samples under scheme B,
they move larger distances (with greater persistence in direction).
In summary, there may not be much to separate the two schemes. Our recommendation at present is to use scheme A, as this is simpler to code.
Extension to covariate set uncertainty
In this section we discuss an extension of the above methods to accommodate covariate
set uncertainty.
Auxiliary variable approaches are ideal for these scenarios, as they
allow for joint updates to the covariate set x and the regression coeﬃcients β, which
leads to eﬃcient dimension jumping moves.
The standard approach to covariate set uncertainty is to adopt a prior distribution
on the covariate set π(x) via a covariate indicator vector γ = {γ1, . . . , γp}, γi ∈{0, 1},
i = 1, . . . , p, such that γi = 1 if the ith covariate is present in the model and γi = 0 if
it is not. A prior on the model space can then be speciﬁed via a prior on the covariate
indicator, π(γ). The parameter vector γ is then included in the model speciﬁcation and
updated as part of the simulation.
Bayesian analysis of models of random dimension have become extremely popular
following the introduction of sampling techniques such as Green . Clyde 
provides a good overview of this so-called Bayesian model averaging approach. However,
Chris C. Holmes and Leonhard Held
it is well known that simulation of variable dimensional models can be problematic, as
a change to the model structure typically causes a large change to the likelihood of any
parameter values carried through to the new model ). A key
advantage of using auxiliary variables is that when updating the model we can condition
on {z, λ} and jointly update {γ, β} from the full conditional distribution given a change
to the covariates. The vector z retains information about the likelihood which allows
for optimal updates to be made to β, given a change in the covariate set. Updating
the β vector jointly with the covariate set is extremely important as typically, when the
covariates are non-orthogonal, there is strong linear dependence between the regression
coeﬃcients.
To sample from the posterior model space we use joint updates:
π(γ, β|z, λ) = π(γ|z, λ)π(β|γ, z).
To generate from π(γ|z) we could use Gibbs sampling on the indicator variables. However, under Peskun ordering it turns out to be more eﬃcient to use a Metropolis-Hastings
step to update the current covariate set, deﬁned by γ, with a joint update to β as well,
q(γ∗, β∗) = π(β∗|γ∗, z, λ) q(γ∗|γ),
where q() denotes a proposal distribution, π(β∗|γ∗, z, λ) is the conditional multivariate
normal posterior distribution (9) given the covariate set deﬁned by γ∗, and q(γ∗|γ) is
a (possibly symmetric) Metropolis-Hastings proposal density that may (or may not) be
based on the current covariate set γ. In this case, some straightforward algebra leads
to the acceptance probability α of the joint move as:
1, |V γ∗|1/2|vγ|1/2
|V γ|1/2|vγ∗|1/2
π(γ∗)q(γ|γ∗)
π(γ)q(γ∗|γ)
where vγ is deﬁned before (3) and {Bγ, V γ} are deﬁned in (9), where the subscripts
indicate that they are conditioned on the covariate set deﬁned by γ. Note that the
realised (drawn) values of {β, β∗} do not appear in the acceptance probability (12),
which resembles the Bayes factor of a standard Bayesian linear model. This implicit
marginalisation of β in the proposal step leads to eﬃcient dimension sampling, as the
β’s are being updated from their full conditional distributions given the change to the
covariate set. Following an update to {γ, β} we then update {z, λ}|{γ, β} using the
scheme outlined above in Section 2.3.
Example: Pima Indian data
To illustrate our approach to covariate uncertainty we consider again the Pima Indian
data from Ripley . The regression task is to predict whether patients will test
positive or negative for diabetes using a set of seven covariate measurements, observed
on a group of adult females of Pima Indian heritage. There are 532 records, selected
from a larger data set, with the following predictor variables: number of pregnancies
Bayesian auxiliary variable models
Covariates
Table 3: Row 1 lists the covariate acronyms for the Pima Indian data set example in Section
3: (NP), number of pregnancies; (Gl), plasma glucose concentration; (BP), distolic blood
pressure; (TST), triceps skin fold thickness; (BMI), body mass index; (DP), diabetes pedigree
function; and, (Ag), age. Row 2 lists the posterior probabilities of covariate selection. In Row
3, we report the MCMC standard deviations of the estimates π(γi = 1|y), taken across nine
consecutive post burn-in regions of size 1,000 MCMC samples.
(NP), plasma glucose concentration (Gl), distolic blood pressure (BP), triceps skin fold
thickness (TST), body mass index (BMI), diabetes pedigree function (DP), and age
(Ag). In Ripley they used a classical (non-Bayesian) logistic regression model
and noted that some of the covariates appeared irrelevant. Ripley went on to
perform stepwise variable selection using an AIC model choice criteria and found that
the covariates blood pressure and skin thickness were dropped from the ﬁnal model. We
performed a Bayesian analysis using independent priors on the covariates and regression
coeﬃcients as, π(γ) = Q
i π(γi), with π(γi = 1) = 0.5 for i = 1, . . . , p and π(β) =
N(0, 100Ip). Updates to the covariate set were made using a Metropolis proposal as
follows. We select a covariate at random and propose γ∗
i = 1, if the current γi = 0, γ∗
0 otherwise. This results in the ﬁnal term, π(γ∗)q(γ|γ∗)
π(γ)q(γ∗|γ) , in (12) being one. Following
updates to {γ, β} we jointly update {z, λ} using the marginal truncated logistic sampler
(see Appendix A3).
We performed a simulation of 10,000 iterations and discarded the ﬁrst 1,000 as a
burn-in. In Table 3, we show the estimates of the posterior probabilities, π(γi = 1|y), for
the seven covariates, along with the standard deviations in these MCMC estimates taken
from nine consecutive regions of the MCMC samples, { , . . ., (9001, 10000)}.
The chain appears to be mixing well under the data augmentation approach. The overall
acceptance rate of the covariate update proposals was around 4%, which is good when
considering the posterior probabilities π(γi|y) shown in Table 3, assuming that the chain
is mixing well as indicated by the standard deviations reported in Row 3. The estimates
of π(γi = 1|y) are in accordance with the observations of Ripley though we ﬁnd
there also appears to be some doubt as to the relevance of age.
Bayesian polychotomous regression
In this section we highlight another useful extension of the logistic auxiliary variable
approach to data where the response is multicategorical. It is straightforward to extend
the logistic regression models of Section 2.3 to ordinal data, such as the cumulative or
the sequential model, following the algorithmic approach discussed by Albert & Chib
 for the probit link. However, the logistic model also allows for a simple
Chris C. Holmes and Leonhard Held
extension to polychomotous data — i.e., when yi ∈{1, . . . , Q} is an unordered category
indicator of one of Q classes. This is known as polychotomous regression .
The polychotomous generalisation of the logistic regression model is deﬁned via,
M(1; θi1, . . . , θiQ),
k=1 exp(xiβk)
where M(1; ·) denotes the single sample multinomial distribution. We note in (13) that
there is now a separate set of coeﬃcients βj for each category. It is usual to ﬁx one
set of coeﬃcient, say βQ, to be zero, so that the logistic regression model is recovered
for Q = 2 and the interpretation of the coeﬃcients are in terms of the change to the
log-odds relative to category Q.
Polychotomous logistic regression with auxiliary variables
The ability to extend the methods discussed in Section 2.3 to the polychotomous case
arises by considering the conditional likelihood of a set of coeﬃcients, say βj, having
ﬁxed the other coeﬃcients, β−j = {β1, . . . , βj−1, βj+1, . . . , βQ}, in the model. In this
case we ﬁnd,
L(βj|y, β−j)
[θik]I(yi=k),
[ηij]I(yi=j)[wi(1 −ηij)]I(yi̸=j),
[ηij]I(yi=j)[1 −ηij]I(yi̸=j),
where I(·) is the logical indicator function, wi is a weight function independent of βj
exp(xiβj −Cij)
1 + exp(xiβj −Cij),
exp(xiβk).
The point here is that the conditional likelihood L(βj|y, β−j) has the form of a logistic
regression on class indicator I(yi = j).
This allows us to use the logistic sampling
technique highlighted in Section 2.3 embedded within a Gibbs step looping over the
Bayesian auxiliary variable models
Q −1 classes.
Appendix A5 lists the polychotomous pseudo-code, generalising the
logistic scheme of A3.
Before discussing a speciﬁc example we note that the auxiliary variable approach
is well tailored to marginal posterior density estimation through Rao-Blackwellization
 . For example, it is often of interest to estimate the marginal
posterior density p(β∗|y) for some subset β∗of β — for example, for the computation
of simultaneous credible regions or related quantities . In higher dimensions, the only feasible way to do this is based on an average of the corresponding full
conditional distributions of the regression coeﬃcients β∗:
ˆp(β∗|y) = 1
p(β∗|y, Θ(j)),
where Θ(j) denotes all other parameters in the model, the upper index j denotes the j-th
sample from the posterior distribution and M is the number of samples. Without our
auxiliary variable approach, two fundamental problems are ﬁrstly, that p(β∗|y, Θ(j))
may not have closed form and secondly, even if it does, the denominator in
p(β∗|y, Θ) = p(y|β∗, Θ) × p(β∗)
which depends on Θ, is typically not known; hence, (16) cannot be applied. However,
in the auxiliary variable method for MCMC simulation, p(β∗|y) is estimated via
ˆp(β∗|y) = 1
N(B∗(j), V ∗(j)),
where N(B∗(j), V ∗(j)) denotes the multivariate normal distribution (9) with appropriate
submatrices obtained from the MCMC samples. Clearly the normalizing constant is then
known. Estimation of the marginal posterior of the regression coeﬃcients can thus be
simply achieved through the Gaussian mixture (17). We note that this approach is not
restricted to the polychotomous regression model; it can also be used in all formulations
discussed in Section 2.
Example: Smoking restrictions data
To illustrate the polychotomous method we analysed the data discussed in Bull .
The study in Bull relates to workplace attitudes to smoking restrictions within
Toronto, and in particular to changes in attitude before and after the introduction
of a bylaw regulating smoking in the workplace. The data was collected by random
telephone surveys of households in the Toronto region. The response variable, yi, has
three categories relating to the subjects beliefs that smoking in the workplace should
be “prohibited”, “restricted” or “unrestricted”. Bull provides full details and
background to the data. In the original study, Bull analysed a variety of models constructed from subsets of 12 covariates recorded on each telephone response. Here we
Chris C. Holmes and Leonhard Held
restrict our attention to the four “bylaw-related” covariates: time-of-survey, x1 ∈{0, 1}
denoting whether the data was recorded before or after the introduction of the bylaw;
place-of-work, {x2, x3}, where x2 ∈{0, 1} denotes workplace outside the city or not
and x3 ∈{0, 1} denotes workplace outside the home or not; ﬁnally place-of-residence
x4 ∈{0, 1}, denoting residence within the city of Toronto or otherwise.
The primary interest is in quantifying changes to attitudes from before to after the
implementation of the bylaw. We ran the polychotomous Gibbs sampler on the data,
and following Bull , we used “restricted” as the baseline category with associated
β set to zero. The sampler was run for 10,000 iterations, the ﬁrst 5,000 being discarded.
We then examined the marginal posterior eﬀect of time-of-survey.
In Figure 1 we show the marginal densities for the regression coeﬃcients associated
with the time-of-study covariate for the “unrestricted” category (top) and the “prohibited” category (bottom) relative to the “restricted” case. These plots were obtained
as Gaussian mixture models (17). Using the model (13) we interpret these plots as
quantifying the uncertainty in the change over time to the log-odds of the two categories relative to the “restricted” response. This interpretation is gained from using the
conditional logit link (13). The two plots are interesting; the top plot indicates that the
“unrestricted” subjects were more likely to prefer restrictions following the introduction
of the bylaw, while those subjects previously prefering prohibition see a hardening of
attitudes, being more likely to vote for prohibition after the bylaw came into action.
We can use the Gaussian mixture representation (17) to also estimate the sign of
the eﬀect, Pr(β ≤0), by using the distribution function of the normal density. ¿From
this we estimate the probability of a negative time-of-survey eﬀect on the log odds of
unrestricted to restricted is 0.7552; while the probability of a positive time-of-study
eﬀect on the log odds of prohibited to restricted is 0.7426.
Discussion
We have discussed a variety of auxiliary variable methods for Bayesian binary and
polychotomous regression. All of the algorithms are fully automatic with no user-set
parameters and as such they are ideal for embedding in statistical software. Although
concentrating here on GLMs, the methods are readily applicable to non-linear modelling
using free-knot regression splines .
Popular current alternatives for MCMC simulation in Bayesian logistic regression
models are found in Albert & Chib and Gamerman .
Albert & Chib
 note that specifying a scale mixture for λi in (8) as λi ∼Gamma(4, 4) induces a
t-distribution for ǫi with 8 degrees of freedom, which gives a good approximation to the
logistic distribution (up to a change in scale). However, this remains an approximation
and a qq-plot of the true logistic distribution against that found using the Student
approximation reveals considerable departure in the tails (see Figure 2).
In applications it will be diﬃcult to assess the eﬀect of this bias on the posterior
Bayesian auxiliary variable models
p(β11 | y)
p(β21 | y)
Figure 1: Plot showing the marginal densities of β11 and β21, the eﬀect of covariate time-ofsurvey for the unrestricted (top plot) and prohibited (bottom plot) relative to the restricted
class, using the data from Section 3.
The plots are obtained using the Gaussian mixture
representation (17).
distribution of the regression coeﬃcients. Our approach, however, is exact and provides
a fast, eﬃcient and automatic algorithm for inference in logistic regression models.
An alternative algorithm without auxiliary variables is described in Gamerman
 . Gamerman suggests a “weighted least squares” Metropolis-Hastings proposal
based on a linear Taylor-approximation of the likelihood. This algorithm works well in
practice, in particular if the number of parameters to be updated is not too large. However, there are no guarantees on the acceptance rates and this detracts signiﬁcantly when
implementing this in generic software. In contrast, due to the use of auxiliary variables
the corresponding acceptance rates in our approach will always be unity, other than in
the variable dimension case (Section 2.5) where we speciﬁcally choose the Metropolis-
Hastings over the available Gibbs sampler. Moreover, the extension of Gamerman’s
approach to variable dimension settings is non-trivial, whereas we have shown in Sec-
Chris C. Holmes and Leonhard Held
t quantile
logistic quantile
Figure 2: Plot of t-quantiles against logistic quantiles for probabilities between 0.0001 and
0.9999 (Solid line). The dashed line gives the reference line if the two distributions are identical.
tion 2.5 this to be straightforward for our approach.
Appendix: A0 Pseudo-code
This section lists the pseudo-code for the algorithms. The code assumes that the prior
on β is π(β) = N(0, v) and the design matrix X is of dimension (n × p). Comment
lines are preceeded by % %.
A[i] denotes the ith element of a column matrix A; A[i, j] denotes the ith, jth
element of a matrix A; A[i, ] denotes the ith row of A, A[, j] denotes the jth column;
AB denotes matrix multiplication of A and B; A[i, ]B[, j] denotes the row, column inner
A1: Procedure for joint sampling in Bayesian probit, β ∼π(β|Y, X, v)
% % First record constants unaltered within MCMC loop
V ←(XT X + v−1)−1
L ←Chol(V )
% % L stores the lower triangular Cholesky factorisation of V
Bayesian auxiliary variable models
FOR j=1 to number of observations
H[j] ←X[j, ]S[, j]
% % H stores the diagonal elements of hat matrix (XS)
W[j] ←H[j]/(1 −H[j])
Q[j] ←W[j] + 1
% % Initialise latent variable Z, from truncated normal
Z ∼N(0, In)Ind(Y, Z)
% % B denotes the conditional mean of
FOR i=1 to MCMC iterations
FOR j=1 to number of observations
zold ←Z[j]
m ←X[j, ]B
m ←m −W[j](Z[j] −m)
% % now draw Z[j] from truncated normal
Z[j] ∼N(m, Q[j])Ind(Y [j], Z[j])
% % make change to B
B ←B + (Z[j] −zold)S[, j]
% % now draw new value of β
T ∼N(0, Ip)
β[, i] ←B + LT
END MCMC iterations; RETURN β
% % Note, to convert to conventional iterative sampling: Leave out initial loop on
line 6 to line 11 which calculates {W[j], Q[j]} prior to the MCMC loop; Set B ←0
as the ﬁrst line within the MCMC loop; Change innermost loop to: m ←X[j, ]β[, i];
Z[j] ∼N(m, 1)Ind([Y [j]]); B ←B + Z[j]S[, j].
Chris C. Holmes and Leonhard Held
A2: Procedure for sampling Bayesian logistic model, β ∼π(β|Y, X, v),
using joint update to {z, β}.
% % Initialise mixing weights Λ to the (n × n) identity matrix
% % draw Z from truncated normal
Z ∼N(0, In)Ind(Y, Z)
FOR i = 1 to number of MCMC iterations
V ←(XT Λ−1X + v−1)−1
% % note that Λ−1 is a diagonal matrix and
hence simple to invert
L ←Chol(V )
% % So L stores the lower triangular Cholesky factorisation of V
FOR j=1 to number of observations
zold ←Z[j]
H[j] ←X[j, ]S[, j]
W[j] ←H[j]/(Λ[j, j] −H[j])
m ←X[j, ]B
m ←m −W[j](Z[j] −m)
q ←Λ[j, j](W[j] + 1)
% % draw Z[j] from truncated normal
Z[j] ∼N(m, q)Ind(Y [j], Z[j])
% % make change to B
% % now draw new value of β
T ∼N(0, Ip)
β[, i] ←B + LT
% % now draw new values for mixing variances
Bayesian auxiliary variable models
FOR j=1 to number of observations
R ←(Z[j] −X[j, ]β[, i])
Λ[j][j] ∼π(λ|R2)
% % See program A4.
END MCMC iterations; RETURN β
A3: Procedure for sampling Bayesian logistic model, β ∼π(β|Y, X, v),
using joint update to {z, λ}.
% % Initialise mixing weights Λ to the
(n × n) identity matrix
% % draw Z from truncated logistic
Z ∼Lo(0, 1)Ind(Y, Z)
FOR i = 1 to number of MCMC iterations
% % draw value of β
V ←(XT Λ−1X + v−1)−1
% % note that Λ−1 is a diagonal matrix and
hence simple to invert
L ←Chol(V )
% % So L stores the lower triangular Cholesky factorisation of V
B ←V XTΛ−1Z
T ∼N(0, Ip)
β[, i] ←B + LT
% % Now update {Z, Λ}
FOR j=1 to number of observations
m ←X[j, ]β[, i]
% % draw Z[j] from truncated logistic
Z[j] ∼Lo(m, 1)Ind(Y [j], Z[j])
% % now draw new value for mixing variance
Chris C. Holmes and Leonhard Held
R ←(Z[j] −m)
Λ[j][j] ∼π(λ|R2)
% % See program A4.
END MCMC iterations; RETURN β
Sampling the mixing weights, π(Λ|R2)
In this section we describe how to sample from the full conditional distribution of the
auxilliary variables Λ = {λ1, . . . , λn} in the logistic regression model of Section 2.3.
The conditional distribution does not have a standard form, however, sampling from
the density can be achieved eﬃciently using rejection sampling.
As a rejection sampling density we suggest using the Generalised Inverse Gaussian
distribution, GIG(λ, ψ, χ). Using the parameterisation of Devroye , we
set λ = 0.5, bψ = 1 and χ = (zi −xiβ)2 = R2.
When sampling from the GIG we make use of the equality, GIG(0.5, 1, r2) = r/IG(1, r),
where IG denotes the inverse Gaussian. The inverse Gaussian is easier to sample from
than the GIG as it can be done using an inversion algorithm .
Let g(λ) denote the GIG(0.5,1,r2) rejection sampling density, where r2 = (zi−xiβ)2.
Following a draw from g(·) the sample is accepted with probability α(·),
α(λ) = l(λ)π(λ)
where M ≥supλ
, l(λ) denotes the likelihood, l(λ) ∝λ−1/2 exp(−0.5r2/λ), and
π(λ) is the prior,
4λ−1/2KS(1
where KS(·) denotes the Kolmogorov-Smirnov density. The prior (19) follows from the
transformation of random variables λi = (2ψi)2 in (8).
We note that we can set M = 1 and cancelling terms leaves the acceptance probability (18) as
α(λ) = exp(0.5λ)π(λ).
The direct evaluation of α(λ) is problematic as the KS density is only known up to
an inﬁnite series. However, there is an alternating series representation given in Devroye
 that allows for an eﬃcient set of squeezing functions to be adopted for
the rejection sampling algorithm. Following Devroye we partition the λ space
into two regions within which we can construct a monotone alternating series. The
Bayesian auxiliary variable models
breakpoint for this mixture method can be anywhere in the interval [4/3, π2]. We have
used the value 4/3, as the rightmost interval is faster to evaluate.
The pseudo-code follows below. The method is least eﬃcient as r2
i →0, though we
still observe an acceptance rate of around 0.25 for r2
i as small as 10−10. For r2
acceptance is around 0.5 rising to nearly one for r2
A4: Procedure to sample λ ∼π(λ|r2)
% % Note, r2 = (zi −xiβ)2
% % To begin we must
draw a sample from the rejection sampling density
Y ∼N(0, 1)
Y ←1 + (Y −
Y (4r + Y ))/(2r)
U ∼U 
IF U ≤1/(1 + Y )
THEN λ ←r/Y
ELSE λ ←rY
% % Now, λ ∼GIG(0.5, 1, r2)
U ∼U 
IF λ > 4/3
OK ←rightmost-interval(U, λ)
OK ←leftmost-interval(U, λ)
WHILE NOT OK
The procedure above calls two functions, rightmost-interval()and leftmost-interval(),
depending on the value of the proposed λ. The pseudo-code for these functions follows:
Chris C. Holmes and Leonhard Held
OK ←rightmost-interval(U, λ)
X ←exp(−0.5λ)
% % Squeezing
Z ←Z −(j + 1)2X(j+1)2−1
IF Z > U THEN RETURN OK ←1
Z ←Z + (j + 1)2X(j+1)2−1
IF Z < U THEN RETURN OK ←0
The pseudo-code for the left region is
OK ←leftmost-interval(U, λ)
H ←0.5 log(2) + 2.5 log(π) −2.5 log(λ) −π2
lU ←log(U)
X ←exp(−π2/(2λ))
% % Squeezing
Z ←Z −KXj2−1
IF H + log(Z) > lU THEN RETURN OK ←1
Bayesian auxiliary variable models
Z ←Z + (j + 1)2X(j+1)2−1
IF H + log(Z) < lU THEN RETURN OK ←0
A5: Procedure for sampling the Bayesian polychotomous model, β ∼
π(β|Y, X, v), using joint update to {z, λ}.
% % Let, Y [i][j] denote the category indicator variable, Y [i][j] = 1 if the ith
observation is of class j, j ∈{1, . . ., Q}, Y [i][j] = 0 otherwise.
% % Initialise mixing weights, Λ[, , q] for each category to the (n×n) identity
FOR q=1 to Q −1
Λ[, , q] ←In
% % draw Z from truncated logistic
Z[, q] ∼Lo(0, 1)Ind(Y [, q], Z[, q])
FOR i = 1 to number of MCMC iterations
FOR q=1 to Q −1
V ←(XT Λ[, , q]−1X + v−1)−1
% % note that Λ−1 is a diagonal matrix and
hence simple to invert
L ←Chol(V )
% % So L stores the lower triangular Cholesky factorisation of V
B ←V XT Λ[, , q]−1Z[, q]
T ∼N(0, Ip)
β[, q, i] ←B + LT
% % Now update {Z, Λ}
FOR j=1 to number of observations
m ←X[j, ]β[, q, i]
C ←sum (exp(X[j, ]β[, −q, i]))
% % Hence, C records the sum of the Q−2 terms, exp(X[j, ]β[, t, i]),
% % for, t ∈{1, . . ., q −1, q + 1, . . . , Q −1}.
Chris C. Holmes and Leonhard Held
% % Now draw Z[j, q] from truncated logistic
Z[j, q] ∼Lo(m −log C, 1)Ind(Y [j, q], Z[j, q])
% % now draw new value for mixing variance
R ←(Z[j, q] −m)
Λ[j, j, q] ∼π(λ|R2)
% % See program A4.
END MCMC iterations; RETURN β