Learning Graphical Models
for Stationary Time Series
Francis R. Bach
Computer Science Division
University of California
Berkeley, CA 94114, USA
 
Michael I. Jordan
Computer Science Division
and Department of Statistics
University of California
Berkeley, CA 94114, USA
 
September 29, 2003
Technical Report 650
Department of Statistics
University of California, Berkeley
Probabilistic graphical models can be extended to time series by considering probabilistic
dependencies between entire time series.
For stationary Gaussian time series, the graphical
model semantics can be expressed naturally in the frequency domain, leading to interesting
families of structured time series models that are complementary to families deﬁned in the
time domain.
In this paper, we present an algorithm to learn the structure from data for
directed graphical models for stationary Gaussian time series. We describe an algorithm for
eﬃcient forecasting for stationary Gaussian time series whose spectral densities factorize in
a graphical model. We also explore the relationships between graphical model structure and
sparsity, comparing and contrasting the notions of sparsity in the time domain and the frequency
domain. Finally, we show how to make use of Mercer kernels in this setting, allowing our ideas
to be extended to nonlinear models.
Introduction
Time series arise in many problems in signal processing, bioinformatics computer vision and machine
learning. In the statistical modeling of time series, the assumption of stationarity makes possible
the use of tools from spectral analysis .
Much of the algorithmic research eﬀort in this area
has dealt with making such tools scalable with the respect to the number of observed samples, for
example via algorithms that exploit the Fast Fourier Transform, or forecasting algorithms such as
the Durbin-Levinson algorithm .
Domains with large number of variables—in which Markovian or general graphical models have
excelled—have not attracted the same attention in the time series ﬁeld. Graphical models 
provide a general framework for deﬁning probabilistic models over large numbers of variables, by
building global models out of local interaction models. Numerous special cases are familiar in signal
processing, including the Kalman ﬁlter, hidden Markov models and factor analysis. In addition,
these models come equipped with standard, numerically-eﬃcient learning and inference algorithms,
algorithms that have had applications beyond signal processing and machine learning, such as in
error-control coding .
Graphical models for time series are generally deﬁned in the time domain. That is, they deﬁne a
transition probability distribution on a set of state variables, conditioning on values of these variables
at previous time steps. Such models, which are often referred to as dynamic Bayesian networks, have
had signiﬁcant applications in areas such as bioinformatics and speech processing . In this
paper we consider an extension of the basic notion of graphical model which considers dependencies
between entire time series . As we show, this extension can be naturally expressed in the
frequency domain, making use of the spectral representations for stationary Gaussian time series. In
this paper, we present an algorithm to learn the structure of directed graphical models for spectral
representations of time series from data. The algorithm, presented in Section 5, is a direct extension
of algorithms for learning directed graphical models for Gaussian data .
We also discuss the problem of forecasting, the problem of predicting the future given the past.
For stationary Gaussian time series, algorithms for forecasting can be deﬁned in either the time or
frequency domain. In Section 4 we present a novel algorithm for eﬃcient forecasting with graphical
models. While classical algorithms such as the Durbin-Levinson algorithm incur a cubic time
complexity, our new algorithm, by making use of the structure of the graphical model, has a quadratic
complexity (in the number of variables). This new algorithm can also be used for factor analysis
models for time series .
While our basic focus is on structured linear time series models, we also present an extension to
nonlinear models in Section 6. In particular, we make use of our previous work in learning graphical
models for hybrid data , enabling the ﬁtting of augmented models that include nonlinearities
or discrete variables. The principle of the algorithm is very simple: although variables may not
be Gaussian, by mapping them into a high-dimensional feature space, they can be considered as
Gaussian for the purpose of model selection. This mapping is made implicit and computationally
eﬃcient through the use of kernel methods .
The graphical models that we describe in this paper could have several possible applications,
paralleling the many applications of graphical models for non-temporal data, such as feature selection
for regression or classiﬁcation, or sparse and statistically sound models for forecasting. In Section 7,
we perform simulations on synthetic and real datasets, to illustrate the validity of our algorithm and
compare them to other approaches for modeling stationary time series.
Stationary Gaussian processes
We consider a multiple time series x(t), where for each t ∈Z, x(t) = (x1(t), . . . , xm(t)) has m
univariate real components1. Throughout this paper we assume that x(t) is a zero-mean Gaussian
process; that is, all ﬁnite marginals are jointly Gaussian. In addition, we assume that the process
x(t) is stationary: for Gaussian processes, x(t) is stationary if and only if Ex(t + h)x(t)⊤does not
depend on t, or equivalently, all marginals are invariant by time translation. Given a stationary
process, the autocovariance function is deﬁned as the matrix-valued function Γ(h) on Z, deﬁned as
Γ(h) = Ex(t + h)x(t)⊤= Ex(h)x(0)⊤.
For each h ∈Z, Γ(h) is a symmetric m × m real matrix, and the function h 7→Γ(h) is nonnegative,
that is, for all sets of vectors αi ∈Rm, indexed by i ∈I, we have P
i Γ(i−j)αj ⩾0. Essentially,
the Gaussian stationarity assumption is equivalent to modeling the variables as jointly Gaussian with
tied parameters; i.e., the covariance matrix of any successive variables x(t), x(t + 1), . . . , x(t + h −1)
1Note that the theory of stationary Gaussian processes and most of the results of this paper can be naturally
extended to the complex case.
is Toeplitz by blocks:
Spectral density matrix
We assume that P∞
−∞||Γ(h)|| < ∞, so that the spectral density matrix f(ω) is well deﬁned, as a
matrix-valued function on R:
Γ(h)e−ihω.
For each ω, f(ω) is an m × m Hermitian matrix. In addition the function ω 7→f(ω) is 2π-periodic.
Also for real valued random variables, we have f(−ω) = f(ω)⊤, which implies that we only need to
consider the spectral density matrix for ω ∈[0, π].
Since the function Γ(h) is nonnegative, the spectral density matrix is pointwise nonnegative, that
is ∀ω ∈[−π, π], ∀α ∈Cm, α∗f(ω)α ∈R+. Note that knowing the spectral density f is equivalent
to knowing the autocovariance function Γ, since they form a Fourier transform pair. In particular,
f(ω)eihωdω.
The rate of decay of Γ(h) as h grows is determined by the smoothness of the spectral density. In
particular, when designing a spectral density, care must be taken regarding its smoothness so that
the resulting autocovariance function has attractive decay properties (which themselves govern the
complexity of prediction).
Spectral representation
Any Gaussian stationary time series with absolutely summable autocovariance function has a spectral
representation of the following form :
f(ω)dZ(ω),
where Z(ω) is a random process with orthogonal increments and such that for each ω, EZ(ω)Z(ω)∗=
In other words, x(t) is a superposition of inﬁnitely many independent random signals at
diﬀerent frequencies.
Autoregressive models
In this paper, we consider stationary autoregressive (AR) models. Stationary can be imposed either
by assuming that the process is initialized according to the stationary distribution or that the process
is causal and extends to negative inﬁnity. An autoregressive model has the following formulation:
x(t) = Φ1x(t −1) + · · · + Φpx(t −p) + z(t)
where the variables z(t) are mutually independent, each with covariance matrix Σ, and also independent from x(u), u < t. Each Φi is an m × m matrix.
The parameters Φi and the order p of AR models can be eﬃciently estimated from data by
using classical regression methods for parameter estimation and variable selection . If sparsity is
desired, one can attempt to ﬁnd zeros in the covariance matrix Σ or its inverse, as well as in the
regression weight matrices Φi, in a manner analogous to subset selection for linear regression . In
this paper we consider sparsity in the frequency domain, a notion that is complementary to sparsity
in the time domain.
Finite sample
In this section, we brieﬂy review methods for estimating the autocovariance function Γ(h) and the
spectral density matrix f(ω) from data x(0), . . . , x(T −1).
Sample autocovariance and periodogram
The sample autocovariance function is deﬁned as
(x(t + h) −¯x)(x(t) −¯x)⊤
for h ∈[0, T −1], extended by symmetry for negative h and equal to zero for |h| equal or greater
than T (the vector ¯x = 1
t=1 x(t) is the sample mean of the data). The sample autocovariance
function is consistent and asymptotically normal under weak assumptions .
The periodogram is deﬁned as the Fourier transform of the sample autocovariance function. More
precisely, let d(0), . . . , d(T −1) be the discrete Fourier transform of the data:
x(t)e−ikt.
At the frequencies ωk = 2πk/T, ωk ∈[−π, π], the periodogram is deﬁned as I(ωk) = d(k)d(k)∗
and can readily be computed using m fast Fourier transforms (FFT). It is usually extended as a
piecewise constant periodic function on R.
The periodogram does not provide a consistent estimator of the spectral density and is a notoriously bad estimator of the spectral density. However when it is appropriately smoothed, it is a good
estimator.
Smoothing the periodogram
The periodogram is smoothed by convolving it with a smoothing window Wr(j), leading to the
following estimator, at frequency ωk:
ˆf r(ωk) =
Wr(j)I(ωj+k).
Wr(j) is a smoothing window that is required to be symmetric, with ﬁnite support, and to sum to
one. In our simulations, we used the window Wr(j) = r
j r2/2, which has favorable properties
both in time (it is positive) and frequency (it is smooth). Note that if the number of samples T
tends to inﬁnity with r(T) tending to inﬁnity such that r(T)/T tends to zero, then Eq. (4) provides
a consistent estimate of the spectral density matrix .
Note that by simply inverting the Fourier transform using the FFT, we can recover the autocovariance function. With the type of smoothing we chose, we know in advance an upper bound on the
decay of the autocovariance as h grows. Indeed, we can simply verify that the autocovariance function Γr derived from the estimate ˆf r, satisﬁes ||Γr(h)||2 ⩽e−h2/r2||ˆΓ(0)||2. Thus we can choose the
time horizon H to be a constant times r (in simulations we took H = 4r). Limiting the time horizon
is equivalent to limiting the resolution of the spectra; that is, the spectral density is represented by
Input: data xi(t), i ∈{1, . . . , m}, t ∈{0, . . . , T −1}
Algorithm:
1. Compute the m discrete Fourier transforms using m FFTs: di(.) = FFT(xi(.))
2. Compute the periodogram I(k) = d(k)d(k)∗
3. Determine optimal smoothing r and degree of freedom df by minimizing S(r) in Eq. (6), for
r−1 ranging from T 1/5 to T 4/5 with a grid of step T 3/10, i.e. O(T 1/2) steps
4. Compute the spectral density matrix fk
ˆf(ω) at frequencies ωk
k ∈{0, . . . , T −1} using Eq. (4)
5. Take H = 2⌊log2(4df)⌋
6. Subsample the sequence fk T/H times.
Output: df = r
2π, fk, k ∈{0, . . . , H −1}
Figure 1: Periodogram smoothing with automatic selection of the bandwidth.
its values on the grid ωk = 2πk/H for a given H. All integrals involving the density are computed
using Riemannian sums. Note that by the Nyquist theorem, if Γ(h) is equal to zero for |h| > H,
then the spectral density can be exactly represented by a ﬁnite sample with even steps 2π/H. The
periodogram gives a sample with precision 2π/T; to obtain a sample with precision 2π/H, we need
to subsample the periodogram with a positive linear ﬁlter (in order to maintain its positivity).
Selecting the bandwidth
We use the Akaike information criterion (AIC) to select the optimal bandwidth for a given dataset.
This requires an eﬃcient computation of the likelihood, as well as the eﬀective number of parameters
or degrees of freedom df .
We use the Whittle approximation of the likelihood:
log | ˆf(ωk)| + tr{ ˆf(ωk)−1I(ωk)}
2Tm log 2π,
where |A| denotes the determinant of the matrix A. The Whittle approximation relies on the fact that
the discrete Fourier transform of the data is asymptotically normal with independent components
and with variance the spectral density.
In order to determine df, we notice that the smoothing deﬁned in Eq. (4) is a linear smoothing;
that is, ˆf(ωk) is obtained from I(ωk) by applying a linear matrix Hr. The eﬀective number of
parameters is the trace of Hr, which in our case this is simply obtained as dfr = tr Hr = T ×W(0) =
Thus, in order to ﬁnd the optimal bandwidth, we minimize the following AIC criterion with
respect to the smoothing parameter r:
S(r) = −ℓw + dfr
(m2 is the number of real parameters necessary to encode an m×m Hermitian matrix, and the 1/2
comes from the fact that since our signals are real, we only need the spectral density on [0, π]). The
resulting algorithm is presented in Figure 1. Note that we estimate the joint spectral density matrix
ﬁrst and then learn the structure of the graphical model (the topic of Section 5). In Section 5.3, we
show how adaptive smoothing of the periodogram can be achieved in a manner that depends on the
structure of the learned graphical model.
Prediction - forecasting
Given a ﬁnite sample from time 1 to H and a model (an autocovariance function or a spectral
density), forecasting is the task of predicting values for times H + 1 and later. In the Gaussian case,
the best prediction is a linear approximation of x(H + 1), based on x(1), . . . , x(H). More precisely,
we look for matrix coeﬃcients Φj such that the expected error
x(H + 1) −PH
j=1 Φjx(H + 1 −j)
is minimal.
Minimizing e(Φ) in Eq. (7) is equivalent to solving the following system of equations, the so-called
Yule-Walker equations (obtained by setting the derivatives to zero in Eq. (7)):
ΨiΓ(i −j) = Γ(j).
We denote by γH the H × m vector such that γ⊤
H = (Γ(1) Γ(2) · · · Γ(H)), and Ψ the H × m vector
such that Ψ⊤= (Ψ1 Ψ2 · · · ΨH). We also use the notation
· · · Γ(H−1)
Then Eq. (8) can be written as:
In order to solve the system in Eq. (8) or Eq. (9), the Toeplitz structure of the problem can
be exploited, to avoid the O(m3H3) complexity associated with the unstructured linear system. In
particular, the innovations algorithm or the Durbin-Levinson algorithm can be used to iteratively
compute the prediction and the error covariance in O(m3H2) operations .
When the model is deﬁned through the spectral density sampled at H points, then we ﬁrst
compute the autocovariance function using the FFT in time O(m2H log H), and incur a cubic time
complexity for inference. In Section 4, we use a diﬀerent technique to solve the Yule-Walker equations
in time O(m2H(d+log H)), for spectral densities that factorize in a graphical model with maximum
Graphical models for time series
The graphical model framework can be extended to multivariate time series in several ways. We
follow and consider dependencies between whole time series, that is between the entire sets
xi ≜{xi(t), t ∈Z}, for i = 1, . . . m. Thus the time series xi and xj are independent if and only if
the random inﬁnite vectors {xi(t), t ∈Z} and {xj(t), t ∈Z} are independent. Similarly, time series
xi and xj are conditionally independent given xk if and only if {xi(t), t ∈Z} and {xj(t), t ∈Z} are
conditionally independent given {xk(t), t ∈Z}.
For classical graphical models and Gaussian variables, marginal and conditional independence
statements can be read out from zeros in the covariance matrix or its inverse . It turns out that
for Gaussian stationary time series, these results can be naturally extended, essentially replacing the
covariance matrix by the spectral density matrix, as we now describe.
Figure 2: Graphical model for three time series: x1 is independent from x3 given x2
Gaussian time series and independence
We consider a Gaussian stationary multivariate time series x(t) with m components and with positive
(i.e. invertible) spectral density matrix f(ω). Marginal independence and conditional independence
are easily characterized in the frequency domain, as the following proposition shows :
Proposition 1: The time series xi and xj are marginally independent if and only if
∀ω ∈[0, 2π], fij(ω) = 0.
The time series xi and xj are conditionally independent given all other time series xk, k ̸= i, j, if
and only if
∀ω ∈[0, 2π], (f(ω)−1)ij = 0.
Intuitively, this proposition enables us to consider each frequency component independently of
the other components.
As a ﬁnal step towards full equivalence between a Gaussian stationary time series and the concatenation of independent variables at each frequency, we have the following proposition, which gives
a closed form expression for the KL divergence , paralleling the expression for the KL divergence
between zero mean Gaussian vectors :
Proposition 2: The KL divergence between two zero-mean stationary vector-valued Gaussian
processes U and V , with invertible spectral density matrices f(ω) and g(ω), is equal to
J(f(ω)||g(ω)) = 1
I(f(ω)||g(ω))dω,
where I(F||G) = −1
2 log det[FG−1] −1
2 tr(I −FG−1) is the KL divergence between two covariance
As we will see in Section 5, what is needed when learning a graphical model from data is the
expression of the entropy of the random variable deﬁned through a spectral density. The expression
of interest is the entropy rate, deﬁned as h = limT →∞1
T H(x(1), . . . , x(T)). For Gaussian stationary
time series, it can be readily computed:
log det[4π2ef(ω)]dω,
a result known as Sz¨ego’s theorem .
Note that h is also equal to the conditional entropy of x(0) given the inﬁnite past , that is,
if G is the covariance matrix of x(0) given the inﬁnite past, we have:
T H(x(0)|x(−1), . . . , x(−T)) = 1
2 log det 2πeG
Directed graphical models for Gaussian time series
In this paper, we consider directed graphical model representations for time series. This section
reviews the semantics of directed graphical models , so as to make the paper self-contained. We
ﬁrst review the classical results for Gaussian vectors and covariance matrices and present extensions
of these to Gaussian stationary time series and spectral densities.
Directed models and conditional independence
Let x be a Gaussian variable with covariance matrix Σ. The variable x, or the covariance matrix
Σ, are said to factorize in a directed acyclic graph G, if and only if for all i, xi is independent from
its non-descendant variables given its parents . For a characterization in terms of the covariance
matrix, see Section 3.2.3.
We generalize this deﬁnition to stationary time series: the time series x(t), with spectral density
f(ω), is said to factorize in G, if for all ω ∈[−π, π], the (complex) covariance matrix f(ω) factorizes
in G. This is exactly equivalent to the following property: for all i, xi is independent from its nondescendant variables given its parents, where conditional independence between time series should
be understood as in the previous section.
Factorization of the KL divergence
Since the KL divergence factorizes in a directed graphical model for Gaussian variables , the
previous propositions immediately imply that the KL divergence factorizes in directed models for
Gaussian stationary time series. Thus parameter estimation in a directed graphical model with m
time series decouples into m distinct conditional density estimates for univariate time series. This
makes possible the eﬃcient learning of the structure of a directed graphical model for time series
from data, as we show in Section 5.
Sparse representation of the covariance and precision matrices
In this section, we show how matrix-vector products Σα and Σ−1α can be computed eﬃciently,
when a covariance matrix Σ factorizes in a directed graphical model. This is easily seen from the
“regression view” of Gaussian directed graphical models , which implies that Σ−1 and Σ can be
written as
(I −W)∗D−1(I −W)
(I −W)−1D(I −W)−∗
where W is a strictly lower triangular matrix (i.e. with zero diagonal) and D is diagonal. In this
interpretation, W are the regression weights and D the conditional variances. In other words, a
distribution that factorizes in G is deﬁned by the parameters W and D. Note that these parameters
can be easily found from the full joint covariance matrix as follows:
Σi,i −Σi,πiΣ−1
πi,πiΣπi,i
If the maximal fan-in of the graph G is d, then for any vector α, we can compute Σ−1α in O(dm)
operations, because the product Wα can be computed in O(md) operations. In order to be able to
compute Σα, we just need to be able to solve the systems of the form (I −W)x = α, which can be
done in O(md) operations, since the matrix W is triangular and has at most md nonzero elements.
Eﬃcient forecasting with directed graphical models
In this section, we describe a novel algorithm for eﬃcient forecasting when the spectral density
matrix has a sparse structure. We learn the one-step-ahead predictor from the potentially inﬁnite
past, that is we learn the predictor from the H previous time steps, where the required “horizon”
H is determined by the smoothness of the spectral density. We essentially need to solve the Yule-
Walker equation of order H. Direct methods such as the Durbin-Levinson algorithm do not scale
well with the number of variables m. However, by solving the linear system using the conjugate
gradient technique, we can make use of the structure of our spectral density.
Problem set-up
We assume that the spectral density is known through H samples fk at frequencies ωk = 2kπ/H,
and that H is large enough so that the autocovariance function is equal to the discrete Fourier
transform of the sequence fk. Also we assume that for each k, fk has a sparse structure. By sparse
structure we mean that for each vector α ∈Rm, the product fkα and f −1
can be computed in
O(sm) operations, where s is a constant independent of m, instead of O(m2). A ﬁrst example is
when fk factorizes in a directed graphical model with maximal fan-in less than d, where we have
s = O(d), as shown in Section 3.2.3. A second example is where the covariance f = fk follows a
factor analysis model, that is f = WW ⊤+ ψ, where W is an m × p matrix and ψ is diagonal. In
that case it is easy to show that s = O(p3) (the argument is trivial for fα, and it follows by the
matrix inversion lemma for f −1α).
In this section, our interest is the design of an algorithm to determine the one-step-ahead predictor
from the last H steps, so we need to solve the following Yule-Walker equations QH(Γ)Ψ = γH, as
deﬁned in Eq. (9). We assume that the values Γ(h), for h = 0, . . . , H/2, are obtained as the ﬁrst
H/2 + 1 values of the FFT of order H of the sequence (f0, . . . , fH−1).
Conjugate gradient methods and Toeplitz systems
Conjugate gradient methods can be used to solve large linear systems of equations Ax = b of
size n, where the matrix A is Hermitian positive, and where the evaluation of the product Ax
can be performed in O(n) operations (instead of O(n2)). It is an iterative method where at each
iteration one matrix-vector product has to be performed. The number of iterations is governed by
the condition number of the matrix A—the ratio between its largest eigenvalue and its smallest
eigenvalue. Unfortunately, in many cases A is ill-conditioned and the number of iterations can be
very large. A common solution is to precondition the matrix A: instead of solving Ax = b, the system
C−1AC−1y = C−1b is solved, which yields a solution for the original system through x = C−1y. In
order to make conditioning practical there are two requirements for the matrix C: the linear system
involving C2 must be solved cheaply and the condition number of C−1AC−1 must be small.
For Toeplitz matrices, there is a natural choice of C obtained through the approximate diagonalization of Toeplitz matrices in the Fourier basis . Indeed, any block Toeplitz matrix of the
· · · T−H+1
TH−1 · · ·
can be approximated by a circulant matrix of the form
· · · C−H+1
where Ck = Tk for |k| ⩽H
2 −1 and Ck = Cn−k (which deﬁnes the values for |k| ⩾H
The circulant approximation is very useful because it can be diagonalized in a ﬁxed basis, that
where F is deﬁned by blocks, that is Fjk =
H exp(2ijkπ/H)I and D is block diagonal with
diagonal blocks bCk =
j=0 Ck exp(−2ijkπ/H). Since circulant matrices can be diagonalized
in the Fourier basis F, they can be easily inverted as
C−1 = FD−1F∗.
Also the computation of the product between a circulant matrix and an Hm × Hm matrix requires
O(H log Hm2 +Hm3) operations, or only O(H log Hm2 +Hm2s) operations if matrices with sparse
structure are used. Finally any Toeplitz matrix can be embedded in a circulant matrix of size 2H,
which makes it possible to multiply Toeplitz matrices in time O(2H log(2H)) = O(H log H).
The circulant matrix obtained when embedding the Toeplitz matrix QH is exactly the 2H-th
order circulant matrix with generating sequence ¯fk. To precondition, we have two choices, either (a)
to precondition with the Toeplitz matrix RH with generating function 1/ ¯fk, or (b) to precondition
directly with the inverse circulant matrix associated with TH. Both choices have been studied 
and we chose the ﬁrst option, since it does not require subsampling the spectral density. In Figure 3,
we give an outline of the resulting algorithm.
Computing ν-step-ahead predictors
Using the concept of spectral factorization, we can compute any ν-step-ahead predictor and error
covariances eﬃciently. The solution of the linear system is the optimal one-step-ahead ﬁlter, with
transfer function A1(ω) = P∞
k=0 Ψk+1e−ikω. Let Aν be the transfer function of the ν-step-ahead
ﬁlter, and Gν its error. We have the following theorem :
Theorem 1 : The error covariance G = G1 of the one-step-ahead predictor is
(I −e−iωA1(ω))f(ω)dω.
If ψ(ω) = I −e−iωA1(ω) and φ(ω) = ψ−1(ω) = I −P∞
k=1 φke−ikω, the ν-step-ahead ﬁlter can be
written as:
Aν(ω) = eiνω(I −
and the error covariance is Gν = Pν−1
Thus, once the one-step-ahead ﬁlter is found, all other ﬁlters and errors can be computed.
However, this requires inverting H matrices of size m. This can be avoided by noticing that φ(ω) can
be obtained by computing the one-step-ahead predictor corresponding to the spectral density ¯f −1(ω).
If m is very large, such that multiplying two full matrices of size m is prohibitive, we can obtain
the the ν-step-ahead predictor and error covariance by solving the Toeplitz system QHΨν = γH,ν,
where γH,ν = (Γ(ν), . . . , Γ(ν + H −1)).
Convergence analysis
Results from show that the number of iterations is independent of the order H, and depends on
the smoothness of the spectral density as well as how far f(ω) is from singular.
Interpretation in the frequency domain
The previous algorithm has an interpretation in the frequency domain which links it to previous
approaches that do not rely on eﬃcient methods for linear systems.
Total complexity
Let κ be the condition number of the matrices f(ω), H the number of samples required for the
computation of the Fourier transforms, m the number of samples, d the maximum fan-in. We have:
• Each iteration: O(m2H log H) for the FFTs, O(m2dH) for the multiplication by f(ω).
• Overall complexity of computing the predictor: O(m2H(log H + d)).
Learning directed graphical models
In this section, we present our algorithm for learning directed graphical models for stationary Gaussian time series.
Not surprisingly, we make heavy use of the corresponding algorithms for the
Gaussian temporally independent case. We cast the structure learning as a model selection problem where the model is deﬁned by the directed graph G. We use the Akaike Information Criterion
(AIC) in order to deﬁne the score J(G) to be minimized. Once the score is determined, the task of
minimizing it is performed with greedy algorithms, as detailed in Section 5.2.
AIC score for time series
We are given T consecutive samples of m univariate times series, xi(t), i ∈{1, . . . , m} and t ∈
{1, . . . , T}. We ﬁrst estimate the joint spectral density matrix ˆf(ω) as well as the estimated degrees
of freedom df, as shown in Section 2.4. The spectral density is represented as a set of H samples fk
at frequencies ωk = 2kπ/H.
For directed models, the AIC score J(G) is equal to the maximum of the likelihood plus a penalty
score. It is known to factorize :
where πi = πi(G) are the parents of node i in G and
Ji(πi(G)) = −T ˆH(xi|xπi) + #{i, πi(G)}
where #{i, πi(G)} is the number of parameters required for encoding the conditional probability
distribution of xi given the parents xπi(G), and ˆH(x|y) is conditional entropy of the random vector x
given the random vector y, computed using the estimated distribution of the joint (spectral) density.
Using the expression of the KL divergence and entropy rate in Eq. (10) and Eq. (11), we get the
following expression for the local score (where we omit the terms that are independent of the choice
of the graph G):
Ji(πi) = −T
log | ˆf{i}∪πi(ω)|
| ˆfπi(ω)|
dω + (2|πi| + 1)df
where |πi| is the cardinality of πi (i.e. the number of parents) and fA(ω) denotes the square block
(A, A) of the m×m matrix f(ω). The previous local score is approximated using the samples of
Ji(πi) = −T
log |(fk){i}∪πi|
+ (2|πi| + 1)df
Input: Spectral density values fk ∈Rm×m, at frequencies ωk = kπ/2H, k ∈{0, . . . , H −1},
precision ε
Algorithm:
1. Computing covariances:
∀h ∈{0, . . . , H−1}, r(:, :, h) ←¯f(:, :, h)
∀(i, j) ∈{1, . . . , m}2, r(i, j, :) ←IFFT(r(i, j, :))
r(i, j, :) ←r(i, j, 0 : H/2−1)
2. Initialization: k ←0, γ1 ←0, ρ ←P
i,j,t |r(i, j, t)|2
3. Compute Ψ: while √ρ > ε :
∀(i, j) ∈{1, . . . , m}2, z(i, j, :) ←[r(i, j, :); 0 . . . 0]
z(i, j, :) ←IFFT(z(i, j, :))
∀h ∈{0, . . . , H−1}, z(:, :, h) ←¯f(:, :, h)−1z(:, :, h)
∀(i, j) ∈{1, . . . , m}2, z(i, j, :) ←FFT(z(i, j, :))
z(i, j, :) ←z(i, j, 0 : H/2−1)
γ0 ←γ1, γ1 ←P
i,j,t ¯r(i, j, t)z(i, j, t)
if k = 1 then p = z else β ←γ1/γ0, p ←z + βp
∀(i, j) ∈{1, . . . , m}2, w(i, j, :) ←[p(i, j, :); 0 . . . 0]
w(i, j, :) ←IFFT(w(i, j, :))
∀h ∈{0, . . . , H−1}, w(:, :, h) ←¯f(:, :, h)w(:, :, h)
∀(i, j) ∈{1, . . . , m}2, w(i, j, :) ←FFT(w(i, j, :))
w(i, j, :) ←w(i, j, 0 : H/2−1)
i,j,t ¯p(i, j, t)w(i, j, t), α ←γ1/α
i,j,t |r(i, j, t)|2
4. Compute G and Ψi :
∀h ∈{0, . . . , H/2−1}, g(:, :, h) ←g(:, :, h)⊤
∀(i, j) ∈{1, . . . , m}2, w(i, j, :) ←[0; g(i, j, :); 0 . . . 0]
w(i, j, :) ←FFT(w(i, j, :))
∀h ∈{0, . . . , H−1}, w(:, :, h) ←(I −w(:, :, h)) ¯f(:, :, h)
t w(:, :, t)
∀h ∈{1, . . . , H/2}, Ψh = g(:, :, h −1)
Output: G, Ψi, h ∈{1, . . . , H/2}
Figure 3: Solving the Yule-Walker equations using the spectral density.
Input: data xi(t), i ∈{1, . . . , m}, t ∈{1, . . . , T}
Algorithm:
1. Estimate the joint spectral density fk ˆf(ωk) and the degree of freedom df using the algorithm
in Figure 1, for ωk = 2kπ/H
2. Minimization of the AIC score J(G)
a. Initialization: G = ∅
b. while no decrease J(G), select best local moves: addition, deletion or reversal
3. Compute optimal smoothing ri separately for each clique {i} ∪πi(G) using local smoothing
4. Compute the spectral density matrix f G
k that factorizes in G using the local suﬃcient statistics
Output: G, f G
Figure 4: Learning graphical models for stationary Gaussian time series.
Learning algorithm
In order to learn the graph structure G, we need to minimize the score J(G) deﬁned by Eq. (12)
and Eq. (13). This minimization problem is known to be NP-complete , and greedy algorithms
are commonly used. In particular, since the score J(G) factorizes as a sum of local scores, hillclimbing search using local moves—edge addition, deletion or removal—is computationally eﬃcient
(in particular through the caching of already computed local scores) and, with the possible use of
random restarts, yields good local minima . An outline of the ﬁnal algorithm is presented
in Figure 4. The exact complexity of the search procedure which uses caching of local scores is
presented in Section 5.4.
Local smoothing and eﬃciency
One of the major gains from learning a sparse structure for the spectral density matrix is that
we can perform (and optimize) the smoothing of the periodogram locally in the graph, restricting
ourselves to elements that share a clique; that is, instead of applying the algorithm of Figure 1 once
with m variables, we can apply it m times with |πi(G)| + 1 variables. In the algorithm presented in
Figure 4, a joint pilot estimate of the spectral density is used to determine the graph G, then the
local smoothing is performed. We now explore some of the issues, both numerical and statistical,
associated with such smoothing.
Numerically, learning the best smoothing parameter for the joint spectral density is an O(m3T log T)
operation. In order to overcome the cubic time complexity in m, it is possible to learn a diﬀerent
smoothing parameter ri for each local potential that is required, which makes the algorithm only
O(d3T log T) where d is the maximal fan-in of the directed graph. Note that in the algorithm of
Figure 4, it is possible to avoid the cubic time complexity of the computation of the pilot estimate
simply by considering an amount of smoothing that is the maximum of the amounts of smoothing
for each of the variables taken separately (note that this has a tendency to undersmooth and make
the graphical model sparser).
Statistically, learning a smoothing parameter for the joint density when there is a strong local
structure would lead to oversmoothing (since the AIC penalty becomes relatively too important):
local smoothing is substantially more eﬃcient. In the algorithm presented in Figure 4, the local
smoothing is performed once the structure is learned, thus requiring m distinct smoothing parameter
Finally, alternating between learning local smoothing parameters and directed graphs is possible,
in a manner analogous to the procedure of . In that situation the global score to be optimized
is the concatenation of Eq. (6), Eq. (12) and Eq. (13). More precisely, if ri denotes the smoothing
parameters for the clique {i} ∪πi(G) and ˆf ri denotes the smoothed periodogram with parameter
ri, we need to compute the Whittle likelihood, and then add the number of parameters, as shown
in the next theorem.
Theorem 2: The Whittle likelihood for a spectral density that factorizes in a directed graph G,
where the conditional spectral density are computed using a smoothing constant ri and with resulting
estimates ˆf ri
{i}∪πi(ω) for the cliques {i} ∪πi is approximately equal to:
ℓW (G, r) = −T
k ){i}∪πi|
{i}∪πiI{i}∪πi(ωk)
πi Iπi(ωk)
The overall AIC score is thus equal to:
J(G, r) = ℓW (G, r) +
(2|πi| + 1)dfi
Note that the optimal local smoothing that results from optimizing ri independently from G and
other smoothing constants rj, j ̸= i, is diﬀerent from the optimal smoothing on the clique πi ∪{i}.
In simulations on a wide variety of examples, it turns out that the diﬀerence between the two types
of smoothing is very small, with a slight advantage for the conditional smoothing.
Running time complexity
In this section, we study more closely the computational aspects of our algorithm in cases where
the number of variables is very large, potentially on the order of thousands. We denote by m the
number of these variables, d the maximum fan-in that we impose on our networks, T the number
of observations and H the time horizon. As seen in Figure 4, the structure learning has several
successive stages, whose running time complexities are:
1. Estimate (marginal) smoothing parameters ri for each of the m variables. Cost: O(mT 3/2 log T).
2. Take r = 1
i ri, and compute/smooth the periodogram for the joint density. Cost: O(m2T log T).
3. Structure learning using greedy search: when using the eﬃcient scheme of , it can be made
O(m3d2 + m2d4H).
4. Estimate local smoothing parameters. Cost: O(md3T 3/2 log T).
5. Overall cost: O(md3T 3/2 log T + m2T log T + m3d2 + m2d4H)).
Non-linearity through Mercer kernels
In this section, we show how the methods presented thus far can be extended to nonlinear models.
In particular, in Section 6.2, we show how these methods can be “kernelized”; based on this we
develop algorithms for learning in Section 6.3 and prediction in Section 6.4. As already mentioned,
the basic principle underlying our approach is to ﬁrst map the data x into a “feature space” F using
a “feature map” Φ(x), and to use the algorithms developed in earlier sections on the transformed
data Φ(x). What makes the approach feasible is that only dot products between data points are
needed by these algorithms, as seen in Section 6.2 and Section 6.3. Thus, the algorithm only involves
manipulation of the values k(x, y) = ⟨Φ(x), Φ(y)⟩for x and y being two data points in the original
space. k(x, y) is usually referred to as the kernel function. A function k(x, y) is a Mercer kernel if
and only there exists a feature space F and a feature map Φ(x) such that k(x, y) = ⟨Φ(x), Φ(y)⟩.
Since the algorithm only involves dot products and thus kernel values, algorithms can beneﬁt
from the expressive power of the feature space, without incurring the computational cost of actually
mapping the points into this feature space, a method usually referred to as the “kernel trick” .
Mercer kernels can be deﬁned on many diﬀerent input spaces, thus making the applicability of the
presented technique wide.
We assume that the variables xi(t) are mapped to ϕi(t) = Φi(xi(t)) ∈Fi, where Φi are given feature
maps from the “input space” Xi to the feature space Fi. Let ki(xi, yi) = ⟨Φi(xi), Φi(yi)⟩be the dot
product between two mapped elements. We are given T samples, from t = 0 to t = T −1. We can
build m “Gram matrices” Ki (one per variable) as T × T matrices composed of the pairwise kernel
values ki(xi(u), xi(v)) between two data points.
Kernelization of the periodogram
In this section, we show how we can ﬁnd a basis in which the periodogram has a particularly simple
expression involving products of Gram matrices. Since the model selection scores are independent
of the basis, we can use these expressions in Section 6.3 to construct our model selection criteria.
We assume that the data are centered2, that is, P
t ϕi(t) = 0. The sample covariance matrix
bΓ(h), which is deﬁned by blocks, has its (i, j)-block equal to, for h ⩾0:
bΓij(h) = 1
ϕi(t)ϕj(t −h)⊤.
For s, u ∈{0, . . . , T −1}, we have
ϕi(s)⊤bΓij(h)ϕj(u)
ϕi(s)⊤ϕi(t)ϕj(t −h)⊤ϕj(u)
(Ki)s,t(Ki)t−h,u = 1
T δsKiJhK⊤
where δs is the vector in RT with only zeros except a one at index s, Ki is the Gram matrix associated
with variable i, and Jh is the T × T matrix such that (Jh)ab = δ(a −b = h). The only non-zero
elements of Jh belong to the h-th lower diagonal. Thus, in the data basis deﬁned by the data points,
the autocovariance function has its (i, j)-th block equal to 1
T KiJhKj. The Fourier transform of the
autocovariance has the following expression:
Γij(h)e−ihω
KiJhe−ihωK⊤
T Kifω(Kifω)∗
2The data can be implicitly centered in feature space by replacing Gram matrices K by (I −1
T 1) K (I −1
where 1 is a T × T matrix composed of ones .
where fω is the vector in RT with components e−ihω, h ∈{0, . . . , T −1}. The periodogram is exactly
Iij(ωk) for ωk = 2kπ/T.
Since the feature space might have inﬁnite dimension, smoothing (in space) is usually required to
limit the number of parameters that are implicitly estimated. Smoothing (in space) the periodogram
by an isotropic Gaussian is equivalent to adding κI; in the data basis, the regularized (cross)periodogram is thus
Iij(ω) = 1
j + κδijKi.
Smoothing (in frequency) can be done as in the non-kernelized version by averaging consecutive
values of the periodogram. We let denote (fk)ij, k = 1, . . . , H the values of the estimated spectral
density after smoothing. Each (fk)ij is an T × T matrix.
Model selection criteria
Following , the eﬀective dimension of variables i is taken to be di = tr(Ki + κI)−1Ki and we use
the following AIC score:
log |(fk){i}∪πi,{i}∪πi|
|(fk)πi,πi|
The same optimization techniques used for classical graphical models can be used to optimize this
score. Eﬃcient implementation techniques based on low-rank approximations are presented in .
Prediction with kernels
Once we have a model, we would like to perform prediction. Given our framework, we obtain a
predicted value ˆφ = (ˆφ1, . . . , ˆφm) in the feature space, and this predicted value might not correspond
to an an xi such that ˆφi = Φi(xi). As for the covariance error, it is not deﬁned in input space. In
order to get an estimate and an error, we propose the following two-stage procedure. Once the
predicting ﬁlters ˆφi(t) = fi(φ(t −1), . . . , φ(t −H)) are computed:
1. For each variable i and each index t ∈[H, T −1], and ﬁnd
ˆxi(t) = arg min
xi ||Φi(xi) −fi(φ(t −1), . . . , φ(t −H))||.
This is usually referred to as the “pre-image” and several techniques are available .
2. Compute the average errors. In particular, if all variables are continuous, we get:
t∈[H,T −1]
(ˆx(t) −x(t))(ˆx(t) −x(t))⊤
Simulations
Synthetic examples
Our ﬁrst goal is to see whether, when the data are generated from a sparse graphical model, the
search procedure can ﬁnd this model. We use the following procedure: we generate a random spectral
density matrix, project the spectral density matrix function to a random graph with maximal fan-in
equal to two, generate data from the new spectral density matrix with various numbers of samples,
learn the model from data using the algorithm described in Figure 4. We then compute the KL
divergence from the generating model as well as the number of undirected edges not found by the
Figure 5: Synthetic examples. Left: KL divergence from the truth vs. number of samples. (dotted)
fully connected graphical model, (dashed) learned graphical model without post-smoothing, (plain)
learned graphical model with post-smoothing. Right: number of non recovered edges vs. number of
number of variables
iid Gaussian graphical model
Sparse AR model
Independent spectral densities
Graphical model for time series
Figure 6: Results for the climate datasets: negative log likelihood
learning algorithm. We compare the performance in KL divergence for the following models: a fullyconnected graphical model, a learned graphical model without post-smoothing, a learned graphical
model with post-smoothing (as described in Section 5.3). We can see in Figure 5 that the structure
learning algorithm manages to predict signiﬁcantly better than the fully-connected graphical model
and that the post-smoothing is essential.
Real-life datasets
We compare our algorithms to a classical approach that exhibits reasonable scaling of computing
time to large datasets. Note in particular that with large datasets such as climate data, two issues
need to be addressed: (a) there are a very large number of variables m, (b) there are relatively
few time samples compared to the number of variables. The model we compare to is the sparse
autoregressive model, SAR(p), of maximal order p. These are AR(p) models which favor zeros in
the weights Φi and are such that the error matrix Σ factorizes in a sparse graphical model G. They
are learned using forward selection of edges with the AIC criterion.
We used climate datasets extracted from the National Climatic Data Center (NCDC) “summary
of the day” data, which are composed of daily mean temperatures and precipitation for more than
10,000 stations across the United States. We subsampled this dataset by chosing random locations
and choosing the closest stations. The number of samples was 1024 consecutive days. Each of the
datasets that was so constructed was divided in a training set (of size 1024 days) and a testing
set. For each of these sets, the obvious seasonal component (year) is removed by using the ﬁrst
six corresponding Fourier components. We report the negative log likelihood of the one-step-ahead
prediction of the test data. We normalize the result by subtracting the log likelihood of the corresponding independent Gaussian variables. We also report the average prediction error. We report
results in Figure 6 and 7: we can see that the approach based on sparsity in the frequency domain
outperforms the approaches based on sparsity in the time domain.
number of variables
iid Gaussian graphical model
Sparse AR model
Independent spectral densities
Graphical model for time series
Figure 7: Results for the climate datasets: average prediction error
Conclusions
Probabilistic graphical models provide an elegant general framework for the representation of complex sets of dependencies among an interacting set of variables. Standard algorithms are available
for probabilistic inference, and a variety of parameter estimation and model selection algorithms
have been devised. The graphical structure of these models is essential for making these algorithms
computationally eﬃcient. It has important statistical implications as well, yielding models in which
the graphical structure corresponds directly to a natural notion of sparsity of the representation.
Applications of graphical models to time series analysis have generally taken the form of state
space models. In this setting, the graphical model machinery is aimed at capturing structure in the
state transition matrix, and sparsity in the graph has an interpretation in the time domain—a given
state variable has a probabilistic dependence on a limited number of variables in the past.
In the current paper, we have described an alternative methodology for making use of graphical
models in time series analysis. In particular, we have developed a frequency domain approach in
which the structure captured by a graphical model is related to sparseness in the spectral density
matrix. We have described parameter estimation and structure learning algorithms that are geared
for this setting. We have provided computational complexity analyses throughout, emphasizing the
development of methods that are appropriate for large-scale time series models.
As seen in the experiments with climate data, methods that attempt to uncover structure in the
frequency domain can lead to better predictive performance than analogous methods operating in
the time domain.
Finally, it is worth noting that while we have restricted ourselves to regularly sampled time series
in this paper, our algorithms apply immediately to irregularly sampled time series, once the spectral
density is estimated .