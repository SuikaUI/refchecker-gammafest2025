Optimal Decisions from Probabilistic Models: the Intersection-over-Union Case
Sebastian Nowozin
Microsoft Research
 
A probabilistic model allows us to reason about the
world and make statistically optimal decisions using
Bayesian decision theory.
However, in practice the intractability of the decision problem forces us to adopt simplistic loss functions such as the 0/1 loss or Hamming loss
and as result we make poor decisions through MAP estimates or through low-order marginal statistics. In this
work we investigate optimal decision making for more realistic loss functions.
Speciﬁcally we consider the popular intersection-over-union (IoU) score used in image
segmentation benchmarks and show that it results in a
hard combinatorial decision problem. To make this problem tractable we propose a statistical approximation to
the objective function, as well as an approximate algorithm based on parametric linear programming. We apply the algorithm on three benchmark datasets and obtain improved intersection-over-union scores compared to
maximum-posterior-marginal decisions. Our work points
out the difﬁculties of using realistic loss functions with probabilistic computer vision models.
1. Introduction
A popular viewpoint on computer vision tasks is to posit
them as a probabilistic inference task. The classic recipe is
as follows , shown in Figure 1: 1. specify a probabilistic model p(x, z) of the quantity of interest z and the
observed signal x; 2. observe x and obtain the conditional
posterior distribution p(z|x) of the quantity of interest; 3.
using the posterior infer summary statistics or decisions.
In the last decade this recipe has been successfully used
and adapted. In particular, nowadays models for semantic image segmentation and human pose estimation are often discriminatively and hence conditionally speciﬁed as
p(z|x; θ), where θ is a high-dimensional parameter vector . This changes the ﬁrst step of the recipe, but
the 2nd and 3rd steps remain unchanged.
A large body of work is available describing probabilistic
models and sophisticated inference procedures, but the decision task (step 3.) is often overlooked. For example, 
simply output the most likely state argmaxz p(z|x) as an
inference result. Deciding for the most likely state correp(x, z; θ)
p(x, z; θ∗)
{(xn, zn)}n
p(z|x; θ∗)
observation
training data
estimation
decision making
Figure 1. Typical workﬂow when working with a probabilistic
model: 1. Estimation: using training data to ﬁnd a good model
within a speciﬁed model class, 2. Inference: infering a posterior
distribution given an observation x, 3. Decision Making: using
posterior beliefs and a given utility to make optimal decision.
sponds to a particular choice of loss function, namely the
0-1 loss, ℓ(z, y) = 1{z̸=y}. This loss is unrealistic and does
not match the way we assess prediction quality in computer
vision benchmarks. Our work ﬁxes this mismatch; we start
from decision theory : given our beliefs p(z|x) and a
loss or utility function that measures the beneﬁt of a decision
y when the true world state is z, we would like to make optimal decisions, that is minimize the expected loss or equivalently maximize expected utility. In this work we consider
optimal decision making with the intersection-over-union
utility (IoU) when using probabilistic models.
Another method to deal with task-speciﬁc loss functions is to directly learn a decision function using empirical
risk minimization (ERM) , popular in computer vision
through the structured SVM. However, there are advantages
to maintaining a probabilistic model: ﬁrst, we can use different loss functions at test time; second, probabilities facilitate combining separate submodels, each of which can be
separately designed and trained; and third, there is currently
no consistent ERM method for structured prediction .
1.1. Prior Work
Previous work has investigated empirical risk minimization (ERM) with higher-order loss functions; in computer vision the ﬁrst work is who showed how to learn
with the intersection-over-union loss restricted to bounding
boxes. For segmentation Ranjbar et al. approximate the
intersection-over-union score for empirical risk minimization, and for the binary case, Tarlow and Zemel propose efﬁcient inference procedures for the loss-augmented
inference problem. Kr¨ahenb¨uhl and Koltun extend 
to handle higher-order loss functions. Beside the higherorder IoU score other higher-order loss functions such as
label count losses and their approximability with simple
models has been investigated by Pletscher and Kohli 
and K¨uttel and Ferrari . This prior work puts emphasis
in learning with higher-order losses; however, for learning
they all use the empirical risk learning objective. Our work
is different in that we follow the “classic” recipe and ﬁrst
construct a probabilistic model, then solve the optimal decision task using the higher-order loss function. The only
prior work that has considered this optimal decision task is
Tarlow and Adams , who proposed a greedy algorithm.
Contributions.
Our work makes the following contributions. 1. We derive a closed-form statistical approximation, (8), to the intersection-over-union score for the case
of conditionally independent beliefs. 2. We propose an algorithm, Alg. 2, for making approximately optimal decisions under the intersection-over-union score. 3. We experimentally evaluate the algorithm on multiple data sets and
demonstrate an increase in intersection-over-union score.
2. Problem and Statistical Approximation
We ﬁrst formalize what it means to make optimal decisions under uncertainty .
Problem 1 (Optimal Decision Making). Given a set Y =
{1, . . . , K} of classes, an index set V = {1, . . . , n}, an
utility function U : YV × YV →R, and a probability distribution p over the set YV, ﬁnd the optimal decision
y∗= argmax
Ez∼p [U(z, y)] .
Intuitively, (1) optimizes our expected utility under every
possibility z, weighted by our beliefs about the state of the
world as encoded in p. The problem could be generalized
by allowing more general decision domains but for most
applications in computer vision (1) is sufﬁcient.
Solving (1) is difﬁcult for three reasons. First, it is an
optimization problem over a large set whose size grows exponentially in |V|. Second, the expectation Ez∼p[·] requires
computation of an average over the same large set. Third,
the function U may not have enough structure for efﬁcient
computation. In practice the tractability of (1) depends on
whether we can replace the expectation expression with a
simple closed-form solution. For example, in the simple
case when U is the negative Hamming loss it decomposes
additively over V so that the expectation commutes and each
variable can be optimized separately, yielding the maximum
posterior marginal (MPM) solution . This is not the case
for more complicated utility functions, as we now illustrate.
2.1. Intersection-over-Union Utility
The intersection-over-union score is a popular benchmark score for semantic segmentation. It has become popular in the computer vision community due to the PASCAL
VOC segmentation challenges . It is deﬁned as follows.
Deﬁnition 2 (Intersection-over-Union Utility). Given a
ground truth assignment y
YV and a prediction
∈YV, the intersection-over-union utility is Uiou
iou (z, y), where
iou (z, y) =
i∈V 1{zi=k∧yi=k}
i∈V 1{zi=k∨yi=k}
intersection-over-union
1{predicate} is the indicator function which is one in case the
predicate is true and zero otherwise.
For the PASCAL VOC segmentation benchmark the set
V is the set of all pixels in all test set images. Therefore (2)
contains ratios of sums over all pixels and does not decompose over pixels. This property makes (1) difﬁcult to solve.
In particular we obtain the following specialization of (1) to
the intersection-over-union utility.
Ez∼p[Uiou(z, y)] = 1
i∈V 1{zi=k∧yi=k}
i∈V 1{zi=k∨yi=k}
Equation (3) contains an expectation of a ratio, which
does not have a closed form solution. We now show how
this expectation can be approximated using techniques from
asymptotic statistics. In particular, we have the following
result due to Rice .
Proposition 3 (Rice ). Let S and T be two real-valued
random variables with ﬁnite moments of all order and with
P(T = 0) = 0 and ET ̸= 0. Then
Ψj = (−1)j (ES)⟨jT⟩+ ⟨S,j T⟩
Here ⟨jT⟩denotes the j’th central moment of T so that
⟨1T⟩= 0 and we write ⟨S,j T⟩= E[(S −ES)(T −ET)j].
In applications of this result the inﬁnite sum is often truncated. Similar but less general results are discussed in 
and [42, Section 4.10], and a discussion of expectations of
ratios is given by Heijmans .
To apply Proposition 3 to our expectation (3) we deﬁne
i∈V 1{zi=k∧yi=k} and Tk = P
i∈V 1{zi=k∨yi=k}
for each k ∈Y so that we have E[U (k)
iou (z, y)] = E[Sk/Tk].
We are interested in expanding E[Sk/Tk] = E[Sk]/E[Tk]+
j=1 Ψj, so we need to compute E[Sk], E[Tk], and Ψj.
For general p these expectations do not have simple closed-form solutions, but we can make progress
if we assume conditional independence, i.e. p(z|x)
i∈V pi(zi|x), where x would be an observed image. Assuming conditional independence is a natural modeling assumption in many probabilistic models in computer vision
and does not imply unconditional independence between
different zi’s.
For example, conditional independence is
assumed when using random forests to predict pixel
marginals in semantic segmentation. Under this assumption
we have the following result, as also derived by .
Proposition 4. For the above deﬁnition of Sk and Tk
and for a conditionally independent distribution p(z) =
i∈V pi(zi) we have, as a function of the decision y,
pi(k) 1{yi=k},
(1{yi=k} + pi(k) 1{yi̸=k}).
We give the proof in the supplementary materials. We
now have simple closed-form expressions for E[Sk] and
E[Tk]. Let us examine the additional expansion terms Ψj.
For the ﬁrst term Ψ1 we have the following result.
Proposition 5.
For the intersection-over-union utility
E[Sk/Tk] we have Ψ1 = 0 in expansion (4).
We give the proof in the supplementary materials. It is
possible to analyze the higher order terms Ψj, j ≥2, but
we stop here because Proposition 5 already implies a strong
guarantee on the quality of the approximation. In particular
by standard results for the delta method, [42, Section 4.3],
we have asymptotically for n →∞that
Together with (6) and (7) this yields a closed-form approximation to E[Sk/Tk]. Note that in a typical computer vision
application n will be large (n ≫103) and therefore the
approximation error will be small. This guarantee also explains the empirical success of this approximation as used
by where the approximation was derived heuristically and described as “surrogate” and “relaxation”. We
now examine the guarantee (8) experimentally.
2.2. Experimental Validation of the Approximation
We perform the following simulation experiment. For
each of N = 5000 binary variables we sample a probability distribution pi from a Dirichlet prior with uniform parameter α = 0.2. We then select n ∈{100, 200, . . . , N}
and perform a Monte Carlo evaluation of E[S1/T1], E[S1],
and E[T1], where the expectation is evaluated by simulating
yi ∼pi and zi ∼pi one hundred thousand times. We then
compare E[S1/T1] with E[S1]/E[T1] and plot n against the
error on a log-log plot, see Figure 2.
Error E[Sk/Tk]−(E[Sk]/E[Tk])
Sample size
Approximation error
Best linear fit
Figure 2. Approximation error of (8) as a function of n. Note the
log-log axes. The results are obtained using Monte Carlo simulation of (8). The predicted O(n−1) behavior of the error is clearly
visible and the ﬁtted slope coefﬁcient is −0.9954 ≈−1.
Because the theory predicts a O(n−1) behavior of the error we ﬁt a line to the second half of the error observations.
The slope of the line agrees with an O(n−1) error.
We now develop a method for making optimal decisions
under the intersection-over-union utility function. By specializing (1) to the intersection-over-union utility and by using approximation (8) with the closed form solutions (6)
and (7) we obtain the following problem, as in .
Problem 6 (Approximately Optimal Decision Making under the Intersection-over-Union Utility). Given marginal
beliefs pi over K classes, ﬁnd the prediction by solving
i∈V pi(k) λi,k
i∈V [pi(k) + (1 −pi(k)) λi,k]
λi,k ∈{0, 1},
Here λi,k is an indicator variable, selecting for each
yi one label λi,k = 1 so that yi = k.
Problem 6 is
a sum of ratios of afﬁne functions.
The number of ratios equals the number K of classes in the problem. This
type of optimization problem is known in the optimization
community by different names: sum-of-ratio linear fractional program , multiple ratio hyperbolic 0-1 programming problem , and generalized linear fractional programs . For binary variables these problems are known
to be NP-hard in general even for a single ratio. But if we relax the constraint λi,k ∈{0, 1} to the interval λi,k ∈ ,
the known results are surprising: a sum containing only a
single ratio is solvable in polynomial time by the Charnes-
Cooper transformation , a sum of two ratios is also solvable in polynomial-time and is known to be pseudoconvex under restrictive conditions [6, Section 7.5] that do
not apply in our case. For three and more ratios the problem
remains hard even in the relaxed domain .
One sensible approach to solving (9) is proposed in ,
where the authors use greedy local search, iteratively changing one variable at a time to improve the objective. The
method is simple to implement, efﬁcient, and maintains a
feasible solution at all times; we describe it in the supplementary materials.
However, in many optimization
problems—for example discrete energy minimization problems —such a simple greedy approach can be outperformed by approaches that make use of the problem structure. This is generally the case when the problem has an intrinsic complexity leading to local optima where the greedy
method could get stuck in. Our proposed method uses the
problem structure more globally but is based on solving
a relaxation and therefore could lead to non-integral solutions. The question which method is preferable is then
an empirical question of how intrinsically complex the objective (9) really is. If it is sufﬁciently simple the greedy
method may work better, but if it is complex we may see
the more global method to work better.
Our approach to solving (9) will be to iteratively improve
a solution y by optimizing over blocks of variables corresponding to pairs of labels, holding all other variables ﬁxed.
Doing so yields a tractable subproblem where only two fractions appear in (9) and we use Konno’s algorithm to
solve the corresponding relaxed subproblem.
3.1. Optimizing over Two Classes
The strategy to optimize over a large subset of variables depending on the current candidate solution has been
used in the α-β-swap graphcut algorithm and is also
used for solving a number of hard combinatorial problems
in the framework of very large scale neighborhood search
(VLSN) , [30, Section 4.5.1]. Applying it to (9) we obtain the following subproblem restricted to two classes.
Problem 7 (Two Class Problem). For two distinct classes
k1, k2 ∈Y, and a reference labeling y ∈YV, let W ⊆
Wk1k2 = {i ∈V : yi = k1 ∨yi = k2}, W ̸= ∅, that
is, an arbitrary non-empty subset of the variables currently
labeled k1 or k2. To ﬁnd the optimal labeling restricted to
the set W, we set λi,k1 = µi, λi,k2 = 1 −µi, and solve
i∈W biµi + c0 + P
i∈W diµi ,
µi ∈{0, 1},
where from the terms involving k1 and k2 in (9) we have
i∈V\W pi(k1) 1{yi=k1},
i∈V pi(k1) + P
i∈V\W(1 −pi(k1)) 1{yi=k1},
1 −pi(k1),
i∈V\W pi(k2) 1{yi=k2} + P
i∈W pi(k2),
 pi(k2) + (1 −pi(k2)) 1{yi=k2}
pi(k2) −1,
To solve Problem 7 we adapt the algorithm of Konno
et al. . This algorithm is based on ﬁrst relaxing µi ∈
 , then transforming (10) by means of the Charnes-
Cooper transformation followed by a parametric simplex method for linear programming .
The Charnes-Cooper transformation deﬁnes a set ui of
variables and one additional variable u0 and introduces the
coupling constraints u0 = 1/(d0 + P
i∈W diµi), and ui =
µiu0. Problem 7 can now be rewritten in the new variables
as a sum of a ratio and a linear function as follows.
i∈W biui + c0u0 +
To get rid of the ﬁnal ratio we introduce an auxiliary parameter ξ = b0u0 + P
i∈W biui, and obtain the following
family of parametric linear programs.
ciui + a0u0 +
The algorithm of Konno et al. is an efﬁcient parametric linear programming method to globally maximize P by
sweeping ξ from the minimum possible value to the maximum possible value, tracing the optimal solution for every
value of ξ. The solution path is a sequence of quadratic
functions in ξ and we visualize a typical example in Figure 3. Compared to standard parametric programming for
linear programs [3, Section 5.5] as used in computer vision , problem P(ξ) is more challenging because the
parameter ξ appears both in the objective function as well
as in the constraint (11). As a result, if we want to increase ξ
while maintaining an optimal solution, we need to perform
either a primal simplex update or a dual simplex update .
3.2. Implementation
Algorithm 1 describes the main loop of the algorithm.
The objective function—we show an example in Figure 3—
is deﬁned between ξmin and ξmax and composed of piecewise quadratic functions in ξ. The algorithm starts with
Figure 3. Typical example of function P(ξ) to be maximized in
ξ ∈[ξmin, ξmax] by the parametric simplex method. P(ξ) is piecewise quadratic, in this case comprised of 441 segments.
Algorithm 1 Konno’s algorithm for (10)
1: function PARASIMPLEX(a0, ai, b0, bi, c0, ci, d0, di)
ξmin ←b0/d0
ξmax ←(b0 + P
i∈W bi)/(d0 + P
i∗←argmaxi∈W(ai + ξminci)
B ←{u0, ui∗} ∪{si : i ∈W}
▷feasible basis
B ←PRIMALSIMPLEX(B)
▷optimal basis
while ξ < ξmax do
(ξ, ξ, type, v) ←COMPUTERANGE(B)
ˆξ ←argmaxξ∈[ξ,ξ] P(ξ)
▷1D quadratic
Compute maximizer µ(ˆξ)
▷next interval
if type = primal then
B ←PRIMALSIMPLEX(B, v) ▷v enters B
else if type = dual then
B ←DUALSIMPLEX(B, v)
▷v leaves B
return global maximizer µ(ξ∗) of P
20: end function
ξ = ξmin and in each iteration increases ξ such that the
next interval deﬁning another quadratic function is reached
(lines 12). Within each interval we obtain the closed form
solution (line 10) and keep track of the global optimal solution ξ∗. We transform (12) to the standard slack form
ui −u0 + si = 0, where si ≥0 is a slack variable.
Implementing Algorithm 1 is challenging because both a
primal and dual simplex method need to be implemented . With care a matrix-free implementation can be achieved
by deriving closed-form solutions to two linear systems related to the constraint system deﬁning P(ξ) and by using a
non-standard update strategy to the reduced costs . Our
optimized C++ source code is available from the author’s
homepage. Despite our optimizations to the implementation the overall runtime of Algorithm 1 remains O(n2). We
show typical runtimes in Fig. 4 and observe that they are
empirically independent of the coefﬁcients.
3.3. Optimizing (9) by Local Search
Our proposed method is fast enough to optimize predictions for a single image. Unfortunately, because the current
use of IoU in segmentation benchmarks applies to the entire test data set the quadratic runtime prohibits us to optimize over the decision variables of all images simultaneously. We therefore divide the set of all variables into random subsets of a speciﬁed size. Each subset is optimized
sequentially and because each problem corresponds to a
neighborhood around the current solution we can guarantee
monotonic improvement of the objective (9). The overall
procedure is shown in Algorithm 2. We use a subproblem
size M = 2000 for the experiments. For a subproblem size
of M = 1 Algorithm 2 becomes the greedy method of .
Algorithm 2 Large Neighborhood Local Search for (9)
1: function OPTIMIZEIOU(p, V, K, iters, M)
yi ←argmaxk=1,...,K pi(k) ▷initialize with MAP
for t = 1, . . . , iters do
S ←RANDOMSHUFFLE((1, 2, . . . , K))
for j = 1, . . . , ⌊K/2⌋do
▷in parallel
k1 ←S(2j −1),
Wk1k2 ←(i ∈V : yi = k1 ∨yi = k2)
Wk1k2 ←RANDOMSHUFFLE(Wk1k2)
for r = 0, 1, . . . , ⌊|Wk1k2|/M⌋do
W ←Wk1k2(rM + 1, . . . , (r + 1)M)
Compute a0, ai, b0, bi, c0, ci, d0, di
µ ←PARASIMPLEX(a, b, c, d)
for i ∈W do
yi ←k1 if µi ≥1
2, k2 otherwise
return approximately optimal decision y
20: end function
4. Experiments and Results
We now validate the key contribution; that is, we would
like to demonstrate that given the same marginal posteriors
our method can optimize the intersection-over-union score.
We use three baselines: the greedy method , the simple
Number of decision variables
Runtime (s)
Figure 4. O(n2) runtime of Algorithm 1 as a function of n with
one unit standard deviation over 10 replications with randomly
chosen problem coefﬁcients. For n = 2000, the size we adopt
in Algorithm 2, we have a runtime of 156ms on a single core of an
Intel Xeon E5-1650 3.20GHz CPU.
MAP decision (RF-MAP/MPM), which in our case is identical to the maximum posterior marginal (MPM) , and a
simple “inverse weighted” MAP method (iwMAP). For this
method we sum for each class k our total beliefs over all
variables, as γk = P
i∈V pi(k), then compute the inverseweighted beliefs ˜pi(k) = (pi(k)/γk)/(P
ℓpi(ℓ)/γℓ). We
then compute the MAP decision using ˜p. The intuition is
that the reweighting makes our new total beliefs over all
classes uniform and hence can give more probability mass
to apriori less likely classes. For our methods we also report the believed accuracy and IoU score. The believed IoU
score is simply our objective (9), and the believed accuracy
i∈V pi(yi))/|V|. We run Algorithm 2 for 30 and 60 iterations to obtain approximately optimal decisions RF-IoUopt30 and RF-IoU-opt60 under the IoU utility.
For all experiments we use marginals obtained using a
random forest applied to each pixel . We use the decision tree implementation released by the authors of 
and augment the features used with simple Histogram-of-
Gradients, integral image, and image location features but
otherwise use the default settings. We use three semantic
scene segmentation data sets: LabelMeFacade , Stanford Background , and PASCAL VOC 2012 . Although we downscale images for optimizing our decision
objective, we always assess accuracy and intersection-overunion score on the original full resolution images.
4.1. LabelMeFacade Dataset
The LabelMeFacade data set uses nine semantic labels describing facade elements.
There are 845 test images which we downscale by a factor of 0.125 for a total of
4,429,952 decision variables. The baseline results from random forests in Table 1 are comparable in accuracy (71.28%)
with the state-of-the-art (67.33%), but yield an improved
IoU score after optimization (IoU-opt60, IoU-greedy).
Acc. belief/actual
IoU belief/actual
ICFHGWS+ 
RF-MAP/MPM
78.69 / 71.28
34.79 / 31.74
55.30 / 53.76
32.35 / 32.01
RF-IoU-greedy 
75.75 / 69.60
38.75 / 35.96
RF-IoU-opt30
75.60 / 69.47
38.67 / 35.91
RF-IoU-opt60
75.72 / 69.56
38.70 / 35.91
Table 1. LabelMeFacade results (845 test images) .
4.2. Stanford Background Dataset
The Stanford background dataset uses 8 semantic
classes and 715 images; the standard setup is ﬁve-fold cross
validation. We produce full resolution marginal posteriors
for each of the ﬁve test-folds and then downscale the images
by a factor of 0.25 for a total of 3,377,600 decision variables. The results are reported in Table 2. Our multiclass
accuracy (74.7%) is roughly comparable with the state-ofthe-art (81.9% in ). Among our results the MAP/MPM
decision has the highest accuracy and the IoU-opt60/IoUgreedy decisions have the highest IoU score.
Acc. belief/actual
IoU belief/actual
Gould et al. 
Kumar/Koller 
Lempitsky et al. 
Tighe/Lazebnik 
Farabet et al. 
RF-MAP/MPM
75.75 / 74.70
50.42 / 50.51
69.21 / 69.90
47.67 / 49.59
RF-IoU-greedy 
75.15 / 74.58
51.79 / 52.36
RF-IoU-opt30
75.14 / 74.56
51.78 / 52.34
RF-IoU-opt60
75.14 / 74.58
51.79 / 52.36
Table 2. Five fold cross-validation results (715 images) for the
Stanford Background dataset .
4.3. PASCAL VOC 2012 Dataset
The PASCAL VOC semantic segmentation benchmark is a challenging segmentation dataset with 21
classes. We train a random forest on the “train” subset and
produce a posterior for “val”, and also train on the “trainval”
subset and produce a posterior for “test”. We downscale the
posterior with a factor or 0.125 to obtain 4,104,672 decision
variables for “val” (1449 images) and 4,123,073 decision
variables for “test” (1456 images). The results are shown in
Table 3 and Table 4. For this challenging dataset we do not
come close to the state-of-the-art performance; however, on
“val” the IoU score achieved by the MAP/MPM decision
is improved from 3.51% to 11.08%. Because both decisions are obtained from the same posterior marginals this
increase is directly attributable to our algorithm. As with
the previous datasets the best accuracy is achieved by the
MAP/MPM decision (RF-MAP/MPM), the best IoU score
by the IoU-decision (RF-IoU-greedy).
5. Discussions
The results demonstrate that methods which explicitly
optimize for the IoU performance outperform methods that
are unaware of the utility function.
However, the simple greedy local search method is surprisingly good,
slightly outperforming our global method. This is an indication that the objective (9) is simple and does not have
many local optima. For practical purposes one can therefore use the more efﬁcient greedy method. Note that this
behaviour is different to what is observed in discrete energy
minimization problems where greedy methods like iterated conditional modes (ICM) are regularly outperformed
Acc. belief/actual
IoU belief/actual
RF-MAP/MPM
80.47 / 73.33
3.82 / 3.51
21.00 / 24.61
4.09 / 6.68
RF-IoU-greedy 
68.43 / 69.17
7.60 / 11.65
RF-IoU-opt30
65.98 / 66.95
7.20 / 10.68
RF-IoU-opt60
68.03 / 68.63
7.38 / 11.08
Table 3. PASCAL VOC 2012 validation set results (1449 images).
Acc. belief/actual
IoU belief/actual
BONN O2PCPMC
RF-MAP/MPM
4.01 / 3.61
4.02 / 7.53
RF-IoU-greedy 
7.67 / 12.47
RF-IoU-opt30
7.27 / 11.49
RF-IoU-opt60
7.58 / 12.27
Table 4. PASCAL VOC 2012 test set results (1456 images).
by more global methods.
Further analysis of the objective (9) is needed to explain this observation.
More generally our work raises a number of issues when
using probabilistic models in computer vision and some
speciﬁc points with the IoU utility.
Computational tractability of higher-order utility/loss
functions.
We have assumed conditionally independent
beliefs pi for each variable i ∈V. Arguably this is a strong
simplifying assumption. Yet, even with this assumption the
decision problem (Problem 6) remains a hard combinatorial optimization problem. It is reasonable to assume that
for more complex models the task would remain at least
as hard. Given this intractability, it is then interesting to
note that in the recent “marginal-based learning” framework
of it is possible to do gradient-based parameter optimization using the intersection-over-union utility as demonstrated in . Although the framework is based on empirical risk minimization (ERM) it retains the probabilistic inference method as internal component. It may be the case
that ERM is better suited for tractably learning with higherorder loss functions because the hard part—handling the
higher-order loss—can be handled during training, whereas
test-time prediction remains efﬁcient .
The importance of calibrated probabilities.
seen in the experimental results that our believed accuracy
and believed intersection-over-union scores deviate from
the actual scores. Could it be that we have “overﬁtted” our
decisions by optimizing (1)? No, this is not possible: we already have beliefs p, and we merely make the best decision
under what we believe. The explanation for the observed
deviation is that our probabilities p are not perfectly calibrated. That is, we may believe that a variable yi is in a certain state with a certain probability, but we systematically
over- or underestimate this probability. As a consequence
the expectation (1) does not correctly estimate the consequences of our decisions, as also observed in .
How can we improve the calibration of our posterior
probabilities? If our probabilistic model class is sufﬁciently
accurate then known results guarantee that we will eventually be well-calibrated . However, in realistic computer
vision applications our model is misspeciﬁed and even when
using Bayesian inference there is a systematic miscalibration . Then a more pragmatic approach to calibration
may be to use bagging and recalibration methods, .
Sampling theory and decomposability.
In the VOC segmentation challenge the IoU utility is evaluated on the
confusion matrix of the entire test data set. We argue against
this on the basis that the data set was created by sampling a
set of images at random from a large population of images
on the internet. Therefore one unit of observation is a single image and the utility function should decompose along
these independent units. Applying the IoU utility on the entire data set is not meaningful: if you do, your decisions on
a particular image can potentially be improved by considering future independent observables. On the other hand,
applying the IoU utility to individual images is meaningful.
6. Conclusion
In this work we have studied the intersection-over-union
score, a popular higher-order utility function used in image
segmentation benchmarks. Starting with decision theory we
proposed a statistical approximation and algorithm for making approximately optimal decisions under the IoU utility.
The experiments have conﬁrmed improved results. We hope
to stimulate further research into learning and optimal decision making with higher-order loss functions.
Acknowledgements.
The author thanks Danny Tarlow
and Christoph Lampert for discussions and feedback.