MobileNets: Efﬁcient Convolutional Neural Networks for Mobile Vision
Applications
Andrew G. Howard
Menglong Zhu
Dmitry Kalenichenko
Weijun Wang
Tobias Weyand
Marco Andreetto
Hartwig Adam
Google Inc.
{howarda,menglong,bochen,dkalenichenko,weijunw,weyand,anm,hadam}@google.com
We present a class of efﬁcient models called MobileNets
for mobile and embedded vision applications. MobileNets
are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep
neural networks. We introduce two simple global hyperparameters that efﬁciently trade off between latency and
accuracy. These hyper-parameters allow the model builder
to choose the right sized model for their application based
on the constraints of the problem.
We present extensive
experiments on resource and accuracy tradeoffs and show
strong performance compared to other popular models on
ImageNet classiﬁcation. We then demonstrate the effectiveness of MobileNets across a wide range of applications and
use cases including object detection, ﬁnegrain classiﬁcation, face attributes and large scale geo-localization.
1. Introduction
Convolutional neural networks have become ubiquitous
in computer vision ever since AlexNet popularized
deep convolutional neural networks by winning the ImageNet Challenge: ILSVRC 2012 . The general trend
has been to make deeper and more complicated networks
in order to achieve higher accuracy . However, these advances to improve accuracy are not necessarily making networks more efﬁcient with respect to size and
speed. In many real world applications such as robotics,
self-driving car and augmented reality, the recognition tasks
need to be carried out in a timely fashion on a computationally limited platform.
This paper describes an efﬁcient network architecture
and a set of two hyper-parameters in order to build very
small, low latency models that can be easily matched to the
design requirements for mobile and embedded vision applications. Section 2 reviews prior work in building small
models. Section 3 describes the MobileNet architecture and
two hyper-parameters width multiplier and resolution multiplier to deﬁne smaller and more efﬁcient MobileNets. Section 4 describes experiments on ImageNet as well a variety
of different applications and use cases. Section 5 closes
with a summary and conclusion.
2. Prior Work
There has been rising interest in building small and efﬁcient neural networks in the recent literature, e.g. . Many different approaches can be generally
categorized into either compressing pretrained networks or
training small networks directly.
This paper proposes a
class of network architectures that allows a model developer to speciﬁcally choose a small network that matches
the resource restrictions (latency, size) for their application.
MobileNets primarily focus on optimizing for latency but
also yield small networks. Many papers on small networks
focus only on size but do not consider speed.
MobileNets are built primarily from depthwise separable
convolutions initially introduced in and subsequently
used in Inception models to reduce the computation in
the ﬁrst few layers. Flattened networks build a network
out of fully factorized convolutions and showed the potential of extremely factorized networks. Independent of this
current paper, Factorized Networks introduces a similar
factorized convolution as well as the use of topological connections. Subsequently, the Xception network demonstrated how to scale up depthwise separable ﬁlters to out
perform Inception V3 networks. Another small network is
Squeezenet which uses a bottleneck approach to design
a very small network. Other reduced computation networks
include structured transform networks and deep fried
convnets .
A different approach for obtaining small networks is
shrinking, factorizing or compressing pretrained networks.
Compression based on product quantization , hashing
 
Landmark Recognition
Finegrain Classification
Object Detection
MobileNets
Photo by Sharon VanderKaay (CC BY 2.0)
Photo by Juanedc (CC BY 2.0)
Photo by HarshLight (CC BY 2.0)
Face Attributes
Google Doodle by Sarah Harrison
Figure 1. MobileNet models can be applied to various recognition tasks for efﬁcient on device intelligence.
 , and pruning, vector quantization and Huffman coding
 have been proposed in the literature. Additionally various factorizations have been proposed to speed up pretrained networks .
Another method for training
small networks is distillation which uses a larger network to teach a smaller network. It is complementary to
our approach and is covered in some of our use cases in
section 4. Another emerging approach is low bit networks
 .
3. MobileNet Architecture
In this section we ﬁrst describe the core layers that MobileNet is built on which are depthwise separable ﬁlters.
We then describe the MobileNet network structure and conclude with descriptions of the two model shrinking hyperparameters width multiplier and resolution multiplier.
3.1. Depthwise Separable Convolution
The MobileNet model is based on depthwise separable
convolutions which is a form of factorized convolutions
which factorize a standard convolution into a depthwise
convolution and a 1×1 convolution called a pointwise convolution. For MobileNets the depthwise convolution applies a single ﬁlter to each input channel. The pointwise
convolution then applies a 1×1 convolution to combine the
outputs the depthwise convolution. A standard convolution
both ﬁlters and combines inputs into a new set of outputs
in one step. The depthwise separable convolution splits this
into two layers, a separate layer for ﬁltering and a separate
layer for combining. This factorization has the effect of
drastically reducing computation and model size. Figure 2
shows how a standard convolution 2(a) is factorized into a
depthwise convolution 2(b) and a 1 × 1 pointwise convolution 2(c).
A standard convolutional layer takes as input a DF ×
DF × M feature map F and produces a DF × DF × N
feature map G where DF is the spatial width and height
of a square input feature map1, M is the number of input
channels (input depth), DG is the spatial width and height of
a square output feature map and N is the number of output
channel (output depth).
The standard convolutional layer is parameterized by
convolution kernel K of size DK×DK×M×N where DK
is the spatial dimension of the kernel assumed to be square
and M is number of input channels and N is the number of
output channels as deﬁned previously.
The output feature map for standard convolution assuming stride one and padding is computed as:
Ki,j,m,n · Fk+i−1,l+j−1,m
Standard convolutions have the computational cost of:
DK · DK · M · N · DF · DF
where the computational cost depends multiplicatively on
the number of input channels M, the number of output
channels N the kernel size Dk × Dk and the feature map
size DF × DF . MobileNet models address each of these
terms and their interactions. First it uses depthwise separable convolutions to break the interaction between the number of output channels and the size of the kernel.
The standard convolution operation has the effect of ﬁltering features based on the convolutional kernels and combining features in order to produce a new representation.
The ﬁltering and combination steps can be split into two
steps via the use of factorized convolutions called depthwise
1We assume that the output feature map has the same spatial dimensions as the input and both feature maps are square. Our model shrinking
results generalize to feature maps with arbitrary sizes and aspect ratios.
separable convolutions for substantial reduction in computational cost.
Depthwise separable convolution are made up of two
layers: depthwise convolutions and pointwise convolutions.
We use depthwise convolutions to apply a single ﬁlter per
each input channel (input depth). Pointwise convolution, a
simple 1×1 convolution, is then used to create a linear combination of the output of the depthwise layer. MobileNets
use both batchnorm and ReLU nonlinearities for both layers.
Depthwise convolution with one ﬁlter per input channel
(input depth) can be written as:
ˆKi,j,m · Fk+i−1,l+j−1,m
where ˆK is the depthwise convolutional kernel of size
DK × DK × M where the mth ﬁlter in ˆK is applied to
the mth channel in F to produce the mth channel of the
ﬁltered output feature map ˆG.
Depthwise convolution has a computational cost of:
DK · DK · M · DF · DF
Depthwise convolution is extremely efﬁcient relative to
standard convolution. However it only ﬁlters input channels, it does not combine them to create new features. So
an additional layer that computes a linear combination of
the output of depthwise convolution via 1 × 1 convolution
is needed in order to generate these new features.
The combination of depthwise convolution and 1 × 1
(pointwise) convolution is called depthwise separable convolution which was originally introduced in .
Depthwise separable convolutions cost:
DK · DK · M · DF · DF + M · N · DF · DF
which is the sum of the depthwise and 1 × 1 pointwise convolutions.
By expressing convolution as a two step process of ﬁltering and combining we get a reduction in computation of:
DK · DK · M · DF · DF + M · N · DF · DF
DK · DK · M · N · DF · DF
MobileNet uses 3 × 3 depthwise separable convolutions
which uses between 8 to 9 times less computation than standard convolutions at only a small reduction in accuracy as
seen in Section 4.
Additional factorization in spatial dimension such as in
 does not save much additional computation as very
little computation is spent in depthwise convolutions.
(a) Standard Convolution Filters
(b) Depthwise Convolutional Filters
(c) 1×1 Convolutional Filters called Pointwise Convolution in the context of Depthwise Separable Convolution
Figure 2. The standard convolutional ﬁlters in (a) are replaced by
two layers: depthwise convolution in (b) and pointwise convolution in (c) to build a depthwise separable ﬁlter.
3.2. Network Structure and Training
The MobileNet structure is built on depthwise separable
convolutions as mentioned in the previous section except for
the ﬁrst layer which is a full convolution. By deﬁning the
network in such simple terms we are able to easily explore
network topologies to ﬁnd a good network. The MobileNet
architecture is deﬁned in Table 1. All layers are followed by
a batchnorm and ReLU nonlinearity with the exception
of the ﬁnal fully connected layer which has no nonlinearity
and feeds into a softmax layer for classiﬁcation. Figure 3
contrasts a layer with regular convolutions, batchnorm and
ReLU nonlinearity to the factorized layer with depthwise
convolution, 1 × 1 pointwise convolution as well as batchnorm and ReLU after each convolutional layer. Down sampling is handled with strided convolution in the depthwise
convolutions as well as in the ﬁrst layer. A ﬁnal average
pooling reduces the spatial resolution to 1 before the fully
connected layer. Counting depthwise and pointwise convolutions as separate layers, MobileNet has 28 layers.
It is not enough to simply deﬁne networks in terms of a
small number of Mult-Adds. It is also important to make
sure these operations can be efﬁciently implementable. For
3x3 Depthwise Conv
Figure 3. Left: Standard convolutional layer with batchnorm and
ReLU. Right: Depthwise Separable convolutions with Depthwise
and Pointwise layers followed by batchnorm and ReLU.
instance unstructured sparse matrix operations are not typically faster than dense matrix operations until a very high
level of sparsity. Our model structure puts nearly all of the
computation into dense 1 × 1 convolutions. This can be implemented with highly optimized general matrix multiply
(GEMM) functions. Often convolutions are implemented
by a GEMM but require an initial reordering in memory
called im2col in order to map it to a GEMM. For instance,
this approach is used in the popular Caffe package .
1×1 convolutions do not require this reordering in memory
and can be implemented directly with GEMM which is one
of the most optimized numerical linear algebra algorithms.
MobileNet spends 95% of it’s computation time in 1 × 1
convolutions which also has 75% of the parameters as can
be seen in Table 2. Nearly all of the additional parameters
are in the fully connected layer.
MobileNet models were trained in TensorFlow using RMSprop with asynchronous gradient descent similar to Inception V3 . However, contrary to training
large models we use less regularization and data augmentation techniques because small models have less trouble
with overﬁtting. When training MobileNets we do not use
side heads or label smoothing and additionally reduce the
amount image of distortions by limiting the size of small
crops that are used in large Inception training . Additionally, we found that it was important to put very little or
no weight decay (l2 regularization) on the depthwise ﬁlters
since their are so few parameters in them. For the ImageNet
benchmarks in the next section all models were trained with
same training parameters regardless of the size of the model.
3.3. Width Multiplier: Thinner Models
Although the base MobileNet architecture is already
small and low latency, many times a speciﬁc use case or
application may require the model to be smaller and faster.
In order to construct these smaller and less computationally
expensive models we introduce a very simple parameter α
called width multiplier. The role of the width multiplier α is
to thin a network uniformly at each layer. For a given layer
Table 1. MobileNet Body Architecture
Type / Stride
Filter Shape
Input Size
3 × 3 × 3 × 32
224 × 224 × 3
Conv dw / s1
3 × 3 × 32 dw
112 × 112 × 32
1 × 1 × 32 × 64
112 × 112 × 32
Conv dw / s2
3 × 3 × 64 dw
112 × 112 × 64
1 × 1 × 64 × 128
56 × 56 × 64
Conv dw / s1
3 × 3 × 128 dw
56 × 56 × 128
1 × 1 × 128 × 128
56 × 56 × 128
Conv dw / s2
3 × 3 × 128 dw
56 × 56 × 128
1 × 1 × 128 × 256
28 × 28 × 128
Conv dw / s1
3 × 3 × 256 dw
28 × 28 × 256
1 × 1 × 256 × 256
28 × 28 × 256
Conv dw / s2
3 × 3 × 256 dw
28 × 28 × 256
1 × 1 × 256 × 512
14 × 14 × 256
5× Conv dw / s1
3 × 3 × 512 dw
14 × 14 × 512
1 × 1 × 512 × 512
14 × 14 × 512
Conv dw / s2
3 × 3 × 512 dw
14 × 14 × 512
1 × 1 × 512 × 1024
7 × 7 × 512
Conv dw / s2
3 × 3 × 1024 dw
7 × 7 × 1024
1 × 1 × 1024 × 1024
7 × 7 × 1024
Avg Pool / s1
Pool 7 × 7
7 × 7 × 1024
1024 × 1000
1 × 1 × 1024
Softmax / s1
1 × 1 × 1000
Table 2. Resource Per Layer Type
Parameters
Conv 1 × 1
Conv DW 3 × 3
Conv 3 × 3
Fully Connected
and width multiplier α, the number of input channels M becomes αM and the number of output channels N becomes
The computational cost of a depthwise separable convolution with width multiplier α is:
DK · DK · αM · DF · DF + αM · αN · DF · DF
where α ∈(0, 1] with typical settings of 1, 0.75, 0.5 and
0.25. α = 1 is the baseline MobileNet and α < 1 are
reduced MobileNets. Width multiplier has the effect of reducing computational cost and the number of parameters
quadratically by roughly α2. Width multiplier can be applied to any model structure to deﬁne a new smaller model
with a reasonable accuracy, latency and size trade off. It
is used to deﬁne a new reduced structure that needs to be
trained from scratch.
3.4. Resolution Multiplier: Reduced Representation
The second hyper-parameter to reduce the computational
cost of a neural network is a resolution multiplier ρ. We ap-
Table 3. Resource usage for modiﬁcations to standard convolution.
Note that each row is a cumulative effect adding on top of the
previous row. This example is for an internal MobileNet layer
with DK = 3, M = 512, N = 512, DF = 14.
Layer/Modiﬁcation
Parameters
Convolution
Depthwise Separable Conv
ply this to the input image and the internal representation of
every layer is subsequently reduced by the same multiplier.
In practice we implicitly set ρ by setting the input resolution.
We can now express the computational cost for the core
layers of our network as depthwise separable convolutions
with width multiplier α and resolution multiplier ρ:
DK · DK · αM · ρDF · ρDF + αM · αN · ρDF · ρDF (7)
where ρ ∈(0, 1] which is typically set implicitly so that
the input resolution of the network is 224, 192, 160 or 128.
ρ = 1 is the baseline MobileNet and ρ < 1 are reduced
computation MobileNets. Resolution multiplier has the effect of reducing computational cost by ρ2.
As an example we can look at a typical layer in MobileNet and see how depthwise separable convolutions,
width multiplier and resolution multiplier reduce the cost
and parameters. Table 3 shows the computation and number
of parameters for a layer as architecture shrinking methods
are sequentially applied to the layer. The ﬁrst row shows
the Mult-Adds and parameters for a full convolutional layer
with an input feature map of size 14 × 14 × 512 with a kernel K of size 3 × 3 × 512 × 512. We will look in detail
in the next section at the trade offs between resources and
4. Experiments
In this section we ﬁrst investigate the effects of depthwise convolutions as well as the choice of shrinking by reducing the width of the network rather than the number of
layers. We then show the trade offs of reducing the network based on the two hyper-parameters: width multiplier
and resolution multiplier and compare results to a number
of popular models. We then investigate MobileNets applied
to a number of different applications.
4.1. Model Choices
First we show results for MobileNet with depthwise separable convolutions compared to a model built with full convolutions. In Table 4 we see that using depthwise separable convolutions compared to full convolutions only reduces
Table 4. Depthwise Separable vs Full Convolution MobileNet
Parameters
Conv MobileNet
Table 5. Narrow vs Shallow MobileNet
Parameters
0.75 MobileNet
Shallow MobileNet
Table 6. MobileNet Width Multiplier
Width Multiplier
Parameters
1.0 MobileNet-224
0.75 MobileNet-224
0.5 MobileNet-224
0.25 MobileNet-224
Table 7. MobileNet Resolution
Resolution
Parameters
1.0 MobileNet-224
1.0 MobileNet-192
1.0 MobileNet-160
1.0 MobileNet-128
accuracy by 1% on ImageNet was saving tremendously on
mult-adds and parameters.
We next show results comparing thinner models with
width multiplier to shallower models using less layers. To
make MobileNet shallower, the 5 layers of separable ﬁlters
with feature size 14 × 14 × 512 in Table 1 are removed.
Table 5 shows that at similar computation and number of
parameters, that making MobileNets thinner is 3% better
than making them shallower.
4.2. Model Shrinking Hyperparameters
Table 6 shows the accuracy, computation and size trade
offs of shrinking the MobileNet architecture with the width
multiplier α. Accuracy drops off smoothly until the architecture is made too small at α = 0.25.
Table 7 shows the accuracy, computation and size trade
offs for different resolution multipliers by training MobileNets with reduced input resolutions. Accuracy drops
off smoothly across resolution.
Figure 4 shows the trade off between ImageNet Accuracy and computation for the 16 models made from the
cross product of width multiplier α ∈{1, 0.75, 0.5, 0.25}
and resolutions {224, 192, 160, 128}. Results are log linear
with a jump when models get very small at α = 0.25.
Figure 4. This ﬁgure shows the trade off between computation
(Mult-Adds) and accuracy on the ImageNet benchmark. Note the
log linear dependence between accuracy and computation.
Figure 5. This ﬁgure shows the trade off between the number of
parameters and accuracy on the ImageNet benchmark. The colors
encode input resolutions. The number of parameters do not vary
based on the input resolution.
Figure 5 shows the trade off between ImageNet Accuracy and number of parameters for the 16 models
made from the cross product of width multiplier α ∈
{1, 0.75, 0.5, 0.25} and resolutions {224, 192, 160, 128}.
Table 8 compares full MobileNet to the original
GoogleNet and VGG16 .
MobileNet is nearly
as accurate as VGG16 while being 32 times smaller and
27 times less compute intensive. It is more accurate than
GoogleNet while being smaller and more than 2.5 times less
computation.
Table 9 compares a reduced MobileNet with width multiplier α = 0.5 and reduced resolution 160 × 160. Reduced
MobileNet is 4% better than AlexNet while being 45×
smaller and 9.4× less compute than AlexNet. It is also 4%
better than Squeezenet at about the same size and 22×
less computation.
Table 8. MobileNet Comparison to Popular Models
Parameters
1.0 MobileNet-224
Table 9. Smaller MobileNet Comparison to Popular Models
Parameters
0.50 MobileNet-160
Squeezenet
Table 10. MobileNet for Stanford Dogs
Parameters
Inception V3 
1.0 MobileNet-224
0.75 MobileNet-224
1.0 MobileNet-192
0.75 MobileNet-192
Table 11. Performance of PlaNet using the MobileNet architecture. Percentages are the fraction of the Im2GPS test dataset that
were localized within a certain distance from the ground truth. The
numbers for the original PlaNet model are based on an updated
version that has an improved architecture and training dataset.
Im2GPS PlaNet 
Continent (2500 km)
Country (750 km)
Region (200 km)
City (25 km)
Street (1 km)
4.3. Fine Grained Recognition
We train MobileNet for ﬁne grained recognition on the
Stanford Dogs dataset . We extend the approach of 
and collect an even larger but noisy training set than 
from the web. We use the noisy web data to pretrain a ﬁne
grained dog recognition model and then ﬁne tune the model
on the Stanford Dogs training set. Results on Stanford Dogs
test set are in Table 10. MobileNet can almost achieve the
state of the art results from at greatly reduced computation and size.
4.4. Large Scale Geolocalizaton
PlaNet casts the task of determining where on earth
a photo was taken as a classiﬁcation problem. The approach
divides the earth into a grid of geographic cells that serve as
the target classes and trains a convolutional neural network
on millions of geo-tagged photos. PlaNet has been shown
to successfully localize a large variety of photos and to outperform Im2GPS that addresses the same task.
We re-train PlaNet using the MobileNet architecture on
the same data. While the full PlaNet model based on the Inception V3 architecture has 52 million parameters and
5.74 billion mult-adds. The MobileNet model has only 13
million parameters with the usual 3 million for the body and
10 million for the ﬁnal layer and 0.58 Million mult-adds.
As shown in Tab. 11, the MobileNet version delivers only
slightly decreased performance compared to PlaNet despite
being much more compact. Moreover, it still outperforms
Im2GPS by a large margin.
4.5. Face Attributes
Another use-case for MobileNet is compressing large
systems with unknown or esoteric training procedures. In
a face attribute classiﬁcation task, we demonstrate a synergistic relationship between MobileNet and distillation ,
a knowledge transfer technique for deep networks.
seek to reduce a large face attribute classiﬁer with 75
million parameters and 1600 million Mult-Adds.
classiﬁer is trained on a multi-attribute dataset similar to
YFCC100M .
We distill a face attribute classiﬁer using the MobileNet
architecture. Distillation works by training the classi-
ﬁer to emulate the outputs of a larger model2 instead of the
ground-truth labels, hence enabling training from large (and
potentially inﬁnite) unlabeled datasets. Marrying the scalability of distillation training and the parsimonious parameterization of MobileNet, the end system not only requires
no regularization (e.g. weight-decay and early-stopping),
but also demonstrates enhanced performances.
It is evident from Tab. 12 that the MobileNet-based classiﬁer is resilient to aggressive model shrinking: it achieves a similar
mean average precision across attributes (mean AP) as the
in-house while consuming only 1% the Multi-Adds.
4.6. Object Detection
MobileNet can also be deployed as an effective base network in modern object detection systems. We report results
for MobileNet trained for object detection on COCO data
based on the recent work that won the 2016 COCO challenge . In table 13, MobileNet is compared to VGG
and Inception V2 under both Faster-RCNN and
SSD framework. In our experiments, SSD is evaluated
with 300 input resolution (SSD 300) and Faster-RCNN is
compared with both 300 and 600 input resolution (Faster-
RCNN 300, Faster-RCNN 600). The Faster-RCNN model
evaluates 300 RPN proposal boxes per image. The models
are trained on COCO train+val excluding 8k minival images
2The emulation quality is measured by averaging the per-attribute
cross-entropy over all attributes.
Table 12. Face attribute classiﬁcation using the MobileNet architecture. Each row corresponds to a different hyper-parameter setting (width multiplier α and image resolution).
Width Multiplier /
Resolution
Mult-Adds Parameters
1.0 MobileNet-224
0.5 MobileNet-224
0.25 MobileNet-224 87.2%
1.0 MobileNet-128
0.5 MobileNet-128
0.25 MobileNet-128 86.4%
Table 13. COCO object detection results comparison using different frameworks and network architectures. mAP is reported with
COCO primary challenge metric (AP at IoU=0.50:0.05:0.95)
Resolution
Mult-Adds Parameters
deeplab-VGG 21.1%
Inception V2 22.0%
Faster-RCNN
Inception V2 15.4%
Faster-RCNN
Inception V2 21.9%
Figure 6. Example objection detection results using MobileNet
and evaluated on minival. For both frameworks, MobileNet
achieves comparable results to other networks with only a
fraction of computational complexity and model size.
4.7. Face Embeddings
The FaceNet model is a state of the art face recognition
model . It builds face embeddings based on the triplet
loss. To build a mobile FaceNet model we use distillation
to train by minimizing the squared differences of the output
Table 14. MobileNet Distilled from FaceNet
Parameters
FaceNet 
1.0 MobileNet-160
1.0 MobileNet-128
0.75 MobileNet-128
0.75 MobileNet-128
of FaceNet and MobileNet on the training data. Results for
very small MobileNet models can be found in table 14.
5. Conclusion
We proposed a new model architecture called MobileNets based on depthwise separable convolutions. We
investigated some of the important design decisions leading
to an efﬁcient model. We then demonstrated how to build
smaller and faster MobileNets using width multiplier and
resolution multiplier by trading off a reasonable amount of
accuracy to reduce size and latency. We then compared different MobileNets to popular models demonstrating superior size, speed and accuracy characteristics. We concluded
by demonstrating MobileNet’s effectiveness when applied
to a wide variety of tasks. As a next step to help adoption
and exploration of MobileNets, we plan on releasing models in Tensor Flow.