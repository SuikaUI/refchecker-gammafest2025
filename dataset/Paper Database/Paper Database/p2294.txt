c⃝2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to
servers or lists, or reuse of any copyrighted component of this work in other works.
J¨org Wagner, Jan Mathias K¨ohler, Tobias Gindele, Leon Hetzel, Jakob Thadd¨aus Wiedemer, Sven Behnke; The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019, pp. 9097-9107; the ﬁnal, published version of this paper is available on IEEE Xplore.
Interpretable and Fine-Grained Visual Explanations for
Convolutional Neural Networks
J¨org Wagner1,2
Jan Mathias K¨ohler1
Tobias Gindele1,∗
Leon Hetzel1,∗
Jakob Thadd¨aus Wiedemer1,∗
Sven Behnke2
1Bosch Center for Artiﬁcial Intelligence (BCAI), Germany
2University of Bonn, Germany
 ; 
To verify and validate networks, it is essential to gain
insight into their decisions, limitations as well as possible
shortcomings of training data. In this work, we propose
a post-hoc, optimization based visual explanation method,
which highlights the evidence in the input image for a speciﬁc prediction. Our approach is based on a novel technique
to defend against adversarial evidence (i.e. faulty evidence
due to artefacts) by ﬁltering gradients during optimization.
The defense does not depend on human-tuned parameters.
It enables explanations which are both ﬁne-grained and
preserve the characteristics of images, such as edges and
colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid
model inputs. We qualitatively and quantitatively evaluate
our approach on a multitude of models and datasets.
1. Introduction
Convolutional Neural Networks (CNNs) have proven
to produce state-of-the-art results on a multitude of vision benchmarks, such as ImageNet , Caltech or
Cityscapes which led to CNNs being used in numerous
real-world systems (e.g. autonomous vehicles) and services
(e.g. translation services).
Though, the use of CNNs in
safety-critical domains presents engineers with challenges
resulting from their black-box character. A better understanding of the inner workings of a model provides hints for
improving it, understanding failure cases and it may reveal
shortcomings of the training data. Additionally, users generally trust a model more when they understand its decision
process and are able to anticipate or verify outputs .
To overcome the interpretation and transparency disadvantage of black-box models, post-hoc explanation meth-
∗contributed while working at BCAI. We additionally thank Volker
Fischer, Michael Herman, Anna Khoreva for discussions and feedback.
Figure 1: Fine-grained explanations computed by removing irrelevant pixels. a) Input image with softmax score
p(cml) of the most-likely class. Our method tries to ﬁnd
a sparse mask (c) with irrelevant pixels set to zero. The resulting explanation (b), i.e.: ’image × mask’, is optimized
in the image space and, thus, can directly be used as model
input. The parameter λ is optimized to produce an explanation with a softmax score comparable to the image.
ods have been introduced . These
methods provide explanations for individual predictions and
thus help to understand on which evidence a model bases its
decisions. The most common form of explanations are visual, image-like representations, which depict the important
pixels or image regions in a human interpretable manner.
In general, an explanation should be easily interpretable
(Sec. 4.1).
Additionally, a visual explanation should be
class discriminative and ﬁne-grained (Sec. 4.2). The
latter property is particularly important for classiﬁcation
tasks in the medical domain, where ﬁne structures
(e.g. capillary hemorrhages) have a major inﬂuence on the
classiﬁcation result (Sec. 5.2). Besides, the importance of
different color channels should be captured, e.g. to uncover
 
a color bias in the training data (Sec. 4.3).
Moreover, explanations should be faithful, meaning they
accurately explain the function of the black-box model .
To evaluate the faithfulness (Sec. 5.1), recent work introduce metrics which are based on model predictions
of explanations. To be able to compute such metrics without
having to rely on proxy measures , it is beneﬁcial to
employ explanation methods which directly generate valid
model inputs (e.g. a perturbed version of the image).
A major concern of optimization based visual explanation methods is adversarial evidence, i.e. faulty evidence
generated by artefacts introduced in the computation of the
explanation. Therefore, additional constraints or regularizations are used to prevent such faulty evidence .
A drawback of these defenses are added hyperparameters
and the necessity of either a reduced resolution of the explanation or a smoothed explanation (Sec. 3.2), thus, they
are not well suited for displaying ﬁne-grained evidence.
Our main contribution is a new adversarial defense technique which selectively ﬁlters gradients in the optimization which would lead to adversarial evidence otherwise
(Sec. 3.2). Using this defense, we extend the work of 
and propose a new ﬁne-grained visual explanation method
(FGVis). The proposed defense is not dependend on hyperparameters and is the key to produce ﬁne-grained explanations (Fig. 1) as no smoothing or regularizations are necessary. Like other optimization-based approaches, FGVis
computes a perturbed version of the original image, in
which either all irrelevant or the most relevant pixels are removed. The resulting explanations (Fig 1 b) are valid model
inputs and their faithfulness can, thus, be directly veriﬁed
(as in methods from ). Moreover, they are additionally ﬁne-grained (as in methods from ).
To the best of our knowledge, this is the ﬁrst method to be
able to produce ﬁne-grained explanations directly in the image space. We evaluate our defense (Sec. 3.2) and FGVis
(Sec. 4 and 5) qualitatively and quantitatively.
2. Related Work
Various methods to create explanations have been introduced. Thang et al. and DU et al. provide a survey
of these. In this section, we give an overview of explanation
methods which generate visual, image-like explanations.
Backpropagation Based Methods (BBM). These methods
generate an importance measure for each pixel by backpropagating an error signal to the image.
Simonyan et
al. , which build on work of Baehrens et al. , use
the derivative of a class score with respect to the image
as an importance measure. Similar methods have been introduced in Zeiler et al. and Springenberg et al. ,
which additionally manipulate the gradient when backpropagating through ReLU nonlinearities.
Integrated Gradients additionally accumulates gradients along a path
from a base image to the input image. SmoothGrad 
and VarGrad visually sharpen explanations by combining multiple explanations of noisy copies of the image.
Other BBMs such as Layer-wise Relevance Propagation , DeepLift or Excitation Backprop 
utilize top-down relevancy propagation rules. BBMs are
usually fast to compute and produce ﬁne-grained importance/relevancy maps. However, these maps are generally
of low quality and are less interpretable. To verify
their faithfulness it is necessary to apply proxy measures or
use pre-processing steps, which may falsify the result.
Activation Based Methods (ABM). These approaches use
a linear combination of activations from convolutional layers to form an explanation. Prominent methods of this category are CAM (Class Activation Mapping) and its generalizations Grad-CAM and Grad-CAM++ . These
methods mainly differ in how they calculate the weights of
the linear combination and what restrictions they impose on
the CNN. Extensions of such approaches have been proposed in Selvaraju et al. and Du et al. , which
combine ABMs with backpropagation or perturbation based
approaches. ABMs generate easy to interpret heat-maps
which can be overlaid on the image. However, they are generally not well suited to visualize ﬁne-grained evidence or
color dependencies. Additionally, it is not guaranteed that
the resulting explanations are faithful and reﬂect the decision making process of the model .
Perturbation Based Methods (PBM). Such approaches
perturb the input and monitor the prediction of the model.
Zeiler et al. slide a grey square over the image and
use the change in class probability as a measure of importance. Several approaches are based on this idea, but
use other importance measures or occlusion strategies. Petsiuk et al. use randomly sampled occlusion masks and
deﬁne importance based on the expected model score over
masks. LIME uses a super-pixel based occlusion strategy and a surrogate model to compute importance scores.
Further super-pixel or segment based methods are introduced in Seo et al. and Zhou et al. . The so far
mentioned approaches do not need access to the internal
state or structure of the model. Though, they are often quite
time consuming and only generate coarse explanations.
Other PBMs generate an explanation by optimizing for
a perturbed version of the image . The perturbed image e is deﬁned by e = m·x+(1−m)·r, where
m is a mask, x the input image, and r a reference image
containing little information (Sec. 3.1).
To avoid adversarial evidence, these approaches need additional regularizations , constrain the explanation (e.g. optimize for a
coarse mask ), introduce stochasticity , or utilize regularizing surrogate models . These approaches
generate easy to interpret explanations in the image space,
which are valid model inputs and faithful (i.e. a faithfulness
measure is incorporated in the optimization).
Our method also optimizes for a perturbed version of the
input. Compared to existing approaches we propose a new
adversarial defense technique which ﬁlters gradients during
optimization. This defense does not need hyperparameters
which have to be ﬁne-tuned. Besides, we optimize each
pixel individually, thus, the resulting explanations have no
limitations on the resolution and are ﬁne-grained.
3. Explaining Model Predictions
Explanations provide insights into the decision-making
process of a model.
The most universal form of explanations are global ones which characterize the overall
model behavior. Global explanations specify for all possible model inputs the corresponding output in an intuitive manner. A decision boundary plot of a classiﬁer in
a low-dimensional vector space, for example, represents a
global explanation. For high-dimensional data and complex models, it is practically impossible to generate such
explanations. Current approaches therefore utilize local explanations1, which focus on individual inputs. Given one
data point, these methods highlight the evidence on which
a model bases its decisions.
As outlined in Sec. 2, the
deﬁnition of highlighting depends on the used explanation
method. In this work, we follow the paradigm introduced in
 and directly optimize for a perturbed version of the input image. Such an approach has several advantages: 1) The
resulting explanations are interpretable due to their imagelike nature; 2) Explanations represent valid model inputs
and are thus testable; 3) Explanations are optimized to be
faithful. In Sec. 3.1 we brieﬂy review the general paradigm
of optimization based explanation methods before we introduce our novel adversarial defense technique in Sec. 3.2.
3.1. Perturbation based Visual Explanations
Following the paradigm of optimization based explanation methods, which compute a perturbed version of the image , an explanation can be deﬁned as:
Explanation by Preservation: The smallest region of the
image which must be retained to preserve the original model
output (i.e. minimal sufﬁcient evidence).
Explanation by Deletion: The smallest region of the image
which must be deleted to change the model output.
To formally derive an explanation method based on this
paradigm, we assume that a CNN fcnn is given which
maps an input image x ∈R3×H×W to an output yx =
fcnn(x; θcnn). The ouput yx ∈RC is a vector representing
the softmax scores yc
x of the different classes c. Given an
input image x, an explanation e∗
cT of a target class cT (e.g.
the most-likely class cT = cml) is computed by removing either relevant (deletion) or irrelevant, not supporting
1For the sake of brevity, we will use the term explanations as a synonym for local explanations throughout this work.
cT , information (preservation) from the image. Since it is
not possible to remove information without replacing it, and
we do not have access to the image generating process, we
have to use an approximate removal operator . A common approach is to use a mask based operator Φ, which
computes a weighted average between the image x and a
reference image r, using a mask mcT ∈ 3×H×W :
ecT = Φ(x, mcT ) = x · mcT + (1 −mcT ) · r.
Common choices for the reference image are constant
values (e.g. zero), a blurred version of the original image, Gaussian noise, or sampled references of a generative
model . In this work, we take a zero image as reference. In our opinion, this reference produces
the most pleasing visual explanations, since irrelevant image areas are set to zero2 (Fig. 1) and not replaced by other
structures. In addition, the zero image (and random image)
carry comparatively little information and lead to a model
prediction with a high entropy. Other references, such as a
blurred version of the image, usually result in lower prediction entropies, as shown in Sec. A3.1. Due to the additional
computational effort, we have not considered model-based
references as proposed in Chang et al. .
In addition, a similarity metric ϕ(ycT
e ) is needed,
which measures the consistency of the model output generated by the explanation ycT
and the output of the image
with respect to a target class cT . This similarity metric should be small if the explanation preserves the output
of the target class and large if the explanation manages to
signiﬁcantly drop the probability of the target class .
Typical choices for the metric are the cross-entropy with
the class cT as a hard target or the negative softmax
score of the target class cT . The similarity metric ensures
that the explanation remains faithful to the model and thus
accurately explains the function of the model, this property
is a major advantage of PBMs.
Using the mask based deﬁnition of an explanation with
a zero image as reference (r = 0) as well as the similarity
metric, a preserving explanation can be computed by:
cT = arg min
e ) + λ · ∥mcT ∥1}.
We will refer to the optimization in Eq. 2 as the preservation game. Masks (Fig. 2 / b2)3 generated by this game are
sparse (i.e. many pixels are zero / appear black; enforced by
minimizing ∥mcT ∥1) and only contain large values at most
important pixels. The corresponding explanation is computed by multiplying the mask with the image (Fig. 2 / c2).
2Tensors x, e, r are assumed to be normalized according to the training of the CNN. A value of zero for these thus corresponds to a grey color
(i.e. the color of the data mean).
3Fig. 2 / b2: Figure 2, column b, 2nd row
Figure 2: Visualization types calculated for VGG using deletion / preservation game. For the repression / generation game the
same characteristics hold. Subscript cT ommited to ease readability. a) Input image. b) Mask obtained by the optimization.
Colors in a deletion mask are complementary to the image colors. c) Explanation directly obtained by the optimization.
d) Complementary mask with a true-color representation for the deletion game. e) Explanation highlighting the important
evidence for the deletion game. f) Mean mask: mask / comp. mask averaged over colors. — To underline important evidence,
we use e for the explanation of the preservation / generation game and ˜e for the deletion / repression game.
Alternatively, we can compute a deleting explanation using:
cT = arg max
e ) + λ · ∥mcT ∥1}.
This optimization will be called deletion game henceforward. Masks (Fig. 2 / b1) generated by this game contain
mainly ones (i.e. appear white; enforced by maximizing
∥mcT ∥1 in Eq. 3) and only small entries at pixels, which
provide the most prominent evidence for the target class.
The colors in a mask of the deletion game are complementary to the image colors. To obtain a true-color representation analogous to the preservation game, one can alternatively visualize the complementary mask (Fig. 2 / d1):
cT = (1 −m∗
cT ). A resulting explanation of the deletion
game, as deﬁned in Eq. 3, is visualized in Fig. 2 / c1. This
explanation is visually very similar to the original image as
only a few pixels need to be deleted to change the model
output. In the remaining of the paper for better visualization, we depict a modiﬁed version of the explanation for the
deletion game: ˜e∗
cT = x · (1 −m∗
cT ). This explanation has
the same properties as the one of the preservation game, i.e.
it only highlights the important evidence. We observe that
the deletion game generally produces sparser explanations
compared to the preservation game, as less pixels have to
be removed to delete evidence for a class than to maintain
evidence by preserving pixels.
To solve the optimization in Eq. 2 and Eq. 3, we utilize Stochastic Gradient Descent and start with an explanation e0
cT = 1 · x identical to the original image (i.e. a
mask initialized with ones). As an alternative initialization
of the masks, we additionally explore a zero initialization
cT = 0. In this setting the initial explanation contains
no evidence towards any class and the optimization iteratively has to add relevant (generation game) or irrelevant,
not supporting the class cT , information (repression game).
The visualizations of the generation game are equivalent to
those of the preservation game, the same holds for the deletion and repression game. In our experiments the deletion
game produces the most ﬁne-grained and visually pleasing
explanations. Compared to the other games it usually needs
the least amount of optimization iterations since we start
cT = 1 and comparatively few mask values have to
be changed to delete the evidence for the target class. A
comparison and additional characteristics of the four optimization settings (i.e. games) are included in Sec. A3.5.
3.2. Defending against Adversarial Evidence
CNNs have been proven susceptible to adversarial images , i.e. a perturbed version of a correctly
classiﬁed image crafted to fool a CNN. Due to the computational similarity of adversarial methods and optimization based visual explanation approaches, adversarial noise
is also a concern for the latter methods and one has to ensure that an explanation is based on true evidence present
in the image and not on false adversarial evidence introduced during optimization. This is particularly true for the
generation/repression game as their optimization start with
cT = 0 and iteratively adds information.
 and showed the vulnerability of optimization
based explanation methods to adversarial noise. To avoid
adversarial evidence, explanation methods use stochastic
operations , additional regularizations , optimize on a low-resolution mask with upsampling of the computed mask , or utilize a regularizing surrogate
Figure 3: Explanations computed for the adversarial class
limousine and the predicted class agama using the generation game and VGG16 with and without our adversarial defense. An adversarial for class limousine can only be computed without the defense. d) Mean mask enhanced by a
factor of 7 to show small adversarial structures.
model . In general, these operations impede the generation of adversarial noise by obscuring the gradient direction in which the model is susceptible to false evidence, or
by constraining the search space for potential adversarials.
These techniques help to reduce adversarial evidence, but
also introduce new drawbacks: 1) Defense capabilities usually depend on human-tuned parameters; 2) Explanations
are limited to being low resolution and/or smooth, which
prevents ﬁne-grained evidence from being visualized.
A novel Adversarial Defense. To overcome these drawbacks, we propose a novel adversarial defense which ﬁlters
gradients during backpropagation in a targeted way. The
basic idea of our approach is: A neuron within a CNN is
only allowed to be activated by the explanation ecT if the
same neuron was also activated by the original image x.
If we regard neurons as indicators for the existence of features (e.g. edges, object parts, ...), the proposed constraint
enforces that the explanation ecT can only contain features
which exist at the same location in the original image x. By
ensuring that the allowed features in ecT are a subset of the
features in x it prevents the generation of new evidence.
This defense technique can be integrated in the introduced explanation methods via an optimization constraint:
i(ecT ) ≤hl
i(ecT ) ≥hl
otherwise,
i is the activation of the i-th neuron in the l-th layer
of the network after the nonlinearity. For brevity, the index i references one speciﬁc feature at one spatial position
in the activation map. This constraint is applied after all
nonlinearity-layers (e.g. ReLU-Layers) of the network, besides the ﬁnal classiﬁcation layer. It ensures that the absolute value of activations can only be reduced towards values representing lower information content (we assume that
zero activations have the lowest information as commonly
applied in network pruning ). To solve the optimization with subject to Eq. 4, one could incorporate the constraints via a penalty function in the optimization loss. The
drawback is one additional hyperparameter. Alternatively,
one could add an additional layer ¯hl
i after each nonlinearity
which ensures the validity of Eq. 4:
i(ecT ) = min(bu, max(bl, hl
i(ecT ))),
bu = max(0, hl
bl = min(0, hl
i(ecT ) is the actual activation of the original
nonlinearity-layer and ¯hl
i(ecT ) the adjusted activation after ensuring the bounds bu, bl of the original input. For
instance, for a ReLU nonlinearity, the upper bound bu is
equal to hl
i(x) and the lower bound bl is zero. We are not
applying this method as it changes the architecture of the
model which we try to explain. Instead, we clip gradients
in the backward pass of the optimization, which lead to a
violation of Eq. 4. This is equivalent to adding an additional clipping-layer after each nonlinearity which acts as
the identity in the forward pass and uses the gradient update of Eq. 5 in the backward pass. When backpropagating
an error-signal ¯γl
i through the clipping-layer, the gradient
update rule for the resulting error γl
i is deﬁned by:
i(ecT ) ≤bu] · [hl
i(ecT ) ≥bl],
where [ · ] is the indicator function and bl, bu the bounds
computed in Eq. 5. This clipping only affects the gradients of the similarity metric ϕ(· , ·) which are propagated
through the network. The proposed gradient clipping does
not add hyperparameters and keeps the original structure
of the model during the forward pass. Compared to other
adversarial defense techniques ( , , ), it imposes
no constraint on the explanation (e.g. resolution/smoothness
constraints), enabling ﬁne-grained explanations.
Validating the Adversarial Defense. To evaluate the
performance of our defense, we compute an explanation for
a class cA for which there is no evidence in the image (i.e.
it is visually not present).
We approximate cA with the
least-likely class cll considering only images which yield
very high predictive conﬁdence for the true class p(ctrue) ≥
0.995. Using cll as the target class, the resulting explanation
method without defense is similar to an adversarial attack
(the Iterative Least-Likely Class Method ).
A correct explanation for the adversarial class cA should
be “empty” (i.e. grey), as seen in Fig. 3 b, top row, when
using our adversarial defense. If, on the other hand, the
explanation method is susceptible to adversarial noise, the
optimization procedure should be able to perfectly generate
an explanation for any class. This behavior can be seen in
Fig. 3 c, top row. The shown explanation for the adversarial
No Defense
VGG16 
AlexNet 
ResNet50 
GoogleNet 
Table 1: Ratio how often an adversarial class cA was generated, using the generation game with no sparsity loss on
VGG16 with and without our defense.
class (cA: limousine) contains primarily artiﬁcial structures
and is classiﬁed with a probability of 1 as limousine.
We also depict the explanation of the predicted class
(cpred: agama). The explanation with our defense results
in a meaningful representation of the agama (Fig. 3 b, bottom row); without defense (Fig. 3 c / d, bottom row) it is
much more sparse. As there is no constraint to change pixel
values arbitrarily, we assume the algorithm introduces additional structures to produce a sparse explanation.
A quantitative evaluation of the proposed defense is reported in Tab. 1. We generate explanations for 1000 random ImageNet validation images and use a class cA as the
explanation target4. To ease the generation of adversarial
examples, we set the sparsity loss to zero and only use the
similarity metric which tries to maximize the probability of
the target class cA. Without an employed defense technique,
the optimization is able to generate an adversarial explanation for 100% of the images. Applying our defense (Eq. 6),
the optimization nearly never was able to do so. The two
adverarial examples generated in VGG16 have a low conﬁdence, so we assume that there has been some evidence for
the chosen class cA in the image. Our proposed technique
is thus well suited to defend against adversarial evidence.
4. Qualitative Results
Implementation details are stated in Sec. A2.
4.1. Interpretability
Comparison of methods. Using the deletion game we
compute mean explanation masks for GoogleNet and compare these in Fig. 5 with state-of-the-art methods.
method delivers the most ﬁne-grained explanation by deleting important pixels of the target object. Especially explanations b), f), and g) are coarser and, therefore, tend to include background information not necessary to be deleted
to change the original prediction. The majority of pixels
highlighted by FGVis form edges of the object. This cannot
be seen in other methods. The explanations from c) and d)
are most similar to ours. However, our masks are computed
to directly produce explanations which are viable network
4For cA we used the least-likely class, as described before. We use the
second least-likely class, if the least-likely class coincidentally matches the
predicted class for the zero image.
inputs and are, therefore, veriﬁable — The deletion of the
highlighted pixels prevents the model from correctly predicting the object. This statement does not necessarily hold
for explanations calculated with methods c) and d).
Architectural insights. As ﬁrst noted in explanations using backpropagation based approaches show a gridlike pattern for ResNet. In general, demonstrate that
the network structure inﬂuences the visualization and assume that for ResNet the skip connections play an important role in their explanation behavior. As shown in Fig 6
this pattern is also visible in our explanations to an even
ﬁner degree. Interestingly, the grid pattern is also visible to
a lesser extent outside the object. A detailed investigation
of this phenomenon is left for future research. See A3.4 for
a comparison of explanations between models.
4.2. Class Discriminative / Fine-Grained
Visual explanation methods should be able to produce
class discriminative (i.e. focus on one object) and ﬁnegrained explanations . To test FGVis with respect to
these properties, we generate explanations for images containing two objects. The objects are chosen from highly
different categories to ensure little overlapping evidence. In
Fig. 4, we visualize explanations of three such images, computed using the deletion game and GoogleNet. Additional
results can be found in Sec. A3.2.
FGVis is able to generate class discriminative explanations and only highlights pixels of the chosen target class.
Even partially overlapping objects, as the elkhound and ball
in Fig. 4, ﬁrst row, or the bridge and schooner in Fig. 4,
Figure 4: Explanation masks for images with multiple objects computed using the deletion game and GoogleNet.
FGVis produces class discriminating explanations, even
when objects partially overlap. Additionally, FGVis is able
to visualize ﬁne-grained details down to the pixel level.
Figure 5: Comparison of mean explanation masks: a) Image, b) BBMP , c) Gradient , d) Guided Backprop ,
e) Contrastive Excitation Backprop , f) Grad-CAM , g) Occlusion , h) FGVis (ours). The masks of all reference
methods are based on work by . Due to our detailed and sparse masks, we plot them in a larger size.
Figure 6: Visual explanations computed using the deletion
game for ResNet50. The masks (b, d) show a grid-like pattern, as also observed in for ResNet50.
third row, are correctly discriminated. One major advantage
of FGVis is its ability to visualize ﬁne-grained details. This
property is especially visible in Fig 4, second row, which
shows an explanation for the target class fence. Despite the
ﬁne structure of the fence, FGVis is able to compute a precise explanation which mainly contains fence pixels.
4.3. Investigating Biases of Training Data
An application of explanation methods is to identify a
bias in the training data. Especially for safety-critical, highrisk domains (e.g. autonomous driving), such a bias can lead
to failures if the model does not generalize to the real world.
Learned objects. One common bias is the coexistence
of objects in images which can be depicted using FGVis. In
Sec. A3.3, we describe such a bias in ImageNet for sports
equipment appearing in combination with players.
Learned color. Objects are often biased towards speciﬁc colors. FGVis can give a ﬁrst visual indication for the
importance of different color channels. We investigate if a
VGG16 model trained on ImageNet shows such a bias using the preservation game. We focus on images of school
buses and minivans and compare explanations (Fig. 7; all
correctly predicted images in Fig. A6 and A8). Explanations of minivans focus on edges, not consistently preserving the color compared to school buses with yellow dominating those explanations. This is a ﬁrst indication for the
importance of color for the prediction of school buses.
To verify the qualitative ﬁnding, we quantitatively give
an estimation of the color bias. As an evaluation we swap
each of the three color channels BGR to either RBG or GRB
and calculate the ratio of maintained true classiﬁcations on
the validation data after the swap. For minivans 83.3% (averaged over RBG and GRB) of the 21 correctly classiﬁed images keep their class label, for school buses it is only 8.3%
of 42 images. For 80 ImageNet classes at least 75% of images are no longer truly classiﬁed after the color swap. We
show the results for the most and least affected 19 classes
and minivan / school bus in Tab. A3.
To the best of our knowledge, FGVis is the ﬁrst method used
to highlight color channel importance.
5. Quantitative Results
5.1. Faithfulness of Explanations
The faithfulness of generated visual explanations to the
underlying neural network is an important property of explanation methods .
To quantitatively compare the
faithfulness of methods, Petsiuk et al. proposed causal
metrics which do not depend on human labels. These metrics are not biased towards human perception and are thus
well suited to verify if an explanation correctly represents
the evidence on which a model bases its prediction.
We use the deletion metric to evaluate the faith-
Figure 7: Explanations computed using the preservation
game for VGG16. Explanations of the class minivan focus
on edges, hardly preserving the color, compared to the class
school bus, with yellow dominating the explanations.
fulnes of explanations generated by our method. This metric measures how the removal of evidence effects the prediction of the used model. The metric assumes that an importance map is given, which ranks all image pixels with
respect to their evidence for the predicted class cml. By iteratively removing important pixels from the input image and
measuring the resulting probability of the class cml a deletion curve can be generated, whose area under the curve
AUC is used as a measure of faithfulness (Sec. A4.1).
In Tab. 2, we report the deletion metric of FGVis, computed on the validation split of ImageNet using different
models. We use the deletion game to generate masks mml,
which determine the importance of each pixel. A detailed
description of the experiment settings as well as additional
ﬁgures, can be found in Sec. A4.1. FGVis outperforms the
other explanation methods on both models by a large margin. This performance increase can be attributed to the ability of FGVis to visualize ﬁne-grained evidence. All other
approaches are limited to coarse explanations, either due
to computational constraints or due to the used measures
to avoid adversarial evidence. The difference between the
two model architectures can most likely be attributed to the
superior performance of ResNet50, resulting in on average
higher softmax scores over all validation images.
Grad-Cam 
Sliding Window 
FGVis (ours)
Table 2: Deletion metric computed on the ImageNet validation dataset (lower is better). The results for all reference
methods were taken from Petsiuk et al. .
5.2. Visual explanation for medical images
We evaluate FGVis on a real-world use case to identify
regions in eye fundus images which lead a CNN to classify
the image as being affected with referable diabetic retinopathy (RDR). Using the deletion game we derive a weaklysupervised approach to detect RDR lesions. The setup, used
network, as well as details on the disease and training data
are described in A4.2. To evaluate FGVis, the DiaretDB1
dataset is used containing 89 fundus images with different lesion types, ground truth marked by four experts. To
quantitatively judge the performance, we compare in Tab. 3
the image level sensitivity of detecting if a certain lesion
type is present in an image. The methods 
use supervised approaches on image level without reporting
a localization. propose an unsupervised approach to
extract salient regions. use a comparable setting to ours
applying CAM in a weakly-supervised way to highlight important regions. To decide if a lesion is detected,
 suggest an overlap of 50% between proposed regions
and ground truth. As our explanation masks are ﬁne-grained
and the ground truth is coarse, we compare using a 25%
overlap and for completeness report a 50% overlap.
It is remarkable that FGVis performs comparable or outperforms fully supervised approaches which are designed
to detect the presence of one lesion type. The strength of
FGVis is especially visible in detecting RSD, as these small
lesions only cover some pixels in the image. In Fig. A21 we
show fundus images, ground truth and our predictions.
Zhou et al. 
Liu et al. 
Haloi et al. 
Mane et al. 
Zhao et al. 
Gondal et al. 
Ours (25% Overlap)
Ours (50% Overlap)
Table 3: Image level sensitivity in % (higher is better) for
four different lesions H, HE, SE, RSD: Hemorrhages, Hard
Exudates, Soft Exudates and Red Small Dots.
6. Conclusion
We propose a method which generates ﬁne-grained visual explanations in the image space using on a novel technique to defend adversarial evidence. Our defense does not
introduce hyperparameters. We show the effectivity of the
defense on different models, compare our explanations to
other methods, and quantitatively evaluate the faithfulness.
Moreover, we underline the strength in producing class discriminative visualizations and point to characteristics in explanations of a ResNet50. Due to the ﬁne-grained nature of
our explanations, we achieve remarkable results on a medical dataset. Besides, we show the usability of our approach
to visually indicate a color bias in training data.