HAL Id: hal-00869417
 
Submitted on 9 Jan 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin√©e au d√©p√¥t et √† la diffusion de documents
scientifiques de niveau recherche, publi√©s ou non,
√©manant des √©tablissements d‚Äôenseignement et de
recherche fran√ßais ou √©trangers, des laboratoires
publics ou priv√©s.
Unsupervised Visual Domain Adaptation Using
Subspace Alignment
Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars
To cite this version:
Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars. Unsupervised Visual Domain
Adaptation Using Subspace Alignment.
ICCV 2013, Dec 2013, Sydney, Australia.
pp.2960-2967.
Ôøøhal-00869417Ôøø
Unsupervised Visual Domain Adaptation Using Subspace Alignment
Basura Fernando1, Amaury Habrard2, Marc Sebban2, and Tinne Tuytelaars1
1KU Leuven, ESAT-PSI, iMinds, Belgium
2Laboratoire Hubert Curien UMR 5516, 18 rue Benoit Lauras, 42000 St-Etienne, France
In this paper, we introduce a new domain adaptation
(DA) algorithm where the source and target domains are
represented by subspaces described by eigenvectors. In this
context, our method seeks a domain adaptation solution by
learning a mapping function which aligns the source subspace with the target one. We show that the solution of the
corresponding optimization problem can be obtained in a
simple closed form, leading to an extremely fast algorithm.
We use a theoretical result to tune the unique hyperparameter corresponding to the size of the subspaces. We run our
method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.
1. Introduction
In classiÔ¨Åcation, it is typically assumed that the labeled
training data comes from the same distribution as that of the
test data. However, many real world applications, especially
in computer vision, challenge this assumption (see, e.g., the
study on dataset bias in ). In this context, the learner
must take special care during the learning process to infer
models that adapt well to the test data they are deployed
on. For example, images collected from a web camera are
different from those taken with a DSLR camera. A classi-
Ô¨Åer that would be trained on the former would likely fail to
classify the latter correctly if applied without adaptation.
We refer to these different but related marginal distributions as domains. In order to build robust classiÔ¨Åers, it is
necessary to take into account the shift between these two
distributions. This issue is known as domain adaptation
(DA). DA typically aims at making use of information coming from both source and target domains during the learning
process to adapt automatically. We usually differentiate two
different scenarios: (1) the unsupervised setting where the
training data consists of labeled source data and unlabeled
target examples (see for a survey); and (2) the semisupervised case where a large number of labels is available
for the source domain and only a few labels are provided
for the target domain. In this paper, we focus on the most
difÔ¨Åcult, unsupervised scenario.
As illustrated by recent results , subspace based domain adaptation seems a promising approach to tackle unsupervised visual DA problems. In , Gopalan et al. generate intermediate representations in the form of subspaces
along the geodesic path connecting the source subspace and
the target subspace on the Grassmann manifold. Then, the
source data are projected onto these subspaces and a classi-
Ô¨Åer is learned. In , Gong et al. propose a geodesic Ô¨Çow
kernel which aims to model incremental changes between
the source and target domains. In both papers, a set of intermediate subspaces is used to model the shift between the
two distributions.
In this paper, we also make use of subspaces (composed
of ùëëeigenvectors induced by a PCA), one for each domain.
However, following the theoretical recommendations of ,
we rather suggest to directly reduce the discrepancy between the two domains by moving closer the source and
target subspaces. This is achieved by optimizing a mapping
function that transforms the source subspace into the target
one. From this simple idea, we design a new DA approach
based on subspace alignment. The advantage of our method
is two-fold: (1) by adapting the bases of the subspaces, our
approach is global. This allows us to induce robust classi-
Ô¨Åers not subject to local perturbations; and (2) by aligning
the source and target subspaces, our method is intrinsically
regularized: we do not need to tune regularization parameters in the objective as imposed by a lot of optimizationbased DA methods.
Our subspace alignment is achieved by optimizing a
mapping function which takes the form of a transformation
matrix ùëÄ. We show that the optimal solution corresponds
in fact to the covariance matrix between the source and target eigenvectors. From this transformation matrix, we derive a similarity function ùëÜùëñùëö(yS, yT) to compare a source
data yS with a target example yT. Thanks to a consistency
theorem, we prove that ùëÜùëñùëö(yS, yT), which captures the
idiosyncrasies of the training data, converges uniformly to
its true value. We show that we can make use of this theoretical result to tune the hyperparameter ùëë, that tends to
make our method parameter-free. The similarity function
ùëÜùëñùëö(yS, yT) can be used directly in a nearest neighbour
classiÔ¨Åer. Alternatively, we can also learn a global classiÔ¨Åer
such as support vector machines (SVM) on the source data
after mapping them onto the target subspace.
As suggested by Ben-David et al. , a reduction of the
divergence between the two domains is required to adapt
In other words, the ability of a DA algorithm to
actually reduce that discrepancy is a good indication of
its performance. A usual way to estimate the divergence
consists in learning a linear classiÔ¨Åer ‚Ñéto discriminate
between source and target instances, respectively pseudolabeled with 0 and 1. In this context, the higher the error of
‚Ñé, the smaller the divergence. While such a strategy gives
us some insight about the ability for a global learning algorithm (e.g. SVM) to be efÔ¨Åcient on both domains, it does
not seem to be suited to deal with local classiÔ¨Åers, such as
the ùëò-nearest neighbors. To overcome this limitation, we
introduce a new empirical divergence speciÔ¨Åcally dedicated
to local classiÔ¨Åers. We show through our experimental results that our DA method allows us to drastically reduce
both empirical divergences.
The rest of the paper is organized as follows. We present
the related work in section 2. Section 3 is devoted to the presentation of our DA method and the consistency theorem on
the similarity measure deduced from the learned mapping
function. In section 4, a comparative study is performed on
various datasets. We conclude in section 5.
2. Related work
DA has been widely studied in the literature and is of
great importance in many areas such as natural language
processing or computer vision . In this paper, we
focus on the unsupervised domain adaptation setting that is
well suited to vision problems since it does not require any
labeling information from the target domain. This setting
makes the problem very challenging and an important issue
is to Ô¨Ånd out a relationship between the two domains. A
common approach is to assume the existence of a domain
invariant feature space and the objective of a large range of
DA work is to approximate this space.
A classical strategy related to our work consists of learning a new domain-invariant feature representation by looking for a new projection space. PCA based DA methods
have then been naturally investigated in order to
Ô¨Ånd a common latent space where the difference between
the marginal distributions of the two domains is minimized
with respect to the Maximum Mean Discrepancy (MMD)
divergence.
Other strategies have been explored as well
such as using metric learning approaches or canonical correlation analysis methods over different views of the
data to Ô¨Ånd a coupled source-target subspace where one
assumes the existence of a performing linear classiÔ¨Åer on
the two domains.
In the structural correspondence learning method ,
Blitzer et al. propose to create a new feature space by identifying correspondences among features from different domains by modeling their correlations with pivot features.
Then, they concatenate source and target data using this feature representation and apply PCA to Ô¨Ånd a relevant common projection. In , Chang transforms the source data
into an intermediate representation such that each transformed source sample can be linearly reconstructed by the
target samples. This is however a local approach that may
fail to capture the global structure information of the source
domain. Moreover it is sensitive to noise and outliers of
the source domain that have no correspondence in the target
Our method is also related to manifold alignment whose main objective is to align two datasets from
two different manifolds such that they can be projected to
a common subspace. Most of these methods need
correspondences from the manifolds and all of them exploit
the local statistical structure of the data.
Recently, subspace based DA has demonstrated good
performance in visual DA .
These methods share
the same principle: Ô¨Årst they compute a domain speciÔ¨Åc
d-dimensional subspace for the source data and another one
for the target data, independently created by PCA. Then,
they project source and target data into intermediate subspaces along the shortest geodesic path connecting the two
d-dimensional subspaces on the Grassmann manifold. They
actually model the distribution shift by looking for the best
intermediate subspaces. These approaches are the closest to
ours but, as mentioned in the introduction, it is more appropriate to align the two subspaces directly, instead of computing a large number of intermediate subspaces which can
potentially be a costly tuning procedure. The effectiveness
of our idea is supported by our experimental results.
As a summary, our approach has the following differences with existing methods:
We exploit the global covariance statistical structure of
the two domains during the adaptation process in contrast
to the manifold alignment methods that use local statistical
structure of the data . We project the source
data onto the source subspace and the target data onto the
target subspace in contrast to methods that project source
data to the target subspace or target data to the source subspace such as . Moreover, we do not project data to a
large number of subspaces as in . Our method is totally unsupervised and does not require any target label information like constraints on cross-domain data or
correspondences from across datasets . We do not
apply PCA on cross-domain data like in as these
approaches exploit only shared features in both domains. In
contrast, we make use of the correlated features in both domains. Some of these features can be speciÔ¨Åc to one domain
yet correlated to some other features in the other one allowing us to use both shared and domain speciÔ¨Åc features. As
far as we know, this is the Ô¨Årst attempt to use a subspace
alignment method in the context of domain adaptation.
3. DA using unsupervised subspace alignment
In this section, we introduce our new subspace based DA
method. We assume that we have a set ùëÜof labeled data
(resp. a set ùëáof unlabeled data) both lying in a given ùê∑dimensional space and drawn i.i.d. according to a Ô¨Åxed but
unknown source (resp. target) distribution ùíüùëÜ(resp. ùíüùëá).
We denote the transpose operation by ‚Ä≤.
In section 3.1, we explain how to generate the source and
target subspaces of size ùëë. Then, we present our DA method
in section 3.2 which consists in learning a transformation
matrix ùëÄthat maps the source subspace to the target one.
From ùëÄ, we design a similarity function for which we derive a consistency theorem in section 3.3. This upper bound
gives us some insight about how to tune the parameter ùëë.
3.1. Subspace generation
Even though both the source and target data lie in the
same ùê∑-dimensional space, they have been drawn according to different marginal distributions. Consequently, rather
than working on the original data themselves, we suggest
to handle more robust representations of the source and target domains and to learn the shift between these two domains. First, we transform every source and target data in
the form of a ùê∑-dimensional z-normalized vector (i.e. of
zero mean and unit standard deviation). Then, using PCA,
we select for each domain ùëëeigenvectors corresponding to
the ùëëlargest eigenvalues. These eigenvectors are used as
bases of the source and target subspaces, respectively denoted by ùëãùëÜand ùëãùëá(ùëãùëÜ, ùëãùëá‚àà‚Ñùùê∑√óùëë). Note that ùëã‚Ä≤
ùëáare orthonormal (thus, ùëã‚Ä≤
ùëÜùëãùëÜ= ùêºùëëand ùëã‚Ä≤
where ùêºùëëis the identity matrix of size ùëë). In the following, ùëãùëÜand ùëãùëáare used to learn the shift between the two
3.2. Domain adaptation with subspace alignment
As presented in section 2, two main strategies are used in
subspace based DA methods. The Ô¨Årst one consists in projecting both source and target data to a common shared subspace. However, since this only exploits shared features in
both domains, it is not always optimal. The second one aims
to build a (potentially large) set of intermediate representations. Beyond the fact that such a strategy can be costly,
projecting the data to an intermediate common shared subspace may lead to information loss in both source and target
In our method, we suggest to project each source (yS)
and target (yT) data (where yS, yT ‚àà‚Ñù1√óùê∑) to its respective subspace ùëãùëÜand ùëãùëáby the operations ySùëãùëÜand
yTùëãùëá, respectively. Then, we learn a linear transformation function that align the source subspace coordinate system to the target one. This step allows us to directly compare source and target samples in their respective subspaces
without unnecessary data projections. To achieve this task,
we use a subspace alignment approach. We align basis vectors by using a transformation matrix ùëÄfrom ùëãùëÜto ùëãùëá.
ùëÄis learned by minimizing the following Bregman matrix
divergence:
ùêπ(ùëÄ) = ‚à£‚à£ùëãùëÜùëÄ‚àíùëãùëá‚à£‚à£2
ùëÄ‚àó= ùëéùëüùëîùëöùëñùëõùëÄ(ùêπ(ùëÄ))
where ‚à£‚à£.‚à£‚à£2
ùêπis the Frobenius norm. Since ùëãùëÜand ùëãùëá
are generated from the Ô¨Årst ùëëeigenvectors, it turns out that
they tend to be intrinsically regularized. Therefore, we do
not add a regularization term in the equation 1. It is thus
possible to obtain a simple solution of equation 2 in closed
form. Because the Frobenius norm is invariant to orthonormal operations, we can re-write equation 1 as follows:
ùêπ(ùëÄ) = ‚à£‚à£ùëã‚Ä≤
From this result, we can conclude that the optimal ùëÄ‚àó
is obtained as ùëÄ‚àó= ùëã‚Ä≤
ùëÜùëãùëá. This implies that the new
coordinate system is equivalent to ùëãùëé= ùëãùëÜùëã‚Ä≤
call ùëãùëéthe target aligned source coordinate system. It is
worth noting that if the source and target domains are the
same, then ùëãùëÜ= ùëãùëáand ùëÄ‚àóis the identity matrix.
Matrix ùëÄtransforms the source subspace coordinate
system into the target subspace coordinate system by aligning the source basis vectors with the target ones. If a source
basis vector is orthogonal to all target basis vectors, it is ignored. On the other hand, a high weight is given to a source
basis vector that is well aligned with the target basis vectors.
In order to compare a source data yS with a target data
yT, one needs a similarity function ùëÜùëñùëö(yS, yT). Projecting yS and yT in their respective subspace ùëãùëÜand ùëãùëáand
applying the optimal transformation matrix ùëÄ‚àó, we can de-
Ô¨Åne ùëÜùëñùëö(yS, yT) as follows:
ùëÜùëñùëö(yS, yT)
(ySùëãùëÜùëÄ‚àó)(yTùëãùëá)‚Ä≤ = ySùëãùëÜùëÄ‚àóùëã‚Ä≤
where ùê¥= ùëãùëÜùëã‚Ä≤
ùëá. Note that Eq. 4 looks like a
generalized dot product (even though ùê¥is not necessarily
Figure 1. Classifying ImageNet images using Caltech-256 images
as the source domain. In the Ô¨Årst row, we show an ImageNet query
image. In the second row, the nearest neighbour image selected by
our method is shown.
positive semideÔ¨Ånite) where ùê¥encodes the relative contributions of the different components of the vectors in their
original space.
We use ùëÜùëñùëö(yS, yT) directly to perform a ùëò-nearest
neighbor classiÔ¨Åcation task.
On the other hand, since
ùëÜùëñùëö(yS, yT) is not PSD we can not make use of it to learn
a SVM directly. As we will see in the experimental section, an alternative solution will consist in (i) projecting the
source data via ùëãùëéinto the target aligned source subspace
and the target data into the target subspace (using ùëãùëá), (ii)
learn a SVM from this ùëë-dimensional space. The pseudocode of our algorithm is presented in Algorithm 1.
Data: Source data ùëÜ, Target data ùëá, Source labels ùêøùëÜ,
Subspace dimension ùëë
Result: Predicted target labels ùêøùëá
ùëãùëÜ‚ÜêùëÉùê∂ùê¥(ùëÜ, ùëë) ;
ùëãùëá‚ÜêùëÉùê∂ùê¥(ùëá, ùëë) ;
ùêøùëá‚Üêùê∂ùëôùëéùë†ùë†ùëñùëìùëñùëíùëü(ùëÜùëé, ùëáùëá, ùêøùëÜ) ;
Algorithm 1: Subspace alignment DA algorithm
3.3. Consistency theorem on ùëÜùëñùëö(yS, yT)
The unique hyperparameter of our algorithm is the number ùëëof eigenvectors. In this section, inspired by concentration inequalities on eigenvectors , we derive an upper bound on the similarity function ùëÜùëñùëö(yS, yT). Then,
we show that we can make use of this theoretical result to
efÔ¨Åciently tune ùëë.
Let Àúùê∑ùëõbe the covariance matrix of a sample ùê∑of size
ùëõdrawn i.i.d. from a given distribution and Àúùê∑its expected
value over that distribution.
Theorem 1. We start by using a theorem from . Let
ùêµbe s.t. for any vector x, ‚à•x‚à•‚â§ùêµ, let ùëãùëë
the orthogonal projectors of the subspaces spanned by the
Ô¨Årst d eigenvectors of Àúùê∑and Àúùê∑ùëõ. Let ùúÜ1 > ùúÜ2 > ... >
ùúÜùëë> ùúÜùëë+1 ‚â•0 be the Ô¨Årst ùëë+ 1 eigenvalues of Àúùê∑, then for
with probability
at least 1 ‚àíùõøwe have:
‚àöùëõ(ùúÜùëë‚àíùúÜùëë+1)
From the previous theorem, we can derive the following
lemma for the deviation between ùëãùëë
For the sake of simpliÔ¨Åcation, we will use in the following
the same notation ùê∑(resp. ùê∑ùëõ) for deÔ¨Åning either the sample ùê∑(resp. ùê∑ùëõ) or its covariance matrix Àúùê∑(resp. Àúùê∑ùëõ).
Lemma 1. Let ùêµs.t. for any x, ‚à•x‚à•‚â§ùêµ, let ùëãùëë
the orthogonal projectors of the subspaces spanned by the
Ô¨Årst d eigenvectors of ùê∑and ùê∑ùëõ. Let ùúÜ1 > ùúÜ2 > ... >
ùúÜùëë> ùúÜùëë+1 ‚â•0 be the Ô¨Årst ùëë+ 1 eigenvalues of ùê∑, then for
with probability
at least 1 ‚àíùõøwe have:
The last inequality is obtained by the fact that the eigenvectors are normalized and thus ‚à•ùëãùê∑‚à•‚â§
ùëëand application
of Theorem 1 twice.
We now give a theorem for the projector of our DA
Theorem 2. Let ùëãùëë
ùëÜùëõ(resp. ùëãùëë
ùëáùëõ) be the d-dimensional
projection operator built from the source (resp.
sample of size ùëõùëÜ(resp. ùëõùëá) and ùëãùëë
ùëÜ(resp. ùëãùëë
ùëá) its expected value with the associated Ô¨Årst ùëë+ 1 eigenvalues
1 > ... > ùúÜùëÜ
ùëë+1 (resp. ùúÜùëá
1 > ... > ùúÜùëá
then we have with probability at least 1 ‚àíùõø
where ùëÄùëõis the solution of the optimization problem of
Eq 2 using source and target samples of sizes ùëõùëÜand ùëõùëá
respectively, and ùëÄis its expected value.
The Ô¨Årst equality is obtained by replacing ùëÄand
ùëÄùëõby their corresponding optimal solutions ùëãùëë
‚Ä≤ from Eq 3. The last inequality is obtained by
applying twice Lemma 1 and bounding the projection operators.
From Theorem 2, we can deduce a bound on the deviation between two successive eigenvalues. We can make use
of this bound as a cutting rule for automatically determining the size of the subspaces. Let ùëõùëöùëñùëõ= min(ùëõùëÜ, ùëõùëá) and
ùëë+1) = min((ùúÜùëá
ùëë+1)) and let
ùõæ> 0 be a given allowed deviation such that:
‚àöùëõùëöùëñùëõ(ùúÜùëöùëñùëõ
Given a conÔ¨Ådence ùõø> 0 and a Ô¨Åxed deviation ùõæ> 0, we
can select the maximum dimension ùëëùëöùëéùë•such that:
) (16ùëë3/2ùêµ
For each ùëë‚àà{ùëë‚à£1 . . . ùëëùëöùëéùë•}, we then have the guarantee
‚Ä≤‚à•‚â§ùõæ. In other words, as
long as we select a subspace dimension d such that ùëë‚â§
ùëëùëöùëéùë•, the solution ùëÄ‚àóis stable and not over-Ô¨Åtting.
3.4. Divergence between source and target domains
The pioneer work of Ben-David et al. provides a generalization bound on the target error which depends on the
source error and a measure of divergence, called the ùêªŒîùêª
divergence, between the source and target distributions ùíüùëÜ
ùúñùëá(‚Ñé) = ùúñùëÜ(‚Ñé) + ùëëùêªŒîùêª(ùíüùëÜ, ùíüùëá) + ùúÜ,
where ‚Ñéis a learned hypothesis, ùúñùëá(‚Ñé) the generalization
target error, ùúñùëÜ(‚Ñé) the generalization source error, and ùúÜthe
error of the ideal joint hypothesis on ùëÜand ùëá, which is supposed to be a negligible term if the adaptation is possible.
Eq. 6 tells us that to adapt well, one has to learn a hypothesis which works well on ùëÜwhile reducing the divergence
between ùíüùëÜand ùíüùëá. To estimate ùëëùêªŒîùêª(ùíüùëÜ, ùíüùëá), a usual
way consists in learning a linear classiÔ¨Åer ‚Ñéto discriminate
between source and target instances, respectively pseudolabeled with 0 and 1. In this context, the higher the error of
‚Ñé, the smaller the divergence. While such a strategy gives
us some insight about the ability for a global learning algorithm (e.g. SVM) to be efÔ¨Åcient on both domains, it does
not seem to be suited to deal with local classiÔ¨Åers, such as
the ùëò-nearest neighbors. To overcome this limitation, we
introduce a new empirical divergence speciÔ¨Åcally dedicated
to local classiÔ¨Åers. Based on the recommendations of ,
we propose a discrepancy measure to estimate the local density of a target point w.r.t. a given source point. This discrepancy, called Target density around source TDAS counts
how many target points can be found on average within a ùúñ
neighborhood of a source point. More formally:
‚à£{yT‚à£ùëÜùëñùëö(yS, yT) ‚â•ùúñ}‚à£.
Note that TDAS is associated with similarity measure
ùëÜùëñùëö(yS, yT) = ySùê¥yT‚Ä≤ where ùê¥is the learned metric.
As we will see in the next section, TDAS can be used to
evaluate the effectiveness of a DA method under the covariate shift assumption and probabilistic Lipschitzness assumption .
The larger the TDAS, the better the DA
4. Experiments
We evaluate our method in the context of object recognition using a standard dataset and protocol for evaluating
visual domain adaptation methods as in . In
addition, we also evaluate our method using various other
image classiÔ¨Åcation datasets.
4.1. DA datasets and data preparation
We provide three series of experiments on different
datasets. In the Ô¨Årst series, we use the OfÔ¨Åce dataset 
and Caltech10 dataset that contain four domains altogether to evaluate all DA methods. The OfÔ¨Åce dataset consists of images from web-cam (denoted by W), DSLR images (denoted by D) and Amazon images (denoted by A).
The Caltech10 images are denoted by C. We follow the
same setup as in . We use each source of images as a
domain, consequently we get four domains (A, C, D and
W) leading to 12 DA problems. We denote a DA problem
by the notation ùëÜ‚Üíùëá. We use the image representations
provided by for OfÔ¨Åce and Caltech10 datasets (SURF
features encoded with a visual dictionary of 800 words). We
follow the standard protocol of for generating
the source and target samples1.
In a second series, we evaluate the effectiveness of our
DA method using other datasets, namely ImageNet (I), LabelMe (L) and Caltech-256 (C). In this setting we consider
each dataset as a domain. We select Ô¨Åve common objects
(bird, car, chair, dog and person) for all three datasets leading to a total of 7719 images. We extract dense SIFT features and create a bag-of-words dictionary of 256 words using kmeans. Afterwards, we use LLC encoding and a spatial
pyramid (2 √ó 2 quadrants + 3 √ó 1 horizontal + 1 full image)
to obtain a 2048 dimensional image representation (similar
data preparation as in ).
In the last series, we evaluate the effectiveness of our DA
method using larger datasets, namely PASCAL-VOC-2007
and ImageNet. We select all the classes of PASCAL-VOC-
2007. The objective here is to classify PASCAL-VOC-2007
test images using classiÔ¨Åers that are built from the ImageNet
dataset. To prepare the data, we extract dense SIFT features
and create a bag-of-words dictionary of 256 using only ImageNet images. Afterwards, we use LLC encoding and spatial pyramids (2√ó2 + 3√ó1 + 1) to obtain a 2048 dimensional
image representation.
4.2. Experimental setup
We compare our subspace DA approach with two other
DA methods and three baselines. Each of these methods
deÔ¨Ånes a new representation space and our goal is to compare the performance of a 1-Nearest-Neighbor (NN) classi-
Ô¨Åer and a SVM classiÔ¨Åer on DA problems in the subspace
We consider the DA methods Geodesic Flow Kernel
(GFK ) and Geodesic Flow Sampling (GFS ). They
have indeed demonstrated state of the art performances
achieving better results than metric learning methods 
and better than those reported by Chang‚Äôs method in .
Moreover, these methods are the closest to our approach.
We also report results obtained by the following three baselines: Baseline 1: where we use the projection deÔ¨Åned
by the PCA subspace ùëãùëÜbuilt from the source domain to
project both source and target data and work in the resulting representation. Baseline 2: where we use similarly the
projection deÔ¨Åned by the PCA subspace ùëãùëábuilt from the
target domain. No adaptation NA: where no projection is
made, we use the original input space without learning a
new representation.
For each method, we compare the performance of a 1-
Nearest-Neighbor (NN) classiÔ¨Åer and of a SVM classiÔ¨Åer
1See supplementary material section 1.1 for the experimental details
and additional results.
(with C parameter set to the mean similarity value obtained from the training set) in the subspace deÔ¨Åned by
each method. For each source-target DA problem in the
Ô¨Årst two series of experiments, we evaluate the accuracy
of each method on the target domain over 20 random trials. For each trial, we consider an unsupervised DA setting
where we randomly sample labeled data in the source domain as training data and unlabeled data in the target domain as testing examples. In the last series involving the
PASCAL-VOC dataset, we rather evaluate the approaches
by measuring the mean average precision over target data
using SVM.
We have also compared the behavior of the approaches
in a semi-supervised scenario by adding 3 labelled target
examples to the training set for OfÔ¨Åce+Caltech10 series and
50 for the PASCAL-VOC series. This can be found in the
supplementary material.
4.3. Selecting the optimal dimensionality
In this section, we present our procedure for selecting the
space dimensionality d in the context of our method. The
same dimensionality is used for Baseline1 and Baseline2.
For GFK and GFS we follow the published procedures to
obtain optimal results as presented in . First, we perform a PCA on the two domains and compute the deviation
ùëë+1 for all possible ùëëvalues. Then, using the theoretical bound of Eq: 5, we can estimate a ùëëùëöùëéùë•<< ùê∑that
provides a stable solution with Ô¨Åxed deviation ùõæ> 0 for a
given conÔ¨Ådence ùõø> 0. Afterwards, we consider the subspaces of dimensionality from ùëë= 1 to ùëëùëöùëéùë•and select the
best ùëë‚àóthat minimizes the classiÔ¨Åcation error using a 2 fold
cross-validation over the labelled source data. This procedure is founded by the theoretical result of Ben-David et al.
of Eq 6 where the idea is to try to move the domain distribution closer while maintaining a good accuracy on the
source domain. As an illustration, the best dimensions for
the OfÔ¨Åce dataset vary between 10 ‚àí50. For example, for
the DA problem W ‚ÜíC, taking ùõæ= 105 and ùõø= 0.1, we
obtain ùëëùëöùëéùë•= 22 (see Figure 2) and by cross validation we
found that the optimal dimension is ùëë‚àó= 20.
4.4. Evaluating DA with divergence measures
Here, we propose to evaluate the capability of our
method to move the domain distributions closer according
to the measures presented in Section 3.4: the TDAS adapted
to NN classiÔ¨Åcation where a high value indicates a better
distribution closeness and the ùêªŒîùêªusing a SVM where a
value close to 50 indicates close distributions. We compute
these discrepancy measures for the 12 DA problems coming
from the OfÔ¨Åce and Caltech datasets and report the mean
values over the 12 problems for each method in Table 1.
We can remark that our approach reduces signiÔ¨Åcantly the
discrepancy between the source and target domains com-
Figure 2. Finding a stable solution and a subspace dimensionality
using the consistency theorem.
Baseline 1
Baseline 2
Table 1. Several distribution discrepancy measures averaged over
12 DA problems using OfÔ¨Åce dataset.
pared to the other baselines (highest TDAS value and lowest ùêªŒîùêªmeasure). Both GFK and our method have lower
ùêªŒîùêªvalues meaning that these methods are more likely
to perform well2.
4.5. ClassiÔ¨Åcation Results
Visual domain adaptation performance with Of-
Ô¨Åce/Caltech10 datasets: In this experiment we evaluate the
different methods using OfÔ¨Åce /Caltech10 datasets
which consist of four domains (A, C, D and W). The results for the 12 DA problems in the unsupervised setting
using a NN classiÔ¨Åer are shown in Table 2. In 9 out of the
12 DA problems our method outperforms the other ones.
The results obtained in the semi-supervised DA setting (see
supplementary material) conÔ¨Årm this behavior. Here our
method outperforms the others in 10 DA problems.
The results obtained with a SVM classiÔ¨Åer in the unsupervised DA case are shown in Table 3. Our method outperforms all the other methods in 11 DA problems. These
results indicate that our method works better than other DA
methods not only for NN-like local classiÔ¨Åers but also with
more global SVM classiÔ¨Åers.
Domain adaptation on ImageNet,
LabelMe and
Caltech-256 datasets : Results obtained for unsupervised
DA using NN classiÔ¨Åers are shown in Table 4. First, we can
remark that all the other DA methods achieve poor accuracy when LabelMe images are used as the source domain,
while our method seems to adapt the source to the target
reasonably well. On average, our method signiÔ¨Åcantly outperforms all other DA methods.
A visual example where we classify ImageNet images
2See section 1.4 of supplementary material for more details.
Baseline 1
Baseline 2
Baseline 1
Baseline 2
Table 2. Recognition accuracy with unsupervised DA using a NN
classiÔ¨Åer (OfÔ¨Åce dataset + Caltech10).
Baseline 1
Baseline 2
Baseline 1
Baseline 2
Table 3. Recognition accuracy with unsupervised DA using a SVM
classiÔ¨Åer(OfÔ¨Åce dataset + Caltech10).
Table 4. Recognition accuracy with unsupervised DA with NN
classiÔ¨Åer (ImageNet (I), LabelMe (L) and Caltech-256 (C)).
using Caltech-256 images is shown in Figure 1. The nearest neighbor coming from Caltech-256 corresponds to the
same class, even though the appearance of images are very
different from the two datasets.
In Table 5 we report results using a SVM classiÔ¨Åer for
the unsupervised DA setting. In this case our method outperforms all other DA methods, conÔ¨Årming the good behavior of our approach.
Classifying PASCAL-VOC-2007 images using classi-
Ô¨Åers built on ImageNet : In this experiment, we compare
the average precision obtained on PASCAL-VOC-2007 by
a SVM classiÔ¨Åer in both unsupervised and semi-supervised
DA settings. We use ImageNet as the source domain and
PASCAL-VOC-2007 as the target domain. The results are
shown in Figure 3 for the unsupervised case and in the sup-
Table 5. Recognition accuracy with unsupervised DA with SVM
classiÔ¨Åer (ImageNet (I), LabelMe (L) and Caltech-256 (C)).
Figure 3. Train on ImageNet and classify PASCAL-VOC-2007 images using unsupervised DA with SVM.
plementary material for the semi-supervised one.
Our method achieves the best results for all the categories in both settings and outperforms all the methods on
average. The semi-supervised DA seems to improve unsupervised DA by 10% (relative) in mAP. In the unsupervised
DA setting, GFK improves by 7% in mAP over no adaptation while our method improves by 27% in mAP over GFK.
In the semi-supervised setting our method improves by 13%
in mAP over GFK and by 46% over no adaptation.
5. Conclusion
We present a new visual domain adaptation method using subspace alignment.
In this method, we create subspaces for both source and target domains and learn a linear
mapping that aligns the source subspace with the target subspace. This allows us to compare the source domain data
directly with the target domain data and to build classiÔ¨Åers
on source data and apply them on the target domain. We
demonstrate excellent performance on several image classi-
Ô¨Åcation datasets such as OfÔ¨Åce dataset, Caltech, ImageNet,
LabelMe and Pascal-VOC. We show that our method outperforms state of the art domain adaptation methods using
both SVM and nearest neighbour classiÔ¨Åers. We experimentally show that our method can be used on tasks such
as labelling PASCAL-VOC images using ImageNet dataset
for training. Due to its simplicity and theoretically founded
stability, we believe that our method has the potential to be
applied on large datasets consisting of millions of images.
As future work we plan to extend our domain adaptation
method to large scale image retrieval and on the Ô¨Çy learning
of classiÔ¨Åers.
Acknowledgements : The authors acknowledge the support of the FP7 ERC Starting Grant 240530 COGNIMUND,
ANR LAMPADA 09-EMER-007-02 project and PASCAL
2 network of Excellence.