Return of the Devil in the Details:
Delving Deep into Convolutional Nets
Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman
Visual Geometry Group, Department of Engineering Science, University of Oxford
{ken,karen,vedaldi,az}@robots.ox.ac.uk
Abstract—The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, signiﬁcantly raising the interest of the community in
these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous
state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper
conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on
a common ground, identifying and disclosing important implementation details. We identify several useful properties
of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced
signiﬁcantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods
that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to
CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source
code and models to reproduce the experiments in the paper is made publicly available.
INTRODUCTION
the single most important design
choice in current state-of-the-art image classiﬁcation and object recognition systems is the choice
of visual features, or image representation. In fact,
most of the quantitative improvements to image
understanding obtained in the past dozen years can
be ascribed to the introduction of improved representations, from the Bag-of-Visual-Words (BoVW) ,
 to the (Improved) Fisher Vector (IFV) . A
common characteristic of these methods is that
they are largely handcrafted. They are also relatively
simple, comprising dense sampling of local image
patches, describing them by means of visual descriptors such as SIFT, encoding them into a highdimensional representation, and then pooling over
the image. Recently, these handcrafted approaches
have been substantially outperformed by the introduction of the latest generation of Convolutional
Neural Networks (CNNs) to the computer vision
ﬁeld. These networks have a substantially more
sophisticated structure than standard representations, comprising several layers of non-linear feature extractors, and are therefore said to be deep
(in contrast, classical representation will be referred
to as shallow). Furthermore, while their structure
is handcrafted, they contain a very large number
of parameters learnt from data. When applied to
standard image classiﬁcation and object detection
benchmark datasets such as ImageNet ILSVRC 
and PASCAL VOC such networks have demonstrated excellent performance , , , , ,
signiﬁcantly better than standard image encodings .
Despite these impressive results, it remains unclear how different deep architectures compare to
each other and to shallow computer vision methods such as IFV. Most papers did not test these
representations extensively on a common ground,
so a systematic evaluation of the effect of different design and implementation choices remains
largely missing. As noted in our previous work ,
which compared the performance of various shallow visual encodings, the performance of computer
vision systems depends signiﬁcantly on implementation
details. For example, state-of-the-art methods such
as not only involve the use of a CNN, but
also include other improvements such as the use
of very large scale datasets, GPU computation, and
data augmentation (also known as data jittering or
virtual sampling). These improvements could also
transfer to shallow representations such as the IFV,
potentially explaining a part of the performance
 
In this study we analyse and empirically clarify
these issues, conducting a large set of rigorous
experiments (Sect. 4), in many ways picking up
the story where it last ended in with the
comparison of shallow encoders. We focus on methods to construct image representations, i.e. encoding functions φ mapping an image I to a vector
φ(I) ∈Rd suitable for analysis with a linear classi-
ﬁer, such as an SVM. We consider three scenarios
(Sect. 2, Sect. 3): shallow image representations,
deep representations pre-trained on outside data,
and deep representation pre-trained and then ﬁnetuned on the target dataset. As part of our tests,
we explore generally-applicable best practices that
are nevertheless more often found in combination with CNNs or, alternatively, with shallow
encoders , porting them with mutual beneﬁt.
These are (Sect. 2): the use of colour information,
feature normalisation, and, most importantly, the
use of substantial data augmentation. We also determine scenario-speciﬁc best-practices, improving
the ones in , and others, including dimensionality reduction for deep features. Finally, we
achieve performance competitive with the state
of the art , on PASCAL VOC classiﬁcation
using less additional training data and signiﬁcantly
simpler techniques. As in , the source code and
models to reproduce all experiments in this paper
is available on the project website1.
This section introduces the three types of image
representation φ(I) considered in this paper, describing them within the context of three different
scenarios. Having outlined details speciﬁc to each,
general methodologies which apply to all three
scenarios are reviewed, such as data augmentation
and feature normalisation, together with the linear
classiﬁer (trained with a standard hinge loss). We
also specify here the benchmark datasets used in
the evaluation.
Scenario 1: Shallow representation (IFV)
Our reference shallow image representation is the
IFV . Our choice is motivated by the fact that
IFV usually outperforms related encoding methods
such as BoVW, LLC , and VLAD . Given an
image I, the IFV φFV(I) is obtained by extracting
1. eval/
a dense collection of patches and corresponding
local descriptors xi ∈RD (e.g. SIFT) from the image
at multiple scales. Each descriptor xi is then softquantized using a Gaussian Mixture Model with
K components. First and second order differences
between each descriptor xi and its Gaussian cluster
mean µk are accumulated in corresponding blocks
uk, vk in the vector φFV(I) ∈R2KD, appropriately
weighed by the Gaussian soft-assignments and
covariance, leading to a 2KD-dimensional image
representation φFV(I) = [u⊤
1 , . . . u⊤
improved version of the Fisher vector involves postprocessing φFV by computing the signed squareroot of its scalar components and normalising the
result to a unit ℓ2 norm. The details of this construction can be found in ; here we follow the
notation of .
Scenario 2: Deep representation (CNN) with
pre-training
Our deep representations are inspired by the success of the CNN of Krizhevsky et al. . As shown
in , , the vector of activities φCNN(I) of the
penultimate layer of a deep CNN, learnt on a large
dataset such as ImageNet , can be used as a powerful image descriptor applicable to other datasets.
Numerous CNN architectures that improve the
previous state of the art obtained using shallow
representations have been proposed, but choosing
the best one remains an open question. Many are inspired by : DeCAF , , Caffe , Oquab et
al. . Others use larger networks with a smaller
stride of the ﬁrst convolutional layer: Zeiler and
Fergus and OverFeat , . Other differences
include the CNN pre-training protocols. Here we
adopt a single learning framework and experiment
with architectures of different complexity exploring
their performance-speed trade-off.
Scenario 3: Deep representation (CNN) with
pre-training and ﬁne-tuning
In Scenario 2 features are trained on one (large)
dataset and applied to another (usually smaller).
However, it was demonstrated that ﬁne-tuning
a pre-trained CNN on the target data can signiﬁcantly improve the performance. We consider
this scenario separately from that of Scenario 2, as
the image features become dataset-speciﬁc after the
ﬁne-tuning.
Commonalities
We now turn to what is in common across the
scenarios.
Data augmentation
Data augmentation is a method applicable to shallow and deep representations, but that has been
so far mostly applied to the latter , . By
augmentation we mean perturbing an image I by
transformations that leave the underlying class unchanged (e.g. cropping and ﬂipping) in order to
generate additional examples of the class. Augmentation can be applied at training time, at test time,
or both. The augmented samples can either be taken
as-is or combined to form a single feature, e.g. using
sum/max-pooling or stacking.
Linear predictors
All the representations φ(I) in the three scenarios
are used to construct linear predictors ⟨w, φ(I)⟩for
each class to be recognized. These predictors are
learnt using Support Vector Machines (SVM) by
ﬁtting w to the available training data by minimizing an objective function balancing a quadratic
regularizer and the hinge-loss. The parameter C
in the SVM, trading-off regularizer and loss, is
determined using an held-off validation subset of
the data. Here we use the same learning framework
with all representations. It is common experience
that linear classiﬁers are particularly sensitive to
the normalisation of the data and that, in particular,
SVMs tend to beneﬁt from ℓ2 normalisation (an
interpretation is that after normalisation the inner
product corresponds to the cosine similarly).
Benchmark data
As reference benchmark we use the PASCAL
VOC data as already done in . The VOC-
2007 edition contains about 10,000 images split into
train, validation, and test sets, and labelled with
twenty object classes. A one-vs-rest SVM classiﬁer
for each class is learnt and evaluated independently
and the performance is measured as mean Average
Precision (mAP) across all classes. The VOC-2012
edition contains roughly twice as many images
and does not include test labels; instead, evaluation
uses the ofﬁcial PASCAL Evaluation Server. To train
deep representations we use the ILSVRC-2012 challenge dataset. This contains 1,000 object categories
from ImageNet with roughly 1.2M training images, 50,000 validation images, and 100,000 test
images. Performance is evaluated using the top-5
classiﬁcation error. Finally, we also evaluate over
the Caltech-101 and Caltech-256 image classiﬁcation benchmarks , . For Caltech-101, we
followed the protocol of , and considered three
random splits into training and testing data, each
of which comprises 30 training and up to 30 testing
images per class. For Caltech-256, two random
splits were generated, each of which contains 60
training images per class, and the rest are used for
testing. On both Caltech datasets, performance is
measured using mean class accuracy.
This section gives the implementation details of the
methods introduced in Sect. 2.
Improved Fisher Vector details
Our IFV representation uses a slightly improved
setting compared to the best result of .
Computation starts by upscaling the image I by a
factor of 2 , followed by SIFT features extraction
with a stride of 3 pixels at 7 different scales with
scale increments. These features are square-rooted
as suggested by , and decorrelated and reduced
in dimension from 128D to 80D using PCA. A
GMM with K = 256 components is learnt from features sampled from the training images. Hence the
Fisher Vector φFV(I) has dimension 2KD = 40, 960.
Before use in classiﬁcation, the vector is signedsquare-rooted and l2-normalised (square rooting
correspond to the Hellinger’s kernel map ). As
in , square-rooting is applied twice, once to the
raw encodings, and once again after sum pooling
and normalisation. In order to capture weak geometrical information, the IFV representation is used
in a spatial pyramid . As in , the image is
divided into 1×1, 3×1, and 2×2 spatial subdivisions
and corresponding IFVs are computed and stacked
with an overall dimension of 8 × 2KD = 327, 680
In addition to this standard formulation, we experiment with a few modiﬁcations. The ﬁrst one
is the use of intra-normalisation of the descriptor
blocks, an idea recently proposed for the VLAD
descriptor . In this case, the ℓ2 normalisation
is applied to the individual sub-blocks (uk, vk) of
the vector φFV(I), which helps to alleviate the
local feature burstiness . In the case of the
improved intra-normalised features, it was found
that applying the square-rooting only once to the
ﬁnal encoding produced the best results.
The second modiﬁcation is the use of spatiallyextended local descriptors instead of a spatial
pyramid. Here descriptors xi are appended with
their image location (xi, yi) before quantization
with the GMM. Formally, xi is extended, after PCA
projection, with its normalised spatial coordinates:
i , xi/W −0.5, yi/H −0.5]⊤, where W × H are the
dimensions of the image. Since the GMM quantizes
both appearance and location, this allows for spatial
information to be captured directly by the softquantization process. This method is signiﬁcantly
more memory-efﬁcient than using a spatial pyramid. Speciﬁcally, the PCA-reduced SIFT features
are spatially augmented by appending (x, y) yielding D = 82 dimensional descriptors pooled in a
2KD = 41, 984 dimensional IFV.
The third modiﬁcation is the use of colour features in addition to SIFT descriptors. While colour
information is used in CNNs and by the original FV paper , it was not explored in our previous comparison . We do so here by adopting
the same Local Colour Statistics (LCS) features as
used by . LCS is computed by dividing an input
patch into a 4 × 4 spatial grid (akin to SIFT), and
computing the mean and variance of each of the
Lab colour channels for each cell of the grid. The
LCS dimensionality is thus 4 × 4 × 2 × 3 = 96. This
is then encoded in a similar manner to SIFT.
Convolutional neural networks details
The CNN-based features are based on three CNN
architectures representative of the state of the art
(shown in Table 1) each exploring a different accuracy/speed trade-off. To ensure a fair comparison
between them, these networks are trained using the
same training protocol and the same implementation, which we developed based on the opensource Caffe framework . ℓ2-normalising the
CNN features φCNN(I) before use in the SVM was
found to be important for performance.
Our Fast (CNN-F) architecture is similar to the
one used by Krizhevsky et al. . It comprises
8 learnable layers, 5 of which are convolutional,
and the last 3 are fully-connected. The input image
size is 224 × 224. Fast processing is ensured by the
4 pixel stride in the ﬁrst convolutional layer. The
main differences between our architecture and that
of are the reduced number of convolutional
layers and the dense connectivity between convolutional layers ( used sparse connections to enable
training on two GPUs).
Our Medium (CNN-M) architecture is similar
to the one used by Zeiler and Fergus . It is
characterised by the decreased stride and smaller
receptive ﬁeld of the ﬁrst convolutional layer, which
was shown to be beneﬁcial on the ILSVRC dataset.
At the same time, conv2 uses larger stride (2 instead
of 1) to keep the computation time reasonable. The
main difference between our net and that of is
we use less ﬁlters in the conv4 layer (512 vs. 1024).
Our Slow (CNN-S) architecture is related to the
‘accurate’ network from the OverFeat package .
It also uses 7 × 7 ﬁlters with stride 2 in conv1.
Unlike CNN-M and , the stride in conv2 is
smaller (1 pixel), but the max-pooling window in
conv1 and conv5 is larger (3×3) to compensate for
the increased spatial resolution. Compared to ,
we use 5 convolutional layers as in the previous
architectures ( used 6), and less ﬁlters in conv5
(512 instead of 1024); we also incorporate an LRN
layer after conv1 ( did not use contrast normalisation).
CNN training
follows that of , learning on ILSVRC-2012 using gradient descent with momentum. The hyperparameters are the same as used by : momentum 0.9; weight decay 5 · 10−4; initial learning rate
10−2, which is decreased by a factor of 10, when
the validation error stop decreasing. The layers are
initialised from a Gaussian distribution with a zero
mean and variance equal to 10−2. We also employ
similar data augmentation in the form of random
crops, horizontal ﬂips, and RGB colour jittering.
Test time crop sampling is discussed in Sect. 3.3;
at training time, 224 × 224 crops are sampled randomly, rather than deterministically. Thus, the only
notable difference to is that the crops are taken
from the whole training image P × 256, P ≥256,
rather than its 256 × 256 centre. Training was performed on a single NVIDIA GTX Titan GPU and
the training time varied from 5 days for CNN-F to
3 weeks for CNN-S.
CNN ﬁne-tuning on the target dataset
In our experiments, we ﬁne-tuned CNN-S using
VOC-2007, VOC-2012, or Caltech-101 as the target
data. Fine-tuning was carried out using the same
framework (and the same data augmentation), as
we used for CNN training on ILSVRC. The last
st. 4, pad 0
st. 1, pad 2
st. 1, pad 1
st. 1, pad 1
st. 1, pad 1
dropdropsoft-
LRN, x2 pool
LRN, x2 pool
st. 2, pad 0
st. 2, pad 1
st. 1, pad 1
st. 1, pad 1
st. 1, pad 1
dropdropsoft-
LRN, x2 pool
LRN, x2 pool
st. 2, pad 0
st. 1, pad 1
st. 1, pad 1
st. 1, pad 1
st. 1, pad 1
dropdropsoft-
LRN, x3 pool
CNN architectures. Each architecture contains 5 convolutional layers (conv 1–5) and three
fully-connected layers (full 1–3). The details of each of the convolutional layers are given in three sub-rows:
the ﬁrst speciﬁes the number of convolution ﬁlters and their receptive ﬁeld size as “num x size x size”; the
second indicates the convolution stride (“st.”) and spatial padding (“pad”); the third indicates if Local
Response Normalisation (LRN) is applied, and the max-pooling downsampling factor. For full 1–3, we
specify their dimensionality, which is the same for all three architectures. Full6 and full7 are regularised
using dropout , while the last layer acts as a multi-way soft-max classiﬁer. The activation function for all
weight layers (except for full8) is the REctiﬁcation Linear Unit (RELU) .
fully-connected layer (conv8) has output dimensionality equal to the number of classes, which
differs between datasets, so we initialised it from
a Gaussian distribution (as used for CNN training
above). Now we turn to dataset-speciﬁc ﬁne-tuning
VOC-2007 and VOC-2012. Considering that PAS-
CAL VOC is a multi-label dataset (i.e. a single
image might have multiple labels), we replaced the
softmax regression loss with a more appropriate
loss function, for which we considered two options:
one-vs-rest classiﬁcation hinge loss (the same loss
as used in the SVM experiments) and ranking hinge
loss. Both losses deﬁne constraints on the scores of
positive (Ipos) and negative (Ineg) images for each
class: wcφ(Ipos) > 1 −ξ, wcφ(Ineg) < −1 + ξ for the
classiﬁcation loss, wcφ(Ipos) > wcφ(Ineg) + 1 −ξ
for the ranking loss (wc is the c-th row of the
last fully-connected layer, which can be seen as a
linear classiﬁer on deep features φ(I); ξ is a slack
variable). Our ﬁne-tuned networks are denoted as
“CNN S TUNE-CLS” (for the classiﬁcation loss)
and “CNN S TUNE-RNK” (for the ranking loss).
In the case of both VOC datasets, the training
and validation subsets were combined to form a
single training set. Given the smaller size of the
training data when compared to ILSVRC-2012, we
controlled for over-ﬁtting by using lower initial
learning rates for the ﬁne-tuned hidden layers.
The learning rate schedule for the last layer /
hidden layers was: 10−2/10−4 →10−3/10−4 →
10−4/10−4 →10−5/10−5.
Caltech-101
dataset contains a single class label
per image, so ﬁne-tuning was performed using the
softmax regression loss. Other settings (including
the learning rate schedule) were the same as used
for the VOC ﬁne-tuning experiments.
Low-dimensional CNN feature training
Our baseline networks (Table 1) have the same
dimensionality of the last hidden layer (full7): 4096.
This design choice is in accordance with the stateof-the-art architectures , , , and leads to
a 4096-D dimensional image representation, which
is already rather compact compared to IFV. We
further trained three modiﬁcations of the CNN-M
network, with lower dimensional full7 layers of:
2048, 1024, and 128 dimensions respectively. The
networks were learnt on ILSVRC-2012. To speedup training, all layers aside from full7/full8 were
set to those of the CNN-M net and a lower initial
learning rate of 10−3 was used. The initial learning
rate of full7/full8 was set to 10−2.
Data augmentation details
We explore three data augmentation strategies. The
ﬁrst strategy is to use no augmentation. In contrast to IFV, however, CNNs require images to
be transformed to a ﬁxed size (224 × 224) even
when no augmentation is used. Hence the image is
downsized so that the smallest dimension is equal
to 224 pixels and a 224 × 224 crop is extracted
from the centre.2 The second strategy is to use
ﬂip augmentation, mirroring images about the yaxis producing two samples from each image. The
third strategy, termed C+F augmentation, combines
cropping and ﬂipping. For CNN-based representations, the image is downsized so that the smallest
dimension is equal to 256 pixels. Then 224 × 224
crops are extracted from the four corners and the
centre of the image. Note that the crops are sampled
from the whole image, rather than its 256 × 256
centre, as done by . These crops are then ﬂipped
about the y-axis, producing 10 perturbed samples
per input image. In the case of the IFV encoding, the
same crops are extracted, but at the original image
resolution.
This section describes the experimental results,
comparing different features and data augmentation schemes. The results are given in Table 2 for
VOC-2007 and analysed next, starting from generally applicable methods such as augmentation and
then discussing the speciﬁcs of each scenario. We
then move onto other datasets and the state of the
art in Sect. 4.7.
Data augmentation
We experiment with no data augmentation (denoted Image Aug=– in Tab. 2), ﬂip augmentation (Image Aug=F), and C+F augmentation (Image
Aug=C). Augmented images are used as standalone samples (f), or by fusing the corresponding
descriptors using sum (s) or max (m) pooling or
stacking (t). So for example Image Aug=(C) f s in
row [f] of Tab. 2 means that C+F augmentation is
used to generate additional samples in training (f),
and is combined with sum-pooling in testing (s).
Augmentation
consistently
performance by ∼3% for both IFV (e.g. [d] vs. [f]) and
CNN (e.g. [o] vs. [p]). Using additional samples for
training and sum-pooling for testing works best
([p]) followed by sum-pooling [r], max pooling [q],
and stacking [s]. In terms of the choice of transformations, ﬂipping improves only marginally ([o] vs.
2. Extracting a 224 × 224 centre crop from a 256 × 256
image resulted in worse performance.
[u]), but using the more expensive C+F sampling
improves, as seen, by about 2 ∼3% ([o] vs. [p]). We
experimented with sampling more transformations,
taking a higher density of crops from the centre of
the image, but observed no beneﬁt.
Colour information can be added and subtracted in
CNN and IFV. In IFV replacing SIFT with the colour
descriptors of (denoted COL in Method) yields
signiﬁcantly worse performance ([j] vs. [h]). However, when SIFT and colour descriptors are combined by stacking the corresponding IFVs (COL+)
there is a small but signiﬁcant improvement of
around ∼1% in the non-augmented case (e.g. [h]
vs. [k]) but little impact in the augmented case
(e.g. [i] vs. [l]). For CNNs, retraining the network
after converting all the input images to grayscale
(denoted GS in Methods) has a more signiﬁcant
impact, resulting in a performance drop of ∼3%
([w] vs. [p], [v] vs. [o]).
Scenario 1: Shallow representation (IFV)
The baseline IFV encoding using a spatial pyramid
[a] performs slightly better than the results [I] taken
from Chatﬁeld et al. , primarily due to a larger
number of spatial scales being used during SIFT
feature extraction, and the resultant SIFT features
being square-rooted. Intra-normalisation, denoted as
IN in the Method column of the table, improves the
performance by ∼1% (e.g. [c] vs. [d]). More interestingly, switching from spatial pooling (denoted
spm in the SPool column) to feature spatial augmentation (SPool=(x,y)) has either little effect on the
performance or results in a marginal increase ([a] vs.
[c], [b] vs. [d]), whilst resulting in a representation
which is over 10× smaller. We also experimented
with augmenting with scale in addition to position
as in but observed no improvement. Finally,
we investigate pushing the parameters of the representation setting K = 512 (rows [h]-[l]). Increasing
the number of GMM centres in the model from
K = 256 to 512 results in a further performance
increase (e.g. [h] vs. [d]), but at the expense of
higher-dimensional codes (125K dimensional).
Scenario 2: Deep representation (CNN) with
pre-training
CNN-based methods consistently outperform the
shallow encodings, even after the improvements
Image Aug.
(II) DECAF
(h) FK IN 512
(i) FK IN 512
(j) FK IN COL 512
(k) FK IN 512 COL+
(l) FK IN 512 COL+
(v) CNN M GS
(w) CNN M GS
(x) CNN M 2048
(y) CNN M 1024
(z) CNN M 128
(α) FK+CNN F
(β) FK+CNN M 2048
(γ) CNN S TUNE-RNK
VOC 2007 results (continued overleaf). See Sect. 4 for details.
discussed above, by a large ∼10% mAP margin
([i] vs. [p]). Our small architecture CNN-F, which is
similar to DeCAF , performs signiﬁcantly better
than the latter ([II] vs. [s]), validating our implementation. Both medium CNN-M [m] and slow
CNN-S [p] outperform the fast CNN-F [m] by a
signiﬁcant 2 ∼3% margin. Since the accuracy
of CNN-S and CNN-M is nearly the same, we
focus on the latter as it is simpler and marginally
(∼25%) faster. Remarkably, these good networks
work very well even with no augmentation [o].
Another advantage of CNNs compared to IFV is
the small dimensionality of the output features,
although IFV can be compressed to an extent. We
explored retraining the CNNs such that the ﬁnal
layer was of a lower dimensionality, and reducing
from 4096 to 2048 actually resulted in a marginal
performance boost ([x] vs. [p]). What is surprising
is that we can reduce the output dimensionality
further to 1024D [y] and even 128D [z] with only
a drop of ∼2% for codes that are 32× smaller
(∼650× smaller than our best performing IFV [i]).
Note, ℓ2-normalising the features accounted for up
to ∼5% of their performance over VOC 2007; it
should be applied before input to the SVM and
after pooling the augmented descriptors 
applicable).
Scenario 3: Deep representation (CNN) with
pre-training and ﬁne-tuning
We ﬁne-tuned our CNN-S architecture on VOC-
2007 using the ranking hinge loss, and achieved
a signiﬁcant improvement: 2.7% ([γ] vs. [n]). This
demonstrates that in spite of the small amount
of VOC training data (5,011 images), ﬁne-tuning
is able to adjust the learnt deep representation to
better suit the dataset in question.
Combinations
For the CNN-M 2048 representation [x], stacking
deep and shallow representations to form a higherdimensional descriptor makes little difference ([x]
vs. [β]). For the weaker CNN-F it results in a small
boost of ∼0.8% ([m] vs. [α]).
Comparison with the state of the art
In Table 3 we report our results on ILSVRC-2012,
VOC-2007, VOC-2012, Caltech-101, and Caltech-256
datasets, and compare them to the state of the art.
First, we note that the ILSVRC error rates of our
CNN-F, CNN-M, and CNN-S networks are better
ILSVRC-2012
Caltech-101
Caltech-256
(top-5 error)
(accuracy)
(accuracy)
(a) FK IN 512
87.15 ± 0.80
77.03 ± 0.46
(d) CNN M 2048
86.64 ± 0.53
76.88 ± 0.35
87.76 ± 0.66
77.61 ± 0.12
(f) CNN S TUNE-CLS
88.35 ± 0.56
77.33 ± 0.56
(g) CNN S TUNE-RNK
(h) Zeiler & Fergus 
86.5 ± 0.5
74.2 ± 0.3
(i) Razavian et al. , 
(j) Oquab et al. 
78.7 (82.8*)
(k) Oquab et al. 
(l) Wei et al. 
81.5 (85.2*)
81.7 (90.3*)
(m) He et al. 
91.4 ± 0.7
Comparison with the state of the art on ILSVRC2012, VOC2007, VOC2012, Caltech-101, and
Caltech-256. Results marked with * were achieved using models pre-trained on the extended ILSVRC
datasets . All other results were achieved using CNNs
pre-trained on ILSVRC-2012 (1000 classes).
than those reported by , , and for the
related conﬁgurations. This validates our implementation, and the difference is likely to be due to
the sampling of image crops from the uncropped
image plane (instead of the centre). When using
our CNN features on other datasets, the relative
performance generally follows the same pattern
as on ILSVRC, where the nets are trained – the
CNN-F architecture exhibits the worst performance,
with CNN-M and CNN-S performing considerably
Further ﬁne-tuning of CNN-S on the VOC
datasets turns out to be beneﬁcial; on VOC-2012,
using the ranking loss is marginally better than
the classiﬁcation loss ([g] vs. [f]), which can be
explained by the ranking-based VOC evaluation
criterion. Fine-tuning on Caltech-101 also yields a
small improvement, but no gain is observed over
Caltech-256.
Our CNN-S net is competitive with recent CNNbased approaches , , , , , and
on a number of datasets and sets the state of the
art on VOC-2007 and VOC-2012 across methods
pre-trained solely on ILSVRC-2012 dataset. While
the CNN-based methods of , achieve better
performance on VOC (86.3% and 90.3% respectively), they were trained using extended ILSVRC
datasets, enriched with additional categories semantically close to the ones in VOC. Additionally, used a signiﬁcantly more complex classiﬁcation pipeline, driven by bounding box proposals , pre-trained on ILSVRC-2013 detection
dataset. Their best reported result on VOC-2012
(90.3%) was achieved by the late fusion with a
complex hand-crafted method of ; without fusion, they get 84.2%. On Caltech-101, achieves
the state of the art using spatial pyramid pooling
of conv5 layer features, while we used full7 layer
features consistently across all datasets (for full7
features, they report 87.08%).
In addition to achieving performance comparable
to the state of the art with a very simple approach
(but powerful CNN-based features), with the modiﬁcations outlined in the paper (primarily the use
of data augmentation similar to the CNN-based
methods) we are able to improve the performance
of shallow IFV to 68.02% (Table 2, [i]).
Performance Evolution on VOC-2007
A comparative plot of the evolution in the performance of the methods evaluated in this paper,
along with a selection from our earlier review of
shallow methods is presented in Fig. 1. Classiﬁcation accuracy over PASCAL VOC was 54.48%
mAP for the BoVW model in 2008, 61.7% for the
Fig. 1. Evolution of Performance on PASCAL VOC-2007 over the recent years. Please refer to Table 2
for details and references.
IFV in 2010 , and 73.41% for DeCAF and
similar , CNN-based methods introduced in
late 2013. Our best performing CNN-based method
(CNN-S with ﬁne-tuning) achieves 82.42%, comparable to the most recent state-of-the-art.
Timings and dimensionality
One of our best-performing CNN representations
CNN-M-2048 [x] is ∼42× more compact than the
best performing IFV [i] (84K vs. 2K) and CNN-M
features are also ∼50× faster to compute (∼120s
vs. ∼2.4s per image with augmentation enabled,
over a single CPU core). Non-augmented CNN-M
features [o] take around 0.3s per image, compared
to ∼0.4s for CNN-S features and ∼0.13s for CNN-
F features.
CONCLUSION
In this paper we presented a rigorous empirical evaluation of CNN-based methods for image
classiﬁcation, along with a comparison with more
traditional shallow feature encoding methods. We
have demonstrated that the performance of shallow representations can be signiﬁcantly improved
by adopting data augmentation, typically used in
deep learning. In spite of this improvement, deep
architectures still outperform the shallow methods
by a large margin. We have shown that the performance of deep representations on the ILSVRC
dataset is a good indicator of their performance
on other datasets, and that ﬁne-tuning can further
improve on already very strong results achieved
using the combination of deep representations and
a linear SVM. Source code and CNN models to
reproduce the experiments presented in the paper
are available on the project website in the
hope that it would provide common ground for
future comparisons, and good baselines for image
representation research.
ACKNOWLEDGEMENTS
This work was supported by the EPSRC and ERC
grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the
donation of the GPUs used for this research.