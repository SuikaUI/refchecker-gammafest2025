A Collective Variational Autoencoder for Top-N
Recommendation with Side Information
Yifan Chen
University of Amsterdam
 
Maarten de Rijke
University of Amsterdam
 
Recommender systems have been studied extensively due to their
practical use in many real-world scenarios. Despite this, generating effective recommendations with sparse user ratings remains a
challenge. Side information associated with items has been widely
utilized to address rating sparsity. Existing recommendation models
that use side information are linear and, hence, have restricted expressiveness. Deep learning has been used to capture non-linearities
by learning deep item representations from side information but as
side information is high-dimensional existing deep models tend to
have large input dimensionality, which dominates their overall size.
This makes them difficult to train, especially with small numbers
of inputs.
Rather than learning item representations, which is problematic
with high-dimensional side information, in this paper, we propose
to learn feature representation through deep learning from side
information. Learning feature representations, on the other hand,
ensures a sufficient number of inputs to train a deep network. To
achieve this, we propose to simultaneously recover user ratings and
side information, by using a Variational Autoencoder (VAE). Specifically, user ratings and side information are encoded and decoded
collectively through the same inference network and generation
network. This is possible as both user ratings and side information
are data associated with items. To account for the heterogeneity of
user rating and side information, the final layer of the generation
network follows different distributions depending on the type of
information. The proposed model is easy to implement and efficient to optimize and is shown to outperform state-of-the-art top-N
recommendation methods that use side information.
Top-N recommendation, Side information, Collective Variational
autoencoder
INTRODUCTION
Recommender systems have become increasingly indispensable.
Applications include top-N recommendations, which are widely
adopted to recommend users ranked lists of items. For e-commerce,
typically only a few recommendations are shown to the user each
time and recommender systems are often evaluated based on the
performance of the top-N recommendations.
Collaborative Filtering (CF) based methods are a fundamental
building block in many recommender systems. CF based recommender systems predict what items a user will prefer by discovering
DLRS 2018, October 6, 2018, Vancouver, Canada
2018. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
 
and exploiting similarity patterns across users and items. The performance of CF-based methods often drops significantly when ratings
are very sparse. With the increased availability of so-called side
information, that is, additional information associated with items
such as product reviews, movie plots, etc., there is great interest in
taking advantage of such information so as to compensate for the
sparsity of ratings.
Existing methods utilizing side information are linear models ,
which have a restricted model capacity. A growing body of work
generalizes linear model by deep learning to explore non-linearities
for large-scale recommendations . State-of-the-art
performance is achieved by applying Variational Autoencoders
(VAEs) for CF . These deep models learn item representations from side information. Thus, the dimension of side
information determines the input dimension of the network, which
dominates the overall size of the model. This is problematic since
side information is generally high-dimensional . As shown in
our experiments, existing deep models fail to beat linear models due
to the high-dimensionality of side information and an insufficient
number of samples.
To avoid the impact from the high-dimensionality while taking
the effectiveness of VAE, we propose to learn feature representations from side information. In this way, the dimensions of the
side information correspond to the number of samples rather than
the input dimension of deep network. To instantiate this idea, in
this paper, we propose collective Variational Autoencoder (cVAE),
which learns to recover user ratings and side information simultaneously through VAE. While user ratings and side information
are different sources of information, both are information associated with items. Thus, we take ratings from each user and each
dimension of side information over all items as the input for VAE,
so that samples from both sources of information have the same
dimensionality (number of items). We can then feed ratings and
side information into the same inference network and generation
network. cVAE complements the sparse ratings with side information, as feeding side information into the same VAE increases the
number of samples for training. The high-dimensionality of side
information is not a problem for cVAE, as it increases the sample
size rather than the network scale. To account for the heterogeneity
of user rating and side information, the final layer of the generation network follows different distributions depending on the type
of information. Training a VAE by feeding it side information as
input acts like a pre-training step, which is a crucial step for developing a robust deep network. Our experiments show that the
proposed model,cVAE, achieves state-of-the-art performance for
top-N recommendation with side information.
 
Inference network
Generation network
Figure 1: Collective Variational Autoencoder
The remainder of the paper is organized as follows. We present
preliminaries in Section 2. We introduce the cVAEmodel and optimization in Section 3. Section 4 describes the experimental setup
and results. We review related work in Section 5 and conclude in
Section 6.
PRELIMINARIES
We introduce relevant notation in this section. We use m, n and d
to denote the number of users, items and the dimension of side information, respectively. We study the problem of top-N recommendation with high-dimensional side information, where d ≫n. We
write X ∈Rn×d for the matrix for side information and Y ∈Rm×n
for user ratings. We summarize our notation in Table 1.
Linear models for top-N recommendation
Sparse LInear Method (SLIM) achieves state-of-the-art performance for top-N recommendation. SLIM learns to reproduce the
user rating marix Y through:
Here, W ∈Rn×n is the coefficient matrix, which is analogous to
the item similarity matrix. The performance of SLIM is heavily
affected by the rating sparsity . Side information ha been utilized
to overcome this issue . As a typical example of a method
that uses side information, collective SLIM (cSLIM) learns W from
both user rating and side information. Specifically, X,Y are both
reproduced through:
cSLIM learns the coefficient matrixW collectively from both side information X and user rating Y, a strategy that can help to overcome
rating sparsity by side information. However, cSLIM is restricted by
the fact that it is a linear model, which has limited model capacity.
Table 1: Notation used in the paper.
Description
number of users
number of items
dimension of side information
dimension of latent item representation
number of recommended items
matrix of side information
matrix of user rating
matrix of latent user representation
matrix of latent item representation
matrix of latent feature representation
hidden layer of inference network
hidden layer of generation network
the mean of latent input representation
the variance of latent user or feature representation
non-linear transformation of inference network
non-linear transformation of generation network
the activation function to get µ
the activation function to get σ
the sigmoid function
Autoencoders for collaborative filtering
Recently, autoencoders have been used to address CF problems . Autoencoders are neural networks popularized by Kramer
 . They are unsupervised networks where the output of the network aims to be a reconstruction of the input.
In the context of CF, the autoencoder is fed with incomplete
rows (resp. columns) of the user rating matrix Y. It then outputs
a vector that predicts the missing entries. These approaches perform a non-linear low-rank approximation of Y in two different
ways, using a User-side Autoencoder (UAE) (Figure 2(a)) or Itemside Autoencoder (IAE) (Figure 2(b)), which recover Y respectively
V ∼f (YT ),
YT ∼д(V ),
where U ∈Rm×k is the user representation and V ∈Rn×k is the
item representation. Moreover, f (·) and д(·) are the encode network
and decode network, respectively. UAEs encode Y to learn a user
latent representation U and then recover Y from U . In contrast,
IAEs encode the transpose of Y to learn item latent representation
V and then recover the transpose of Y fromV . Note that UAEs work
in a similar way as SLIM, as both can be viewed as reproducing Y
through Y ∼д(f (Y)), which also captures item similarities.
When side information associated with items is available, the
Feature-side Autoencoder (FAE) is utilized to learn item representations:
V ′ ∼f (X),
X ∼д(V ′),
where V ′ ∈Rn×k is the item representation. Existing hybrid methods incorporate FAE with IAE as both learn item representations.
However, this way of incorporating side information needs to estimate two separate VAEs, which is not an effective way to address
(a) User-side Autoencoder
(b) Item-side Autoencoder
Figure 2: Autoencoders for collaborative filtering
rating sparsity. They are also vulnerable to the high dimensionality
of side information.
In this section, we propose a new way to incorporate side information with user ratings by combining the effectiveness of both cSLIM
and autoencoders. We propose to reproduce X by a FAE and Y by
a UAE. In this way, the input for autoencoders of both X and Y are
of the same dimension, i.e., the number of items n. Thus, we can
feed X and Y into the same autoencoder rather than two different
autoencoders, which helps to overcome rating sparsity.
Collective variational autoencoder
We propose a collective Variational Autoencoder (cVAE) to generalize the linear models for top-N recommendation with side
information to non-linear models, by taking advantage of Variational Autoencoders (VAEs). Specifically, we propose to recover
X,Y through
Y ∼fθ (U ),
X ∼fθ (Z),
where fϕ(·) and fθ (·) correspond to the inference network and
generation network parameterized by ϕ and θ, respectively. An
overview of cVAE is depicted in Figure 1. Unlike previous work utilizing VAEs, the proposed model encodes and decodes user rating
and side information through the same inference and generation
networks. Our model can be viewed as a non-linear generalization
of cSLIM, so as to learn item similarities collectively from user ratings and side information. While user ratings and side information
are two different types of information, cSLIM fails to distinguish
them. In contrast, cVAE assumes the output of the generation network to follow different distributions according to the type of input
it has been fed.
Next, we describe the cVAE model in detail. Following common
practice for VAE, we first assume the latent variables u and z to
follow a Gaussian distribution:
u ∼N(0, I),
z ∼N(0, I),
where I ∈Rk×k is an identity matrix. While X and Y are fed into
the same network, we would like to distinguish them via different
distributions. In this paper, we assume that Y is binarized to capture
implicit feedback, which is a common setting for top-N recommendation . Thus we follow Lee et al. and assume that the rating
of user j over all items follows a Bernoulli distribution:
yj | uj ∼Bernoulli(ς(fθ (uj))),
where ς(·) is the sigmoid function. This defines the loss function
when feeding user rating as input, i.e., the logistic log-likelihood
for user j:
logpθ (yj | uj) =
yji logς(fji) + (1 −yji) log  1 −ς(fji) , (1)
where fji is the i-th element of the vector fθ (uj) and fθ (uj) is
normalized through a sigmoid function so that fji is within (0, 1).
For side information, we study numerical features so that we
assume the j-th dimension of side information from all items follows
a Gaussian distribution:
xj | zj ∼N(fθ (zj), I).
This defines the loss function when feeding side information as
input, i.e., the Gaussian log-likelihood for dimension j:
logpθ (xj | zj) =
2(xji −fji)2,
where fji is the i-th element of vector fθ (zj). Note that although
we assume x and y to be generated from z and u respectively, the
generation has shared parameters θ.
The generation procedure is summarized as follows:
(1) for each user j = 1, . . . ,m:
(a) draw uj ∼N(0, I);
(b) draw yj ∼Bernoulli  ς(fθ (uj))
(2) for each dimension of side information j = 1, . . . ,d:
(a) draw zj ∼N(0, I);
(b) draw xj ∼N(fθ (zu), I).
Once the cVAE is trained, we can generate recommendations for
each user j with items ranked in descending order of fθ (uj). Here,
uj is calculated as uj = µ(fϕ(yj)), that is, we take the mean of uj
for prediction.
Next, we discuss how to perform inference for cVAE.
Variational inference
The log-likelihood of cVAE is intractable due to the non-linear transformations of the generation network. Thus, we resort to variational
inference to approximate the distribution. Variational inference approximates the true intractable posterior with a simpler variational
distribution q(U,Z). We follow the mean-field assumption by
setting q(U,Z) to be a fully factorized Gaussian distribution:
q(uj) = N(µj, diag(σ2
q(zj) = N(µm+j, diag(σ2
While we can optimize {µj,σ j } by minimizing the Kullback-Leiber
divergence KL(qϕ ∥pθ ), the number of parameters to learn grows
with the number of users and dimensions of side information. This
can become a bottleneck for real-world recommender systems with
millions of users and high-dimensional side information. The VAE
replaces individual variational parameters with a data-dependent
function through an inference network parameterized by ϕ, i.e., fϕ,
where µj and σ j are generated as:
µj = µ(fϕ(uj)),
σ j = σ(fϕ(pj)),
∀j = 1, . . . ,m
µm+j = µ(fϕ(zj)),
σm+j = σ(fϕ(zj)),
∀j = 1, . . . ,d.
Putting together pϕ(z | x) and pϕ(u | y) with pθ (x | z) and pθ (y |
u) forms the proposed cVAE (Figure 1).
We follow to derive the Evidence Lower Bound (ELBO):
L(q) = Eqϕ [logpθ (X,Y | U,Z)] −KL
qϕ ∥p(U ,Z)
We use a Monte Carlo gradient estimator to infer the expectation in Equation (3). We draw L samples of uj and zj from qϕ and
perform stochastic gradient ascent to optimize the ELBO. In order
to take gradients with respect to ϕ through sampling, we follow
the reparameterization trick to sample uj and zj as:
= µ(fϕ(yj)) + ϵ(l)
1 ⊙σ(fϕ(yj)),
= µ(fϕ(xj)) + ϵ(l)
2 ⊙σ(fϕ(xj)),
As the KL-divergence can be analytically derived , we can then
rewrite L(q) as:
logpθ (yj | u(l)
logpθ (xj | z(l)
1 + 2 log(σ j) −µ2
We then maximize ELBO given in Equation (4) to learn θ and ϕ.
Implementation details
We discuss the implementation of cVAE in detail. As we feed the
user rating matrix Y and the item side information X through the
same input layer with n neurons, we need to ensure that the input
from both types of information are of the same format. In this
paper, we assume that user ratings are binarized to capture implicit
feedback and that side information is represented as a bag-of-words.
We propose to train cVAEs through a two-phase algorithm. We first
feed it side information to train, which works as pre-training. We
then refine the VAE by feeding user ratings. We follow the typical
setting by taking fθ as a Multi-Layer Perceptron (MLP); fϕ is also
taken to be a MLP of the identical network structure with fθ . We
also introduce two parameters, i.e., α and β, to extend the model
and make it more suitable for the recommendation task.
Parameter α. For both X and Y, most entires are zeros.
We introduce a parameter α to balance between positive samples
and negative samples. Specifically, the loss functions in Equation (1)
and (2) become
logpθ (yj | uj) ≃α
logς(fji) +
log  1 −ς(fji)
logpθ (xj | zj) ≃−α
(1 −fji)2 −1
Parameter β. We can adopt different perspectives about
the ELBO derived in Equation (3) as: the first term can be interpreted as the reconstruction error, while the second KL term can
be viewed as regularization. The ELBO is often over-regularized for
recommendation tasks . Therefore, a parameter β is introduced
to control the strength of regularization, so that the ELBO becomes:
L(q) = Eqϕ [logpθ (X,Y | U,Z)] −βKL
qϕ ∥p(U ,Z)
We propose to train the cVAE in two phases. We first pre-train the
cVAE by feeding it side information only. We then refine the model
by feeding it user ratings. While Liang et al. suggests to set
β small to avoid over-regularization, we opt for a larger value for
β during refinement, for two reasons: (1) the model is effectively
pre-trained with side information; it would be reasonable to require
the posterior to comply more with this prior; and (2) refinement
with user ratings can easily overfit due to the sparsity of ratings; it
would be reasonable to regularize heavier so as to avoid overfitting.
EXPERIMENTS
Experimental setup
Dataset. We conduct experiments on two datasets, Games
and Sports, constructed from different categories of Amazon products . For each category, the original dataset contains transactions between users and items, indicating implicit user feedback.
The statistics of the datasets are presented in Table 2. We use the
product reviews as item featured. We extract unigram features
from the review articles and remove stopwords. We represent each
product item as a bag-of-words feature vector.
Table 2: Statistics of the datasets used.
#Dimension
Methods for comparison. We contrast the performance
of cVAE with that of existing existing VAE-based methods for CF:
cfVAE and rVAE . Note that the performance of cfVAE will
be affected greatly by the high-dimensionality of side information.
Besides, as cfVAE is designed originally for the rating prediction
task, the recommendations provided by cfVAE will be less effective.
While rVAE is effective for top-N recommendation, it suffers from
rating sparsity as side information is not utilized.
We also compare with the state-of-the-art linear model for top-N
recommendation with side information, i.e., cSLIM . By comparing with cSLIM, we can evaluate the capacity of cVAE as it can
be regarded as a deep extension of cSLIM. We also compare with
fVAE, which is the pre-trained model of cVAE with side information
only. Note that cVAE is the refinement over fVAE by user rating.
For all the VAE-based methods, we follow Kingma and Welling
 to set the batch size as 100 so that we can set L = 1. We choose
a two-layer network architecture for the inference network and
generation network. For cfVAE and rVAE, the scale is 200-100 for
inference network and 100-200 for generation network. For fVAE
and cVAE, the scale is 1000-100 and 100-1000, respectively. The
reason that the network scale for cfVAE and rVAE is relatively
smaller is that (1) the input for cfVAE is high-dimensional with
relatively fewer samples; and (2) the input for rVAE is sparse, which
easily overfits for larger network scale. In comparison, we can select
more hidden neurons for fVAE as it takes each dimension of the
features over all items as input, so that the input for the network has
relatively fewer dimensions and the number of samples is sufficient.
This is similar with cVAE, which uses side information to overcome
rating sparsity.
Evaluation method. To evaluate the performance of top-N
recommendation, we split the user rating matrix R into Rtrain,Rvalid
and Rtest, respectively, for training the model, selecting parameters
and testing the recommendation accuracy. Specifically, for each
user, we randomly hold 10% of the ratings in the validation set
and 10% in the test set and put the other ratings in the training
set. For each user, the unrated items are sorted in decreasing order
according to the predicted score and the first N items are returned
as the top-N recommendations for that user.
Given the list of top-N recommended items for user u, Precision
at N (Pre@N) and Recall at N (Rec@N) are defined as
Rre@N = |relevant items ∩recommended items|
Rec@N = |relevant items ∩recommended items|
|relevant items|
Average precision at N (AP@N) is a ranked precision metric that
gives larger credit to correctly recommended items in the top-N
ranks. AP@N is defined as the average of precisions computed at
all positions with an adopted item, namely
k=1 Pre@k × rel(k)
min {N, |relevant items|},
where Pre@k is the precision at cut-offk in the top-N recommended
list. Here, rel(k) is an indicator function
if the item ranked at k is relevant,
otherwise.
Mean average precision at N (MAP@N) is defined as the mean
of the AP scores for all users. Following Wu et al. , the list
of recommended items is evaluated with Rtest using Rec@N and
Experimental results
Parameter selection. To compare the performance of alternative top-N recommendation methods, we first select parameters for all the methods through validation. Specifically, for cSLIM,
we select α, β and λ from 0, 10−4, 10−3, 10−2, 10−1, 1, 10. For cf-
VAE, we select λu from 0, 10−4, 10−3, 10−2, 10−1, 1, 10 and λv from
0, 0.1, 0.2, . . . , 1. For rVAE and fVAE, we select α from 1, 2, . . . , 10
and β from 0, 0.1, 0.2, . . . , 1. For cVAE, we select α from 1, 2, . . . , 10
and β from 0, 0.5, 1, . . . , 3. Note that we tune β with larger values
to possibly regularize heavier during the refinement.
The result of parameter selection is shown in Table 3.
Table 3: Parameter selection.
Parameters
α = 10−2, β = 10−3, λ = 1
α = 0, β = 10, λ = 10
λu = 10, λv = 0.8
λu = 10, λv = 1
α = 4, β = 0.1
α = 8, β = 0.1
α = 6, β = 0.1
α = 5, β = 0.9
α = 2, β = 2
α = 1, β = 2
Performance comparison. We present the results in terms
of Rec@N and MAP@N in Table 4, where N is respectively set as
N = 5, 10, 15, 20. We show the best score in boldface. We attach
asterisks to the best score if the improvement over the second best
score is statistically significant; to this end, we conducted two-sided
tests for the null hypothesis that cVAE and the second best have
identical average values; we use one asterisk if p < 0.1 and two
asterisks if p < 0.05.
As shown in Table 4, cVAE outperforms other methods according to all metrics and on both datasets. The improvement is also
significant in many settings. A general trend is revealed that the
significance of improvements become more evident when N gets
larger. Note that the other three methods utilizing VAE are less
effective with high-dimensional side information. Actually, they
even fail to beat linear models. In contrast, cVAE improves over
cSLIM by using VAE for non-linear low-rank approximation. This
demonstrates the effectiveness of our proposed cVAE model.
Specifically, on the Games dataset, cVAE shows significant improvements over the state-of-the-art methods. Apart from cVAE,
cfVAE provides the best recommendation among all VAE-based CF
methods, although it fails to beat cSLIM. This is followed by fVAE,
which utilizes side information only. rVAE performs the worst, due
to the rating sparsity.
On the Sports dataset, significant improvements can only be
observed for Rec@15 and Rec@20. The results yield an interesting
insight. If we look at the parameter selection for cSLIM, we can
see that α is set to 0, which means cSLIM performs the best recommendation when no side information is utilized. This does not
necessarily mean that the side information of Sports is useless for
recommendation. Actually, fVAE provides acceptable recommendations by utilizing side information only. Therefore, the way of
incorporating side information by cSLIM is not effective. In comparison, cVAE improves over cSLIM by utilizing side information.
Table 4: Comparison of top-N recommendation performance. * indicates a statistically significant difference between cVAE
and the best performing baseline, where * indicates p < 0.1 and ** indicates p < 0.05
Effect of the number of recommended items. Table 4 reveals
a possible trend that the recommendation improvement becomes
more evident when more items are recommended. We use Figure 3
to illustrate this, where N is increased from 5 to 1000.
As depicted in Figure 3(a), the gaps between cVAE and other
methods is getting larger with the growth of N. It is interesting to
note that fVAE surpasses cfVAE when N = 500 and N = 1000. This
further demonstrates the effectiveness of a pre-train phase with
side information proposed in this model.
In Figure 3(c), both fVAE and cfVAE outperform cSLIM when
N ≥200, and fVAE outperforms cfVAE when N ≥100. This shows
that deep models are superior to linear models when more items
are recommended. In comparison, the improvement achieved by
cVAE is more evident when N ≥100, and the gap between cVAE
and the second best method is always substantial.
On the other hand, the performance w.r.t. MAP@N does not
show big differences when N grows. Note that on the Games dataset
(Figure 3(b)), cVAE performs much better than cSLIM when N is
small. The improvement becomes less evident when N grows.
RELATED WORK
We review related work on linear models for top-N recommendation with side information and on deep models for collaborative
filtering.
Top-N recommendation with side
information
Various methods have been developed to incorporate side information in recommender systems. Most of these methods have been
developed in the context of the rating prediction problem, whereas
the top-N recommendation problem has received less attention. In
the rest of this section we only review methods addressing top-N
recommendation problems.
Ning and Karypis propose several methods to incorporate
side information with SLIM . Among all these methods, cSLIM
generally achieves the best performance as it can well compensate
sparse ratings with side information. Zhao and Guo , Zhao et al.
 proposed a joint model to combine self-recovery for user rating
and predication from side information. Side information is also utilized to address cold-start top-N recommendation. Elbadrawy and
Karypis learn feature weights for calculating item similarities.
Sharma et al. further improve over by studying feature
interactions. While these methods generate the state-of-the-art performance for top-N recommendation, they are all linear models,
which have restricted model capacity.
Deep learning for hybrid recommendation
Several authors have attempted to combine deep learning with collaborative filtering. Wu et al. utilize a denoising autoencoder
to encode ratings and recover the score prediction. Zhuang et al.
 propose a dual-autoencoder to learn representations for both
users and items. He et al. generalize matrix factorization for
collaborative filtering by a neural network. These methods utilize
user ratings only, that is, side information is not utilized. Wang
et al. propose stacked denoising autoencoders to learn item
representations from side information and form a collaborative
deep learning method. Later, Li et al. reduce the computational
cost of training by replacing stacked denoising autoencoders by
a marginalized denoising autoencoder. Rather than manually corrupt input, variational autoencoders were later utilized for representation learning . These models achieve state-of-the-art performance among hybrid recommender systems, but they are less
effective when side information is high-dimensional. For more discussions on deep learning based recommender systems, we refer to
a recent survey .
CONCLUSION
In this paper, we have proposed an alternative way to feed side information to neural network so as to overcome the high-dimensionality.
We propose collective Variational Autoencoder (cVAE), which can
be regarded as the non-linear generalization of cSLIM. cVAE overcomes rating sparsity by feeding both ratings and side information
into the same inference network and generation network. To cater
for the heterogeneity of information (rating and side information),
we assume different sources of information to follow different distributions, which is reflected in the use of different loss function.
(a) Rec@N, Games
(b) MAP@N, Games
(c) Rec@N, Sports
(d) MAP@N, Sports
Figure 3: Performance of top-N recommendation
As for the implementation, we introduce a parameter α to balance
the positive samples and negative samples. We also introduce β
as the parameter for regularization, which controls how much the
latent variable should be complied with the prior distribution. We
conduct experiments over Amazon datasets. The results show the
superiority of cVAE over other methods under the scenario with
high-dimensional side information.
In conclusion, deep models are effective as long as the number
of inputs are sufficient. Thus, using side information to pre-train
cVAE helps to overcome the high-dimensionality. A general ruleof-thumb is, regularizing cVAE lightly during pre-train and heavily
during the refinement of training.