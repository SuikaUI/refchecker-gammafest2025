BEIT: BERT Pre-Training of Image Transformers
Hangbo Bao†∗, Li Dong‡, Songhao Piao†, Furu Wei‡
† Harbin Institute of Technology
‡ Microsoft Research
 
We introduce a self-supervised vision representation model BEIT, which stands
for Bidirectional Encoder representation from Image Transformers. Following
BERT [DCLT19] developed in the natural language processing area, we propose a
masked image modeling task to pretrain vision Transformers. Speciﬁcally, each
image has two views in our pre-training, i.e., image patches (such as 16×16 pixels),
and visual tokens (i.e., discrete tokens). We ﬁrst “tokenize” the original image into
visual tokens. Then we randomly mask some image patches and fed them into
the backbone Transformer. The pre-training objective is to recover the original
visual tokens based on the corrupted image patches. After pre-training BEIT, we
directly ﬁne-tune the model parameters on downstream tasks by appending task
layers upon the pretrained encoder. Experimental results on image classiﬁcation
and semantic segmentation show that our model achieves competitive results with
previous pre-training methods.
Introduction
Transformer [VSP+17] has achieved promising performance in computer vision [DBK+20,
TCD+20]. However, empirical studies show that vision Transformers require more training data than
convolutional neural networks. In order to solve the data-hungry issue [LSB+21], self-supervised
pre-training is a promising solution to leverage large-scale image data. Several strands of methods
have been explored for vision Transformers, such as contrastive learning [CXH21, XLY+21], and
self-distillation [CTM+21].
Concurrently, BERT [DCLT19] has achieved great success in natural language processing. Its
masked language modeling task ﬁrst randomly masks some proportion of tokens within a text, and
then recovers the masked tokens based on the Transformer encoding results of the corrupted text.
Motivated by BERT, we turn to the denoising auto-encoding idea to pretrain vision Transformers,
which has not been well studied by the vision community. It is challenging to directly apply BERTstyle pre-training for image data. First of all, there is no pre-exist vocabulary for vision Transformer’s
input unit, i.e., image patches. So we cannot simply employ a softmax classiﬁer to predict over all
possible candidates for masked patches. In contrast, the language vocabulary, such as words and
BPE [SHB16], is well-deﬁned and eases auto-encoding prediction. A straightforward alternative
is regarding the task as a regression problem, which predicts the raw pixels of masked patches.
However, such pixel-level recovery task tends to waste modeling capability on pre-training shortrange dependencies and high-frequency details [RPG+21]. Our goal is to overcome the above issues
for pre-training of vision Transformers.
In this work, we introduce a self-supervised vision representation model BEIT, which stands for
Bidirectional Encoder representation from Image Transformers. Inspired by BERT, we propose a
pre-training task, namely, masked image modeling (MIM). As shown in Figure 1, MIM uses two
∗Contribution during internship at Microsoft. Correspondence to: Li Dong< >, Furu
Wei< >
 
123 234 456 567
987 876 765 543
112 223 334 445
211 322 433 544
BEIT Encoder
Visual Tokens
Masked Image Modeling Head
Reconstructed
Unused During
Pre-Training
Figure 1: Overview of BEIT pre-training. Before pre-training, we learn an “image tokenizer” via
autoencoding-style reconstruction, where an image is tokenized into discrete visual tokens according
to the learned vocabulary. During pre-training, each image has two views, i.e., image patches, and
visual tokens. We randomly mask some proportion of image patches (gray patches in the ﬁgure) and
replace them with a special mask embedding [M]. Then the patches are fed to a backbone vision
Transformer. The pre-training task aims at predicting the visual tokens of the original image based
on the encoding vectors of the corrupted image.
views for each images, i.e., image patches, and visual tokens. We split the image into a grid of patches
that are the input representation of backbone Transformer. Moreover, we “tokenize” the image to
discrete visual tokens, which is obtained by the latent codes of discrete VAE [RPG+21]. During
pre-training, we randomly mask some proportion of image patches, and feed the corrupted input to
Transformer. The model learns to recover the visual tokens of the original image, instead of the raw
pixels of masked patches.
We perform self-supervised learning and then ﬁne-tune the pretrained BEIT on two downstream
tasks, i.e., image classiﬁcation, and semantic segmentation. Experimental results indicate that BEIT
outperforms both from-scratch training and previous strong self-supervised models. Moreover, BEIT
is complementary to supervised pre-training. Performance of BEIT can be further improved by
intermediate ﬁne-tuning with ImageNet labels. Ablation studies show that our proposed techniques
are critical to the effectiveness of BERT-style pre-training for image data. Apart from performance,
the improvements of convergence speed and stability of ﬁne-tuning reduce training costs on end tasks.
In addition, we demonstrate that self-supervised BEIT can learn reasonable semantic regions via
pre-training, unleashing the rich supervision signals contained in images.
Our contributions are summarized as follows:
• We propose a masked image modeling task to pretrain vision Transformers in a self-supervised
manner. We also provide a theoretical explanation from the perspective of variational autoencoder.
• We pretrain BEIT and conduct extensive ﬁne-tuning experiments on downstream tasks, such as
image classiﬁcation, and semantic segmentation.
• We present that the self-attention mechanism of self-supervised BEIT learns to distinguish
semantic regions and object boundaries, although without using any human annotation.
Given an input image x, BEIT encodes it to contextualized vector representations. As shown
in Figure 1, BEIT is pretrained by the masked image modeling (MIM) task in a self-supervised
learning manner. MIM aims at recovering the masked image patches based on encoding vectors. For
downstream tasks (such as image classiﬁcation, and semantic segmentation), we append task layers
upon pretrained BEIT and ﬁne-tune the parameters on the speciﬁc datasets.
Image Representations
The images have two views of representations in our method, namely, image patch, and visual tokens.
The two types serve as input and output representations during pre-training, respectively.
Image Patch
The 2D image is split into a sequence of patches [DBK+20], so that a standard Transformer can
directly accept image data. Formally, we reshape the image x ∈RH×W ×C into N = HW/P 2
patches xp ∈RN×(P 2C), where C is the number of channels, (H, W) is the input image resolution,
and (P, P) is the resolution of each patch. The image patches {xp
i=1 are ﬂattened into vectors
and are linearly projected, which is similar to word embeddings in BERT [DCLT19]. Image patches
preserve raw pixels and are used as input features in BEIT.
In our experiments, we split each 224 × 224 image into a 14 × 14 grid of image patches, where each
patch is 16 × 16.
Visual Token
Similar to natural language, we represent the image as a sequence of discrete tokens obtained by an
“image tokenizer”, instead of raw pixels. Speciﬁcally, we tokenize the image x ∈RH×W ×C into
z = [z1, . . . , zN] ∈Vh×w, where the vocabulary V = {1, . . . , |V|} contains discrete token indices.
Following [RPG+21], we use the image tokenizer learned by discrete variational autoencoder (dVAE).
There are two modules during visual token learning, namely, tokenizer and decoder. The tokenizer
qφ(z|x) maps image pixels x into discrete tokens z according to a visual codebook (i.e., vocabulary).
The decoder pψ(x|z) learns to reconstruct the input image x based on the visual tokens z. The
reconstruction objective can be written as Ez∼qφ(z|x)[log pψ(x|z)]. Because the latent visual tokens
are discrete, the model training is non-differentiable. Gumbel-softmax relaxation [JGP17, MMT17]
is employed to train the model parameters. Moreover, a uniform prior is put on qφ during dVAE
training. Refer to [RPG+21] for more training details of the image tokenizer.
We tokenize each image to a 14 × 14 grid of visual tokens. Notice the number of visual tokens and
the number of image patches for one image are the same. The vocabulary size is set to |V| = 8192.
In our work, we directly use the publicly available2 image tokenizer described in [RPG+21]. We also
compare it with a re-implemented tokenizer in Appendix C.
Backbone Network: Image Transformer
Following ViT [DBK+20], we use the standard Transformer [VSP+17] as the backbone network. So
the results can be directly compared with previous work in terms of the network architecture.
The input of Transformer is a sequence of image patches {xp
i=1. The patches are then linearly
projected to obtain patch embeddings Exp
i , where E ∈R(P 2C)×D. Moreover, we prepend a
special token [S] to the input sequence. We also add standard learnable 1D position embeddings
Epos ∈RN×D to patch embeddings. The input vectors H0 = [e[S], Exp
i , . . . , Exp
N] + Epos is fed
into Transformer. The encoder contains L layers of Transformer blocks Hl = Transformer(Hl−1),
where l = 1, . . . , L. The output vectors of the last layer HL = [hL
1 , . . . , hL
N] are used as the
encoded representations for the image patches, where hL
i is the vector of the i-th image patch.
Pre-Training BEIT: Masked Image Modeling
We propose a masked image modeling (MIM) task. We randomly mask some percentage of image
patches, and then predict the visual tokens that are corresponding to the masked patches.
2 
Figure 1 shows the overview of our method. As presented in Section 2.1, given an input image
x, we split it into N image patches ({xp
i=1), and tokenize it to N visual tokens ({zi}N
randomly mask approximately 40% image patches, where the masked positions are denoted as
M ∈{1, . . . , N}0.4N. Next we replace the masked patches with a learnable embedding e[M] ∈RD.
The corrupted image patches xM = {xp
i : i /∈M}N
S{e[M] : i ∈M}N
i=1 are then fed into the
L-layer Transformer as described in Section 2.2. The ﬁnal hidden vectors {hL
i=1 are regarded as
encoded representations of the input patches. For each masked position {hL
i : i ∈M}N
i=1, we use a
softmax classiﬁer to predict the corresponding visual tokens pMIM(z′|xM) = softmaxz′(WchL
where xM is the corrupted image, Wc ∈R|V|×D, and bc ∈R|V|. The pre-training objective is to
maximize the log-likelihood of the correct visual tokens zi given the corrupted image:
log pMIM(zi|xM)
where D is the training corpus, M represents randomly masked positions, and xM is the corrupted
image that is masked according to M.
Algorithm 1 Blockwise Masking
Input: N(= h × w) image patches
Output: Masked positions M
s ←Rand(16, 0.4N −|M|)
▷Block size
r ←Rand(0.3,
▷Aspect ratio of block
a ←√s · r; b ←
t ←Rand(0, h −a) ; l ←Rand(0, w −b)
M ←M S{(i, j) : i ∈[t, t + a), j ∈[l, l + b)}
until |M| > 0.4N
▷Masking ratio is 40%
Rather than randomly choosing patches
for the masked positions M, we employ
blockwise masking in our work. As summarized in Algorithm 1, a block of image
patches is masked each time. For each
block, we set the minimum number of
patches to 16. Then we randomly choose
an aspect ratio for the masking block. We
repeat the above two steps until obtaining
enough masked patches, i.e., 0.4N, where
N is the total number of image patches,
and 0.4 is masking ratio.
The MIM task is greatly inspired by masked language modeling [DCLT19], which is one of the most
successful pre-training objective in natural language processing. Moreover, blockwise (or n-gram)
masking is also widely applied in BERT-like models [JCL+20, BDW+20, RSR+20]. However,
directly using pixel-level auto-encoding (i.e., recovering the pixels of masked patches) for vision pretraining pushes the model to focus on short-range dependencies and high-frequency details [RPG+21].
BEIT overcomes the above issue by predicting discrete visual tokens, which summarizes the details
to high-level abstractions. Ablation studies in Section 3.3 show that our proposed method signiﬁcantly
outperforms pixel-level auto-encoding.
From the Perspective of Variational Autoencoder
The BEIT pre-training can be viewed as variational autoencoder [KW14] training. Let x denote the
original image, ˜x the masked image, and z the visual tokens. Considering the evidence lower bound
(ELBO) of the log-likelihood p(x|˜x), i.e., recovering the original image from its corrupted version:
(xi,˜xi)∈D
log p(xi|˜xi) ≥
(xi,˜xi)∈D
 Ezi∼qφ(z|xi)[log pψ(xi|zi)]
Visual Token Reconstruction
−DKL[qφ(z|xi), pθ(z|˜xi)]
where (1) qφ(z|x) denotes the image tokenizer that obtains visual tokens; (2) pψ(x|z) decodes the
original image given input visual tokens; (3) pθ(z|˜x) recovers the visual tokens based on the masked
image, which is our MIM pre-training task.
We learn the model following a two-stage procedure similar to [vdOVK17, RvdOV19]. In the ﬁrst
stage, we obtain the image tokenizer as a discrete variational autoencoder [RPG+21]. Speciﬁcally,
the ﬁrst stage minimizes the reconstruction loss −Ezi∼qφ(z|xi)[log pψ(xi|zi)] with an uniform prior
as described in Equation (2). In the second stage, we learn the prior pθ while keeping qφ and
pψ ﬁxed. We simplify qφ(z|xi) to a one-point distribution with the most likely visual tokens
ˆzi = arg maxz qφ(z|xi). Then Equation (2) can be rewritten as:
(xi,˜xi)∈D
 Ezi∼qφ(z|xi)[log pψ(xi|zi)]
Stage 1: Visual Token Reconstruction
log pθ(ˆzi|˜xi)
Stage 2: Masked Image Modeling
where the second term is our BEIT pre-training objective.
Pre-Training Setup
The network architecture of BEIT follows that of ViT-Base [DBK+20] for a fair comparison. We
use a 12-layer Transformer with 768 hidden size, and 12 attention heads. The intermediate size of
feed-forward networks is 3072. We employ the default 16 × 16 input patch size. We directly borrow
the image tokenizer trained by [RPG+21]. The vocabulary size of visual tokens is 8192.
We pretrain BEIT on the training set of ImageNet-1K [RDS+15], which contains about 1.2M
images. Our augmentation policy includes random resized cropping, horizontal ﬂipping, color
jittering [WXYL18]. Notice that we do not use the labels for self-supervised learning. We use the
224 × 224 resolution in our experiments. So the input is split to 14 × 14 image patches, and the same
amount of visual tokens. We randomly mask at most 75 patches (i.e., roughly 40% of total image
The pre-training runs for about 500k steps (i.e., 800 epochs) with 2k batch size. Adam [LH19] with
β1 = 0.9, β2 = 0.999 is employed for optimization. The learning rate is set to 1.5e-3, with a warmup
of 10 epochs, and cosine learning rate decay. The weight decay is 0.05. We employ stochastic
depth [HSL+16] with a 0.1 rate, and disable dropout. The 500k training steps take about ﬁve days
using 16 Nvidia Telsa V100 32GB GPU cards.
We ﬁnd that proper initialization is important to stabilize Transformer, especially for large-scale pretraining. We ﬁrst randomly initialize all the parameters within a small range, such as [−0.02, 0.02].
Then, for the l-th Transformer layer, we rescale the output matrices (i.e., the last linear projection
within each sub-layer) of the self-attention module and the feed-forward network by
Fine-Tuning BEIT on Downstream Vision Tasks
After pre-training BEIT, we append a task layer upon the Transformer, and ﬁne-tune the parameters
on downstream tasks, like BERT. We take image classiﬁcation and semantic segmentation as examples
in our work. It is straightforward to leverage the pre-training-then-ﬁne-tuning paradigm on other
vision tasks with BEIT.
Image classiﬁcation.
For image classiﬁcation tasks, we directly employ a simple linear classiﬁer as the task layer.
Speciﬁcally, we use average pooling to aggregate the representations, and feed the global to a softmax classiﬁer.
The category probabilities are computed
as softmax(avg({hL
i=1Wc)), where hL
i is the ﬁnal encoding vector of the i-th image patch,
Wc ∈RD×C is a parameter matrix, and C is the number of labels. We maximize the likelihood of
labeled data by updating the parameters of BEIT and the softmax classiﬁer.
Semantic segmentation.
For semantic segmentation, we follow the task layer used in SETR-
PUP [ZLZ+20]. To be speciﬁc, we use pretrained BEIT as a backbone encoder, and incorporate
several deconvolution layers as decoder to produce segmentation. The model is also end-to-end
ﬁne-tuned similar to image classiﬁcation.
Intermediate ﬁne-tuning.
After self-supervised pre-training, we can further train BEIT on a datarich intermediate dataset (i.e., ImageNet-1K in our work), and then ﬁnetune the model on the target
downstream tasks. Such intermediate ﬁne-tuning is the common practice of BERT ﬁne-tuning in
NLP [PPL+20]. We directly follow the method for BEIT.
Experiments
We conduct full ﬁne-tuning experiments on image classiﬁcation and semantic segmentation. Moreover,
we present various ablation studies for pre-training and analyze the representations learned by BEIT.
We also report linear probes on ImageNet in Appendix D.
Image Classiﬁcation
The image classiﬁcation task classiﬁes input images to various categories. We evaluate BEIT on the
ILSVRC-2012 ImageNet dataset [RDS+15] with 1k classes and 1.3M images. We directly follow
the most of hyperparameters of DeiT [TCD+20] in our ﬁne-tuning experiments for a fair comparison.
We reduce ﬁne-tuning epochs compared with training from scratch, as BEIT has been pre-trained.
Accordingly, we use a larger learning rate with layer-wise decay. The detailed hyperparameters are
summarized in Appendix H.
Table 1 reports top-1 accuracy on image classiﬁcation. We compare BEIT with vision Transformers
trained by random initialization, supervised pre-training, and previous self-supervised learning
methods. All the compared models are base-size, except iGPT has 1.36B parameters. Pre-training is
conducted on ImageNet for the comparison purpose, except ViT-JFT300M is pretrained on Google’s
in-house 300M images.
Compared with the models trained by random initialization, we ﬁnd that pre-trained BEIT signiﬁcantly improves performance on both datasets. BEIT improves the performance on ImageNet, which
shows the effectiveness under the rich-resource setting.
Moreover, we compare BEIT with previous state-of-the-art self-supervised methods for Transformer,
such as DINO [CTM+21], and MoCo v3 [CXH21]. Our proposed method outperforms previous
models on ImageNet ﬁne-tuning. Among them, iGPT-1.36B [CRC+20] uses much more parameters
(i.e., 1.36B vs 86M), and ViT-JFT300M [DBK+20] is pretrained on larger corpus (i.e., 300M vs
1.3M), while others pretrain ViT-Base on ImageNet-1K. iGPT-1.36B and ViT-JFT300M are the
most comparable methods, which also follows auto-encoding pre-training for vision Transformer.
Speciﬁcally, iGPT uses clustered image tokens as both input and output for image GPT or image
BERT. In contrast, we use image patches as input to preserve raw pixels, and employ discrete visual
tokens as a prediction bottleneck. ViT-JFT300 predicts the mean, 3-bit color of each masked patch,
rather than visual tokens learned by discrete VAE. We also pretrain the self-supervised tasks of BEIT
and DINO in a multi-task learning manner, which is presented in Appendix E.
In addition, we evaluate our proposed method with intermediate ﬁne-tuning. In other words, we ﬁrst
pretrain BEIT in a self-supervised manner, and then ﬁne-tune the pretrained model on ImageNet with
labeled data. The results show that BEIT is complementary to supervised pre-training, achieving
additional gain after intermediate ﬁne-tuning on ImageNet.
Fine-tuning to 384 × 384 resolution.
After ﬁne-tuning with resolution 224 × 224, we additionally
ﬁne-tune the model on 384×384 images by 10 more epochs. We follow the standard higher-resolution
setting of DeiT [TCD+20], except using fewer epochs. Notice that we keep patch size the same for
both 224 × 224 and 384 × 384 images. So the input sequence length of Transformers becomes longer
for higher resolutions. Table 1 shows that higher resolution improves the BEIT results by 1+ points
on ImageNet. More importantly, BEIT384 pretrained on ImageNet-1K even outperforms supervised
pre-training ViT384 that uses ImageNet-22K, when they use the same input resolution.
Scaling up to larger size.
We further scale up BEIT to the large size (same as ViT-L). As shown in
Table 1, ViT384-L is worse than ViT384 on ImageNet, when training from scratch. The results veriﬁes
the data-hungry issue of vision Transformers. Supervised pre-training on ImageNet-22K partially
relieves the issue, where ViT384-L ﬁnally outperforms ViT384 by 1.2. In comparison, BEIT-L is
better than BEIT by 2.0, and BEIT384-L outperforms BEIT384 by 1.7. In other words, the beneﬁts
of scaling up BEIT from base to large are greater than supervised pre-training with ImageNet-22K.
More importantly, comparing between BEIT384 with ViT384 that conducts supervised pre-training
on ImageNet-22K, the improvements of BEIT become greater along with scaling the size from base
(i.e., 0.6) to large (i.e., 1.1). The results suggest that BEIT tends to help more for extremely larger
models (such as 1B, or 10B), especially when labeled data are insufﬁcient3 to conduct supervised
pre-training4 for such large models.
3[ZKHB21] report that supervised pre-training of a 1.8B-size vision Transformer requires billions of labeled
4Appendix B shows that BEIT ﬁne-tuned on ImageNet-22K (14M) can match the performance of supervised
pre-training on Google’s in-house JFT-3B [ZKHB21], while using 214x less labels. We also demonstrate that
large-size BEIT ﬁne-tuned on 70M labeled images can achieve 89.5% top-1 accuracy on ImageNet and 58.4%
mIoU on ADE20K, creating new state-of-the-art results for large-size vision Transformers.
Model Size
Resolution
Training from scratch (i.e., random initialization)
ViT384-B [DBK+20]
ViT384-L [DBK+20]
DeiT-B [TCD+20]
DeiT384-B [TCD+20]
Supervised Pre-Training on ImageNet-22K (using labeled data)
ViT384-B [DBK+20]
ViT384-L [DBK+20]
Self-Supervised Pre-Training on ImageNet-1K (without labeled data)
iGPT-1.36B† [CRC+20]
ViT384-B-JFT300M‡ [DBK+20]
MoCo v3-B [CXH21]
MoCo v3-L [CXH21]
DINO-B [CTM+21]
BEIT-B (ours)
BEIT384-B (ours)
BEIT-L (ours)
BEIT384-L (ours)
Table 1: Top-1 accuracy on ImageNet-1K. We evaluate base- (“-B”) and large-size (“-L”) models at
resolutions 224 × 224 and 384 × 384. †: iGPT-1.36B contains 1.36 billion parameters, while others
are base-size models. ‡: ViT384-B-JFT300M is pretrained with the “masked patch prediction” task
on Google’s in-house 300M images, while others use ImageNet.
Top-1 Acc.
DeiT (Training from scratch)
BEiT (Fine-tuning)
Table 2: Convergence curves of training
DeiT from scratch and ﬁne-tuning BEIT on
ImageNet-1K.
Supervised Pre-Training on ImageNet
DINO [CTM+21]
BEIT (ours)
BEIT + Intermediate Fine-Tuning (ours)
Results of semantic segmentation on
ADE20K. We use SETR-PUP [ZLZ+20] as the task
layer and report results of single-scale inference.
Convergence curves.
Figure 2 compares the convergence curves of the training-from-scratch and
pre-training-then-ﬁne-tuning paradigms. We ﬁnd that ﬁne-tuning BEIT not only achieves better
performance, but also converging much faster than training DeiT from scratch. Moreover, ﬁne-tuning
BEIT can reach reasonable numbers within very few epochs.
Semantic Segmentation
Semantic segmentation aims to predict a corresponding class for each pixel of the input image. We
evaluate BEIT on the ADE20K benchmark [ZZP+19] with 25K images and 150 semantic categories.
We report the metric of mean Intersection of Union (mIoU) averaged over all semantic categories. As
presented in Section 2.6, we directly follow the task layer and the most of hyperparameters described
in SETR-PUP [ZLZ+20]. On ADE20K, we use Adam [LH19] as the optimizer. The learning rate is
set to 1e-3 with layer-wise decay similar to image classiﬁcation. We conduct ﬁne-tuning for 160K
steps. The batch size is 16. The detailed hyperparameters are described in Appendix I.
As shown in Table 3, we compare BEIT with supervised pre-training that relies on labeled data
of ImageNet. We ﬁnd that our proposed method achieves better performance than supervised pretraining, although BEIT does not require manual annotations for pre-training. Moreover, we employ
BEIT (300 Epochs)
−Blockwise masking
−Visual tokens (i.e., recover masked pixels)
−Visual tokens −Blockwise masking
+ Recover 100% visual tokens
−Masking + Recover 100% visual tokens
Pretrain longer (800 epochs)
Table 4: Ablation studies for BEIT pre-training on image classiﬁcation and semantic segmentation.
intermediate ﬁne-tuning for BEIT on ImageNet, i.e., we ﬁrst ﬁne-tune pretrained BEIT on ImageNet,
and then ﬁne-tune the model on ADE20K. The results indicate that intermediate ﬁne-tuning further
improves BEIT on semantic segmentation.
Ablation Studies
We conduct ablation studies to analyze the contributions of each component in BEIT. The models
are evaluated on image classiﬁcation (i.e., ImageNet) and semantic segmentation (i.e., ADE20K). We
set the default pre-training steps to 300 epochs for the ablation studies, which is 37.5% of the total
steps used in the previous experiments.
Table 4 reports the results of various model variants. First, we ablate blockwise masking by randomly
sample masked positions. We ﬁnd that blockwise masking is beneﬁcial on both tasks, especially on
semantic segmentation. Second, we ablate the usage of visual tokens by predicting the raw pixels of
masked patches, i.e., the pre-training task becomes a pixel regression problem to recover masked
patches. Our proposed masked image modeling task signiﬁcantly outperforms naive pixel-level
auto-encoding. Compared with the results in Table 1, the ablation result is worse than training vision
Transformer from scratch on two tasks. The results indicate that the prediction of visual tokens is the
key ingredient of BEIT. Third, we ablate the usage of visual tokens and blockwise masking together.
We ﬁnd that blockwise masking is even more helpful for pixel-level auto-encoding, which relieves
the suffering of short-distance dependency. Forth, recovering all the visual tokens harms performance
on downstream tasks. Fifth, we compare BEIT with different training steps. Pre-training the model
longer can further improve performance on downstream tasks.
Analysis of Self-Attention Map
We show that the self-attention mechanism in BEIT can separate objects, even though our pre-training
does not rely on any manual annotation at all. Similar properties are also observed by [CTM+21].
The probing images are taken from the MS COCO [LMB+14] corpus to avoid appearing in the
pre-training data.
As shown in Figure 2, we plot the self-attention map for different reference points within an image.
The visualizations are produced by attention scores computed via query-key product in the last layer.
For each reference point, we use the corresponding patch as query, and show which patch it attends
to. After pre-training, BEIT learns to distinguish semantic regions using self-attention heads, without
any task-speciﬁc supervision. The property partially indicates the reason why BEIT is able to help
downstream tasks. Such knowledge acquired by BEIT potentially improves the generalization ability
of ﬁne-tuned models, especially on small-scale datasets.
Related Work
Self-supervised visual representation learning.
Various methods have been introduced over the
years to pretrain vision models in a self-supervised manner. Pioneering works design clever pretext
tasks, such as predicting the patch orderings [NF16], colorization [ZIE16], and predicting rotation
angles [KG18]. In addition, [TLL19] propose to mask some patches within an image, and classify
whether the masked patches are real or fake for each masked position. The method is similar to the
Figure 2: Self-attention map for different reference points. The self-attention mechanism in BEIT is
able to separate objects, although self-supervised pre-training does not use manual annotations.
masked version of Jigsaw pre-training [NF16]. The recent strand of research follows contrastive
paradigm [WXYL18, OLV18, HFLM+19, BHB19, HFW+20, CKNH20, CFGH20]. The models
typically regard various data augmentations as different views of an image, and then make the
representations of positive pairs similar while pushing negative pairs away. In order to obtain enough
informative negative samples in contrastive learning, the methods usually rely on large memory
banks [WXYL18, HFW+20] or large batch size [CKNH20]. BYOL [GSA+20] and SimSiam [CH20]
further eliminate the requirement of negative samples, using various techniques to avoid representation
collapse. Another strand of methods use clustering to organize image examples [CBJD18, ARV20,
CMM+20, LZXH21].
Self-supervised vision Transformers.
Pre-training vision Transformers has received signiﬁcant
attention recently due to the data-hungry issue. iGPT [CRC+20] ﬁrst creates a 9-bit color palette
by k-means clustering RGB pixels, and then uses the clustered tokens to represent images. Next
iGPT uses the tasks of BERT and GPT to pretrain Transformers. In comparison, our proposed
method uses image patches as input without losing pixel-level information. Moreover, our visual
tokens are obtained by discrete VAE instead of clustering. ViT [DBK+20] conducts a preliminary
exploration with the masked patch prediction task, which predicts the 3-bit mean color of the masked
patches. [DBK+20] also report that pixel-level auto-encoding performs worse, although it is the
most straightforward translation of BERT from NLP to CV. Rather than using heuristically designed
pre-training tasks, our proposed model leverages visual tokens learned by discrete VAE, which not
only achieves better performance but also is better theoretically motivated. Apart from masked
auto-encoding, other mainstream research works use contrastive learning [CXH21, XLY+21], and
self-distillation [CTM+21]. In comparison, BEIT can achieve several times of improvement in terms
of pre-training throughput (Appendix E), and memory consumption. The advantages make BEIT
appealing to scale up vision Transformers.
Conclusion
We introduce a self-supervised pre-training framework for vision Transformers, achieving strong
ﬁne-tuning results on downstream tasks, such as image classiﬁcation, and semantic segmentation.
We show that the proposed method is critical to make BERT-like pre-training (i.e., auto-encoding
with masked input) work well for image Transformers. We also present the intriguing property
of automatically acquired knowledge about semantic regions, without using any human-annotated
data. In the future, we would like to scale up BEIT pre-training in terms of data size and model
size. Moreover, we will conduct multimodal pre-training in a more uniﬁed way, using the similar
objectives and the shared architecture for texts and images.
Acknowledgement
We would like to acknowledge Yue Cao, Han Hu, Hang Hua, Jingdong
Wang, Zheng Zhang for the helpful discussions, and Yaru Hao for some analysis experiments
using [HDWX20].