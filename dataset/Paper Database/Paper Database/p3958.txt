3D Shape Generation and Completion through Point-Voxel Diffusion
Linqi Zhou
Stanford University
Stanford University
!"#$%&'%(%)#*+,(
-./*+0-,1#/&!"#$%&2,3$/%*+,(
Figure 1: The proposed Point-Voxel Diffusion (PVD) is a new framework for generative modeling of 3D shapes. Left: tables, cars, and
planes generated by our PVD. It learns to sample from a Gaussian prior and to progressively remove noise to obtain sharp shapes. Right:
two possible shapes completed from a real RGB-D image, each visualized in input and canonical views.
We propose a novel approach for probabilistic generative modeling of 3D shapes. Unlike most existing models
that learn to deterministically translate a latent vector to a
shape, our model, Point-Voxel Diffusion (PVD), is a unified,
probabilistic formulation for unconditional shape generation and conditional, multi-modal shape completion. PVD
marries denoising diffusion models with the hybrid, pointvoxel representation of 3D shapes. It can be viewed as a
series of denoising steps, reversing the diffusion process
from observed point cloud data to Gaussian noise, and is
trained by optimizing a variational lower bound to the (conditional) likelihood function. Experiments demonstrate that
PVD is capable of synthesizing high-fidelity shapes, completing partial point clouds, and generating multiple completion results from single-view depth scans of real objects.
1. Introduction
Generative modeling of 3D shapes has extensive applications across vision, graphics, and robotics. To perform
Project page at 
well in these downstream applications, a good 3D generative models should be faithful and probabilistic. A faithful model generates shapes that are realistic to humans and,
in cases where conditional inputs such as depth maps are
available, respects such partial observations. A probabilistic model captures the under-determined, multi-modal nature of the generation and completion problem: it may sample and produce diverse shapes from scratch or from partial
observations. As shown in Figure 1, when only the back of
a chair is visible, good generative models should be able to
produce multiple possible completed chairs, including those
with arms and those without.
Existing shape generation models can be roughly divided into two categories. The first operates on 3D voxels , a natural extension of 2D pixels.
While being straightforward to use, voxels demand prohibitively large memory when scaled to high dimensions,
and are thus unlikely to produce results with high fidelity.
The second class of models studies point cloud generation and has produced promising results. While being more faithful, these approaches typically
view point cloud generation as a point generation process
conditioned on shape encoding, which is obtained by deterministic encoders. When performing shape completion,
 
these approaches are therefore unable to capture the multimodal nature of the completion problem.
Recently, a new class of generative models, named probabilistic diffusion models, have achieved impressive performance on 2D image generation .
These approaches learn a probabilistic model over a denoising process. Diffusion is supervised to gradually denoise a Gaussian noise to a target output, such as an image. Methods
along this line, such as DDPM , are inherently probabilistic and produce highly realistic 2D images.
Extending diffusion models to 3D is, however, technically highly nontrivial: a direct application of diffusion
models on either voxel and point representation results in
poor generation quality. This is because, first, pure voxels are binary and therefore not suitable for the probabilistic nature of diffusion models; second, point clouds
demand permutation-invariance, which imposes infeasible
constraints on the model. Experiments in Section 4.1 also
verifies that a straightforward extension does not lead to reasonable results.
We propose Point-Voxel Diffusion (PVD), a probabilistic and flexible shape generation model that tackles the above challenges by marrying denoising diffusion
models with the hybrid, point-voxel representation of 3D
shapes . A point-voxel representation builds structured
locality into point cloud processing; integrated with denoising diffusion models, PVD suggests a novel, probabilistic
way to generate high-quality shapes by denoising a Gaussian noise and to produce multiple completion results from
a partial observation, as shown in Figure 1.
A unique strength of PVD is that it is a unified, probabilistic formulation for unconditional shape generation and
conditional, multi-modal shape completion. While multimodal shape completion is a highly desirable feature in
applications such as digital design or robotics, past works
on shape generation primarily use deterministic shape encoders and decoders to output a single possible completion
in voxels or a point cloud. In contrast, PVD can perform
both unconditional shape generation and conditional shape
completion in an integrated framework, requiring only minimal modifications to the training objective. It is thus capable of sampling multiple completion results depending on
diffusion initialization.
Experiments demonstrate that PVD is capable of synthesizing high-fidelity shapes, outperforming multiple stateof-the-art methods. PVD also delivers high-quality results
on multi-modal shape completion from partial observations
such as a partial point cloud or a depth map. In particular, we show that PVD does well on multi-modal completion on multiple synthetic and real datasets, including
ShapeNet , PartNet , and single-view depth scans of
real objects in the Redwood dataset .
2. Related Works
Point cloud generative models.
Many prior works
have explored point cloud generation in terms of autoencoding , single-view reconstruction , and adversarial generation . Many of
them rely on directly optimizing heuristic loss functions
such as Chamfer Distance (CD) and Earth Mover’s Distance
(EMD), which are also used to evaluate generative quality.
Some recent works take a different approach, viewing the 3D point clouds in light of probabilistic distributions.
For example, Sun et al. view the point
clouds from a probabilistic perspective and introduce autoregressive generation, but doing so requires ordering of
the point clouds. GAN-based models and flow-based models also adopt a probabilistic view but
separate shape-level distribution from point-level distribution. Among these models, PointFlow applies normalizing
flow to 3D point clouds, and Discrete PointFlow follows up using discrete normalizing flow with affine coupling layers . Shape Gradient Fields , unlike flowbased works, directly learn a gradient field that samples
point clouds using Langevin dynamics. Our model is different from these models in that we do not distinguish point
and shape distributions, and that we directly generate entire
shapes starting from random noise.
Point-voxel representation.
3D shapes were conventionally rasterized into voxel grids and processed using 3D convolution . Due to the correspondence between voxels and 2D pixels, many works have explored voxel-based
classification and segmentation using volumetric convolution . Voxel-based generative models have similarly proven successful . However,
voxel grids are memory-intensive and they grow cubically
with increase in dimension, so they cannot be scaled to a
high resolution.
Point clouds, on the other hand, are detailed samples
from smooth surfaces and do not suffer from the grid effect
of usually low-resolution voxels and do not require as much
memory for processing. Researchers have explored point
cloud classification and segmentation and most
assume point cloud processing networks are permutationinvariant. Permutation-invariance is a strong condition to
be imposed on the architecture and we empirically find
that direct extension of 2D methods to either permutationinvariant point clouds or voxels do not work well. We therefore explore a separate point-voxel representation ,
and our work is most related to point-voxel CNN ,
which proposes to voxelize the point clouds for 3D convolution. We use it as the backbone of our generative model due
to its exploitation of the strong spacial correlation inherent
in point cloud data.
Figure 2: Visualization of the diffusion and generative process. To
generate, Gaussian noise is sampled from p(xT ) and noise is progressively removed by pθ(xt|xt+1). Symmetrically, the diffusion
process gradually adds noise by q(xt+1|xt). We utilize a closedform expression for each q(xt+1|xt), allowing pθ(xt|xt+1) to be
learned by simply matching the posterior q(xt|xt+1, x0) of the
corresponding forward transition probability.
Energy-based models and denoising diffusion models.
Energy-based models (EBMs) and denoising diffusion
models are two classes of generative models that formulate generation as an iterative refinement procedure. Energy
based models learn an energy landscape over
input data, where local minima correspond to high-fidelity
samples, which are obtained by Langevin dynamics .
In contrast, denoising diffusion models learn a
probabilistic model over a denoising process on inputs. Diffusion is supervised to gradually denoise a Gaussian noise
to a target output. This form of supervision can be seen
as supervision of the gradient of a log probability distribution as in score matching EBM . Our work
builds on these related existing approaches and we explore
the 3D domain, which is challenging and fundamentally different from 2D images. A concurrent work on point cloud
diffusion model views point cloud generation as a conditional generation problem and uses an additional encoder
for shape latents. Ours, however, adopts an unconditional
approach, ridding the need for additional shape encoders,
and uses a different hybrid, point-voxel representation for
processing shapes. In addition to generating high-quality
3D shapes, we also show that our model can be modified
with no architectural change to perform on conditional generation tasks such as shape completion. We also demonstrate its effectiveness on real-world scans.
3. Point-Voxel Diffusion
In this section we introduce Point-Voxel Diffusion
(PVD), a denoising diffusion probabilistic model for 3D
point clouds. We start by describing our formulation, followed by the training objective for shape generation, and
end with the modified objective we proposed for shape completion from partial observation. For all our discussions below, we assume each of our data points are a set of N points
with xyz-coordinates and is denoted as x ∈RN×3. Our
model is parameterized as a single point-voxel CNN .
3.1. Formulation
The denoising diffusion probabilistic model is a generative model where generation is modeled as a denoising process. Starting from Gaussian noise, denoising is performed
until a sharp shape is formed. In particular, the denoising
process produces a series of shape variables with decreasing levels of noise, denoted as xT , xT −1, ..., x0, where xT
is sampled from a Gaussian prior and x0 is the final output.
To learn our generative model, we define a ground truth
diffusion distribution q(x0:T ) (defined by gradually adding
Gaussian noise to the ground truth shape), and learn a diffusion model pθ(x0:T ), which aims to invert the noise corruption process. We factor both probability distributions into
products of Markov transition probabilities:
\lab e l {eq:
rize} \begi
n {alig n e d} q (
0:T}) &= q(\x _0)\prod _{t=1}^{T} q(\x _t | \x _{t-1}),\\ p_\theta (\x _{0:T}) &= p(\x _T)\prod _{t=1}^{T} p_\theta (\x _{t-1} | \x _t), \end {aligned}
where q(x0) is the data distribution and p(xT ) is a standard Gaussian prior. Here, q(xt|xt−1) is named the forward
process, diffusing data into noise; accordingly, q(xt−1|xt)
is named the reverse process. pθ(xt−1|xt) is named the
generative process, which we learn, that generates realistic samples by approximates the reverse process. To enable
closed-form evaluation, the transition probabilities are also
parameterized as Gaussian distributions. We illustrate the
processes in Figure 2. Given a pre-determined increasing
sequence of Gaussian noise values β1, ..., βT ,1 each transition probability can be defined as
\label {e q: tr
n s ition} \begi
n {aligned} q (\x _t | \x _
{ t-1}) &:= \N (\sqrt {1-\beta _t} \x _{t-1}, \beta _t \I ), \\ p_\theta ( \x _{t-1}|\x _t) &:= \N (\mu _\theta (\x _t, t), \sigma _t^2 \I ). \end {aligned}
, where µθ(xt, t) represents the predicted shape from our
generative model at timestep t −1. Empirically, we found
that setting σ2
t = βt works well. Intuitively, the forward
process can be seen as gradually injecting more random
noise to the data, with the generative process learning to
progressively remove noise to obtain realistic samples by
mimicking the reverse process.
Training objective.
To learn the marginal likelihood
pθ(x), we maximize a variational lower bound of log data
likelihood that involves all of x0, ..., xT :1
\E _{q(\ x _0)}[ \ log p_\ t
eta (\x _0 )
{q(\x _{0:T})}\Big [\log \frac {p_\theta (\x _{0:T})}{q(\x _{1:T}|\x _0)}\Big ].
1We leave derivation and implementation details to Appendix.
In the above objective, the forward process q(xt|xt−1) is
fixed and p(xT ) is defined as a Gaussian prior, so they do
not affect the learning of θ. Therefore, the final objective
can be reduced to maximum likelihood given the complete
data likelihood with joint posterior q(x1:T |x0):
bel {eq:final- obj} \m ax _
a } \E _{\x _0
\sim q(\x _0), \x _{1:T} \sim q(\x _{1:T}|\x _0)}\left [\sum _{t=1}^{T}\log p_\theta (\x _{t-1}|\x _t)\right ].
Joint posterior q(x1:T |x0) can be factorized into
t=1 q(xt−1|xt, x0). Each factored ground-truth posterior
is denoted as q(xt−1|xt, x0) and is analytically tractable.
It can be shown that it is also parameterized by Gaussian
distributions:
\label {e q:p o
ligne d } & q(
t-1 } | \x _t
= \\ &\N \left (\frac {\sqrt {\tilde {\alpha }_{t-1}} \beta _t}{1-\tilde {\alpha }_t} \x _0 + \frac {\sqrt {\alpha _t} (1-\tilde {\alpha }_{t-1})}{1-\tilde {\alpha }_t} \x _t, \frac {(1-\tilde {\alpha }_{t-1})}{1-\tilde {\alpha }_t}\beta _t \I \right ). \end {aligned}
where αt = 1 −βt and ˜αt = Qt
This property allows each timestep to learn independently, i.e., each
pθ(xt−1|xt) only needs to match q(xt−1|xt, x0).
Since both pθ(xt−1|xt) and q(xt−1|xt, x0) are Gaussian, we can reparameterize the model to output noise and
the final loss can be reduced to an L2 loss between the
model output ϵθ(xt, t) and noise ϵ:1
\ l abel { eq:l 2 - e ps} \norm {\bm {\epsilon } - \bm {\epsilon }_\theta (\x _t, t)}^2,\;\; \bm {\epsilon } \sim \N (0, \I ),
Intuitively, the model seeks to predict the noise vector necessary to decorrupt the 3D shape.
Point clouds can then be generated by progressively sampling from pθ(xt−1|xt) as t = T, ..., 1 using the following
q: g e n er
\frac {1}{\sqrt {\alpha _t}}\left (\x _t - \frac {1 - \alpha _t}{\sqrt {1-\tilde {\alpha }_t}} \bm {\epsilon }_\theta (\x _t, t)\right ) + \sqrt {\beta _t} \mathbf {z},
where z ∼N(0, I), corresponding to the gradual denoising
of a shape from noise.1
3.2. Shape Completion
Our objective can be simply modified to learn a conditional generative model given partial shapes, which we introduce in this section.
Denote a point cloud sample as x0 = (z0, ˜x0), where
z0 ∈RM×3 is the fixed partial shape, and any intermediate
shapes as free points xt = (z0, ˜xt). We can then define a
conditional forward process, where the partial shape is fixed
at z0 for all time. Our conditional forward and generative
processes, as well as each transition probability, can then be
parametrized as
\label {eq: tra ns it
o n -partial } \be
gin {aligned} q( \t ilde {\x }_ t | \
t ilde {\x }_{t-1}, \z _0) &:= \N (\sqrt {1-\beta _t} \tilde {\x }_{t-1}, \beta _t \I ), \\ p_\theta ( \tilde {\x }_{t-1}|\tilde {\x }_t, \z _0) &:= \N (\mu _\theta (\x _t, \z _0, t), \sigma _t^2 \I ). \end {aligned}
forward/generative transition probabilities for the free points
˜xt, while z0 stays unchanged for all timesteps. Intuitively,
this process is the same as unconditional generation, while
we hold the partial shape z0 fixed and diffuse only the missing parts.
The modified training objective also maximizes the likelihood conditioned on partial shapes z0:
\label {eq:final-ob j-parti al} \E _
de {\x }_0, \z _ 0)
sim q(x_0), \x _{1:T} \sim q(\x _{1:T}|\tilde {\x }_0,\z _0)}\left [\sum _{t=1}^{T}\log p_\theta (\tilde {\x }_{t-1}|\tilde {\x }_t, \z _0)\right ],
where each posterior q(˜xt−1|˜xt, ˜x0, z0) is known and its
derivation is similar to the unconditional generative model.
Using the same reasoning as before, we can arrive at a similar L2 loss:
\ l ab e l {eq:l 2-e ps-p artial} \mathcal {L}_t = \norm {\bm {\epsilon } - \bm {\epsilon }_\theta (\tilde {\x }_t, \z _0, t)}^2,
where ϵ ∼N(0, I). Additionally, since the partial shape is
always fixed during both forward and generative processes,
we can mask away the subset of model output that affects
z0 and minimize L2 distance between ˜ϵ(˜xt, z0, t) and random noise, which only affects ˜xt. In practice, we input z0
and xt into the model and obtain xt−1, where only the subset ˜xt−1 is used for L2 loss. In shape completion, ˜xt−1 is
concatenated with z0 to be the input into the model again.
This allows the exact same training architecture to do both
generation and shape completion by simply changing the
training objective.
4. Experiments
We demonstrate here that our model outperforms previous point generative models in Section 4.1, is capable
of completing partial shapes sampled from single views in
Section 4.2, and can generate diverse shapes given partial
shape constraints in Section 4.3. Architecture and hyperparameter details are provided in Appendix.
4.1. Shape Generation
We choose ShapeNet Airplane, Chair, and Car
to be our main datasets for generation, following most previous works . We use the provided datasets
in , which contain 15,000 sampled points for each
shape. We sample 2,048 points for training and testing, respectively, and process our data following procedures provided in PointFlow .
Evaluation metrics.
Previous works such as have used Chamfer Distance (CD) and Earth Mover’s
Distance (EMD) as their distance metrics in calculating
Jensen-Shannon Divergence (JSD), Coverage (COV), Minimum Matching Distance (MMD), and 1-Nearest Neighbor
PVD (ours)
Figure 3: Results on unconditional shape generation with 2,048 points. The l-GAN results are from the EMD variant.
l-GAN (CD) 
l-GAN (EMD) 
PointFLow 
SoftFlow 
DPF-Net 
Shape-GF 
PVD (ours)
Table 1: Generation results on Airplane, Chair, Car compared with
baselines using 1-NN as the metric. Both CD and EMD as the distance measure are calculated. Lower scores indicate better quality
and diversity.
(1-NN), which are four main metrics to measure generative quality. However, as discussed by , JSD, COV, and
MMD each has limitations and does not necessarily indicate
better quality. Some generation results achieve even better
scores than ground-truth datasets. 1-NN is robust and correlates with generation quality, as supported by , which
also proposes 1-NN as the better metric. Therefore, we use
1-NN directly for evaluating generation quality and we provide comparison of remaining metrics in Appendix. As we
also discover that EMD score can vary widely depending
on its implementation, We evaluate all baselines using our
implementation of the metrics.
Baselines and results.
We quantitatively compare our results with r-GAN , l-GAN , PointFlow , DPF-
Net , SoftFlow , and Shape-GF on generating
2048 points. In evaluating the baselines, we follow the same
data processing and evaluation procedure as PointFlow, and
follow the provided baseline implementations to evaluate
their models. Our comparisons are shown in Table 1. Our
model noticeably achieves the best generation quality.
We also investigated pure voxel and point representations for shape generation. We noticed that simply extending diffusion models to pure point representation using conventional permutation-invariant architectures such as Point-
Net++ fails to generate any visible shapes. Extending diffusion models to pure voxel representation generates
noisy results due to the binary nature of voxels which is different from our Gaussian assumption. We visually compare
with baselines including a voxel diffusion model (Vox-Diff)
in Figure 3. For Vox-Diff, 2048 points are sampled from
voxel surfaces. We provide additional quantitative comparison in Appendix.
4.2. Shape Completion
In various graphics applications users usually do not
have access to all viewpoints of an object. A shape often
needs to be completed knowing a partial shape from a single
depth map. Therefore, the ability to complete partial shapes
become practically useful. In this section, we use the same
model architecture (see Appendix) from Section 4.1 and test
our shape completion models.
For shape completion, we use the benchmark provided by GenRe , which contains renderings of each
shape in ShapeNet from 20 random views. We sample 200
points as our partial point clouds obtained from the provided
depth images, and we evaluate shape completion on all 20
partial shapes per ground-truth sample.
For shape completion, as the ground-truth data
are involved, Chamfer Distance and Earth Mover’s Distance
suffice to evaluate the reconstruction results.
Baselines.
Since our approach is probabilistic, we selected major distribution-fitting models such as Point-
Flow , DPF-Net , and SoftFlow for comparison. We directly evaluate pre-trained models, if provided,
otherwise we re-train them using baselines’ provided implementation on our benchmark. We also compared with
Shape-GF as its encoder can similarly receive an arbitrary
Ground-truths
PVD (ours)
Figure 4: Our shape completion visualization (right) compared to baseline models (left). From left to right: depth images, ground-truth
shapes, partial shapes sampled from depth images, completion from baselines, and our results.
SoftFlow 
PointFlow 
DPF-Net 
PVD (ours)
SoftFlow 
PointFlow 
DPF-Net 
PVD (ours)
SoftFlow 
PointFlow 
DPF-Net 
PVD (ours)
Table 2: Quantitative comparison against baselines. CD is multiplied by 103 and EMD is multiplied by 102.
number of points. However, it is experimentally found that
the model is sensitive to the input partial shapes and completion is not realistic after Langevin sampling. Therefore,
we leave them out of the comparison.
Quantitative results are presented in Table 2 and
a visual comparison is shown in Figure 4. From Table 2,
we observe that our model achieves best on EMD scores
while worse on CD compared to some baselines.
we note EMD is a better metric for measuring completion
quality because by solving the linear assignment problem
it forces model outputs to have the same density as the
ground-truths and it is known that CD is blind to visual
inferiority . Our better EMD score is more indicative of
higher visual quality.
Next, we investigate the reason our CD is inferior to our
baselines. We discover that a typical case when our CD
is higher is as shown in Figure 5, where from the input
PVD (ours)
Figure 5: Typical case when CD is higher than baseline models.
Column 1 shows input depth image and ground-truth point clouds.
The next columns show completion from the input viewpoint (top)
and from the canonical viewpoint (bottom). CD is multiplied by
103 and EMD is multiplied by 102 scores.
view the ground-truth shape is largely unknown. The baseline models tend to output a mean shape when encountered
with such an unconventional angle. Naturally, mean shapes
are more frequently closer to the ground-truths than other
shapes, as exemplified by the figure. However, with each
noise initialization, our model seeks a possible completion
that matches well with the partial shape and may be further
away from the ground-truth than the mean shape. In the
case shown, our completion is a van instead of a sedan but
is equally realistic.
Our model also enables controlled completion given
multiple partial shapes, and we leave details to Appendix.
4.3. Multi-Modal Completion
Our baselines for shape completion adopt an encoderdecoder structure that takes in a partial shape and outputs
a single completion. While some offer impressive results,
their completion ability is deterministic, much different
Figure 6: Multi-modal completion visualization on PartNet. Each column presents five completion modes from a model.
Chair Table Lamp Avg. Chair Table Lamp Avg.
KNN-latent 0.96
PVD (ours)
Table 3: Quantitative comparison for multi-modal completion on
PartNet. TMD (higher the better) measures completion diversity,
and MMD (lower the better) measures completion quality. Chamfer Distance (CD) is used as the distance measure.
from humans who can often imagine different completion
possibilities given single views. Our PVD, however, adopts
a probabilistic approach to shape completion, where each
noise initialization can result in a different completion.
We follow experiment setups from cGAN and
train our model on Chair, Table, and Lamp from Part-
1024 points are given and 2048 points are
generated as completion.
In addition, we show diversity of our completion on ShapeNet. Different from Part-
Net, ShapeNet’s partial shapes are sampled from customrendered depth images.
We follow cGAN and use Total Mutual Difference (TMD) to measure diversity and Minimal Matching
Distance (MMD) to measure quality with Chamfer Distance
(CD) as distance measure. Since our model is different from
cGAN and only completes the 1024 free points, TMD is
calculated only on the free points for our model and on a
subsampled set of 1024 points for our baselines. MMD is
calculated using the completed 2048 points and re-sampled
2048 ground-truth points.
Results on PartNet.
Our model is compared with
cGAN and KNN-latent . Results are shown in Table 3 and visual comparisons are shown in Figure 6. Our
model outperforms both baselines in terms of average diversity and quality.
Results on ShapeNet.
The shape completion model
trained in Section 4.2 is directly used to demonstrate completion diversity on ShapeNet, as shown in Figure 7. We
choose a bottom view of a chair and show that, in the top
row, all of our completion results match well with the constrained viewpoint, and in the bottom row, our completion
results are noticeably diverse from the canonical viewpoint.
Results on real scans.
We further investigate how our
model pre-trained on ShapeNet can perform on scans of
real objects.
We use the Redwood 3DScans dataset 
and test our model on partial shapes of chairs and tables,
sampled from its depth images. Since the GenRe benchmark does not provide table data, the training data for
the table category are generated by randomly sampling 20
views from ShapeNet meshes, following GenRe’s procedure. Within each example, we present the real RGB-D
scans and ground-truths from the input views.
Figure 8 shows results on two views of two different
chairs and tables. For chair scans, the left example shows
the front view, and the completion results only vary slightly
!"#$%&''"()*$+&,%)*-"&.'$(/$&01$2345$*6+7$'7&8.$".$-8&$9"*8'
:.%0-$;*%-7$,6%
Figure 7: Multi-modal completion results on ShapeNet. Left: ground-truth bottom view depth image of a chair. Right: six different
possible shape completion results. Top: completion from the depth image viewpoint. Bottom: completion from the canonical viewpoint.
Completion Results
Completion Results
Figure 8: Application of our model on scans from the Redwood 3DScans dataset. PVD takes partial point clouds induced from the depth
maps, not the RGB image as input. Left: from a more complete view, the model outputs stable, similar completions. Right: from an
uncertain viewpoint, the model outputs multiple completions with a larger variation.
across different runs. The right chair example shows a back
view, and the uncertainty allows more varied completion.
Similarly, the left table scan shows a large part of the table,
so the completion varies less than the right example, which
only shows the top of the table.
5. Conclusion
We have introduced PVD, a unified framework for both
shape generation and shape completion. Our model, trained
on a simple L2 loss, is based on diffusion probabilistic models and learns to reverse a diffusion process by progressively
removing noise from noise-initialized samples. A minor
modification on the objective also results in a shape completion model without the need for any architectural change.
Experimentally, we show the failure with straight-forward
extension of diffusion models to either pure voxel or point
representations. With the point-voxel representation, our
model demonstrates superior generative power and impressive shape completion quality. Unlike most baseline models
which use deterministic encoder-decoder structures, PVD
can output multiple possible completion results given a partial shape. Additionally, it can complete real 3D scans, thus
offering practical usage in various downstream applications.
Acknowledgements:
Yilun Du is funded by an NSF graduate fellowship. This work is in part supported by ARMY
MURI grant W911NF-15-1-0479, Samsung Global Research Outreach (GRO) program, Amazon Research Award
(ARA), IBM, Stanford HAI, and Autodesk.