MIT Open Access Articles
Undoing the Damage of Dataset Bias
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Kholsa, Aditya et al. "Undoing the Damage of Dataset Bias." European Conference on
Computer Vision, September 2012, Munich, Germany, Springer Nature, 2012 © 2012 Springer-
As Published: 
Publisher: Springer Nature
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of use: Creative Commons Attribution-Noncommercial-Share Alike
Undoing the Damage of Dataset Bias
Aditya Khosla1, Tinghui Zhou2, Tomasz Malisiewicz1,
Alexei A. Efros2, and Antonio Torralba1
1Massachusetts Institute of Technology
2Carnegie Mellon University
{khosla,tomasz,torralba}@csail.mit.edu,
{tinghuiz,efros}@cs.cmu.edu
Abstract. The presence of bias in existing object recognition datasets
is now well-known in the computer vision community. While it remains
in question whether creating an unbiased dataset is possible given limited resources, in this work we propose a discriminative framework that
directly exploits dataset bias during training. In particular, our model
learns two sets of weights: (1) bias vectors associated with each individual
dataset, and (2) visual world weights that are common to all datasets,
which are learned by undoing the associated bias from each dataset. The
visual world weights are expected to be our best possible approximation
to the object model trained on an unbiased dataset, and thus tend to
have good generalization ability. We demonstrate the e↵ectiveness of our
model by applying the learned weights to a novel, unseen dataset, and
report superior results for both classiﬁcation and detection tasks compared to a classical SVM that does not account for the presence of bias.
Overall, we ﬁnd that it is beneﬁcial to explicitly account for bias when
combining multiple datasets.
Introduction
Recent progress in object recognition has been largely built upon e↵orts to create large-scale, real-world image datasets . Such datasets have been widely
adopted by the computer vision community for both training and evaluating
recognition systems. An important question recently explored by Torralba and
Efros , and earlier by Ponce et al , is whether these datasets are representative of the visual world, or in other words, unbiased. Unfortunately, experiments
in strongly suggest the existence of various types of bias (e.g. selection bias,
capture bias, and negative set bias) in popular image datasets.
In the ideal world, more data should lead to better generalization ability but
as shown in , it is not necessarily the case; performance on the test set of a
particular dataset often decreases when the training data is augmented with data
from other datasets. This is surprising as in most machine learning problems, a
model trained with more examples is expected to better characterize the input
space of the given task, and thus yield better performance. The fact that this
common belief does not hold in object recognition suggests that the input space
of each image dataset is dramatically di↵erent, i.e. the datasets are biased.
Undoing the Damage of Dataset Bias
Our key observation for undoing the dataset bias is that despite the presence
of di↵erent biases in di↵erent datasets, images in each dataset are sampled from
a common visual world (shown in Figure 1). In other words, di↵erent image
datasets are biased samples of a more general dataset—the visual world. We
would expect that an object model trained on the visual world would have the
best generalization ability, but it is conceivably very diﬃcult, if not impossible,
to create such a dataset.
In this paper, we propose a discriminative framework that explicitly deﬁnes
a bias associated with each dataset and attempts to approximate the weights
for the visual world by undoing the bias from each dataset (shown in Figure 1).
Speciﬁcally, our model is a max-margin framework that takes the originating
dataset of each example into account during training. We assume that the bias
of all examples from a given dataset can be modeled using the same bias vector,
and jointly learn a visual world weight vector together with the bias vector for
each dataset by max-margin learning. In order to model both contexual bias
and object-speciﬁc bias, we apply our algorithm to the tasks of classiﬁcation
and detection, showing promising results in both domains.
Visual World
e.g. Caltech 101
e.g. LabelMe
Seen datasets
e.g. PASCAL
Novel dataset
Visual World
Dataset Speciﬁc
Dataset Speciﬁc
Fig. 1. Left: Sampling from the Visual World. Image datasets are sampled from
the same visual world. Each dataset we have collected, such as Caltech101, SUN or
LabelMe, has a certain bias that is represented as ∆i, i = 1, . . . , n. Dn+1 represents an
independent test set that has not been seen by the model but is sampled from the same
visual world. Right: Overview of our algorithm. We model the biases as additive
linear vectors ∆i for each dataset. Our goal is to learn a model for the visual world wvw
which has good generalization ability. The dataset speciﬁc model wi tends to perform
well on the corresponding dataset but not generalize well to novel datasets.
The rest of the paper is organized as follows: Section 2 reviews related work.
Section 3 presents the details of our model, including the problem formulation
and the optimization algorithm. Experimental results that demonstrate the e↵ectiveness of our model in both classiﬁcation and detection settings are presented in
Section 4 . Section 5 concludes the paper with a summary of our contributions.
Undoing the Damage of Dataset Bias
Related Work
Recently, domain adaptation and transfer learning techniques have been successfully applied to object recognition problems. This line of research addresses
the problem of domain shift , i.e. mismatch of the joint distribution of inputs
between source and target domains. In particular, Saenko et al. provide one
of the ﬁrst studies of domain adaptation for object recognition. The key idea of
their work is to learn a regularized transformation using information-theoretic
metric learning that maps data in the source domain to the target domain. Kulis
et al. generalize the method in to handle asymmetric transformations in
which the feature dimensionality in source and target domain can be di↵erent.
However, both of the above methods require labeled data from the target
domain as the input consists of paired similar and dissimilar points between the
source and the target domain. In contrast, Gopalan et al. propose a domain
adaptation technique for an unsupervised setting, where data from the target
domain is unlabeled. The domain shift in this case is obtained by generating
intermediate subspaces between the source and target domain, and projecting
both the source and target domain data onto the subspaces for recognition. While
most domain adaptation models need to be re-trained for every new domain, Jain
and Learned-Miller proposed an online method based on Gaussian process
regression that rapidly adapts to the new domain without re-training.
A mathematical framework similar to ours is proposed in for multitask learning, where solutions to multiple tasks are tied through a common
weight vector. The common weight vector is used to share information among
tasks but is not contrained to perform well on any task on its own. This is the
crucial di↵erence between and our setting: our goal is to learn a common
weight vector that can be used independently and is expected to perform well
on a new dataset.
We note that our model is di↵erent from conventional transfer learning approaches. In terms of problem setting, transfer learning techniques generally fall
into three categories : (1) Inductive transfer learning, (2) Transductive transfer learning, and (3) Unsupervised transfer learning. The fundamental di↵erence
between our approach and transfer learning approaches is that there is no data
available from the target domain during training, and that the target task is the
same as the source task.
We evaluate our algorithms on cross-dataset generalization in a holdone dataset out fashion. However, unlike previous works our algorithm explicitly
models the dataset bias to mitigate its negative e↵ects. To the best of our knowledge, the problem we address is novel. We hope that our work will provide new
insights for the object recognition community on building systems that work in
real-world scenarios, and encourage the evaluation of algorithms with respect to
better cross-dataset generalization capability.
Undoing the Damage of Dataset Bias
Discriminative Framework for Undoing Dataset Bias
Our aim is to design an algorithm to learn a visual world model, and the bias for
each dataset with the following properties: (1) We would expect the visual world
model to perform well on average, but not necessarily the best on any particular
dataset, since it is not biased towards any one dataset. (2) On the other hand, we
would expect the biased model, obtained by combining the visual world model
and the learned bias, to perform the best on the dataset that it is biased towards
but not necessarily generalize well to other datasets. To this end, we propose a
discriminative framework to jointly learn a weight vector corresponding to the
visual world object model, wvw, and a set of bias vectors, ∆i, for each dataset,
Di, that, when combined with the visual world weights result in an object model
speciﬁc to the dataset. Speciﬁcally, we formulate the problem in a max-margin
learning (SVM) framework that resembles .
Terminology and Assumptions
In this section, we deﬁne the terminology used in the algorithm and some of the
assumptions of our model.
Terminology: Assume that we are given n datasets, D1, . . . , Dn with a
common object class. Each dataset Di = {(xi
1), . . . , (xi
si)}, consists of si
training examples, (xi
j), where xi
j 2 Rm represents the m-dimensional feature
vector and yi
j 2 {−1, 1} represents the label for example j from dataset Di. In our
algorithm, we learn one set of weights, ∆i 2 Rm, corresponding to the bias of each
dataset Di, and another set of weights, wvw 2 Rm, corresponding to the visual
world. The weights are related by the equation, wi = ⌦(wvw, ∆i) = wvw + ∆i,
where wi 2 Rm corresponds to the weight vector for dataset Di.
Assumptions: Our method is general and can be applied to any number of
datasets containing a common object class. We assume that the features used
are common for all images from all datasets. Further, we assume that the bias
between datasets can be identiﬁed in feature space (i.e. the features are rich
enough to capture the bias in the images). This assumption allows us to model
the weights learned for a speciﬁc dataset as a function, ⌦, of bias and weights for
the visual world. This relationship (linear additive) is kept ﬁxed in our paper,
but there are other possible ways to model this relationship (e.g. multiplicative,
non-linear) that would likely a↵ect the optimization algorithm.
Our algorithm is largely based on max-margin learning (SVM), and explicitly
models the bias vector in feature space for each dataset.
Undoing the Damage of Dataset Bias
Learning wvw and ∆i amounts to solving the following optimization problem:
wvw,∆i,⇠,⇢
2||wvw||2 + λ
||∆i||2 + C1
subject to
wi = wvw + ∆i
i = 1 . . . n, j = 1 . . . si
i = 1 . . . n, j = 1 . . . si
i = 1 . . . n, j = 1 . . . si
where C1, C2 and λ are hyperparameters, and ⇠and ⇢are the slack variables.
We note the changes from the regular SVM setting: (1) the bias vectors,
∆i regularized to encourage the dataset speciﬁc weights to be similar to the
visual world weights, (2) additional constraints (described below), and (3) the
hyperparameters C1, C2 and λ (described below).
Constraints: Equation 2: This deﬁnes the relationship between wvw, wi
and ∆i. We choose a simple relationship to ensure that the objective function is
convex. Equation 3: The slack variable ⇠corresponds to the loss incurred across
all datasets when using the visual world weights wvw. The visual world weights
are expected to generalize across all datasets, so this loss is minimized across all
training images from all datasets. Equation 4: The slack variable ⇢corresponds to
the loss incurred when an example is incorrectly classiﬁed by the biased weights.
For each dataset, only the corresponding biased weights are required to classify
the example correctly, i.e. when training with Caltech101 and SUN, the biased
weights for Caltech101 are not penalized if they incorrectly classify an image
from the SUN dataset.
Hyperparameters: The hyperparameters C1 and C2 are similar to the standard SVM parameter used to balance terms in the learning objective function.
C1 and C2 allow us to control the relative importance between the two contraints
of optimizing loss on the visual world and the individual datasets. λ deﬁnes the
weight between learning independent weights and a common set of weights for
all datasets, i.e. when λ ! 1, the biases ∆i tend towards zero, leading to a
common set of weights for all datasets, while λ = 0 results in the weights for
each dataset being independent as there is no restriction on the biases.
Optimization
In this section, we describe how to optimize Equation 1 described in Section 3.2.
We observe that the objective function is convex, thus can be optimized using
stochastic subgradient descent. We use the same optimization algorithm for both
classiﬁcation and detection experiments.
We rewrite the objective in an unconstrained form, in terms of wvw and ∆i’s:
2||wvw||2 +
2 ||∆i||2 −L(wvw, ∆i)
Undoing the Damage of Dataset Bias
where L(wvw, ∆i) =
C1 min(1, yi
j)+C2 min(1, yi
j(wvw +∆i)·xi
Then, we ﬁnd the subgradients with respect to both w and ∆i’s:
vw = wvw −
i = λ∆i −C2
where Ji = {j|yi
Ki = {j|yi
j(wvw + ∆i) · xi
Implementation details: In our experiments, we set the learning rate,
↵= 0.2/i, where i is the number of iterations. We use a batch size of one example
for stochastic subgradient descent with an adaptive cache, similar to . Our
classiﬁcation algorithm takes ⇠8 minutes to compute when combining 4 datasets
(containing more than 30,000 examples) on a single core. In our experiments, we
set the value of C2 to be some fraction of C1 to better model a trade-o↵between
loss on visual world and individual datasets.
Experiments
To evaluate our framework, we apply our algorithm to two tasks: object classiﬁcation (identifying whether an object is present in the image) and object
detection (localizing an object in the image). We apply our framework to both
classiﬁcation and detection in order to capture di↵erent types of biases. In classiﬁcation, we capture contexual bias as we use global image features that include
both the object and its surroundings (i.e. context), while in detection we capture
object-speciﬁc bias as we only use the information in the provided bounding box
annotation. We use four datasets in our experiments, namely PASCAL2007 ,
LabelMe , Caltech101 , and SUN09 . The experiments are performed
on ﬁve common object categories: “bird”, “car”, “chair”, “dog” and “person”.
Our experiments demonstrate that our framework is e↵ective at reducing the
e↵ects of bias in both classiﬁcation and detection tasks.
In our experiments, we use a regular SVM as baseline because it outperforms
the common weight vector from (veriﬁed experimentally). This is expected
as the common weight vector is not constrained to perform any task in as
their goal is to improve performance on individual tasks, and the common weight
vector is only used to share information across tasks.
Object Classiﬁcation
Setup Our method is ﬂexible to allow the use of many di↵erent visual descriptors. In our experiments, we use a bag-of-words representation. First, we densely
Undoing the Damage of Dataset Bias
(a) Train on all, test on one at a time
wP as wLab
0.638 0.511 0.548 0.495
0.690 0.729 0.719 0.733
0.894 0.928 0.998 0.918
0.427 0.515 0.530 0.603 0.568
0.662 0.671 0.698 0.687 0.709
(b) Train + test on one
Table 1. Average precision (AP) of “car” classiﬁcation on seen datasets. In
this case, the train set of the dataset used for testing is available during training (Section 4.1). Pas, Lab, Cal and SUN refer to the four datasets, PASCAL2007, LabelMe,
Caltech101 and SUN09 respectively. SVMone refers to a linear SVM that is trained
only on the train set of the corresponding test set, while SVMall refers to a linear SVM
trained on a combination of all the data from the train set of all datasets. Our visual
world model outperforms SVMall indicating improved generalization, and the biased
models are comparable to SVMone (0.742 vs 0.743).
extract grayscale SIFT descriptors on each image at multiple patch sizes of
8, 12, 16, 24 and 30 with a grid spacing of 4. Using k-means clustering on randomly sampled descriptors from the training set of all datasets, we construct a
vocabulary of 256 codewords. Then, we use Locality-constrained Linear Coding
(LLC) to assign the descriptors to codewords. A 3-level spatial pyramid 
with a linear kernel is used for all experiments in this section. The baseline SVM
is implemented using Liblinear and the results are evaluated using average
precision (AP).
Classiﬁcation on seen datasets Before we demonstrate the generalization
performance of our model on novel datasets, we ﬁrst show how our model performs on the same datasets it is trained on. Speciﬁcally, we use all four datasets
for training the model and apply the learned weight vectors to one test set at a
time. The results for “car” classiﬁcation are shown in Table 1. We compare our
results against two baseline SVM models, one trained on all datasets (SVMall,
Table 1(a)) and another trained on individual datasets (SVMone, Table 1(b)).
The main observations are as follows: (1) The pronounced diagonals in Table 1(a) indicate that each biased model better adapts to its source dataset
than other weight vectors (including wvw), and is comparable to training on one
dataset at a time (SVMone). (2) The performance of SVMone is signiﬁcantly better than SVMall, which shows that additional training examples are not always
beneﬁcial (also shown in ). Together with (1) it implies a clear presence of
dataset bias that can signiﬁcantly impact performance when left untreated. (3)
The visual world weights wvw outperform the baseline SVMall in most cases,
demonstrating the improved generalization ability of our model as compared to
SVMall, which does not explicitly model dataset bias, and naively concatenates
data from all datasets.
Classiﬁcation on unseen datasets In this experiment, we evaluate the generalization performance of our method by testing on an unseen dataset, i.e. a
Undoing the Damage of Dataset Bias
dataset whose examples are not available during training. During each experiment, we hold out one dataset as the unseen test set, and train the model on the
other three datasets (referred to as seen sets). For example, if Caltech101 is the
current unseen test set, then the model is trained on PASCAL2007, LabelMe,
and SUN09. We also train a linear SVM on the seen sets for baseline comparison.
The results are summarized in Figure 2. We observe that when testing on an
unseen dataset, the visual world weights, wvw, typically outperform the SVM
trained directly on the seen sets. This is because our model treats examples from
each dataset as biased samples of the visual world, and in this way learns visual
world weights with better generalization ability than the naive SVM. In fact, the
naive SVM is a special case of our model with ∆i’s equal to zero, i.e. assuming all
datasets are bias-free. Overall, our algorithm outperforms the baseline by 2.8%
across all object categories.
%AP increment over baseline
Fig. 2. Classiﬁcation on unseen datasets. The graphs show improvement in percent average precision (%AP) of classiﬁcation on unseen datasets (Section 4.1) over the
baseline (SVM). The labels on the x-axis ‘P’, ‘L’, ‘C’, and ‘S’ represent the datasets
PASCAL2007, LabelMe, Caltech101 and SUN09 respectively, while ‘M’ represents
the Mean AP increment over all datasets. The ﬁve left-most plots correspond to individual object categories while the right-most plot combines the result over all object
categories. Overall, our algorithm outperforms the baseline in 24 out of 25 cases, with
an overall improvement of 2.8% mAP.
Dataset classiﬁcation In this experiment, we qualitatively and quantitatively
evaluate the signiﬁcance of the learned bias vectors through the task of dataset
classiﬁcation (similar to ‘Name That Dataset!’ in ). We uniformly sample a
set of positive images from the test set of di↵erent datasets, and predict which
dataset each image belongs to using the bias vectors. If the bias vectors are
indeed learning the bias as proposed, they would be able to successfully perform
this task despite not being explicitly trained for it.
For “car”, the test set consists of 4⇥90 = 360 positive images, and similarly,
4 ⇥400 = 1600 for “person” (restricted by the smallest number of positive
images in each test set). If the bias vector is learning as proposed, we should
expect that images from the i-th dataset would be better classiﬁed by ∆i than
by bias vectors of other datasets. To verify this we ﬁrst train our model on all
four datasets, and then apply the learned biases, ∆i’s, to the test set of positive
images. The classiﬁcation performance of ∆i is measured using average precision.
The quantitative results are shown in Table 2, while some qualitative results are
shown in Figure 8.
Undoing the Damage of Dataset Bias
∆P as ∆Lab
∆Cal ∆SUN ∆P as ∆Lab
PASCAL2007
0.572 0.254 0.299 0.314 0.445 0.251 0.250 0.382
0.250 0.373 0.252 0.315
0.250 0.536 0.251 0.314
Caltech101
0.262 0.548 0.731 0.250
0.324 0.250 0.954 0.250
0.314 0.251 0.250 0.593 0.292 0.330 0.251 0.314
Table 2. Name that dataset! Average precision of dataset classiﬁcation using the
bias vectors (Section 4.1). Each row represents one dataset, while each column represents a particular bias applied to that dataset. We observe that the bias vector
corresponding to the particular dataset performs best for this task, suggesting that the
bias is being learned as proposed. Note that Caltech101 is the easiest to distinguish
from other datasets for both categories (as per our expectation).
The classiﬁcation results clearly indicate that the bias vectors are indeed
learning the speciﬁc biases for each dataset. This validates our method of modeling the biases in the chosen way (linear additive in feature space). We emphasize
that this result is surprising as the bias vectors were not trained to perform this
task, and yet, did surprisingly well on it. Furthermore, from Figure 8(a), we can
easily identify the contexual bias for cars in each dataset, e.g. SUN09 contains
cars on the highways with a prevalent view of the sky, while LabelMe tends to
have cars in more urban settings. We can draw similar conclusions from Figure 8(b). It is interesting to note that while many of the top images for person
are wrong for LabelMe and SUN09, they share similar visual appearance.
%AP increment over baseline
Fig. 3. Detection on unseen datasets. Improvement in percent average precision
(%AP) of detection on unseen datasets over the baseline. Refer to the caption of Figure 2 for more details. Note that the graphs indicate that our algorithm outperforms
or is comparable to the baseline in most cases, with an overall improvement of 0.7%
mAP over the baseline.
Object Detection
Setup In this setting, we use our learning algorithm in the deformable partsbased model (DPM) framework by Felzenszwalb et al . We learn the DPM
without parts and use 2 mixture components to learn the models, for both our
algorithm and the baseline (SVM). The mixture models are learned by combining
all the images from di↵erent datasets and dividing them into 2 components based
on aspect ratios. We use the same number of latent positive example mining and
hard negative mining updates with the same cache size for all cases.
Undoing the Damage of Dataset Bias
(a) Detection results
Visual World
Baseline Model
LabelMe PASCAL2007 SUN09
Caltech101
Unseen Dataset
(b) HOG templates
Fig. 4. (a) Comparison of detection results of visual world vs baseline. Top
detection results for “chairs” on Caltech101 comparing visual world model and baseline (SVM). Green/red borders indicate correct/incorrect detections respectively. The
scores decrease from left to right and from top to bottom. (b) Comparison of HOG
templates of “chair” for visual world vs baseline. The visual world weights tend
to be more similar to each other, compared to the baseline, despite being trained on
di↵erent datasets suggesting improved generalization ability of our model. We further
observe that there is less ‘noise’ in the visual world models, likely due to the di↵erent
biases of the datasets. This ﬁgure is best viewed on screen due to the ﬁne di↵erences
between the templates.
Detection on unseen datasets We use the same experimental setting as Section 4.1 for detection, where a model is tested on one dataset at a time, while
using the other three for training. The results are summarized in Figure 3. Using our framework, which models dataset bias explicitly, we observe performance
improvement in the detection task for most cases. Overall, our algorithm outperforms the baseline by 0.7% mAP across all object categories and datasets. We
note that this is a diﬃcult task with high variance in object appearance across
datasets, and a limited number of training examples available in some of the
Figure 4(a) shows top “chair” detection of our model and the baseline. We
observe that our model is not as easily confused by chair-like objects as the
baseline. To explore this further, we visualize the HOG templates for “chair” in
Figure 4(b). We observe that the models learned by our algorithm tend to be less
‘noisy’ than the baseline, i.e. as compared to the visual world model, the baseline
models depict many gradients that don’t correspond to the dominant shape of
the object. We speculate that this occurs because the baseline model is forced
to ﬁt the biases of all datasets into a single model, while our visual world model
is able to identify the common components across the datasets and attribute
the remaining edges to various biases. Finally, in Figure 5, we randomly select
some detections found by the visual world weights but not others (including
the baseline), and construct a heatmap. The heatmap is based on the detection
activation for each HOG cell by the corresponding weights.
Undoing the Damage of Dataset Bias
Further, we investigate what is learned by the di↵erent bias vectors. We visualize the top detection results for “car” and “person” categories in Figure 6
when applying dataset speciﬁc models. The biases of the di↵erent models are
clearly reﬂected in the detection results. Further, we note that the visual world
model has the most ‘diverse’ detection results compared to dataset speciﬁc models. Additionally, we visualize the learned HOG templates in Figure 7. As shown,
the HOG templates for the bias terms are quite di↵erent for di↵erent datasets,
implying the e↵ectiveness of our model in capturing the object-speciﬁc bias of
di↵erent datasets. Together with the performance improvement (shown in Figure 3), this implies that our model is e↵ective at modeling and undoing the
damage of dataset bias.
Fig. 5. Spatial distribution of detection weights. Figure showing unique detections and their heatmaps, i.e. detections that are identiﬁed by the visual world model
but not the baseline. The spatial distribution of weights for “car” and “person” is
shown. Red represents the highest score, while blue represents the lowest score. We observe that there are many di↵erences in two sets of heatmaps. The visual world model
is better able to generalize by robustly identifying multiple regions corresponding to
the object ignored by the baseline method, such as tire/hood for “car”, and face/eyes
for “person”.
Conclusion
In this paper, we presented a framework for undoing the damage of dataset bias
when combining multiple datasets to train object models, and demonstrated its
positive e↵ects in both classiﬁcation and detection tasks using popular computer
vision datasets. Speciﬁcally, we introduced a max-margin based model that explicitly deﬁnes and exploits the e↵ects of dataset bias. We further demonstrated
that the learned bias is indeed indicative of membership to a particular dataset,
and hence likely learning both contextual biases and object-speciﬁc biases as expected. We would like to emphasize that our framework for learning the visual
world model is a ﬁrst step in building models that explicitly include dataset bias
in their mathematical formulation with the goal of mitigating its e↵ect. Further,
we hope that this work will encourage the evaluation of algorithms with respect
to cross-dataset generalization performance.
Undoing the Damage of Dataset Bias
Fig. 6. Top scoring “car” and “person” detections on PASCAL2007. wi indicates the dataset speciﬁc bias used for the given row. Green/red borders indicate
correct/incorrect detections respectively. The scores decrease from left to right. The
biases learned for each dataset are quite clear, e.g. for “car”, wLab tends to prefer cars
at an angle of approximately 45◦, while wSun prefers front/back-facing cars and wCal
prefers sides of cars. Similarly for “person”, we observe that wLab prefers full/half
body, while wSun prefers full body and wCal prefers faces. This matches our intuition
of the types of examples present in each of these datasets.
Caltech101
Visual World Model
LabelMe bias
Caltech101 bias
Visual World Model
LabelMe bias
Caltech101 bias
Caltech101
(b) Person
Fig. 7. HOG templates for (a)“car” and (b)“person”. PASCAL2007 is the unseen dataset for both categories (i.e. the models are trained on the datasets shown in
the ﬁgure). To visualize both the positive and negative components of the learned bias,
we decompose it into two terms: ∆i = ∆+
i . As shown, the learned bias for each
dataset model reﬂects the bias of the particular dataset, e.g. Caltech101 bias strongly
prefers side view of cars with strong positive weights on tires, and frontal faces with
focus on facial features, etc.
Acknowledgements
We thank the anonymous reviewers for their valuable feedback. The paper was
co-sponsored by ONR MURIs N000141010933 and N000141010934.
Undoing the Damage of Dataset Bias
(b) Person
Fig. 8. Dataset classiﬁcation retrieval results. Top images retrieved by di↵erent
bias vectors on a pool of positive images sampled from all four datasets. Colored borders
indicate the dataset each image is sampled from. For instance, images with a red
border are from PASCAL2007, while images with a green border are from Caltech101.
Heatmaps in the second column illustrate the importance of each image region for
classiﬁcation (Importance decreases in the order red > green > blue.). The heatmaps
are generated using a sum of SVM weights corresponding to di↵erent spatial pyramid
regions. We observe that the heatmaps conﬁrm our intuition of what is being learned
by the model. The heatmaps for cars show that ∆Sun tends to give high importance to
the sky region (as seen in retrieved images), while ∆Cal places more importance closer
to the center of the image (and we know that cars are centrally located in Caltech101),
and similarly for ∆Lab, we observe that the context of street and buildings plays a more
important role as cars tend to be small and diﬃcult to localize. We can draw similar
intuitions from the person heatmaps.
Undoing the Damage of Dataset Bias