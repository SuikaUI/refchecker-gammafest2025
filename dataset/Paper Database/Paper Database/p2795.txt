Imbalanced Deep Learning by Minority Class
Incremental Rectiﬁcation
Qi Dong, Shaogang Gong, and Xiatian Zhu
Abstract—Model learning from class imbalanced training data is a long-standing and signiﬁcant challenge for machine learning. In
particular, existing deep learning methods consider mostly either class balanced data or moderately imbalanced data in model training,
and ignore the challenge of learning from signiﬁcantly imbalanced training data. To address this problem, we formulate a class
imbalanced deep learning model based on batch-wise incremental minority (sparsely sampled) class rectiﬁcation by hard sample
mining in majority (frequently sampled) classes during model training. This model is designed to minimise the dominant effect of
majority classes by discovering sparsely sampled boundaries of minority classes in an iterative batch-wise learning process. To that
end, we introduce a Class Rectiﬁcation Loss (CRL) function that can be deployed readily in deep network architectures. Extensive
experimental evaluations are conducted on three imbalanced person attribute benchmark datasets (CelebA, X-Domain, DeepFashion)
and one balanced object category benchmark dataset (CIFAR-100). These experimental results demonstrate the performance
advantages and model scalability of the proposed batch-wise incremental minority class rectiﬁcation model over the existing
state-of-the-art models for addressing the problem of imbalanced data learning.
Index Terms—Class imbalanced deep learning, Multi-label learning, Inter-class boundary rectiﬁcation, Hard sample mining, Facial
attribute recognition, Clothing attribute recognition, Person attribute recognition.
INTRODUCTION
ACHINE learning from class imbalanced data, in
which the distribution of training data across different object classes is signiﬁcantly skewed, is a long-standing
problem . Most existing learning algorithms produce
inductive bias (learning bias) towards the frequent (majority) classes if training data are not balanced, resulting in
poor minority class recognition performance. However, accurately detecting minority classes is often important, e.g. in
rare event discovery . A simple approach to overcoming
class imbalance in model learning is to re-sample the training data (a pre-process), e.g. by down-sampling majority
classes, over-sampling minority classes, or some combinations . Another common approach is cost-sensitive
learning, which reformulates existing learning algorithms
by weighting the minority classes more .
Over the past two decades, a range of class imbalanced
learning methods have been developed . However, they
mainly investigate the single-label binary-class imbalanced
learning problem in small scale data with class imbalance
ratios being small, e.g. within 1:100. These methods are
limited when applied to learning from big scale data in computer vision. Visual data are often interpreted by multi-label
semantics, e.g. web person images with multi-attributes on
clothing and facial characteristics. Automatic recognition
of these nameable properties is very useful , but
challenging for model learning due to: (1) Very large scale
imbalanced training data , with clothing and facial
attribute labelled data exhibiting power-law distributions
(Fig. 1). (2) Subtle appearance discrepancy between different
Qi Dong and Shaogang Gong are with the School of Electronic Engineering and Computer Science, Queen Mary University of London, UK. Email: {q.dong, s.gong}@qmul.ac.uk. Xiatian Zhu is with Vision Semantics
Ltd., London, UK. E-mail: .
Attribute index
Woollen coat
Cotton coat
Number of images
Number of images
Attribute index
Fig. 1. Imbalanced training data class distributions: (a) clothing attributes
(X-Domain ), (b) facial attributes (CelebA ).
ﬁne-grained attribute classes, e.g. “Woollen-Coat” can appear very similar to “Cotton-Coat”, whilst “Mustache” can
be visually hard to be distinct (Fig. 1(b)). To discriminate
subtle classes from multi-labelled images at large scale,
standard learning algorithms require a vast quantity of class
balanced training data for all labels .
There is a vast quantity of severely imbalanced visual
data on the Internet. Conventional learning algorithms are poorly suited for three reasons: First, conventional
imbalanced data learning methods without deep learning
rely on hand-crafted features extracted from small data,
which are inferior to big data deep learning based richer
 
Comparing large datasets w.r.t. training data imbalance in terms of
class imbalance ratio (the sample size ratio between the smallest and
largest classes). The ratios are for the standard train/val/test data split if
available, otherwise the whole dataset. For MS-COCO , no
numbers are available for calculating the imbalance ratio, because their
images often contain simultaneously multiple classes of objects and
multiple instances of a speciﬁc class.
ILSVRC2012-14 
MS-COCO 
VOC2012 
CIFAR-100 
Caltech 256 
CelebA 
DeepFashion 
X-Domain 
feature representations . Second, deep learning
in itself also suffers from class imbalanced training data
 (Table 9 and Sec. 4.3). Third, directly incorporating
existing imbalanced data learning algorithms into a deep
learning framework does not provide effective solutions
 .
Overall, imbalanced big data deep learning is understudied partly due to that popular image benchmarks for
large scale deep learning, e.g. ILSVRC, do not exhibit signiﬁcant class imbalance after some careful sample ﬁltering
being applied in those benchmark constructions (Table 1).
More recently, there are a few emerging large scale clothing and facial attribute datasets that are signiﬁcantly more
imbalanced in class labelled data distributions (Fig. 1), as
these datasets are drawn from online Internet sources without artiﬁcial sample ﬁltering . For example, the
imbalance-ratio (lower is more extreme) between the minority
classes and the majority classes in the CelebA face attribute
dataset is 1:43 (3,713 : 159,057 samples), whilst the X-
Domain clothing attributes are even more imbalanced with
an imbalance-ratio of 1:4,162 (20 : 204,177) (Table 1).
This work addresses the problem of large scale imbalanced data deep learning for multi-label classiﬁcation. This
problem is characterised by (1) Large scale training data; (2)
Multi-label per data sample; (3) Extremely imbalanced training data with an imbalance-ratio being greater than 1:1000;
(4) Variable per-label attribute values, ranging from binary
to multiple attribute values per label. The contributions of
this work are: (I) We solve the large scale imbalanced data
deep learning problem. This differs from the conventional
imbalanced data learning studies focusing on small scale
data single-labelled with a limited number of classes and
small data imbalance-ratio. (II) We present a novel approach
to imbalanced deep learning by minority class incremental
rectiﬁcation using batch-wise mining of hard samples on the
minority classes in a batch-wise optimisation process. This
differs from contemporary multi-label learning methods
 , which either assume class balanced training
data or simply ignore the imbalanced data learning problem
all together. (III) We formulate a Class Rectiﬁcation Loss
(CRL) regularisation algorithm for minority class incremental rectiﬁcation. In particular, the computational complexity
of imposing this rectiﬁcation loss is restrained by iterative
mini-batch-wise model optimisation (small data pools). This
is in contrast to the global model optimisation over the
entire training data pool of the Large Margin Local Embedding (LMLE) algorithm1 . There are two advantages
of our approach: First, the model only requires incremental class imbalanced data learning for all attribute labels
concurrently without any additional single-label sampling
assumption (e.g. per-label oriented quintuplet construction);
Second, model learning is independent to the overall training
data size, the number of class labels, and without predetermined global data clustering. This makes the model
much more scalable to learning from large training data.
Extensive evaluations were performed on the CelebA
face attribute and X-Domain clothing attribute 
benchmarks, with further evaluation on the DeepFashion
clothing attribute benchmark. These experimental results show a clear advantage of CRL over 12 state-of-the-art
models compared, including 7 attribute models (PANDA
 , ANet , Triplet-kNN , FashionNet , DARN
 , LMLE , MTCT ). We further evaluated the CRL
method on the class balanced single-label object recognition
benchmark CIFAR-100 , and constructed different class
imbalance-ratios therein to quantify the model performance
gains under controlled varying degrees of imbalance-ratio
in training data.
RELATED WORK
Class imbalanced learning aims to mitigate model learning
bias towards majority classes by lifting the importance of
minority classes . Existing methods include: (1) Datalevel: Aiming to rebalance the class prior distributions in a
pre-processing procedure. This scheme is attractive as the
only change needed is to the training data rather than to
the learning algorithms. Typical methods include downsampling majority classes, over-sampling minority classes,
or both . However, over-sampling can easily
cause model overﬁtting owing to repeatedly visiting duplicated samples . Down-sampling, on the other hand,
throws away valuable information . (2) Algorithmlevel: Modifying existing algorithms to give more emphasis
on the minority classes . One
strategy is the cost-sensitive learning which assigns varying
costs to different classes, e.g. a higher penalty for minority class samples . However, it is in general
difﬁcult to optimise the cost matrix or relationships. Often,
it is given by experts therefore problem-speciﬁc and nonscalable. In contrast, the threshold-adjustment technique
changes the decision threshold in test time .
1. In LMLE, a computationally expensive data pre-processing (including clustering and quintuplet construction) is required for each
round of deep model learning. In particular, to cluster n (e.g. 150,000+)
training images w.r.t. an attribute label by k-means, its operation
complexity is super-polynomial with the need for at least 2Ω(√n) (Ωthe
lower bound complexity) iterations of cluster reﬁnement on n samples
 . As each iteration is linear to k and n, a clustering takes the complexity at k×O(n)×2Ω(√n) (O the upper bound complexity). To create
a quintuplet for each data sample, four cluster- and class-level searches
are needed, each proportion to the training data size n with the overall
search complexity as quadratic to n (O(n2)). Given a large scale training
set, it is likely that this pairwise search part takes the most signiﬁcant
cost in the pre-processing. Both clustering and quintuplet operations
are needed for each attribute label, and their costs are proportional to
the total number nval of attribute values, e.g. 80 times for CelebA and
178 times for X-domain. Consequently, the total complexity of the preprocessing per round is nval ×
 k × O(n) × 2Ω(√n) + O(n2)
-30 -20 -10 0 10 20 30
-20 -15 -10 -5 0 5 10 15
-30 -20 -10 0 10 20 30
Fig. 2. Visualisation of deep feature distributions learned by the ResNet32 models trained on (a) class balanced data, (b) class imbalanced
data, and (c) class imbalanced data with our CRL model. Customised CIFAR-100 object category training image sets were used (more details in
Sec. 4.3). In the illustration, we showed only 3 (2 majority and 1 minority) classes for the visual clarity sake. It is observed that the minority class
(Min-1) is overwhelmed more severely by the two majority classes (Maj-1, Maj-2) when deploying the existing CNN model given class imbalanced
training data. The proposed CRL approach rectiﬁes clearly this undesired model induction bias inherent to existing deep learning architectures.
(3) Hybrid: Combined data-level and algorithm-level rebalancing . These methods still only consider small
scale imbalanced data learning, characterised by: (a) Limited number of data samples and classes, (b) Non-extreme
imbalance ratios, (c) Single-label classiﬁcation, (d) Problemspeciﬁc hand-crafted low-dimensional features. In large,
these classical techniques are poor for severely imbalanced
data learning given big visual data.
There are early neural network based methods for imbalanced data learning . However,
these works still only address small scale imbalanced data
learning with neural networks merely acting as nonlinear
classiﬁers without end-to-end learning. A few recent studies
 have exploited classic strategies in singlelabel deep learning. For example, the binary-class classiﬁcation problem is studied by per-class mean square error loss
 , synthetic minority sample selection , and constant
class ratio in mini-batch . The multi-class classiﬁcation
problem is addressed by online cost-sensitive loss . More
recently, the idea of preserving local class structures (LMLE)
was proposed for imbalanced data deep learning, but without end-to-end model training and with only single-label
oriented training unit design . In contrast, our model is
designed for end-to-end imbalanced data deep learning for
multi-label classiﬁcation, scalable to large training data.
Hard sample mining has been extensively exploited in
computer vision, e.g. object detection , face recognition , image categorisation , and unsupervised
representation learning . The rational for mining hard
negatives (unexpected) is that, they are more informative
than easy negatives (expected) as they violate a model class
boundary by being on the wrong side and also far away
from it. Therefore, hard negative mining enables a model
to improve itself quicker and more effectively with less
training data. Similarly, model learning can also beneﬁt
from mining hard positives (unexpected), i.e. those on the
correct side but very close to or even across a model class
boundary. In our model learning, we only consider hard
mining on the minority classes for efﬁciency. Moreover,
our batch-balancing hard mining strategy differs from that
of LMLE by eliminating exhaustive searching of the
entire training set (all classes), hence computationally more
scalable than LMLE.
Deep metric learning is based on the idea of combining
deep neural networks with metric loss functions in a joint
end-to-end learning process . Whilst adopting
similarly a generic margin based loss function , deep
metric learning does not consider the class imbalanced data
learning problem. In contrast, our method is speciﬁcally designed to address this problem by incrementally rectifying
the structural signiﬁcance of minority classes in a batchwise end-to-end learning process, so to achieve scalable
imbalanced data deep learning.
Deep learning of clothing and facial attributes has been
recently exploited , given the availability
of large scale datasets and deep models’ strong capacity for
learning from big training data. However, existing methods
ignore mostly imbalanced class data distributions, resulting in suboptimal model learning and poor model performance on the minority classes. One exception is the LMLE
model which studies imbalanced data deep learning
 . Compared to our end-to-end learning using mini-batch
hard sample mining on the minority classes only, LMLE is
not end-to-end learning and with global hard mining over
the entire training data, it is computationally complex and
expensive, not lending itself naturally to big training data.
SCALABLE IMBALANCED DEEP LEARNING
For the problem of imbalanced data deep learning from
large training data, we consider the problem of person
attribute recognition, both facial and clothing attributes.
This is a multi-label multi-class learning problem given
imbalanced training data, a generalisation of the more
common single-label binary-/multi-class recognition problem. Speciﬁcally, we wish to construct a deep learning
model capable of recognising multi-labelled person attributes
j=1 in web images, with a total of nattr different attribute labels. Each label zj has its respective class value
range Zj, e.g. multi-class clothing attribute or binaryclass facial attribute. Suppose we have n training images
i=1 with their attribute annotation vectors {ai}n
ai = [ai,1, . . . , ai,j, . . . , ai,nattr] where ai,j refers to the jth attribute class value of the image Ii. The number of
images available for different attribute classes varies greatly
(Fig. 1) therefore imposing a signiﬁcant multi-label imbalanced
class data distribution challenge to model learning. Most
attributes are localised to image regions, even though the
location information is not annotated (weakly labelled). We
consider to jointly learn features and all the attribute label
classiﬁers from class imbalanced training data in an end-toend process. Speciﬁcally, we introduce incremental minority
class discrimination learning by formulating a Class Recti-
ﬁcation Loss (CRL) regularisation. The CRL imposes an
additional batch-wise class balancing on top of the crossentropy loss so to rectify model learning bias due to the
over-representation of the majority classes by promoting
under-represented minority classes (Fig. 3).
Mini-batch
Minority class hard mining
Class Rectification Loss
Class imbalanced data
Fig. 3. Overview of the proposed Class Rectiﬁcation Loss (CRL) regularisation for large scale class imbalanced deep learning.
Limitations of Cross-Entropy Classiﬁcation Loss
Convolutional Neural Networks (CNN) are designed to
take inputs as two-dimensional images for recognition tasks
 . For learning a multi-class (per-label) classiﬁcation CNN
model (details in “Network Architecture”, Sections 4.1, 4.2,
and 4.3), the Cross-Entropy (CE) loss function is commonly
used by ﬁrstly predicting the j-th attribute posterior probability of Ii over the ground truth ai,j:
p(yi,j = ai,j|xi,j) =
k=1 exp(W ⊤
where xi,j refers to the feature vector of Ii for the j-th
attribute label, and Wk is the corresponding classiﬁcation
function parameter. Then compute the overall loss on a
mini-batch of nbs images as the average additive summation
of attribute-level loss with equal weight over all labels:
p(yi,j = ai,j|xi,j)
By design, the cross-entropy loss enforces model learning to respect two conditions: (1) The same-class samples
should have class distributions with the identical peak
position corresponding to the groundtruth one-hot label.
(2) Each class corresponds to a different peak position in
the class distribution. As such, the model is supervised
end-to-end to separate the class boundaries explicitly in the
prediction space and implicitly in the feature space by some
in-between linear or nonlinear transformation. The CE loss
minimises the amount of training error by assuming that
individual samples and classes are equally important. To
achieve model generalisation with discriminative inter-class
boundary separation, it is necessary to have a large training
set with sufﬁciently balanced class distributions (Fig. 2(a)).
Minority class
Majority class
Other classes
Inter-class structure modelling
Inner-class sample modelling
Sample index
Class index
Fig. 4. Illustration of (a) inter-class structure rectiﬁcation around decision
boundary in the CRL, as complementary to (b) the cross-entropy loss
with single-class independent modelling (indicated by dashed arrow).
However, given highly class imbalanced training data,
e.g. X-Domain benchmark, model learning by the conventional cross-entropy loss is suboptimal. The model suffers from generalising inductive decision boundaries biased towards majority classes with ignorance on minority
classes (Fig. 2(b)). To address this problem, we reformulate
the learning objective loss function by explicitly imposing
structural discrimination of minority classes against others, i.e. inter-class geometry structure modelling (Fig. 4). This
stresses the structural signiﬁcance of minority classes in
model learning, orthogonal and complementary to the uniform single-class independent modelling enforced by the crossentropy loss (Fig. 4(b)). Conceptually, this design may bring
simultaneous beneﬁts to majority class boundary learning
as shown in our experimental evaluations (Tables 3 and 5).
Minority Class Hard Sample Mining
We explore a hard sample mining strategy to enhance minority class manifold rectiﬁcation by selectively “borrowing” majority class samples from class decision boundary
marginal (border) regions. Speciﬁcally, we estimate minority
class neighbourhood structure by mining both hard-positive
and hard-negative samples for every selected minority class
in each mini-batch of training data2. Our idea is to rectify
incrementally the per-batch class distribution bias of multilabels in model learning. Hence, each improved intermediate model from per-batch training is less inclined towards
the over-sampled majority classes and more discriminative
to the under-sampled minority classes (Fig. 2(c)). Unlike
LMLE which aims to preserve the local structures of both
majority and minority classes by global clustering of and
sampling from the entire training data, our model design
aims to enhance progressively minority class discrimination
by incremental projective structure reﬁnement. This idea
is inherently compatible with batch-wise hard-positive and
hard-negative sample mining along the model training trajectory. This eliminates the LMLE’s drawback in assuming
that local group structures of all classes can be estimated
reliably by ofﬂine global clustering before model learning.
Incremental Batch-Wise Class Proﬁling For hard sample
mining, we ﬁrst proﬁle the minority and majority classes per
label in each training mini-batch with nbs training samples.
We proﬁle the class distribution hj = [hj
1, . . . , hj
k, . . . hj
2. We only consider those minority classes having at least two sample
images or more in each batch, i.e. ignoring those minority classes
having only one sample image or none. This enables a more ﬂexible loss
function selection, e.g. triplet loss functions which typically requires at
least two matched samples.
Sample index
Probability on class c
In Feature space
Hard positive samples
Hard negative samples
Hard positive samples
Hard negative samples
Fig. 5. Illustration of minority class hard sample mining. (a) Class-level
mining: For each minority class c, hard-positives are those samples from
class c but with low class prediction scores on c by the current model
(red solid circles). Hard-negatives are those with high class c prediction
scores but on the wrong class (blue solid circles). (b) Instance-level
mining: For each sample (dotted red box) of a minority class c, hardpositives are the samples of class c (solid red box) further away from the
given sample of class c in the feature space (pointed by a red arrow).
Hard-negatives are those close to the given sample but from different
classes (pointed by blue arrow). Top-3 hard positive and negative samples are shown in the two examples for conceptual illustration.
over Zj class for each attribute (label) j, where hj
the number of training samples with the j-th attribute value
assigned to class k. We then sort hj
k in the descent order. As
such, we deﬁne the minority classes for attribute label j in
this mini-batch as the smallest classes Cj
min subject to:
k ≤ρ · nbs
In the most studied two-class setting , the minority (majority) class is deﬁned as the one with fewer (more) samples,
i.e. under (above) 50%. However, to our best knowledge
there is no standard deﬁnition for the multi-class case. For
the deﬁnition in Eqn. (3) being conceptually consistent to
the two-class setting, we also set ρ=50%. This means that
all minority classes collectively account for at most half or less
samples per batch. The remaining classes are deemed as the
majority classes. We analysed the effect of choosing different
ρ values on model performance (Table 13).
Given the minority classes, we then consider hard mining therein at two levels: class-level (Fig. 5(a)) and instancelevel (Fig. 5(b)). Let us next deﬁne the “hardness” metrics,
hard samples and their selection.
Hardness Metrics For hard sample mining, it is necessary
to have quantitative metrics for “hardness” measurement.
Two metrics are considered: (1) Score based: A model’s class
prediction score, suitable for class-level hard mining. (2)
Feature based: The feature distance between data points,
suitable for instance-level hard mining.
Class-Level Hard Samples At the class-level, we quantify
the sample hardness regarding a given class per label.
Particularly, for any minority class c of the attribute label
j, we refer “hard-positives” to the images xi,j of class c
(ai,j = c with ai,j the groundtruth class of the attribute
j) given low prediction scores p(yi,j = c|xi,j) on class c
by the thus-far model, i.e. poor recognitions. Conversely, by
“hard-negatives”, we refer to the images xi,j of other classes
(ai,j ̸= c) given high prediction scores on class c by thus-far
model, i.e. obvious mistakes. Formally, we deﬁne them as:
c,j = {xi,j|ai,j = c, low p(yi,j = c|xi,j)}
c,j = {xi,j|ai,j ̸= c, high p(yi,j = c|xi,j)}
where Pcls
c,j and N cls
c,j denote the hard positive and negative
sample sets of a minority class c of the attribute label j.
Instance-Level Hard Samples
At the instance-level, we
quantify the sample hardness regarding any speciﬁc sample
instance xi,j (groundtruth class ai,j = c) from each minority
class c of the attribute label j. Speciﬁcally, we deﬁne “hardpositives” of xi,j as those class c images xk,j (groundtruth
class ak,j = c) by thus-far model with large distances (low
matching scores) from xi,j in the feature space. In contrast,
we deﬁne “hard-negatives” as those images xk,j not from
class c (ak,j ̸=c) with small distances (high matching scores)
to xi,j in the feature space. We formally deﬁne them as:
i,c,j = {xk,j|ak,j = c, large dist(xi,j, xk,j)}
i,c,j = {xk,j|ak,j ̸= c, small dist(xi,j, xk,j)}
where Pins
i,c,j and N ins
i,c,j are the hard positive and negative
sample sets w.r.t. sample instance xi,j of minority class c in
the attribute label j, dist(·) is the Euclidean distance metric.
Hard Mining Intuitively, mining hard-positives enables the
model to discover and expand sparsely sampled minority
class boundaries, whilst mining hard-negatives aims to ef-
ﬁciently improve the margin structures of minority class
boundary corrupted by visually similar distracting classes.
To facilitate and expedite model training, we adopt the top-
κ hard samples mining (selection) strategy. Speciﬁcally, at
training time, for a minority class c of attribute label j (or
a minority class instance xi,j) in each training batch data,
we select κ hard-positives as the bottom-κ scored on c (or
bottom-κ (largest) distances to xi,j), and κ hard-negatives as
the top-κ scored on c (or top-κ (smallest) distance to xi,j),
given the current model (or feature space).
The proposed hard sample mining strategy encourages model learning to concentrate particularly on either weak recognitions or obvious mistakes when discriminating sparsely sampled class margins of the minority classes.
In doing so, the overwhelming bias towards the majority
classes in model learning is mitigated by explicitly stressing
minority class discriminative boundary characteristics. To
avoid useful information of unselected “easier” data samples being lost, we perform scalable hard sample mining
independently in each mini-batch during model training and
incrementally so over successive mini-batches. As a result, all
training samples are utilised randomly in the full learning
cycle. Our model can facilitate naturally both class prediction score and instance feature distance based matching.
The experiments show that class score rectiﬁcation yields
superior performance due to a better compatibility effect
with the score based cross-entropy loss.
Minority Class Neighbourhood Rectiﬁcation
We introduce a Class Rectiﬁcation Loss (CRL) regularisation
Lcrl to rectify model learning bias of the standard CE loss
(Eqn. (2)) due to class imbalanced training data. This is
achieved by incrementally reinforcing the minority class
decision boundary margins with CRL aiming to discover latent class boundaries whilst maximising their discriminative
margins either directly in the decision score space or indirectly in the feature space. We design the CRL regularisation
by the learning-to-rank principle speciﬁcally on
the minority class hard samples, and re-formulate the model
learning objective loss function Eqn. (2) as:
Lbln = α Lcrl + (1 −α) Lce,
α = η Ωimb
where α is a parameter designed to be linearly proportional
to a training class imbalance measure Ωimb. Given different
individual class data sample sizes, we deﬁne Ωimb as the
minimum percentage count of data samples required over
all classes in order to form an overall uniform (i.e. balanced)
class distribution in the training data. Eqn. (8) imposes an
imbalance-adaptive learning mechanism in CRL regularisation – more weighting is assigned to more imbalanced
labels3, whilst less weighting for less imbalanced labels.
Moreover, η is independent of the per-label imbalance,
therefore a model hyper-parameter estimated by crossvalidation (independent of individual class imbalance). In
this study, we explore three loss criteria for Lcrl at both classlevel and instance-level.
(I) Relative Comparison
First, we consider the seminal
triplet ranking loss to model the relative relationship
constraint between intra-class and inter-class. Considering
the small number of training samples in minority classes, it
is sensible to make full use of them in order to effectively
handle the underlying learning bias. Hence, we regard each
minority class sample as an “anchor” in the triplet construction to compute the batch loss balancing regularisation.
Speciﬁcally, for each anchor sample xa,j, we ﬁrst construct a set of triplets based on the mined top-κ hardpositives and hard-negatives associated with either the corresponding class c of attribute label j (for class-level hard
miming), or the sample instance itself xa,j (for instancelevel hard mining). In this way, we form at most κ2 triplets
T = {(xa,j, x+,j, x−,j)s}κ2
s=1 w.r.t. xa,j, and a total of at
most |Xmin| × κ2 triplets T for all the anchors Xmin across
all the minority classes of every attribute label. We then
formulate the following triplet ranking loss to impose a CRL
class balancing constraint:
0, mj + d(xa,j, x+,j) −d(xa,j, x−,j)
where mj denotes the class margin of attribute j and d(·) is
the distance between two samples. We consider both classlevel and instance-level model learning rectiﬁcations4.
For class-level rectiﬁcation, we consider the model predictions between matched and unmatched pairs:
d(xa,j, x+,j) = |pa,j −p+,j|,
d(xa,j, x−,j) = pa,j −p−,j (10)
where p∗,j denotes the model prediction score of x∗,j on the
target minority class c of attribute label j, with ∗∈{a, +, −}.
The intuition is that, the matched pair is constrained to have
similar prediction scores on the true class (both directions
with absolute values), higher than that of any negative sample by a margin mj in a single direction (without absolute
operation). For the triplet ranking, a ﬁxed inter-class margin
is often utilised and we set mj = 0.5 for all attribute
3. Multi-label multi-class, e.g. an attribute label has 6∼55 classes.
4. The maximum operation in Eqn. (9) is implemented by a ReLU
(rectiﬁed linear unit) in TensorFlow.
labels j ∈{1, · · · , nattr}. This ensures a correct classiﬁcation
by the maximum a posteriori probability estimation.
For instance-level rectiﬁcation, we consider the sample
pairwise distance in the feature space as:
d(xa,j, x∗,j) = ∥f(a,j) −f(∗,j)∥2,
where f(·,j) denotes the attribute j feature vector of the corresponding image sample. We adopt the Euclidean distance.
In this case, the mj (Eqn. (9)) speciﬁes the class margin in
the feature space. We apply a geometrically intuitive design:
projecting uniformly all the class centres along a unit circle
and using the arc length between nearby centres as the class
margin. That is, we set the class margin for attribute j as:
where |Zj| is the number of classes.
(II) Absolute Comparison
Second, we consider the contrastive loss to enforce absolute pairwise constraints
on positive and negative pairs of minority classes. This
constraint aims to optimise the boundary of minority classes
by incrementally separating the overlapped (confusing)
majority class samples in batch-wise optimisation. Specifically, for each sample xa,j in a minority class c of an
attribute j, we use the mined hard samples to build positive
P + = {xa,j, x+,j} and negative P −= {xa,j, x−,j} pairs in
each training batch. Intuitively, we require the positive pairs
to be close whilst the negative pairs to be far away in either
model score or sample feature space. Thus, we deﬁne the
d(xa,j, x+,j)2 +
mac −d(xa,j, x−,j), 0
where mac is the between-class margin, which can be set theoretically to an arbitrary positive number . We compute
the average loss separately for positive and negative sets to
balance their importance even given different sizes.
For class-level rectiﬁcation, we consider the model prediction scores of pairs as deﬁned in Eqn. (10). We set mac=0.5 to
encourage correct prediction. For instance-level rectiﬁcation,
we use the Euclidean distance in the feature space (Eqn. (11))
for pairwise comparison. We empirically set mac=1, which
gives satisfactory converging speed and stability in our
experiments.
(III) Distribution Comparison
Third, we formulate class
rectiﬁcation for minority classes by modelling the distribution relationship of positive and negative pairs (built as in
“Absolute Comparison”). This distribution based CRL aims
to guide model learning by mining minority class decisive
regions non-deterministically. In spirit of , we represent
the distribution of positive (P +) and negative (P −) pair sets
with histograms H+ =[h+
1 , · · · , h+
τ ] and H−=[h−
1 , · · · , h−
of τ uniformly spaced bins [b1, · · · , bτ]. We compute the
positive histogram H+ as:
d(xa,j,x+,j)−bt−1
, if d(xa,j, x+,j) ∈[bt−1, bt]
bt+1−d(xa,j,x+,j)
, if d(xa,j, x+,j) ∈[bt, bt+1]
and ∆deﬁnes the pace length between two adjacent bins.
The negative histogram H−can be constructed similarly. To
make minority classes distinguishable from majority classes,
we enforce the two histogram distributions as disjoint as
possible. Formally, we deﬁne the CRL regularisation by how
much overlapping between the two distributions:
Statistically, this CRL distribution loss estimates the probability that the distance of a random negative pair is smaller
than that of a random positive pair, either in the score space
or the feature space. Similarly, we consider both class-level
(Eqn. (10)) and instance-level (Eqn. (11)) rectiﬁcation.
In our experiments (Sec. 4), we compared all six CRL
loss designs. By default we deploy the class-level Relative
Comparison CRL in our experiments if not stated otherwise.
Further Remarks We do not consider exemplars as anchors
from majority classes in CRL because the conventional CE
loss can already model the majority classes well given their
frequent sampling. As demonstrated in our experiments, additional rectiﬁcation on majority classes gives some beneﬁt
but focusing only on minority classes makes the CRL model
more cost-effective (Table 12). Due to the batch-wise design,
the class balancing effect by our proposed regularisor is
incorporated throughout the whole training process progressively. Conceptually, our CRL shares a similar principle
to Batch Normalisation in achieving learning scalability.
EXPERIMENTS
For evaluations, we used both imbalanced and
balanced benchmark datasets. Given Table 1, we selected
the CelebA , X-Domain , and CIFAR-100 (see
Table 2 for statistics) due to: (1) The CelebA provides a
class imbalanced learning test on multiple binary-class facial
attributes with imbalance ratios up to 1:43. Speciﬁcally, it
has 202,599 web in-the-wild images from 10,177 person
identities with on average 20 images per person. Each image
is annotated by 40 attribute labels. Following , we
used 162,770 images for model training (including 10,000
images for validation), and the remaining 19,867 for test. (2)
The X-Domain offers an extremely class imbalanced learning
test on multiple multi-class clothing attributes with the imbalance ratios upto 1:4,162. This dataset consists of 245,467
shop images extracted from online retailers. Each image is
annotated by 9 attribute labels. Each attribute has a different
set of mutually exclusive class values, sized from 6 (“sleevelength”) to 55 (“colour”). In total, there are 178 distinctive
attribute classes over the 9 labels. We randomly selected
165,467 images for training (including 10,000 images for validation) and the remaining 80,000 for test. (3) The CIFAR-100
provides a single-label class balanced learning test. This benchmark contains 100 classes with each having 600 images. This
test provides a complementary evaluation of the proposed
method against a variety of benchmarking methods, and
moreover, facilitates extra in-depth model analysis under
simulated class imbalanced settings. We used the standard
490/10/100 training/validation/test split per class .
Performance Metrics
The classiﬁcation accuracy 
that treats all classes uniformly is not appropriate for class
imbalanced test, as a naive classiﬁer that predicts every test
sample as majority classes can still achieve a high overall
accuracy although it fails all minority class samples. Since
we consider the multi-class imbalanced classiﬁcation test,
the common true/false (positive/negative) rates for binaryclass classiﬁcation are no longer valid. In this work, we
adopt the sensitivity measure that leads to a class-balanced
accuracy by considering particularly the class distribution
statistics and generalises the conventional binary-class
criterion . Formally, we compute the per-class sensitivity
based on the classiﬁcation confusion matrix as:
Si = n(i,i)
n(i,j), i ∈{1, 2, · · · , c}
where n(i,j) is the number of class i test samples predicted
by a model as class j, and ni is the size of class i (totally
c classes). Therefore, the confusion matrix diagonal refers
to correctly classiﬁed sample numbers of individual classes
whilst the off-diagonal to the incorrect numbers. We deﬁne
the class-balanced accuracy (i.e. mean sensitivity) as:
The above metric is for the single-label case. For the multilabel test, we average the mean sensitivity measures over all
labels (attributes) to give the overall class-balanced accuracy.
Imbalanced Learning Methods for Comparison We considered ﬁve existing class imbalanced learning methods: (1)
Over-Sampling : A multi-label re-sampling strategy to
build a more balanced set before model learning through
over-sampling minority classes by random replication. (2)
Down-Sampling : Another training data re-sampling
method based on under-sampling majority classes with
random sample removal. (3) Cost-Sensitive : A classweighting strategy by assigning greater misclassiﬁcation
penalties to minority classes and smaller penalties to majority classes in loss design. We assign the class weight as
wi = exp(−ri) where ri speciﬁes the ratio of class i in
training data. (4) Threshold-Adjustment : Adjusting the
model decision threshold in test time by incorporating the
class probability prior ri, e.g. moderating the original model
prediction pi to ˜pi = pi ∗exp(−ri)T where T ∈{1, 2, 3, 4, 5}
is a temperature (softening) parameter estimated by cross
validation. Given ˜pi, we then use the maximum a posteriori
probability for class prediction. (5) LMLE : A state-ofthe-art class imbalanced deep learning model exploiting
the class structure for improving minority class modelling.
For fair comparisons, all the methods were implemented
on the same network architecture (details below), with the
parameters set by following the authors’ suggestions if
available or cross-validation. All models were trained on
the same training data, and evaluated on the same test data.
We adopted the class-level relative comparison CRL for all
remaining experiments if not stated otherwise.
Statistics of the three datasets utilised in our evaluations.
Total Images
Training Images
Test Images
CelebA 
Facial Attribute
Multiple (40)
Binary (2)
162,770 (3,713∼135,779/class) 19,867 (432∼17,041/class)
X-Domain 
Clothing Attribute
Multiple (9)
Multiple (6∼55)
165,467 (13∼132,870/class)
80,000 (4∼64,261/class)
CIFAR-100 
Object Category
Single (1)
Multiple (100)
50,000 (500/class)
10,000 (100/class)
Facial attribute recognition on the CelebA benchmark . “*”: Class imbalanced learning models. Metric: Class-balanced accuracy (%). Instance
level hard mining. The 1st/2nd best results are indicated in red/blue. MthOpen: Mouth Open; HighChb: High Cheekbones; HvMkup: Heavy Makeup;
WvHair: Wavy Hair; OvFace: Oval Face; PntNose: Pointy Nose; ArEyeb: Arched Eyebrows; BlkHair: Black Hair; StrHair: Straight Hair; BrwHair:
Brown Hair; BldHair: Blond Hair; GrHair: Gray Hair; NrwEye: Narrow Eyes; RcdHl: Receding Hairline; 5Shdw: 5 oclock Shadow; BshEb: Bushy
Eyebrows; RsChk: Rosy Cheeks; DbChn: Double Chin; EyeGls: Eyeglasses; SdBurn: Sideburns; Mstch: Mustache; PlSkin: Pale Skin.
Attributes
Attractive
Imbalance ratio (1:x)
Triplet-kNN 
PANDA 
DeepID2 
Over-Sampling* 
Down-Sampling* 
Cost-Sensitive* 
Threshold-Adjustment* 
LMLE* 
Attributes
Imbalance ratio (1:x)
Triplet-kNN 
PANDA 
DeepID2 
Over-Sampling* 
Down-Sampling* 
Cost-Sensitive* 
Threshold-Adjustment* 
LMLE* 
CRL* (Ours)
Comparisons on Facial Attributes Recognition
Competitors We compared the proposed CRL model with 9
existing methods including the 5 class imbalanced learning
models above and other 4 state-of-the-art deep learning
models for facial attribute recognition on the CelebA benchmark: (1) PANDA , (2) ANet , (3) Triplet-kNN ,
and (4) DeepID2 .
Network Architecture
We adopted the 5-layers CNN architecture DeepID2 as the base network for training
all class imbalanced learning methods including CRL and
LMLE. Training DeepID2 was based on the conventional
CE loss (Eqn. (2)). This provides a baseline for evaluations
with and without CRL. Moreover, the CRL allows multi-task
learning in the spirit of , with an additional 64-dim
FC2 feature layer and a 2-dim binary prediction layer for
each face attribute.
Parameter Settings We trained the CRL from scratch by the
learning rate at 0.001, the decay at 0.0005, the momentum at
0.9, the batch size at 256, and the epoch at 921. We set the
loss weight η (Eqn. (8)) to 0.01.
Overall Evaluation
Facial attribute recognition performance comparisons are shown in Table 3. It is evident that
CRL outperforms all competitors including the attribute
recognition models and class imbalanced learning methods
on the overall mean accuracy. Compared to the best nonimbalanced learning model DeepID2, CRL improves the
average accuracy by 6%. Compared to the state-of-theart imbalanced learning model LMLE, CRL is better on
Accuracy gain (%)
Attractive
Month Open
Wear Lipstick
High Cheekbones
Heavy Makeup
Pointy Nose
Arche Eyebrows
Black Hair
Straight Hair
Bags under Eyes
Wear Earrings
Blond Hair
Bushy Eyebrows
Wear Necklace
Narrow eyes
5 clock shadow
Receding hairline
Wear Necktie
Eyeglasses
Rosy Cheeks
Double Chin
Imbalance ratio increases
Brown Hair
Performance additional gain over the DeepID2 by the LMLE
and CRL models on the 40 CelebA binary-class facial attributes .
Attributes are sorted from left to right in increasing class imbalance ratio.
average accuracy by 3%. Other classic imbalanced learning
methods perform inferiorly than both CRL and LMLE. In
particular, Over-Sampling brings only marginal gain with
no clear difference across all imbalance degrees, suggesting
that replication based data rebalance is limited in introducing useful information. Cost-Sensitive is largely similar to
Over-Sampling. The performance drop by Down-Sampling
and Threshold-Adjustment is due to discarding useful data
in balancing the class data distribution and imposing potentially inconsistent adjustment to model prediction. This
shows that (1) not all class imbalanced learning methods
are helpful, and (2) the clear superiority of our batch-wise
incremental minority class rectiﬁcation method in handling
biased model learning over alternative methods.
Performance additional gain over the DeepID2 by LMLE and CRL on
bottom-20 and top-20 CelebA facial attributes in mean class-balanced
accuracy (%). Negative number means performance drop.
Imbalance Ratio
Bottom-20 (1:1∼1:5)
Top-20 (1:6∼1:43)
Fig. 7. Examples (3 pairs) of facial attribute recognition (imbalance ratio
in bracket). For each pair (attribute), DeepID2 missed both, whilst CRL
identiﬁed the image with green box but failed the image with red box.
Further Analysis We examined the characteristics of model
performance on individual attributes exhibiting different
class imbalance ratios. In particular, we further analysed
CRL and the best competitor LMLE against the base model
DeepID2 without class imbalanced learning. To that end,
we split the 40 facial attributes into two groups at a 1:5
imbalance ratio: bottom-20 (the ﬁrst 20 in Table 3) and
top-20 (the remaining) imbalanced attributes. Figure 6 and
Table 4 show that: (1) CRL improves the prediction accuracy on all attributes (above “0”), whilst LMLE can give
weaker prediction than DeepID2 especially on highly imbalanced attributes. This suggests that CRL is more robust
in coping with different imbalanced attributes, especially
more extremely imbalanced classes. (2) LMLE is better at
the bottom-20 imbalanced attributes, improving the mean
accuracy by 7% versus 5% by CRL. For instance, CRL is
outperformed by LMLE on the “Attractive” (balanced) and
“Heavy Makeup” attributes by 7% and 8%, respectively.
This suggests that LMLE is better for less-extremely imbalanced attributes. (3) LMLE performance degrades on top-20
imbalanced attributes by 2% in mean accuracy. Speciﬁcally,
LMLE performs worse than DeepID2 on most attributes
with imbalance ratio greater than 1:7, starting from “Wear
Necklace” in Table 3. This is in contrast to CRL which
achieves an even better performance gain at 6% on top-
20. On some very imbalanced attributes, CRL outperforms
LMLE signiﬁcantly, e.g. by 20% on “Mustache” and 27%
on “Blurry”. Interestingly, the “Blurry” attribute is visually
challenging due to its global characteristics not deﬁned by
local features therefore very subtle, similar to the “Mustache” attribute (see Fig. 7). This demonstrates that CRL
is superior and more scalable than LMLE in coping with
severely imbalanced data learning. This is due to (1) incremental batch-wise minority class predictive boundary
rectiﬁcation which is independent to global training data
class structure, and (2) end-to-end deep learning for joint
feature and classiﬁer optimisation which LMLE lacks.
Model Training Cost Analysis
We analysed the model
training cost of CRL and LMLE on a workstation with 1
NVIDIA Tesla K40 GPU and 20 E5-2680 @ 2.70GHz CPUs.
For LMLE, we used the codes released by the authors5 with
the original settings (4 rounds of training each with 5, 000
5. The k-means clustering function is not included in the original
codes. We used the VLFeat’s implementation with the default
setting as 1, 000 maximum iterations and 10 repetitions.
iterations of CNN optimisation). The training was initialised
by pre-trained DeepID2 face recognition features. On our
workstation, LMLE took a total of 264.8 hours to train6,
with each round taking 66.2 hours including 24.5 hours
for “clustering+quintuplet construction” and 41.7 hours for
“CNN model optimisation”. In contrast, CRL took 27.2
hours, that is 9.7 (264.8/27.2) times faster than LMLE.
We further examined model convergence rate. Speciﬁcally, LMLE converges quicker than CRL on training batch
iterations, LMLE’s 20,000 versus CRL’s 540,000. This is reasonable as LMLE beneﬁts uniquely from both a speciﬁcally
designed data structural pre-processing (building quintuplets) of the entire training data which is a computationally
expensive procedure, and a model pre-training process on
auxiliary face recognition labels. However, LMLE is significantly slower than CRL in the overall CNN training time:
LMLE’s 166.6 hours versus CRL’s 27.2 hours.
Comparisons on Clothing Attributes Recognition
Competitors Except the ﬁve imbalanced learning methods,
we also compared CRL against four other state-of-the-art
clothing attribute recognition models:
(1) DDAN , (2)
DARN , (3) FashionNet7 , and (4) MTCT .
Network Architecture We used the same network structure
as the MTCT . Speciﬁcally, this network is composited of
ﬁve stacked NIN conv units and nattr parallel branches
with each a 3-FC-layers sub-network for modelling a distinct
attribute respectively, in the multi-task learning spirit .
We trained MTCT using the CE loss (Eqn. (2)).
Parameter Settings
We pre-trained the base network on
ImageNet-1K at the learning rate 0.01, then ﬁne-tuned
the CRL model on the X-Domain images at a lower learning
rate 0.001. We set the decay to 0.0005, the momentum to 0.9,
the batch size to 128, and the epoch to 256. We set the loss
weight η (Eqn. (8)) to 0.01.
Overall Evaluation Table 5 shows the comparative evaluation of 10 different models on the X-Domain benchmark. It
is evident that CRL surpasses all prior state-of-the-art models on all attribute labels. This shows the superiority and
scalability of our incremental minority class rectiﬁcation in
tackling extremely imbalanced attribute data, with the maximum imbalance ratio 4,162 versus 43 in CelebA attributes.
For example, CRL surpasses the best competitor LMLE
by 4.65% in mean accuracy. Traditional class imbalanced
learning methods behave similarly as on facial attributes,
except that Threshold-Adjustment also yields a small gain
similar as Cost-Sensitive. Other models without an explicit
imbalanced learning mechanism like DDAN, FashionNet,
DARN and MTCT suffer notably.
Further Analysis
We further examined the performance
of CRL and LMLE in comparison to the base model MTCT.
Similar to CelebA, we split the 9 attributes into two groups
at a 1:5 class imbalance ratio: bottom-1 (the ﬁrst column
6. We did not consider the time cost for pre-training the DeepID2
(needed for extracting the initial features for the ﬁrst round of data preprocessing) on face identity labels from CelebFaces+ due to lacking
the corresponding codes and details. We used the pre-trained DeepID2
model thanks to the helpful sharing by the LMLE authors.
7. We implemented this FashionNet without the landmark detection
branch since no landmark labels are available in the X-Domain dataset.
Clothing attributes recognition on the X-Domain dataset . “*”: Imbalanced data learning models. Metric: Class-balanced accuracy (%). Slv-Shp:
Sleeve-Shape; Slv-Len: Sleeve-Length. The 1st/2nd best results are highlighted in red/blue.
Attributes
Imbalance ratio (1:x)
FashionNet 
Over-Sampling* 
Down-Sampling* 
Cost-Sensitive* 
Threshold-Adjustment* 
LMLE* 
CRL* (Ours)
Performance additional gain over the MTCT by LMLE and CRL on
bottom-1 and top-8 X-Domain clothing attributes in mean accuracy (%).
Imbalance Ratio
Bottom-1 (1:2)
Top-8 (1:138∼1:4,162)
Imbalance ratio increases.
Accuracy gain (%)
Pattern Clothing
Fig. 8. Performance additional gain over the MTCT by the LMLE and
CRL models on 9 X-Domain multi-class clothing attributes with the
imbalance ratios (numbers under the bars) increasing from left to right.
in Table 5) and top-8 (the remaining). Figure 8 and Table
6 show that: (1) LMLE improves MTCT on all clothing
attributes with varying imbalance ratios. This suggests that
LMLE does address the imbalanced data learning problem
in a multi-class setting by embedding local class structures
into deep feature learning. (2) Compared to LMLE, CRL
achieves more signiﬁcant performance gains on more
severely imbalanced attributes. On the top-8 imbalanced
attributes, CRL achieves mean accuracy gain of 7.10% versus
2.10% by LMLE (Table 6). In particular, our CRL improves
LMLE by 10.03% in accuracy for recognising “Sleeve
Shape”, a ﬁne-grained and visually ambiguous attribute
due to its locality and subtle inter-class discrepancy (Fig. 9).
This evidence is interesting as it shows that class training
data distribution affects a model’s ability to learn effectively
ﬁne-grained class discrimination. Importantly, a model’s
ability in coping effectively with class imbalanced data
learning can help improve its learning of ﬁne-grained class
discrimination. This further demonstrates the strength of
CRL over existing models for mitigating model learning
bias given severely imbalanced ﬁne-grain labelled classes
in an end-to-end deep learning framework.
Model Training Cost Analysis
We examined the model
training cost of LMLE and CRL on X-Domain using the
same workstation as on CelebA. We used the original author released codes with the suggested optimisation setting,
e.g. trained the LMLE for 4 rounds each with 5,000 CNN
FormalSkirt
SolidColor
LongSleeve
BatWingSlv
CottonClothes
SolidColor
WoolenCoat
HiddenButton
SolidColor
SolidColor
SolidColor
LanternSlv
SingleBreasted1
SolidColor
Examples of clothing attribute recognition by the CRL model,
with false attribute prediction in red (Red box: clothing auto-detection).
training iterations. We started with the ImageNet-1K trained
VGGNet16 features . For model training, LMLE took
429.9 hours, with each round taking 107.5 hours including
27.6 hours for “clustering+quintuplet construction” and 79.9
hours for “CNN model optimisation”. In contrast, CRL took
60.4 hours, that is 7.1 (429.9/60.4) times faster than LMLE.
Evaluation of CRL on clothing attribute recognition with the
DeepFashion benchmark . Metric: Class-balanced accuracy (%).
ImbRatio (1:x)
Further Evaluation We further evaluated the CRL on the
DeepFashion clothing attribute dataset with a controlled experiment. We adopted a test setting that is consistent with all the other experiments: (1) The standard
multi-label classiﬁcation setting without using the clothing
landmark and category labels (used in ). (2) ResNet50
 as the base network trained by the CE loss. (3) Top-
5 attribute predictions in a class-balanced accuracy metric
other than a class-biased metric as in . We adopted the
standard data split: 209,222/40,000/40,000 images for model
training/validation/test. We trained the deep models from
scratch with the learning rate as 0.01, the decay as 0.00004,
the batch size as 64, and the epoch as 141. We focused on
evaluating the additional effect of CRL on top of the CE
loss. Table 7 shows that CRL yields a 2.36% (54.56-52.20)
boost in mean accuracy.
Comparisons on Object Category Recognition
We evaluated the CRL on a popular class balanced singlelabel object category benchmark CIFAR-100 .
Object classiﬁcation performance (%) on CIFAR-100 .
Accuracy (%)
Class index
Sample number
Accuracy (%)
Fig. 10. (a) Simulated imbalanced training data distributions on CIFAR-
100 . (b) Performance gains of ResNet32 by the CRL on differently
imbalanced training data. Metric: Mean class-balanced accuracy (%).
Network Architecture We evaluated the CRL in three stateof-the-art CNN models: (1) CifarNet , (2) ResNet32 ,
and (3) DenseNet . Each CNN model was trained by the
conventional CE loss (Eqn. (2)). The purpose is to test their
performance gains in single-label object classiﬁcation when
incorporating the proposed CRL regularisation (Eqn. (8)).
Parameter Settings We trained each CNN from scratch with
the learning rate at 0.1, the decay at 0.0005, the momentum
at 0.9, the batch size at 256, and the epoch at 200. In the class
balanced test, we cannot directly deploy our loss formulation
(Eqn. (8)) as the imbalance measure Ωimb =0 hence eliminating the CRL. Instead, we integrated our CRL with the CE
loss using equal weight by setting α = 0.5 for all models.
For the class imbalanced cases, we set η = 0.01/0.5/0.5 for
CifarNet/ResNet32/DenseNet, respectively.
(I) Comparative Evaluation
Table 8 shows the singlelabel object classiﬁcation accuracy. Interestingly, it is found
that our CRL approach can consistently improve state-ofthe-art CNN models, e.g. increasing the accuracy of CifarNet/ResNet32/DenseNet by 3.6%/1.2%/0.8%, respectively. This shows that the advantages of our batch-wise
minority class rectiﬁcation method remain on class balanced
cases. The plausible reasons are: (1) Whilst the global class
distribution is balanced, random sampling of mini-batch
adopted by common deep learning may introduce some imbalance in each iteration. Our per-batch balancing strategy
hence has the chance to regularise inter-class margin and
beneﬁt the overall model learning. (2) The CRL considers
the optimisation of class-level structural separation, which
can provide a complementary beneﬁt to the CE loss that
instead performs per-sample single-class optimisation.
(II) Effect Analysis of Imbalanced Training Data
further evaluated the deep model performance and the CRL
under different imbalance ratios. To this end, we carried
out a controlled experiment by simulating class imbalance
Effect of the CRL in different CNN models given class imbalanced
training data (γ = 1). Metric: Mean class-balanced accuracy (%).
Training Dataset
CIFAR-100bln(1)
CIFAR-100imb(1)
CIFAR-100imb(1)
cases in training data. Speciﬁcally: (1) We simulated class
imbalanced training data by a power-law class distribution
as (Fig. 10(a)): fCS(i) =
iγ+b, where i ∈{1, 2, · · · , 100}
is the class index, γ represents a preset parameter for
controlling the imbalanced degree, a and b are two numbers estimated by the largest (500) and smallest (25) class
size. We call the resulted training set “CIFAR-100imb(γ)”. (2)
We constructed a corresponding dataset “CIFAR-100bln(γ)”,
subject to having the same number of images covering all
classes as “CIFAR-100imb(γ)” and being class balanced (i.e.
all classes are equally sized). This is necessary as “CIFAR-
100imb(γ)” and “CIFAR-100” differ in both data balance
and size thus not directly comparable. (3) We trained the
CNN models with and without CRL on these simulated
training sets separately and tested their performances on
the same standard test data. (4) To compare deep learning
methods with conventional models, we also evaluated the
k-nearest neighbour classiﬁer with the HOG feature .
Table 9 shows the results when γ = 1. We observed that:
(1) Given class imbalanced training data, all three CNN
models are adversely affected, with accuracy decreased by
3.9% (CifarNet), 6.1% (ResNet32), and 6.4% (DenseNet)
respectively. Interestingly, the stronger CNNs suffer more
performance degradation. (2) CRL improves all three CNN
models by 2.0% ∼3.2% in accuracy, which show the effectiveness of CRL. (3) All three deep learning models are
sensitive to imbalanced training data with similar relative
performance drops as the conventional non-deep-learning
HOG+kNN model. This suggests that deep learning models
are not necessarily superior in tackling the class imbalanced
learning challenge. Moreover, we evaluated the CRL with
ResNet32 given different imbalance cases γ ranging from
0.2 to 1.0. Figure 10 (b) shows its performance gains across
all these settings. We observed no clear trend between model
performance and γ since their relationship is non-linear. In
particular, the model generalisation depends not only on
the class distribution but also on other factors such as the
speciﬁc training samples, i.e. information content is variable
(and unknown) over training samples.
Further Evaluations and Discussions
We conducted component analysis for providing more insights on CRL. By default, we adopted the class-level relative comparison based CRL (Eqn. (9)) and used the most
imbalanced X-Domain dataset, unless declared otherwise.
Comparing hard mining schemes (Class/Instance) and CRL loss
functions (Relative (Rel), Absolute (Abs), and Distribution (Dis)).
Metric: Gain in the mean class-balanced accuracy (%).
CelebA 
X-Domain 
Loss Design
Instance Level
Class Level
CRL Design We evaluated the two hard mining schemes
(class-level and instance-level, Sec. 3.2), and three loss types
(relative, absolute, and distribution comparison, Sec. 3.3). We
tested therefore 6 CRL design combinations in the comparison with baseline models without imbalanced learning:
“DeepID2” on CelebA and “MTCT” on X-domain. Table 10
shows that: (1) All CRL models improve the mean accuracy
consistently, with the CRL(Class+Rel) the best. (2) With the
same loss type, the class-level design is superior in most
cases. This suggests that regularising the score space is more
effective than the feature space. A plausible explanation is
that the former is more compatible with the conventional
CE loss which also operates with class scores.
Attribute index
Accuracy (%)
η=0.1 (78.64)
η=0.01 (80.41)
η=0.001 (76.17)
Clothing Attributes
η=0.1 (78.64)
η=0.01 (80.41)
η=0.001 (76.17)
Fig. 11. Effect of the weight between the CE loss and our CRL on Xdomain by varying η in Eqn. (8). Metric: Class-balanced accuracy (%).
The mean accuracy for each setting is given in the parentheses.
Loss Weight Optimisation We evaluated the effectiveness
of the weight between the CE loss and the CRL loss by
tuning the coefﬁcient η in Eqn. (8) on a range from 0.001
to 0.1 using the X-Domain benchmark. Figure 11 shows
that the best weight selection is η = 0.01. Moreover, it
is found that the change in η affects the performance on
most or all attributes consistently. This indicates that the
CRL formulation with loss weighting is imbalance adaptive,
capable of effectively modelling multiple attribute labels
with diverse class imbalance ratios by a single-value hyperparameter (η) optimisation using cross-validation.
Effect of the CRL hard mining (HM) and joint learning (JL) in
comparison to the LMLE on X-Domain.
CRL(HM+JL)
Mean Accuracy (%)
Hard Mining and Joint Learning We further evaluated
the individual effects of Hard Mining (HM) and Joint
Learning (JL) the features and classiﬁer in the CRL model
“CRL(HM+JL)”, as in comparison to LMLE8 . Table 11
shows the performance beneﬁt of the proposed CRL Hard
Mining (HM) as compared to CRL without HM “CRL(JL)”,
with a 1.53% (80.42-78.89) mean accuracy advantage on X-
Domain. It also shows that the joint learning in CRL has a
mean accuracy advantage of 3.12% (78.89-75.77) over LMLE
which has no joint learning but has hard mining.
Top-κ We examined the effect of different κ values in hard
mining from 1 to 175 with step-size 25. Figure 12 shows that
when κ=1 (i.e. hardest mining), the model fails to capture
a good converging trajectory. This is because the hardest
mining represents over sparse and possibly incorrect (due to
outlier noise) class boundaries, which hence causes poorer
optimisation. When κ>=25, there is no further improvement to model learning. Given that larger κ increases the
model training cost, we set κ=25 for all our experiments.
8. In this evaluation, we treat LMLE as a whole without separating/removing its built-in hard mining mechanism.
Accuracy (%)
Training epoch number
Accuracy (%)
Fig. 12. Effect of κ (sample numbers) in hard mining on X-Domain.
Effect of the CRL class scope on X-Domain.
CRL Class Scope
Mean Accuracy (%)
Training Time
Minority Classes
60.4 Hours
All Classes
77.6 Hours
Class Scope We also evaluated the effect of the class scope
(Minority-Classes and All-Classes) on which the CRL regularisation is enforced in terms of both accuracy and model
training cost. Table 12 shows that applying the CRL to all
classes in each batch yields superior performance. Speciﬁcally, relative to the baseline MTCT’s 73.53% mean accuracy
(Table 5), the scopes of Minority-Classes and All-Classes
bring 6.89% (80.42-73.53) and 7.78% (81.30-73.53) accuracy
gain, respectively. That is, the latter yields additional 12.9%
((7.78-6.89)/6.89) gain but at 28.5% ((77.6-60.4)/60.4) extra
training cost. This suggests better cost-effectiveness by focusing only on minority classes in imbalanced data learning.
Effect of minority class criterion (Eqn. (3)) on X-Domain.
Minority Class Criterion
ρ=50% (Ours)
Mean Accuracy (%)
Minority Class Criterion At last, we evaluated the effect of
minority class criterion (ρ in Eqn. (3)) on a range from 10%
to 50% to generalise the two-class minority class deﬁnition
to a multi-class setting. Table 13 shows the effect on model
performance when ρ changes, demonstrating that a minority class criterion setting with ρ=50% is both most effective
and conceptually consistent with the two-class setting.
CONCLUSION
In this work, we introduced an end-to-end class imbalanced deep learning framework for large scale visual data
learning. The proposed Class Rectiﬁcation Loss (CRL) approach is characterised by batch-wise incremental minority
class rectiﬁcation with a scalable hard mining principle.
Speciﬁcally, the CRL is designed to regularise the inherently
biased deep model learning behaviour given extremely
imbalanced training data. Importantly, CRL preserves the
model optimisation convergence characteristics of stochastic
gradient descent, therefore allowing for efﬁcient end-toend deep learning on signiﬁcantly imbalanced training data
with multi-label semantic interpretations. Comprehensive
experiments were carried out to show the clear advantages
and scalability of the CRL method over not only the state-ofthe-art imbalanced data learning models but also dedicated
deep learning visual recognition methods. For example, the
CRL surpasses the best alternative LMLE by 3% on the
CelebA facial attribute benchmark and 5% on the extremely
imbalanced X-Domain clothing attribute benchmark, whilst
enjoying over 7× faster model training advantage. Our
experiments also show the beneﬁts of the CRL in learning
standard deep models given class balanced training data.
Finally, we provided detailed component analysis for giving
insights into the characteristics of the CRL model design.
ACKNOWLEDGEMENTS
We shall thank Victor Lempitsky for providing the histogram
loss code, Chen Huang and Chen Change Loy for sharing the
pre-trained DeepID2 face recognition model. This work was
partly supported by the China Scholarship Council, Vision Semantics Ltd., the Royal Society Newton Advanced Fellowship
Programme (NA150459), and Innovate UK Industrial Challenge
Project on Developing and Commercialising Intelligent Video
Analytics Solutions for Public Safety (98111-571149).