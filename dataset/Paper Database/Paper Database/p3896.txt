Diﬀerentiable Learning of Quantum Circuit Born Machine
Jin-Guo Liu1 and Lei Wang1,2∗
1Institute of Physics, Chinese Academy of Sciences, Beijing 100190, China and
2CAS Center for Excellence in Topological Quantum Computation,
University of Chinese Academy of Sciences, Beijing 100190, China
Quantum circuit Born machines are generative models which represent the probability distribution of classical
dataset as quantum pure states. Computational complexity considerations of the quantum sampling problem
suggest that the quantum circuits exhibit stronger expressibility compared to classical neural networks. One can
eﬃciently draw samples from the quantum circuits via projective measurements on qubits. However, similar
to the leading implicit generative models in deep learning, such as the generative adversarial networks, the
quantum circuits cannot provide the likelihood of the generated samples, which poses a challenge to the training.
We devise an eﬃcient gradient-based learning algorithm for the quantum circuit Born machine by minimizing
the kerneled maximum mean discrepancy loss. We simulated generative modeling of the Bars-and-Stripes
dataset and Gaussian mixture distributions using deep quantum circuits. Our experiments show the importance
of circuit depth and gradient-based optimization algorithm. The proposed learning algorithm is runnable on
near-term quantum device and can exhibit quantum advantages for generative modeling.
INTRODUCTION
Unsupervised generative modeling is at the forefront of
deep learning research . Unlike the extremely successful
discriminative tasks such as supervised classiﬁcation and
regression, the goal of generative modeling is to model the
probability distribution of observed data and generate new
samples accordingly. Generative modeling ﬁnds wide applications in computer vision , speech synthesis , as well
as chemical design .
And it is believed to be a crucial
component towards artiﬁcial general intelligence. However,
generative modeling is more challenging than discriminative
tasks since it requires one to eﬃciently represent, learn and
sample from high-dimensional probability distributions .
In parallel to the rapid development of deep learning, there
is heated ongoing research to fabricate intermediate scale
quantum circuits . Quantum hardware may show greater
potential than their classical counterparts in generative tasks.
Multiple schemes have been proposed to boost the performance of classical generative models using quantum devices.
For example, quantum Boltzmann machines generalize
the energy function of classical Boltzmann machines to
quantum Hamiltonian for possible stronger representational
power and faster training.
A concern which may prevent
the quantum Boltzmann machine from surpassing its classical
counterpart is the limited connectivity on the actual quantum
hardware . Ref. 12 introduces a quantum generalization
of the probabilistic graphical model , which can be
exponentially more powerful than its classical counterpart
and has exponential speedup in training and inference at
least for some instances under reasonable assumptions in
computational complexity theory.
Another class of arguably simpler quantum generative
models named Born machines directly exploit the
inherent probabilistic interpretation of quantum wavefunctions . Born machines represent probability distribution
∗ 
using quantum pure state instead of the thermal distribution
like the Boltzmann machines . Therefore, Born machines
can directly generate samples via projective measurement on
the qubits, in contrast to the slow mixing Gibbs sampling approach. Moreover, computational complexity considerations
on quantum sampling problems suggest that a quantum circuit
can produce probability distribution that is #P-hard , which is infeasible to simulate eﬃciently using classical
algorithms. The same reasoning underlines the current eﬀorts
towards "quantum supremacy" experiments by sampling outcomes of random quantum circuits . Ref. 14 performed
a classical simulation of the Born machine using the matrix
product state representation of the quantum state. It will be
even more promising to realize the Born machines using quantum circuits since one can at least eﬃciently prepare some of
the tensor networks on a quantum computer . Recently,
Ref. demonstrated experimental realization of the Born
machine on a four qubits shallow quantum circuit trained by
gradient-free optimization of measurement histograms.
To further scale up the quantum circuit Born machine
(QCBM) to larger number of qubits and circuit depth, one
needs to devise an appropriate objective function for the
generative tasks without explicit reference to the model probability. Unlike the tensor network simulation , QCBM
belongs to implicit generative models since one does not have
access to the wavefunction of an actual quantum circuit. Thus,
QCBM can be used as a simulator to generate samples without
access to their likelihoods, which is similar to the notable
generative adversarial networks (GAN) . Compared
to generative models with explicit likelihoods such as the
Boltzmann machines , normalizing ﬂows , and
variational autoencoders , the implicit generative
models can be more expressive due to less restrictions in their
network structures. On the other hand, having no direct access
to the output probability also poses challenge to the scalable
training of quantum circuits.
Moreover, one also needs better learning algorithm than
the gradient-free optimization scheme , especially given
the noisy realization of current quantum circuits. Similarly,
scalability of the optimization scheme is also a crucial concern
 
in deep learning, in which deep neural networks can even
reach billions of parameters . In the history of machine
learning, gradient-free algorithms were employed to optimize
small-scale neural networks .
However, they failed to
scale up to a larger number of parameters. It is the backpropagation algorithm which can eﬃciently compute the
gradient of the neural network output with respect to the
network parameters enables scalable training of deep neural
nets. It is thus highly demanded to have scalable quantum algorithms for estimating gradients on actual quantum circuits.
Recently, gradient-based learning of quantum circuits has
been devised for quantum control and discriminative
tasks . Although they are still less eﬃcient compared
to the back-propagation algorithm for neural networks, these
unbiased gradient algorithms can already greatly accelerate
the quantum circuit learning. Unfortunately, direct application
of these gradient algorithms to QCBM training is still
non-trivial since the output of the generative model is genuinely bit strings which follow high-dimensional probability
distributions.
In fact, it is even an ongoing research topic
in deep learning to perform diﬀerentiable learning of implicit
generative model with discrete outputs .
In this paper, we develop an eﬃcient gradient-based learning algorithm to train the QCBM. In what follows, we ﬁrst
present a practical quantum-classical hybrid algorithm to train
the quantum circuit as a generative model in Sec. II, thus
realize a Born machine.
Then we apply the algorithm on
3 × 3 Bars-and-Stripes and double Gaussian peaks datasets
in Sec. III. We show that the training is robust to moderate
sampling noise, and is scalable in circuit depth. Increasing
the circuit depth signiﬁcantly improves the representational
power for generative tasks. Finally, we conclude and discuss
caveats and future research directions about the QCBM in
MODEL AND LEARNING ALGORITHM
Given a dataset D = {x} containing independent and identically distributed (i.i.d.) samples from a target distribution
π(x), we set up a QCBM to generate samples close to the
unknown target distribution. As shown in Fig. 1, the QCBM
takes the product state |0⟩as an input and evolves it to a
ﬁnal state |ψθ⟩by a sequence of unitary gates. Then we can
measure this output state on computation basis to obtain a
sample of bits x ∼pθ(x) = |⟨x|ψθ⟩|2. The goal of the training
is to let the model probability distribution pθ approach to π.
We employ a classical-quantum hybrid feedback loop as
the training strategy. The setup is similar to the Quantum
Approximate Optimization Algorithm (QAOA) and
the Variational Quantum Eigensolver (VQE) .
constructing the circuits and performing measurements repeatedly we collect a batch of samples from the QCBM.
Then we introduce two-sample test as a measure of distance
between generated samples and training set, which is used
as our diﬀerentiable loss. Using a classical optimizer which
takes the gradient information of the loss function, we can
push the generated sample distribution towards the target
loss & gradient
two-sample test
classical data
Figure 1. Illustration of the diﬀerentiable QCBM training scheme.
Top left is the quantum circuit which produce bit string samples. The
dashed box on the right denotes two-sample test on the generated
samples and training samples, with the loss function (Eq. (1)) and
corresponding gradients (Eq. (2)) as outputs. ∆θ is the amount of
updated to be applied to the circuit parameters, which are computed
by a classical optimizer. The outcome of the training is to produce
a quantum circuit which generates samples according to the learned
probability distribution on the computational basis.
distribution.
Quantum Circuit Architecture Design
The overall circuit layout is similar to the IBM variational
quantum eigensolver , where one interweaves single qubit
rotation layers and entangler layers shown in Fig. 1.
rotation layers are parameterized by rotation angles θ = {θα
where the layer index l runs from 0 to d, with d the maximum
depth of the circuit. α is a combination of qubit index j and
arbitrary rotation gate index, where the arbitrary rotation gate
has the form U(θ j
l ) = Rz(θ j,1
l )Rx(θ j,2
l )Rz(θ j,3
l ) with Rm(θ) ≡
. The total number of parameters in this QCBM
is (3d + 1)n, with n the number of qubits .
We employ CNOT gates with no learnable parameters for
the entangle layers to induce correlations between qubits. In
light of experimental constraints on the connectivity of the
circuits, we make the connection of the entangle layers to be
sparse by requiring its topology as a tree (i.e. the simplest
connected graph).
From the classical probabilistic graphical model’s perspective , the tree graph that captures
information content of the dataset most eﬃciently is Chow-
Liu tree .
Since controlled unitary gates have a close
relation with classical probability graphical models , we
employ the same Chow-Liu tree as the topology of CNOT
gates. To construct the Chow-Liu tree we ﬁrst compute mutual
information between all pairs of the bits for samples in the
training set as weights, and then construct the maximum
spanning tree using, for example, the Kruskal’s algorithm.
The assignment of the control bit and the target bit on a bond
is random, since the Chow-Liu algorithm treated directed
and undirected graphs the same.
In the case where this
connection structure is not directly supported by the hardware,
a combination of SWAP gates and CNOT gates can be used to
eﬃciently simulate the required structure .
The performance of entangle layers constructed in this
procedure is better than most random connections with the
same number of gates.
This data-driven quantum circuit
architecture design scheme respects the information content
of the classical dataset easier and may alleviate issues of
vanishing gradients for large-scale applications .
Loss Function and Gradient-based Optimization
Viewing the QCBM as an implicit generative model ,
we train it by employing the kernel two-sample test . The
idea is to compare the distance in the kernel feature space on
the samples drawn from the target and the model distributions.
We refer the following loss function as the squared maximum
mean discrepancy (MMD) 
pθ(x)φ(x) −
K(x, y) −2
K(x, y) +
K(x, y).
The summation in the ﬁrst line runs over the whole Hilbert
space. The expectation values in the second line are for the
corresponding probability distributions. The function φ maps
x to a high-dimensional reproducing kernel Hilbert space .
However, as common in the kernel tricks, by deﬁning a kernel
function K(x, y) = φ(x)Tφ(y) one can avoid working in the
high-dimensional feature space.
We employ a mixture of
Gaussians kernel K(x, y) =
2σi |x −y|2
diﬀerences between the two distributions under various scales.
Here, σi is the bandwidth parameter which controls the width
of the Gaussian kernel. The sample x can either be a bit string
(vector) or an integer (scalar) depending on the representation.
When x is a bit string, |x| stands for the ℓ2-norm in the vector
space. The MMD loss with Gaussian kernels asymptotically
approaches zero if and only if the output distribution matches
the target distribution exactly . The same loss function
was used to train the generative moment matching networks
(GMMN) .
To learn the QCBM as a generative model, we compute
gradient of the loss function Eq. (1) with respect to the circuit
parameters
x∼pθ+,y∼pθ
K(x, y) −
x∼pθ−,y∼pθ
K(x, y) +
K(x, y).
Here, pθ+(x) and pθ−(x) are output probabilities of QCBM
under circuit parameters θ± = θ ± π
l , where eα
(l, α)-th unit vector in parameter space (i.e.
2, with other angles unchanged).
In contrast to the ﬁnite
diﬀerence methods like simultaneous perturbation stochastic
approximation (SPSA) , Eq. (2) is an unbiased estimator
of the exact gradient. Its detailed derivations is in Appendix
In order to estimate the gradient [Eq. (2)] on an actual quantum circuit, one can repeatedly send rotation and entangle
pulses to the device according to the circuit parameters θ(±),
and then perform projective measurements on the computational basis to collect binary samples x ∼pθ(±). While for
x ∼π, one can simply take a batch of data from the training
dataset . The sampling noise in the estimated gradient
is controlled by the number of measurements N, denoted as
the batch size. After one has obtained a suﬃciently accurate
gradient, one can use a classical optimizer to update the circuit
parameters similar to the stochastic gradient descent training
of deep neural nets.
Parameter learning of quantum circuits is adaptive in the
sense that the implementation of quantum gates can even
be non-ideal. One can obtain gradients with high accuracy
as long as the parametrized single qubits rotation gates are
precise, which is relatively easier to achieve experimentally.
The optimization scheme is independent of the detailed form
of non-parametrized entangle gates. Thus, the CNOT gate
in the setup can be replaced by any gate which can generate
desired quantum entanglements.
It is instructive to compare the training of the QCBM to that
of classical implicit generative models such as GAN 
and GMMN . Classically, one does not have access
to the likelihood either. The gradient is thus obtained via the
chain rule ∂L
∂θ, which then does not apply for discrete
data. On the other hand, the unbiased gradient estimator of the
QCBM takes advantage of the known structure of the unitary
evolution and the MMD loss (see Appendix A), despite that
the probability of the outcome is unknown.
In this sense,
quantum circuits exhibit a clear quantum advantage over
classical neural nets since they ﬁll the gap of diﬀerentiable
learning of implicit generative models of discrete data.
NUMERICAL EXPERIMENTS
We carry out numerical experiments by simulating the
learning of QCBM on a classical computer. These experiments reveal advantages of gradient based optimization over
gradient-free optimization, and demonstrate stronger expressibility of deep circuits over shallow ones. The code can be
found at the Github repository 
Bars-and-Stripes Dataset
We ﬁrst train a QCBM on the Bars-and-Stripes dataset , which is a prototypical image dataset consists of
vertical bars and horizontal stripes. On a grid of 3 × 3, the
dataset contains 14 valid conﬁgurations. We model the pixels
with a quantum circuit of 9 qubits. The Chow-Liu tree for
this dataset is shown in Fig. 2 (a).
Bonds are either rowwise or column-wise since correlations of pixels sharing the
Figure 2. (a) Connectivity of the CNOT gates for 3 × 3 Bars-and-
Stripes dataset generated via the chow-Liu tree algorithm. The qubits
are arranged on a 3 × 3 grid, with some of them shifted a bit in order
to visualize the edges clearly. (b) Chow-Liu tree for double Gaussian
peak model, the numbers represent the position of a bit in the digit,
0 for the big end and 9 for the little end. In this plot, the darkness of
edges indicates the amount of mutual information between two sites.
same row/column index are dominant in this dataset.
bandwidths used in Gaussian kernels of MMD loss are σ =
0.5, 1, 2, 4.
For circuit depth d = 10, our gradient-based training is
able to reduce the MMD loss eﬃciently. The loss function for
diﬀerent iteration steps is shown in Fig. 3(a). We ﬁrst perform
L-BFGS-B optimization (black dashed line) using the
exact gradient computed via the wavefunction (N = ∞) to test
the expressibility of the quantum circuit. A loss of 2.4 × 10−7
can be achieved, showing that the circuit is quite expressive in
terms of the two-sample test.
In practice, one has to perform projective measurements
on the qubits to collect statistics of the gradient since the
wavefunction is inaccessible. This situation is similar to the
mini-batch estimate of the gradient in deep learning . As
is well known in the deep learning applications , the L-
BFGS-B algorithm is not noise tolerant. Thus, it is unsuitable
for quantum circuit learning in realistic situation. One needs
to employ an alternate optimizer which is robust to the
sampling noise to train the quantum circuit with noisy gradient
estimator.
We employ the stochastic gradient optimizer Adam 
with the learning rate 0.1. The sampling noise in the gradients
can be controlled by tuning the batch size N = 2000, 20000, ∞
of the measurements. The solid lines in Fig. 3 (a) show that
as the sample size increases, the ﬁnal MMD loss reduces
systematically. The scatters in the inset conﬁrmed that the
model probability of learned quantum circuit and the target
probability aligns better with lower MMD loss.
To visualize the quality of the samples, we generated a few
samples from the QCBM trained under diﬀerent measurement
batch size N in Fig. 4.
Here, we deﬁne a valid rate χ ≡
p(x is a bar or a stripe) as a measure of generation quality.
The valid rate increases as the batch size increases. However,
even with a moderate number of measurement N = 2000 one
can achieve a valid rate χ = 88.6%. Here, we should mention
that the best valid rate of d = 10 layer circuit is achieved by
L-BFGS-B optimizer with N = ∞, which is χ = 99.9%.
To highlight the importance of using a gradient-based
optimizer, we compare our approach to the covariance matrix
adaptation evolution strategy (CMA-ES) , a state-of-
Training Step
Training Step
The MMD loss (Eq. (1)) as a function of training
steps under diﬀerent sampling errors that governed by batch size
N. (a) Gradient based training, solid colored lines are for Adam,
and the dashed black line is for L-BFGS-B with N = ∞.
is a comparison of probability distribution between the training set
and QCBM output, points on dashed line means exact match. (b)
Gradient free CMA-ES training counterpart, where each point in the
graph represents a mean loss of its population.
the-art gradient-free stochastic optimizer. The input of CMA-
ES is the scalar loss function measured on the circuit instead
of the vector gradient information. The CMA-ES optimizer
is able to optimize non-smooth non-convex loss functions
eﬃciently , thus in general performs better than other
gradient-free methods such as the SPSA in training noisy
quantum circuits. We have conﬁrmed this in our simulation.
In the absence of sampling noise N = ∞in Fig. 3 (b),
we do observe that the CMA-ES optimizer is able to achieve
similar performance as the Adam optimizer after 104 steps of
optimization with a population size of 50. The total number
of generated samples is 104 × 50 × N, which is comparable to
the Adam training in Fig. 3 (a) .
However, the performance of CMA-ES deteriorates signiﬁcantly once taking sampling noise into consideration, as
shown for N = 2000 and N = 20000 in Fig. 3 (b). A possible
explanation is that in each step of CMA-ES, its evolution
strategy chooses the direction to go by inspecting the center
of top 20% instances.
This process can be understood
as an eﬀective ﬁnite diﬀerence gradient estimation base on
the losses of its population.
However, extracting gradient
information from noisy losses is diﬃcult, even one has plenty
Another advantage of using gradient-based learning is the
eﬃciency comparing with gradient-free methods, which gets
particularly signiﬁcant when circuits get deeper and the number of parameters increases. In the following, we address the
necessity of using deep circuits. Fig. 5 (a) shows the MMD
loss as a function of L-BFGS-B training steps for diﬀerent
circuit depth.
One obtains lower loss for deeper quantum
circuit after 500 optimization steps.
Fig. 5 (b) shows the
Kullback-Leibler (KL) divergence calculated using the
circuit parameters in (a) at diﬀerent training steps. Note that
this quantity is inaccessible for large-scale problems since one
has no access to the target nor the output probability. We
Figure 4. 3 × 3 Bars-and-Stripes samples generated from QCBMs.
Circuit parameters used here are from the ﬁnal stages of Adam
training with diﬀerent batch sizes N in Fig. 3 (a).
χ is the rate
of generating valid samples in the training dataset. For illustrative
purpose, we only show 12 samples for each situation with batch size
Training Step
Training Step
KL Divergence
Figure 5. Losses as a function of training step for circuit depth d =
1, . . . , 10. (a) The MMD loss Eq. (1), and (b) the corresponding KL
divergence. Here, we use L-BFGS-B optimizer with exact gradient.
compute the KL divergence for the toy model to demonstrate
that the MMD loss is a good surrogate for practical training.
The result indeed shows a consistency between MMD loss and
KL divergence. And it also supports the observation that deep
circuits have stronger representational power. Similar to deep
neural networks, deep circuits can achieve better performance
also due to that one is less prone to be trapped in a poor local
minima with larger amount of parameters .
Another advantage of the QCBM over traditional deep
neural networks is that its training not suﬀer from gradient vanishing/exploding problem as the circuit goes deeper.
Gradient vanishing/exploding is a common problem for a
traditional deep neural network which originates from
multiplications of a long chain of matrices in the backpropagation algorithm. Training of the deep quantum circuits
prob. distri.
Training Step
Figure 6. (a) The MMD loss as a function of Adam training step.
(b) Histogram for samples generated by a trained QCBM with a
bin width 20 (green bars), in comparison with the exact probability
density function (black dashed line).
naturally circumvented this problem by due to the unitary
property of the time evolution. Similar idea was exploited in
constructing classical recurrent neural networks with unitary
building blocks . More numerical simulation and analytical explanations can be found in Appendix B.
Mixture of Gaussians
Next, we train a QCBM to model a mixture of Gaussians
distribution
Here, x = 1, . . . , xmax is an integer encoded by the qubits,
with xmax = 2n and n is the number of qubits. It is diﬀerent
from Bars-and-Stripes dataset, in which case a sample x is
represented as a bit string. We choose ν = 1
8 xmax, the centers
7 xmax and µ2 = 5
7 xmax. The distribution is shown as the
dashed line in Fig. 6(b).
In the following discussion, we use n = 10 qubits and set
circuit depth d = 10. Unlike the case of Bars-and-Stripes
, the Gaussian mixture distribution is smooth and non-zero
for all basis state.
Here, we generate 105 i.i.d.
from the target distribution as the training set. Its Chow-Liu
tree is shown in Fig. 2(b). In this graph, we see the main
contributions of mutual information are from bits near the big
end (most signiﬁcant bits labeled by small indices). This is
because the bit near the little end only determines the local
translation of the probability on data axis. But for a smooth
probability distribution, the value of the little end is nearly
independent from values of the rest bits. For example, the
value of the big-end 0-th bit being 0/1 corresponds to the
global left/right peak in Fig. 6 (b). While the probability for
the little end being 0/1 corresponds to x being even/odd.
We use mixture of three Gaussian kernels with bandwidths
σ = 0.25, 10, 1000. σ = 0.25 captures the local diﬀerence
in distribution, and σ = 1000, which has the same scale as
xmax, which captures the overall diﬀerences in the probability
distribution.
Fig. 6 (a) shows the MMD loss as a function of the Adam
optimization steps, with a sample size N = 20000. After 2000
training steps, the MMD loss decreased from 9.6 × 10−2 to
6.4 × 10−4. To see whether this low MMD loss represents a
good generative model, we then generate 20000 samples from
the QCBM and plot its binned histogram in Fig. 6 (b). We see
an excellent match between the histogram (green bars) and the
exact probability distribution (black dashed curve). Thus, we
conclude that MMD loss with Adam optimizer can also learn
a smooth probability distributions over the qubit index of a
QCBM. Here, we acknowledge that the unbinned histogram
appears more spiky, partly due to the MMD loss does not
capture the local variation of the probability distribution.
Better circuit architecture design for representing continuous
distribution may help alleviate this problem.
DISCUSSIONS
We presented a practical gradient-based learning scheme to
train quantum circuit Born machine as a generative model.
The key component of the learning algorithm is to measure
the gradient of the MMD two-sample test loss function Eq. (2)
on a quantum computer unbiasedly and eﬃciently. Besides
possessing stronger representation power, the QCBM does
not suﬀer from the gradient vanishing/exploding problem as
circuit depth increases compared to the classical deep neural
While compared to other quantum generative
models , the quantum circuit Born machine has fewer
restrictions on hardware and circuit design, while both the
training and sampling can be eﬃciently carried out.
A recent work pointed out that the quantum circuit
also faces gradient vanishing problem as the number of qubits
increases. They found that the variance of gradient amplitudes
decreases exponentially as the number of qubits increases in a
random quantum circuit. However, it is reasonable to believe
that with better circuit structure design and parametrization
strategy, such Sec. II A and Ref. , or using shared weights
such as done in the convolutional neural networks , the
gradient vanishing problem can be alleviated. How gradient
vanishing really aﬀects gradient-based training in a largescale QCBM needs further systematic investigation.
Our simulation of the quantum circuits Born machine is
limited to a small number of qubits, thus the training set
contains all patterns and we are pushing the circuits towards
the memorization limit. In future applications, one shall focus
on the generalization ability of the quantum circuits. In those
cases, the structure and depth of a quantum circuit provide
means of regularization since they can be design to express
inductive bias in the natural distributions.
In terms of the
learning, the randomness in the stochastic gradient is also in
favor of the generalization .
Besides the two-sample test loss employed in this paper,
one can explore alternative training schemes, e.g. adversarial
training . Alternatively, learning of the kernel function
used in Eq. (1) may also improve the generation quality .
Finally, diﬀerentiable learning of the QCBM may be used
for solving combinatorial optimization problems and structure
learning tasks, where the outputs are encoded in discrete bit
ACKNOWLEDGMENT
Simulations of quantum circuits were mainly performed
using the ProjectQ library .
We thank Alejandro
Perdomo-Ortiz, Miles Stoudenmire, Damian Steiger, Xun
Gao and Pan Zhang for helpful discussions.
The authors
are supported by the National Natural Science Foundation
of China under the Grant No. 11774398 and research program of the Chinese Academy of Sciences under Grant No.
Appendix A: Unbiased gradient estimator of the probability of a
quantum circuit
In the following, we derive Eq. (2) in the main text, starting
from the partial derivative of Eq. (1)
pθ(y)∂pθ(x)
+ pθ(x)∂pθ(y)
K(x, y)∂pθ(x)
The partial derivative on the right-hand side of this equation
can be unbiasedly computed using the approach of .
For a circuit containing a unitary gate parametrized as U(η) =
2 ηΣ with Σ2 = 1 (e.g. Σ can be Pauli operators, CNOT or
SWAP), the gradient of the expected value of an observable B
with respect to the parameter η reads
⟨B⟩η+ −⟨B⟩η−
where ⟨·⟩η(±) represents expectation values of observables with
respect to the output quantum wave function generated by
the same circuit with circuit parameter η± ≡η ± π
Eq. (A2) is an unbiased estimate of the gradient in contrast to
the ﬁnite diﬀerence approachs which are sensitive to noise of
the quantum circuit.
Since the quantum circuit Born machine employs the same
parametrization, we identify |x⟩⟨x| as the observable and apply
Eq. (A2) to the output probability of the circuit
2 (pθ+(x) −pθ−(x)) ,
where θ± ≡θ ± π
l , with eα
l the (l, α)-th unit vector in
parameter space. Substituting Eq. (A3) into Eq. (A1) we have
K(x, y)pθ(y)pθ+(x) −
K(x, y)pθ(y)pθ−(x)
K(x, y)pθ(x)pθ+(y) −
K(x, y)pθ(x)pθ−(y)
K(x, y)pθ+(x)π(y) −
K(x, y)pθ−(x)π(y)
Using the symmetric condition of the kernel K(x, y) = K(y, x),
we arrive at Eq. (2) in the main text. This gradient algorithm
for QCBM scales as O(d2) with the depth of the circuit,
which is less eﬃcient compared to the linear scaling backpropagation algorithm for classical neural networks. It is still
unknown whether one can reach similar scaling on a quantum computer since the back-propagation algorithm requires
cached intermediate results in the forward evaluation pass. We
have checked the correctness of the gradient estimator Eq. (2)
against numerical ﬁnite diﬀerence.
Generalization to V-statistic
To gain a better understanding of what kind of losses for a
quantum circuit can be easily diﬀerentiated, we generalize the
above result by considering an arbitrary function f(X), with a
sequence of bit strings X ≡{x1, x2, . . . , xr} as its arguments.
Let’s deﬁne the following expectation of this function
{xi∼pθ+γi}r
Here, Γ = {γ1, γ2, . . . , γr} is the oﬀset angles applied to
circuit parameters, which means the probability distributions
of generated samples is {pθ+γ1, pθ+γ2, . . . , pθ+γr}. Writing out
the above expectation explicitly, we have
pθ+γi(xi),
where index i runs from 1 to r. Its partial derivative with
respect to θα
∂pθ+γ j(xj)
Again, using Eq. (A3), we have
f(X)pθ+γ j+s π
E f ({γi + sδij
If f is symmetric, Ef (0) becomes a V-statistic , then
Eq. (A8) can be further simpliﬁed to
l , γ1, . . . , γr}
probability
Sample Size
Figure 7. (a) The probability distribution of gradient elements in
diﬀerent layers grouped by depth. The circuit parameters are chosen
randomly. (b) The variance of MMD loss gradients as a function of
sample size N.
which contains only two terms. This result can be readily
veriﬁed by calculating the gradient of MMD loss, noticing the
expectation of a kernel function is a V-statistic of degree 2. By
repeatedly applying Eq. (A8), we will be able to obtain higher
order gradients.
Gradient of the KL-Divergence
The KL divergence reads
KL(π||pθ) = −
Its gradient is
π(x) pθ+(x) −pθ−(x)
where the pθ+(x)−pθ−(x)
term cannot be transformed to a sampling problem on the quantum circuit. Since minimizing the
negative log-likelihood is equivalent to minimizing the KLdivergence, it faces the same problem.
Appendix B: Gradient Analysis
In this appendix, we analyze the eﬀect of depth and sampling error to gradients.
For a 9 qubit quantum circuit of
depth d = 100 with random parameters used in generating
the Bars-and-Stripes dataset in the main text. Histogram of
MMD loss gradient values in diﬀerent layers are shown in
Fig. 7 (a). Distributions of gradients in diﬀerent layers have no
signiﬁcant diﬀerences, thus deep QCBM does not suﬀer from
vanishing or exploding gradients when the layers go deeper.
This property has its physical origin in how the wave function
changes when a perturbation is applied to θα
l of the quantum
circuit. Suppose we have a quantum circuit that performing
the following evolution,
|ψ⟩= UN:1|0⟩
Introduce a perturbation δ to θα
|ψ′⟩= UN:kei
2 ΣkUk−1:1|0⟩
Its ﬁdelity with respect to unperturbed wave function is
F = |⟨ψ|ψ′⟩|
2⟨φ|Σk|φ⟩| + O(δ3)
4 (1 −⟨φ|Σk|φ⟩2)
Here, |φ⟩≡Uk−1:1|0⟩can be assumed to be random in a deep
circuit. Thus we have the ﬁdelity susceptibility χF =
= 0.25(1 −⟨Σk⟩2) which is independent of θk. Since we
k = 1, χF is bounded by 0.25, and is independent of the
layer index.
Next, we show the error in gradient estimation will decrease
systematically as the number of samples increases for MMD
loss. As can be seen from the curve of variance calculated
using 100 copies of independent samples in Fig. 7 (b). In
our case, to give a valid estimation of gradients, the standard
error should be lower than the typical amplitude of gradients.
As can be referred from Fig. 7 (a), the typical amplitude of
gradient is approximately 5×10−3, thus the sample size should
be larger than 103 in order to give a good estimation to these
gradients.
 I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning
 
 J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, arXiv:1703.10593.
 A. Van Den Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior,
K. Kavukcuoglu, arXiv:1609.03499.
 R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M.
Hernández-Lobato,
Sánchez-Lengeling,
J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams,
A. Aspuru-Guzik, ACS Central Science .
 J. Preskill, arXiv:1801.00862 .
 W. Knight, “IBM Raises the Bar with a 50-Qubit Quantum
J. Kelly, “A Preview of Bristlecone,
Google’s New Quantum Processor,” .
 M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy,
R. Melko, arXiv:1601.02036.
 A. Khoshaman, W. Vinci, B. Denis, E. Andriyash, and M. H.
Amin, arXiv:1802.05779.
 M. Benedetti, J. Realpe-Gómez, R. Biswas, and A. Perdomo-
Ortiz, Phys. Rev. X 7, 041052 .
 D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, Cognitive
Science 9, 147 .
 V. Dumoulin, I. J. Goodfellow, A. C. Courville, and Y. Bengio,
in AAAI, Vol. 2014 pp. 1199–1205.
 X. Gao, Z. Zhang, and L. Duan, arXiv:1711.02038.
 D. Koller and N. Friedman, Probabilistic graphical models:
principles and techniques .
 Z.-Y. Han, J. Wang, H. Fan, L. Wang,
and P. Zhang,
 
 S. Cheng, J. Chen, and L. Wang, arXiv:1712.04144.
 M. Benedetti, D. Garcia-Pintos, Y. Nam,
and A. Perdomo-
Ortiz, arXiv:1801.07686.
 M. Born, Zeitschrift für Physik 38, 803 .
 L. Fortnow and J. Rogers, Journal of Computer and System
Sciences 59, 240 .
 S. Aaronson and A. Arkhipov, in Proceedings of the forty-third
annual ACM symposium on Theory of computing 
pp. 333–342.
 A. Lund, M. J. Bremner, and T. Ralph, npj Quantum Information 3, 15 .
 S. Boixo, S. V. Isakov, V. N. Smelyanskiy, R. Babbush, N. Ding,
Z. Jiang, M. J. Bremner, J. M. Martinis,
and H. Neven,
 
 W. Huggins, P. Patel, K. B. Whaley, and E. M. Stoudenmire,
 
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, in Advances in
Neural Information Processing Systems pp. 2672–2680.
 I. Goodfellow, arXiv:1701.00160.
 G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 .
 A. v. d. Oord, N. Kalchbrenner,
and K. Kavukcuoglu,
 
 L. Dinh, J. Sohl-Dickstein, and S. Bengio, arXiv:1605.08803.
 D. P. Kingma, T. Salimans, X. Chen,
and I. Sutskever,
 
 D. J. Rezende and S. Mohamed, arXiv:1505.05770.
 G. Papamakarios, I. Murray, and T. Pavlakou, in Advances in
Neural Information Processing Systems pp. 2335–2344.
 D. P. Kingma and M. Welling, arXiv:1312.6114.
 
 K. Simonyan and A. Zisserman, arXiv:1409.1556.
 M. Minsky and S. A. Papert, Perceptrons: An introduction to
computational geometry .
 D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Nature 323,
533 .
 J. Li, X. Yang, X. Peng, and C.-P. Sun, Phys. Rev. Lett. 118,
150503 .
 E. Farhi and H. Neven, arXiv:1802.06002.
 K. Mitarai, M. Negoro, M. Kitagawa,
and K. Fujii,
 
 Y. Li and R. E. Turner, arXiv:1705.07107.
 E. Farhi, J. Goldstone, and S. Gutmann, (), arXiv:1411.4028.
 E. Farhi, J. Goldstone, S. Gutmann,
and H. Neven,
 
 J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick, M. Block,
B. Bloom, S. Caldwell, N. Didier, E. S. Fried, S. Hong, et al.,
 
 A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q.
Zhou, P. J. Love, A. Aspuru-Guzik, and J. L. O’brien, Nature
communications 5, 4213 .
 P. J. J. O’Malley, R. Babbush, I. D. Kivlichan, J. Romero, J. R.
McClean, R. Barends, J. Kelly, P. Roushan, A. Tranter, N. Ding,
B. Campbell, Y. Chen, Z. Chen, B. Chiaro, A. Dunsworth,
A. G. Fowler, E. Jeﬀrey, E. Lucero, A. Megrant, J. Y. Mutus,
M. Neeley, C. Neill, C. Quintana, D. Sank, A. Vainsencher,
J. Wenner, T. C. White, P. V. Coveney, P. J. Love, H. Neven,
A. Aspuru-Guzik, and J. M. Martinis, Phys. Rev. X 6, 031007
 A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink,
J. M. Chow, and J. M. Gambetta, Nature 549, 242 .
 The number of parameters here is not 3(d+1)n here because the
leading and trailing Rz gates can be omitted without aﬀecting
the output probability distribution.
 C. Chow and C. Liu, IEEE Transactions on Information Theory
14, 462 .
 G. H. Low, T. J. Yoder, and I. L. Chuang, Phys. Rev. A 89,
062315 .
 A. Zulehner, A. Paler, and R. Wille, arXiv:1712.04722.
 S. Mohamed and B. Lakshminarayanan, arXiv:1610.03483.
 N. Anderson, P. Hall, and D. Titterington, Journal of Multivariate Analysis 50, 41 .
 A. Gretton, K. M. Borgwardt, M. Rasch, B. Schölkopf, and
A. J. Smola, in Advances in Neural Information Processing
Systems pp. 513–520.
 A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf,
and A. Smola, Journal of Machine Learning Research 13, 723
 T. Hofmann, B. Schölkopf,
and A. J. Smola, The annals of
statistics , 1171 .
 Y. Li, K. Swersky, and R. Zemel, (), arXiv:1502.02761.
 G. K. Dziugaite,
D. M. Roy,
and Z. Ghahramani,
 
 C.-L. Li, W.-C. Chang, Y. Cheng, Y. Yang, and B. Póczos, (),
 
 J. C. Spall, IEEE Transactions on Aerospace and Electronic
Systems 34, 817 .
 For the small dataset employed in our numerical experiment,
we can aﬀord to average over the whole training dataset.
 
 D. J. MacKay, Information theory, inference and learning
algorithms .
 R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu, SIAM Journal on
Scientiﬁc Computing 16, 1190 .
 D. P. Kingma and J. Ba, arXiv:1412.6980.
 N. Hansen, “The CMA Evolution Strategy: A Comparing Review,” in Towards a New Evolutionary Computation: Advances
in the Estimation of Distribution Algorithms, edited by J. A.
Lozano, P. Larrañaga, I. Inza,
and E. Bengoetxea pp. 75–102.
 L. M. Rios and N. V. Sahinidis, Journal of Global Optimization
56, 1247 .
 Notice for this circuit of depth d
10 that having 279
parameters, we need to make 279 × N × 2 + 1 measurements
in a gradient estimation.
 S. Kullback and R. A. Leibler, The annals of mathematical
statistics 22, 79 .
 A. Choromanska, M. Henaﬀ, M. Mathieu, G. B. Arous, and
Y. LeCun, in Artiﬁcial Intelligence and Statistics pp.
 K. He, X. Zhang, S. Ren, and J. Sun, in Proceedings of the
IEEE conference on computer vision and pattern recognition
 pp. 770–778.
 L. Jing, Y. Shen, T. Dubcek, J. Peurifoy, S. Skirlo, Y. LeCun,
M. Tegmark, and M. Soljacic, arXiv:1612.05231.
 J. R. Mcclean, S. Boixo, V. N. Smelyanskiy, R. Babbush, and
H. Neven, arXiv:1803.11173.
 D. S. Steiger, T. Häner, and M. Troyer, arXiv:1612.08091.
 T. Häner, D. S. Steiger, K. Svore, and M. Troyer, Quantum
Science and Technology .
 R. v. Mises, Ann. Math. Statist. 18, 309 .
 W.-L. You, Y.-W. Li, and S.-J. Gu, Phys. Rev. E 76, 022101
 L. Wang, Y.-H. Liu, J. Imriska, P. N. Ma, and M. Troyer, Phys.
Rev. X 5, 031007 .