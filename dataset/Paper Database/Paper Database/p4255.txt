OctNet: Learning Deep 3D Representations at High Resolutions
Gernot Riegler1
Ali Osman Ulusoy2
Andreas Geiger2,3
1Institute for Computer Graphics and Vision, Graz University of Technology
2Autonomous Vision Group, MPI for Intelligent Systems T¨ubingen
3Computer Vision and Geometry Group, ETH Z¨urich
 
{osman.ulusoy,andreas.geiger}@tue.mpg.de
We present OctNet, a representation for deep learning
with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are
both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each
leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the
relevant dense regions and enables deeper networks without
compromising resolution. We demonstrate the utility of our
OctNet representation by analyzing the impact of resolution
on several 3D tasks including 3D object classiﬁcation, orientation estimation and point cloud labeling.
1. Introduction
Over the last several years, convolutional networks have
lead to substantial performance gains in many areas of computer vision. In most of these cases, the input to the network
is of two-dimensional nature, e.g., in image classiﬁcation
 , object detection or semantic segmentation .
However, recent advances in 3D reconstruction and
graphics allow capturing and modeling large amounts
of 3D data. At the same time, large 3D repositories such as
ModelNet , ShapeNet or 3D Warehouse1 as well as
databases of 3D object scans are becoming increasingly
available. These factors have motivated the development of
convolutional networks that operate on 3D data.
Most existing 3D network architectures replace the 2D pixel array by its 3D analogue, i.e., a dense and
regular 3D voxel grid, and process this grid using 3D convolution and pooling operations. However, for dense 3D
data, computational and memory requirements grow cubically with the resolution. Consequently, existing 3D networks are limited to low 3D resolutions, typically in the
order of 303 voxels. To fully exploit the rich and detailed
1 
Dense 3D ConvNet
Dense 3D ConvNet
(a) Layer 1: 323
(b) Layer 2: 163
(c) Layer 3: 83
Motivation.
For illustration purposes, we
trained a dense convolutional network to classify 3D shapes
from . Given a voxelized bed as input, we show the
maximum response across all feature maps at intermediate
layers (a-c) of the network before pooling. Higher activations are indicated with darker colors. Voxels with zero
activation are not displayed. The ﬁrst row visualizes the
responses in 3D while the second row shows a 2D slice.
Note how voxels close to the object contour respond more
strongly than voxels further away. We exploit the sparsity
in our data by allocating memory and computations using a
space partitioning data structure (bottom row).
geometry of our 3D world, however, much higher resolution
networks are required.
In this work, we build on the observation that 3D data is
often sparse in nature, e.g., point clouds, or meshes, resulting in wasted computations when applying 3D convolutions
 
na¨ıvely. We illustrate this in Fig. 1 for a 3D classiﬁcation
example. Given the 3D meshes of we voxelize the input
at a resolution of 643 and train a simple 3D convolutional
network to minimize a classiﬁcation loss. We depict the
maximum of the responses across all feature maps at different layers of the network. It is easy to observe that high
activations occur only near the object boundaries.
Motivated by this observation, we propose OctNet, a 3D
convolutional network that exploits this sparsity property.
Our OctNet hierarchically partitions the 3D space into a set
of unbalanced octrees . Each octree splits the 3D space
according to the density of the data. More speciﬁcally, we
recursively split octree nodes that contain a data point in its
domain, i.e., 3D points, or mesh triangles, stopping at the
ﬁnest resolution of the tree. Therefore, leaf nodes vary in
size, e.g., an empty leaf node may comprise up to 83 = 512
voxels for a tree of depth 3 and each leaf node in the octree
stores a pooled summary of all feature activations of the
voxel it comprises. The convolutional network operations
are directly deﬁned on the structure of these trees. Therefore, our network dynamically focuses computational and
memory resources, depending on the 3D structure of the input. This leads to a signiﬁcant reduction in computational
and memory requirements which allows for deep learning
at high resolutions. Importantly, we also show how essential network operations (convolution, pooling or unpooling)
can be efﬁciently implemented on this new data structure.
We demonstrate the utility of the proposed OctNet on
three different problems involving three-dimensional data:
3D classiﬁcation, 3D orientation estimation of unknown
object instances and semantic segmentation of 3D point
clouds. In particular, we show that the proposed OctNet enables signiﬁcant higher input resolutions compared to dense
inputs due to its lower memory consumption, while achieving identical performance compared to the equivalent dense
network at lower resolutions. At the same time we gain signiﬁcant speed-ups at resolutions of 1283 and above. Using
our OctNet, we investigate the impact of high resolution inputs wrt. accuracy on the three tasks and demonstrate that
higher resolutions are particularly beneﬁcial for orientation
estimation and semantic point cloud labeling. Our code is
available from the project website2.
2. Related Work
While 2D convolutional networks have proven very successful in extracting information from images , there exists comparably little work
on processing three-dimensional data. In this Section, we
review existing work on dense and sparse models.
Dense Models: Wu et al. trained a deep belief network
on shapes discretized to a 303 voxel grid for object classi-
2 
ﬁcation, shape completion and next best view prediction.
Maturana et al. proposed VoxNet, a feed-forward convolutional network for classifying 323 voxel volumes from
RGB-D data. In follow-up work, Sedaghat et al. showed
that introducing an auxiliary orientation loss increases classiﬁcation performance over the original VoxNet. Similar
models have also been exploited for semantic point cloud
labeling and scene context has been integrated in .
Recently, generative models and auto-encoders have demonstrated impressive performance in learning
low-dimensional object representations from collections of
low-resolution (323) 3D shapes. Interestingly, these lowdimensional representations can be directly inferred from a
single image or a sequence of images .
Due to computational and memory limitations, all aforementioned methods are only able to process and generate
shapes at a very coarse resolution, typically in the order of
303 voxels. Besides, when high-resolution outputs are desired, e.g., for labeling 3D point clouds, inefﬁcient slidingwindow techniques with a limited receptive ﬁeld must be
adopted . Increasing the resolution na¨ıvely 
reduces the depth of the networks and hence their expressiveness. In contrast, the proposed OctNets allow for training deep architectures at signiﬁcant higher resolutions.
Sparse Models:
There exist only few network architectures which explicitly exploit sparsity in the data. As these
networks do not require exhaustive dense convolutions they
have the potential of handling higher resolutions.
Engelcke et al. proposed to calculate convolutions
at sparse input locations by pushing values to their target
locations. This has the potential to reduce the number of
convolutions but does not reduce the amount of memory required. Consequently, their work considers only very shallow networks with up to three layers.
A similar approach is presented in where sparse
convolutions are reduced to matrix operations.
Unfortunately, the model only allows for 2 × 2 convolutions and
results in indexing and copy overhead which prevents processing volumes of larger resolution (the maximum resolution considered in is 803 voxels). Besides, each
layer decreases sparsity and thus increases the number of
operations, even at a single resolution. In contrast, the number of operations remains constant in our model.
Li et al. proposed ﬁeld probing networks which
sample 3D data at sparse points before feeding them into
fully connected layers.
While this reduces memory and
computation, it does not allow for exploiting the distributed
computational power of convolutional networks as ﬁeld
probing layers can not be stacked, convolved or pooled.
Jampani et al. introduced bilateral convolution layers (BCL) which map sparse inputs into permutohedral
space where learnt convolutional ﬁlters are applied. Their
work is related to ours with respect to efﬁciently exploiting
the sparsity in the input data. However, in contrast to BCL
our method is speciﬁcally targeted at 3D convolutional networks and can be immediately dropped in as a replacement
in existing network architectures.
3. Octree Networks
To decrease the memory footprint of convolutional networks operating on sparse 3D data, we propose an adaptive
space partitioning scheme which focuses computations on
the relevant regions. As mathematical operations of deep
networks, especially convolutional networks, are best understood on regular grids, we restrict our attention to data
structures on 3D voxel grids. One of the most popular space
partitioning structures on voxel grids are octrees which
have been widely adopted due to their ﬂexible and hierarchical structure. Areas of application include depth fusion , image rendering and 3D reconstruction .
In this paper, we propose 3D convolutional networks on octrees to learn representations from high resolution 3D data.
An octree partitions the 3D space by recursively subdividing it into octants. By subdividing only the cells which
contain relevant information (e.g., cells crossing a surface
boundary or cells containing one or more 3D points) storage
can be allocated adaptively. Densely populated regions are
modeled with high accuracy (i.e., using small cells) while
empty regions are summarized by large cells in the octree.
Unfortunately, vanilla octree implementations have
several drawbacks that hamper its application in deep networks. While octrees reduce the memory footprint of the
3D representation, most versions do not allow for efﬁcient
access to the underlying data. In particular, octrees are typically implemented using pointers, where each node contains
a pointer to its children. Accessing an arbitrary element (or
the neighbor of an element) in the octree requires a traversal
starting from the root until the desired cell is reached. Thus,
the number of memory accesses is equal to the depth of the
tree. This becomes increasingly costly for deep, i.e., highresolution, octrees. Convolutional network operations such
as convolution or pooling require frequent access to neighboring elements. It is thus critical to utilize an octree design
that allows for fast data access.
We tackle these challenges by leveraging a hybrid gridoctree data structure which we describe in Section 3.1. In
Section 3.2, we show how 3D convolution and pooling operations can be implemented efﬁciently on this data structure.
3.1. Hybrid Grid-Octree Data Structure
The above mentioned problems with the vanilla octree
data structure increase with the octree depth. Instead of representing the entire high resolution 3D input with a single
unbalanced octree, we leverage a hybrid grid-octree structure similar to the one proposed by Miller et al. . The
key idea is to restrict the maximal depth of an octree to a
Figure 2: Hybrid Grid-Octree Data Structure. This example illustrates a hybrid grid-octree consisting of 8 shallow octrees indicated by different colors. Using 2 shallow
octrees in each dimension with a maximum depth of 3 leads
to a total resolution of 163 voxels.
(a) Shallow Octree
(b) Bit-Representation
Figure 3: Bit Representation.
Shallow octrees can be
efﬁciently encoded using bit-strings. Here, the bit-string
1 01010000 000000000101000000000000010100000...
deﬁnes the octree in (a). The corresponding tree is shown
in (b). The color of the voxels corresponds to the split level.
small number, e.g., three, and place several such shallow
octrees along a regular grid (Fig. 2). While this data structure may not be as memory efﬁcient as the standard octree,
signiﬁcant compression ratios can still be achieved. For instance, a single shallow octree that does not contain input
data stores only a single vector, instead of 83 = 512 vectors
for all voxels at the ﬁnest resolution at depth 3.
An additional beneﬁt of a collection of shallow octrees
is that their structure can be encoded very efﬁciently using a bit string representation which further lowers access
time and allows for efﬁcient GPGPU implementations .
Given a shallow octree of depth 3, we use 73 bit to represent the complete tree. The ﬁrst bit with index 0 indicates,
if the root node is split, or not. Further, bits 1 to 8 indicate
if one of the child nodes is subdivided and bits 9 to 72 denote splits of the grandchildren, see Fig. 3. A tree depth of
3 gives a good trade-off between memory consumption and
computational efﬁciency. Increasing the octree depth results
in an exponential growth in the required bits to store the tree
structure and further increases the cell traversal time.
Using this bit-representation, a single voxel in the shallow octree is fully characterised by its bit index. This index
determines the depth of the voxel in the octree and therefore
also the voxel size. Instead of using pointers to the parent
and child nodes, simple arithmetic can be used to retrieve
the corresponding indices of a voxel with bit index i:
ch(i) = 8 · i + 1 .
In contrast to , we associate a data container (for storing
features vectors) with all leaf nodes of each shallow tree.
We allocate the data of a shallow octree in a contiguous data
array. The offset associated with a particular voxel in this
array can be computed as follows:
data idx(i) = 8
bit(j) + 1
#nodes above i
#split nodes pre i
+ mod (i −1, 8)
Here, mod denotes the modulo operator and bit returns the
tree bit-string value at i. See supp. document for an example. Both sum operations can be efﬁciently implemented
using bit counting intrinsics (popcnt). The data arrays of
all shallow octrees are concatenated into a single contiguous
data array during training and testing to reduce I/O latency.
3.2. Network Operations
Given the hybrid grid-octree data structure introduced in
the previous Section, we now discuss the efﬁcient implementation of network operations on this data structure. We
will focus on the most common operations in convolutional
networks : convolution, pooling and unpooling.
Note that point-wise operations, like activation functions,
do not differ in their implementation as they are independent of the data structure.
Let us ﬁrst introduce the notation which will be used
throughout this Section. Ti,j,k denotes the value of a 3D
tensor T at location (i, j, k). Now assume a hybrid gridoctree structure with D × H × W unbalanced shallow octrees of maximum depth 3. Let O[i, j, k] denote the value
of the smallest cell in this structure which comprises the
voxel (i, j, k). Note that in contrast to the tensor notation,
O[i1, j1, k1] and O[i2, j2, k2] with i1 ̸= i2 ∨j1 ̸= j2 ∨k1 ̸=
k2 may refer to the same voxel in the hybrid grid-octree,
depending on the size of the voxels. We obtain the index
of the shallow octree in the grid via (⌊i
8⌋) and the
local index of the voxel at the ﬁnest resolution in that octree
by (mod (i, 8), mod (j, 8), mod (k, 8)).
(a) Standard Convolution
(b) Efﬁcient Convolution
Figure 4: Convolution. This ﬁgure illustrates the convolution of a 33 kernel (red) with a 83 grid-octree cell (black).
Only 2 of the 3 dimensions are shown. A na¨ıve implementation evaluates the kernel at every location (i, j, k) within
a grid-octree cell as shown in (a). This results in ∼14k multiplications for this example. In contrast, (b) depicts our
efﬁcient implementation of the same operation which requires only ∼3k multiplications. As all 83 voxels inside the
grid-octree cell are the same value, the convolution kernel
inside the cell needs to be evaluated only once. Voxels at the
cell boundary need to integrate information from neighboring cells. This can be efﬁciently implemented by summing
truncated kernels. See our supp. document for details.
Given this notation, the mapping from a grid-octree O to
a tensor T with compatible dimensions is given by
oc2ten : Ti,j,k = O[i, j, k] .
Similarly, the reverse mapping is given by
ten2oc : O[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]
(T¯i,¯j,¯k) ,
where pool voxels (·) is a pooling function (e.g., averageor max-pooling) which pools all voxels in T over the smallest grid-octree cell comprising location (i, j, k), denoted by
Ω[i, j, k]. This pooling is necessary as a single voxel in O
can cover up to 83 = 512 elements of T, depending on its
size |Ω[i, j, k]|.
Remark: With the two functions deﬁned above, we could
wrap any network operation f deﬁned on 3D tensors via
g(O) = ten2oc(f(oc2ten(O))) .
However, this would require a costly conversion from the
memory efﬁcient grid-octrees to a regular 3D tensor and
back. Besides, storing a dense tensor in memory limits the
maximal resolution. We therefore deﬁne our network operations directly on the hybrid grid-octree data structure.
Convolution
The convolution operation is the most important, but also the most computational expensive operation in deep convolutional networks. For a single feature
(b) Output
Figure 5: Pooling. The 23 pooling operation on the gridoctree structure combines 8 neighbouring shallow octrees
(a) into one shallow octree (b). The size of each voxel is
halved and copied to the new shallow octree structure. Voxels on the ﬁnest resolution are pooled. Different shallow
octrees are depicted in different colors.
map, convolving a 3D tensor T with a 3D convolution kernel W ∈RL×M×N can be written as
Wl,m,n · T in
ˆi,ˆj,ˆk ,
with ˆi = i −l + ⌊L/2⌋, ˆj = j −m + ⌊M/2⌋, ˆk = k −n +
⌊N/2⌋. Similarly, the convolutions on the grid-octree data
structure are deﬁned as
Oout[i, j, k] = pool voxels
(¯i,¯j,¯k)∈Ω[i,j,k]
(T¯i,¯j,¯k)
Wl,m,n · Oin[ˆi, ˆj, ˆk] .
While this calculation yields the same result as the tensor
convolution in Eq. (7) with the oc2ten, ten2oc wrapper, we
are now able to deﬁne a computationally more efﬁcient convolution operator. Our key observation is that for small convolution kernels and large voxels, Ti,j,k is constant within
a small margin of the voxel due to its constant support
Oin[ˆi, ˆj, ˆk]. Thus, we only need to compute the convolution within the voxel once, followed by convolution along
the surface of the voxel where the support changes due to
adjacent voxels taking different values (Fig. 4). This minimizes the number of calculations by a factor of 4 for voxels
of size 83, see supp. material for a detailed derivation. At
the same time, it enables a better caching mechanism.
Another important operation in deep convolutional networks is pooling. Pooling reduces the spatial resolution of the input tensor and aggregates higher-level information for further processing, thereby increasing the receptive ﬁeld and capturing context. For instance, strided
23 max-pooling divides the input tensor T in into 23 nonoverlapping regions and computes the maximum value
(b) Output
Figure 6: Unpooling. The 23 unpooling operation transforms a single shallow octree of depth d as shown in (a)
into 8 shallow octrees of depth d −1, illustrated in (b). For
each node at depth zero one shallow octree is spawned. All
other voxels double in size. Different shallow octrees are
depicted in different colors.
within each region. Formally, we have
l,m,n∈ 
2i+l,2j+m,2k+n
where T in ∈R2D×2H×2W and T out ∈RD×H×W .
To implement pooling on the grid-octree data structure
we reduce the number of shallow octrees. For an input gridoctree Oin with 2D × 2H × 2W shallow octrees, the output
Oout contains D × H × W shallow octrees. Each voxel
of Oin is halved in size and copied one level deeper in the
shallow octree. Voxels at depth 3 in Oin are pooled. This
can be formalized as
Oout[i, j, k] =
Oin[2i, 2j, 2k]
if vxd(2i, 2j, 2k) < 3
l,m,n∈ (Oin[2i + l, 2j + m, 2k + n]) ,
where vxd(·) computes the depth of the indexed voxel in
the shallow octree. A visual example is depicted in Fig. 5.
For several tasks such as semantic segmentation, the desired network output is of the same size as the
network input. While pooling is crucial to increase the receptive ﬁeld size of the network and capture context, it loses
spatial resolution.
To increase the resolution of the network, U-shaped network architectures have become popular which encode information using pooling operations and increase the resolution in a decoder part using
unpooling or deconvolution layers , possibly in combination with skip-connections to increase precision. The simplest unpooling strategy uses nearest neighbour interpolation and can be formalized on dense input
T in ∈RD×H×W and output T out ∈R2D×2H×2W tensors
as follows:
i,j,k = T in
⌊i/2⌋,⌊j/2⌋,⌊k/2⌋.
Again, we can deﬁne the analogous operation on the hybrid
grid-octree data structure by
Oout[i, j, k] = Oin[⌊i/2⌋, ⌊j/2⌋, ⌊k/2⌋] .
This operation also changes the data structure: The number
of shallow octrees increases by a factor of 8, as each node
at depth 0 spawns a new shallow octree. All other nodes
double their size. Thus, after this operation the tree depth is
decreased. See Fig. 6 for a visual example of this operation.
Remark: To capture ﬁne details, voxels can be split again
at the ﬁnest resolution according to the original octree of
the corresponding pooling layer. This allows us to take full
advantage of skip connections. We follow this approach in
our semantic 3D point cloud labeling experiments.
4. Experimental Evaluation
In this Section we leverage our OctNet representation to
investigate the impact of input resolution on three different
3D tasks: 3D shape classiﬁcation, 3D orientation estimation
and semantic segmentation of 3D point clouds. To isolate
the effect of resolution from other factors we consider simple network architectures. Orthogonal techniques like data
augmentation, joint 2D/3D modeling or ensemble learning
are likely to further improve the performance of our models.
Implementation
We implemented the grid-octree data
structure, all layers including the necessary forward and
backward functions, as well as utility methods to create the
data structure from point clouds and meshes, as a standalone C++/CUDA library. This allows the usage of our code
within all existing deep learning frameworks. For our experimental evaluation we used the Torch3 framework.
4.1. 3D Classiﬁcation
We use the popular ModelNet10 dataset for the 3D
shape classiﬁcation task. The dataset contains 10 shape categories and consists of 3991 3D shapes for training and 908
3D shapes for testing. Each shape is provided as a triangular mesh, oriented in a canonical pose. We convert the
triangle meshes to dense respective grid-octree occupancy
grids, where a voxel is set to 1 if it intersects the mesh. We
scale each mesh to ﬁt into a 3D grid of (N −P)3 voxels,
where N is the number of voxels in each dimension of the
input grid and P = 2 is a padding parameter.
We ﬁrst study the inﬂuence of the input resolution on
memory usage, runtime and classiﬁcation accuracy.
Towards this goal, we create a series of networks of different
input resolution from 83 to 2563 voxels. Each network consists of several blocks which reduce resolution by half until
we reach a resolution of 83. Each block comprises two convolutional layers (33 ﬁlters, stride 1) and one max-pooling
3 
Input Resolution
Memory [GB]
(a) Memory
Input Resolution
Runtime [s]
(b) Runtime
Input Resolution
(c) Accuracy
Input Resolution
(d) Accuracy
Figure 7: Results on ModelNet10 Classiﬁcation Task.
layer (23 ﬁlters, stride 2). The number of feature maps in the
ﬁrst block is 8 and increases by 6 with every block. After the
last block we add a fully-connected layer with 512 units and
a ﬁnal output layer with 10 units. Each convolutional layer
and the ﬁrst fully-connected layer are followed by a recti-
ﬁed linear unit as activation function and the weights
are initialized as described in . We use the standard
cross-entropy loss for training and train all networks for 20
epochs with a batch size of 32 using Adam . The initial
learning rate is set to 0.001 and we decrease the learning
rate by a factor of 10 after 15 epochs.
Overall, we consider three different types of networks:
the original VoxNet architecture of Maturana et al. 
which operates on a ﬁxed 323 voxel grid, the proposed Oct-
Net and a dense version of it which we denote “DenseNet”
in the following.
While performance gains can be obtained using orthogonal approaches such as network ensembles or a combination of 3D and 2D convolutional networks , in this paper we deliberately focus on “pure”
3D convolutional network approaches to isolate the effect of
resolution from other inﬂuencing factors.
Fig. 7 shows our results. First, we compare the memory consumption and run-time of our OctNet wrt. the dense
baseline approach, see Fig. 7a and 7b. Importantly, OctNets
require signiﬁcantly less memory and run-time for high input resolutions compared to dense input grids.
batch size of 32 samples, our OctNet easily ﬁts in a modern
GPU’s memory (12GB) for an input resolution of 2563. In
contrast, the corresponding dense model ﬁts into the memory only for resolutions ≤643. A more detailed analysis of
the memory consumption wrt. the sparsity in the data is provided in the supp. document. OctNets also run faster than
their dense counterparts for resolutions >643. For resolutions ≤643, OctNets run slightly slower due to the overhead
incurred by the grid-octree representation and processing.
Figure 8: Voxelized 3D Shapes from ModelNet10.
Leveraging our OctNets, we now compare the impact
of input resolution with respect to classiﬁcation accuracy.
Fig. 7c shows the results of different OctNet architectures
where we keep the number of convolutional layers per block
ﬁxed to 1, 2 and 3. Fig. 7d shows a comparison of accuracy
with respect to DenseNet and VoxNet when keeping the capacity of the model, i.e., the number of parameters, constant
by removing max-pooling layers from the beginning of the
network. We ﬁrst note that despite its pooled representation,
OctNet performs on par with its dense equivalent. This con-
ﬁrms our initial intuition (Fig. 1) that sparse data allows for
allocating resources adaptively without loss in performance.
Furthermore, both models outperform the shallower VoxNet
architecture, indicating the importance of network depth.
Regarding classiﬁcation accuracy we observed improvements for lower resolutions but diminishing returns beyond
an input resolution of 323 voxels. Taking a closer look at
the confusion matrices in Fig. 9, we observe that higher input resolution helps for some classes, e.g., bathtub, while
others remain ambiguous independently of the resolution,
e.g., dresser vs. night stand. We visualize this lack of discriminative power by showing voxelized representations of
3D shapes from the ModelNet10 database Fig. 8. While
bathtubs look similar to beds (or sofas, tables) at low resolution they can be successfully distinguished at higher resolutions. However, a certain ambiguity between dresser and
night stand remains.
4.2. 3D Orientation Estimation
In this Section, we investigate the importance of input
resolution on 3D orientation estimation. Most existing approaches to 3D pose estimation assume that
the true 3D shape of the object instance is known. To assess
0.01 0.01 0.30
0.87 0.05 0.08
0.02 0.96 0.01
0.02 0.03 0.12
Figure 9: Confusion Matrices on ModelNet10.
Input Resolution
Mean Angular Error µ(φ)[◦]
(a) Mean Angular Error
Input Resolution
Mean Angular Error µ(φ)[◦]
(b) Mean Angular Error
Figure 10: Orientation Estimation on ModelNet10.
the generalization ability of 3D convolutional networks, we
consider a slightly different setup where only the object category is known. After training a model on a hold-out set of
3D shapes from a single category, we test the ability of the
model to predict the 3D orientation of unseen 3D shapes
from the same category.
More concretely, given an instance of an object category
with unknown pose, the goal is to estimate the rotation with
respect to the canonical pose. We utilize the 3D shapes from
the chair class of the ModelNet10 dataset and rotate them
randomly between ±15◦around each axis. We use the same
network architectures and training protocol as in the classi-
ﬁcation experiment, except that the networks regress orientations. We use unit quaternions to represent 3D rotations
and train our networks with an Euclidean loss. For small angles, this loss is a good approximation to the rotation angle
φ = arccos(2⟨q1, q2⟩2 −1) between quaternions q1, q2.
Fig. 10 shows our results using the same naming convention as in the previous Section. We observe that ﬁne
details are more important compared to the classiﬁcation
task. For the OctNet 1-3 architectures we observe a steady
increase in performance, while for networks with constant
capacity across resolutions (Fig. 10b), performance levels
beyond 1283 voxels input resolution. Qualitative results of
the latter experiment are shown in Fig. 11. Each row shows
10 different predictions for two randomly selected chair
instance over several input resolutions, ranging from 163
to 1283. Darker colors indicate larger errors which occur
more frequently at lower resolutions. In contrast, predictions at higher network resolutions cluster around the true
pose. Note that learning a dense 3D representation at a resolution of 1283 voxels or beyond would not be feasible.
Figure 11: Orientation Estimation on ModelNet10. This
ﬁgure illustrates 10 rotation estimates for 3 chair instances
while varying the input resolution from 163 to 1283. Darker
colors indicate larger deviations from the ground truth.
4.3. 3D Semantic Segmentation
In this Section, we evaluate the proposed OctNets on the
problem of labeling 3D point cloud with semantic information. We use the RueMonge2014 dataset that provides
a colored 3D point cloud of several Haussmanian style facades, comprising ∼1 million 3D points in total. The labels
are window, wall, balcony, door, roof, sky and shop.
For this task, we train a U-shaped network on
three different input resolutions, 643, 1283 and 2563, where
the voxel size was selected such that the height of all buildings ﬁts into the input volume. We ﬁrst map the point cloud
into the grid-octree structure. For all leaf nodes which contain more than one point, we average the input features and
calculate the majority vote of the ground truth labels for
training. As features we use the binary voxel occupancy, the
RGB color, the normal vector and the height above ground.
Due to the small number of training samples, we augment
the data for this task by applying small rotations.
Our network architecture comprises an encoder and a decoder part. The encoder part consists of four blocks which
comprise 2 convolution layers (33 ﬁlters, stride 1) followed
by one max-pooling layer each. The decoder consists of
four blocks which comprise 2 convolutions (33 ﬁlters, stride
1) followed by a guided unpooling layer as discussed in the
previous Section. Additionally, after each unpooling step all
features from the last layer of the encoder at the same resolution are concatenated to provide high-resolution details.
All networks are trained with a per voxel cross entropy loss
using Adam and a learning rate of 0.0001.
Table 1 compares the proposed OctNet to several state
of the art approaches on the facade labeling task following
the extended evaluation protocol of . The 3D points of
Riemenschneider et al. 
Martinovic et al. 
Gadde et al. 
OctNet 643
OctNet 1283
OctNet 2563
Table 1: Semantic Segmentation on RueMonge2014.
(a) Voxelized Input
(b) Voxel Estimates
(c) Estimated Point Cloud
(d) Ground Truth Point Cloud
Figure 12: OctNet 2563 Facade Labeling Results.
the test set are assigned the label of the corresponding gridoctree voxels. As evaluation measures we use overall pixel
TP+FN over all 3D points, average class accuracy, and intersection over union
TP+FN+FP over all classes.
Here, FP, FN and TP denote false positives, false negatives
and true positives, respectively.
Our results clearly show that increasing the input resolution is essential to obtain state-of-the-art results, as ﬁner
details vanish at coarser resolutions. Qualitative results for
one facade are provided in Fig. 12. Further results are provided in the supp. document.
5. Conclusion and Future Work
We presented OctNet, a novel 3D representation which
makes deep learning with high-resolution inputs tractable.
We analyzed the importance of high resolution inputs on
several 3D learning tasks, such as object categorization,
pose estimation and semantic segmentation.
Our experiments revealed that for ModelNet10 classiﬁcation lowresolution networks prove sufﬁcient while high input (and
output) resolution matters for 3D orientation estimation and
3D point cloud labeling. We believe that as the community
moves from low resolution object datasets such as Model-
Net10 to high resolution large scale 3D data, OctNet will
enable further improvements. One particularly promising
avenue for future research is in learning representations for
multi-view 3D reconstruction where the ability to process
high resolution voxelized shapes is of crucial importance.