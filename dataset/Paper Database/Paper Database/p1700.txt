TECHNICAL WORKING PAPER SERIES
ON THE FAILURE OF THE BOOTSTRAP FOR MATCHING ESTIMATORS
Alberto Abadie
Guido W. Imbens
Technical Working Paper 325
 
NATIONAL BUREAU OF ECONOMIC RESEARCH
1050 Massachusetts Avenue
Cambridge, MA 02138
We are grateful for comments by Peter Bickel, Stéphane Bonhomme, Whitney Newey, and seminar
participants at Princeton, CEMFI, and Harvard/MIT. Financial support for this research was generously
provided through NSF grants SES-0350645 (Abadie) and SES-0136789 and SES-0452590 (Imbens). Abadie:
John F. Kennedy School of Government, Harvard University, 79 John F. Kennedy Street, Cambridge, MA
Electronic
correspondence:
 ,
 Imbens: Department of Economics, and Department of Agricultural
and Resource Economics, University of California at Berkeley, 330 Giannini Hall, Berkeley, CA
94720-3880,
Electronic
correspondence:
 ,
 The views expressed herein are those of the author(s) and do not
necessarily reflect the views of the National Bureau of Economic Research.
©2006 by Alberto Abadie and Guido W. Imbens. All rights reserved. Short sections of text, not to exceed
two paragraphs, may be quoted without explicit permission provided that full credit, including © notice, is
given to the source.
On the Failure of the Bootstrap for Matching Estimators
Alberto Abadie and Guido W. Imbens
NBER Technical Working Paper No. 325
JEL No. C14, C21, C52
Matching estimators are widely used for the evaluation of programs or treatments. Often researchers
use bootstrapping methods for inference. However, no formal justification for the use of the
bootstrap has been provided. Here we show that the bootstrap is in general not valid, even in the
simple case with a single continuous covariate when the estimator is root-N consistent and
asymptotically normally distributed with zero asymptotic bias. Due to the extreme non-smoothness
of nearest neighbor matching, the standard conditions for the bootstrap are not satisfied, leading the
bootstrap variance to diverge from the actual variance. Simulations confirm the difference between
actual and nominal coverage rates for bootstrap confidence intervals predicted by the theoretical
calculations. To our knowledge, this is the first example of a root-N consistent and asymptotically
normal estimator for which the bootstrap fails to work.
Alberto Abadie
John F. Kennedy School of Government
Harvard University
79 John F. Kennedy Street
Cambridge, MA 02138
 
Guido W. Imbens
Department of Economics
Department of Agricultural and Resource Economics
University of California at Berkeley
330 Giannini Hall
Berkeley, CA 94720-3880
 
Introduction
Matching methods have become very popular for the estimation of treatment eﬀects.1
Often researchers use bootstrap methods to calculate the standard errors of matching
estimators.2 Bootstrap inference for matching estimators has not been formally justiﬁed.
Because of the non-smooth nature of some matching methods and the lack of evidence
that the resulting estimators are asymptotically linear (e.g., nearest neighbor matching
with a ﬁxed number of neighbors), there is reason for concern about their validity of the
bootstrap in this context.
At the same time, we are not aware of any example where an estimator is root-N
consistent, as well as asymptotically normally distributed with zero asymptotic bias and
yet where the standard bootstrap fails to deliver valid conﬁdence intervals.3 This article
addresses the question of the validity of the bootstrap for nearest-neighbor matching
estimators with a ﬁxed number of neighbors. We show in a simple case with a single
continuous covariate that the standard bootstrap does indeed fail to provide asymptotically valid conﬁdence intervals, in spite of the fact that the estimator is root-N consistent
and asymptotically normal with no asymptotic bias. We provide some intuition for this
failure. We present theoretical calculations for the asymptotic behavior of the diﬀerence
between the variance of the matching estimator and the average of the bootstrap variance.
These theoretical calculations are supported by Monte Carlo evidence. We show that the
bootstrap conﬁdence intervals can have over-coverage as well as under-coverage. The
1E.g., Dehejia and Wahba, . See Rosenbaum and Imbens for surveys.
2A partial list of recent papers using matching with bootstrapped standard errors includes Agodini
and Dynarski , Dehejia and Wahba , Galasso and Ravaillon , Guarcello, Mealli,
and Rosati , Heckman, Ichimura and Todd , Ichino and Becker , Imai , Jalan
and Ravallion , Lechner , Myers, Olsen, Seftor, Young, and Tuttle Pradhan, and
Rawlings , Puhani , Sianesi , Smith and Todd , Yamani, Lauer, Starling,
Pothier, Tuzcu, Ratliﬀ, Cook, Abdo, McNeil, Crowe, Hobbs, Rincon, Bott-Silverman, McCarthy and
Young .
3Familiar examples of failure of the bootstrap for estimators with non-normal limiting distributions
arise in the contexts of estimating the maximum of the support of a random variable , estimating the average of a variable with inﬁnite variance , and
super-eﬃcient estimation . Resampling inference in these contexts can be conducted using
alternative methods such as subsampling 
and versions of the bootstrap where the size of the bootstrap sample is smaller than the sample size
 . See Hall and Horowitz for general discussions.
results do not address whether nearest neighbor estimators with the number of neighbors
increasing with the sample size do satisfy asymptotic linearity or whether the bootstrap
is valid for such estimators as in practice many researchers have used estimators with
very few (e.g., one) nearest neighbor(s).
In Abadie and Imbens we have proposed analytical estimators of the asymptotic variance of matching estimators. Because the standard bootstrap is shown to be
invalid, together with subsampling these are now the
only available methods of inference that are formally justiﬁed.4
The rest of the article is organized as follows. Section 2 reviews the basic notation
and setting of matching estimators. Section 3 presents theoretical results on the lack of
validity of the bootstrap for matching estimators, along with simulations that conﬁrm
the formal results. Section 4 concludes. The appendix contains proofs.
Basic Model
In this article we adopt the standard model of treatment eﬀects under unconfoundedness
 . The goal is to evaluate the eﬀect of a treatment on the basis
of data on outcomes and covariates for treated and control units. We have a random sample of N0 units from the control population, and a random sample of N1 units from the
treated population, with N = N0 + N1. Each unit is characterized by a pair of potential
outcomes, Yi(0) and Yi(1), denoting the outcomes under the control and active treatment
respectively. We observe Yi(0) for units in the control sample, and Yi(1) for units in the
treated sample. For all units we observe a covariate vector, Xi.5 Let Wi indicate whether
a unit is from the control sample (Wi = 0) or the treatment sample (Wi = 1). For each
unit we observe the triple (Xi, Wi, Yi) where Yi = Wi Yi(1)+(1−Wi) Yi(0) is the observed
4Politis, Romano, and Wolf show that subsampling produces valid inference for statistics with
stable asymptotic distributions.
5To simplify our proof of lack of validity of the bootstrap we will consider in our calculations the case
with a scalar covariate. With higher dimensional covariates there is the additional complication of biases
that may dominate the asymptotic distribution of matching estimators .
outcome. Let X an N-column matrix with column i equal to Xi, and similar for Y and
W. Also, let X0 denote the N-column matrix with column i equal to (1 −Wi) Xi, and
X1 the N-column matrix with column i equal to Wi Xi. The following two assumptions
are the identiﬁcation conditions behind matching estimators.
Assumption 2.1: (unconfoundedness) For almost all x, (Yi(1), Yi(0)) is independent of Wi conditional on Xi = x, or
Yi(0), Yi(1)
¯¯¯ Xi = x,
Assumption 2.2: (overlap) For some c > 0, and almost all x
c ≤Pr(Wi = 1|Xi = x) ≤1 −c.
In this article we focus on matching estimation of the average treatment eﬀect for the
τ = E[Yi(1) −Yi(0)|Wi = 1].
A nearest neighbor matching estimator of τ matches each treated unit i to the control
unit j with the closest value for the covariate, and then averages the within-pair outcome
diﬀerences, Yi −Yj, over the N1 matched pairs. Here we focus on the case of matching
with replacement, so each control unit can be used as a match for more than one treated
Formally, for all treated units i (that is, units with Wi = 1) let Di be the distance
between the covariate value for observation i and the covariate value for the closest
(control) match:
j=1,...,N:Wj=0 ∥Xi −Xj∥.
J (i) = {j ∈{1, 2, . . . , N} : Wj = 0, ∥Xi −Xj∥= Di}
6In many cases, the interest is in the average eﬀect for the entire population. We focus here on the
average eﬀect for the treated because it simpliﬁes the calculations below. Since the overall average eﬀect
is the weighted sum of the average eﬀect for the treated and the average eﬀect for the controls it suﬃces
to show that the bootstrap is not valid for one of the components.
be the set of closest matches for treated unit i. If unit i is a control unit, then J (i)
is deﬁned to be the empty set. When Xi is continuously distributed, the set J (i) will
consist of a single index with probability one, but for bootstrap samples there will often
be more than one index in this set (because an observation from the original sample may
appear multiple times in the bootstrap sample). For each treated unit, i, let
be the average outcome in the set of the closest matches for observation i, where #J (i)
is the number of elements of the set J (i). The matching estimator of τ is then
Yi −ˆYi(0)
For the subsequent discussion it is useful to write the estimator in a diﬀerent way. Let
Ki denote the weighted number of times unit i is used as a match (if unit i is a control
unit, with Ki = 0 if unit i is a treated unit):
if Wi = 1,
1{i ∈J (j)}
if Wi = 0.
Then we can write
(Wi −Ki)Yi.
if Wi = 1,
1{i ∈J (j)}
if Wi = 0.
Abadie and Imbens prove that under certain conditions (for example, when X is
a scalar variable) the nearest-neighbor matching estimator in (2.2) is root-N consistent
and asymptotically normal with zero asymptotic bias.7 Abadie and Imbens propose two
7More generally, Abadie and Imbens propose a bias correction that makes matching estimators
root-N consistent and asymptotically normal regardless of the dimension of X.
variance estimators:
bV AI,I = 1
bσ2(Xi, Wi),
bV AI,II = 1
Yi −ˆYi(0) −ˆτ
i) bσ2(Xi, Wi),
where bσ2(Xi, Wi) is an estimator of the conditional variance of Yi given Wi and Xi based
on matching. Let lj(i) be the j-th closest match to unit i, in terms of the covariates,
among the units with the same value for the treatment (that is, units in the treatment
groups are matched to units in the treatment group, and units in the control group are
matched to units in the control group).8 Deﬁne
bσ2(Xi, Wi) =
Let V(ˆτ) be the variance of ˆτ, and let V(ˆτ|X, W) the variance of ˆτ conditional on X
and W. Abadie and Imbens show that (under weak regularity conditions) the
normalized version of ﬁrst variance estimator, N1 bV AI,I is consistent for the normalized
conditional variance, N1V(ˆτ|X, W):
N1 (V(ˆτ|X, W) −bV AI,I)
for ﬁxed J as N →∞.
The normalized version of the second variance estimator,
N1 bV AI,II, is consistent for the normalized marginal variance, N1V(ˆτ):
N1 (V(ˆτ) −bV AI,II)
for ﬁxed J as N →∞.
8To simplify the notation, here we consider only the case without matching ties. The extension to
accommodate ties is immediate , but it is not required
for the purpose of the analysis in this article.
The Bootstrap
We consider two versions of the bootstrap in this discussion. The ﬁrst version centers the
bootstrap variance at the matching estimate in the original sample. The second version
centers the bootstrap variance at the mean of the bootstrap distribution of the matching
estimator.
Consider a random sample Z = (X, W, Y) with N0 controls and N1 treated units.
The matching estimator, ˆτ, is a functional t(·) of the original sample: ˆτ = t(Z). We
construct a bootstrap sample, Zb, with N0 controls and N1 treated by sampling with
replacement from the two subsamples. We then calculate the bootstrap estimator, ˆτb,
applying the functional t(·) to the bootstrap sample: ˆτb = t(Zb). The ﬁrst version of the
bootstrap variance is the second moment of (ˆτb −ˆτ) conditional on the sample, Z:
V B,I = vI(Z) = E
(ˆτb −ˆτ)2¯¯ Z
The second version of the bootstrap variance centers the bootstrap variance at the bootstrap mean, E[ˆτb|Z], rather than at the original estimate, ˆτ:
V B,II = vII(Z) = E
(ˆτb −E [ˆτb|Z])2¯¯ Z
Although these bootstrap variances are deﬁned in terms of the original sample Z, in
practice an easier way to calculate them is by drawing B bootstrap samples. Given B
bootstrap samples with bootstrap estimates ˆτb, for b = 1, . . . , B, we can obtain unbiased
estimators for these two variances as
ˆV B,I = 1
(ˆτb −ˆτ)2 ,
We will focus on the ﬁrst bootstrap variance, V B,I, and its unconditional expectation,
E[V B,I]. We shall show that in general N1 E[V B,I] does not converge to N1 V(ˆτ). We will
show that in some cases the limit of N1(E[V B,I]−V(ˆτ)) is positive and that in other cases
this limit is negative. As a result, we will show that N1V B,I is not a consistent estimator
of the limit of N1V(ˆτ). This will indirectly imply that N1V B,II is not consistent either.
(ˆτb −ˆτ)2¯¯ Z
(ˆτb −E [ˆτb|Z])2¯¯ Z
it follows that E[V B,I] ≥E[V B,II]. Thus in the cases where the limit of N1(E[V B,I]−V(ˆτ))
is smaller than zero, it follows that the limit of N1(E[V B,II] −V(ˆτ)) is also smaller than
In most standard settings, both centering the bootstrap variance at the estimate in
the original sample or at the average of the bootstrap distribution of the estimator lead
to valid conﬁdence intervals.
In fact, in many settings the average of the bootstrap
distribution of an estimator is identical to the estimate in the original sample.
example, if we are interested in constructing a conﬁdence interval for the population
mean µ = E[X] given a random sample X1, . . . , XN, the expected value of the bootstrap
statistic, E[ˆµb|X1, . . . , XN], is equal to the sample average for the original sample, ˆµ =
i Xi/N. For matching estimators, however, it is easy to construct examples where the
average of the bootstrap distribution of the estimator diﬀers from the estimate in the
original sample. As a result, the two bootstrap variance estimators will lead to diﬀerent
conﬁdence intervals with potentially diﬀerent coverage rates.
An Example where the Bootstrap Fails
In this section we discuss in detail a speciﬁc example where we can calculate the limits
of N1V(ˆτ) and N1E[V B,I] and show that they diﬀer.
Data Generating Process
We consider the following data generating process:
Assumption 3.1: The marginal distribution of the covariate X is uniform on the interval 
Assumption 3.2: The ratio of treated and control units is N1/N0 = α for some α > 0.
Assumption 3.3: The propensity score e(x) = Pr(Wi = 1|Xi = x) is constant as a
function of x.
Assumption 3.4: The distribution of Yi(1) is degenerate with Pr(Yi(1) = τ) = 1, and
the conditional distribution of Yi(0) given Xi = x is normal with mean zero and variance
The implication of Assumptions 3.2 and 3.3 is that the propensity score is e(x) = α/(1+
Exact Variance and Large Sample Distribution
The data generating process implies that conditional on X = x the treatment eﬀect
is equal to E[Y (1) −Y (0)|X = x] = τ for all x.
Therefore, the average treatment
eﬀect for the treated is equal to τ. Under this data generating process P
i Wi Yi/N1 =
i Wi Yi(1)/N1 = τ, which along with equation (2.3) implies:
ˆτ −τ = −1
Conditional on X and W the only stochastic component of ˆτ is Y. By Assumption 3.4
the Yi-s are mean zero, unit variance, and independent of X. Thus E[ˆτ −τ|X, W] = 0.
Because (i) E[Yi Yj|Wi = 0, X, W] = 0 for i ̸= j, (ii) E[Y 2
i |Wi = 0, X, W] = 1 and (iii)
Ki is a deterministic function of X and W, it also follows that the conditional variance
of ˆτ given X and W is
V(ˆτ|X, W) = 1
Because V(E[ˆτ|X, W]) = V(τ) = 0, the (exact) unconditional variance of the matching
estimator is therefore equal to the expected value of the conditional variance:
V(ˆτ) = N0
Lemma 3.1: (Exact Variance of Matching Estimator)
Suppose that Assumptions 2.1, 2.2, and 3.1-3.4 hold. Then
(i) the exact variance of the matching estimator is
(N1 −1)(N0 + 8/3)
N1(N0 + 1)(N0 + 2),
(ii) as N →∞,
N1 V(ˆτ) →1 + 3
and (iii),
N 1 (ˆτ −τ)
All proofs are given in the Appendix.
The Bootstrap Variance
Now we analyze the properties of the bootstrap variance, V B,I in (2.5). As before, let
Z = (X, W, Y) denote the original sample. We will look at the distribution of statistics
both conditional on the original sample, as well as over replications of the original sample
drawn from the same distribution. Notice that
(ˆτb −ˆτ)2¯¯ Z
(ˆτb −ˆτ)2¤
is the expected bootstrap variance. The following lemma establishes the limit of N1 E[V B,I]
under our data generating process.
Lemma 3.2: (Bootstrap Variance I) Suppose that Assumptions 3.1-3.4 hold. Then,
N1 E[V B,I] →1 + 3
2 α 5 exp(−1) −2 exp(−2)
3 (1 −exp(−1))
+ 2 exp(−1).
Recall that the limit of the normalized variance of ˆτ is 1 + (3/2) α. For small values of
α the limit of the expected bootstrap variance exceeds the limit variance by the third
term in (3.11), 2 exp(−1) ≃0.74, or 74%. For large values of α the second term in
(3.11) dominates and the ratio of the limit expected bootstrap and limit variance is
equal to the factor in the second term of (3.11) multiplying (3/2)α. Since (5 exp(−1) −
2 exp(−2))/(3 (1 −exp(−1))) ≃0.83, it follows that as α increases, the ratio of the limit
expected bootstrap variance to the limit variance asymptotes to 0.83, suggesting that in
large samples the bootstrap variance can under as well as over estimate the true variance.
So far, we have established the relation between the limiting variance of the estimator
and the limit of the average bootstrap variance. We end this section with a discussion
of the implications of the previous two lemmas for the validity of the bootstrap. The
ﬁrst version of the bootstrap provides a valid estimator of the asymptotic variance of the
simple matching estimator if:
(bτb −bτ)2¯¯ Z
Lemma 3.1 shows that:
N1V(bτ) −→1 + 3
Lemma 3.2 shows that
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
Assume that the ﬁrst version of the bootstrap provides a valid estimator of the asymptotic
variance of the simple matching estimator. Then,
(bτb −bτ)2¯¯ Z
Because N1E [(bτb −bτ)2| Z] ≥0, it follows by Portmanteau Lemma that, as N →∞,
2 α ≤lim E
(bτb −bτ)2¯¯ Z
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
However, the algebraic inequality
2 α ≤1 + 3
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1),
does not hold for large enough α. As a result, the ﬁrst version of the bootstrap does not
provide a valid estimator of the asymptotic variance of the simple matching estimator.
The second version of the bootstrap provides a valid estimator of the asymptotic
variance of the simple matching estimator if:
(bτb −E[bτb|Z])2¯¯ Z
Assume that the second version of the bootstrap provides a valid estimator of the asymptotic variance of the simple matching estimator. Then,
(bτb −E[bτb|Z])2¯¯ Z
Notice that E [(bτb −E[bτb|Z])2| Z] ≤E [(bτb −bτ)2| Z]. By Portmanteau Lemma, as N →∞
2 α ≤lim inf E
(bτb −E[bτb|Z])2¯¯ Z
(bτb −bτ)2¯¯ Z
(bτb −bτ)2¤
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).
Again, this inequality does not hold for large enough α. As a result, the second version
of the bootstrap does not provide a valid estimator of the asymptotic variance of the
simple matching estimator.
Simulations
We consider three designs: N0 = N1 = 100 (Design I), N0 = 100, N1 = 1000 (Design II),
and N0 = 1000, N1 = 100 (Design III), We use 10,000 replications, and 100 bootstrap
samples in each replication. These designs are partially motivated by Figure 1, which
gives the ratio of the limit of the expectation of the bootstrap variance (given in equation
(3.11)) to limit of the actual variance (given in equation (3.9)), for diﬀerent values of α.
On the horizontal axis is the log of α. As α converges to zero the variance ratio converges
to 1.74; at α = 1 the variance ratio is 1.19; and as α goes to inﬁnity the variance ratio
converges to 0.83. The vertical dashed lines indicate the three designs that we adopt in
our simulations: α = 0.1, α = 1, and α = 10.
The simulation results are reported in Table 1.
The ﬁrst row of the table gives
normalized exact variances, N1V(ˆτ), calculated from equation (3.8). The second and
third rows present averages (over the 10,000 simulation replications) of the normalized
variance estimators from Abadie and Imbens . The second row reports averages of
N1 bV AI,I and the third row reports averages of N1 bV AI,II. In large samples, N1 bV AI,I and
N1 bV AI,II are consistent for N1V(ˆτ|X, W) and N1V(ˆτ), respectively. Because, for our data
generating process, the conditional average treatment eﬀect is zero for all values of the
covariates, N1 bV AI,I and N1 bV AI,II converge to the same parameter. Standard errors (for
the averages over 10,000 replications) are reported in parentheses. The ﬁrst three rows
of Table 1 allow us to assess the diﬀerence between the averages of the Abadie-Imbens
(AI) variance estimators and the theoretical variances. For example, for Design I, the
normalized AI Var I estimator (N1 bV AI,I) is on average 2.449, with a standard error of
0.006. The theoretical variance is 2.480, so the diﬀerence between the theoretical and AI
variance I is approximately 1%, although it is statistically signiﬁcant at about 5 standard
errors. Given the theoretical justiﬁcation of the variance estimator, this diﬀerence is a
ﬁnite sample phenomenon.
The fourth row reports the limit of normalized expected bootstrap variance, N1E[V B,I],
calculated as in (3.11). The ﬁfth and sixth rows give normalized averages of the estimated
bootstrap variances, N1 bV B,I and N1 bV B,II, over the 10,000 replications. These variances
are estimated for each replication using 100 bootstrap samples, and then averaged over
all replications. Again it is interesting to compare the average of the estimated bootstrap
variance in the ﬁfth row to the limit of the expected bootstrap variance in the fourth
row. The diﬀerences between the fourth and ﬁfth rows are small (although signiﬁcantly
diﬀerent from zero as a result of the small sample size). The limited number of bootstrap
replications makes these averages noisier than they would otherwise be, but it does not
aﬀect the average diﬀerence. The results in the ﬁfth row illustrate our theoretical calculations in Lemma 3.2: the average bootstrap variance can over-estimate or under-estimate
the variance of the matching estimator.
The next two panels of the table report coverage rates, ﬁrst for nominal 90% conﬁdence intervals and then for nominal 95% conﬁdence intervals. The standard errors for
the coverage rates reﬂect the uncertainty coming from the ﬁnite number of replications
(10,000). They are equal to
p (1 −p)/R where for the second panel p = 0.9 and for
the third panel p = 0.95, and R = 10, 000 is the number of replications.
The ﬁrst rows of the last two panels of Table 1 report coverage rates of 90% and 95%
conﬁdence intervals constructed in each replication as the point estimate, ˆτ, plus/minus
1.645 and 1.96 times the square root of the variance in (3.8). The results show coverage
rates which are statistically indistinguishable from the nominal levels, for all three designs
and both levels (90% and 95%). The second row of the second and third panels of Table
1 report coverage rates for conﬁdence intervals calculated as in the preceding row but
using the estimator bV AI,I in (2.5). The third row report coverage rates for conﬁdence
intervals constructed with the estimator bV AI,II in (2.6). Both bV AI,I and bV AI,II produce
conﬁdence intervals with coverage rates that are statistically indistinguishable from the
nominal levels.
The last two rows of the second panel of Table 1 report coverage rates for bootstrap
conﬁdence intervals obtained by adding and subtracting 1.645 times the square root of
the estimated bootstrap variance in each replication, again over the 10,000 replications.
The third panel gives the corresponding numbers for 95% conﬁdence intervals.
Our simulations reﬂect the lack of validity of the bootstrap found in the theoretical calculations. Coverage rates of conﬁdence intervals constructed with the bootstrap
estimators of the variance are diﬀerent from nominal levels in substantially important
and statistically highly signiﬁcant magnitudes. In Designs I and III the bootstrap has
coverage larger the nominal coverage. In Design II the bootstrap has coverage smaller
than nominal. In neither case the diﬀerence is huge, but it is important to stress that
this diﬀerence will not disappear with a larger sample size, and that it may be more
substantial for diﬀerent data generating processes.
The bootstrap calculations in this table are based on 100 bootstrap replications. Increasing the number of bootstrap replications signiﬁcantly for all designs was infeasible
as matching is already computationally expensive.9 We therefore investigated the implications of this choice for Design I, which is the fastest to run. For the same 10,000
9Each calculation of the matching estimator requires N1 searches for the minimum of an array of
length N0, so that with B bootstrap replications and R simulations one quickly requires large amounts
of computer time.
replications we calculated both the coverage rates for the 90% and 95% conﬁdence intervals based on 100 bootstrap replications and based on 1,000 bootstrap replications.
For the conﬁdence intervals based on bV B,I the coverage rate for a 90% nominal level
was 0.002 (s.e. 0.001) higher with 1,000 bootstrap replications than with 100 bootstrap
replications. The coverage rate for the 95% conﬁdence interval was 0.003 (s.e., 0.001)
higher with 1,000 bootstrap replications than with 100 bootstrap replications. Because
the diﬀerence between the bootstrap coverage rates and the nominal coverage rates for
this design are 0.031 and 0.022 for the 90% and 95% conﬁdence intervals respectively, the
number of bootstrap replications can only explain approximately 6-15% of the diﬀerence
between the bootstrap and nominal coverage rates. We therefore conclude that using
more bootstrap replications would not substantially change the results in Table 1.
Conclusion
In this article we prove that the bootstrap is not valid for the standard nearest-neighbor
matching estimator with replacement. This is a somewhat surprising discovery, because
in the case with a scalar covariate the matching estimator is root-N consistent and
asymptotically normally distributed with zero asymptotic bias. However, the extreme
non-smooth nature of matching estimators and the lack of evidence that the estimator is
asymptotically linear explain the lack of validity of the bootstrap. We investigate a special
case where it is possible to work out the exact variance of the estimator as well as the limit
of the average bootstrap variance. We show that in this case the limit of the average
bootstrap variance can be greater or smaller than the limit variance of the matching
estimator. This implies that the standard bootstrap fails to provide valid inference for
the matching estimator studied in this article. A small Monte Carlo study supports the
theoretical calculations. The implication for empirical practice of these results is that
for nearest-neighbor matching estimators with replacement one should use the variance
estimators developed by Abadie and Imbens or the subsampling bootstrap . It may well be that if the number of neighbors increases with
the sample size the matching estimator does become asymptotically linear and suﬃciently
regular for the bootstrap to be valid. However, the increased risk of a substantial bias
has led many researchers to focus on estimators where the number of matches is very
small, often just one, and the asymptotics based on an increasing number of matches
may not provide a good approximation in such cases.
Finally, our results cast doubts on the validity of the standard bootstrap for other
estimators that are asymptotically normal but not asymptotically linear .
Before proving Lemma 3.1 we introduce some notation and preliminary results. Let X1, . . . , XN be a
random sample from a continuous distribution. Let Mj be the index of the closest match for unit j.
That is, if Wj = 1, then Mj is the unique index (ties happen with probability zero), with WMj = 0,
such that ∥Xj −XMj∥≤∥Xj −Xi∥, for all i such that Wi = 0. If Wj = 0, then Mj = 0. Let Ki be the
number of times unit i is the closest match for a treated observation:
Ki = (1 −Wi)
Wj 1{Mj = i}.
Following this deﬁnition Ki is zero for treated units. Using this notation, we can write the estimator for
the average treatment eﬀect on the treated as:
(Wi −Ki) Yi.
Also, let Pi be the probability that the closest match for a randomly chosen treated unit j is unit i,
conditional on both the vector of treatment indicators W and on vector of covariates for the control
Pi = Pr(Mj = i|Wj = 1, W, X0).
For treated units we deﬁne Pi = 0.
The following lemma provides some properties of the order statistics of a sample from the standard
uniform distribution.
Lemma A.1: Let X(1) ≤X(2) ≤· · · ≤X(N) be the order statistics of a random sample of size N from
a standard uniform distribution, U(0, 1). Then, for 1 ≤i ≤j ≤N,
(i)(1 −X(j))s] = i[r](N −j + 1)[s]
(N + 1)[r+s]
where for a positive integer, a, and a non-negative integer, b: a[b] = (a + b −1)!/(a −1)!. Moreover, for
1 ≤i ≤N, X(i) has a Beta distribution with parameters (i, N −i + 1); for 1 ≤i ≤j ≤N, (X(j) −X(i))
has a Beta distribution with parameters (j −i, N −(j −i) + 1).
Proof: All the results of this lemma, with the exception of the distribution of diﬀerences of order
statistics, appear in Johnson, Kotz, and Balakrishnan . The distribution of diﬀerences of order
statistics can be easily derived from the joint distribution of order statistics provided in Johnson, Kotz,
and Balakrishnan .
Notice that the lemma implies the following results:
for 1 ≤i ≤N,
(N + 1)(N + 2)
for 1 ≤i ≤N,
E[X(i)X(j)]
(N + 1)(N + 2)
for 1 ≤i ≤j ≤N.
First we investigate the ﬁrst two moments of Ki, starting by studying the conditional distribution of Ki
given X0 and W.
Lemma A.2: (Conditional Distribution and Moments of Ki)
Suppose that assumptions 3.1-3.3 hold. Then, the distribution of Ki conditional on Wi = 0, W, and X0
is binomial with parameters (N1, Pi):
Ki|Wi = 0, W, X0 ∼B(N1, Pi).
Proof: By deﬁnition Ki = (1 −Wi) PN
j=1 Wj 1{Mj = i}. The indicator 1{Mj = i} is equal to one if
the closest control unit for Xj is i. This event has probability Pi. In addition, the events 1{Mj1 = i}
and 1{Mj2 = i}, for Wj1 = Wj2 = 1 and j1 ̸= j2, are independent conditional on W and X0. Because
there are N1 treated units the sum of these indicators follows a binomial distribution with parameters
N1 and Pi.
This implies the following conditional moments for Ki:
E[Ki|W, X0] = (1 −Wi) N1 Pi,
i |W, X0] = (1 −Wi)
N1 Pi + N1 (N1 −1) P 2
To derive the marginal moments of Ki we need ﬁrst to analyze the properties of the random variable
Pi. Exchangeability of the units implies that the marginal expectation of Pi given N0, N1 and Wi = 0
is equal to 1/N0. To derive the second moment of Pi it is helpful to express Pi in terms of the order
statistics of the covariates for the control group. For control unit i let ι(i) be the order of the covariate
for the ith unit among control units:
(1 −Wj) 1{Xj ≤Xi}.
Furthermore, let X0(k) be the kth order statistic of the covariates among the control units, so that
X0(1) ≤X0(2) ≤. . . X0(N0), and for control units X0(ι(i)) = Xi.
Ignoring ties, a treated unit with
covariate value x will be matched to control unit i if
X0(ι(i)−1) + X0(ι(i))
≤x ≤X0(ι(i)+1) + X0(ι(i))
if 1 < ι(i) < N0. If ι(i) = 1, then x will be matched to unit i if
x ≤X0(2) + X0(1)
and if ι(i) = N0, x will be matched to unit i if
X0(N0−1) + X0(N0)
To get the value of Pi we need to integrate the density of X conditional on W = 1, f1(x), over these
sets. With a uniform distribution for the covariates in the treatment group (f1(x) = 1, for x ∈ ),
we get the following representation for Pi:
(X0(2) + X0(1))/2
if ι(i) = 1,
X0(ι(i)+1) −X0(ι(i)−1)
if 1 < ι(i) < N0,
1 −(X0(N0−1) + X0(N0))/2
if ι(i) = N0.
Lemma A.3: (Moments of Pi)
Suppose that Assumptions 3.1–3.3 hold. Then
(i), the second moment of Pi conditional on Wi = 0 is
i |Wi = 0] =
2N0(N0 + 1)(N0 + 2),
and (ii), the Mth moment of Pi is bounded by
i |Wi = 0] ≤
Proof: First, consider (i). Conditional on Wi = 0, Xi has a uniform distribution on the interval .
Using Lemma A.1 and equation (A.2), for interior i (i such that 1 < ι(i) < N0), we have that
E[Pi|1 < ι(i) < N0, Wi = 0] =
i |1 < ι(i) < N0, Wi = 0
(N0 + 1)(N0 + 2).
For the smallest and largest observations:
E[Pi|ι(i) ∈{1, N0}, Wi = 0] = 3
i |ι(i) ∈{1, N0}, Wi = 0] =
2(N0 + 1)(N0 + 2).
Averaging over all units includes two units at the boundary and N0 −2 interior values, we obtain:
E[Pi|Wi = 0] = N0 −2
(N0 + 1) + 2
(N0 + 1) = 1
i |Wi = 0] = N0 −2
(N0 + 1)(N0 + 2) + 2
(N0 + 1)(N0 + 2) =
2N0(N0 + 1)(N0 + 2).
For (ii) notice that
(X0(2) + X0(1))/2 ≤X0(2)
if ι(i) = 1,
X0(ι(i)+1) −X0(ι(i)−1)
X0(ι(i)+1) −X0(ι(i)−1)
if 1 < ι(i) < N0,
1 −(X0(N0−1) + X0(N0))/2 ≤1 −X0(N0−1)
if ι(i) = N0.
Because the right-hand sides of the inequalities in equation (A.3) all have a Beta distribution with
parameters (2, N0 −1), the moments of Pi are bounded by those of a Beta distribution with parameters
2 and N0−1. The Mth moment of a Beta distribution with parameters α and β is QM−1
j=0 (α+j)/(α+β+j).
This is bounded by (α+M −1)M/(α+β)M, which completes the proof of the second part of the Lemma.
Proof of Lemma 3.1:
First we prove (i). The ﬁrst step is to calculate E[K2
i |Wi = 0]. Using Lemmas A.2 and A.3,
i |Wi = 0] = N1 E[Pi|Wi = 0] + N1 (N1 −1) E[P 2
i |Wi = 0]
N1(N1 −1)(N0 + 8/3)
N0(N0 + 1)(N0 + 2) .
Substituting this into (3.7) we get:
V(ˆτ) = N0
i |Wi = 0] = 1
(N1 −1)(N0 + 8/3)
N1(N0 + 1)(N0 + 2),
proving part (i).
Next, consider part (ii). Multiply the exact variance of ˆτ by N1 and substitute N1 = α N0 to get
N1 V(ˆτ) = 1 + 3
(α N0 −1)(N0 + 8/3)
(N0 + 1)(N0 + 2)
Then take the limit as N0 →∞to get:
N→∞N1 V(ˆτ) = 1 + 3
Finally, consider part (iii). Let S(r, j) be a Stirling number of the second kind. The Mth moment of
Ki given W and X0 is :
i |X0, Wi = 0] =
S(M, j)N0! P j
Therefore, applying Lemma A.3 (ii), we obtain that the moments of Ki are uniformly bounded:
i |Wi = 0]
S(M, j)N0!
(N0 −j)! E[P j
i |Wi = 0] ≤
S(M, j)N0!
S(M, j)(1 + M)j.
Notice that
i |Wi = 0] →1 + 3
i |Wi = 0) →0,
because cov(K2
j |Wi = Wj = 0, i ̸= j) ≤0 . Therefore:
Finally, we write
where ξi = −Ki Yi. Conditional on X and W the ξi are independent, and the distribution of ξi is
degenerate at zero for Wi = 1 and normal N(0, K2
i ) for Wi = 0. Hence, for any c ∈R:
N1(ˆτ −τ) ≤c
where Φ(·) is the cumulative distribution function of a standard normal variable. Integrating over the
distribution of X and W yields:
N1(ˆτ −τ) ≤c
Now, Slustky’s Theorem implies (iii).
Next we introduce some additional notation. Let Rb,i be the number of times unit i is in the bootstrap
sample. In addition, let Db,i be an indicator for inclusion of unit i in the bootstrap sample, so that
Db,i = 1{Rb,i > 0}.
Let Nb,0 = PN
i=1(1 −Wi) Db,i be the number of distinct control units in the
bootstrap sample. Finally, deﬁne the binary indicator Bi(x), for i = 1 . . . , N to be the indicator for the
event that in the bootstrap sample a treated unit with covariate value x would be matched to unit i.
That is, for this indicator to be equal to one the following three conditions need to be satisﬁed: (i) unit
i is a control unit, (ii) unit i is in the bootstrap sample, and (iii) the distance between Xi and x is less
than or equal to the distance between x and any other control unit in the bootstrap sample. Formally:
if |x −Xi| = mink:Wk=0,Db,k=1 |x −Xk|,
and Db,i = 1, Wi = 0,
otherwise.
For the N units in the original sample, let Kb,i be the number of times unit i is used as a match in the
bootstrap sample.
Wj Bi(Xj) Rb,j.
We can write the estimated treatment eﬀect in the bootstrap sample as
Wi Rb,i Yi −Kb,i Yi.
Because Yi(1) = τ by Assumption 3.4, and PN
i=1 WiRb,i = N1, then
ˆτb −τ = −1
The diﬀerence between the original estimate ˆτ and the bootstrap estimate ˆτb is
ˆτb −ˆτ = 1
(Ki −Kb,i) Yi =
(Ki −Kb,i) Yi.
We will calculate the expectation
N1 E[V B,I] = N1 · E[(ˆτb −ˆτ)2] =
(Ki −Kb,i) Yi(Kj −Kb,j) Yj
Using the facts that E[Y 2
i |X, W, Wi = 0] = 1, and E[Yi Yj|X, W, Wi = Wj = 0] = 0 if i ̸= j, this is
N1 E[V B,I] = 1
(Kb,i −Ki)2|Wi = 0
The ﬁrst step in deriving this expectation is to establish some properties of Db,i, Rb,i, Nb,0, and Bi(x).
Lemma A.4: (Properties of Db,i, Rb,i, Nb,0, and Bi(x))
Suppose that Assumptions 3.1-3.3 hold. Then, for w ∈{0, 1}, and n ∈{1, . . . , N0}
Rb,i|Wi = w, Z ∼B(Nw, 1/Nw),
Db,i|Wi = w, Z ∼B
1, 1 −(1 −1/Nw)Nw¢
Pr(Nb,0 = n) =
Pr(Bi(Xj) = 1|Wj = 1, Wi = 0, Db,i = 1, Nb,0) =
(v) for l ̸= j
Pr(Bi(Xl) Bi(Xj) = 1|Wj = Wl = 1, Wi = 0, Db,i = 1, Nb,0) =
2Nb,0(Nb,0 + 1)(Nb,0 + 2),
E[Nb,0/N0] = 1 −(1 −1/N0)N0 →1 −exp(−1),
V(Nb,0) = (N0 −1) (1 −2/N0)N0 +(1 −1/N0)N0 −N0 (1 −1/N0)2N0 →exp(−1)(1−2 exp(−1)).
Proof: Parts (i), (ii), and (iv) are trivial. Part (iii) follows easily from equation (3.6) in page 110 of
Johnson and Kotz . Next, consider part (v). First condition on X0b and Wb (the counterparts of
X0 and W in the b-th bootstrap sample), and suppose that Db,i = 1. The event that a randomly chosen
treated unit will be matched to control unit i conditional on X0b and Wb depends on the diﬀerence in
order statistics of the control units in the bootstrap sample. The equivalent in the original sample is
Pi. The only diﬀerence is that the bootstrap control sample is of size N0,b. The conditional probability
that two randomly chosen treated units are both matched to control unit i is the square of the diﬀerence
in order statistics. It marginal expectation is the equivalent in the bootstrap sample of E[P 2
i |Wi = 0],
again with the sample size scaled back to Nb,0. Parts (vi) and (vii) can be derived by making use of
equation (3.13) on page 114 in Johnson and Kotz .
Next, we prove a general result for the bootstrap. Consider a sample of size N, indexed by i = 1, . . . , N.
Let Db,i be an indicator whether observation i is in bootstrap sample b. Let Nb = PN
i=1 Db,i be the
number of distinct observations in bootstrap sample b.
Lemma A.5: (Bootstrap) For all m ≥0:
1 −exp(−1)
Proof: From parts (vi) and (vii) of Lemma A.4 we obtain that (N −Nb)/N
p→exp(−1). By the
Continuous Mapping Theorem, N/Nb
p→1/(1 −exp(−1)). To obtain convergence of moments it suﬃces
that, for any m ≥0, E[((N −Nb)/N)m] and E[(N/Nb)m] are uniformly bounded in N . For E[((N −Nb)/N)m] uniform boundedness follows from the fact that, for any m ≥0,
((N −Nb)/N)m ≤1. For E[(N/Nb)m] the proof is a little bit more complicated. As before, it is enough
to show that E[(N/Nb)m] is uniformly bounded for N ≥1.
Let λ(N) = (1 −1/N)N.
Notice that
λ(N) < exp(−1) for all N ≥1. Let 0 < θ < exp(1) −1. Notice that, 1 −(1 + θ)λ(N) > 0. Therefore:
1 −(1 + θ)λ(N)
1 −(1 + θ)λ(N)
1 −(1 + θ) exp(−1)
1 −(1 + θ)λ(N)
1 −(1 + θ)λ(N)
1 −(1 + θ) exp(−1)
1 −(1 + θ)λ(N)
Therefore, for the expectation E[(N/Nb)m] to be uniformly bounded, it is suﬃcient that the probability
Pr(N/Nb ≥(1 −(1 + θ)λ(N))−1) converges to zero at an exponential rate as N →∞. Notice that
1 −(1 + θ)λ(N)
N −Nb −Nλ(N) ≥θNλ(N)
|N −Nb −Nλ(N)| ≥θNλ(N)
Theorem 2 in Kamath, Motwani, Palem, and Spirakis implies:
|N −Nb −Nλ(N)| ≥θNλ(N)
−θ2λ(N)2(N −1/2)
Because for N ≥1, λ(N)2/(1−λ(N)2) is increasing in N (converging to (exp(2)−1)−1 > 0 as N →∞),
the last equation establishes an exponential bound on the tail probability of N/Nb.
Lemma A.6: (Approximate Bootstrap K Moments)
Suppose that assumptions 3.1 to 3.3 hold. Then,
b,i | Wi = 0] →2α + 3
(1 −exp(−1)),
E[Kb,i Ki|Wi = 0] →(1 −exp(−1))
+ α2 exp(−1).
Proof: First we prove part (i). Notice that for i, j, l, such that Wi = 0, Wj = Wl = 1
(Rb,j, Rb,l) ⊥⊥Db,i, Bi(Xj), Bi(Xl).
Notice also that {Rb,j : Wj = 1} are exchangeable with:
Rb,j = N1.
Therefore, applying Lemma A.4(i), for Wj = Wl = 1:
cov(Rb,j, Rb,l) = −V(Rb,j)
(N1 −1) = −1 −1/N1
(N1 −1) −→0.
As a result,
E[Rb,j Rb,l | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l]
E[Rb,j | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l]
By Lemma A.4(i),
E[Rb,j | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l] = 1.
Therefore,
E[Rb,j Rb,l | Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wi = 0, Wj = Wl = 1, j ̸= l] −→1.
In addition,
b,j | Db,i = 1, Bi(Xj) = 1, Wj = 1, Wi = 0
= N1(1/N1) + N1(N1 −1)(1/N 2
Notice that
Pr(Db,i = 1|Wi = 0, Wj = Wl = 1, j ̸= l, Nb,0) = Pr(Db,i = 1|Wi = 0, Nb,0) = Nb,0
Using Bayes’ Rule:
Pr(Nb,0 = n|Db,i = 1, Wi = 0, Wj = Wl = 1, j ̸= l) = Pr(Nb,0 = n|Db,i = 1, Wi = 0)
= Pr(Db,i = 1|Wi = 0, Nb,0 = n) Pr(Nb,0 = n)
Pr(Db,i = 1|Wi = 0)
Pr(Nb,0 = n)
1 −(1 −1/N0)N0 .
Therefore,
N0 Pr(Bi(Xj) = 1|Db,i = 1, Wi = 0, Wj = 1)
Pr(Bi(Xj) = 1|Db,i = 1, Wi = 0, Wj = 1, Nb,0 = n)
× Pr(Nb,0 = n|Db,i = 1, Wi = 0, Wj = 1)
Pr(Nb,0 = n)
1 −(1 −1/N0)N0 =
1 −(1 −1/N0)N0 −→
1 −exp(−1).
In addition,
0 Pr(Bi(Xj)Bi(Xl)|Db,i = 1, Wi = 0, Wj = Wl = 1, j ̸= l, Nb,0)
0 (Nb,0 + 8/3)
Nb,0(Nb,0 + 1)(Nb,0 + 2)
1 −exp(−1)
0 Pr(Bi(Xj)Bi(Xl)|Db,i = 1, Wi = 0, Wj = Wl = 1, j ̸= l, Nb,0)
× Pr(Nb,0 = n|Db,i = 1, Wi = 0, Wj = Wl = 1, j ̸= l)
0 (n + 8/3)
n(n + 1)(n + 2)
Pr(Nb,0 = n)
1 −(1 −1/N0)N0
1 −exp(−1)
0 (n + 8/3)2
Pr(Nb,0 = n).
Notice that
0 (n + 8/3)2
Pr(Nb,0 = n) ≤
Pr(Nb,0 = n),
which is bounded away from inﬁnity (as shown in the proof of Lemma A.5). Convergence in probability
of a random variable along with boundedness of its second moment implies convergence of the ﬁrst
moment . As a result,
0 Pr(Bi(Xj)Bi(Xl)|Db,i = 1, Wi = 0, Wj = Wl = 1, j ̸= l) −→3
1 −exp(−1)
Then, using these preliminary results, we obtain:
b,i | Wi = 0]
WjWl Bi(Xj) Bi(Xl) Rb,j Rb,l
¯¯¯ Wi = 0
Wj Bi(Xj) R2
¯¯¯ Wi = 0
Wj Wl Bi(Xj) Bi(Xl) Rb,j Rb,l
¯¯¯ Wi = 0
b,j | Db,i = 1, Bi(Xj) = 1, Wj = 1, Wi = 0
× Pr (Bi(Xj) = 1 | Db,i = 1, Wj = 1, Wi = 0) Pr(Db,i = 1 | Wj = 1, Wi = 0)
N1(N1 −1)E [Rb,j Rb,l| Db,i = 1, Bi(Xj) = Bi(Xl) = 1, Wj = Wl = 1, j ̸= l, Wi = 0]
× Pr (Bi(Xj)Bi(Xl) = 1 | Db,i = 1, Wj = Wl = 1, j ̸= l, Wi = 0)
× Pr(Db,i = 1 | Wj = Wl = 1, j ̸= l, Wi = 0)
(1 −exp(−1)).
This ﬁnishes the proof of part (i). Next, we prove part (ii).
E[KiKb,i|X0, W, Db,i = 1, Wi = 0]
Wj1{Mj = i}
WlBi(Xl)Rb,l
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}Bi(Xl)Rb,l
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml = i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml = i}
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
Conditional on X0, Wi = 0, and Db,i = 1, the probability that a treated observation, l, that was not
matched to i in the original sample, is matched to i in a bootstrap sample does not depend on the
covariate values of the other treated observations (or on W). Therefore:
E[Bi(Xl)|X0, W, Db,i = 1, Wi = 0, Wj = Wl = 1, Mj = i, Ml ̸= i]
E[Bi(Xl)|X0, Db,i = 1, Wi = 0, Wl = 1, Ml ̸= i].
As a result:
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ X0, W, Db,i = 1, Wi = 0
WjWl1{Mj = i}1{Ml ̸= i}
¯¯¯ X0, W, Db,i = 1, Wi = 0
Ki(N1 −Ki)
¯¯¯ X0, W, Db,i = 1, Wi = 0
Ki(N1 −Ki)
¯¯¯ X0, Wi = 0
Conditional on X0 and Wi = 0, Ki has a Binomial distribution with parameters (N1, Pi). Therefore:
E[Ki(N1 −Ki) | X0, Wi = 0]
1 Pi −N1Pi −N1(N1 −1)P 2
N1(N1 −1)Pi(1 −Pi).
Therefore:
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ ι(i), Pi, Db,i = 1, Wi = 0
i | ι(i), Pi, Db,i = 1, Wi = 0]N1(N1 −1)Pi(1 −Pi).
In addition, the probability that r speciﬁed observations do not appear in a bootstrap sample conditional
on that another speciﬁed observation appears in the sample is (apply Bayes’ theorem):
1{r ≤N0 −1}
Notice that for a ﬁxed r this probability converges to exp(−r), as N0 →∞.
Notice also that this
probability is bounded by exp(−r)/(1 −exp(−1)), which is integrable:
1 −exp(−1) =
(1 −exp(−1))2 .
As a result, by the dominated convergence theorem for inﬁnite sums:
1{r ≤N0 −1}
1 −exp(−1).
For k, d ∈{1, . . . , N0 −1} and k + d ≤N0, let ∆d(k) = X0(k+d) −X0(k). In addition, let ∆d(0) = X0(d),
and for k + d = N0 + 1 let ∆d(k) = 1 −X0(k). Notice that:
µ∆1(ι(i)+r)
+ 1{r = N0 −ι(i)}∆N0−ι(i)+1(ι(i))
µ∆1(ι(i)−1−r)
+ 1{r = ι(i) −1}∆ι(i)(0)
In addition, using the results in Lemma A.1 we obtain that, for 1 < ι(i) < N0, and 1 ≤r ≤N0 −ι(i),
∆1(ι(i)+r)Pi | ι(i), Db,i = 1, Wi = 0
(N0 + 1)(N0 + 2),
∆N0−ι(i)+1(ι(i))Pi | ι(i), Db,i = 1, Wi = 0
= (N0 −ι(i)) + 3/2
(N0 + 1)(N0 + 2).
For 1 < ι(i) < N0, and 1 ≤r ≤ι(i) −1, we have:
∆1(ι(i)−1−r)Pi | ι(i), Db,i = 1, Wi = 0
(N0 + 1)(N0 + 2),
∆ι(i)(0)Pi | ι(i), Db,i = 1, Wi = 0
(ι(i) −1) + 3/2
(N0 + 1)(N0 + 2).
Therefore, for 1 < ι(i) < N0:
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ ι(i), Db,i = 1, Wi = 0
2(N0 + 1)(N0 + 2)
1 + 1{r = N0 −ι(i)}(N0 −ι(i) + 3/2)
1 + 1{r = ι(i) −1}(ι(i) + 1/2)
For ι(i) = 1 and 1 ≤r ≤N0 −1, we obtain:
∆1(1+r)Pi | ι(i) = 1, Db,i = 1, Wi = 0
(N0 + 1)(N0 + 2),
∆N0(1)Pi | ι(i) = 1, Db,i = 1, Wi = 0
(N0 + 1)(N0 + 2),
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ ι(i) = 1, Db,i = 1, Wi = 0
3N1(N1 −1)
4(N0 + 1)(N0 + 2)
1 + 1{r = N0 −1}(N0 + 1/3)
with analogous results for the case ι(i) = N0. Let
T(N0, N1, n) = E
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ ι(i) = n, Db,i = 1, Wi = 0
T(N0, N1, n) =
(N0 + 1)(N0 + 2)
RN0(n) + UN0(n)
RN0(n) = 1
UN0(n) = 1
(N0 −n + 3/2)
+ (n −1 + 3/2)
for 1 < n < N0. For n = 1:
RN0(1) = 3
UN0(1) = 3
4(N0 + 1/3)
with analogous expressions for n = N0.
Let T = α2 exp(−1)/(1 −exp(−1)). Then,
T −T(N0, N1, n)
1 −exp(−1) −RN0(n) −UN0(n)
(N0 + 1)(N0 + 2)
RN0(n) + UN0(n)
Notice that, for 0 < n < N0,
1 −exp(−1)
1 −exp(−1)
(1 −exp(−1))2 .
Notice also that, because log λ ≤λ −1 for any λ > 0, we have that for all n such that 0 < n < N0,
log(n(1 −n/N0)N0) = log(n) + N0 log(1 −n/N0) ≤log(n) −n ≤−1, and therefore n(1 −n/N0)N0 ≤
exp(−1). This implies that the quantities (N0−n+3/2)(1−(N0−n)/N0)N0 and (n+1/2)(1−(n−1)/N0)N0
are bounded by exp(−1) + 3/2. Therefore, for 0 < n < N0:
UN0(n) ≤exp(−1) + 3/2
1 −exp(−1) .
Similarly, for n ∈{1, N0}, we obtain
(1 −exp(−1))2 ,
exp(−1) + 4/3
1 −exp(−1) .
Consequently, RN0(n) and UN0(n) are uniformly bounded by some constants ¯R and ¯U for all N0, n ∈N
(the set of positive integers) with n ≤N0. Therefore, for all N0, n ∈N with n ≤N0
|T −T(N0, N1, n)| ≤α2
1 −exp(−1) + ¯R + ¯U
(N0 + 1)(N0 + 2)
( ¯R + ¯U).
Because every convergent sequence in R is bounded, (N1(N1 −1))/((N0 + 1)(N0 + 2)) is bounded by
some constant ¯α2. As a result, there exist some constant ¯D such that |T −T(N0, N1, n)| ≤¯D.
Notice that for all n such that 0 < n < N0, RN0(n) = R∗
N0(n) + VN0(n), where
VN0(n) = 1
Notice that for all n such that 0 < n < N0, 0 ≤VN0(n) ≤¯VN0, where
1{r ≤N0 −1}
which does not depend on n. Applying the dominated convergence theorem, it is easy to show that
¯VN0 →0, as N0 →∞.
For some δ such that 0 < δ < 1, let Iδ,N0 = {n ∈N : 1 + N δ
0 < n < N0 −N δ
0 } and ¯Iδ,N0 = {n ∈N : n ≤
0 } ∪{n ∈N : N0 −N δ
0 ≤n ≤N0}.
Notice that
1 −exp(−1).
For n ∈Iδ,N0, N0 −n > N δ
0 and n −1 > N δ
0. Therefore,
1 −exp(−1) −
It follows that
1 −exp(−1) −R∗
¯¯¯¯ < ¯DN0.
Notice that ¯DN0 does not depend on n. Also, applying the dominated convergence theorem, it is easy
to show that ¯DN0 →0, as N0 →∞.
In addition, for n ∈IN0,δ, it has to be the case that n ≥2 and n ≤N0 −1. As a result,
(N0 −n) + 3/2
< N0 exp(−N δ
(n −1) + 3/2
< N0 exp(−N δ
Therefore, for n ∈IN0,δ, UN0(n) < ¯UN0, where
1 −exp(−1) N0 exp(−N δ
Notice that ¯UN0 →0, as N0 →∞.
The last set of results imply that for n ∈Iδ,N0,
|T −T(N0, N1, n)|
α2 ¡ ¯DN0 + ¯VN0 + ¯UN0
(N0 + 1)(N0 + 2)
¯¯¯¯ ( ¯R + ¯U).
Let #Iδ,N0 and #¯Iδ,N0 be the cardinalities of the sets Iδ,N0 and ¯Iδ,N0 respectively. Notice that #Iδ,N0 <
N0, #Iδ,N0/N0 →1 and #¯Iδ,N0/N0 →0.
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ Db,i = 1, Wi = 0
|T −T(N0, N1, n)| 1{1 ≤n ≤N0}/N0
|T −T(N0, N1, n)| 1{n ∈Iδ,N0}/N0
|T −T(N0, N1, n)| 1{n ∈¯Iδ,N0}/N0.
Using the bounds established above and the fact that #Iδ,N0 < N0, we obtain:
|T −T(N0, N1, n)| 1{n ∈Iδ,N0}/N0
α2 ¡ ¯DN0 + ¯VN0 + ¯UN0
1{n ∈Iδ,N0}/N0
(N0 + 1)(N0 + 2)
¯¯¯¯ ( ¯R + ¯U)
1{n ∈Iδ,N0}/N0
α2 ¡ ¯DN0 + ¯VN0 + ¯UN0
(N0 + 1)(N0 + 2)
¯¯¯¯ ( ¯R + ¯U) −→0.
Notice also that:
|T −T(N0, N1, n)| 1{n ∈¯Iδ,N0}/N0 ≤¯D 1
1{n ∈¯Iδ,N0} = ¯D #¯Iδ,N0
As a result, we obtain:
WjWl1{Mj = i}1{Ml ̸= i}Bi(Xl)
¯¯¯ Db,i = 1, Wi = 0
1 −exp(−1).
Now, because
i | Db,i = 1, Wi = 0] = E[K2
i | Wi = 0] →α + 3
E[KiKb,i | Db,i = 1, Wi = 0] →α + 3
1 −exp(−1).
Therefore, because E[KiKb,i | Db,i = 0, Wi = 0] = 0, we obtain
E[KiKb,i | Wi = 0] →
1 −exp(−1)
(1 −exp(−1)).
Proof of Lemma 3.2: From previous results:
N1E[V B,I]
b,i|Wi = 0] −2 E[Kb,i Ki|Wi = 0] + E[K2
i |Wi = 0]
(1 −exp(−1)) −2(1 −exp(−1))
1 −exp(−1) α2
2(1 −exp(−1) −3(1 −exp(−1)) −2 exp(−1) + 3
+ 2 −2 + 2 exp(−1) + 1
2 α 5 exp(−1) −2 exp(−2)
3(1 −exp(−1))
+ 2 exp(−1).