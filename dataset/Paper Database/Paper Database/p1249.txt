An overview and comparative analysis of Recurrent Neural Networks for
Short Term Load Forecasting
Filippo Maria Bianchi1a, Enrico Maiorinob, Michael C. Kampﬀmeyera, Antonello Rizzib, Robert Jenssena
aMachine Learning Group, Dept. of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway
bDept. of Information Engineering, Electronics and Telecommunications, Sapienza University, Rome, Italy
The key component in forecasting demand and consumption of resources in a supply network is an accurate
prediction of real-valued time series. Indeed, both service interruptions and resource waste can be reduced
with the implementation of an eﬀective forecasting system. Signiﬁcant research has thus been devoted to
the design and development of methodologies for short term load forecasting over the past decades. A class
of mathematical models, called Recurrent Neural Networks, are nowadays gaining renewed interest among
researchers and they are replacing many practical implementation of the forecasting systems, previously based
on static methods. Despite the undeniable expressive power of these architectures, their recurrent nature
complicates their understanding and poses challenges in the training procedures. Recently, new important
families of recurrent architectures have emerged and their applicability in the context of load forecasting
has not been investigated completely yet. In this paper we perform a comparative study on the problem
of Short-Term Load Forecast, by using diﬀerent classes of state-of-the-art Recurrent Neural Networks. We
test the reviewed models ﬁrst on controlled synthetic tasks and then on diﬀerent real datasets, covering
important practical cases of study. We provide a general overview of the most important architectures and
we deﬁne guidelines for conﬁguring the recurrent networks to predict real-valued time series.
Short Term Load Forecast, Recurrent Neural Networks, Time Series Prediction, Echo State
Networks, Long Short Term Memory, Gated Recurrent Units, NARX Networks.
1. Introduction
Forecasting the demand of resources within a distribution network of energy, telecommunication or transportation is of fundamental importance for managing the limited availability of the assets. An accurate
Short Term Load Forecast (STLF) system can reduce high cost of over- and under-contracts on balancing
markets due to load prediction errors. Moreover, it keeps power markets eﬃcient and provides a better
understanding of the dynamics of the monitored system . On the other hand, a wrong prediction could
cause either a load overestimation, which leads to the excess of supply and consequently more costs and
contract curtailments for market participants, or a load underestimation, resulting in failures in gathering
enough provisions, thereby more costly supplementary services . These reasons motivated the research
of forecasting models capable of reducing this ﬁnancial distress, by increasing the load forecasting accuracy
even by a small percent .
The load proﬁle generally follows cyclic and seasonal patterns related to human activities and can be
represented by a real-valued time series. The dynamics of the system generating the load time series can vary
signiﬁcantly during the observation period, depending on the nature of the system and on latent, external
inﬂuences. For this reason, the forecasting accuracy can change considerably among diﬀerent samples even
when using the same prediction model . Over the past years, the STLF problem has been tackled in
several research areas by means of many diﬀerent model-based approaches, each one characterized by
diﬀerent advantages and drawbacks in terms of prediction accuracy, complexity in training, sensitivity to
the parameters and limitations in the tractable forecasting horizon .
 
Autoregressive and exponential smoothing models represented for many years the baseline among systems for time series prediction . Such models require to properly select the lagged inputs to identify the
correct model orders, a procedure which demands a certain amount of skill and expertise . Moreover,
autoregressive models make explicit assumptions about the nature of system under exam. Therefore, their
use is limited to those settings in which such assumptions hold and where a-priori knowledge on the system
is available . Taylor showed that for long forecasting horizons a very basic averaging model, like
AutoRegressive Integrated Moving Average or Triple Exponential Smoothing, can outperform more sophisticated alternatives. However, in many complicated systems the properties of linearity and even stationarity
of the analyzed time series are not guaranteed. Nonetheless, given their simplicity, autoregressive models
have been largely employed as practical implementations of forecast systems.
The problem of time series prediction has been approached within a function approximation framework,
by relying on the embedding procedure proposed by Takens . Takens’ theorem transforms the prediction
problem from time extrapolation to phase space interpolation. In particular, by properly sampling a time
dependent quantity s(t), it is possible to predict the value of the k-th sample from the previous samples, given
an appropriate choice of the sampling frequency τ and the number of samples m: s[k] = f(s[k −τ], . . . , s[k −
m · τ]). Through the application of phase-space embedding, regression methods, such as Support Vector
Regression (an extension of Support Vector Machines in the continuum) have been applied in time series
prediction , either by representing the sequential input as a static domain, described by frequency and
phase, or by embedding sequential input values in time windows of ﬁxed length. The approach can only
succeed if there are no critical temporal dependencies exceeding the windows length, making the SVM unable
to learn an internal state representation for sequence learning tasks involving time lags of arbitrary length.
Other universal function approximators such as Feed-Forward Artiﬁcial Neural Networks and ANFIS
(Adaptive Network-Based Fuzzy Inference System) have been employed in time series prediction tasks
by selecting a suitable interval of past values from the time series as the inputs and by training the network
to forecast one or a ﬁxed number of future values . The operation is repeated
to forecast next values by translating the time window of the considered inputs . While this approach
proved to be eﬀective in many circumstances , it does not treat temporal ordering as an explicit
feature of the time series and, in general, is not suitable in cases where the time series have signiﬁcantly
diﬀerent lengths. On this account, a Recurrent Neural Network (RNN) is a more ﬂexible model, since it
encodes the temporal context in its feedback connections, which are capable of capturing the time varying
dynamics of the underlying system .
RNNs are a special class of Neural Networks characterized by internal self-connections, which can, in
principle, any nonlinear dynamical system, up to a given degree of accuracy . RNNs and their variants
have been used in many contexts where the temporal dependency in the data is an important implicit feature
in the model design. Noteworthy applications of RNNs include sequence transduction , language modeling
 , speech recognition , learning word embeddings , audio modeling , handwriting
recognition , and image generation .
In many of these works a popular variant of RNN was
used, called Long-Short Term Memory . This latter has recently earned signiﬁcant attention due to its
capability of storing information for very long periods of time.
As an RNN processes sequential information, it performs the same operations on every element of the
input sequence. Its output, at each time step, depends on previous inputs and past computations. This
allows the network to develop a memory of previous events, which is implicitly encoded in its hidden state
variables. This is certainly diﬀerent from traditional feedforward neural networks, where it is assumed that
all inputs (and outputs) are independent of each other. Theoretically, RNNs can remember arbitrarily long
sequences. However, their memory is in practice limited by their ﬁnite size and, more critically, by the
suboptimal training of their parameters. To overcome memory limitations, recent research eﬀorts have led
to the design of novel RNN architectures, which are equipped with an external, permanent memory capable
of storing information for indeﬁnitely long amount of time .
Contrarily to other linear models adopted for prediction, RNNs can learn functions of arbitrary complexity
and they can deal with time series data possessing properties such as saturation or exponential eﬀects and
nonlinear interactions between latent variables. However, if the temporal dependencies of data are prevalently
contained in a ﬁnite and small time interval, the use of RNNs can be unnecessary. In these cases performances,
both in terms of computational resources required and accuracy, are generally lower than the ones of timewindow approaches, like ARIMA, SVM, Multi-Layer Perceptron and ANFIS. On the other hand, in many
load forecasting problems the time series to be predicted are characterized by long temporal dependencies,
whose extent may vary in time or be unknown in advance. In all these situations, the use of RNNs may turn
out to be the best solution.
Despite the STLF problem has been one of the most important applications for both early RNNs models
 and most recent ones , an up-to-date and comprehensive analysis of the modern RNN architectures
applied to the STLF problem is still lacking. In several recent works on STFL, NARX networks (see Sec.
4.1) or Echo State Networks (see Sec. 4.2) are adopted for time series prediction and their performance
is usually compared with standard static models, rather than with other RNN architectures. With this
paper, we aim to ﬁll these gaps by performing a comparative study on the problem of STLF using diﬀerent
classes of state-of-the-art RNNs. We provide an introduction to the RNN framework, presenting the most
important architectures and their properties. We also furnish the guidelines for conﬁguring and training the
diﬀerent RNN models to predict real-valued time series. In practice, we formulate the STLF problem as the
prediction of a real-valued univariate time series, given its past values as input. In some cases, beside the
time series of past target values, additional “context” time series are fed to the network in order to provide
exogenous information related to the environment in which the system to be modeled operates.
The paper is structured as follows.
In Sec. 2 we provide a general overview of a standard RNN architecture and we discuss its general properties. We also discuss the main issues encountered in the training phase, the most common methodologies
for learning the model parameters and common ways of deﬁning the loss function to be optimized during
the training.
In Sec. 3, we present the most basic architecture, called Elman RNN, and then we analyze two important
variants, namely the Long-Short Term Memory and Gated Recurrent Units networks. Despite the recent
popularity of these architectures , their application to prediction of real-valued time series has been
limited so far . For each RNN, we provide a brief review, explaining its main features, the approaches
followed in the training stage and a short list of the main works concerning time series prediction in which
the speciﬁc network has been applied.
Successively, in Sec. 4 we illustrate two particular RNN architectures, which diﬀer from the previous
ones, mainly due to their training procedure. In particular, we analyze the Nonlinear AutoRegressive with
eXogenous inputs (NARX) neural network and the Echo State Network (ESN). These architectures have
been successfully applied in the literature of time series prediction and they provide important advantages
with respect to traditional models, due to their easy applicability and fast training procedures.
In Sec. 5 we describe three synthetic datasets, used to test and to compare the computational capabilities
of the ﬁve RNN architectures in a controlled environment.
6, we present three real-world datasets of time series relative to the load proﬁle in energy
distribution and telecommunication networks. For each dataset, we perform a series of analysis with the
purpose of choosing a suitable preprocessing for the data.
Sec. 7 is dedicated to the experiments and to the discussion of the performance of the RNN models. The
ﬁrst part of the experimental section focuses on the benchmark tests, while in the second part we employ
the RNNs to solve STLF tasks on real-world time series.
Finally, in Sec. 8 we discuss our conclusions.
2. Properties and Training in Recurrent Neural Networks
RNNs are learning machines that recursively compute new states by applying transfer functions to previous states and inputs. Typical transfer functions are composed by an aﬃne transformation followed by a
nonlinear function, which are chosen depending on the nature of the particular problem at hand. It has been
shown by Maass et al. that RNNs possess the so-called universal approximation property, that is, they
are capable of approximating arbitrary nonlinear dynamical systems (under loose regularity conditions) with
arbitrary precision, by realizing complex mappings from input sequences to output sequences . However,
the particular architecture of an RNN determines how information ﬂows between diﬀerent neurons and its
correct design is crucial in the realization of a robust learning system. In the context of prediction, an RNN
is trained on input temporal data x(t) in order to reproduce a desired temporal output y(t). y(t) can be
any time series related to the input and even a temporal shift of x(t) itself. The most common training
procedures are gradient-based, but other techniques have been proposed, based on derivative-free approaches
or convex optimization . The objective function to be minimized is a loss function, which depends
on the error between the estimated output ˆy(t) and the actual output of the network y(t). An interesting
aspect of RNNs is that, upon suitable training, they can also be executed in generative mode, as they are
capable of reproducing temporal patterns similar to those they have been trained on .
The architecture of a simple RNN is depicted in Fig. 1. In its most general form an RNN can be seen as
Input layer
Hidden layer
Output layer
Figure 1: Schematic depiction of a simple
RNN architecture. The circles represent
input x, hidden, h, and output nodes, y,
respectively. The solid squares Wh
h are the matrices which represent input, hidden and output weights
respectively. Their values are commonly
tuned in the training phase through gradient descent. The polygon represents the
non-linear transformation performed by
neurons and z-1 is the unit delay operator.
a weighted, directed and cyclic graph that contains three diﬀerent kinds of nodes, namely the input, hidden
and output nodes . Input nodes do not have incoming connections, output nodes do not have outgoing
connections, hidden nodes have both. An edge can connect two diﬀerent nodes which are at the same or
at diﬀerent time instants. In this paper, we adopt the time-shift operator zn to represent a time delay of n
time steps between a source and a destination node. Usually n = −1, but also lower values are admitted and
they represent the so called skip connections . Self-connecting edges always implement a lag operator
with |n| ≥1. In some particular cases, the argument of the time-shift operator is positive and it represents a
forward-shift in time . This means that a node receives as input the content of a source node in a future
time interval. Networks with those kind of connections are called bidirectional RNNs and are based on the
idea that the output at a given time may not only depend on the previous elements in the sequence, but also
on future ones . These architectures, however, are not reviewed in this work as we only focus on RNNs
with n = −1.
While, in theory, an RNN architecture can model any given dynamical system, practical problems arise
during the training procedure, when model parameters must be learned from data in order to solve a target
task. Part of the diﬃculty is due to a lack of well established methodologies for training diﬀerent types of
models. This is also because a general theory that might guide designer decisions has lagged behind the
feverish pace of novel architecture designs . A large variety of novel strategies and heuristics have
arisen from the literature in the past the years and, in many cases, they may require a considerable
amount of expertise from the user to be correctly applied. While the standard learning procedure is based
on gradient optimization, in some RNN architectures the weights are trained following diﬀerent approaches
 , such as real-time recurrent learning , extended Kalman ﬁlters or evolutionary algorithms
 , and in some cases they are not learned at all .
Figure 2: The diagram depicts the RNN from Fig. 1, being unfolded (or unrolled) into a FFNN. As we can see from the image,
each input xt and output yt are relative to diﬀerent time intervals. Unlike a traditional deep FFNN, which uses diﬀerent
parameters in each layer, an unfolded RNN shares the same weights across every time step. In fact, the input weights matrix
i , the hidden weights matrix Wh
h and the output weights matrix Wo
h are constrained to keep the same values in each time
2.1. Backpropagation Through Time
Gradient-based learning requires a closed-form relation between the model parameters and the loss function. This relation allows to propagate the gradient information calculated on the loss function back to the
model parameters, in order to modify them accordingly. While this operation is straightforward in models
represented by a directed acyclic graph, such as a FeedForward Neural Network (FFNN), some caution must
be taken when this reasoning is applied to RNNs, whose corresponding graph is cyclic. Indeed, in order to
ﬁnd a direct relation between the loss function and the network weights, the RNN has to be represented as an
equivalent inﬁnite, acyclic and directed graph. The procedure is called unfolding and consists in replicating
the network’s hidden layer structure for each time interval, obtaining a particular kind of FFNN. The key
diﬀerence of an unfolded RNN with respect to a standard FFNN is that the weight matrices are constrained
to assume the same values in all replicas of the layers, since they represent the recursive application of the
same operation.
Fig. 2 depicts the unfolding of the RNN, previously reported in Fig. 1. Through this transformation the
network can be trained with standard learning algorithms, originally conceived for feedforward architectures.
This learning procedure is called Back Propagation Through Time (BPTT) and is one of the most
successful techniques adopted for training RNNs. However, while the network structure could in principle
be replicated an inﬁnite number of times, in practice the unfolding is always truncated after a ﬁnite number
of time instants. This maintains the complexity (depth) of the network treatable and limits the issue of the
vanishing gradient (as discussed later). In this learning procedure called Truncated BPPT , the folded
architecture is repeated up to a given number of steps τb, with τb upperbounded by the time series length T.
The size of the truncation depends on the available computational resources, as the network grows deeper by
repeating the unfolding, and on the expected maximum extent of time dependencies in data. For example,
in a periodic time series with period t it may be unnecessary, or even detrimental, to set τb > t.
Another variable we consider is the frequency τf at which the BPTT calculates the backpropagated
gradients. In particular, let us deﬁne with BPTT(τb, τf) the truncated backpropagation that processes the
sequence one time step at a time, and every τf time steps, it runs BPTT for τb time steps . Very often
the term τf is omitted in the literature, as it is assumed equal to 1, and only the value for τb is speciﬁed.
We refer to the case τf = 1 and τb = n as true BPTT, or BPTT(n, 1).
In order to improve the computational eﬃciency of the BPTT, the ratio τb/τf can be decremented,
eﬀectively reducing the frequency of gradients evaluation. An example, is the so-called epochwise BPTT or
BPTT(n, n), where τb = τf . In this case, the ratio τb/τf = 1. However, the learning procedure is in
general much less accurate than BPTT(n, 1), since the gradient is truncated too early for many values on
the boundary of the backpropagation window.
A better approximation of the true BPTT is reached by taking a large diﬀerence τb −τf, since no
error in the gradient is injected for the earliest τb −τf time steps in the buﬀer. A good trade-oﬀbetween
accuracy and performance is BPTT(2n, n), which keeps the ratio τb/τf = 2 suﬃciently close to 1 and the
diﬀerence τb −τf = n is large as in the true BPTT . Through preliminary experiments, we observed
that BPTT(2n, n) achieves comparable performance to BPTT(n, 1), in a signiﬁcantly reduced training time.
Therefore, we followed this procedure in all our experiments.
2.2. Gradient descent and loss function
Training a neural network commonly consists in modifying its parameters through a gradient descent
optimization, which minimizes a given loss function that quantiﬁes the accuracy of the network in performing
the desired task. The gradient descent procedure consists in repeating two basic steps until convergence is
reached. First, the loss function Lk is evaluated on the RNN conﬁgured with weights Wk, when a set of
input data Xk are processed (forward pass). Note that with Wk we refer to all network parameters, while
the index k identiﬁes their values at epoch k, as they are updated during the optimization procedure. In
the second step, the gradient ∂Lk/∂Wk is back-propagated through the network in order to update its
parameters (backward pass).
In a time series prediction problem, the loss function evaluates the dissimilarity between the predicted
values and the actual future values of the time series, which is the ground truth. The loss function can be
Lk = E (Xk, Y∗
k; Wk) + Rλ (Wk) ,
where E is a function that evaluates the prediction error of the network when it is fed with inputs in Xk,
in respect to a desired response Y∗
k. Rλ is a regularization function that depends on a hyperparameter λ,
which weights the contribution of the regularization in the total loss.
The error function E that we adopt in this work is Mean Square Error (MSE). It is deﬁned as
MSE(Yk, Y∗
where yx ∈Yk is the output of the RNN (conﬁgured with parameters Wk) when the input x ∈Xk is
processed and y∗
k is the ground-truth value that the network must learn to reproduce.
The regularization term Rλ introduces a bias that improves the generalization capabilities of the RNN,
by reducing overﬁtting on the training data. In this work, we consider four types of regularization:
1. L1: the regularization term in Eq. 1 has the form Rλ(Wk) = λ1∥Wk∥1. L1 regularization enforces
sparsity in the network parameters, is robust to noisy outliers and it can possibly deliver multiple
optimal solutions. However, this regularization can produce unstable results, in the sense that a small
variation in the training data can yield very diﬀerent outcomes.
2. L2: in this case, Rλ(Wk) = λ2∥Wk∥2. This function penalizes large magnitudes in the parameters,
favouring dense weight matrices with low values. This procedure is more sensitive to outliers, but is
more stable than L1. Usually, if one is not concerned with explicit features selection, the use of L2 is
preferred.
3. Elastic net penalty: combines the two regularizations above, by joining both L1 and L2 terms as
Rλ(Wk) = λ1 ∥Wk∥1 + λ2∥Wk∥2. This regularization method overcomes the shortcomings of the L1
regularization, which selects a limited number of variables before it saturates and, in case of highly
correlated variables, tends to pick only one and ignore the others. Elastic net penalty generalizes the
L1 and L2 regularization, which can be obtained by setting λ2 = 0 and λ1 = 0, respectively.
4. Dropout: rather than deﬁning an explicit regularization function Rλ(·), dropout is implemented by
keeping a neuron active during each forward pass in the training phase with some probability. Specifically, one applies a randomly generated mask to the output of the neurons in the hidden layer. The
probability of each mask element to be 0 or 1 is deﬁned by a hyperparameter pdrop. Once the training is
over, the activations are scaled by pdrop in order to maintain the same expected output. Contrarily to
feedforward architectures, a naive dropout in recurrent layers generally produces bad performance and,
therefore, it has usually been applied only to input and output layers of the RNN . However, in a
recent work, Gal and Ghahramani shown that this shortcoming can be circumvented by dropping
the same network units in each epoch of the gradient descent. Even if this formulation yields a slightly
reduced regularization, nowadays this approach is becoming popular and is the one followed
in this paper.
Beside the ones discussed above, several other kinds of regularization procedures have been proposed in
the literature. Examples are the stochastic noise injection and the max-norm constraint , which,
however, are not considered in our experiments.
2.3. Parameters update strategies
Rather than evaluating the loss function over the entire training set to perform a single update of the
network parameters, a very common approach consists in computing the gradient over mini-batches Xk of
the training data. The size of the batch is usually set by following rules of thumb .
This gradient-update method is called Stochastic Gradient Descent (SGD) and, in presence of a nonconvex function, its convergence to a local minimum is guaranteed (under some mild assumptions) if the
learning rate is suﬃciently small . The update equation reads
Wk+1 = Wk + η∇Lk(Wk),
where η is the learning rate, an important hyperparameter that must be carefully tuned to achieve an eﬀective
training . In fact, a large learning rate provides a high amount of kinetic energy in the gradient descent,
which causes the parameter vector to bounce, preventing the access to narrow area of the search space, where
the loss function is lower. On the other hand, a strong decay can excessively slow the training procedure,
resulting in a waste of computational time.
Several solutions have been proposed over the years, to improve the convergence to the optimal solution
 . During the training phase it is usually helpful to anneal η over time or when the performance stops
increasing. A method called step decay reduces the learning rate by a factor α, if after a given number of
epochs the loss has not decreased. The exponential decay and the fractional decay instead, have mathematical
forms η = η0e−αk and η =
(1+αk), respectively. Here α and η0 are hyperparameters, while k is the current
optimization epoch. In our experiments, we opted for the step decay annealing, when we train the networks
Even if SGD usually represents a safe optimization procedure, its rate of convergence is slow and the
gradient descent is likely to get stuck in a saddle point of the loss function landscape . Those issues
have been addressed by several alternative strategies proposed in the literature for updating the network
parameters. In the following we describe the most commonly used ones.
Momentum. In this ﬁrst-order method, the weights Wk are updated according to a linear combination of
the current gradient ∇Lk(Wk) and the previous update Vk−1, which is scaled by a hyperparameter µ:
Vk = µVk−1 −η∇Lk(Wk),
Wk+1 = Wk + Vk.
With this approach, the updates will build up velocity toward a direction that shows a consistent gradient
 . A common choice is to set µ = 0.9.
A variant of the original formulation is the Nesterov momentum, which often achieves a better convergence
rate, especially for smoother loss functions .
Contrarily to the original momentum, the gradient is
evaluated at an approximated future location, rather than at the current position. The update equations
Vk = µVk−1 −η∇Lk(Wk + µVk−1),
Wk+1 = Wk + Vk.
Adaptive learning rate. The ﬁrst adaptive learning rate method, proposed by Duchi et al. , is Adagrad.
Unlike the previously discussed approaches, Adagrad maintains a diﬀerent learning rate for each parameter.
Given the update information from all previous iterations ∇Lk (Wj), with j ∈{0, 1, · · · , k}, a diﬀerent
update is speciﬁed for each parameter i of the weight matrix:
k+1 = W(i)
where ϵ is a small term used to avoid division by 0. A major drawback with Adagrad is the unconstrained
growth of the accumulated gradients over time. This can cause diminishing learning rates that may stop the
gradient descent prematurely.
A procedure called RMSprop attempts to solve this issue by using an exponential decaying average
of square gradients, which discourages an excessive shrinkage of the learning rates:
(1 −δ) · v(i)
k−1 + δ∇Lk
(1 −δ) · v(i)
k+1 = W(i)
According to the update formula, if there are oscillation in gradient updates, the learning rate is reduced by
1 −δ, otherwise it is increased by δ. Usually the decay rate is set to δ = 0.01.
Another approach called Adam and proposed by Kingma and Ba , combines the principles of Adagrad
and momentum update strategies. Usually, Adam is the adaptive learning method that yields better results
and, therefore, is the gradient descent strategy most used in practice.
Like RMSprop, Adam stores an
exponentially decaying average of gradients squared, but it also keeps an exponentially decaying average of
the moments of the gradients. The update diﬀerence equations of Adam are
mk = β1mk−1 + (1 −β1)∇Lk
vk = β2vk−1 + (1 −β2)∇Lk
Wk+1 = Wk +
√ˆvk + ϵ ˆmk .
m corresponds to the ﬁrst moment and v is the second moment. However, since both m and v are initialized
as zero-vectors, they are biased towards 0 during the ﬁrst epochs. To avoid this eﬀect, the two terms are
corrected as ˆmt and ˆvt. Default values of the hyperparameters are β1 = 0.9, β2 = 0.999 and ε = 10−8.
Second-order methods. The methods discussed so far only consider ﬁrst-order derivatives of the loss function.
Due to this approximation, the landscape of the loss function locally looks and behaves like a plane. Ignoring
the curvature of the surface may lead the optimization astray and it could cause the training to progress
very slowly. However, second-order methods involve the computation of the Hessian, which is expensive and
usually untreatable even in networks of medium size. A Hessian-Free (HF) method that considers derivatives
of the second order, without explicitly computing the Hessian, has been proposed by Martens . This
latter, unlike other existing HF methods, makes use of the positive semi-deﬁnite Gauss-Newton curvature
matrix and it introduces a damping factor based on the Levenberg-Marquardt heuristic, which permits to
train networks more eﬀectively. However, Sutskever et al. showed that HF obtains similar performance
to SGD with Nesterov momentum. Despite being a ﬁrst-order approach, Nestrov momentum is capable of
accelerating directions of low-curvature just like a HF method and, therefore, is preferred due to its lower
computational complexity.
2.4. Vanishing and exploding gradient
Increasing the depth in an RNN, in general, improves the memory capacity of the network and its
modeling capabilities . For example, stacked RNNs do outperform shallow ones with the same hidden
size on problems where it is necessary to store more information throughout the hidden states between the
input and output layer . One of the principal drawback of early RNN architectures was their limited
memory capacity, caused by the vanishing or exploding gradient problem , which becomes evident when
the information contained in past inputs must be retrieved after a long time interval . To illustrate
the issue of vanishing gradient, one can consider the inﬂuence of the loss function Lt (that depends on the
network inputs and on its parameters) on the network parameters Wt, when its gradient is backpropagated
through the unfolded The network Jacobian reads as
In the previous equation, the partial derivatives of the states with respect to their previous values can
be factorized as
∂h[t −1] . . . ∂h[τ + 1]
To ensure local stability, the network must operate in a ordered regime , a property ensured by the
condition |f
t| < 1. However, in this case the product expanded Eq. 10 rapidly (exponentially) converges
to 0, when t −τ increases. Consequently, the sum in Eq. 9 becomes dominated by terms corresponding to
short-term dependencies and the vanishing gradient eﬀect occurs. As principal side eﬀect, the weights are
less and less updated as the gradient ﬂows backward through the layers of the network. On the other hand,
the phenomenon of exploding gradient appears when |f
t| > 1 and the network becomes locally unstable.
Even if global stability can still be obtained under certain conditions, in general the network enters into a
chaotic regime, where its computational capability is hindered .
Models with large recurrent depths exacerbate these gradient-related issues, since they posses more
nonlinearities and the gradients are more likely to explode or vanish. A common way to handle the exploding
gradient problem, is to clip the norm of the gradient if it grows above a certain threshold. This procedure
relies on the assumption that exploding gradients only occur in contained regions of the parameters space.
Therefore, clipping avoids extreme parameter changes without overturning the general descent direction .
On the other hand, diﬀerent solutions have been proposed to tackle the vanishing gradient issue. A
simple, yet eﬀective approach consists in initializing the weights to maintain the same variance withing the
activations and back-propagated gradients, as one moves along the network depth. This is obtained with
a random initialization that guarantees the variance of the components of the weight matrix in layer l to
be Var(Wl) = 2/(Nl−1 + Nl+1), Nl−1 and Nl+1 being the number of units in the previous and the next
layer respectively . He et al. proposed to initialize the network weights by sampling them from
an uniform distribution in and then rescaling their values by 1/√Nh, Nh being the total number of
hidden neurons in the network. Another option, popular in deep FFNN, consists in using ReLU as
activation function, whose derivative is 0 or 1, and it does not cause the gradient to vanish or explode.
Regularization, besides preventing unwanted overﬁtting in the training phase, proved to be useful in dealing
with exploding gradients. In particular, L1 and L2 regularizations constrain the growth of the components
of the weight matrices and consequently limit the values assumed by the propagated gradient . Another
popular solution is adopting gated architectures, like Long Short-Term Memory (LSTM) or Gated Recurrent
Unit (GRU), which have been speciﬁcally designed to deal with vanishing gradients and allow the network
to learn much longer-range dependencies. Srivastava et al. proposed an architecture called Highway
Network, which allows information to ﬂow across several layers without attenuation. Each layer can smoothly
vary its behavior between that of a plain layer, implementing an aﬃne transform followed by a non-linear
activation, and that of a layer which simply passes its input through. Optimization in highway networks is
virtually independent of depth, as information can be routed (unchanged) through the layers. The Highway
architecture, initially applied to deep FFNN , has recently been extended to RNN where it dealt with
several modeling and optimization issues .
Finally, gradient-related problems can be avoided by repeatedly selecting new weight parameters using
random guess or evolutionary approaches ; in this way the network is less likely to get stuck in
local minima. However, convergence time of these procedures is time-consuming and can be impractical in
many real-world applications. A solution proposed by Schmidhuber et al. , consists in evolving only the
weights of non-linear hidden units, while linear mappings from hidden to output units are tuned using fast
algorithms for convex problem optimization.
3. Recurrent Neural Networks Architectures
In this section, we present three diﬀerent RNN architectures trainable through the BPPT procedure,
which we employ to predict real-valued time series. First, in Sec. 3.1 we present the most basic version
of RNN, called Elman RNN. In Sec. 3.2 and 3.3 we discuss two gated architectures, which are LSTM and
GRU. For each RNN model, we provide a quick overview of the main applications in time series forecasting
and we discuss its principal features.
3.1. Elman Recurrent Neural Network
The Elman Recurrent Neural Network (ERNN), also known as Simple RNN or Vanillan RNN, is depicted
in Fig. 1 and is usually considered to be the most basic version of RNN. Most of the more complex RNN
architectures, such as LSTM and GRU, can be interpreted as a variation or as an extension of ERNNs.
ERNN have been applied in many diﬀerent contexts. In natural language processing applications, ERNN
demonstrated to be capable of learning grammar using a training set of unannotated sentences to predict
successive words in the sentence . Mori and Ogasawara studied ERNN performance in shortterm load forecasting and proposed a learning method, called “diﬀusion learning” (a sort of momentumbased gradient descent), to avoid local minima during the optimization procedure. Cai et al. trained
a ERNN with a hybrid algorithm that combines particle swarm optimization and evolutionary computation
to overcome the local minima issues of gradient-based methods. Furthermore, ERNNs have been employed
by Cho in tourist arrival forecasting and by Mandal et al. to predict electric load time series.
Due to the critical dependence of electric power usage on the day of the week or month of the year, a
preprocessing step is performed to cluster similar days according to their load proﬁle characteristics. Chitsaz
et al. proposes a variant of ERNN called Self-Recurrent Wavelet Neural Network, where the ordinary
nonlinear activation functions of the hidden layer are replaced with wavelet functions. This leads to a sparser
representation of the load proﬁle, which demonstrated to be helpful for tackling the forecast task through
smaller and more easily trainable networks.
The layers in a RNN can be divided in input layers, hidden layers and the output layers (see Fig. 1). While
input and output layers are characterized by feedforward connections, the hidden layers contain recurrent
ones. At each time step t, the input layer process the component x[t] ∈RNi of a serial input x. The time
series x has length T and it can contain real values, discrete values, one-hot vectors, and so on. In the input
layer, each component x[t] is summed with a bias vector bi ∈RNh (Nh is the number of nodes in the hidden
layer) and then is multiplied with the input weight matrix Wh
i ∈RNi×Nh. Analogously, the internal state
of the network h[t −1] ∈RNh from the previous time interval is ﬁrst summed with a bias vector bh ∈RNh
and then multiplied by the weight matrix Wh
h ∈RNh×Nh of the recurrent connections. The transformed
current input and past network state are then combined and processed by the neurons in the hidden layers,
which apply a non-linear transformation. The diﬀerence equations for the update of the internal state and
the output of the network at a time step t are:
i (x[t] + bi) + Wh
h (h[t −1] + bh)
y[t] = g (Wo
h (h[t] + bo)) ,
where f(·) is the activation function of the neurons, usually implemented by a sigmoid or by a hyperbolic
tangent. The hidden state h[t] conveys the content of the memory of the network at time step t, is typically
initialized with a vector of zeros and it depends on past inputs and network states. The output y[t] ∈RNo is
computed through a transformation g(·), usually linear, on the matrix of the output weights Wo
applied to the sum of the current state h[t] and the bias vector bo ∈RNo. All the weight matrices and biases
can be trained through gradient descent, according to the BPPT procedure. Unless diﬀerently speciﬁed, in
the following to compact the notation we omit the bias terms by assuming x = [x; 1], h = [h; 1], y = [y; 1]
and by augmenting Wh
h with an additional column.
3.2. Long Short-Term Memory
The Long Short-Term Memory (LSTM) architecture was originally proposed by Hochreiter and Schmidhuber and is widely used nowadays due to its superior performance in accurately modeling both short
and long term dependencies in data. LSTM tries to solve the vanishing gradient problem by not imposing any bias towards recent observations, but it keeps constant error ﬂowing back through time. LSTM
works essentially in the same way as the ERNN architecture, with the diﬀerence that it implements a more
elaborated internal processing unit called cell.
LSTM has been employed in numerous sequence learning applications, especially in the ﬁeld of natural
language processing. Outstanding results with LSTM have been reached by Graves and Schmidhuber in
unsegmented connected handwriting recognition, by Graves et al. in automatic speech recognition, by
Eck and Schmidhuber in music composition and by Gers and Schmidhuber in grammar learning.
Further successful results have been achieved in the context of image tagging, where LSTM have been paired
with convolutional neural network, to provide annotations on images automatically .
However, few works exist where LSTM has been applied to prediction of real-valued time series. Ma
et al. evaluated the performances of several kinds of RNNs in short-term traﬃc speed prediction and
compared them with other common methods like SVMs, ARIMA, and Kalman ﬁlters, ﬁnding that LSTM
networks are nearly always the best approach. Pawlowski and Kurach utilized ensembles of LSTM
and feedforward architectures to classify the danger from concentration level of methane in a coal mine, by
predicting future concentration values. By following a hybrid approach, Felder et al. trains a LSTM
network to output the parameter of a Gaussian mixture model that best ﬁts a wind power temporal proﬁle.
While an ERNN neuron implements a single nonlinearity f(·) (see Eq. 11), a LSTM cell is composed of
5 diﬀerent nonlinear components, interacting with each other in a particular way. The internal state of a
cell is modiﬁed by the LSTM only through linear interactions. This permits information to backpropagate
smoothly across time, with a consequent enhancement of the memory capacity of the cell. LSTM protects
and controls the information in the cell through three gates, which are implemented by a sigmoid and a
pointwise multiplication. To control the behavior of each gate, a set of parameters are trained with gradient
descent, in order to solve a target task.
Since its initial deﬁnition , several variants of the original LSTM unit have been proposed in the
literature. In the following, we refer to the commonly used architecture proposed by Graves and Schmidhuber
 . A schema of the LSTM cell is depicted in Fig. 3.
The diﬀerence equations that deﬁne the forward pass to update the cell state and to compute the output
Figure 3: Illustration of a cell in the LSTM architecture. Dark
gray circles with a solid line are the variables whose content
is exchanged with the input and output of the cell. Dark gray
circles with a dashed line represent the internal state variables,
whose content is exchanged between the cells of the hidden
layer. Operators g1 and g2 are the non-linear transformation,
usually implemented as a hyperbolic tangent. White circles
with + and × represent linear operations, while σf, σu and σo
are the sigmoids used in the forget, update and output gates
respectively.
are listed below.
forget gate : σf[t] = σ (Wfx[t] + Rfy[t −1] + bf) ,
candidate state : ˜h[t] = g1 (Whx[t] + Rhy[t −1] + bh) ,
update gate : σu[t] = σ (Wux[t] + Ruy[t −1] + bu) ,
cell state : h[t] = σu[t] ⊙˜h[t] + σf[t] ⊙h[t −1],
output gate : σo[t] = σ (Wox[t] + Roy[t −1] + bo) ,
output : y[t] = σo[t] ⊙g2(h[t]).
x[t] is the input vector at time t. Wf, Wh, Wu, and Wo are rectangular weight matrices, that are applied
to the input of the LSTM cell. Rf, Rh, Ru, and Ro are square matrices that deﬁne the weights of the
recurrent connections, while bf, bh, bu, and bo are bias vectors. The function σ(·) is a sigmoid 2, while
g1(·) and g2(·) are pointwise non-linear activation functions, usually implemented as hyperbolic tangents
that squash the values in [−1, 1]. Finally, ⊙is the entrywise multiplication between two vectors (Hadamard
Each gate in the cell has a speciﬁc and unique functionality. The forget gate σf decides what information
should be discarded from the previous cell state h[t −1]. The input gate σu operates on the previous state
h[t −1], after having been modiﬁed by the forget gate, and it decides how much the new state h[t] should
be updated with a new candidate ˜h[t]. To produce the output y[t], ﬁrst the cell ﬁlters its current state with
a nonlinearity g2(·). Then, the output gate σo selects the part of the state to be returned as output. Each
gate depends on the current external input x[t] and the previous cells output y[t −1].
As we can see from the Fig. 3 and from the forward-step equations, when σf = 1 and σu = 0, the
current state of a cell is transferred to the next time interval exactly as it is. By referring back to Eq. 10,
it is possible to observe that in LSTM the issue of vanishing gradient does not occur, due to the absence of
nonlinear transfer functions applied to the cell state. Since in this case the transfer function f(·) in Eq. 10
applied to the internal states is an identity function, the contribution from past states remains unchanged
over time. However, in practice, the update and forget gates are never completely open or closed due to the
functional form of the sigmoid, which saturates only for inﬁnitely large values. As a result, even if long term
memory in LSTM is greatly enhanced with respect to ERNN architectures, the content of the cell cannot
be kept completely unchanged over time.
2the logistic sigmoid is deﬁned as σ(x) =
Figure 4: Illustration of a recurrent unit in the GRU architecture. Dark gray circles with a solid line are the variables whose
content is exchanged with the input and output of the network. Dark gray circles with a dashed line represent the internal state variables, whose content is exchanged within the cells
of the hidden layer. The operator g is a non-linear transformation, usually implemented as a hyperbolic tangent. White
circles with ’+’, ’−1’ and ’×’ represent linear operations, while
σr and σu are the sigmoids used in the reset and update gates
respectively.
3.3. Gated Recurrent Unit
The Gated Recurrent Unit (GRU) is another notorious gated architecture, originally proposed by Cho
et al. , which adaptively captures dependencies at diﬀerent time scales.
In GRU, forget and input
gates are combined into a single update gate, which adaptively controls how much each hidden unit can
remember or forget. The internal state in GRU is always fully exposed in output, due to the lack of a control
mechanism, like the output gate in LSTM.
GRU were ﬁrstly tested by Cho et al. on a statistical machine translation task and reported mixed
results. In an empirical comparison of GRU and LSTM, conﬁgured with the same amount of parameters,
Chung et al. concluded that on some datasets GRU can outperform LSTM, both in terms of generalization capabilities and in terms of time required to reach convergence and to update parameters. In an
extended experimental evaluation, Zaremba employed GRU to (i) compute the digits of the sum or
diﬀerence of two input numbers, (ii) predict the next character in a synthetic XML dataset and in the large
words dataset Penn TreeBank, (iii) predict polyphonic music. The results showed that the GRU outperformed the LSTM on nearly all tasks except language modeling when using a naive initialization. Bianchi
et al. compared GRU with other recurrent networks on the prediction of superimposed oscillators. However, to the best of author’s knowledge, at the moment there are no researches where the standard GRU
architecture has been applied in STLF problems.
A schematic depiction of the GRU cell is reported in Fig. 4. GRU makes use of two gates. The ﬁrst
is the update gate, which controls how much the current content of the cell should be updated with the
new candidate state. The second is the reset gate that, if closed (value near to 0), can eﬀectively reset the
memory of the cell and make the unit act as if the next processed input was the ﬁrst in the sequence. The
state equations of the GRU are the following:
reset gate : r[t] = σ (Wrh[t −1] + Rrx[t] + br) ,
current state : h′[t] = h[t −1] ⊙r[t],
candidate state : z[t] = g (Wzh′[t] + Rzx[t] + bz) ,
update gate : u[t] = σ (Wuh[t −1] + Rux[t] + bu) ,
new state : h[t] = (1 −u[t]) ⊙h[t −1] + u[t] ⊙z[t].
Here, g(·) is a non-linear function usually implemented by a hyperbolic tangent.
In a GRU cell, the number of parameters is larger than in the an ERNN unit, but smaller than in a
LSTM cell. The parameters to be learned are the rectangular matrices Wr, Wz, Wu, the square matrices
Rr, Rz, Ru, and the bias vectors br, bz, bu.
4. Other Recurrent Neural Networks Models
In this section we describe two diﬀerent types of RNNs, which are the Nonlinear AutoRegressive eXogenous inputs neural network (NARX) and the Echo State Network (ESN). Both of them have been largely
employed in STLF. These two RNNs diﬀer from the models described in Sec. 3, both in terms of their
architecture and in the training procedure, which is not implemented as a BPPT. Therefore, some of the
properties and training approaches discussed in Sec. 2 do not hold for these models.
4.1. NARX Network
NARX networks are recurrent dynamic architectures with several hidden layers and they are inspired
by discrete-time nonlinear models called Nonlinear AutoRegressive with eXogenous inputs . Diﬀerently
from other RNNs, the recurrence in the NARX network is given only by the feedback on the output, rather
than from the whole internal state.
NARX networks have been employed in many diﬀerent applicative contexts, to forecast future values
of the input signal . Menezes and Barreto showed that NARX networks perform better on
predictions involving long-term dependencies. Xie et al. used NARX in conjunction with an input
embedded according to Takens method, to predict highly non-linear time series. NARX are also employed
as a nonlinear ﬁlter, whose target output is trained by using the noise-free version of the input signal
 . NARX networks have also been adopted by Plett in a gray-box approach for nonlinear system
identiﬁcation.
A NARX network can be implemented with a MultiLayer Perceptron (MLP), where the next value of the
output signal y[t] ∈RNy is regressed on dy previous values of the output signal and on dx previous values of
an independent, exogenous input signal x[t] ∈RNx . The output equation reads
y[t] = φ (x[t −dx], . . . , x[t −1], x[t], y[t −dy], . . . , y[t −1], Θ) ,
where φ(·) is the nonlinear mapping function performed by the MLP, Θ are the trainable network parameters,
dx and dy are the input and the output time delays. Even if the numbers of delays dx and dy is a ﬁnite
(often small) number, it has been proven that NARX networks are at least as powerful as Turing machines,
and thus they are universal computation devices .
The input i[t] of the NARX network has dxNx + dyNy components, which correspond to a set of two
Tapped-Delay Lines (TDLs), and it reads
(x[t −dx], . . . , x[t −1])T
(y[t −dy], . . . , y[t −1])T
The structure of a MLP network consists of a set of source nodes forming the input layer, Nl ≥1 layers
of hidden nodes, and an output layer of nodes. The output of the network is governed by the following
diﬀerence equations
h1[t] = f (i[t], θi) ,
hl[t] = f (hl−1[t −1], θhl) ,
y[t] = g (hNl[t −1], θo) ,
where hl[t] ∈RNhl is the output of the lth hidden layer at time t, g(·) is a linear function and f(·) is the
transfer function of the neuron, usually implemented as a sigmoid or tanh function.
The weights of the neurons connections are deﬁned by the parameters Θ =
θi, θo, θh1, . . . , θhNl
particular, θi =
∈RdxNx+dyNy×Nh1, bh1 ∈RNh1
are the parameters that determine the weights in
the input layer, θo =
hNl ∈RNNl×Ny, bo ∈RNy
are the parameters of the output layer and θhl =
Figure 5: Architecture of the NARX network. Circles represent input x and output y, respectively. The two TDL blocks are
the tapped-delay lines. The solid squares Wh1
hNl , bi, and bo are the weight matrices and the bias relative to the input
and the output respectively. The dashed squares are the weight matrices and the biases relative to the Nl hidden layers – in
the ﬁgure, we report Wh2
h1 and bh2, relative to the ﬁrst hidden layer. The polygon with the sigmoid symbol represents the
nonlinear transfer function of the neurons and the one with the oblique line is a linear operation. Finally, z-1 is the backshift/lag
hl−1 ∈RNhl−1×Nhl , bhl ∈RNhl
are the parameters of the lth hidden layer. A schematic depiction of a
NARX network is reported in Fig. 5.
Due to the architecture of the network, it is possible to exploit a particular strategy to learn the parameters
Θ. Speciﬁcally, during the training phase the time series relative to the desired output y∗is fed into the
network along with the input time series x. At this stage, the output feedback is disconnected and the
network has a purely feed-forward architecture, whose parameters can be trained with one of the several,
well-established standard backpropagation techniques. Notice that this operation is not possible in other
recurrent networks such as ERNN, since the state of the hidden layer depends on the previous hidden state,
whose ideal value is not retrievable from the training set. Once the training stage is over, the teacher signal
of the desired output is disconnected and is replaced with the feedback of the predicted output y computed
by the network. The procedure is depicted in Fig. 6.
Feed Forward
Neural Network
(a) Training mode
Feed Forward
Neural Network
(b) Operational mode
Figure 6: During the training, the desired input y∗is fed directly to the network. Once the network parameters have been
optimized, the teacher signal is removed and the output y produced by the network is connected to the input with a feedback
Similar to what discussed in Sec. 2.2 for the previous RNN architectures, the loss function employed in
the gradient descent is deﬁned as
L(x, y∗; Θ) = MSE(y, y∗) + λ2∥Θ∥2,
where MSE is the error term deﬁned in Eq. 2 and λ2 is the hyperparameter that weights the importance of
the L2 regularization term in the loss function. Due to the initial transient phase of the network, when the
estimated output y is initially fed back as network input, the ﬁrst initial outputs are discarded.
Even if it reduces to a feed-forward network in the training phase, NARX network is not immune to
the problem of vanishing and exploding gradients. This can be seen by looking at the Jacobian Jh(t, n)
of the state-space map at time t expanded for n time step. In order to guarantee network stability, the
Jacobian must have all of its eigenvalues inside the unit circle at each time step. However, this results in
n→∞Jh(t, n) = 0, which implies that NARX networks suﬀer from vanishing gradients, like the other RNNs
4.2. Echo State Network
While most hard computing approaches and ANNs demand long training procedures to tune the parameters through an optimization algorithm , recently proposed architectures such as Extreme Learning
Machines and ESNs are characterized by a very fast learning procedure, which usually consists in
solving a convex optimization problem. ESNs, along with Liquid State Machines , belong to the class
of computational dynamical systems implemented according to the so-called reservoir computing framework
ESN have been applied in a variety of diﬀerent contexts, such as static classiﬁcation , speech recognition , intrusion detection , adaptive control , detrending of nonstationary time series ,
harmonic distortion measurements and, in general, for modeling of various kinds of non-linear dynamical
systems .
ESNs have been extensively employed to forecast real valued time series. Niu et al. trained an
ESN to perform multivariate time series prediction by applying a Bayesian regularization technique to the
reservoir and by pruning redundant connections from the reservoir to avoid overﬁtting. Superior prediction
capabilities have been achieved by projecting the high-dimensional output of the ESN recurrent layer into
a suitable subspace of reduced dimension . An important context of application with real valued time
series is the prediction of telephonic or electricity load, usually performed 1-hour and a 24-hours ahead
 . Deihimi et al. and Peng et al. decomposed the time series in wavelet components,
which are predicted separately using distinct ESN and ARIMA models, whose outputs are combined to
produce the ﬁnal result. Important results have been achieved in the prediction of chaotic time series by
Li et al. . They proposed an alternative to the Bayesian regression for estimating the regularization
parameter and a Laplacian likelihood function, more robust to noise and outliers than a Gaussian likelihood.
Jaeger and Haas applied an ESN-based predictor on both benchmark and real dataset, highlighting the
capability of these networks to learn amazingly accurate models to forecast a chaotic process from almost
noise-free training data.
An ESN consists of a large, sparsely connected, untrained recurrent layer of nonlinear units and a linear,
memory-less read-out layer, which is trained according to the task that the ESN is demanded to solve. A
visual representation of an ESN is shown in Fig. 7
Figure 7: Schematic depiction of the ESN architecture. The circles represent input x, state h,
and output y, respectively. Solid squares Wo
i , are the trainable matrices of the readout, while dashed squares, Wr
are randomly initialized matrices. The polygon represents the non-linear transformation
performed by neurons and z-1 is the unit delay
The diﬀerence equations describing the ESN state-update and output are, respectively, deﬁned as follows:
h[t] =f(Wr
rh[t −1] + Wr
i x[t] + Wr
oy[t −1] + ϵ),
y[t] =g(Wo
i x[t] + Wo
where ϵ is a small noise term. The reservoir contains Nh neurons whose transfer/activation function f(·)
is typically implemented by a hyperbolic tangent. The readout instead, is implemented usually by a linear
function g(·). At time instant t, the network is driven by the input signal x[t] ∈RNi and produces the output
y[k] ∈RNo, Ni and No being the dimensionality of input and output, respectively. The vector h[t] has Nh
components and it describes the ESN (instantaneous) state. The weight matrices Wr
r ∈RNr×Nr (reservoir
connections), Wr
i ∈RNi×Nr (input-to-reservoir), and Wr
o ∈RNo×Nr (output-to-reservoir feedback) contain
real values in the [−1, 1] interval drawn from a uniform distribution and are left untrained. Alternative
options have been explored recently by Rodan and Tiˇno and Appeltant et al. to generate the
connection weights. The sparsity of the reservoir is controlled by a hyperparameter Rc, which determines the
number of nonzero elements in Wr
r. According to the ESN theory, the reservoir Wr
r must satisfy the so-called
“echo state property” (ESP) . This means that the eﬀect of a given input on the state of the reservoir
must vanish in a ﬁnite number of time-instants. A widely used rule-of-thumb to obtain this property suggests
to rescale the matrix Wr
r in order to have ρ(Wr
r) < 1, where ρ(·) denotes the spectral radius. However,
several theoretical approaches have been proposed in the literature to tune ρ more accurately, depending on
the problem at hand .
On the other hand, the weight matrices Wo
r are optimized for the target task. To determine
them, let us consider the training sequence of Ttr desired input-outputs pairs given by:
(x , y∗ ) . . . , (x[Ttr], y[Ttr]),
where Ttr is the length of the training sequence. In the initial phase of training, called state harvesting, the
inputs are fed to the reservoir, producing a sequence of internal states h , . . . , h[Ttr], as deﬁned in Eq. (20).
The states are stacked in a matrix S ∈RTtr×Ni+Nr and the desired outputs in a vector y∗∈RTtr:
xT , hT 
xT [Ttr], hT [Ttr]
The initial rows in S (and y∗) are discarded, since they refer to a transient phase in the ESN’s behavior.
The training of the readout consists in learning the weights in Wo
r so that the output of the
ESN matches the desired output y∗. This procedure is termed teacher forcing and can be accomplished by
solving a convex optimization problem, for which several closed form solution exist in the literature. The
standard approach, originally proposed by Jaeger , consists in applying a least-square regression, deﬁned
by the following regularized least-square problem:
ls = arg min
2∥SW −y∗∥2 + λ2
where W = [Wo
r]T and λ2 ∈R+ is the L2 regularization factor.
A solution to problem (25) can be expressed in closed form as
 ST S + λ2I
−1 ST y∗,
which can be solved by computing the Moore-Penrose pseudo-inverse. Whenever Nh + Ni > Ttr, Eq. (26)
can be computed more eﬃciently by rewriting it as
ls = ST  SST + λ2I
5. Synthetic time series
We consider three diﬀerent synthetically generated time series in order to provide controlled and easily
replicable benchmarks for the architectures under analysis. The three forecasting exercises that we study have
a diﬀerent level of diﬃculty, given by the nature of the signal and the complexity of the task to be solved by
the RNN. In order to obtain a prediction problem that is not too simple, it is reasonable to select as forecast
horizon a time interval tf that guarantees the measurements in the time series to become decorrelated.
Hence, we consider the ﬁrst zero of the autocorrelation function of the time series. Alternatively, the ﬁrst
minimum of the average mutual information or of the correlation sum could be chosen to select
a tf where the signal shows a more-general form of independence. All the time series introduced in the
following consist of 15.000 time steps. We use the ﬁrst 60% of the time series as training set, to learn the
parameters of the RNN models. The next 20% of the data are used as validation set and the prediction
accuracy achieved by the RNNs on this second dataset is used to tune the hyperparameters of the models.
The ﬁnal model performance is evaluated on a test set, corresponding to the last 20% of the values in the
time series.
Mackey-Glass time series. The Mackey-Glass (MG) system is commonly used as benchmark for prediction
of chaotic time series. The input signal is generated from the MG time-delay diﬀerential system, described
by the following equation:
αx(t −τMG)
1 + x(t −τMG)10 −βx(t).
For this prediction task, we set τMG = 17, α = 0.2, β = 0.1, initial condition x(0) = 1.2, 0.1 as integration
step for (28) and the forecast horizon tf = 12.
NARMA signal. The Non-Linear Auto-Regressive Moving Average (NARMA) task, originally proposed by
Jaeger , consists in modeling the output of the following r-order system:
y(t + 1) = 0.3y(t) + 0.05y(t)
+ 1.5x(t −r)x(t) + 0.1.
The input to the system x(t) is uniform random noise in , and the model is trained to reproduce y(t+1).
The NARMA task is known to require a memory of at least r past time-steps, since the output is determined
by input and outputs from the last r time-steps. For this prediction task we set r = 10 and the forecast step
tf = 1 in our experiments.
Multiple superimposed oscillator. The prediction of a sinusoidal signal is a relatively simple task, which
demands a minimum amount of memory to determine the next network output. However, superimposed
sine waves with incommensurable frequencies are extremely diﬃcult to predict, since the periodicity of the
resulting signal is extremely long.
The time series we consider is the Multiple Superimposed Oscillator
(MSO) introduced by Jaeger and Haas , and it is deﬁned as
y(t) = sin(0.2t) + sin(0.311t) + sin(0.42t) + sin(0.51t).
This academic, yet important task, is particularly useful to test the memory capacity of a recurrent neural
network and has been studied in detail by Xue et al. in a dedicated work. Indeed, to accurately predict
the unseen values of the time series, the network requires a large amount of memory to simultaneously
implement multiple decoupled internal dynamics . For this last prediction task, we chose a forecast step
6. Real-world load time series
In this section, we present three diﬀerent real-world dataset, where the time series to be predicted contain
measurements of electricity and telephonic activity load. Two of the dataset contain exogenous variables,
which are used to provide additional context information to support the prediction task. For each dataset, we
perform a pre-analysis to study the nature of the time series and to ﬁnd the most suitable data preprocessing.
In fact, forecast accuracy in several prediction models, among which neural networks, can be considerably
improved by applying a meaningful preprocessing .
6.1. Orange dataset – telephonic activity load
The ﬁrst real-world dataset that we analyze is relative to the load of phone calls registered over a mobile
network. Data come from the Orange telephone dataset , published in the Data for Development (D4D)
challenge . D4D is a collection of call data records, containing anonymized events of Orange’s mobile
phone users in Ivory Coast, in a period spanning from December 1, 2011 to April 28, 2012. More detailed
information on the data are available in Ref. . The time series we consider are relative to antenna-toantenna traﬃc. In particular, we selected a speciﬁc antenna, retrieved all the records in the dataset relative
to the telephone activity issued each hour in the area covered by the antenna and generated 6 time series:
• ts1: number of incoming calls in the area covered by the antenna;
• ts2: volume in minutes of the incoming calls in the area covered by the antenna;
• ts3: number of outgoing calls in the area covered by the antenna;
• ts4: volume in minutes of the outgoing calls in the area covered by the antenna;
• ts5: hour when the telephonic activity is registered;
• ts6: day when the telephonic activity is registered.
In this work, we focus on predicting the volume (in minutes) of the incoming calls in ts1 of the next
day. Due to the hourly resolution of the data, the STFL problem consists of a 24 step-ahead prediction.
The proﬁle of ts1 for 300 hours is depicted in Fig. 8(a). The remaining time series are treated as exogenous
Time (hours)
Load volume
(a) Load proﬁle
Sample autocorrelation
(b) Autocorrelation functions
Figure 8: In (a), the load proﬁle of ts1, the incoming calls volume, for 300 time intervals (hours). In (b), the autocorrelation
functions of the time series ts1 before (gray line) and after (black line) a seasonal diﬀerentiation. The original time series shows
a strong seasonal pattern at lag 24, while after seasonal diﬀerencing, the time series does not show any strong correlation or
variables and, according to a common practice in time series forecasting , they are fed into the network
to provide the model with additional information for improving the prediction of the target time series. Each
time series contain 3336 measurements, hourly sampled. We used the ﬁrst 70% as training set, the successive
15% as validation set and the remaining 15% as test set. The accuracy of each RNN model is evaluated on
this last set.
In each time series there is a (small) fraction of missing values. In fact, if in a given hour no activities are
registered in the area covered by the considered antenna, the relative entries do not appear in the database.
As we require the target time series and the exogenous ones to have same lengths and to contain a value in
each time interval, we inserted an entry with value “0” in the dataset to ﬁll the missing values. Another issue
is the presence of corrupted data, marked by a “-1” in the dataset, which are relative to periods when the
telephone activity is not registered correctly. To address this problem, we followed the procedure described
by Shen and Huang and we replaced the corrupted entries with the average value of the corresponding
periods (same weekday and hour) from the two adjacent weeks. Contrarily to some other works on STLF
 , we decided to not discard outliers, such as holidays or days with an anomalous number of calls,
nor we modeled them as separate variables.
As next step in our pre-analysis, we identify the main seasonality in the data. We analyze ts1, but
similar considerations hold also for the remaining time series. Through frequency analysis and by inspecting
the autocorrelation function, depicted as a gray line in Fig. 8(b), it emerges a strong seasonal pattern every
24 hours. As expected, data experience regular and predictable daily changes, due to the nature of the
telephonic traﬃc. This cycle represents the main seasonality and we ﬁlter it out by applying a seasonal
diﬀerencing with lag 24. In this way, the RNNs focus on learning to predict the series of changes in each
seasonal cycle. The practice of removing the seasonal eﬀect from the time series, demonstrated to improve
the prediction accuracy of models based on neural networks . The black line in Fig. 8(b) depicts
the autocorrelation of the time series after seasonal diﬀerentiation. Except from the high anticorrelation at
lag 24, introduced by the diﬀerentiation, the time series appears to be uncorrelated elsewhere and, therefore,
we can exclude the presence of a second, less obvious seasonality.
Due to the nature of the seasonality in the data, we expect a strong relationship between the time series
of the loads (ts1 - ts4) and ts5, which is relative to the hour of the day. On the other hand, we envisage a
lower dependency of the loads with ts6, the time series of the week days, since we did not notice the presence
of a second seasonal cycle after the diﬀerentiation at lag 24. To conﬁrm our hypothesis, we computed the
mutual information between the time series, which are reported in the Hinton diagram in Fig. 9. The size
Figure 9: Hinton diagram of the mutual information between the time series in the Orange dataset. The size of
each block is proportional to the degree of mutual information among the time series. The measurements indicates
a strong relationship between the load time series and the
daily hours (ts5), while the dependency with the day of the
week (ts6) is low.
of the blocks is proportional to the degree of mutual information among the time series. Due to absence of
strong relationships, we decided to discard ts6 to reduce the complexity of the model by excluding a variable
with potentially low impact in the prediction task. We also discarded ts5 because the presence of the cyclic
daily pattern is already accounted by doing the seasonal diﬀerencing at lag 24. Therefore, there is not need
to provide daily hours as an additional exogenous input.
Beside diﬀerentiation, a common practice in STLF is to apply some form of normalization to the data. We
applied a standardization (z-score), but rescaling into the interval [−1, 1] or are other viable options.
Additionally, a nonlinear transformation of the data by means of a non-linear function (e.g., square-root
or logarithm) can remove some kinds of trend and stabilize the variance in the data, without altering too
much their underlying structure . In particular, a log-transform is suitable for a set of random
variables characterized by a high variability in their statistical dispersion (heteroscedasticity), or for a process
whose ﬂuctuation of the variance is larger than the ﬂuctuation of the mean (overdispersion). To check those
properties, we analyze the mean and the variance of the telephonic traﬃc within the main seasonal cycle
across the whole dataset. The solid black line in Fig. 10(a), represents the mean load of ts1, while the
Load volume
(a) Raw data
Load volume (Log)
(b) Log-transformed data
Figure 10: Average weekly load (solid black line) and the standard deviation (shaded gray area) of the telephonic activity in
the whole dataset.
shaded gray area illustrates the variance. As we can see, the data are not characterized by overdispersion,
since the ﬂuctuations of the mean are greater than the ones of the variance. However, we notice the presence
of heteroscedasticity, since the amount of variance changes in diﬀerent hours of the day. In fact, the central
hours where the amount of telephonic activity is higher, are characterized by a greater standard deviation
in the load. In Fig. 10(b), we observe that by applying a log-transform we signiﬁcantly reduce the amount
of variance in the periods characterized by a larger traﬃc load. However, after the log-transformation the
mean value of the load become more ﬂattened and the variance relative to periods with lower telephonic
activity is enhanced. This could cause issues during the training of the RNN, hence in the experiments we
evaluate the prediction accuracy both with and without applying the log-transformation to the data.
Preprocessing transformations are applied in this order: (i) log-transform, (ii) seasonal diﬀerencing at
lag 24, (iii) standardization. Each preprocessing operation is successively reversed to evaluate the forecast
produced by each RNN.
6.2. ACEA dataset – electricity load
The second time series we analyze is relative to the electricity consumption registered by ACEA (Azienda
Comunale Energia e Ambiente), the company which provides the electricity to Rome and some neighbouring
The ACEA power grid in Rome consists of 10.490 km of medium voltage lines, while the low
voltage section covers 11.120 km. The distribution network is constituted of backbones of uniform section,
exerting radially and with the possibility of counter-supply if a branch is out of order. Each backbone is
fed by two distinct primary stations and each half-line is protected against faults through the breakers.
Additional details can be found in Ref. . The time series we consider concerns the amount of supplied
electricity, measured on a medium voltage feeder from the distribution network of Rome. Data are collected
every 10 minutes for 954 days of activity (almost 3 years), spanning from 2009 to 2011, for a total of
137444 measurements. Also in this case, we train the RNNs to predict the electricity load 24h ahead, which
corresponds to 144 time step ahead prediction. For this forecast task we do not provide any exogenous time
series to the RNNs. In the hyperparameter optimization, we use the load relative to the ﬁrst 3 months as
training set and the load of the 4th month as validation set. Once the best hyperparameter conﬁguration
is identiﬁed, we ﬁne-tune each RNN on the ﬁrst 4 months and we use the 5th month as test set to evaluate
and to compare the accuracy of each network.
Time(minutes)
Load volume (kV)
(a) Load proﬁle
Sample autocorrelation
Di,erentiated
(b) Autocorrelation functions
Figure 11: In (a), the load proﬁle in kiloVolts (kV) of the electricity consumption registered over one week. The sampling time
is 10 minutes. In (b), the autocorrelation functions of the ACEA time series before (gray line) and after (black line) a seasonal
diﬀerentiation at lag 144. The original time series shows a strong seasonal pattern at lag 144, which corresponds to a daily
cycle. After seasonal diﬀerencing, a previously hidden pattern is revealed at lag 1008, which corresponds to a weekly cycle.
A proﬁle of the electric consumption over one week (1008 measurements), is depicted in Fig. 11(a).
In the ACEA time series there are no missing values, but 742 measurements (which represent 0.54% of
the whole dataset) are corrupted. The consumption proﬁle is more irregular in this time series, with respect
to the telephonic data from the Orange dataset. Therefore, rather than replacing the corrupted values with
an average load, we used a form of imputation with a less strong bias. Speciﬁcally, we ﬁrst ﬁt a cubic spline
to the whole dataset and then we replaced the corrupted entries with the corresponding values from the
ﬁtted spline. In this way, the imputation better accounts for the local variations of the load.
Also in this case, we perform a preemptive analysis in order to understand the nature of the seasonality,
to detect the presence of hidden cyclic patterns, and to evaluate the amount of variance in the time series.
By computing the autocorrelation function up to a suﬃcient number of lags, depicted as a gray line in
Fig. 11(b), it emerges a strong seasonality pattern every 144 time intervals. As expected, this corresponds
exactly to the number of measurements in one day. By diﬀerencing the time series at lag 144, we remove
the main seasonal pattern and the trend. Also in this case, the negative peak at lag 144 is introduced by
the diﬀerentiation. If we observe the autocorrelation plot of the time series after seasonal diﬀerencing (black
line in 11(b)), a second strong correlation appears each 1008 lags. This second seasonal pattern represents
a weekly cycle, that was not clearly visible before the diﬀerentiation. Due to the long periodicity of the
time cycle, to account this second seasonality a predictive model would require a large amount of memory to
store information for a longer time interval. While a second diﬀerentiation can remove this second seasonal
pattern, we would have to discard the values relative to the last week of measurements. Most importantly,
the models we train could not learn the similarities in consecutive days at a particular time, since they would
be trained on the residuals of the load at the same time and day in two consecutive weeks. Therefore, we
decided to apply only the seasonal diﬀerentiation at lag 144.
To study the variance in the time series, we consider the average daily load over the main seasonal
cycle of 144 time intervals. As we can see from Fig. 12(a), data appear to be aﬀected by overdispersion,
as the standard deviation (gray shaded areas) ﬂuctuates more than the mean. Furthermore, the mean load
value (black solid line) seems to not change much across the diﬀerent hours, while it is reasonable to expect
signiﬁcant diﬀerences in the load between night and day. However, we remind that the Acea time series
spans a long time lapse (almost 3 years) and that the electric consumption is highly related to external
factors such as temperature, daylight saving time, holidays and other seasonal events that change over
time. Therefore, in diﬀerent periods the load proﬁle may vary signiﬁcantly. For example, in Fig. 12(b) we
report the load proﬁle relative to the month of January, when temperatures are lower and there is a high
consumption of electricity, also in the evening, due to the usage of heating. In June instead (Fig. 12(c)),
the overall electricity consumption is lower and mainly concentrated on the central hours of the day. Also,
Load volume
(a) Whole dataset
Load volume
(b) January
Load volume
Figure 12: In (a) we report the mean load (black line) and the standard deviation (gray area) of the electricity consumption in
a week, accounting the measurements from all the dataset. In (b) and (c), the measurements are relative only to one month of
activity, which are January and June respectively.
it is possible to notice that the load proﬁle is shifted due to the daylight saving time. As we can see, the
daily averages within a single month are characterized by a much lower standard deviation (especially in the
summer months, with lower overall load consumption) and the mean consumption is less ﬂat. Henceforth, a
non-linear transformation for stabilizing the variance is not required and, also in this case, standardization
is suitable for normalizing the values in the time series. Since we focus on a short term forecast, having a
high variance in loads relative to very distant periods is not an issue, since the model prediction will depends
mostly on the most recently seen values.
To summarize, as preprocessing operation we apply: (i) seasonal diﬀerencing at lag 144, (ii) standardization. As before, the transformations are reverted to estimate the forecast.
6.3. GEFCom2012 dataset – electricity load
The last real world dataset that we study is the time series of electricity consumption from the Global
Energy Forecasting Competition 2012 (GEF-Com2012) . The GEFCom 2012 dataset consists of 4 years
 of hourly electricity load collected from a US energy supplier. The dataset comprehends time
series of consumption measurements, from 20 diﬀerent feeders in the same geographical area. The values
in each time series represent the average hourly load, which varies from 10.000kWh to 200.000kWh. The
dataset also includes time series of the temperatures registered in the area where the electricity consumption
is measured.
The forecast task that we tackle is the 24 hours ahead prediction of the aggregated electricity consumption,
which is the sum of the 20 diﬀerent load time series in year 2006. The measurements relative to the to ﬁrst
10 months of the 2006 are used as training set, while the 11th month is used as validation set for guiding the
hyperparameters optimization. The time series of the temperature in the area is also provided to the RNNs
as an exogenous input. The prediction accuracy of the optimized RNNs is then evaluated on the last month
of the 2006. A depiction of the load proﬁle of the aggregated load time series is reported in Fig. 13(a). We
can observe a trend in the time series, which indicates a decrement in the energy demand over time. This can
be related to climate conditions since, as the temperature becomes warmer during the year, the electricity
consumption for the heating decreases.
To study the seasonality in the aggregated time series, we evaluate the autocorrelation function, which
is depicted as the gray line in Fig. 13(b). From the small subplot in top-right part of the ﬁgure, relative to a
small segment of the time series, it emerges a strong seasonal pattern every 24 hours. By applying a seasonal
diﬀerentiation with lag 24 the main seasonal pattern is removed, as we can see from the autocorrelation
function of the diﬀerentiated time series, depicted as a black line in the ﬁgure. After diﬀerentiation, the
autocorrelation becomes close to zero after the ﬁrst lags and, therefore, we can exclude the presence of a
second, strong seasonal pattern (e.g. a weekly pattern).
Similarly to what we did previously, we analyze the average load of the electricity consumption during
one week. As for the ACEA dataset, rather than considering the whole dataset, we analyze separately the
Time(minutes)
Load volume (kWh)
(a) Load proﬁle
Sample autocorrelation
Di,erentiated
(b) Autocorrelation functions
Figure 13: In (a), the load proﬁle in kilowatt-hour (kWh) of the aggregated electricity consumption registered in the ﬁrst 4
months of activity in 2006, from the GEFCom dataset. The sampling time in the time series is 1 hour. In (b), the autocorrelation
functions of the GEFCom time series before (gray line) and after (black line) a seasonal diﬀerentiation at lag 24. The small
subplot on the top-right part of the ﬁgure reports a magniﬁed version of the autocorrelation function before diﬀerentiation at
lag t = 200.
Load volume
(a) January
Load volume
Figure 14: In (a) the average load (solid black line) and the standard deviation (shaded gray area) of the electricity consumption
during one week, in the month of January. In (b), we report the measurements relative to the month of June.
load in one month of winter and one month in summer. In Fig. 14(a), we report the mean load (black line)
and standard deviation (gray area) in January. Fig. 14(b) instead, depicts the measurements for May. It is
possible to notice a decrement of the load during the spring period, due to the reduced usage of heating. It is
also possible to observe a shift in the consumption proﬁle to later hours in the day, due to the time change.
By analyzing the amount of variance and the ﬂuctuations of the mean load, we can exclude the presence of
overdispersion and heteroscedasticity phenomena in the data.
To improve the forecasting accuracy of the electricity consumption, a common practice is to provide to
the prediction system the time series of the temperature as an exogenous variable. In general, the load and
the temperature are highly related, since both in the coldest and in the warmest months electricity demand
increases, due to the usage of heating and air conditioning, respectively. However, the relationship between
temperature and load cannot be captured by the linear correlation, since the consumption increases both
when temperatures are too low or too high. Indeed, the estimated correlation between the aggregated load
time series of interest and the time series of the temperature in the area yields only a value of 0.2. However,
their relationship is evidenced by computing a 2-dimensional histogram of the two variables, proportional
to their estimated joint distribution, which is reported in Fig 15. The V-shape, denotes an increment of the
electricity consumption for low and high temperatures with respect to a mean value of about 22◦C.
The preprocessing operations we apply on the GEFCom dataset are: (i) seasonal diﬀerencing at lag 24,
Temperature (°C)
Load volume (kWh) # 105
Figure 15: 2-dimensional histogram of the aggregated electricity load and temperature in GEFCom dataset.
Darker areas
represent more populated bins. The bar on the right indicates the number of elements in each bin. The characteristic V-shape
of the resulting pattern is because of the increased use of heating and cooling devices in presence of hot and cold temperatures.
(ii) standardization. Also in this case, these transformations are reverted to estimate the forecast.
7. Experiments
In this section we compare the prediction performance achieved by the network architectures presented
in Sec. 3 and 4 on diﬀerent time series. For each architecture, we describe the validation procedure we
follow to tune the hyperparameters and to ﬁnd an optimal learning strategy for training the weights. During
the validation phase, diﬀerent conﬁgurations are randomly selected from admissible intervals and, once the
training is over, their optimality is evaluated as the prediction accuracy achieved on the validation set. We
opted for a random search as it can ﬁnd more accurate results than a grid search, when the same number
of conﬁgurations are evaluated . Once the (sub)optimal conﬁguration is identiﬁed, we train each model
10 times on the training and validation data, using random and independent initializations of the network
parameters, and we report the highest prediction accuracy obtained on the unseen values of the test set.
To compare the forecast capability of each model, we evaluate the prediction accuracy ψ as ψ = 1 −
NRMSE. NRMSE is the Normalized Root Mean Squared Error that reads
NRMSE (Y, Y∗) =
⟨∥Y −Y∗∥2⟩
⟨∥Y −⟨Y∗⟩∥2⟩,
where ⟨·⟩computes the mean, Y are the RNN outputs and Y∗are the ground-truth values.
In the following, we present two types of experiments. The ﬁrst experiment consists in the prediction of
the synthetic time series presented in Sec. 5, commonly considered as benchmarks in forecast applications,
and the results are discussed in Sec. 7.2. In the second experiment we forecast the real-world telephonic and
electricity load time series, presented in Sec. 6. The results of this second experiment are discussed in Sec.
7.1. Experimental settings
ERNN, LSTM and GRU. The three RNNs described in Sec. 3 have been implemented in Python, using
Keras library with Theano as backend3.
To identify an optimal conﬁguration for the speciﬁc task at hand, we evaluate for each RNN diﬀerent
values of the hyperparameters and training procedures. The conﬁgurations are selected randomly and their
3Keras library is available at Theano library is available at 
net/software/theano/
performances are evaluated on the validation set, after having trained the network for 400 epochs. To get
rid of the initial transient phase, we drop the ﬁrst 50 outputs of the network.
A total of 500 random
conﬁgurations for each RNN are evaluated and, once the optimal conﬁguration is found, we compute the
prediction accuracy on the test set. In the test phase, each network is trained for 2000 epochs.
The optimization is performed by assigning to each hyperparameter a value uniformly sampled from a
given interval, which can be continuous or discrete. The gradient descent strategies are selected from a set
of possible alternatives, which are SGD, Nesterov momentum and Adam. For SGD and Nesterov, we anneal
the learning rate with a step decay of 10−6 in each epoch. The learning rate η is sampled from diﬀerent
intervals, depending on the strategy selected. Speciﬁcally, for SGD we set η = 10c, with c uniformly sampled
in [−3, −1]. For Nesterov and Adam, since they beneﬁt from a smaller initial value of the learning rate, we
sample c uniformly in [−4, −2]. The remaining hyperparameters used in the optimization strategies are kept
ﬁxed to their default values (see Sec. 2.3). Regarding the number Nh of hidden units in the recurrent hidden
layer, we randomly chose for each architecture four possible conﬁgurations that yield an amount of trainable
parameters approximately equal to 1800, 3900, 6800, and 10000. This corresponds to Nh = {40, 60, 80, 100}
in ERNN, Nh = {20, 30, 40, 50} in LSTM and Nh = {23, 35, 46, 58} in GRU. For each RNNs, Nh is randomly
selected from these sets. To deal with the problem of vanishing gradient discussed in Sec. 2.4, we initialize
the RNN weights by sampling them from an uniform distribution in and then rescaling their values
by 1/√Nh. For the L1 and L2 regularization terms, we sample independently λ1 and λ2 from [0, 0.1], an
interval containing values commonly assigned to these hyperparameters in RNNs . We apply the same
regularization to input, recurrent and output weights. As suggested by Gal and Ghahramani , we drop
the same input and recurrent connections at each time step in the BPTT, with a dropout probability pdrop
drawn from {0, 0.1, 0.2, 0.3, 0.5}, which are commonly used values .
If pdrop ̸= 0, we also apply a
L2 regularization. This combination usually yields a lowest generalization error than dropout alone .
Note that another possible approach combines dropout with the max-norm constraint, where the L2 norm
of the weights is clipped whenever it grows beyond a given constant, which, however, introduces another
hyperparameter.
For the training we consider the backpropagation through time procedure BPTT(τb, τf) with τb = 2τf.
The parameter τf is randomly selected from the set {10, 15, 20, 25, 30}. As we discussed in Sec. 2.1, this
procedure diﬀers from both the true BPTT and the epochwise BPTT , which is implemented as default
by popular deep learning libraries such as TensorFlow .
NARX. This RNN is implemented using the Matlab Neural Network toolbox4.
We conﬁgured NARX
network with an equal number of input and output lags on the TDLs (dx = dy) and with the same number
of neurons Nh in each one of the Nl hidden layers. Parameters relative to weight matrices and bias values
θ,θo, θh1, . . . , θhNl
are trained with a variant of the quasi Newton search, called Levenberg-Marquardt
optimization algorithm. This is an algorithm for error backpropagation that provides a good tradeoﬀbetween
the speed of the Newton algorithm and the stability of the steepest descent method . The loss function
to be minimized is deﬁned in Eq. 19.
NARX requires the speciﬁcation of 5 hyperparameters, which are uniformly drawn from diﬀerent intervals. Speciﬁcally, TDL lags are drawn from {2, 3, . . . , 10}; the number of hidden layers Nl is drawn from
{1, 2, . . . , 5}; the number of neurons Nh in each layer is drawn from {5, 6, . . . , 20}; the regularization hyperparameter λ2 in the loss function is randomly selected from {2−1, 2−2, . . . , 2−10}; the initial value η of
learning rate is randomly selected from {2−5, 2−6, . . . , 2−25}.
A total of 500 random conﬁgurations for NARX are evaluated and, for each hyperparameters setting,
the network is trained for 1000 epochs in the validation. In the test phase, the network conﬁgured with
the optimal hyperparameters is trained for 2000 epochs. Also in this case, we discard the ﬁrst 50 network
outputs to get rid of the initial transient phase of the network.
4 
ESN. For the ESN, we used a modiﬁed version of the Python implementation5, provided by Løkse et al.
 . Learning in ESN is fast, as the readout is trained by means of a linear regression. However, the training
does not inﬂuence the internal dynamics of the random reservoir, which can be controlled only through the
ESN hyperparameters.
This means that a more accurate (and computationally intensive) search of the
optimal hyperparametyers is required with respect to the other RNN architectures. In RNNs, the precise,
yet slow gradient-based training procedure is mainly responsible for learning the necessary dynamics and it
can compensate a suboptimal choice of the hyperparameters.
Therefore, in the ESN validation phase we evaluate a larger number of conﬁgurations (5000), by uniformly
drawing 8 diﬀerent hyperparameters from speciﬁc intervals. In particular, the number of neurons in the
reservoir, Nh, is drawn from {400, 450, . . . , 900}; the reservoir spectral radius, ρ, is drawn in the interval
[0.5, 1.8]; the reservoir connectivity Rc is drawn from [0.15, 0.45]; the noise term ξ in Eq. (20) comes from
a Gaussian distribution with zero mean and variance drawn from [0, 0.1]; scaling of input signal ωi and
desired response ωo are drawn from [0.1, 1]; scaling of output feedback ωf is drawn from [0, 0.5]; the linear
regression regularization parameter λ2 is drawn from [0.001, 0.4]. Also in this case, we discarded the ﬁrst 50
ESN outputs relative to the initial transient phase.
7.2. Results on synthetic dataset
In Fig. 16 we report the prediction accuracy obtained by the RNNs on the test set of the three synthetic
problems. The best conﬁgurations of the architectures identiﬁed for each task through random search are
reported in Tab. 1.
Prediction Error (NRMSE)
Figure 16: NRMSE values achieved on the test sets by each
RNN architecture on the three synthetic prediction tasks.
First of all, we observe that the best performing RNN is diﬀerent in each task. In the MG task, ESN
outperforms the other networks. This result conﬁrms the excellent and well-known capability of the ESN
in predicting chaotic time series .
In particular, ESN demonstrated to be the most accurate
architecture for the prediction of the MG system . The ESN achieves the best results also in the MSO
task, immediately followed by ERNN. On the NARMA task instead, ESN performs poorly, while the LSTM
is the RNN that predicts the target signal with the highest accuracy.
5 
Table 1: Optimal RNNs conﬁgurations for solving the three synthetic prediction tasks, MG, NARMA and MSO. The acronyms
in the table are: Nh – number of nodes in the hidden layer; Nl – number of hidden layers; TDL – number of lags on the
tapped delay lines; η – learning rate; λ1 – L1 regularization parameter; λ2 – L2 regularization parameter; OPT – gradient
descent strategy; τf – number of new time steps processed before computing the BPTT; τb – number of time step the gradient
is propagated back in BPTT; pdrop – dropout probability; ρ – spectral radius of ESN reservoir; Rc – percentage of sparsity in
ESN reservoir; ξ – noise in ESN state update; ωi, ωo, ωf – scaling of input, teacher and feedback weights.
RNN Conﬁguration
In each test, NARX struggles in reaching performance comparable with the other architectures.
particular, in NARMA and MSO task the NRMSE prediction error of NARX is 0.53 and 1.99, respectively
(note that we cut the y-axis to better show the remaining bars). Note that, since the NRMSE is normalized
by the variance of the target signal, an error greater than 1 means that the performance is worse than a
constant predictor, with value equal to the mean of the target signal.
It is also interesting to notice that in MSO, ERNN achieves a prediction accuracy higher than GRU and
LSTM. Despite the fact that the MSO task demands a large amount of memory, due to the extremely long
periodicity of the target signal, the two gated architectures (LSTM and GRU) are not able to outperform
the ERNN. We can also notice that for MSO the optimal number of hidden nodes (Nh) is lower than in the
other tasks. A network with a limited complexity is less prone to overﬁt on the training data, but it is also
characterized by an inferior modeling capability. Such a high modeling capability is not needed to solve the
MSO task, given that the network manages to learn correctly the frequencies of the superimposed sinusoidal
Finally, we observe that LSTM and GRU performs similarly on the each task, but there is not a clear
This ﬁnding is in agreement with previous studies, which, after several empirical evaluations,
concluded that it is diﬃcult to choose in advance the most suitable gated RNN to solve a speciﬁc problem
Regarding the gradient descent strategies used to train the parameters in RNN, LSTM and GRU, we
observe in Tab. 1 that Adam is often identiﬁed as the optimal strategy. The standard SGD is selected
only for GRU in the MG task. This is probably a consequence of the lower convergence rate of the SGD
minimization, which struggles to discover a conﬁguration that achieves a good prediction accuracy on the
validation set in the limited amount (400) of training epochs. Also, the Nesterov approach seldom results
to be as the optimal strategy and a possible explanation is its high sensitivity to the (randomly selected)
learning rate. In fact, if the latter is too high, the gradient may build up too much momentum and bring
the weights into a conﬁguration where the loss function is very large. This results in even greater gradient
updates, which leads to rough oscillations of the weights that can reach very large values.
From the optimal conﬁgurations in Tab. 1, another striking behavior about the optimal regularization
procedures emerges. In fact, we observe that in each RNN and for each task, only the L2 norm of the weights
is the optimal regularizer. On the other hand, the parameters λ1 and pdrop relative to the L1 norm and
the dropout are always zero. This indicates that, to successfully solve the synthetic prediction tasks, it is
suﬃcient to train the networks with small weights in order to prevent the overﬁtting.
Finally, we notice that the best results are often found using network with a high level of complexity, in
terms of number of neurons and long windows in BPTT or TDL, for Narx. In fact, in most cases the validation
procedure identiﬁes the optimal values for these variables to be close to the upper limit of their admissible
intervals. This is somehow expected, since a more complex model can achieve higher modeling capabilities,
if equipped with a suitable regularization procedure to prevent overﬁtting during training. However, the
tradeoﬀin terms of computational resources for training more complex models is often very high and small
increments in the performance are obtained at the cost of much longer training times.
7.3. Results on real-world dataset
The highest prediction accuracies obtained by the RNNs on the test set (unseen data) of the real-world
load time series, are reported in Fig. 17. As before, in Tab. 2 we report the optimal conﬁguration of each
RNN for the diﬀerent tasks.
Prediction Error (NRMSE)
Figure 17: NRMSE values achieved on the test sets
by each RNN architecture on the three real-world
STLF problems. Note that scales are diﬀerent for
each dataset
Orange. All the RNNs achieve very similar prediction accuracy on this dataset, as it is possible to see
from the ﬁrst bar plot in Fig. 17. In Fig. 18 we report the residuals, depicted as black areas, between the
target time series and the forecasts of each RNN. The ﬁgure gives immediately a visual quantiﬁcation of
the accuracy, as the larger the black areas, the greater the prediction error in that parts of the time series.
In particular, we observe that the values which the RNNs fail to predict are often relative to the same
interval. Those values represent ﬂuctuations that are particularly hard to forecast, since they correspond to
Table 2: Optimal RNNs conﬁgurations adopted in the three real-world STLF problems. Refer to Tab. 1 for the deﬁnition of
the acronyms in this table.
RNN Conﬁguration
unusual increments (or decrements) of load, which diﬀer signiﬁcantly from the trend observed in the past.
For example, the error increases when the load suddenly grows in the last seasonal cycle in Fig. 18.
In the Orange experiment we evaluate the results with or without applying a log transform to the data.
We observed sometime log-transform yields slightly worse result (∼0.1%), but in most cases the results are
For ERNN SGD is found as optimal, which is a slower yet more precise update strategy and is more
suitable for gradient descent if the problem is diﬃcult. ERNN takes into account a limited amount of past
information, as the window in the BPTT procedure is set to a relatively small value.
Like ERNN, also for GRU the validation procedure identiﬁed SGD as the optimal gradient descent
strategy. Interestingly, L1 regularization is used, while in all the other cases it is not considered. On the
other hand, the L2 regularization parameter is much smaller.
In the optimal NARX conﬁguration, TDL is set to a very small value. In particular, since the regression
is performed only on the last 2 time intervals, the current output depends only on the most recent inputs
and estimated outputs. From the number of hidden nodes and layers, we observe that the optimal size of
the network is relatively small.
Relatively to the ESN conﬁguration, we notice a very small spectral radius. This means that, also in this
case, the network is conﬁgured with a small amount of memory. This results in reservoir dynamics that are
more responsive and, consequently, in outputs that mostly depend on the recent inputs. As a consequence,
the value of input scaling is small, since there is no necessity of quickly saturating the neurons activation.
ACEA. The time series of the electricity load is quite regular except for few, erratic ﬂuctuations. As for
the Orange dataset, RNN predictions are inaccurate mainly in correspondence of such ﬂuctuations, while
they output a correct prediction elsewhere. This behavior is outlined by the plots in Fig. 19, where we
observe that the residuals are small and, in each RNN prediction, they are mostly localized in common time
intervals. From the NRMSE values in Fig. 17, we see that ESN performs better than the other networks.
The worst performance is obtained by NARX, while the gradient-based RNNs yield better results, which are
very similar to each other.
Figure 18: Orange dataset – The plots on the left show the residuals of predictions of each RNN with respect to the ground
truth; black areas indicate the errors in the predictions. The plots on right depict a magniﬁcation of the area in the gray boxes
from the left graphics; the dashed black line is the ground truth, the solid gray line is the prediction of each RNN.
In ERNN and GRU, the optimal regularization found is the L2 norm, whose coeﬃcient assumes a small
value. In LSTM instead, beside the L2 regularization term, the optimal conﬁguration includes also a dropout
regularization with a small probability. The BPTT windows have comparable size in all the gradient-based
The optimal NARX conﬁguration for ACEA is very similar to the one identiﬁed for Orange and is
characterized by a low complexity in terms of number of hidden nodes and layers. Also in this case the
TDLs are very short.
Similarly to the optimal conﬁguration for Orange, the ESN spectral radius assumes a small value, meaning
that the network is equipped with a short-term memory and it captures only short temporal correlations
in the data. The reservoir is conﬁgured with a high connectivity, which yields more homogeneous internal
Figure 19: ACEA dataset – The plots on the left show the residuals of predictions of each RNN with respect to the ground
truth; black areas indicate the errors in the predictions. The plots on right depict a magniﬁcation of the area in the gray boxes
from the left graphics; the dashed black line is the ground truth, the solid gray line is the prediction of each RNN.
GEFCom. This time series is more irregular than the previous ones, as it shows a more noisy behavior that
is harder to be predicted. From Fig. 20 we see that the extent of the black areas of the residual is much larger
than in the other datasets, denoting a higher prediction error. From the third panel in Fig. 17 we observe
larger diﬀerences in the results with respect to the previous cases. In this dataset, the exogenous time series
of temperature plays a key role in the prediction, as it conveys information that are particularly helpful to
yield a high accuracy. The main reason of the discrepancy in the results for the diﬀerent networks may be in
their capability of correctly leveraging this exogenous information for building an accurate forecast model.
From the results, we observe that the gradient-based RNNs yield the best prediction accuracy.
particular, ERNN and GRU generate a prediction with the lowest NRMSE with respect to the target signal.
ESN, instead, obtains considerably lower performance. Like for the syntethic datasets NARMA and MSO,
NARX produces a very inaccurate prediction, scoring a NRMSE which is above 1.
The optimal ERNN conﬁguration consists of only 60 nodes.
For LSTM, the optimal conﬁguration includes only 20 hidden units, which is the lowest amount admitted
in the validation search and SGD is the best as optimizer.
The optimal conﬁguration for GRU is characterized by a large BPTT window, which assumes the maximum value allowed. This means that the network beneﬁts from considering a large amount of past values
to compute the prediction. As in LSTM, the number of processing units is very low. The best optimizer
is Adam initialized with a particularly small learning rate, which yields a slower but more precise gradient
The optimal conﬁguration of NARX network is characterized by a quite large number of hidden nodes
and layers, which denote a network of higher complexity with respect to the ones identiﬁed in the other
tasks. This can be related to the TDL larger values, which require to be processed by a network with greater
modeling capabilities.
For ESN, we notice an extremely large spectral radius, close to the maximum value admitted in the
random search. Consequently, also the value of the input scaling is set to a high number, to increase the
amount of nonlinerarity in the processing units. The output scaling is set close to 1, meaning that the
teacher signal is almost unchanged when fed into the training procedure. A feedback scaling close to zero
means that the feedback is almost disabled and it is not used by the ESN to update its internal state.
8. Conclusions
In this paper we studied the application of recurrent neural networks to time series prediction, focusing
on the problem of short term load forecasting. We reviewed ﬁve diﬀerent architectures, ERNN, LSTM, GRU,
NARX, and ESN, explaining their internal mechanisms, discussing their properties and the procedures for
the training. We performed a comparative analysis of the prediction performance obtained by the diﬀerent
networks on several time series, considering both synthetic benchmarks and real-world short term forecast
problems. For each network, we outlined the scheme we followed for the optimization of its hyperparameters.
Relative to the real-world problems, we discussed how to preprocess the data according to a detailed analysis
of the time series. We completed our analysis by comparing the performance of the RNNs on each task and
discussing their optimal conﬁgurations.
From our experiments we can draw the following important conclusions.
There is not a speciﬁc RNN model that outperforms the others in every prediction problem. The choice
of the most suitable architecture depends on the speciﬁc task at hand and it is important to consider more
training strategies and conﬁgurations for each RNN. On average, the NARX network achieved the lowest
performance, especially on synthetic problems NARMA and MSO, and on the GEFCom dataset.
The training of gradient-based networks (ERNN, LSTM and GRU) is slower and in general more complex,
due to the unfolding and backpropagation through time procedure. However, while some precautions need
to be taken in the design of these networks, satisfactory results can be obtained with minimal ﬁne-tuning
and by selecting default hyperparameters. This implies that a strong expertise on the data domain is not
always necessary.
The results obtained by the ESN are competitive in most tasks and the simplicity of its implementation
makes it an appealing instrument for time series prediction.
ESN is characterized by a faster training
procedure, but the performance heavily depends on the hyperparameters. Therefore, to identify the optimal
conﬁguration in the validation phase, ESN requires a search procedure of the hyperparameters that is more
accurate than in gradient-based models.
Another important aspect highlighted by our results is that the gated RNNs (LSTM and GRU) did not
perform particularly better than an ERNN, whose architecture is much simpler, as well as its training. While
LSTM and GRU achieve outstanding results in many sequence learning problems, the additional complexity
of the complicated gated mechanisms seems to be unnecessary in many time series predictions tasks.
We hypothesize as a possible explanation that in sequence learning problems, such as the ones of Natural
Language Processing , the temporal dependencies are more irregular than in the dynamical systems
underlying the load time series. In natural language for example, the dependency from a past word can
persist for a long time period and then terminate abruptly when a sentence ends. Moreover, there could
Figure 20: GEFCom dataset – The plots on the left show the residuals of predictions of each RNN with respect to the ground
truth; black areas indicate the errors in the predictions. The plots on right depict a magniﬁcation of the area in the gray boxes
from the left graphics; the dashed black line is the ground truth, the solid gray line is the prediction of each RNN.
exist relations between very localized chunks of the sequence. In this case, the RNN should focus on a speciﬁc
temporal segment.
LSTM and GRU can eﬃciently model these highly nonlinear statistical dependencies, since their gating
mechanisms allow to quickly modify the memory content of the cells and the internal dynamics. On the
other hand, traditional RNNs implement smoother transfer functions and they would require a much larger
complexity (number of units) to approximate such nonlinearities.
However, in dynamical systems with
dependencies that decay smoothly over time, the features of the gates may not be necessary and a simple
RNN could be more suitable for the task.
Therefore, we conclude by arguing that ERNN and ESN may represent the most convenient choice in
time series prediction problems, both in terms of performance and simplicity of their implementation and