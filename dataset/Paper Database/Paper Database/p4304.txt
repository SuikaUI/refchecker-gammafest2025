Multi-Pointer Co-Attention Networks for Recommendation
Nanyang Technological University
 
Luu Anh Tuan
Institute for Infocomm Research
 
Siu Cheung Hui
Nanyang Technological University
 
Many recent state-of-the-art recommender systems such as D-ATT,
TransNet and DeepCoNN exploit reviews for representation learning. This paper proposes a new neural architecture for recommendation with reviews. Our model operates on a multi-hierarchical
paradigm and is based on the intuition that not all reviews are
created equal, i.e., only a selected few are important. The importance, however, should be dynamically inferred depending on the
current target. To this end, we propose a review-by-review pointerbased learning scheme that extracts important reviews from user
and item reviews and subsequently matches them in a word-byword fashion. This enables not only the most informative reviews
to be utilized for prediction but also a deeper word-level interaction. Our pointer-based method operates with a gumbel-softmax
based pointer mechanism that enables the incorporation of discrete vectors within differentiable neural architectures. Our pointer
mechanism is co-attentive in nature, learning pointers which are
co-dependent on user-item relationships. Finally, we propose a
multi-pointer learning scheme that learns to combine multiple
views of user-item interactions. We demonstrate the effectiveness
of our proposed model via extensive experiments on 24 benchmark
datasets from Amazon and Yelp. Empirical results show that our
approach significantly outperforms existing state-of-the-art models,
with up to 19% and 71% relative improvement when compared to
TransNet and DeepCoNN respectively. We study the behavior of
our multi-pointer learning mechanism, shedding light on ‘evidence
aggregation’ patterns in review-based recommender systems.
Deep Learning; Recommendation; Collaborative Filtering; Reviewbased Recommender Systems; Information Retrieval; Natural Language Processing
ACM Reference Format:
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. Multi-Pointer Co-Attention
Networks for Recommendation. In KDD ’18: The 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, August 19–
23, 2018, London, United Kingdom. ACM, New York, NY, USA, 10 pages.
 
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from .
KDD ’18, August 19–23, 2018, London, United Kingdom
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5552-0/18/08...$15.00
 
INTRODUCTION
On most e-commerce platforms today, the ability to write and share
reviews is not only a central feature but is also a strongly encouraged act. Reviews are typically informative, pooling an extensive
wealth of knowledge for prospective customers. However, the extensive utility of reviews do not only end at this point. Reviews are
also powerful in capturing preferences of authors, given the rich
semantic textual information that cannot be conveyed via implicit
interaction data or purchase logs. As such, there have been immense interest in collaborative filtering systems that exploit review
information for making better recommendations .
Recent advances in deep learning has spurred on various innovative models that exploit reviews for recommendation .
The intuition is simple yet powerful, i.e., each user is represented
as all reviews he (she) has written and an item is represented by
all reviews that was written for it. All reviews are concatenated to
form a single user (item) document. Subsequently, a convolutional
encoder is employed to learn a single latent representation for the
user (item). User and item embeddings are then matched using a
parameterized function such as Factorization Machines . This
has shown to be highly performant , outperforming a wide
range of traditionally strong baselines such as Matrix Factorization
(MF). Models such as DeepCoNN , TransNets and D-ATT
 are recent state-of-the-art models that are heavily grounded
in this paradigm.
Intuitively, this modeling paradigm leaves a lot to be desired.
Firstly, the naive concatenation of reviews into a single document is
unnatural, ad-hoc and noisy. In this formulation, reviews are treated
indiscriminatingly irregardless of whether they are important or
not. A user’s bad review about a coffee shop should be mostly
irrelevant when deciding if a Spa is a good match. Secondly, user
and item representations are static irregardless of the target match.
For example, when deciding if a coffee shop is a good match for a
user, the user’s past reviews about other coffee shops (and eateries)
should be highly relevant. Conversely, reviews about spas and gyms
should not count. Hence, a certain level of dynamism is necessary.
Finally, the only accessible interaction between user and item is
through a fixed dimensional representation, which is learned via
excessive compression of large user-item review banks into lowdimensional vector representations. For richer modeling of user and
item reviews, deeper and highly accessible interaction interfaces
between user-item pairs should be mandatory.
Recall that reviews were fundamentally written independently,
at different times, and for different products (or by different people).
There should be no reason to squash everything into one long
document if they can be modeled independently and then combined
later. More importantly, a user may write hundreds of reviews over
an extended period of time while an item may effortlessly receive
a thousand of reviews. As such, existing modeling paradigms will
 
KDD ’18, August 19–23, 2018, London, United Kingdom
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui
eventually hit a dead-end. Overall, this work proposes four major
overhauls have to be made to existing models, i.e., (1) reviews should
be modeled independently, (2) not all reviews are equally important
and should be weighted differently, (3) the importance of each
review should be dynamic and dependent on the target match and
finally, (4) user and item reviews should interact not only through
compressed vector representations but also at a deeper granularity,
i.e., word-level.
To this end, we propose a Multi-Pointer Co-Attention Network
(MPCN), a novel deep learning architecture that elegantly satisfies our key desiderata. Our model is multi-hierarchical in nature,
i.e., each user is represented by n reviews of ℓwords each. Subsequently, all user and item reviews (for this particular instance pair)
are matched to determine the most informative reviews. In order to
do so, we design a novel pointer-based co-attention mechanism. The
pointer mechanism extracts the named reviews for direct reviewto-review matching. At this stage, another co-attention mechanism
learns a fixed dimensional representation, by modeling the wordlevel interaction between these matched reviews. This forms the
crux of our review-by-review modeling paradigm. Finally, we introduce a multi-pointer learning scheme that can be executed an
arbitrary k times, extracting multiple multi-hierarchical interactions between user and item reviews.
Our Contributions
In summary, the prime contributions of this paper are as follows:
• We propose a state-of-the-art neural model for recommendation with reviews. Our proposed model exploits a novel
pointer-based learning scheme. This enables not only noisefree but also deep word-level interaction between user and
• We conduct experiments on 24 benchmark datasets. Our
proposed MPCN model outperforms all state-of-the-art baselines by a significant margin across all datasets. Our compared baselines are highly competitive, encompassing not
only review-based models but also state-of-the-art interactiononly models. We outperform models such as Neural Matrix
Factorization (NeuMF) , DeepCoNN , D-ATT 
and TransNet . Performance improvement over Deep-
CoNN, TransNets and D-ATT are up to 71%, 19% and 5%
respectively.
• We investigate the inner workings of our proposed model
and provide insight about how MPCN works under the hood.
Additionally, analyzing the behavior of our pointer mechanism allows us to better understand the nature of the problem. Through analysis of our pointer mechanism, we show
that different problem domains have different patterns of ‘evidence aggregation’, which might require different treatment
and special care.
RELATED WORK
In this section, we identify and review several key areas which are
highly relevant to our work.
Review-based Recommendation
The utility of exploiting reviews for recommendations have been
extensively discussed and justified in many works .
This not only enables a mitigation of cold-start issues but also provides a richer semantic modeling of user and item characteristics.
While relatively earlier works have mainly concentrated efforts
on topic modeling and language modeling approaches ,
the recent shift towards deep learning models is prominent. The
advantages of neural architectures are clear, i.e., not only do these
models dispense with laborious feature engineering altogether, they
are often highly competitive. In many recent works, Convolutional
neural networks (CNN) act as automatic feature extractors, encoding a user (item) into a low-dimensional vector representation. User
and item embeddings are then compared with a matching function.
An earlier neural model, the Deep Co-operative Neural Networks
(DeepCoNN) represents a user as all the reviews that he (she)
has written. Likewise, an item is represented as all the reviews ever
written (by other users) for it. User and item documents are then
encoded with CNNs and passed into a Factorization Machine (FM)
 for matching. It was later argued that DeepCoNN’s competitive
performance exploits the fact that test reviews were leaked (into the
training set) . As such, this reduces the recommendation problem
to resemble a noisy adaptation of standard document-level sentiment analysis. To this end, proposed TransNets, augmenting a
DeepCoNN-like neural network with an additional multi-task learning scheme. More specifically, it learns to transform the penultimate
hidden layer of DeepCoNN into a CNN-encoded representation of
the test review. This signal was found to be useful, improving the
performance on multiple benchmarks.
DeepCoNN and TransNet are relatively simple model architectures. Another recently proposed model, the Dual Attention CNN
model (D-ATT) proposed augmenting CNNs with neural attention. The key idea of neural attention is to emphasize important
segments of documents by weighting each word by a learned attention vector. The final representation comprises a weighted linear
combination of all input embeddings. Two variants of attention
mechanism are proposed, i.e., local and global, both modeling different views of user-item review documents. However, these models
are not without flaws. As mentioned, these models follow the same
paradigm of representing user and item as a giant concatenated
document of all their reviews which suffers inherent drawbacks
such as (1) noise, (2) lack of dynamic target-dependent and (3) lack
of interaction interfaces.
Text Matching and Co-Attentional Models
Our work is closely related to the problem domain of sequence
pair modeling. A wide spectrum of models have been proposed
for modeling relationships between two sequences, e.g., questionanswer , premise-hypothesis which are very similar
to the user-item modeling problem at hand. In these fields, learning representations without fine-grained interaction modeling, i.e.,
absence of interaction interfaces in DeepCoNN, is known to be far
outperformed by new models which utilize co-attentional mechanisms . Co-attentions learn pairwise attention between two
sequences (or modalities ), enabling pair-aware attention
weights to be learned.
Multi-Pointer Co-Attention Networks for Recommendation
KDD ’18, August 19–23, 2018, London, United Kingdom
Recent Advances in Deep Learning
Notably, our network solely relies on attention mechanisms, and
showcases the potential neural architectures that do not use convolutional and recurrent layers. This is inspired by the Transformer
 architecture which uses multi-headed attention, concatenating
outputs of each attention call. Consequently, our proposed model
does not use any recurrent or convolution layers, and solely relies
on attention.
Additionally, our work is characterized by the usage of pointers, which have been popularized by both Pointer Networks .
These networks learn to predict an output token which exists in
the sequence itself, i.e., pointing to a token. The usage of pointers is
primarily motivated for discrete problems and has also been widely
adopted for answer span prediction in question-answering . In
these models, pointers are commonly applied at the last layer and
have no issues since the model optimizes a loss function such as
the cross entropy loss. However, our model requires the usage of
pointers within the network (between layers). As such, a form of
hard attention is required. Due to the non-differentiability of hard
attention, it has been much less popular than the standard soft attention. In order to generate discrete pointers, we utilize the recent
gumbel-softmax trick which is capable of learning one-hot encoded vectors. Notably, the gumbel-softmax was recently adapted
for automatic compositional learning of Gumbel TreeLSTMs , in
which the gumbel-softmax is exploited to adaptively learn to make
merging decisions.
Deep Learning for Recommendation
Factorization-based models were popular standard machine learning baselines for interaction-based recommendation.
Today, we see a shift into deep learning, in which neural models are
claiming state-of-the-art performance . He et al. 
proposed a neural framework that combines a generalized matrix
factorization formulation with multi-layered perceptrons. He and
Chua proposed a neural adaptation of factorization machines.
 proposed recurrent models for sequential modeling of interaction data. Tay et al. proposed a translation-based framework
that exploits neural attention for modeling user-item relations. Li
et al. proposed a encoder-decoder based framework for both
rating prediction and generating tips. A separate class of deep models based on auto-encoders has also been proposed for
recommendation tasks.
OUR PROPOSED MODEL
In this section, we present a layer-by-layer description of our proposed model. The overall model architecture is illustrated in Figure
Input Encoding
Our model accepts two input sequences, a (user) and b (item). Each
input sequence is a list of reviews {r1,r2 · · ·rℓd } where ℓd is the
maximum number of reviews.
Embedding Layer. Each review is a sequence of ℓw words
which are represented as one-hot encoded vectors. For a and b, we
pass all words into an embedding matrix Wd×|V | where V is the
Embeddings
User Reviews
Item Reviews
Review-level
Co-Attention
Max Pooling
Word-level
Word-level
Word-level
Co-Attention
Figure 1: Illustration of proposed model architecture for
Pointer-based Learning (Best viewed in color). This example
illustrates a one pointer example. Review gating mechanism
and multi-pointer learning is omitted for clarity.
vocabulary, retrieving a d dimensional vector for each word. Our
network is hierarchical in nature, i.e., instead of representing all
user (or item) reviews as one long document, we represent each user
(or item) as a sequence of reviews. Each review is then hierarchically
constructed from a sequence of words.
Review Gating Mechanism. Each review is represented
as a sum of its constituent word embeddings to form the vector
x ∈Rd. Intuitively, not all reviews that a user writes and not all
reviews written for a product is important. We design a first-stage
filter, i.e., a review gating mechanism that accepts each review as
an input and controls how much information passes through to the
next level. Given the input x ∈Rℓr ×d, which represents either a or
¯xi = σ(Wдxi) + bд ⊙tanh(Wuxi + bu)
where ⊙is the Hadamard product and σ is the sigmoid activation
function. xi is the i-th review of sequence x. Wд, Wu ∈Rd×d and
bд,bu ∈Rn are parameters of this layer. While the purpose of
the co-attention layer is to extract important reviews, we hypothesize that applying a pre-filter (gating mechanism) helps improve
performance on certain datasets.
KDD ’18, August 19–23, 2018, London, United Kingdom
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui
Review-level Co-Attention
In this layer, the aim is to select the most informative review from
the review bank of each user and item respectively.
Affinity Matrix. Given a list of review embeddings from
each user (a ∈Rℓr ×d) and item (b ∈Rℓr ×d) banks, we calculate an
affinity matrix between them. This is described as follows:
sij = F(ai)⊤M F(bj)
where Md×d and S ∈Rℓr ×ℓr . F(.) is a feed-forward neural network
function with l layers. In practice, l is tuned amongst where
l = 0 reverts Equation (2) to the bilinear form.
Pooling Function. By taking the row and column wise
maximum of the matrix s and using them to weight the original list
of reviews a and b, we are able to derive the standard co-attention
mechanism. This is described as follows:
a′ = (G(max
col (s)))⊤a and
b′ = (G(max
row (s)))⊤b
There are different choices for the pooling operation. Max pooling
is used here because, intuitively it selects the review which has the
maximum influence (or affinity) with all reviews from its partner.
This type of co-attention is known to be extractive, characterized
by its usage of max pooling.
Note that we apply the functionG(.) to maxcol(s) and maxrow(s).
In most applications, G(.) would correspond to the standard softmax function, which converts the input vector into a probability
distribution. The vectors a′,b′ would then be the co-attentional
vector representations. However, in our case, we desire further operations on the selected reviews and therefore do not make use
of these representations. Instead, G(.) has to return a one-hot encoded vector, pointing to the selected reviews which forms the real
objective behind this co-attentional layer. However, the Softmax
function returns a continuous vector, which is unsuitable for our
use-case. The usage of discrete vectors in neural architectures is
known to be difficult, as the arg max operation is non-differentiable.
Hence, we leverage a recent advance, the Gumbel-Max trick, for
learning to point. The next section describes this mechanism.
Review Pointers
We leverage a recent advance, the Gumbel-Softmax , for incorporating discrete random variables in the network.
Gumbel-Max. In this section, we describe Gumbel-Max
 , which facilitates the key mechanism of our MPCN model.
Gumbel-Max enables discrete random variables (e.g., one-hot vectors) to be utilized within an end-to-end neural network architecture. Consider a k-dimensional categorical distribution where class
probabilities p1, · · ·pk are defined in terms of unnormalized log
probabilities π1, · · · πk:
exp(log(πi))
j=1 exp(log(πj))
A one-hot sample z = (z1, · · ·zk) ∈Rk from the distribution can
be drawn by using the following:
1 ,i = arg maxj(log(πj) + дj)
0 , otherwise
дi = −log(−log(ui))
ui ∼Unif orm(0, 1)
where дi is the Gumbel noise which perturbs each log(πi) term
such that the arg max operation is equivalent to drawing a sample
weighted by pi, · · ·pk.
Straight-Through Gumbel-Softmax. In the Gumbel-Softmax,
the key difference is that the arg max function is replaced by the
differentiable softmax function which is described as follows:
exp( log(πi)+дi
j=1 exp( log(πj)+дi
where τ, the temperature parameter, controls the extend of how
much the output approaches a one hot vector. More concretely,
as τ approaches zero, the output of the Gumbel-Softmax distribution becomes cold, i.e., becomes closer to a one-hot vector. In the
straight-through (ST) adaptation, the forward-pass is discretized
by converting the vector output to a one-hot vector via arg max:
1 ,i = arg maxj(yj)
0 , otherwise
However, the backward pass maintains the flow of continuous
gradients which allows the model to be trained end-to-end. This
is useful as we only want important reviews to be selected (i.e.,
hard selection) to be considered in computation of the loss function.
Notably, alternatives such as REINFORCE exist. However, it is
known to suffer from high variance and slow convergence .
Learning to Point. In order to compute a review pointer
(for user and item), we set G(.) in Equation (3) to use the Gumbel
Softmax. However, since we are interested only in the pointer (to
be used in subsequent layers), the pointer is then calculated as:
pa = (Gumbel(max
col (s))) and
pb = (Gumbel(max
By applying pa to a, we select the pa −th review of user a and
pb −th review of item b. The selected reviews are then passed into
the next layer where rich interactions are extracted between these
Word-level Co-Attention
The review-level co-attention smooths over word information as
it compresses each review into a single embedding. However, the
design of the model allows the most informative reviews to be extracted by the use of pointers. These reviews can then be compared
and modeled at word-level. This allows a user-item comparison of
finer granularity which facilitates richer interactions as compared
to simply composing the two review embeddings. Let ¯a, ¯b be the
selected reviews using the pointer learning scheme. Similar to the
review-level co-attention, we compute a similarity matrix between
¯a and ¯b. The key difference is that the affinity matrix is computed
word-by-word and not review-by-review.
wij = F(¯ai)⊤Mw F(¯bj)
where Mw ∈Rd×d and w ∈Rℓw ×ℓw . Next, to compute the coattentional representation of reviews ¯a, ¯b, we take the mean pooling.
¯a′ = (S(avдcol(w)))⊤¯a and
¯b′ = (S(avдrow(w)))⊤¯b
Multi-Pointer Co-Attention Networks for Recommendation
KDD ’18, August 19–23, 2018, London, United Kingdom
where S(.) is the standard Softmax function and F(.) is a standard
feed-forward neural network with l layers. The rationale for using
the average pooling operator here is also intuitive. At the reviewlevel, a large maximum affinity score of a review with respect
to all opposite reviews (even when a low average affinity socre)
warrants it being extracted, i.e., a strong signal needs to be further
investigated. However, at a word-level, max-pooling may be biased
towards identical words and may have a great inclination to act
as a word matching operator. Hence, we want to maintain a stable
co-attentive extractor. Our early empirical experiments also justify
this design. Finally ¯a′ and ¯b′ are the output representations.
Note that, implementation of co-attention layers (review-level
and word-level) is equivalent to only two simple matmul operations
(in Tensorflow). As such, scalability is not really a concern in our
approach since this is quite efficiently optimized on GPUs.
Multi-Pointer Learning
While our objective is to eliminate noisy reviews by the usage of
hard pointers, there might be insufficient information if we point
to only a single pair of reviews. Hence, we devise a multi-pointer
composition mechanism. The key idea is to use multiple pointers
where the number of pointers np is a user-defined hyperparameter.
Our model runs the Review-level Co-Attention np times, with each
generating a unique pointer. Each of the np review pairs is then
modeled with the Word-level Co-Attention mechanism. The overall
output is a list of vectors {¯a′
1, · · · ¯a′np } and {¯b′
1, · · · ¯b′np }. Additionally, we also found it useful to include the sum embedding of all
words belonging to the user (item). This mainly helps in robustness,
in case where user and item do not have any matching signals that
were found by our pointer mechanism. We explore three ways to
compose these embeddings.
• Concatenation - All pointer outputs are concatenated, e.g.,
1; · · · ¯a′np ]. This has implications to the subsequent layers,
especially if np is large. Hence we consider the next two
alternatives.
• Additive Composition - All pointer outputs are summed,
e.g., sum(¯a′
1, · · · ¯a′np ).
• Neural Network - All pointer outputs are concatenated
and passed through a single non-linear layer with ReLU (σr )
activations. e.g., σr (W ([¯a′
1; · · · ¯a′np ]) + b) which maps the
concatenated vector into a d dimensional vector.
Note that this is applied to ¯b′ as well but omitted for brevity. Let
the output of this layer be af and bf . In our experiments, we tune
amongst the three above-mentioned schemes. More details are
provided in the ablation study.
Prediction Layer
This layer accepts af ,bf as an input. The concatenation of [af ;bf ]
is passed into a factorization machine (FM) . FM accepts a realvalued feature vector and models the pairwise interactions between
features using factorized parameters. The FM function is defined
as follows:
F(x) = w0 +
⟨vi,vj⟩xi xj
Multi-Pointer
User Reviews Item Reviews
Neural Network
Multi-Pointer
User Reviews
Item Reviews
Multi-Pointer
Figure 2: Illustration of Neural Network (left) and Additive
(right) based Multi-Pointer Learning.
where x ∈Rk is a real-valued input feature vector. ⟨., .⟩is the
dot product. The parameters {v1 . . .vn} are factorized parameters
(vectors of v ∈Rk) used to model pairwise interactions (xi,xj).
w0 is the global bias and Ín
i=1 wi xi represents a linear regression
component. The output of F(x) is a scalar, representing the strength
of the user-item interaction. The network is trained end-to-end by
minimizing the standard mean squared error loss following .
EMPIRICAL EVALUATION
In this section, we present our experimental setup and empirical
evaluation. Our experiments are designed to answer the following
research questions (RQs):
(1) RQ1 - Does our proposed approach outperform state-of-theart models such as D-ATT and DeepCoNN? How much is
the relative improvement?
(2) RQ2 - What are the impacts of some of the design / architectural choices of MPCN?
(3) RQ3 - What are the effects of the key hyperparameters of our
model (e.g., number of pointers, etc.) on model performance?
(4) RQ4 - Are we able to derive any insight about how MPCN
works by analyzing the behavior of the pointer layer?
We utilize datasets from two different sources which are described
as follows:
(1) Yelp Dataset Challenge - Yelp is an online review platform
for businesses such as restaurants, bars, spas, etc. We use
the dataset from the latest challenge1.
(2) Amazon Product Reviews - Amazon is a well-known Ecommerce platform. Users are able to write reviews for products they have purchased. We use 23 datasets from the Amazon Product Review corpus2 .
In total, we provide model comparisons over 24 benchmark datasets.
For all datasets, we split interactions into training, development
and testing sets. We utilize a time-based split, i.e., the last item of
each user is added to the test set while the penultimate is used
for development. For Amazon, the datasets are preprocessed in a
1 
2 
KDD ’18, August 19–23, 2018, London, United Kingdom
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui
5-core fashion (i.e., each user and item have at least 5 reviews to be
included). Since the datasets can be found in the official webpage,
we do not restate their statistics to save space. For Yelp, we use a
20-core setting, providing a comparison on a denser dataset. We
tokenize the reviews using NLTK and retain words that appear at
least 10 times in the vocabulary. We would like to emphasize that,
when building user and item representations using their respective
reviews, all reviews belonging to interactions from the test and development sets were not included. This is to prevent the problem
from reverting to a sentiment analysis task, albeit noisier .
Compared Methods
We compare against a series of competitive baselines.
• Matrix Factorization (MF) is a standard and well-known
baseline for CF. It represents the user and item rating with
the inner product, i.e., p⊤q.
• Factorization Machines (FM) are general purpose machine learning algorithms that use factorized parameters to
model pairwise interaction within a real-valued feature vector. We concatenate the user-item latent embedding together
and pass it through the FM model.
• Multi-layered Perceptrons (MLP) are strong neural baselines for CF and were used as a baseline in . We use the
same pyramidal scheme of 3 layers.
• Neural Matrix Factorization (NeuMF) is the state-ofthe-art model for interaction-only CF. It casts the MF model
within a neural framework and combines the output with
multi-layered perceptrons.
• Deep Co-Operative Neural Networks (DeepCoNN) 
is a review-based convolutional recommendation model. It
trains convolutional representations of user and item and
passes the concatenated embedding into a FM model.
• TransNet is an improved adaptation of DeepCoNN which
incorporates transform layers and an additional training step
that enforces the transformed representation to be similar
to the embedding of the actual target review.
• Dual Attention CNN Model (D-ATT) is a recently
proposed state-of-the-art CNN-based model that uses reviews for recommendation. This model is characterized by
its usage of two forms of attentions (local and global). A final
user (item) representation is learned by concatenating representations learned from both local and global attentions.
The dot product between user and item representations is
then used to estimate the rating score.
Given our already extensive comparisons against the state-of-theart models, we omit comparisons with models such as HFT ,
Collaborative Topic Regression , Collaborative Deep Learning
(CDL) and ConvMF since they have been outperformed
by the recently proposed DeepCoNN or D-ATT model.
Experimental Setup
The evaluation metric is the well-known (and standard) meansquared error which measures the square error between the rating
prediction and ground truth. We implement all models ourselves in
Tensorflow. All models are trained with Adam with an initial
learning rate of 10−3. We train all models for a maximum of 20
epochs with early stopping (i.e., if model performance does not
improve for 5 epochs) and report the test result from the best performing model on the development set. We found that models tend
to converge before 20 epochs. However, an exception is that the
MF baseline requires many more epochs to converge. As such, we
train the MF model till convergence. For interaction only models,
the embedding size is set to 50. For TransNet and DeepCoNN, the
number of filters is set to 50 and the filter size is 3. For D-ATT, the
global attention layer uses filter sizes of . The word embedding layer is also set to 50 dimensions. We regularize models with
a dropout rate of 0.2 and a fixed L2 regularization of 10−6. Dropout
is applied after all fully-connected and convolutional layers. We
use two transform layers in the TransNet model. All word embeddings are learned from scratch as we found that using pretrained
embeddings consistently degrades performance across all datasets
(and models). The maximum document length is set to 600 words
(20 reviews of 30 tokens each) which we empirically found to be a
reasonable length-specific performance bound. We assign a special
delimiter token to separate reviews within a user (item) document
for DeepCoNN, TransNet and D-ATT. If FM is used, the number of
factors is set to 10. For our proposed model, the number of pointers
p is tuned amongst {1, 3, 5, 8, 10}. On most datasets, the optimal
performance is reached with 2 −3 pointers.
Experimental Results
Table 1 reports the results of our experiments. Firstly, we observe
that our proposed MPCN is the top performing model on all 24
benchmark datasets. This ascertains the effectiveness of our proposed model and clearly answers RQ1. MPCN consistently and
significantly outperforms DeepCoNN, TransNet and D-ATT, which
are all recent competitive review-based methods for recommendation. The relative improvement is also encouraging with gains
of up to 71% (DeepCoNN), 19% (TransNet) and 5% (D-ATT). On
majority of the datasets, performance gains are modest, seeing an
improvement of 1% −3% for most models. Notably, the average
percentage improvement of MPCN over DeepCoNN is 16%. The
average performance gain over TransNet and D-ATT is a modest
3.2% and 2.2% respectively.
Pertaining to the relative ranking of the review-based models,
our empirical evaluation reaffirms the claim of , showing that
TransNet always outperforms DeepCoNN. However, the relative
ranking of D-ATT and TransNet switches positions frequently.
Notably, TransNet uses the test review(s) as an additional data
source (albeit as a training target) while D-ATT does not make
use of this information. The additional training step of TransNet is
actually quite effective and hypothetically could be used to enhance
D-ATT or MPCN as well. However, we leave that for future work.
Next, the performance of interaction-only models (MF, FM, etc.)
is consistently lower than review-based models (e.g., DeepCoNN).
The relative performance of all interaction models is generally quite
inconsistent over various datasets. However, one consistent fact is
that MF performs worse than other models most of the time. The
top scoring interaction model often switches between FM and MLP.
Finally, we give readers a sense of computational runtime. We
provide an estimate that we found generally quite universal across
multiple datasets. Let t be the runtime of DeepCoNN, the runtime of
Multi-Pointer Co-Attention Networks for Recommendation
KDD ’18, August 19–23, 2018, London, United Kingdom
Interaction-based
Review-based
Improvement (%)
Instant Video
Instruments
Digital Music
Patio / Lawn
Gourmet Food
Automotive
Pet Supplies
Office Products
Android Apps
Tools / Home
Video Games
Toys / Games
Sports / Outdoors
Kindle Store
Home / Care
CDs / Vinyl
Movies / TV
Electronics
Table 1: Performance comparison (mean squared error) on 24 benchmark datasets. The best performance is in boldface.
∆DC, ∆T N , ∆DA are the relative improvements (%) of MPCN over DeepCoNN (D-CON), TransNet (T-NET) and D-ATT respectively. MPCN achieves state-of-the-art performance, outperforming all existing methods on 24 benchmark datasets.
MPCN p = 1 is approximately ≈0.4t. MPCN with 2 and 3 pointers
are 0.8t and 1.2t respectively. TransNet and D-ATT run at ≈2t. On
medium size datasets (e.g., Amazon Beauty), t ≈40s on a GTX1060
GPU (batch size is 128). We found that if popt ≤2, then MPCN is
faster than DeepCoNN. While popt ≤5 is the threshold for being
equal with D-ATT and TransNet in terms of runtime.
HYPERPARAMETER & ABLATION
In this section, we study the impact of key hyperparameter settings
and various architectural choices on model performance.
Ablation Analysis
We study the impacts of various architectural decisions on model
performance (RQ2). Table 2 reports an ablation analysis conducted
on the development sets of four benchmark datasets (Beauty, Office,
Musical Instruments (M-Instr) and Amazon Instant Video (Inst-Vid)).
We report the results of several different model variations. We first
begin describing the default setting in which ablation reports are
deviated from. In the default setting, we use the standard model
with all components (review gates, word-level co-attention and FM
prediction layer). The Multi-Pointer aggregation (aggr) is set to use
a neural network (single layer nonlinear transform). The number
of layers in the co-attention layer is set to l = 1.
We report the validation results from 8 different variations, with
the aims of clearly showcasing the importance of each component.
In (1), we remove the review gating mechanism. In (2), we replace
the FM with the simple inner product. In (3-4), we investigate the
effects of different pointer aggregation (aggr) functions. They are
the concatenate and additive operators respectively. In (5-6), we
set l = 0 (remove layer) and l = 2. In (7), we remove the word
level co-attention layer. In this case, the representation for user and
item is simply the pointed review embedding. In (8), we remove
the review-level co-attention (RLCA). This variation is no longer
‘hierarchical’, and simply applies word-level co-attention to user
and item reviews.
Architecture
(0) Default
(1) Remove Gates
(2) Remove FM
(3) Agrr (Concat)
(4) Aggr (Additive)
(5) set l = 0
(6) set l = 2
(7) Remove WLCA
(8) Remove RLCA
Table 2: Ablation analysis (validation MSE) on four datasets.
KDD ’18, August 19–23, 2018, London, United Kingdom
Yi Tay, Luu Anh Tuan, and Siu Cheung Hui
Firstly, we observe the default setting is not universally the best
across four datasets. As mentioned, the review gating mechanism
helps in 3 out of 4 presented datasets. In the Office dataset, removing the review gating layer improves performance. We found this
to be true across most datasets, i.e., the review gating mechanism
helps more often than not, but not always. The impacts of removing
FM is quite easily noticed, leading to huge performance degradation on the M-Instr dataset. Deprovement on Inst-Vid and Office
is also significant. On the other hand, removing FM marginally
improved performance on Beauty. We also discovered that there is
no straightforward choice of aggr functions. Notably, the relative
ranking of all three variants (concat, additive and neural network)
are always interchanging across different datasets. As such, they
have to be tuned. We also noticed that the choice of l = 1 is safe
across most datasets, as increasing or decreasing would often lead to
performance degradation. Finally, removing the WLCA and RLCA
consistently lowered performance on all datasets, which ascertains
the effectiveness of the WLCA layer. Notably, removing RLCA
seems to hurt performance more, which signifies that modeling at
a review-level is essential.
Effect of Number of Pointers
Table 3 reports the effect of varying pointers on performance (RQ3).
We use 4 datasets of varying sizes as an illustrative example (Patio,
Automotive, Sports and Video Games). The datasets shown are sorted
from smallest to largest in terms of number of interactions. Clearly,
we observe that the optimal number of pointers varies across all
datasets. We found this to be true for the remainder datasets that are
not shown. This seems to be more of a domain-dependent setting
since we were not able to find any correlation with dataset size. For
most datasets, the optimal pointers falls in the range of 1 −3. In
exceptional cases (Video Games), the optimal number of pointers
Automotive
Video Games
Table 3: Validation MSE on various datasets when varying
the number of pointers. The best result is in boldface. The
optimal number of pointers is domain-dependent.
IN-DEPTH MODEL ANALYSIS
In this section, we present several insights pertaining to the inner
workings of our proposed model. This aims to answer RQ4.
What are the pointers pointing to?
In this section, we list some observations by analyzing the behavior
of the MPCN model. Firstly, we observed that pointers react to
aspects and product sub-categories. In many cases, we observed
that the pointer mechanism tries to find the most relevant review
written by the user for a given product. If the target item is a phone
case, then our model tries to find a review written by the user
which is directed towards another phone case product. Intuitively,
we believe that this is trying to solicit the user’s preferences about
a type of product. We provide some qualitative examples in Table
4. Consider the context of video games, it finds that the user has
written a review about rpg (roleplaying games). At the same time, it
finds that the item review consists of a (positive) review of the item
being a good rpg game. As a result, it surfaces both reviews and
concurrently points to both of them. This follows suit for the other
examples, e.g., it finds a matching clue of puzzle games (turn-based)
in the second example. The last example is taken from the Gourmet
Food dataset. In this case, it discovers that the user likes cocoa,
and concurrently finds out that the product in question has some
relevant information about chocolate.
Behavior of Multi-Pointer Learning
In this section, we study the behavior of our multi-pointer mechanism. First and foremost, this serves as another sanity check and
to observe if multi-pointers are really necessary, i.e., if pointers
are not pointing all to the same reviews. Hence, this section aims
to provide better insight into the inner workings of our proposed
model. We trained a MPCN model with four pointers. A quick observation is that all four pointers point to different reviews (given
the same user item pair). This is automatic, and does not require
any special optimization constraint (such as explicitly enforcing the
model to choose different reviews through an extra optimization
term). Moreover, we analyze the affinity matrix from the reviewlevel co-attention. Figure 3 shows the affinity matrix for pointers
one to four.
10 11 12 13 14 15 16 17 18 19 20
10 11 12 13 14 15 16 17 18 19 20
10 11 12 13 14 15 16 17 18 19 20
10 11 12 13 14 15 16 17 18 19 20
Figure 3: Visualisation of Review-level Co-Attention. Matching patterns differ significantly across multiple calls, hence
generating different pointers.
Secondly, it is also intuitive that it is not absolutely necessary
for MPCN to always point to unique reviews given the same useritem pair. We observed a relatively significant one-to-many pointer
Multi-Pointer Co-Attention Networks for Recommendation
KDD ’18, August 19–23, 2018, London, United Kingdom
User Review
Item Review
game is really beautiful! coolest rpg ever...
..a gift for a friend who really loves rpg games. he
really loved it!
.. game is completely turned based, so you have time
to ponder you actions..
..is a great and engaging puzzler game but wasn’t too
challenging
just love this little guy ... phone is reasonably easy to
put in and take out
..case really fits the 5s like a glove..
is a nice charger..but after a few momths it wasn’t
charging...
it is clearly a used or refurbished battery..
cocoa is a wonderful, rich tasting, not overly sweet
product ..
used to eat the dark and milk chocolate, and then i
tried this and can’t explain how good they are
Table 4: Excerpts from top matching User and Item reviews extracted by MPCN’s pointer mechanism.
(1) All unique
(2) 1 Repeated
(3) All Repeated
(4) One-to-Many
Table 5: Analysis of Multi-Pointer Behavior of MPCN on five datasets using np = 3.
pattern on top of the usual one-to-one pattern. In this case, the same
review for user (item) is being matched with n (many) different
reviews from the item (user). This is also observed to be dataset /
domain dependent. In a small minority of cases, all pointers pointed
to the same reviews constantly (all repeated condition). However,
this is understandable as there is just insufficient information in the
user and item review bank. Additionally, we analyzed a small sample
from the test set, determining if any of the following conditions
hold for each test case.
Table 5 reports the percentages of test samples which falls into
each category. We report results on five datasets Digital Music (D-
Music), Android Apps (Apps), Video Games (V-Games), Gourmet Food
(Food) and Yelp17. Here, we observe that pointer behavior is largely
influenced by domain. The first three are concerned with electronic
domains while the last two are more focused on food (and restaurants). We clearly observe that Food and Yelp have very similar
pointer behavior. In general, the electronic domains usually make
an inference using a fewer subsets of reviews. This is made evident
by the high one-to-many ratio which signifies that there is often
one important review written by the user (or item) that contributes
more (and needs to be matched with multiple opposing reviews).
Conversely, the food domains require more evidences across multiple reviews. We believe this is one of the biggest insights that our
work offers, i.e., shedding light on how evidence aggregation works
(and differs across domains) in review-based recommendation.
CONCLUSION
We proposed a new state-of-the-art neural model for recommendation with reviews. Our proposed Multi-Pointer Co-Attention
Networks outperforms many strong competitors on 24 benchmark
datasets from Amazon and Yelp. We conduct extensive analysis
on the inner workings of our proposed multi-pointer learning
mechanism. By analyzing the pointer behavior across multiple
domains, we conclude that different domains (such as food-related
and electronics-related) have different ‘evidence aggregation’ patterns. While our model dynamically handles this aspect, we believe
this warrants further investigation.
ACKNOWLEDGEMENTS
The authors thank anonymous reviewers of KDD 2018 for their
time and effort to review this paper.