Received April 8, 2019, accepted May 7, 2019, date of publication May 14, 2019, date of current version May 31, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2916793
Machine Learning for Forecasting Mid-Price
Movements Using Limit Order Book Data
PARASKEVI NOUSI1, AVRAAM TSANTEKIDIS
1, NIKOLAOS PASSALIS
ADAMANTIOS NTAKARIS2, JUHO KANNIAINEN
2, ANASTASIOS TEFAS1, (Member, IEEE),
MONCEF GABBOUJ
2, (Fellow, IEEE), AND ALEXANDROS IOSIFIDIS
3, (Senior Member, IEEE)
1Department of Informatics, Aristotle University of Thessaloniki, 541 24 Thessaloniki, Greece
2Faculty of Information Technology and Communication Sciences, Tampere University, 33720 Tampere, Finland
3Department of Engineering, Electrical and Computer Engineering, Aarhus University, 8000 Aarhus, Denmark
Corresponding authors: Paraskevi Nousi ( ) and Avraam Tsantekidis ( )
This work was supported by the H2020 Project BigDataFinance under Grant MSCA-ITN-ETN 675044, and in part by the Training for Big
Data in Financial Research and Risk Management.
ABSTRACT Forecasting the movements of stock prices is one of the most challenging problems in ﬁnancial
markets analysis. In this paper, we use machine learning (ML) algorithms for the prediction of future
price movements using limit order book data. Two different sets of features are combined and evaluated:
handcrafted features based on the raw order book data and features extracted by the ML algorithms, resulting
in feature vectors with highly variant dimensionalities. Three classiﬁers are evaluated using combinations
of these sets of features on two different evaluation setups and three prediction scenarios. Even though the
large scale and high frequency nature of the limit order book poses several challenges, the scope of the
conducted experiments and the signiﬁcance of the experimental results indicate that the ML highly beﬁts
this task carving the path towards future research in this ﬁeld.
INDEX TERMS Limit order book, feature extraction, mid price forecasting.
I. INTRODUCTION
Forecasting of ﬁnancial time series is a very challenging
problem and has attracted scientiﬁc interest in the past few
decades. Due to the inherently noisy and non-stationary
nature of ﬁnancial time series, statistical models are unsuitable for the task of modeling and forecasting such data.
Thus, the nature of ﬁnancial data necessitates the utilization
of more sophisticated methods, capable of modeling complex non-linear relationships between data, such as Machine
Learning (ML) algorithms.
Early Machine Learning approaches to this problem
included shallow Neural Networks (NNs) , , and Support Vector Machines (SVMs) – . However, the lack of
appropriate training and regularization algorithms for Neural
Networks at the time, such as the dropout technique ,
rendered them susceptible to over ﬁtting the training data.
Support Vector Machines were deemed as better candidates
for this task, as their solution implicitly involves the generalization error.
The associate editor coordinating the review of this manuscript and
approving it for publication was Chao Tong.
The development of effective and efﬁcient training algorithms for deeper architectures , in conjunction with
the improved results such models presented, steered scientiﬁc interests towards Deep Learning techniques in many
domains. Deep Learning methods are capable of modeling
highly non-linear, very complex data, making them suitable
for application to ﬁnancial data , as well as time series
forecasting .
Furthermore, ML techniques which perform feature
extraction may uncover robust features, better-suited to the
speciﬁc task at hand. Autoencoders , are Neural Networks which learn new features extracted from the original
input space, which can be used to enhance the performance
of various tasks, such as classiﬁcation or regression. Bagof-Features (BoF) models comprise another feature extraction method that can be used to extract representations of
objects described by multiple feature vectors, such as timeseries , . The process of feature extraction is of major
importance as it can signiﬁcantly affect the performance of
the used machine learning algorithms.
In this paper we utilize various Machine Learning algorithms and data preprocessing techniques for the prediction of
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
future price movements of stocks. The dataset we use comes
from the temporal progression of the limit order book (LOB)
which is the highest resolution possible one can observe
the stock markets in. The characteristics of LOB data are
discussed in detail in Section IV-A, along with a comprehensive description of expertly hand-crafted features that can be
extracted from a LOB.
The main contribution of this paper is a very extensive
study into the signiﬁcance of the information provided by
the limit order book for the task of predicting future mid
price movements of stocks. Handcrafted features, founded on
ﬁnancial expertise, and features extracted by sophisticated
Machine Learning techniques are combined and utilized
towards this purpose. Three scenarios are assessed regarding
the span of time for which predictions are made: a scenario
where the movement of the mid price of the immediately
succeeding sample in time is predicted, one where the average
mid price movement of the next ﬁve samples is predicted and,
last, one where the average movement of the mid price for
the next ten samples is predicted. Multiple ML classiﬁers are
evaluated on two experimental setups for all three scenarios
and for various combinations of the extracted features, taking
into consideration the class imbalance which accompanies
the data as well as the real-time requirements of High Frequency Trading. The results achieved indicate that the LOB
contains valuable information which, in conjunction with
various Machine Learning algorithms, can give meaningful
insight into the stock market trend and lead the models to
make accurate predictions, without any external human intervention. Although Machine Learning techniques have been
widely used to model other types of ﬁnancial data , ,
only recently have they begun to be applied and evaluated on
LOB data .
The rest of the paper is organized as follows. Section II
presents previous related work upon which we build.
A description of the data contained in a limit order book
and used for this work is presented in Section III. The ML
algorithms used to extract features from the original ﬁnancial
data are described in Section IV, followed by the classiﬁcation algorithms used for the prediction of the mid price
direction. In Section V the experimental setup and results
are described and analyzed. Finally, Section VI summarizes
the conclusions drawn from studying the application of the
described Machine Learning algorithms to ﬁnancial data.
II. RELATED WORK
The dynamics of the high frequency limit order book comprise a challenging ﬁeld of study which has been investigated
in past literature. An extensive survey on stochastic models
and statistical techniques for modeling high frequency limit
order book data can be found in , highlighting the inadequacies of statistical models as well as the need for more
complex models, such as Machine Learning ones. Statistical
models often make unrealistic assumptions about the distribution of the data, such as assuming a Poisson distribution
of limit order events. Machine Learning techniques make
no assumptions on the distribution of the data. Furthermore,
the distribution of limit order events changes rapidly, not just
from one day to the next but also within the same day. It is
thus very challenging for statistical models which typically
assume stationary signals to be used effectively used for
modeling limit order book data. The drawbacks of statistical models and advantages of Machine Learning approaches
have also been examined in . An extensive analysis of
high frequency ﬁnancial data can be found at , and the
dynamics of limit order books are explained in detail in 
Machine learning has been used very extensively to analyze the ﬁnancial market from many different aspects. NNs
and SVMs have been two of the most popular techniques for
this task, as indicated by a number of works, e.g., – ,
which use models based on these architectures. In an
SVM model is trained to predict the direction of the movement of the NIKKEI 225 index. An SVM and Multilayer Perceptron (MLP) comparison can be found in , where daily
direction of the price of the Korea Composite stock index is
predicted, using 12 different indexes as input features. Using
two different windows, one long and one short term, in order
to capture both the trend and higher frequency information
of the time series of treasury bond returns, utilizes an
MLP model and attempts to predict the movement of the
future bonds returns. In a feedforward neural network
the structure of which is determined in a data-driven manner
is used for mid-price direction predictcon. In the authors
compare the performance of SVMs, MLPs and Radial Basis
Function (RBF) NNs in predicting price changes of future
asset contracts. In a tensor-based regression model is
used, which is further extended for tensor-based NN classiﬁcation in . In different dataset sizes are used to
train a neural network model and it is shown that using too
many samples that span too far into the past can degrade
the prediction quality. The evaluation comparison is based
on the proﬁt that each model produces. In Deep Portfolio
Theory , the authors use autoencoders to optimize the
performance of a portfolio and beat the proﬁt benchmarks
such as the biotechnology IBB Index.
In a similar fashion to this work, uses several handcrafted time sensitive and insensitive features, extracted from
the limit order book. These features include bid-ask spreads
and mid prices, price differences, mean prices and volumes,
along with derivatives of the price and volume, average and
relative intensity indicators, totaling to 144 different features.
However, in the proposed methods are evaluated on a
very small dataset that contains about 400,000 order book
rows. In contrast, in our case a large-scale dataset that contains information for 10 days and 5 stocks is used, with the
raw data being more than 4 million samples.
To the best of our knowledge, this is the ﬁrst extensive study on using different Machine Learning techniques,
including feature extraction methods and various classiﬁers,
for a stock mid price prediction problem using on large
scale high frequency limit order book data. The scope of the
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
conducted experiments and the signiﬁcance of the observed
results indicate that Machine Learning highly beﬁts this task.
The presented baseline results carve the path towards future
research in this ﬁeld that can potentially achieve even more
accurate and signiﬁcant predictions. To this end, we provide
strong, yet easy to use and tune, machine learning baselines, as well as guidelines on how they should be applied
on limit order book data. In contrast to other studies, such
as – , – , that employ advanced Deep Learning architectures, e.g., recurrent and convolutional neural
networks, which require exhaustive hyper-parameter tuning,
this paper focuses on simpler and easier to use traditional
machine learning approaches. Having solid machine learning
baselines is an important step towards developing more powerful Deep Learning models, since they provide a valuable
tool for validating the more advanced models. To this end,
we also conducted rigorous tests and statistical validation of
the presented methods to demonstrate the predictive capabilities of the developed models.
III. HIGH FREQUENCY LIMIT ORDER BOOK
In ﬁnancial equity markets a limit order is a type of order
to buy or sell a speciﬁc number of shares within a set price.
For example, a sell limit order (ask) of $10 with volume
of 100 indicates that the seller wishes to sell the 100 shares for
no less that $10 a piece. Respectively, a buy limit order (bid)
of $10 means that the buyer wishes to buy a speciﬁed amount
of shares for no more than $10 each. An in-depth survey of
the properties of the LOB can be found in .
Consequently, the order book, which contains the above
information, has two sides: the bid side, containing buy orders
with prices pb(t) and volumes vb(t), and the ask side, containing sell orders with prices pa(t) and volumes va(t). The orders
are sorted on both sides based on the price. On the bid side
b (t) is the highest available buy price and on the ask side
a (t) is the lowest available sell price.
Whenever a bid order price exceeds an ask order price
b (t) > p(j)
a (t), where p(i)
b (t) is the i-th element of the bid side
at time step t and p(j)
a (t) is the j-th element of the ask side at the
same time step, they ‘‘annihilate’’, executing the orders and
exchanging the traded assets between the investors. Typically,
an order that leads to an immediate execution is called a
market order. In this case, an investor makes an order to
buy or sell a speciﬁc number of shares immediately, at the
best available current price. Since the orders do not usually
have the same requested volume, the order with the greater
size remains in the order book with the remaining unfulﬁlled
Several tasks arise from this data, ranging from the prediction of the price trend and the regression of the future value of
a metric, e.g., volatility, to the detection of anomalous events
that cause price jumps, either upwards or downwards. These
tasks can lead to interesting applications, such as protecting the investments when market conditions are unreliable,
or taking advantage of such conditions to create automated
trading techniques for proﬁt.
The data used in this work consists of 10 orders for each
side of the LOB. Each order is described by 2 values, the price
and the volume, yielding a total of 40 values for each time
step The stock data, provided by Nasdaq Nordic, come from
the Finnish companies Kesko Oyj, Outokumpu Oyj, Sampo,
Rautaruukki and Wärtsilä Oyj. The time period used for
collecting that data ranges from the 1st to the 14th June 2010
(only business days are included), and the data is provided
by the Nasdaq Nordic data feeds , . The dataset is
made up of 10 days for 5 different stocks and the total number
of messages is about 4.5 million with equally many separate
IV. PROPOSED METHODOLOGY
In this Section, we brieﬂy review the data preprocessing and
the feature extraction procedure. Then, we introduce two feature learning methods that are used to learn low-dimensional
features using the extracted handcrafted features. Finally,
we introduce the classiﬁcations methods that are used to
predict the mid price movements.
A. HANDCRAFTED FEATURES
The raw order book data is ﬁrst preprocessed by removing the
unnecessary messages from the exchange, e.g., event messages, and then the features proposed in are extracted.
More speciﬁcally, ﬁrst, a basic set of features which includes
the prices and volumes for every level of the ask and bid
side of the order book is extracted. This information yields
40 values at each time step. Then, time-insensitive features
describing the spread, mid-price, price and accumulated price
differences between the bid and ask orders of each depth
level, and price and volume spreads are extracted. Finally,
time-sensitive features are extracted corresponding to the
average intensity for trades, orders, cancellations, deletion,
execution of visible limit orders and execution of hidden limit
orders. This set of features also includes the price and volume
average values at each level of the LOB, the average intensity
per trading type as well as comparisons between the intensities and limit activity acceleration (derivatives of average
intensities). Because of the non-linear nature of time in the
LOB data, we follow an event-based inﬂow. The interested
reader is referred to , for a more detailed description
of the extracted features. Note that one feature vector is
extracted for every 10 limit order events that change the LOB,
effectively subsampling the data by a factor of 10. The total
number of the collected limit order events is about 4.5 million,
leading to a total of 453.975 extracted feature vectors.
Instead of using only the feature vector extracted from
the current time step, as proposed in , we propose three
additional ways to extract representations capable of capturing more temporal information. Therefore, the following
four different feature vectors are produced and used as inputs
to the evaluated models, using a time sliding window of
1) A single feature vector with 144 values as described
above, corresponding to the last sample in the sliding
window (abbreviated as last).
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
FIGURE 1. Representations extracted by using a sliding window of length
5 on the handcrafted feature vectors. The last⊕mean representation is
the concatenation of the depicted last and mean representations.
2) The mean of the 5 samples currently in the window,
which is also a 144-dimensional vector (abbreviated as
3) The concatenation of the last sample and the mean of all
5 samples in the window, yielding a 288-dimensional
feature vector (abbreviated as last⊕mean – the circled
plus symbol is used to denote the concatenation operation).
4) The concatenation of all 5 samples, yielding a
720-dimensional
(abbreviated
Figure 1 illustrates the process of obtaining the above
representations.
Using this sliding window approach allows for temporal
information to be incorporated into the representations, which
are subsequently used to predict the movement of the stock’s
mid price. By averaging over the ﬁve entries contained in the
window, some of the inherent noise of the features is alleviated. Concatenating different representations, e.g., the last
feature vector and the average of the last 5 feature vectors,
allows for more temporal information to be introduced in the
ﬁnal feature vector.
Since normalization is crucial for most ML techniques, we normalize the extracted features using z-score
standardization:
x = xraw −¯xraw
where xraw is the vector of values to be normalized, ¯xraw is
the mean vector of the data and σraw is the standard deviation
vector of the data. Both the mean and the standard deviation
are computed element-wise.
B. PREDICTION LABELS
Of the values accompanying LOB data, the tick price, which
is the price of the last executed trade, typically varies wildly
between the two sides of the margins, introducing great
amounts of noise to the prediction labels. The so-called market micro-structure noise can be partially reduced by using
mid-prices, i.e. the mean of the best ask and best bid prices,
instead of transaction prices. Thus, in this paper, we considered mid-prices as the stock price observations. Another
advantage of using mid-prices instead of transaction prices is
that they are observable every time as long as there are orders
on both bid and ask sides while transaction prices are updated
only at transactions. The mid price p(t) is deﬁned as:
p(t) = p(1)
a (t) + p(1)
where p(1)
are the best bid and best ask price. Note
that the mid price of the stock is one of the features in the
set of 144 features derived by following the feature crafting
process described in .
An averaging ﬁlter is applied over the past Nβ values
(including the current time step t) of the mid price of the
samples to reduce the impact of the noise in the signal:
p(t −i + 1)
In our experiments we use Nβ = 9. We have also evaluated
the models for different values of Nβ, 5, 7 and 11, obtaining
similar results.
We compare this price at time t with the mean of the succeeding Nα smoothed mid prices (not including the current
time step t):
For each sample, the movement of the mid price is deﬁned
by comparing the current smoothed mid price mβ(t) to the
mean of the next Nα smoothed mid prices mα(t). Small
changes of the price should be considered insigniﬁcant.
To this end, we introduce a parameter γ to control the threshold a price movement must surpass in order to be considered
as either upwards or downwards. Thus, the label l(t) to be
predicted for time step t is computed as:
if mα(t) > mβ(t) · (1 + γ )
if mα(t) < mβ(t) · (1 −γ )
In other words, we treat the problem of forecasting the
mid price movements of stocks as a classiﬁcation problem
with three possible outcomes: upwards movement, downwards movement and no change with labels 1, −1, and 0
respectively as deﬁned in Equation (5). Three sets of such
labels are generated through the above process, for Nα =
1, 5 and 10 and γ = 0.0001, 0.0002 and 0.0003 respectively.
All the classiﬁers were extensively evaluated using these
three different prediction horizons.
In practice, the mid price of a stock very rarely remains
the same through consecutive time steps, thus for γ = 0
the majority of the mid price movements are categorized as
either upwards of downwards, depleting the no-change class
of samples. The value of the γ parameter can be crucial
as even small ﬂuctuations signiﬁcantly affect the balance
between the three possible classes. As γ increases, so does
the number of samples belonging to the no-change class.
Although raising this threshold would lead to a more balanced
problem, i.e., all three classes would contain about the same
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
number of samples, high values of γ are undesirable as they
allow upwards and downwards movements to be categorized
as not having changed even though the movement could be
signiﬁcant. Choosing a meaningful value for γ constitutes a
trade-off between the balance among classes and the meaningfulness of the produced labels.
The values used for γ are minuscule, which is meaningful since larger values would deprive the upwards and
downwards classes of samples. On the other hand, smaller
values would introduce a lot of noise in these classes by
causing insigniﬁcant changes to be considered signiﬁcant.
Nonetheless, the selected values still produce an imbalanced
dataset with most of the samples being classiﬁed as not
having changed.
The resulting class imbalance constitutes a problem which
must be taken into consideration by the ML algorithms used
for the classiﬁcation task. In this work, we deal with this problem by introducing class weights inversely proportional to
the number of samples in each class. The interested reader is
referred to , and , for a detailed review of techniques
that can be used to deal with the problem of class imbalance.
C. FEATURE LEARNING
Apart from the four handcrafted feature vectors proposed in
the previous subsection, two feature learning methods were
deployed and evaluated in our experiments. The methods
and the details of their application to the problem at hand
are described below. The extracted representations are used
as inputs to the evaluated classiﬁers, on their own or in
combination with the described representations created from
the handcrafted features.
1) AUTOENCODERS
Autoencoders (AEs) are neural networks which map their
input data to itself, through multiple levels of non-linear
neurons , , . Thus, the input and output layers
consist of as many neurons as the dimension of the data. Such
networks are comprised of an encoding part, which maps
the input to an intermediate representation, and a decoding
part, which maps the intermediate representation learned to
the desired output and is symmetrical to the encoding part
layer-wise.
Typically, an AE is used for dimensionality reduction
as well as feature extraction, which means that the intermediate representation learned is lower-dimensional than
the input data. The layers of both parts of the network
l = 1, . . . , lenc, . . . , L, where lenc is the encoding layer, are
accompanied by weights W(l) which multiply each layer’s
input to produce an output. A bias term b(l) is also added to
the output of the neuron, and a non-linearity s(·) called the
activation function of the neuron is applied to this output to
produce the neuron’s activation value. The output x(l)
out of the
l-th layer is given by:
out = s(W(l)x(l)
in + b(l))
where x(l)
in is the input to the l-th layer, which is equal to the
output of the previous layer, or:
where x ∈RD denotes an input sample. The last representation is used as the input to the AE, i.e., the original input
dimension is D = 144.
The network’s parameters can be learned using the
well-known backpropagation algorithm , combined with
an optimization method, such as Stochastic Gradient Descent
(SGD) , with the ﬁnal objective being the optimization
of the network’s loss function. Speciﬁcally for autoencoders,
their objective is to minimize the reconstruction error, i.e., the
squared l2-norm between the network’s output x(L)
out and the
desired output x, which is the same as the network’s input:
ℓ= ∥x −x(L)
The objective of the network is to minimize the mean of errors
over all data samples.
As the training process converges, the activations of the
intermediate layers can be used as learned feature representations of the input data. Let xenc denote the output of the lenc
xenc = x(lenc)
Then xenc can be used as the low-dimensional representation
of the data in the subsequent classiﬁcation task.
2) BAG-OF-FEATURES
Bag-of-Features (BoF) models , , allow for extracting constant-length representations of samples that consist of
multiple feature vectors, e.g., feature vectors extracted from
various locations of an image or from various time points of
a time series, such as the 5 feature vectors contained in the
current sliding window. BoF models, originate from and comprise an extension of the standard Bag-of-Words model ,
which uses word frequencies as features to describe each
document. Similarly, the BoF model describes each sample
as a histogram over a set of predeﬁned codewords, which is
also called codebook or dictionary.
To encode our data using the Bag-of-Features model,
we must ﬁrst learn the dictionary. To this end, we pick a
random subsample of the data and apply the k-means clustering algorithm to ﬁnd K centers that best partition the data
into clusters . The k-means algorithm ﬁrstly picks K
random cluster centers and assigns each sample to the cluster
whose center lies the closest to it. The cluster centers are
then updated to be the mean of the samples belonging to each
cluster and the process is repeated until the centers converge.
The ﬁnal cluster centers vk, k = 1, . . . , K form the dictionary
of the BoF model.
The learned clusters act as histogram bins in which the
feature vectors are quantized. For every sample to be encoded
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
we compute its similarity to each of the codewords (cluster
centers) as:
di,k = exp
−∥vk −xi∥2
where xi is the i-th (i = 1, . . . , 5) feature vector of the current
window, di,k denotes the k-th element of the di vector and g
is a scaling parameter which controls the participation score
of the feature vectors to each bin. All ﬁve samples contained
in the sliding window as described in Section IV-A are used
for this process. Adjusting the scaling parameters g alters
the fuzziness of the quantization process, with larger values
leading to more fuzzy assignment . Equation (10) is then
normalized to have unit l1 norm as follows:
The vector ui expresses the membership of the i-th feature
vector to each of the clusters.
To obtain the ﬁnal histogram representation h of the sample
we average over the membership vectors ui:
where NF is the length of the current window, i.e., NF = 5 in
this work. This histogram vector h can be then used for the
subsequent classiﬁcations tasks alone or in combination with
the other extracted feature vectors.
D. CLASSIFIERS
Three different classiﬁers were evaluated, combined with
eight sensible combinations of the handcrafted input data and
the features learned from that data.
1) SUPPORT VECTOR MACHINES
Support Vector Machines are binary, linear classiﬁers of the
form f (x) = sign(wT x + b) whose aim is to maximize the
margin of the hyperplane separating the two classes. The
vector w is orthogonal to the separating hyperplane, while the
offset of the hyperplane is determined by the value of b. Multiple such binary classiﬁers can be used to solve multi-class
classiﬁcation problems. The optimization problem can be
formulated as:
w∈Rd,ξi∈R+ ∥w∥2
s.t. yi(wT xi + b) ≥1 −ξi,
where ξi, i = 1, . . . , n are slack variables which allow the
classiﬁer to achieve better generalization, C is a regularization parameter which controls the width of the margin learned
by the SVM, and yi ∈{−1, 1} are the binary classiﬁcation
targets. In our experiments, the parameter C was selected by
performing 3-fold cross-validation on the training set (possible C values range from 0.00001 to 0.1).
For a multiclass classiﬁcation problem with unbalanced
label distributions over the data, the multiple SVM classiﬁers
trained can have different C values. Choosing large values
for the separation of less represented classes will lead to
fewer misclassiﬁed samples. This is important to avoid deteriorated class-wise performance of the classiﬁer for the less
represented classes. To account for the class-imbalance in the
training set, the regularizer is set to be inversely proportional
to the frequency of each class in the training dataset, i.e., Ci =
Nci C, where Ci is the regularizer for the SVM responsible
of recognizing samples of the i-th class, N is the total number
of training samples, Nci is the number of training samples
that belong to the i-th class and C the global regularization
parameter selected using cross-validation.
The direct solution of the above optimization problem
requires the storage in memory of an N × N matrix containing the inner products between all pairs of samples, where
N is the number of samples in the training set. This can
prohibit the computation of the direct solution when the
dataset is large, as in our case. To combat this issue, gradient
descent-based optimization techniques have been utilized,
including SGD . The learning process of following the
gradient of the optimization objective might end near but not
exactly at the global minimum. However, it allows for training
the classiﬁer by using minibatches of data , thus leading
to better generalization error by allowing for more samples to
contribute towards the classiﬁer’s learning process.
2) SINGLE HIDDEN LAYER FEEDFORWARD NEURAL
Non-linear
kernel methods, such as Kernel SVMs , and Kernel Ridge
Regression , can signiﬁcantly increase the classiﬁcation
accuracy over their linear counterparts. However, these kernel
methods are even more computationally intensive than their
linear variants, requiring the calculation of the kernel matrix
between all the training samples. To alleviate this problem,
approximate methods have been proposed, such as Prototype Vector Machines , and Approximate Kernel Extreme
Learning Machines . These methods employ Single Hidden Layer Feedforward Neural Networks (SLFNs) to approximate the kernel solution through a non-linear hidden layer.
In this work, a max-margin SLFN formulation is used,
i.e., the output layer is trained using a max-margin objective,
as in . First, the hidden layer weights are learned by
clustering the data into NH clusters, where the centroid wk of
each cluster corresponds to a prototype vector. The activation
of the k-th hidden neuron xhid,k is calculated by measuring
the similarity between the input vector x to each prototype
vector wk using a Radial Basis Function (RBF):
xhid,k = exp
||x −wk||2
where σ is scaling parameter that alters the spread of RBFs.
Typically σ is set to the mean distance between the input
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
FIGURE 2. Pipeline of the stock mid price prediction system used. Limit order events generated by market events are used to
extract feature vectors which describe the stocks present in the LOB, and a classifier is trained to recognize the movement of the
mid-price of those stocks. Dashed lines indicate the deployment phase, where a trader receives classification predictions and
potentially acts on them thus influencing the market.
samples. Then, the output of each binary classiﬁer is calculated as:
f (x) = Wxhid + b
where W and b are the weights and the biases of the output layer. The output weights are learned by using gradient descent and a max-margin objective (as described in
Equation (13)). As before, the regularization parameter C
is selected using 3-fold cross-validation and appropriately
weighted considering the distribution of the class labels.
3) MULTILAYER PERCEPTRONS
Multilayer Perceptrons (MLPs) , also known as Multilayer Feedforward Neural Networks, consist of several layers
of weighted connections through which the input is processed. A non-linear activation function is applied after each
layer. The calculation of each layer’s output is similar to the
autoencoder’s:
out = s(W(l)x(l)
in + b(l))
where x(l)
in is the input to a layer, obtained by Equation (7),
s(·) is the activation function, W(l) is the weight matrix
accompanying the current layer l, and b(l) is the current
layer’s bias vector. Equation (16) is applied for every layer
in the neural network up until the last layer lout where each
neuron represents a different class, meaning that in the lout
there must be as many neurons as classes in our dataset,
i.e., NC = 3 neurons. The softmax function σ(·) is applied
to the output of the ﬁnal layer of the MLP, to produce a
probability distribution over the existing classes:
σ(xout)j =
k=1 exout,k
where σ(xout)j is the predicted probability for class j and xout,j
is the output of the j-th neuron of the network.
To encode the prediction objective as a differentiable cost
function, the categorical cross entropy function is used:
yj log σ(xout)j
where yj is the desired output for the j-th output neuron, xout
denotes the output of the MLP, and σ(xout)j is the predicted
probability for the j-th class. As with AEs, the weights of each
layer can be learned using SGD. However, instead of using
plain SGD to optimize the parameters of the MLP, the ADAM
algorithm , which is a more advanced optimization algorithm that allows for faster and more stable convergence,
V. EXPERIMENTAL EVALUATION
A. EVALUATION SETUP
An overview of our proposed system of analysis is shown
in Figure 2. Events taking place in the market are described
in the limit order book, from which we can then extract
handcrafted features as described in Section IV-A as well
as target values corresponding to the movement of the midprice. The data used consist of combinations of the handcrafted feature vectors and features learned from these as
discussed in Section IV-C. The classiﬁers are then trained
incrementally using minibatches of data. Once training has
converged, the classiﬁer makes predictions about unseen data
which may inﬂuence the behavior of a trader, who then takes
place in creating market events. To evaluate the performance
of our models two different experimental setups are used,
which are described below.
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
FIGURE 3. In the anchored walk forward evaluation method, the first
d = 1, . . . 9 days are used progressively as the training dataset and
evaluation is performed on the (d + 1)-th day. The solid line is used to
depict days used for the training dataset, whereas the dashed line
indicates the day used for the evaluation. The first three progressions of
this evaluation setup are shown.
1) ANCHORED WALK FORWARD
In this case, the ﬁrst d =
1, . . . 9 day samples of all the
stocks are progressively used to train our models and the
sample belonging to the (d + 1)-th day is used for testing. Since our data consists of 10 days for each stock, this
means we have a total of 9 different folds to run our models.
This evaluation method is known as anchored walk forward
analysis, as the starting point in time remains ﬁxed .
Figure 3 illustrates the ﬁrst three progressions of this evaluation method. By using this evaluation method, we can
determine whether the models are able to learn features which
capture temporal information, so as to be able to predict future
mid price movements.
2) HOLD-OUT PER STOCK
For this setup, the models are trained on data describing
4 stocks and evaluated on the last unknown stock. The experiments are executed 5 times so that every stock is used as the
unseen stock the models are evaluated on. This evaluation
method is used to determine whether the models can learn
features from stocks that can be applied to an unknown
B. EVALUATION RESULTS
Three metrics are used for the evaluation of the described
models: average precision per class (macro precision), average recall per class (macro recall) and average F-score
(macro F-score). The precision is deﬁned as the ratio of the
true positives over the sum of the true positives and the false
positives, while the recall is the ratio of the true positives
over the sum of the true positives and the false negatives. The
F-score is deﬁned as the harmonic mean of the precision and
the recall. However, as the F-score is also macro-averaged
over the three classes, its values might lie outside the range
of the mean of the precision and recall.
Because of the class imbalance caused by considering
only meaningful changes in the movement of the mid prices,
a classiﬁer biased towards the no-change class would achieve
very high accuracy, as the majority of the predicted labels
would match the ground truth. The accuracy metric becomes
meaningless in such severely imbalanced problems and is
omitted. For the macro-averaged metrics, the corresponding
metric is ﬁrst computed for each class and ﬁnally all three
values — one for each class — are averaged. Thus, low results
in the less represented classes will equally affect the ﬁnal
representation,
architecture
of 144-72-24-72-144 neurons for each respective layer is
used. The intermediate 24-dimensional representation xenc is
extracted and used as input to the evaluated classiﬁers. As for
the BoF model, the fuzziness parameter g is set to 0.01 and K
is equal to 128, thus producing 128-dimensional histograms
to be used as representations of the input for the classiﬁcation
The representations obtained by the sliding window over
the handcrafted features in combination with the features
extracted by the AE, and BoF models are used as the input
to the described classiﬁers. The circled plus symbol is used
to denote the concatenation operation in Tables 1-6 which
summarize the evaluation results. For example, last⊕BoF
denotes the concatenation of the last representation with the
representation extracted by the BoF model. To evaluate the
models under a wide range of conditions we have conducted
extensive experiments using three different prediction horizons (Na), i.e., for Nα = 1, Nα = 5 and Nα = 10, and for
both of the experimental setups discussed.
1) SVM RESULTS
For the linear SVM classiﬁer trained with SGD, the class
imbalance accompanying the data is rectiﬁed by introducing
weights associated with each class and adjusting them to be
inversely proportional to the number of samples belonging to
that class. This ensures that the less represented classes will
be taken into account in the optimization process, making the
classiﬁer less biased towards the better represented class and
thus more useful for practical applications.
Table 1 contains the results achieved for all three different
sets of prediction targets for the anchored day evaluation
setup, whereas Table 2 contains the results for stock hold-out
setup. The Nα column contains the number of succeeding
samples taken into consideration for the production of the
prediction targets, as discussed in Section IV-B. The Input
column contains the representation used as input to the classiﬁer. The macro-averaged precision, recall and F-score are
presented, as well as the standard deviation observed for these
metrics over the progressive experiments for both evaluation
For the anchored day setup, the results indicate that information from the past and the present can be used to generalize
and make useful predictions about future movements of the
stocks’ mid prices. Taking into consideration all of the metrics, the results seem to improve when the prediction target
is derived as the mean of next 5 and 10 samples, and slightly
deteriorate when the prediction target is the movement of the
next 1 sample. This means that the classiﬁer is able to better
capture the average movement of the mid price over a few
succeeding time steps, which is expected as the movement of
the mid price in the directly succeeding time step can be more
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
TABLE 1. Anchored walk forward evaluation: SVM.
TABLE 2. Hold-out per stock evaluation: SVM.
Moreover, for all prediction targets, as past values are
taken into consideration, i.e., for all representations excluding last and AE but including their concatenations with the
rest, the classiﬁer’s performance improves. Thus, information
from not only the current time step but also from a few
steps back seems to be important in the generalization of the
classiﬁer and the correctness of the predicted movements.
Reducing the dimensionality of the data, i.e., using the
AE, or the BoF representation, improves the speed of the
classiﬁcation process but at the same time harms the prediction metrics. However, combining the learned features with
the handcrafted features seems to improve the classiﬁcation
metrics. For example, combining the last or the AE features,
that only contain information of the current time sample, with
the BoF representation, that captures the temporal dynamics
of the stock as expressed in the last 5 time steps, improves
the performance over simply using the last features. This can
provide useful insight for further developing deep learning
techniques that will capture both the current tendency as well
as the temporal progression of the time series data.
Furthermore, note that the performance of the SVM
is further improved upon when the dimensionality of the
data increases. This is reﬂected by the deteriorated performance of the AE representation, where the input is only
24-dimensional, as well as by the improved performance
of the concatenated representations, e.g., last⊕mean, concat, AE⊕BoF, where the dimensionality of the data is augmented by the concatenation operation. This behavior is to
be expected when using a linear classiﬁer, as data lying in
high-dimensional spaces are more easily separated by linear
hyperplanes.
As for the hold-out evaluation setup, the results demonstrate that information from other stocks can be utilized to
make predictions for an unknown stock. Thus, the classiﬁer
captures the general trend of the stock-market by extracting
information from a few stocks and is able to generalize to
another stock, which hasn’t contributed to its training process.
As with the previous evaluation setup, the results seem to
slightly improve when Nα is equal to 5 or 10, instead of 1,
reafﬁrming the stipulation that the average movement of the
mid price over the next few samples is easier to predict than
its direct movement in the next 1 sample. However, the results
for Na = 1 are still signiﬁcant and outperform a biased classiﬁer. Note that a perfectly biased random classiﬁer cannot
consistently achieve a (macro-averaged) F-score higher than
33.33% on the considered 3-class problems.
Once again, due to the linearity of the classiﬁer, the lowerdimensional representations perform slightly more poorly
than the higher-dimensional ones. The performance of the
classiﬁer is better when predicting the average movement
of the mid price for the next 10 samples and when using
higher-dimensional representations, such as the concat representation. The predictions made by the classiﬁer reﬂect
the general trend of the market rather than the individual
tendencies of each stock. This is further corroborated by the
high standard deviations in the F-score especially in the case
where Nα = 1, which indicate that information only from
other stocks is insufﬁcient when making predictions about an
unseen stock.
2) SLFN RESULTS
For the SLFN, the size of the hidden layer is set to 1000 and
σ is set to the mean pairwise distance between the training
feature vectors. The k-means clustering algorithm is used for
the computation of the weights of the hidden layer, whose
activations are RBFs which measure the similarity between
the input and each of the 1000 prototype vectors learned.
Similarly to the linear SVM, class weights are used to manage the imbalance between classes. The performance of this
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
TABLE 3. Anchored walk forward evaluation: SLFN.
classiﬁer is summarized in Table 3 for the anchored day setup
and in Table 4 for the hold-out setup.
For the anchored day setup, the performance of the classi-
ﬁer is more or less consistent for all three prediction targets.
This means that the classiﬁer is able to make generalizations
about the movement of the mid price in the immediately
succeeding time step, just as well as for its average movement
in the next 5 and 10 time steps. In comparison to the linear
SVM classiﬁer, the SLFN achieves somewhat better precision
at the cost of slightly worse recall and F-score values. In other
words, the predictions made by this model are more precise,
but at the same time the model falsely classiﬁes many positive
samples as negative.
Although the model takes into consideration the class
imbalance in the ﬁnal classiﬁcation task, it relies heavily
on its ﬁrst, unsupervised part which performs clustering on
the input data without taking into account the distribution
of the classes. Thus, the prototype vectors might be unfairly
distributed over the classes, favoring one over the others,
depending on the geometric distribution of the data in the
input space. If an input sample is highly similar to one or
more prototype vectors which the model has been trained to
map to one of the classes, the model conﬁdently classiﬁes this
sample as positive, i.e., as belonging to that class. Thus, such
samples are more likely to be correctly classiﬁed, leading to
more precise predictions. However, due to the unsupervised
nature of the clustering step, an input sample is more likely
to resemble prototype vectors mapped to multiple classes.
Such samples lead the model to false negative predictions,
accounting for the low recall scores. This phenomenon is even
more severe when distribution shift and concept drift issues
exist, as in the case of the hold-out evaluation setup (Table 4).
For the hold-out evaluation setup, the performance of
the classiﬁer is consistently inferior in comparison to the
performance achieved by the SVM. This can be attributed to
TABLE 4. Hold-out per stock evaluation: SLFN.
the fact that these representations are derived in an unsupervised fashion and used as the input to the also unsupervised
clustering algorithm, which greatly affects the overall performance of the classiﬁer. In combination with the fact that the
hidden layer of the SLFN is not trainable, this constitutes the
most major drawback of this classiﬁer. Moreover, the F-score
exhibits very high variance in all three prediction targets. This
is indicative of the fact that the classiﬁer fails to generalize
and make correct predictions about an unseen stock, by using
data only from other stocks. Thus, the SLFN fails to capture
the general trend of the stock market and apply its knowledge
to unknown data.
3) MLP RESULTS
The architecture of the MLP includes an input layer with as
many neurons as the dimension of the input representation,
two successive hidden layers each with 512 neurons, and
the output layer consisting of three neurons, corresponding
to the three possible classes. The results for the anchored
day and the hold-out evaluation setups are shown in Table 5
and Table 6 respectively. The results are notably better than
the ones achieved by the previous classiﬁers. This can be
attributed to the fact that MLPs are capable of capturing more
complex non-linear relations between the data, are inherently
more robust to noisy inputs and can better handle distribution
shift phenomena.
For the anchored day setup, the best performance is
achieved when the prediction target is derived as the average
movement of the mid price of the next ﬁve samples, that is
Nα = 5 and when higher-dimensional representations are
used as input, i.e., the combinations of the handcrafted features. The dimensionality of the input data seems to slightly
affect the performance of the classiﬁer, although even the
low-dimensional representations derived by the AE achieve
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
TABLE 5. Anchored walk forward evaluation: MLP.
TABLE 6. Hold-out per stock evaluation: MLP.
competitive results. The concat and last⊕mean representations, which exhibit the highest dimension amongst the
representations, yield the best results. Moreover, the classiﬁer
achieves great performance even when trying to predict the
mid price movement of the immediately succeeding 1 sample.
For the hold-out setup, the classiﬁer seems capable of
making correct predictions for all three sets of labels evaluated, i.e., for Nα = 1, 5 and 10. In fact, the MLP achieves
the best performance of all three classiﬁers in this setup,
meaning that it is capable of making better generalizations
about unknown stock data, by learning from other stocks.
Also, as dimensionality increases so does the performance of
the classiﬁer.
TABLE 7. Per class scores achieved by the MLP classifier using the concat
representation.
TABLE 8. Comparing different MLP architectures using the concat
representation (anchored walk forward setup, Nα = 10).
Furthermore, to provide better insight on the performance
of the best model (MLP) we report the evaluated metrics separately for each class using the best available representation
(concat) for the Na = 10 scenario, which a trader might be
most interested in. The results are summarized in Table 7.
The results, in particular the precision scores, indicate that
the classiﬁer is capable of making a correct decision for the
up and down classes at a rate higher than random guessing.
However note that the recall and, by extension, the F-scores
are highly affected by the very large number of samples in
our dataset and don’t necessarily reﬂect the requirements
a trader might expect from a classiﬁer, as traders act on
positive signals. The lower performance of the classiﬁer for
short-term forecasts can be also attributed to the noisy nature
of the mid-price at very short prediction horizons — the
model performs signiﬁcantly better for predicting the longer
term behavior of the stocks.
Finally, to examine the effect of the used MLP architecture
on the quality of the learned model, we evaluated the MLP
using a number of different architectures by varying the
number of hidden layers and neurons per layer. The results
are reported in Table 8. Only the evaluation results for the
anchored walk forward setup using the concat representation
are reported due to lack of space. However, similar results
were obtained for the rest of the representations and evaluation setups. Even though the used ‘‘512-512-3’’ architecture
leads to the best F-score, using a different architecture does
not severely impact the quality of the learned model.
The activations of the hidden layers of the MLP can be
thought of as features learned from the input representation, which are biased towards the task of classiﬁcation.
Thus, the MLP performs a kind of supervised feature extraction, where the extracted features are learned via optimizing
the classiﬁcation error. On the contrary, the AE and BoF
models perform unsupervised feature extraction, which has
no guarantee of being suitable for the task at hand. The
classiﬁcation-biased feature learning process that occurs in
parallel with the classiﬁer’s training, has the potential to produce very robust features and lead to improved predictions.
This is reﬂected by the slightly deteriorated performance of
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
TABLE 9. Computational complexity analysis of the used classifiers.
the classiﬁer, when the input representation is derived by
unsupervised feature extraction techniques. In combination
with the supervised feature learning performed by training the
MLP, this further reafﬁrms the assumption that the activations
of the MLP’s hidden layers learn representations of the data
which highly beﬁt the task of classiﬁcation.
C. COMPUTATIONAL COMPLEXITY ANALYSIS
Financial exchanges generate a vast amount of data that must
be processed in real-time in order to quickly respond to the
volatile conditions of the markets. Therefore, the speed of the
deployed models is equally important with the forecasting
accuracy in real-word applications where large amounts of
data must be processed under strict time constrains. In this
Subsection we provide both an asymptotic forecasting time
analysis and an empirical study of the run time of the used
models. Our analysis is focused on the prediction complexity,
since the real-time constrains can be relaxed during training
by using a slightly outdated model. Furthermore, all the used
models can be incrementally trained and, as a result, adapt to
the available computational resources during the training.
Let Nr be the size of the used representation, NC be the
number of possible mid-price movements (NC = 3) and Nh
be the size of the hidden representation (used only for the
SLFN and the MLP models). Table 9 summarizes the time
complexity of the models. The SVM has signiﬁcantly lower
time requirements than the other two models. The complexity
of the SLFN and MLP models can also be adjusted by altering
the size of the hidden layers (Nh = 1000 for the SLFN and
Nh = 512 for the MLP). Note that the used SVM operates in
the primal space and therefore the time complexity does not
depend on the number of selected support vectors.
The results of the empirical run time analysis, reported
in Table 10, also conﬁrm the previous ﬁndings. All the classi-
ﬁers were implemented using GPU-accelerated libraries 
and a mid-range GPU with 6GB of RAM was used for the
conducted experiments. To ensure a fair comparison of the
compared classiﬁers we report the time needed after the feature extraction step. Note that even the MLP classiﬁer, which
is the most computationally-intensive model, is able to process almost 7,000 transactions per second using a mid-range
GPU. On the other hand, the SVM achieves the best trade-off
between forecasting accuracy and run time, being capable of
processing more than 14,000 transactions per second.
D. STATISTICAL ANALYSIS
To validate the signiﬁcance of the obtained results we performed a series of statistical tests. First, the three different
classiﬁers used for forecasting the mid price movements were
TABLE 10. Run time analysis of the used classifiers. The mean time
observed over 1000 runs measured in milliseconds is reported.
FIGURE 4. Nemenyi post-hoc test: Comparing different classifiers.
compared using the Friedman’s test . The null hypothesis
was deﬁned as: ‘‘There is no statistical signiﬁcant difference
(a = 0.1) between the SVM, SLFN and MLP classiﬁers.’’
To compare the classiﬁers the F-score for the different representations and predictions horizons was used. The null
hypothesis was rejected (p = 3.9 × 10−26), meaning that
at least one classiﬁer is signiﬁcantly better than the others.
The Nemenyi post-hoc test was then used to evaluate
the differences between the classiﬁers, as shown in Figure 4.
The SVM and MLP classiﬁers are signiﬁcantly better than
the SLFN classiﬁer. Note that even though the MLP classiﬁer
performs better than the SVM, the differences between the
MLP and the SVM are not statistically signiﬁcant (a = 0.1).
We also compared the eight different representations used
for the conducted experiments. The null hypothesis was
deﬁned as: ‘‘There is no statistical signiﬁcant difference (a =
0.1) between the eight used representations.’’ Again, the null
hypothesis was strongly rejected using the Friedman’s test
(p = 5.5 × 10−35). To further compare the representations
the Nemenyi post-hoc test was used as before. The results
are shown in Figure 5. The concat, last + mean and mean
representations are signiﬁcantly better than most of the other
used representations. Also, note that the dimensionality of
a representation seems to be correlated with its predictive
power. However, even though the last and mean representations have the same dimensionality, the mean representation
leads to signiﬁcantly better results. This is mainly due to the
de-noising effect of the averaging process used for extracting the mean representation, effectively suppressing possible
E. EVALUATION SUMMARY
As corroborated by the statistical tests performed, the handcrafted representations and especially those that incorporate
VOLUME 7, 2019
P. Nousi et al.: Machine Learning for Forecasting Mid-Price Movements Using Limit Order Book Data
FIGURE 5. Nemenyi post-hoc test: Comparing different representations.
temporal information yield the most signiﬁcant results. The
BoF and last⊕BoF representations follow, as they also take
into consideration past values during the feature extraction
process, but as the input dimension decreases so does the
discriminative ability of the classiﬁers. The MLP classiﬁer
performs better when the input representation is derived
by the handcrafted features, as its hidden layers extract
a classiﬁcation-based representation of its input. The low
dimensionality of the AE, and BoF representations in combination with their unsupervised nature lead the MLP to make
fewer correct predictions.
The low dimensionality of these learned representations
allows for faster computations, albeit at the cost of achieving slightly deteriorated performances. The SVM classiﬁer’s
performance increases with the dimensionality of the input
representation, but falls behind the MLP’s performance, as it
is less resistant to the inherent noise of the data. Finally,
the SLFN’s performance is plagued by the high number of
false negative predictions. The incorporation of past information does somewhat alleviate this drawback, but its overall
performance falls back in comparison to the other classiﬁers.
VI. CONCLUSIONS AND DISCUSSION
In this work, an extensive study into the information provided
by the high frequency limit order book with respect to the
forecasting of future mid price movements was presented.
Several representations derived by handcrafted features as
well as features learned by Machine Learning algorithms,
ranging from 24 to 720-dimensional feature vectors, were
considered and used as input to various classiﬁers for the
forecasting task. Three scenarios were assessed regarding the
span of time for which predictions are made. Finally, two
evaluation analysis methods were examined for each classi-
ﬁer, scenario and input representation. Through the anchored
walk forward setup, the ability of the evaluated classiﬁers to
learn from past stock data and apply this knowledge to future,
unknown data is determined. The hold-out setup serves to
examine whether the classiﬁers are able to capture the general
trends and movements of the stock market by learning from
some stocks and applying this knowledge to unseen stocks.
The results achieved are remarkable in all cases, indicating
that Machine Learning techniques are capable of correctly
predicting mid price movements. Different classiﬁers seem
to perform better in different aspects, such as the precision or
recall of the predicted movements. We have provided the ﬁrst,
to the best of our knowledge, in depth review and evaluation
that addresses the challenging characteristics of LOB data,
such as the high velocity, variance, volume and strict real-time
constraints, and uses ML techniques to predict the mid price
movement, providing insight into the information contained
in a LOB. The prediction results are improved when combining the extracted feature representations with the handcrafted
ones, indicating that the feature extraction models are able
to uncover latent, auxiliary knowledge. Finally, the learned
representations also yield signiﬁcant results when used alone
and systematically improve the time-wise performance of all
classiﬁers by reducing the dimensionality of the input data.
ACKNOWLEDGEMENTS
(Paraskevi Nousi and Avraam Tsantekidis are co-ﬁrst