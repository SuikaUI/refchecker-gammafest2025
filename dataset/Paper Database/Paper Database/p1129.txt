Squeeze-and-Excitation Networks
Jie Hu[0000−0002−5150−1003]
Li Shen[0000−0002−2283−4976]
Samuel Albanie[0000−0001−9736−5134]
Gang Sun[0000−0001−6913−6799]
Enhua Wu[0000−0002−2174−1428]
Abstract—The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to
construct informative features by fusing both spatial and channel-wise information within local receptive ﬁelds at each layer. A broad
range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of
a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel
relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates
channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be
stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate
that SE blocks bring signiﬁcant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost.
Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classiﬁcation submission which won ﬁrst place and
reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%. Models and code are
available at 
Index Terms—Squeeze-and-Excitation, Image representations, Attention, Convolutional Neural Networks.
INTRODUCTION
ONVOLUTIONAL neural networks (CNNs) have proven
to be useful models for tackling a wide range of visual
tasks , , , . At each convolutional layer in the network, a collection of ﬁlters expresses neighbourhood spatial
connectivity patterns along input channels—fusing spatial
and channel-wise information together within local receptive ﬁelds. By interleaving a series of convolutional layers
with non-linear activation functions and downsampling operators, CNNs are able to produce image representations
that capture hierarchical patterns and attain global theoretical receptive ﬁelds. A central theme of computer vision
research is the search for more powerful representations that
capture only those properties of an image that are most
salient for a given task, enabling improved performance.
As a widely-used family of models for vision tasks, the
development of new neural network architecture designs
now represents a key frontier in this search. Recent research
has shown that the representations produced by CNNs can
be strengthened by integrating learning mechanisms into
the network that help capture spatial correlations between
features. One such approach, popularised by the Inception
family of architectures , , incorporates multi-scale processes into network modules to achieve improved perfor-
Jie Hu and Enhua Wu are with the State Key Laboratory of Computer
Science, Institute of Software, Chinese Academy of Sciences, Beijing,
100190, China.
They are also with the University of Chinese Academy of Sciences, Beijing,
100049, China.
Jie Hu is also with Momenta and Enhua Wu is also with the Faculty of
Science and Technology & AI Center at University of Macau.
E-mail: 
 
Gang Sun is with LIAMA-NLPR at the Institute of Automation, Chinese
Academy of Sciences. He is also with Momenta.
E-mail: 
Li Shen and Samuel Albanie are with the Visual Geometry Group at the
University of Oxford.
E-mail: {lishen,albanie}@robots.ox.ac.uk
mance. Further work has sought to better model spatial
dependencies , and incorporate spatial attention into
the structure of the network .
In this paper, we investigate a different aspect of network
design - the relationship between channels. We introduce
a new architectural unit, which we term the Squeeze-and-
Excitation (SE) block, with the goal of improving the quality
of representations produced by a network by explicitly modelling the interdependencies between the channels of its convolutional features. To this end, we propose a mechanism
that allows the network to perform feature recalibration,
through which it can learn to use global information to
selectively emphasise informative features and suppress less
useful ones.
The structure of the SE building block is depicted in
Fig. 1. For any given transformation Ftr mapping the
input X to the feature maps U where U ∈RH×W ×C,
e.g. a convolution, we can construct a corresponding SE
block to perform feature recalibration. The features U are
ﬁrst passed through a squeeze operation, which produces a
channel descriptor by aggregating feature maps across their
spatial dimensions (H × W). The function of this descriptor
is to produce an embedding of the global distribution of
channel-wise feature responses, allowing information from
the global receptive ﬁeld of the network to be used by
all its layers. The aggregation is followed by an excitation
operation, which takes the form of a simple self-gating
mechanism that takes the embedding as input and produces a collection of per-channel modulation weights. These
weights are applied to the feature maps U to generate
the output of the SE block which can be fed directly into
subsequent layers of the network.
It is possible to construct an SE network (SENet) by
simply stacking a collection of SE blocks. Moreover, these
SE blocks can also be used as a drop-in replacement for the
original block at a range of depths in the network architecarXiv:1709.01507v4 [cs.CV] 16 May 2019
Fig. 1. A Squeeze-and-Excitation block.
ture (Section 6.4). While the template for the building block
is generic, the role it performs at different depths differs
throughout the network. In earlier layers, it excites informative features in a class-agnostic manner, strengthening
the shared low-level representations. In later layers, the SE
blocks become increasingly specialised, and respond to different inputs in a highly class-speciﬁc manner (Section 7.2).
As a consequence, the beneﬁts of the feature recalibration
performed by SE blocks can be accumulated through the
The design and development of new CNN architectures
is a difﬁcult engineering task, typically requiring the selection of many new hyperparameters and layer conﬁgurations. By contrast, the structure of the SE block is simple and
can be used directly in existing state-of-the-art architectures
by replacing components with their SE counterparts, where
the performance can be effectively enhanced. SE blocks are
also computationally lightweight and impose only a slight
increase in model complexity and computational burden.
To provide evidence for these claims, we develop several
SENets and conduct an extensive evaluation on the ImageNet dataset . We also present results beyond ImageNet
that indicate that the beneﬁts of our approach are not
restricted to a speciﬁc dataset or task. By making use of
SENets, we ranked ﬁrst in the ILSVRC 2017 classiﬁcation
competition. Our best model ensemble achieves a 2.251%
top-5 error on the test set1. This represents roughly a 25%
relative improvement when compared to the winner entry
of the previous year (top-5 error of 2.991%).
RELATED WORK
Deeper architectures. VGGNets and Inception models showed that increasing the depth of a network could
signiﬁcantly increase the quality of representations that
it was capable of learning. By regulating the distribution
of the inputs to each layer, Batch Normalization (BN) 
added stability to the learning process in deep networks
and produced smoother optimisation surfaces . Building
on these works, ResNets demonstrated that it was possible to learn considerably deeper and stronger networks
through the use of identity-based skip connections , .
Highway networks introduced a gating mechanism to
regulate the ﬂow of information along shortcut connections.
Following these works, there have been further reformulations of the connections between network layers , ,
1. 
which show promising improvements to the learning and
representational properties of deep networks.
An alternative, but closely related line of research has
focused on methods to improve the functional form of
the computational elements contained within a network.
Grouped convolutions have proven to be a popular approach for increasing the cardinality of learned transformations , . More ﬂexible compositions of operators can
be achieved with multi-branch convolutions , , ,
 , which can be viewed as a natural extension of the
grouping operator. In prior work, cross-channel correlations
are typically mapped as new combinations of features, either independently of spatial structure , or jointly
by using standard convolutional ﬁlters with 1 × 1
convolutions. Much of this research has concentrated on the
objective of reducing model and computational complexity,
reﬂecting an assumption that channel relationships can be
formulated as a composition of instance-agnostic functions
with local receptive ﬁelds. In contrast, we claim that providing the unit with a mechanism to explicitly model dynamic,
non-linear dependencies between channels using global information can ease the learning process, and signiﬁcantly
enhance the representational power of the network.
Algorithmic Architecture Search. Alongside the works
described above, there is also a rich history of research
that aims to forgo manual architecture design and instead
seeks to learn the structure of the network automatically.
Much of the early work in this domain was conducted in
the neuro-evolution community, which established methods
for searching across network topologies with evolutionary
methods , . While often computationally demanding, evolutionary search has had notable successes which
include ﬁnding good memory cells for sequence models
 , and learning sophisticated architectures for largescale image classiﬁcation , , . With the goal of reducing the computational burden of these methods, efﬁcient
alternatives to this approach have been proposed based on
Lamarckian inheritance and differentiable architecture
search .
By formulating architecture search as hyperparameter
optimisation, random search and other more sophisticated model-based optimisation techniques , can
also be used to tackle the problem. Topology selection
as a path through a fabric of possible designs and
direct architecture prediction , have been proposed
as additional viable architecture search tools. Particularly
strong results have been achieved with techniques from
reinforcement learning , , , , . SE blocks
can be used as atomic building blocks for these search
algorithms, and were demonstrated to be highly effective
in this capacity in concurrent work .
Attention and gating mechanisms. Attention can be interpreted as a means of biasing the allocation of available
computational resources towards the most informative components of a signal , , , , , . Attention
mechanisms have demonstrated their utility across many
tasks including sequence learning , , localisation
and understanding in images , , image captioning
 , and lip reading . In these applications, it
can be incorporated as an operator following one or more
layers representing higher-level abstractions for adaptation
between modalities. Some works provide interesting studies
into the combined use of spatial and channel attention ,
 . Wang et al. introduced a powerful trunk-and-mask
attention mechanism based on hourglass modules that is
inserted between the intermediate stages of deep residual
networks. By contrast, our proposed SE block comprises a
lightweight gating mechanism which focuses on enhancing
the representational power of the network by modelling
channel-wise relationships in a computationally efﬁcient
SQUEEZE-AND-EXCITATION BLOCKS
A Squeeze-and-Excitation block is a computational unit
which can be built upon a transformation Ftr mapping an
input X ∈RH′×W ′×C′ to feature maps U ∈RH×W ×C.
In the notation that follows we take Ftr to be a convolutional operator and use V = [v1, v2, . . . , vC] to denote
the learned set of ﬁlter kernels, where vc refers to the
parameters of the c-th ﬁlter. We can then write the outputs
as U = [u1, u2, . . . , uC], where
uc = vc ∗X =
Here ∗denotes convolution, vc = [v1
c, . . . , vC′
[x1, x2, . . . , xC′] and uc ∈RH×W . vs
c is a 2D spatial kernel
representing a single channel of vc that acts on the corresponding channel of X. To simplify the notation, bias terms
are omitted. Since the output is produced by a summation
through all channels, channel dependencies are implicitly
embedded in vc, but are entangled with the local spatial
correlation captured by the ﬁlters. The channel relationships
modelled by convolution are inherently implicit and local
(except the ones at top-most layers). We expect the learning
of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able
to increase its sensitivity to informative features which can
be exploited by subsequent transformations. Consequently,
we would like to provide it with access to global information
and recalibrate ﬁlter responses in two steps, squeeze and
excitation, before they are fed into the next transformation.
A diagram illustrating the structure of an SE block is shown
in Fig. 1.
Squeeze: Global Information Embedding
In order to tackle the issue of exploiting channel dependencies, we ﬁrst consider the signal to each channel in the
output features. Each of the learned ﬁlters operates with
a local receptive ﬁeld and consequently each unit of the
transformation output U is unable to exploit contextual
information outside of this region.
To mitigate this problem, we propose to squeeze global
spatial information into a channel descriptor. This is
achieved by using global average pooling to generate
channel-wise statistics. Formally, a statistic z ∈RC is generated by shrinking U through its spatial dimensions H × W,
such that the c-th element of z is calculated by:
zc = Fsq(uc) =
Discussion. The output of the transformation U can be
interpreted as a collection of the local descriptors whose
statistics are expressive for the whole image. Exploiting
such information is prevalent in prior feature engineering
work , , . We opt for the simplest aggregation
technique, global average pooling, noting that more sophisticated strategies could be employed here as well.
Excitation: Adaptive Recalibration
To make use of the information aggregated in the squeeze
operation, we follow it with a second operation which aims
to fully capture channel-wise dependencies. To fulﬁl this
objective, the function must meet two criteria: ﬁrst, it must
be ﬂexible (in particular, it must be capable of learning
a nonlinear interaction between channels) and second, it
must learn a non-mutually-exclusive relationship since we
would like to ensure that multiple channels are allowed to
be emphasised (rather than enforcing a one-hot activation).
To meet these criteria, we opt to employ a simple gating
mechanism with a sigmoid activation:
s = Fex(z, W) = σ(g(z, W)) = σ(W2δ(W1z)),
where δ refers to the ReLU function, W1 ∈R
r . To limit model complexity and aid generalisation, we parameterise the gating mechanism by forming
a bottleneck with two fully-connected (FC) layers around
the non-linearity, i.e. a dimensionality-reduction layer with
reduction ratio r (this parameter choice is discussed in Section 6.1), a ReLU and then a dimensionality-increasing layer
returning to the channel dimension of the transformation
output U. The ﬁnal output of the block is obtained by
rescaling U with the activations s:
exc = Fscale(uc, sc) = sc uc,
where eX = [ex1, ex2, . . . , exC] and Fscale(uc, sc) refers to
channel-wise multiplication between the scalar sc and the
feature map uc ∈RH×W .
Discussion. The excitation operator maps the inputspeciﬁc descriptor z to a set of channel weights. In this
regard, SE blocks intrinsically introduce dynamics conditioned on the input, which can be regarded as a selfattention function on channels whose relationships are not
conﬁned to the local receptive ﬁeld the convolutional ﬁlters
are responsive to.
Global pooling
SE-Inception Module
Inception Module
Fig. 2. The schema of the original Inception module (left) and the SE-
Inception module (right).
Instantiations
The SE block can be integrated into standard architectures
such as VGGNet by insertion after the non-linearity
following each convolution. Moreover, the ﬂexibility of the
SE block means that it can be directly applied to transformations beyond standard convolutions. To illustrate this point,
we develop SENets by incorporating SE blocks into several
examples of more complex architectures, described next.
We ﬁrst consider the construction of SE blocks for Inception networks . Here, we simply take the transformation
Ftr to be an entire Inception module (see Fig. 2) and by
making this change for each such module in the architecture, we obtain an SE-Inception network. SE blocks can
also be used directly with residual networks (Fig. 3 depicts
the schema of an SE-ResNet module). Here, the SE block
transformation Ftr is taken to be the non-identity branch
of a residual module. Squeeze and Excitation both act before
summation with the identity branch. Further variants that
integrate SE blocks with ResNeXt , Inception-ResNet
 , MobileNet and ShufﬂeNet can be constructed
by following similar schemes. For concrete examples of
SENet architectures, a detailed description of SE-ResNet-50
and SE-ResNeXt-50 is given in Table 1.
One consequence of the ﬂexible nature of the SE block
is that there are several viable ways in which it could
be integrated into these architectures. Therefore, to assess
sensitivity to the integration strategy used to incorporate SE
blocks into a network architecture, we also provide ablation
experiments exploring different designs for block inclusion
in Section 6.5.
MODEL AND COMPUTATIONAL COMPLEXITY
For the proposed SE block design to be of practical use, it
must offer a good trade-off between improved performance
and increased model complexity. To illustrate the computational burden associated with the module, we consider
a comparison between ResNet-50 and SE-ResNet-50 as an
example. ResNet-50 requires ∼3.86 GFLOPs in a single
forward pass for a 224 × 224 pixel input image. Each SE
block makes use of a global average pooling operation in
SE-ResNet Module
Global pooling
ResNet Module
Fig. 3. The schema of the original Residual module (left) and the SE-
ResNet module (right).
the squeeze phase and two small FC layers in the excitation
phase, followed by an inexpensive channel-wise scaling
operation. In the aggregate, when setting the reduction ratio
r (introduced in Section 3.2) to 16, SE-ResNet-50 requires
∼3.87 GFLOPs, corresponding to a 0.26% relative increase
over the original ResNet-50. In exchange for this slight additional computational burden, the accuracy of SE-ResNet-50
surpasses that of ResNet-50 and indeed, approaches that
of a deeper ResNet-101 network requiring ∼7.58 GFLOPs
(Table 2).
In practical terms, a single pass forwards and backwards
through ResNet-50 takes 190 ms, compared to 209 ms for
SE-ResNet-50 with a training minibatch of 256 images (both
timings are performed on a server with 8 NVIDIA Titan X
GPUs). We suggest that this represents a reasonable runtime
overhead, which may be further reduced as global pooling
and small inner-product operations receive further optimisation in popular GPU libraries. Due to its importance
for embedded device applications, we further benchmark
CPU inference time for each model: for a 224 × 224 pixel
input image, ResNet-50 takes 164 ms in comparison to 167
ms for SE-ResNet-50. We believe that the small additional
computational cost incurred by the SE block is justiﬁed by
its contribution to model performance.
We next consider the additional parameters introduced
by the proposed SE block. These additional parameters
result solely from the two FC layers of the gating mechanism
and therefore constitute a small fraction of the total network
capacity. Concretely, the total number introduced by the
weight parameters of these FC layers is given by:
where r denotes the reduction ratio, S refers to the number
of stages (a stage refers to the collection of blocks operating on feature maps of a common spatial dimension), Cs
denotes the dimension of the output channels and Ns denotes the number of repeated blocks for stage s (when bias
terms are used in FC layers, the introduced parameters and
computational cost are typically negligible). SE-ResNet-50
introduces ∼2.5 million additional parameters beyond the
(Left) ResNet-50 . (Middle) SE-ResNet-50. (Right) SE-ResNeXt-50 with a 32×4d template. The shapes and operations with speciﬁc parameter
settings of a residual building block are listed inside the brackets and the number of stacked blocks in a stage is presented outside. The inner
brackets following by fc indicates the output dimension of the two fully connected layers in an SE module.
Output size
SE-ResNet-50
SE-ResNeXt-50 (32 × 4d)
conv, 7 × 7, 64, stride 2
max pool, 3 × 3, stride 2
conv, 1 × 1, 64
conv, 3 × 3, 64
conv, 1 × 1, 256
conv, 1 × 1, 64
conv, 3 × 3, 64
conv, 1 × 1, 256
fc, 
conv, 1 × 1, 128
conv, 3 × 3, 128
conv, 1 × 1, 256
fc, 
conv, 1 × 1, 128
conv, 3 × 3, 128
conv, 1 × 1, 512
conv, 1 × 1, 128
conv, 3 × 3, 128
conv, 1 × 1, 512
fc, 
conv, 1 × 1, 256
conv, 3 × 3, 256
conv, 1 × 1, 512
fc, 
conv, 1 × 1, 256
conv, 3 × 3, 256
conv, 1 × 1, 1024
conv, 1 × 1, 256
conv, 3 × 3, 256
conv, 1 × 1, 1024
fc, 
conv, 1 × 1, 512
conv, 3 × 3, 512
conv, 1 × 1, 1024
fc, 
conv, 1 × 1, 512
conv, 3 × 3, 512
conv, 1 × 1, 2048
conv, 1 × 1, 512
conv, 3 × 3, 512
conv, 1 × 1, 2048
fc, 
conv, 1 × 1, 1024
conv, 3 × 3, 1024
conv, 1 × 1, 2048
fc, 
global average pool, 1000-d fc, softmax
Single-crop error rates (%) on the ImageNet validation set and complexity comparisons. The original column refers to the results reported in the
original papers (the results of ResNets are obtained from the website: To enable a fair
comparison, we re-train the baseline models and report the scores in the re-implementation column. The SENet column refers to the
corresponding architectures in which SE blocks have been added. The numbers in brackets denote the performance improvement over the
re-implemented baselines. † indicates that the model has been evaluated on the non-blacklisted subset of the validation set (this is discussed in
more detail in ), which may slightly improve results. VGG-16 and SE-VGG-16 are trained with batch normalization.
re-implementation
top-1 err.
top-5 err.
top-1 err.
top-5 err.
top-1 err.
top-5 err.
ResNet-50 
23.29(1.51)
6.62(0.86)
ResNet-101 
22.38(0.79)
6.07(0.45)
ResNet-152 
21.57(0.85)
5.73(0.61)
ResNeXt-50 
21.10(1.01)
5.49(0.41)
ResNeXt-101 
20.70(0.48)
5.01(0.56)
VGG-16 
25.22(1.80)
7.70(1.11)
BN-Inception 
24.23(1.15)
7.14(0.75)
Inception-ResNet-v2 
19.80(0.57)
4.79(0.42)
∼25 million parameters required by ResNet-50, corresponding to a ∼10% increase. In practice, the majority of these
parameters come from the ﬁnal stage of the network, where
the excitation operation is performed across the greatest
number of channels. However, we found that this comparatively costly ﬁnal stage of SE blocks could be removed at
only a small cost in performance (<0.1% top-5 error on
ImageNet) reducing the relative parameter increase to ∼4%,
which may prove useful in cases where parameter usage
is a key consideration (see Section 6.4 and 7.2 for further
discussion).
EXPERIMENTS
In this section, we conduct experiments to investigate the
effectiveness of SE blocks across a range of tasks, datasets
and model architectures.
Image Classiﬁcation
To evaluate the inﬂuence of SE blocks, we ﬁrst perform
experiments on the ImageNet 2012 dataset which
comprises 1.28 million training images and 50K validation
images from 1000 different classes. We train networks on
the training set and report the top-1 and top-5 error on the
validation set.
Each baseline network architecture and its corresponding SE counterpart are trained with identical optimisation
schemes. We follow standard practices and perform data
augmentation with random cropping using scale and aspect ratio to a size of 224 × 224 pixels (or 299 × 299
for Inception-ResNet-v2 and SE-Inception-ResNet-v2)
and perform random horizontal ﬂipping. Each input image is normalised through mean RGB-channel subtraction.
All models are trained on our distributed learning system
ROCS which is designed to handle efﬁcient parallel training
of large networks. Optimisation is performed using synchronous SGD with momentum 0.9 and a minibatch size
of 1024. The initial learning rate is set to 0.6 and decreased
by a factor of 10 every 30 epochs. Models are trained for 100
epochs from scratch, using the weight initialisation strategy
described in . The reduction ratio r (in Section 3.2) is set
to 16 by default (except where stated otherwise).
When evaluating the models we apply centre-cropping
so that 224 × 224 pixels are cropped from each image, after
Single-crop error rates (%) on the ImageNet validation set and complexity comparisons. MobileNet refers to “1.0 MobileNet-224” in and
ShufﬂeNet refers to “ShufﬂeNet 1 × (g = 3)” in . The numbers in brackets denote the performance improvement over the re-implementation.
re-implementation
top-1 err.
top-5 err.
top-1 err.
top-5 err.
top-1 err.
top-5 err.
MobileNet 
ShufﬂeNet 
Fig. 4. Training baseline architectures and their SENet counterparts on ImageNet. SENets exhibit improved optimisation characteristics and produce
consistent gains in performance which are sustained throughout the training process.
its shorter edge is ﬁrst resized to 256 (299 × 299 from
each image whose shorter edge is ﬁrst resized to 352 for
Inception-ResNet-v2 and SE-Inception-ResNet-v2).
Network depth. We begin by comparing SE-ResNet against
ResNet architectures with different depths and report the
results in Table 2. We observe that SE blocks consistently
improve performance across different depths with an extremely small increase in computational complexity. Remarkably, SE-ResNet-50 achieves a single-crop top-5 validation error of 6.62%, exceeding ResNet-50 (7.48%) by 0.86%
and approaching the performance achieved by the much
deeper ResNet-101 network (6.52% top-5 error) with only
half of the total computational burden (3.87 GFLOPs vs.
7.58 GFLOPs). This pattern is repeated at greater depth,
where SE-ResNet-101 (6.07% top-5 error) not only matches,
but outperforms the deeper ResNet-152 network (6.34%
top-5 error) by 0.27%. While it should be noted that the SE
blocks themselves add depth, they do so in an extremely
computationally efﬁcient manner and yield good returns
even at the point at which extending the depth of the base
architecture achieves diminishing returns. Moreover, we see
that the gains are consistent across a range of different
network depths, suggesting that the improvements induced
by SE blocks may be complementary to those obtained by
simply increasing the depth of the base architecture.
Integration with modern architectures. We next study the
effect of integrating SE blocks with two further state-ofthe-art architectures, Inception-ResNet-v2 and ResNeXt
(using the setting of 32 × 4d) , both of which introduce
additional computational building blocks into the base network. We construct SENet equivalents of these networks,
SE-Inception-ResNet-v2 and SE-ResNeXt (the conﬁguration
of SE-ResNeXt-50 is given in Table 1) and report results
in Table 2. As with the previous experiments, we observe
signiﬁcant performance improvements induced by the introduction of SE blocks into both architectures. In particular, SE-ResNeXt-50 has a top-5 error of 5.49% which is
superior to both its direct counterpart ResNeXt-50 (5.90%
top-5 error) as well as the deeper ResNeXt-101 (5.57% top-5
error), a model which has almost twice the total number of
parameters and computational overhead. We note a slight
difference in performance between our re-implementation
of Inception-ResNet-v2 and the result reported in .
However, we observe a similar trend with regard to the
effect of SE blocks, ﬁnding that SE counterpart (4.79% top-5
error) outperforms our reimplemented Inception-ResNet-v2
baseline (5.21% top-5 error) by 0.42% as well as the reported
result in .
We also assess the effect of SE blocks when operating on
non-residual networks by conducting experiments with the
VGG-16 and BN-Inception architecture . To facilitate
the training of VGG-16 from scratch, we add Batch Normalization layers after each convolution. We use identical training schemes for both VGG-16 and SE-VGG-16. The results of
the comparison are shown in Table 2. Similarly to the results
reported for the residual baseline architectures, we observe
that SE blocks bring improvements in performance on the
non-residual settings.
To provide some insight into inﬂuence of SE blocks on
the optimisation of these models, example training curves
for runs of the baseline architectures and their respective
SE counterparts are depicted in Fig. 4. We observe that SE
blocks yield a steady improvement throughout the optimisation procedure. Moreover, this trend is fairly consistent
across a range of network architectures considered as baselines.
Mobile setting. Finally, we consider two representative
architectures from the class of mobile-optimised networks,
MobileNet and ShufﬂeNet . For these experiments,
we used a minibatch size of 256 and slightly less aggressive
data augmentation and regularisation as in . We trained
the models across 8 GPUs using SGD with momentum (set
to 0.9) and an initial learning rate of 0.1 which was reduced
by a factor of 10 each time the validation loss plateaued. The
total training process required ∼400 epochs (enabling us
to reproduce the baseline performance of ). The results
reported in Table 3 show that SE blocks consistently improve
Classiﬁcation error (%) on CIFAR-10.
ResNet-110 
ResNet-164 
WRN-16-8 
Shake-Shake 26 2x96d + Cutout 
Classiﬁcation error (%) on CIFAR-100.
ResNet-110 
ResNet-164 
WRN-16-8 
Shake-Even 29 2x4x64d + Cutout 
the accuracy by a large margin at a minimal increase in
computational cost.
Additional datasets. We next investigate whether the bene-
ﬁts of SE blocks generalise to datasets beyond ImageNet. We
perform experiments with several popular baseline architectures and techniques (ResNet-110 , ResNet-164 ,
WideResNet-16-8 , Shake-Shake and Cutout ) on
the CIFAR-10 and CIFAR-100 datasets . These comprise
a collection of 50k training and 10k test 32 × 32 pixel RGB
images, labelled with 10 and 100 classes respectively. The integration of SE blocks into these networks follows the same
approach that was described in Section 3.3. Each baseline
and its SENet counterpart are trained with standard data
augmentation strategies , . During training, images
are randomly horizontally ﬂipped and zero-padded on each
side with four pixels before taking a random 32 × 32 crop.
Mean and standard deviation normalisation is also applied.
The setting of the training hyperparameters (e.g. minibatch
size, initial learning rate, weight decay) match those suggested by the original papers. We report the performance
of each baseline and its SENet counterpart on CIFAR-10
in Table 4 and performance on CIFAR-100 in Table 5. We
observe that in every comparison SENets outperform the
baseline architectures, suggesting that the beneﬁts of SE
blocks are not conﬁned to the ImageNet dataset.
Scene Classiﬁcation
We also conduct experiments on the Places365-Challenge
dataset for scene classiﬁcation. This dataset comprises
8 million training images and 36, 500 validation images
across 365 categories. Relative to classiﬁcation, the task of
scene understanding offers an alternative assessment of a
model’s ability to generalise well and handle abstraction.
This is because it often requires the model to handle more
complex data associations and to be robust to a greater level
of appearance variation.
We opted to use ResNet-152 as a strong baseline to
assess the effectiveness of SE blocks and follow the training
and evaluation protocols described in , . In these
experiments, models are trained from scratch. We report
the results in Table 6, comparing also with prior work. We
observe that SE-ResNet-152 (11.01% top-5 error) achieves a
lower validation error than ResNet-152 (11.61% top-5 error),
Single-crop error rates (%) on Places365 validation set.
top-1 err.
top-5 err.
Places-365-CNN 
ResNet-152 (ours)
SE-ResNet-152
Faster R-CNN object detection results (%) on COCO minival set.
AP@IoU=0.5
SE-ResNet-50
ResNet-101
SE-ResNet-101
providing evidence that SE blocks can also yield improvements for scene classiﬁcation. This SENet surpasses the
previous state-of-the-art model Places-365-CNN which
has a top-5 error of 11.48% on this task.
Object Detection on COCO
We further assess the generalisation of SE blocks on the
task of object detection using the COCO dataset . As
in previous work , we use the minival protocol, i.e.,
training the models on the union of the 80k training set
and a 35k val subset and evaluating on the remaining
5k val subset. Weights are initialised by the parameters
of the model trained on the ImageNet dataset. We use
the Faster R-CNN detection framework as the basis
for evaluating our models and follow the hyperparameter
setting described in (i.e., end-to-end training with the
’2x’ learning schedule). Our goal is to evaluate the effect
of replacing the trunk architecture (ResNet) in the object
detector with SE-ResNet, so that any changes in performance can be attributed to better representations. Table 7
reports the validation set performance of the object detector
using ResNet-50, ResNet-101 and their SE counterparts as
trunk architectures. SE-ResNet-50 outperforms ResNet-50
by 2.4% (a relative 6.3% improvement) on COCO’s standard AP metric and by 3.1% on AP@IoU=0.5. SE blocks
also beneﬁt the deeper ResNet-101 architecture achieving
a 2.0% improvement (5.0% relative improvement) on the
AP metric. In summary, this set of experiments demonstrate
the generalisability of SE blocks. The induced improvements
can be realised across a broad range of architectures, tasks
and datasets.
ILSVRC 2017 Classiﬁcation Competition
SENets formed the foundation of our submission to the
ILSVRC competition where we achieved ﬁrst place. Our
winning entry comprised a small ensemble of SENets that
employed a standard multi-scale and multi-crop fusion
strategy to obtain a top-5 error of 2.251% on the test set.
As part of this submission, we constructed an additional
model, SENet-154, by integrating SE blocks with a modiﬁed
ResNeXt (the details of the architecture are provided
in Appendix). We compare this model with prior work on
the ImageNet validation set in Table 8 using standard crop
Single-crop error rates (%) of state-of-the-art CNNs on ImageNet
validation set with crop sizes 224 × 224 and 320 × 320 / 299 × 299.
320 × 320 /
ResNet-152 
ResNet-200 
Inception-v3 
Inception-v4 
Inception-ResNet-v2 
ResNeXt-101 (64 × 4d) 
DenseNet-264 
Attention-92 
PyramidNet-200 
DPN-131 
Comparison (%) with state-of-the-art CNNs on ImageNet validation set
using larger crop sizes/additional training data. †This model was
trained with a crop size of 320 × 320.
Very Deep PolyNet 
NASNet-A (6 @ 4032) 
PNASNet-5 (N=4,F=216) 
SENet-154†
AmoebaNet-C 
ResNeXt-101 32 × 48d 
sizes (224 × 224 and 320 × 320). We observe that SENet-154
achieves a top-1 error of 18.68% and a top-5 error of 4.47%
using a 224 × 224 centre crop evaluation, which represents
the strongest reported result.
Following the challenge there has been a great deal of
further progress on the ImageNet benchmark. For comparison, we include the strongest results that we are currently
aware of in Table 9. The best performance using only ImageNet data was recently reported by . This method
uses reinforcement learning to develop new policies for
data augmentation during training to improve the performance of the architecture searched by . The best overall
performance was reported by using a ResNeXt-101
32×48d architecture. This was achieved by pretraining their
model on approximately one billion weakly labelled images
and ﬁnetuning on ImageNet. The improvements yielded by
more sophisticated data augmentation and extensive
pretraining may be complementary to our proposed
changes to the network architecture.
ABLATION STUDY
In this section we conduct ablation experiments to gain a
better understanding of the effect of using different con-
ﬁgurations on components of the SE blocks. All ablation
experiments are performed on the ImageNet dataset on a
single machine (with 8 GPUs). ResNet-50 is used as the
backbone architecture. We found empirically that on ResNet
architectures, removing the biases of the FC layers in the
excitation operation facilitates the modelling of channel
dependencies, and use this conﬁguration in the following
Single-crop error rates (%) on ImageNet and parameter sizes for
SE-ResNet-50 at different reduction ratios. Here, original refers to
ResNet-50.
top-1 err.
top-5 err.
experiments. The data augmentation strategy follows the
approach described in Section 5.1. To allow us to study the
upper limit of performance for each variant, the learning
rate is initialised to 0.1 and training continues until the
validation loss plateaus2 (∼300 epochs in total). The learning rate is then reduced by a factor of 10 and then this
process is repeated (three times in total). Label-smoothing
regularisation is used during training.
Reduction ratio
The reduction ratio r introduced in Eqn. 5 is a hyperparameter which allows us to vary the capacity and computational cost of the SE blocks in the network. To investigate
the trade-off between performance and computational cost
mediated by this hyperparameter, we conduct experiments
with SE-ResNet-50 for a range of different r values. The
comparison in Table 10 shows that performance is robust to
a range of reduction ratios. Increased complexity does not
improve performance monotonically while a smaller ratio
dramatically increases the parameter size of the model. Setting r = 16 achieves a good balance between accuracy and
complexity. In practice, using an identical ratio throughout
a network may not be optimal (due to the distinct roles
performed by different layers), so further improvements
may be achievable by tuning the ratios to meet the needs
of a given base architecture.
Squeeze Operator
We examine the signiﬁcance of using global average pooling
as opposed to global max pooling as our choice of squeeze
operator (since this worked well, we did not consider more
sophisticated alternatives). The results are reported in Table 11. While both max and average pooling are effective,
average pooling achieves slightly better performance, justifying its selection as the basis of the squeeze operation.
However, we note that the performance of SE blocks is fairly
robust to the choice of speciﬁc aggregation operator.
Excitation Operator
We next assess the choice of non-linearity for the excitation
mechanism. We consider two further options: ReLU and
tanh, and experiment with replacing the sigmoid with these
2. For reference, training with a 270 epoch ﬁxed schedule (reducing
the learning rate at 125, 200 and 250 epochs) achieves top-1 and top-5
error rates for ResNet-50 and SE-ResNet-50 of (23.21%, 6.53%) and
(22.20%, 6.00%) respectively.
Effect of using different squeeze operators in SE-ResNet-50 on
ImageNet (error rates %).
top-1 err.
top-5 err.
Effect of using different non-linearities for the excitation operator in
SE-ResNet-50 on ImageNet (error rates %).
Excitation
top-1 err.
top-5 err.
alternative non-linearities. The results are reported in Table 12. We see that exchanging the sigmoid for tanh slightly
worsens performance, while using ReLU is dramatically
worse and in fact causes the performance of SE-ResNet-50
to drop below that of the ResNet-50 baseline. This suggests
that for the SE block to be effective, careful construction of
the excitation operator is important.
Different stages
We explore the inﬂuence of SE blocks at different stages by
integrating SE blocks into ResNet-50, one stage at a time.
Speciﬁcally, we add SE blocks to the intermediate stages:
stage 2, stage 3 and stage 4, and report the results in Table 13. We observe that SE blocks bring performance beneﬁts
when introduced at each of these stages of the architecture.
Moreover, the gains induced by SE blocks at different stages
are complementary, in the sense that they can be combined
effectively to further bolster network performance.
Integration strategy
Finally, we perform an ablation study to assess the inﬂuence
of the location of the SE block when integrating it into existing architectures. In addition to the proposed SE design, we
consider three variants: (1) SE-PRE block, in which the SE
block is moved before the residual unit; (2) SE-POST block,
in which the SE unit is moved after the summation with
the identity branch (after ReLU) and (3) SE-Identity block,
in which the SE unit is placed on the identity connection in
parallel to the residual unit. These variants are illustrated
in Figure 5 and the performance of each variant is reported
in Table 14. We observe that the SE-PRE, SE-Identity and
proposed SE block each perform similarly well, while usage
Effect of integrating SE blocks with ResNet-50 at different stages on
ImageNet (error rates %).
top-1 err.
top-5 err.
SE Stage 2
SE Stage 3
SE Stage 4
Effect of different SE block integration strategies with ResNet-50 on
ImageNet (error rates %).
top-1 err.
top-5 err.
SE-Identity
Effect of integrating SE blocks at the 3x3 convolutional layer of each
residual branch in ResNet-50 on ImageNet (error rates %).
top-1 err.
top-5 err.
of the SE-POST block leads to a drop in performance. This
experiment suggests that the performance improvements
produced by SE units are fairly robust to their location,
provided that they are applied prior to branch aggregation.
In the experiments above, each SE block was placed
outside the structure of a residual unit. We also construct
a variant of the design which moves the SE block inside
the residual unit, placing it directly after the 3 × 3 convolutional layer. Since the 3 × 3 convolutional layer possesses
fewer channels, the number of parameters introduced by the
corresponding SE block is also reduced. The comparison in
Table 15 shows that the SE 3×3 variant achieves comparable
classiﬁcation accuracy with fewer parameters than the standard SE block. Although it is beyond the scope of this work,
we anticipate that further efﬁciency gains will be achievable
by tailoring SE block usage for speciﬁc architectures.
ROLE OF SE BLOCKS
Although the proposed SE block has been shown to improve network performance on multiple visual tasks, we
would also like to understand the relative importance of
the squeeze operation and how the excitation mechanism
operates in practice. A rigorous theoretical analysis of the
representations learned by deep neural networks remains
challenging, we therefore take an empirical approach to
examining the role played by the SE block with the goal of
attaining at least a primitive understanding of its practical
Effect of Squeeze
To assess whether the global embedding produced by the
squeeze operation plays an important role in performance,
we experiment with a variant of the SE block that adds an
equal number of parameters, but does not perform global
average pooling. Speciﬁcally, we remove the pooling operation and replace the two FC layers with corresponding
1 × 1 convolutions with identical channel dimensions in
the excitation operator, namely NoSqueeze, where the excitation output maintains the spatial dimensions as input.
In contrast to the SE block, these point-wise convolutions
can only remap the channels as a function of the output
of a local operator. While in practice, the later layers of a
deep network will typically possess a (theoretical) global
(a) Residual block
(b) Standard SE block
(c) SE-PRE block
(d) SE-POST block
(e) SE-Identity block
Fig. 5. SE block integration designs explored in the ablation study.
(a) SE_2_3
(b) SE_3_4
(c) SE_4_6
(d) SE_5_1
(e) SE_5_2
(f) SE_5_3
Fig. 6. Activations induced by the Excitation operator at different depths in the SE-ResNet-50 on ImageNet. Each set of activations is named
according to the following scheme: SE_stageID_blockID. With the exception of the unusual behaviour at SE_5_2, the activations become
increasingly class-speciﬁc with increasing depth.
Effect of Squeeze operator on ImageNet (error rates %).
top-1 err.
top-5 err.
receptive ﬁeld, global embeddings are no longer directly
accessible throughout the network in the NoSqueeze variant.
The accuracy and computational complexity of both models
are compared to a standard ResNet-50 model in Table 16. We
observe that the use of global information has a signiﬁcant
inﬂuence on the model performance, underlining the importance of the squeeze operation. Moreover, in comparison
to the NoSqueeze design, the SE block allows this global
information to be used in a computationally parsimonious
Role of Excitation
To provide a clearer picture of the function of the excitation
operator in SE blocks, in this section we study example
activations from the SE-ResNet-50 model and examine their
distribution with respect to different classes and different
input images at various depths in the network. In particular,
we would like to understand how excitations vary across
images of different classes, and across images within a class.
We ﬁrst consider the distribution of excitations for different classes. Speciﬁcally, we sample four classes from the
ImageNet dataset that exhibit semantic and appearance diversity, namely goldﬁsh, pug, plane and cliff (example images
from these classes are shown in Appendix). We then draw
ﬁfty samples for each class from the validation set and
compute the average activations for ﬁfty uniformly sampled
channels in the last SE block of each stage (immediately
prior to downsampling) and plot their distribution in Fig. 6.
For reference, we also plot the distribution of the mean
activations across all of the 1000 classes.
We make the following three observations about the
role of the excitation operation. First, the distribution across
different classes is very similar at the earlier layers of the
network, e.g. SE 2 3. This suggests that the importance of
feature channels is likely to be shared by different classes in
the early stages. The second observation is that at greater
(a) SE_2_3
(b) SE_3_4
(c) SE_4_6
(d) SE_5_1
(e) SE_5_2
(f) SE_5_3
Fig. 7. Activations induced by Excitation in the different modules of SE-ResNet-50 on image samples from the goldﬁsh and plane classes of
ImageNet. The module is named “SE_stageID_blockID”.
depth, the value of each channel becomes much more
class-speciﬁc as different classes exhibit different preferences to the discriminative value of features, e.g. SE 4 6 and
SE 5 1. These observations are consistent with ﬁndings in
previous work , , namely that earlier layer features
are typically more general (e.g. class agnostic in the context
of the classiﬁcation task) while later layer features exhibit
greater levels of speciﬁcity .
Next, we observe a somewhat different phenomena in
the last stage of the network. SE 5 2 exhibits an interesting
tendency towards a saturated state in which most of the
activations are close to one. At the point at which all
activations take the value one, an SE block reduces to the
identity operator. At the end of the network in the SE 5 3
(which is immediately followed by global pooling prior
before classiﬁers), a similar pattern emerges over different
classes, up to a modest change in scale (which could be
tuned by the classiﬁers). This suggests that SE 5 2 and
SE 5 3 are less important than previous blocks in providing
recalibration to the network. This ﬁnding is consistent with
the result of the empirical investigation in Section 4 which
demonstrated that the additional parameter count could be
signiﬁcantly reduced by removing the SE blocks for the last
stage with only a marginal loss of performance.
Finally, we show the mean and standard deviations of
the activations for image instances within the same class
for two sample classes (goldﬁsh and plane) in Fig. 7. We
observe a trend consistent with the inter-class visualisation,
indicating that the dynamic behaviour of SE blocks varies
over both classes and instances within a class. Particularly
in the later layers of the network where there is considerable diversity of representation within a single class, the
network learns to take advantage of feature recalibration to
improve its discriminative performance . In summary,
SE blocks produce instance-speciﬁc responses which nevertheless function to support the increasingly class-speciﬁc
needs of the model at different layers in the architecture.
CONCLUSION
In this paper we proposed the SE block, an architectural
unit designed to improve the representational power of a
network by enabling it to perform dynamic channel-wise
feature recalibration. A wide range of experiments show
the effectiveness of SENets, which achieve state-of-the-art
performance across multiple datasets and tasks. In addition,
SE blocks shed some light on the inability of previous
architectures to adequately model channel-wise feature dependencies. We hope this insight may prove useful for other
tasks requiring strong discriminative features. Finally, the
feature importance values produced by SE blocks may be
of use for other tasks such as network pruning for model
compression.
ACKNOWLEDGMENTS
The authors would like to thank Chao Li and Guangyuan
Wang from Momenta for their contributions in the training
system optimisation and experiments on CIFAR dataset. We
would also like to thank Andrew Zisserman, Aravindh Mahendran and Andrea Vedaldi for many helpful discussions.
The work is supported in part by NSFC Grants (61632003,
61620106003, 61672502, 61571439), National Key R&D Program of China (2017YFB1002701), and Macao FDCT Grant
 . Samuel Albanie is supported by EPSRC
AIMS CDT EP/L015897/1.
APPENDIX: DETAILS OF SENET-154
SENet-154 is constructed by incorporating SE blocks into a
modiﬁed version of the 64×4d ResNeXt-152 which extends
(a) goldﬁsh
Fig. 8. Sample images from the four classes of ImageNet used in the
experiments described in Sec. 7.2.
the original ResNeXt-101 by adopting the block stacking strategy of ResNet-152 . Further differences to the
design and training of this model (beyond the use of SE
blocks) are as follows: (a) The number of the ﬁrst 1 × 1
convolutional channels for each bottleneck building block
was halved to reduce the computational cost of the model
with a minimal decrease in performance. (b) The ﬁrst 7 × 7
convolutional layer was replaced with three consecutive
3 × 3 convolutional layers. (c) The 1 × 1 down-sampling
projection with stride-2 convolution was replaced with a
3 × 3 stride-2 convolution to preserve information. (d) A
dropout layer (with a dropout ratio of 0.2) was inserted
before the classiﬁcation layer to reduce overﬁtting. (e) Labelsmoothing regularisation (as introduced in ) was used
during training. (f) The parameters of all BN layers were
frozen for the last few training epochs to ensure consistency
between training and testing. (g) Training was performed
with 8 servers (64 GPUs) in parallel to enable large batch
sizes . The initial learning rate was set to 1.0.