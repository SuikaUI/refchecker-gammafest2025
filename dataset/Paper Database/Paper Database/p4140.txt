SelFlow: Self-Supervised Learning of Optical Flow
Pengpeng Liu‚Ä†‚àó, Michael Lyu‚Ä†, Irwin King‚Ä†, Jia Xu¬ß
‚Ä† The Chinese University of Hong Kong, ¬ß Tencent AI Lab
We present a self-supervised learning approach for optical Ô¨Çow.
Our method distills reliable Ô¨Çow estimations
from non-occluded pixels, and uses these predictions as
ground truth to learn optical Ô¨Çow for hallucinated occlusions. We further design a simple CNN to utilize temporal information from multiple frames for better Ô¨Çow estimation. These two principles lead to an approach that yields
the best performance for unsupervised optical Ô¨Çow learning on the challenging benchmarks including MPI Sintel,
KITTI 2012 and 2015. More notably, our self-supervised
pre-trained model provides an excellent initialization for supervised Ô¨Åne-tuning. Our Ô¨Åne-tuned models achieve stateof-the-art results on all three datasets. At the time of writing, we achieve EPE=4.26 on the Sintel benchmark, outperforming all submitted methods.
1. Introduction
Optical Ô¨Çow estimation is a core building block for a variety of computer vision systems . Despite
decades of development, accurate Ô¨Çow estimation remains
an open problem due to one key challenge: occlusion. Traditional approaches minimize an energy function to encourage association of visually similar pixels and regularize incoherent motion to propagate Ô¨Çow estimation from nonoccluded pixels to occluded pixels . However,
this family of methods is often time-consuming and not applicable for real-time applications.
Recent studies learn to estimate optical Ô¨Çow end-toend from images using convolutional neural networks
(CNNs) . However, training fully supervised CNNs requires a large amount of labeled training
data, which is extremely difÔ¨Åcult to obtain for optical Ô¨Çow,
especially when there are occlusions. Considering the recent performance improvements obtained when employing
hundreds of millions of labeled images , it is obvious
that the size of training data is a key bottleneck for optical
Ô¨Çow estimation.
‚àóWork mainly done during an internship at Tencent AI Lab.
In the absence of large-scale real-world annotations,
existing methods turn to pre-train on synthetic labeled
datasets and then Ô¨Åne-tune on small annotated
datasets . However, there usually exists a large
gap between the distribution of synthetic data and natural scenes. In order to train a stable model, we have to
carefully follow speciÔ¨Åc learning schedules across different
datasets .
One promising direction is to develop unsupervised optical Ô¨Çow learning methods that beneÔ¨Åt from unlabeled data.
The basic idea is to warp the target image towards the reference image according to the estimated optical Ô¨Çow, then
minimize the difference between the reference image and
the warped target image using a photometric loss .
Such idea works well for non-occluded pixels but turns to
provide misleading information for occluded pixels. Recent
methods propose to exclude those occluded pixels when
computing the photometric loss or employ additional spatial and temporal smoothness terms to regularize Ô¨Çow estimation . Most recently, DDFlow proposes a data distillation approach, which employs random
cropping to create occlusions for self-supervision. Unfortunately, these methods fails to generalize well for all natural
occlusions. As a result, there is still a large performance
gap comparing unsupervised methods with state-of-the-art
fully supervised methods.
Is it possible to effectively learn optical Ô¨Çow with occlusions? In this paper, we show that a self-supervised approach can learn to estimate optical Ô¨Çow with any form of
occlusions from unlabeled data. Our work is based on distilling reliable Ô¨Çow estimations from non-occluded pixels,
and using these predictions to guide the optical Ô¨Çow learning for hallucinated occlusions. Figure 1 illustrates our idea
to create synthetic occlusions by perturbing superpixels. We
further utilize temporal information from multiple frames to
improve Ô¨Çow prediction accuracy within a simple CNN architecture. The resulted learning approach yields the highest accuracy among all unsupervised optical Ô¨Çow learning
methods on Sintel and KITTI benchmarks.
Surprisingly, our self-supervised pre-trained model provides an excellent initialization for supervised Ô¨Åne-tuning.
At the time of writing, our Ô¨Åne-tuned model achieves the
 
(a) Reference Image ùêºùë°
(b) Target Image ùêºùë°+1
(c) Ground Truth Flow ùê∞ùë°‚Üíùë°+1
(d) Warped Target Image ùêºùë°+1‚Üíùë°
(e) SILC Superpixel
(h) New Occlusion Map ùëÇ ùë°‚Üíùë°+1
(i) Self-Supervision Mask ùëÄùë°‚Üíùë°+1
(g) Occlusion Map ùëÇùë°‚Üíùë°+1
Figure 1. A toy example to illustrate our self-supervised learning idea. We Ô¨Årst train our NOC-model with the classical photometric loss
(measuring the difference between the reference image (a) and the warped target image(d)), guided by the occlusion map (g). Then we
perturbate randomly selected superpixels in the target image (b) to hallucinate occlusions. Finally, we use reliable Ô¨Çow estimations from
our NOC-Model to guide the learning of our OCC-Model for those newly occluded pixels (denoted by self-supervision mask (i), where
value 1 means the pixel is non-occluded in (g) but occluded in (h)). Note the yellow region is part of the moving dog. Our self-supervised
approach learns optical Ô¨Çow for both moving objects and static scenes.
highest reported accuracy (EPE=4.26) on the Sintel benchmark. Our approach also signiÔ¨Åcantly outperforms all published optical Ô¨Çow methods on the KITTI 2012 benchmark,
and achieves highly competitive results on the KITTI 2015
benchmark. To the best of our knowledge, it is the Ô¨Årst time
that a supervised learning method achieves such remarkable
accuracies without using any external labeled data.
2. Related Work
Classical Optical Flow Estimation. Classical variational
approaches model optical Ô¨Çow estimation as an energy
minimization problem based on brightness constancy and
spatial smoothness .
Such methods are effective for
small motion, but tend to fail when displacements are large.
Later works integrate feature matching to initialize sparse
matching, and then interpolate into dense Ô¨Çow maps in
a pyramidal coarse-to-Ô¨Åne manner .
works use convolutional neural networks (CNNs) to improve sparse matching by learning an effective feature embedding . However, these methods are often computationally expensive and can not be trained end-to-end. One
natural extension to improve robustness and accuracy for
Ô¨Çow estimation is to incorporate temporal information over
multiple frames. A straightforward way is to add temporal
constraints such as constant velocity , constant
acceleration , low-dimensional linear subspace ,
or rigid/non-rigid segmentation . While these formulations are elegant and well-motivated, our method is much
simpler and does not rely on any assumption of the data. Instead, our approach directly learns optical Ô¨Çow for a much
wider range of challenging cases existing in the data.
Supervised Learning of Optical Flow. One promising direction is to learn optical Ô¨Çow with CNNs. FlowNet 
is the Ô¨Årst end-to-end optical Ô¨Çow learning framework. It
takes two consecutive images as input and outputs a dense
Ô¨Çow map. The following work FlowNet 2.0 stacks
several basic FlowNet models for iterative reÔ¨Ånement, and
signiÔ¨Åcantly improves the accuracy. SpyNet proposes
to warp images at multiple scales to cope with large displacements, resulting in a compact spatial pyramid network.
Correlation
Correlation
Cost Volume
Cost Volume
Cost Volume
Cost Volume
Resolution
Figure 2. Our network architecture at each level (similar to PWC-
Net ). Àôwl denotes the initial coarse Ô¨Çow of level l and ÀÜF l denotes the warped feature representation. At each level, we swap
the initial Ô¨Çow and cost volume as input to estimate both forward and backward Ô¨Çow concurrently. Then these estimations are
passed to layer l ‚àí1 to estimate higher-resolution Ô¨Çow.
Recently, PWC-Net and LiteFlowNet propose to
warp features extracted from CNNs and achieve state-ofthe-art results with lightweight framework. However, obtaining high accuracy with these CNNs requires pre-training
on multiple synthetic datasets and follows speciÔ¨Åc training
schedules . In this paper, we reduce the reliance on
pre-training with synthetic data, and propose an effective
self-supervised training method with unlabeled data.
Unsupervised Learning of Optical Flow. Another interesting line of work is unsupervised optical Ô¨Çow learning.
The basic principles are based on brightness constancy and
spatial smoothness . This leads to the most popular
photometric loss, which measures the difference between
the reference image and the warped image. Unfortunately,
this loss does not hold for occluded pixels. Recent studies
propose to Ô¨Årst obtain an occlusion map and then exclude
those occluded pixels when computing the photometric difference . Janai et al. introduces to estimate
optical Ô¨Çow with a multi-frame formulation and more advanced occlusion reasoning, achieving state-of-the-art unsupervised results. Very recently, DDFlow proposes
a data distillation approach to learning the optical Ô¨Çow of
occluded pixels, which works particularly well for pixels
near image boundaries. Nonetheless, all these unsupervised
learning methods only handle speciÔ¨Åc cases of occluded
pixels. They lack the ability to reason about the optical
Ô¨Çow of all possible occluded pixels. In this work, we address this issue by a superpixel-based occlusion hallucination technique.
Self-Supervised Learning. Our work is closely related to
the family of self-supervised learning methods, where the
supervision signal is purely generated from the data itself. It
is widely used for learning feature representations from unlabeled data . A pretext task is usually employed, such
as image inpainting , image colorization , solving
Forwardbackward
consistency
ùêºt‚àí2 & ùêºùë°‚àí1 & ùêºùë°
ùêºt‚àí1 & ùêºùë° & ùêºùë°+1
ùêºt & ùêºùë°+1 & ùêºùë°+2
Figure 3. Data Ô¨Çow for self-training with multiple-frame. To estimate occlusion map for three-frame Ô¨Çow learning, we use Ô¨Åve images as input. This way, we can conduct a forward-backward consistency check to estimate occlusion maps between It and It+1,
between It and It‚àí1 respectively.
Jigsaw puzzles . Pathak et al. propose to explore
low-level motion-based cues to learn feature representations
without manual supervision. Doersch et al. combine
multiple self-supervised learning tasks to train a single visual representation. In this paper, we make use of the domain knowledge of optical Ô¨Çow, and take reliable predictions of non-occluded pixels as the self-supervision signal
to guide our optical Ô¨Çow learning of occluded pixels.
In this section, we present our self-supervised approach
to learning optical Ô¨Çow from unlabeled data. To this end,
we train two CNNs (NOC-Model and OCC-Model) with
the same network architecture. The former focuses on accurate Ô¨Çow estimation for non-occluded pixels, and the latter
learns to predict optical Ô¨Çow for all pixels. We distill reliable non-occluded Ô¨Çow estimations from NOC-Model to
guide the learning of OCC-Model for those occluded pixels. Only OCC-Model is needed at testing. We build our
network based on PWC-Net and further extend it to
multi-frame optical Ô¨Çow estimation (Figure 2). Before describing our approach in detail, we Ô¨Årst deÔ¨Åne our notations.
3.1. Notation
Given three consecutive RGB images It‚àí1, It, It+1, our
goal is to estimate the forward optical Ô¨Çow from It to It+1.
Let wi‚Üíj denote the Ô¨Çow from Ii to Ij, e.g., wt‚Üít+1 denotes the forward Ô¨Çow from It to It+1, wt‚Üít‚àí1 denotes
the backward Ô¨Çow from It to It‚àí1. After obtaining optical Ô¨Çow, we can backward warp the target image to reconstruct the reference image using Spatial Transformer Network . Here, we use Iw
j‚Üíi to denote warping Ij to
Ii with Ô¨Çow wi‚Üíj. Similarly, we use Oi‚Üíj to denote the
occlusion map from Ii to Ij, where value 1 means the pixel
in Ii is not visible in Ij.
In our self-supervised setting, we create the new target
image eIt+1 by injecting random noise on superpixels for
occlusion generation. We can inject noise to any of three
consecutive frames and even multiple of them as shown in
Figure 1. For brevity, here we choose It+1 as an example.
(a) Reference Image
(b) GT Flow
(c) Our Flow
(d) GT Occlusion
(e) Our Occlusion
Figure 4. Sample unsupervised results on Sintel and KITTI dataset. From top to bottom, we show samples from Sintel Final, KITTI 2012
and KITTI 2015. Our model can estimate both accurate Ô¨Çow and occlusion map. Note that on KITTI datasets, the occlusion maps are
sparse, which only contain pixels moving out of the image boundary.
If we let It‚àí1, It and eIt+1 as input, then ew, eO, eIw represent
the generated optical Ô¨Çow, occlusion map and warped image
respectively.
3.2. CNNs for Multi-Frame Flow Estimation
In principle, our method can utilize any CNNs. In our
implementation, we build on top of the seminar PWC-
Net . PWC-Net employs pyramidal processing to increase the Ô¨Çow resolution in a coarse-to-Ô¨Åne manner and
utilizes feature warping, cost volume construction to estimate optical Ô¨Çow at each level. Based on these principles,
it has achieved state-of-the-art performance with a compact
model size.
As shown in Figure 2, our three-frame Ô¨Çow estimation
network structure is built upon two-frame PWC-Net with
several modiÔ¨Åcations to aggregate temporal information.
First, our network takes three images as input, thus produces three feature representations Ft‚àí1, Ft and Ft+1. Second, apart from forward Ô¨Çow wt‚Üít+1 and forward cost volume, out model also computes backward Ô¨Çow wt‚Üít‚àí1 and
backward cost volume at each level simultaneously. Note
that when estimating forward Ô¨Çow, we also utilize the initial backward Ô¨Çow and backward cost volume information.
This is because past frame It‚àí1 can provide very valuable
information, especially for those regions that are occluded
in the future frame It+1 but not occluded in It‚àí1. Our network combines all this information together and therefore
estimates optical Ô¨Çow more accurately.
Third, we stack
initial forward Ô¨Çow Àôwl
t‚Üít+1, minus initial backward Ô¨Çow
t+1‚Üít, feature of reference image F l
t, forward cost volume and backward cost volume to estimate the forward Ô¨Çow
at each level. For backward Ô¨Çow, we just swap the Ô¨Çow and
cost volume as input. Forward and backward Ô¨Çow estimation networks share the same network structure and weights.
For initial Ô¨Çow at each level, we upscale optical Ô¨Çow of the
next level both in resolution and magnitude.
3.3. Occlusion Estimation
For two-frame optical Ô¨Çow estimation, we can swap two
images as input to generate forward and backward Ô¨Çow,
then the occlusion map can be generated based on the
forward-backward consistency prior . To make this
work under our three-frame setting, we propose to utilize
the adjacent Ô¨Åve frame images as input as shown in Figure 3.
SpeciÔ¨Åcally, we estimate bi-directional Ô¨Çows between It and It+1, namely wt‚Üít+1 and wt+1‚Üít. Similarly,
we also estimate the Ô¨Çows between It and It‚àí1. Finally,
we conduct a forward and backward consistency check to
reason the occlusion map between two consecutive images.
For forward-backward consistency check, we consider
one pixel as occluded when the mismatch between the forward Ô¨Çow and the reversed forward Ô¨Çow is too large. Take
Ot‚Üít+1 as an example, we can Ô¨Årst compute the reversed
forward Ô¨Çow as follows,
ÀÜwt‚Üít+1 = wt+1‚Üít(p + wt‚Üít+1(p)),
A pixel is considered occluded whenever it violates the following constraint:
|wt‚Üít+1 + ÀÜwt‚Üít+1|2 < Œ±1(|wt‚Üít+1|2 + |ÀÜwt‚Üít+1|2) + Œ±2,
where we set Œ±1 = 0.01, Œ±2 = 0.05 for all our experiments.
Other occlusion maps are computed in the same way.
3.4. Occlusion Hallucination
During our self-supervised training, we hallucinate occlusions by perturbing local regions with random noise. In
a newly generated target image, the pixels corresponding
to noise regions automatically become occluded.
are many ways to generate such occlusions.
Sintel Clean
Sintel Final
KITTI 2012
KITTI 2015
Unsupervised
BackToBasic+ft 
DSTFlow+ft 
UnFlow-CSS 
OccAwareFlow+ft 
MultiFrameOccFlow-None+ft 
MultiFrameOccFlow-Soft+ft 
DDFlow+ft 
Supervised
FlowNetS+ft 
FlowNetC+ft 
SpyNet+ft 
FlowFieldsCNN+ft 
DCFlow+ft 
FlowNet2+ft 
UnFlow-CSS+ft 
LiteFlowNet+ft-CVPR 
LiteFlowNet+ft-axXiv 
PWC-Net+ft-CVPR 
PWC-Net+ft-axXiv 
ProFlow+ft 
ContinualFlow+ft 
MFF+ft 
Table 1. Comparison with state-of-the-art learning based optical Ô¨Çow estimation methods. Our method outperforms all unsupervised
optical Ô¨Çow learning approaches on all datasets. Our supervised Ô¨Åne-tuned model achieves the highest accuracy on the Sintel Final dataset
and KITTI 2012 dataset. All numbers are EPE except for the last column of KITTI 2012 and KITTI 2015 testing sets, where we report
percentage of erroneous pixels over all pixels (Fl-all). Missing entries (-) indicate that the results are not reported for the respective method.
Parentheses mean that the training and testing are performed on the same dataset. Bold fonts highlight the best results among unsupervised
and supervised methods respectively.
straightforward way is to randomly select rectangle regions.
However, rectangle occlusions rarely exist in real-world sequences. To address this issue, we propose to Ô¨Årst generate superpixels , then randomly select several superpixels and Ô¨Åll them with noise. There are two main advantages
of using superpixel. First, the shape of a superpixel is usually random and superpixel edges are often part of object
boundaries. The is consistent with the real-world cases and
makes the noise image more realistic. We can choose several superpixels which locate at different locations to cover
more occlusion cases. Second, the pixels within each superpixel usually belong to the same object or have similar
Ô¨Çow Ô¨Åelds. Prior work has found low-level segmentation is
helpful for optical Ô¨Çow estimation . Note that the random noise should lie in the pixel value range.
Figure 1 shows a simple example, where only the dog
extracted from the COCO dataset is moving. Initially,
the occlusion map between It and It+1 is (g). After randomly selecting several superpixels from (e) to inject noise,
the occlusion map between It and eIt+1 change to (h). Next,
we describe how to make use of these occlusion maps to
guide our self-training.
3.5. NOC-to-OCC as Self-Supervision
Our self-training idea is built on top of the classical photometric loss , which is highly effective for nonoccluded pixels. Figure 1 illustrates our main idea. Suppose
pixel p1 in image It is not occluded in It+1, and pixel p‚Ä≤
its corresponding pixel. If we inject noise to It+1 and let
It‚àí1, It, eIt+1 as input, p1 then becomes occluded. Good
news is we can still use the Ô¨Çow estimation of NOC-Model
as annotations to guide OCC-Model to learn the Ô¨Çow of p1
from It to eIt+1. This is also consistent with real-world occlusions, where the Ô¨Çow of occluded pixels can be estimated
based on surrounding non-occluded pixels. In the example
of Figure 1, self-supervision is only employed to (i), which
represents those pixels non-occluded from It to It+1 but become occluded from It to eIt+1.
3.6. Loss Functions
Similar to previous unsupervised methods, we Ô¨Årst apply
photometric loss Lp to non-occluded pixels. Photometric
Reference Image (training)
Ground Truth
W/O Occlusion
W/O Self-Supervision
Two-frame Superpixel
Superpixel
Reference Image (testing)
Target image
W/O Occlusion
W/O Self-Supervision
Two-frame Superpixel
Superpixel
Figure 5. Qualitative comparison of our model under different settings on Sintel Clean training and Sintel Final testing dataset. Occlusion
handling, multi-frame formulation and self-supervision consistently improve the performance.
loss is deÔ¨Åned as follows:
P œà(Ii ‚àíIw
j‚Üíi) ‚äô(1 ‚àíOi)
where œà(x) = (|x|+œµ)q is a robust loss function, ‚äôdenotes
the element-wise multiplication. We set œµ = 0.01, q = 0.4
for all our experiments. Only Lp is necessary to train the
NOC-Model.
To train our OCC-Model to estimate optical Ô¨Çow of occluded pixels, we deÔ¨Åne a self-supervision loss Lo for those
synthetic occluded pixels (Figure 1(i)). First, we compute a
self-supervision mask M to represent these pixels,
Mi‚Üíj = clip( eOi‚Üíj ‚àíOi‚Üíj, 0, 1)
Then, we deÔ¨Åne our self-supervision loss Lo as,
P œà(wi‚Üíj ‚àíewi‚Üíj) ‚äôMi‚Üíj
For our OCC-Model, we train with a simple combination of
Lp + Lo for both non-occluded pixels and occluded pixels.
Note our loss functions do not rely on spatial and temporal consistent assumptions, and they can be used for both
classical two-frame Ô¨Çow estimation and multi-frame Ô¨Çow
estimation.
3.7. Supervised Fine-tuning
After pre-training on raw dataset, we use real-world annotated data for Ô¨Åne-tuning. Since there are only annotations for forward Ô¨Çow wt‚Üít+1, we skip backward Ô¨Çow estimation when computing our loss. Suppose that the ground
truth Ô¨Çow is wgt
t‚Üít+1, and mask V denotes whether the pixel
has a label, where value 1 means that the pixel has a valid
ground truth Ô¨Çow. Then we can obtain the supervised Ô¨Ånetuning loss as follows,
t‚Üít+1 ‚àíwt‚Üít+1) ‚äôV )/
During Ô¨Åne-tuning, We Ô¨Årst initialize the model with the
pre-trained OCC-Model on each dataset, then optimize it
4. Experiments
We evaluate and compare our methods with stateof-the-art unsupervised and supervised learning methods
on public optical Ô¨Çow benchmarks including MPI Sintel , KITTI 2012 and KITTI 2015 .
ensure reproducibility and advance further innovations,
we make our code and models publicly available at
 
4.1. Implementation Details
Data Preprocessing. For Sintel, we download the Sintel
movie and extract ‚àº10, 000 images for self-training. We
Ô¨Årst train our model on this raw data, then add the ofÔ¨Åcial
Sintel training data (including both ‚ÄùÔ¨Ånal‚Äù and ‚Äùclean‚Äù versions). For KITTI 2012 and KITTI 2015, we use multi-view
extensions of the two datasets for unsupervised pre-training,
similar to . During training, we exclude the image
pairs with ground truth Ô¨Çow and their neighboring frames
(frame number 9-12) to avoid the mixture of training and
testing data.
Reference Image (training)
Ground Truth
W/O Occlusion
W/O Self-Supervision
Two-frame Superpixel
Superpixel
Reference Image (testing)
Target image
W/O Occlusion
W/O Self-Supervision
Two-frame Superpixel
Superpixel
Figure 6. Qualitative comparison of our model under different settings on KITTI 2015 training and testing dataset. Occlusion handling,
multi-frame formulation and self-supervision consistently improve the performance.
We rescale the pixel value from to for
unsupervised training, while normalizing each channel to
be standard normal distribution for supervised Ô¨Åne-tuning.
This is because normalizing image as input is more robust
for luminance changing, which is especially helpful for optical Ô¨Çow estimation. For unsupervised training, we apply
Census Transform to images, which has been proved
robust for optical Ô¨Çow estimation .
Training procedure. We train our model with the Adam
optimizer and set batch size to be 4 for all experiments.
For unsupervised training, we set the initial learning rate to
be 10‚àí4, decay it by half every 50k iterations, and use random cropping, random Ô¨Çipping, random channel swapping
during data augmentation. For supervised Ô¨Åne-tuning, we
employ similar data augmentation and learning rate schedule as .
For unsupervised pre-training, we Ô¨Årst train our NOC-
Model with photometric loss for 200k iterations. Then, we
add our occlusion regularization and train for another 500k
iterations. Finally, we initialize the OCC-Model with the
trained weights of NOC-Model and train it with Lp+Lo for
500k iterations. Since training two models simultaneously
will cost more memory and training time, we just generate the Ô¨Çow and occlusion maps using the NOC-Model in
advance and use them as annotations (just like KITTI with
sparse annotations).
For supervised Ô¨Åne-tuning, we use the pre-trained OCC-
Model as initialization, and train the model using our supervised loss Ls with 500k iterations for KITTI and 1, 000k
iterations for Sintel. Note we do not require pre-training
our model on any labeled synthetic dataset, hence we do
not have to follow the speciÔ¨Åc training schedule (FlyingChairs ‚ÜíFlyingThings3D ) as .
Evaluation Metrics. We consider two widely-used metrics
to evaluate optical Ô¨Çow estimation: average endpoint error
(EPE), percentage of erroneous pixels (Fl). EPE is the ranking metric on the Sintel benchmark, and Fl is the ranking
metric on KITTI benchmarks.
4.2. Main Results
As shown in Table 1, we achieve state-of-the-art results
for both unsupervised and supervised optical Ô¨Çow learning on all datasets under all evaluation metrics. Figure 4
shows sample results from Sintel and KITTI. Our method
estimates both accurate optical Ô¨Çow and occlusion maps.
Unsupervised Learning. Our method achieves the highest accuracy for unsupervised learning methods on leading
benchmarks. On the Sintel Ô¨Ånal benchmark, we reduce the
previous best EPE from 7.40 to 6.57, with 11.2% relative improvements. This is even better than several fully
supervised methods including FlowNetS, FlowNetC ,
and SpyNet .
On the KITTI datasets, the improvement is more significant. For the training dataset, we achieve EPE=1.69 with
28.1% relative improvement on KITTI 2012 and EPE=4.84
with 15.3% relative improvement on KITTI 2015 compared with previous best unsupervised method DDFlow. On
KITTI 2012 testing set, we achieve Fl-all=7.68%, which
is better than state-of-the-art supervised methods including FlowNet2 , PWC-Net , ProFlow , and
MFF . On KITTI 2015 testing benchmark, we achieve
Fl-all 14.19%, better than all unsupervised methods. Our
unsupervised results also outperform some fully supervised
methods including DCFlow and ProFlow .
Supervised Fine-tuning. We further Ô¨Åne-tune our unsupervised model with the ground truth Ô¨Çow. We achieve stateof-the-art results on all three datasets, with Fl-all=6.19% on
KITTI 2012 and Fl-all=8.42% on KITTI 2015. Most importantly, our method yields EPE=4.26 on the Sintel Ô¨Ånal
dataset, achieving the highest accuracy on the Sintel benchmark among all submitted methods. All these show that
our method reduces the reliance of pre-training with syn-
Self-Supervision
Self-Supervision
Sintel Clean
Sintel Final
KITTI 2012
KITTI 2015
Superpixel
Table 2. Ablation study. We report EPE of our unsupervised results under different settings over all pixels (ALL), non-occluded pixels
(NOC) and occluded pixels (OCC). Note that we employ Census Transform when computing photometric loss by default. Without Census
Transform, the performance will drop.
Unsupervised Pre-training
Sintel Clean
Sintel Final
KITTI 2012
KITTI 2015
Table 3. Ablation study. We report EPE of supervised Ô¨Åne-tuning
results on our validation datasets with and without unsupervised
pre-training.
thetic datasets and we do not have to follow speciÔ¨Åc training
schedules across different datasets anymore.
4.3. Ablation Study
To demonstrate the usefulness of individual technical
steps, we conduct a rigorous ablation study and show the
quantitative comparison in Table 2. Figure 5 and Figure 6
show the qualitative comparison under different settings,
where ‚ÄúW/O Occlusion‚Äù means occlusion handling is not
considered, ‚ÄúW/O Self-Supervision‚Äù means occlusion handling is considered but self-supervision is not employed,
‚ÄúRectangle‚Äù and ‚ÄúSuperpixel‚Äù represent self-supervision
is employed with rectangle and superpixel noise injection respectively.
‚ÄúTwo-Frame Superpixel‚Äù means selfsupervision is conducted with only two frames as input.
Two-Frame vs Multi-Frame. Comparing row 1 and row
2, row 3 and row 4 row 5 and row 7 in Table 2, we can see
that using multiple frames as input can indeed improve the
performance, especially for occluded pixels. It is because
multiple images provide more information, especially for
those pixels occluded in one direction but non-occluded in
the reverse direction.
Occlusion Handling. Comparing the row 1 and row 3, row
2 and row 4 in Table 2, we can see that occlusion handling
can improve optical Ô¨Çow estimation performance over all
pixels on all datasets. This is due to the fact that brightness
constancy assumption does not hold for occluded pixels.
Self-Supervision. We employ two strategies for our occlusion hallucination: rectangle and superpixel. Both strategies improve the performance signiÔ¨Åcantly, especially for
occluded pixels.
Take superpixel setting as an example,
EPE-OCC decrease from 26.63 to 22.06 on Sintel Clean,
from 29.80 to 25.42 on Sintel Final, from 19.11 to 6.95
on KITTI 2012, and from 40.99 to 19.68 on KITTI 2015.
Such a big improvement demonstrates the effectiveness of
our self-supervision strategy.
Comparing superpixel noise injection with rectangle
noise injection, superpixel setting has several advantages.
First, the shape of the superpixel is random and edges are
more correlated to motion boundaries. Second, the pixels in
the same superpixel usually have similar motion patterns.
As a result, the superpixel setting achieves slightly better
performance.
Self-Supervised Pre-training.
Table 3 compares supervised results with and without our self-supervised pretraining on the validation sets. If we do not employ selfsupervised pre-training and directly train the model using
only the ground truth, the model fails to converge well due
to insufÔ¨Åcient training data. However, after utilizing our
self-supervised pre-training, it converges very quickly and
achieves much better results.
5. Conclusion
We have presented a self-supervised approach to learning accurate optical Ô¨Çow estimation. Our method injects
noise into superpixels to create occlusions, and let one
model guide the another to learn optical Ô¨Çow for occluded
pixels. Our simple CNN effectively aggregates temporal
information from multiple frames to improve Ô¨Çow prediction. Extensive experiments show our method signiÔ¨Åcantly
outperforms all existing unsupervised optical Ô¨Çow learning
methods. After Ô¨Åne-tuning with our unsupervised model,
our method achieves state-of-the-art Ô¨Çow estimation accuracy on all leading benchmarks. Our results demonstrate it
is possible to completely reduce the reliance of pre-training
on synthetic labeled datasets, and achieve superior performance by self-supervised pre-training on unlabeled data.
6. Acknowledgment
This work is supported by the Research Grants Council
of the Hong Kong Special Administrative Region, China
(No. CUHK 14208815 and No. CUHK 14210717 of the
General Research Fund). We thank anonymous reviewers
for their constructive suggestions.