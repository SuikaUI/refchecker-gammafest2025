Learning to Segment Every Thing
Ronghang Hu1,2,∗
Piotr Doll´ar2
Kaiming He2
Trevor Darrell1
Ross Girshick2
1BAIR, UC Berkeley
2Facebook AI Research (FAIR)
Most methods for object instance segmentation require
all training examples to be labeled with segmentation
masks. This requirement makes it expensive to annotate new
categories and has restricted instance segmentation models
to ∼100 well-annotated classes. The goal of this paper is to
propose a new partially supervised training paradigm, together with a novel weight transfer function, that enables
training instance segmentation models on a large set of categories all of which have box annotations, but only a small
fraction of which have mask annotations. These contributions allow us to train Mask R-CNN to detect and segment
3000 visual concepts using box annotations from the Visual
Genome dataset and mask annotations from the 80 classes
in the COCO dataset. We evaluate our approach in a controlled study on the COCO dataset. This work is a ﬁrst
step towards instance segmentation models that have broad
comprehension of the visual world.
1. Introduction
Object detectors have become signiﬁcantly more accurate (e.g., ) and gained important new capabilities.
One of the most exciting is the ability to predict a foreground segmentation mask for each detected object (e.g.,
 ), a task called instance segmentation. In practice, typical instance segmentation systems are restricted to a narrow
slice of the vast visual world that includes only around 100
object categories.
A principle reason for this limitation is that state-of-theart instance segmentation algorithms require strong supervision and such supervision may be limited and expensive
to collect for new categories . By comparison, bounding box annotations are more abundant and less expensive
 . This fact raises a question: Is it possible to train highquality instance segmentation models without complete instance segmentation annotations for all categories? With
this motivation, our paper introduces a new partially supervised instance segmentation task and proposes a novel
transfer learning method to address it.
∗Work done during an internship at FAIR.
kitchen 0.54
stove 0.56
shelf 0.61
refrigerator 0.54
burner 0.52
shelf 0.73
handle 0.51
bottle 0.56
Figure 1. We explore training instance segmentation models
with partial supervision: a subset of classes (green boxes) have
instance mask annotations during training; the remaining classes
(red boxes) have only bounding box annotations.
This image
shows output from our model trained for 3000 classes from Visual
Genome, using mask annotations from only 80 classes in COCO.
We formulate the partially supervised instance segmentation task as follows: (1) given a set of categories of interest, a small subset has instance mask annotations, while
the other categories have only bounding box annotations;
(2) the instance segmentation algorithm should utilize this
data to ﬁt a model that can segment instances of all object
categories in the set of interest. Since the training data is a
mixture of strongly annotated examples (those with masks)
and weakly annotated examples (those with only boxes), we
refer to the task as partially supervised.
The main beneﬁt of partially supervised vs. weaklysupervised training (c.f. ) is it allows us to build a largescale instance segmentation model by exploiting both types
of existing datasets: those with bounding box annotations
over a large number of classes, such as Visual Genome
 , and those with instance mask annotations over a small
number of classes, such as COCO . As we will show,
this enables us to scale state-of-the-art instance segmentation methods to thousands of categories, a capability that is
critical for their deployment in real world uses.
To address partially supervised instance segmentation,
we propose a novel transfer learning approach built on
Mask R-CNN .
Mask R-CNN is well-suited to our
 
task because it decomposes the instance segmentation problem into the subtasks of bounding box object detection and
mask prediction. These subtasks are handled by dedicated
network ‘heads’ that are trained jointly. The intuition behind our approach is that once trained, the parameters of
the bounding box head encode an embedding of each object
category that enables the transfer of visual information for
that category to the partially supervised mask head.
We materialize this intuition by designing a parameterized weight transfer function that is trained to predict a category’s instance segmentation parameters as a function of
its bounding box detection parameters. The weight transfer
function can be trained end-to-end in Mask R-CNN using
classes with mask annotations as supervision. At inference
time, the weight transfer function is used to predict the instance segmentation parameters for every category, thus enabling the model to segment all object categories, including
those without mask annotations at training time.
We explore our approach in two settings. First, we use
the COCO dataset to simulate the partially supervised
instance segmentation task as a means of establishing quantitative results on a dataset with high-quality annotations
and evaluation metrics. Speciﬁcally, we split the full set of
COCO categories into a subset with mask annotations and
a complementary subset for which the system has access to
only bounding box annotations. Because the COCO dataset
involves only a small number (80) of semantically wellseparated classes, quantitative evaluation is precise and reliable. Experimental results show that our method improves
results over a strong baseline with up to a 40% relative increase in mask AP on categories without training masks.
In our second setting, we train a large-scale instance
segmentation model on 3000 categories using the Visual
Genome (VG) dataset . VG contains bounding box annotations for a large number of object categories, however
quantitative evaluation is challenging as many categories
are semantically overlapping (e.g., near synonyms) and the
annotations are not exhaustive, making precision and recall
difﬁcult to measure. Moreover, VG is not annotated with
instance masks. Instead, we use VG to provide qualitative
output of a large-scale instance segmentation model. Output of our model is illustrated in Figure 1 and 5.
2. Related Work
Instance segmentation. Instance segmentation is a highly
active research area , with
Mask R-CNN representing the current state-of-the-art.
These methods assume a fully supervised training scenario
in which all categories of interest have instance mask annotations during training. Fully supervised training, however,
makes it difﬁcult to scale these systems to thousands of categories. The focus of our work is to relax this assumption
and enable training models even when masks are available
for only a small subset of categories. To do this, we develop
a novel transfer learning approach built on Mask R-CNN.
Weight prediction and task transfer learning. Instead of
directly learning model parameters, prior work has explored
predicting them from other sources (e.g., ). In , image classiﬁers are predicted from the natural language description of a zero-shot category. In , a model regression network is used to construct the classiﬁer weights from
few-shot examples, and similarly in , a small neural network is used to predict the classiﬁer weights of the composition of two concepts from the classiﬁer weights of each individual concept. Here, we design a model that predicts the
class-speciﬁc instance segmentation weights used in Mask
R-CNN, instead of training them directly, which is not possible in our partially supervised training scenario.
Our approach is also a type of transfer learning 
where knowledge gained from one task helps with another
Most related to our work, LSDA transforms
whole-image classiﬁcation parameters into object detection
parameters through a domain adaptation procedure. LSDA
can be seen as transferring knowledge learned on an image classiﬁcation task to an object detection task, whereas
we consider transferring knowledge learned from bounding
box detection to instance segmentation.
Weakly supervised semantic segmentation. Prior work
trains semantic segmentation models from weak supervision. Image-level labels and object size constraints are used
in , while other methods use boxes as supervision for
expectation-maximization or iterating between proposals generation and training . Point supervision and objectness potentials are used in . Most work in this area
addresses only semantic segmentation (not instance segmentation), treats each class independently, and relies on
hand-crafted bottom-up proposals that generalize poorly.
Weakly supervised instance segmentation is addressed in
 by training an instance segmentation model over the
bottom-up GrabCut foreground segmentation results
from the bounding boxes. Unlike , we aim to exploit
all existing labeled data rather than artiﬁcially limiting it.
Our work is also complementary in the sense that bottom-up
segmentation methods may be used to infer training masks
for our weakly-labeled examples. We leave this extension
to future work.
Visual embeddings. Object categories may be modeled by
continuous ‘embedding’ vectors in a visual-semantic space,
where nearby vectors are often close in appearance or semantic ontology. Class embedding vectors may be obtained
via natural language processing techniques (e.g. word2vec
 and GloVe ), from visual appearance information
(e.g. ), or both (e.g. ). In our work, the parameters
of Mask R-CNN’s box head contain class-speciﬁc appearance information and can be seen as embedding vectors
training data
network module
learned weights
output tensor
predicted weights
box labels in
predictions
predictions
mask labels in
weight transfer function
!456 = @ !?58 ; 1
<: classes with box & mask data
>: classes with only box data
class-agnostic mask MLP
Mask R-CNN
(shaded region)
Figure 2. Detailed illustration of our MaskX R-CNN method. Instead of directly learning the mask prediction parameters wseg, MaskX
R-CNN predicts a category’s segmentation parameters wseg from its corresponding box detection parameters wdet, using a learned weight
transfer function T . For training, T only needs mask data for the classes in set A, yet it can be applied to all classes in set A ∪B at test
time. We also augment the mask head with a complementary fully connected multi-layer perceptron (MLP).
learned by training for the bounding box object detection
task. The class embedding vectors enable transfer learning
in our model by sharing appearance information between
visually related classes. We also compare with the NLPbased GloVe embeddings in our experiments.
3. Learning to Segment Every Thing
Let C be the set of object categories (i.e., ‘things’ )
for which we would like to train an instance segmentation
model. Most existing approaches assume that all training
examples in C are annotated with instance masks. We relax this requirement and instead assume that C = A ∪B
where examples from the categories in A have masks, while
those in B have only bounding boxes. Since the examples
of the B categories are weakly labeled w.r.t. the target task
(instance segmentation), we refer to training on the combination of strong and weak labels as a partially supervised
learning problem. Noting that one can easily convert instance masks to bounding boxes, we assume that bounding
box annotations are also available for classes in A.
Given an instance segmentation model like Mask R-
CNN that has a bounding box detection component and a
mask prediction component, we propose the MaskX R-
CNN method that transfers category-speciﬁc information
from the model’s bounding box detectors to its instance
mask predictors.
3.1. Mask Prediction Using Weight Transfer
Our method is built on Mask R-CNN , because it
is a simple instance segmentation model that also achieves
state-of-the-art results. In brief, Mask R-CNN can be seen
as augmenting a Faster R-CNN bounding box detection model with an additional mask branch that is a small
fully convolutional network (FCN) . At inference time,
the mask branch is applied to each detected object in order to predict an instance-level foreground segmentation
mask. During training, the mask branch is trained jointly
and in parallel with the standard bounding box head found
in Faster R-CNN.
In Mask R-CNN, the last layer in the bounding box
branch and the last layer in the mask branch both contain category-speciﬁc parameters that are used to perform
bounding box classiﬁcation and instance mask prediction,
respectively, for each category.
Instead of learning the
category-speciﬁc bounding box parameters and mask parameters independently, we propose to predict a category’s
mask parameters from its bounding box parameters using a
generic, category-agnostic weight transfer function that can
be jointly trained as part of the whole model.
For a given category c, let wc
det be the class-speciﬁc object detection weights in the last layer of the bounding box
head, and wc
seg be the class-speciﬁc mask weights in the
mask branch. Instead of treating wc
seg as model parameters, wc
seg is parameterized using a generic weight prediction
function T (·):
seg = T (wc
where θ are class-agnostic, learned parameters.
The same transfer function T (·) may be applied to any
category c and, thus, θ should be set such that T generalizes to classes whose masks are not observed during
training. We expect that generalization is possible because
the class-speciﬁc detection weights wc
det can be seen as an
appearance-based visual embedding of the class.
T (·) can be implemented as a small fully connected neural network. Figure 2 illustrates how the weight transfer
function ﬁts into Mask R-CNN to form MaskX R-CNN. As
a detail, note that the bounding box head contains two types
of detection weights: the RoI classiﬁcation weights wc
the bounding box regression weights wc
box. We experiment
with using either only a single type of detection weights (i.e.
box) or using the concatenation of
the two types of weights (i.e. wc
3.2. Training
During training, we assume that for the two sets of
classes A and B, instance mask annotations are available
only for classes in A but not for classes in B, while all
classes in A and B have bounding box annotations available. As shown in Figure 2, we train the bounding box
head using the standard box detection losses on all classes in
A ∪B, but only train the mask head and the weight transfer
function T (·) using a mask loss on the classes in A. Given
these losses, we explore two different training procedures:
stage-wise training and end-to-end training.
Stage-wise training. As Mask R-CNN can be seen as augmenting Faster R-CNN with a mask head, a possible training strategy is to separate the training procedure into detection training (ﬁrst stage) and segmentation training (second
stage). In the ﬁrst stage, we train a Faster R-CNN using only
the bounding box annotations of the classes in A ∪B, and
then in the second stage the additional mask head is trained
while keeping the convolutional features and the bounding
box head ﬁxed.
In this way, the class-speciﬁc detection
weights wc
det of each class c can be treated as ﬁxed class embedding vectors that do not need to be updated when training the second stage. This approach has the practical bene-
ﬁt of allowing us to train the box detection model once and
then rapidly evaluate design choices for the weight transfer
function. It also has disadvantages, which we discuss next.
End-to-end joint training. It was shown that for Mask R-
CNN, multi-task training can lead to better performance
than training on each task separately. The aforementioned
stage-wise training mechanism separates detection training
and segmentation training, and may result in inferior performance. Therefore, we would also like to jointly train
the bounding box head and the mask head in an end-toend manner. In principle, one can directly train with backpropagation using the box losses on classes in A ∪B and
the mask loss on classes in A. However, this may cause
a discrepancy in the class-speciﬁc detection weights wc
between set A and B, since only wc
det for c ∈A will receive gradients from the mask loss through the weight transfer function T (·). We would like wc
det to be homogeneous
between A and B so that the predicted wc
seg = T (wc
trained on A can better generalize to B.
To address this discrepancy, we take a simple approach:
when back-propagating the mask loss, we stop the gradient
with respect to wc
det, that is, we only compute the gradient of
the predicted mask weights T (wc
det; θ) with respect to transfer function parameter θ but not bounding box weight wc
This can be implemented as wc
seg = T (stop grad(wc
in most neural network toolkits.
3.3. Baseline: Class-Agnostic Mask Prediction
DeepMask established that it is possible to train
a deep model to perform class-agnostic mask prediction
where an object mask is predicted regardless of the category. A similar result was also shown for Mask R-CNN
with only a small loss in mask quality . In additional
experiments, demonstrated if the class-agnostic model
is trained to predict masks on a subset of the COCO categories (speciﬁcally the 20 from PASCAL VOC ) it can
generalize to the other 60 COCO categories at inference
time. Based on these results, we use Mask R-CNN with
a class-agnostic FCN mask prediction head as a baseline.
Evidence from and suggest that this is a strong
baseline. Next, we introduce an extension that can improve
both the baseline and our proposed weight transfer function.
We also compare with a few other baselines for unsupervised or weakly supervised instance segmentation in §4.3.
3.4. Extension: Fused FCN+MLP Mask Heads
Two types of mask heads are considered for Mask R-
CNN in : (1) an FCN head, where the M × M mask
is predicted with a fully convolutional network, and (2) an
MLP head, where the mask is predicted with a multi-layer
perceptron consisting of fully connected layers, more similar to DeepMask. In Mask R-CNN, the FCN head yields
higher mask AP. However, the two designs may be complementary. Intuitively, the MLP mask predictor may better
capture the ‘gist’ of an object while the FCN mask predictor
may better capture the details (such as the object boundary).
Based on this observation, we propose to improve both the
baseline class-agnostic FCN and our weight transfer function (which uses an FCN) by fusing them with predictions
from a class-agnostic MLP mask predictor.
Our experiments will show that this extension brings improvements
to both the baseline and our transfer approach.
When fusing class-agnostic and class-speciﬁc mask predictions for K classes, the two scores are added into a ﬁnal
K × M × M output, where the class-agnostic mask scores
(with shape 1×M×M) are tiled K times and added to every
class. Then, the K×M×M mask scores are turned into perclass mask probabilities through a sigmoid unit, and resized
to the actual bounding box size as ﬁnal instance mask for
that bounding box. During training, binary cross-entropy
loss is applied on the K × M × M mask probabilities.
4. Experiments on COCO
We evaluate our method on the COCO dataset ,
which is small scale w.r.t. the number of categories but
contains exhaustive mask annotations for 80 categories.
This property enables rigorous quantitative evaluation using standard detection metrics, like average precision (AP).
4.1. Evaluation Protocol and Baselines
We simulate the partially supervised training scenario on
COCO by partitioning the 80 classes into sets A and B, as
described in §3. We consider two split types: (1) The 20/60
split used by DeepMask that divides the COCO categories based on the 20 contained in PASCAL VOC and
the 60 that are not. We refer to these as the ‘voc’ and ‘nonvoc’ category sets from here on. (2) We also conduct expervoc →non-voc
non-voc →voc
AP on B AP on A AP on B AP on A
transfer w/ randn
transfer w/ GloVe 
transfer w/ cls
transfer w/ box
transfer w/ cls+box
class-agnostic (baseline)
fully supervised (oracle)
(a) Ablation on input to T . ‘cls’ is RoI classiﬁcation weights, ‘box’
is box regression weights, and‘cls+box’ is both weights. We also compare with the NLP-based GloVe vectors . Our transfer function T
improves the AP on B while remaining on par with the oracle on A.
voc →non-voc
non-voc →voc
AP on B AP on A AP on B AP on A
transfer w/ 1-layer, none
transfer w/ 2-layer, ReLU
transfer w/ 2-layer, LeakyReLU
transfer w/ 3-layer, ReLU
transfer w/ 3-layer, LeakyReLU
(b) Ablation on the structure of T . We vary the number of fully connected layers in the weight transfer function T , and try both ReLU and LeakyReLU as activation function in the hidden layers. The results show that ‘2-layer, LeakyReLU’
works best, but in general T is robust to speciﬁc implementation choices.
voc →non-voc
non-voc →voc
AP on B AP on A AP on B AP on A
class-agnostic
class-agnostic+MLP
transfer+MLP
(c) Impact of the MLP mask branch. Adding the class-agnostic MLP
mask branch (see §3.4) improves the performance of classes in set B
for both the class-agnostic baseline and our weight transfer approach.
voc →non-voc
non-voc →voc
AP on B AP on A AP on B AP on A
class-agnostic
class-agnostic
(d) Ablation on the training strategy. We try both stage-wise (‘sw’) and endto-end (‘e2e’) training (see §3.2), and whether to stop gradient from T to wdet.
End-to-end training improves the results and it is crucial to stop gradient on wdet.
Table 1. Ablation study of our method. We use ResNet-50-FPN as our backbone network, and ‘cls+box’ and ‘2-layer, LeakyReLU’ as
the default input and structure of T . Results in (a,b,c) are based on stage-wise training, and we study the impact of end-to-end training in
(d). Mask AP is evaluated on the COCO dataset val2017 split between the 20 PASCAL VOC categories (‘voc’) and the 60 remaining
categories (‘non-voc’), as in . Performance on the strongly supervised set A is shown in gray.
iments using multiple trials with random splits of different
sizes. These experiments allow us to characterize any bias
in the voc/non-voc split and also understand what factors in
the training data lead to better mask generalization.
Implementation details. We train our model on the COCO
train2017 split and test on val2017.1 Each class has a
1024-d RoI classiﬁcation parameter vector wc
cls and a 4096d bounding box regression parameter vector wc
box in the
detection head, and a 256-d segmentation parameter vector wc
seg in the mask head. The output mask resolution is
M × M = 28 × 28. In all our experimental analysis below, we use either ResNet-50-FPN or ResNet-101-FPN 
as the backbone architecture for Mask R-CNN, initialized
from a ResNet-50 or a ResNet-101 model pretrained
on the ImageNet-1k image classiﬁcation dataset .
We follow the training hyper-parameters suggested for
Mask R-CNN in . Each minibatch has 16 images ×
512 RoIs-per-images, and the network is trained for 90k iterations on 8 GPUs. We use 1e-4 weight decay and 0.9
momentum, and an initial learning rate of 0.02, which is
multiplied by 0.1 after 60k and 80k iterations. We evaluate
instance segmentation performance using average precision
(AP), which is the standard COCO metric and equal to the
mean of average precision from 0.5 to 0.95 IoU threshold
of all classes.
Baseline and oracle. We compare our method to classagnostic mask prediction using either an FCN or fused
1The COCO train2017 and val2017 splits are the same as the
trainval35k and minival splits used in prior work, such as .
FCN+MLP structure. In these approaches, instead of predicting each class c’s segmentation parameters wc
its bounding box classiﬁcation parameters wc
det, all the categories share the same learned segmentation parameters wc
(no weight transfer function is involved). Evidence from
DeepMask and Mask R-CNN, as discussed in §3.3, suggests
that this approach is a strong baseline. In addition, we compare our approach with unsupervised or weakly supervised
instance segmentation approaches in §4.3.
We also evaluate an ‘oracle’ model:
Mask R-CNN
trained on all classes in A ∪B with access to instance
mask annotations for all classes in A and B at training time.
This fully supervised model is a performance upper bound
for our partially supervised task (unless the weight transfer
function can improve over directly learning wc
4.2. Ablation Experiments
Input to T . In Table 1a we study the impact of the input
to the weight transfer function T . For transfer learning to
work, we expect that the input should capture information
about how the visual appearance of classes relate to each
other. To see if this is the case, we designed several inputs to T : a random Gaussian vector (‘randn’) assigned
to each class, an NLP-based word embedding using pretrained GloVe vectors for each class, the weights from
the Mask R-CNN box head classiﬁer (‘cls’), the weights
from the box regression (‘box’), and the concatenation of
both weights (‘cls+box’). We compare the performance of
our transfer approach with these different embeddings to the
strong baseline: class-agnostic Mask R-CNN.
First, Table 1a shows that the random control (‘randn’)
10000 12500 15000 17500 20000 22500 25000
average #training-samples-per-class in A
relative AP increase (percent) in B
20 random classes
30 random classes
40 random classes
50 random classes
60 random classes
20 VOC classes
60 non-VOC classes
Figure 3. Each point corresponds to our method on a random A/B
split of COCO classes. We vary |A| from 20 to 60 classes and
plot the relative change in mask AP on the classes in set B (those
classes without mask annotations) vs. the average number of mask
annotations per class in set A.
yields results on par with the baseline; they are slightly better on voc→non-voc and worse in the other direction, which
may be attributed to noise. Next, the GloVe embedding
shows a consistent improvement over the baseline, which
indicates that these embeddings may capture some visual
information as suggested in prior work . However, inputs ‘cls’, ‘box’ and ‘cls+box’ all strongly outperform the
NLP-based embedding (with ‘cls+box’ giving the best results), which matches our expectation since they encode visual information by design.
We note that all methods compare well to the fully supervised Mask R-CNN oracle on the classes in set A. In
particular, our transfer approach slightly outperforms the
oracle for all input types. This results indicates that our
approach does not sacriﬁce anything on classes with strong
supervision, which is an important property.
Structure of T . In Table 1b we compare different implementations of T : as a simple afﬁne transformation, or as a
neural network with 2 or 3 layers. Since LeakyReLU 
is used for weight prediction in , we try both ReLU and
LeakyReLU as activation function in the hidden layers. The
results show that a 2-layer MLP with LeakyReLU gives the
best mask AP on set B. Given this, we select the ‘cls+box,
2-layer, LeakyReLU’ implementation of T for all subsequent experiments.
Comparison of random A/B splits. Besides splitting
datasets into voc and non-voc, we also experiment with random splits of the 80 classes in COCO, and vary the number
of training classes. We randomly select 20, 30, 40, 50 or 60
classes to include in set A (the complement forms set B),
perform 5 trials for each split size, and compare the performance of our weight transfer function T on classes in B to
the class-agnostic baseline. The results are shown in Figure
3, where it can be seen that our method yields to up to over
40% relative increase in mask AP. This plot reveals a correlation between relative AP increase and the average number
of training samples per class in set A. This indicates that to
maximize transfer performance to classes in set B it may be
more effective to collect a larger number of instance mask
samples for each object category in set A.
Impact of the MLP mask branch. As discussed in §3.4,
a class-agnostic MLP mask branch can be fused with either
the baseline or our transfer approach. In Table 1c we see
that either mask head fused with the MLP mask branch consistently outperforms the corresponding unfused version.
This conﬁrms our intuition that FCN-based mask heads and
MLP-based mask heads are complementary in nature.
Effect of end-to-end training. Up to now, all ablation
experiments use stage-wise training, because it is signiﬁcantly faster (the same Faster R-CNN detection model can
be reused for all experiments). However, as noted in §3.2,
stage-wise training may be suboptimal.
Thus, Table 1d
compares stage-wise training to end-to-end training. In the
case of end-to-end training, we investigate if it is necessary to stop gradients from T to wdet, as discussed. Indeed, results match our expectation that end-to-end training can bring improved results, however only when backpropagation from T to wdet is disabled. We believe this
modiﬁcation is necessary in order to make the embedding
of classes in A homogeneous with those in B; a property
that is destroyed when only the embeddings for classes in
A are modiﬁed by back-propagation from T .
4.3. Results and Comparison of Our Full Method
Table 2 compares our full MaskX R-CNN method (i.e.,
Mask R-CNN with ‘transfer+MLP’ and T implemented
as ‘cls+box, 2-layer, LeakyReLU’) and the class-agnostic
baseline using end-to-end training.
In addition, we also
compare with the following baseline approaches: a) unsupervised mask prediction using GrabCut foreground
segmentation over the Faster R-CNN detected object boxes
(Faster R-CNN tested w/ GrabCut) and b) weakly supervised instance segmentation similar to , which trains an
instance segmentation method (here we use Mask R-CNN)
on the GrabCut segmentation of the ground-truth boxes
(Mask R-CNN trained w/ GrabCut).
MaskX R-CNN outperforms these approaches by a large
margin (over 20% relative increase in mask AP). We also
experiment with ResNet-101-FPN as the backbone network
in the bottom half of Table 2. The trends observed with
ResNet-50-FPN generalize to ResNet-101-FPN, demonstrating independence of the particular backbone used thus
Figure 4 shows example mask predictions from the
class-agnostic baseline and our approach.
5. Large-Scale Instance Segmentation
Thus far, we have experimented with a simulated version of our true objective: training large-scale instance segmentation models with broad visual comprehension. We
person 1.00
person 0.98
person 0.99
person 0.99
frisbee 0.98
person 1.00
frisbee 0.99
person 0.99
person 0.99
person 0.99
person 0.99
chair 0.96
person 0.91
person 1.00
person 0.97
person 0.95
person 0.97
person 0.99
person 0.95
person 1.00
person 0.99
person 0.99
person 0.9
person 0.98
person 1.00
person 0.98
person 0.99
frisbee 0.99
person 0.99
frisbee 0.99
person 1.00
person 1.00
person 0.99
person 0.98
person 0.95
person 1.00
chair 0.92
person 0.93
person 0.99
person 0.98
person 0.96
person 1.00
person 0.99
person 0.99
person 0.9
Figure 4. Mask predictions from the class-agnostic baseline (top row) vs. our MaskX R-CNN approach (bottom row). Green boxes
are classes in set A while the red boxes are classes in set B. The left 2 columns are A = {voc} and the right 2 columns are A = {non-voc}.
voc →non-voc: test on B = {non-voc}
non-voc →voc: test on B = {voc}
class-agnostic
Faster R-CNN tested w/ GrabCut
Mask R-CNN trained w/ GrabCut
MaskX R-CNN (ours)
fully supervised (oracle)
class-agnostic
Faster R-CNN tested w/ GrabCut
Mask R-CNN trained w/ GrabCut
MaskX R-CNN (ours)
fully supervised (oracle)
Table 2. End-to-end training of MaskX R-CNN. As in Table 1, we use ‘cls+box, 2-layer, LeakyReLU’ implementation of T and add the
MLP mask branch (‘transfer+MLP’), and follow the same evaluation protocol. We also report AP50 and AP75 (average precision evaluated
at 0.5 and 0.75 IoU threshold respectively), and AP over small (APS), medium (APM), and large (APL) objects. Our method signiﬁcantly
outperforms the baseline approaches in §4.3 on set B without mask training data for both ResNet-50-FPN and ResNet-101-FPN backbones.
believe this goal represents an exciting new direction for
visual recognition research and that to accomplish it some
form of learning from partial supervision may be required.
To take a step towards this goal, we train a largescale MaskX R-CNN model following the partially supervised task, using bounding boxes from the Visual Genome
(VG) dataset and instance masks from the COCO
dataset . The VG dataset contains 108077 images, and
over 7000 category synsets annotated with object bounding
boxes (but not masks). To train our model, we select the
3000 most frequent synsets as our set of classes A ∪B for
instance segmentation, which covers all the 80 classes in
COCO. Since the VG dataset images have a large overlap
with COCO, when training on VG we take all the images
that are not in COCO val2017 split as our training set,
and validate our model on the rest of VG images. We treat
all the 80 VG classes that overlap with COCO as our set A
with mask data, and the remaining 2920 classes in VG as
our set B with only bounding boxes.
Training. We train our large-scale MaskX R-CNN model
using the stage-wise training strategy. Speciﬁcally, we train
a Faster R-CNN model to detect the 3000 classes in VG
using ResNet-101-FPN as our backbone network following
the hyper-parameters in §4.1. Then, in the second stage,
we add the mask head using our weight transfer function
T and the class-agnostic MLP mask prediction (i.e., ‘transfer+MLP’), with the ‘cls+box, 2-layer, LeakyReLU’ implementation of T . The mask head is trained on subset of 80
COCO classes (set A) using the mask annotations in the
train2017 split of the COCO dataset.
Qualitative results. Mask AP is difﬁcult to compute on
VG because it contains only box annotations. Therefore
we visualize results to understand the performance of our
model trained on all the 3000 classes in A ∪B using our
weight transfer function. Figure 5 shows mask prediction
examples on validation images, where it can be seen that
our model predicts reasonable masks on those VG classes
not overlapping with COCO (set B, shown in red boxes).
This visualization shows several interesting properties of
our large-scale instance segmentation model. First, it has
learned to detect abstract concepts, such as shadows and
paths. These are often difﬁcult to segment. Second, by simply taking the ﬁrst 3000 synsets from VG, some of the concepts are more ‘stuff’ like than ‘thing’ like. For example,
building 0.55
shadow 0.73
shadow 0.64
trunk 0.52
shrub 0.79
chair 0.91
bench 0.66
grass 0.58
window 0.51
shirt 0.70
window 0.77
chair 0.64
curtain 0.74
headboard 0.80
pillow 0.67
pillow 0.5
pillow 0.92
pillow 0.85
pillow 0.82
table 0.72
electric_refrigerator 0.54
floor 0.71
chair 0.95
window 0.65
handle 0.84
handle 0.58
building 0.62
building 0.83
bicycle 0.93
window 0.62
window 0.64
window 0.58
window 0.65
window 0.75
window 0.68
window 0.52
window 0.59
window 0.60
window 0.5
bicycle 0.94
jacket 0.53
window 0.64
window 0.66
window 0.53
window 0.52
laptop 0.71
trouser 0.74
keyboard 0.64
necklace 0.81
accountant
necktie 0.81
necktie 0.50
button 0.92
button 0.72
button 0.79
building 0.80
train 0.79
fence 0.76
window 0.7
window 0.54
window 0.57
pole 0.54light 0.59
building 0.79
truck 0.79
building 0.67
truck 0.84
bench 0.57
truck 0.72
window 0.50
window 0.52
shirt 0.91
bicycle 0.98
bicycle 0.92
bicycle 0.88
shirt 0.57
helmet 0.90
helmet 0.77
helmet 0.64
helmet 0.83
helmet 0.84
helmet 0.89
helmet 0.76
helmet 0.84
short_pants 0.50
helmet 0.78
helmet 0.82
floor 0.87
ceiling 0.61
floor 0.64
doorway 0.54
window 0.70
pillow 0.81
pillow 0.53
towel 0.62
pillow 0.74
woman 0.64
jean 0.68jean 0.62
shirt 0.84
shirt 0.88
shirt 0.53
frisbee 0.93
frisbee 0.94
beard 0.87
woman 0.61
table 0.65
shirt 0.53
glove 0.72
window 0.56
traffic_light 0.51
clock 0.69
building 0.87
railing 0.62
traffic_light 0.53
light 0.62
window 0.51
light 0.54
floor 0.75
chair 0.75
counter 0.69
ceiling 0.73
pillow 0.61
pillow 0.60
microwave 0.95
fixture 0.56
switch 0.57
switch 0.66
building 0.70
building 0.63
sidewalk 0.55
fence 0.58
fence 0.68
window 0.51
fence 0.56
window 0.54
window 0.68
building 0.68
building 0.63
building 0.72
window 0.65
window 0.55
window 0.62
window 0.68
window 0.65
window 0.68
window 0.53
window 0.59
window 0.65
mirror 0.74
light 0.52
window 0.50
license_plate 0.56
window 0.53
chimney 0.86
field 0.50
uniform 0.56
trouser 0.65
trouser 0.93
glove 0.69
Figure 5. Example mask predictions from our MaskX R-CNN on 3000 classes in Visual Genome. The green boxes are the 80 classes
that overlap with COCO (set A with mask training data) while the red boxes are the remaining 2920 classes not in COCO (set B without
mask training data). It can be seen that our model generates reasonable mask predictions on many classes in set B. See §5 for details.
the model does a reasonable job segmenting isolated trees,
but tends to fail at segmentation when the detected ‘tree’ is
more like a forest. Finally, the detector does a reasonable
job at segmenting whole objects and parts of those objects,
such as windows of a trolley car or handles of a refrigerator. Compared to a detector trained on 80 COCO categories,
these results illustrate the exciting potential of systems that
can recognize and segment thousands of concepts.
6. Conclusion
This paper addresses the problem of large-scale instance
segmentation by formulating a partially supervised learning paradigm in which only a subset of classes have instance masks during training while the rest have box annotations. We propose a novel transfer learning approach,
where a learned weight transfer function predicts how each
class should be segmented based on parameters learned
for detecting bounding boxes. Experimental results on the
COCO dataset demonstrate that our method greatly improves the generalization of mask prediction to categories
without mask training data. Using our approach, we build a
large-scale instance segmentation model over 3000 classes
in the Visual Genome dataset. The qualitative results are encouraging and illustrate an exciting new research direction
into large-scale instance segmentation. They also reveal that
scaling instance segmentation to thousands of categories,
without full supervision, is an extremely challenging problem with ample opportunity for improved methods.