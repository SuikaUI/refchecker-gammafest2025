NISP: Pruning Networks using Neuron Importance Score Propagation
Ruichi Yu1
Chun-Fu Chen2
Jui-Hsin Lai5†
Vlad I. Morariu4∗
Xintong Han1
Mingfei Gao1
Ching-Yung Lin6†
Larry S. Davis1
1University of Maryland, College Park
2IBM T. J. Watson Research
4Adobe Research
6Graphen.ai
{richyu, xintong, mgao, lsd}@umiacs.umd.edu, 
 , , , 
To reduce the signiﬁcant redundancy in deep Convolutional Neural Networks (CNNs), most existing methods
prune neurons by only considering statistics of an individual layer or two consecutive layers (e.g., prune one layer
to minimize the reconstruction error of the next layer), ignoring the effect of error propagation in deep networks. In
contrast, we argue that it is essential to prune neurons in
the entire neuron network jointly based on a uniﬁed goal:
minimizing the reconstruction error of important responses
in the “ﬁnal response layer” (FRL), which is the secondto-last layer before classiﬁcation, for a pruned network to
retrain its predictive power. Speciﬁcally, we apply feature
ranking techniques to measure the importance of each neuron in the FRL, and formulate network pruning as a binary
integer optimization problem and derive a closed-form solution to it for pruning neurons in earlier layers. Based on
our theoretical analysis, we propose the Neuron Importance
Score Propagation (NISP) algorithm to propagate the importance scores of ﬁnal responses to every neuron in the
network. The CNN is pruned by removing neurons with
least importance, and then ﬁne-tuned to retain its predictive
power. NISP is evaluated on several datasets with multiple
CNN models and demonstrated to achieve signiﬁcant acceleration and compression with negligible accuracy loss.
1. Introduction
CNNs require a large number of parameters and high
computational cost in both training and testing phases. Recent studies have investigated the signiﬁcant redundancy
in deep networks and reduced the number of neurons
and ﬁlters by pruning the unimportant ones.
However, most current approaches that prune neurons and
∗This work was done while the author was at University of Maryland.
†This work was done while the author was at IBM.
Top$ranked Neurons/Filters
Importance
Pre$trained Network
Pruned Network
Fine tuning
Pre$defined
Pruning ratios
Figure 1. We measure the importance of neurons in the ﬁnal response layer (FRL), and derive Neuron Importance Score Propagation (NISP) to propagate the importance to the entire network.
Given a pre-deﬁned pruning ratio per layer, we prune the neurons/ﬁlters with lower importance score. We ﬁnally ﬁne-tune the
pruned model to recover its predictive accuracy.
ﬁlters consider only the statistics of one layer (e.g., prune
neurons with small magnitude of weights ), or two
consecutive layers to determine the “importance” of
a neuron. These methods prune the “least important” neurons layer-by-layer either independently or greedily
 , without considering all neurons in different layers
One problem with such methods is that neurons deemed
unimportant in an early layer can, in fact, contribute signiﬁcantly to responses of important neurons in later layers. Our
experiments (see Sec.4.4) reveal that greedy layer-by-layer
pruning leads to signiﬁcant reconstruction error propagation, especially in deep networks, which indicates the need
for a global measurement of neuron importance across different layers of a CNN.
To address this problem, we argue that it is essential
for a pruned model to retain the most important responses
of the second-to-last layer before classiﬁcation (“ﬁnal re-
 
sponse layer” (FRL)) to retrain its predictive power, since
those responses are the direct inputs of the classiﬁcation
task (which is also suggested by feature selection methods,
e.g., ). We deﬁne the importance of neurons in early layers based on a uniﬁed goal: minimizing the reconstruction
errors of the responses produced in FRL. We ﬁrst measure
the importance of responses in the FRL by treating them
as features and applying some feature ranking techniques
(e.g., ), then propagate the importance of neurons backwards from the FRL to earlier layers. We prune only nodes
which have low propagated importance (i.e., those whose
removal does not result in large propagated error). From
a theoretical perspective, we formulate the network pruning problem as a binary integer programming objective that
minimizes the weighted ℓ1 distance (proportional to the importance scores) between the original ﬁnal response and the
one produced by a pruned network. We obtain a closed-form
solution to a relaxed version of this objective to infer the
importance score of every neuron in the network. Based on
this solution, we derive the Neuron Importance Score Propagation (NISP) algorithm, which computes all importance
scores recursively, using only one feature ranking of the ﬁnal response layer and one backward pass through the network as illustrated in Fig. 1.
The network is then pruned based on the inferred neuron
importance scores and ﬁne-tuned to retain its predictive capability. We treat the pruning ratio per layer as a pre-deﬁned
hyper-parameter, which can be determined based on different needs of speciﬁc applications (e.g., FLOPs, memory
and accuracy constraints). The pruning algorithm is generic,
since feature ranking can be applied to any layer of interest
and the importance scores can still be propagated. In addition, NISP is not hardware speciﬁc. Given a pretrained
model, NISP outputs a smaller network of the same type,
which can be deployed on the hardware devices designed
for the original model.
We evaluate our approach on MNIST , CIFAR10
 and ImageNet using multiple standard CNN architectures such as LeNet , AlexNet , GoogLeNet
 and ResNet . Our experiments show that CNNs
pruned by our approach outperform those with the same
structures but which are either trained from scratch or randomly pruned. We demonstrate that our approach outperforms magnitude-based and layer-by-layer pruning. A comparison of the theoretical reduction of FLOPs and number
of parameters of different methods shows that our method
achieves faster full-network acceleration and compression
with lower accuracy loss, e.g., our approach loses 1.43%
accuracy on Alexnet and reduces FLOPs by 67.85% while
Figurnov et al. loses more (2%) and reduces FLOPs
less (50%). With almost zero accuracy loss on ResNet-56,
we achieve a 43.61% FLOP reduction, signiﬁcantly higher
than the 27.60% reduction by Li et al. .
1.1. Contribution
We introduce a generic network pruning algorithm which
formulates the pruning problem as a binary integer optimization and provide a closed-form solution based on ﬁnal
response importance. We present NISP to efﬁciently propagate the importance scores from ﬁnal responses to all other
neurons. Experiments demonstrate that NISP leads to fullnetwork acceleration and compression for all types of layers
in a CNN with small accuracy loss.
2. Related Work
There has been recent interest in reducing the redundancy of deep CNNs to achieve acceleration and compression. In the redundancy in the parameterization of deep
learning models has been studied and demonstrated. Cheng
et al. exploited properties of structured matrices and
used circulant matrices to represent FC layers, reducing
storage cost. Han et al. studied the weight sparsity and
compressed CNNs by combining pruning, quantization, and
Huffman coding. Sparsity regularization terms have been
use to learn sparse CNN structure in . Miao et
al. studied network compression based on ﬂoat data
quantization for the purpose of massive model storage.
To accelerate inference in convolution layers, Jaderberg
et al. constructed a low rank basis of ﬁlters that are
rank-1 in the spatial domain by exploiting cross-channel or
ﬁlter redundancy. Liu et al. imposed a scaling factor in
the training process and facilitated one channel-level pruning. Figurnov et al. speeded up the convolutional layers by skipping operations in some spatial positions, which
is based on loop perforation from source code optimization.
In , low-rank approximation methods have been
utilized to speed up convolutional layers by decomposing
the weight matrix into low-rank matrices. Molchanov et al.
 prune CNNs based on Taylor expansion.
Focusing on compressing the fully connected (FC) layers, Srinivas et al. pruned neurons that are similar to
each other. Yang et al. applied the “Fastfood” transform to reparameterize the matrix-vector multiplication of
FC layers. Ciresan et al. reduced the parameters by randomly pruning neurons. Chen et al. used a low-cost
hash function to randomly group connection weights into
hash buckets and then ﬁne-tuned the network with backpropagation. Other studies focused on ﬁxed point computation rather than exploiting the CNN redundancy . Another work studied the fundamental idea about knowledge
distillation . Wu et al. proposed to skip layers for
speeding up inference. Besides the above work which focuses on network compression, other methods speedup deep
network inference by reﬁning the pipelines of certain tasks
 . Our method prunes a pre-trained network
and requires a fast-converging ﬁne-tuning process, rather
than re-training a network from scratch. To measure the importance of neurons in a CNN, the exact solution is very
hard to obtain given the complexity of nonlinearity. Some
previous works approximate it using 2nd-order
Taylor expansion. Our work is a different approximation
based on the Lipschitz continuity of a neural network.
Most similar to our approach, Li et al. pruned ﬁlters
by their weight magnitude. Luo et al. utilized statistics
information computed from the next layer to guide a greedy
layer-by-layer pruning. In contrast, we measure neuron importance based not only on a neuron’s individual weight but
also the properties of the input data and other neurons in
the network. Meanwhile, instead of pruning layer-by-layer
in greedy fashion under the assumption that one layer can
only affect its next layer, which may cause error propagation, we measure the importance across the entire network
by propagating the importance from the ﬁnal response layer.
3. Our Approach
An overview of NISP is illustrated in Fig. 1. Given a
trained CNN, we ﬁrst apply a feature ranking algorithm on
this ﬁnal response layer and obtain the importance score
of each neuron. Then, the proposed NISP algorithm propagates importance scores throughout the network. Finally,
the network is pruned based on the importance scores of
neurons and ﬁne-tuned to recover its accuracy.
3.1. Feature Ranking on the Final Response Layer
Our intuition is that the ﬁnal responses of a neural network should play key roles in full network pruning since
they are the direct inputs of the classiﬁcation task. So, in the
ﬁrst step, we apply feature ranking on the ﬁnal responses.
It is worth noting that our method can work with any
feature selection that scores features w.r.t. their classiﬁcation power. We employ the recently introduced ﬁltering
method Inf-FS because of its efﬁciency and effectiveness on CNN feature selection. Inf-FS utilizes properties of
the power series of matrices to efﬁciently compute the importance of a feature with respect to all the other features,
i.e., it is able to integrate the importance of a feature over
all paths in the afﬁnity graph1.
3.2. Neuron Importance Score Propagation (NISP)
Our goal is to decide which intermediate neurons to
delete, given the importance scores of ﬁnal responses, so
that the predictive power of the network is maximally retained. We formulate this problem as a binary integer programming (optimization) and provide a closed-form approximate solution. Based on our theoretical analysis, we
1Details of the method are introduced in and its codes taken from
 
54763-infinite-feature-selection-2016.
Figure 2. We propagate the neuron importance from the ﬁnal response layer (FRL) to previous layers, and prune bottom-ranked
neurons (with low importance scores shown in each node) given
a pre-deﬁned pruning ratio per layer in a single pass. The importance of pruned neurons (with backslash) is not propagated.
develop the Neuron Importance Score Propagation algorithm to efﬁciently compute the neuron importance for the
whole network.
Problem Deﬁnition
The goal of pruning is to remove neurons while minimizing accuracy loss. Since model accuracy is dependent on
the ﬁnal responses, we deﬁne our objective as minimizing
the weighted distance between the original ﬁnal responses
and the ﬁnal responses after neurons are pruned of a speciﬁc layer. In following, we use bold symbols to represent
vectors and matrices.
Most neural networks can be represented as a nested
function. Thus, we deﬁne a network with depth n as a function F (n) = f (n) ◦f (n−1) ◦· · · ◦f (1). The l-th layer f (l) is
represented using the following general form,
f (l)(x) = σ(l)(w(l)x + b(l)),
where σ(l) is an activation function and w(l), b(l) are weight
and bias, and f(n) represents the ”ﬁnal response layer”. Networks with branch connections such as the skip connection in ResNet can be transformed to this representation by
padding weights and merging layers.
We deﬁne the neuron importance score as a non-negative
value w.r.t. a neuron, and use sl to represent the vector of
neuron importance scores in the l-th layer. Suppose Nl neurons are to be kept in the l-th layer after pruning; we deﬁne
the neuron prune indicator of the l-th layer as a binary vector s∗
l , computed based on neuron importance scores sl such
l,i = 1 if and only if sl,i is among top Nl values in sl.
Objective Function
The motivation of our objective is that the difference between the responses produced by the original network and
the one produced by the pruned network should be minimized w.r.t. important neurons. Let F (n) be a neural net-
work with n layers. Suppose we have a dataset of M samples, and each is represented using x(m)
. For the m-th sample, we use x(m)
to represent the response of the l-th layer
(which is the input to the (l + 1)-th layer). The ﬁnal output
of the network is x(m)
and its corresponding non-negative
neuron importance is sn. We deﬁne
G(i,j) = f (j) ◦f (j−1) ◦· · · ◦f (i)
as a sub-network of F (n) starting from the i-th layer to the
j-th layer. Our goal is to compute for the l-th layer the neuron prune indicator s∗
l so that the inﬂuence of pruning the
l-th layer on the important neurons of the ﬁnal response is
minimized. To accomplish this, we deﬁne an optimization
objective w.r.t. the l-th layer neuron prune indicator, i.e.,
, sn; G(l+1,n)) ,
which is accumulated over all samples in the dataset. The
objective function for a single sample is deﬁned as
l |x, sn; F) = ⟨sn, |F(x) −F(s∗
where ⟨·, ·⟩is dot product, ⊙is element-wise product and
| · | is element-wise absolute value. The solution to Eq. 3
indicates which neurons should be pruned in an arbitrary
The network pruning problem can be formulated as a binary
integer program, ﬁnding the optimal neuron prune indicator
in Eq. 3. However, it is hard to obtain efﬁcient analytical solutions by directly optimizing Eq. 3. So we derive an upper
bound on this objective, and show that a sub-optimal solution can be obtained by minimizing the upper bound. Interestingly, we ﬁnd a feasible and efﬁcient formulation for the
importance scores of all neurons based on this sub-optimal
Recall that the k-th layer is deﬁned as f (k)(x)
σ(k)(w(k)x+b(k)). We assume the activation function σ(k)
is Lipschitz continuous since it is generally true for most
of the commonly used activations in neural networks such
as Identity, ReLU, sigmoid, tanh, PReLU, etc. Then we
know for any x, y, there exists a constant C(k)
|σ(k)(x) −σ(k)(y)| ≤C(k)
σ |x −y|. Then it is easy to see
|f (k)(x) −f (k)(y)| ≤C(k)
σ |w(k)| · |x −y| ,
where | · | is the element-wise absolute value. From Eq. 2,
we see that G(i,j) = f (j) ◦G(i,j−1). Therefore, we have,
|G(i,j)(x) −G(i,j)(y)|
σ |w(j)||G(i,j−1)(x) −G(i,j−1)(y)| .
Applying Eq. 5 and Eq. 6 repeatedly, we have, ∀i ≤j ≤n,
|G(i,n)(x) −G(i,n)(y)| ≤C(i,n)
W(i,n)|x −y|,
where W(i,j) = |w(j)||w(j−1)| · · · |w(i)|, and C(i,j)
σ . Substituting x = x(m)
into Eq. 7, we have
|G(l+1,n)(x(m)
) −G(l+1,n)(s∗
W(l+1,n)|x(m)
Since sn is a non-negative vector,
, sn; G(l+1,n))
= ⟨sn, |G(l+1,n)(x(m)
) −G(l+1,n)(s∗
≤⟨sn, C(l+1,n)
W(l+1,n)|x(m)
= C(l+1,n)
⟨W(l+1,n)⊺sn, (1 −s∗
l ) ⊙|x(m)
Let us deﬁne rl = W(l+1,n)⊺sn; then
, sn; G(l+1,n))
m=1⟨rl, (1 −s∗
l ) ⊙|x(m)
i rl,i(1 −s∗
= C(l+1,n)
i rl,i(1 −s∗
Since |x(m)
l,i | is bounded, there must exist a constant Cx such
l,i | ≤Cx, ∀i. Thus, we have
, sn; F (l+1)) ≤C
rl,i(1 −s∗
l,i), (15)
where C = C(l+1,n)
Cx is a constant factor.
Eq. 15 reveals an upper-bound of our objective in Eq. 3.
Thus, we minimize this upper-bound, i.e.,
rl,i(1 −s∗
l,i) ⇔arg max
The optimal solution to Eq.16 is sub-optimal with respect
to the original objective in Eq. 3, however it still captures
the importance of neurons. It is easy to see that if we keep
Nx neurons in the l-th layer after pruning, then the solution
to Eq. 16 is that s∗
l,i = 1 if and only if rl,i is among the
highest Nx values in rl. According to the deﬁnition of neuron prune indicator in Sec. 3.2.1, rl = W(l+1,n)⊺sn is a
feasible solution to the importance scores of the l-th layer
response. This conclusion can be applied to every layer in
the network. Based on this result, we deﬁne the neuron importance of a network as follows.
Deﬁnition 1 (Neuron importance score). Given a neural
network F (n) containing n layers and the importance score
s(n) of the last layer response, the importance score of the
k-th layer response can be computed as
sk = |w(k+1)|⊺|w(k+2)|⊺· · · |w(n)|⊺sn,
where w(i) is the weight matrix of the i-th layer.
An important property of neuron importance is that it can
be computed recursively (or propagated) along the network.
Proposition 2 (Neuron importance score propagation). The
importance score of the kth layer response can be propagated from the importance score of the (k + 1)th layer by
sk = |w(k+1)|⊺sk+1,
where w(k+1) is the weight matrix of the (k + 1)th layer.
We propose the Neuron Importance Score Propagation
(NISP) algorithm (shown in Fig. 2) based on Proposition 2.
Initially, we have the importance score of every neuron in
the ﬁnal response layer of the network. Deﬁnition 1 shows
that the importance score of every other layer in the network
is directly correlated with the importance of the ﬁnal response. However, instead of computing the importance expensively using Deﬁnition 1, we see from Eq. 18 that the
importance score of a lower layer can be propagated directly from the adjacent layer above it. An equivalent form
of Eq. 18 is
where sk,j is the importance score of the j-th neuron in the
k-th layer response.
We conclude from Eq. 19 that the importance of a neuron is a weighted sum of all the subsequent neurons that are
directly connected to it. This conclusion also applies to normalization, pooling and branch connections in the network
(i.e., a layer is directly connected with multiple layers)2.
The NISP algorithm starts with the importance in FRL and
repeats the propagation (Eq. 19) to obtain the importance
of all neurons in the network with a single backward pass
3.3. Pruning Networks Using NISP
Given target pruning ratios for each layer, we propagate
the importance scores, compute the prune indicator of neurons based on their importance scores and remove neurons
with prune indicator value 0. The importance propagation
and layer pruning happens jointly in a single backward pass,
2See supplementary material for more details and proofs.
and the importance of a pruned neuron is not propagated to
any further low-level layers. For fully connected layers, we
prune each individual neuron. For convolution layers, we
prune a whole channel of neurons together. The importance
score of a channel is computed as the summation of the importance scores of all neurons within this channel2.
4. Experiments
We evaluate our approach on standard datasets with popular CNN networks. We ﬁrst compare to random pruning and training-from-scratch baselines to demonstrate the
effectiveness of our method. We then compare to two
other baselines, magnitude-based pruning and layer-bylayer pruning to highlight the contributions of feature ranking and neuron importance score propagation, respectively.
Finally, we benchmark the pruning results and compare to
existing methods such as .
4.1. Experimental Setting
We conduct experiments on three datasets, MNIST ,
CIFAR10 and ImageNet , for the image classiﬁcation
task. We evaluate using ﬁve commonly used CNN architectures: LeNet , Cifar-net3, AlexNet , GoogLeNet
 and ResNet .
All experiments and time benchmarks are obtained using Caffe . The hyper-parameter of Inf-FS is a loading
coefﬁcient α ∈ , which controls the inﬂuence of variance and correlation when measuring the importance. We
conduct PCA accumulated energy analysis (results shown
in the supplementary material) as suggested in to guide
our choice of pruning ratios.
4.2. Comparison with Random Pruning and Trainfrom-scratch Baselines
We compare to two baselines: (1) randomly pruning the
pre-trained CNN and then ﬁne-tuning, and (2) training a
small CNN with the same number of neurons/ﬁlters per
layer as our pruned model from scratch. We use the same
experimental settings for our method and baselines except
for the initial learning rate. For training from scratch, we set
the initial learning rate to the original one, while for ﬁnetuning tasks (both NISP and random pruning), the initial
learning rate is reduced by a factor of 10.
LeNet on MNIST: We prune half of the neurons in FC
layers and half of the ﬁlters in both convolution layers in
Fig. 3(a). Our method is denoted as NISPHalf, while the
baseline methods that prune randomly or train from scratch
are denoted as RandomHalf and ScratchHalf. Our method outperforms the baselines in three aspects. First, for ﬁne-tuning
(after pruning), unlike the baselines, our method has very
small accuracy loss at iteration 0; this implies that it retains
3 
Accuracy Loss
RandomHalf
ScratchHalf
Accuracy Loss
RandomHalf
ScratchHalf
(b) CIFAR10
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
Accuracy Loss
(c) ImageNet: AlexNet
Accuracy Loss
NISPno1 ∗1
Randomno1 ∗1
RandomHalf
Scratchno1 ∗1
ScratchHalf
(d) ImageNet: GoogeLeNet
Figure 3. Learning curves of random pruning and training from scratch baselines and NISP using different CNNs on different datasets. The
pruning ratio of neurons and ﬁlters is 50%. Networks pruned by NISP (orange curves) converge the fastest with the lowest accuracy loss.
the most important neurons, pruning only redundant or less
discriminative ones. Second, our method converges much
faster than the baselines. Third, our method has the smallest
accuracy loss after ﬁne-tuning. For LeNet on MNIST, our
method only decreases 0.02% top-1 accuracy with a pruning ratio of 50% as compared to the pre-pruned network.
Cifar-net on CIFAR10: The learning curves are shown
in Fig. 3(b). Similar to the observations from the experiment
for LeNet on MNIST, our method outperforms the baselines
in the same three aspects: the lowest initial loss of accuracy,
the highest convergence speed and the lowest accuracy loss
after ﬁne-tuning. Our method has less than 1% top-1 accuracy loss with 50% pruning ratio for each layer.
AlexNet on ImageNet: To demonstrate that our method
works on large and deep CNNs, we replicate experiments on
AlexNet with a pruning ratio of 50% for all convolution layers and FC layers (denoted as NISPCF when we prune both
conv and FC layers). Considering the importance of FC layers in AlexNet, we compare one more scenario in which our
approach only prunes half of the ﬁlters but without pruning
neurons in FC layers (denoted as NISPC). We reduce the initial learning rate by a factor of 10, then ﬁne-tune 90 epochs
and report top-5 accuracy loss. Fig. 3(c) shows that for both
cases (pruning both convolution and FC layers and pruning only convolution layers), the advantages we observed on
MNIST and CIFAR10 still hold. Layer-wise computational
reduction analysis that shows the full-network acceleration
can be found in supplementary materials.
GoogLeNet on ImageNet: We denote the reduction layers in an inception module as “Reduce”, and the 1×1 convolution layer without reduction as “1×1”. We use the quick
solver from Caffe in training. We conduct experiments between our method and the baselines for 3 pruning strategies:
(Half) pruning all convolution layers by half; (noReduce)
pruning every convolution layer except for the reduction
layers in inception modules by half; (no1x1) pruning every
convolution layer by half except the 1×1 layers in inception
modules. We show results for two of them in Fig. 3(d), and
observe similar patterns to the experiments on other CNN
networks4. For all GoogLeNet experiments, we train/ﬁnetune for 60 epochs and report top-5 accuracy loss.
4.3. Feature Selection v.s. Magnitude of Weights
How to deﬁne neuron importance is an open problem.
Besides using feature ranking to measure neuron importance, other methods measure neuron importance by magnitude of weights. To study the effects of different criteria to determine neuron importance, we conduct
experiments by ﬁxing other parts of NISP and only comparing the pruning results with different measurements of importance: 1. using feature selection method in (NISP-
FS) and 2. considering only magnitude of weights (NISP-
Mag). For the Magnitude-based pruning, the importance of
a neuron in the ﬁnal response layer equals the absolute sum
of all weights connecting the neuron with its previous layer.
To compare only the two metrics of importance, we rank
the importance of neurons in the ﬁnal response layer based
on the magnitude of their weight values, and propagate their
importance to the lower layers. Finally, we prune and ﬁnetune the model in the same way as the NISP method.
For the “NISP-Mag” baseline, we use both AlexNet and
Cifar-net architectures. The learning curves of those baselines are shown in Fig. 4. We observe that “NISP-FS” yields
much smaller accuracy loss with the same pruning ratio than
“NISP-Mag”, but “NISP-Mag” still outperforms the random pruning and train-from-scratch baselines, which shows
the effectiveness of NISP with different measurement of importance. In the remainder of this paper, we employ the feature ranking method proposed in in NISP.
4.4. NISP v.s. Layer-by-Layer Pruning
To demonstrate the advantage of the NISP’s importance
propagation, we compare with a pruning method that conducts feature ranking on every layer to measure the neuron
importance and prune the unimportant neurons of each layer
independently. All other settings are the same as NISP. We
call this method “Layer-by-Layer” (LbL) pruning.
4See supplementary materials for the results of noReduce.
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
Accuracy Loss
(a) AlexNet on ImageNet
Accuracy Loss
(b) Cifar-net on CIFAR10
Figure 4. Comparison with layer-by-layer (LbL) and magnitude
based (Mag) pruning baselines. We prune 50% of neurons and ﬁlters in all layers for both CNNs. NISP-FS outperforms NISP-Mag
and LbL in terms of prediction accuracy.
One challenge for the “LbL” baseline is that the computational cost of measuring neuron importance on each layer
is huge. So we choose a small CNN structure trained on the
CIFAR10 dataset. Fig. 4(b) shows that although the “LbL”
method outperforms the baselines, it performs much worse
than NISP in terms of the ﬁnal accuracy loss with the same
pruning ratio, which shows the need for measuring the neuron importance across the entire network using NISP.
To further study the advantage of NISP over layer-bylayer pruning, we deﬁne the Weighted Average Reconstruction Error (WARE) to measure the change of the important
neurons’ responses on the ﬁnal response layer after pruning
(without ﬁne-tuning) as:
i=1 si · |ˆyi,m−yi,m|
where M and N are the number of samples and number of
retained neurons in the ﬁnal response layer; si is the importance score; yi,m and ˆyi,m is the response on the mth
sample of the ith neuron before/after pruning.
We design different Cifar-net-like CNNs with different
numbers of Conv layers, and apply NISP and LbL pruning with different pruning ratios. We report the WARE on
the retained neurons in the ﬁnal response layer (“ip1” layer
in Cifar-net-like CNNs) in Fig. 5. We observe that: 1. As
network depth increases, the WARE of the LbL-pruned network dramatically increases, which indicates the error propagation problem of layer-by-layer pruning, especially when
the network is deep, and suggests the need for a global pruning method such as NISP; 2. The WARE of the LbL method
becomes much larger when the pruning ratio is large, but
is more stable when using NISP to prune a network; 3.
NISP methods always reduce WARE on the retained neurons compared to LbL. The small reconstruction errors on
the important neurons in the ﬁnal response layer obtained by
NISP provides a better initialization for ﬁne-tuning, which
leads to much lower accuracy loss of the pruned network.
Number of Conv Layers
Average Reconstruction Error
Figure 5. Weighted Average Reconstruction Error (WARE) on the
ﬁnal responses without ﬁne-tuning: we set pruning ratios as 25%
and 50% and evaluate the WARE on the ﬁnal responses of models
with different depths pruned using NISP or LbL. It is clear that
networks pruned by NISP have the lowest reconstruction errors.
on ImageNet
Perforated 
Tucker 
Learning 
on ImageNet
Tucker 
on CIFAR10
110-A 
110-B 
on ImageNet
Res34 
Res50 
Table 1. Compression Benchmark. [Accu.↓%] denotes the absolute accuracy loss; [FLOPs↓%] denotes the reduction of computations; [Params.↓%] demotes the reduction of parameter numbers;
4.5. Comparison with Existing Methods
We compare our method with existing pruning methods
on AlexNet, GoogLeNet and ResNet, and show results in
We show benchmarks of several pruning strategies in
Table 1, and provide additional results in the supplementary materials. In Table 1, for AlexNet, the pruning ratio
is 50%. NISP-A denotes pruning all Conv layers; NISP-B
5A negative value here indicates an improved model accuracy.
denotes pruning all Conv layers except for Conv5; NISP-
C denotes pruning all Conv layers except for Conv5 and
Conv4; NISP-D means pruning Conv2, Conv3 and FC6 layers. For GoogLeNet, we use the similar the pruning ratios
of the 3×3 layers in , and we prune 20% of the reduce
layers. Our method is denoted as “NISP”.
To compare theoretical speedup, we report reduction in
the number of multiplication and the number of parameters
following and , and denote them as [FLOPs↓%]
and [Params.↓%] in the table. Pruning a CNN is a tradeoff between efﬁciency and accuracy. We compare different
methods by ﬁxing one metric and comparing the other.
On AlexNet, by achieving smaller accuracy loss (1.43%
ours vs. 2.00% ), our method NISP-A manages to reduce signiﬁcantly more FLOPs (67.85%) than the one in
 (50%), denoted as “Perforate” in the table; compare
to the method in (denoted as “Learning”), our method
NISP-C achieves much smaller accuracy loss (0.54% ours
vs. 1.20%) and prunes more FLOPs (53.70% ours vs.
48.19%). We manage to achieve 0 accuracy loss and reduce over 40% FLOPs and 47.09% parameters (NISP-D).
On GoogLeNet, Our method achieves similar accuracy loss
with larger FLOPs reduction (58.34% vs. 51.50%) Using
ResNet on Cifar10 dataset, with top-1 accuracy loss similar
to (56-A, 56-B. 110-A and 110-B), our method reduces
more FLOPs and parameters.
We also conduct our ResNet experiments on ImageNet
 . We train a ResNet-34 and a ResNet-50 for 90 epochs.
For both ResNet models, we prune 15% and 25% of ﬁlters for each layer (denote as “NISP-X-A” and “NISP-X-
B” (“X” indicates the ResNet model) in Table 1), and obtain 27-44% FLOPs and parameter reduction with tiny top-
1 accuracy loss, which shows superior performance when
compared with the state-of-the-art methods .
4.6. Additional Analysis
Below, we provide case studies and ablation analysis to
help understand the proposed NISP pruning algorithm.
Similar Predictive Power of Networks Before/After
Pruning. To check whether the pruned network performs
similarly with the original network, we compare the ﬁnal
classiﬁcation results of the original AlexNet and the pruned
one with ﬁne-tuning using the ILSVRC2012 validation set.
85.9% of the top 1 predictions of the two networks agree
with each other, and 95.1% top 1 predictions of the pruned
network can be found in the top 5 predictions of the original network. The above experiments show that the network
pruned by NISP performs similarly with the original one.
Sensitivity of pruning ratios. The selection of per-layer
pruning ratios given a FLOPs budget is a challenging open
problem with a large search space. Due to time limitation,
we either choose a single pruning ratio for all layers or replicate the pruning ratios of baseline methods (e.g., ), and
Accuracy Loss
NISPQuarter
RandomQuarter
RandomTenth
ScratchQuarter
ScratchTenth
(a) LeNet Prune 75% and 90%
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
Accuracy Loss
(b) AlexNet Prune 75%
Figure 6. Evaluations for different pruning ratios (a) LeNet: pruning 75% and 90%, (b) AlexNet: pruning 75%. CNNs pruned by
NISP converge fastest with the lowest accuracy loss.
NISP achieves smaller accuracy loss, which shows the effectiveness of NISP. In practice, if time and GPU resources
permit, one can search the optimal hyper-parameters by trying different pruning ratio combinations on a validation set.
We also evaluate NISP with very large pruning ratios.
We test on pruning ratios of 75% (denoted as Quarter in the
ﬁgures) and 90% using LeNet (Fig. 6(a)) (denoted as Tenth)
for both Conv and FC layers. For AlexNet (Fig. 6(b)), we
test on pruning ratios of 75% (Quarter) for both convolution and FC layers, and we test two pruning strategies: (1)
prune 75% of neurons in FC layers and ﬁlters in Conv layers, denoted as FC; and (2) only prune 75% of the convolution ﬁlters without pruning FC layers, denoted as C.
The above experiments show that NISP still outperforms
all baselines signiﬁcantly with large pruning ratios, in terms
of both convergence speed and ﬁnal accuracy.
5. Conclusion
We proposed a generic framework for network compression and acceleration based on identifying the importance
levels of neurons. Neuron importance scores in the layer
of interest (usually the last layer before classiﬁcation) are
obtained by feature ranking. We formulated the network
pruning problem as a binary integer program and obtained a
closed-form solution to a relaxed version of the formulation.
We presented the Neuron Importance Score Propagation algorithm that efﬁciently propagates the importance to every
neuron in the whole network. The network is pruned by removing less important neurons and ﬁne-tuned to retain its
predicative capability. Experiments demonstrated that our
method effectively reduces CNN redundancy and achieves
full-network acceleration and compression.
Acknowledgement
The research was partially supported by the Ofﬁce of
Naval Research under Grant N000141612713: Visual Common Sense Reasoning for Multi-agent Activity Prediction
and Recognition.
6. Supplementary Material
Despite their impressive predictive power on a wide
range of tasks ,
the redundancy in the parameterization of deep learning
models has been studied and demonstrated . We present
NISP to efﬁciently propagate the importance scores from ﬁnal responses to all other neurons to guide network pruning
to achieve acceleration and compression of a deep network.
In the supplementary materials, we show details on how to
propagate neuron importance from the ﬁnal response layer,
and some additional experiments.
6.1. Neuron Importance Score Propagation (NISP)
Given the importance of a neuron, we ﬁrst identify the
positions in the previous layer that are used as its input,
then propagate the importance to the positions proportional
to the weights. We only propagate the importance of the
selected feature extractors to the previous layers and ignore
the pruned ones. The NISP process can be divided into three
classes: from a 1-way tensor to a 1-way tensor, e.g. between
FC layers; from a 1-way tensor to a 3-way tensor, e.g., from
an FC layer to a conv/pooling layer; from a 3-way tensor to
a 3-way tensor, e.g., from a pooling layer to a conv layer.
We simplify NISP by ignoring the propagation of bias.
6.2. NISP: from 1-way tensor to 1-way tensor
Given an FC layer with M input neurons and N output
neurons, the N-by-1 importance vector (S) of the output
feature is SFCout = [SF Cout1, SF Cout2 . . . SF CoutN]T. We
use WFC ∈RM×N to denote the weights of the FC layer.
The importance vector of the input neurons is:
SFCin = |WFC| · SFCout ,
where | · | is element-wise absolute value.
6.3. NISP: from 1-way tensor to 3-way tensor
Given an FC layer with a 3-way tensor as input and N
output neurons, the input has a size of X × X × C, where
X is the spatial size and C is the number of input channels.
The input can be the response of a convolutional layer or a
pooling layer. We use WFC ∈R(X2×C)×N to denote the
weights of the FC layer. The ﬂattened importance vector
Sin ∈R(X2×C)×1 of the input tensor is:
Sin = |WFC| · SFCout.
6.4. NISP: from 3-way tensor to 3-way tensor
Convolution Layer.
We derive NISP for a convolutional layer, which is the most
complicated case of NISP between 3-way tensors. NISP for
Algorithm 1 NISP: convolutional layer
1: Input : weights of the conv layer W ∈RX×X×N×F
2: , ﬂattened importance of the f th output channel
out ∈R1×(X×X)
4: for n in 1 ...N do
for f in 1 ...F do
kfn ←|W[:, :, n, f]|
Construct BPfn
conv as (25) and (26)
out · BPfn
10: Sin ←[S1
in . . . , SN
pooling and local response normalization (LRN) can be derived similarly.
For a convolutional layer with the input 3-way tensor convin ∈RX×X×N and output tensor convout ∈
RY ×Y ×F ), the ﬁlter size is k, stride is s and the number of
padded pixels is p. During the forward propagation, convolution consists of multiple inner products between a kernel
kf ∈Rk×k×N, and multiple corresponding receptive cubes
to produce an output response. Fixing input channel n and
output channel f, the spatial convolutional kernel is kfn.
For position i in the nth channel of the input tensor, the corresponding response of the output channel f at position i is
deﬁned as Equation 23:
kfn · in(i),
where in(i) is the corresponding 2-D receptive ﬁeld. Given
the importance cube of the output response Sout
RY ×Y ×F , we use a similar linear computation to propagate
the importance from the output response to the input:
kfn · Sout(i),
where Sn(i) is the importance of position i in the nth input
channel, and Sout(i) is the corresponding 2-D matrix that
contains the output positions whose responses come from
the value of that input position during forward propagation.
We propagate the importance proportionally to the weights
as described in Algorithm 1.
The propagation matrices used in algorithm 1 are deﬁned
in (25) and (26)
1 . . . bfn
1 . . . bfn
1 . . . bfn
c is the building block of size Y × X deﬁned as:
kfn[i, 1] . . . . . . kfn[i, k]
kfn[i, 1] . . . . . . kfn[i, k]
kfn[i, 1] . . . . . . kfn[i, k]
Equation 24 implies that the propagation of importance
between 3-way tensors in convolutional layers can be decomposed into propagation between 2-D matrices. Fixing
the input channel n and the output channel f, the input layer
size is X × X and the output size is Y × Y . Given the ﬂattened importance vector Sout
f ∈R1×(Y ×Y ) of the output
layer, the propagation matrix BPfn
conv ∈R(Y ×Y )×(X×X)
is used to map from Sout
f to the importance of input layer
fn ∈R1×(X×X). BPfn
conv(i, j) ̸= 0, implies that the ith
position in the output layer comes from a convolution operation with the jth position in the input layer, and we propagate the importance between the two positions. We use a
Y × X matrix bfn
to represent the mapping between a row
in the output layer to the corresponding row in the input
layer. In each row of bfn
i , there are k non-zeros since each
position in the output layer is obtained from a region with
width k of the input layer. The non-zeros of each row of bfn
are the ith row of the convolutional kernel kfn. The offset
of the beginning of the weights in each row is the stride
s. The entire propagation matrix BPfn
conv is a block matrix
with each submatrix being a Y × X matrix of either bfn
a zero matrix. Each row of BPfn
conv has bfn
the height of a convolutional kernel is k. The offset of the
beginning of the bs in each row of BPfn
conv is the stride s.
We use the case when X = 4, Y = 2, k = 3, s = 1 as an
example shown in Figure 7.
Pooling Layer.
Assume a pooling layer with input tensor of size X ×X ×F
and output size Y × Y × F. The pooling ﬁlter size is k and
the stride is s. The basic idea of most pooling techniques is
the same: use a ﬁxed 2-dimensional ﬁlter to abstract local
responses within each channel independently. For example,
in max pooling each output response consists of the max of
k×k values from the input responses. Due to the large variance of input data, it is safe to assume a uniform distribution
on which value within the receptive ﬁeld is the largest is a
uniform distribution. Consequently, for an output response
location, the contributions from the corresponding k×k values of the input response are equal. Since pooling is a spatial operation that does not cross channels, we can propagate
the importance of each channel independently. Given a ﬂattened importance vector of a channel f Sout
f ∈R1×(Y ×Y )
of the output 3-way tensor, the ﬂattened importance vector
of the input tensor is calculated as:
convolution
Figure 7. importance propagation: Convolutional layer. X
4, Y = 2, k = 3, s = 1. Fixing the f th input channel and cth output channel, the upper-left X-by-X grid is the corresponding input
feature map, and the upper-right Y-by-Y grid is the output map
after convolution is applied. kfc is the corresponding 2D convolutional kernel. Given the ﬂattened importance vector for the output
feature map Sout
f,c, we use BPconv to propagate the importance
and obtain Sin
f,c, which contains the importance of the input feature map. The structure of BPconv is determined by the kernel
size k and stride s.
f · BPpooling,
where BPpooling is the back-propagation matrix of size
Y 2 × X2 deﬁned as:
BPpooling =
bp . . . bp . . . bp
bp . . . bp . . . bp
bp . . . bp . . . bp
where bp is the building block of size Y × X deﬁned as:
1 . . . 1 . . . 1
1 . . . 1 . . . 1
1 . . . 1 . . . 1
Consider one channel with input size X × X and the
output size Y × Y . Given the ﬂattened importance vector
f ∈R1×(Y ×Y ) of the output layer, the propagation
matrix BPpooling ∈R(Y ×Y )×(X×X) is used to map from
f to the importance of input layer Sin
f ∈R1×(X×X).
If BPpooling(i, j) = 1, the ith position in the output layer
comes from a pooling operation and involves the jth position in the input layer, so we propagate the importance
between the two positions. We use a Y × X matrix bp to
represent the mapping between a row in the output layer
to the corresponding row in the input layer. In each row of
× BPpooling
BPpooling =
Figure 8. NISP: Pooling layer. X = 4, Y = 2, k = 2, s = 2. The
upper-left X-by-X grid is the f th feature map of the input channel,
and the upper-right Y-by-Y grid is the output channel after pooling
is applied. Given the importance vector Sout
f, we use BPpooling
to propagate the importance and obtain Sin
f, which contains the
importance of each position of the input feature map. The structure
of BPpooling relates to the kernel size k and stride s.
bp, there are k 1′s since each element in the output layer is
pooled from a region with width k of the input layer. The
offset of the beginning of the 1′s is the stride s. The entire
propagation matrix BPpooling is a block matrix with each
submatrix being a Y × X matrix of either bp or a zero matrix. Each row of BPpooling has k bps because the height
of pooling ﬁlter is k. The offset of the beginning of the k
bps is the stride s. The ones in bp will be normalized by the
number of positions covered by a pooling ﬁlter (the same
for LRN layers shown below). The other elements are all
zeros. We use the case that X = 4, Y = 2, k = 2, s = 2 as
an example shown in Figure 8.
Local Response Normalization Layer.
Krizhevsky et al. proposed Local Response Normalization (LRN) to improve CNN generalization. For crosschannel LRN, sums over adjacent kernel maps at the same
spatial position produce a response-normalized activation at
that position. Since LRN is a non-linear operation, it is intractable to conduct exact importance propagation between
the input and output tensors. One way to approximate propagation is to assume the kernel maps at one spatial position
contribute equally to the response at that position of the output tensor when considering the large variance of the input
data. Then, given the X × X × N importance tensor for
the response of a LRN layer with local size = l, which
is the number of adjacent kernel maps summed for a spatial position, considering all N channels of a spatial position (i, j), the importance vector of that spatial position is
out ∈R1×N. The corresponding importance vector of the
Figure 9. Importance propagation: LRN layer (cross-channel).
l = 3, N = 5. The red vector is the cross-channel vector at spatial position (i, j) of the input tensor, and the yellow vector is the
cross-channel vector at the same position of the output tensor after
LRN is applied. Given the Sout
ij, we use BPLRN to propagate
the importance and obtain Sin
ij, which contains the importance of
each position of the input feature map. The structure of BPLRN
relates to the local size l and number of channels N.
in ∈R1×N is:
out · BPLRN,
where BPLRN ∈RN×N is deﬁned as:

1 1 · · · 1
1 1 · · · 1 1
· · · · · ·
1 1 · · · 1 1 · · ·
1 · · · 1 1 · · · 1
1 1 · · · · · · · · ·
1 · · · 1 1
1 · · · 1 1
1 · · · 1 1

For a cross-channel LRN, the output response tensor has
the same shape as the input. For a spatial position (i, j) of
the output tensor, given its importance vector Sij
out, we construct a N × N symmetric matrix BPLRN to propagate
its importance to the corresponding input vectorSij
in at that
position. Since the center of the LRN operation is at position (i, j), the operation will cover l+1
positions to the
left and right. When the operation is conducted on the positions at the center of the vector Sij
out (from column l+1
+ 1), the operation covers l cross-channel position so that the corresponding columns in BPLRN have l
1’s. When the LRN operation is conducted at the margin of
the vector, there are missing cross-channel positions so that
from column l+1
to column 1 (similar for the right-bottom
corner), the 1’s in the corresponding column of BPLRN decreases by 1 per step from the center to the margin. We use
the case when l = 3, N = 5 as an example of LRN layer
with cross-channel in Figure 9.
1000 1500 2000 2500 3000 3500 4000
Num of PCs
Accumulated PCA Energy
PCA Accumulated Energy
(b) AlexNet
Figure 10. PCA accumulated energy analysis: LeNet on MNIST
(a) and AlexNet on ImageNet (b). The y axis measures the PCA
accumulated energy. The x axis shows the number of PCs.
For within-channel LRN, following our equal distribution assumption, the importance can be propagated similarly as in a pooling layer.
6.5. Experiments
6.6. PCA Accumulated Energy Analysis
One way to guide the selection of pruning ratio is the
PCA accumulated energy analysis on the responses of
a pre-pruned layer. The PCA accumulated energy analysis
shows how many PCs it needs for that layer to capture the
majority of variance of the samples, which implies a proper
range of how many neurons/kernels we should keep for that
layer. We show the PCA accumulated energy analysis results on the last FC layers before the classiﬁcation part for
LeNet (ip1) and AlexNet (fc7) in Figure 10(a) and 10(b).
By setting variance threshold as 0.95, 120 out of 500 PCs
are required for LeNet, 2234 out of 4096 PCS are required
for AlexNet to capture the variance.
6.7. Experiments on AlexNet: Convolutional Layers
v.s. FC Layers
From the experiments in the main paper, we found that
FC layers have signiﬁcant inﬂuence on accuracy loss, model
size and memory usage. To exploit the impact of pruning
FC layers and convolutional layers, we conduct experiments
on pruning half of the neurons in FC layers and some convolutional layers. We categorize the 5 convolutional layers
into three-level feature extractors: low (Conv1-Conv2 layers), middle (Conv3 layer) and high (Conv4-Conv5 layers).
Figure 11 displays learning curves and shows that although
FC layers are important in AlexNet, powerful local feature
extractors (more kernels in convolutional layers) can compensate the loss from pruning neurons in FC layers, or even
achieve better predictive power (High and Low curves).
6.8. Experiments on GoogLeNet
The learning curves for “no Reduce” is shown in Figure
12. We observe that our importance based pruning method
Accuracy Loss
Figure 11. Learning Curves of AlexNet on ImageNet: The subscript CF means we prune both convolutional kernels and neurons
in FC layers, and C means we only prune convolutional kernels.
High, Mid and Low mean we prune the entire CNN except for the
high/middle/low level convolutional layers (Conv4-Conv5, Conv3
and Conv1-Conv2 respectively).
Accuracy Loss
NISPnoReduce
ScratchnoReduce
RandomnoReduce
Figure 12. Learning Curves of GoogLeNet on ImageNet: The
pruning ratio is 50%. We prune all layers but the reduction layers in the inception modules. importance based pruning method
converges much faster and can achieve the smallest accuracy loss.
leads to better initialization, faster convergence and smaller
ﬁnal accuracy loss.
6.9. Layer-wise Improvements
In our experiments of AlexNet on Titan X, the empirical computation time for the intermediate layers (all layers except for convolutional layers and FC layers) accounts
for 17% of the entire testing time; therefore, those layers
must be considered as well while designing an acceleration method. One of our advantages over existing methods
is that all layers in the network can be sped up due to the fact
that the data volume or feature dimension at every layer is
Layer-wise Speedupof AlexNet: Pruning Half of
Kernels and Neurons
GPU speed↑
Theo. speed↑
Figure 13. Full-Network Acceleration of AlexNet: Pruning Half of
the Kernels and Neurons.
reduced. For example, by pruning kernels in convolutional
layers, we reduce the number of both output channels of the
current layer and input channels of the next layer. In theory, given a pruning ratio of 50%, except for the ﬁrst layer
whose input channels cannot be pruned, all of the convolutional layers can be sped up by 4×. The intermediate pooling, non-linearity and normalization layers have a theoretical speedup ratio of around 2×. The layer-wise acceleration ratios (both theoretical and empirical) of our method
when the pruning ratio is 50% for both convolutional layers
and FC layers are shown in Figure 13. We observe that the
theoretical and empirical speedup are almost the same for
pooling, non-linearity and normalization.