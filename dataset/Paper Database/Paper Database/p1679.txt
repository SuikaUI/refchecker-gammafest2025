Nonmyopic Active Learning of
Gaussian Processes:
An Exploration–Exploitation Approach
CMU-ML-07-105
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
When monitoring spatial phenomena, such as the ecological condition of a river, deciding where
to make observations is a challenging task.
In these settings, a fundamental question is when
an active learning, or sequential design, strategy, where locations are selected based on previous
measurements, will perform signiﬁcantly better than sensing at an a priori speciﬁed set of locations.
For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an
analysis and eﬃcient algorithms that address this question. Central to our analysis is a theoretical
bound which quantiﬁes the performance diﬀerence between active and a priori design strategies.
We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading
oﬀexploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e.,
near-optimally selecting observations when the parameters are (approximately) known. We discuss
several exploration strategies, and present logarithmic sample complexity bounds for the exploration
phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the
model. A variational approach allows us to perform eﬃcient inference in this class of nonstationary
models. We also present extensive empirical evaluation on several real-world problems.
Keywords: Active Learning, Gaussian Processes, Sequential design, Nonstationarity
Introduction
When monitoring spatial phenomena, such as the ecological condition of a river as in Fig. 1, it
is of fundamental importance to decide on the most informative locations to make observations.
However, to ﬁnd locations which predict the phenomena best, one needs a model of the spatial phenomenon itself. Gaussian processes (GPs) have been shown to be eﬀective models for this purpose
 .
Most previous work on observation selection in GPs has considered the a priori design problem,
in which the locations are selected in advance prior to making observations ; Seo et al. ; Zhu and Stein ). Indeed, if the GP model parameters are completely
known, the predictive variances do not depend on actual observed values, and hence nothing is lost
by committing to sampling locations in advance. In the case of unknown parameters however, this
independence is no longer true. Key questions we strive to understand in this paper are how much
better a sequential algorithm, taking into account previous observations, can perform compared to
a priori design when the parameters are unknown, and how can this understanding lead to better
observation selection methods.
Our main theoretical result is a bound which quantiﬁes the performance diﬀerence between sequential and a priori strategies in terms of the parameter entropy of the prior over kernels. The lower
the uncertainty about the parameters, the less we can potentially gain by using an active learning
(sequential) strategy. This relationship bears a striking resemblance to the exploration–exploitation
tradeoﬀin Reinforcement Learning. If the model parameters are known, we can exploit the model
by ﬁnding a near-optimal policy for sampling using the mutual information criterion . If the parameters are unknown, we present several exploration
strategies for eﬃciently decreasing the uncertainty about the model. Most approaches for active
sampling of GPs have been myopic in nature, in each step selecting observations which, e.g., most
decrease the predictive variance. Our approach however is nonmyopic in nature: we prove logarithmic sample complexity bounds on the duration of the exploration phase, and near optimal
performance in the exploitation phase.
Often, e.g., in spatial interpolation , GP models are assumed to
be isotropic, where the covariance of two locations depends only on their distance, and some (unknown) parameters. Many phenomena of interest however are nonstationary . In our river example (c.f., Figure 1), the pH values are strongly correlated
along the border, but weakly in the turbulent inner region. Our approach is applicable to both
stationary and nonstationary processes. However, nonstationary processes are often deﬁned by a
much larger number of parameters. To address this issue, we extend our algorithm to handle nonstationary GPs with local structure, providing eﬃcient exploration strategies and computational
techniques that handle high dimensional parameter vectors. In summary, our contributions are:
• A theoretical and empirical investigation of the performance diﬀerence between sequential
and a priori strategies for sampling in GPs;
• An exploration–exploitation analysis and sample complexity bounds for sequential design;
• An eﬃcient, nonmyopic, sequential algorithm for observation selection in isotropic GPs;
• Extension of our method to nonstationary GPs;
• Empirical evaluation on several real-world spatial monitoring problems.
Position along transect (m)
Figure 1: Left: Active sampling using the Networked Infomechanical System (NIMS) sensor , deployed at the Merced River. The sensor is attached to a wire, which enables
horizontal traversal of the transect. On ﬁxed horizontal position, it can vertically lower or raise the
sensing unit. Right: Samples of pH acquired along horizontal transect near the conﬂuence of the
San Joaquin and Merced rivers.
Gaussian Processes
Consider, for example, the task of monitoring the ecological state of a river using a robotic sensor,
such as the one shown in Figure 1. We can model the pH values as a random process XV over
the locations V, e.g., V ⊂R2. Hereby, the pH value at every location y ∈V is a random variable Xy. Measurements xA at sensor locations A ⊂V then allow us to predict the pH value at
uninstrumented locations y, by conditioning on the observations, i.e., predicting E[Xy | XA = xA].
It has been shown, that pH values, temperatures and many other spatial phenomena, can be effectively modeled using Gaussian processes (GPs) ; Cressie ).
A GP ) is a random process XV, such that every ﬁnite subset
of variables XA ⊆XV has a (consistent) multivariate normal distribution:
P(XA = xA) =
(2π)n/2|ΣAA|e−1
2 (xA−µA)T Σ−1
AA(xA−µA), where µA is the mean vector and ΣAA is the
covariance matrix. A GP is fully speciﬁed by a mean function M(·), and a symmetric positivedeﬁnite kernel function K(·, ·), often called the covariance function. For each random variable Xu
with index u ∈V, its mean µu is given by M(u), and for each pair of indices u, v ∈V, their
covariance σuv is given by K(u, v). For simplicity of notation, we denote the mean vector of a set
of variables XA by µA, where the entry for element u of µA is M(u). Similarly, we denote their
covariance matrix by ΣAA, where the entry for u, v is K(u, v). The GP representation allows us to
eﬃciently compute predictive distributions, P(Xy | xA), which, e.g., correspond to the predicted
temperature at location y after observing sensor measurements XA = xA. The distribution of Xy
given these observations is a Gaussian whose conditional mean µy|A and variance σ2
µy + ΣyAΣ−1
AA(xA −µA),
K(y, y) −ΣyAΣ−1
where ΣyA is a covariance vector with one entry for each u ∈A with value K(y, u), and ΣAy = ΣT
An important property of GPs is that the posterior variance (2.2) does not depend on the observed
values xA.
In order to compute predictive distributions using (2.1) and (2.2), the mean and kernel functions have to be known. The mean function can usually be estimated using regression techniques.
Estimating kernel functions is diﬃcult, and usually, strongly limiting assumptions are made. For
example, it is commonly assumed that the kernel K(u, v) is stationary, depending only on the difference between the locations, i.e., K(u, v) = Kθ(u −v), where θ is a set of parameters. Very often,
the kernel is even assumed to be isotropic, which means that the covariance only depends on the
distance between locations, i.e., K(u, v) = Kθ(||u −v||2). A common choice for an isotropic kernel
is the exponential kernel, Kθ(δ) = exp(−|δ|
θ ), or the Gaussian kernel, Kθ(δ) = exp(−δ2
θ2 ). Many
other parametric forms are possible.
In Section 3, we address a general form (not necessarily isotropic), where the kernel function
is speciﬁed by a set of parameters θ.
We adopt a hierarchical Bayesian approach and assign
a prior P(θ) to the parameters θ, which we assume to be discretized in our analysis.
P(Xy | XA) = P
θ P(Xy | XA, θ)P(θ | XA). For clarity of presentation, we also assume that the
prior mean function M(·) is zero. This assumption can be relaxed, for example by assigning a
normal prior to the mean function.
Observation selection policies
In order to select informative observations, the entropy criterion has been frequently
used ; Shewry and Wynn ; Gramacy ). This criterion selects
observations A∗⊆V with highest entropy,
A∗= argmaxA⊆V H(XA),
where H(XA) = −
p(xA) log p(xA)dxA is the joint (diﬀerential) entropy of the random variables
XA. We call (3.1) an a priori design criterion, as it does not depend on the actual observed values,
and can be optimized in advance. Maximizing (3.1) is NP-hard , so usually, a
myopic (greedy) algorithm is used. Starting with the empty set, A(0), at each step t it adds the
location yi = argmaxy∈V\Ai−1 H(Xyi | XAi−1) to the set of already selected locations Ai−1.
This a priori greedy rule is readily turned into a sequential algorithm, selecting
yi = argmax
H(Xyi | XAi−1 =xAi−1).
In this sequential setting, the selected location yi depends on the observations xAi−1. More generally,
we deﬁne a policy for selecting variables, which does not need to be greedy: For each instantiation of the process XV = xV, such a sequential policy π can select a diﬀerent set of observations
π(xV) ⊆V. Hereby, the i-th element, πi, deterministically depends on the observations made in
the ﬁrst i −1 steps, i.e., on xπ1:i−1. Hence, a policy can be considered a decision tree, where after
each observation, we decide on the next observation to make. If we apply the greedy policy πGH
to our river example, πGH,i would select the location which has highest entropy for predicting pH,
conditioned on the measurements we have made so far. We write |π| = k to indicate that π selects
sets Xπ of k elements. In analogy to the deﬁnition of H(XA), we can deﬁne the joint entropy of
any sequential policy π as H(Xπ) ≡−
p(xV) log p(xπ)dxV, whereby π = π(xV) denotes the set of
observations selected by the policy in the event XV = xV. H(XA) is the entropy of a ﬁxed set of
variables A. Since π will typically select diﬀerent observations in diﬀerent realizations XV = xV,
H(Xπ) will measure the “entropy” of diﬀerent variables in each realization xV.
Mutual information.
Caselton and Zidek proposed the mutual information criterion for
observation selection, MI(XA) = H(XV\A) −H(XV\A | XA). Guestrin et al. showed that
this criterion selects locations which most eﬀectively reduce the uncertainty at the unobserved locations, hence it often leads to better predictions compared to the entropy criterion. A natural
generalization of mutual information to the sequential setting is
MI(Xπ) = H(XV\π) −H(XV\π | Xπ)
p(xV)[log p(xV\π) −log p(xV\π | xπ)]dxV.
Hereby, for each realization XV = xV, V\π = V\π(xV) is the set of locations not picked by the policy
π. The greedy policy πGMI for mutual information, after some algebraic manipulation, is given by:
πi=argmaxy H(Xy|Xπ1:i−1=xπ1:i−1)−H(Xy |X¯π1:i−1),
where πi ≡πi(xπ1:i−1), and ¯π ≡V \ {y, π(xV)} is the set of “unsensed” locations if XV = xV,
excluding y.
Bounds on the advantage of active learning strategies
A key question in active learning is to determine the potential of improvement of sequential strategies over a priori designs, e.g., how much greater max|π|=k H(Xπ) is than max|A|=k H(XA). If the
GP parameters θ are known, it holds that
H(Xy|XA =xA, θ)= 1
2 log 2πeσ2
Xy|XA=H(Xy|XA, θ),
Xy|XA, as given by Equation (2.2). Thus, the entropy of a set of variables does not depend
on the actual observed values xA. Hence, perhaps surprisingly, in this case, max|π|=k H(Xπ) =
max|A|=k H(XA). More generally, any objective function depending only on the predictive variances, cannot beneﬁt from sequential strategies. Note that for non-Gaussian models, sequential
strategies can strictly outperform a priori designs, even with known parameters.
With unknown parameters, H(XA) = −P
P(xA, θ) log
dxA is the entropy
of a mixture of GPs. Since observed values aﬀect the posterior over the parameters P(Θ|XA =xA),
the predictive distributions now depend on these values. Intuitively, if we have low uncertainty
about our parameters, the predictive distributions should be almost independent of the observed
values, and there should be almost no beneﬁt from sequential strategies. We will now theoretically
formalize this intuition.
The following central result achieves this goal, by bounding H(Xπ) (and similarly for mutual
information) of the optimal policy π by a mixture of entropies of sets H(XAθ | θ), whereby the sets
are Aθ are chosen optimally for each ﬁxed parameter θ (and can thus be selected a priori, without
a sequential policy):
Theorem 1.
|π|=k H(Xπ) ≤
|A|=k H(XA | θ) + H(Θ);
|π|=k MI(Xπ) ≤
|A|=k MI(XA | θ) + H(Θ).
The proofs of all theorems can be found in the Appendix.
Theorem 1 bounds the advantage of sequential designs by two components: The expected advantage by optimizing sets for known parameters, i.e., P
θ P(θ) max|A|=k MI(XA | θ), and the
parameter entropy, H(Θ). This result implies, that if we are able to (approximately) ﬁnd the best
set of observations Aθ for a GP with known parameters θ, we can bound the advantage of using a
sequential design. If this advantage is small, we select the set of observations ahead of time, without
having to wait for the measurements.
Exploration–Exploitation Approach towards Learning GPs
Theorem 1 allows two conclusions: Firstly, if the parameter distribution P(Θ) is very peaked, we
cannot expect active learning strategies to drastically outperform a priori designs. More importantly however, it motivates an exploration–exploitation approach towards active learning of GPs:
If the bound provided by Theorem 1 is close to our current mutual information, we can exploit our
current model, and optimize the sampling without having to wait for further measurements. If the
bound is very loose, we explore, by making observations to improve the bound from Theorem 1.
We can compute the bound while running the algorithm to decide when to stop exploring.
Exploitation using Submodularity
Theorem 1 shows that in order to bound the value of the optimal policy, it suﬃces to bound the
value of the optimal set. Guestrin et al. derived such a bound for mutual information,
using the concept of submodularity.
A set function F on V is called submodular if it satisﬁes
the following diminishing returns property: for all A ⊆B ⊆V and all x /∈B it must hold that
F(A∪{x})−F(A) ≥F(B∪{x})−F(B). Intuitively, this diminishing returns property makes sense
for selecting observations: a new observation decreases our uncertainty more if we know less. A set
function is called nondecreasing if for all A ⊆B ⊆V it holds that F(A) ≤F(B). A fundamental
result about nondecreasing submodular functions is the guarantee that the greedy algorithm, which
greedily adds the element x to A such that F(A∪{x})−F(A) is largest, selects a set AG of k elements
which is at most a constant factor (1 −1/e) worse than the set of k elements of maximal value, i.e.,
F(AG) ≥(1 −1/e) max|A|=k F(A) . Guestrin et al. showed that
mutual information is submodular and approximately non-decreasing. More speciﬁcally:
Theorem 2 ). Let XV be a Gaussian process.
Under suﬃciently ﬁne
discretization V, the greedy algorithm for mutual information is guaranteed to select a set AG of k
sensors for which MI(XAG) ≥(1 −1/e)(OPT −kε), where OPT is the mutual information achieved
by the optimal placement, and ε depends polynomially on the discretization.
Hence, we have the following result about exploitation using the mutual information criterion:
Corollary 3. Choose the discretization of the GP such that Theorem 2 holds for all discrete values
of Θ. Then MI(XAG | Θ) ≤max|π|=k MI(Xπ) ≤(1 −1/e)−1 P
θ P(θ) MI(X (θ)
AG | θ) + kε + H(Θ),
where AG is the greedy set for MI(XA | Θ) = P
θ P(θ) MI(XA | θ), and A(θ)
is the greedy set for
MI(XA | θ).
This result allows us to eﬃciently compute online bounds on how much can be gained by following
a sequential active learning strategy. Intuitively, it states that if this bound is close to our current
mutual information, we can stop exploring, and exploit our current knowledge about the model
by near-optimally ﬁnding the best set of observations. We can also use Corollary 3 as a stopping
criterion: We can use exploration techniques (as described in the next section) until the bound on
the advantage of the sequential strategy drops below a speciﬁed threshold η, i.e., we stop if
(1 −1/e)−1 P
θ P(θ) MI(X (θ)
AG | θ) + kε + H(Θ) −MI(XAG | Θ)
MI(XAG | Θ)
In this case, we can use the greedy a priori design to achieve near-optimal mutual information, and
obtain performance comparable to the optimal sequential policy. This a priori design is logistically
simpler and easier to analyze. Hence, the stopping criterion interpretation of Corollary 3 has strong
practical value, and we are not aware of any other approach for actively learning GPs which allow
to compute such a stopping criterion.
Implicit and Explicit Exploration
In order to practically use Corollary 3 as a stopping criterion for exploration, we have to, for each
parameter θ, solve the optimization problem maxA H(XA | θ). The following theorem shows, that if
the parameter entropy is small enough, the contribution of the term P
θ P(θ) max|A|=k MI(XA | θ)
to the bound diminishes quickly, and hence, we should concentrate solely on minimizing the parameter entropy H(Θ).
Theorem 4. Let M = maxA maxθ1,θ2
MI(XA|θ2) < ∞. Let K = maxθ maxA MI(XA | θ), H(Θ) < 1.
MI(XA∗| Θ) −H(Θ)≤MI(Xπ∗)≤MI(XA∗| Θ) + CH(Θ),
where A∗= argmaxA MI(XA | Θ) and π∗= argmaxπ MI(Xπ), and C =
As a function of H(Θ), C converges to 1 very quickly as H(Θ) decreases. Theorem 4 hence provides
the computational advantage, that, once the parameter entropy is small enough, we do not need
to recompute the term P
θ P(θ) max|A|=k MI(XA | θ) when using Theorem 1 as a criterion for
stopping exploration. Hence, in the following, we concentrate on directly decreasing the parameter
uncertainty. We describe three natural strategies for this goal. As we show in Section 7, none of these
strategies dominates the other; whichever is more appropriate depends on the particular application.
Explicit Exploration via Independence Tests (ITE).
In many cases, the unknown parameter of an isotropic GP is the bandwidth of the kernel, eﬀectively scaling the kernel over space. Let
θ1 < · · · < θm be the possible bandwidths. In the exponential kernel, Kθ(δ) = exp(−|δ|
θ ), or the
Gaussian kernel, Kθ(δ) = exp(−δ2
θ2 ), the correlation between two variables at distance δ decreases
exponentially with their distance δ. Hence, there is an exponentially large gap between the correlation for bandwidths θi and θi+1: There will be a distance ˆδ, for which two random variables within
this distance will appear dependent if the true bandwidth θ is at least θ ≥θi+1, and (roughly)
independent if θ ≤θi. Our goal is to exploit this gap to eﬃciently determine the correct parameter.
First note that if we can separate θi from θi+1, we eﬀectively distinguish any θj, for j ≤i, from θl,
for l ≥i+1, since the bandwidths scale the kernels. Let Ii be a function of Θ, such that (Ii | Θ) = 0
if Θ ≤θi, and (Ii | Θ) = 1 if Θ ≥θi+1. Assume we have tests Ti, using ˆN samples, such that
P(Ti ̸= Ii | θ) ≤α for all θ. We can now use a binary search procedure to identify the true bandwidth with high probability using at most ˆN⌈log2 m⌉samples. Let πG◦IT E be the policy, where we
ﬁrst explore using ITE, and then greedily select the set AG maximizing MI(XAG | Θ, xπIT E). Let
xπIT E be the observations made by ITE, and let A(θ)
be the solution of the greedy algorithm for
optimizing MI(XA | θ).
Theorem 5. Under the assumptions of Corollary 3 for sets of sizes up to k + ˆN⌈log m⌉, if we have
tests Ti using at most ˆN samples, such that for all θ: P(Ti ̸= Ii | θ) ≤α/(⌈log m⌉2(maxθ | MI(XπG◦IT E |
Θ) −MI(XA(θ)
ET [MI(XπG◦IT E | Θ)] ≥(1 −1/e) max
|π|=k MI(Xπ) −kε −α.
In order to make use of Theorem 5, we need to ﬁnd tests Ti such that P(Ti ̸= Ii | θ) is suﬃciently
small for all θ. If only the bandwidth is unknown, we can for example use a test based on Pearson’s correlation coeﬃcient. Since this test requires independent samples, let us ﬁrst assume, that
the kernel function has bounded support (c.f., Storkey (99)), and that the domain of the GP is
suﬃciently large, such that we can get independent samples by sampling pairs of variables outside
the support of the “widest” kernel. The number of samples will depend on the error probability
α, and the diﬀerence ˆρ between the correlations depending on whether Θ ≤θi or Θ ≥θi+1. This
diﬀerence will in turn depend on the distance between the two samples. Let
Kθj(δ) −Kθl(δ)
ˆδi = argmax
Kθj(δ) −Kθl(δ)
ˆρi is the maximum “gap” achievable for separating bandwidths at most θi from those at least θi+1.
ˆδi is the distance at which two samples should be taken to achieve this gap in correlation. If several
feasible pairs of locations are avaible, we choose the one which maximizes mutual information.
Theorem 6. We need ˆNi = O
ρi2 log2 1
independent pairs of samples at distance ˆδi to decide
between θ ≤θi or θ ≥θi+1 with P(Ti ̸= Ii | θ) ≤α for all θ.
In the case of kernels with non-compact support, such as the Gaussian or Exponential kernel1,
we cannot generate such independent samples, since distant points will have some (exponentially
small) correlation. However, these almost independent samples suﬃce:
Corollary 7. Let X have variance σ2, measurement noise σ2
n at each location, ˆρ = mini ˆρi, and
ξ < ˆρ. We can obtain a test Ti with P(Ti ̸= Ii | θ) ≤α using ˆN = O
(ˆρ−ξ)2 log2 1
samples Xs = (Xs1, Xs2) at distance ˆδi, if, for every Xs and Xt in our sample set, Cor(Xsi, Xtj) ≤
N⌈log2 m⌉, for i, j ∈{1, 2}.
Hence, since most kernel functions decay exponentially fast, only a small spatial distance has to be
guaranteed between the pairs of samples of the independence tests. Note that while this discussion
1For the Gaussian and the Exponential kernel for example, we can compute ˆρi analytically.
focused on detecting bandwidths, the technique is general, and can be used to distinguish other
parameters, e.g., variance, as well, as long as appropriate tests are available.
This hypothesis testing exploration strategy gives us sample complexity bounds. It guarantees
that with a small number of samples we can decrease the parameter uncertainty enough such that,
using Theorem 4 as stopping criterion, we can switch to exploitation.
Explicit Exploration based on Information Gain (IGE).
As the bound in Theorem 4 directly depends on H(Θ), another natural exploration strategy is to select samples which have highest
information gain about the parameters, H(Θ). More formally, this strategy, after observing samples
Xπ1:i = xπ1:i, selects the location πi+1 such that πi+1 = argmaxy H(Θ | xπ1:i) −H(Θ | Xy, xπ1:i).
Implicit Exploration (IE).
The following generalization of the “information never hurts” principle to policies shows that any exploration strategy will, in expectation,
decrease H(Θ).
Proposition 8. Let XV be a GP with kernel parameters Θ. Let π be a policy for selecting observations. Then H(Θ | Xπ) ≤H(Θ).
Considering the near-optimal performance of the greedy heuristic in the a priori case, a natural
implicit exploration strategy is the sequential greedy algorithm. Using Eq. (3.2), IE considers the
previous observations, when deciding on the next observation, and, using Proposition 8, implicitly
decreases H(Θ).
Actively learning nonstationary GPs
Many spatial phenomena are nonstationary, being strongly correlated in some areas of the space
and very weakly correlated in others. In our river example, we consider the pH values in the region
just below the conﬂuence of the San Joaquin and Merced rivers. The former was dominated by
agricultural and wetland drainage, whereas, in contrast, the latter was less saline. The data (c.f.,
Figure 2(a)) is very nonstationary. There is very high correlation and low variance in the outer
regions. The turbulent conﬂuence region however exhibits high variance and low correlation.
Modeling nonstationarity has to trade oﬀrichness of the model and computational and statistical
tractability. Even though the covariance function is a an inﬁnite dimensional object, often a parametric form is chosen. For example, Nott and Dunsmuir suggest to model nonstationarity
by a spatially varying linear combination of isotropic processes. In any such a parametric setting,
Corollary 3 holds without additional assumptions; the major diﬀerence is that H(Θ) can be much
larger, increasing the potential for improvement of the active strategy over the a priori design.
Nonstationary model
Motivated by the river monitoring problem, we partition the space into disjoint regions V(1), . . . , V(m),
which are speciﬁed by the user. With each region V(i), we associate an isotropic process X (i)
parameters Θ(i), which are assumed to have independent priors. We deﬁne our GP prior for the
full space V as a linear combination of the local GPs: Xs = P
i λi(s)X (i)
s . Note that such a linear
combination is still a valid GP. How should we choose the weights λi(s)? We want a model which
behaves similar to process X (i)
within region i, and interpolates smoothly between regions. In order to achieve that, we associate a weighting function νi(s) with each region. This function should
achieve its maximum value in region i and decrease with distance to region i. In our river example,
we set the weighting functions as indicated in Figure 2(a). We can then set λi(s) =
i′ νi′(s),
which ensures that the variance at location s is a convex combination of the variances of the local
GPs, with contribution proportional to νi(s). If each X (i)
has zero mean, and kernel Ki(s, t), then
the new, nonstationary GP XV has the kernel P
i λi(s)λi(t) Ki(s, t). By adding a deterministic
function M(s), one can also modify the prior mean of the GP. While the decomposition into prespeciﬁed regions might appear restrictive, in many applications, as in the river monitoring setting,
a good decomposition can be provided by an expert. Furthermore, one can control the amount
of smoothing by a bandwidth parameter, which can be part of the model. By this approach, the
data itself can decide whether two adjacent regions should be joined (high smoothing bandwidth)
or almost independent (low smoothing bandwidth).
Eﬃcient Nonstationary Active Learning
Now, in principle we could apply Corollary 3 to this model to determine when to switch from exploration (e.g., using information gain) to exploitation. However, even if each Θ(i) is discretized so that
the distribution over Θ(i) can be exactly maintained, the joint distribution over Θ = (Θ(1), . . . , Θ(m))
is exponentially large in m. In order to address this problem, let us ﬁrst consider the special case
where each ν(i) is positive only within region i. In this case, an observation made in region i only
aﬀects the prediction and parameter estimation in region i.
The joint distribution over Θ will
always stay fully factorized, and eﬃcient inference is possible. We eﬀectively monitor a collection
of independent GPs, and our active learning algorithm attempts to optimally allocate the samples
to the independent GPs.
Now let us consider the general case, where the weights νi take positive values outside region i.
In this case, an observation s made with positive weights νi(s) > 0 and νj(s) > 0 for two regions
i and j eﬀectively couples the parameters Θ(i) and Θ(j). Eventually, all parameters become dependent, and we need to maintain the full, exponentially large joint distribution. In order to cope
with this complexity, we apply a variational approach: After making an observation, we ﬁnd a fully
factorized2 approximate posterior distribution, which is closest in KL divergence. More formally,
given a prior P(Θ) over the parameters and a set of locations A ⊆V and their values XA = xA,
we seek the distribution
P ′ factorized
KL(P(Θ | XA = xA) || P ′(Θ)).
For the multinomial distribution, the solution bP minimizing the KL divergence can be obtained by
matching the marginals of the exact posterior . The following proposition
shows that this procedure does not invalidate our stopping criterion.
Proposition 9. H( bP(Θ)) ≥H(P(Θ | XA = xA)).
Hence, using Theorem 4, our variational
approach never stops exploring too early.
In order to use this nonstationary model for active learning, we need to condition on observations
and compute mutual information eﬃciently.
2More complex distributions, which still allow eﬃcient inference, such as trees, can be used as well.
Computing conditional distributions.
We assume we have a fully factorized distribution
bP(Θ), which already incorporates previous observations XA = xA, and we want to incorporate
a new observation Xs = xs at location s. We ﬁrst ﬁnd the relevant regions V(i1), . . . , V(im). A
region is relevant3 to location s if νj(s) > 0.
For each joint instantiation of the relevant parameters ¯θ = (θi1, . . . , θim), we compute the likelihood of the observation P(Xs = xs | ¯θ, xA′),
where xA′ are the previous observations made within the relevant regions.
Using Bayes’ rule,
P(¯θ | xs, xA) ∝bP(¯θ)P(xs | xA, ¯θ), we can compute the exact parameter posterior. Remembering all observed data, we can always compute P(¯θ | xs, xA) using GP regression. Now that we
have the exact parameter posterior, we ﬁnd the KL-minimizing fully factorized approximation to
P(¯θ | xs, xA) by marginalisation.
Computing entropy and mutual information.
In order to implement the greedy policy for
mutual information πGMI or entropy πGH, we need to be able to compute H(Xs | XA, θ) for the
location s under consideration, and a set of observations A (or V \(A∪{s}) for mutual information).
We can compute this quantity very similarly to the procedure described above. We ﬁrst ﬁnd the
regions relevant to s, V(i1), . . . , V(im), and set A′ = V′ ∩A, where V′ = V(i1) ∪· · ·∪V(im). As above,
for every joint instantiation of the relevant parameters ¯θ, we compute the conditional entropy on
the GP X ′
V, which we can do eﬃciently in closed form given the parameters ¯θ. We can then compute
H(Xs | XA = xA, Θ) = P
¯θ bP(¯θ)H(Xs | XA = xA, ¯θ).
In summary, our active learning strategy for nonstationary GPs is similar to the isotropic case:
We explore until Corollary 3 proves that the advantage of the sequential strategy is small enough,
then switch to exploitation. The diﬀerence is that we use a variational approach to leverage the
structure of the nonstationary GP as a linear combination of locally supported isotropic GPs.
Experiments
River Monitoring.
We ﬁrst describe results on our river monitoring application. We consider
one high-resolution spatial scan of pH measurements from the NIMS sensor deployed just below
the conﬂuence of the San Joaquin and the Merced rivers in California (denoted by [R]) . We partition the transect into four regions, with smoothing weights indicated in
Figure 2(a), and we use 2 bandwidth and 5 noise variance levels. Figure 2(a) illustrates the samples
chosen by implicit exploration (IE) using the entropy criterion. The bars indicate the sequence of
observations, and larger bars correspond to later observations (i.e., based on more knowledge about
the model). We can observe that while the initial samples are roughly uniformly distributed, the
later samples are mostly chosen in the weakly correlated, high variance turbulent conﬂuence region.
In parentheses, we display the estimated bandwidths and noise standard deviations. Figure 2(b)
presents the results from our algorithms.
The sequential algorithm leads to a quicker decrease
in Root Mean Squared (RMS) error than the a priori design. Initially, the isotropic model with
two parameters provides a better ﬁt than the nonstationary model with 8 parameters, but, after
about 15 samples, the situation is inverted, and the nonstationary model drastically outperforms
the isotropic model after 28 samples, providing more than 50% lower error.
3We assume here that the νi are supported in a small number of regions. If this is not the case, we can use
truncation arguments similar to those by Guestrin et al. .
1 (14.54/0.04)
(13.10/0.03)
(13.82/0.10)
(14.49/0.02)
Coordinates (m)
(a) [R] Selected samples
Number of sensors
nonstationary
nonstationary
(b) [R] Isotropic vs. nonstat.
Number of observations
(c) [T] Exploration strategies
Number of observations
Relative sequential advantage
(d) [T] Sequential advantage
Figure 2: Results on pH [R] and temperature [T] data. (a) Top: sampling locations chosen by active
learning algorithm. Higher bars indicate later (i.e., more informed) choice. Bottom: Smoothing
functions used for spatial partitioning. (b) Comparison of prediction error for pH data. Note that
the sequential algorithm on the nonstationary model eventually reduces the error incurred by the
a priori design and isotropic model by more than 50%. (c) Comparison of exploration strategies,
isotropic model. (d) Bounds on the potential advantage of the sequential algorithm using Theorem 3
(Stopping criterion). Information gain leads to quickest drop of bound, but worse spatial prediction.
Temperature Data.
We consider temperature data [T] from a sensor network deployment with
54 sensors at Intel Research Berkeley.
Our 145 samples consist of measurements taken every
hour by the sensors over 5 days.
We modeled the data as an isotropic process with unknown
variance and an Exponential kernel with unknown bandwidth.
We discretized the variance in
σ2 ∈{12, 22, 32, 42, 52}, and the bandwidth in {3, 5, 7, 9, 11, 13, 15} meters based on expert knowledge. We compared the performance of the active learning strategies, each using a diﬀerent exploration strategy. Figure 2(c) shows the RMS prediction error, and Figure 2(d) presents the potential
relative advantage obtained by Theorem 3 (our stopping criterion). While IE leads to the best prediction, followed by the independence test exploration (ITE), information gain exploration (IGE)
tightens the bound on the sequential advantage the fastest. For example., if we decide to stop exploring once the sequential advantage drops below η = 35%, 5 samples suﬃce for IGE, 8 for ITE and
12 for IE. This analysis (which is also supported by other data sets) indicates that none of the exploration strategies dominates each other, their diﬀerences can be well-characterized, and the choice of
Number of observations
nonstationary
nonstationary
nonstationary
(a) [T] Isotropic vs. nonstat.
Number of observations
Absolute error in bandwidth
a priori design,
nonstationary
nonstationary
nonstationary
(b) [T] Bandwidth error
Number of observations
Parameter entropy
nonstationary
nonstationary
(c) [T] Parameter Entropy
Number of observations
(d) [P] Sequential vs. a priori
Figure 3: Results on temperature [T] and precipitation [P] data.
(a) Comparison of isotropic,
nonstationary model, using random and sequential selection. Information gain achieves worst prediction, but reduces error in bandwidth (b) and parameter entropy (c) fastest. (d) Sequential design
outperforms a priori design on rain data.
strategy depends on the needs of each application. Hence, if the goal is to switch to a priori design
as quickly as possible, IGE might be the right choice, whereas if we can aﬀord to always perform
the logistically more complex sequential design, IE would decrease the predictive RMS error the
fastest. ITE performs well w.r.t. both criteria, and has theoretical sample complexity guarantees.
We also modeled the temperature using a nonstationary GP, with the space partitioned into
four regions, each modeled as an isotropic GP. We adopted a softmax function with smoothing
bandwidth 8 meters to spatially average over the local isotropic GPs. The results in Figure 3(a)
show that the nonstationary model leads to reduced prediction error compared to the isotropic
model. All active learning models drastically outperform random selection. Since the parameter
uncertainty is still very high after 20 samples, IGE leads to worse prediction accuracy than IE.
However, IGE decreases the parameter error Figure 3(b) (compared to the estimates when given all
observations) and parameter entropy H(Θ) Figure 3(c) the fastest. These results indicate (along
with higher log-likelihood), that even though we are estimating its 8 parameters from only up to
20 data points, the nonstationary model provides a better ﬁt to the data.
Precipitation Data.
In another experiment, we considered precipitation data [P] from 167 detector stations in the Paciﬁc Northwest. We followed the preprocessing suggested by Guestrin et al.
 . Figure 3(d) shows the RMS error for 110 samples, spaced roughly three months apart,
using an isotropic GP with 5 bandwidth and 3 variance parameter levels. Here, IE, ITE, IGE all
outperform the a priori design.
Conclusions
In this paper, we presented a nonmyopic analysis for active learning of Gaussian Processes. We
proved bounds on how much better a sequential algorithm can perform than an a priori design when
optimizing observation locations under unknown parameters. Our bounds show that key potential
for improvement is in the parameter entropy, motivating an exploration–exploitation approach to
active learning, and provide insight into when to switch between the two phases. Using submodularity of our objective function, we provided bounds on the quality of our exploitation strategy. We
proposed several natural exploration strategies for decreasing parameter uncertainty, and proved
logarithmic sample complexity results for exploration phase using hypothesis testing. We extended
our algorithm to handle nonstationary GP, exploiting local structure in the model. Here, we used
a variational approach to address the combinatorial growth of the parameter space. In addition
to our theoretical analyses, we evaluated our algorithms on several real-world problems, including
data from a real deployment for monitoring the ecological condition of a river. We believe that our
results provide signiﬁcant new insights on the potential of sequential active learning strategies for
monitoring spatial phenomena using GPs.