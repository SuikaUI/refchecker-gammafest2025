warwick.ac.uk/lib-publications
Manuscript version: Author’s Accepted Manuscript
The version presented in WRAP is the author’s accepted manuscript and may differ from the
published version or Version of Record.
Persistent WRAP URL:
 
How to cite:
Please refer to published version for the most recent bibliographic citation information.
If a published version is known of, the repository item page linked to above, will contain
details on accessing it.
Copyright and reuse:
The Warwick Research Archive Portal (WRAP) makes this work by researchers of the
University of Warwick available open access under the following conditions.
© 2019 Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-
NoDerivatives 4.0 International 
Publisher’s statement:
Please refer to the repository item page, publisher’s statement section, for further
information.
For more information, please contact the WRAP Team at: .
HoVer-Net: Simultaneous Segmentation and
Classiﬁcation of Nuclei in Multi-Tissue Histology
Simon Graham1,2,*, Quoc Dang Vu3,*, Shan E Ahmed Raza2,4, Ayesha
Azam2,5, Yee Wah Tsang5, Jin Tae Kwak 3,+ and Nasir Rajpoot2,6,+
*First authors contributed equally
+Last authors contributed equally
1Mathematics for Real World Systems Centre for Doctoral Training, University of
Warwick, UK
2Department of Computer Science, University of Warwick, UK
3Department of Computer Science and Engineering, Sejong University, South Korea
4 Centre for Evolution and Cancer & Division of Molecular Pathology, The Institute of
Cancer Research, London, UK
5 University Hospitals Coventry and Warwickshire, Coventry, UK
6The Alan Turing Institute, London, UK
Nuclear segmentation and classiﬁcation within Haematoxylin & Eosin stained
histology images is a fundamental prerequisite in the digital pathology work-
ﬂow. The development of automated methods for nuclear segmentation and
classiﬁcation enables the quantitative analysis of tens of thousands of nuclei
within a whole-slide pathology image, opening up possibilities of further analysis
of large-scale nuclear morphometry. However, automated nuclear segmentation
and classiﬁcation is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such
as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional
neural network for simultaneous nuclear segmentation and classiﬁcation that
leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then
utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance
the network predicts the type of nucleus via a devoted up-sampling branch. We
 
September 17, 2019
demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work,
we introduce a new dataset of Haematoxylin & Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with
associated class labels.
Nuclear segmentation, nuclear classiﬁcation, computational
pathology, deep learning
1. Introduction
Current manual assessment of Haematoxylin and Eosin (H&E) stained histology slides suﬀers from low throughput and is naturally prone to intra- and
inter-observer variability . To overcome the diﬃculty in
visual assessment of tissue slides, there is a growing interest in digital pathology
(DP), where digitised whole-slide images (WSIs) are acquired from glass histology slides using a scanning device. This permits eﬃcient processing, analysis
and management of the tissue specimens . Each
WSI contains tens of thousands of nuclei of various types, which can be further analysed in a systematic manner and used for predicting clinical outcome.
Here, the type of nucleus refers to the cell type in which it is located. For example, nuclear features can be used to predict survival 
and also for diagnosing the grade and type of disease . Also,
eﬃcient and accurate detection and segmentation of nuclei can facilitate good
quality tissue segmentation ,
which can in turn not only facilitate the quantiﬁcation of WSIs but may also
serve as an important step in understanding how each tissue component contributes to disease. In order to use nuclear features for downstream analysis
within computational pathology, nuclear segmentation must be carried out as
an initial step. However, this remains a challenge because nuclei display a high
level of heterogeneity and there is signiﬁcant inter- and intra-instance variability in the shape, size and chromatin pattern between and within diﬀerent cell
types, disease types or even from one region to another within a single tissue
sample. Tumour nuclei, in particular, tend to be present in clusters, which gives
rise to many overlapping instances, providing a further challenge for automated
segmentation, due to the diﬃculty of separating neighbouring instances.
As well as extracting each individual nucleus, determining the type of each
nucleus can increase the diagnostic potential of current DP pipelines. For example, accurately classifying each nucleus to be from tumour or lymphocyte
enables downstream analysis of tumour inﬁltrating lymphocytes (TILs), which
have been shown to be predictive of cancer recurrence .
Yet, similar to nuclear segmentation, classifying the type of each nucleus is diﬃcult, due to the high variance of nuclear appearance within each WSI. Typically,
nuclei are classiﬁed using two disjoint models: one for detecting each nucleus and
then another for performing nuclear classiﬁcation . However, it would be preferable to utilise a single uniﬁed model
for nuclear instance segmentation and classiﬁcation.
In this paper, we present a deep learning approach1 for simultaneous segmentation and classiﬁcation of nuclear instances in histology images. The network
is based on the prediction of horizontal and vertical distances (and hence the
name HoVer-Net) of nuclear pixels to their centres of mass, which are subsequently leveraged to separate clustered nuclei. For each segmented instance, the
nuclear type is subsequently determined via a dedicated up-sampling branch.
To the best of our knowledge, this is the ﬁrst approach that achieves instance
segmentation and classiﬁcation within the same network.
We present com-
parative results on six independent multi-tissue histology image datasets and
demonstrate state-of-the-art performance compared to other recently proposed
methods. The main contributions of this work are listed as follows:
• A novel network, targeted at simultaneous segmentation and classiﬁcation
of nuclei, where horizontal and vertical distance map predictions separate
1Model code available at: net
clustered nuclei.
• We show that the proposed HoVer-Net achieves state-of-the-art performance on multiple H&E histology image datasets, as compared to over a
dozen recently published methods.
• An interpretable and reliable evaluation framework that eﬀectively quan-
tiﬁes nuclear segmentation performance and overcomes the limitations of
existing performance measures.
• A new dataset2 of 24,319 exhaustively annotated nuclei within 41 colorectal adenocarcinoma image tiles.
2. Related Work
2.1. Nuclear Instance Segmentation
Within the current literature, energy-based methods, in particular the
watershed algorithm, have been widely utilised to segment nuclear instances.
For example, Yang et al. used thresholding to obtain the markers and
the energy landscape as input for watershed to extract the nuclear instances.
Nonetheless, thresholding relies on a consistent diﬀerence in intensity between
the nuclei and background, which does not hold for more complex images and
hence often produces unreliable results. Various approaches have tried to provide an improved marker for marker-controlled watershed. Cheng et al. 
used active contours to obtain the markers. Veta et al. used a series
of morphological operations to generate the energy landscape. However, these
methods rely on the predeﬁned geometry of the nuclei to generate the markers, which determines the overall accuracy of each method. Notably, Ali and
Madabhushi avoided the trouble of reﬁning the markers for watershed
2The CoNSeP dataset for nuclear segmentation is available at 
fac/sci/dcs/research/tia/data/.
by designing a method that relies solely on the energy landscape. They com-
bined an active contour approach with nuclear shape modelling via a level-set
method to obtain the nuclear instances. Despite its widespread usage, obtaining
suﬃciently strong markers for watershed is a non-trivial task. Some methods
have departed from the energy-based approach by utilising the geometry of the
nuclei. For instance, Wienert et al. , LaTorre et al. and Kwak
et al. computed the concavity of nuclear clusters, while Liao et al. 
used eclipse-ﬁtting to separate the clusters. However, this assumes a predeﬁned
shape, which does not encompass the natural diversity of the nuclei. In addition, these methods tend to be sensitive to the choice of manually selected
parameters.
Recently, deep learning methods have received a surge of interest due to
their superior performance in many computer vision tasks . These approaches are capable of automatically extracting a representative set of features, that strongly correlate with
the task at hand. As a result, they are preferable to hand-crafted approaches,
that rely on a selection of pre-deﬁned features. Inspired by the Fully Convolutional Network (FCN) , U-Net 
has been successfully applied to numerous segmentation tasks in medical image
analysis. The network has an encoder-decoder design with skip connections to
incorporate low-level information and uses a weighted loss function to as-
sist separation of instances. However, it often struggles to split neighbouring
instances and is highly sensitive to pre-deﬁned parameters in the weighted loss
function. A more recently proposed method in Micro-Net 
extends U-Net by utilising an enhanced network architecture with weighted loss.
The network processes the input at multiple resolutions and as a result, gains
robustness against nuclei with varying size. In Graham and Rajpoot , the
authors developed a network that is robust to stain variations in H&E images
by introducing a weighted loss function that is sensitive to the Haematoxylin
intensity within the image.
Other methods exploit information about the nuclear contour (or bound-
ary) within the network, such as DCAN that utilised a dual
architecture that outputs the nuclear cluster and the nuclear contour as two
separate prediction maps. Instance segmentation is then achieved by subtracting the contour from the nuclear cluster prediction. Cui et al. proposed a
network to predict the inner nuclear instance, the nuclear contour and the back-
ground. The network utilised a customised weighted loss function based on the
relative position of pixels within the image to improve and stabilise the inner nuclei and contour prediction. Some other methods have also utilised the nuclear
contour to achieve instance segmentation. For example, Kumar et al. 
employed a deep learning technique for labelling the nuclei and the contours,
followed by a region growing approach to extract the ﬁnal instances. Khoshdeli
and Parvin used the contour predictions as input into a further network for segmentation reﬁnement. Zhou et al. proposed CIA-Net, that
utilises a multi-level information aggregation module between two task-speciﬁc
decoders, where each decoder segments either the nuclei or the contours. A Deep
Residual Aggregation Network (DRAN) was proposed by Vu et al. that
uses a multi-scale strategy, incorporating both the nuclei and nuclear contours
to accurately segment nuclei.
There have been various other methods to achieve instance separation. Instead of considering the contour, Naylor et al. proposed a deep learning
approach to detect superior markers for watershed by regressing the nuclear
distance map. Therefore, the network avoids making a prediction for areas
with indistinct contours.
In line with these developments, the ﬁeld of instance segmentation within
natural images is also rapidly progressing and have had a signiﬁcant inﬂuence
on nuclear instance segmentation methods. A notable example is Mask-RCNN
 , where instance segmentation approach is achieved by ﬁrst
predicting candidate regions likely to contain an object and then deep learning
based segmentation within those proposed regions.
2.2. Nuclear Classiﬁcation
As well as performing instance segmentation, it is desirable to determine the
type of each nucleus to facilitate and improve downstream analysis. It is possible for current models to diﬀerentiate between certain nuclear types in H&E,
however sub-typing of lymphocytes is an extremely hard task due to the high
levels of similarity in morphological appearance between T and B lymphocytes.
Typically, classifying each nucleus is done via a two-stage approach, where the
ﬁrst step involves either nuclear segmentation or nuclear detection. When segmentation is used as the initial step, a series of morphological and textural
features are extracted from each instance, which are then used within a classi-
ﬁer to determine the nuclei classes. For example, Nguyen et al. classiﬁed
nuclei within H&E stained breast cancer images as either tumour, lymphocyte
or stromal based on their morphological features. Yuan et al. performed
nuclear segmentation and then classiﬁed each nucleus with AdaBoost classiﬁer,
utilising the intensity, morphology and texture of nuclei as features. Otherwise,
detection is performed as an initial step and a patch centred at the point of
detection is fed into a classiﬁer, to predict the type of nucleus. Sirinukunwattana et al. proposed a spatially constrained CNN, that initially detects
all nuclei and then for each nucleus an ensemble of associated patches are fed
into a CNN to predict the type to be either epithelial, inﬂammatory, ﬁbroblast
or miscellaneous.
3. Methods
Our overall framework for automatic nuclear instance segmentation and classiﬁcation can be observed in Fig. 1 and the proposed network in Fig. 2. Here,
nuclear pixels are ﬁrst detected and then, a tailored post-processing pipeline is
used to simultaneously segment nuclear instances and obtain the corresponding
nuclear types. The framework is based upon the horizontal and vertical distance
maps, which can be seen in Fig. 3. In the ﬁgure, each nuclear pixel denotes
either the horizontal or vertical distance of pixels to their centres of mass.
Figure 1: Overview of the proposed approach for simultaneous nuclear instance segmentation
and classiﬁcation. When no classiﬁcation labels are available, the network produces the instance segmentation as shown in (a). The diﬀerent colours of the nuclear boundaries represent
diﬀerent types of nuclei in (b).
3.1. Network Architecture
In order to extract a strong and representative set of features, we employ
a deep neural network. The feature extraction component of the network is
inspired by the pre-activated residual network with 50 layers 
(Preact-ResNet50), due to its excellent performance in recent computer vision
tasks and robustness against input perturbation . Compared to the standard Preact-ResNet50 implementation, we
reduce the total down-sampling factor from 32 to 8 by using a stride of 1 in
the ﬁrst convolution and removing the subsequent max-pooling operation. This
ensures that there is no immediate loss of information that is important for performing an accurate segmentation. Various residual units are applied throughout the network at diﬀerent down-sampling levels. A series of consecutive resid-
ual units is denoted as a residual block. The number of residual units within
each residual block is 3, 4, 6 and 3 that are applied at down-sampling levels 1,
2, 4 and 8 respectively. For clarity, a down-sampling level of 2 means that the
input has a reduction in the spatial resolution by a factor of 2.
Figure 2: Overview of the proposed architecture. (a) (Pre-activated) residual unit, (b) dense
unit. m indicates the number of feature maps within each residual unit. The yellow square
within the input denotes the considered region at the output. When the classiﬁcation labels
aren’t available, only the up-sampling branches in the dashed box are considered.
Following Preact-ResNet50, we perform nearest neighbour up-sampling via
three distinct branches to simultaneously obtain accurate nuclear instance segmentation and classiﬁcation.
We name the corresponding branches: (i) nuclear pixel (NP) branch; (ii) HoVer branch and (iii) nuclear classiﬁcation (NC)
branch. The NP branch predicts whether or not a pixel belongs to the nuclei or
background, whereas the HoVer branch predicts the horizontal and vertical dis-
tances of nuclear pixels to their centres of mass. Then, the NC branch predicts
the type of nucleus for each pixel. In particular, the NP and HoVer branches
jointly achieve nuclear instance segmentation by ﬁrst separating nuclear pixels
from the background (NP branch) and then separating touching nuclei (HoVer
branch). The NC branch determines the type of each nucleus by aggregating
the pixel-level nuclear type predictions within each instance.
All three up-sampling branches utilise the same architectural design, which
consists of a series of up-sampling operations and densely connected units (or dense units). By stacking multiple and relatively cheap dense
units, we build a large receptive ﬁeld with minimal parameters, compared to us-
ing a single convolution with a larger kernel size and we ensure eﬃcient gradient
propagation. We use skip connections to incorporate
features from the encoder, but utilise summation as opposed to concatenation.
The consideration of low-level information is particularly important in segmentation tasks, where we aim to precisely delineate the object boundaries. We use
dense units after the ﬁrst and second up-sampling operations, where the number
of units is 4 and 8 respectively. Valid convolution is performed throughout the
two up-sampling branches to prevent poor predictions at the boundary. This
results in the size of the output being smaller than the size of the input. As
opposed to using a dedicated network for each task, a shared encoder makes
it possible to train the nuclear instance segmentation and classiﬁcation model
end-to-end and therefore, reduce the total training time. Furthermore, a shared
encoder can also take advantage of the shared information across multiple tasks
and thus, help to improve the model performance on all tasks.
Finally, if we do not have the classiﬁcation labels of the nuclei, only the
NP and HoVer up-sampling branches are considered. Otherwise, we consider
all three up-sampling branches and perform simultaneous nuclear instance segmentation and classiﬁcation.
We display an overview of the network architecture in Fig. 2, where the
spatial dimension of the input is 270×270 and the output dimension of each
branch is 80×80. The dashed box within Fig. 2 highlights the branches for
nuclear instance segmentation. Additionally, we also show a residual unit and a
dense unit within Fig. 2a and Fig. 2b. We denote m as the number of feature
maps within each convolution of a given residual unit. At each down sampling
level, from left to right, m=256, 512, 1024, 2048 respectively. We keep a ﬁxed
amount of feature maps within each dense unit throughout the two branches as
shown in Fig. 2c.
3.1.1. Loss Function
The proposed network design has 4 diﬀerent sets of weights: w0, w1, w2
and w3 which refer to the weights of the Preact-ResNet50 encoder, the HoVer
Horizontal Map
Prediction
Vertical Map
Prediction
Vertical Map
Ground Truth
Horizontal Map
Ground Truth
Horizontal Map
Prediction
Vertical Map
Prediction
Vertical Map
Ground Truth
Horizontal Map
Ground Truth
Figure 3: Cropped image regions showing horizontal and vertical map predictions, with corresponding ground truth. Arrows highlight the strong instance information encoded within
these maps, where there is a signiﬁcant diﬀerence in the pixel values.
branch decoder, the NP branch decoder and the NC branch decoder. These 4
sets of weights are optimised jointly using the loss L deﬁned as:
L = λaLa + λbLb
HoVer Branch
+ λcLc + λdLd
+ λeLe + λfLf
where La and Lb represent the regression loss with respect to the output of the
HoVer branch, Lc and Ld represent the loss with respect to the output at the
NP branch and and ﬁnally, Le and Lf represent the loss with respect to the
output at the NC branch. We choose to use two diﬀerent loss functions at the
output of each branch for an overall superior performance. λa...λf are scalars
that give weight to each associated loss function. Speciﬁcally, we set λb to 2
and the other scalars to 1, based on empirical selection.
Given the input image I, at each pixel i we deﬁne pi(I, w0, w1) as the re-
gression output of the HoVer branch, whereas qi(I, w0, w2) and ri(I, w0, w3)
denote the pixel-based softmax predictions of the NP and NC branches respectively. We also deﬁne Γi(I), Ψi(I) and Φi(I) as their corresponding ground truth
(GT). Ψi(I) is the GT of the nuclear binary map, where background pixels have
the value of 0 and nuclear pixels have the value 1. On the other hand, Φi(I) is
the nuclear type GT where background pixels have the value 0 and any integer
value larger than 0 indicates the type of nucleus. Meanwhile, Γi(I) denotes the
GT of the horizontal and vertical distances of nuclear pixels to their corresponding centres of mass. For Γi(I), we assign values between -1 and 1 to nuclear
pixels in both the horizontal and vertical directions. We assign the value of the
background and the line crossing the centre of mass within each nucleus to be 0.
For clarity, we denote the horizontal and vertical components of the GT HoVer
map as horizontal map Γi,x and vertical map Γi,y respectively. Visual examples
of the horizontal and vertical maps can be seen in Fig. 3.
At the output of the HoVer branch, we compute a multiple term regression
loss. We denote La as the mean squared error between the predicted horizontal
and vertical distances and the GT. We also propose a novel loss function Lb that
calculates the mean squared error between the horizontal and vertical gradients
of the horizontal and vertical maps respectively and the corresponding gradients
of the GT. We formally deﬁne La and Lb as:
(pi(I; w0, w1) −Γi(I))2
(∇x(pi,x(I; w0, w1)) −∇x(Γi,x(I)))2
(∇y(pi,y(I; w0, w1)) −∇y(Γi,y(I)))2
Within equation (3), ∇x and ∇y denote the gradient in the horizontal x and
vertical y directions respectively.
m denotes total number of nuclear pixels
within the image and M denotes the set containing all nuclear pixels.
At the output of NP and NC branches, we calculate the cross-entropy loss
(Lc and Le) and the dice loss (Ld and Lf). These two losses are then added
together to give the overall loss of each branch. Concretely, we deﬁne the cross
entropy and dice losses as:
Xi,k(I) log Yi,k(I)
Dice = 1 −2 × PN
i=1(Yi(I) × Xi(I)) + ϵ
i=1 Yi(I) + PN
i=1 Xi(I) + ϵ
where X is the ground truth, Y is the prediction, K is the number of classes
and ϵ is a smoothness constant which we set to 1.0e−3. When calculating Lc
and Ld for NP branch, for a given pixel i, we set Xi and Yi as qi(I, w0, w2)
and Ψi respectively. For Lc, we set K to be 2 within equation (4) because the
task of the branch is to perform binary nuclear segmentation. Similarly, for Le
and Lf at NC branch, for a given pixel i, we substitute Xi for Φi(I) and Yi for
ri(I, w0, w3) in equations (4) and (5). K is set as 5 within equation (4) when
calculating Le, denoting the 4 types of nuclei that our model currently predicts
and the background. Note, the value of K is chosen to reﬂect the number of
nuclear types represented in the training set.
It must be noted that the NC branch loss Le and Lf are only calculated
when the classiﬁcation labels are available. In other words, as mentioned in
Section 3.1, the network performs only instance segmentation if there are no
classiﬁcation labels given.
3.2. Post Processing
Within each horizontal and vertical map, pixels between separate instances
have a signiﬁcant diﬀerence. This can be seen in Fig. 3 and is highlighted by the
arrows. Therefore, calculating the gradient can inform where the nuclei should
be separated because the output will give high values between neighbouring
nuclei, where there is a signiﬁcant diﬀerence in the pixel values. We deﬁne:
Sm = max(Hx(px), Hy(py))
where px and py refer to the the horizontal and vertical predictions at the output
of the HoVer branch and Hx and Hy refer to the horizontal and vertical components of the Sobel operator. Speciﬁcally, Hx and Hy compute the horizontal
and vertical derivative approximations and are shown by the gradient maps in
Fig. 1. Therefore, Sm highlights areas where there is a signiﬁcant diﬀerence
in neighbouring pixels within the horizontal and vertical maps. Therefore, ar-
eas such as the ones shown by the arrows in Fig. 3 will result in high values
within Sm. We compute markers M = σ(τ(q, h) −τ(Sm, k)). Here, τ(a, b) is
a threshold function that acts on a and sets values above b to 1 or 0 otherwise. Speciﬁcally, h and k were chosen such that they gave the optimal nuclear
segmentation results. σ is a rectiﬁer that sets all negative values to 0 and q
is the probability map output of the NP branch. We obtain the energy landscape E = [1 −τ(Sm, k)] ∗τ(q, h). Finally, M is used as the marker during
marker-controlled watershed to determine how to split τ(q, h), given the energy
landscape E. This sequence of events can be seen in Fig. 1.
To perform simultaneous nuclear instance segmentation and classiﬁcation, it
is necessary to convert the per-pixel nuclear type prediction at the output of the
NC branch to a prediction per nuclear instance. For each nuclear instance, we
use majority class of the predictions made by the NC branch, i.e., the nuclear
type of all pixels in an instance is assigned to be the class with the highest
frequency count for that nuclear instance.
Please refer to Appendix A for a full analysis on the contribution of our proposed loss function, post-processing method and devoted classiﬁcation branch.
4. Evaluation Metrics
4.1. Nuclear Instance Segmentation Evaluation
Assessment and comparison of diﬀerent methods is usually given by an over-
all score that indicates which method is superior. However, to further investigate
the method, it is preferable to break the problem into sub-tasks and measure the
performance of the method on each sub-task. This enables an in depth analysis, thus facilitating a comprehensive understanding of the approach, which can
help drive forward model development. For nuclear instance segmentation, the
problem can be divided into the following three sub-tasks:
Figure 4: Examples highlighting the limitations of DICE2 and AJI with slightly diﬀerent
predictions. For better visualisation, ground truth contours (red dash line) for each instance
have been overlaid on both the predictions and original images.
Table 1: Comparison between Prediction A and Prediction B from Fig.4 across various measurements.
Prediction A
Prediction B
• Separate the nuclei from the background
• Detect individual nuclear instances
• Segment each detected instance
In the current literature, two evaluation metrics have been mainly adopted to
quantitatively measure the performance of nuclear instance segmentation: 1)
Ensemble Dice (DICE2) , and 2) Aggregated Jaccard Index
(AJI) . Given the ground truth X and prediction Y , DICE2
computes and aggregates DICE per nucleus, where Dice coeﬃcient (DICE) is
deﬁned as 2×(X∩Y )/(|X|+|Y |) and AJI computes the ratio of an aggregated
intersection cardinality and an aggregated union cardinality between X and Y .
These two evaluation metrics only provide an overall score for the instance
segmentation quality and therefore provides no further insight into the sub-tasks
at hand. In addition, these two metrics have a limitation, which we illustrate
From the ﬁgure, although prediction A only diﬀers from predic-
tion B by a few pixels, the DICE2 and AJI scores for B are inferior. These
scores are shown in Table 1. This problem arises due to over-penalisation of the
overlapping regions. By overlaying the GT segment contours (red dashed line)
upon the two predictions, we observe that, although the cyan-coloured instance
within prediction A overlaps mostly with the cyan-coloured GT instance, it also
slightly overlaps with the blue-coloured GT instance. As a result, according to
the DICE2 algorithm, the predicted cyan instance will be penalised by pixels
not only coming from the dominant overlapping cyan-coloured GT instance, but
also from the blue-coloured GT instance. The AJI also suﬀers from the same
phenomenon. However, because AJI only uses the prediction and GT instance
pair with the highest intersection over union, over-penalisation is less likely compared to DICE2. Over-penalisation is likely to occur when the model completely
fails to detect the neighbouring instance, such as in Fig. 4. Nonetheless, when
evaluating methods across diﬀerent datasets, speciﬁcally on samples containing
lots of hard to recognise nuclei such as ﬁbroblasts or nuclei with poor staining,
the number of failed detections may increase and therefore may have a negative
impact on the AJI measurement. Due to the limitations of DICE2 and AJI, it
is clear that there is a need for an improved reliable quantitative measurement.
Panoptic Quality: We propose to use another metric for accurate quantiﬁcation and interpretability to assess the performance of nuclear instance seg-
mentation. Originally proposed by Kirillov et al. , panoptic quality (PQ)
for nuclear instance segmentation is deﬁned as:
Detection Quality(DQ)
(x,y)∈T P IoU(x, y)
Segmentation Quality(SQ)
where x denotes a GT segment, y denotes a prediction segment and IoU denotes
intersection over union. Each (x,y) pair is mathematically proven to be unique
 over the entire set of prediction and GT segments if their
IoU(x,y)>0.5. The unique matching splits all available segments into matched
pairs (TP), unmatched GT segments (FN) and unmatched prediction segments
(FP). From this, PQ can be intuitively analysed as follows: the detection quality (DQ) is the F1 Score that is widely used to evaluate instance detection,
while segmentation quality (SQ) can be interpreted as how close each correctly
detected instance is to their matched GT. DQ and SQ, in a way, also provide
a direct insight into the second and third sub-tasks, deﬁned above.
We believe that PQ should set the standard for measuring the performance of nuclear
instance segmentation methods.
Overall, to fully characterise and understand the performance of each method,
we use the following three metrics: 1) DICE to measure the separation of all
nuclei from the background; 2) Panoptic Quality as a uniﬁed score for comparison and 3) AJI for direct comparison with previous publications3. Panoptic
quality is further broken down into DQ and SQ components for interpretability.
Note, SQ is calculated only within true positive segments and should therefore
be observed together with DQ. Throughout this study, these metrics are calculated for each image and the average of all images are reported as ﬁnal values
for each dataset.
4.2. Nuclear Classiﬁcation Evaluation
Classiﬁcation of the type of each nucleus is performed within the nuclear in-
stances extracted from the instance segmentation or detection tasks. Therefore,
the overall measurement for nuclear type classiﬁcation should also encompass
these two tasks. For all nuclear instances of a particular type t from both the
ground truth and the prediction, the detection task d splits the GT and predicted instances into the following subsets: correctly detected instances (TPd),
misdetected GT instances (FNd) and overdetected predicted instances (FPd).
Subsequently, the classiﬁcation task c further breaks TPd into correctly classi-
ﬁed instances of type t (TPc), correctly classiﬁed instances of types other than
type t (TNc), incorrectly classiﬁed instances of type t (FPc) and incorrectly
classiﬁed instances of types other than type t (FNc). We then deﬁne the Fc
3Evaluation code available at: net/src/metrics
score of each type t for combined nuclear type classiﬁcation and detection as
2(TPc + TNc)
2(TPc + TNc) + α0FPc + α1FNc + α2FPd + α3FNd
where we use α0 = α1 = 2 and α2 = α3 = 1 to give more emphasis to nuclear
type classiﬁcation. Moreover, using the same weighting, if we further extend t
to encompass all types of nuclei T (t ∈T), the classiﬁcation within TPd is then
divided into a correctly classiﬁed set Ac and an incorrectly classiﬁed set Bc. We
can therefore disassemble F t
2(Ac + Bc) + FPd + FNd
2(Ac + Bc)
2(Ac + Bc) + FPd + FNd
= Fd × Classiﬁcation Accuracy within Correctly Detected Instances
where Fd is simply the standard detection quality like DQ while the other term
is the accuracy of nuclear type classiﬁcation within correctly detected instances.
In the case where the GT is not exhaustively annotated for nuclear type clas-
siﬁcation, like in CRCHisto, an amount equal to the number of unlabelled GT
instances in each set is subtracted from Bc and FNc.
Finally, while IoU is utilised as the criteria in DQ for selecting the TP for
detection in instance segmentation, detection methods can not calculate the IoU.
Therefore, to facilitate comparison of both instance segmentation and detection
methods for the nuclear type classiﬁcation tasks, for F t
c, we utilise the notion
of distance to determine whether nuclei have been detected. To be precise, we
deﬁne the region within a predeﬁned radius from the annotated centre of the
nucleus as the ground truth and if a prediction lies within this area, then it is
considered to be a true positive. Here, we are consistent with Sirinukunwattana
et al. and use a radius of 6 pixels at 20× or 12 pixels at 40×.
Summary of the datasets used in our experiments.
UHCW denotes University
Hospitals Conventry and Warwickshire and TCGA denotes The Cancer Genome Atlas. Seg
denotes segmentation masks and Class denotes classiﬁcation labels.
Total Number of Nuclei
Labelled Nuclei
Number of Images
Curie Institute
Magniﬁcation
Size of Images
400×400 to 1000×600
500×500 to 600×600
Number of Cancer Types
Figure 5: Sample cropped regions extracted from each of the ﬁve nuclear instance segmentation
datasets used in our experiments. From left to right: Kumar ; CoNSeP;
CPM-15; CPM-17 and TNBC . The diﬀerent colours of
nuclear contours highlight individual instances.
Malignant/dysplastic
epithelium
Normal epithelium
Inflammatory
Fibroblast
Miscellaneous
Endothelial
Figure 6: Sample cropped regions extracted from the CoNSeP datasets, where the colour of
each nuclear boundary denotes the category.
5. Experimental Results
5.1. Datasets
As part of this work, we introduce a new dataset that we term as the colorectal nuclear segmentation and phenotypes (CoNSeP) dataset4, consisting
of 41 H&E stained image tiles, each of size 1,000×1,000 pixels at 40× objective magniﬁcation. Images were extracted from 16 colorectal adenocarcinoma
(CRA) WSIs, each belonging to an individual patient, and scanned with an
Omnyx VL120 scanner within the department of pathology at University Hospitals Coventry and Warwickshire, UK. We chose to focus on a single cancer
type, so that we are able to display the true variation of tissue within colorectal
adenocarcinoma WSIs, as opposed to other datasets that instead focus on using
a small number of visual ﬁelds from various cancer types. Within this dataset,
stroma, glandular, muscular, collagen, fat and tumour regions can be observed.
Beside incorporating diﬀerent tissue components, the 41 images were also cho-
4This dataset is available at 
sen such that diﬀerent nuclei types were present, including: normal epithelial;
tumour epithelial; inﬂammatory; necrotic; muscle and ﬁbroblast. Here, by type
we are referring to the type of cell from which the nucleus originates from.
Within the dataset, there are many signiﬁcantly overlapping nuclei with indistinct boundaries and there exists various artifacts, such as ink. As a result of the
diversity of the dataset, it is likely that a model trained on CoNSeP will perform
well for unseen CRA cases. For each image tile, every nucleus was annotated by
one of two expert pathologists (A.A, Y-W.T). After full annotation, each annotated sample was reviewed by both of the pathologists; therefore reﬁning their
own and each others’ annotations. By the end of the annotation process, each
pathologist had fully checked every sample and consensus had been reached.
Annotating the data in this way ensured that minimal nuclei were missed in the
annotation process. However, we can not avoid inevitable pixel-level diﬀerences
between the annotation and the true nuclear boundary in challenging cases.
In addition to delineating the nuclear boundaries, every nucleus was labelled
as either: normal epithelial, malignant/dysplastic epithelial, ﬁbroblast, muscle,
inﬂammatory, endothelial or miscellaneous. Within the miscellaneous category,
necrotic, mitotic and cells that couldn’t be categorised were grouped. For our
experiments, we grouped the normal and malignant/dysplastic epithelial nuclei
into a single class and we grouped the ﬁbroblast, muscle and endothelial nuclei
into a class named spindle-shaped nuclei.
Overall, six independent datasets are utilised for this study. A full summary
for each of them is provided in Table 2.
Five of these datasets are used to
evaluate the instance segmentation performance which we refer to as: CoNSeP;
Kumar ; CPM-15; CPM-17 and TNBC
 . Example images from each of the ﬁve datasets can be
seen in Fig. 7. Meanwhile, we utilise CoNSeP and a further dataset, named
CRCHisto, to quantify the performance of the nuclear classiﬁcation model. The
CRCHisto dataset consists of the same nuclei types that are present in CoNSeP.
It is also worth noting that the CRCHisto dataset is not exhaustively annotated
for nuclear class labels.
5.2. Implementation and Training Details
We implemented our framework with the open source software library TensorFlow version 1.8.0 on a workstation equipped with two
NVIDIA GeForce 1080 Ti GPUs. During training, data augmentation including
ﬂip, rotation, Gaussian blur and median blur was applied to all methods. All
networks received an input patch with a size ranging from 252×252 to 270×270.
This size diﬀerence is due to the use of valid convolutions in some architectures,
such as HoVer-Net and U-Net. Regarding HoVer-Net, we initialised the model
with pre-trained weights on the ImageNet dataset , trained
only the decoders for the ﬁrst 50 epochs, and then ﬁne-tuned all layers for another 50 epochs. We train stage one for around 120 minutes and stage two for
around 260 minutes. Therefore, the overall training time is around 380 minutes. Stage two takes longer to train because unfreezing the encoder utilises
more memory and therefore a smaller batch size needs to be used. Speciﬁcally,
we used a batch size of 8 and 4 on each GPU for stage one and two respectively. We used Adam optimisation with an initial learning rate of 10−4 and
then reduced it to a rate of 10−5 after 25 epochs. This strategy was repeated
for ﬁne-tuning. On the whole, training of the network is stable, where the usage
of fully independent decoders helps the network to converge each time. The
network was trained with an RGB input, normalised between 0 and 1.
5.3. Comparative Analysis of Segmentation Methods
Experimental Setting: We evaluated our approach by employing a full
independent comparison across the three largest known exhaustively labelled
nuclear segmentation datasets: Kumar; CoNSeP and CPM-17 and utilised the
metrics as described in Section 4.1. For this experiment, because we do not
have the classiﬁcation labels for all datasets, we perform instance segmentation
without classiﬁcation. This enables us to fully leverage all data and allows us
to rigorously evaluate the segmentation capability of our model. In the same
way as Kumar et al. , we split the Kumar dataset into two diﬀerent sub-
datasets: (i) Kumar-Train, a training set with 16 image tiles , CoNSeP and CPM-17
 datasets. WS denotes watershed-based post processing.
Cell Proﬁler 
QuPath 
FCN8 
FCN8 + WS 
SegNet 
SegNet + WS 
U-Net 
Mask-RCNN 
DCAN 
Micro-Net 
DIST 
CNN3 
CIA-Net 
0.620 0.754
DRAN 
0.618 0.770 0.773 0.597 0.853 0.571 0.702 0.778 0.547 0.869 0.705 0.854 0.814 0.697
4 kidney and 4 prostate) and (ii) Kumar-Test, a test set with 14 image tiles (2
breast, 2 liver, 2 kidney and 2 prostate, 2 bladder, 2 colon, 2 stomach). Note,
we utilise the exact same image split used by other recent approaches , but we do not separate the
test set into two subsets. We do this to ensure that the test set is large enough,
ensuring a reliable evaluation. For CoNSeP, we devise a suitable train and test
set that contains 26 and 14 images respectively. The images within the test set
were selected to ensure the true diversity of nuclei types within colorectal tissue
are represented. For CPM-17, we utilise the same split that had been employed
for the challenge, with 32 images in both the training and test datasets.
We compared our proposed model to recent segmentation approaches used in
computer vision ,
medical imaging and also to methods speciﬁcally
tuned for the task of nuclear segmentation .
We also compared
the performance of our model to two open source software applications: Cell
Proﬁler and QuPath .
Proﬁler is a software for cell-based analysis, with several suggested pipelines for
Ground Truth
Ground Truth
Ground Truth
Figure 7: Example visual results on the CPM-17, Kumar and CoNSeP datasets. For each
dataset, we display the 4 models that achieve the highest PQ score from left to right. The
diﬀerent colours of the nuclear boundaries denote separate instances.
computational pathology. The pipeline that we adopted applies a threshold to
the greyscale image and then uses a series of post processing operations. QuPath
is an open source software for digital pathology and whole slide image analysis.
To achieve nuclear segmentation, we used the default parameters within the
application. FCN, SegNet, U-Net, DCAN, Mask-RCNN and DIST have been
implemented by the authors of the paper (S.G, Q.D.V). For Mask-RCNN, we
slightly modiﬁed the original implementation by using smaller anchor boxes.
The default conﬁguration is ﬁne-tuned for natural images and therefore, this
modiﬁcation was necessary to perform a successful nuclear segmentation. DIST
was implemented with the assistance of the ﬁrst author of the corresponding
approach in order to ensure reliability during evaluation.
This also enabled
us to utilise DIST for further comparison in our experiments. For Micro-Net,
we used the same implementation that was described by Raza et al. 
and was implemented by the ﬁrst author of the corresponding paper (S.E.A.R).
For CNN3 and CIA-Net, we report the results on the Kumar dataset that are
given in their respective original papers. The authors of CIA-Net and DRAN
provided their segmentation output, which meant that we were able to obtain
all metrics on the datasets that the models were applied to.
Therefore, we
report results of CIA-Net on the Kumar dataset and results of DRAN on the
CPM-17 dataset. Note, for all self-implemented approaches we are consistent
with our pre-processing strategy. However, DRAN, CNN3 and CIA-Net results
are directly taken from their respective papers and therefore we can’t guarantee
the same pre-processing steps. CNN3 and CIA-Net also use stain normalisation,
whereas other methods described in this paper do not.
Comparative Results: Table 3 and the box plots in Fig. 8a and 8b show
detailed results of this experiment.
Within the box plots, we choose not to
show AJI, due to its limitations as discussed in Section 4.1. A large variation in
performance between methods within each dataset is observed. This variation
is particularly evident in the Kumar and CoNSeP datasets, where there exists a
large number of overlapping nuclei. Both Cell Proﬁler 
and QuPath achieve sub-optimal performance for all
datasets. In particular, both software applications consistently achieve a low
DICE score, suggesting that their inability to distinguish nuclear pixels from
(b) CoNSeP
Figure 8: Box plots highlighting the performance of competing methods on the Kumar and
CoNSeP datasets.
the background is a major limiting factor. FCN-based approaches improve the
capability of models to detect nuclear pixels, yet often fail due to their inability
to separate clustered instances. For example, despite a higher DICE score than
Cell Proﬁler and QuPath, networks built only for semantic segmentation like
FCN8 and SegNet suﬀer from low PQ values. Therefore, methods that incorporate strong instance-aware techniques are favourable. Within CPM-17, there
are less overlapping nuclei which explains why methods that are not instanceaware are still able to achieve a satisfactory performance.
We observe that
the weighted cross entropy loss that is used in both U-Net and Micro-Net can
help to separate joined nuclei, but its success also depends on the capacity of
the network. This is reﬂected by the increased performance of Micro-Net over
DCAN is able to better distinguish between separate instances than FCN8,
which uses a very similar encoder based on the VGG16 network. Therefore,
incorporating additional information at the output of the network can improve
the segmentation performance.
This is also exempliﬁed by the fairly strong
performances of CNN3, DIST, DRAN and CIA-Net. In a diﬀerent way, Mask-
RCNN is able to successfully separate clustered nuclei by utilising a region
proposal based approach.
However, Mask-RCNN is less eﬀective than other
methods at detecting nuclear pixels, which is reﬂected by a lower DICE score.
Due to the reasoning given in Section 4, we place a larger emphasis on PQ to
determine the success of diﬀerent models. In particular, we consistently obtain
an improved performance over DIST, which justiﬁes the use of our proposed
horizontal and vertical maps as a regression target. We also report a better
performance than the winners of the Computational Precision Medicine and
MoNuSeg challenges , that utlised the CPM-
17 and Kumar datasets respectively. Therefore, HoVer-Net achieves state-of-the
art performance for nuclear instance segmentation compared to all competing
methods on multiple datasets that consist of a variety of diﬀerent tissue types.
Our approach also outperforms methods that were ﬁne-tuned for the task of
nuclear segmentation.
5.4. Generalisation Study
Experimental Setting: The goal of any automated method is to perform
well on unseen data, with high accuracy. Therefore, we conducted a large scale
study to assess how all methods generalise to new H&E stained images. To analyse the generalisation capability, we assessed the ability to segment nuclei from:
i) new organs (variation in nuclei shapes) and ii) diﬀerent centres (variation in
staining).
The ﬁve instance segmentation datasets used within our experiments can be
grouped into three groups according to their origin: TCGA (Kumar, CPM-15,
CPM-17), TNBC and CoNSeP. We used Kumar as the training and validation
set, due to its size and diversity, whilst the combined CPM (CPM-15 and CPM-
17), TNBC and CoNSeP datsets were used as three independent test sets. We
split the test sets in this way in accordance with their origin. Note, for this
experiment we use both the training and test sets of CPM-17 and CoNSeP to
form the independent test sets. Kumar was split into three subsets, as explained
in Section 5.1, and Kumar-Train was used to train all models, i.e. trained with
samples originating from the following organs: breast; prostate; kidney and
Despite all samples being extracted from TCGA, CPM samples come
from the brain, head & neck and lungs regions. Therefore, testing with CPM
reﬂects the ability for the model to generalise to new organs, as mentioned above
by the ﬁrst generalisation criterion. TNBC contains samples from an already
seen organ (breast), but the data is extracted from an independent source with
diﬀerent specimen preservation and staining practice. Therefore, this reﬂects the
second generalisation criterion. CoNSeP contains samples taken from colorectal
tissue, which is not represented in Kumar-Train, and is also extracted from a
source independent to TCGA. Therefore, this reﬂects both the ﬁrst and second
generalisation criteria.
Also, as mentioned in Section 5.1, CoNSeP contains
challenging samples, where there exists various artifacts and there is variation
in the quality of slide preparation. Therefore, the performance on this dataset
also reﬂects the ability of a model to generalise to diﬃcult samples.
Comparative Results: The results are reported in Table 4, where we only
Table 4: Comparative results, highlighting the generalisation capability of diﬀerent models.
All models are initially trained on Kumar and then the Combined CPM ,
TNBC and CoNSeP datasets are processed.
Combined CPM
All CoNSeP
FCN8 + WS 
SegNet + WS 
U-Net 
Mask-RCNN 
DCAN 
Micro-Net 
DIST 
0.626 0.774 0.778 0.606
0.590 0.743 0.759 0.578
0.404 0.529 0.764 0.408
display the results of methods that employ an instance-based technique. We
observe that our proposed model is able to successfully generalise to unseen
data in all three cases. However, some methods prove to perform poorly with
unseen data, where in particular, U-Net and DIST perform worse than other
competing methods on all three datasets.
Both SegNet with watershed and
Mask-RCNN achieve a competitive performance across all three generalisation
tests. However, similar to the results reported in Table 3, Mask-RCNN is not
able to distinguish nuclear pixels from the background as well as other competing
methods, which has an adverse eﬀect on the overall segmentation performance
shown by PQ. On the other hand, SegNet proves to successfully detect nuclear
pixels, reporting a greater DICE score than HoVer-Net on both the TNBC and
CoNSeP datasets. However, the overall segmentation result for HoVer-Net is
superior because it is better able to separate nuclear instances by incorporating
the horizontal and vertical maps at the output of the network.
5.5. Comparative Analysis of Classiﬁcation Methods
Experimental Setting: We converted the top four performing nuclear in-
stance segmentation algorithms, based on their panoptic quality on the CoNSeP
dataset, such that they were able to perform simultaneous instance segmentation
and classiﬁcation. As mentioned in Section 5.1, the nuclear categories that we
use in our experiments are: miscellaneous, inﬂammatory, epithelial and spindle-
shaped. Speciﬁcally, we compared HoVer-Net with Micro-Net, Mask-RCNN and
DIST. For Micro-Net, we used an output depth of 5 rather than 2, where each
channel gave the probability of a pixel being either background, miscellaneous,
inﬂammatory, epithelial or spindle-shaped. For Mask-RCNN, there is a devoted
classiﬁcation branch that predicts the class of each instance and therefore is
well suited to a multi-class setting. DIST performs regression at the output of
the network and therefore converting the model such that it is able to classify
nuclei into multiple categories is non-trivial. Instead, we add an extra 1×1 convolution at the output of the network that performs nuclear classiﬁcation. As
well as comparing to the aforementioned methods, we compared our approach
to a spatially constrained CNN (SC-CNN), that achieves detection and classi-
ﬁcation. Note, because SC-CNN does not produce a segmentation mask, we do
not report the PQ for this method.
Comparative Results: We trained our models on the training set of the
CoNSeP dataset and then we evaluated the model on both the test set of CoN-
SeP and also the entire CRCHisto dataset. Table 5 displays the results of the
multi-class models on the CoNSeP and the CRCHisto datasets respectively,
where the given metrics are described in Section 4.2. For CoNSeP, along with
the classiﬁcation metrics, we provide PQ as an indication of the quality of instance segmentation.
However, in CRCHisto, only the nuclear centroids are
given and therefore, we exclude PQ from the CRCHisto evaluation because it
can’t be calculated without the instance segmentation masks. We observe that
HoVer-Net achieves a good quality simultaneous instance segmentation and classiﬁcation, compared to competing methods. It must be noted, that we should
expect a lower F1 score for the miscellaneous class because there are signiﬁcantly less nuclei represented. Also, there is a high diversity of nuclei types
that have been grouped within this class, belonging to: mitotic; necrotic and
cells that are uncategorisable. Despite this, HoVer-Net is able to achieve a satisfactory performance on this class, where other methods fail.
Furthermore,
compared to other methods, our approach achieves the best F1 score for epithelial, inﬂammatory and spindle classes. Therefore, due to HoVer-Net obtaining a
strong performance for both nuclear segmentation and classiﬁcation, we suggest
that our model may be used for sophisticated subsequent cell-level downstream
analysis in computational pathology.
Table 5: Comparative results for nuclear classiﬁcation on the CoNSeP and CRCHisto datasets.
Fd denotes the F1 score for nuclear detection, whereas Fe
denote the F1
classiﬁcation score for the epithelial, inﬂammatory, spindle-shaped and miscellaneous classes
respectively.
SC-CNN 
DIST 
Micro-Net 
Mask-RCNN 
0.639 0.503 0.537
0.516 0.748 0.635 0.631 0.566 0.426 0.688 0.486 0.573 0.302 0.178
6. Discussion and Conclusions
Analysis of nuclei in large-scale histopathology images is an important step
towards automated downstream analysis for diagnosis and prognosis of cancer.
Nuclear features have been often used to assess the degree of malignancy . However, visual analysis of nuclei is a very time consuming task
because there are often tens of thousands of nuclei within a given whole-slide
image (WSI). Performing simultaneous nuclear instance segmentation and clas-
siﬁcation enables subsequent exploration of the role that nuclear features play
in predicting clinical outcome. For example, Lu et al. utilised nuclear
features from histology TMA cores to predict survival in early-stage estrogen
receptor-positive breast cancer. Restricting the analysis to some speciﬁc nuclear types only may be advantageous for accurate analysis in computational
pathology.
In this paper, we have proposed HoVer-Net for simultaneous segmentation
and classiﬁcation of nuclei within multi-tissue histology images that not only
detects nuclei with high accuracy, but also eﬀectively separates clustered nuclei. Our approach has three up-sampling branches: 1) the nuclear pixel branch
that separates nuclear pixels from the background; 2) the HoVer branch that
regresses the horizontal and vertical distances of nuclear pixels to their centres
of mass and 3) the nuclear classiﬁcation branch that determines the type of each
nucleus. We have shown that the proposed approach achieves the state-of-theart instance segmentation performance compared to a large number of recently
published deep learning models across multiple datasets, including tissues that
have been prepared and stained under diﬀerent conditions.
This makes the
proposed approach likely to translate well to a practical setting due its strong
generalisation capacity, which can therefore be eﬀectively used as a prerequisite step before nuclear-based feature extraction. We have shown that utilising
the horizontal and vertical distances of nuclear pixels to their centres of mass
provides powerful instance-rich information, leading to state-of-the-art performance in histological nuclear segmentation. When the classiﬁcation labels are
available, we show that our model is able to successfully segment and classify
nuclei with high accuracy.
Region proposal (RP) methods, such as Mask-RCNN, show great potential
in dealing with overlapping instances because there is no notion of separating
instances; instead nuclei are segmented independently. However, a major limitation of the RP methods is the diﬃculty in merging instance predictions between
neigbouring tiles during processing. For example, if a sub-segment of a nucleus
at the boundary is assigned a label, one must ensure that the remainder of the
nucleus in the neighbouring tile is also assigned the same label. To overcome
this diﬃculty, for Mask-RCNN, we utilised an overlapping tile mechanism such
that we only considered non-boundary nuclei.
Regarding the processing time, the average time to process a 1,000×1,000
image tile over 10 runs using Mask-RCNN for segmentation and classiﬁcation
was 106.98 seconds. Meanwhile, HoVer-Net only took an average of 11.04 seconds to complete the same operation; approximately 9.7× faster. On the other
hand, the average processing time for DIST and Micro-Net was 0.600 and 0.832
seconds respectively. Mask-RCNN inherently stores a single instance per chan-
nel, which leads to very large arrays in memory when there are many nuclei in a
single image patch, which also contributes to the much longer processing time as
seen above. Overall, FCN methods seem to better translate to WSI processing
compared to Mask-RCNN or RPN methods in general. It must be stressed that
the timing is not exact and is dependent on hardware speciﬁcations and software
implementation. With optimised code and sophisticated hardware, we expect
these timings to be considerably diﬀerent. Additionally, the inference time is
also dependent on the size of the output. In particular, with a smaller output
size, a smaller stride is also required during processing. For instance, if we used
padded convolution in the up-sampling branches of HoVer-Net, then we observe
5.6× speed up and the average processing time is 1.97 seconds per 1000×1000
image tile. For fair comparison, all models were processed on a single GPU
with 12GB RAM and we ﬁxed the batch size to a size of one. Future work will
explore the trade-oﬀbetween the eﬃciency of HoVer-Net and its potential to
accurately perform instance segmentation and classiﬁcation.
A major bottleneck for the development of successful nuclear segmentation
algorithms is the limitation of data; particularly with additional associated class
labels. In this work, we introduce the colorectal adenocarcinoma nuclear segmentation and phenotypes (CoNSeP) dataset, containing over 24K labelled nuclei from challenging samples to reﬂect the true diﬃculty of segmenting nuclei in
whole-slide images. Due to the abundance of nuclei with an associated nuclear
category, CoNSeP aims to help accelerate the development of further simultaneous nuclear instance segmentation and classiﬁcation models to further increase
the sophistication of cell-level analysis within computational pathology.
We analysed the common measurements used to assess the true performance
of nuclear segmentation models and discussed their limitations.
Due to the
fact that these measurements did not always reﬂect the instance segmentation
performance, we proposed a set of reliable and informative statistical measures.
We encourage researchers to utilise the proposed measures to not only maximise
the interpretability of their results, but also to perform a fair comparison with
other methods.
Finally, methods have surfaced recently that explore the relationship of various nuclear types within histology images , yet these methods are limited to spatial analysis because the segmentation masks are not available. Utilising our model for nuclear segmentation
and classiﬁcation enables the exploration of the spatial relationship between various nuclear types combined with nuclear morphological features and therefore
may provide additional diagnostic and prognostic value. Currently, our model is
trained on a single tissue type, yet due to the strong performance of our instance
segmentation model across multiple tissues, we are conﬁdent that our model will
perform well if we were to incorporate additional tissue types. We observe a low
F1 classiﬁcation score for the miscellaneous category in the classiﬁcation model
because there are signiﬁcantly less samples within this category and there exists
high intra-class variability. Future work will involve obtaining more samples
within this category, including necrotic and mitotic nuclei, to improve the class
balance of the data.
Acknowledgments
This work was supported by the National Research Foundation of Korea
(NRF) grant funded by the Korea government (MSIP) (No. 2016R1C1B2012433)
and by the Ministry of Science and ICT (MSIT) (No. 2018K1A3A1A74065728).
We also acknowledge the ﬁnancial support from EPSRC and MRC, provided as
part of the Mathematics for Real-World Systems CDT. We thank Peter Naylor
for his assistance in the implementation of the DIST network.
Appendix A. Ablation Studies
To gain a full understanding of the contribution of our method, we investi-
gated several of its components. Speciﬁcally, we performed the following ablation experiments: (i) contribution of the proposed loss strategy; (ii) Sobel-based
post processing technique compared to other strategies and (iii) contribution of
the dedicated classiﬁcation branch. Here, we utilised the Kumar and CoNSeP
datasets for (i) and (ii) due to the large number of nuclei present, whereas for
(iii) we use CoNSeP and CRCHisto because we do not have the classiﬁcation
labels for Kumar.
Loss Terms: We conducted an experiment to understand the contribution
of our proposed loss strategy. First, we used mean squared error (MSE) of the
horizontal and vertical distances La as the loss function of the HoVer branch and
binary cross entropy (BCE) loss Lc as the loss function for the NP branch. We
refer to this combination as the standard strategy because MSE and BCE are the
two most commonly used loss functions for regression and binary classiﬁcation
tasks respectively. Next, we introduced the MSE of the horizontal and vertical
gradients Lb to the HoVer branch and the dice loss Ld to the NP branch. The
intuition behind our novel Lb is that it enforces the correct structure of the
horizontal and vertical map predictions and therefore helps to correctly separate
neighbouring instances. The dice loss was introduced because it can help the
network to better distinguish between background and nuclear pixels and is
particularly useful when there is a class-imbalance. We present the results in
Table A1, where we observe an increase in all performance measures for our
proposed multi-term loss strategy. Therefore, the additional loss terms boost
the network’s ability to diﬀerentiate between nuclear and background pixels
(DICE) and separate individual nuclei (DQ and PQ). In particular, there is a
signiﬁcant boost in the SQ for both Kumar and CoNSeP, which suggests that
our proposed loss function Lb is necessary to precisely determine where nuclei
should be split.
Post Processing: Usually, markers obtained from applying a threshold to
an energy landscape (such as the distance map) is enough to provide a competitive input for watershed, as seen by DIST in Table 3. Although HoVer-Net is
not directly built upon an energy landscape, we devised a Sobel-based method
to derive both the energy landscape and the markers. To compare with other
methods, we implemented two further techniques for obtaining the energy landscape and the markers. We then exhaustively compared all energy landscape
and marker combinations to assess which post processing strategy is the best.
We start by linking HoVer to the distance map by calculating the square sum
χ2 + ϕ2, which can be seen as the distance from a pixel to its nearest nuclear
centroid. In other words, this is a pseudo distance map. Additionally, χ and ϕ
values can be interpreted as Cartesian coordinates with each nuclear centroid as
the origin. By thresholding the values between a certain range, we can obtain
the markers. The results of all combinations are shown in Table A2. Note, our
gradient-based post processing technique is speciﬁcally designed for the HoVer
branch output.
Classiﬁcation Branch: In order to assess the importance of a devoted
branch for concurrent nuclear segmentation and classiﬁcation, we compared the
proposed three branch setup of HoVer-Net to a two branch setup. Here, the
two branch setup extends the NP branch to a multi-class setting, by predicting
each nuclear type at the output. Then, to obtain the binary mask, the positive
channels are combined together after nuclear type prediction. Utilising three
branches decouples the tasks of nuclear classiﬁcation and nuclear detection,
where a separate branch is devoted to each task. For this ablation study, we
train on the CoNSeP training set and then process both the CoNSeP test set
and the entire CRCHisto dataset.
We report results in Table A3, where we observe that utilising a separate
branch devoted to the task of nuclear classiﬁcation leads to an improved overall
performance of simultaneous nuclear instance segmentation and classiﬁcation in
both the CoNSeP and CRCHisto datasets. We can see that if the classiﬁcation
takes place at the output of NP branch, then the network’s ability to determine
the nuclear type is compromised. This is because the task of nuclear classiﬁcation is challenging and therefore the network beneﬁts from the introduction of
a branch dedicated to the task of classiﬁcation.
Table A1: Ablation study highlighting the contribution of the proposed loss strategy.
Standard Loss
Proposed Loss
Table A2: Ablation study for post processing techniques: Sobel-based versus thresholding to
get markers and Sobel-based versus naive conversion to get energy landscape
0.618 0.770
0.773 0.597
0.571 0.702 0.778
Table A3: Ablation study showing the contribution of the classiﬁcation branch in HoVer-Net
on the CoNSeP dataset. Fd denotes the F1 score for nuclear detection, whereas Fe
c denote the F1 classiﬁcation score for the epithelial, inﬂammatory, spindle-shaped and
miscellaneous classes respectively.
NP & HoVer
NP & HoVer & NC