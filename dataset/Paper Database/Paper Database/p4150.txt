Discovering Phase Transitions with Unsupervised Learning
Beijing National Lab for Condensed Matter Physics and Institute of Physics,
Chinese Academy of Sciences, Beijing 100190, China
Unsupervised learning is a discipline of machine learning which aims at discovering patterns in big
data sets or classifying the data into several categories without being trained explicitly. We show
that unsupervised learning techniques can be readily used to identify phases and phases transitions
of many body systems. Starting with raw spin conﬁgurations of a prototypical Ising model, we use
principal component analysis to extract relevant low dimensional representations the original data
and use clustering analysis to identify distinct phases in the feature space. This approach successfully
ﬁnds out physical concepts such as order parameter and structure factor to be indicators of the phase
transition. We discuss future prospects of discovering more complex phases and phase transitions
using unsupervised learning techniques.
Classifying phases of matter and identifying phase
transitions between them is one of the central topics of
condensed matter physics research. Despite an astronomical number of constituting particles, it often suﬃces to
represent states of a many-body system with only a few
variables. For example, a conventional approach in condensed matter physics is to identify order parameters via
symmetry consideration or analyzing low energy collective degree of freedoms and use them to label phases of
matter .
However, it is harder to identify phases and phase transitions in this way in an increasing number of new states
of matter, where the order parameter may only be deﬁned
in an elusive nonlocal way . These new developments
call for new ways of identifying appropriate indicators of
phase transitions.
To meet this challenge, we use machine learning techniques to extract information of phases and phase transitions directly from many-body conﬁgurations. In fact,
application of machine learning techniques to condensed
matter physics is a burgeoning ﬁeld . For example, regression approaches are used to predict crystal
structures , to approximate density functionals , and
to solve quantum impurity problems ; artiﬁcial neural
networks are trained to classify phases of classical statistical models . However, most of those applications
use supervised learning techniques (regression and classiﬁcation), where a learner needs to be trained with the
previously solved data set (input/output pairs) before it
can be used to make predictions.
On the other hand, in the unsupervised learning, there
is no such explicit training phase. The learner should by
itself ﬁnd out interesting patterns in the input data. Typical unsupervised learning tasks include cluster analysis
and feature extraction. Cluster analysis divides the input
data into several groups based on certain measures of similarities. Feature extraction ﬁnds a low-dimensional representation of the dataset while still preserving essential
characteristics of the original data. Unsupervised learning methods have broad applications in data compression, visualization, online advertising and recommender
system, etc. They are often being used as a preprocessor
of supervised learning to simplify the training procedure.
In many cases, unsupervised learning also lead to better
human interpretations of complex datasets.
In this paper, we explore the application of unsupervised learning in many-body physics with a focus on
phase transitions. The advantage of unsupervised learning is that one assumes neither the presence of the phase
transition nor the precise location of the critical point.
Dimension reduction techniques can extract salient features such as order parameter and structure factor from
the raw conﬁguration data. Clustering analysis can then
divide the data into several groups in the low-dimensional
feature space, representing diﬀerent phases. Our studies
show that unsupervised learning techniques have great
potentials of addressing the big data challenge in the
many-body physics and making scientiﬁc discoveries.
As an example, we consider the prototypical classical
Ising model
where the spins take two values σi = {−1, +1}.
consider the model (1) on a square lattice with periodic
boundary conditions and set J = 1 as the energy unit.
The system undergoes a phase transition at temperature
T/J = 2/ln(1 +
2) ≈2.269 . A discrete Z2 spin
inversion symmetry is broken in the ferromagnetic phase
below Tc and is restored in the disordered phase at temperatures above Tc.
We generate 100 uncorrelated spin conﬁguration samples using Monte Carlo simulation at temperatures
T/J = 1.6, 1.7, . . . , 2.9 each and collect them into an
M × N matrix
↑↓↑. . . ↑↑↑
↓↑↓. . . ↑↓↑
where M = 1400 is the total number of samples, and N
is the number of lattice sites. The up and down arrows
 
Figure 1: The ﬁrst few explained variance ratios obtained
from the raw Ising conﬁgurations.
The inset shows the
weights of the ﬁrst principal component on an N = 402 square
in the matrix denote σi = ±1. Such a matrix is the only
data we feed to the unsupervised learning algorithm.
Our goal is to discover possible phase transition of the
model (1) without assuming its existence. This is diﬀerent from the supervised learning task, where exact knowledge of Tc was used to train a learner . Moreover,
the following analysis does not assume any prior knowledge about the lattice geometry and the Hamiltonian.
We are going to use the unsupervised learning approach
to extract salient features in the data and then use this
information to cluster the samples into distinct phases.
Knowledge about the temperature of each sample and
the critical temperature Tc of the Ising model is used to
verify the clustering.
Interpreting each row of X as a coordinate of an Ndimensional space, the M data points form a cloud centered around the origin of a hypercube . Discovering
a phase transition amounts to ﬁnd a hypersurface which
divides the data points into several groups, each representing a phase. The task is akin to the standard unsupervised learning technique: cluster analysis , where
numerous algorithms are available, and they group the
data based on diﬀerent criteria.
However, direct applying clustering algorithms to the
Ising conﬁgurations may not be very enlightening. The
reasons are twofold. First, even if one manages to separate the data into several groups, clusters in high dimensional space may not directly oﬀer useful physical
insights. Second, many clustering algorithms rely on a
good measure of similarity between the data points. Its
deﬁnition is, however, ambiguous without supplying of
domain knowledge such as the distance between two spin
conﬁgurations.
On the other hand, the raw spin conﬁguration is a
Figure 2: Projection of the samples onto the plane of the
leading two principal components. The color bar on the right
indicates the temperature T/J of the samples. The panels
(a-c) are for N = 202, 402 and 802 sites respectively.
highly redundant description of the system’s state because there are correlations among the spins. Moreover,
as the temperature varies, there is an overall tendency
in the raw spin conﬁgurations, such as lowering the total
magnetization. In the following, we will try to ﬁrst identify some crucial features in the raw data. They provide
an eﬀective low dimensional representation of the original
data. And in terms of these features, the meaning of the
distance between conﬁgurations becomes more transparent. The separation of phases is also often clearly visible
and comprehensible by the human in the reduced space
spanned by these features. Therefore, feature extraction
does not only simpliﬁes the subsequent clustering analysis but also provides eﬀective means of visualizing and
oﬀering physical insights. We denote the crucial features
extracted by the unsupervised learning as indicators of
the phase transition. In general, they do not necessarily
need to be the same as the conventional order parameters deﬁned in condensed matter physics. This unsupervised learning approach nevertheless provides an alternative view of phases and phase transitions.
Principal component analysis (PCA) is a widely
used feature extraction technique. The principal components are mutually orthogonal directions along which the
variances of the data decrease monotonically. PCA ﬁnds
the principal components through a linearly transformation of the original coordinates Y = XW. When applied
to the Ising conﬁgurations in Eq. (2), PCA ﬁnds the most
signiﬁcant variations of the data changing with the temperature. We interpret them as relevant features in the
data and use them as indicators of the phase transition
if there is any.
We write the orthogonal transformation into column
vectors W = (w1, w2, . . . , wN) and denote wℓas weights
Figure 3: Typical conﬁgurations of the COP Ising model at
below (a,b) and above (c) the critical temperature. Red and
blue pixels indicate up and down spins. There are exactly half
of the pixels are red/blue due to the constraint P
of the principal components in the conﬁguration space.
They are determined by an eigenproblem 
XT Xwℓ= λℓwℓ.
The eigenvalues are nonnegative real numbers sorted in
a descending order λ1 ≥λ2 . . . ≥λN ≥0. Using the
terminology of PCA, we denote the normalized eigenvalues ˜λℓ= λℓ/ PN
ℓ=1 λℓas explained variance ratio. When
keeping only the ﬁrst few principal components, PCA is
an eﬃcient dimension reduction approach which captures
most variations of the original data. Moreover, PCA also
yields an optimal approximation of the data in the sense
of minimizing the squared reconstruction error .
Figure 1 shows the ﬁrst few explained variance ratios
for various system sizes. Notably, there is only one dominant principal component. As the temperature changes
the Ising conﬁgurations vary most signiﬁcantly along the
ﬁrst principal component, whose weight is shown in the
inset of Fig. 1. The ﬂat distribution all over the lattice
sites means the transformation actually gives the uniform
magnetization
i σi. In this sense, PCA has identi-
ﬁed the order parameter of the Ising model (1) upon a
phase transition.
Next, we project the samples in the space spanned
by the ﬁrst two principal components, shown in Figure 2. The color of each sample indicates its temperature.
The projected coordinates are given by the matrix-vector
The variation of the data along the ﬁrst principal axis y1
is indeed much stronger than that along the second principal axis y2. Most importantly, one clearly observes that
as the system size enlarges the samples tend to split into
three clusters. The high-temperature samples lie around
the origin while the low-temperature samples lie symmetrically at ﬁnite y1. The samples at the critical temperature (light yellow dots) have broad spread because of
large critical ﬂuctuations. We note that Ref. presents
a diﬀerent low dimension visualization of the Ising conﬁgurations using stochastic neighbor embedding technique.
Figure 4: Explained variance ratios of the COP Ising model.
Insets show the weights corresponding to the four leading
principal components.
When folding the horizontal axis of Fig. 2 into P
i σi)2 the two clusters associated with the lowtemperature phase merge together. With such a linear
separable low dimensional representation of the original
data, a cluster analysis can easily divide the samples
into two phases, thus identifying the phase transition.
Notice that our unsupervised learning analysis does not
only ﬁnds the phase transition and an estimate of the
critical temperature but also provides insight into the
order parameter.
Having established the baseline of applying the unsupervised learning techniques in the prototypical Ising
model, we now turn to a more challenging case where
the learner can make nontrivial ﬁndings.
For this, we
consider the same Ising model Eq. (1) with a conserved
order parameter (COP) P
i σi ≡0. This model describes
classical lattice gasses , where the occupation of each
lattice site can be either one or zero and the particles
interact via a short-range attraction. The conserved total magnetization corresponds to the constraint of a half
ﬁlled lattice.
On a square lattice with periodic boundary conditions,
the spins tend to form two domains at low-temperatures
shown in Fig. 3(a,b). The two domain walls wrap around
the lattice either horizontally or vertically to minimize
the domain wall energy . Besides, the domains can
also shift in space due to translational invariance.
the temperature increases, these domain walls melt and
the system restores both the translational and rotational symmetries in the high-temperature phase shown
in Fig. 3(c). At zero total magnetization, the critical temperature of such solid-gas phase transition is the same as
the Ising transition Tc/J ≈2.269 . However, since
the total magnetization is conserved, simply summing up
the Ising spins can not be used as an indicator to distinguish the two phases. In fact, it is unclear to the author
which quantity signiﬁes the phase transition before this
Figure 5: Projections of the COP Ising samples to the four
leading principal components.
study. It is, therefore, a good example to demonstrate
the ability of the unsupervised learning approach.
We perform the same PCA on the COP Ising conﬁgurations sampled with Monte Carlo simulation and
show the ﬁrst few explained variance ratios in Fig. 4.
Notably, there are four instead of one leading principal components. Their weights plotted in the insets of
Fig. 4 show notable nonuniformity over the lattice sites.
This indicates that in the COP Ising model the spatial
distribution of the spins varies drastically as the temperature changes. Denote Euclidean coordinate of site
i as (µi, νi), where µi, νi = 1, 2, . . . ,
N. The weights
of the four leading principal components can be written as cos(θi), cos(φi), sin(θi), sin(φi), where (θi, φi) =
(µi, νi)×2π/
N . Note these four mutually orthogonal weights correspond to the two orientations of the domain walls shown in Fig. 3(a,b). Therefore, the PCA correctly ﬁnds out the rotational symmetry breaking caused
by the domain wall formation.
To visualize the samples in the four-dimensional feature space spanned by the ﬁrst few principal components, we plot two-dimensional projections in Fig. 5. In
all cases, the high-temperature samples are around the
origin while the low-temperature samples form a surrounding cloud. Motivated by the circular shapes of all
these projections, we further reduce to a two-dimensional
space via a nonlinear transformation (y1, y2, y3, y4) 7→
As shown in Fig. 6(a), the line
ℓ= const (a four dimensional sphere of a constant
radius) separates the low and high temperature samples.
This motivates a further dimension reduction to a single
variable P4
ℓas an indicator of the phase transition
in the COP Ising model.
Substituting weights of the four principal components
cos(θi), cos(φi), sin(θi), sin(φi), the sum P4
Figure 6: (a) Further projection of the COP Ising samples to
a two-dimensional space. (b) The structure factor Eq. (5) of
the COP Ising model versus temperature for various system
portional to
σiσj [cos (θi −θj) + cos (φi −φj)] .
Even though such structure factor was unknown to the
author before it was discovered by the learner, one can
convince himself it indeed captures the domain wall formation at low temperatures shown in Fig. 3(a,b). Figure 6(b) shows the structure factor versus temperature
for various system sizes. It decreases as the temperature
increases and clearly serves as a good indicator of the
phase transition. We emphasis that the input spin con-
ﬁgurations contain no information about the lattice geometry nor the Hamiltonian. However, the unsupervised
learner has by itself extracted meaningful information related to the breaking of the orientational order. Therefore, even without the knowledge of the lattice and the
analytical understanding of the structure factor Eq. (5),
ℓplays the same role of separating the phases in
the projected space.
It is interesting to compare our analysis of phase transitions to standard imagine recognition applications. In
the Ising model example, the learner essentially ﬁnds out
the brightness of the imagine P
i σi as an indicator of the
phase transition. While in the COP Ising model example, instead of detecting sharpness of the edges (melting
of domain walls) following the ordinary imagine recognition routine, the PCA learner ﬁnds out the structure
factor Eq. (5) related to symmetry breaking, which is a
fundamental concept in phase transition and condensed
matter physics.
Considering PCA is arguably one of the simplest unsupervised learning techniques, the obtained results are
rather encouraging. In essence, our analysis ﬁnds out the
dominant collective modes of the system related to the
phase transition. The approach can be readily generalized to more complex cases such as models with emergent
symmetry and order by disorder . The unsupervised
learning approach is particularly proﬁtable in the case of
hidden or multiple intertwined orders, where it can help
to single out various phases.
Although nonlinear transformation of the raw conﬁguration Eq. (5) was discovered via visualization in Fig. 5,
simple PCA is however limited to linear transformations.
Therefore, it remains challenging to identify more subtle
phase transitions related to the topological order, where
the indicators of the phase transition are nontrivial nonlinear functions of the original conﬁgurations. For this
purpose, it would be interesting to see if a machine learning approach can comprehend concepts such as duality
transformation , Wilson loop and string order parameter . A judicial apply of kernel techniques or
neural network based deep autoencoders may achieve
some of these goals.
Furthermore, although our discussions focus on thermal phase transitions of the classical Ising model, the
unsupervised learning approaches can also be used to analyze quantum many-body systems and quantum phase
transitions . In these applications, diagnosing quantum states of matter without knowledge of Hamiltonian
is a useful paradigm for cases with only access to wavefunctions or experimental data.
Acknowledgment
The author thanks Xi Dai, Ye-Hua
Liu, Yuan Wan, QuanSheng Wu and Ilia Zintchenko
for discussions and encouragement.
The author also
thanks Zi Cai for discussions and careful readings of the
manuscript. L.W. is supported by the start-up funding
of IOP-CAS.
 P. W. Anderson, Basic notions of condensed matter
physics .
 S. Curtarolo, D. Morgan, K. Persson, J. Rodgers,
G. Ceder, Physical Review Letters 91, 135503 .
 O. S. Ovchinnikov, S. Jesse, P. Bintacchit, S. Trolier-
McKinstry, and S. V. Kalinin, Physical Review Letters
103, 157203 .
 G. Hautier, C. C. Fischer, A. Jain, T. Mueller,
G. Ceder, Chemistry of Materials 22, 3762 .
 J. C. Snyder, M. Rupp, K. Hansen, K.-R. Müller,
K. Burke, Physical Review Letters 108, 253002 .
 Y. Saad, D. Gao, T. Ngo, S. Bobbitt, J. R. Chelikowsky,
and W. Andreoni, Physical Review B 85, 104104 .
 E. LeDell, Prabhat, D. Y. Zubarev, B. Austin,
W. A. Lester, Journal of Mathematical Chemistry 50,
2043 .
 M. Rupp, A. Tkatchenko, K.-R. Müller, and O. A. von
Lilienfeld, Physical Review Letters 108, 058301 .
 L.-F. Arsenault, A. Lopez-Bezanilla, O. A. von Lilienfeld,
and A. J. Millis, Physical Review B 90, 155136 .
 G. Pilania, J. E. Gubernatis, and T. Lookman, Physical
Review B 91, 214302 .
 Z. Li, J. R. Kermode, and A. De Vita, Physical Review
Letters 114, 096405 .
 J. Carrasquilla and R. G. Melko, , 1605.01735 .
 L. Onsager, Physical Review 65, 117 .
 U. Wolﬀ, Physical Review Letters 62, 361 .
 B. S. Everitt, S. Landau, M. Leese, and D. Stahl, Cluster
Analysis .
 K. Pearson, Philosophical Magazine 2, 559 .
 I. Jolliﬀe, Principal Component Analysis .
 M. Newman and G. T. Barkema, Monte Carlo methods
in statistical physics .
 C. N. Yang, Physical Review 85, 808 .
 R. Moessner and S. L. Sondhi, Physical Review B 63,
224401 .
 F. J. Wegner, Journal of Mathematical Physics 12, 2259
 K. G. Wilson, Physical Review D 10, 2445 .
 M. den Nijs and K. Rommelse, Physical Review B 40,
4709 .
 B. Schölkopf, A. Smola, and K. R. Müller, Neural computation .
 G. E. Hinton and R. R. Salakhutdinov, Science 313, 504
 S. Sachdev, Quantum phase transitions .
 L. Saitta, A. Giordana, and A. Cornuéjols, Phase Transitions in Machine Learning , 1410.3831 .
Stoudenmire
1605.05775v1 .
 S. Lloyd, M. Mohseni,
and P. Rebentrost,
1307.0411 .
 S. R. White, Physical Review Letters 69, 2863 .
 We also note application of physics ideas such as phase
transition , renormalization group , tensor networks and quantum computation to machine
 Each column of X sums up to zero since on average each
site has zero magnetization.
 In practice this eigenproblem is often solved by singular value decomposition of X. In fact, replacing the input data X (raw spin conﬁgurations collected at various
temperature) by the wave function of a one-dimensional
quantum system, the math here is identical to the truncation of Schmidt coeﬃcients in the density-matrix renormalization group calculations .
scikit-learn cluster module 
stable/modules/clustering.html
 The weights shown in the inset of Fig. 4 are linear mixtures of them.