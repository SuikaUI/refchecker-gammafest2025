Frequency Separation for Real-World Super-Resolution
Manuel Fritsche
Shuhang Gu
Radu Timofte
Computer Vision Lab, ETH Z¨urich, Switzerland
{manuelf, shuhang.gu, radu.timofte}@ethz.ch
Most of the recent literature on image super-resolution
(SR) assumes the availability of training data in the form
of paired low resolution (LR) and high resolution (HR) images or the knowledge of the downgrading operator (usually bicubic downscaling).
While the proposed methods
perform well on standard benchmarks, they often fail to
produce convincing results in real-world settings. This is
because real-world images can be subject to corruptions
such as sensor noise, which are severely altered by bicubic
downscaling. Therefore, the models never see a real-world
image during training, which limits their generalization capabilities. Moreover, it is cumbersome to collect paired LR
and HR images in the same source domain.
To address this problem, we propose DSGAN to introduce natural image characteristics in bicubically downscaled images. It can be trained in an unsupervised fashion on HR images, thereby generating LR images with the
same characteristics as the original images. We then use
the generated data to train a SR model, which greatly improves its performance on real-world images. Furthermore,
we propose to separate the low and high image frequencies
and treat them differently during training. Since the low
frequencies are preserved by downsampling operations, we
only require adversarial training to modify the high frequencies.
This idea is applied to our DSGAN model as
well as the SR model. We demonstrate the effectiveness of
our method in several experiments through quantitative and
qualitative analysis. Our solution is the winner of the AIM
Challenge on Real World SR at ICCV 2019.
1. Introduction
The goal of image super-resolution (SR) is to increase
the resolution in images. With the advent of convolutional
neural networks (CNNs), the ﬁeld has received increasing
attention over the last couple of years. Modern techniques
are now able to generate photo-realistic results on clean
benchmark datasets. However, most state-of-the-art models perform poorly on real-world images, which
Figure 1. ×4 SR comparison of ESRGAN and our method applied on a noisy input image. ESRGAN ampliﬁes the corruptions,
while our model preserves the noise level in the output.
can be subject to corruptions such as sensor-noise. These
characteristics usually lead to strange artifacts in the superresolved images as shown in Figure 1.
The reason for this lies in the way these SR models are
trained. Most of them rely on supervised training, which
requires high resolution (HR) and corresponding low resolution (LR) image pairs.
Since it is difﬁcult to collect
HR and LR images of the exact same scene, LR images
are typically generated from HR images. In most cases,
this is done by simply applying bicubic downscaling on HR
images. While this method is easy and provides good results in clean settings, it comes with a signiﬁcant problem:
bicubic downscaling alters image characteristics. For example, it reduces corruptions in LR images, i.e. makes them
“cleaner”. Therefore, the model is only trained for input LR
images that are altered by the downsampling operator. This
leads to a signiﬁcant performance drop when the model is
applied on images that are not bicubically downscaled.
Since many real-world images have visible corruptions,
the state-of-the-art SR methods are not very useful in practice. Current generations of smartphones are equipped with
hardware that allows the deployment of powerful neural
networks. Therefore, robust SR methods can be very useful
for improving the quality of images taken from smartphone
cameras. This work is focused on improving the performance of SR models in such real-world settings.
To achieve this, we aim to generate LR images that have
the same characteristics as the images we want to superresolve. These images allow SR models to be trained with a
similar type of data that they encounter during application.
In the ﬁrst step, we create LR images by downsampling the
 
HR images with bicubic downscaling. In a second step, we
alter the characteristics of these LR images to match those
of the source images. This is done by using a Generative
Adversarial Network (GAN) setup , which allows us to
train a neural network to make our LR images indistinguishable from the source images. However, training a GAN is
very difﬁcult and needs to be stabilized to converge to the
desired result. Similar to Ignatov et al. , we achieve
this stabilization by combining multiple loss functions: a
color loss that forces the network to keep the low frequencies of the original image and an adversarial loss that uses
the discriminator to produce high frequencies that look similar to the ones in the source images. Finally, we also add a
perceptual loss that pushes the output towards solutions that
look similar to the input images. Thereby, it ensures that the
high frequencies generated by the GAN are still matching
the low frequencies that are supervised by the color loss.
This setup is based on the following idea: during the process of downsampling an image, the high frequencies are
removed, while the low frequencies remain. Therefore, the
resulting LR images lack the high frequency characteristics
found in the original images. On the other hand, low image
frequencies such as the color are preserved to an extend that
depends on the downscaling factor. By limiting the adversarial loss to high frequencies, we greatly reduce the complexity of the task. This helps the discriminator to focus
on the relevant image features and leaves others untouched.
Therefore, our setup is more stable, converges faster and
produces better results than a standard GAN.
Furthermore, we also apply our idea of separating the
low and high image frequencies to train the SR model.
Thereby, we use a similar strategy as mentioned above: use
a pixel-wise loss to stabilize the low frequencies and apply
the adversarial loss only on the high frequencies. Since this
separates the pixel-wise and the adversarial loss, it simpli-
ﬁes the task of the discriminator. We also provide theoretical reasoning for why it makes sense to only train the high
frequencies with a GAN and use a simple pixel-wise loss
for the low frequencies.
We evaluate our methods on multiple datasets with arti-
ﬁcial and natural corruptions1. To show the effectiveness of
our implementation, we use the DF2K dataset that is a combination of the DIV2K and Flickr2K datasets.
Since this dataset contains clean images, it allows us to introduce artiﬁcial corruptions and create ground truth (GT)
HR and LR image pairs by adding the same corruptions to
both of them. We ran experiments with sensor noise as well
as compression artifacts. In both cases we demonstrate the
effectiveness of our methods through quantitative and qualitative evaluations. Furthermore, we ran our methods on
real-world images from the DPED dataset , which were
1Our code and models are publicly available at 
com/ManuelFritsche/real-world-sr
collected by an iPhone 3 camera. We only provide qualitative evaluation in this case since no GT is available. Finally, we also participated in the AIM 2019 Challenge on
Real World Super-Resolution associated with the AIM
workshop at ICCV 2019. Our method won the ﬁrst place in
both tracks for source domain and target domain. None of
our methods are speciﬁcally designed for a certain type of
data. They can also be applied to images with other characteristics than the ones we used in our experiments.
2. Related Work
In recent years, the ﬁeld of image super-resolution has
been dominated by CNNs, which achieve state-of-the-art
performance . Dong et al. introduced the
ﬁrst top performing CNNs trained end-to-end to map LR to
HR images. Based on this pioneering work, several reﬁnements have been proposed . Thereby, deeper
networks with residual layers such as EDSR produce
better results than standard CNNs.
Additional improvements were made by using different variants of densely connected residual blocks as building blocks of the
model. These blocks allow to further increase the depth of
the networks, resulting in very powerful models.
Most of the previously mentioned methods are based on
optimizing the L1 or L2 distance between the SR image and
the ground truth HR image. While this strategy achieves
state-of-the-art performance in image ﬁdelity metrics such
as PSNR, the resulting images are often blurry. This is because the human perception of visual similarity only has a
limited correlation with such pixel-wise errors. Therefore,
more recent SR methods are based on loss functions and
training methods that are better suited to produce visually
pleasing images. Gatys et al. show that high-level features extracted from pre-trained networks can be used to design perceptual loss functions. Such a loss function is used
in to enhance the visual quality of super-resolved images. The SRGAN model is trained with an additional
adversarial loss to push the output to the manifold of natural images, which allows to generate photo-realistic results.
Several works have proposed further improvements with approaches that focus on perceptual similarity .
The recently introduced RankSRGAN uses a method to
train SR models on indifferentiable perceptual metrics. Our
experiments are based on ESRGAN , the winner of the
PIRM 2018 challenge on perceptual super-resolution .
It introduces several improvements to the SRGAN model,
thereby achieving state-of-the-art perceptual performance.
All of the previously mentioned models are trained with
HR/LR image pairs, generated through bicubic downscaling. Therefore, these models perform poorly in real-world
scenarios.
One way of addressing this issue is by directly collecting paired data, which is explored in recent
work . However, these approaches rely on complicated hardware and require the collection of new data for
each camera source. Other methods try to make SR more
robust, tailor them to the test image. Liang et al. proposed to ﬁne-tune a pre-trained SR model to the test image. ZSSR is a lightweight CNN that is trained by
only using the test image, which allows the network to focus
only on image speciﬁc details. However, both approaches
still rely on a known downsampling operation during training. Additionally, training a network for each test image
results in very slow predictions. Yuan et al. propose
a model that learns a mapping from the original input to a
clean input space, after which they apply super-resolution.
They use a complex framework with two cycle-consistency
losses, which increases training time. Their initial cleaning step improves the performance of the model on corrupted images, but also increases the complexity of their
model. Conversely, our approach mainly focuses on generating training data. We only make small modiﬁcations
in the discriminator and loss functions, which does not introduce more complexity in the model. Similar to our work,
some novel methods generate paired data artiﬁcially. Kim et
al. propose an auto-encoder-based framework to jointly
learn downsampling and upsampling. While their superresolution method performs well on images downsampled
by their model, it is not applicable to unknown downsampling operations. Bulat et al. explore a method to learn
the downsampling operation. However, they focus only on
faces, but not the general super-resolution problem, which
makes the task a lot easier. In contrast, we do not make any
assumptions on the content of the images.
3. Proposed Method
3.1. Real-World Super-Resolution
State-of-the-art SR models rely on fully supervised training of neural networks on paired HR and LR images. While
collecting images is not a difﬁcult task, obtaining paired
images from two different sources is difﬁcult and cumbersome. For this reason, the SR ﬁeld mainly relies on using bicubic downscaling to generate image pairs. Although
this approach has helped the development of promising SR
models, it is limiting the generalization to real-world images because it can drastically alter certain image characteristics such as sensor noise. An example of how this downsampling operation affects a real-world image can be seen
in Figure 2.
For our analysis, we assume that we are given a set of
source images, which have similar characteristics (e.g. the
same sensor noise distribution). One can then deﬁne the
HR images y ∈Y either directly as the source images or as
modiﬁed versions thereof. Finally, the LR images x ∈X
are generated by downsampling the HR images. As traditional downsampling operations alter the image characterisoriginal
Figure 2. This image is taken from the DPED dataset and
shows strong corruptions.
When comparing a cropped region
(middle) to the bicubically downscaled version of equal size
(right), one can clearly see how this operation reduces corruptions.
Figure 3. Visualizes the structure of the downsampling setup. B
denotes the bicubic downscaling method, while the purple ﬁelds
display the high- and low-pass ﬁlters. The red triangles denote the
loss functions and the orange ﬁelds the neural networks.
tics, images from X differ from images in Z. Since our goal
is to upsample images from the domain Z, we aim to have
X = Z. In other words, the LR images seen in training
should be indistinguishable from the source images. Therefore, we aim to map images from X to Z, while preserving
the image content.
3.2. Downsampling with Domain Translation
In the following, we describe a model that can produce
realistic LR images in the source domain Z, given HR images in some Y domain. The complete overall structure is
shown in Figure 3.
In the ﬁrst step, we bicubically downscale the HR image
y to obtain a LR image xb = B(y). Since xb is now in the
wrong domain X, we use a neural network Gd(·) to translate
it to the source domain Z. We call this new image xd with
xd = Gd(xb). To train Gd(·) we use a standard GAN 
setup with an additional discriminator Dd(·). The discriminator is trained with the generator output Gd(B(y)) as fake
data and uses the source images z ∈Z as real data.
Network Architectures
The generator network consists
mainly out of residual blocks with two convolutional
layers and ReLU activations in between. Except for the output layer, all convolutional layers use a 3×3 kernel with 64
As image characteristics do not change the global image
content, but only introduce local changes, we use a patchbased discriminator . This discriminator is fully
convolutional and returns a 2D array that determines which
regions of the image look real or fake. The discriminator
applies four convolutional layers with 5 × 5 kernels on the
low-pass ﬁltered input image. The number of output features of each convolutional layer increases from 64 to 128
and 256 with the ﬁnal layer only using one feature map.
Between the convolutional layers we apply Batch Normalization and LeakyReLU activations.
3.3. Frequency Separation
As described in Section 3.2, we are using a standard
GAN setting to translate the original LR images to the
source domain Z. However, we do not just want any image in this source domain, but the one that is closest to our
original LR image xb. One way of achieving this is by using multiple loss functions. By introducing a perceptual and
a pixel loss, one can restrict the possible solutions that the
generator produces. Unfortunately, such a loss function is
hard to balance because we need the output of the generator
xd to stay close to the input xb and at the same time introduce the image characteristics of the source domain. The
result is that we have to deal with a trade-off and neither of
the goals will be achieved perfectly.
This naive approach ignores the fact that we are dealing
with downsampled images. As we discuss in Section 3.4,
the downsampling process removes the high image frequencies and keeps the low frequency information within a reduced number of pixels. This leads to high frequency characteristics being lost, while low frequency information such
as color and context remain. As the low image frequencies
are preserved, we only need to alter the high frequencies in
our mapping from X to Z. Therefore, we propose to apply
the discriminator only on the high frequencies xH,d of xd
and keep the low frequencies xL,d close to the original ones.
This greatly reduces the complexity of the problem, making
it easier for the discriminator to focus on the relevant image
features. In our experiments, we found this method to be
crucial in training the GAN. It not only speeds up the training process, but also produces better results. Furthermore,
our GAN setup reduces undesired color shifts, because the
discriminator ignores the low image frequencies.
We separate the low and high image frequencies by using simple linear ﬁlters. For this purpose, we ﬁrst deﬁne a
low-pass ﬁlter wL,d. The low and high frequencies can be
obtained by simple convolutions:
xL,d = wL,d ∗xd,
xH,d = xd −xL,d = (δ −wL,d) ∗xd.
Therefore, we deﬁne our high-pass ﬁlter as wH,d = δ−wL,d.
After applying a high-pass ﬁlter, we feed the remaining frequencies xH,d to the discriminator Dd(·). The same highpass ﬁlter is applied to the source images z ∈Z. In our
experiments, we empirically chose a moving average with
kernel size 5 as low-pass ﬁlter. However, our method is not
limited to any speciﬁc ﬁlter.
Loss Functions
We combine multiple loss functions to
train our model. The generator loss combines three different
losses: color loss Lcol,d, perceptual loss Lper,d and texture
loss Ltex,d, which is an adversarial loss.
The color loss is focusing on the low frequencies of the
image. Since we do not want to change the low frequencies
of xb, we apply an L1 loss on these frequencies to keep
them close to the input. The color loss is deﬁned as
Lcol,d = 1
−wL,d ∗x(i)
where m denotes the batch size.
As discussed in Section 3.3, we only apply the GAN loss
on the high frequencies of the output xd, which results in the
following loss for generator and discriminator:
Ltex,d = 9 1
wH,d ∗z(i)
Since the discriminator Dd(·) returns a 2D array of values
and not just a single one, we take the mean over all these
values in the loss function.
Finally, to ensure that the high and low frequencies ﬁt
together, we also apply a perceptual loss Lper,d to xb and
Gd(xb). For this loss we use LPIPS , which is based on
the features of the VGG network .
We deﬁne the complete loss functions of the generator as
LGd = Lcol,d + 0.005 · Ltex,d + 0.01 · Lper,d.
3.4. Frequency Separation for Super-Resolution
We also apply our idea of frequency separation directly
on ESRGAN . However, the approach is not limited to
this SR model and can easily be adapted for other methods.
Our changes to the model are visualized in Figure 4.
For our analysis, we look at an image y that we downsample by a factor r. Let us assume that x is a downsampled version of y without aliasing. The Sampling Theorem
tells us that x allows us to infer the lowest 1/r fraction of
the possible frequencies of the original image y, which we
denote as yL in the following. The remaining high frequencies are deﬁned as yH = y −yL. There is no need to consider the context and generate fake details to map from x
to yL. In contrast to the mapping from x to y, the mapping from x to yL is a one-to-one mapping, which allows
it to be reconstructed directly. For reconstructing yH on the
other hand, we can only rely on the context information, because it contains all the high frequencies that are removed
by the downsampling and anti-aliasing process. Thus, the
mapping from x to yL is considerably easier to learn than
the mapping from x to yH.
Similar to Section 3.3, we use a low-pass ﬁlter wL and
a high-pass ﬁlter wH to split up the low and high image
frequencies. In our experiments, we found that a simple
moving average with kernel size 9 works well. The low frequencies of G(x) can be learned directly over the L1 loss,
since there is only one possible yL given x. Since the colors
of an image are mainly deﬁned in the low frequencies, we
call this loss the color loss Lcol:
The high frequencies of G(x) on the other hand have multiple ground truth values and cannot be learned through a
pixel-based loss. Therefore, we use the adversarial loss only
on these high frequencies, by simply adding a high-pass ﬁlter in front of the discriminator. This greatly reduces the
complexity of the task, as the discriminator does not have
to deal with the low frequencies.
To make sure the high frequency details generated by the
GAN loss match the low frequencies, the perceptual loss
is applied on the full output. This results in the following
adapted loss function for the ESRGAN generator:
LG = Lper + 0.005 · Ladv + 0.01Lcol,
This loss function simpliﬁes the task of the discriminator,
which allows the model to produce outputs that match the
target distribution more closely.
4. Experiments
4.1. Experimental Setup
Dataset Generation
For all experiments, we use a scaling
factor of 4 between the HR and LR images. We generate the
HR/LR image pairs by using the model described in Section 3.1, which we call DSGAN (DownSampleGAN). We
train it with 512×512 image patches, which we bicubically
downscale with the MATLAB imresize method. For the discriminator, we use random 128 × 128 crops of the source
images z ∈Z. Using a batch size of 16 image patches,
we train the model for 200 or 300 epochs, depending on
the size of the dataset. Similar to CycleGAN , we use
Adam optimizer with β1 = 0.5 and an initial learning
rate of 2·1094. The learning rate is kept constant for the ﬁrst
half of the epochs and then linearly decayed to 0 during the
remaining epochs.
Figure 4. Visualizes our changes to the ESRGAN structure. Only
the purple blocks are added to ﬁlter the images.
Same Domain Super-Resolution (SDSR): In this setup,
we aim to generate a training dataset with HR and LR images that are both in the source domain Y = Z. We use the
source images directly as the HR images and train our DS-
GAN model to map from the domain of bicubically downscaled LR images X to the domain of the HR images Y.
Target Domain Super-Resolution (TDSR): If the images
in the source domain have corruptions such as sensor noise,
it is often desirable to remove these in the SR process.
Therefore, in the TDSR setting, we aim to use HR images
in a clean domain Y ̸= Z and only have our LR images
in the source domain Z. We generate the HR images by
bicubically downscaling the source images with a factor of
2. Since we use a scaling factor of 4 between the HR and
LR images, bicubic downscaling removes almost all corruptions. Therefore, we assume that the bicubically downscaled HR images from the SDSR setting and the TDSR
setting to be approximately in the same domain X. Thus,
we apply the same model trained for SDSR in our TDSR
setting to generate the LR images in the Z domain.
ESRGAN and ESRGAN-FS
In the second step, we use
the HR/LR image pairs to train our SR model with either ESRGAN or our modiﬁed ESRGAN described in Section 3.4, which we denote as ESRGAN-FS in the following.
We initialize training with the fully pre-trained ESRGAN
weights in both cases, as training ESRGAN from scratch
takes a long time. We then perform 50k training iterations
with an initial learning rate of 10−4. The learning rate is
halved after 5k, 10k, 20k and 30k iterations. We use Adam
optimizer with β1 = 0.9 and β2 = 0.999 for both generator and discriminator training.
For our experiments with artiﬁcial corruptions,
we use the DF2K dataset, which is a merger of the
DIV2K and Flickr2K datasets. It contains 3450
train images and 100 validation images with very little corruptions. To evaluate our method, we introduce two kinds
of corruptions: sensor noise and compression artifacts. The
sensor noise is modeled by adding pixel-wise independent
Gaussian noise with zero mean and a standard deviation of
8 pixels to the images. The compression artifacts are introduced by converting the images to JPEG with a quality of
30. Thereby, we create ground truth HR/LR pairs by applying the same degradation on both HR and LR images.
To test our methods on real-world data, we use images
from the DPED dataset . More precisely, we use the
5614 train and 113 test images taken with an iPhone 3 camera. Since we cannot generate any ground truth for these
images, we only compare the results visually.
Furthermore, we also participated in the AIM 2019 Challenge on Real World Super-Resolution . In this challenge 2650 corrupted source images and 800 clean target
images are provided for training. The corruptions in the
source data are artiﬁcial but unknown. The validation and
test set contain 100 images each and have the same corruptions as the source data .
Quantitative Evaluation
For our quantitative evaluation,
we use the popular PSNR and SSIM methods, which we
calculate with the scikit-image measure module . While
SSIM and PSNR are often used for measuring similarity to
ground truth images, the resulting similarity values often
correlate poorly with actual perceived similarity. Therefore,
we also use LPIPS for comparison. As mentioned in
Section 3.3, this measure is based on the features of pretrained neural networks, which have been shown empirically to correlate better with human perception than handcrafted methods. We use the LPIPS method that is based on
the features of AlexNet for evaluation.
4.2. Comparison with State-of-the-Art
In this section, we compare our methods with four other
state-of-the-art methods. The ﬁrst one is ESRGAN ,
where we report the results with and without additional ﬁnetuning on the corrupted dataset (using bicubic downscaling). The ﬁne-tuned model is referred to as ESRGAN (FT).
We also look at another method called RankSRGAN .
This method uses an additional model called Ranker to simulate the behavior of indifferentiable perceptual metrics. It
then trains the model with these simulated perceptual metrics. We use the pre-trained weights based on the NIQE
metric . Furthermore, we look at EDSR , which is
a method optimized for PSNR-based super-resolution. We
also include ZSSR in our comparison, which applies a
Zero-Shot learning strategy on each image it super-resolves.
Experiments on Corrupted Images
In this experiment,
our methods are ﬁne-tuned on the corrupted DF2K datasets
as described in Section 4.1. In Table 1, we compare the
PSNR, SSIM and LPIPS values of all methods on the
DIV2K validation set. As ground truth, we use the corrupted and original HR images in the SDSR and TDSR
Figure 5. Qualitative comparison of our approaches (bold) with
other state-of-the-art methods on images with additional sensor
noise (upper two rows) or corruption artifacts (lower two rows).
ours refers to our ESRGAN-FS trained with data generated by
our DSGAN method in the two different settings.
sensor noise
compression artifacts
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
ESRGAN (FT)
ESRGAN 
RankSRGAN 
ESRGAN (FT)
ESRGAN 
RankSRGAN 
Table 1. We report the mean scores over the DIV2K validation set
with added corruptions. The arrows indicate if high ↑or low ↓
values are desired. ours refers to our ESRGAN-FS trained with
data generated by our DSGAN method.
settings, respectively. In our discussion of the quantitative
evaluation, we focus on the LPIPS measure, as it has the
best correlation with image similarity. A qualitative comparison is shown in Figure 5.
All state-of-the-art methods introduce severe artifacts in
the output for both sensor noise and compression artifacts.
Speciﬁcally, ESRGAN achieves the worst LPIPS value in
both the TDSR and SDSR settings. This value can be improved through ﬁne-tuning on the corrupted images. However, the visual quality does not show any notable improvements. RankSRGAN achieves the best LPIPS value of the
state-of-the-art methods, but still introduces signiﬁcant corruptions in the output. EDSR and ZSSR manage to reduce
the corruptions as they generally produce more blurry results. However, the LPIPS value is still close to the value
for ESRGAN. In contrast, our methods produce satisfying
Figure 6. Qualitative comparison of our approaches (bold) with
other state-of-the-art methods on real-world images from the
DPED dataset taken with an iPhone 3 camera. ours refers to our
ESRGAN-FS trained with data generated by our DSGAN method
in the two different settings.
results in all cases. This is reﬂected in the LPIPS values, as
well as the visual quality of the images, which show almost
no corruptions. This is due to our model being trained with
input images with similar characteristics as the validation
images that were upsampled to generate these results.
Experiments on Real-World Images
We also evaluate our method on real-world images from the DPED
dataset , where we use the train images captured by an
iPhone 3 camera for ﬁne-tuning. Since we do not have access to ground truth, we only compare the results visually,
which is done in Figure 6. Due to the signiﬁcant amount
of corruptions (cf. Figure 2), none of the state-of-the-art
methods produces satisfying results on this dataset. ESR-
GAN and RankSRGAN introduce strong artifacts, which
are slightly reduced in the PSNR based EDSR. ZSSR produces visually very similar results, which are rather blurry.
In contrast, our models produce images that are sharp and
greatly reduce the amount of corruptions.
The AIM 2019 Challenge
We also participated in the
AIM 2019 Real World Super-Resolution Challenge .
Thereby, the SR (×4) images are supposed to either match
the source domain in Same Domain Real World Super-
Resolution (SDSR) or a clean target domain in Target Domain Real World Super-Resolution (TDSR).
We ﬁrst train our DSGAN model on the corrupted source
dataset. To increase the diversity in the data, we randomly
Figure 7. Qualitative comparison of our approaches (bold) with
other state-of-the-art methods on the AIM2019 Real World SR test
set. ours refers to our ESRGAN-FS trained with data generated by
our DSGAN method in the two different settings.
ﬂip and rotate the images. For SDSR, our HR dataset contains the source and target datasets. Since the latter contains
clean images, we use DSGAN to add the corruptions. In
TDSR, the HR dataset is constructed by combining the target dataset and the bicubically downscaled source dataset,
for which we use a scaling factor of 2. In both cases, the LR
images are created by applying DSGAN on the HR images.
We then ﬁne-tune ESRGAN-FS on these datasets.
In Figure 7, we provide qualitative results for our SDSR
and TDSR method and compare them to other state-ofthe-art methods. As expected, PSNR based methods like
EDSR produce blurry results. Perception-driven methods, such as ESRGAN , generate sharper images, but
they also increase the effect of image corruptions. On the
other hand, our method produces sharp images with only
few corruptions in both the SDSR and TDSR case. Most
notably, the block structure corruptions caused by compression artifacts in the input image are removed in our models.
Furthermore, we present the results from the challenge
in Table 2. In addition to PSNR, SSIM and LPIPS, a Mean-
Opinion-Score (MOS) was conducted to evaluate how similar the results are to ground truth. For both SDSR and
TDSR our method won the challenge by achieving the lowest MOS. More information on the evaluation and competing methods can be found in the challenge report .
4.3. Ablation Study
In this section, we compare different versions and combinations of our models. We evaluate our method in the
PSNR↑SSIM↑LPIPS↓MOS↓
MadDemon (ours), winner
ACVLab-NPUST
Image Speciﬁc NN for RWSR
MadDemon (ours), winner
Image Speciﬁc NN for RWSR
Table 2. This table reports the quantitative results from the AIM
2019 Challenge on Real World SR . The arrows indicate if
high ↑or low ↓values are desired.
same setting as used in Section 4.2. Although we also report PSNR and SSIM, we focus mostly on LPIPS in our
discussion as it correlates best with perceptual similarity.
As ground truth, we use the corrupted and clean HR images
in the SDSR and TDSR settings, respectively. Our quantitative analysis is provided in Table 3. Furthermore, we
provide visual results on the DIV2K validation set
in Figure 8 for sensor noise and compression artifacts.
We vary two things in our experiments: the model that
is used for SR and the method that is used to generate
the HR/LR image pairs. We compare ESRGAN and
ESRGAN-FS (with frequency separation) as our SR models. In each case, one of these models is ﬁne-tuned with one
of the following datasets.
bicubic: This standard method results in very poor SR
performance. In all cases, it produces strong corruptions in
the SR images. This is also reﬂected in the LPIPS values,
which are the worst of all compared methods.
DSGAN: In all cases, using DSGAN greatly improves
the performance of the method compared to using bicubic downscaling. Thereby, ESRGAN-FS tends to produce
slightly sharper images than ESRGAN, which leads to a
slightly worse LPIPS score for ESRGAN-FS, as the introduced details are often different from ground truth. Furthermore, ESRGAN-FS better matches the source characteristics in the output, which can be seen in the SDSR setting
with compression artifacts. ESRGAN does not always manage to produce realistic artifacts, while the artifacts generated by ESRGAN-FS look convincing in all image regions.
GT: In case of input images with sensor noise, this results in images that look very similar to the ones that were
generated by using DSGAN. This means that our DSGAN
method is very accurate in reproducing the image characteristics of the source images. In case of compression artifacts, the difference is more apparent, as the models trained
with the DSGAN dataset introduce some additional corruptions. Interestingly, the models trained with DSGAN produce sharper results than the models trained with GT. This
might be because we use bicubic downscaling to generate
these image pairs, which not only alters corruptions but also
other image characteristics.
JPEG - TDSR
JPEG - SDSR
noise - TDSR
noise - SDSR
Figure 8. Ablation study on the DIV2K validation set with sensor
noise (noise) or compression artifacts (JPEG). The ﬁrst line below the images denotes the dataset used for ﬁne-tuning, while the
second line refers to the SR method.
sensor noise
compression artifacts
PSNR↑SSIM↑LPIPS↓PSNR↑SSIM↑LPIPS↓
DSGAN ESRGAN
DSGAN ESRGAN-FS
DSGAN ESRGAN
DSGAN ESRGAN-FS
Table 3. This table reports the quantitative results of our ablation
study. The arrows indicate if high ↑or low ↓values are desired.
5. Conclusion
We propose DSGAN to generate paired HR and LR images with similar characteristics. We argue that the relevant
characteristics mainly appear in the high frequencies of an
image, which we exploit by applying the adversarial loss
only on these frequencies. Furthermore, we also apply our
idea of frequency separation to the SR model, which allows
it to match the target distribution more closely. Our multiple
experiments with artiﬁcial and natural corruptions demonstrate the effectiveness of our approach for real-world SR.
We not only beat the state-of-the-art methods in these experiments, but also won the AIM 2019 Challenge on Real
World Super-Resolution .
Acknowledgments.
This work was partly supported by
ETH General Fund and by Nvidia through a GPU grant.