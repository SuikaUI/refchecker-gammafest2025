WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
Unsupervised Learning of Generative Topic
Saliency for Person Re-identiﬁcation
Hanxiao Wang
 
Shaogang Gong
 
 
School of Electronic Engineering and
Computer Science,
Queen Mary, University of London
London E1 4NS, UK
Existing approaches to person re-identiﬁcation (re-id) are dominated by supervised
learning based methods which focus on learning optimal similarity distance metrics.
However, supervised learning based models require a large number of manually labelled
pairs of person images across every pair of camera views. This thus limits their ability to
scale to large camera networks. To overcome this problem, this paper proposes a novel
unsupervised re-id modelling approach by exploring generative probabilistic topic modelling. Given abundant unlabelled data, our topic model learns to simultaneously both (1)
discover localised person foreground appearance saliency (salient image patches) that are
more informative for re-id matching, and (2) remove busy background clutters surrounding a person. Extensive experiments are carried out to demonstrate that the proposed
model outperforms existing unsupervised learning re-id methods with signiﬁcantly simpliﬁed model complexity. In the meantime, it still retains comparable re-id accuracy
when compared to the state-of-the-art supervised re-id methods but without any need for
pair-wise labelled training data.
Introduction
Person re-identiﬁcation (re-id) is a challenging problem for computer vision . Recent
efforts on solving the re-id problem are dominated by supervised learning based methods that
aim to learn an optimal matching function or distance metric . More
speciﬁcally, for each pair of camera views, a labelled training set is constructed. It consists
of a set of people for which images of each individual must be annotated manually with an
identity label across both views. A matching function is learned from the training set subject
to a set of constraints, that is, a pair of images of the same person should have larger matching
score/smaller similarity distance compared to that of two different people given the labelling
information, regardless their visual appearance dissimilarity/similarity. By satisfying these
constraints the learned model can implicitly discover visual features that are more stable
against intra-class appearance variations. These variations are typically caused by viewing
condition changes across a particular pair of camera views. However, there is a signiﬁcant
limitation of these supervised learning based methods – a large set of people must be labelled
manually across every pair of camera views. Moreover, even for the same pair of camera
views, once the conditions change (e.g. different time of the day), new labelling may be
needed again to update the matching function. Therefore, such approaches are inherently
c⃝2014. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
Figure 1: Each of (a)-(c) shows (left to right): person image, topic model detected background map and
foreground saliency map. The saliency maps capture localised appearance features (e.g. brown jacket,
red shoes, blue sleeve pattern, pink handbag, green bottom, pink shirt). (d) show that the distributions
of the foreground saliency maps from two different camera views of the same person are stable and
useful for re-id. Best viewed in colour.
limited in their scalability to different camera pairs at different times without the need for
exhaustive and repeated manual labelling. This is impractical for large camera networks of
hundreds of cameras.
An attractive method to overcome this limitation of supervised learning based re-id models is to explore unsupervised learning, which aims to make re-id models more scalable to
new camera views even though it may sacriﬁce re-id matching accuracy. A key question is
what can be learned in images for re-id without person’s identity labels explicitly annotated
to images across camera views. Among the few reported unsupervised learning based re-id
methods, most are focused on learning view or illumination change invariant (stable) feature descriptors of human appearance . More recently, an unsupervised learning
model has also been proposed to discover localised visual appearance saliency for re-id .
This is based on very intuitive principles – humans often identify people by their salient appearances (local/small area) such as wearing a rare-coloured coat or a strange-shaped hat,
and ignore the more common traits in people’s appearance. However, this saliency model
is exhaustively data-driven therefore computationally complex. This is due to the fact that
the model is based on constructing a different saliency model for every local image patch
in every image against a reference set whilst each image is decomposed into hundreds of
patches. That is, if there are M images to be matched across two camera views and each
image is decomposed to N patches, there are M×N different saliency models required to
be constructed against the reference set. This data-driven approach to unsupervised saliency
learning also makes it potentially unstable to large scale problems. For these problems, many
images of people (from hundreds to thousands) need be matched across camera views and
people’s appearance necessarily exhibits greater variety.
In this paper, a novel unsupervised modelling approach to saliency detection for person
re-id is proposed based on probabilistic generative topic modelling. This is signiﬁcantly
different from previous attempts, which are data-driven and discriminative. More speciﬁcally, given abundant unlabelled data, our model aims to learn simultaneously what people
look like (background removal in a bounding box) and how their typical appearance can be
represented by a collection of local and visually coherent parts. This is achieved by learning a set of latent topics that correspond to both typical and localised human appearance
components, e.g. blue jeans and dark suit. This component-based typical appearance representation is then deployed for identifying atypical appearance by discovering local saliency.
This generative topic model based representation is also inherently capable of differentiating background clutters from typical human appearance in a detected person bounding box
(Figure 1), beneﬁcial to person re-id in cluttered scenes .
Our proposed Generative Topic Saliency (GTS) model is based on unsupervised topic
modelling designed speciﬁcally to discover re-id relevant saliency that corresponds to atypi-
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
cal appearance of individual people (foreground). It also simultaneously removes surrounding background clutter in a person detection bounding box. It has two advantages over the
existing saliency model for person re-id : (1) Interpretability - each learned topic has
clear semantic meaning. (2) Complexity - only a single model is needed for computing
saliency for all the images in a camera view, in contrast to having to construct a different saliency model for every image patch of every image. Comparative evaluations on the
VIPeR and iLIDS datasets demonstrate that the proposed GTS model not only outperforms existing unsupervised learning based saliency model, but also is competitive to the
state-of-the-art supervised learning models without the need for expensive data labelling.
Related Work
Supervised learning – Most existing learning based re-id methods rely on supervised learning of discriminative distance metrics given pairwise labelled data from different cameras
 . The scalability limitation of these supervised methods has motivated
a number of transfer learning-based methods to utilise previously labelled
data elsewhere to minimise the need for labelling images of every new camera pair. However, a small set of labelled images are still required at each new pair and model updating
(re-learning) remains necessary when the lighting and view conditions change.
Unsupervised learning – Earlier unsupervised learning re-id methods are focused on feature design , rather than learning saliency. This is because without labelling,
any saliency measure has to rely on general principles without knowledge of person-speciﬁc
appearance characteristics. This is a much harder problem than supervised learning from
labelled information on person-speciﬁc appearance. Recently, Liu et al. proposed a
feature importance mining scheme, aiming to optimise the weights for global feature types.
Alternatively, Zhao et al. proposed a patch-based representation to learn local saliency
in a person’s appearance. Their method relies on exhaustively learning a very large number
of data-driven discriminative models (k-NN or one-class SVM) through constructing different saliency models for every patch of every image. In contrast, our approach learns a
single generative model for computing saliency map for all the images in a camera view,
signiﬁcantly reducing model complexity. Moreover, our model segments simultaneously
foreground and background, giving more accurate saliency detection compared to as
the latter is sensitive to false saliency detection caused by confusing background as salient
foreground.
Topic modelling – Probabilistic topic models have been used for image analysis which
can be considered as a dimensionality reduction technique that represents image content in
a low-dimensional latent topic space. Topic models have been employed to perform various
tasks such as scene understanding, object classiﬁcation and annotation . However, to the best of our knowledge this is the ﬁrst time that probabilistic topic modelling is
explored for unsupervised learning of human appearance saliency for re-id. Our model is
related to the work of Shi et al. . However, their method is a weakly-supervised model
designed to localise different categories of objects in an image. In contrast, our model is
fully unsupervised and designed to optimise the selection of human appearance saliency by
learning localised topics for a component-based human appearance representation.
Contributions – Our contributions are: (1) A novel re-id model, Generative Topic Saliency
(GTS), for localised human appearance saliency selection in re-id by exploiting unsupervised generative topic modelling. (2) The GTS model is capable of simultaneous foreground
saliency detection and background clutter removal. (3) The GTS model yeilds state-of-theart re-id performance against existing unsupervised learning based re-id methods.
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
Methodology
Image Representation
Similar to , we adopt an over-sampled local patch based representation for each person
image. More precisely, each image is represented by 50% overlapped uniform-sized square
patches on a dense grid. From each patch, a 32-bin color histogram is computed in the LAB
color space with 3 levels down-sampled. SIFT features are also computed in the 3 color
channels, with each patch divided into 4×4 cells and 8-bin orientations of local gradients.
The ﬁnal patch descriptor is computed by L2 normalisation and concatenation of the colour
histogram and SIFT, giving a 672 dimensional feature vector (32×3×3+8×4×4×3). Patch
size and grid step length are 10 and 4 pixels respectively. Our overall image representation
builds on the patch descriptors and differs from that of . Speciﬁcally, a topic model
treats each document (image) as a certain combination of visual words and requires a bagof-words representation. Given the patch feature vectors from each image, we cluster all
the patch feature vectors from an unlabelled training set into a Nv = 2000 words codebook
by K-means clustering. Given this codebook, each patch is assigned with a word label by
its cluster index. An image Im is then represented by Nm words together with their image
positions, denoted as {wnm,lxnm,lynm}Nm
n=1, with wnm the word label of a patch, lxi j and lyij the
image coordinates of that patch.
Joint Modelling Human Appearance and Camera Background
Given a set of M images of people in bounding boxes, typically extracted from a person
detector, we wish to learn a joint topic model capable of capturing the typical appearance of
people in foreground patches and simultaneously separating the background patches within
each bounding box, without any labelling information. The topic model essentially factorises the image patches and attempts to ﬁnd localised coherent patches (not necessarily
connected) that correspond to common appearance traits of people such as grey top and blue
jeans, without any supervised learning. However, the bounding boxes inevitably contain
backgrounds, which are often also spatially and visually coherent. To differentiate them,
background patches are also modelled explicitly by the generative topic modelling. We thus
learn two types of latent topics in our model corresponding to foreground and background
respectively. Since foreground appearance are in general more ‘compact’ than background,
similar to we choose a Gaussian distribution to encode foreground human appearance
topics and a Uniform distribution to encode more spread-out background topics.
Model Description – Our model is a generalisation of the Latent Dirichlet Allocation (LDA)
model with an added spatial variable to make the learned topics spatially coherent. Given
a dataset of M images, each image will be factorised (clustered) into a unique combination
of K shared topics, with each topic generating its own proportion of words on that image.
Conceptually, one topic encodes a certain distribution of visual words (patches), whose vocabulary and spatial location revealing certain patterns, in our case the visual characteristics
of human appearances and backgrounds. Among these K topics, Kha topics are used to model
foreground human appearance, and Kcb = K −Kha topics represent background within the
bounding boxes from the entire training dataset. In this work we set Kcb = Kha = 20. Suppose Dir, Multi, NW, N denote respectively Dirichlet, Multinomial, Normal-Wishart and
Normal distributions, the generative process of this model is:
1. For each topic tk ∈{t1,t2,...,tK}, draw its appearance distribution βk ∼Dir(β 0
2. For each image Im ∈{I1,I2,...,IM}, draw the human appearance and camera background topic distribution θm ∼Dir(α). Each human appearance topic tk ∈T ha is as-
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
signed with a Gaussian distribution parameters to reﬂect the spatial location and size
of the human appearance on Im: {µkm,σk j} ∼NW(µk
3. For each patch Pnm ∈{P1m,P2m,...,PNmm}, draw its topic znm ∼Multi(θm), draw its
vocabulary wnm ∼Multi(βznm) and draw its location lnm. If znm is a human appearance topic then its location is Gaussian distributed, lnm ∼N(µznmm,σ−1
znmm); if znm is a
camera background topic then its location is Uniformly distributed, lnm ∼Uniform.
Model Learning – The learning task for this model is to infer the following quantities:
(1) The vocabulary distribution of each human appearance and background topics βk, (2)
all topics’ word proportion θm and their locations {µmk,σmk} in each image, and (3) each
patch’s topic assignment znm. The joint distribution of observed data set O, latent variables
set L and hyper-parameters set H is given by:
Pr(O,L|H) =
Pr(µmk,σmk|µk
0)Pr(θm|αm)
Pr(wnm|znm,θm)Pr(znm|θm)
This model is intractable by exact solutions. An approximate solution can be learned
by the EM algorithm with a variational inference strategy, through introducing a Dirichlet
parameter γ and a multinomial parameter ϕ as variational parameters. Under this variational inference framework, γ is learned for each image, with γmk modelling the proportion
of patches which belong to topic tk in image Im. ϕ is learned for each patch, with ϕnmk
modelling the probability of patch Pnm on image Im being generated by topic tk. The hyperparameter α is set to 1 for all human appearance and camera background topics because our
method is completely unsupervised and thus all topics may appear in any images.
Determining Patch Prevalence
A key objective of our model is to discover local foreground patches in a person’s image
that make the person stand out from other people, i.e. the model seeks not only visually
distinctive but also atypical localised appearance characteristics of a person. To compute
such a saliency value, let us ﬁrst consider to compute a ‘prevalence’ value of each patch and
deﬁne saliency as the inverse of prevalence, as the former is more naturally computable by
the topic model. Speciﬁcally, for a patch PA on image IA, its saliency value is measured by
how unlikely this patch will appear in a training set IR of J images at the proximity of a
particular spatial location in the images. PA’s saliency score is the inverse of its prevalence
value in IR. For computing patch prevalence value, suppose the learned latent variables set is
L and their hyper-parameter set is H. The topic appearance vector βvk reﬂects the probability
that vocabulary (the collection of words in the codebook) v is generated under topic tk. The
multinomial parameter ϕnmk refers to the probability that patch Pnm’s topic is tk given the
learned model parameters:
βkv = Pr(w = v|tk,L,H), v = 1,2,...,Nv;
ϕnmk = Pr(znm = tk|L,H), k = 1,2,...,K (2)
Based on the Bayes’ Theorem, combining the two equations in Eqn. (2) gives the joint likelihood of observed word wnm and its topic znm as:
Pr(wnm = v,znm = tk|L,H) = Pr(w = v|tk,L,H)Pr(znm = tk|L,H)
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
By margining out the topic variable znm over t1 to tK, we obtain the likelihood of patch Pnm’s
vocabulary wnm. This likelihood value reﬂects our model’s conﬁdence for the visual word
wnm to be vocabulary v: (v = 1,2,...,Nv):
L(wnm) = Pr(wnm = v|L,H) =
Pr(wnm = v,znm = tk|L,H)
To measure the probability of patch PA appearing in image Im, we impose a simple but
reasonable human prior knowledge on people’s images, that is, a person’s position within a
bounding box is relatively stable, and a patch’s horizontal shift caused by viewpoint change
is far larger than its vertical shift. This assumption is typically valid for a pedestrian captured
in a bounding box. Based on this assumption, in each image Im in IR we build a patch set ˆPAm
by taking all the patches in the same horizontal row as PA. The elements in ˆPAm are referred
m,r, with r as the row index. Given PA’s vocabulary wPA = v0, the probability that patch
PA repeatedly appears in image Im of IR is measured by the maximum probability for ˆPAm
patches’ vocabulary equalling to v0:
P(PA in Im) = max
wPAm,r = v0|L,H
Patch PA’s prevalence level is computed by accumulating P(PA in Im) for all the images Im
Prevalence(PA) = ∑
P(PA in Im),
Computing Saliency
Given the prevalence value of each patch (Eqn. (6)), its saliency score is initialised by applying an inverse function h(x) on its prevalence value. These saliency scores are then further
reﬁned by two basic principles as follows. First, a patch with high probability of belonging
to background topics should have low saliency scores. Second, even if a patch belongs to
a human appearance topic, but if this topic is very dominant/popular in the training dataset
(e.g. many people wearing jeans), the patch also should have low saliency score.
The learned Dirichlet parameter γmk reveals the proportion of patches on Im belonging to
topic tk, which can be treated as a pseudo count for the amount of patches falling into each
topic on Im. We then model the popularity of topic tk by accumulating γmk over all images in
the probe set I p and gallery set Ig:
Popularity(tk) = ∑
Im ∈{I p,Ig},tk ∈T ha
The M foreground topics with highest Popularity values is treated as popular human appearance topics, and deployed to form a topic set T pop. In practice, we take M = Kha/2, i.e. 50%
of all human appearance topics with higher popularity scores are considered to be statistically common/typical. The ﬁnal saliency score of patch PA is computed by combining its
prevalence level, the probability of its topic not belonging to a background topic, and being
less popular (atypical) among foreground appearance topics, i.e.
Saliency(PA) = h(Prevalence(PA))−η1 · ∑
Pr(zA = tk|L,H)
Pr(zA = tk|L,H),
0 < η1,η2 < 1
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
where h(x) is a inverse function deﬁned as taking the additive inverse and normalising the
result into the interval. Prevalence(PA) is given by Eqn. (6). The last two terms can be
calculated through Eqn. (2), whilst η1,η2 are their weights to affect the saliency score, determined by cross-validation during our experiment. If one considers that Prevalence(PA) simply measures how likely the exact same patch appears repeatedly across images, its topic’s
popularity takes much larger amounts of patches into consideration. These patches may
even be visually different from PA, but they are inherently related by the same topic. This
model avoids the topic being simply data-driven; it also considers more inherent structure of
the large-scaled data. It is worth pointing out that the model of selects two independent
reference training datasets (one for the gallery camera view and another for the probe camera view) and trains many patch-speciﬁc and view-speciﬁc discriminative models: a different
model for every patch of every probe image and every gallery image in order to match the
probe image against a set of gallery images for re-id. In contrast, our method only requires
to train a single model for each camera view given an independent training dataset from that
view. Then only two models are required for all patches of all the probe images and all the
gallery images respectively. Some examples of the saliency maps obtained using our method
are shown in Figure 2.
Figure 2: Saliency maps comparison (left to right): A person image in detected bounding box, GTScomputed background map, GTS-computed saliency map, saliency map computed by the model of
 (green bounding box).
Re-id Matching
Given the saliency score, we adopt the same patch-based image matching scheme of 
to compute a matching score between a set of gallery images and a probe image from an
independent test set. First we build a corresponding pairwise relationship for all the patches
in a probe image IA and a gallery image IB. In each patch pair (image location indexed), one
patch P1 is from IA and the other P2 from IB. More precisely, a pair of (P1, P2) patch is the
nearest neighbour match searched in the proximity of P1 in IB or vice versa P2 in IA. The
matching similarity distance metric is given by s = exp(−d2/2a2), where d is the Euclidean
distance between two patch feature vectors and a is the bandwidth of a Gaussian function.
The overall similarity between the two images is computed by a weighted sum accumulating
all the patch pairs’ similarities weighted by the saliency scores of patches in each pair, i.e. an
accumulation over the quantity Saliency(P1) · s(P1,P2) · Saliency(P2), where P1 and P2 are
two patches in one pair. It is worth pointing out that the published code of utilizes
foreground masks to remove background patches in VIPeR images. The similarity score
between a pair of images is only computed in the foreground region. A similar process of
background removal is adopted by many existing works . Body parts information
are not explored in our experiments.
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
Experiments
Datasets and Settings – We evaluate our method on two widely used benchmark datasets,
VIPeR and iLIDS . The VIPeR dataset contains 632 pedestrian image pairs. Each
pair of images contain the same individual, but were taken from different camera views.
Following the experimental setting of , we randomly choose half of the dataset, i.e.
316 image pairs, as our training sets. On this training set, we train two topic models, one
for each camera view. Among the 316 pairs of training images, we choose 100 pairs as
our reference sets for computing saliency and use one reference set per camera view, same
as . The iLIDS dataset contains 476 images of 119 people. We followed the same single
shot experiment protocol as , i.e. randomly choose all images of p = 50 people as test
set, and use the other images as training set. In the test set, one image per person is chosen
to form a gallery set, while all the remaining images compose a probe set. We run our
experiments for 10 trials with different splits, and report the average of these 10 trials as
our ﬁnal result. The performance is evaluated using the Cumulated Matching Characteristics
(CMC) curves.
Comparison to non-learning and unsupervised learning models – We ﬁrst compare our
GTS model against non-learning based methods, i.e. template matching with a distance
measure. L1-norm and L2-norm distances are used as the baseline models for comparison. Figures 3 and 4 show respectively the results on VIPeR and iLIDS. It is evident that
our method signiﬁcantly outperforms the baseline non-learning methods, e.g. Rank-1 about
150% (VIPeR) and 14% (iLIDS) relative improvement over L1-norm. This suggests that the
unlabelled data indeed helps improve re-id matching accuracy.
Cumulative Matching Characteristic (CMC) Curves − VIPeR dataset
Matches (%)
eSDC_ocsvm
Figure 3: VIPeR test: CMC comparison of unsupervised learning based re-id models.
LMNN-R 
KISSME 
Table 1: VIPeR test: Comparing the GTS model to
supervised learning based models.
Next we compare GTS to a number of contemporary unsupervised learning methods including eSDC_knn , eSDC_ocsvm 1, LDFV and SDALF . Figures 3 and 4
show that our model is clearly superior to LDFV and SDALF, e.g. Rank-1 27% (VIPeR) relative improvement over SDALF. These results show that modelling human saliency gives the
GTS model an advantage over the feature-design based unsupervised learning approaches.
Comparing with eSDC_knn and eSDC_ocsvm, which are also patch based unsupervised
saliency learning methods, the GTS model still shows a notable improvement, e.g. Rank-
1 5% (VIPeR) and 15% (iLIDS) relative improvement over eSDC_ocsvm. Figure 2 sheds
1The results of KNN and OCSVM in our experiments are obtained by running the author published code under
our experiment settings. The results are thus slightly different from those reported in .
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID
some light into why the GTS model outperforms these two models in . It is evident that
a better saliency map is obtained using the GTS model. This is mainly because our topic
model explicitly models human appearance as well as background so that the background
cannot be mistaken as distractions to true foreground local salient region discovery. In contrast, the model of can give false high saliency scores due to confusion with background
regions, while the saliency scores for those real salient regions on those image are pulled
down due to the interference of backgrounds, thus cannot be utilised in the re-id process.
Computationally, the GTS model is also twice as fast to compute when compared to .
Cumulative Matching Characteristic (CMC) Curves − iLIDS dataset
Matches (%)
eSDC_ocsvm
Figure 4: iLIDS test: CMC comparison of unsupervised learning based re-id models.
SDC_knn 
SDC_ocsvm 
Table 2: iLIDS test: Comparing the GTS model
against other unsupervised (top) and supervised
(bottom) learning based models.
Comparison to supervised learning models – We also compared our GTS model against
some recently proposed supervised learning based re-id models. In general, supervised learning of discriminative models are expected to provide better re-id performance due to the use
of labelled information for learning strong discriminative functions, with a high price for
labelling the data. Tables 1 and 2 show results on VIPeR and iLIDS respectively. It is clear
that without using any labelled data for model training, the GTS model is competitive against
these supervised learning methods without the beneﬁt from learning strong discriminative
functions using labelled data. Moreover, the GTS model is able to outperform a number of
the supervised learning models by some notable margins, e.g. Rank-1 20% (VIPeR) and
13% (iLIDS) relative improvement over PRDC, LMNN and KISSME (Tables 1 and 2). This
suggests that the GTS model is scalable to large scale applications when manual annotations
of identity labels across camera views are not available or feasible.
Conclusion
We proposed a novel unsupervised generative saliency learning framework for person reidentiﬁcation. The core of this framework is a probabilistic topic model speciﬁcally designed
for modelling jointly typical human appearance and the surrounding background appearance.
The model can be deployed to simultaneously learn a saliency map and foreground segmentation for a more accurate and scalable person re-identiﬁcation model. Compared with existing unsupervised learning methods, the GTS model improves re-id accuracy signiﬁcantly,
especially on Rank-1. The GTS model is also competitive against the state-of-the-art supervised learning based methods, but without requiring manual labelling of data, resulting in
greater scalability to large scale re-id problems in many practical applications.
WANG ET AL: UNSUPERVISED GENERATIVE TOPIC SALIENCY FOR RE-ID