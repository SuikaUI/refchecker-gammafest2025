Published as a conference paper at ICLR 2015
SEMANTIC IMAGE SEGMENTATION WITH DEEP CON-
VOLUTIONAL NETS AND FULLY CONNECTED CRFS
Liang-Chieh Chen
Univ. of California, Los Angeles
 
George Papandreou ∗
Google Inc.
 
Iasonas Kokkinos
CentraleSup´elec and INRIA
 
Kevin Murphy
Google Inc.
 
Alan L. Yuille
Univ. of California, Los Angeles
 
Deep Convolutional Neural Networks (DCNNs) have recently shown state of the
art performance in high level vision tasks, such as image classiﬁcation and object detection. This work brings together methods from DCNNs and probabilistic
graphical models for addressing the task of pixel-level classiﬁcation (also called
”semantic image segmentation”). We show that responses at the ﬁnal layer of
DCNNs are not sufﬁciently localized for accurate object segmentation. This is
due to the very invariance properties that make DCNNs good for high level tasks.
We overcome this poor localization property of deep networks by combining the
responses at the ﬁnal DCNN layer with a fully connected Conditional Random
Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment
boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic
image segmentation task, reaching 71.6% IOU accuracy in the test set. We show
how these results can be obtained efﬁciently: Careful network re-purposing and a
novel application of the ’hole’ algorithm from the wavelet community allow dense
computation of neural net responses at 8 frames per second on a modern GPU.
INTRODUCTION
Deep Convolutional Neural Networks (DCNNs) had been the method of choice for document recognition since LeCun et al. , but have only recently become the mainstream of high-level vision
research. Over the past two years DCNNs have pushed the performance of computer vision systems to soaring heights on a broad array of high-level problems, including image classiﬁcation
 , object detection , ﬁne-grained categorization , among others. A common theme in these works is that DCNNs trained in an end-to-end
manner deliver strikingly better results than systems relying on carefully engineered representations,
such as SIFT or HOG features. This success can be partially attributed to the built-in invariance of
DCNNs to local image transformations, which underpins their ability to learn hierarchical abstractions of data . While this invariance is clearly desirable for high-level vision
tasks, it can hamper low-level tasks, such as pose estimation and semantic segmentation - where we want precise localization, rather than abstraction of
spatial details.
There are two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance). The ﬁrst problem relates to the reduction of signal
resolution incurred by the repeated combination of max-pooling and downsampling (‘striding’) performed at every layer of standard DCNNs . Instead, as in Papandreou et al. , we employ the ‘atrous’ (with holes)
algorithm originally developed for efﬁciently computing the undecimated discrete wavelet transform
 . This allows efﬁcient dense computation of DCNN responses in a scheme substantially simpler than earlier solutions to this problem .
The second problem relates to the fact that obtaining object-centric decisions from a classiﬁer requires invariance to spatial transformations, inherently limiting the spatial accuracy of the DCNN
model. We boost our model’s ability to capture ﬁne details by employing a fully-connected Conditional Random Field (CRF). Conditional Random Fields have been broadly used in semantic segmentation to combine class scores computed by multi-way classiﬁers with the low-level information
captured by the local interactions of pixels and edges or
superpixels . Even though works of increased sophistication have been proposed
to model the hierarchical dependency 
and/or high-order dependencies of segments , we use the fully connected pairwise CRF proposed by
Kr¨ahenb¨uhl & Koltun for its efﬁcient computation, and ability to capture ﬁne edge details
while also catering for long range dependencies. That model was shown in Kr¨ahenb¨uhl & Koltun
 to largely improve the performance of a boosting-based pixel-level classiﬁer, and in our work
we demonstrate that it leads to state-of-the-art results when coupled with a DCNN-based pixel-level
classiﬁer.
The three main advantages of our “DeepLab” system are (i) speed: by virtue of the ‘atrous’ algorithm, our dense DCNN operates at 8 fps, while Mean Field Inference for the fully-connected CRF
requires 0.5 second, (ii) accuracy: we obtain state-of-the-art results on the PASCAL semantic segmentation challenge, outperforming the second-best approach of Mostajabi et al. by a margin
of 7.2% and (iii) simplicity: our system is composed of a cascade of two fairly well-established modules, DCNNs and CRFs.
RELATED WORK
Our system works directly on the pixel representation, similarly to Long et al. . This is in contrast to the two-stage approaches that are now most common in semantic segmentation with DCNNs:
such techniques typically use a cascade of bottom-up image segmentation and DCNN-based region
classiﬁcation, which makes the system commit to potential errors of the front-end segmentation system. For instance, the bounding box proposals and masked regions delivered by are used in Girshick et al. and as inputs
to a DCNN to introduce shape information into the classiﬁcation process. Similarly, the authors of
Mostajabi et al. rely on a superpixel representation. A celebrated non-DCNN precursor to
these works is the second order pooling method of which also assigns labels
to the regions proposals delivered by . Understanding the perils
of committing to a single segmentation, the authors of Cogswell et al. build on to explore a diverse set of CRF-based segmentation proposals, computed also by
 . These segmentation proposals are then re-ranked according to a
DCNN trained in particular for this reranking task. Even though this approach explicitly tries to
handle the temperamental nature of a front-end segmentation algorithm, there is still no explicit ex-
Published as a conference paper at ICLR 2015
ploitation of the DCNN scores in the CRF-based segmentation algorithm: the DCNN is only applied
post-hoc, while it would make sense to directly try to use its results during segmentation.
Moving towards works that lie closer to our approach, several other researchers have considered
the use of convolutionally computed DCNN features for dense image labeling. Among the ﬁrst
have been Farabet et al. who apply DCNNs at multiple image resolutions and then employ a
segmentation tree to smooth the prediction results; more recently, Hariharan et al. propose to
concatenate the computed inter-mediate feature maps within the DCNNs for pixel classiﬁcation, and
Dai et al. propose to pool the inter-mediate feature maps by region proposals. Even though
these works still employ segmentation algorithms that are decoupled from the DCNN classiﬁer’s
results, we believe it is advantageous that segmentation is only used at a later stage, avoiding the
commitment to premature decisions.
More recently, the segmentation-free techniques of directly apply DCNNs to the whole image in a sliding window fashion, replacing the last fully connected layers of a DCNN by convolutional layers. In order to deal with the spatial localization
issues outlined in the beginning of the introduction, Long et al. upsample and concatenate
the scores from inter-mediate feature maps, while Eigen & Fergus reﬁne the prediction result
from coarse to ﬁne by propagating the coarse results to another DCNN.
The main difference between our model and other state-of-the-art models is the combination of
pixel-level CRFs and DCNN-based ‘unary terms’. Focusing on the closest works in this direction,
Cogswell et al. use CRFs as a proposal mechanism for a DCNN-based reranking system,
while Farabet et al. treat superpixels as nodes for a local pairwise CRF and use graph-cuts for
discrete inference; as such their results can be limited by errors in superpixel computations, while ignoring long-range superpixel dependencies. Our approach instead treats every pixel as a CRF node,
exploits long-range dependencies, and uses CRF inference to directly optimize a DCNN-driven cost
function. We note that mean ﬁeld had been extensively studied for traditional image segmentation/edge detection tasks, e.g., , but recently Kr¨ahenb¨uhl & Koltun showed that the inference can be very efﬁcient for
fully connected CRF and particularly effective in the context of semantic segmentation.
After the ﬁrst version of our manuscript was made publicly available, it came to our attention that
two other groups have independently and concurrently pursued a very similar direction, combining
DCNNs and densely connected CRFs . There are several
differences in technical aspects of the respective models. Bell et al. focus on the problem
of material classiﬁcation, while Zheng et al. unroll the CRF mean-ﬁeld inference steps to
convert the whole system into an end-to-end trainable feed-forward network.
We have updated our proposed “DeepLab” system with much improved methods and results in our
latest work . We refer the interested reader to the paper for details.
CONVOLUTIONAL NEURAL NETWORKS FOR DENSE IMAGE LABELING
Herein we describe how we have re-purposed and ﬁnetuned the publicly available Imagenetpretrained state-of-art 16-layer classiﬁcation network of (VGG-16)
into an efﬁcient and effective dense feature extractor for our dense semantic image segmentation
EFFICIENT DENSE SLIDING WINDOW FEATURE EXTRACTION WITH THE HOLE
Dense spatial score evaluation is instrumental in the success of our dense CNN feature extractor. As
a ﬁrst step to implement this, we convert the fully-connected layers of VGG-16 into convolutional
ones and run the network in a convolutional fashion on the image at its original resolution. However
this is not enough as it yields very sparsely computed detection scores (with a stride of 32 pixels). To
compute scores more densely at our target stride of 8 pixels, we develop a variation of the method
previously employed by Giusti et al. ; Sermanet et al. . We skip subsampling after
the last two max-pooling layers in the network of Simonyan & Zisserman and modify the
convolutional ﬁlters in the layers that follow them by introducing zeros to increase their length . We can implement this
more efﬁciently by keeping the ﬁlters intact and instead sparsely sample the feature maps on which
they are applied on using an input stride of 2 or 4 pixels, respectively. This approach, illustrated
in Fig. 1 is known as the ‘hole algorithm’ (‘atrous algorithm’) and has been developed before for
efﬁcient computation of the undecimated wavelet transform . We have implemented
this within the Caffe framework by adding to the im2col function (it converts multichannel feature maps to vectorized patches) the option to sparsely sample the underlying feature
map. This approach is generally applicable and allows us to efﬁciently compute dense CNN feature
maps at any target subsampling rate without introducing any approximations.
We ﬁnetune the model weights of the Imagenet-pretrained VGG-16 network to adapt it to the image
classiﬁcation task in a straightforward fashion, following the procedure of Long et al. . We
replace the 1000-way Imagenet classiﬁer in the last layer of VGG-16 with a 21-way one. Our
loss function is the sum of cross-entropy terms for each spatial position in the CNN output map
(subsampled by 8 compared to the original image). All positions and labels are equally weighted in
the overall loss function. Our targets are the ground truth labels (subsampled by 8). We optimize the
objective function with respect to the weights at all network layers by the standard SGD procedure
of Krizhevsky et al. .
During testing, we need class score maps at the original image resolution. As illustrated in Figure 2
and further elaborated in Section 4.1, the class score maps (corresponding to log-probabilities) are
quite smooth, which allows us to use simple bilinear interpolation to increase their resolution by a
factor of 8 at a negligible computational cost. Note that the method of Long et al. does not
use the hole algorithm and produces very coarse scores (subsampled by a factor of 32) at the CNN
output. This forced them to use learned upsampling layers, signiﬁcantly increasing the complexity
and training time of their system: Fine-tuning our network on PASCAL VOC 2012 takes about 10
hours, while they report a training time of several days (both timings on a modern GPU).
CONTROLLING THE RECEPTIVE FIELD SIZE AND ACCELERATING DENSE
COMPUTATION WITH CONVOLUTIONAL NETS
Another key ingredient in re-purposing our network for dense score computation is explicitly controlling the network’s receptive ﬁeld size. Most recent DCNN-based image recognition methods
rely on networks pre-trained on the Imagenet large-scale classiﬁcation task. These networks typically have large receptive ﬁeld size: in the case of the VGG-16 net we consider, its receptive ﬁeld
is 224×224 (with zero-padding) and 404×404 pixels if the net is applied convolutionally. After
converting the network to a fully convolutional one, the ﬁrst fully connected layer has 4,096 ﬁlters of large 7×7 spatial size and becomes the computational bottleneck in our dense score map
computation.
We have addressed this practical problem by spatially subsampling (by simple decimation) the ﬁrst
FC layer to 4×4 (or 3×3) spatial size. This has reduced the receptive ﬁeld of the network down to
128×128 (with zero-padding) or 308×308 (in convolutional mode) and has reduced computation time
for the ﬁrst FC layer by 2 −3 times. Using our Caffe-based implementation and a Titan GPU, the
resulting VGG-derived network is very efﬁcient: Given a 306×306 input image, it produces 39×39
Published as a conference paper at ICLR 2015
dense raw feature scores at the top of the network at a rate of about 8 frames/sec during testing. The
speed during training is 3 frames/sec. We have also successfully experimented with reducing the
number of channels at the fully connected layers from 4,096 down to 1,024, considerably further
decreasing computation time and memory footprint without sacriﬁcing performance, as detailed in
Section 5. Using smaller networks such as Krizhevsky et al. could allow video-rate test-time
dense feature computation even on light-weight GPUs.
DETAILED BOUNDARY RECOVERY: FULLY-CONNECTED CONDITIONAL
RANDOM FIELDS AND MULTI-SCALE PREDICTION
DEEP CONVOLUTIONAL NETWORKS AND THE LOCALIZATION CHALLENGE
As illustrated in Figure 2, DCNN score maps can reliably predict the presence and rough position
of objects in an image but are less well suited for pin-pointing their exact outline. There is a natural
trade-off between classiﬁcation accuracy and localization accuracy with convolutional networks:
Deeper models with multiple max-pooling layers have proven most successful in classiﬁcation tasks,
however their increased invariance and large receptive ﬁelds make the problem of inferring position
from the scores at their top output levels more challenging.
Recent work has pursued two directions to address this localization challenge. The ﬁrst approach is
to harness information from multiple layers in the convolutional network in order to better estimate
the object boundaries . The second approach is to employ
a super-pixel representation, essentially delegating the localization task to a low-level segmentation
method. This route is followed by the very successful recent method of Mostajabi et al. .
In Section 4.2, we pursue a novel alternative direction based on coupling the recognition capacity
of DCNNs and the ﬁne-grained localization accuracy of fully connected CRFs and show that it is
remarkably successful in addressing the localization challenge, producing accurate semantic segmentation results and recovering object boundaries at a level of detail that is well beyond the reach
of existing methods.
FULLY-CONNECTED CONDITIONAL RANDOM FIELDS FOR ACCURATE LOCALIZATION
Image/G.T.
DCNN output
CRF Iteration 1
CRF Iteration 2
CRF Iteration 10
Figure 2: Score map (input before softmax function) and belief map (output of softmax function) for
Aeroplane. We show the score (1st row) and belief (2nd row) maps after each mean ﬁeld iteration.
The output of last DCNN layer is used as input to the mean ﬁeld inference. Best viewed in color.
Traditionally, conditional random ﬁelds (CRFs) have been employed to smooth noisy segmentation
maps . Typically these models contain energy terms that
couple neighboring nodes, favoring same-label assignments to spatially proximal pixels. Qualitatively, the primary function of these short-range CRFs has been to clean up the spurious predictions
of weak classiﬁers built on top of local hand-engineered features.
Compared to these weaker classiﬁers, modern DCNN architectures such as the one we use in this
work produce score maps and semantic label predictions which are qualitatively different. As illustrated in Figure 2, the score maps are typically quite smooth and produce homogeneous classiﬁcation
results. In this regime, using short-range CRFs can be detrimental, as our goal should be to recover
detailed local structure rather than further smooth it. Using contrast-sensitive potentials is upsampled by bi-linear interpolation. A fully connected CRF is applied
to reﬁne the segmentation result. Best viewed in color.
et al., 2004) in conjunction to local-range CRFs can potentially improve localization but still miss
thin-structures and typically requires solving an expensive discrete optimization problem.
To overcome these limitations of short-range CRFs, we integrate into our system the fully connected
CRF model of Kr¨ahenb¨uhl & Koltun . The model employs the energy function
θij(xi, xj)
where x is the label assignment for pixels. We use as unary potential θi(xi) = −log P(xi), where
P(xi) is the label assignment probability at pixel i as computed by DCNN. The pairwise potential
is θij(xi, xj) = µ(xi, xj) PK
m=1 wm · km(f i, f j), where µ(xi, xj) = 1 if xi ̸= xj, and zero
otherwise (i.e., Potts Model). There is one pairwise term for each pair of pixels i and j in the image
no matter how far from each other they lie, i.e. the model’s factor graph is fully connected. Each km
is the Gaussian kernel depends on features (denoted as f) extracted for pixel i and j and is weighted
by parameter wm. We adopt bilateral position and color terms, speciﬁcally, the kernels are
−||pi −pj||2
−||Ii −Ij||2
−||pi −pj||2
where the ﬁrst kernel depends on both pixel positions (denoted as p) and pixel color intensities
(denoted as I), and the second kernel only depends on pixel positions. The hyper parameters σα, σβ
and σγ control the “scale” of the Gaussian kernels.
Crucially, this model is amenable to efﬁcient approximate probabilistic inference . The message passing updates under a fully decomposable mean ﬁeld approximation b(x) = Q
i bi(xi) can be expressed as convolutions with a Gaussian kernel in feature space.
High-dimensional ﬁltering algorithms signiﬁcantly speed-up this computation
resulting in an algorithm that is very fast in practice, less that 0.5 sec on average for Pascal VOC
images using the publicly available implementation of .
MULTI-SCALE PREDICTION
Following the promising recent results of we have also
explored a multi-scale prediction method to increase the boundary localization accuracy. Specifically, we attach to the input image and the output of each of the ﬁrst four max pooling layers a
two-layer MLP (ﬁrst layer: 128 3x3 convolutional ﬁlters, second layer: 128 1x1 convolutional ﬁlters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate
feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels. We only adjust
the newly added weights, keeping the other network parameters to the values learned by the method
of Section 3. As discussed in the experimental section, introducing these extra direct connections
from ﬁne-resolution layers improves localization performance, yet the effect is not as dramatic as
the one obtained with the fully-connected CRF.
Published as a conference paper at ICLR 2015
mean IOU (%)
DeepLab-CRF
DeepLab-MSc
DeepLab-MSc-CRF
DeepLab-7x7
DeepLab-CRF-7x7
DeepLab-LargeFOV
DeepLab-CRF-LargeFOV
DeepLab-MSc-LargeFOV
DeepLab-MSc-CRF-LargeFOV
mean IOU (%)
TTI-Zoomout-16
DeepLab-CRF
DeepLab-MSc-CRF
DeepLab-CRF-7x7
DeepLab-CRF-LargeFOV
DeepLab-MSc-CRF-LargeFOV
Table 1: (a) Performance of our proposed models on the PASCAL VOC 2012 ‘val’ set (with training
in the augmented ‘train’ set). The best performance is achieved by exploiting both multi-scale
features and large ﬁeld-of-view. (b) Performance of our proposed models (with training in the
augmented ‘trainval’ set) compared to other state-of-art methods on the PASCAL VOC 2012 ‘test’
EXPERIMENTAL EVALUATION
We test our DeepLab model on the PASCAL VOC 2012 segmentation benchmark , consisting of 20 foreground object classes and one background class. The
original dataset contains 1, 464, 1, 449, and 1, 456 images for training, validation, and testing, respectively. The dataset is augmented by the extra annotations provided by Hariharan et al. ,
resulting in 10, 582 training images. The performance is measured in terms of pixel intersectionover-union (IOU) averaged across the 21 classes.
We adopt the simplest form of piecewise training, decoupling the DCNN and CRF training stages, assuming the unary terms provided by the DCNN are ﬁxed during CRF training.
For DCNN training we employ the VGG-16 network which has been pre-trained on ImageNet. We
ﬁne-tuned the VGG-16 network on the VOC 21-way pixel-classiﬁcation task by stochastic gradient
descent on the cross-entropy loss function, as described in Section 3.1. We use a mini-batch of 20
images and initial learning rate of 0.001 (0.01 for the ﬁnal classiﬁer layer), multiplying the learning
rate by 0.1 at every 2000 iterations. We use momentum of 0.9 and a weight decay of 0.0005.
After the DCNN has been ﬁne-tuned, we cross-validate the parameters of the fully connected CRF
model in Eq. (2) along the lines of Kr¨ahenb¨uhl & Koltun . We use the default values of
w2 = 3 and σγ = 3 and we search for the best values of w1, σα, and σβ by cross-validation on a
small subset of the validation set (we use 100 images). We employ coarse-to-ﬁne search scheme.
Speciﬁcally, the initial search range of the parameters are w1 ∈ , σα ∈[50 : 10 : 100] and
σβ ∈[3 : 1 : 10] (MATLAB notation), and then we reﬁne the search step sizes around the ﬁrst
round’s best values. We ﬁx the number of mean ﬁeld iterations to 10 for all reported experiments.
Evaluation on Validation set
We conduct the majority of our evaluations on the PASCAL ‘val’
set, training our model on the augmented PASCAL ‘train’ set. As shown in Tab. 1 (a), incorporating
the fully connected CRF to our model (denoted by DeepLab-CRF) yields a substantial performance
boost, about 4% improvement over DeepLab. We note that the work of Kr¨ahenb¨uhl & Koltun
 improved the 27.6% result of TextonBoost to 29.1%, which makes the
improvement we report here (from 59.8% to 63.7%) all the more impressive.
Turning to qualitative results, we provide visual comparisons between DeepLab and DeepLab-CRF
in Fig. 7. Employing a fully connected CRF signiﬁcantly improves the results, allowing the model
to accurately capture intricate object boundaries.
Multi-Scale features
We also exploit the features from the intermediate layers, similar to Hariharan et al. ; Long et al. . As shown in Tab. 1 (a), adding the multi-scale features to our
Published as a conference paper at ICLR 2015
kernel size
input stride
receptive ﬁeld
# parameters
mean IOU (%)
Training speed (img/sec)
DeepLab-CRF-7x7
DeepLab-CRF
DeepLab-CRF-4x4
DeepLab-CRF-LargeFOV
Table 2: Effect of Field-Of-View. We show the performance (after CRF) and training speed on the
PASCAL VOC 2012 ‘val’ set as the function of (1) the kernel size of ﬁrst fully connected layer, (2)
the input stride value employed in the atrous algorithm.
DeepLab model (denoted as DeepLab-MSc) improves about 1.5% performance, and further incorporating the fully connected CRF (denoted as DeepLab-MSc-CRF) yields about 4% improvement.
The qualitative comparisons between DeepLab and DeepLab-MSc are shown in Fig. 4. Leveraging
the multi-scale features can slightly reﬁne the object boundaries.
Field of View
The ‘atrous algorithm’ we employed allows us to arbitrarily control the Field-of-
View (FOV) of the models by adjusting the input stride, as illustrated in Fig. 1. In Tab. 2, we
experiment with several kernel sizes and input strides at the ﬁrst fully connected layer. The method,
DeepLab-CRF-7x7, is the direct modiﬁcation from VGG-16 net, where the kernel size = 7×7 and
input stride = 4. This model yields performance of 67.64% on the ‘val’ set, but it is relatively slow
(1.44 images per second during training). We have improved model speed to 2.9 images per second
by reducing the kernel size to 4×4. We have experimented with two such network variants with
different FOV sizes, DeepLab-CRF and DeepLab-CRF-4x4; the latter has large FOV (i.e., large
input stride) and attains better performance. Finally, we employ kernel size 3×3 and input stride =
12, and further change the ﬁlter sizes from 4096 to 1024 for the last two layers. Interestingly, the
resulting model, DeepLab-CRF-LargeFOV, matches the performance of the expensive DeepLab-
CRF-7x7. At the same time, it is 3.36 times faster to run and has signiﬁcantly fewer parameters
(20.5M instead of 134.3M).
The performance of several model variants is summarized in Tab. 1, showing the beneﬁt of exploiting
multi-scale features and large FOV.
Figure 4: Incorporating multi-scale features improves the boundary segmentation. We show the
results obtained by DeepLab and DeepLab-MSc in the ﬁrst and second row, respectively. Best
viewed in color.
Mean Pixel IOU along Object Boundaries
To quantify the accuracy of the proposed model near
object boundaries, we evaluate the segmentation accuracy with an experiment similar to Kohli et al.
 ; Kr¨ahenb¨uhl & Koltun . Speciﬁcally, we use the ‘void’ label annotated in val set,
which usually occurs around object boundaries. We compute the mean IOU for those pixels that
are located within a narrow band (called trimap) of ‘void’ labels. As shown in Fig. 5, exploiting
the multi-scale features from the intermediate layers and reﬁning the segmentation results by a fully
connected CRF signiﬁcantly improve the results around object boundaries.
Comparison with State-of-art
In Fig. 6, we qualitatively compare our proposed model, DeepLab-
CRF, with two state-of-art models: FCN-8s and TTI-Zoomout-16 on the ‘val’ set (the results are extracted from their papers). Our model is able to
capture the intricate object boundaries.
Published as a conference paper at ICLR 2015
Pixelwise Accuracy (%)
Trimap Width (pixels)
DL−MSc−CRF
DeepLab−CRF
DeepLab−MSc
mean IOU (%)
Trimap Width (pixels)
DL−MSc−CRF
DeepLab−CRF
DeepLab−MSc
Figure 5: (a) Some trimap examples (top-left: image. top-right: ground-truth. bottom-left: trimap
of 2 pixels. bottom-right: trimap of 10 pixels). Quality of segmentation result within a band around
the object boundaries for the proposed methods. (b) Pixelwise accuracy. (c) Pixel mean IOU.
(a) FCN-8s vs. DeepLab-CRF
(b) TTI-Zoomout-16 vs. DeepLab-CRF
Figure 6: Comparisons with state-of-the-art models on the val set. First row: images. Second row:
ground truths. Third row: other recent models (Left: FCN-8s, Right: TTI-Zoomout-16). Fourth
row: our DeepLab-CRF. Best viewed in color.
Reproducibility
We have implemented the proposed methods by extending the excellent Caffe
framework . We share our source code, conﬁguration ﬁles, and trained models that
allow reproducing the results in this paper at a companion web site 
deeplab/deeplab-public.
Test set results
Having set our model choices on the validation set, we evaluate our model variants
on the PASCAL VOC 2012 ofﬁcial ‘test’ set. As shown in Tab. 3, our DeepLab-CRF and DeepLab-
MSc-CRF models achieve performance of 66.4% and 67.1% mean IOU1, respectively. Our models
outperform all the other state-of-the-art models , FCN-8s , and MSRA-CFM ). When we increase the FOV
of the models, DeepLab-CRF-LargeFOV yields performance of 70.3%, the same as DeepLab-CRF-
7x7, while its training speed is faster. Furthermore, our best model, DeepLab-MSc-CRF-LargeFOV,
attains the best performance of 71.6% by employing both multi-scale features and large FOV.
1 
challengeid=11&compid=6
Published as a conference paper at ICLR 2015
Figure 7: Visualization results on VOC 2012-val. For each row, we show the input image, the
segmentation result delivered by the DCNN (DeepLab), and the reﬁned segmentation result of the
Fully Connected CRF (DeepLab-CRF). We show our failure modes in the last three rows. Best
viewed in color.
Published as a conference paper at ICLR 2015
bkg aero bike bird boat bottle
chair cow table dog horse mbike person plant sheep sofa train
75.7 26.7 69.5 48.8
81.0 69.2 73.3 30.0 68.7 51.5 69.1
44.4 58.9 53.5
76.8 34.2 68.9 49.4
75.3 74.7 77.6 21.4 62.5 46.8 71.8
37.4 70.9 55.1
TTI-Zoomout-16
89.8 81.9 35.1 78.2 57.4
80.5 74.0 79.8 22.4 69.6 53.7 74.0
40.2 68.9 55.3
DeepLab-CRF
92.1 78.4 33.1 78.2 55.6
81.3 75.5 78.6 25.3 69.2 52.7 75.2
45.1 73.3 56.2
DeepLab-MSc-CRF
92.6 80.4 36.8 77.4 55.2
81.5 77.5 78.9 27.1 68.2 52.7 74.3
45.2 72.7 59.3
DeepLab-CRF-7x7
92.8 83.9 36.6 77.5 58.4
84.6 79.7 83.1 29.5 74.6 59.3 78.9
49.2 78.0 60.7
DeepLab-CRF-LargeFOV
92.6 83.5 36.6 82.5 62.3
85.4 78.5 83.7 30.4 72.9 60.4 78.5
48.8 73.7 63.3
DeepLab-MSc-CRF-LargeFOV
93.1 84.4 54.5 81.5 63.6
85.1 79.1 83.4 30.7 74.1 59.8 79.0
50.4 73.1 63.7
Table 3: Labeling IOU (%) on the PASCAL VOC 2012 test set, using the trainval set for training.
DISCUSSION
Our work combines ideas from deep convolutional neural networks and fully-connected conditional
random ﬁelds, yielding a novel method able to produce semantically accurate predictions and detailed segmentation maps, while being computationally efﬁcient. Our experimental results show that
the proposed method signiﬁcantly advances the state-of-art in the challenging PASCAL VOC 2012
semantic image segmentation task.
There are multiple aspects in our model that we intend to reﬁne, such as fully integrating its two
main components (CNN and CRF) and train the whole system in an end-to-end fashion, similar to
Kr¨ahenb¨uhl & Koltun ; Chen et al. ; Zheng et al. . We also plan to experiment
with more datasets and apply our method to other sources of data such as depth maps or videos. Recently, we have pursued model training with weakly supervised annotations, in the form of bounding
boxes or image-level labels .
At a higher level, our work lies in the intersection of convolutional neural networks and probabilistic
graphical models. We plan to further investigate the interplay of these two powerful classes of
methods and explore their synergistic potential for solving challenging computer vision tasks.
ACKNOWLEDGMENTS
This work was partly supported by ARO 62250-CS, NIH Grant 5R01EY022247-03, EU Project
RECONFIG FP7-ICT-600825 and EU Project MOBOT FP7-ICT-2011-600796. We also gratefully
acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.
We would like to thank the anonymous reviewers for their detailed comments and constructive feedback.
PAPER REVISIONS
Here we present the list of major paper revisions for the convenience of the readers.
Submission to ICLR 2015. Introduces the model DeepLab-CRF, which attains the performance
of 66.4% on PASCAL VOC 2012 test set.
Rebuttal for ICLR 2015. Adds the model DeepLab-MSc-CRF, which incorporates multi-scale
features from the intermediate layers. DeepLab-MSc-CRF yields the performance of 67.1% on
PASCAL VOC 2012 test set.
Camera-ready for ICLR 2015. Experiments with large Field-Of-View. On PASCAL VOC 2012
test set, DeepLab-CRF-LargeFOV achieves the performance of 70.3%. When exploiting both mutliscale features and large FOV, DeepLab-MSc-CRF-LargeFOV attains the performance of 71.6%.
Reference to our updated “DeepLab” system with much improved results.