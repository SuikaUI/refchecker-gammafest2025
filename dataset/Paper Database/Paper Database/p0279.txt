Mach Learn 81: 121–148
DOI 10.1007/s10994-010-5188-5
The security of machine learning
Marco Barreno · Blaine Nelson · Anthony D. Joseph ·
J.D. Tygar
Received: 8 April 2008 / Revised: 15 April 2010 / Accepted: 15 April 2010 / Published online: 20 May 2010
© The Author(s) 2010. This article is published with open access at Springerlink.com
Abstract Machine learning’s ability to rapidly evolve to changing and complex situations
has helped it become a fundamental tool for computer security. That adaptability is also
a vulnerability: attackers can exploit machine learning systems. We present a taxonomy
identifying and analyzing attacks against machine learning systems. We show how these
classes inﬂuence the costs for the attacker and defender, and we give a formal structure
deﬁning their interaction. We use our framework to survey and analyze the literature of
attacks against machine learning systems. We also illustrate our taxonomy by showing how
it can guide attacks against SpamBayes, a popular statistical spam ﬁlter. Finally, we discuss
how our taxonomy suggests new lines of defenses.
Keywords Security · Adversarial learning · Adversarial environments
1 Introduction
If we hope to use machine learning as a general tool for computer applications, it is incumbent on us to investigate how well machine learning performs under adversarial conditions.
When a learning algorithm performs well in adversarial conditions, we say it is an algorithm for secure learning. This raises the natural question: how do we evaluate the quality
of a learning system and determine whether it satisﬁes requirements for secure learning?
Editors: Pavel Laskov and Richard Lippmann.
M. Barreno () · B. Nelson · A.D. Joseph · J.D. Tygar
Computer Science Division, University of California, Berkeley, CA 94720-1776, USA
e-mail: 
e-mail: 
A.D. Joseph
e-mail: 
J.D. Tygar
e-mail: 
Mach Learn 81: 121–148
Machine learning advocates have proposed learning-based systems for a variety of security applications, including spam detection and network intrusion detection. Their vision is
that machine learning will allow a system to respond to evolving real-world inputs, both hostile and benign, and learn to reject undesirable behavior. The danger is that an attacker will
attempt to exploit the adaptive aspect of a machine learning system to cause it to fail. Failure consists of causing the learning system to produce errors: if it misidentiﬁes hostile input
as benign, hostile input is permitted through the security barrier; if it misidentiﬁes benign
input as hostile, desired input is rejected. The adversarial opponent has a powerful weapon:
the ability to design training data that will cause the learning system to produce rules that
misidentify inputs. If users detect the failure, they may lose conﬁdence in the system and
abandon it. If users do not detect the failure, then the risks can be even greater.
It is well established in computer security that evaluating a system involves a continual process of ﬁrst, determining classes of attacks on the system; second, evaluating
the resilience of the system against those attacks; and third, strengthening the system
against those classes of attacks. Our paper follows exactly this model in evaluating secure
First, we identify different classes of attacks on machine learning systems (Sect. 2).
While many researchers have considered particular attacks on machine learning systems,
previous research has not presented a comprehensive view of attacks. In particular, we show
that there are at least three interesting dimensions to potential attacks against learning systems: (1) they may be Causative in that they alter the training process, or they may be
Exploratory and exploit existing weaknesses; (2) they may be attacks on Integrity aimed at
false negatives (allowing hostile input into a system) or they may be attacks on Availability aimed at false positives (preventing benign input from entering a system); and (3) they
may be Targeted at a particular input or they may be Indiscriminate in which inputs fail.
Each of these dimensions operates independently, so we have at least eight distinct classes
of attacks on machine learning system. We can view secure learning as a game between
an attacker and a defender; the taxonomy determines the structure of the game and cost
Second, we consider how resilient existing systems are against these attacks (Sect. 3).
There has been a rich set of work in recent years on secure learning systems, and we evaluate
many attacks against machine learning systems and proposals for making systems secure
against attacks. Our analysis describes these attacks in terms of our taxonomy and secure
learning game, demonstrating that our framework captures the salient aspects of each attack.
Third, we investigate some potential defenses against these attacks (Sect. 4). Here the
work is more tentative, and it is clear that much remains to be done, but we discuss a variety
of techniques that show promise for defending against different types of attacks.
Finally, we illustrate our different classes of attacks by considering a contemporary machine learning application, the SpamBayes spam detection system (Sect. 5). We construct
realistic, effective attacks by considering different aspects of the threat model according to
our taxonomy, and we discuss a defense that mitigates some of the attacks.
This paper provides system designers with a framework for evaluating machine learning
systems for security applications (illustrated with our evaluation of SpamBayes) and suggests directions for developing highly robust secure learning systems. Our research not only
proposes a common language for thinking and writing about secure learning, but goes beyond that to show how our framework works, both in algorithm design and in real system
evaluation. We present an essential ﬁrst step if machine learning is to reach its potential as a
tool for use in real systems in potentially adversarial environments.
Mach Learn 81: 121–148
1.1 Notation
We focus on binary classiﬁcation for security applications, in which a defender attempts
to separate instances of input (data points), some or all of which come from a malicious
attacker, into harmful and benign classes. This setting covers many interesting security applications, such as host and network intrusion detection, virus and worm detection, and spam
ﬁltering. In detecting malicious activity, the positive class (label 1) indicates malicious intrusion instances while the negative class (label 0) indicates benign normal instances. A classi-
ﬁcation error is a false positive (FP) if a normal instance is classiﬁed as positive and a false
negative (FN) if an intrusion instance is classiﬁed as negative.
It may be interesting as well to consider cases where a classiﬁer has more than two
classes, or even a real-valued output. Indeed, the spam ﬁlter SpamBayes, which we consider
in our experiments in Sect. 5, uses three labels so it can explicitly label some messages
unsure. However, generalizing the analysis of errors to more than two classes is not straightforward, and furthermore most systems in practice make a single fundamental distinction
(for example, spam messages that the user will never see vs. non-spam and unsure messages
that the user will see). For these reasons, and in keeping with common practice in the literature, we limit our analysis to binary classiﬁcation and leave extension to the multi-class or
real-valued cases as future work.
In the supervised classiﬁcation problem, the learner trains on a dataset of N instances,
X = {(x,y) | x ∈X,y ∈Y}N, given an instance space X and the label space Y = {0,1}.
Given some hypothesis class Ω, the goal is to learn a classiﬁcation hypothesis (classiﬁer)
f ∗∈Ω to minimize errors when predicting labels for new data, or if our model includes a
cost function over errors, to minimize the total cost of errors. The cost function assigns a numeric cost to each combination of data instance, true label, and classiﬁer label. The defender
chooses a procedure H, or learning algorithm, for selecting hypotheses. The classiﬁer may
periodically interleave training steps with the evaluation, retraining on some or all of the
accumulated old and new data. In adversarial environments, the attacker controls some of
the data, which may be used for training. We assume that the learner has some way to get the
true labels for its training data and for the purpose of computing cost; the true label might
come from manual classiﬁcation of a training set or from observing the effect of instances
on a test system, for example.
The procedure can be any method of selecting a hypothesis; in statistical machine learning, a common procedure is (regularized) empirical risk minimization. This procedure is an
optimization problem where the objective function has an empirical risk term and a regularization term. Since true cost is often not representable precisely and efﬁciently, we calculate
risk as the expected loss given by a loss function ℓthat approximates true cost; the regularization term ρ captures some notion of hypothesis complexity to prevent overﬁtting the
training data, using a weight λ to quantify the trade-off. This procedure ﬁnds the hypothesis
minimizing:
f ∗= argmin
ℓ(y,f (x)) + λρ(f )
Many learning methods make a stationarity assumption: training data and evaluation
data are drawn from the same distribution. This assumption allows us to minimize the risk
on the training set as a surrogate for risk on the evaluation data, since evaluation data are
not known at training time. However, real-world sources of data often are not stationary
and, even worse, attackers can easily break the stationarity assumption with some control of
Mach Learn 81: 121–148
Table 1 Notation in this paper
Space of data instances
Space of data labels; for classiﬁcation Y = {0,1}
Space of distributions over (X × Y)
Space of hypotheses f : X →Y
Training distribution
Evaluation distribution
Distribution for training and evaluation (Sect. 4.2.3)
Data instance
Data label
X,E,Z,C,Ti,Qi ∈(X × Y)N
H : (X × Y)N →Ω
Procedure for selecting hypothesis
AT ,AE : X N × Ω →D
Procedures for selecting distribution
ℓ: Y × Y →R0+
Loss function
C : X × Y × Y →R
Cost function
Hypothesis (classiﬁer)
f ∗: X →Y
Best hypothesis
Number of data points
Number of repetitions of a game
Number of experts (Sect. 4.2.3)
Trade-off parameter for regularized risk minimization
either training or evaluation instances. Analyzing and strengthening learning methods in the
face of a broken stationarity assumption is the crux of the secure learning problem.
We model attacks on machine learning systems as a game between two players, the
attacker and the defender. The game consists of a series of moves, or steps. Each move
encapsulates a choice by one of the players: the attacker alters or selects data; the defender
chooses a training procedure for selecting the classiﬁcation hypothesis.
Table 1 summarizes the notation we use in this paper.
2 Framework
2.1 Security analysis
Properly analyzing the security of a system requires identifying security goals and a threat
model. Security is concerned with protecting assets from attackers. A security goal is a
requirement that, if violated, results in the partial or total compromise of an asset. A threat
model is a proﬁle of attackers, describing motivation and capabilities. Here we analyze the
security goals and threat model for machine learning systems.
Classiﬁers are used to make distinctions that advance security goals. For example, a virus
detection system has the goal of reducing susceptibility to virus infection, either by detecting the virus in transit prior to infection or by detecting an extant infection to expunge.
Another example is an intrusion detection system (IDS), which identiﬁes compromised systems, usually by detecting malicious trafﬁc to and from the system or by detecting suspicious
Mach Learn 81: 121–148
behavior in the system. A closely related concept is the intrusion prevention system (IPS),
which detects intrusion attempts and then acts to prevent them from succeeding.
In this section we describe security goals and a threat model that are speciﬁc to machine
learning systems.
2.1.1 Security goals
In a security context the classiﬁer’s purpose is to classify malicious events and prevent them
from interfering with system operations. We split this general learning goal into two goals:
– Integrity goal: To prevent attackers from reaching system assets.
– Availability goal: To prevent attackers from interfering with normal operation.
There is a clear connection between false negatives and violation of the integrity goal: malicious instances that pass through the classiﬁer can wreak havoc. Likewise, false positives
tend to violate the availability goal because the learner itself denies benign instances.
2.1.2 Threat model
Attacker goal/incentives
In general the attacker wants to access system assets (typically
with false negatives) or deny normal operation (usually with false positives). For example, a
virus author wants viruses to pass through the ﬁlter and take control of the protected system
(a false negative). On the other hand, an unscrupulous merchant may want sales trafﬁc to a
competitor’s web store to be blocked as intrusions (false positives).
We assume that the attacker and defender each have a cost function that assigns a cost
to each labeling for any given instance. Cost can be positive or negative; a negative cost
is a beneﬁt. It is usually the case that low cost for the attacker parallels high cost for the
defender and vice-versa; the attacker and defender would not be adversaries if their goals
were aligned. In this paper, unless otherwise stated, for ease of exposition we assume that
every cost for the defender corresponds to a similar beneﬁt for the attacker and vice-versa.
This assumption is not essential to our arguments, which extend easily to arbitrary cost
functions. We take the defender’s point of view, so we use “high-cost” to mean high positive
cost for the defender.
Attacker capabilities
We assume that the attacker has knowledge of the training algorithm,
and in many cases partial or complete information about the training set, such as its distribution. For example, the attacker may have the ability to eavesdrop on all network trafﬁc
over the period of time in which the learner gathers training data. The attacker may be able
to modify or generate data used in training; we consider cases in which the attacker can
and cannot control some of the learner’s training data. While we think that this is the most
accurate assumption for most cases, if we do err, we wish to follow the common practice in
computer security research of erring on the side of overestimating the attacker’s capabilities
rather than underestimating them.
In general we assume the attacker can generate arbitrary instances; however, many settings do impose signiﬁcant restrictions on the instances generated by the attacker. For example, when the learner trains on data from the attacker, sometimes it is safe to assume that the
attacker cannot choose the label for training, such as when training data is carefully hand
labeled. As another example, an attacker may have complete control over data packets being
sent from the attack source, but routers in transit may add to or alter the packets as well as
affect their timing and arrival order.
When the attacker controls training data, an important limitation to consider is what
fraction of the training data the attacker can control and to what extent. If the attacker has
Mach Learn 81: 121–148
arbitrary control over 100% of the training data, it is difﬁcult to see how the learner can learn
anything useful; however, even in such cases there are learning strategies that can make the
attacker’s task more difﬁcult (see Sect. 4.2.3). We primarily examine intermediate cases and
explore how much inﬂuence is required for the attacker to defeat the learning procedure.
2.2 Taxonomy
We present a taxonomy categorizing attacks against learning systems along three axes:
– Causative attacks inﬂuence learning with control over training data.
– Exploratory attacks exploit misclassiﬁcations but do not affect training.
SECURITY VIOLATION
– Integrity attacks compromise assets via false negatives.
– Availability attacks cause denial of service, usually via false positives.
SPECIFICITY
– Targeted attacks focus on a particular instance.
– Indiscriminate attacks encompass a wide class of instances.
The ﬁrst axis describes the capability of the attacker: whether (a) the attacker has the
ability to inﬂuence the training data that is used to construct the classiﬁer (a Causative
attack) or (b) the attacker does not inﬂuence the learned classiﬁer, but can send new instances
to the classiﬁer and possibly observe its decisions on these carefully crafted instances (an
Exploratory attack).
The second axis indicates the type of security violation the attacker causes: either (a) allowing harmful instances to slip through the ﬁlter as false negatives (an Integrity violation);
or (b) creating a denial of service in which benign instances are incorrectly ﬁltered as false
positives (an Availability violation).
The third axis refers to how speciﬁc the attacker’s intention is: whether (a) the attack is
highly Targeted to degrade the classiﬁer’s performance on one particular instance or (b) the
attack aims to cause the classiﬁer to fail in an Indiscriminate fashion on a broad class of
instances. Each axis, especially this one, is actually a spectrum of choices.
A preliminary version of this taxonomy appears in previous work .
Here we extend the framework and show how the taxonomy shapes the game played by the
attacker and defender (see Sect. 2.4). The INFLUENCE axis of the taxonomy determines the
structure of the game and the move sequence. The SPECIFICITY and SECURITY VIOLATION
axes of the taxonomy determine the general shape of the cost function: an Integrity attack
beneﬁts the attacker on false negatives, and therefore focuses high cost (to the defender) on
false negatives, and an Availability attack focuses high cost on false positives; a Targeted
attack focuses high cost only on a small number of instances, while an Indiscriminate attack
spreads high cost over a broad range of instances.
2.3 Examples
Here we give four hypothetical attack scenarios, each with two variants, against a tokenbased spam ﬁlter that uses machine learning, such as the SpamBayes ﬁlter discussed in
Sect. 5. Table 2 summarizes the taxonomy and shows where these examples ﬁt within it.
Mach Learn 81: 121–148
Table 2 Our taxonomy of attacks against machine learning systems, with examples from Sect. 2.3
Availability
The spam foretold: mis-train
The rogue ﬁlter: mis-train ﬁlter to
a particular spam
block a certain message
Indiscriminate
The spam foretold: mis-train any of
The rogue ﬁlter: mis-train ﬁlter to
several spams
broadly block normal email
Exploratory
The shifty spammer: obfuscate
The unwanted reply: ﬂood a particular
a chosen spam
target inbox
Indiscriminate
The shifty spammer: obfuscate
The unwanted reply: ﬂood any of
several target inboxes
This section is meant to give the reader an intuition for how the taxonomy organizes attacks against machine learning systems. There are many practical considerations that may
make attacks difﬁcult. In Sect. 3 we discuss some concrete attacks that have been published
in the literature, which more fully address relevant practical concerns 81: 121–148
benign messages when the defender is collecting training data to train the spam ﬁlter. The
attack messages include both a spam component and benign words, which subsequently also
become erroneously associated with spam. When the learner trains on the attack data, the
spam ﬁlter will start to ﬁlter normal messages as spam. The primary impediment to this
attack is for the attacker to ensure their messages are used to train the ﬁlter, though in many
cases this is realistic since all available messages are used for training.
We more fully explore an attack of this type in Sect. 5.
This attack could be Targeted to block a particular message (see focused attacks in
Sect. 5). On the other hand, it might be Indiscriminate and attempt to block a signiﬁcant
portion of all legitimate messages (see dictionary attacks in Sect. 5).
2.3.3 Exploratory Integrity attack: The shifty spammer
In an Exploratory Integrity attack, the attacker crafts spam so as to evade the classiﬁer without direct inﬂuence over the classiﬁer itself.
Example: an attacker modiﬁes and obfuscates spam, such as by changing headers or by
exchanging common spam words for less common synonyms. If successful, these modiﬁcations prevent the ﬁlter from recognizing the altered spam as malicious, so they are placed
into the user’s inbox (see Sect. 3.3 for additional discussion of these attacks that use good
words to sanitize spam). A practical consideration for this attack is whether the attacker has
a feedback mechanism to observe the success of the evasion. Also, the obfuscated message
may have less value to the spammer (it may be less effective at generating sales), so there
might be a limit on the extent of obfuscation that is practical.
In the Targeted version of this attack, the attacker has a particular spam to get past the
ﬁlter. In the Indiscriminate version, the attacker has no particular preference and can search
for any spam that succeeds, such as by modifying a number of different spam campaigns to
see which modiﬁcations evade the ﬁlter (of course, in this case the attacker must take care
not to appear anomalous simply due to the large number of exploratory instances).
2.3.4 Exploratory Availability attack: The mistaken identity
In an Exploratory Availability attack, the attacker interferes with legitimate operation without inﬂuence over training.
Example: an attacker wishes to interfere with the target’s ability to read legitimate email
messages. The attacker creates a spam campaign in which the target’s email address appears
as the From: address of the spam messages. The target will receive a ﬂood of spurious,
non-spam messages such as bounces for nonexistent addresses, vacation replies, and angry
responses demanding the spam to stop. For a large enough spam campaign, these messages
will completely overwhelm the target’s inbox, making the task of ﬁnding the legitimate
messages among them so costly as to be impractical. This attack is within the means of
most large-scale spammers; however, since it requires a large number of messages to use the
same From: address, the number of targets may not be very high. Furthermore, some spam
ﬁlters now block many bounce messages, so the volume of the spam campaign may need
to be high enough to overwhelm the target solely with vacation responses and hand-written
It does not make sense to consider this attack targeted to speciﬁc messages, since it
necessarily affects the entire inbox; however, the attacker might target certain people or a
broader set. In the Targeted version, the attacker has a particular inbox to ﬂood with replies.
In the Indiscriminate version, the attacker has a broader set of inboxes to choose among,
such as when going after an organization rather than a single individual.
Mach Learn 81: 121–148
2.4 The adversarial learning game
This section models attacks on learning systems as games where moves represent strategic
choices. The choices and computations in a move depend on information produced by previous moves (when a game is repeated, this includes previous iterations). In an Exploratory
attack, the attacker chooses a procedure AE that affects the evaluation data E, and in a
Causative attack, the attacker also chooses a procedure AT to manipulate the training data X.
The defender chooses a learning algorithm H. This formulation gives us a theoretical basis
for analyzing the interactions between attacker and defender. Refer back to Table 1 for a
summary of notation.
The choices made by the attacker and defender offer a variety of different domain-speciﬁc
strategies for both players. In Sects. 3 and 4, we discuss how attack and defense strategies
have been developed in different domains, and we highlight important aspects of the game in
those settings. In Sects. 3.1 and 3.2, we discuss practical examples of the attacker’s choices
in the Causative game, and we discuss the defender’s choice in Sect. 4.2. Similarly, for an
Exploratory attack, we discuss realistic instances of the attacker’s choice for AE in Sects. 3.3
and 3.4, and we discuss the defender’s choice for an algorithm H in Sect. 4.1.
2.4.1 Exploratory game
First we present the formal version of the game for Exploratory attacks, and then we explain
it in greater detail:
1. Defender Choose procedure H for selecting hypothesis
2. Attacker
Choose procedure AE for selecting distribution
3. Evaluation:
– Reveal distribution PT
– Sample dataset X from PT
– Compute f ←H(X)
– Compute PE ←AE(X,f )
– Sample dataset E from PE
– Assess total cost: 
(x,y)∈E C(x,f (x),y)
The defender’s move is to choose a learning algorithm (procedure) H for creating hypotheses from datasets. Many procedures used in machine learning have the form of (1).
For example, the defender may choose a support vector machine (SVM) with a particular
kernel, loss, regularization, and cross-validation plan. The attacker’s move is then to choose
a procedure AE to produce a distribution on which to evaluate the hypothesis that H generates. (The degree of control the attacker has in generating the dataset and the degree of
information about X and f that AE has access to are setting-speciﬁc.)
After the defender and attacker have both made their choices, the game is evaluated.
A training dataset X is drawn from some ﬁxed and possibly unknown distribution PT, and
training produces f = H(X). The attacker’s procedure AE produces distribution PE, which
is based in general on X and f , and an evaluation dataset E is drawn from PE. Finally, the
attacker and defender incur cost based on the performance of f evaluated on E.
The procedure AE generally depends on X and f , but the amount of information an
attacker actually has is setting speciﬁc (in the least restrictive case the attacker knows X and
f completely). The attacker may know a subset of X or the family Ω of f . However, the
procedure AE may also involve acquiring information dynamically. For instance, in many
cases, the procedure AE can query the classiﬁer, treating it as an oracle that provides labels
Mach Learn 81: 121–148
for query instances; this is one particular degree of information that AE can have about f .
Attacks that use this technique are probing attacks. Probing can reveal information about
the classiﬁer. On the other hand, with sufﬁcient prior knowledge about the training data and
algorithm, the attacker may be able to ﬁnd high-cost instances without probing.
2.4.2 Causative game
The game for Causative attacks is similar:
1. Defender Choose procedure H for selecting hypothesis
2. Attacker
Choose procedures AT and AE for selecting distributions
3. Evaluation:
– Compute PT ←AT
– Sample dataset X from PT
– Compute f ←H(X)
– Compute PE ←AE(X,f )
– Sample dataset E from PE
– Assess total cost: 
(x,y)∈E C(x,f (x),y)
This game is very similar to the Exploratory game, but the attacker can choose AT to
affect the training data X. The attacker may have various types of inﬂuence over the data,
ranging from arbitrary control over some fraction of instances to a small biasing inﬂuence
on some aspect of data production; details depend on the setting.
Control over data used for training opens up new strategies to the attacker. Cost is based
on the interaction of f and E. In the Exploratory game the attacker chooses E while the
defender controls f ; in the Causative game the attacker also has inﬂuence on f . With this
inﬂuence, the attacker can proactively cause the learner to produce bad classiﬁers.
2.4.3 Iteration
We have analyzed these games as one-shot games, in which players minimize cost when
each move happens only once. We can also consider an iterated game, in which the game
repeats several times and players minimize total accumulated cost. In this setting, we assume
players have access to all information from previous iterations of the game.
3 Attacks: categorizing related work
This section surveys examples of learning in adversarial environments from the literature.
Our taxonomy provides a basis for evaluating the resilience of the systems described, analyzing the attacks against them in preparation for constructing defenses.
3.1 Causative Integrity attacks
Contamination in PAC learning
Kearns and Li extend Valiant’s probably approximately correct (PAC) learning framework to prove bounds for maliciously chosen errors in the training data. In PAC learning, an algorithm succeeds if it can,
with probability at least 1 −δ, learn a hypothesis that has at most probability ε of making
an incorrect prediction on an example drawn from the same distribution. Kearns and Li examine the case where an attacker has arbitrary control over some fraction β of the training
Mach Learn 81: 121–148
Table 3 Related work in the taxonomy
Availability
Kearns and Li ,
Kearns and Li , Newsome et al. ,
Newsome et al. 
Chung and Mok , Nelson et al. 
Indiscriminate
Kearns and Li ,
Kearns and Li , Newsome et al. ,
Newsome et al. 
Chung and Mok , Nelson et al. 
Exploratory
Tan et al. ,
Moore et al. 
Lowd and Meek ,
Wittel and Wu ,
Lowd and Meek 
Indiscriminate
Fogla and Lee ,
Moore et al. 
Lowd and Meek ,
Wittel and Wu 
examples (this speciﬁes the form that AT takes in our Causative game). They prove that in
general the attacker can prevent the learner from succeeding if β ≥ε/(1 + ε), and for some
classes of learners they show this bound is tight.
This work provides an interesting and useful bound on the ability to succeed at PAClearning. The analysis broadly concerns both Integrity and Availability attacks as well as
both Targeted and Indiscriminate. However, not all learning systems fall into the PAClearning model.
Red herring attack
Newsome et al. present Causative Integrity and Causative
Availability attacks against Polygraph , a polymorphic virus detector
that learns virus signatures using both a conjunction learner and a naive-Bayes-like learner.
They present red herring attacks against conjunction learners that exploit certain weaknesses
not present in other learning algorithms (these are Causative Integrity attacks, both Targeted
and Indiscriminate).
The idea is that the attacker chooses PT to introduce spurious features into all malicious
instances that the defender uses for training. The malicious instances produced by PE, however, lack the spurious features and therefore bypass the ﬁlter, which erroneously generalized
that the spurious features were necessary elements of the malicious behavior.
3.2 Causative Availability attacks
Correlated outlier attack
Newsome et al. also suggest a correlated outlier attack,
which attacks a naive-Bayes-like learner by adding spurious features to positive training instances, causing the ﬁlter to block benign trafﬁc with those features (an Availability attack).
As with the red herring attacks, these correlated outlier attacks ﬁt neatly into our causative
game; this time PT includes spurious features in malicious instances, causing H to produce
an f that classiﬁes many benign instances as malicious.
Allergy attack
Chung and Mok present Causative Availability attacks against
the Autograph worm signature generation system . Autograph operates
Mach Learn 81: 121–148
in two phases. First, it identiﬁes infected nodes based on behavioral patterns, in particular
scanning behavior. Second, it observes trafﬁc from the identiﬁed nodes and infers blocking
rules based on observed patterns. Chung and Mok describe an attack that targets trafﬁc
to a particular resource. In the ﬁrst phase, an attack node convinces Autograph that it is
infected by scanning the network. In the second phase, the attack node sends crafted packets
mimicking targeted trafﬁc, causing Autograph to learn rules that block legitimate access and
create a denial of service.
In the context of our causative game, the attacker’s choice of PT provides the trafﬁc
for both phases of Autograph’s learning. When Autograph produces a hypothesis f that
depends on the carefully crafted trafﬁc from the attacker, it will block access to legitimate
trafﬁc from PE that shares patterns with the malicious trafﬁc.
Attacking SpamBayes
Nelson et al. demonstrate Causative Availability attacks
(both Targeted and Indiscriminate) against the SpamBayes statistical spam classiﬁer. We
examine these attacks in depth in Sect. 5.
3.3 Exploratory Integrity attacks
Some Exploratory Integrity attacks mimic statistical properties of the normal trafﬁc to camouﬂage intrusions. In the Exploratory game, the attacker’s move produces instances E that
statistically resemble normal trafﬁc in the training data X as measured by the learning procedure H.
Polymorphic blending attack
Polymorphic blending attacks encrypt attack trafﬁc in such
a way that it appears statistically identical to normal trafﬁc. Fogla and Lee present
a formalism for reasoning about and generating polymorphic blending attack instances to
evade intrusion detection systems.
Attacking a sequence-based IDS
Tan et al. describe a mimicry attack against
the stide anomaly-based intrusion detection system (IDS). They modify exploits of the
passwd and traceroute programs to accomplish the same ends using different sequences of system calls: the shortest subsequence in attack trafﬁc that does not appear in
normal trafﬁc is longer than the IDS window size, evading detection. In subsequent work
Tan et al. characterize their attacks as part of a larger class of information hiding
techniques which they demonstrate can make exploits mimic either normal call sequences
or the call sequence of another less severe exploit.
Independently, Wagner and Soto have also developed mimicry attacks against a
sequence-based IPS called pH.1 Using the machinery of ﬁnite automata, they construct a
framework for testing whether an IDS is susceptible to mimicry for a particular exploit. In
doing so, they develop a tool for validating IDSs on a wide-range of variants of a particular attack and suggest that similar tools should be more broadly employed to identify the
vulnerabilities of an IDS.
Overall, these mimicry attacks against sequence-based anomaly detection systems underscore critical weaknesses in these systems that allow attackers to obfuscate critical elements
of their exploits to avoid detection. Further they highlight how an IDS may appear to perform
well against a known exploit but, unless it captures necessary elements of the intrusion, the
exploit can easily be adapted to circumvent the detector. See Sect. 4.1.1 for more discussion.
1Wagner and Soto treat pH as an IDS rather than an IPS, though that distinction is not relevant to the
attacks they present.
Mach Learn 81: 121–148
Good word attacks
Several authors demonstrate Exploratory integrity attacks using similar
principles against spam ﬁlters. Lowd and Meek and Wittel and Wu develop
attacks against statistical spam ﬁlters that add good words, or words the ﬁlter considers
indicative of non-spam, to spam emails. This type of modiﬁcation can make spam emails
appear innocuous to the ﬁlter, especially if the words are chosen to be ones that appear often
in non-spam email and rarely in spam email.
Reverse engineering classiﬁers
Lowd and Meek approach the Exploratory Integrity attack problem from a different angle: they give an algorithm for an attacker to reverse engineer a classiﬁer. The attacker seeks the highest cost (lowest cost for the attacker)
instance that the classiﬁer labels negative. This work is interesting in its use of a cost function over instances for the attacker rather than simple positive/negative classiﬁcation. We
explore this work in more detail in Sect. 4.1.2.
3.4 Exploratory Availability attacks
Exploratory Availability attacks against non-learning systems abound in the literature: almost any denial of service (DoS) attack falls into this category, such as those described by
Moore et al. .
However, Exploratory Availability attacks against the learning components of systems
are not common. We suspect this is because if an attacker wishes to cause denial of service
but has no control over the learning process, other avenues of attack are more fruitful.
Here is one hypothetical scenario: if a learning IPS has trained on intrusion trafﬁc and
has the policy of blocking hosts that originate intrusions, an attacker could send intrusions
that appear to originate from a legitimate host, convincing the IPS to block that host. Another possibility is to take advantage of a computationally expensive learning component:
for example, spam ﬁlters that use image processing to detect advertisements in graphical
attachments can take signiﬁcantly more time than text-based ﬁltering . An attacker could exploit such overhead by sending many emails with
images, causing the expensive processing to delay and perhaps even block messages.
3.5 Practical considerations for attacks
The attacks described in this section highlight several practical considerations that must be
overcome to design an effective attack against a machine learning system. In many cases,
realistic attacks on an IPS need to be valid executables—we discuss several such attacks
that create valid sequences of system calls in Sect. 3.3 .
Other attacks require the attacker to spoof normal behavior—features such as the address
of the target system may be difﬁcult to spoof, but Chung and Mok demonstrate that
certain components of HTTP requests sometimes used as features for detection can easily be
spoofed. Mahoney and Chan and Chung and Mok suggest that poor choices of
features can facilitate attacks on an IPS, which we discuss further in Sect. 4.1.1. And ﬁnally,
Fogla and Lee discuss in depth the opportunities and constraints of generating attack
trafﬁc that appears statistically identical to benign trafﬁc, as mentioned in Sect. 3.3.
4 Defenses
This section gives an overview of possible defenses against the types of attacks introduced
in previous sections. We discuss both defense mechanisms from prior work as well as ideas
Mach Learn 81: 121–148
for new defenses. The game between attacker and defender and the taxonomy that we introduce in Sect. 3 provide a foundation on which to construct defense strategies against broad
classes of attacks. We address Exploratory and Causative attacks separately; in Sect. 4.2.3
we discuss the broader setting of an iterated game.
In all cases, defenses present a trade-off: changing the algorithms to make them more
robust against (worst-case) attacks will generally make them less effective on average. Analyzing this trade-off is an important part of developing defenses.
4.1 Defending against Exploratory attacks
Exploratory attacks do not corrupt the training data but attempt to ﬁnd vulnerabilities in
the learned hypothesis. When producing the evaluation distribution, the attacker attempts
to construct an unfavorable evaluation distribution concentrating probability mass on highcost instances; in other words, the attacker’s procedure AE tries to construct an evaluation
distribution PE on which the learner predicts poorly (violating stationarity) and the cost
computed in the last step of the Exploratory game is high. This section examines defender
strategies that make it difﬁcult for the attacker to construct such a distribution.
In the Exploratory game, the defender makes a move before observing contaminated
data. The defender can impede the attacker’s ability to reverse engineer the classiﬁer by
limiting access to information about the training procedure and data. With less information,
AE has difﬁculty producing an unfavorable evaluation distribution. Nonetheless, even with
incomplete information, the attacker may be able to construct an unfavorable evaluation
distribution using a combination of prior knowledge and probing.
4.1.1 Defenses against attacks without probing
Part of our security analysis involves identifying aspects of the system that should be kept
secret. In securing a learner, we limit information to make it difﬁcult for an attacker to
conduct an attack.
Training data
Preventing the attacker from knowing the training data limits the attacker’s
ability to reconstruct internal states of the classiﬁer. There is a tension between collecting
training data that fairly represents the real world instances and keeping all aspects of that
data secret. In most situations, it is difﬁcult to use completely secret training data, though
the attacker may have only partial information about its distribution.
Feature selection
We can harden classiﬁers against attack through attention to features in
the feature selection and learning steps (which are both internal steps of the defender’s hypothesis selection procedure H). Feature selection is the process of choosing a feature map
that transforms raw measurements into the feature space used by the learning algorithm. In
the learning step, the learning algorithm builds its model or signature using particular features from the map’s feature space; this choice of features for the model or signature is also
sometimes referred to as feature selection, though we consider it to be part of the learning
process, after the feature map has been established. For example, one feature map for email
message bodies might transform each token to a Boolean feature indicating its presence;
another map might specify a real-valued feature indicating the relative frequency of each
word in the message compared to its frequency in natural language; yet another map might
count sequences of n characters and specify an integer feature for each character n-gram indicating how many times it appears. In each of these cases, a learner will construct a model
Mach Learn 81: 121–148
or signature that uses certain features (tokens present or absent; relative frequency of words
present; character n-gram counts) to decide whether an instance is benign or malicious.
Obfuscation of spam-indicating words (an attack on the feature set) is a common Targeted
Exploratory Integrity attack. Sculley et al. use inexact string matching in feature selection to defeat obfuscations of words in spam emails. They choose a feature map based on
character subsequences that are robust to character addition, deletion, and substitution.
Globerson and Roweis present a feature-based learning defense for the feature
deletion exploratory attack on the evaluation data E. In feature deletion, features present
in the training data, and perhaps highly predictive of an instance’s class, are removed from
the evaluation data by the attacker. For example, words present in training emails may not
occur in evaluation messages, and network packets in training data may contain values for
optional ﬁelds that are missing from future trafﬁc. Globerson and Roweis formulate a modiﬁed support vector machine classiﬁer that is robust in its choice of features against deletion
of high-value features.
One particularly important consideration when the learner builds its model or signature
is to ensure that the learner uses features related to the intrusion itself. In their study of
the DARPA/Lincoln Laboratory IDS evaluation dataset, Mahoney and Chan demonstrate that spurious artifacts in training data can cause an IDS to learn to distinguish normal
from intrusion trafﬁc based on those artifacts rather than relevant features. Ensuring that the
learner builds a model from features that describe the fundamental differences between malicious and benign instances should mitigate the effects of mimicry attacks (Sect. 3.3) and
red herring attacks (Sect. 3.1).
Using spurious features in constructing a model or signature is especially problematic in
cases where any given intrusion attempt may cause harm only probabilistically or depending
on some internal state of the victim’s system. If the features relevant to the intrusion are
consistent for some set of instances but the actual cost of those instances varies widely, then
a learner risks attributing the variation to other nonessential features.
Hypothesis space/learning procedures
A complex hypothesis space may make it difﬁcult
for the attacker to infer precise information about the learned hypothesis. However, hypothesis complexity must be balanced with capacity to generalize, such as through regularization.
Wang et al. present Anagram, an anomaly detection system using n-gram models
of bytes to detect intrusions. They incorporate two techniques to defeat Exploratory attacks
that mimic normal trafﬁc (mimicry attacks): (1) they use high-order n-grams (with n typically between 3 and 7), which capture differences in intrusion trafﬁc even when that trafﬁc
has been crafted to mimic normal trafﬁc on the single-byte level; and (2) they randomize feature selection by randomly choosing several (possibly overlapping) subsequences of bytes
in the packet and testing them separately, so the attack will fail unless the attacker makes
not only the whole packet but also any subsequence mimic normal trafﬁc.
Dalvi et al. develop a cost-sensitive game-theoretic classiﬁcation defense to
counter Exploratory Integrity attacks. In their model, the attacker can alter natural instance
features in AE but incurs a known cost for each change. The defender can measure each
feature at a different known cost. Each has a known cost function over classiﬁcation/true
label pairs. The classiﬁer H is a cost-sensitive naive Bayes learner that classiﬁes instances
to minimize its expected cost, while the attacker modiﬁes features to minimize its own expected cost. Their defense constructs an adversary-aware classiﬁer by altering the likelihood
function of the learner to anticipate the attacker’s changes. They adjust the likelihood that
an instance is malicious by considering that the observed instance may be the result of an
attacker’s optimal transformation of another instance. This defense relies on two assumptions: (1) the defender’s strategy is a step ahead of the attacker’s strategy 81: 121–148
their game differs from ours in that the attacker’s procedure AE cannot take f into account),
and (2) the attacker plays optimally against the original cost-sensitive classiﬁer. It is worth
noting that while their approach defends against optimal attacks, it doesn’t account for nonoptimal attacks. For example, if the attacker doesn’t modify any data, the adversary-aware
classiﬁer misclassiﬁes some instances that the original classiﬁer correctly classiﬁes.
4.1.2 Defenses against probing attacks
The ability for AE to query a classiﬁer gives an attacker powerful additional attack options.
Analysis of reverse engineering
Lowd and Meek observe that the attacker need
not model the classiﬁer explicitly, but only ﬁnd lowest-attacker-cost instances as in the Dalvi
et al. setting. They formalize a notion of reverse engineering as the adversarial classiﬁer reverse engineering (ACRE) problem. Given an attacker cost function, they analyze the complexity of ﬁnding a lowest-attacker-cost instance that the classiﬁer labels as negative. They
assume no general knowledge of training data, though the attacker does know the feature
space and also must have one positive example and one negative example. A classiﬁer is
ACRE-learnable if there exists a polynomial-query algorithm that ﬁnds a lowest-attackercost negative instance. They show that linear classiﬁers are ACRE-learnable with linear
attacker cost functions and some other minor restrictions.
The ACRE-learning problem provides a means of qualifying how difﬁcult it is to use
queries to reverse engineer a classiﬁer from a particular hypothesis class using a particular
feature space. We now suggest defense techniques that can increase the difﬁculty of reverse
engineering a learner.
Randomization
A randomized hypothesis may decrease the value of feedback to an attacker. Instead of choosing a hypothesis f : X →{0,1}, we generalize to hypotheses that
predict a real value on . This generalized hypothesis returns a probability of classifying x as 1. By randomizing, the expected performance of the hypothesis may decrease on
regular data drawn from a non-adversarial distribution, but it also may decrease the value of
the queries for the attacker.
Randomization in this fashion does not reduce the information available in principle to
the attacker, but merely requires more work from the attacker for the information. It is likely
that this defense is appropriate in only a small number of scenarios.
Limiting/misleading feedback
Another potential defense is to limit the feedback given
to an attacker. For example, common techniques in the spam domain include eliminating
bounce emails, remote image loading, and other potential feedback channels. It is impossible to remove all feedback channels; however, limiting feedback increases work for the
attacker. In some settings, it may be possible to mislead the attacker by sending fraudulent
Actively misleading the attacker by fabricating feedback suggests an interesting battle of
information between attacker and defender. In some scenarios the defender may be able to
give the attacker no information via feedback, and in others the defender may even be able
to return feedback that causes the attacker to come to incorrect conclusions.
4.2 Defending against Causative attacks
In Causative attacks, the attacker has a degree of control over not only the evaluation distribution but also the training distribution. Therefore the learning procedures we consider must
Mach Learn 81: 121–148
be resilient against contaminated training data, as well as to the evaluation considerations
discussed in Sect. 4.1.
Two general strategies for defense are to remove malicious data from the training set and
to harden the learning algorithm against malicious training data. We ﬁrst present one method
of our own for the former and then describe two approaches to the latter that appear in the
literature.
4.2.1 The RONI defense
We propose the Reject On Negative Impact (RONI) defense, a technique that measures the
empirical effect of each training instance and eliminates from training those points that have
a substantial negative impact on classiﬁcation accuracy. To determine whether a candidate
training instance is malicious or not, we can train a classiﬁer on a base training set, then
add the candidate instance to our training set and train a second classiﬁer. We apply both
classiﬁers to a quiz set of instances with known labels, measuring the difference in accuracy
between the two. If adding the candidate instance to our training set causes the resulting
classiﬁer to produce substantially more classiﬁcation errors, we reject the instance as detrimental in its effect.
We reﬁne and explore the RONI defense experimentally in Sect. 5.3.
4.2.2 Robustness
The ﬁeld of Robust Statistics explores procedures that limit the impact of a small fraction
of deviant (adversarial) training data. In the setting of Robust Statistics, it is assumed that
the bulk of the data is generated from a known model, but a fraction of the data comes
from an unknown model—to bound the effect of this unknown source it is assumed to be
adversarial. There are a number of measures of a procedure’s robustness: the breakdown
point is the level of contamination required for the attacker to arbitrarily manipulate the
procedure and the inﬂuence function measures the impact of contamination on the procedure.
Robustness measures can be used to assess the susceptibility of an existing system and to
suggest alternatives that reduce or eliminate the vulnerability. Ideally one would like to use a
procedure with a high breakdown point and a bounded inﬂuence function. In this way, these
measures can be used to compare candidate procedures and to design procedures H that are
optimally robust against adversarial contamination of the training data. For a full treatment,
see the books by Huber , Hampel et al. , and Maronna et al. .
Recent research has highlighted the importance of robust procedures in security and
learning tasks. Wagner observes that common sensor net aggregation procedures,
such as computing a mean, are not robust to adversarial point contamination, and he identiﬁes robust replacements. Christmann and Steinwart study robustness for a general
family of learning methods. Their results suggest that certain commonly used loss functions, along with proper regularization, lead to robust procedures with a bounded inﬂuence function. These results suggest such procedures have desirable properties for secure
4.2.3 Online prediction with experts
Another approach to combatting Causative attacks is to dynamically adapt to the data in an
online fashion. In this setting, we allow for the attacker to potentially have arbitrary control
of the training data, but instead of attempting to learn on this arbitrarily corrupted data, the
Mach Learn 81: 121–148
online learner attempts to optimize with respect to a set of experts. For instance, the experts may be a set of classiﬁers each designed to provide different security properties. The
experts provide advice (predictions) to the defender who forms a composite classiﬁer that
weighs the advice based on each experts past performance and thus produces a composite prediction. We make no assumption about how the experts form their advice or about
their performance. However, rather than evaluating the cost of the composite classiﬁer’s
predictions directly, we instead compare the cost incurred by the composite classiﬁer relative to the cost of the best expert in hindsight; that is, we compute the regret the composite
classiﬁer has for not heeding the advice of the best expert. In designing strategies that optimize regret, the composite classiﬁers for these online games create a moving target and
force the attacker to construct attacks that succeed against a set of experts rather than a
single one.
In this setting, the learner forms a prediction from the M expert predictions and adapts
the hypothesis based on their performance during K repetitions. At each step k of the game,
the defender receives a prediction g(k)
m from each expert; this may be based on the data but we
make no assumptions about its behavior. More formally, the k-th round of the expert-based
prediction game is:
1. Defender Update function h(k) : YM →Y
2. Attacker
Choose distribution P(k)
3. Evaluation:
– Sample an instance (x(k),y(k)) ∼P(k)
– Compute expert advice {g(k)
– Predict ˆy(k) = h(k)(g(k)
1 ,...,g(k)
– Assess cost C(x(k), ˆy(k),y(k))
This game has a slightly different structure from the games we present in Sect. 2.4—here
the defender chooses one strategy at the beginning of the game and then in each iteration
updates the function h(k) according to that strategy. The attacker, however, may select a new
strategy at each iteration.
The setting of online expert-based prediction splits risk minimization into two subproblems: (1) minimizing the average loss of each expert and (2) minimizing the average regret—
the difference between the loss of our composite learner and the loss of the best overall expert in hindsight. The other defenses we have discussed approach the ﬁrst problem. Online
game theory addresses the second problem: the defender chooses a strategy for updating
h(k) to minimize regret based only on the expert’s past performance. For certain variants of
the game, there exist composite predictors whose regret is o(K)—that is, the average regret
approaches 0 as the K increases. Thus, the composite learner can perform almost as well
as the best expert without knowing ahead of time which expert is best. A full description of
this setting and several results appear in Cesa-Bianchi and Lugosi .
Importantly, the online prediction setting allows the defender to adapt to an adversary and
forces the adversary to design attack strategies that succeed against an entire set of experts
(each of which can have its own security design considerations). Thus, we can incorporate
several classiﬁers with desirable security properties into a composite approach. Moreover,
if a particular attack is succeeding, we can design a new expert against the identiﬁed vulnerability and add it to our set of experts to patch the exploit. This makes online prediction
well-suited to the changing attack landscape.
Mach Learn 81: 121–148
5 Case study: attacking SpamBayes
We have put our framework to use studying attacks against the SpamBayes statistical spam
ﬁlter . Here we review that work and demonstrate how our framework
informs and structures the analysis.
SpamBayes is a content-based statistical spam ﬁlter that classiﬁes email using token
counts in a model proposed by Robinson and inspired by Graham . Meyer
and Whateley describe the system in detail. SpamBayes computes a score for each
token in the training corpus; this score is similar to a smoothed estimate of the posterior
probability that an email containing that token is spam. Each token’s score is an estimator of
the conditional likelihood that a message is spam given that it contains the token.2 In terms
of (1), the loss function ℓ(·,·) takes the form of the logarithm of the posterior probability
for a binomial model, the regularization term ρ(·) takes the form of the logarithm of a prior
probability using a beta prior to smooth the estimate, and λ = 1 is the regularizer’s weight.
The ﬁlter computes a message’s spam score by assuming token scores are independent and
applying Fisher’s method for combining signiﬁcance tests . The message score
is compared against two thresholds to select the label spam, ham (non-spam), or unsure.
5.1 Causative Availability attacks on SpamBayes
In analyzing the vulnerabilities of SpamBayes, we are motivated by our taxonomy of
attacks. Known attacks that spammers use against deployed spam ﬁlters tend to be
Exploratory Integrity attacks: either they obfuscate the especially spam-like content of
a spam email or they include content not indicative of spam. Both tactics aim to get
the modiﬁed message into the victim’s inbox. This category of attack has been studied in detail in the literature . However, we ﬁnd the study of Causative attacks more compelling because they are unique to machine learning systems and potentially more harmful.
In particular, a Causative Availability attack can create a powerful denial of service. For
example, if a spammer causes enough legitimate messages to be ﬁltered away by the user’s
spam ﬁlter, the user is likely to disable the ﬁlter and therefore see the spammer’s advertisements. As another example, an unscrupulous business owner may wish to use spam ﬁlter denial of service to prevent a competitor from receiving email orders from potential customers.
In this section, we present two novel Causative Availability attacks against SpamBayes: the
dictionary attack is Indiscriminate and the focused attack is Targeted.
We consider an attacker with the following capabilities. We allow the attacker’s procedures AT and AE to craft email messages with arbitrary message bodies but realistically
limited control over the headers. Because we limit our study to Availability attacks, we assume that all malicious messages generated by AT are labeled as spam when included in
the training set X; if we allow the attacker to create ham-labeled training messages then
Integrity attacks become easier, but the advantage for Availability attacks is small.
Dictionary attack
Our ﬁrst attack is an Indiscriminate attack—the attacker wants to cause
a large number of false positives so that the user loses conﬁdence in the ﬁlter and must
manually sort through spam and ham emails. There are three variants of this attack. In the
ﬁrst, the attacker maximizes the expected spam score of any future message in an optimal
2The estimator used by Meyer and Whateley for conditional likelihood deviates slightly from a traditional maximum likelihood estimator.
Mach Learn 81: 121–148
attack by simply including all possible tokens (words, symbols, misspellings, etc.) in attack
emails, causing SpamBayes to learn that all tokens are indicative of spam. In practice this
optimal attack is intractable, but we approximate its effect by using a large set of common
words such as a dictionary—hence these are dictionary attacks. The two other variants of the
dictionary attack use the Aspell dictionary and a dictionary compiled from the most common
tokens observed in a Usenet corpus.
Focused attack
Our second attack is a Targeted attack—the attacker has some knowledge
of a speciﬁc legitimate email to target. If the attacker has exact knowledge of the target
email, placing all of its tokens in attack emails produces an optimal attack. Realistically, the
attacker has partial knowledge about the target email and can guess only some of its tokens
to include in attack emails. We model this knowledge by letting the attacker probabilistically
guess tokens from the target email. This is the focused attack.
5.2 Experiments with SpamBayes
We have constructed Causative Availability attacks on SpamBayes; here we summarize results of our attack experiments, described in full in our earlier paper . We
use the Text Retrieval Conference (TREC) 2005 spam corpus ,
which is based on the Enron email corpus and contains 92,189 emails
(52,790 spam and 39,399 ham). From this dataset, we construct sample inboxes and measure
the effect of injecting our attacks into them.
Figure 1 shows the average effect of our dictionary and focused attacks. In both graphs,
the x-axis is the contamination percent of the training set. For the dictionary attacks,
the y-axis is the percent of test ham messages misclassiﬁed. For the focused attack, the
y-axis is the percent misclassiﬁcation of the target message, averaged over 200 random target messages. Although the graphs do not include error bars, we observe that the variation
Fig. 1 Effect of the dictionary and focused attacks. We plot percent of ham classiﬁed as spam (dashed lines)
and as unsure or spam (solid lines) against percent of the training set contaminated. Left: Three dictionary
attacks on an initial training set of 10,000 messages (50% spam). We show the optimal attack (black △), the
Usenet dictionary attack (blue □), and the Aspell dictionary attack (green ⃝). Right: The average effect of
200 focused attacks on their targets when the attacker guesses each target token with 50% probability. The
initial inbox contains 5000 emails (50% spam)
Mach Learn 81: 121–148
Fig. 2 Effect of the focused attack on three representative emails. Each point is a token. The x-axis is the
token spam score before the attack (0 means ham and 1 means spam); the y-axis is the spam score after the
attack. The red ×’s are tokens included in the attack and the blue ⃝’s are tokens that were not in the attack.
Histograms show the distribution of token scores before the attack (at bottom) and after the attack (at right)
The optimal attack quickly causes the ﬁlter to mislabel all legitimate emails as spam. The
Usenet dictionary attack (90,000 top-ranked words from the Usenet corpus) causes signiﬁcantly more misclassiﬁcations than the Aspell dictionary attack, since it contains common
tokens, such as misspellings and slang terms, that are not present in an English dictionary.
The focused attack (where each token in the target message is guessed with 50% probability)
has an effect on its target comparable to the Usenet dictionary attack.
To better understand the results of the focused attack, we examine its effect on individual
tokens. Each of the panels in Fig. 2 represents the tokens from a single target email: the
upper-left email is a ham message misclassiﬁed as spam, the upper-right email is a ham
message misclassiﬁed as unsure, and the bottom-middle email is a ham message correctly
classiﬁed as ham. Each point in the graph represents the before and after spam scores of a
token. A point above the line y = x signiﬁes that the token’s score increases (becomes more
indicative of spam) because of the attack and a point below the line signiﬁes that the token’s
score decreases. The scores of tokens included in the attack messages typically increase
signiﬁcantly while those not included decrease slightly. The increase in score for included
Mach Learn 81: 121–148
tokens is more signiﬁcant than the decrease in score for excluded tokens, so the attack has
substantial impact even when the attacker guesses only a fraction of the tokens.
All of our attacks require relatively few attack emails to signiﬁcantly degrade Spam-
Bayes’s accuracy; even the 10% false positive rate induced by the Aspell dictionary attack
renders a spam ﬁlter unusable.
5.3 The Reject On Negative Impact (RONI) defense
In Sect. 4.2.1, we mention the possibility of the Reject On Negative Impact (RONI) defense.
As we stated in that section, the RONI defense measures the empirical effect of each training
instance and eliminates from training those points that have a substantial negative impact on
classiﬁcation accuracy. To determine whether a candidate training instance is malicious or
not, we can train a classiﬁer on a base training set, then add the candidate instance to our
training set and train a second classiﬁer. We apply both classiﬁers to a quiz set of instances
with known labels, measuring the difference in accuracy between the two. If adding the
candidate instance to our training set causes the resulting classiﬁer to produce substantially
more classiﬁcation errors, we reject the instance as detrimental in its effect. In this section
we describe the RONI defense in greater detail and discuss our experimental results.
We assume we are given an initial training set X and a set Z of additional candidate
training points. We evaluate the points in Z as follows: ﬁrst we set aside a calibration set C,
which is a randomly chosen subset of X. Then using the remaining portion of X, we sample a number of independent and potentially overlapping training/quiz set pairs (Ti,Qi),
where the points within a pair of sets are sampled without replacement. To assess the impact
(empirical effect) of a data point (x,y) ∈Z, for each pair of sets (Ti,Qi) we construct a
before classiﬁer fi trained on Ti and an after classiﬁer ˆfi trained on Ti + (x,y). Our RONI
defense then compares the classiﬁcation accuracy of fi and ˆfi on the quiz set Qi, using the
change in true positives and true negatives caused by adding (x,y) to Ti. If either change
is signiﬁcantly negative when averaged over training/quiz set pairs, we consider (x,y) to
be detrimental, so it should not be added to X. To determine the signiﬁcance of a change,
we compare the shift in accuracy to the average shift caused by points in the calibration
set C. Each point in C is evaluated in a way analogous to evaluation of the points in Z. We
compute the median and standard deviation of their true positive and true negative changes,
and we use the third standard deviation below the median as our signiﬁcance threshold.
For our experiments with the RONI defense, we again sample inboxes from the TREC
2005 spam corpus. In this assessment, we use 20-fold cross validation to get an initial training inbox X of about 1000 messages (50% spam) and a test set E of about 50 messages. We
also sample a separate set Z of 1000 additional messages from the TREC corpus to test as a
baseline. In each fold of cross validation, we run ﬁve separate trials of the RONI defense. For
each trial, we use a calibration set of 25 ham and 25 spam messages, and we sample three
training/quiz set pairs of 100 training and 100 quiz messages from the remaining 950 messages. We train two classiﬁers on each training set for each message in Z, one with and one
without the message, measuring performance on the corresponding quiz set and comparing
it to the magnitude of change measured from the calibration set.
We perform the RONI defense evaluation for each message in Z as just described to
see the effect on non-attack emails. We ﬁnd that the RONI defense (incorrectly) rejects an
average of 2.8% of the ham and 3.1% of the spam from Z. To evaluate the performance of
the post-RONI defense ﬁlter, we train a classiﬁer on all messages in Z and a second classiﬁer
on the messages in Z not rejected by the RONI defense. When trained on all 1000 messages,
the resulting ﬁlter correctly classiﬁes 97% of ham and 80% of the spam. After removing the
Mach Learn 81: 121–148
Table 4 Effect of the RONI defense on the accuracy of SpamBayes in the absence of attacks. Each confusion
matrix shows the breakdown of SpamBayes’s predicted labels for both ham and spam messages. Left: The
average performance of SpamBayes on training inboxes of about 1,000 messages (50% spam). Right: The
average performance of SpamBayes after the training inbox is censored using the RONI defense. On average,
the RONI defense removes 2.8% of ham and 3.1% of spam from the training sets. (Numbers may not add up
to 100% because of rounding error)
Before the RONI defense
Predicted Label
True Label
After the RONI defense
Predicted Label
True Label
messages rejected by the RONI defense and training from scratch, the resulting ﬁlter still
correctly classiﬁes 95% of ham and 87% of the spam. The overall effect of the RONI defense
on classiﬁcation accuracy is shown in Table 4.
Since the RONI defense is removing non-attack emails in this test, and therefore removing potentially useful information from the training data, we expect classiﬁcation accuracy
to be hurt. It is interesting to see that test performance on spam actually improves after removing some emails from the training set. This result seems to indicate that some non-attack
emails confuse the ﬁlter more than they help when used in training, perhaps because they
happen naturally to ﬁt some of the characteristics that attackers use in emails.
Next we evaluate the performance of the RONI defense where Z instead consists of
attack emails from the attacks described earlier in Sect. 5.1. The RONI defense rejects every
single dictionary attack from any of the dictionaries (optimal, Aspell, and Usenet). In fact,
the degree of change in misclassiﬁcation rates for each dictionary message is greater than
ﬁve standard deviations from the median, suggesting that these attacks are easily eliminated
with only minor impact on the performance of our ﬁlter. See Table 5.
A similar experiment with attack emails from the focused attack shows that the RONI
defense is much less effective against focused attack messages. The likely explanation is
simple: the dictionary attack broadly affects many different messages with its wide scope
of tokens, so its consequences are likely to be seen in our quiz sets. However, the focused
attack is targeted at a single future email, which may not bear any signiﬁcant similarity to
the messages in the quiz sets. However, as the fraction of tokens correctly guessed by the
attacker increases, the RONI defense identiﬁes increasingly many attack messages: only
7% are removed when the attacker guesses 10% of the tokens but 25% of the attacks are
removed when the attacker guesses 100% of the tokens. This is likely due to the fact that
with more correctly guessed tokens, the overlap with other messages increases sufﬁciently
to trigger the RONI defense more frequently. However, the attack is still successful in spite
of the increased number of detections. See Table 6.
The RONI defense is a successful mechanism that thwarts a broad range of dictionary attacks—or more generally Indiscriminate Causative Availability attacks. However, the
RONI defense also has costs. First, as we show above, this defense yields a slight decrease
in ham classiﬁcation (from 98% to 95%). Second, the RONI defense requires a substantial
amount of computation—testing each message in Z requires us to train and compare the
performance of several classiﬁers. Each new training message must be added to each of the
training sets Ti and the resulting new classiﬁers must be reevaluated on the quiz sets Qi.
Thus, to add a single message to a classiﬁer, we must ﬁrst add it to several smaller classi-
ﬁers (in our case ﬁve) then evaluate a set of messages for each 81: 121–148
Table 5 We apply the RONI defense to dictionary attacks with 1% contamination of training inboxes of
about 1,000 messages (50% spam) each. Left: The average effect of optimal, Usenet, and Aspell attacks on
the SpamBayes ﬁlter’s classiﬁcation accuracy. The confusion matrix shows the breakdown of SpamBayes’s
predicted labels for both ham and spam messages after the ﬁlter is contaminated by each dictionary attack.
Right: The average effect of the dictionary attacks on their targets after application of the RONI defense. By
using the RONI defense, all of these dictionary attacks are caught and removed from the training set, which
dramatically improves the accuracy of the ﬁlter
Dictionary Attacks (Before the RONI defense)
Predicted Label
True Label
True Label
True Label
Dictionary Attacks (After the RONI defense)
Predicted Label
True Label
True Label
True Label
Table 6 We apply the RONI defense to focused attacks with 1% contamination of training inboxes of about
1,000 messages (50% spam) each. Left: The average effect of 35 focused attacks on their targets when the
attacker correctly guesses 10, 30, 50, 90, and 100% of the target’s tokens. Right: The average effect of the
focused attacks on their targets after application of the RONI defense. By using the RONI defense, more of
the target messages are correctly classiﬁed as ham, but the focused attacks still succeed at misclassifying
most targeted messages
Focused Attacks (Before the RONI defense)
Predicted Label of Target
10% guessed
30% guessed
50% guessed
90% guessed
100% guessed
Focused Attacks (After the RONI defense)
Predicted Label of Target
10% guessed
30% guessed
50% guessed
90% guessed
100% guessed
per classiﬁer). While this RONI defense method can be parallelized, this additional work
does increase the training time required; our non-parallelized, non-optimized experiments
increased training time by two orders of magnitude because we trained a full classiﬁer several times for each message in the training set. Finally, the RONI defense may slow the
learning process. For instance, when a user correctly labels a new type of spam for training,
the RONI defense may reject those instances because the new spam may be very different
from spam previously seen and more similar to some non-spam messages in the training set.
This effect is hard to quantify and may or may not be an issue in practice.
We believe there is still ample room for optimization and reﬁnement of the RONI defense that may ameliorate one or both of these problems. The question of whether the RONI
Mach Learn 81: 121–148
defense is cost effective in practice is an open one, depending in part on the resource limitations and attack risks of any particular environment.
6 Discussion
Our framework opens a number of new research directions. One of the most promising research directions is measuring the amount of information leaked from a learning system
to an attacker. An adversary may try to gain information about the internal state of a machine learning system to: (a) extract personal information encoded in the internal state (for
example, in a machine learning anti-spam system, knowledge about how certain keywords
are handled may leak information about the contents or senders of typical email messages
handled by that system); or (b) derive information that will allow the adversary to more
effectively attack the system in the future.
We can measure the amount of information leaked in terms of the number of bits of
the information, but note that different pieces of information may be more or less relevant
to the security of a system than others. For example, exposing a person’s 9-digit social
security number can be very damaging, but exposing that person’s 9-digit postal code is
much more benign. This suggests an open problem: is it possible to develop a “theory of
information for security” that can measure the value of leaked information? An answer to
this question is likely to build on Shannon’s classical theory of information 
as well as computationally-based variants of it due to Kolmogorov and Yao .
A “theory of information for security” could have wide applicability, not only in the context
of understanding adversarial attacks on machine learning system, but also in quantifying the
risk associated with various side channel attacks that exploit leaked information.
We also open new directions for evaluating defenses. In Sect. 4, we introduce several
promising ideas for defenses against learning attacks. The next step is to explore general
defenses against larger classes of attack.
Developing a general framework for constructing and evaluating defenses would be a
valuable contribution. Measuring the adversarial effort required to perform an attack as well
as the effectiveness of the defense could help design secure learning systems. The ACRElearning framework of Lowd and Meek provides a computational analysis of the
complexity of reverse engineering a hypothesis using queries in an Exploratory attack. The
problem of Causative attacks may be more difﬁcult; here the ﬁelds of robust statistics and
online prediction games provide a foundation on which to build new defenses.
An interesting aspect of attacks on machine learning is the link between complexity and
attacks. In some cases, greater model complexity seems to confer advantage: text generated
by a bigram model cannot be distinguished from its source material by a unigram model, but
a trigram model can differentiate them. Wang et al. demonstrate that an increase in
model complexity can defend against some attacks. Similarly, Tan and Maxion have
shown that a sequence-based IDS must use a sequence length at least as large as the minimal
foreign subsequence of an attack, or smallest necessary subsequence of events in the attack,
for the detector to be successful (see also earlier work on mimicry attacks in Sect. 3.3). Does
this indicate a general trend? Can the advantages gained from a more complex model offset
the chance of overﬁtting, the need for more training data, and other downsides of model
complexity?
Mach Learn 81: 121–148
7 Conclusion
We have presented a framework for articulating a comprehensive view of different classes
of attacks on machine learning systems in terms of three independent dimensions and an
adversarial learning game. Guided by our framework, we survey relevant prior research
and explore the effects of different types of learning attacks on systems and their defenses
against these attacks. We provide a concrete extended example by applying our framework
to a machine learning-based application, the SpamBayes spam detection system, and show
how the attacks motivated by our framework can successfully cause SpamBayes to fail.
We also develop and demonstrate a concrete defense that succeeds against Indiscriminate
Causative Availability attacks on SpamBayes and has potential for wider applicability. Our
framework provides a solid foundation for analyzing attacks on machine learning systems
and developing defenses against them, taking a signiﬁcant step towards the goal of secure
Acknowledgements
We would like to thank Russell Sears, Benjamin Rubinstein, David Molnar, Peter
Bartlett, Michael Jordan, Satish Rao, Carla Brodley, and our anonymous reviewers for their insightful discussions, comments and suggestions regarding this research.
We gratefully acknowledge the support of our sponsors. This work was supported in part by TRUST
(Team for Research in Ubiquitous Secure Technology), which receives support from the National Science
Foundation (NSF award number CCF-0424422) and the following organizations: AFOSR (#FA9550-06-1-
0244), BT, Cisco, DoCoMo USA Labs, EADS, ESCHER, HP,IBM, iCAST, Intel, Microsoft, ORNL, Pirelli,
Qualcomm, Sun, Symantec,TCS, Telecom Italia, and United Technologies; in part by RAD Lab (Reliable
Adaptive Distributed Systems Laboratory), which receives support from California state Microelectronics
Innovation and Computer Research Opportunities grants (MICRO ID#06-148 and #07-012) and the following organizations: Amazon Web Services, CISCO, Cloudera, eBay, Facebook, Fujitsu Labs of America,
Google, Hewlett Packard, Intel, Microsoft, NetApp, SAP, Sun, VMWare, and Yahoo!; and in part by the
cyber-DEfense Technology Experimental Research laboratory (DETERlab), which receives support from the
Department of Homeland Security Homeland Security Advanced Research Projects Agency (HSARPA award
#022412) and AFOSR (#FA9550-07-1-0501). The opinions expressed here are solely those of the authors and
do not necessarily reﬂect the opinions of any funding agency, the State of California, or the US government.
Open Access
This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided
the original author(s) and source are credited.