Proceedings of NAACL-HLT 2018, pages 1662–1675
New Orleans, Louisiana, June 1 - 6, 2018. c⃝2018 Association for Computational Linguistics
Deep Communicating Agents for Abstractive Summarization
Asli Celikyilmaz1, Antoine Bosselut2, Xiaodong He3 and Yejin Choi2,4
1Microsoft Research
2Paul G. Allen School of Computer Science & Engineering, University of Washington
3JD AI Research
4Allen Institute for Artiﬁcial Intelligence
{aslicel}@microsoft.com
{xiaodong.he}@jd.com
{antoineb, yejin}@cs.washington.edu
We present deep communicating agents
in an encoder-decoder architecture to address the challenges of representing a
long document for abstractive summarization.
With deep communicating agents,
the task of encoding a long text is divided
across multiple collaborating agents, each
in charge of a subsection of the input text.
These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused
and coherent summary. Empirical results
demonstrate that multiple communicating
encoders lead to a higher quality summary
compared to several strong baselines, including those based on a single encoder or
multiple non-communicating encoders.
Introduction
We focus on the task of abstractive summarization of a long document. In contrast to extractive
summarization, where a summary is composed of
a subset of sentences or words lifted from the input text as is, abstractive summarization requires
the generative ability to rephrase and restructure
sentences to compose a coherent and concise summary. As recurrent neural networks (RNNs) are
capable of generating ﬂuent language, variants of
encoder-decoder RNNs have shown promising results on the abstractive summarization task .
The fundamental challenge, however, is that the
strong performance of neural models at encoding
short text does not generalize well to long text.
The motivation behind our approach is to be able
to dynamically attend to different parts of the input
to capture salient facts. While recent work in sum-
Tired of counting sheep ? Try
one of these remedies and get a
good nights sleep: Aroma ..
How to use it: Massage a dab of
aroma therapeutic balm or oil…
paragraphs
Figure 1: Illustration of deep communicating agents
presented in this paper. Each agent a and b encodes
one paragraph in multiple layers. By passing new messages through multiple layers the agents are able to coordinate and focus on the important aspects of the input
marization addresses these issues using improved
attention models , pointer
networks with coverage mechanisms , and coherence-focused training objectives
 , an effective mechanism for representing a long document
remains a challenge.
Simultaneous work has investigated the use of
deep communicating agents for collaborative tasks such as logic puzzles
 , visual dialog , and reference games . Our work builds on these approaches to
propose the ﬁrst study on using communicating
agents to encode long text for summarization.
The key idea of our model is to divide the hard
task of encoding a long text across multiple collaborating encoder agents, each in charge of a different subsection of the text (Figure 1). Each of these
agents encodes their assigned text independently,
and broadcasts their encoding to others, allowing
agents to share global context information with
one another about different sections of the document. All agents then adapt the encoding of their
assigned text in light of the global context and re-
source paragraph-1
source paragraph-2
source paragraph-3
Melatoninsupplement
word context
attention (t)
word context
word context
agent context
vector ( )
Fragrances
vocabulary distribution
final distribution
attention (t-1)
Contextual
Contextual
Figure 2: Multi-agent-encoder-decoder overview. Each agent a encodes a paragraph using a local encoder followed
by multiple contextual layers with agent communication through concentrated messages z(k)
at each layer k.
Communication is illustrated in Figure 3. The word context vectors ct
a are condensed into agent context c∗
Agent speciﬁc generation probabilities, pt
a, enable voting for the suitable out-of-vocabulary words (e.g., ’yen’) in
the ﬁnal distribution.
peat the process across multiple layers, generating new messages at each layer. Once each agent
completes encoding, they deliver their information
to the decoder with a novel contextual agent attention (Figure 2). Contextual agent attention enables
the decoder to integrate information from multiple
agents smoothly at each decoding step. The network is trained end-to-end using self-critical reinforcement learning to generate focused and coherent summaries.
Empirical results on the CNN/DailyMail and
New York Times datasets demonstrate that multiple communicating encoders lead to higher quality
summaries compared to strong baselines, including those based on a single encoder or multiple
non-communicating encoders. Human evaluations
indicate that our model is able to produce more focused summaries. The agents gather salient information from multiple areas of the document, and
communicate their information with one another,
thus reducing common mistakes such as missing
key facts, repeating the same content, or including
unnecessary details. Further analysis reveals that
our model attains better performance when the decoder interacts with multiple agents in a more balanced way, conﬁrming the beneﬁt of representing
a long document with multiple encoding agents.
We extend the CommNet model of Sukhbaatar
et al. for sequence generation.
Each document d is a sequence of
paragraphs xa, which are split across multiple encoding agents a=1,..,M (e.g., agent-1 encodes the
ﬁrst paragraph x1, agent-2 the second paragraph
x2, so on). Each paragraph xa={wa,i}I, is a sequence of I words. We construct a V -sized vocabulary from the training documents from the most
frequently appearing words.
Each word wa,i is
embedded into a n-dimensional vector ea,i. All
W variables are linear projection matrices.
Multi-Agent Encoder
Each agent encodes the word sequences with the
following two stacked encoders.
Local Encoder The ﬁrst layer is a local encoder
of each agent a, where the tokens of the corresponding paragraph xa are fed into a single layer
bi-directional LSTM (bLSTM), producing the local encoder hidden states, h(1)
i , ←−h (1)
= bLSTM(ei, −→h (1)
i−1, ←−h (1)
= W1[−→h (1)
i , ←−h (1)
where H is the hidden state dimensionality. The
output of the local encoder layer is fed into the
contextual encoder.
Contextual Encoder Our framework enables
agent communication cycles across multiple encoding layers. The output of each contextual encoder is an adapted representation of the agent’s
encoded information conditioned on the information received from the other agents. At each layer
k=1,..,K, each agent a jointly encodes the information received from the previous layer (see Figure 3). Each cell of the (k+1)th contextual layer
is a bLSTM that takes three inputs: the hidden
states from the adjacent LSTM cells, −→h (k+1)
or ←−h (k+1)
∈RH, the hidden state from the previous layer h(k)
, and the message vector from other
agents z(k)∈RH and outputs h(k+1)
, ←−h (k+1)
=bLSTM(f(h(k)
, ←−h (k+1)
= W2[−→h (k+1)
, ←−h (k+1)
where i=1..I indicates the index of each token in
the sequence.
The message z(k) received by any agent a in
layer k is the average of the outputs of the other
agents from layer k:
where h(k)
m,I is the last hidden state output from the
kth contextual layer of each agent where m ̸= a.
Here, we take the average of the messages received from other encoder agents, but a parametric
function such as a feed forward model or an attention over messages could also be used.
The message z(k) is projected with the agent’s
previous encoding of its document:
, z(k)) = vT
1tanh(W3h(k)
where v1, W3, and W4 are learned parameters
shared by every agent. Equation (7) combines the
information sent by other agents with the context
of the current token from this paragraph.
yields different features about the current context
in relation to other topics in the source document.
At each layer, the agent modiﬁes its representation of its own context relative to the information
received from other agents, and updates the information it sends to other agents accordingly.
Multi-agent encoder message passing.
Agents b and c transmit the last hidden state output (I)
of the current layer k as a message, which are passed
through an average pool (Eq. (6)). The receiving agent
a uses the new message z(k)
as additional input to its
next layer.
Decoder with Agent Attention
The output from the last contextual encoder layer
of each agent {h(K)
a,i }I, which is a sequence of
hidden state vectors of each token i, is sent to
the decoder to calculate word-attention distributions. We use a single-layer LSTM for the decoder
and feed the last hidden state from the ﬁrst agent
1,I as the initial state. At each time step t,
the decoder predicts a new word in the summary
wt and computes a new state st by attending to
relevant input context provided by the agents.
The decoder uses a new hierarchical attention
mechanism over the agents. First, a word attention
distribution lt
a ) is computed over every token {h(K)
a,i }I for each agent a:
a = softmax(vT
2 tanh(W5h(K)
+ W6st + b1))
a∈ I is the attention over all tokens in
a paragraph xa and v2, W5, W6, b1 are learned parameters. For each decoding step t, a new decoder
context is calculated for each agent:
which is the weighted sum of the encoder hidden
states of agent a. Each word context vector represents the information extracted by the agent from
the paragraph it has read. Here the decoder has to
decide which information is more relevant to the
current decoding step t. This is done by weighting
each context vector by an agent attention yielding
the document global agent attention distribution
gt (see Figure 2):
gt = softmax(vT
3tanh(W7ct + W8st + b2))
where v3, W7, W8, and b2 are learned, and
gt ∈ M is a soft selection over M agents.
Then, we compute the agent context vector c∗
The agent context c∗
t ∈RH is a ﬁxed length vector
encoding salient information from the entire document provided by the agents. It is then concatenated with the decoder state st and fed through
a multi-layer perception to produce a vocabulary
distribution (over all vocabulary words) at time t:
P voc(wt|st, wt−1) = softmax(MLP([st, c∗
To keep the topics of generated sentences intact,
it is reasonable that the decoder utilize the same
agents over the course of short sequences (e.g.,
within a sentence).
Because the decoder is designed to select which agent to attend to at each
time step, we introduce contextual agent attention
(caa) to prevent it from frequently switching between agents. The previous step’s agent attention
t−1 is used as additional information to the decoding step to generate a distribution over words:
P voc(wt|·) = softmax(MLP([st, c∗
t−1])) (13)
Multi-Agent Pointer Network
Similar to See et al. , we allow for copying
candidate words from different paragraphs of the
document by computing a generation probability
value for each agent pt
a ∈ at each timestep t
using the context vector ct
a, decoder state st and
decoder input yt:
where b is a learned scalar, yt is the groundtruth/predicted output (depending on the training/testing time). The generation probability determines whether to generate a word from the vocabulary by sampling from P voc(w|·), or copying a word from the corresponding agent’s input
paragraph xa by sampling from its attention distribution lt
a. This produces an extended vocabulary
that includes words in the document that are considered out-of-vocabulary (OOV). A probability
distribution over the extended vocabulary is computed for each agent:
P a(wt|·) = pt
aP voc(wt|·) + (1 −pt
a,w is the sum of the attention for all
instances where w appears in the source document. The ﬁnal distribution over the extended vocabulary, from which we sample, is obtained by
weighting each agent by their corresponding agent
attention values gt
P(wt|st, wt−1) = P
aP a(wt|·)
In contrast to a single-agent baseline , our model allows each agent to vote for different OOV words at time t (Equation (16)). In
such a case, only the word that is relevant to the
generated summary up to time t is collaboratively
voted as a result of agent attention probability gt
Mixed Objective Learning
To train the deep communicating agents, we use
a mixed training objective that jointly optimizes
multiple losses, which we describe below.
Our baseline multi-agent model uses maximum likelihood training for sequence generation. Given y∗= {y∗
T } as the groundtruth output sequence (human summary word sequences) for a given input document d, we minimize the negative log-likelihood of the target word
LMLE = −PN
t=1 logp(y∗
1 . . . y∗
Semantic Cohesion
To encourage sentences in
the summary to be informative without repetition,
we include a semantic cohesion loss to integrate
sentence-level semantics into the learning objective. As the decoder generates the output word sequence {y1, y2 . . . yT }, it keeps track of the end
of sentence delimiter token (‘.’) indices. The hidden state vectors at the end of each sentence s′
q=1. . . Q, where s′
q∈{st:yt=‘·’, 1≤t≤T}, are used
to compute the cosine similarity between two consecutively generated sentences. To minimize the
similarity between end-of-sentence hidden states
we deﬁne a semantic cohesion loss:
q=2 cos(s′
The ﬁnal training objective is then:
LMLE-SEM = LMLE + λLSEM
where λ is a tunable hyperparameter.
Reinforcement Learning (RL) Loss
gradient methods can directly optimize discrete
target evaluation metrics such as ROUGE that
are non-differentiable . At each time step, the word generated by
the model can be viewed as an action taken by an
RL agent. Once the full sequence ˆy is generated, it
is compared against the ground truth sequence y∗
to compute the reward r(ˆy).
Our model learns using a self-critical training
approach , which learns by exploring new sequences and comparing them to the
best greedily decoded sequence. For each training
example d, two output sequences are generated:
ˆy, which is sampled from the probability distribution at each time step, p(ˆyt|ˆy1 . . . ˆyt−1, d), and ˜y,
the baseline output, which is greedily generated by
argmax decoding from p(˜yt|˜y1 . . . ˜yt−1, d).
training objective is then to minimize:
LRL = (r(˜y) −r(ˆy)) PN
t=1 logp(ˆyt|ˆy1 . . . ˆyt−1, d)
This loss ensures that, with better exploration, the
model learns to generate sequences ˆy that receive
higher rewards compared to the baseline ˜y, increasing overall reward expectation of the model.
Mixed Loss
While training with only MLE loss
will learn a better language model, this may not
guarantee better results on global performance
measures. Similarly, optimizing with only RL loss
may increase the reward gathered at the expense
of diminished readability and ﬂuency of the generated summary . A combination of the two objectives can yield improved
task-speciﬁc scores while maintaining ﬂuency:
LMIXED = γLRL + (1 −γ)LMLE
where γ is a tunable hyperparameter used to balance the two objective functions.
We pre-train
our models with MLE loss, and then switch to the
mixed loss. We can also add the semantic cohesion
loss term: LMIXED-SEM = γLRL+(1−γ)LMLE-SEM
to analyze its impact in RL training.
Intermediate Rewards
We introduce sentencebased rewards as opposed to end of summary rewards, using differential ROUGE metrics, to promote generating diverse sentences. Rather than rewarding sentences based on the scores obtained at
the end of the generated summary, we compute incremental rouge scores of a generated sentence ˆoq:
r(ˆoq) = r([ˆo1, . . . ˆoq]) −r([ˆo1, . . . ˆoq−1])
Sentences are rewarded for the increase in
ROUGE they contribute to the full sequence, ensuring that the current sentence contributed novel
information to the overall summary.
Experimental Setup
Datasets We conducted experiments on two summarization datasets: CNN/DailyMail and New York
Times (NYT) . We replicate the
preprocessing steps of Paulus et al. to obtain the same data splits, except that we do not
anonymize named entities. For our DCA models,
we initialize the number of agents before training,
and partition the document among the agents (i.e.,
three agent →three paragraphs). Additional details can be found in Appendix A.1.
Training Details
During training and testing we
truncate the article to 800 tokens and limit the
length of the summary to 100 tokens for training and 110 tokens at test time. We distribute the
truncated articles among agents for multi-agent
models, preserving the paragraph and sentences
as possible. For both datasets, we limit the input and output vocabulary size to the 50,000 most
frequent tokens in the training set. We train with
up to two contextual layers in all the DCA models
as more layers did not provide additional performance gains. We ﬁx γ = 0.97 for the RL term in
Equation (21) and λ = 0.1 for the SEM term in
MLE and MIXED training. Additional details are
provided in Appendix A.2.
Evaluation
ROUGE-1 (unigram recall), ROUGE-2 (bigram
recall) and ROUGE-L (longest common sequence).1
We select the MLE models with the
lowest negative log-likelihood and the MLE+RL
models with the highest ROUGE-L scores on a
sample of validation data to evaluate on the test
1We use pyrouge (pypi.python.org/pypi/pyrouge/0.1.3).
SummaRuNNer 
graph-based attention 
pointer generator 
pointer generator + coverage 
controlled summarization with ﬁxed values 
RL, with intra-attention 
ML+RL, with intra-attention 
(m1) MLE, pgen, no-comm (1-agent) (our baseline-1)
(m2) MLE+SEM, pgen, no-comm (1-agent) (our baseline-2)
(m3) MLE+RL, pgen, no-comm (1-agent) (our baseline-3)
(m4) DCA MLE+SEM, pgen, no-comm (3-agents)
(m5) DCA MLE+SEM, mpgen, with-comm (3-agents)
(m6) DCA MLE+SEM, mpgen, with-comm, with caa (3-agents)
(m7) DCA MLE+SEM+RL, mpgen, with-comm, with caa (3-agents)
Table 1: Comparison results on the CNN/Daily Mail test set using the F1 variants of Rouge. Best model models
are bolded.
ML, no intra-attention 
RL, no intra-attention 
ML+RL, no intra-attention 
(m1) MLE, pgen, no-comm (1-agent) (our baseline-1)
(m2) MLE+SEM, pgen, no-comm (1-agent) (our baseline-2)
(m3) MLE+RL, pgen, no-comm (1-agent) (our baseline-3)
(m4) DCA MLE+SEM, pgen, no-comm (3-agents)
(m5) DCA MLE+SEM, mpgen, with-comm (3-agents)
(m6) DCA MLE+SEM, mpgen, with-comm, with caa (3-agents)
(m7) DCA MLE+SEM+RL, mpgen with-comm, with caa (3-agents)
Table 2: Comparison results on the New York Times test set using the F1 variants of Rouge. Best model models
are bolded.
set. At test time, we use beam search of width 5
on all our models to generate ﬁnal predictions.
Baselines We compare our DCA models against
previously published models:
SummaRuNNer
 , a graph-based attentional
neural model an RNN-based extractive summarizer that combines abstractive features during training; Pointer-networks with and
without coverage , RL-based
training for summarization with intra-decoder attention ), and Controllable Abstractive Summarization which
allows users to deﬁne attributes of generated summaries and also uses a copy mechanism for source
entities and decoder attention to reduce repetition.
Ablations We investigate each new component
of our model with a different ablation, producing
seven different models. Our ﬁrst three ablations
are: a single-agent model with the same local encoder, context encoder, and pointer network architectures as the DCA encoders trained with MLE
loss (m1); the same model trained with additional
semantic cohesion SEM loss (m2), and the same
model as the (m1) but trained with a mixed loss
and end-of-summary rewards (m3).
The rest of our models use 3 agents and incrementally add one component.
First, we add
the semantic cohesion loss (m4). Then, we add
multi-agent pointer networks (mpgen) and agent
communication (m5).
Finally, we add contextual agent attention (caa) (m6), and train with the
mixed MLE+RL+SEM loss (m7). All DCA models use pointer networks.
Quantitative Analysis
We show our results on the CNN/DailyMail and
NYT datasets in Table 1 and
2 respectively.
Overall, our (m6) and (m7) models with multiagent encoders, pointer generation, and communication are the strongest models on ROUGE-1
and ROUGE-2. While weaker on ROUGE-L than
the RL model from Paulus et al. , the human evaluations in that work showed that their
model received lower readability and relevance
scores than a model trained with MLE, indicating
the additional boost in ROUGE-L was not correlated with summary quality. This result can also
account for our best models being more abstractive. Our models use mixed loss not just to op-
timize for sentence level structure similarity with
the reference summary (to get higher ROUGE as
reward), but also to learn parameters to improve
semantic coherence, promoting higher abstraction
(see Table 4 and Appendix B for generated summary examples).
Table 3: Comparison of multi-agent models varying the
number of agents using ROUGE results of model (m7)
from Table 1 on CNN/Daily Maily Dataset.
Single vs. Multi-Agents All multi-agent models
show improvements over the single agent baselines. On the CNN/DailyMail dataset, compared
to MLE published baselines, we improve across
all ROUGE scores.
We found that the 3-agent
models generally outperformed both 2- and 5agent models (see Table 3). This is in part because we truncate documents before training and
the larger number of agents might be more efﬁcient for multi-document summarization.
Independent vs. Communicating Agents When
trained on multiple agents with no communication
(m4), the performance of our DCA models is similar to the single agent baselines (m1) and (m3).
With communication, the biggest jump in ROUGE
is seen on the CNN/DailyMail data, indicating that
the encoders can better identify the key facts in the
input, thereby avoiding unnecessary details.
Contextual Agent Attention (caa) Compared to
the model with no contextualized agent attention (m5), the (m6) model yields better ROUGE
scores. The stability provided by the caa helps the
decoder avoid frequent switches between agents
that would dilute the topical signal captured by
each encoder.
Repetition Penalty As neurally generated summaries can be redundant, we introduced the semantic cohesion penalty and incremental rewards
for RL to generate semantically diverse summaries.
Our baseline model optimized together
with SEM loss (m2) improves on all ROUGE
scores over the baseline (m1).
Similarly, our
model trained with reinforcement learning uses
sentence based intermediate rewards, which also
improves ROUGE scores across both datasets.
Human Evaluations
We perform human evaluations to establish that
our model’s ROUGE improvements are correlated
with human judgments.
We measure the communicative multi-agent network with contextual
agent attention in comparison to a single-agent
network with no communication. We use the following as evaluation criteria for generated summaries: (1) non-redundancy, fewer of the same
ideas are repeated, (2) coherence, ideas are expressed clearly; (3) focus, the main ideas of the
document are shared while avoiding superﬂuous
details, and (4) overall, the summary effectively
communicates the article’s content. The focus and
non-redundancy dimensions help quantify the impact of multi-agent communication in our model,
while coherence helps to evaluate the impact of the
reward based learning and repetition penalty of the
proposed models.
Evaluation Procedure We randomly selected 100
samples from the CNN/DailyMail test set and
use workers from Amazon Mechanical Turk as
judges to evaluate them on the four criteria deﬁned
above. Judges are shown the original document,
the ground truth summary, and two model summaries and are asked to evaluate each summary on
the four criteria using a Likert scale from 1 (worst)
to 5 (best). The ground truth and model summaries
are presented to the judges in random order. Each
summary is rated by 5 judges and the results are
averaged across all examples and judges.
We also performed a head-to-head evaluation
(more common in DUC style evaluations) and randomly show two model generated summaries. We
ask the human annotators to rate each summary
on the same metrics as before without seeing the
source document or ground truth summaries.
Results Human evaluators signiﬁcantly prefer
summaries generated by the communicating encoders. In the rating task, evaluators preferred the
multi-agent summaries to the single-agent cases
for all metrics. In the head-to-head evaluation, humans consistently preferred the DCA summaries
to those generated by a single agent. In both the
head-to-head and the rating evaluation, the largest
improvement for the DCA model was on the focus question, indicating that the model learns to
generate summaries with more pertinent details by
capturing salient information from later portions
of the document.
Mr Turnbull was interviewed about his childhood and his political stance. He also admitted he planned to run
for prime minister if Tony Abbott had been successfully toppled in February’s leadership spill. The words
’primed minister’ were controversially also printed on the cover.
Malcolm Turnbull is set to feature on the front cover of the GQ Australia in a bold move that will no doubt set senators’ tongues wagging. Posing in a suave blue suit with a pinstriped shirt and a contrasting red tie , Mr Turnbull’s
conﬁdent demeanour is complimented by the bold, confronting words printed across the page: ’primed minister’.
Malcolm Turnbull was set to run for prime minister if Tony Abbott had been successfully toppled in February’s leadership spill. He is set to feature on the front cover of the liberal party’s newsletter.
Daphne Selfe has been modelling since the ﬁfties. She has recently landed a new campaign with vans and & other
stories. The 86-year-old commands 1,000 a day for her work.
Daphne Selfe, 86, shows off the collaboration between the footwearsuper-brandand theetherealhigh street store
with uncompromisinggrace. Daphne said of the collection , in which she appears with 22-year-old ﬂo dron: ’the
& other stories collection that is featured in this story is truly relaxed and timeless with a modern twist’. The
shoes are then worn with pieces from the brands ss2015 collection.
Daphne Selfe, 86, has starred in the campaign for vans and & other stories. The model appears with 22-year-old
ﬂo dron & other hair collection . She was still commanding 1,000 a day for her work.
Table 4: Comparison of a human summary to best single- and multi-agent model summaries, (m3) and (m7)
from CNN/DailyMail dataset. Although single-agent model generates a coherent summary, it is less focused and
contains more unnecessary details ( highlighed red ) and misses keys facts that the multi-agent model successfully
captures (bolded).
Head-to-Head
Score Based
non-redundancy
Table 5: Head-to-Head and score-based comparison
of human evaluations on random subset of CNN/DM
dataset. SA=single, MA=multi-agent. ∗indicates statistical signiﬁcance at p < 0.001 for focus and p <
0.03 for the overall.
Communication improves focus
To investigate how much the multi-agent models
discover salient concepts in comparison to single agent models, we analyze ROUGE-L scores
based on the average attention received by each
agent. We compute the average attention received
by each agent per decoding time step for every
generated summary in the CNN/Daily Mail test
corpus, bin the document-summary pairs by the
attention received by each agent, and average the
ROUGE-L scores for the summaries in each bin.
Figure 4 outlines two interesting results. First,
summaries generated with a more distributed attention over the agents yield higher ROUGE-L
scores, indicating that attending to multiple areas
of the document allows the discovery of salient
concepts in the later sections of the text. Second,
if we use the same bins and generate summaries
for the documents in each bin using the singleagent model, the average ROUGE-L scores for the
single-agent summaries are lower than for the cor-
Multi Agent #1 vs Single Agent
Multi Agent #2 vs Single Agent
Attention received by Agent
Multi Agent #3 vs Single Agent
Figure 4: The average ROUGE-L scores for summaries
that are binned by each agent’s average attention when
generating the summary (see Section 5.2). When the
agents contribute equally to the summary, the ROUGE-
L score increases.
responding multi-agent summaries, indicating that
even in cases where one agent dominates the attention, communication between agents allows the
model to generate more focused summaries.
Qualitatively, we see this effect in Table 4,
where we compare the human generated summaries against our best single agent model (m3)
and our best multi-agent model (m7). Model (m3)
generates good summaries but does not capture all
the facts in the human summary, while (m7) is able
to include all the facts with few extra details, generating more relevant and diverse summaries.
Related Work
Several recent works investigate attention mechanisms for encoder-decoder models to sharpen the
context that the decoder should focus on within the
input encoding . For example, Luong et al. proposes global and local attention networks for machine translation, while others investigate hierarchical attention networks for
document classiﬁcation , sentiment classiﬁcation , and dialog
response selection .
Attention mechanisms have shown to be crucial
for summarization as well , and pointer
networks , in particular,
help address redundancy and saliency in generated
summaries . While
we share the same motivation as these works,
our work uniquely presents an approach based on
CommNet, the deep communicating agent framework . Compared to prior
multi-agent works on logic puzzles , language learning and starcraft games
 , we present the ﬁrst study in
using this framework for long text generation.
Finally, our model is related to prior works that
address repetitions in generating long text. See
et al. introduce a post-trained coverage network to penalize repeated attentions over the same
regions in the input, while Paulus et al. use
intra-decoder attention to punish generating the
same words. In contrast, we propose a new semantic coherence loss and intermediate sentencebased rewards for reinforcement learning to discourage semantically similar generations (§3).
Conclusions
We investigated the problem of encoding long
text to generate abstractive summaries and demonstrated that the use of deep communicating agents
can improve summarization by both automatic and
manual evaluation.
Analysis demonstrates that
this improvement is due to the improved ability of
covering all and only salient concepts and maintaining semantic coherence in summaries.