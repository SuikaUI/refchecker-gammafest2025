Testing equality of functions under monotonicity constraints
C´ecile Durot, Piet Groeneboom and Hendrik P. Lopuha¨a
September 4, 2018
We consider the problem of testing equality of functions fj : [a, b] →R for j =
1, 2, . . . , J on the basis of J independent samples from possibly diﬀerent distributions
under the assumption that the functions are monotone. We provide a uniform approach
that covers testing equality of monotone regression curves, equality of monotone densities
and equality of monotone hazards in the random censorship model. Two test statistics are
proposed based on L1-distances. We show that both statistics are asymptotically normal
and we provide bootstrap implementations, which are shown to have critical regions with
asymptotic level α.
Introduction
A classical statistical problem is the k-sample problem, where one has to decide whether different samples can be regarded as coming from the same population. In the non-parametric
setting, and introduced the ﬁrst two-sample tests, one based on the distance between
the empirical distribution functions of the two samples and one based on ranks. Generalizations of these methods to the k-sample problem, with k ≥2, are given in and ,
respectively. Although Wilcoxon-type tests remain very popular, they are known to be able
to detect only a limited range of alternatives. To overcome this diﬃculty, several solutions
have been proposed. For an overview and references on this topic, see , who developed a
two-sample test inspired by the smooth Neymann test, in which the problem is reparametrized
and modeled via some multivariate exponential family with an unknown parameter in such
a way that the two-sample problem amounts to testing that this parameter is zero. Another
recent approach consist of comparing empirical characteristic functions, e.g., see , who
extended the univariate two-sample tests from and to the multivariate k-sample setting, k ≥2. For procedures based on comparing kernel density estimators, see Anderson,
Hall and Titterington , who consider the L2-distance in the multivariate two sample setting, and , who uses the L1- and the L∞-distance, and see and references therein, for
methods based on empirical likelihood. A generalization of the Smirnov test to the case of
multivariate observations is considered in , among others. See also for more
references on multivariate k-sample tests. The recent paper by points out that most
of the rank based tests proposed in the literature, unless one-sided, could be biased against
alternatives of interest. In that paper, multivariate distribution free two-sample tests, based
on the ranks of suitable distances of multivariate observations, are developed. Unbiasedness
and power of those tests are studied against Lehmann alternatives.
The k-sample problem arises naturally in survival analysis, where the observations are
typically right censored. Various two-sample tests inspired by the Wilcoxon test have been
proposed for right censored data, see and references therein. Other approaches
are based for instance on comparison of quantile estimators, see Li, Tiwari and Welles .
The k-sample problem also arises in the regression setting. For instance, in medical studies
one wishes to compare the mean response of a treatment group of subjects to that of a control
group, taking into account a covariate such as the dose of drug. In such cases, one wishes
to compare two or more regression curves. In this setting, developed a two-sample test
calibrated by the bootstrap. The test statistic, which is essentially a scaled version of an
integrated L2-distance between smooth estimators of the regression curves (see also King,
Hart and Wehrly ), is based on diﬀerences between the response variables at given values
of the covariate. Another two-sample test based on these diﬀerences, which in spirit resembles
the Kolmogorov-Smirnov statistic, is proposed in . In the case where the covariate values as
well as the sample size possibly diﬀer in the two considered samples, developed a test based
on quasi-residuals. In this setting with possibly heteroscedastic errors, consider a test
based on the estimation of the L2-distance between the two regression curves and generalized
the method to the case of k-samples, k ≥2, whereas developed a test calibrated by the
bootstrap, which is based on the diﬀerence of two marked empirical processes. Most of the
aforementioned procedures involve the choice of a tuning parameter.
Nonparametric methods for k-sample problems under shape constraints are quite limited.
 considers likelihood ratio type two-sample tests in the current status model, which is
closely related to other shape constrained nonparametric models. The test statistics are shown
to be asymptotically normal under the null hypothesis of equal distributions and the test is
calibrated using the bootstrap. The problem of comparing two monotone fractile regression
functions, which is similar in spirit to the problem of comparing two monotone regression
functions, is considered by .
A test based on the L2-distance between two monotone
estimators is discussed. The test is calibrated by the bootstrap, but no limit distribution is
The above mentioned testing problems have one common feature, i.e., they essentially test
the equality of two or more functions in various sorts of statistical models, e.g., distribution
functions, densities, characteristic functions, hazard rates, or regression curves. In this paper
we consider the following testing problem
H0 : f1 = f2 = · · · = fJ
H1 : fi ̸= fj for some i ̸= j
subject to the constraint that fj : [a, b] 7→R is decreasing for all j = 1, 2, . . . , J, where a, b ∈R
are known. This general framework includes a k-sample test for samples from a decreasing
density and a test for equality of decreasing regression curves or decreasing hazard rates. We
propose two test statistics based on L1-distances between non-parametric Grenander-type
estimators bfnj.
The ﬁrst one compares mutual distances between the diﬀerent individual
estimators
| bfni(t) −bfnj(t)| dt.
The second one compares the distances between each individual bfnj and a pooled estimator bfn0
for the mutual f0 under the null hypothesis:
| bfnj(t) −bfn0(t)| dt.
We show that both test statistics are asymptotically normal and propose a bootstrap procedure to calibrate the test. Finally, we discuss the special cases of testing equality of monotone
regression functions, monotone densities and monotone hazard rates under random censorship, and show in each of these cases that the bootstrap works.
In Section 2 we describe the general setup, state our main results, and discuss the diﬀerences between the approach in this paper and the ones used in Groeneboom, Hooghiemstra
and Lopuha¨a , and to prove similar results. In Section 3 we discuss the calibration
of the test and show that the bootstrap works in the previous mentioned statistical models.
All main proofs are postponed to an appendix at the end of the paper, and details are given
in the supplement.
Main results
For estimating the functions fj : [a, b] →R, we suppose that for each j = 1, 2, . . . , J, and
t ∈[a, b],
is well deﬁned and that we have an estimator Fnj at hand based on nj observations. We
denote by n = P
j nj the total number of observations and for notational convenience (and
possibly arguing along subsequences), we assume that nj = cj · n where cj > 0 does not
depend on n. Thus, P
j cj = 1. Denote f0 and F0 for the corresponding quantities under the
null hypothesis, where we estimate F0 by
Then, for all j = 0, 1 . . . , J, we deﬁne bfnj as the left-hand slope of the least concave majorant
of Fnj, with bfnj(a) = limt↓a bfnj(t). We will frequently make use of results from and .
Similar to these papers, we will work under the following assumptions:
(A0) The estimators Fn1, Fn2, . . . , FnJ are independent and for every j = 1, 2, . . . , J, the
estimator Fnj : [a, b] →R is a cadlag step process.
(A1) For each j = 1, 2, . . . , J, the function fj : [a, b] 7→R is decreasing and continuously
diﬀerentiable, such that 0 < inft∈[a,b] |f ′
j(t)| ≤supt∈[a,b] |f ′
j(t)| < ∞.
(A2) For each j = 1, 2, . . . , J, there exists a constant Cj > 0, such that for all x ≥0 and
t = a, b, the process Mnj = Fnj −Fj satisﬁes
u∈[a,b], x/2≤|t−u|≤x
(Mnj(t) −Mnj(u))2
Furthermore, we assume that there exists an embedding either into Brownian motion or into
Brownian bridge.
(A3) For each j = 1, 2, . . . , J, there exists a Brownian motion or Brownian bridge Bnj, an
increasing function Lj : [a, b] 7→R with inft∈[a,b] L′
j(t) > 0, and constants q > 6 and
C > 0, such that for all x ∈(0, nj]:
Mnj(t) −n−1/2
Bnj ◦Lj(t)
It should be noticed that, since the Fnj are assumed to be independent, we can assume
without loss of generality that the Bnj are independent. Note that, for j = 1, 2, . . . , J, we
Bnj(t) = Wnj(t) −ξnjt,
for t ∈[a, b],
where the Wnj are independent Brownian motions and ξnj ≡0, if Bnj is Brownian motion,
and ξnj ∼N(0, 1) independent of Bnj, if Bnj is Brownian bridge. Finally, we require the
following smoothness assumption.
(A4) There exist a θ ∈(3/4, 1] and C > 0, such that for all x, y ∈[a, b] and j = 1, 2, . . . , J,
j(y)| ≤C|x −y|θ and |L′′
j(y)| ≤C|x −y|θ.
These are the usual assumptions when studying the Lp-error of isotonic estimators.
explained in that several classical models are covered by the above general framework.
As an example we mention the model where one observes J independent samples where the
random variables in the jth sample have a decreasing smooth density function fj. In this
example, Fnj is the empirical distribution function based on the jth sample and Lj = Fj.
This example will be studied in detail in Section 3.4. Two other examples, where fj is either
a regression function or a failure rate, are studied in Sections 3.3 and 3.5.
Our main result is that under the above assumptions both test statistics deﬁned in (1)
and (2) are asymptotically Gaussian under the null hypothesis. In order to formulate these
results more precisely, we introduce the random variables
ζj(c) = argmax
Wj(u + c) −u2
for c ∈R and j = 1, 2, . . . , J,
where the argmax function is the supremum of the times at which the maximum is attained,
W1, W2, . . . , WJ are independent standard two-sided Brownian motions. We are now in the
position to establish asymptotic normality for test statistic Sn1.
Theorem 1 Assume (A0), (A1), (A2), (A3), (A4) and let Sn1 be deﬁned by (1). Let ζj be de-
ﬁned in (6), for j = 1, 2, . . . , J, with independent standard Brownian motions W1, W2, . . . , WJ.
If f0 = f1 = · · · = fJ, then n1/6(n1/3Sn1 −m1) converges in distribution, as n →∞, to the
Gaussian law with mean zero and variance σ2
cov (|Ysi(t) −Ysj(t)|, |Ysl(0) −Ysm(0)|) dt ds,
0(s)|1/3E |Ysi(0) −Ysj(0)| ds,
for j = 1, 2, . . . , J.
In the special case where all Lj = L are the same, after change of variables u = t/L(s)1/3, we
0(s)L′(s)|1/3 ds
This resembles the normalizing constants in Theorem 2 in for the case p = 1. An example
is the monotone density model, where under the null hypothesis L′
j = fj = f0, in which
case m1 and σ2
1 coincide with the normalizing constants in Theorem 1.1 in . In particular,
the limiting variance σ2
1 does not depend on the underlying density f0. Another example,
where the limiting variance does not depend on f0, is the monotone regression model, where
Lj(t) = (t −a)(b −a)τ 2
j , with τ 2
j being the variance of the measurement error.
To establish a similar result for Sn2 is more complex, due to the fact that we have to
deal with diﬀerences between a marginal estimator bfni and the pooled estimator bfn0, which
are both (partly) build from the same ith sample. First of all, we need that, under the null
hypothesis f0 = f1 = · · · = fJ, the above assumptions also hold to some extent for j = 0.
Clearly, assumption (A0) then becomes irrelevant and (A1) is immediate, as well as the ﬁrst
part of (A4). Because, under the null hypothesis,
Mn0(t) = Fn0(t) −F0(t) =
cj(Fnj(t) −Fj(t)) =
the inequality (Jensen)
(a1 + · · · + ak)2 ≤k(a2
1 + · · · + a2
yields that (A2) also holds for j = 0. The remaining assumptions require the deﬁnitions
of L0 and Bn0. To have (A3) for j = 0, we need to show that Mn0 can be approximated
by n−1/2Bn0 ◦L0, for a given increasing function L0 : [a, b] →R with inft∈[a,b] L′
0(t) > 0 and a
Gaussian process Bn0. From relation (8) and assumption (A3), for j = 1, 2, . . . , J, it follows
that we then must have
Bnj ◦Lj ◦L−1
Furthermore, when Bnj = Wnj, for j = 1, 2, . . . , J in (A3), then
Bn0 ◦L0(t) =
Wnj ◦Lj(t) d= W
where W denotes Brownian motion. Hence, using the monotonicity of Lj, for j = 0, 1, . . . , J,
from comparing the covariance functions, it follows that we must have
This L0 is increasing such that inft∈[a,b] L′
0(t) > 0 and the second part of (A4) for j = 0 follows
immediately from the one for j = 1, 2, . . . , n. Note that, in contrast to Bnj, for j = 1, 2, . . . , J,
the process Bn0 is not necessarily a Brownian motion or a Brownian bridge. However, we do
have the following version of condition (A3).
Lemma 1 Assume (A3) and suppose f0 = f1 = · · · = fJ. Let Bn0 and L0 be deﬁned by (10)
and (11), and let Mn0 = Fn0 −F0, where Fn0 is deﬁned in (4). Then, there exists C > 0,
such that for all x ∈(0, n]:
n1−1/q sup
Mn0(t) −n−1/2Bn0 ◦L0(t)
Now that we have established assumptions (A0)-(A4) for the pooled estimator, we proceed
by introducing a suitable variable, such as the one deﬁned in (6), for the case j = 0. However,
this case is more complex and we have to distinguish between two of them. First, for each
ﬁxed t ∈[a, b], deﬁne
eζt0(c) = argmax
Wt0(u + c) −u2o
bζt0(c) = argmax
Wt0(u + c) −u2o
0 (L0(t) + n−1/3u) −Lj(t)
with W1, W2, . . . , WJ being the independent standard Brownian motions used to deﬁne (6)
and L0 deﬁned in (11). Note that for t ∈[a, b] ﬁxed, due to (11), the processes f
are distributed as standard Brownian motion, which means that eζt0(c) and bζt0(c) have the
same distribution as ζj(c). We are now in the position to formulate our second main theorem.
Theorem 2 Assume (A0), (A1), (A2), (A3), (A4) and let Sn2 be deﬁned by (2). Let ζj, eζt0
and bζt0 be deﬁned in (6) and (13), respectively, with independent standard Brownian motions
W1, W2, . . . , WJ. If f0 = f1 = · · · = fJ, then n1/6(n1/3Sn2 −m2) converges in distribution, as
n →∞, to the Gaussian law with mean zero and variance σ2
cov (|Ysi(t) −Ys0(t)|, |Ysj(0) −Ys0(0)|) dt ds,
with Ysj deﬁned in (7) and
Ys0(t) = L′
0(s)1/3eζt0
Furthermore, m2 may depend on n and is deﬁned by
0(t)1/3bζt0(0) −
If in addition, Lj = ajL, for all j = 1, 2, . . . , J, for a given function L : [a, b] →R and given
real numbers aj, then bζt0 = eζt0 and m2 no longer depends on n.
The diﬀerence between the limiting bias m2 and E(n1/3Sn2) will be shown to be of the
order o(n−1/6). Although bζt0(0) can be approximated further by eζt0(0), this approximation
is not suﬃciently strong to cancel the factor n1/6. This diﬃculty does not play a role for
var(n1/3Sn2), for which we only need a consistent approximation. For this reason the limiting
bias m2 may still depend on n, whereas the limiting variance σ2
2 is independent of n. Only
in speciﬁc cases, such as Lj = ajL, the limiting bias will also not depend on n. Examples
are the monotone density model, where under the null hypothesis Lj = Fj = F0, and the
monotone regression model, where Lj(t) = (t −a)(b −a)τ 2
j . Similar to Theorem 1, in these
two cases the limiting variance is again independent of the underlying distribution.
The explicit expressions for the normalizing constants in Theorems 1 and 2 are intractable
for the purpose of building a statistical test because they depend on the fj’s and the Lj’s
in a complicated manner. Therefore, in order to implement our statistical test, we prefer to
approximate the limit distribution of our test statistics using bootstrap methods, as described
in the following section.
Before doing so, we believe it is useful to give the main line of reasoning used to prove
Theorems 1 and 2 and explain the main diﬀerences with the type of argument used to prove
similar results in , and . First note that it suﬃces to prove the results for the case
[a, b] = . This is explained in more detail in the following remark.
Theorem 1 Suppose that for t ∈[a, b] and j = 1, 2, . . . , J, fj(t) satisﬁes conditions (A0),
(A1), (A2), (A3) and (A4) with corresponding Fj, Lj and Fnj on [a, b]. Then this case can
be transformed to the case by considering (b −a)fj(a + x(b −a)) for x ∈ . It is
straightforward to see that, for j = 1, 2, . . . , J, these are functions on that satisfy (A0),
(A1), (A2), (A3) and (A4) with corresponding functions Fj(a + x(b −a)), Lj(a + x(b −
a)) and Fnj(a + x(b −a)) for x ∈ .
Moreover, note that the transformed estimator
(b −a) bfnj(a + x(b −a)) is the left-hand slope of the least concave majorant of the process
{Fnj(a + u(b −a)), u ∈ } at the point u = x.
When Theorems 1 and 2 have been
established for the case , then the results for the general case [a, b] follow immediately
and the expressions of the limiting constants can by found by plugging in the transformed
expressions.
Hence, in the rest of the section we assume [a, b] = . When we deﬁne
Wnj ◦Lj ◦L−1
ξnjLj ◦L−1
with Bn0 and L0 deﬁned by (10) and (11), respectively, then similar to (5), also Bn0(t) =
Wn0(t) −ξn0(t), where the process Wn0(t) is a standard Brownian motion and ξn0 is independent of Bn0. For every j = 0, 1, . . . , J, deﬁne
nj(t) = Fnj(t),
nj(t) = n−1/2
Bnj(Lj(t)) + Fj(t),
nj (t) = n−1/2
Wnj(Lj(t)) + Fj(t).
and for S = E, B, W
Obviously, F E
nj is the estimator for (3) or F0 and, although F B
nj and F W
nj are not estimators
in the sense that they are built from observations, we can deﬁne the corresponding slope
nj(t) = left slope of the least concave majorant (LCM) of F S
nj(u) at u = t,
for j = 0, 1, . . . , J and S = E, B, W. When investigating the asymptotic behavior of bf E
one typically exploits the fact that F E
ni can be approximated by F W
ni , using (A3) and (17).
However, if two processes are uniformly close, then the slopes of the concave majorants of both
processes are not necessarily uniformly close. For this reason, we introduce the (generalized)
inverse of bf S
nj, deﬁned for a ∈R by bU S
nj(a) = sup{t ∈ : bf S
nj(t) ≥a}, with the convention
that the supremum of an empty set is zero. It is fairly easy to see that
nj(a) = argmax
for a ≥0, and that
if and only if
This means bU S
nj is closely connected to bf S
nj, but its asymptotic behavior is more tractable
because if two processes are close, then also the locations of their maxima are close. For this
reason, the approach used in , and , which is originally due to , is to switch
from Lp-errors in terms of bf E
ni to Lp-errors in terms of bU E
ni. The next lemma provides such
an approximation suitable for our purposes. For S = E, this result is similar to Corollary 2.1
in , Lemma 2.1 in and equality (21) in . For later purposes, e.g, see (22), we also
establish the approximation for the cases S = B, W.
Lemma 2 Assume (A1), (A2), (A3), and suppose f0 = f1 = · · · = fJ. Then for i, j =
0, 1, . . . , J and S = E, B, W,
ni(t) −bf S
nj(t)| dt = n1/3
ni(a) −bU S
nj(a)| da + op(n−1/6).
Proceeding in the spirit of , and , the next step would be to replace bU E
the right hand side of (21) by bU W
nj . In statistical models where (A3) holds with Bnj being
Brownian motion, e.g., the regression model and the random censoring model, this can be
done by means of Lemma 5 in . However, in statistical models, where Bnj is Brownian
bridge rather than Brownian motion, e.g., the density model, this is no longer possible. In
such models, the approximation of bU E
nj by bU B
nj is relatively easy, due to assumption (A3).
This assumption will ensure that F E
nj is of order smaller than n−5/6, which in turn
guarantees that bU E
nj will be suﬃciently small.
Lemma 3 Assume (A1), (A3), (A4) and suppose f0 = f1 = · · · = fJ.
Then, for each
j = 0, 1, 2, . . . , J,
nj(a) −bU B
nj(a)| da = op(n−1/6).
This result is similar to Corollary 3.1 in for the density model, but is now extended to
our general setup. However, it is not possible to establish a similar result for bU B
ni and bU W
because F B
ni is of order n−1/2, which is too large. This diﬃculty is solved in , 
and , by subtle use of relationship (5). These approaches apply to the Lp-error in terms of
a single inverse bU B
ni, but they do not extend to our current situation, because the right hand
side of (21) involves the L1-error between two diﬀerent inverses bU B
ni and bU B
For our current setup, we solve this problem by returning to the slopes themselves.
Whereas closeness of F E
nj and F B
nj is not suﬃcient to obtain suitable bounds on bf E
situation is diﬀerent for F B
nj and F W
nj . The reason is that the diﬀerence between Brownian
bridge Bnj and Brownian motion Wnj is only a straight line. We can then obtain the following
slope equivalent of Corollary 3.3 in , which is concerned with a similar approximation for
the location processes deﬁned in (19).
Lemma 4 Assume (A2), (A3). Then, for i, j = 0, 1, . . . , J,
ni(t) −bf B
nj(t)| dt = n1/3
ni (t) −bf W
nj (t)| dt + op(n−1/6).
After having established Lemmas 2, 3 and 4, in order to prove Theorems 1 and 2, we will use
the following line of reasoning:
ni(t) −bf E
nj(t)| dt = n1/3
ni(a) −bU E
nj(a)| da + op(n−1/6)
ni(a) −bU B
nj(a)| da + op(n−1/6)
ni(t) −bf B
nj(t)| dt + op(n−1/6)
ni (t) −bf W
nj (t)| dt + op(n−1/6)
ni (a) −bU W
nj (a)| da + op(n−1/6).
Of course, in models where Bnj is Brownian motion, then bU B
nj , so that Lemma 4
becomes irrelevant, and we can obtain
ni(t) −bf E
nj(t)| dt = n1/3
ni(a) −bU E
nj(a)| da + op(n−1/6)
ni (a) −bU W
nj (a)| da + op(n−1/6),
immediately, either using Lemma 5 in or as a special case of Lemma 3. Once the test
statistics (1) and (2) can be expressed in terms of integrals
ni (a) −bU W
nj (a)| da,
the proof of asymptotic normality follows the same line of reasoning as used in , 
and , using the independent increments property of Brownian motion.
Calibration of the test
This section is devoted to the calibration of the test in three diﬀerent models that are covered
by the general setup for testing equality of monotone functions on [a, b]. Because the limit
distribution of the test statistic under the null hypothesis is intractable, we use a bootstrap
procedure to calibrate the test, see Subsection 3.1.
The bootstrap procedure involves an
estimator that is deﬁned in Subsection 3.2. The three diﬀerent models that we investigate
are described in Subsections 3.3, 3.4 and 3.5 below.
The bootstrap procedure
It is known that the standard bootstrap typically does not work for Grenander-type estimators, e.g., see . These authors propose a smooth bootstrap based on generating
from a kernel smoothed Grenander-type estimator. and discuss a smooth bootstrap
based on a monotonized kernel estimator, which consists of replacing the equal weights 1/n
of the kernel estimator by general weights pj, j = 1, 2, . . . , n in such a way that the resulting
estimator is monotone.
Here we also consider a smoothed bootstrap.
This will require the use of a smooth
estimator efn which, under the null hypothesis f1 = · · · = fJ = f0, satisﬁes bootstrap versions
of assumptions (A0)-(A4). The following general property will be suﬃcient for our purposes.
(A⋆) The estimator efn is continuously diﬀerentiable on [a, b]. Furthermore, there exists an
event An and real numbers θ ∈(3/4, 1] and εn > 0, such that P(An) →1 and nγεn →∞
for any γ > 0, as n →∞, and such that the following three properties hold on An:
| efn(t) −f0(t)|
and for all x, y ∈[a, b],
n(x) −ef ′
n(y)| ≤|x −y|θ/εn.
Condition (23) comes naturally from minimax rates considerations for kernel density estimators, in situations where the underlying density satisﬁes (A4). Condition (24) ensures the
bootstrap version of assumption (A1): if both (A1) and (24) hold, then there exist positive
numbers C0, C1 such that on An, the function efn is decreasing with
t∈[a,b] | ef ′
n(t)| ≤sup
n(t)| < C1.
Condition (25) ensures part of the bootstrap version of assumption (A4). It will have the
same eﬀect as assumption (A4), because nγεn →∞for all γ > 0 (typically one should think
of εn = 1/ log n). Bootstrap versions of (A0), (A2), (A3), and the second part of (A4) require
the deﬁnitions of estimators for Fj and Lj. This will be taken care oﬀin Sections 3.3, 3.4
and 3.5 for the three diﬀerent models that are covered by the general setup.
By means of the estimator efn, we aim to build bootstrap versions S⋆
nk of test statistics Snk,
for k = 1, 2, in such a way that under the null hypothesis and conditionally on the original
observations, n1/6(S⋆
nk −mk) converges in distribution to the Gaussian law with mean zero
and variance σ2
k, in probability, i.e.,
in probability, as n →∞,
where mk and σ2
k are the limit bias and variance given in Theorems 1 and 2, Φ denotes the
distribution function of the standard Gaussian law, and P⋆is the conditional probability
given the original observations. In this case, for a ﬁxed level α ∈(0, 1), one can compute (or
merely approximate thanks to Monte-Carlo simulations) the α-upper percentile point q⋆
of the conditional distribution of S⋆
nk and consider the critical region
If assumptions (A0)–(A4)
are fulﬁlled, then Theorems 1 and 2 together with (27) ensures
that the test with critical region (28) has asymptotic level α.
Below, we will provide an estimator efn satisfying (A⋆) under the null hypothesis in the
general framework of Section 2. Subsequently, we provide, in the three diﬀerent models that
are covered by this general framework, a construction of S⋆
nk that ensures that the test with
critical region (28) has asymptotic level α.
Estimators for the bootstrap procedure
In this subsection, we discuss possible estimators to be used in the bootstrap procedure. For
simplicity, we assume here that under the null hypothesis, the function f0 = f1 = · · · = fJ is
twice continuously diﬀerentiable. We consider a sequence of positive real numbers hn and a
kernel function K : R →R supported on [−1, 1], which is symmetric around zero and three
times continuously diﬀerentiable on R, such that
K(t) dt = 1.
Based on hn and K, we consider a kernel-type estimator efn, corrected at the boundaries in
such a way that ˜fn and ef ′
n converge to f0 and f ′
0, respectively, with a fast rate over the whole
interval [a, b] (whereas we recall that the non-corrected kernel estimator may show diﬃculties
at the boundaries). For every t ∈[a + hn, b −hn] we deﬁne
efn(t) = 1
At the boundaries [a, a + hn) and (b −hn, b], we discuss two possible bias corrections.
The ﬁrst one is local linear ﬁtting (see e.g. ) that was used by in a similar context.
It is deﬁned as follows: for every t ∈[a, a + hn] ∪[b −hn, b],
efn(t) = efn(un) + ef ′
n(un)(t −un),
where un = a + hn for t ∈[a, a + hn] and un = b −hn for t ∈[b −hn, b]. Note that (25)
holds with θ = 1 provided that the second derivative of efn is bounded from above by 1/εn.
It can be proved that with the boundary correction (30), the supremum norm of ef ′′
order 1 + h−5/2
log(1/hn) if hn is of order at least n−2/3 and f0 is twice continuously
diﬀerentiable, so the optimal choice hn ∼n−1/5 is allowed thanks to the presence of εn.
In , the author requires the second derivative to be bounded independently of n, which
rules out the choice hn ∼n−1/5.
However, it can be checked that his result still holds
under the assumption that the supremum norm of ef ′′
n is bounded by some 1/εn satisfying our
assumptions, which means that the choice hn ∼n−1/5 is actually allowed in his case.
Another method to correct the bias is the use of boundary kernels (see e.g. , ). One
possibility is to construct linear combinations of K(u) and uK(u) with coeﬃcients depending
on the value near the boundary (see e.g. ). For t ∈[a, a + hn] ∪[b −hn, b], deﬁne
t ∈[a, a + hn],
t ∈[b −hn, b],
for u ∈R, where for s ∈[−1, 1], the coeﬃcients φ(s) and ψ(s) are determined by
K(u) du + ψ(s)
uK(u) du = 1,
uK(u) du + ψ(s)
u2K(u) du = 0.
The following lemma guarantees that efn with one of the above two boundary corrections
satisﬁes condition (A⋆). The proof is somewhat technical and has been put in the supplement.
Lemma 5 Let efn(t) be deﬁned by (29) for all t ∈[a+hn, b−hn] and either by (30) or by (31)
on the boundaries [a, a + hn) and (b −hn, bn]. Assume hn = Rnn−γ, where 0 < Rn + R−1
OP (1) and γ ∈(1/6, 1/5]. If f0 = f1 = · · · = fJ is twice continuously diﬀerentiable on [a, b]
and (A3) holds with supt∈ L′
j(t) < ∞, for each j = 1, 2, . . . , J, then efn satisﬁes (A⋆).
It may be more natural to use the least concave majorant bFn0 of Fn0 instead of Fn0 in the
deﬁnition of efn. In this case the estimator is a smoothed Grenander type estimator, corrected
at the boundaries. Whether this estimator satisﬁes (A⋆) will depend on how close bFn0 and
Fn0 are. For the density model, showed that the diﬀerence between bFn0 −Fn0 is of the
order n−2/3 log n, and a similar result has been obtained by for the regression model; see
also . This type of result for our general setting is proved in , whereby we prove the
following lemma in the Appendix.
Lemma 6 Let efn(t) be deﬁned by (29) for all t ∈[a + hn, b −hn] and either by (30) or
by (31) on the boundaries [a, a + hn) and (b −hn, bn], with Fn0 replaced by its least concave
majorant bFn0. Assume hn = Rnn−γ, where 0 < Rn + R−1
= OP (1) and γ ∈(1/6, 1/5].
If f0 = f1 = · · · = fJ is twice continuously diﬀerentiable on [a, b] and (A3)
holds with
supt∈ L′
j(t) < ∞, for each j = 1, 2, . . . , J, then efn satisﬁes (A⋆).
It is tempting to consider the pooled Grenander type estimator efn = bfn0 itself in the bootstrap
procedure since bfn0 does not depend on any tuning parameter. Such a bootstrap procedure
was used in in a similar context as our (a two-sample test for monotone fractile regression
functions), but no theoretical result was provided for their procedure. Also in our context,
we were not able to prove that bootstrapping from bfn0 works, since bfn0 does not satisfy (A⋆).
From the results in and it appears that bootstrapping from the Grenander does not
work when the statistic of interest is bfn0 at a ﬁxed point. However, in our situation we are
bootstrapping statistics that are integrals of the diﬀerence of two Grenander estimators, so
it is not clear whether the results by and apply. We investigated bootstrapping from
bfn0 in a simulation study reported in Section 4.
We end this section, by discussing possible estimators Fnj in the three models that are
covered by our setup. Furthermore, for these models, we propose bootstrap versions of our
test statistic for which we show that the test with critical region (28) has asymptotic level α.
It suﬃces to specify bootstrap versions F ⋆
nj, for j = 1, 2, . . . , J. Consequently, F ⋆
n0 is deﬁned
similar to (4), and for k = 1, 2, bootstrap versions S⋆
nk are deﬁned similar to (1) and (2),
nj being the left-hand slope of the least concave majorant of F ⋆
Monotone regression function
For each j = 1, 2, . . . , J, we have observations Yij, for i = 1, 2, . . . , nj, satisfying Yij =
fj(tij) + ǫij, where E(ǫij) = 0 and tij = a + (b −a)i/nj, which means that the observation
points are uniformly spread on [a, b]. We assume that the ǫij’s are independent and that for
each j = 1, 2, . . . , J, the variables ǫij, i = 1, 2, . . . , nj, have the same distribution with a ﬁnite
variance τ 2
j > 0. In this case, the estimator for Fj is
Fnj(t) = 1
Yij1{tij ≤t}.
To deﬁne the bootstrap version of the test statistic, we ﬁrst deﬁne bǫij = Yij −efn(tij), where ˜fn
satisﬁes (A⋆) under H0. One can consider for instance one of the estimators from Lemmas 5
and 6 with Fn0 deﬁned by (4) and (33). Then we deﬁne
eǫij = bǫij −¯ǫj,
where ¯ǫj = 1
for j = 1, 2, . . . , J and i = 1, 2, . . . , nj. Then, conditionally on the original observations Yij,
we deﬁne independent random variables ǫ⋆
ij as follows. For j = 1, 2, . . . , J ﬁxed, each random
variable ǫ⋆
ij is uniformly distributed on {eǫmj, m = 1, 2, . . . , nj}. Finally we set
ij = efn(tij) + ǫ⋆
for j = 1, 2, . . . , J and i = 1, 2, . . . , nj. For j = 0, 1, . . . , J, we deﬁne bootstrap versions F ⋆
the same manner as Fnj in (33) and (4), just by replacing Yij by Y ⋆
ij. The following theorem
states that the bootstrap calibration (28) is consistent under appropriate assumptions.
Theorem 3 Suppose max1≤j≤J E|ǫij|q < ∞, for some q > 6, and that (A1) and (A4) hold
with Lj(t) = (t −a)(b −a)τ 2
Let efn be an estimator that satisﬁes (A⋆) under H0 with
f0 = f1 = · · · = fJ, then for k = 1, 2, the test with critical region (28) has asymptotic level α.
Monotone density
For each j = 1, 2, . . . , J, we have independent observations Xij, for i = 1, 2, . . . , nj, with
density fj : [a, b] →R, where a and b are known real numbers. The J samples are assumed
to be independent and we aim at testing that all observations are from the same density, that
is f1 = · · · = fJ. The estimator for Fj in this case is the empirical distribution function
Fnj(t) = 1
1{Xij ≤t}.
Let efn be a genuine estimator for f0 = f1 = · · · = fJ in the sense that efn is a density function.
One possibility is to use one of the estimators efn from Lemmas 5 and 6 as a starting point.
Note that this function efn need not integrate to one and may even be negative. However,
the function can be shifted upwards by −min{inf efn, 0}, so that it is positive on [a, b], and
then normalized so that it integrates to one. It can be shown that if the original function
satisﬁes condition (A⋆), so does the shifted and normalized version. Moreover, the normalizing
constant need not being computed when generating from this function by means of rejection
To deﬁne the bootstrap versions of Fnj, conditionally on the original observations Xij,
we deﬁne independent random variables X⋆
ij, j = 1, 2, . . . , J, i = 1, 2, . . . , nj, with the same
density efn. Then we deﬁne F ⋆
nj in the same manner as Fnj, just by replacing Xij by X⋆
Theorem 4 Suppose that (A1) and (A4) hold with L′
j = fj and inft∈[a,b] fj(t) > 0, for each
j = 1, 2, . . . , J. Let efn be a genuine estimator that satisﬁes (A⋆) under H0 with f0 = f1 =
· · · = fJ, then for k = 1, 2, the test with critical region (28) has asymptotic level α.
Random censorship with monotone hazard
For each j = 1, 2, . . . , J, we have right-censored observations (Xij, ∆ij), for i = 1, 2, . . . , nj,
where Xij = min(Tij, Yij) and ∆ij = 1{Tij ≤Yij}.
For each j = 1, 2, . . . , J, the failure
times Tij are assumed to be nonnegative independent with density gj and to be independent
of the i.i.d. censoring times Yij that have distribution function Hj. The J samples are assumed
to be independent. The parameters of interest are the failure rates fj = gj/(1 −Gj) on [0, b],
where Gj = 1 −exp(−Fj) is the distribution function corresponding to gj. Note that in this
setting, we only consider the case a = 0, since this is more natural.
The estimator for the cumulative hazard Fj is deﬁned via the Nelson-Aalen estimator Nnj
as follows: let t1j < · · · < tmj denote the ordered distinct uncensored failure times in the
jth sample and nkj the number of i ∈{1, 2, . . . , nj} with Xij ≥tkj, then Nnj is constant
on [tij, ti+1,j) with
Nnj(tij) =
and Nnj(t) = 0 for all t < t1j and Nnj(t) = Nnj(tmj) for all t ≥tmj. The estimator Fnj is
the restriction of Nnj to [0, b]. Finally, as an estimator for the distribution function Hj, we
take the Kaplan-Meier estimator Hnj based on the jth sample.
Let efn be a genuine estimator for f0 = f1 = · · · = fJ in the sense that efn is a non-negative
failure rate. One possibility is to use one of the estimators efn from Lemmas 5 and 6 as a
starting point, and to shift it upwards by −min{−inf efn, 0}, so that it is positive on [0, b]. It
can be shown that if the original function satisﬁes condition (A⋆), so does the shifted version.
To deﬁne a bootstrap version of Fnj, conditionally on the original observations, we ﬁrst deﬁne
independent random variables T ⋆
ij and Y ⋆
ij, for j = 1, 2, . . . , J and i = 1, 2, . . . , nj, where T ⋆
has failure rate efn and Y ⋆
ij has distribution function Hnj. Then we set X⋆
ij = min(T ⋆
ij = 1{T ⋆
ij}. Finally, we deﬁne F ⋆
nj in the same manner as Fnj, just replacing the
(Xij, ∆ij)’s by the (X⋆
ij)’s in the deﬁnition.
Theorem 5 Suppose that (A1) and (A4) hold with L′
j = fj/((1 −Gj)(1 −Hj)) and that for
each j = 1, 2, . . . , J, inft∈[0,b] fj(t) > 0, Gj(b) < 1, limt↑b Hj(t) < 1, and Hj has a bounded
continuous ﬁrst derivative on [0, b]. Let efn be a non-negative estimator that satisﬁes (A⋆) under H0 with f0 = f1 = · · · = fJ, then for k = 1, the test with critical region (28) has
asymptotic level α.
If, in addition, we assume that the censoring variables all have the same distribution function H, then, instead of generating Y ⋆
ij from distribution function Hnj as above, one should
merely generate, the bootstrap censoring times Y ⋆
ij as an n-sample of independent random
variables with common distribution function Hn, the Kaplan-Meier estimator of H based on
all n observations. With this construction of the bootstrap censoring variables we obtain a
similar result.
Theorem 6 Under the assumptions of Theorem 5 with H = H1 = · · · = HJ, for k = 1, 2,
the test with critical region (28) has asymptotic level α.
Simulation Study
To investigate the performance of bootstrapping the test statistics we have performed a simulation study. To alleviate notation, in this section we sometimes omit subscript n, so a
bandwidth is denoted by h rather than hn. Moreover, we deﬁne Kh(x) = h−1K(x/h).
We consider a 3-sample test in the monotone density model. The three densities f1, f2 and
f3 are chosen from the family of exponential densities truncated to the interval :
λe−λx(1 −e−3λ)−1
(a) LSCV(h) for the smoothed Grenander (solid)
with boundary correction (30).
(b) LSCV(h) for the ordinary kernel estimate
(solid) with boundary correction (30).
Figure 1: Cross-validation functions for boundary correction (30) (solid) and for the smoothed
Grenander with boundary correction (31) (dashed).
for x ∈ and f(x, λ) = 0 otherwise. Under the null hypothesis f1 = f2 = f3 = f0 the
bootstrap samples are generated from a pooled estimate for f0 based on the pooled sample
of size n = n1 + n2 + n3. We have several options to construct the smooth estimator efn,h.
One can either smooth the empirical distribution function or smooth the Grenander estimator. Furthermore we can correct the estimator at the boundaries either by (30) or by (31).
According to Lemmas 5 and 6 the bootstrap works for each of these combinations. For the different possibilities, we ﬁrst investigated their performance when determining the bandwidth
of the kernel estimate in a data-adaptive way.
Choice of bandwidth.
We choose to use a so-called “ﬁrst generation method” instead of a
“second generation” adaptive plug-in method, where we would have to assume the existence
of third derivatives. These methods also would be more complicated if we wish to take the
Grenander estimator as the starting point of our smoothing method. The ﬁrst generation
method of our preference is least squares cross-validation, adapted to the present situation,
where we possibly want to smooth the Grenander estimator instead of the empirical distribution function, as in ordinary density estimation. For illustrative purposes, we consider the
family of truncated exponentials in (35). In our experiments, the least squares cross validation
function, as a function of the bandwidth h, is given by
efn,h(t)2 dt −
efn,h(x) dFn0(x) +
where efn,h is the (smooth) estimate of the density, based on the pooled samples, with bandwidth h and Fn0 is the empirical distribution function of the pooled samples. Note that if
efn,h is the ordinary kernel estimator determined with the empirical distribution function Fn0,
then LSCV(h) +
f 2(t) dt is an unbiased estimator of the mean integrated squared error.
(a) LSCV(h) for the smoothed Grenander (solid)
and ordinary kernel estimator (dashed).
(b) Smoothed Grenander (solid), ordinary kernel
estimate (dotted), and the true density (dashed).
Figure 2: Cross-validation functions for the smoothed Grenander and the ordinary kernel
estimator with boundary correction (31) and the resulting density estimates.
It turns out that least squares cross-validation does not work very well for the boundary
correction method (30).
One typically gets a very non-convex function, as illustrated in
Figure 1. Figure 1(a) displays the cross-validation curve (36), for h ∈ , for the smoothed
Grenander and Figure 1(b) displays the same curve for the ordinary kernel estimator, both
with boundary correction (30) (solid curves), for a pooled sample of size n = 300 from
a truncated exponential with parameter λ = 1.
It seems clear that the kernel estimate
based on the Grenander gives a somewhat smoother cross-validation function than the kernel
estimate based on the empirical distribution function, but both curves are very non-convex.
For comparison, the cross-validation function for the smoothed Grenander with boundary
correction (31) has been added (dashed curves).
Boundary correction method (31) generally leads to a convex cross-validation curve with
a clear minimum, as shown in Figure 2(a).
The cross-validation curve for the smoothed
Grenander (solid) with the boundary correction (31) attains its minimum for a value of h
close to 0.6, and the resulting kernel estimate (solid) is shown in Figure 2(b). The crossvalidation curve in Figure 2(a) of the ordinary kernel estimate (dashed) with the boundary
correction (31) lies completely below the cross-validation curve of the kernel estimate based
on the Grenander and Figure 2(b) indeed shows that this kernel estimate is closer to the real
density (dashed) than the smoothed Grenander for h = 0.6. On the other hand, this kernel
estimate will not necessarily be decreasing and we actually prefer a decreasing density like
the smoothed Grenander, which belongs to the allowed class of densities, for generating the
bootstrap samples. After a thorough investigation of the diﬀerent possibilities, the overall
performance of the smoothed Grenander with boundary correction (31) seems to be the best.
Therefore, for t ∈[h, 3 −h] our smooth estimate is deﬁned as
efn,h(t) =
Kh(t −x) d bFn0(x),
where K is a symmetric kernel with support [−1, 1] and bFn0 is the least concave majorant
of the empirical distribution function Fn0.
We correct the kernel density estimate at the
boundaries of by means of (31) with bFn0 instead of Fn0.
Simulating the level and power under alternatives.
To investigate the ﬁnite sample
power at a given combination (λ1, λ2, λ3), we generate three samples of sizes nj from fj =
f(·, λj), for j = 1, 2, 3, and compute the value of the test statistics Sn1 and Sn2, as deﬁned
in (1) and (2). We then generate 1000 times three bootstrap samples of sizes n1, n2 and n3
from the pooled estimate efn,h, compute the values S⋆
n2 of both test statistics and
determine their 5th upper-percentiles q⋆
nk(0.05), for k = 1, 2. This whole procedure is repeated
B times and we count the number of times the values of the test statistics Sn1 and Sn2 exceed
the corresponding 5th upper-percentiles q⋆
n1(0.05) and q⋆
n2(0.05), respectively. By dividing
this number by B, this provides an approximation of the ﬁnite sample power of both test
statistics at underlying truncated exponentials with parameters λ1, λ2 and λ3.
In view of the comments made right after Lemma 6, we compare the behavior of the
smooth bootstrap procedure with bootstrapping from the pooled Grenander estimate itself.
To this end, we also run the same procedure as described above, but then generate the
bootstrap samples from bfn0 instead of the smooth estimate efn,h.
To investigate the performance under the null hypothesis, we take λ1 = λ2 = λ3 equal to
the values 0.1, 0.5, 1, 2, . . . , 6 and equal sample sizes nj = 100 and nj = 250, for j = 1, 2, 3.
The simulated levels are determined by means of B = 10 000 repetitions. The simulations to
investigate the ﬁnite sample power at alternatives are done with sample sizes n1 = n2 = n3 =
100 and alternatives for which λ1 = λ2 = 1 and λ3 varies between 0 and 3.5 by steps of 0.1.
To save computer time, we determined the simulated power at each λ3 by means of B = 1000
repetitions.
Benchmark with true power.
Finally, in order to calibrate the ﬁnite sample power obtained from bootstrapping, we also approximate the true ﬁnite sample power for a given
choice (λ1, λ2, λ3). To this end, we generate 10 000 samples of size n = n1 + n2 + n3 from the
mixture density
f0(x) = c1
1 −e−λ1 + c2
1 −e−λ2 + c3
where cj = nj/n, for j = 1, 2, 3. We consider this as the least favorable density among all
densities under the null hypothesis, in case of three truncated exponentials with parameters
λ1, λ2 and λ3.
For each of the samples we compute the value of the test statistics Sn1
and Sn2, and use this to determine the 5th upper-percentiles qnk(0.05), k = 1, 2, for both test
statistics. Next, we generate another 10 000 times three samples of sizes nj from fj = f(·, λj),
compute both test statistics and count the number of times it exceeds the corresponding 5th
percentile qnk(0.05).
Dividing these numbers by 10 000 provides an approximation of the
true ﬁnite sample power for a given choice (λ1, λ2, λ3). Note that such a calibration is not
implementable in practice since it requires knowledge of f1, f2 and f3, but it may serve as a
benchmark for the power obtained from bootstrapping, in the simulations.
Implementation
We believe it useful to spend some words on how bootstrapping from the smoothed Grenander
has been implemented. First consider the estimate deﬁned in (37) for t ∈[h, 3 −h]. One
possibility to implement this estimate would be to use numerical integration of bfn0. However,
one can also avoid this by using summation by parts. Let p1, . . . , pm be the jump sizes of the
Grenander estimator at the points of jump τ1 < · · · < τm ∈(0, 3), where τm is the largest
order statistic. Note that bfn0 is left-continuous and that bfn0 always has a jump down to zero
at the last order statistic. We now deﬁne
In our simulations we took K(u) = (35/32)
 1 −u23 1[−1,1](u). Then, when deﬁning τ0 = 0,
for t ∈[h, 3 −h], we can write,
efn,h(t) =
Kh(t −x) dx =
Kh(t −x) dx
pjKh(t −τj),
so that for t ∈[h, 3 −h], the estimate efn,h(t) can now be computed as a ﬁnite sum over
the jumps pi of the Grenander estimator bfn0. We then still have to deﬁne efn,h(t) for t ∈
[0, h) ∪(3 −h, 3]. To this end, for j = 0, 1, 2, let
ujK(u) du.
Note that K(0)
h (t) = 1 −Kh(t), where Kh is deﬁned in (38). As before, we get for t < h,
efn,h(t) =
Kh(t −x) + ψ
bfn0(x) dx
K(u) du + ψ
h (t) −K(0)
h (t) −K(1)
where φ and ψ are deﬁned by (32), and similarly for t > 3 −h,
efn,h(t) = φ
h (t −τj).
This means that also near the boundaries of , the estimator efn,h(t) can be computed in
terms of ﬁnite sums over the jumps of the Grenander estimator bfn0.
We ﬁrst investigate the level of the tests under the null hypothesis of all λ’s equal to some λ0,
where we vary λ0 over 0.1, 0.5, 1, 2, . . . , 6. We set the signiﬁcance level α = 0.05 and perform
n1 = n2 = n3 = 100
n1 = n2 = n3 = 250
Table 1: Simulated levels of Sn1 and Sn2 under the null hypothesis.
the bootstrap experiments with n1 = n2 = n3 = 100 and n1 = n2 = n3 = 250 . The results
are listed in Table 1. It can be seen that close to λ0 = 0, which corresponds to the uniform
distribution, the attained level is much too small. For large λ0 the attained levels tend to be
somewhat too large. Note that the simulated levels obtained from bootstrapping from the
Grenander itself are comparable to the ones obtained from the smooth bootstrap.
Next, we investigate the power under alternatives of the form f1 = f2 = f(·, 1) and
f3 = f(·, λ) with n1 = n2 = n3 = 100. A picture of the power estimates of the smoothed
Grenander, using cross-validation for the bandwidth choice, is shown in Figure 3 together
with the estimates obtained by bootstrapping from the Grenander estimator. Figure 3(a)
displays the powers simulated by generating bootstrap samples from the ordinary Grenander
estimator (solid curves) and the direct estimates of the true power (dashed curves). The
top solid and dashed curves correspond to test statistic Sn2. This test statistic seems to be
uniformly more powerful than test statistic Sn1, which corresponds to the bottom solid and
dashed curves. Figure 3(b) displays the powers simulated by generating bootstrap samples
from the smoothed Grenander estimator (solid curves) and the same direct estimates of the
true power (dashed curves). Again the top solid and dashed curves correspond to test statistic
The simulated powers in Figure 3(a), based on bootstrapping from the ordinary Grenander, tend to be conservative. The simulated powers in Figure 3(b), based on bootstrapping
from the smoothed Grenander tend to be slightly anti-conservative. Note that, similar to the
simulated levels in Table 1, there is hardly any diﬀerence between the results when using the
smooth bootstrap or when bootstrapping from the ordinary Grenander. Although, we have
no theoretical evidence, up to this point there is no reason to think that bootstrapping from
the ordinary Grenander does not work.
(a) Simulated powers (solid) from bootstrapping
the Grenander.
(b) Simulated powers (solid) from smooth bootstrap.
Figure 3: Simulated powers (solid) from bootstrapping and estimated true powers (dashed)
of Sn1 and Sn2, for λ = 0, 0.1, 0.2, . . . , 3.5. The level of the test is taken to be 0.05.
Proof of the lemmas in Section 2
The proof of Lemma 1 is straightforward and has been put in the supplement. The proof
of Lemma 2 is along the lines of the proof of equality (21) in and has also been put in
the supplement. Similarly, the proof of Lemma 3 follows the same reasoning as the proof of
Corollary 3.1 in and has been deferred to the supplement.
We proceed by establishing Lemma 4 to make the transition to Brownian motion. For this
we ﬁrst prove that under f0 = f1 = · · · = fJ, standardized slopes converge in distribution to
the slopes of the LCM of the process W(s)−s2 +2xs. For j = 0, 1, 2, . . . , J and S = E, B, W,
nj(t) = n1/3
nj(t) −fj(t)
nj(t) is the slope at s = 0 of the LCM of the process
nj,t(u) = n2/3
 t, t + n−1/3
 t, t + n−1/3
−fj(t)n−1/3
For j = 0, 1, . . . , J and t ∈ , deﬁne scaling constants
j(t)|1/3L′
j(t)1/3 > 0
and let Inj(t) =
u : t + n−1/3
Bj(t)u ∈ 
Lemma 7 Assume (A2), (A3). Suppose f0 = f1 = · · · = fJ and for t ∈(0, 1), let
nj,t(x) = Aj(t)bφS
 t + n−1/3
nj is deﬁned by (39). Then, for S = E, B, W, and x ∈TJ
j=0 Inj(t) ﬁxed, the vector
n0,t(x), ΦS
n1,t(x), . . . , ΦS
nJ,t(x)) converges in distribution to (eΦt0(x), Φ1(x), . . . , ΦJ(x)), where
Φj(x) = the slope at s = x of the LCM of the process Wj(s) −s2 + 2xs,
where W1, W2, . . . , WJ are independent standard Brownian motions and eΦt0 is deﬁned similarly with the standard Brownian motion f
Wt0 deﬁned in (14).
Proof. For j = 0, 1, . . . , J, t ∈(0, 1) ﬁxed and aj ∈R, consider the event
nj,t(x) ≤aj
Aj(t)−1aj,
which, according to (20), is equivalent to
Bj(t)−1n1/3
fj(t + n−1/3
Bj(t)x) + n−1/3
where n0 = n. By (19), the left hand side of (41) is the argmax over u ∈Inj(t) of the process
t, t + n−1/3
t, t + n−1/3
−Aj(t)−1Bj(t)aju.
To cover all cases j = 0, 1, . . . , J simultaneously, write
, j = 1, 2, . . . , J.
where ξn0 is deﬁned in (16). For t + n−1/3
s ∈ , write
 t, t + n−1/3
 t, t + n−1/3
  t, t + n−1/3
 t, t + n−1/3
nj and n0 are deﬁned in (43). According to (A3) and Lemma 1,
  t, t + n−1/3
For every j = 1, 2, . . . , J and k > 0 ﬁxed, we have
 t, t + n−1/3
j(t) = op(1).
Furthermore
 t, t + n−1/3
 t, t + n−1/3
Hence, we have
 t, t + n−1/3u
j(t) = op(1).
Finally, if we deﬁne
(Lj(0) −Lj(gj(a))), n1/3
(Lj(1) −Lj(gj(a)))
Wtj(y) = n1/6
 Lj(t) + n−1/3
y) −Wnj(Lj(t)
with Wnj, for j = 1, 2, . . . , J, being independent Brownian motions from (5) and Wn0 is the
Brownian motion deﬁned by (15), then for j = 1, 2, . . . , J,
Znt,j(s) = n1/6
 t, t + n−1/3
 Lj(t + n−1/3
Because Brownian motion is uniformly continuous on compacta, it follows that for each j =
1, 2, . . . , J, the process Znj(s) converges in the uniform topology on compacta to the process
Ztj(s) = L′
j(t)1/2Wj(s), where W1, W2, . . . , WJ are independent standard Brownian motions.
Furthermore, for j = 0, according to (44), we have that
Znt,0(s) = n1/6Wn0 ◦L0
 t, t + n−1/3s)
Znt,j(c1/3
which converges in distribution to the process
j(t)1/2Wj(s) = L′
Wt0 is deﬁned in (14). We then conclude that for each j = 1, 2, . . . , J and S = E, B, W,
the process in (42) converges in the uniform topology on compacta to the process
j(t)1/2Bj(t)1/2Wj (u) −1
j(t)|Bj(t)2u2 + |f ′
j(t)|Bj(t)2xu −Aj(t)−1Bj(t)aju
j(t)1/2Bj(t)1/2 
Wj(u) −u2 + 2xu −aju
0(t)1/2B0(t)1/2{f
Wt0(u) −u2 + 2xu −a0u} in the case j = 0. According to Lemma 4
in , together with assumptions (A1) and (A2), the argmax on the left hand side of (41) is
of order Op(1). This means we can apply Theorem 2.7 from . Together with the fact that
argmax{H(u)} = argmax{aH(u) + b} for constants a > 0 and b ∈R, this yields that for each
j = 0, 1, . . . , J, t ∈(0, 1) and S = E, B, W ﬁxed, the argmax in (41) converges in distribution
to νj(x −aj/2) and eνt0(x −a0/2)), respectively, where
νj(c) = argmax
Wj(u) −(u −c)2
j = 1, 2, . . . , J,
eνt0(c) = argmax
Wt0(u) −(u −c)2o
To extend this to joint convergence, note that since the processes Znt,j, for j = 1, 2, . . . , J, are
independent and Znt,0 satisﬁes (45), they converge in distribution jointly to ( eZt0, Zt1, . . . , ZtJ).
This implies joint convergence of the argmax’s in (41); see e.g., Theorem 6.1 in , which
is only proven for two argmax’s but which can trivially be extended to joint convergence of
more than two. We conclude that
nj,t(x) ≤aj
{2νj (0) ≤aj}
using the fact that νj(c) −c d= νj(0), for all j = 0, 1, . . . , J and c ∈R. Now, let
Dj(s, x) = left hand slope of Wj(u) −u2 + 2xu at u = s.
Then, similar to (20), one has νj(c) ≤t if and only if Dj(t, 0) ≤−2c, and it is straightforward
to deduce that 2νj(c) has the same distribution as Dj(c, 0) + 4c. Furthermore, by properties
of the LCM, one also has Dj(s, x) = Dj(s, 0) + 2x. It follows that
{2νj (0) ≤aj}
{Dj(x, x) ≤aj}
and since Φj(x) = Dj(x, x), this proves the lemma.
Another ingredient to establish Lemma 4, is a mixing property of the Brownian motion slope
Lemma 8 Suppose that f0 = f1 = · · · , fJ. Then the process
n0(t), bf W
n1(t), . . . , bf W
: t ∈ 
is strong mixing. More speciﬁcally, for d > 0,
sup |P(A ∩B) −P(A)P(B)| ≤αn(d) = C1e−C2nd3,
where C1, C2 > 0 only depend on f0 = f1 = · · · = fJ and c1, c2, . . . , cJ, where the supremum
is taken over all sets A ∈σ{ bf W
nj (s) : j = 0, 1, . . . , J, 0 < s ≤t} and B ∈σ{ bf W
nj (u) : j =
0, 1, . . . , J, t + d ≤u < 1}.
Its proof is along the lines of the proof of Lemma 4.6 in and has been put in the supplement. Finally, we need the following result on the slopes of dependent Brownian motions
with drift.
Lemma 9 For i, j = 0, 1, . . . , J, i ̸= j, c > 0, and t ∈ ﬁxed, we have P (cΦi(0) = Φj(0)) =
0, where Φ1, Φ2, . . . , ΦJ and Φ0 = eΦt0 are deﬁned in Lemma 7.
Proof: When i, j ≥1, the statement is trivially true, because Φj(0) has the same distribution
as 2νj(0), as deﬁned in (46), which has a bounded symmetric density according to Lemma 3.3
in . Consider the case i = 0 and j ≥1. Because eΦt0(0) also has the same distribution as
2eνt0(0), it is equivalent to prove that c−1νj(0) = eνt0(0) with probability zero. By Brownian
scaling, we have that
c−1νj(0) = argmax
Wj(u) −c3/2u2o
and from (14) it follows that eνt0(0) = argmaxu∈R
ajWj(u) + aW(u) −u2
Note that Wj and W are independent standard Brownian motions. According to , with
probability one there does not exist u ∈R such that u is a local maximum for the process
X1(u) = Wj(u) −c3/2u2 and the process X2(u) = ajWj(u) + aW(u) −u2. This proves the
Proof of Lemma 4. We follow the line of reasoning as in Corollary 3.3 in . To cover all
cases j = 0, 1, . . . , J simultaneously, ﬁrst introduce
, j = 1, 2, . . . , J,
ξn0(L0(s))
where ξnj is deﬁned in (5), for j = 1, 2, . . . , J, and in (16), for j = 0. Note that according
to (16), Xn0(s) = PJ
Xnj(s). Next, for j = 0, 1, . . . , J, introduce the process
nj,t(s) = ZB
nj,t(s) + n−1/6
nj,t is deﬁned in (40) and n0 = n, and denote bφξ
nj(t) as the slope of the least concave
majorant of Zξ
nj,t(s) at s = 0. Then
nj(t) = bφB
nj(t) + n−1/6
Because ZB
nj,t(s) = ZW
nj,t(s) −n1/6
{Xnj(t + n−1/3
s) −Xnj(t)}, it follows that
nj,t(s) = ZW
nj,t(s) −n1/6
Xnj(t + n−1/3
s) −Xnj(t) −n−1/3
Let [τ1, τ2] be the segment of the LCM of Zξ
nj,t that contains zero, and [τ ′
2] the segment
of the LCM of ZW
nj,t that contains zero. Deﬁne a = max(τ1, τ ′
1) ≤0 and b = min(τ2, τ ′
Note that we always have a < b, otherwise τ1 = 0 or τ ′
1 = 0, which is impossible by deﬁnition
of the argmax. Then for any j = 0, 1, . . . , J,
nj,t(a) −Zξ
nj,t(a) −ZW
nj(t) −bφW
uniformly in t. This means that it remains to show that
ni(t) −bφB
nj(t)| dt =
ni(t) −bφξ
nj(t)| dt + op(n−1/6).
However, if we deﬁne for S = B, W and i, j = 0, 1, . . . , J,
ij(t) = bφS
ni(t) −bφS
Xij(t) = c−1/6
Xni(t) −c−1/6
where c0 = 1, then according to (47), it is equivalent to show
ij(t) + n−1/6X′
ij(t)| −| bψB
dt = op(1).
Let ǫ > 0. Then
ij(t) + n−1/6X′
ij(t)| −| bψB
ij(t) + n−1/6X′
ij(t)| −| bψB
1[0,ǫ](| bψB
ij(t)|) dt
ij(t) + n−1/6X′
ij(t)| −| bψB
1(ǫ,∞)(| bψB
ij(t)|) dt.
Because of the independence between ξnj and Bnj, and hence between Xnj(t) and bφB
nj(t), the
expectation of the ﬁrst term on the right hand side is bounded from above by
According to Lemma 7, it follows that for all i, j = 0, 1, . . . , J,
ci1(t) −Φj(0)
where Φ0 is short for eΦt0. By right continuity and Lemma 9,
ci1(t) −Φ0(0)
ci1(t) = Φ0(0)
It follows that
ǫ↓0 lim sup
ij(t) + n−1/6X′
ij(t)| −| bψB
1[0,ǫ](| bψB
ij(t)|) dt
For the remaining integral we write
ij(t) + n−1/6X′
ij(t)| −| bψB
1(ǫ,∞)(| bψB
ij(t)|) dt
1(ǫ,∞)(| bψB
ij(t) + n−1/6X′
ij(t) + n−1/6X′
ij(t)| + | bψB
1(ǫ,∞)(| bψB
ij(t) + n−1/6X′
ij(t)| + | bψB
dt + Op(n−1/6)
ij(t) sign( bψB
ij(t))1(ǫ,∞)(| bψB
ij(t)|) dt + Op(n−1/6),
using the fact that for | bψB
ij(t)| > ǫ,
ij(t) + n−1/6X′
ij(t)| + | bψB
+ Op(n−1/6).
For t ∈ and S = B, W, let bY S
ij (t) = sign( bψS
ij(t))1(ǫ,∞)(| bψS
ij(t)|). Then, again by independence of ξnj and Bnj, we get
ij (s)bY B
for the cases j = 1, 2, . . . , J and according to (9),
i0 (s)bY B
where for i, j = 1, 2, . . . , J,
Furthermore, for all i, j = 0, 1, . . . , J,
ij (s)bY B
ij (t) −EbY W
ij (s)bY W
ij (s) −bY W
ij (s)| + E|bY B
ij (t) −bY W
For every t ∈ , we have
ij (t) −bY W
ij (t)| ≤2P(| bψB
ij(t) −bψW
ij (t)| > 2ǫ) + P(| bψB
ij(t)| ≤ǫ) + P(| bψW
ij (t)| ≤ǫ).
Note that for all j = 0, 1, . . . , J,
nj(t) −bφW
nj(t)| = O(n−1/6
This can be shown similar to (48) using that
nj,t(s) = ZW
nj,t(s) −n1/6
Xnj(t + n−1/3
s) −Xnj(t)
|Xnj(t + n−1/3
b) −Xnj(t + n−1/3a)| ≤n−1/3
(b −a) sup
The Markov inequality together with (49) yields that P(| bψB
ij(t) −bψW
ij (t)| > 2ǫ) tends to zero,
uniformly in t ∈ . As before, according to Lemma 7,
ǫ↓0 lim sup
n→∞P(| bψB
ij(t)| ≤ǫ) = 0
and likewise for P(| bψW
ij (t)| ≤ǫ). We conclude that for all 0 < s < t < 1, i, j = 0, 1, . . . , J
and ǫ > 0,
ǫ↓0 lim sup
ij (s)bY B
ij (t) −EbY W
ij (s)bY W
Finally, for all j = 0, 1, . . . , J, write
ij (s)bY W
ij (s), bY W
ij (s)]E[bY W
Because (bφW
ni (t), bφW
nj(t)) is strong mixing according to Lemma 8, also bψW
ij (t) is strong mixing.
Then according to (see also Lemma 3.1 in ), for every 0 < s < t < 1 we get that
ij (s), bY W
≤Ke−C2n(t−s) →0.
Also for every t ∈(0, 1), according to Lemma 7, writing V0 and Φ0 for eνt0 and eΦt0,
ij (t)] = P( bψW
ij (t) > ǫ) −P( bψW
ij (t) < −ǫ)
c1i(t) −Φj(0)
c1i(t) −Φj(0)
Aj(t) < −ǫ
c1i(t) −2νj(0)
ci1(t) −2νj(0)
Aj(t) < −ǫ
because (−Vi(0), −νj(0)) has the same distribution as (Vi(0), νj(0)). It follows that
ǫ↓0 lim sup
This proves the lemma.
Proof of Theorems 1 and 2
In this section we assume that assumptions (A0), (A1), (A2), (A3), (A4) hold. Now that we
have established (22) thanks to Lemmas 2, 3 and 4, we investigate integrals of the type
ni (a) −bU W
nj (a)| da.
We proceed as in step 2 in and approximate n1/3
nj (a) −Lj(gj(a))
by eVnj(gj(a)),
where for all j = 0, 1, . . . , J,
eVnj(t) = argmax
with Wtj the Brownian motion deﬁned in (44).
Lemma 10 There exists a C > 0, such that for each j = 0, 1, . . . , J and a ∈[fj(1) +
n−1/3(log n)2, fj(0) −n−1/3(log n)2],
nj (a)) −Lj(gj(a))
−eVnj(gj(a))
The proof is along the lines of the proof of step 2 in and has been put in the supplement.
Combining this with (22) yields the following lemma, whose proof is straightforward and has
been put in the supplement.
Lemma 11 Assume f0 = f1 = · · · = fJ. Then for every i, j = 0, 1, . . . , J, we have
ni(t) −bf E
nj(t)| dt =
0(t)| dt + op(n−1/6).
From Lemma 11 we conclude that under f0 = f1 = · · · = fJ, for k = 1, 2, the test statistic Snk
Ynk(t) dt + op(n−1/6),
Therefore, in order to prove Theorems 1 and 2, it remains to show that under f0 = f1 = · · · =
fJ, for k = 1, 2,
Ynk(t) dt −mk
converges in distribution to a centered Gaussian variable with a ﬁnite variance σ2
k. To determine mk and σ2
k, we have to deal with joint distributions of eVni(t) and eVnj(t), for diﬀerent
i, j = 0, 1, . . . , J, and with covariances between eVni(s) and eVni(t), for s and t close to each
We will approximate eVnj(s) and eVnj(t) with the variable Vtj(s) deﬁned as follows. For all
t ∈(0, 1) and for j = 1, 2, . . . , J, we deﬁne
Vtj(s) = argmax
where the process Wtj(u) is deﬁned in (44). Recall that the processes Wtj, for j = 1, 2, . . . , J,
are independent Brownian motions, whose joint distribution of (Wt1, Wt2, . . . , WtJ) does not
depend on n. Note that from (44) it follows that
0 (L0(t) + n−1/3u) −Lj(t)
Although, due to (11), Wt0 itself is distributed as standard Brownian motion, the joint distribution of (Wt0, Wt1, . . . , WtJ) does depend on n.
For this reason, we approximate Wt0
Vt0(s) = argmax
Note that from (11) it follows that also f
Wt0 is distributed like a standard Brownian motion,
but this time it is a linear combination of the Wtj’s not depending on n, so that the joint
distribution of (f
Wt0, Wt1, Wt2, . . . , WtJ) is independent of n. The latter is important to determine an expression for the limiting variance σ2
2 not depending on n. On the other hand,
the approximation of eVn0(t) by Vt0(t) is not suﬃcient to replace the expectation of n1/3Sn2
by a constant m2 not depending on n. For this we will use
t0(s) = argmax
where Wt0 is from (53). The following lemma provides approximations of eVnj(t). Its proof
is somewhat technical, but uses the same kind of reasoning as in step 3 in and has been
deferred to the supplement.
Lemma 12 For r > 1,
eVnj(t) −Vtj(t)
= o(n−1/6),
for j = 1, 2 . . . , J,
eVn0(t) −V ′
= o(n−1/6),
uniformly in t ∈(0, 1). Furthermore, let A > 0 and r ∈(1, 2θ), where θ > 3/4 is taken
from (A4). Then,
eVnj(t) −Vtj(s)
= o(log n)−1,
uniformly in j = 0, 1, . . . , J, and s, t ∈(0, 1), such that |s −t| ≤An−1/3 log n.
In the following lemma, we prove that the variance of the above variable has a ﬁnite limit
under f1 = f2 = · · · = fJ. As the proof follows the line of reasoning used in the proof of
step 5 in , we only present a sketch of the proof. A detailed proof can be found in the
supplement.
Lemma 13 For k = 1, 2, let
Under f1 = f2 = · · · = fJ, n1/3vnk has a ﬁnite limit σ2
k, as n →∞, where σ2
1 is deﬁned in
Theorem 1 and σ2
2 is deﬁned in Theorem 2.
Sketch of proof. For k = 1, 2, we have
cov(Ynk(t), Ynk(s)) dt ds.
Note that by deﬁnition of eVnj(t) in (50), the random variable Ynk(t), deﬁned in (51), depends only on the increments of Wnj over a neigbourhood of Lj(t) with radius of the order O(n−1/3 log n), for j = 0, 1, . . . , J. But for every s, t ∈ and j = 0, 1, . . . , J, we have
|Lj(t) −Lj(s)| ≥|t −s| infu∈ |L′
j(u)|, where the inﬁmum is positive according to (A3).
Setting an = An−1/3 log n, for some large enough A > 0, we ﬁnd that Ynk(t) is independent
of Ynk(s) for every |t −s| ≥an. This means that in for k = 1, together with Lemma 12,
Z 1∧(s+an)
cov(Yn1(t), Yn1(s)) dt ds
Z 1∧(s+an)
0(s)|2Ciljm(s, t) dt ds + o(n−1/3),
Ciljm(s, t) = cov
L′m(s)c1/3
Let dj(s) = |f ′
0(s)|/(2L′
j(s)2). From (52) and (44), we have for j = 1, 2, . . . , J, that dj(s)2/3Vtj(s)
has the same distribution as
u + n1/3(t −s)|f ′
0(s)/2|2/3
n(s, t, u)
where Wj is a standard Brownian motion and where for every |t −s| ≤an, and R′
n(s, t, u)
can be shown to be negligible. For j = 1, 2, . . . , J, let ζj be deﬁned by (6). We then conclude
that for j = 1, 2, . . . , J,
n1/3(t −s)|f ′
0(s)/2|2/3
= o(1/ log n).
Change of variable t′ = n1/3(t −s)|f ′
0(s)/2|2/3, then gives
n1/3vn1 = 8
 |Ysi(t′) −Ysj(t′)|, |Ysl(0) −Ysm(0)|
dt′ ds + o(1),
n = A log n|f ′
0(s)/2|2/3 and where for j = 1, 2, . . . , J, Ysj(t) is deﬁned in (7). We ﬁnish
the proof for the case k = 1, by showing that there exist absolute constants K and K′ such
|cov (|Ysi(t) −Ysj(t)|, |Ysl(0) −Ysm(0)|) | ≤K exp(−K′t3),
because then, for k = 1, the lemma follows from the dominated convergence theorem. The
proof for the case k = 2 is similar.
We are now in position to complete the proofs of Theorems 1 and 2.
Proof of Theorems 1 and 2. Deﬁne
where Vtj(t) and V ′
t0(t) are deﬁned in (52) and (56). From Lemma 12 we then obtain for
= o(n−1/6).
Note that for j = 1, 2, . . . , J,
Vtj(t) d= |4f ′
0(t)|1/3 L′
ζj(0) = |4f ′
0(t)|1/3Ysj(0),
where ζj and Ysj are deﬁned in (6) and (7), so that
0(t)|1/3E |Ysi(0) −Ysj(0)| dt
t0(t) d= |4f ′
0(t)|1/3L′
0(t)1/3eζ′
t0 is deﬁned in (13), so that
0(t)1/3bζt0(0) −
Therefore, in order to prove Theorems 1 and 2, it remains to show that, for k = 1, 2, under
f1 = f2 = · · · = fJ,
Ynk(t) dt −E
converges in distribution to a centered Gaussian variable with variance σ2
k. This can be done
using the method of big blocks and small blocks, similar to the proof of Theorem 4.1 in .
The details are omitted.
Proofs for Section 3
The following Kiefer-Wolfowitz type result for our general setting is proved in and will
be used to prove Lemma 6.
Theorem 7 Assume (A1), (A2), (A3) with some q ≥3, and (A4) with some θ > 0. For
S = E, B, W, let bF S
n be the least concave majorant of F S
n (x) −F S
n (x)| = Op
n−2/3(log n)2/3
Proof of Lemma 6. For convenience, we denote by efn(t) the estimator deﬁned by (29)
for all t ∈[a + hn, b −hn] and either by (30) or by (31) on the boundaries [a, a + hn) and
(b −hn, bn], and we denote by bfn the estimator deﬁned in the same manner as efn with Fn0
replaced by bFn0. For l = 0, 1, 2, we have
t∈[a+hn,b−hn]
n (t) −ef (l)
( bFn0(t −uhn) −Fn0(t −uhn))K(l+1)(u) du
| bFn0(s) −Fn0(s)|
|K(l+1)|(u)| du
n−2/3(log n)2/3
Moreover, in the proof of Lemma 5, it is proved that both functions φ and ψ are bounded, and
that the supremum norm of ef ′′
n is of order h−5/2
log(1/hn) if f0 is twice continuously
diﬀerentiable Hence, one easily derives Lemma 6 from (59) and Lemma 5.
Sketch of proof for Theorems 3, 4, 5 and 6.
Note ﬁrst that it suﬃces to prove the
results in the particular case [a, b] = , see Remark 1, so in the sequel, we consider only
[a, b] = . That assumptions (A1),. . . ,(A4)
are fulﬁlled in all the considered models is
proved in , see her Theorems 3, 5, 6. In the monotone regression model, the embedding
is with a Brownian motion and Lj(t) = tτ 2
j ; in the density model, the embedding is with a
Brownian Bridge and Lj = Fj; in the random censorship model, the embedding is with a
Brownian motion and
(1 −Gj(x))(1 −Hj(x)) dx.
Moreover, Assumption (A0) is clearly satisﬁed since all the original observations are mutually
independent, so Theorems 1 and 2 apply in all these models and it suﬃces to prove that (27)
also holds under H0. Hereafter, we assume f1 = · · · = fJ = f0 and efn satisﬁes (A⋆).
Note that, in order to prove convergence in probability, we can restrict ourselves to an
event whose probability tends to one as n →∞. Thus, thanks to Lemma 5, we assume in the
sequel that efn is decreasing on and satisﬁes (25), (23), (24) and (26) for some positive
C0, C1, s ∈(3/4, 1], and εn such that n−γ/εn tends to zero as n →∞for any γ > 0. Moreover,
recall that a sequence of random variables Xn converges in probability to a random variable
X if, and only if, every subsequence has a further subsequence along which Xn converges
almost surely to X. Thus, in the three considered models, if a sequence of random variables,
which is measurable with respect to the original observations, converges in probability, we can
assume for simplicity that it converges almost surely (otherwise, argue along subsequences).
Then, we aim to prove that almost surely, n1/6(S⋆
nk −mk) converges in distribution to the
Gaussian law with mean zero and variance σ2
In each model, we deﬁne the bootstrap versions of f0 and Fnj as efn and F ⋆
nj respectively,
while the bootstrap version of Mnj is M⋆
nj −eFn with eFn(t) =
0 efn(u) du.
what precedes, a bootstrap version of assumptions (A0)
holds (we mean that
conditionally on the original observations, these assumptions hold with f0 and Fnj replaced
by their bootstrap version). Moreover, it can be proved following the line of reasonning used
in the proof of Theorems 3,5,6 in , that a bootstrap version of assumptions (A2)
(A3) hold. The boostrap version L⋆
j of Lj we consider in each considered model is given
1. In the regression model, L⋆
j(t) = tbτ 2
j with bτ 2
j the conditional variance of ǫ⋆
ij. Let us
notice that the conditional moment of order q of ǫ⋆
ij is equal to
|bǫij −¯ǫj|q.
Under the assumptions of Theorem 3, we have supt | efn(t) −f0(t)| = op(1) so it follows
from the law of large numbers that ¯ǫj = op(1) for all j = 1, . . . , J. Using again the law
of large numbers we obtain that
ij|q = max
E|ǫij|q + op(1).
Thus, we can assume here that maxi,j E⋆|ǫ⋆
ij|q ≤C for some positive number C that
does not depend on n, which is the main ingredient to establish the bootstrap version
of (A3) in the regression model. Likewise, we can assume that bτ 2
j + o(n−1/3) for
all j = 1, . . . , J.
2. In the density model, L⋆
j = eFn for every j. Note that f0 is assumed bounded from above
and from below, so thanks to (23) we can assume here that efn is bounded independently
of n from above and from below.
3. In the random censorship model, we still can assume that efn is bounded independently
of n from above and from below.
Besides, we can assume that eGn(1) < 1 −ε and
limt↑1 Hnj(t) < 1 −ε for some positive number ε that does not depend on n, where
eGn = 1−exp(−eFn) is the distribution function of the T ⋆
ij’s. Now, the natural bootstrap
version of Lj to consider is
(1 −eGn(x))(1 −Hnj(x))
But the maximal heigth of the jumps of Hnj on [0, 1) is of the order Op(1/n) and
therefore,
|Hnj(t) −eHj(t)| = Op(1/n),
where eHj is the continuous version of Hnj (we mean, the polygonal function on 
that coincides with Hnj at every discontinuity points of Hnj on [0, 1) and such that
eHj(1) = limt↑1 Hnj(t)). One can then check that the bootstrap version of the embedding
also holds with the bootstrap version of Lj deﬁned by
(1 −eGn(x))(1 −eHj(x))
The advantage of this proposal as compared to (60) is that it has a continuous derivative
that is bounded from below and from above.
Finally, with the above deﬁnitions of the bootstrap versions of f0 and Lj, the following
bootstrap version of (A4) clearly holds in both regression and density models: there exist an
s ∈(3/4, 1] that does not depend on n such that for all x, y, j,
n(x) −ef ′
n(y)| ≤|x −y|s/ǫn
′′(y)| ≤|x −y|s/ǫn.
In the random censorship model, ef ′
n satisﬁes the above H¨olderian assumption, but L⋆
not twice diﬀerentiable so the bootstrap version of the smoothness assumption on Lj is not
satisﬁed. Instead, we have
j(t)| = op(n−1/3),
j is smooth.
Now that we have bootstrap versions of (A0),. . . ,(A4), it can be proved, following the
line of reasoning used in the proof of Theorems 1 and 2, that n1/6(S⋆
nk −mk) converges in
distribution to the Gaussian law with mean zero and variance σ2
k. This can be done at the
price of additional diﬃculties which are mainly due to the facts that the bootstrap versions
efn and L⋆
j of f and Lj depend on n, and L⋆
j is less smooth than the original Lj in the
random censorship model. In particular, in this model we cannot use a bootstrap version of
Lemma 6.3 in the supplement, which is the key lemma that makes the transition possible
from Brownian bridge to Brownian motion. However, here q can be chosen as large as we
wish and Bnj is a Brownian motion (see assumption (A3)), so in this model one can avoid
the use of a bootstrap version for Lemma 6.3 in the supplement and directly obtain
ni (t) −bU E⋆
nj (t)| dt = n1/3
ni (a) −bU W ⋆
nj (a)| da + op(n−1/6)
(where as usual, bU E⋆
ni and bU W ⋆
are deﬁned in the same manner as bU E
ni and bU W
ni respectively,
just replacing the original observations with their bootstrap version in the deﬁnition), by using
Lemma 5 in . On the other hand, the presence of ǫn in the bootstrap version of (A4) does
not cause any trouble thanks to the assumption that n−γ/ǫn tends to zero as n →∞for any
positive γ. Nevertheless, the additional diﬃculties are not essential so, to alleviate the paper,
we do not provide a detailed proof for the consistency of the bootstrap.
It should be mentioned that the proof of Theorems 1 and 2 could be simpliﬁed if, instead
of considering the general framework of Section 2, we restrict ourselves to one of models considered in Section 3; the proof of the bootstrap version simpliﬁes in the same manner. For
example, in the regression model as well as in the random censoring model, the embedding in
(A3) is with a Brownian motion Bnj so the cases S = B and S = W coincide in these models; in particular, Lemmas 7, 8 and 4 are pointless and there is no need to prove a bootstrap
version of these lemmas. On the other hand, in the monotone regression model as well as in
the monotone density model, the processes Wt0 and f
Wt0 coincide since the functions Lj’s are
proportional to each other, see (53) and (54). The proof thus simpliﬁes a bit, and the same
hold with its bootstrap version.
Acknowledgements. The research of C´ecile Durot is partly supported by the French Agence
Nationale de la Recherche [ANR 2011 BS01 010 01 projet Calibration].