Dynamic Slimmable Network
Changlin Li1
Guangrun Wang2
Bing Wang3
Xiaodan Liang4
Zhihui Li5
Xiaojun Chang1
1 GORSE Lab, Dept. of DSAI, Monash University
2 Univeristy of Oxford
3 Alibaba Group
4 Sun Yat-Sen University
5 Shandong Artiﬁcial Intelligence, Qilu University of Technology
 , , ,
 , , 
Current dynamic networks and dynamic pruning methods
have shown their promising capability in reducing theoretical computation complexity. However, dynamic sparse
patterns on convolutional ﬁlters fail to achieve actual acceleration in real-world implementation, due to the extra
burden of indexing, weight-copying, or zero-masking. Here,
we explore a dynamic network slimming regime, named Dynamic Slimmable Network (DS-Net), which aims to achieve
good hardware-efﬁciency via dynamically adjusting ﬁlter
numbers of networks at test time with respect to different
inputs, while keeping ﬁlters stored statically and contiguously in hardware to prevent the extra burden. Our DS-Net
is empowered with the ability of dynamic inference by the
proposed double-headed dynamic gate that comprises an
attention head and a slimming head to predictively adjust
network width with negligible extra computation cost. To
ensure generality of each candidate architecture and the
fairness of gate, we propose a disentangled two-stage training scheme inspired by one-shot NAS. In the ﬁrst stage, a
novel training technique for weight-sharing networks named
In-place Ensemble Bootstrapping is proposed to improve the
supernet training efﬁcacy. In the second stage, Sandwich
Gate Sparsiﬁcation is proposed to assist the gate training
by identifying easy and hard samples in an online way. Extensive experiments demonstrate our DS-Net consistently
outperforms its static counterparts as well as state-of-the-art
static and dynamic model compression methods by a large
margin (up to 5.9%). Typically, DS-Net achieves 2-4× computation reduction and 1.62× real-world acceleration over
ResNet-50 and MobileNet with minimal accuracy drops on
ImageNet.1
1. Introduction
As deep neural networks are becoming deeper and wider
to achieve higher performance, there is an urgent need to
explore efﬁcient models for common mobile platforms, such
1Code release: 
MAdds (Millions)
Top-1 Accuracy (%)
Accuracy vs MAdds
DS-Net (Ours)
Figure 1. Universally accuracy-complexity comparison of our DS-
Net and Universally Slimmable Network (US-Net) (based on
MobileNetV1 ).
Table 1. Latency comparison of ResNet-50 with 25% channels
 . Both masking and indexing lead to
inefﬁcient computation waste, while slicing achieves comparable
acceleration with ideal (the individual ResNet-50 0.25×).
slicing (ours)
as self-driving cars, smartphones, drones and robots. In
recent years, many different approaches have been proposed to improve the inference efﬁciency of neural networks, including network pruning ,
weight quantization , knowledge distillation ,
manually and automatically designing of efﬁcient networks and dynamic
inference .
Among the above approaches, dynamic inference methods, including networks with dynamic depth and dynamic width have attracted increasing
attention because of their promising capability of reducing
computational redundancy by automatically adjusting their
architecture for different inputs. As illustrated in Fig. 2, the
dynamic network learns to conﬁgure different architecture
routing adaptively for each input, instead of optimizing the
architecture among the whole dataset like Neural Architecture Search (NAS) or Pruning. A performance-complexity
 
easy & hard samples
static nets
dynamic nets
Easy samples
Hard samples
Use less computation
Use more computation
Figure 2. The motivation for designing dynamic networks to achieve efﬁcient inference. Left: A simulation diagram of accuracy-complexity
comparing a series of static networks (searched by NAS) with 20 dynamic inference schemes of different resource allocate proportion for
easy and hard samples on a hypothetical classiﬁcation dataset with evenly distributed easy and hard samples. Right: Illustration of dynamic
networks on efﬁcient inference. Input images are routed to use different architectures regarding their classiﬁcation difﬁculty.
trade-off simulated with exponential functions is also shown
in Fig. 2, the optimal solution of dynamic networks is superior to the static NAS or pruning solution. Ideally, dynamic network routing can signiﬁcantly improve model performance under certain complexity constraints.
However, networks with dynamic width, i.e., dynamic
pruning methods , unlike its orthogonal counterparts with dynamic depth, have never achieved actual acceleration in a real-world implementation. As natural extensions
of network pruning, dynamic pruning methods predictively
prune the convolution ﬁlters with regard to different input
at runtime. The varying sparse patterns are incompatible
with computation on hardware. Actually, many of them are
implemented as zero masking or inefﬁcient path indexing,
resulting in a massive gap between the theoretical analysis
and the practical acceleration. As shown in Tab. 1, both
masking and indexing lead to inefﬁcient computation waste.
To address the aforementioned issues in dynamic networks, we propose Dynamic Slimmable Network (DS-Net),
which achieves good hardware-efﬁciency via dynamically
adjusting ﬁlter numbers of networks at test time with respect
to different inputs. To avoid the extra burden on hardware
caused by dynamic sparsity, we adopt a scheme named dynamic slicing to keep ﬁlters static and contiguous when
adjusting the network width. Speciﬁcally, we propose a
double-headed dynamic gate with an attention head and
a slimming head upon slimmable networks to predictively
adjust the network width with negligible extra computation
cost. The training of dynamic networks is a highly entangled
bilevel optimization problem. To ensure generality of each
candidate’s architecture and the fairness of gate, a disentangled two-stage training scheme inspired by one-shot NAS
is proposed to optimize the supernet and the gates separately.
In the ﬁrst stage, the slimmable supernet is optimized with a
novel training method for weight-sharing networks, named
In-place Ensemble Bootstrapping (IEB). IEB trains the
smaller sub-networks in the online network to ﬁt the output
logits of an ensemble of larger sub-networks in the momentum target network. Learning from the ensemble of different
sub-networks will reduce the conﬂict among sub-networks
and increase their generality. Using the exponential moving average of the online network as the momentum target
network can provide a stable and accurate historical representation, and bootstrap the online network and the target
network itself to achieve higher overall performance. In the
second stage, to prevent dynamic gates from collapsing into
static ones in the multiobjective optimization problem, a
technique named Sandwich Gate Sparsiﬁcation (SGS) is
proposed to assist the gate training. During training, SGS
identiﬁes easy and hard samples online and further generates
the ground truth label for the dynamic gates.
Overall, our contributions are three-fold as follows:
• We propose a new dynamic network routing regime,
achieving good hardware-efﬁciency by predictively adjusting ﬁlter numbers of networks at test time with
respect to different inputs. Unlike dynamic pruning
methods, we dynamically slice the network parameters
while keeping them stored statically and contiguously
in hardware to prevent the extra burden of masking,
indexing, and weight-copying. The dynamic routing is
achieved by our proposed double-headed dynamic gate
with negligible extra computation cost.
• We propose a two-stage training scheme with IEB and
SGS techniques for DS-Net. Proved experimentally,
IEB stabilizes the training of slimmable networks and
boosts its accuracy by 1.8% and 0.6% in the slimmest
and widest sub-networks respectively. Moreover, we
empirically show that the SGS technique can effectively
sparsify the dynamic gate and improves the ﬁnal performance of DS-Net by 2%.
• Extensive experiments demonstrate our DS-Net outperforms its static counterparts as well as state-ofthe-art static and dynamic model compression methods
by a large margin (up to 5.9%, Fig. 1). Typically, DS-
Net achieves 2-4× computation reduction and 1.62×
real-world acceleration over ResNet-50 and MobileNet
with minimal accuracy drops on ImageNet. Gate visualization proves the high dynamic diversity of DS-Net.
n- Cn×Hn×Wn
Pooling & Classifier
Figure 3. Architecture of DS-Net. The width of each supernet stage is adjusted adaptively by the slimming ratio ρ predicted by the gate.
2. Related works
Anytime neural networks 
are single networks that can execute with their sub-networks
under different budget constraints, thus can deploy instantly
and adaptively in different application scenarios. Anytime
neural networks have been studied in two orthogonal directions: networks with variable depth and variable width.
Networks with variable depth are ﬁrst
studied widely, beneﬁting from the naturally nested structure in depth dimension and residual connections in ResNet
 and DenseNet . Network with variable width was
ﬁrst studied in . Recently, slimmable networks 
using switchable batch normalization and in-place distillation achieve higher performance than their stand-alone
counterparts in any width. Some recent works 
also explore anytime neural networks in multiple dimensions,
e.g. depth, width, kernel size, etc.
Dynamic neural networks change their
architectures based on the input data. Dynamic networks
for efﬁcient inference aim to reduce average inference cost
by using different sub-networks adaptively for inputs with
diverse difﬁculty levels. Networks with dynamic depth
 achieve efﬁcient inference in two ways,
early exiting when shallower sub-networks have high classi-
ﬁcation conﬁdence , or skipping residual blocks
adaptively .
Recently, dynamic pruning methods using a variable subset of convolution ﬁlters
have been studied. Channel Gating Neural Network and
FBS identify and skip the unimportant input channels at
run-time. In GaterNet , a separate gater network is used
to predictively select the ﬁlters of the main network. Please
refer to for a more comprehensive review of dynamic
neural networks.
Weight sharing NAS , aiming at designing neural network architectures automatically
and efﬁciently, has been developing rapidly in recent two
years. They integrate the whole search space of NAS into
a weight sharing supernet and optimize network architecture by pursuing the best-performing sub-networks. These
methods can be roughly divided into two categories: jointly
optimized methods , in which the weight of the
supernet is jointly trained with the architecture routing agent
(typically a simple learnable factor for each candidate route);
and one-shot methods , in which the
training of the supernet parameters and architecture routing
agent are disentangled. After fair and sufﬁcient training, the
agent is optimized with the weights of supernet frozen.
3. Dynamic Slimmable Network
Our dynamic slimmable network achieves dynamic routing for different samples by learning a slimmable supernet
and a dynamic gating mechanism. As illustrated in Fig. 3,
the supernet in DS-Net refers to the whole module undertaking the main task. In contrast, the dynamic gates are a
series of predictive modules that route the input to use subnetworks with different widths in each stage of the supernet.
In previous dynamic networks , the dynamic routing agent and the main network are jointly trained, analogous to jointly optimized NAS
methods . Inspired by one-shot NAS methods
 , we propose a disentangled two-stage training scheme to ensure the generality of every path in our
DS-Net. In Stage I, we disable the slimming gate and train
the supernet with the IEB technique, then in Stage II, we ﬁx
the weights of the supernet and train the slimming gate with
the SGS technique.
3.1. Dynamic Supernet
In this section, we ﬁrst introduce the hardware efﬁcient
channel slicing scheme and our designed supernet, then
present the IEB technique and details of training Stage I.
Supernet and Dynamic Channel Slicing. In some of dynamic networks, such as dynamic pruning and conditional convolution , the convolution ﬁlters W are
conditionally parameterized by a function A(θ, X) to the
input X. Generally, the dynamic convolution has a form of:
Y = WA(θ,X) ∗X,
where WA(θ,X) represents the selected or generated inputdependent convolution ﬁlters. Here ∗is used to denote a
matrix multiplication. Previous dynamic pruning methods
 reduce theoretical computation cost by varying the
channel sparsity pattern according to the input. However,
they fail to achieve real-world acceleration because their
hardware-incompatible channel sparsity results in repeatedly
indexing and copying selected ﬁlters to a new contiguous
memory for multiplication. To achieve practical acceleration,
ﬁlters should remain contiguous and relatively static during
Target Network
Online Network
Figure 4. Training process of slimmable supernet with In-place
Ensemble Bootstrapping.
dynamic weight selection. Base on this analysis, we design
a architecture routing agent A(θ) with the inductive bias of
always outputting a dense architecture, e.g. a slice-able architecture. Speciﬁcally, we consider a convolutional layer with
at most N output ﬁlters and M input channels. Omitting the
spatial dimension, its ﬁlters can be denoted as W ∈RN×M.
The output of the architecture routing agent A(θ) for this
convolution would be a slimming ratio ρ ∈(0, 1] indicating
that the ﬁrst piece-wise ρ × N of the output ﬁlters are selected. Then, a dynamic slice-able convolution is deﬁned as
Y = W[ : ρ × N] ∗X,
where [ : ] is a slice operation denoted in a python-like style.
Remarkably, the slice operation [ : ] and the dense matrix
multiplication ∗are much more efﬁcient than an indexing
operation or a sparse matrix multiplication in real-world
implementation, which guarantees a practical acceleration
of using our slice-able convolution.
After aggregating the slice-able convolutions sequentially,
a supernet executable at different widths is formed. Paths
with different widths can be seen as sub-networks. By disabling the routing agent, the supernet is analogous to a
slimmable network , and can be trained similarly.
In-place Ensemble Bootstrapping. The sandwich rule and
in-place distillation techniques proposed for Universally
Slimmable Networks enhanced their overall performance. In
in-place distillation, the widest sub-network is used as the
target network generating soft labels for other sub-networks.
However, acute ﬂuctuation appeared in the weight of the
widest sub-network can cause convergence hardship, especially in the early stage of training. As observed in BigNAS
 , training a more complex model with in-place distillation could be highly unstable. Without residual connection
and special weight initialization tricks, the loss exploded at
the early stage and can never converge. To overcome the convergence hardship in slimmable networks and improve the
overall performance of our supernet, we proposed a training
scheme named In-place Ensemble Bootstrapping (IEB).
In recent years, a growing number of self-supervised
methods with bootstrapping and semi-supervised
methods based on consistency regularization use
their historical representations to produce targets for the online network. Inspired by this, we propose to bootstrap on
previous representations in our supervised in-place distillation training. We use the exponential moving average (EMA)
of the model as the target network that generates soft labels.
Let θ and θ′ denote the parameters of the online network and
the target network, respectively. We have:
t−1 + (1 −α)θt,
where α is a momentum factor controlling the ratio of the
historical parameter and t is a training timestamp which is
usually measured by a training iteration. During training, the
EMA of the model are more stable and more precise than
the online network, thus can provide high quality target for
the slimmer sub-networks.
As pointed out in , an ensemble of teacher networks can generate more diverse, more accurate and more
general soft labels for distillation training of the student network. In our supernet, there are tons of sub-models with
different architectures, which can generate different soft labels. Motivated by this, we use different sub-networks as a
teacher ensemble when performing in-place distillation. The
overall train process is shown in Fig. 4. Following the sandwich rule , the widest (denoted with L), the slimmest
(denoted with S) and n random width sub-networks (denoted
with R) are sampled in each training step. Sub-network at
the largest width is trained to predict the ground truth label
Y; n sub-networks with random width are trained to predict
the soft label generated by the widest sub-network of the
target network, Y′
L(θ′); the slimmest sub-network is trained
to predict the probability ensemble of all the aforementioned
sub-networks in the target network:
Y′L,R(θ′) =
To sum up, the IEB losses for the supernet training are:
(θ) = LCE(YL(θ), Y),
(θ) = LCE(YR(θ), Y′
(θ) = LCE(YS(θ), c
Y′L,R(θ′)),
3.2. Dynamic Slimming Gate
In this section, we design the channel gate function
A(θ, X) that generates the factor ρ in Eqn. (2) and present
the double-headed design of the dynamic gate. Then, we
introduce the details of training stage II with an advanced
technique that is sandwich gate sparsiﬁcation (SGS).
Double-headed Design. There are two possible ways to
transform a feature map into a slimming ratio ρ in Eqn. (2):
(i) scalar design directly output a sigmoid activated scalar
ranging from 0 to 1 to be the slimming ratio; (ii) one-hot
design use an argmax/softmax activated one-hot vector to
choose the respective slimming ratio ρ in a discrete candidate
list vector Lρ. Both of the implementations are evaluated
and compared in Sec. 4.4. Here, we thoroughly describe our
dynamic slimming gate with the better-performing one-hot
design. To reduce the input feature map X to a one-hot
vector, we divide A(θ, X) to two functions:
A(θ, X) = F(E(X)),
where E is an encoder that reduces feature maps to a vector
and the function F maps the reduced feature to a one-hot
vector used for the subsequent channel slicing. Considering the n-th gate in Fig. 3, given a input feature X with
dimension ρn−1Cn × Hn × Wn, E(X) reduces it to a vector
XE ∈Rρn−1Cn which can be further mapped to a one-hot
vector. By computing the dot product of this one-hot vector
and Lρ, we have the newly predicted slimming ratio:
ρn = A(θ, X) · Lρ.
Similar to prior works on channel attention and
gating, we simply utilize average pooling as a light-weight
encoder E to integrate spatial information. As for feature
mapping function F, we adopt two fully connected layers
with weights W1 ∈Rd×Cn and W2 ∈Rg×d (where d represents the hidden dimension and g represents the number of
candidate slimming ratio) and a ReLU non-linearity layer σ
in between to predict scores for each slimming ratio choice.
An argmax function is subsequently applied to generate a
one-hot vector indicating the predicted choice:
F(XE) = argmax(W2(σ(W1[:, : ρn−1Cn](XE)))).
Note that input X with dynamic channel number ρ × C is
projected to a vector with ﬁxed length by the dynamically
sliced weight W1[:, : ρn−1Cn].
Our proposed channel gating function has a similar form
with recent channel attention methods . The attention
mechanism can be integrated into our gate with nearly zero
cost, by adding another fully connected layer with weights
W3 that projects the hidden vector back to the original channel number ρn−1Cn. Based on the conception above, we
propose a double-headed dynamic gate with a soft channel attention head and a hard channel slimming head.The
channel attention head can be deﬁned as follows:
X = X ∗δ(W3[: ρn−1Cn, :](σ(W1[:, : ρn−1Cn](X)))), (9)
where δ(x) = 1 + tanh(x) is the activation function adopted
for the attention head. Unlike the slimming head, the channel
attention head is activated in training stage I.
Sandwich Gate Sparsiﬁcation. In training stage II, we
propose to use the end-to-end classiﬁcation cross-entropy
loss Lcls and a complexity penalty loss Lcplx to train
the gate, aiming to choose the most efﬁcient and effective sub-networks for each instance. To optimize the nondifferentiable slimming head of dynamic gate with Lcls, we
use gumbel-softmax , a classical way to optimize neural networks with argmax by relaxing it to differentiable
softmax in gradient computation.
However, we empirically found that the gate easily collapses into a static one even if we add Gumbel noise to
help the optimization of gumbel-softmax. Apparently, using only gumbel-softmax technique is not enough for this
multi-objective dynamic gate training. To further overcome
the convergence hardship and increase the dynamic diversity
of the gate, a technique named Sandwich Gate Sparsiﬁcation
(SGS) is further proposed. We use the slimmest sub-network
and the whole network to identify easy and hard samples
online and further generate the ground truth slimming factors
for the slimming heads of all the dynamic gates.
As analysed in , wider sub-networks should always
be more accurate because the accuracy of slimmer ones can
always be achieved by learning new connections to zeros.
Thus, given a well-trained supernet, input samples can be
roughly classiﬁed into three difﬁculty levels: a) Easy samples Xeasy that can be correctly classiﬁed by the slimmest
sub-network; b) Hard samples Xhard that can not be correctly classiﬁed by the widest sub-network; c) Dependent
samples Xdep: Other samples in between. In order to minimize the computation cost, easy samples should always
be routed to the slimmest sub-network (i.e.
gate target
T (Xeasy) = [1, 0, . . . , 0]). For dependent samples and hard
samples, we always encourage them to pass through the
widest sub-network, even if the hard samples can not be
correctly classiﬁed (i.e. T (Xhard) = T (Xdep) = [0, . . . , 0, 1]).
Another gate target strategy is also discussed in Sec. 4.4.
Based on the generated gate target, we deﬁne the SGS
loss that facilitates the gate training:
LSGS = Tslim(X) ∗LCE(X, T (Xeasy))
+ (¬Tslim(X)) ∗LCE(X, T (Xhard))
where Tslim(X) ∈{0, 1} represents whether X is truely
predicted by the slimmest sub-network and LCE(X, T ) =
−P T ∗log(X) is the Cross-Entropy loss over softmax
activated gate scores and the generated gate target.
4. Experiments
Dataset. We evaluate our method on two classiﬁcation
datasets (i.e., ImageNet and CIFAR-10 ) and a standard object detection dataset (i.e., PASCAL VOC ). The
ImageNet dataset is a large-scale dataset containing 1.2 M
train set images and 50 K val set images in 1000 classes.
We use all the training data in both of the two training stages.
Our results are obtained on the val set with image size of
224 × 224. We also test the transferability of our DS-Net on
CIFAR-10, which comprises 10 classes with 50,000 training
and 10,000 test images. Note that few previous works on
dynamic networks and network pruning reported results on
object detection. We take PASCAL VOC, one of the standard datasets for evaluating object detection performance,
as an example to further test the generality of our dynamic
networks on object detection. All the detection models are
trained with the combined dataset from 2007 trainval and
2012 trainval and tested on VOC 2007 test set.
Architecture details. Following previous works on static
and dynamic network pruning, we use two representative
networks, i.e., the heavy residual network ResNet 50 
and the lightweight non-residual network MobileNetV1 ,
to evaluate our method.
In Dynamic Slimmable ResNet 50 (DS-ResNet), we insert our double-headed gate in the begining of each residual
blocks. The slimming head is only used in the ﬁrst block of
each stage. Each one of those blocks contains a skip connection with a projection layer, i.e. 1 × 1 convolution. The ﬁlter
number of this projection convolution is also controlled by
the gate to avoid channel inconsistency when adding skip
features with residual output. In other residual blocks, the
slimming heads of the gates are disabled and all the layers
in those blocks inherit the widths of the ﬁrst blocks of each
stage. To sum up, there are 4 gates (one for each stage) with
both heads enabled. Every gates have 4 equispaced candidate slimming ratios, i.e. ρ ∈{0.25, 0.5, 0.75, 1}. The total
routing space contains 44 = 256 possible paths with different computation complexities. All batch normalization (BN)
layers in DS-ResNet are replaced with group normalization
to avoid test-time representation shift caused by inaccurate
BN statistics in weight-sharing networks .
Unlike DS-ResNet, we only use one single slimming gate
after the ﬁfth depthwise separable convolution block of Dynamic Slimmable MobileNetV1 (DS-MBNet). Speciﬁcally,
a ﬁxed slimming ratio ρ = 0.5 is used in the ﬁrst 5 blocks
while the width of the rest 8 blocks are controlled by the gate
with the candidate slimming ratios ρ ∈[0.35 : 0.05 : 1.25].
This architecture with only 18 paths in its routing space
is similar to an uniform slimmable network , guaranteeing itself the practicality to use batch normalization.
Following , we perform BN recalibration for all the 18
paths in DS-MBNet after the supernet training stage.
Training details. We train our supernet with 512 total batch
size on ImageNet, using SGD optimizer with 0.2 initial
learning rate for DS-ResNet and 0.08 initial learning rate
for DS-MBNet, respectively. We use cosine learning rate
scheduler to reduce the learning rate to its 1% in 150 epochs.
Other settings are following previous works on slimmable
networks . For gate training, we use SGD optimizer with 0.05 initial learning rate for a total batch size
of 512. The learning rate decays to 0.9× of its value in
every epoch. It takes 10 epochs for the gate to converge.
For transfer learning experiments on CIFAR-10, we follow
similar settings with and . We transfer our supernet
for 70 epochs including 15 warm-up epochs and use cosine
learning rate scheduler with an initial learning rate of 0.7 for
a total batch size of 1024. For object detection task , we train
all the networks following and with a total batch
size of 128 for 300 epochs. The learning rate is set to 0.004
at the ﬁrst, then divided by 10 at epoch 200 and 250.
Table 2. Comparison of state-of-the-art efﬁcient inference methods
on ImageNet. Brown denotes network pruning methods, Blue
denotes dynamic inference methods, Orange denotes architecture
search methods and Purple denotes our method.
Top-1 Acc.
ThiNet-70 
MetaPruning 0.85 
ConvNet-AIG-50 
AutoSlim 
DS-ResNet-L (Ours)
ResNet-50 0.75× 
S-ResNet-50 
ThiNet-50 
MetaPruning 0.75 
MSDNet 
AutoSlim 
DS-ResNet-M (Ours)
ResNet-50 0.5× 
ThiNet-30 
MetaPruning 0.5 
GFNet 
DS-ResNet-S (Ours)
Top-1 Acc.
MBNetV1 1.0× 
US-MBNetV1 1.0× 
AutoSlim 
DS-MBNet-L (Ours)
MBNetV1 0.75× 
US-MBNetV1 0.75× 
NetAdapt 
Meta-Pruning 
EagleEye 
CG-Net-A 
AutoSlim 
DS-MBNet-M (Ours)
MBNetV1 0.5× 
US-MBNetV1 0.5× 
AutoSlim 
DS-MBNet-S (Ours)
4.1. Main Results on ImageNet
We ﬁrst validate the effectiveness of our method on ImageNet. As shown in Tab. 2 and Fig. 5, DS-Net with
different computation complexity consistently outperforms
recent static pruning methods, dynamic inference methods
and NAS methods. First, our DS-ResNet and DS-MBNet
models achieve 2-4× computation reduction over ResNet-50
(76.1% ) and MobileNetV1 (70.9% ) with minimal
accuracy drops (0% to -1.5% for ResNet and +0.9% to -0.8%
for MobileNet). We also tested the real world latency on
efﬁcient networks. Compare to the ideal acceleration tested
on channel scaled MobileNetV1, which is 1.31× and 1.91×,
our DS-MBNet achieves comparable 1.17× and 1.62× acceleration with much higher performance. In paticular, our
DS-MBNet surpasses the original and the channel scaled
MobileNetV1 by 3.6%, 4.4% and 6.8% with similar
MAdds and minor increase in latency. Second, our method
outperforms classic and state-of-the-art static pruning methods in a large range. Remarkably, DS-MBNet outperforms
the SOTA pruning methods EagleEye and Meta-Pruning
 by 1.9% and 2.2%. Third, our DS-Net maintain superiority comparing with powerful dynamic inference methods
MAdds (Billions)
Top-1 Accuracy (%)
Acc. vs MAdds
ResNet50 based models
DS-ResNet (Ours)
MetaPruning
MAdds (Millions)
Top-1 Accuracy (%)
Acc. vs MAdds
MobileNetV1 based models
DS-MBNet (Ours)
MetaPruning
Figure 5. Accuracy vs. complexity on ImageNet.
Table 3. Comparison of transfer learning performance on CIFAR-
10. GT stands for gate transfer.
Top-1 Acc.
ResNet-50 
ResNet-101 
DS-ResNet w/o GT
DS-ResNet w/ GT
Table 4. Performance comparision of DS-MBNet and MobileNet
with FSSD on VOC object detection task.
MBNetV1 + FSSD 
DS-MBNet-S + FSSD
DS-MBNet-M + FSSD
DS-MBNet-L + FSSD
with varying depth, width or input resolution. For example,
our DS-MBNet-M surpasses dynamic pruning method CG-
Net by 2.5%. Fourth, our DS-Net also consistently
outperforms its static counterparts. Our DS-MBNet-S surpasses AutoSlim and US-Net by 2.2% and 5.9%.
4.2. Transferability
To evaluate the transferability of DS-Net and its dynamic
gate, we perform transfer learning in two settings: (i) DS-
Net w/o gate transfer: we transfer the supernet without
slimming gate to CIFAR-10 and retrain the dynamic gate.
(ii) DS-Net w/ gate transfer: we ﬁrst transfer the supernet
then load the ImageNet trained gate and perform transfer
leaning for the gate. The results along with the transfer
learning results of the original ResNets are shown in Tab. 3.
Gate transfer boosts the performance of DS-ResNet by 0.4%
on CIFAR-10, demonstrating the transferability of dynamic
gate. Remarkably, both of our transferred DS-ResNet outperforms the original ResNet-50 in a large range (0.6% and
1.0%) with about 2.5 × computation reduction. Among
them, DS-ResNet with gate transfer even outperforms the
larger ResNet-101 with 4.9× fewer computation complexity,
proving the superiority of DS-Net in transfer learning.
Top-1 Accuracy (%)
Accuracy of the slimmest child model
IEB (Ours)
EMA target
Inplace Distillation
Figure 6. Evaluation accuracy of the slimmest sub-network during
supernet training with three different training schemes.
Table 5. Ablation analysis of In-place Ensemble Bootstrapping.
4.3. Object Detection
In this section, we evaluate and compare the performance
of original MobileNet and DS-MBNet used as feature extractor in object detection with Feature Fusion Single Shot
Multibox Detector(FSSD) . We use the features from
the 5-th, 11-th and 13-th depthwise convolution blocks (with
the output stride of 8, 16, 32) of MobileNet for the detector.
When using DS-MBNet as the backbone, all the features
from dynamic source layers are projected to a ﬁxed channel
dimention by the feature transform module in FSSD .
Results on VOC 2007 test set are given in Tab. 4. Comparing to MobileNetV1, DS-MBNet-M and DS-MBNet-L
with FSSD achieves 0.9 and 1.8 mAP improvement with
1.59× and 1.34× computation reduction respectively, which
demonstrates that our DS-Net remain its superiority after
deployed as the backbone network in object detection task.
4.4. Ablation study
In-place Ensemble Bootstrapping. We statistically analysis the effect of IEB technique with MobileNetV1. We
train a Slimmable MobileNetV1 supernet with three settings: original in-place distillation, in-place distillation with
EMA target and our complete IEB technique. As shown in
Tab. 5, the slimmest and widest sub-network trained with
EMA target surpassed the baseline by 1.6% and 0.3% respectively. With IEB, the supernet improves 1.8% and 0.6%
on its slimmest and widest sub-networks comparing with
in-place distillation. The evaluation accuracy progression
curves of the slimmest sub-networks trained with these three
settings are illustrated in Fig. 6. The beginning stage of
in-place distillation is unstable. Adopting EMA target improves the performance. However, there are a few sudden
drops of accuracy in the middle of the training with EMA
MAdds (Billions)
Accuracy (%)
Accuracy vs MAdds
DS-Net (ours)
w/o SGS loss
scalar head
w/o weight sharing
Figure 7. (Left) Illustration of accuracy vs. complexity of models
in Tab. 6 and Tab. 7. (Right) Gate distribution of DS-ResNet-
M. The height of those colored blocks illustrate the partition of
input samples that are routed to the sub-networks with respective
slimming ratio ρ.
target. Though being able to recover in several epochs, the
model may still be potentially harmed by those ﬂuctuation.
After fully adopting IEB, the model converges to a higher
ﬁnal accuracy without any conspicuous ﬂuctuations in the
training process, demonstrating the effectiveness of our IEB
technique in stablizing the training and boosting the overall
performance of slimmable networks.
Effect of losses. To examine the impact of the three losses
used in our gate training, i.e. target loss Lcls, complexity
loss Lcplx and SGS loss LSGS, we conduct extensive experiments with DS-ResNet on ImageNet, and summarize the
results in Tab. 6 and Fig. 7 left. Firstly, as illustrated in Fig.
7 left, models trained with SGS (red line) are more efﬁcient
than models trained without it (purple line). Secondly, as
shown in Tab. 6, with target loss, the model pursues better performance while ignoring computation cost; complexity loss pushes the model to be lightweight while ignoring
the performance; SGS loss itself can achieve a balanced
complexity-accuracy trade-off by encouraging easy and hard
samples to use slim and wide sub-networks, respectively.
SGS strategy. Though we always want the easy samples to
be routed to the slimmest sub-network, there are two possible
target deﬁnition for hard samples in SGS loss: (i) Try Best:
Encourage the hard samples to pass through the widest subnetwork, even if they can not be correctly classiﬁed (i.e.
T (Xhard) = [0, . . . , 0, 1]). (ii) Give Up: Push the hard
samples to use the slimmest path to save computation cost
(i.e. T (Xhard) = [1, 0, . . . , 0]). In both of the strategies,
dependent samples are encouraged to use the widest subnetwork (i.e. T (Xdependent) = [0, . . . , 0, 1]). The results for
both of the strategies are shown in Tab. 6 and Fig. 7 left. As
shown in the third and fourth lines in Tab. 6, Give Up strategy
lowers the computation complexity of the DS-ResNet but
greatly harms the model performance. The models trained
with Try Best strategy (red line in Fig. 7 left) outperform
the one trained with Give Up strategy (blue dot in Fig. 7
left) in terms of efﬁciency. This can be attribute to Give
Up strategy’s optimization difﬁculty and the lack of samples
that targeting on the widest path (dependent samples only
account for about 10% of the total training samples). These
results prove our Try Best strategy is easier to optimize and
Table 6. Ablation analysis of losses on ImageNet. Results in bold
that use SGS loss achieve good performance-complexity trade-off.
Complexity
Top-1 Acc.
Table 7. Ablation analysis of gate design on DS-ResNet.
weight sharing
slimming head
MAdds Top-1 Acc.
can generalize better on validation set or new data.
Gate design. First, to evaluate the effect of our weightsharing double-headed gate design, we train a DS-ResNet
without sharing the the ﬁrst fully-connected layer for comparison with SGS loss only. As shown in Tab. 7 and Fig. 7
left, the performance of DS-ResNet increase substantially
(3.9%) by applying the weight sharing design (green dot vs.
red line in Fig. 7 left). This might be attribute to overﬁtting of the slimming head. As observed in our experiment,
sharing the ﬁrst fully-connected layer with attention head
can greatly improve the generality. Second, we also trained
a DS-ResNet with scalar design (refer to Sec 3.2) of the
slimming head to compare with one-hot design. Both of the
networks are trained with SGS loss only. The results are
present in Tab. 7 and Fig. 7 left. The performance of scalar
design (orange dot in Fig. 7 left) is much lower than the
one-hot design (red line in Fig. 7 left), indicating that the
scalar gate could not route the input to the correct paths.
4.5. Gate visualization
To demonstrate the dynamic diversity of our DS-Net,
we visualize the gate distribution of DS-ResNet over the
validation set of ImageNet in Fig. 7 right. In block 1 and
2, about half of the inputs are routed to the slimmest subnetwork with 0.25 slimming ratio, while in higher level
blocks, about half of the inputs are routed to the widest
sub-network. For all the gate, the slimming ratio choices
are highly input-dependent, demonstrating the high dynamic
diversity of our DS-Net.
5. Conclusion
In this paper, we have proposed Dynamic Slimmable
Network (DS-Net), a novel dynamic network on efﬁcient
inference, achieving good hardware-efﬁciency by predictively adjusting the ﬁlter numbers at test time with respect
to different inputs. We propose a two stage training scheme
with In-place Ensemble Bootstrapping (IEB) and Sandwich
Gate Sparsiﬁcation (SGS) technique to optimize DS-Net.
We demonstrate that DS-Net can achieve 2-4× computation
reduction and 1.62× real-world acceleration over ResNet-50
and MobileNet with minimal accuracy drops on ImageNet.
Proved empirically, DS-Net and can surpass its static counterparts as well as state-of-the-art static and dynamic model
compression method on ImageNet by a large margin (>2%)
and can generalize well on CIFAR-10 classiﬁcation task and
VOC object detection task.
Acknowledgments
This work was supported in part by National Key R&D
Program of China under Grant No. 2020AAA0109700,
National Natural Science Foundation of China (NSFC) under Grant No.U19A2073, No.61976233 and No.61906109,
Guangdong Province Basic and Applied Basic Research
(Regional Joint Fund-Key) Grant No.2019B1515120039,
Shenzhen Outstanding Youth Research Project (Project
No. RCYX20200714114642083), Shenzhen Basic Research
Project (Project No. JCYJ20190807154211365), Leading
Innovation Team of the Zhejiang Province (2018R01017)
and CSIG Young Fellow Support Fund. Dr Xiaojun Chang
is partially supported by the Australian Research Council
(ARC) Discovery Early Career Researcher Award (DECRA)
(DE190100626).