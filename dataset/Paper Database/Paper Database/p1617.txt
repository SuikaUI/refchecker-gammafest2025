Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1638–1649
Melbourne, Australia, July 15 - 20, 2018. c⃝2018 Association for Computational Linguistics
Learning to Write with Cooperative Discriminators
Ari Holtzman†
Maxwell Forbes†
Antoine Bosselut†
David Golub†
Yejin Choi†‡
†Paul G. Allen School of Computer Science & Engineering, University of Washington
‡Allen Institute for Artiﬁcial Intelligence
{ahai,jbuys,mbforbes,antoineb,golubd,yejin}@cs.washington.edu
Despite their local ﬂuency, long-form text
generated from RNNs is often generic,
repetitive, and even self-contradictory. We
propose a uniﬁed learning framework that
collectively addresses all the above issues
by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators
each specialize in a different principle of
communication, such as Grice’s maxims,
and are collectively combined with the
base RNN generator through a composite decoding objective. Human evaluation
demonstrates that text generated by our
model is preferred over that of baselines
by a large margin, signiﬁcantly enhancing
the overall coherence, style, and information of the generations.
Introduction
Language models based on Recurrent Neural Networks (RNNs) have brought substantial advancements across a wide range of language tasks .
However, when used for longform text generation, RNNs often lead to degenerate text that is repetitive, self-contradictory, and
overly generic, as shown in Figure 1.
We propose a uniﬁed learning framework that
can address several challenges of long-form text
generation by composing a committee of discriminators each specializing in a different principle
of communication.
Starting with an RNN language model, our framework learns to construct
a more powerful generator by training a number
of discriminative models that can collectively address limitations of the base RNN generator, and
Context: The two guards thudded into the door on the
other side and began pounding on it, shouting furiously.
Scious raised the large bunch of keys then placed them in
a large pocket in his coat. "Come, we have to go," he
whispered and moved up the corridor, the guards still
hammering on the door. Jared looked around. The table
next to the room they had been in was bare.
LM: There was a small table set into the center of the table,
and a table with two chairs and a table. A chair sat next to
the couch. A large man stood in the doorway of the room,
his arms crossed over his chest. "What 's up?" He asked.
L2W: Empty. It looked like some kind of prison. There were
no windows, no doors, no windows. No sign of anyone in the
house. No one had ever been here before.
Reference: Only a ﬂagon of wine and a couple of mugs
plus a crude dice game. Light ﬂickered in the stone
corridor from lanterns on the walls. The place was dank
and gloomy, moss in scattered patches on the ﬂoor and
walls. The corridor ended just beyond the door to their
former prison. No one else was about.
Figure 1: Sample generations from an RNN language model (LM) and our system (L2W) conditioning on the context shown on the top. The
red, underlined text highlights repetitions, while
the blue, italicized text highlights details that have
a direct semantic parallel in the reference text.
then learns how to weigh these discriminators to
form the ﬁnal decoding objective. These “cooperative” discriminators complement each other and
the base language model to form a stronger, more
global decoding objective.
The design of our discriminators are inspired
by Grice’s maxims of quantity, quality, relation, and manner. The discriminators learn to encode these qualities through the
selection of training data (e.g.
distinguishing a
true continuation from a randomly sampled one
as in §3.2 Relevance Model), which includes generations from partial models (e.g. distinguishing
a true continuation from one generated by a language model as in §3.2 Style Model). The system
then learns to balance these discriminators by initially weighing them uniformly, then continually
updating its weights by comparing the scores the
system gives to its own generated continuations
and to the reference continuation.
Empirical results (§5) demonstrate that our
learning framework is highly effective in converting a generic RNN language model into a substantially stronger generator. Human evaluation con-
ﬁrms that language generated by our model is preferred over that of competitive baselines by a large
margin in two distinct domains, and signiﬁcantly
enhances the overall coherence, style, and information content of the generated text. Automatic
evaluation shows that our system is both less repetitive and more diverse than baselines.
Background
RNN language models learn the conditional probability P(xt|x1, ..., xt−1) of generating the next
word xt given all previous words.
This conditional probability learned by RNNs often assigns
higher probability to repetitive, overly generic sentences, as shown in Figure 1 and also in Table 3.
Even gated RNNs such as LSTMs and GRUs have difﬁculties in properly incorporating
long-term context due to explaining-away effects
 , diminishing gradients , and lack of inductive bias for the network to learn discourse structure or global coherence beyond local patterns.
Several methods in the literature attempt to address these issues. Overly simple and generic generation can be improved by length-normalizing the
sentence probability , future cost
estimation , or a diversityboosting objective function .
Repetition can be reduced by prohibiting recurrence of the trigrams as
a hard rule . However, such
hard constraints do not stop RNNs from repeating
through paraphrasing while preventing occasional
intentional repetition.
We propose a uniﬁed framework to address all
these related challenges of long-form text generation by learning to construct a better decoding objective, generalizing over various existing modiﬁcations to the decoding objective.
The Learning Framework
We propose a general learning framework for conditional language generation of a sequence y given
a ﬁxed context x. The decoding objective for generation takes the general form
fλ(x, y) = log(Plm(y|x))+
λksk(x, y), (1)
where every sk is a scoring function.
proposed objective combines the RNN language
model probability Plm (§3.1) with a set of additional scores sk(x, y) produced by discriminatively trained communication models (§3.2),
which are weighted with learned mixture coefﬁcients λk (§3.3). When the scores sk are log probabilities, this corresponds to a Product of Experts
(PoE) model .
Generation is performed using beam search
(§3.4), scoring incomplete candidate generations
y1:i at each time step i.
The RNN language
model decomposes into per-word probabilities via
the chain rule.
However, in order to allow for
more expressivity over long range context we do
not require the discriminative model scores to factorize over the elements of y, addressing a key
limitation of RNNs.
More speciﬁcally, we use
an estimated score s′
k(x, y1:i) that can be computed for any preﬁx of y = y1:n to approximate the objective during beam search, such that
k(x, y1:n) = sk(x, y). To ensure that the training method matches this approximation as closely
as possible, scorers are trained to discriminate pre-
ﬁxes of the same length (chosen from a predetermined set of preﬁx lengths), rather than complete
continuations, except for the entailment module as
described in §3.2 Entailment Model. The preﬁx
scores are re-estimated at each time-step, rather
than accumulated over beam search.
Base Language Model
The RNN language model treats the context x and
the continuation y as a single sequence s:
log Plm(s) =
log Plm(si|s1:i−1).
Cooperative Communication Models
We introduce a set of discriminators, each of
which encodes an aspect of proper writing that
RNNs usually fail to capture.
Each model is
trained to discriminate between good and bad generations; we vary the model parameterization and
training examples to guide each model to focus on
a different aspect of Grice’s Maxims. The discriminator scores are interpreted as classiﬁcation probabilities (scaled with the logistic function where
necessary) and interpolated in the objective function as log probabilities.
Let D = {(x1, y1), . . . (xn, yn)} be the set of
training examples for conditional generation. Dx
denote all contexts and Dy all continuations. The
scoring functions are trained on preﬁxes of y to
simulate their application to partial continuations
at inference time.
In all models the ﬁrst layer embeds each word
w into a 300-dimensional vector e(w) initialized
with GloVe pretrainedembeddings.
Repetition Model
This model addresses the maxim of Quantity by
biasing the generator to avoid repetitions.
goal of the repetition discriminator is to learn to
distinguish between RNN-generated and gold continuations by exploiting our empirical observation
that repetitions are more common in completions
generated by RNN language models. However, we
do not want to completely eliminate repetition, as
words do recur in English.
In order to model natural levels of repetition, a
score di is computed for each position in the continuation y based on pairwise cosine similarity between word embeddings within a ﬁxed window of
the previous k words, where
j=i−k...i−1(CosSim(e(yj), e(yi))),
such that di = 1 if yi is repeated in the window.
The score of the continuation is then deﬁned as
srep(y) = σ(w⊤
r RNNrep(d)),
where RNNrep(d) is the ﬁnal state of a unidirectional RNN ran over the similarity scores d =
d1 . . . dn and wr is a learned vector. The model
is trained to maximize the ranking log likelihood
log σ(srep(yg) −srep(ys)), (5)
which corresponds to the probability of the gold
ending yg receiving a higher score than the ending
sampled from the RNN language model.
Entailment Model
Judging textual quality can be related to the natural language inference (NLI) task of recognizing
textual entailment : we would like to guide the generator
to neither contradict its own past generation (the
maxim of Quality) nor state something that readily follows from the context (the maxim of Quantity). The latter case is driven by the RNNs habit
of paraphrasing itself during generation.
We train a classiﬁer that takes two sentences a
and b as input and predicts the relation between
them as either contradiction, entailment or neutral. We use the neutral class probability of the
sentence pair as discriminator score, in order to
discourage both contradiction and entailment. As
entailment classiﬁer we use the decomposable attention model , a competitive,
parameter-efﬁcient model for entailment classiﬁcation.1 The classiﬁer is trained on two large entailment datasets, SNLI and
MultiNLI , which together
have more than 940,000 training examples. We
train separate models based on the vocabularies of
each of the datasets we use for evaluation.
In contrast to our other communication models,
this classiﬁer cannot be applied directly to the full
context and continuation sequences it is scoring.
Instead every completed sentence in the continuation should be scored against all preceding sentences in both the context and continuation.
Let t(a, b) be the log probability of the neutral class. Let S(y) be the set of complete sentences in y, Slast(y) the last complete sentence,
and Sinit(y) the sentences before the last complete
We compute the entailment score of
Slast(y) against all preceding sentences in x and
y, and use the score of the sentence-pair for which
we have the least conﬁdence in a neutral classiﬁcation:
sentail(x, y) = mina∈S(x)∪Sinit(y)t(a, Slast(y)).
Intuitively, we only use complete sentences because the ending of a sentence can easily ﬂip entailment. As a result, we carry over entailment
score of the last complete sentence in a generation until the end of the next sentence, in order to
maintain the presence of the entailment score in
the objective. Note that we check that the current
1We use the version without intra-sentence attention.
Data: context x, beam size k, sampling temperature t
Result: best continuation
best = None
beam = [x]
for step = 0; step < max steps; step = step +1 do
next beam = []
for candidate in beam do
next beam.extend(next k(candidate))
if termination score(candidate) > best.score
best = candidate.append(term)
for candidate in next beam do
▷score with models
candidate.score += fλ(candidate)
▷sample k candidates by score
beam = sample(next beam, k, t)
if learning then
update λ with gradient descent by comparing best
against the gold.
return best
Algorithm 1: Inference/Learning in the Learning
to Write Framework.
sentence is not directly entailed or contradicted
by a previous sentence and not the reverse. 2 In
contrast to our other models, the score this model
returns only corresponds to a subsequence of the
given continuation, as the score is not accumulated across sentences during beam search. Instead
the decoder is guided locally to continue complete
sentences that are not entailed or contradicted by
the previous text.
Relevance Model
The relevance model encodes the maxim of Relation by predicting whether the content of a candidate continuation is relevant to the given context.
We train the model to distinguish between true
continuations and random continuations sampled
from other (human-written) endings in the corpus,
conditioned on the given context.
First both the context and continuation sequences are passed through a convolutional layer,
followed by maxpooling to obtain vector representations of the sequences:
a = maxpool(conva(e(x))),
b = maxpool(convb(e(y))).
2If the current sentence entails a previous one it may simply be adding more speciﬁc information, for instance: “He
hated broccoli. Every time he ate broccoli he was reminded
that it was the thing he hated most.”
The goal of maxpooling is to obtain a vector representing the most important semantic information
in each dimension.
The scoring function is then deﬁned as
l · (a ◦b),
where element-wise multiplication of the context
and continuation vectors will amplify similarities.
We optimize the ranking log likelihood
log σ(srel(x, yg) −srel(x, yr)),
where yg is the gold ending and yr is a randomly
sampled ending.
Lexical Style Model
In practice RNNs generate text that exhibit much
less lexical diversity than their training data. To
counter this effect we introduce a simple discriminator based on observed lexical distributions
which captures writing style as expressed through
word choice. This classiﬁer therefore encodes aspects of the maxim of Manner.
The scoring function is deﬁned as
sbow(y) = wT
s maxpool(e(y)).
The model is trained with a ranking loss using negative examples sampled from the language
model, similar to Equation 5.
Mixture Weight Learning
Once all the communication models have been
trained, we learn the combined decoding objective. In particular we learn the weight coefﬁcients
λk in equation 1 to linearly combine the scoring
functions, using a discriminative loss
(fλ(x, y) −fλ(x, A(x))2, (12)
where A is the inference algorithm for beam
search decoding. The weight coefﬁcients are thus
optimized to minimize the difference between the
scores assigned to the gold continuation and the
continuation predicted by the current model.
Mixture weights are learned online: Each successive generation is performed based on the current values of λ, and a step of gradient descent
is then performed based on the prediction. This
has the effect that the objective function changes
BookCorpus
TripAdvisor
ADAPTIVELM
Table 1: Results for automatic evaluation metrics for all systems and domains, using the original continuation as the reference. The metrics are: Length - Average total length per example; Trigrams - %
unique trigrams per example; Vocab - % unique words per example.
dynamically during training: As the current samples from the model are used to update the mixture
weights, it creates its own learning signal by applying the generative model discriminatively. The
SGD learning rate is tuned separately for each
Beam Search
Due to the limitations of greedy decoding and the
fact that our scoring functions do not decompose
across time steps, we perform generation with a
beam search procedure, shown in Algorithm 1.
The naive approach would be to perform beam
search based only on the language model, and then
rescore the k best candidate completions with our
full model. We found that this approach leads to
limited diversity in the beam and therefore cannot
exploit the strengths of the full model.
Instead we score the current hypotheses in the
beam with the full decoding objective: First, each
hypothesis is expanded by selecting the k highest scoring next words according to the language
model (we use beam size k = 10). Then k sequences are sampled from the k2 candidates according to the (softmax normalized) distribution
over the candidate scores given by the full decoding objective. Sampling is performed in order
to increase diversity, using a temperature of 1.8,
which was tuned by comparing the coherence of
continuations on the validation set.
At each step, the discriminator scores are recomputed for all candidates, with the exception of
the entailment score, which is only recomputed for
hypotheses which end with a sentence terminating symbol. We terminate beam search when the
termination score, the maximum possible score
achievable by terminating generation at the current
position, is smaller than the current best score.
Experiments
We use two English corpora for evaluation. The
ﬁrst is the TripAdvisor corpus ,
a collection of hotel reviews with a total of 330
million words.3
The second is the BookCorpus
 , a 980 million word collection
of novels by unpublished authors.4
In order to
train the discriminators, mixing weights, and the
SEQ2SEQ and SEQGAN baselines, we segment
both corpora into sections of length ten sentences,
and use the ﬁrst 5 sentence as context and the second 5 as the continuation. See supplementary material for further details.
ADAPTIVELM
Our ﬁrst baseline is the same
Adaptive Softmax language
model used as base generator in our framework
(§3.1). This enables us to evaluate the effect of
our enhanced decoding objective directly. A 100k
vocabulary is used and beam search with beam
size of 5 is used at decoding time. ADAPTIVELM
achieves perplexity of 37.46 and 18.81 on Book-
Corpus and TripAdvisor respectively.
As another LM baseline we include
a continuous cache language model as implemented by Merity et al. ,
which recently obtained state-of-the-art perplexity on the Penn Treebank corpus . Due to memory constraints, we use a vocabulary size of 50k for CACHELM. To generate,
beam search decoding is used with a beam size 5.
CACHELM obtains perplexities of 70.9 and 29.71
on BookCorpus and TripAdvisor respectively.
3 
4 
BookCorpus
Speciﬁc Criteria
Overall Quality
Repetition
Contradiction
ADAPTIVELM
LM VS. REFERENCE
L2W VS. REFERENCE
TripAdvisor
Speciﬁc Criteria
Overall Quality
Repetition
Contradiction
ADAPTIVELM
LM VS. REFERENCE
L2W VS. REFERENCE
Table 2: Results of crowd-sourced evaluation on different aspects of the generation quality as well as
overall quality judgments. For each sub-criteria we report the average of comparative scores on a scale
from -2 to 2. For the overall quality evaluation decisions are aggregated over 3 annotators per example.
As our evaluation can be framed as
sequence-to-sequence transduction, we compare
against a seq2seq model directly trained to predict
5 sentence continuations from 5 sentences of context, using the OpenNMT attention-based seq2seq
implementation . Similarly to
CACHELM, a 50k vocabulary was used and beam
search decoding was performed with a beam size
Finally, as our use of discriminators is related to Generative Adversarial Networks
(GANs), we use SeqGAN , a
GAN for discrete sequences trained with policy
gradients.5 This model is trained on 10 sentence
sequences, which is signiﬁcantly longer than previous experiments with GANs for text; the vocabulary is restricted to 25k words to make training
tractable. Greedy sampling was found to outperform beam search. For implementation details, see
the supplementary material.
Evaluation Setup
We pose the evaluation of our model as the task
of generating an appropriate continuation given an
initial context. In our open-ended generation setting the continuation is not required to be a speciﬁc length, so we require our models and baselines to generate 5-sentence continuations, consistent with the way the discriminator and seq2seq
baseline datasets are constructed.
Previous work has reported that automatic mea-
5We use the implementation available at https://
github.com/nhynes/abc.
sures such as BLEU and
Meteor do not lead
to meaningful evaluation when used for long
or creative text generation where there can be
high variance among acceptable generation outputs .
However, we still report these measures as one
component of our evaluation. Additionally we report a number of custom metrics which capture
important properties of the generated text: Length
– Average sequence length per example; Trigrams
– percentage of unique trigrams per example; Vocab – percentage of unique words per example.
Endings generated by our model and the baselines
are compared against the reference endings in the
original text. Results are given in Table 1.
For open-ended generation tasks such as our
own, human evaluation has been found to be the
only reliable measure . For human evaluation, two possible endings are presented to a human, who assesses the text according to several criteria, which
are closely inspired by Grice’s Maxims: repetition, contradiction, relevance and clarity. See supplementary material for examples of the evaluation forms we used. For each criterion, the two
continuations are compared using a 5-point Likert
scale, to which we assign numerical values of −2
to 2. The scale measures whether one generation
is strongly or somewhat preferred above the other,
or whether they are equal. Finally, the human is
asked to make a judgement about overall quality:
which ending is better, or are they of equal qual-
We thoroughly enjoyed staying at this hotel. We travelled with our kids aged 4 and 6. The rooms were
clean, spacious and well maintained. The beds were great and all of us slept well. The hotel provided
vouchers towards breakfast at the adjoining restaurant.
We didn’t have breakfast, so can’t comment on that. The concierge was very helpful, and gave us directions, etc. Overall, we would deﬁnitely recommend this hotel to others. We would stay here again in a
heartbeat. If you’re looking for a great location, and good value, I’d recommend this hotel.
ADAPTIVELM
The breakfast buffet was very good, and the staff was very friendly and helpful, and it was a great way to
start our day! I would deﬁnitely stay here again, and would recommend it to my friends, and would stay
there again if I ever go back to Montreal. The location was great, and the staff was friendly and helpful.
I would deﬁnitely stay here again!!
The staff was very friendly and helpful. The hotel is located in a great location. It is right in the middle
of the gaslamp district. I would recommend this hotel to anyone. I would deﬁnitely stay here again.
The staff were very friendly and helpful. The hotel is in a great location. The staff were very friendly and
helpful. The hotel is in a great location. The staff were very friendly and helpful.
We had a breakfast at Shula’s & a delicious breakfast. The staff was very helpful and helpful. The
breakfast was great as well. The staff was very helpful and friendly. We had a great service and the food
was excellent.
The restaurant was great and we used the vouchers towards whatever breakfast we ordered. The hotel
had amazing grounds with a putting golf course that was fun for everyone. The pool was fantastic and
we lucked out with great weather. We spent many hours in the pool, lounging, playing shufﬂeboard and
snacking from the attached bar. The happy hour was great perk.
Table 3: Example continuations generated by our model (L2W) and various baselines (all given the
same context from TripAdvisor) compared to the reference continuation. For more examples go to
 
The human evaluation is performed on 100 examples selected from the test set of each corpus,
for every pair of generators that are compared. We
present the examples to workers on Amazon Mechanical Turk, using three annotators for each example. The results are given in Table 2. For the
Likert scale, we report the average scores for each
criterion, while for the overall quality judgement
we simply aggregate votes across all examples.
Results and Analysis
Quantitative Results
The absolute performance of all the evaluated systems on BLEU and Meteor is quite low (Table 1),
as expected. However, in relative terms L2W is
superior or competitive with all the baselines, of
which ADAPTIVELM performs best. In terms of
vocabulary and trigram diversity only SEQGAN
is competitive with L2W, likely due to the fact
that sampling based decoding was used. For generation length only L2W and ADAPTIVELM even
approach human levels, with the former better on
BookCorpus and the latter on TripAdvisor.
Under the crowd-sourced evaluation (Table 2),
on BookCorpus our model is consistently favored
over the baselines on all dimensions of comparison. In particular, our model tends to be much
less repetitive, while being more clear and relevant than the baselines.
ADAPTIVELM is the
most competitive baseline owing partially to the
robustness of language models and to greater vocabulary coverage through the adaptive softmax.
SEQGAN, while failing to achieve strong coherency, is surprisingly diverse, but tended to produce far shorter sentences than the other models.
CACHELM has trouble dealing with the complex
vocabulary of our domains without the support of
either a hierarchical vocabulary structure (as in
ADAPTIVELM) or a structured training method
(as with SEQGAN), leading to overall poor results. While the SEQ2SEQ model has low conditional perplexity, we found that in practice it is
less able to leverage long-distance dependencies
than the base language model, producing more
generic output. This reﬂects our need for more
complex evaluations for generation, as such models are rarely evaluated under metrics that inspect
characteristics of the text, rather than ability to
predict the gold or overlap with the gold.
For the TripAdvisor corpus, L2W is ranked
higher than the baselines on overall quality, as well
as on most individual metrics, with the exception
that it fails to improve on contradiction and clarity over the ADAPTIVELM (which is again the
most competitive baseline). Our model’s strongest
improvements over the baselines are on repetition
and relevance.
Trip Advisor Ablation
Ablation vs. LM
Repetition
Contradiction
REPETITION ONLY
ENTAILMENT ONLY
RELEVANCE ONLY
LEXICAL STYLE ONLY
Table 4: Crowd-sourced ablation evaluation of generations on TripAdvisor. Each ablation uses only one
discriminative communication model, and is compared to ADAPTIVELM.
To investigate the effect of individual discriminators on the overall performance, we report the results of ablations of our model in Table 4. For each
ablation we include only one of the communication modules, and train a single mixture coefﬁcient for combining that module and the language
model. The diagonal of Table 4 contains only positive numbers, indicating that each discriminator
does help with the purpose it was designed for.
Interestingly, most discriminators help with most
aspects of writing, but all except repetition fail to
actually improve the overall quality over ADAP-
The repetition module gives the largest boost by
far, consistent with the intuition that many of the
deﬁciencies of RNN as a text generator lie in semantic repetition. The entailment module (which
was intended to reduce contradiction) is the weakest, which we hypothesize is the combination of
(a) mismatch between training and test data (since
the entailment module was trained on SNLI and
MultiNLI) and (b) the lack of smoothness in the
entailment scorer, whose score could only be updated upon the completion of a sentence.
Crowd Sourcing
Surprisingly, L2W is even preferred over the original continuation of the initial text on BookCorpus. Qualitative analysis shows that L2W’s continuation is often a straightforward continuation
of the original text while the true continuation
is more surprising and contains complex references to earlier parts of the book. While many of
the issues of automatic metrics have been alleviated by
crowd-sourcing, we found it difﬁcult to incentivize
crowd workers to spend signiﬁcant time on any
one datum, forcing them to rely on a shallower understanding of the text.
Qualitative Analysis
L2W generations are more topical and stylistically coherent with the context than the baselines.
Table 3 shows that L2W, ADAPTIVELM, and
SEQGAN all start similarly, commenting on the
breakfast buffet, as breakfast was mentioned in the
last sentence of the context. The language model
immediately offers generic compliments about the
breakfast and staff, whereas L2W chooses a reasonable but less obvious path, stating that the previously mentioned vouchers were not used.
fact, L2W is the only system not to use the line
“The staff was very friendly and helpful.”, despite this sentence appearing in less than 1% of
reviews. The semantics of this sentence, however,
is expressed in many different surface forms in the
training data (e.g., “The staff were kind and quick
to respond.”).
The CACHELM begins by generating the
same over-used sentence and only produce short,
generic sentences throughout. Seq2Seq simply repeats sentences that occur often in the training
set, repeating one sentence three times and another twice. This indicates that the encoded context is essentially being ignored as the model fails
to align the context and continuation.
The SEQGAN system is more detailed, e.g.
mentioning a speciﬁc location “Shula’s” as would
be expected given its highly diverse vocabulary (as
seen in Table 1). Yet it repeats itself in the ﬁrst sentence. (e.g. “had a breakfast”, “and a delicious
breakfast”). Consequently SEQGAN quickly devolves into generic language, repeating the incredibly common sentence “The staff was very helpful
and friendly.”, similar to SEQ2SEQ.
The L2W models do not ﬁx every degenerate
characteristic of RNNs. The TripAdvisor L2W
generation consists of meaningful but mostly disconnected sentences, whereas human text tends
to build on previous sentences, as in the reference continuation. Furthermore, while L2W re-
peats itself less than any of our baselines, it still
paraphrases itself, albeit more subtly: “we would
deﬁnitely recommend this hotel to others.” compared to “I’d recommend this hotel.”
This example also exposes a more ﬁne-grained issue:
L2W switches from using “we” to using “I” midgeneration. Such subtle distinctions are hard to
capture during beam re-ranking and none of our
models address the linguistic issues of this subtlety.
Related Work
Alternative Decoding Objectives
A number of
papers have proposed alternative decoding objectives for generation .
et al. proposed a diversity-promoting objective that interpolates the conditional probability score with negative marginal or reverse conditional probabilities. Yu et al. also incorporate the reverse conditional probability through
a noisy channel model in order to alleviate the
explaining-away problem, but at the cost of signiﬁcant decoding complexity, making it impractical for paragraph generation. Modiﬁed decoding
objectives have long been a common practice in
statistical machine translation and remain common with neural machine
translation, even when an extremely large amount
of data is available .
by all the above approaches, our work presents a
general learning framework together with a more
comprehensive set of composite communication
Pragmatic Communication Models
for pragmatic reasoning about communicative
goals such as Grice’s maxims have been proposed in the context of referring expression generation . Andreas and
Klein proposed a neural model where candidate descriptions are sampled from a generatively trained speaker, which are then re-ranked
by interpolating the score with that of the listener, a discriminator that predicts a distribution
over choices given the speaker’s description. Similar to our work the generator and discriminator
scores are combined to select utterances which follow Grice’s maxims. Yu et al. proposed
a model where the speaker consists of a convolutional encoder and an LSTM decoder, trained with
a ranking loss on negative samples in addition to
optimizing log-likelihood.
Generative
Adversarial
 are another alternative
to maximum likelihood estimation for generative
backpropagating through
discrete sequences and the inherent instability
of the training objective both
present signiﬁcant challenges.
While solutions
have been proposed to make it possible to train
GANs for language they have not yet been shown to produce
high quality long-form text, as our results conﬁrm.
Generation with Long-term Context
prior works studied paragraph generation using
sequence-to-sequence models for image captions
 , product reviews , sport reports
 , and recipes . While these prior works focus on developing neural architectures for learning domain speciﬁc discourse patterns, our work proposes a general framework for learning a generator that is
more powerful than maximum likelihood decoding from an RNN language model for an arbitrary
target domain.
Conclusion
We proposed a uniﬁed learning framework for the
generation of long, coherent texts, which overcomes some of the common limitations of RNNs
as text generation models. Our framework learns a
decoding objective suitable for generation through
a learned combination of sub-models that capture
linguistically-motivated qualities of good writing.
Human evaluation shows that the quality of the
text produced by our model exceeds that of competitive baselines by a large margin.
Acknowledgments
We thank the anonymous reviewers for their insightful feedback and Omer Levy for helpful discussions. This research was supported in part by
NSF (IIS-1524371), DARPA CwC through ARO
(W911NF-15-1-0543), Samsung AI Research, and
gifts by Tencent, Google, and Facebook.