C2AE: Class Conditioned Auto-Encoder for Open-set Recognition
Poojan Oza and Vishal M. Patel
Department of Electrical and Computer Engineering
Johns Hopkins University, 3400 N. Charles St, Baltimore, MD 21218, USA
 , 
Models trained for classiﬁcation often assume that all
testing classes are known while training.
As a result,
when presented with an unknown class during testing, such
closed-set assumption forces the model to classify it as one
of the known classes. However, in a real world scenario,
classiﬁcation models are likely to encounter such examples. Hence, identifying those examples as unknown becomes critical to model performance.
A potential solution to overcome this problem lies in a class of learning
problems known as open-set recognition. It refers to the
problem of identifying the unknown classes during testing,
while maintaining performance on the known classes. In
this paper, we propose an open-set recognition algorithm
using class conditioned auto-encoders with novel training
and testing methodology. In contrast to previous methods,
training procedure is divided in two sub-tasks, 1. closedset classiﬁcation and, 2. open-set identiﬁcation (i.e. identifying a class as known or unknown).
Encoder learns
the ﬁrst task following the closed-set classiﬁcation training
pipeline, whereas decoder learns the second task by reconstructing conditioned on class identity. Furthermore, we
model reconstruction errors using the Extreme Value Theory of statistical modeling to ﬁnd the threshold for identifying known/unknown class samples.
Experiments performed on multiple image classiﬁcation datasets show proposed method performs signiﬁcantly better than state of the
1. Introduction
Recent advancements in computer vision have resulted
in signiﬁcant improvements for image classiﬁcation systems , , , .
Especially the rise of Deep
Convolutional Neural Network has resulted in classiﬁcation error rates surpassing the human-level performance
 . These promising results, enable their potential use in
many real world applications. However, when deployed in
Figure 1: Open-set recognition problem: Data samples
from Blue Jay, Seal, Dog and Penguin are from the known
class set (K). Also, many classes not known during training, will be present at testing, i.e., samples from unknown
class set (U). The goal is to correctly classify any sample
coming from set K, as either Blue Jay, Seal, Dog or Penguin
and identify samples coming from U as unknown.
a real world scenario, such systems are likely to observe
samples from classes not seen during training (i.e.
unknown classes also referred as “unknown unknowns” ).
Since, the traditional training methods follow this closedset assumption, the classiﬁcation systems observing any unknown class samples are forced to recognize it as one of the
known classes. As a result, it affects the performance of
these systems, as evidenced by Jain et al. with digit recognition example. Hence, it becomes critical to correctly identify test samples as either known or unknown for a classiﬁcation model. This problem setting of identifying test
samples as known/unknown and simultaneously correctly
classifying all of known classes, is referred to as open-set
recognition . Fig. 1 illustrates a typical example of classiﬁcation in the open-set problem setting.
In an open-set problem setting, it becomes challenging to
identify unknown samples due to the incomplete knowledge
of the world during training (i.e. only the known classes
are accessible). To overcome this problem many open-set
 
methods in the literature , , , adopt recognition score based thresholding models. However, when
using these models one needs to deal with two key questions, 1) what is a good score for open-set identiﬁcation?
(i.e., identifying a class as known or unknown), and given a
score, 2) what is a good operating threshold for the model?.
There have been many methods that explore these questions
in the context of traditional methods such as Support Vector Machines , , Nearest Neighbors , and
Sparse Representation . However, these questions are
relatively unexplored in the context of deep neural networks
 , , , , .
Even-though deep neural networks are powerful in learning highly discriminative representations, they still suffer
from performance degradation in the open-set setting .
In a naive approach, one could apply a thresholding model
on SoftMax scores. However, as shown by experiments in
 , that model is sub-optimal for open-set identiﬁcation. A
few methods have been proposed to better adapt the Soft-
Max scores for open-set setting. Bendale et al. proposed
a calibration strategy to update SoftMax scores using extreme value modeling . Other strategies, Ge et al. 
and Lawrence et al. follow data augmentation technique using Generative Adversarial Networks (GANs) .
GANs are used to synthesize open-set samples and later
used to ﬁne-tuning to adapt SoftMax/OpenMax scores for
open-set setting. Shu et al. introduced a novel sigmoidbased loss function for training the neural network to get
better scores for open-set identiﬁcation.
All of these methods modify the SoftMax scores, so that
it can perform both open-set identiﬁcation and maintain its
classiﬁcation accuracy. However, it is extremely challenging to ﬁnd a single such score measure, that can perform
both. In Contrast to these methods, in proposed approach
the training procedure for open-set recognition using class
conditional auto-encoders, is divided it into two sub-tasks,
1. closed-set classiﬁcation, and 2. open-set identiﬁcation.
These sub-tasks are trained separately in a stage-wise manner. Experiments show that such approach provides good
open-set identiﬁcation scores and it is possible to ﬁnd a
good operating threshold using the proposed training and
testing strategy.
In summary, this paper makes following contributions,
• A novel method for open-set recognition is proposed with
novel training and testing algorithm based on class conditioned auto-encoders.
• We show that dividing open-set problem in sub-tasks can
help learn better open-set identiﬁcation scores.
• Extensive experiments are conducted on various image
classiﬁcation datasets and comparisons are performed
against several recent state-of-the-art approaches. Furthermore, we analyze the effectiveness of the proposed
method through ablation experiments.
2. Related Work
Open-set Recognition. The open-set recognition methods
can be broadly classiﬁed in to two categories, traditional
methods and neural network-based methods.
Traditional
methods are based on classiﬁcation models such as Support Vector Machines (SVMs), Nearest Neighbors, Sparse
Representation etc. Scheirer et al. extended the SVM
for open-set recognition by calibrating the decision scores
using the extreme value distribution. Speciﬁcally, Scheirer
et al. utilized two SVM models, one for identifying
a sample as unknown (referred as CAP models) and other
for traditional closed-set classiﬁcation. PRM Junior et al.
 proposed a nearest neighbor-based open-set recognition model utilizing the neighbor similarity as a score for
open-set identiﬁcation. PRM Junior et al. later also presented specialized SVM by constraining the bias term to be
negative. This strategy in the case of Radial Basis Function
kernel, yields an open-set recognition model. Zhang et al.
 proposed an extension of the Sparse Representationbased Classiﬁcation (SRC) algorithm for open-set recognition. Speciﬁcally, they model residuals from SRC using the
Generalized-Pareto extreme value distribution to get score
for open-set identiﬁcation.
In neural network-based methods, one of the earliest
works by Bendale et al. introduced an open-set recognition model based on “activation vectors” (i.e.
penultimate layer of the network). Bendale et al. utilized metarecognition for multi-class classiﬁcation by modeling the
distance from “mean activation vector” using the extreme
value distribution.
SoftMax scores are calibrated using
these models for each class. These updated scores, termed
as OpenMax, are then used for open-set identiﬁcation. Ge
et al. introduced a data augmentation approach called
G-OpenMax.
They generate unknown samples from the
known class training data using GANs and use it to ﬁnetune the closed-set classiﬁcation model. This helps in improving the performance for both SoftMax and OpenMax
based deep network. Along the similar motivation, Neal
et al. proposed a data augmentation strategy called
counterfacutal image generation. This strategy also utilizes
GANs to generate images that resemble known class images but belong to unknown classes. In another approach,
Shu et al. proposed a k-sigmoid activation-based novel
loss function to train the neural network. Additionally, they
perform score analysis on the ﬁnal layer activations to ﬁnd
an operating threshold, which is helpful for open-set identiﬁcation. There are some variation of open-set recognition
by relaxing its formulation in the form of anomaly detection
 , , and novelty detection , etc, but for
this paper we only focus on the general open-set recognition
Extreme Value Theory.
Extreme value modeling is a
branch of statistics that deals with modeling of statistical
Classifier
Classification
Closed­Set Model
Closed­set Training
[­1 ­1 . . . 1]
Conditioning
Reconstructions
Reconstructions
Open­set Recognition Model
Non­matched
Reconstruction Error
Reconstruction Error
2 Open­set Training
Non­match Condition
[­1 ­1 . . ­1]
[1 ­1 . . . ­1]
[1 ­1 . . . ­1]
[­1 1 . . . ­1]
[­1 ­1 . . . 1]
Match Condition
Classifier
3 Open­set Testing (k­Inference)
Recognition
[1 ­1 . . . ­1]
[­1 1 . . . ­1]
. . . . . .
[­1 ­1 . . . 1]
Figure 2: Block diagram of the proposed method: 1) Closed-set training, Encoder (F) and Classiﬁer (C) are trained with
the traditional classiﬁcation loss. 2) Open-set Training, To train an open-set identiﬁcation model, auto-encoder network
Encoder (F) with frozen weights, and Decoder (G), are trained to perfectly or poorly reconstruct the images depending on
the label condition vector. Reconstruction errors are then modeled using the extreme value distribution to ﬁnd the operating
threshold of the method. 3) Open-set Testing, Open-set recognition model produces the classiﬁcation prediction (ypred) and
k reconstruction errors, conditioned with each condition vector. If the minimum reconstruction error is below the threshold
value obtained from the EVT model, the test sample is classiﬁed as one of the k classes, or else it is classiﬁed as unknown.
extremes. The use of extreme value theory in vision tasks
largely deals with post recognition score analysis , .
Often for a given recognition model the threshold to reject/accept lies in the overlap region of extremes of match
and non-match score distributions . In such cases, it
makes sense to model the tail of the match and non-match
recognition scores as one of the extreme value distributions.
Hence, many visual recognition methods including some
described above, utilize extreme value models to improve
the performance further , .
In the proposed approach as well, the tail of open-set identiﬁcation scores are
modeled using the extreme value distribution to ﬁnd the optimal threshold for operation.
3. Proposed Method
The proposed approach divides the open-set recognition
problem into two sub-tasks, namely, closed-set classiﬁcation and open-set identiﬁcation.
The training procedure
for these tasks are shown in Fig. 2 as stage-1 and stage-
2. Stage-3 in Fig. 2 provides overview of the proposed approach at inference. In what follows, we present details of
these stages.
3.1. Closed-set Training (Stage 1)
Given images in a batch {X1, X2, ..., XN} ∈K, and
their corresponding labels {y1, y2, ..., yN}. Here N is the
batch size and ∀yi ∈{1, 2, .., k}. The encoder (F) and the
classiﬁer (C) with parameters Θf and Θc, respectively are
trained using the following cross entropy loss,
Lc({Θf, Θc}) = −1
Iyi(j) log[pyi(j)],
where, Iyi is an indicator function for label yi (i.e., one
hot encoded vector) and pyi = C(F(Xi)) is a predicted
probability score vector.
pyi(j) is probability of the ith
sample being from the jth class.
3.2. Open-set Training (Stage 2)
There are two major parts in open-set training, conditional decoder training, followed by EVT modeling of the
reconstruction errors. In this stage, the encoder and classi-
ﬁer weights are ﬁxed and don’t change during optimization.
Conditional Decoder Training
For any batch described in Sec. 3.1, F is used to extract the
latent vectors as, {z1, z2, ..., zN}. This latent vector batch
is conditioned following the work by Perez et al. called
FiLM. FiLM inﬂuences the input feature map by applying
a feature-wise linear modulations (hence the name FiLM)
based on conditioning information. For a input feature z
and vector lj containing conditioning information can be
γj = Hγ(lj),
βj = Hβ(lj),
zlj = γj ⊙z + βj,
+1, x = j,
−1, x ̸= j,
x, j ∈{1, 2, ..., k}.
Here, Hγ and Hβ are neural networks with parameters Θγ
and Θβ. Tensors zlj, γj, βj have the same shape and ⊙
represents the Hadamard product. lj is used for conditioning, and referred to as label condition vector in the paper.
Also, the notation zlj is used to describe the latent vector z
conditioned on the label condition vector lj, i.e, z|lj.
The decoder (G with parameters Θg) is expected to perfectly reconstruct the original input when conditioned on
the label condition vector matching the class identity of the
input, referred here as the match condition vector (lm), can
be viewed as a traditional auto-encoder. However, here G
is additionally trained to poorly reconstruct the original input when conditioned on the label condition vector, that
does not match the class identity of the input, referred here
as the non-match condition vector (lnm). The importance
of this additional constraint on the decoder is discussed in
Sec. 3.2.2 while modeling the reconstruction errors using
EVT. For the rest of this paper, we use superscript m and
nm to indicate match and non-match, respectively.
Now, for a given input Xi from the batch and lm = lym
and lnm = lynm
, for any random ynm
̸= yi sampled from
{1, 2, .., k}, be its corresponding match and non-match condition vectors, the feed forward path for stage-2 can be summarized through the following equations,
zi = F(Xi),
i = Hγ(lym
i = Hβ(lym
zilm = γym
i ⊙zi + βym
zilnm = γynm
⊙zi + βynm
= G(zlnm).
Following the above feed-forward path, the loss functions in the second stage of training to train the decoder (G
with parameters Θg) and conditioning layer (with parameters Θγ and Θβ) are given as follows,
r ({Θg, Θγ, Θβ}) = 1
({Θg, Θγ, Θβ}) = 1
{Θg,Θγ,Θβ}αLm
r ({Θg, Θγ, Θβ})
+(1 −α)Lnm
({Θg, Θγ, Θβ}).
Here, the loss function Lm
r corresponds to the constraint
that output generated using match condition vector ˜Xm
should be perfect reconstruction of Xi. Whereas, the loss
function Lnm
corresponds to the constraint that output generated using non match condition vector ˜Xnm
, should have
poor reconstruction. To enforce the later condition, another
batch {Xnm
, ..., Xnm
N }, is sampled from the training data, such that new batch does not have class identity
consistent with the match condition vector. This in effect
achieves the goal of poor reconstruction when conditioned
. This conditioning strategy in a way, emulates openset behavior (as will be discussed further in Sec. 3.2.2).
Here, the network is speciﬁcally trained to produce poor
reconstructions when class identity of an input image does
not match the condition vector. So, when encountered with
an unknown class test sample, ideally none of the condition vector would match the input image class identity. This
will result in poor reconstruction for all condition vectors.
While, when encountered with the known test sample, as
one of the condition vector will match the input image class
identity, it will produce a perfect reconstruction for that particular condition vector. Hence, training with the non-match
loss helps the network adapt better to open-set setting. Here,
r are weighted with α ∈ .
EVT Modeling
Extreme Value Theory. Extreme value theory is often used
in many visual recognition systems and is an effective tool
for modeling post-training scores , . It has been
used in many applications such as ﬁnance, railway track
inspection etc. , , as well as open-set recognition , , . In this paper we follow the Picklands-
Balkema-deHaan formulation , of the extreme value
theorem. It considers modeling probabilities conditioned
on random variable exceeding a high threshold. For a given
random variable W with a cumulative distribution function
(CDF) FW (w) the conditional CDF for any w exceeding
the threshold u is deﬁned as,
FU(w) = P(w−u ≤w|w > u) = FW (u + w) −FW (u)
Reconstruction Error
Normalized Histogram
(a) Normalized histogram of match and non-match reconstruction errors.
Reconstruction Error
Normalized Histogram
(b) Normalized histogram of known and unknown reconstruction errors.
Figure 3: Histogram of the reconstruction errors corresponding to the SVHN dataset.
where, P(·) denotes probability measure function. Now,
given I.I.D. samples, {Wi, ..., Wn}, the extreme value theorem states that, for large class of underlying distributions and given a large enough u, FU can be well approximated by the Generalized Pareto Distribution (GPD),
G(w; ζ, µ) =
1 −(1 + ζ·w
ζ , if ζ ̸= 0,
, if ζ = 0,
such that −∞< ζ < +∞, 0 < µ < +∞, w > 0 and
ζw > −µ. G(.) is CDF of GPD and for ζ = 0, reduces to
the exponential distribution with parameter µ and for ζ ̸= 0
takes the form of Pareto distribution .
Parameter Estimation. When modeling the tail of any
distribution as GPD, the main challenge is in ﬁnding the
tail parameter u to get the conditional CDF. However, it is
possible to ﬁnd an estimated value of u using mean excess
function (MEF), i.e., E[W −u|W > u] . It has been
shown that for GPD, MEF holds a linear relationship with
u. Many researchers use this property of GPD to estimate
the value of u , . Here, the algorithm for ﬁnding u,
introduced in for GPD is adopted with minor modiﬁcations. See , for more details regarding MEF or
tail parameter estimation. After getting an estimate for u,
since from extreme value theorem , we know that set
{w ∈W | w > u}, follows GPD distribution, rest of the
parameters for GPD, i.e. ζ and µ can be easily estimated
using the maximum likelihood estimation techniques ,
except for some rarely observed cases .
Threshold Calculation
sections, Sec. 3.1 and Sec. 3.2, set of match and nonmatch reconstruction errors are created from training set,
{X1, X2, ..., XNtrain}
K, and their corresponding
match and non match labels, {ym
2 , ..., ym
Ntrain} and
, ..., ynm
Ntrain}. Let, rm
i be the match reconstruction error and rnm
be the non match reconstruction error for
the input Xi, then the set of match and non match errors can
be calculated as,
= G(Hγ(lym
i ) ⊙F(Xi) + Hβ(lym
= G(Hγ(lynm
) ⊙F(Xi) + Hβ(lynm
Sm = {rm ∈R+ ∪{0} | rm
i = ||Xi −˜Xm
Snm = {rnm ∈R+ ∪{0} | rnm
= ||Xi −˜Xnm
∀i ∈{1, 2, ..., Ntrain}.
Typical histograms of Sm (set of match reconstruction
errors) and Snm (set of non-match reconstruction errors)
are shown in Fig. 3a. Note that the elements in these sets
are calculated solely based on what is observed during training (i.e., without utilizing any unknown samples). Fig. 3b
shows the normalized histogram of the reconstruction errors
observed during inference from the test samples of known
class set (K), and unknown class set (U). Comparing these
ﬁgures in Fig. 3, it can be observed that the distribution of
Sm and Snm computed during training, provides a good approximation for the error distributions observed during inference, for test samples from known set (K) and unknown
This observation also validates that non match
training emulates an open-set test scenario (also discussed
in Sec. 3.2) where the input does not match any of the class
labels. This motivates us to use Sm and Snm to ﬁnd an operating threshold for open-set recognition to make a decision
about any test sample being known/unknown.
Now, It is safe to assume that the optimal operating
threshold (τ ∗) lies in the region Sm∩Snm. Here, the underlying distributions of Sm and Snm are not known. However,
as explained in 3.2.2, it is possible to model the tails of Sm
(right tail) and Snm (left tail) with GPD as Gm and Gnm
with G(·) being a CDF. Though, GPD is only deﬁned for
modeling maxima, before ﬁtting Gnm left tail of Snm we
perform inverse transform as S′
nm = −Snm. Assuming the
prior probability of observing unknown samples is pu, the
probability of errors can be formulated as a function of the
threshold τ,
[(1 −pu) ∗Pm(r > τ) + pu ∗Pnm(−r < −τ)]
[(1 −pu) ∗(1 −Gm(τ)) + pu ∗(1 −Gnm(τ))].
Solving the above equation should give us an operating
threshold that can minimize the probability of errors for a
given model and can be solved by a simple line search algorithm by searching for τ ∗in the range {Sm ∩Snm}. Here,
the accurate estimation of τ ∗depends on how well Sm and
Snm represent the known and unknown error distributions.
It also depends on the prior probability pu, effect of this
prior will be further discussed in Sec. 4.3.
3.3. Open-set Testing by k-inference (Stage 3)
Here, we introduce the open-set testing algorithm for
proposed method.
The testing procedure is described in
Algo. 1 and an overview of this is also shown in Fig. 2. This
testing strategy involves conditioning the decoder k-times
with all possible condition vectors to get k reconstruction
errors. Hence, it is referred as k-inference algorithm.
4. Experiments and Results
In this section we evaluate the performance of the proposed approach and compare it with the state of the art
open-set recognition methods. The experiments in Sec. 4.2,
we measure the ability of algorithm to identify test samples
as known or unknown without considering operating threshold. In second set of experiments in Sec. 4.3, we measure
overall performance (evaluated using F-measure) of openset recognition algorithm.
Additionally through ablation
experiments, we analyze contribution from each component
of the proposed method.
4.1. Implementation Details
We use Adam optimizer with learning rate 0.0003
and batch size, N=64.
The parameter α, described in
Sec. 3.2, is set equal to 0.9. For all the experiments, conditioning layer networks Hγ and Hβ are a single layer fully
connected neural networks. Another important factor affecting open-set performance is openness of the problem.
Deﬁned by Scheirer et al. , it quantiﬁes how open the
problem setting is,
2 × Ntrain
Ntest + Ntarget
where, Ntrain is the number of training classes seen during training, Ntest is the number of test classes that will
Algorithm 1 k-Inference Algorithm
Require: Trained network models F, C, G, Hγ, Hβ
Require: Threshold τ from EVT model
Require: Test image X, k condition vectors {l1, . . . , lk}
1: Latent space representation, z = F(X)
2: Prediction probabilities, py = C(z)
3: predict known label, ypred = argmax(py)
4: for i = 1, . . . , k do
zli = Hγ(li) ⊙z + Hβ(li)
˜Xi = G(zli)
Rec(i) = ||X −˜Xi||1
8: end for
9: Recmin = sort(Rec)
10: if Recmin < τ do
predict X as Known, with label ypred
12: else do
predict X as Unknown
14: end if
be observed during testing, Ntarget is the number of target
classes that needs to be correctly recognized during testing.
We evaluate performance over multiple openness value depending on the experiment and dataset.
4.2. Experiment I : Open-set Identiﬁcation
The evaluation protocol deﬁned in is considered and
area under ROC (AUROC) is used as evaluation metric.
AUROC provides a calibration free measure and characterizes the performance for a given score by varying threshold.
The encoder, decoder and classiﬁer architecture for this experiment is similar to the architecture used by in their
experiments. Following the protocol in , we report the
AUROC averaged over ﬁve randomized trials.
Here, we provide summary of these protocols for each
MNIST, SVHN, CIFAR10. For MNIST , SVHN 
and CIFAR10 , openness of the problem is set to O =
13.39%, by randomly sampling 6 known classes and 4 unknown classes.
CIFAR+10, CIFAR+50. For CIFAR+M experiments, 4
classes are sampled from CIFAR10 for training. M non
overlapping classes are used as the unknowns, which are
sampled from the CIFAR100 dataset . Openness of the
problem for CIFAR+10 and CIFAR+50 is O = 33.33% and
62.86%, respectively.
TinyImageNet. For experiments with the TinyImageNet
 , 20 known classes and 180 unknown classes with openness O = 57.35% are randomly sampled for evaluation.
TinyImageNet
OpenMax (CVPR’16)
G-OpenMax (BMVC’17)
OSRCI (ECCV’18)
Proposed Method
Table 1: AUROC for open-set identiﬁcation, values other than the proposed method are taken from .
Comparison with state-of-the-art
For comparing the open-set identiﬁcation performance, we
consider the following methods:
I. SoftMax : SoftMax score of a predicted class is used for
open-set identiﬁcation.
II. OpenMax : The score of k+1th class and score of
the predicted class is used for open-set identiﬁcation.
III. G-OpenMax : It is a data augmentation technique,
which utilizes the OpenMax scores after training the
network with the generated data.
IV. OSRCI : Another data augmentation technique
called counterfactual image generation is used for training
the network for k+1 class classiﬁcation. We refer to this
method as Open-set Recognition using Counterfactual
Images (OSRCI). The score value P(yk+1) −max
is used for open-set identiﬁcation.
Results corresponding to this experiment are shown in
Table 2. As seen from this table, the proposed method outperform the other methods, showing that open-set identiﬁcation training in proposed approach learns better scores for
identifying unknown classes. From the results, we see that
our method on the digits dataset produces a minor improvement compared to the other recent methods. This is mainly
do the reason that results on the digits dataset are almost
saturated. On the other hand, our method performs significantly better than the other recent methods on the object
datasets such as CIFAR and TinyImageNet.
4.3. Experiment II : Open-set Recognition
This experiment shows the overall open-set recognition
performance evaluated with F-measure. For this experiment
we consider LFW Face dataset . We extend the protocol introduced in for open-set face recognition on LFW.
Total 12 classes containing more than 50 images are considered as known classes and divided into training and testing
split by 80/20 ratio. Image size is kept to 64×64. Since,
LFW has 5717 number of classes, we vary the openness
from 0% to 93% by taking 18 to 5705 unknown classes during testing. Since, many classes contain only one image,
instead of random sampling, we sort them according to the
number of images per class and add it sequentially to increase the openness. It is obvious that with the increase in
openness, the probability of observing unknown will also
increase. Hence, it is reasonable to assume that prior probability pu will be a function of openness. For this experiment
we set pu = 0.5 ∗O.
Comparison with state-of-the-art
For comparing the open-set recognition performance, we
consider the following methods:
I. W-SVM (PAMI’14) : W-SVM is used as formulated
in , which trains Weibull calibrated SVM classiﬁer for
open set recognition.
II. SROR (PAMI’16) : SROR is used as formulated in .
It uses sparse representation-based framework for open-set
recognition.
III. DOC (EMNLP’16) : It utilizes a novel sigmoid-based
loss function for training a deep neural network .
To have a fair comparison with these methods, we use
features extracted from the encoder (F) to train W-SVM
and SROR. For DOC, the encoder (F) is trained with the
loss function proposed in . Experiments on LFW are
performed using a U-Net inspired encoder-decoder architecture. More details regarding network architecture is
included in the supplementary material.
Results corresponding to this experiment is shown in
Fig. 4a. From this ﬁgure, we can see that the proposed approach remains relatively stable with the increase in openness, outperforming all other methods.
One interesting
trend noticed here is, that DOC initially performs better than
the statistical methods such as W-SVM and SROR. However with openness more than 50% the performance suffers
signiﬁcantly. While the statistical methods though initially
perform poor compared to DOC, but remain relatively stable and performs better than DOC as the openness is increased (especially over O >50%).
Ablation Study
In this section, we present ablation analysis of the proposed
approach on the LFW Face dataset. The contribution of
individual components to the overall performance of the
method is reported by creating multiple baselines of the proposed approach. Starting with the most simple baseline, i.e.,
thresholding SoftMax probabilities of a closed-set model,
Openness (%)
(a) F-measure comparisons for the open-set recognition experiment.
Openness (%)
(b) F-measure comparisons for the ablation study.
Figure 4: Performance evaluation on the LFW dataset.
each component is added building up to the proposed approach. Detailed descriptions of these baselines are given
as follows,
I. CLS : Encoder (F) and the classiﬁer (C) are trained for
k-class classiﬁcation. Samples with probability score prediction less than 0.5 are classiﬁed as unknown.
II. CLS+DEC : In this baseline, only the networks F, C and
the decoder (G) are trained as described in Sec. 3, except G
is only trained with match loss function, Lm
r . Samples with
more than 95% of maximum train reconstruction error observed, are classiﬁed as unknown.
III. Naive : Here, the networks F, C and G and the conditioning layer networks (Hγ and Hβ) are trained as described
in Sec. 3, but instead of modeling the scores using EVT as
described in Sec. 3.2.2, threshold is directly estimated from
the raw reconstruction errors.
IV. Proposed method (pu = 0.5) : F, C, G and condition layer networks (Hγ and Hβ) are trained as described
in Sec. 3 and to ﬁnd the threshold prior probability of observing unknown is set to pu = 0.5.
V. Proposed method: Method proposed in this paper, with
pu set as described in Sec. 4.3.
Results corresponding to the ablation study are shown
in Fig. 4b. Being a simple SoftMax thresholding baseline,
CLS has weakest performance. However, when added with
a match loss function (Lm
r ) as in CLS+DEC, the openset identiﬁcation is performed using reconstruction scores.
Since, it follows a heuristic way of thresholding, the performance degrades rapidly as openness increases. However,
addition of non match loss function (Lnm
), as in the Naive
baseline, helps ﬁnd a threshold value without relying on
heuristics. As seen from the Fig. 4b performance of Naive
baseline remains relatively stable with increase in openness,
showing the importance of loss function Lnm
. Proposed
method with pu ﬁxed to 0.5, introduces EVT modeling on
reconstruction errors to calculate a better operating threshold. It can be seen from the Fig. 4b, such strategy improves
over ﬁnding threshold based on raw score values.
shows importance applying EVT models on reconstruction
errors. Now, if pu is set to 0.5 ∗O, as in the proposed
method, there is a marginal improvement over the ﬁxed pu
baseline. This shows beneﬁt of setting pu as a function of
openness. It is interesting to note that for large openness
values (as 0.5 ∗O →0.5), both ﬁxed pu baseline and proposed method achieve similar performance.
5. Conclusion
We presented an open-set recognition algorithm based
on class conditioned auto-encoders. We introduced training and testing strategy for these networks.
It was also
shown that dividing the open-set recognition into sub tasks
helps learn a better score for open-set identiﬁcation. During
training, enforcing conditional reconstruction constraints
are enforced, which helps learning approximate known
and unknown score distributions of reconstruction errors.
Later, this was used to calculate an operating threshold for
the model.
Since inference for a single sample needs k
feed-forwards, it suffers from increased test time. However, the proposed approach performs well across multiple image classiﬁcation datasets and providing signiﬁcant
improvements over many state of the art open-set algorithms. In our future research, generative models such as
GAN/VAE/FLOW can be explored to modify this method.
We will revise the manuscript with such details in the conclusion.
Acknowledgements
This research is based upon work supported by the Of-
ﬁce of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via
IARPA R&D Contract No. 2014-14071600012. The views
and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing
the ofﬁcial policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government.