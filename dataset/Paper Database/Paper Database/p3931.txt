Methods for Interpreting and Understanding Deep Neural Networks
Gr´egoire Montavona,∗, Wojciech Samekb,∗, Klaus-Robert M¨ullera,c,d,∗
a Department of Electrical Engineering & Computer Science, Technische Universit¨at Berlin, Marchstr. 23, Berlin 10587, Germany
b Department of Video Coding & Analytics, Fraunhofer Heinrich Hertz Institute, Einsteinufer 37, Berlin 10587, Germany
c Department of Brain & Cognitive Engineering, Korea University, Anam-dong 5ga, Seongbuk-gu, Seoul 136-713, South Korea
d Max Planck Institute for Informatics, Stuhlsatzenhausweg, Saarbr¨ucken 66123, Germany
This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its
predictions.
It is based on a tutorial given at ICASSP 2017.
It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most eﬃcient use of these techniques on real
data. It also discusses a number of practical applications.
deep neural networks, activation maximization, sensitivity analysis, Taylor decomposition, layer-wise
relevance propagation
1. Introduction
Machine learning techniques such as deep neural networks have become an indispensable tool for a wide range
of applications such as image classiﬁcation, speech recognition, or natural language processing. These techniques
have achieved extremely high predictive accuracy, in many
cases, on par with human performance.
In practice, it is also essential to verify for a given task,
that the high measured accuracy results from the use of a
proper problem representation, and not from the exploitation of artifacts in the data . Techniques for interpreting and understanding what the model has learned
have therefore become a key ingredient of a robust validation procedure . Interpretability is especially
important in applications such as medicine or self-driving
cars, where the reliance of the model on the correct features must be guaranteed .
It has been a common belief, that simple models provide
higher interpretability than complex ones. Linear models
or basic decision trees still dominate in many applications
for this reason. This belief is however challenged by recent
work, in which carefully designed interpretation techniques
have shed light on some of the most complex and deepest
machine learning models .
Techniques of interpretation are also becoming increasingly popular as a tool for exploration and analysis in the
In combination with deep nonlinear machine
learning models, they have been able to extract new in-
∗Corresponding authors
Email addresses: (Gr´egoire
Montavon), (Wojciech
Samek), (Klaus-Robert
sights from complex physical, chemical, or biological systems .
This tutorial gives an overview of techniques for interpreting complex machine learning models, with a focus on
deep neural networks (DNN). It starts by discussing the
problem of interpreting modeled concepts (e.g. predicted
classes), and then moves to the problem of explaining individual decisions made by the model. The tutorial abstracts
from the exact neural network structure and domain of application, in order to focus on the more conceptual aspects
that underlie the success of these techniques in practical
applications.
2. Preliminaries
Techniques of interpretation have been applied to a wide
range of practical problems, and various meanings have
been attached to terms such as “understanding”, “interpreting”, or “explaining”. See for a discussion. As a
ﬁrst step, it can be useful to clarify the meaning we associate to these words in this tutorial, as well as the type of
techniques that are covered.
We will focus in this tutorial on post-hoc interpretability,
i.e. a trained model is given and our goal is to understand
what the model predicts (e.g. categories) in terms what
is readily interpretable (e.g. the input variables) .
Post-hoc interpretability should be contrasted to incorporating interpretability directly into the structure of the
model, as done, for example, in .
Also, when using the word “understanding”, we refer
to a functional understanding of the model, in contrast
to a lower-level mechanistic or algorithmic understanding
of it. That is, we seek to characterize the model’s blackbox behavior, without however trying to elucidate its inner
workings or shed light on its internal representations.
 
Throughout this tutorial, we will also make a distinction
between interpretation and explanation, by deﬁning these
words as follows.
Deﬁnition 1. An interpretation is the mapping of an abstract concept (e.g. a predicted class) into a domain that
the human can make sense of.
Examples of domains that are interpretable are images (arrays of pixels), or texts (sequences of words). A human can
look at them and read them respectively. Examples of domains that are not interpretable are abstract vector spaces
(e.g. word embeddings ), or domains composed of undocumented input features (e.g. sequences with unknown
words or symbols).
Deﬁnition 2. An explanation is the collection of features
of the interpretable domain, that have contributed for a
given example to produce a decision (e.g. classiﬁcation or
regression).
An explanation can be, for example, a heatmap highlighting which pixels of the input image most strongly support
the classiﬁcation decision . The explanation can
be coarse-grained to highlight e.g. which regions of the
image support the decision. It can also be computed at
a ﬁner grain, e.g. to include pixels and their color components in the explanation. In natural language processing,
explanations can take the form of highlighted text .
3. Interpreting a DNN Model
This section focuses on the problem of interpreting a
concept learned by a deep neural network (DNN). A DNN
is a collection of neurons organized in a sequence of multiple layers, where neurons receive as input the neuron
activations from the previous layer, and perform a simple
computation (e.g. a weighted sum of the input followed
by a nonlinear activation). The neurons of the network
jointly implement a complex nonlinear mapping from the
input to the output. This mapping is learned from the
data by adapting the weights of each neuron using a technique called error backpropagation .
The learned concept that must be interpreted is usually represented by a neuron in the top layer. Top-layer
neurons are abstract (i.e. we cannot look at them), on the
other hand, the input domain of the DNN (e.g. image or
text) is usually interpretable. We describe below how to
build a prototype in the input domain that is interpretable
and representative of the abstract learned concept. Building the prototype can be formulated within the activation
maximization framework.
3.1. Activation Maximization (AM)
Activation maximization is an analysis framework that
searches for an input pattern that produces a maximum
model response for a quantity of interest .
Consider a DNN classiﬁer mapping data points x to a set
of classes (ωc)c. The output neurons encode the modeled
class probabilities p(ωc|x). A prototype x⋆representative
of the class ωc can be found by optimizing:
log p(ωc|x) −λ∥x∥2.
The class probabilities modeled by the DNN are functions
with a gradient . This allows for optimizing the objective by gradient ascent. The rightmost term of the objective is an ℓ2-norm regularizer that implements a preference
for inputs that are close to the origin. When applied to image classiﬁcation, prototypes thus take the form of mostly
gray images, with only a few edge and color patterns at
strategic locations . These prototypes, although producing strong class response, look in many cases unnatural.
3.2. Improving AM with an Expert
In order to focus on more probable regions of the input
space, the ℓ2-norm regularizer can be replaced by a data
density model p(x) called “expert”, leading to the new
optimization problem:
log p(ωc|x) + log p(x).
Here, the prototype is encouraged to simultaneously produce strong class response and to resemble the data. By
application of the Bayes’ rule, the newly deﬁned objective
can be identiﬁed, up to modeling errors and a constant
term, as the class-conditioned data density p(x|ωc). The
learned prototype thus corresponds to the most likely input x for class ωc.
A possible choice for the expert is
the Gaussian RBM . Its probability function can be
written as:
log p(x) = P
2x⊤Σ−1x + cst.
where fj(x) = log(1 + exp(w⊤
j x + bj)) are factors with
parameters learned from the data. When interpreting concepts such as natural images classes, more complex density
models such as convolutional RBM/DBMs , or pixel-
RNNs are needed.
In practice, the choice of the expert p(x) plays an important role. The relation between the expert and the resulting prototype is given qualitatively in Figure 1, where
four cases (a–d) are identiﬁed. On one extreme, the expert
is coarse, or simply absent, in which case, the optimization
problem reduces to the maximization of the class probability function p(ωc|x). On the other extreme, the expert
is overﬁtted on some data distribution, and thus, the optimization problem becomes essentially the maximization of
the expert p(x) itself. When using AM for the purpose of
model validation, an overﬁtted expert (case d) must be especially avoided, as the latter could hide interesting failure
modes of the model p(ωc|x). A slightly underﬁtted expert
(case b), e.g. that simply favors images with natural colors,
underﬁtted
none or l2
and likely
but unlikely
represents
p(x) instead
choice of "expert" p(x)
prototype:
Figure 1: Cartoon illustrating how the expert p(x) aﬀects the prototype x⋆found by AM. The horizontal axis represents the input
space, and the vertical axis represents the probability.
can already be suﬃcient. On the other hand, when using
AM to gain knowledge on a correctly predicted concept
ωc, the focus should be to prevent underﬁtting. Indeed, an
underﬁtted expert would expose optima of p(ωc|x) potentially distant from the data, and therefore, the prototype
x⋆would not be truly representative of ωc.
3.3. Performing AM in Code Space
In certain applications, data density models p(x) can be
hard to learn up to high accuracy, or very complex such
that maximizing them becomes diﬃcult. An alternative
class of unsupervised models are generative models. They
do not provide the density function directly, but are able
to sample from it, usually via the following two steps:
1. Sample from a simple distribution q(z) ∼N(0, I) de-
ﬁned in some abstract code space Z.
2. Apply to the sample a decoding function g : Z →X,
that maps it back to the original input domain.
One such model is the generative adversarial network .
It learns a decoding function g such that the generated
data distribution is as hard as possible to discriminate
from the true data distribution. The decoding function
g is learned in competition with a discriminant between
the generated and the true distributions. The decoding
function and the discriminant are typically chosen to be
multilayer neural networks.
Nguyen et al. proposed to build a prototype for ωc
by incorporating such generative model in the activation
maximization framework. The optimization problem is redeﬁned as:
log p(ωc | g(z)) −λ∥z∥2,
where the ﬁrst term is a composition of the newly introduced decoder and the original classiﬁer, and where the
second term is an ℓ2-norm regularizer in the code space.
Once a solution z⋆to the optimization problem is found,
the prototype for ωc is obtained by decoding the solution,
that is, x⋆= g(z⋆).
In Section 3.1, the ℓ2-norm regularizer in the input space was understood in the context
of image data as favoring gray-looking images. The eﬀect
of the ℓ2-norm regularizer in the code space can instead
be understood as encouraging codes that have high probability. Note however, that high probability codes do not
necessarily map to high density regions of the input space.
To illustrate the qualitative diﬀerences between the
methods of Sections 3.1–3.3, we consider the problem of
interpreting MNIST classes as modeled by a three-layer
DNN. We consider for this task (1) a simple ℓ2-norm regularizer λ∥x −¯x∥2 where ¯x denotes the data mean for
ωc, (2) a Gaussian RBM expert p(x), and (3) a generative model with a two-layer decoding function, and the
ℓ2-norm regularizer λ∥z −¯z∥2 where ¯z denotes the code
mean for ωc. Corresponding architectures and found prototypes are shown in Figure 2. Each prototype is classiﬁed
with full certainty by the DNN. However, only with an expert or a decoding function, the prototypes become sharp
and realistic-looking.
density function
decoding function
AM + expert
AM + decoder
log p(ωc|x)
log p(ωc|x)
log p(ωc|x)
log p(x|ωc)
architecture
found prototypes
Figure 2: Architectures supporting AM procedures and found prototypes.
Black arrows indicate the forward path and red arrows
indicate the reverse path for gradient computation.
3.4. From Global to Local Analysis
When considering complex machine learning problems,
probability functions p(ωc|x) and p(x) might be multimodal or strongly elongated, so that no single prototype
x⋆fully represents the modeled concept ωc. The issue of
multimodality is raised by Nguyen et al. , who demonstrate in the context of image classiﬁcation, the beneﬁt
of interpreting a class ωc using multiple local prototypes
instead of a single global one.
Producing an exhaustive description of the modeled concept ωc is however not always necessary. One might instead focus on a particular region of the input space. For
example, biomedical data is best analyzed conditioned on
a certain development stage of a medical condition, or in
relation to a given subject or organ.
An expedient way of introducing locality into the analysis is to add a localization term η · ∥x −x0∥2 to the AM
objective, where x0 is a reference point. The parameter
η controls the amount of localization. As this parameter
increases, the question “what is a good prototype of ωc?”
becomes however insubstantial, as the prototype x⋆converges to x0 and thus looses its information content.
Instead, when trying to interpret the concept ωc locally,
a more relevant question to ask is “what features of x make
it representative of the concept ωc?”. This question gives
rise to a second type of analysis, that will be the focus of
the rest of this tutorial.
4. Explaining DNN Decisions
In this section, we ask for a given data point x, what
makes it representative of a certain concept ωc encoded in
some output neuron of the deep neural network (DNN).
The output neuron can be described as a function f(x) of
the input. A common approach is to view the data point
x as a collection of features (xi)d
i=1, and to assign to each
of these, a score Ri determining how relevant the feature
xi is for explaining f(x). An example is given in Figure 3.
output f(x)
(evidence for "boat")
explanation R(x)
Figure 3: Explanation of the DNN prediction “boat” for an image x
given as input.
In this example, an image is presented to a DNN and
is classiﬁed as “boat”.
The prediction (encoded in the
output layer) is then mapped back to the input domain.
The explanation takes the form of a heatmap, where pixels
with a high associated relevance score are shown in red.
4.1. Sensitivity Analysis
A ﬁrst approach to identify the most important input
features is sensitivity analysis. It is based on the model’s
locally evaluated gradient or some other local measure of
variation. A common formulation of sensitivity analysis
deﬁnes relevance scores as
where the gradient is evaluated at the data point x. The
most relevant input features are those to which the output
is most sensitive. The technique is easy to implement for a
deep neural network, since the gradient can be computed
using backpropagation .
Sensitivity analysis has been regularly used in scientiﬁc
applications of machine learning such as medical diagnosis
 , ecological modeling , or mutagenicity prediction
 . More recently, it was also used for explaining the classiﬁcation of images by deep neural networks .
It is important to note, however, that sensitivity analysis does not produce an explanation of the function value
f(x) itself, but rather a variation of it. Sensitivity scores
are indeed a decomposition of the local variation of the
function as measured by the gradient square norm:
i=1 Ri(x) = ∥∇f(x)∥2
Intuitively, when applying sensitivity analysis e.g. to a neural network detecting cars in images, we answer the question “what makes this image more/less a car?”, rather
than the more basic question “what makes this image a
4.2. Simple Taylor Decomposition
The Taylor decomposition is a method that explains the model’s decision by decomposing the function
value f(x) as a sum of relevance scores. The relevance
scores are obtained by identiﬁcation of the terms of a ﬁrstorder Taylor expansion of the function at some root point
ex for which f(ex) = 0. This expansion lets us rewrite the
function as:
i=1 Ri(x) + O(xx⊤)
where the relevance scores
Ri(x) = ∂f
x=ex · (xi −exi)
are the ﬁrst-order terms, and where O(xx⊤) contains all
higher-order terms. Because these higher-order terms are
typically non-zero, this analysis only provides a partial
explanation of f(x).
However, a special class of functions, piecewise linear
and satisfying the property f(t x) = t f(x) for t ≥0, is not
subject to this limitation. Examples of such functions used
in machine learning are homogeneous linear models, or
deep ReLU networks (without biases). For these functions,
we can always ﬁnd a root point ex = limε→0 ε · x, that
incidentally lies on the same linear region as the data point
x, and for which the second and higher-order terms are
zero. In that case, the function can be rewritten as
where the relevance scores simplify to
Ri(x) = ∂f
Relevance can here be understood as the product of sensitivity (given by the locally evaluated partial derivative)
and saliency (given by the input value). That is, an input
feature is relevant if it is both present in the data, and if
the model reacts to it.
Later in this tutorial, we will also show how this simple technique serves as a primitive for building the more
sophisticated deep Taylor decomposition .
4.3. Relevance Propagation
An alternative way of decomposing the prediction of
a DNN is to make explicit use of its feed-forward graph
structure. The algorithm starts at the output of the network, and moves in the graph in reverse direction, progressively redistributing the prediction score (or total relevance) until the input is reached. The redistribution process must furthermore satisfy a local relevance conservation principle.
A physical analogy would be that of an electrical circuit where one injects a certain amount of current at the
ﬁrst endpoint, and measures the resulting current at the
other endpoints. In this physical example, Kirchoﬀ’s conservation laws for current apply locally to each node of
the circuit, but also ensure the conservation property at a
global level.
The propagation approach was proposed by Landecker
et al. to explain the predictions of hierarchical networks, and was also introduced by Bach et al. in the
context of convolutional DNNs for explaining the predictions of these state-of-the-art models.
Let us consider a DNN where j and k are indices for neurons at two successive layers. Let (Rk)k be the relevance
scores associated to neurons in the higher layer. We deﬁne
Rj←k as the share of relevance that ﬂows from neuron k
to neuron j. This share is determined based on the contribution of neuron j to Rk, subject to the local relevance
conservation constraint
j Rj←k = Rk.
The relevance of a neuron in the lower layer is then deﬁned
as the total relevance it receives from the higher layer:
These two equations, when combined, ensure between all
consecutive layers a relevance conservation property, which
in turn also leads to a global conservation property from
the neural network output to the input relevance scores:
i=1 Ri = · · · = P
k Rk = · · · = f(x)
It should be noted that there are other explanation techniques that rely on the DNN graph structure, although not
producing a decomposition of f(x). Two examples are the
deconvolution by Zeiler and Fergus , and guided backprop by Springenberg et al. . They also work by applying a backward mapping through the graph, and generate
interpretable patterns in the input domain, that are associated to a certain prediction or a feature map activation.
4.4. Practical Considerations
Explanation techniques that derive from a decomposition principle provide several practical advantages: First,
they give an implicit quantiﬁcation of the share that can
be imputed to individual input features. When the number of input variables is limited, the analysis can therefore
be represented as a pie chart or histogram. If the number
of input variables is too large, the decomposition can be
coarsened by pooling relevance scores over groups of features.
For example, in RGB images, the three relevance scores
of a pixel can be summed to obtain the relevance score
of the whole pixel. The resulting pixel scores can be displayed as a heatmap. On an object recognition task, Lapuschkin et al. further exploited this mechanism by
pooling relevance over two large regions of the image: (1)
the bounding box of the object to detect and (2) the rest
of the image. This coarse analysis was used to quantify
the reliance of the model on the object itself and on its
spatial context.
In addition, when the explanation technique uses propagation in the model’s graph, the quantity being propagated
can be ﬁltered to only include what ﬂows through a certain
neuron or feature map. This allows to capture individual
components of an explanation, that would otherwise be
entangled in the heatmap.
The pooling and ﬁltering capabilities of each explanation technique are shown systematically in Table 1.
sensitivity analysis
simple Taylor
relevance propagation
deconvolution 
guided backprop 
Table 1: Properties of various techniques for explaining DNN decisions. The ﬁrst three entries correspond to the methods introduced
in Sections 4.1–4.3.
5. The LRP Explanation Framework
In this section, we focus on the layer-wise relevance
propagation (LRP) technique introduced by Bach et al.
 for explaining deep neural network predictions. LRP
is based on the propagation approach described in Section
4.3, and has been used in a number of practical applications, in particular, for model validation and analysis of
scientiﬁc data. Some of these applications are discussed in
Sections 8.1 and 8.2.
LRP is ﬁrst described algorithmically in Section 5.1, and
then shown in Section 5.2 to correspond in some cases to
a deep Taylor decomposition of the model’s decision .
Practical recommendations and tricks to make eﬃcient use
of LRP are then given in Section 6.
5.1. Propagation Rules for DNNs
In the original paper , LRP was applied to bag-ofwords and deep neural network models. In this tutorial,
we focus on the second type of models. Let the neurons of
the DNN be described by the equation
j ajwjk + bk
with ak the neuron activation, (aj)j the activations from
the previous layer, and wjk, bk the weight and bias parameters of the neuron. The function σ is a positive and
monotonically increasing activation function.
One propagation rule that fulﬁlls local conservation
properties, and that was shown to work well in practice
is the αβ-rule given by:
where ()+ and ()−denote the positive and negative parts
respectively, and where the parameters α and β are chosen
subject to the constraints α −β = 1 and β ≥0. To avoid
divisions by zero, small stabilizing terms can be introduced
when necessary. The rule can be rewritten as
k = αRk and R∨
k = −βRk. It can now be interpreted as follows:
Relevance R∧
k should be redistributed to the lowerlayer neurons (aj)j in proportion to their excitatory eﬀect on ak. “Counter-relevance” R∨
be redistributed to the lower-layer neurons (aj)j
in proportion to their inhibitory eﬀect on ak.
Diﬀerent combinations of parameters α, β were shown to
modulate the qualitative behavior of the resulting explanation. As a naming convention, we denote, for example, by
LRP-α2β1, the fact of having chosen the parameters α = 2
and β = 1 for this rule. In the context of image classiﬁcation, a non-zero value for β was shown empirically to
have a sparsifying eﬀect on the explanation . On
the BVLC CaﬀeNet , LRP-α2β1 was shown to work
well, while for the deeper GoogleNet , LRP-α1β0 was
found to be more stable.
When choosing LRP-α1β0, the propagation rule reduces
to the simpler rule:
The latter rule was also used by Zhang et al. as part
of an explanation method called excitation backprop.
5.2. LRP and Deep Taylor Decomposition
In this section, we show for deep ReLU networks a connection between LRP-α1β0 and Taylor decomposition. We
show in particular that when neurons are deﬁned as
j ajwjk + bk
the application of LRP-α1β0 at a given layer can be seen
as computing a Taylor decomposition of the relevance at
that layer onto the lower layer. The name “deep Taylor
decomposition” then arises from the iterative application
of Taylor decomposition from the top layer down to the
input layer.
Let us assume that the relevance for the neuron k can be
written as Rk = akck, a product of the neuron activation
ak and a term ck that is constant and positive. These two
properties allow us to construct a “relevance neuron”
with parameters w′
jk = wjkck and b′
k = bkck. The relevance neuron is shown in Figure 4(a).
(a) relevance
(c) relevance
propagation
(b) function's view
Figure 4: Diagram of the relevance neuron and its analysis.
root search domain is shown with a dashed line, and the relevance
propagation resulting from decomposing Rk is shown in red.
We now would like to propagate the relevance to the
lower layer. For this, we perform a Taylor decomposition
of Rk on the lower-layer neurons. We search for the nearest
root point (eaj)j of Rk on the segment [(aj1w′
jk≤0)j, (aj)j].
The search strategy is visualized in Figure 4(b). Because
the relevance neuron is piecewise linear, the Taylor expansion at the root point contains only ﬁrst-order terms:
(eaj)j· (aj −eaj)
The ﬁrst-order terms correspond to the decomposition of
Rk on the lower-layer neurons and have the closed-form
expression
The resulting propagation of Rk is shown in Figure 4(c).
Summing Rj←k over all neurons k to which neuron j contributes yields exactly the LRP-α1β0 propagation rule of
Equation (2).
We now would like to verify that the procedure can be
repeated one layer below. For this, we inspect the structure of Rj and observe that it can be written as a product
Rj = ajcj, where aj is the neuron activation and
j ajwjk + bk
is positive and also approximately constant.
The latter
property arises from the observation that the dependence
of cj on the activation aj is only very indirect (diluted by
two nested sums), and that the other terms wjk, w+
jk, bk, ck
are constant or approximately constant.
The positivity and near-constancy of cj implies that similar relevance neuron to the one of Equation (3) can be
built for neuron j, for the purpose of redistributing relevance on the layer before. The decomposition process can
therefore be repeated in the lower layers, until the ﬁrst
layer of the neural network is reached, thus, performing a
deep Taylor decomposition .
In the derivation above, the segment on which we search
for a root point incidentally guarantees (1) membership
of the root point to the domain of ReLU activations and
(2) positivity of relevance scores. These guarantees can
also be brought to other types of layers (e.g. input layers
receiving real values or pixels intensities), by searching for
a root point (eaj)j on a diﬀerent segment. This leads to
diﬀerent propagation rules, some of which are listed in
Table 2. Details on how to derive these rules are given in
the original paper . We refer to these rules as “deep
Taylor LRP” rules.
Input domain
ReLU activations
Pixel intensities
(xi ∈[li, hi],
li ≤0 ≤hi)
xiwij −liw+
i xiwij −liw+
Real values
Table 2: Deep Taylor LRP rules derived for various layer types. The
ﬁrst rule applies to the hidden layers, and the next two rules apply
to the ﬁrst layer.
5.3. Handling Special Layers
Practical neural networks are often composed of special layers, for example, ℓp-pooling layers (including sumpooling and max-pooling as the two extreme cases), and
normalization layers. The original paper by Bach et al.
 uses a winner-take-all redistribution policy for maxpooling layers, where all relevance goes to the most activated neuron in the pool. Instead, Montavon et al. 
recommend to apply for ℓp-pooling layers the following
propagation rule:
i.e. redistribution is proportional to neuron activations in
the pool. This redistribution rule ensures explanation continuity (see Section 7.1 for an introduction to this concept).
With respect to normalization layers, Bach et al. proposed to ignore them in the relevance propagation pass.
Alternately, Binder et al. proposed for these layers a
more sophisticated rule based on a local Taylor expansion
of the normalization function, with some beneﬁts in terms
of explanation selectivity.
6. Recommendations and Tricks for LRP
Machine learning methods are often described in papers
at an abstract level, for maximum generality. However,
a good choice of hyperparameters is usually necessary to
make them work well on real-world problems, and tricks
are often used to make most eﬃcient use of these methods
and extend their capabilities . Likewise, the LRP
framework introduced in Section 5, also comes with a list
of recommendations and tricks, some of which are given
6.1. How to Choose the Model to Explain
The LRP approach is aimed at general feedforward computational graphs. However, it was most thoroughly studied, both theoretically and empirically , on speciﬁc
types of models such as convolutional neural networks with
ReLU nonlinearities. This leads to our ﬁrst recommendation:
Apply LRP to classes of models where it was successfully applied in the past. In absence of trained
model of such class, consider training your own.
We have also observed empirically that in order for LRP
to produce good explanations, the number of fully connected layers should be kept low, as LRP tends for these
layers to redistribute relevance to too many lower-layer
neurons, and thus, loose selectivity.
As a ﬁrst try, consider a convolutional ReLU network, as deep as needed, but with not too many
fully connected layers. Use dropout in these
For the LRP procedure to best match the deep Taylor decomposition framework outlined in Section 5.2, sumpooling or average-pooling layers should be preferred to
max-pooling layers, and bias parameters of the network
should either be zero or negative.
Prefer sum-pooling to max-pooling, and force biases to be zero or negative at training time.
Negative biases will contribute to further sparsify the
network activations, and therefore, also to better disentangle the relevance at each layer.
6.2. How to Choose the LRP Rules for Explanation
In presence of a deep neural network that follows the
recommendations above, a ﬁrst set of propagation rules to
be tried are the deep Taylor LRP rules of Table 2, which
exhibit a stable behavior, and that are also well understood
theoretically. These rules produce for positive predictions
a positive heatmap, where input variables are deemed relevant if Ri > 0 or irrelevant if Ri = 0.
As a default choice for relevance propagation, use
the deep Taylor LRP rules given in Table 2.
In presence of predictive uncertainty, a certain number of input variables might be in contradiction with the
prediction, and the concept of “negative relevance” must
therefore be introduced.
Negative relevance can be injected into the explanation in a controlled manner by setting the coeﬃcients of the αβ-rule of Equation (1) to an
appropriate value.
If negative relevance is needed, or the heatmaps
are too diﬀuse, replace the rule LRP-α1β0 by
LRP-α2β1 in the hidden layers.
The LRP-α1β0 and LRP-α2β1 rules were shown to work
well on image classiﬁcation , but there is a potentially
much larger set of rules that we can choose from.
example, the “ϵ-rule” was applied successfully to text
categorization . To choose the most appropriate rule
among the set of possible ones, a good approach is to de-
ﬁne a heatmap quality criterion, and select the rule at
each layer accordingly. One such quality criterion called
“pixel-ﬂipping” measures heatmap selectivity and is later
introduced in Section 7.2.
If the heatmaps obtained with LRP-α1β0 and
LRP-α2β1 are unsatisfactory, consider a larger
set of propagation rules, and use pixel-ﬂipping to
select the best one.
6.3. Tricks for Implementing LRP
Let us consider the LRP-α1β0 propagation rule of Equation (2):
where we have for convenience moved the neuron activation aj outside the sum. This rule can be written as four
elementary computations, all of which can also expressed
in vector form:
element-wise
vector form
In the vector form computations, ⊘and ⊙denote the
element-wise division and multiplication.
The variable
W denotes the weight matrix connecting the neurons of
the two consecutive layers, and W+ is the matrix retaining only the positive weights of W and setting remaining
weights to zero. This vector form is useful to implement
LRP for fully connected layers.
In convolution layers, the matrix-vector multiplications
of Equations (4) and (6) can be more eﬃciently implemented by borrowing the forward and backward methods used for forward activation and gradient propagation.
These methods are readily available in many neural network libraries and are typically highly optimized. Based
on these high-level primitives, LRP can implemented by
the following sequence of operations:
def lrp(layer,a,R):
clone = layer.clone()
clone.W = maximum(0,layer.W)
clone.B = 0
z = clone.forward(a)
c = clone.backward(s)
return a * c
The function lrp receives as arguments the layer
through which the relevance should be propagated, the activations “a” at the layer input, and the relevance scores
“R” at the layer output. The function returns the redistributed relevance at the layer input. Sample code is provided at This modular approach was also used by Zhang et al. to implement the excitation backprop method.
6.4. Translation Trick for Denoising Heatmaps
It is sometimes observed that, for classiﬁers that are not
optimally trained or structured, LRP heatmaps have unaesthetic features. This can be caused, for example, by
the presence of noisy ﬁrst-layer ﬁlters, or a large stride
parameter in the ﬁrst convolution layer. These eﬀects can
be mitigated by considering the explanation not of a single
input image but the explanations of multiple slightly translated versions of the image. The heatmaps for these translated versions are then recombined by applying to them
the inverse translation operation and averaging them up.
In mathematical terms, the improved heatmap is given by:
τ −1(R(τ(x)))
where τ, τ −1 denote the translation and its inverse, and T
is the set of all translations of a few pixels.
6.5. Sliding Window Explanations for Large Images
In applications such as medical imaging or scene parsing,
the images to be processed are typically larger than the
what the neural network receives as input. Let X be this
large image. The LRP procedure can be extended for this
scenario by applying a sliding window strategy, where the
neural network is moved through the whole image, and
where heatmaps produced at various locations must then
be combined into a single large heatmap. Technically, we
deﬁne the quantity to explain as:
where X[s] extracts a patch from the image X at location
s, and S is the set of all locations in that image. Pixels then
receive relevance from all patches to which they belong and
in which they contribute to the function value f(x). This
technique is illustrated in Figure 5.
heatmap R(x)
input image X
aggregated heatmap R(X)
Figure 5: Highlighting in a large image pixels that are relevant for
the CIFAR-10 class “horse”, using the sliding window technique.
The convolutional neural network is a special case that
can technically receive an input of any size. A heatmap
can be obtained directly from it by redistributing the toplayer activations using LRP. This direct approach can provide a computational gain compared to the sliding window
approach. However, it is not strictly equivalent and can
produce unreliable heatmaps, e.g. when the network uses
border-padded convolutions. If in doubt, it is preferable
to use the sliding window formulation.
6.6. Visualize Relevant Pattern
Due to their characteristic spatial structure,
heatmaps readily provide intuition on which input pattern the model has used to make its prediction. However,
in presence of cluttered scenes, a better visualization can
be obtained by using the heatmap as a mask to extract
relevant pixels (and colors) from the image. We call the
result of the masking operation the pattern P (x) that we
compute as:
P (x) = x ⊙R(x).
Here, we assume that the heatmap scores have been preliminarily normalized between 0 and 1 through rescaling
and/or clipping so that the masked image remains in the
original color space. This visualization of LRP heatmaps
makes it also more directly comparable to the visualizations techniques proposed in .
7. Quantifying Explanation Quality
In Sections 4 and 5, we have introduced a number of explanation techniques. While each technique is based on its
own intuition or mathematical principle, it is also important to deﬁne at a more abstract level what are the characteristics of a good explanation, and to be able to test for
these characteristics quantitatively. A quantitative framework allows to compare explanation techniques speciﬁcally
for a target problem, e.g. ILSVRC or MIT Places . We
present in Sections 7.1 and 7.2 two important properties
of an explanation, along with possible evaluation metrics.
7.1. Explanation Continuity
A ﬁrst desirable property of an explanation technique is
that it produces a continuous explanation function. Here,
we implicitly assume that the prediction function f(x) is
also continuous. We would like to ensure in particular the
following behavior:
If two data points are nearly equivalent, then the
explanations of their predictions should also be
nearly equivalent.
Explanation continuity (or lack of it) can be quantiﬁed by
looking for the strongest variation of the explanation R(x)
in the input domain:
∥R(x) −R(x′)∥1
When f(x) is a deep ReLU network, both sensitivity
analysis and simple Taylor decomposition have sharp discontinuities in their explanation function. On the other
hand, deep Taylor LRP produces continuous explanations.
This is illustrated in Figure 6 for the simple function
f(x) = max(x1, x2) in R2
+, here implemented by the twolayer ReLU network
f(x) = max
 0 , 0.5 max(0, x1 −x2)
+ 0.5 max(0, x2 −x1)
+ 0.5 max(0, x1 + x2)
It can be observed that despite the continuity of the
prediction function, the explanations oﬀered by sensitivity
sensitivity analysis
simple Taylor
decomposition
relevance propagation
(deep Taylor LRP)
Figure 6: Explaining max(x1, x2). Function values are represented
as a contour plot, with dark regions corresponding to high values.
Relevance scores are represented as a vector ﬁeld, where horizontal
and vertical components are the relevance of respective input variables.
analysis and simple Taylor decomposition are discontinuous on the line x1 = x2. Here, only deep Taylor LRP
produces a smooth transition.
More generally, techniques that rely on the function’s
gradient, such as sensitivity analysis or simple Taylor decomposition, are more exposed to the derivative noise 
that characterizes complex machine learning models. Consequently, these techniques are also unlikely to score well
in terms of explanation continuity.
Figure 7 shows the function value and the relevance
scores for each technique, when applying them to a convolutional DNN trained on MNIST. Although the function
itself is relatively low-varying, strong variations occur in
the explanations. Here again, only deep Taylor LRP produces reasonably continuous explanations.
explanation with
relevance propagation
input sequence
sensitivity analysis
simple Taylor
decomposition
relevance propagation
(deep Taylor LRP)
Figure 7: Classiﬁcation “2” by a DNN, explained by diﬀerent methods, as we move a handwritten digit from left to right in its receptive ﬁeld. Relevance scores are pooled into four quadrants, and are
tracked as we apply the translation operation.
7.2. Explanation Selectivity
Another desirable property of an explanation is that it
redistributes relevance to variables that have the strongest
impact on the function f(x). Bach et al. and Samek
et al. proposed to quantify selectivity by measuring
how fast f(x) goes down when removing features with
highest relevance scores.
The method was introduced for image data under the
name “pixel-ﬂipping” , and was also adapted to text
data, where words selected for removal have their word
embeddings set to zero . The method works as follows:
repeat until all features have been removed:
• record the current function value f(x)
• ﬁnd feature i with highest relevance Ri(x)
• remove that feature (x ←x −{xi})
make a plot with all recorded function values, and
return the area under the curve (AUC) for that plot.
A sharp drop of function’s value, characterized by a low
AUC score indicates that the correct features have been
identiﬁed as relevant. AUC results can be averaged over a
large number of examples in the dataset.
Figure 8 illustrates the procedure on the same DNN as
in Figure 7. At each iteration, a patch of size 4 × 4 corresponding to the region with highest relevance is set to
black. The plot on the right keeps track of the function
score as the features are being progressively removed. In
this particular case, the plot indicates that deep Taylor
LRP is more selective than sensitivity analysis and simple
Taylor decomposition.
It is important to note however, that the result of the
analysis depends to some extent on the feature removal
process. Various feature removal strategies can be used,
but a general rule is that it should keep as much as possible
the image being modiﬁed on the data manifold. Indeed,
average classiﬁcation score
# features removed
(1) compute current heatmap
(2) remove most relevant features
"pixel-ﬂipping" procedure
explanation
techniques
Figure 8: Illustration of the “pixel-ﬂipping” procedure. At each step,
the heatmap is used to determine which region to remove (by setting
it to black), and the classiﬁcation score is recorded.
this guarantees that the DNN continues to work reliably
through the whole feature removal procedure. This in turn
makes the analysis less subject to uncontrolled factors of
variation.
8. Applications
Potential applications of explanation techniques are vast
and include as diverse domains as extraction of domain
knowledge, computer-assisted decisions, data ﬁltering, or
compliance. We focus in this section on two types of applications: validation of a trained model, and analysis of
scientiﬁc data.
8.1. Model Validation
Model validation is usually achieved by measuring the
error on some validation set disjoint from the training data.
While providing a simple way to compare diﬀerent machine learning models in practice, it should be reminded
that the validation error is only a proxy for the true error and that the data distribution and labeling process
might diﬀer. A human inspection of the model rendered
interpretable can be a good complement to the validation
procedure. We present two recent examples showing how
explainability allows to better validate a machine learning
model by pointing out at some unsuspected qualitative
properties of it.
Arras et al. considered a document classiﬁcation task
on the 20-Newsgroup dataset, and compared the explanations of a convolutional neural network (CNN) trained on
word2vec inputs to the explanations of a support vector
machine (SVM) trained on bag-of-words (BoW) document
representations. They observed that, although both models produce a similar test error, the CNN model assigns
most relevance to a small number of keywords, whereas
Based on Arras et al. "What is relevant in a text document? an interpretable ML approach"
SVM/BoW classiﬁer
Based on Lapuschkin et al. "Analyzing classiﬁers: Fisher vectors and deep neural nets"
CNN/word2vec classiﬁer
input image
"horse" classiﬁcation by
Fisher vectors
"horse" classiﬁcation by
Deep neural networks
Examples taken from the literature of model validation
via explanation. (a) Explanation of the concept “sci.space” by two
text classiﬁers. (b) Unexpected use of copyright tags by the Fisher
vector model for predicting the class “horse”.
the SVM classiﬁer relies on word count regularities. Figure 9(a) displays explanations for an example of the target
class sci.space.
Lapuschkin et al. compared the decisions taken
by convolutional DNN transferred from ImageNet, and a
Fisher vector classiﬁer on PASCAL VOC 2012 images. Although both models reach similar classiﬁcation accuracy
on the category “horse”, the authors observed that they
use diﬀerent strategies to classify images of that category.
Explanations for a given image are shown in Figure 9(b).
The deep neural network looks at the contour of the actual
horse, whereas the Fisher vector model (of more rudimentary structure and trained with less data) relies mostly
on a copyright tag, that happens to be present on many
horse images. Removing the copyright tag in the test images would consequently signiﬁcantly decrease the measured accuracy of the Fisher vector model but leave the
deep neural network predictions unaﬀected.
8.2. Analysis of Scientiﬁc Data
Beyond model validation, techniques of explanation can
also be applied to shed light on scientiﬁc problems where
human intuition and domain knowledge is often limited.
Simple statistical tests and linear models have proved useful to identify correlations between diﬀerent variables of
a system, however, the measured correlations typically remain weak due to the inability of these models to capture
the underlying complexity and nonlinearity of the studied problem. For a long time, the computational scientist
would face a tradeoﬀbetween interpretability and predictive power, where linear models would sometimes be preferred to nonlinear models despite their lower predictive
power. We give below a selection of recent works in various ﬁelds of research, that combine deep neural networks
and explanation techniques to extract insight on the studied scientiﬁc problems.
In the domain of atomistic simulations, powerful machine learning models have been produced to link molecular structure to electronic properties . These
models have been trained in a data-driven manner, without simulated physics involved into the prediction. In particular, Sch¨utt et al. proposed a deep tensor neural
network model that incorporates suﬃcient structure and
representational power to simultaneously achieve high predictive power and explainability. Using a test-charge perturbation analysis (a variant of sensitivity analysis where
one measures the eﬀect on the neural network output of
inserting a charge at a given location), three-dimensional
response maps were produced that highlight for each individual molecule spatial structures that were the most
relevant for explaining the modeled structure-property relationship. Example of response maps are given in Figure 10(a) for various molecules.
Sturm et al. showed that explanation techniques can
also be applied to EEG brain recording data. Because the
input EEG pattern can take diﬀerent forms "Quantumchemical insights from deep tensor neural
Based on Sturm et al. "Interpretable
deep neural networks for single-trial EEG
classiﬁcation"
"right hand"
imagined movement of
portrait explanation
explanation
for gender
Based on Arbabzadah et al. 
"Identifying individual facial expressions
by deconstructing a neural network"
Adapted from Vidovic et al. "Feature importance measure for non-linear learning algorithms"
sequence 1 (true positive)
sequence 2 (false positive)
sequence 3 (false negative)
explanation
Figure 10: Overview of several applications of machine learning explanation techniques in the sciences. (a) Molecular response maps
for quantum chemistry, (b) EEG heatmaps for neuroimaging, (c) extracting relevant information from gene sequences, (d) analysis of
facial appearance.
device), it is important to produce an individual explanation that adapts to these parameters. After training a
neural network to map EEG patterns to a set of movements imagined by the user (“right hand” and “foot”), a
LRP decomposition of that prediction could be achieved
in the EEG input domain (a spatiotemporal signal capturing the electrode measurements at various positions on
the skull and at multiple time steps), and pooled temporally to produce EEG heatmaps revealing from which
part of the brain the decision for “right hand” or “foot”
originates. An interesting property of decomposition techniques in this context is that temporally pooling preserves
the total function value, and thus, still corresponds to a
decomposition of the prediction. Example of these individual EEG brain maps are given in Figure 10(b).
classical linear explanation of neural activation patterns
in cognitive brain science experiments or Brain Computer
Interfacing, see .
Deep neural networks have also been proposed to make
sense of the human genome. Alipanahi et al. trained
a convolutional neural network to map the DNA sequence
to protein binding sites. In a second step, they asked what
are the nucleotides of that sequence that are the most relevant for explaining the presence of these binding sites.
For this, they used a perturbation-based analysis, similar
to the sensitivity analysis described in Section 4.1, where
the relevance score of each nucleotide is measured based
on the eﬀect of mutating it on the neural network prediction. Other measures of feature importance for individual
gene sequences have been proposed that apply to a
broad class of nonlinear models, from deep networks to
weighted degree kernel classiﬁers. Examples of heatmaps
representing relevant genes for various sequences and prediction outcomes are shown in Figure 10(c).
Explanation techniques also have a potential application
in the analysis of face images. These images may reveal
a wide range of information about the person’s identity,
emotional state, or health. However, interpreting them directly in terms of actual features of the input image can
be diﬃcult.
Arbabzadah et al. applied a LRP technique to identify which pixels in a given image are responsible for explaining, for example, the age and gender
attributes. Example of pixel-wise explanations are shown
in Figure 10(d).
9. Conclusion
Building transparent machine learning systems is a convergent approach to both extracting novel domain knowledge and performing model validation. As machine learning is increasingly used in real-world decision processes,
the necessity for transparent machine learning will continue to grow. Examples that illustrate the limitations of
black-box methods were mentioned in Section 8.1.
This tutorial has covered two key directions for improving machine learning transparency: interpreting the concepts learned by a model by building prototypes, and explaining of the model’s decisions by identifying the relevant input variables. The discussion mainly abstracted
from the exact choice of deep neural network, training procedure, or application domain. Instead, we have focused
on the more conceptual developments, and connected them
to recent practical successes reported in the literature.
In particular we have discussed the eﬀect of linking prototypes to the data, via a data density function or a generative model. We have described the crucial diﬀerence
between sensitivity analysis and decomposition in terms
of what these analyses seek to explain. Finally, we have
outlined the beneﬁt in terms of robustness, of treating the
explanation problem with graph propagation techniques
rather than with standard analysis techniques.
This tutorial has focused on post-hoc interpretability,
where we do not have full control over the model’s structure. Instead, the techniques of interpretation should apply to a general class of nonlinear machine learning models,
no matter how they were trained and who trained them –
even fully trained models that are available for download
like BVLC CaﬀeNet or GoogleNet 
In that sense the presented novel technological development in ML allowing for interpretability is an orthogonal strand of research independent of new developments
for improving neural network models and their learning
algorithms. We would like to stress that all new developments can in this sense always proﬁt in addition from
interpretability.
Acknowledgments
We gratefully acknowledge discussions and comments on
the manuscript by our colleagues Sebastian Lapuschkin,
and Alexander Binder.
This work was supported by
the Brain Korea 21 Plus Program through the National Research Foundation of Korea; the Institute for
Information & Communications Technology Promotion
(IITP) grant funded by the Korea government [No. 2017-
0-00451]; the Deutsche Forschungsgemeinschaft (DFG)
[grant MU 987/17-1]; and the German Ministry for Education and Research as Berlin Big Data Center (BBDC)
[01IS14013A]. This publication only reﬂects the authors
views. Funding agencies are not liable for any use that
may be made of the information contained herein.