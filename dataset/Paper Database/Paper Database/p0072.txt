IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
A Formal Framework for Positive and Negative
Detection Schemes
Fernando Esponda, Stephanie Forrest, and Paul Helman
Abstract—In anomaly detection, the normal behavior of a
process is characterized by a model, and deviations from the
model are called anomalies. In behavior-based approaches to
anomaly detection, the model of normal behavior is constructed
from an observed sample of normally occurring patterns. Models
of normal behavior can represent either the set of allowed
patterns (positive detection) or the set of anomalous patterns
(negative detection). A formal framework is given for analyzing
the tradeoffs between positive and negative detection schemes in
terms of the number of detectors needed to maximize coverage.
For realistically sized problems, the universe of possible patterns
is too large to represent exactly (in either the positive or negative
scheme). Partial matching rules generalize the set of allowable
(or unallowable) patterns, and the choice of matching rule affects
the tradeoff between positive and negative detection. A new
match rule is introduced, called
-chunks, and the generalizations
induced by different partial matching rules are characterized in
terms of the crossover closure. Permutations of the representation
can be used to achieve more precise discrimination between
normal and anomalous patterns. Quantitative results are given for
the recognition ability of contiguous-bits matching together with
permutations.
Index Terms—Anamoly detection, artificial immune systems, intrusion detection, negative detection.
NOMENCLATURE
Length of a string, a pattern or a packet
(typically given in bits).
Size of the sliding window.
Window, a specification of
symbol positions in a string. When the
window length
is understood, it suffices
to specify only a start position.
Number of windows in a string of length .
All possible strings of length
for a given
used to denote a sample of
“normal” strings.
used to denote the complete set
of “normal” strings.
String or packet generated at time .
Legitimate packet-generating process.
Manuscript received July 18, 2002; revised March 24, 2003. This work was
supported by the National Science Foundation under Grants ANIR-9986555
and DBI-0309147, the Office of Naval Research under Grant N00014-99-1-
0417, the Defense Advanced Projects Agency under Grant AGR F30602-00-2-
0584, the Intel Corporation, the Santa Fe Institute, and the Consejo Nacional
de Ciencia y Tecnología (México) under Grant 116691/131686. This paper was
recommended by Asoociate Editor D. Cook.
The authors are with the Computer Science Department, University of New
Mexico, Albuquerque, NM 87131 USA (e-mail: ).
Digital Object Identifier 10.1109/TSMCB.2003.817026
Malicious packet-generating process.
Random variable indicating whether
Hidden process governing the generation
Dynamic sample of
at instant .
String detector.
Set of detectors.
Crossover closure generation rule of
a given window size .
projected onto
under match rule
I. INTRODUCTION
N anomaly detection, the normal behavior of a process is
characterized by a model, and deviations from the model are
called anomalies. In behavior-based approaches to anomaly detection, the model of normal behavior is constructed from an observed sample of normally occurring patterns. Models of normal
behavior can represent either the set of allowed instances (positive detection) or the set of all anomalous instances (negative
detection).
For most real problems, the set of positive instances is much
smaller than the set of complementary ones. Thus, it would seem
wise to derive a representation of the smaller set, rather than
its enormous complement. However, there are several reasons
why negative detection merits further consideration. First, it has
been used successfully both in engineering applications and by
naturally occurring biological systems. Second, if we assume a
closed world, then from an information-theory perspective, the
normal and abnormal sets both contain the same amount of information, which suggests that there might be equally compact
representations of the negative patterns . A third point about
negative detection is that it allows the detection process to be
distributed across multiple locations with virtually no communication required among the distributed components. This property allows several forms of distributed processing: checking
small sections of a large object independently, independent detector sets (e.g., each one running on a separate machine), or
1083-4419/04$20.00 © 2004 IEEE
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
Self nonself discrimination. A universe of data points is partitioned into
two sets: self and nonself. Negative detectors cover subsets of nonself.
evaluating each individual detector in a detector set independently. As the scale of anomaly-detection problems increases,
the need for distributed processing is likely to grow. Indeed, the
original inspiration for the negative-detection approach came
from the natural immune system, which uses negative detection
in a massively distributed environment—the body.
The negative-detection approach to anomaly detection, which
was originally published as the negative-selection algorithm ,
 , was modeled after a method used by the natural immune
system to prevent autoimmunity . In the immune system,
certain cells known as T cells undergo a multistage maturation
process in an isolated environment: an organ called the thymus.
While in the thymus, T cells are censored against the normally
occurring peptide patterns of the body called self. T cells that
react with self are deleted in the thymus before they can become active and cause autoimmunity. The only T cells allowed
to mature and leave the thymus are those that survive this censoring operation.1 Such cells then circulate through the body
freely and independently, eliminating any material that they can
bind. Because of the censoring process, such material is implicitly assumed to be foreign and is known as nonself.
The translation of this mechanism into an algorithm for computers is straightforward. First, we assume that the anomaly-detection problem is posed as a set
(real-self) of strings , all
of fixed-length , of which we can access only a sample
any given time. The universe of all -length strings is referred
, and the set of anomalous patterns to be detected is the
. We consider all strings to be binary, although the
analysis is generalizable to a larger alphabet. Candidate detectors are generated randomly and are censored against
that fail to match any of the strings in
are retained as active
detectors. Such detectors are known as negative detectors, and
is a good sample of
, each negative detector will cover
(match) a subset of nonself. The idea is that by generating sufficient numbers of independent detectors, good coverage of the
nonself set will be obtained. Fig. 1 shows the relationship of
these sets pictorially.
This algorithm learns to distinguish a set of normally occurring patterns (self) from its complement (nonself) when only
positive instances of the class are available. In the machine
learning literature, this has been studied as the problem of
1Mature T-cells have survived at least two other censoring operations: one
involving genetic rearrangements and one involving positive selection.
learning from examples of a single class or as concept learning
from positive examples , . Although in principle this
learning problem is solvable for noise-free classes and certain
formal domains, in many practical cases, the problem is known
to be computationally intractable or to lead to substantial
overgeneralization . The statistical community has examined the closely related problems of outlier detection and
robust statistics, finding that the effectiveness of the learning
system depends critically on domain-dependent assumptions
about the distributions of self and nonself data. For example,
in a computer security context, Lane employed time-series
models of user behaviors and a prior bias against false alarms
to differentiate intruders from authorized system users .
There are many well known change-detection and check-sum
algorithms that solve a restricted form of the anomaly-detection
problem, known as change detection. Here, self is known exactly, is small enough to be stored in a single location, remains
constant over time, and can be unambiguously distinguished
from nonself. However, for cases in which these assumptions
do not hold, the discrimination task is more challenging, and in
these situations, the negative-detection approach may be appropriate.
Since its introduction in , interest in negative detection
has been growing, especially for applications in which noticing
anomalous patterns is important, including computer virus detection , , intrusion detection , , , and industrial milling operations . Recently, other categories of applications have been proposed, including color image classification and collaborative filtering and evolutionary design
 . Underlying all this work, however, is the question of negative versus positive detection: When is it advantageous to use a
negative-detection scheme instead of the more straightforward
positive-detection scheme?
To date, there has been little theoretical analysis of the relative
merits of positive versus negative detection, and that is the primary goal of this paper. Such analysis is complicated by several
factors, which we also address. Most important is the choice of
representation and match rule. How is the set of normal patterns
represented? How are anomaly detectors represented? What are
the criteria for determining when a detector has detected a pattern? The analysis is further complicated if the self set is too
large or too distributed to observe completely, if it is nonstationary (self sets that change over time), or if the detectors are
changing over time.
In this paper, a formal framework is given for analyzing the
tradeoffs between positive and negative detection schemes in
terms of the number of detectors needed for maximum coverage.
Earlier analyses focused on the cost of generating detectors (see
Section II), rather than on the coverage provided by a complete
set of detectors. We then discuss the need for partial matching
rules and generalization, as well as what this implies for comparing positive and negative detection. We review the
-contiguous bits (rcb) matching rule, which is the most common
match rule used with negative detection. We introduce a related
match rule, called
-chunks, discuss its advantages, and characterize the generalizations that are induced by it in terms of
a concept known as the crossover closure (see Fig. 3), giving
the expected size of the crossover closure for -chunks. We are
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
then in a position to compare quantitatively the positive and negative detection schemes under -chunks. Finally, we consider a
method for increasing the diversity of coverage, known as permutation masks , , show that permutation masks reduce
the number of nondetectable strings in the nonself set for any
fixed detection threshold, and give quantitative results on how
many permutations are required to maximize this effect.
One consequence of using an approximate scheme for representing a class of explicit instances (e.g., the self set) is that
the class will implicitly contain some instances that were not
in the original sample. If the approximation is a good one, then
this is an advantage, and the approximation is said to generalize
from the set of observed patterns to a useful class. In the case of
anomaly detection, new observations that are similar to the original sample (self) are classified as part of self, and false positives
are reduced. If the generalization is a poor one, new observations will frequently be misclassified, leading to high false-positives and/or false-negatives. In this paper, partial matching rules
specify the approximation, and the
parameter controls how
large the approximation is. We are interested in characterizing
as precisely as possible the kinds of generalizations that are induced by our partial match rules. When we discuss the entire set
covered by the approximation, we refer to it as the generalization. When we discuss individual strings in the generalization
that were not in the original sample, we refer to them as holes
 : strings not in
for which valid fixed- detectors cannot be
generated.
II. RELATED WORK
Outlier detection is a problem arising in many machine
learning and data mining contexts. Abstractly, two families of
approaches can be identified. Outlier detection with respect to
probability distributions attempts to identify those events that
are in the low-density regions of the probability distribution
governing the generation of normal events , , ,
 . Under this approach, which we refer to as statistical
anomaly detection, the degree of suspicion attached to an event
is inversely proportional to the frequency with which the event
has been observed historically.
A second family of approaches to outlier detection attempts
to define a measure of distance on the event space , ,
 , , , . Common distance measures include the
-norm and Hamming distances, Manhattan distance
(also known as rectilinear, which generalizes Hamming distance
to nonbinary coordinate systems), and the vector cosine measure
of Salton . Under this approach, the degree of suspicion attached to an event is directly proportional to the distance of the
event from, for example, the nearest observed normal event or
the center of mass of clusters of normal events.
The main difficulty in applying statistical anomaly detection
is that the probability distribution is not known exactly and must
be estimated from a sample. Often, this sample is sparse, making
density estimation highly error prone. In many applications, the
majority of events we must judge do not appear in the sample at
all and, hence, must be classified as “most suspect.” The main
difficulty in applying distance criteria is in the construction of a
measure that reflects a useful metric of similarity. A poor choice
of measures results in meaningless classifications.
Often, either implicitly or explicitly, the approaches are combined , . For example, data reduction transformations,
such as feature selection and value aggregation, collapse distinct events into clusters of events that are to be treated as if
they were indistinguishable , . An event is assigned the
composite density of the cluster into which it falls. In this way,
many events are clustered together, all of which are classified as
either suspect or not based on the cluster’s composite density.
The most suspect events will cluster with no previously encountered events (forming a low density cluster of its own).
Statistical anomaly detection is often used on the problem
of intrusion detection in computer security, for example, ,
 , , – , . Intrusion-detection systems vary
widely, but they all seek to protect an information system
(e.g., a single computer, a database, a server, or a network of
computers) from violations of the system’s security policy. In
the case of anomaly intrusion-detection systems, protection is
provided by building a model of the normal (legal) operation
of the system and treating discrepancies from the model as
anomalies. The model of normal behavior can be based on
any observable behavior of the system. Audit logs, patterns
of network traffic, user commands, and system calls are all
common choices. Such an approach can work well if the
anomalies in normal operation are well-correlated with security
violations. The extent to which this is true is a topic of current
debate in the intrusion-detection community.
In , the negative-selection algorithm was introduced as a
method for representing and generating distributed sets of detectors to perform change detection. The negative-selection algorithm was demonstrated and analyzed in the context of the
rcb match rule , , which is a partial-matching rule between two strings similar to Hamming distance. In that paper,
a probabilistic expected-case analysis of the algorithm’s performance was given for the case where the protected data and the
detectors are assumed to be static, and the protected data are
randomly generated. The analysis relied on the assumption that
independently generated detectors would have independent detection capabilities. Use of the algorithm was illustrated with
the problem of computer virus detection, and it was noted that
observed performance on real data sets was often significantly
better than that predicted by the expected-case analysis. This
improved performance was ascribed to regularities in the protected data (self).
References and reported a linear time algorithm for
generating detectors (linear in the number of protected strings)
space, where
is a threshold value and
is the length of the strings. This new algorithm addressed an
important limitation as the cost of generating detectors using the
original algorithm increased exponentially with the size of the
protected data. Limits to the precision of coverage that is possible under most plausible matching rules were also identified.
These limits arise because partial matching rules obscure the
boundary between the self and nonself sets. In , the number of
such gaps, called holes, for certain partial matching rules were
counted, and a lower bound on the size of the detector set needed
to achieve a given probability of detecting abnormal strings was
estimated. Following up on this work, Wierzchon developed
a low space-complexity algorithm for generating an optimal
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
repertoire of detector strings – and included an analysis
of the holes in coverage associated with his algorithm. In this
work, it was assumed that both the protected data and the detectors are unchanging through time. Likewise, both projects focused on the problem of generating detectors efficiently, rather
than the problem of how many detectors are needed for maximal performance. The latter question is the one addressed in
this paper.
Lamont and Harmer applied negative selection to the problem
of computer virus detection, focusing on different match rules
 . They developed a prototype implementation to demonstrate that the approach is both scalable and effective. As part
of their work, they evaluated 12 different match rules including
Hamming distance and rcb, concluding that the Rogers and Tanimoto rule (an extension of Hamming distance) was the best fit
for their application because it did the best job of balancing the
generality and specificity of each individual detector.
Hofmeyr , , described a network intrusion-detection system that incorporated a modified form of the negativeselection algorithm, together with several other mechanisms, including the use of permutation maps to achieve diversity of representation. His experiments were the first in this line of work
to consider dynamically changing data sets (e.g., when the definition of normal behavior is either incomplete or nonstationary)
and a distributed environment. The setting was network intrusion detection on a local area network, and the detectors monitored the flow of TCP SYN packets through the network. The
original negative-selection algorithm was modified to accommodate dynamic data sets; detectors were generated asynchronously and allowed to undergo negative selection in the live environment of the network. Thus, the intrusion-detection system
featured rolling sets of detectors in the sense that detectors generated at different times were potentially exposed to slightly differing patterns of self.
Kim and Bentley also implemented a network intrusion-detection system which incorporates negative selection . Their
implementation used a significantly more complicated representation than with a much larger alphabet, shorter strings, and
more general matching thresholds (lower
in -contiguous bits
matching). Like Hofmeyr, they used the original random-generation method of producing detectors. They reported difficulties
generating valid detectors and concluded that negative selection
is infeasible on its own for realistic network intrusion detection. Balthrop et al. propose several explanations for their
results, based largely on an analysis of the parameter settings
that were selected. Williams et al. report positive results
when using negative selection on the network intrusion detection problem. Their results and approach more closely parallel
that taken in Hofmeyr’s work. Their implementation goes beyond and , however, in that it monitors UDP and ICMP
packets as well as TCP packets.
Dasgupta and Gonzalez used negative detection on the network intrusion-detection problem . They worked with the
Lincoln Labs data set , used a real-valued representation,
and divided the Lincoln Labs data into overlapping intervals.
The paper is interesting because it compares the performance
of positive and negative characterization of self, reporting that
the positive approach is more accurate but also computationally more expensive. It is not obvious how meaningful the comparison is, however, because the positive and negative methods
are quite different. The positive-detection method simply memorizes a set of training (attack-free) points and classifies any test
point as “normal” if it is within a given distance of any memorized self-point. The negative-detection method uses a genetic
algorithm to evolve complex rules for detecting nonself. Thus,
the study could more properly be viewed as comparing evolved
rules with a simple memory-based approach.
In addition to intrusion detection, several authors have been
interested in using negative detection for other applications.
Sathyanath and Sahin used negative detection for color image
classification of finished wooden components: specifically,
kitchen cabinets . Bradley and Tyrrell describe a hardware
implementation using field programmable gate arrays (FPGAs)
 – . Their detectors monitor transitions, together with
some input/output information, in finite-state machines to
detect faults. The implementation uses negative detection and
rcb match rules. Detectors are generated offline, and the entire
self set is known at the time detectors are being generated.
Chao and Forrest describe a negative-detection approach
to interactive search algorithms, in which subjective evaluations
drive the exploration of large parameter spaces. Their algorithm
learns what parts of the search space are not useful, based on
the negative-detection strategy of the natural immune system.
The algorithm is capable of finding consensus solutions for
parties with different selection criteria.
Sometimes, the set of anomalous patterns is known, or
thought to be known, exactly, and a set of signatures is used to
cover the dangerous parts of nonself precisely. Examples of this
approach are signature-based scanners for intrusion detection
 and commercial virus-scanners.
III. STATISTICAL MODELING FRAMEWORK
In this section, we cast the anomaly intrusion-detection
problem in a statistical framework. Although the unrestricted
version of the framework is quite complex, it is nevertheless
useful to begin with the overall view. This helps us make
rigorous certain notions which are required later in the paper,
and it allows future work to evolve toward meeting broader
objectives.
Our basic unit of analysis is a length
binary string, called
a packet. At each time step , a packet
is generated. We
assume that
is generated either by the legitimate process
or the malicious process
. In reality, there could be any number
of subprocesses contributing to each of these processes (e.g.,
many legitimate users with different behaviors), but we proceed
as if there were a single
and a single
More precisely, a sequence
binary strings (packets) is generated. A hidden process
governs the generation of the packet
at time . Random variable
if process
. In general, it may be the case that the
distribution of random variable
depends on the sequences
variables, as well as on the identity of the particular time step
. In the remainder of this paper, however, we assume that the
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
distribution of
is independent of and of
, which are also
referred to as the prior probabilities of the processes
, respectively, are unchanging over time.
There are several (typically unknown) next packet distributions of potential interest; estimating these distributions, under
various assumptions to be introduced below, is a major focus of
this paper. The pair of distributions
specify next packet distributions under
, respectively,
conditioned on knowledge of the first
generated, plus a specification (the values of
) of which process
generated each of these
. Estimating these distributions
from the data amounts to a supervised learning problem since
label the training data
, indicating which process generated each packet. Note that these
probabilities are related via Bayes theorem to the posterior distribution on which process
generated the current packet
. See Section III-B for details. Note also that the mixture
distribution for the next packet generated is
In the applications studied in this paper, we have no direct
knowledge of the values of the random variables
, that is,
we do not know with certainty which of the processes
generated the previous packets in the sequence, leaving us with
an unsupervised learning problem. In particular, we will be concerned with estimating the pair of distributions
when we have knowledge of the process priors
(that is, knowledge of the distribution of the hidden process
or with estimating the pair of distributions
when we do not have knowledge of the process priors.
For some problems, the ultimate objective is to identify short
temporal groupings of anomalous packets or the time intervals
most likely to contain packets generated by
. Many intrusiondetection problems have this quality, and the above formulation
supports such an analysis. However, there are important special
cases in which the analysis shifts from sequences of packets to
single packets, and these are the focus here.
The next packet distribution of each process may depend on
the current time step , as well as on the pattern of packets generated at time steps prior to . This allows the possibility that each
is nonstationary, where its distribution depends both
on the actual time step
and the identity of packets previously
generated by both processes. This nonstationarity might reflect,
for example, that we have the following.
• Composite user behavior changes over time (different
populations of users).
• A user’s behavior at time
depends on behaviors at times
(a single user’s action depends on his or her prior
• The nature of attacks changes over time.
• There are changes in the environment (e.g., a new computer added to a network).
We abstract away the question of whether and how the true
distributions of
change over time by specifying our
posterior of the next packet
generated at any instant
time, given some dynamic sample
at instant . That is, in
this paper, we focus on situations in which it suffices to replace
where sample
is an (unordered) set of packets. sample
may include a subset of the packets
restricted
to packets deemed to have been generated by
(i.e., packets
for which it is deemed likely that
) and perhaps
further restricted by other criteria (e.g., recency, random sampling). sample
may also be seeded with an initial population
of packets.
reflects our current state of knowledge of
, and the next packet generated by
is dependent only on this
knowledge—given sample
’s distribution is conditionally independent of all other information (e.g., of other previously generated packets). As sample
changes with time (see
below), the next packet distribution changes. Whether or not
actually is changing over time (or whether, for example, our state of knowledge simply is changing) is immaterial to the model. Further, because sample
is unordered and
assumed to be generated by or otherwise representative of
the detection question of interest is to decide whether the next
is generated by
; our model does not address
the problem of deciding whether a subsequence embedded in
is generated by
In the remainder of this paper, we consider a simple family of
distributions
as our modeling
generates its next packet. Intuitively, we assume a distance measure between an arbitrary packet
and the current
. The closer a packet
is to sample
, the greater
the probability
. Many notions
of distance are possible. For example, the distance of a packet
from sample
’s minimum Hamming distance
to a member of sample
. Note that this measure does not distinguish between members of sample
(assigning each such
string a distance of 0) but does differentiate between packets
not in sample
. Alternatively, distance could be based on frequency counts; the “distance” of
from sample
could be the
reciprocal of the number of instances of
1/0 defined to be some large constant
. This distance measure
fails to distinguish between packets not in sample
 but does differentiate among
packets in sample
. Of course, one could develop a hybrid of
these two measures in which a distance based on Hamming distance is used for strings outside of sample
, and a distance
based on frequency counts is used for strings in sample
We adopt a simple categorical division into “similar to
” versus “dissimilar from sample
” and distinguish these distance categories by means of a generation rule
that attempts to characterize the underlying set from which
is likely drawn. Unlike the two distance measures
mentioned earlier, a generation rule allows only the distinction
between likely and unlikely strings under
, which simplifies
the detection task considerably. Experimental studies have
shown that these simplifications often capture sufficient detail
of process behavior to provide effective detection , .
Definition: A generation rule
is a mapping from a set
strings to a set
strings containing
This is the generalization discussed in Section I.
Given a generation rule
, we wish to specify a distribution
to reflect that each member of
is more likely to
be generated at time
than is each nonmember. Several
simple alternatives exist, though care must be taken to ensure
a proper probability distribution results. For example, for some
applications, it may be appropriate to have the probability of the
be some large multiplicative factor
probability of the complement of this set. We can construct a
probability distribution capturing this relative frequency by first
then (noting
normalizing to obtain proper point probabilities for individual
strings, that is
Assuming there are more strings in the complement of
than in the set itself, the point probability of each
member string would be greater than that of each nonmember
string. Alternatively, in other contexts (e.g., when
is larger than its complement), it would be necessary to specify
directly that the common point probability of each member
is some large multiple of the common point
probability of each nonmember, and then again normalize
to proper probabilities by taking into account the number of
members and nonmembers.
Whatever method is used to set the distribution, we assume
is specified by
As a final comment, we note that under this modeling,
although the generation rule
and measure of distance (e.g.,
member or not of the set
) are assumed fixed and known
exactly (as opposed to being dependent on the data), the
probabilities assigned to
do change as the sample
changes with time. In particular, since sample
typically is
dependent on previously observed packets—evolving with our
knowledge of the true image of process
—our distribution of
the next packet generated changes as the data changes.
A. Example Generating Rules
We now present three sample generation rules that can be
used to characterize the underlying set from which sample
is drawn. The first is Hamming radius, which generalizes
to strings within a given Hamming distance of
strings in the sample. Next, we consider crossover closure,
which generalizes sample
to strings that can be constructed
by pasting together the windows of strings appearing in
. Finally, we examine
-gram matching, which
generalizes sample
to strings whose
-grams have been
observed in the sample
1) Example: Hamming Radius: The Hamming distance
measure discussed above can be simplified to a categorical
measure based on a threshold
. In particular, the generation
is defined to be the set of strings within a
Hamming distance of
or less to some member of
of this generation rule in detection is considered in Section VI.
Hamming distance match rules have been used with negative
detection in the context of immunological modeling, e.g., .
2) Example: Crossover Closure: The generation rule we
consider in the most detail is called “crossover closure.” As
we will show in subsequent sections, the crossover closure is
closely related to the two match rules most commonly used in
conjunction with negative detection.
Given a set
of strings and a fixed
, the crossover
is defined in terms of its length
consecutive string positions) as
In words, string
is in the crossover closure of
only if each of ’s windows exactly matches the corresponding
window of some member of
is such that
, we say that
is closed under crossover closure. As we will
see, the class of sets closed under crossover closure has a central
role in our methods.
One interesting motivation for crossover closure comes from
relational database theory , . The strings of a sample
can be viewed as the tuples of the current instance of
a relation scheme
, where each attribute
corresponds to packet position and has domain {0,1}. For example,
the sample
can be represented as a relation
, whose current instance is shown as
For a variety of reasons (e.g., to reduce redundancy
and enhance data integrity), it is often advantageous to
represent a relation scheme as a decomposition into a
collection of smaller relation schemes.
Consider representing the scheme
as a decomposition
. The instance
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
is the projection of
or, equivalently, is
exactly the set of strings comprising the
. In the previous example, taking
, the instances
are as follows:
In order to reconstruct the original instance of
from these
projections onto the
, one computes the natural join of the
However, it is not always the case that the join of the projection
recovers the original instance of
; in fact, the join of the projected instances is precisely the crossover closure of the set of
tuples (strings) in the original instance of
. In our example, the
of the instances shown above results in the
following instance of
, which can be seen to be the crossover
closure of the strings in the original instance of
Relational database theory has exactly characterized when the
projections of
join to recover the original instance (see Section VII-B and and ). This is known as the lossless join
condition. The lossless join condition thus also characterizes exactly when a set of packets is equal to its crossover closure. One
interpretation of this correspondence is that since many naturally occurring collections of information do in fact satisfy the
lossless join condition, it is plausible that in many contexts, the
most likely set of strings to be generated by
is closed under
crossover closure. When this is the case, sample
, if it is a
representative sample of strings generated by
, can be interpreted as being a sample drawn from
a larger set containing
and itself closed under
crossover. In such situations, it is appropriate to deem packets
that are members of
as relatively likely under
3) Example:
-Grams: The
-gram matching rule has been
used in a wide variety of settings including natural language processing , document classification , and program monitoring , , , for intrusion detection. In the latter application,
-grams can be applied when the behavior of the protected process (for example, an executing computer program)
can be represented as a sequence of letters taken from some
alphabet. A sample of normal behavior
is a collection of
such sequences, called a “trace” collected under normal operating conditions. Different traces can be of different lengths and
are parsed using a sliding window of size
. Each substring of
-gram) is stored in the program’s profile.2 Once
the profile is complete, a new execution is analyzed in terms of
the -grams composing its trace. Any -gram that occurs during
execution, which does not appear in the profile, constitutes an
2The n-gram method described here is a varient of the lookahead pairs
method described in and .
In order to characterize the generation rule for
-grams, we
first restrict our attention to the generalization induced over
traces of length
and create a directed acyclic graph (DAG)
levels, where each level has a node for each
distinct observed
-gram. A node with label
is connected to a node with label
if the last
symbols of
match the first
symbols of
(akin to the
previous example). A similar graphical construction was used
in for determining multiple length
-grams. The set of protected traces of length
can be retrieved by traversing all possible paths, from the first to the last level, in this graph. We write
the generalization induced for traces of size
: the crossover closure at (see Example 2). Finally, restricting traces to specific lengths, the generalization achieved
-gram matching can be expressed as
is the set of acceptable lengths.
There are two differences between this generation rule and
that presented in Section III-A2. In the latter, we considered all
strings to be of the same specific length ; this is a small assumption that could easily be relaxed. More importantly, in Section III-B2, we accept only those patterns that have been previously observed at a particular window position, whereas with
-grams, we disregard the information about window location.
B. Computing Posterior Probabilities on Intrusion Events
In general, deciding whether
is generated by
requires 1) distributions for
as well as
and 2) process priors
. In previous work – , we considered
models in which
is uniform
over all strings in
versus models for specific types of malicious behavior. Modeling
uniform over all
is a conservative approach, and it is consistent with the view that we have little prior information about
likely forms of attack. Modeling
as uniform places a constant
in the numerator of the likelihood ratio
and, hence, equates the posterior probability
with the degree of
’s rareness under
. (With the numerator of the expression for the posterior
We identified this approach with what the literature commonly
refers to as anomaly detection and reported experimental results
with examples both when anomaly detection is sufficient and
when it is not (and specific models of malicious behavior are
necessary for satisfactory detector performance).
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
Here, we assume that
is uniform. With this assumption, we
can either specify process priors
and a detection threshold on the posterior
or, as in and , we can simply prioritize the packets
by the likelihood they were generated by
, that is, rank
the packets by the values of their likelihood ratios; this
gives an ordering equivalent to ranking by the posterior
, regardless of the values of
the priors
. With constant
in the numerator of
the likelihood ratio, and the
process modeled as described in
the previous section, ranking packets by their likelihood ratios
reduces to partitioning packets into two classes: those that are
most likely generated by
and those most likely not generated
. We are interested in compact schemes that allow us
to distinguish quickly between low- and high-probability
anomalous packets. Under the family of distributions proposed
above, this task is equivalent to distinguishing, for a given generation rule
and current sample sample
, between packets
contained in
and those outside this set. In the
following sections, we present positive and negative detection
schemes for addressing this problem. We also compare the time
and space efficiencies of several variants of these schemes.
All of these schemes use simple and efficient string matching
rules for deciding whether or not a packet
is a member of
IV. TAXONOMY OF DETECTION SCHEMES
This section explores several detection schemes based on the
framework outlined above. Given a generation rule
current sample sample
, an instance of a detection scheme
(known simply as a detection algorithm) must be able to decide
whether or not a packet
is a member of
is, a detection algorithm can be viewed as protecting the subset
. Strings within
are regarded
as likely to be part of self, whereas those in
are most likely to be anomalous. From the perspective of formal
languages, a detection scheme is a language recognition model,
and, analogous to the issues arising in the study of formal languages, we are interested in how to construct a detection scheme
that can protect the class of languages implied by a particular
generation rule of interest, as well as in determining the class of
languages that various detection schemes can protect.
Many detection schemes can be constructed from the simple
building blocks outlined so far—the simple string matching
rules and the alternative interpretations associated with a match.
Before proceeding to analyze specific cases of interest, e.g.,
 , we first outline a systematic taxonomy for all the schemes.
This taxonomy consists of three dimensions:
• the form of a single detector together with its match
rule—a binary relation on detectors and packets, specifying when a single detector and single packet match;
• whether positive or negative detection is employed;
• disjunctive versus conjunctive matchings: Whether a
single match is sufficient to trigger a decision (positive or
negative) or whether multiple matches are required.
In the following sections, we study the relative power of the detection schemes that can be constructed from different choices
along the above dimensions. We first compare the classes of languages that can be protected by the schemes and then consider
the resources (time and space) required by the schemes to protect their languages.
A. Form of Detector and Matching Rule
A single detector, as considered in this paper, is either
1) an element of
2) an -chunk, which is a length
binary string along with
a specified window position.
When a detector
is an element of
, we are interested primarily in the rcb match rule, that is, for
We refer to such detectors under the rcb match rule as rcb
detectors. (In Section VI, we contrast the rcb match rule with
Hamming distance, which is an alternative match rule for detectors in
-chunk on window
, the matching
rule we consider is, for
B. Detector Operation
be a set of rcb or -chunk detectors, and assume the corresponding matching rule from the previous subsection. We now
consider the interpretation given to a match between a packet
and a detector. In particular, does a match mean the packet is
“in the language” (positive detection, denoted by
) or “outside the language” (negative detection, denoted by
)? Further,
do we require a single match anywhere in the packet (disjunctive matching, denoted by
), or a match in all packet window
positions (conjunctive matching, denoted by
)? The four combinations of possible answers specify four corresponding detection schemes:
denote a choice of
a choice of
specified, a collection
of detectors (either rcb
or -chunk) acts as a parameter, instantiating a detection scheme
. In particular, given the following interpretations
exactly defines a set of allowable strings; Scheme
protects, or recognizes, this set,
flagging strings as anomalous that are outside Scheme
The language (subset of
) “accepted” by each of the four
detection schemes when instantiated with a fixed set
or -chunk detectors is defined as follows:
1) Negative
disjunctive
is the set of strings
2) Positive
disjunctive
is the set of strings
3) Positive
conjunctive
is the set of strings
4) Negative
conjunctive
is the set of strings
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
-Chunks Matching Subsumes rcb Matching
We say that
is a language recognized by Scheme
using -chunks [rcb] detectors if there exists a set
of -chunks
[rcb] detectors such that Scheme
. From the perspective of language recognition, but not necessarily efficiency,
-chunk detectors subsume rcb detectors, as we now demonstrate.
Lemma 3.1: If
is an rcb detector, there exists a collection
of -chunk detectors such that
Proof: Take
, and the result follows
immediately.
To illustrate the construction of
used in the proof of the
previous lemma, let
be an -contiguous bit detector
, as shown
below, is the following equivalent set of -chunk detectors:
It follows from the lemma that for each of the four detection
schemes defined above, any set
of strings that can be
recognized with a set of rcb detectors can be recognized with
some set of
-chunk detectors. In particular, we have the following theorem.
Theorem 3.1: If
is a set of rcb detectors, and
are any choices for detector dimensions as above, then
Proof: It follows from Lemma 3.1 that for any of the
membership predicates
associated with any of the four
detection schemes Scheme
and for every string
The converse of this theorem is not true in general, since
-chunk detectors have a finer granularity than do rcb detectors;
a proper subset of
may match fewer strings in
Example: Consider the detection scheme Scheme
. Consider the pair of strings 011 and 010. We
claim that whenever both strings 011 and 010 are in the language
of rcb detectors, then so too must
be the string 110. To see this, note that an rcb detector matching
110 must either end in the pattern
or begin with the pattern
, but then, any way either of these patterns is completed (i.e.,
by specifying a bit for the ) results in an rcb detector matching
one of 011 or 010. Consequently, if Scheme
110 from the language it recognizes, it must exclude also at least
one of 011 or 010. In contrast, if the -chunks detector set
consists of the single detector
both 011 and 010 while excluding 110.
The results of the next section imply that the same result holds
for Scheme
, that is, that the class of languages recognized by
using -chunks detectors properly contains the class
recognized by Scheme
using rcb detectors.
D. Detection Schemes and the Crossover Closure
The following theorem, which was established by simple
set-theoretic arguments, helps clarify the relationships between
the classes of languages recognized by the various detection
Theorem 4.1: Let
be any set of rcb or -chunks detectors,
denote the complement of
relative to the universe of
all rcb or -chunks detectors over the same alphabet and of the
same length as the detectors in
. Similarly, the complement of
the set Scheme
(which is a subset of
) is taken relative
Further, the subset containment is proper for some
It follows from Scheme
the class of languages recognized by Scheme
is identical
to the class recognized by Scheme
, when either rcb or
-chunk detectors are considered. Similarly, it follows from
that the class of languages
recognized by Scheme
is identical to the class recognized
, when either rcb or
-chunk detectors are
considered.
As discussed in Section III, one of our most important detection applications is to protect the crossover closure of a sample
. There is a strong relationship between crossover closure and the schemes Scheme
and Scheme
when -chunk
detectors are used.
Let sample
be any subset of
denote the set of
windows present in sample
, that is, the union of the projections of sample
onto each window (the window size is fixed
and understood). Let
denote the set of windows not present
, that is,
. We then have the following
Theorem 4.2: Scheme
Further, crossover closure exactly characterizes the class of
languages recognized by Scheme
and Scheme
-chunk detectors are used. That is, we have the following
Theorem 4.3: The class of languages recognized by
and Scheme
when -chunk detectors are used is
exactly the class of sets closed under crossover closure.
Proof: If a set
is closed under crossover closure, i.e.,
, then we can construct a set of detectors
from the windows present in
. It follows from Theorem
4.2 that Scheme
By the definition of Scheme
all windows
is an arbitrary set of
detectors. We construct a set of detectors
by taking, for
every window
, the projections
. It follows that
. Finally, using
Theorem 4.2, we have Scheme
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
Results of the previous section imply that when rcb detectors are used, Scheme
and Scheme
can recognize only
sets closed under crossover closure but not all sets closed under
crossover closure since the class of languages recognized by
these schemes when rcb detectors are used is properly contained
by the class recognized when -chunks are used.
Finally, consider the schemes Scheme
and Scheme
using -chunks. These schemes do not recognize all sets closed
under crossover closure but do recognize some sets not closed
under crossover closure. To see this, we first observe that there
exist sets
is closed under crossover but
For example, let
and hence,
is closed under crossover.
contains the other
seven length 3 strings, and
Therefore, we have the following.
and Scheme
cannot recognize all sets
closed under crossover because if Scheme
recognizes
closed under crossover such that
is not closed
under crossover, Scheme
implies that Scheme
recognizes
sets not closed under crossover because Scheme
is not closed under crossover, and
V. PARTIAL MATCHING AND GENERALIZATION
This section is devoted to Scheme
and Scheme
the -chunks matching rule. These schemes both recognize the
sets closed under crossover, but they have different properties
in terms of implementation requirements. These differences include distributivity, scalability, and algorithm efficiencies. This
section explores the size of the detector set
and determines
its expected size as a function of the number of strings in a randomly generated sample
A. Expected Number of Unique Detectors Under -Chunks
Given a set
, it is straightforward to compute exactly how
many detectors will be generated for maximal protection, both
for the positive and negative detection schemes (Scheme
and Scheme
) using the obvious generation method. For
, it requires counting the number of distinct patterns
for each of the
windows that comprise the strings in
whereas for Scheme
, enumerating the distinct patterns that
are not present in each window will result in the number of
detectors. To provide an estimate of the average number of
detectors, for both cases, as a function of the size of
the following.
• The number of strings in
with a specific pattern in any
given window of size
• The probability of selecting at random one such string is
• Assuming
is small compared with , we approximate
the probability that a randomly generated self set of
unique strings will contain a specific pattern by
Number of positive and negative detectors as a function of the size of
the self set, assuming the self set was generated randomly. The plot shows both
complete and reduced detector sets.
equation is approximate because it considers trials to be
independent and sampling to take place with replacement.
• Following the previous item, the expected number of distinct patterns
, for a given window of size , is approximated by
The following equation denotes the expected size of a set of
detectors having the property that each member matches one
window and that all windows are matched:
Conversely, the expected number of detectors possible for the
negative detection scheme (i.e. detectors that do not match any
window pattern in
) is given by
To establish when one scheme requires a smaller set of detectors than the other for maximal coverage, we determine the
number of strings in
for which both schemes yield the same
number of detectors, i.e., when
(see Fig. 2):
Solving for
Although we are primarily concerned with binary strings, it is
worthwhile to note that for an arbitrary size alphabet
above equation generalizes to
, which affects
the point at which one scheme is preferable to the other, in terms
of the size of the detector set.
In a worst-case scenario, Scheme
may require
detectors when
. Similarly, Scheme
can also yield up to
detectors but only when there are no self strings whatsoever.
1) Reduced Detector Set for Negative Detection: The
number of detectors needed in Scheme
, to protect
can actually be smaller than the full set described above since
significant redundancy amongst detectors may be present. We
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
examine this property in detail for the case of a binary alphabet.
Number the
windows from left to right.
• Regardless of the composition of
, a detector must be
generated for each pattern in the first window that is not
present in
a window, we have the following.
are present in self, then we cannot
generate any detector for them.
—If neither is present, then we need not generate detectors for them since the preceding window will be
missing its prefix, i.e.,
and a detector in the previous window will also match
strings with such a pattern.
—If only one is present, say
must generate detector
since no string in
contains it.
With this in mind, the average number of detectors needed in
the minimal set is given by
Following the previous rationale, we can also set an upper
bound on the number of detectors required in the minimal set.
The maximum number of self strings we can have without creating crossovers, thereby reducing the number of required detectors, will exhibit only one of every pair of patterns
for each window. This results from a maximum of
distinct self strings for a detector
set size of
. Fig. 2 shows the plots of the expected number
of detectors for both the full and reduced detector sets.
2) Reduced Detector Set Size for Positive Detection: Using
similar reasoning to that of Section V-A1, we can find a significant amount of redundancy amongst detectors in the form
of implicit matches. Consider the case where window
contains patterns
then, it must be that window
has either or both of the patterns
ending with bits
, and therefore, a string matched
by one of these patterns will also be matched in
. We call such a match an implicit match and eliminate
from the detector repertoire. The number of detectors in the resulting set can be expressed as
The size of the sample
the same number of detectors is the same as with the full repertoire, i.e.,
One subtlety about this analysis is that if not all positive detectors are represented explicitly, then some additional information
is required to identify implicit matches. This could be stored explicitly, requiring at the very least one bit per implicit match, or
it could be derived at runtime by determining, once a match (or
implicit match) at window has been established, if both
are absent in window
.3 A plot of
for different
3If the information is stored explicitly, the extra bits can be “recovered” as a
decrease in execution time.
Crossover closure given a sample S of real self data RS.
sample sizes is presented in Fig. 2 without regard to the extra
information required to determine implicit matches.
B. Crossover Closure and Its Expected Size
One useful way to visualize the generalization induced by the
-chunks detectors is to construct DAG
is the number of distinct bit patterns of length
over all windows
of strings in
. Vertices are labeled by a pair ( ,
represents the window number, and
stands for the bit pattern.
is the number of edges, and edge
and the last
bits of pattern
match the first
bits of pattern
. Under this
representation, the crossover closure is exactly the set of strings
formed by all traversals of the graph from levels 1 to .
Take, for instance, a self set
comprised of the following
two strings
(see Fig. 3).
The corresponding graph
. A visual depiction of the graph
(omitting the window number from the vertex labels) is4
Recovering the strings by traversing the preceding graph, we
. In order to determine the size of
for a randomly generated sample,
we note that the number of substrings of length
that contain
is double the number of substrings of length
that contain the pattern in the same window
if there are strings in
that exhibit both
and stays the same if exactly
one of these is present. In terms of our DAG representation, the
number of paths that include a node with a given label doubles
4The labels for window numbers could be recovered from the level by using
a topological sort.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
when such a node has two outgoing edges. We can write this as
a recurrence on the number of windows :
otherwise.
Solving the recurrence yields:
is the probability of a node having two outgoing
edges. In order establish the value for
, consider the following gadget:
The probability for a given edge to be present is approximated
and its absence by
Given that the likelihood of a node having only one outgoing
edge is not independent of the probabilities related to the second
node in the graph (a node at the same level differing only in the
first bit position), we consider the probability
node having one outgoing edge:
Similarly, for a node in the gadget to have no outgoing edges (or
for the node to be absent)
Finally, the probability of an individual node having two outgoing edges is given by
The number of holes
can be derived by simply subtracting
The size of the generalization
can be calculated from
the size of
or from the size of the detector set (obtaining an
estimate for
from either (2) or (1) and substituting in (6). It
is important to note that the actual size will depend on the structure of the specific self set. Nevertheless, the analysis provides
insight into its behavior (see Fig. 4) and enables us to ascertain
the impact of allowing novel strings into the sample. This can be
useful for determining, in a dynamic scenario, when (or at what
rate) should detectors be added or deleted from the working set.
VI. PERMUTATIONS AND DIVERSITY
In and , an additional mechanism was introduced to
improve discrimination between self and nonself. This mechanism is loosely modeled after the diversity of major histocom-
Number of holes as a function of self-sample size for two parameter
settings. The graph on the left illustrates how fast the generalization increases
when r is small compared with the string length. The graph on the right shows
how, after the generalization has reached its maximum, the number of holes
slowly decreases as more strings are added to the sample S.
patibility complex (MHC) molecules used in the natural immune system. In the artificial setting, multiple permutations of
the -length strings (both data and detectors) are stored. This,
combined with the contiguous-bits match rules, provides the
system with diversity of representation and improves the discrimination abilities of the system.
A. Permutation Maps
for Reducing the Number of Holes
, which is the set of holes, is too large, accurate discrimination between
will be unachievable.
Hofmeyr introduced the concept of permutation masks as a way
to control the size of
. A permutation mask is a reversible
mapping that specifies a reordering of bits for all strings in
In his network intrusion-detection data set, he reported an improvement in detection ability by about a factor of three, when
permutation masks were used.5 By controlling the number of
permutations used, he was able to achieve greater control over
the -contiguous bits generalization. One interesting property of
this transformation is that if we consider all nonredundant permutation masks, the -contiguous bits matching rule is able to
protect strictly more sets than Hamming distance.
Theorem 1.1: The class of languages recognized by
-contiguous bits, with permutation masks, properly contains that recognized by Hamming distance.
be strings of length , where
is a detector.
be a function
that returns the length of the longest run of contiguous
bits that match between strings
to match string
be a function
that returns the Hamming distance between strings
is matched by a string
using the Hamming
distance rule if
differ in, at most,
bit positions,
5Hofmeyr’s dataset was collected using a variant of pure permutations that
was computationally more efficient.
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
denote some permutation of string
(Rcb) denote the class of languages recognized by
Rcb with permutation masks for a given
and a detector
denote the class of languages recognized by
and a set of detectors
The class of languages recognized by Rcb, with permutation
masks, contains the class of languages recognized by
for all permutations
, and hence, [ is a valid detector under
matches no member of
(Rcb) under Rcb
and any permutation
2) Further, if
matches a string
, we show there
exists a permutation
Rcb, that is,
(Rcb). Label the bits that differ
the bits that match as
. Clearly,
valid permutation, and Rcb
is a valid detector for
(Rcb)) that matches
is a language recognized by some set of detectors under
, we have shown how to construct an
associated set of permutations such that this same set of
detectors recognizes exactly
also under Rcb.
In contrast,
does not recognize all languages recognized
by Rcb with permutation masks because a detector
might match more than it matches under a single permutation
and Rcb and thus fail to be a valid detector under
Example: Let
. Under Rcb, we can generate a valid detector
, whereas under
, the potential detectors for
000, 001, 010, and 100, all of which have a Hamming distance
less than two from
but also from all strings in
. Therefore,
is undetectable under
but detectable under Rcb with permutation masks; hence
For those sets that Hamming distance can recognize, the Rcb
rule may require multiple sets of detectors
, each with its own
transformation
. This mechanism is suitable for a detection
scheme that requires precise control over the generalization or
that must be distributed.
To illustrate how the permutation masks can in fact reduce
the number of holes, consider the self strings
. There are two crossover
other than
, for which no detector
can be generated since neither string has a distinctive window
that separates it from self:
If we apply the permutation by which the third bit position is
next to the first bit, as the following picture depicts, then we can
generate two detectors
that will be able to match
, respectively.
UNDETECTABLE STRINGS UNDER EVERY PERMUTATION, FOR A GIVEN SET S
AND THE rcb MATCHING RULE
UNDETECTABLE STRINGS UNDER EVERY PERMUTATION, FOR A GIVEN SET
S AND THE r-CHUNKS MATCHING RULE
B. Completeness
We can now ask whether there are languages that cannot be
recognized by rcb, even if permutation masks are used. That is,
are there still strings in nonself
that go undetected
even if every permutation mask is employed? The answer to this
question is yes; as pointed out in , all practical match rules
with a constant matching probability will exhibit holes.
We show, by construction, that there are languages that cannot
be recognized under rcb matching with permutations: Let
, and consider the hole
. Table I lists all the possible permutations of
the substrings out of which a detector could be constructed for
under each permutation.
As can be seen from Table I, under each permutation, there are
no available templates (that is, templates that do not match some
), and thus, neither rcb Scheme
nor rcb Scheme
can accept the strings in
while excluding
. Although we can
construct such examples, it is difficult to characterize the set of
all such holes under permutations.
Lifting the restriction that detectors be of length
-chunk detector to match the string
from the previous
example. Nevertheless, there are languages that cannot be
recognized under the
-chunks matching rule (Scheme
) augmented with permutation masks. Consider the
following scenario: Let
. Table II lists all six permutations of
possible -chunks detectors needed to match
In general, a string
cannot be detected by Scheme
under any permutation if all of its template combinations are in self. The template combinations of a string are the
templates (strings with unspecified bits) that result from all the
possible ways of specifying only
bits. For instance, the template combinations of
denotes “don’t care.” It is easy to see that only if one such template does not exist in
can there exist a permutation that places
these bit positions contiguously, without creating the same contiguous bit pattern in
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
VII. DISCUSSION
The preceding sections developed a formal framework for
studying tradeoffs between negative and positive detection and
presented several theoretical results that we believe are important to a wide variety of practical problems. In this section, we
explore some of the implications and potential extensions of our
theoretical results.
A. Computer Security Applications
Over the past several years, a number of anomaly intrusion-detection systems have been developed for computer
security, which explore different instances of the detection
schemes described in this paper. These systems were reviewed
in Section II; some use positive detection , , , and
some use negative detection , . Likewise, some of these
systems use
-contiguous bits matching , , some use
-chunks , some use
-gram matching , and some use a
variant of
-gram matching known as “lookahead pairs” ,
 . Although the negative-selection strategy of the immune
system has received a great deal of attention, it should be noted
that the immune system also uses positive selection, combining
it advantageously with negative selection.
In some cases, we have good experimental evidence for preferring one scheme over another. For example,
-grams outperformed lookahead pairs on our early data sets in terms of absolute discrimination ability (unpublished). However, when efficiency matters, as in the case of online detection, then the lookahead-pairs method is a clear winner, paying a small penalty
in terms of discrimination ability. A second example occurred
when we compared rcb detection to -chunks on a network intrusion-detection task , . Such results are anecdotal in
the sense that they were obtained by experimentally testing one
or two methods on a limited number of data sets. Without some
theory to guide us, it is difficult to determine how much a given
result depends on the particular data set used and how much it
depends on the choice of method. Likewise, it has been difficult
to determine which aspects of a given detection scheme are most
responsible for its success (or failure), e.g., negative detection,
match rule, or parameter settings.
The results presented in this paper allow us to begin approaching such questions from a theoretical perspective. In
particular, the notion of a crossover closure, and its relation to
-chunks matching, will allow us to understand more
deeply when and why these matching methods are preferable to
more familiar methods such as Hamming distance. Likewise,
the closed-form expressions for detector set sizes, both for
positive and negative detection, allow us for the first time to
predict how large a problem must be before it pays to use a
negative-detection scheme. Until now, the negative-detection
approach has been somewhat of a curiosity, notorious as
much for its immunological metaphor as for its demonstrated
advantages over other methods. Results such as those presented
here will form the basis of a more objective evaluation.
B. Relational Databases
Section III introduced the relationship between crossover closure and relational database theory. Specifically, we showed that
the crossover closure of a set
of strings is equivalent to the natural join of those strings (interpreted as tuples) projected onto
a relational decomposition scheme. There are several aspects of
this equivalence, which we find interesting.
First, we can characterize when a set of strings is closed under
crossover closure in terms of well-studied structural properties
known as database dependencies , . A set of strings
is closed under crossover closure exactly when it decomposes
losslessly into the relation schemes
induced by its
-length windows, as we showed in Section III-A. Although the structural characterization of a lossless
join decomposition is complex in the general case, the specialized form of the above decomposition simplifies the condition
considerably; the characterization reduces to the adherence
to a collection of simple multivalued dependencies on the
original relation scheme
. In order for the
decomposition to be lossless, one of the following multivalued
dependencies on
must hold for each
Note that, for example, the multivalued dependency
asserts that if
contains the tuples
then it must also contain the tuples
This condition describes the semantics of collections of information that occur naturally within a vast number of diverse
application domains. Whether such dependencies arise naturally
among sets of strings we wish to protect remains to be investigated.
Taking the connection one step further, we can investigate
the correspondence between the schemes Scheme
, which recognize the class of sets closed under
crossover closure and query systems for relational databases.
In particular, if one is able recognize with a set of detectors
(possibly significantly pruned to remove redundancy as described in Section V-A2) a given set closed under crossover
closure, one can also represent by the same means any instance
of a relation scheme obeying the dependencies specified above.
The negative detection scheme Scheme
would be most
appropriate for membership queries, whereas Scheme
would be appropriate for queries requiring enumeration of
ESPONDA et al.: FORMAL FRAMEWORK FOR POSITIVE AND NEGATIVE DETECTION SCHEMES
the member tuples. Conversely, known results from database
theory regarding minimal representations translate to lower
bounds on time and space requirements for these detection
C. Dynamic Samples
As mentioned in Section III, we expect our samples—and
hence our distribution of the next packet generated by
change with time. Some factors contributing to this include the
following.
, it may become part of
then properly contains
, and consequently, the distribution of
the string generated at time
differs from that at .
This assumes we have a mechanism for deciding that
is generated by
, despite the fact that we flag it as a low
probability string (e.g., it passes some investigative step).
2) We might want to delete packets from the sample that
have not occurred recently. This might or might not alter
and, hence, the distribution of the string
generated at time
Extending our analytical treatment to include dynamically
changing samples raises a number of questions. In principle,
when a new sample is created at time , a new set of detectors
could be generated de novo. However, this is likely to be extremely inefficient. An area of future investigation is to analyze
the techniques used to update the detector set in response to the
specific types of perturbations in the sample.
D. Distance Measures
If we can define a metric over a match rule, then our detection
schemes can be rewritten in terms of a distance measure. For
instance, using -chunks
is the number of windows that are present
but not in any string in
. We are interested in
exploring schemes that are intermediate between conjunctive
and disjunctive detection (both positive or negative). In these
intermediate schemes, a string may be treated as self, even if
not all its window patterns have been observed before, i.e.,
is some threshold.
Intuitively, strings that differ a small amount from the sample
are more likely to be a part of
that those that differ a lot.
Extending this idea one step further, one can imagine distance
measures that do not assign a uniform value to every match but
instead take into account structural and statistical properties
of the sample in order to weigh the relative merits of distinct
E. Implementation Issues
Although we have emphasized the representational power of
different detection schemes in this paper, there are important
implementation considerations that affect the choice of a
match rule and detection scheme. Here, we briefly discuss
-chunks match rule and contrast it with rcb, although,
as shown in Section IV-C, the class of languages recognized
by them are distinct. Nevertheless, the comparison highlights
some important properties of
-chunks. We have thus far
described our detection schemes as a set of detectors, first
because we find it useful for visualizing and analyzing their
properties and, second, because we consider that some applications might benefit from distributing them throughout several
nodes; thus, we make no assumptions as to their location.
Nevertheless, it is important to note that there is a large body
of work regarding string matching that suggests a more
efficient representation of detectors along with the algorithms
that operate on them but impose additional restrictions on the
ease of distributing them. With this in mind, we consider three
implementation issues: the cost of generating detectors, the
cost of storing detectors, and the cost of locating detectors.
1) Detector Generation: Given a self set
, a straightforward method for generating the appropriate -chunks detectors
is to search the entire sample to determine which patterns are
present in each window, requiring
space. Note that the detector set can be sorted during its generation at no extra cost (using bin sort and assuming
There have been several algorithms proposed for generating detectors for the rcb match rule , , , that report
linear generation time and
extra space.
2) Detector Storage: Intuitively,
-chunks requires more
space than rcb since every rcb detector ( bits each) can be
decomposed into
-chunks detectors (
bits). Nevertheless,
depending on the particular structure of
, there is likely to
be a significant number of repeated patterns in each window
amongst the set of rcb detectors. Because -chunks represents
each distinct pattern once, this could lead to smaller space
requirements in some cases. In general, the space required for
-chunks is
3) Detector Location: Having detectors specific for each
window (as in
-chunks) allows them to be stored in sorted
order for each window. Thus, checking to see if a string belongs
to self or not requires
time, whereas inserting or deleting
a detector is just
. There is also a potential that hashing
schemes could reduce the search time.
F. Crossover Closure and Genetic Algorithms
There is a tantalizing connection between the crossover closure described in this paper and the crossover operator often
used in genetic algorithms. The crossover closure contains only
a subset of all possible one-point crossovers in the self set, and
therefore, the generalization achieved by
-contiguous bits is
not identical with the space of coordinate hyperplanes defined
by the crossover operator. Likewise, the -chunks matching rule
is nothing more than a restricted form of the schema notation
often used to explain genetic algorithm behavior. An additional
area of future investigation is to make these connections more
precise and to determine if the Schema theorem from genetic
algorithms bears any relation to the processing performed by
either of the contiguous bits match rules.
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004
VIII. CONCLUSION
In this paper, we presented a formal framework for analyzing
different positive and negative detection schemes in the context
of approximate matching. Although the framework we presented is quite general, the primary application we have in mind
is anomaly intrusion detection, for example, detecting anomalous TCP connections in a local area network or detecting
anomalous patterns of system calls in executing processes. We
gave examples of how different partial matching rules fit into
our scheme, including Hamming distance,
-contiguous bits,
-grams. We characterized the generalization induced by
-contiguous bits (and its relative
-chunks) by defining the
crossover closure of a sample of -length strings. Next, we
showed that the crossover closure is related to the concept of a
lossless join in relational database theory.
With this formal apparatus in place, we were then able to
give some theoretical results on the relative power of the different detection schemes and matching rules. In particular, we
showed that the
-chunks match rule subsumes
-contiguous
bits matching, that (under rcb or
-chunks) the class of languages recognized by negative disjunctive detection is the same
as that of positive conjunctive detection, and that the crossover
closure of a sample
exactly characterizes the class of languages recognized by negative disjunctive detection and positive conjunctive detection (under -chunks matching).
Next, we considered the number of detectors that are required
to provide maximal discrimination for fixed
under Scheme
and Scheme
. We gave closed-form expressions for both
schemes, allowing us for the first time to estimate how large a
self set must be before negative detection is a computationally
advantageous strategy. We then discussed the expected size of
the crossover closure and explored how the use of permutations
of the representation can reduce the size of the crossover closure. Finally, we showed that the class of languages recognized
-contiguous bits, augmented with permutation masks,
properly contains that recognized by Hamming distance, and
we showed that even with the use of permutations, there are
some languages that cannot be recognized using
-contiguous
bits matching.
The theoretical results presented here are significant because
they address a large and quickly expanding body of experimental work in intrusion detection that uses one of the detection
schemes. It is our hope that these theoretical results will make
it easier to understand the experimental results and to predict
which approaches are most promising for which problems.
ACKNOWLEDGMENT
The authors would also like to thank J. Holland, A. Tyrrell,
W. Langdon, and the anonymous reviewers, who’s comments
greatly improved the presentation of this work.