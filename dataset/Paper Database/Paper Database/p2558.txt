The Annals of Statistics
2005, Vol. 33, No. 5, 1983–2021
DOI: 10.1214/009053605000000426
© Institute of Mathematical Statistics, 2005
RECURSIVE MONTE CARLO FILTERS: ALGORITHMS
AND THEORETICAL ANALYSIS
BY HANS R. KÜNSCH
ETH Zürich
Recursive Monte Carlo ﬁlters, also called particle ﬁlters, are a powerful tool to perform computations in general state space models. We discuss
and compare the accept–reject version with the more common sampling importance resampling version of the algorithm. In particular, we show how
auxiliary variable methods and stratiﬁcation can be used in the accept–reject
version, and we compare different resampling techniques. In a second part,
we show laws of large numbers and a central limit theorem for these Monte
Carlo ﬁlters by simple induction arguments that need only weak conditions.
We also show that, under stronger conditions, the required sample size is independent of the length of the observed series.
1. State space and hidden Markov models.
A general state space or hidden
Markov model consists of an unobserved state sequence (Xt) and an observation
sequence (Yt) with the following properties:
State evolution:
X0,X1,X2,... is a Markov chain with X0 ∼a0(x)dµ(x) and
Xt|Xt−1 = xt−1 ∼at(xt−1,x)dµ(x).
Generation of observations:
Conditionally on (Xt), the Yt’s are independent
and Yt depends on Xt only with
Yt|Xt = xt ∼bt(xt,y)dν(y).
These models occur in a variety of applications. Linear state space models are
equivalent to ARMA models (see, e.g., ) and have become popular under the
name of structural models (see, e.g., ). Nonlinear state space models occur
in ﬁnance (stochastic volatility; see, e.g., ), in various ﬁelds of engineering
(speech, tracking and control problems; see, e.g., ), in biology (ion channels,
DNA and protein sequences) and in geophysics (rainfall at a network of stations,
data assimilation). A more detailed survey with many references is given in .
In order to apply these models, two kinds of problems have to be solved: Inference about the states based on a stretch of observed values ys : t = (yu,s ≤u ≤t)
Received January 2003; revised August 2004.
AMS 2000 subject classiﬁcations. Primary 62M09; secondary 60G35, 60J22, 65C05.
Key words and phrases. State space models, hidden Markov models, ﬁltering and smoothing, particle ﬁlters, auxiliary variables, sampling importance resampling, central limit theorem.
H. R. KÜNSCH
for a given model, that is, at and bt known (this is called prediction, ﬁltering and
smoothing), and inference about unknown parameters in at, bt. From a statistical
point of view, the latter problem is maybe of greater interest, but fast and reliable
algorithms for the former are a prerequisite for computing maximum likelihood
or Bayesian estimators. The reason for this is brieﬂy mentioned in Section 2.1.
This paper is therefore entirely devoted to algorithms for ﬁltering, prediction and
smoothing.
Section 2 recalls the basic recursions for ﬁltering, prediction and smoothing.
Section 3 discusses algorithmic aspects of sequential Monte Carlo methods to implement these recursions. Most algorithms in the literature, beginning with the
pioneering paper by Gordon, Salmond and Smith , use the sampling importance resampling idea of Rubin . An exception is Hürzeler and Künsch 
who use the accept–reject method instead. Here we show how some ideas like
stratiﬁcation and an auxiliary variable method of Pitt and Shephard can be
adapted to rejection sampling, and we give new results on the performance of systematic resampling methods. In addition, we hope that our view of classifying and
comparing approaches is useful.
Section 4 presents results on the convergence of the method as the number of
Monte Carlo replicates tends to inﬁnity. We discuss both laws of large numbers
and a central limit theorem. Recently, many similar results have been published;
see, for example, . The distinctive features of our presentation here are
the weakness of conditions, the use of the total variation distance to measure the
difference between the approximate and the true ﬁlter density and the simplicity
of the techniques used. We basically show that most results follow by induction,
in accordance with the recursive nature of the algorithm. The complications that
occur are due to a counterintuitive property of Bayes’ formula; see Lemma 3.6(ii)
in . As a consequence, although one can obtain consistency with very few conditions on the model, the required sample size seems to grow exponentially with
the number of time steps. For results that guarantee that the required sample size
is independent of the number of time steps (or grows at most logarithmically), one
has to use induction over several time steps which requires rather strong conditions on the dynamics of the states. At the end, we give some results for the case
where (Xt) is a continuous time process and the sampling rate of the observations
increases.
2. Filtering and smoothing recursions.
In general, we will use the symbol p
as the generic notation for a conditional density of its arguments. However, for the
conditional density of Xt given Y1: s = y1: s, we use the notation ft|s(xt|y1: s). The
three cases s < t, s = t and s > t are called prediction, ﬁltering and smoothing,
respectively.
RECURSIVE MONTE CARLO FILTERS
The dependence structure of a state space model can be represented by the following directed acyclic graph:
From this, various conditional independence properties follow which are used together with the law of total probability and Bayes theorem to derive recursions
for the ﬁlter, prediction and smoothing densities. These are well known; see, for
example, , Section 3.3, and we state them without proofs.
The most important result is the following recursion for the ﬁlter density:
Propagation.
From the ﬁlter density, we obtain the one-step-ahead prediction
ft|t−1(xt|y1: t−1) =
ft−1|t−1(x|y1: t−1)at(x,xt)dµ(x).
From the-one-step ahead prediction density, we obtain the ﬁlter density one time step later:
ft|t(xt|y1: t) =
ft|t−1(xt|y1: t−1)bt(xt,yt)
 ft|t−1(x|y1: t−1)bt(x,yt)dµ(x)
∝ft|t−1(xt|y1: t−1)bt(xt,yt).
In parts of the literature, for example, in , Yt depends on Xt−1 and not on Xt.
Then the ﬁlter density is, in our setup, the prediction density which should be kept
in mind when comparing formulae.
2.1. Prediction of observations and likelihood.
The denominator in the update
step (2) is the conditional density of Yt given Y1: t−1:
p(yt|y1: t−1) =
ft|t−1(x|y1: t−1)bt(x,yt)dµ(x).
If ft|t−1 is available, we thus can obtain the likelihood from
p(y1: T ) =
p(yt|y1: t−1).
A different representation of the likelihood is obtained by marginalization:
p(y1: T ) =
at(xt−1,xt)bt(xt,yt)
From this, the likelihood ratio can be expressed as an expectation with respect to
the smoothing distribution; see, for example, .
H. R. KÜNSCH
2.2. Smoothing.
The ﬁlter densities can also be used for the smoothing problem since conditional on y1: T , (XT ,XT −1,...,X0) is an inhomogeneous Markov
chain with starting density fT |T and backward transition densities
p(xt|xt+1,y1: T ) = p(xt|xt+1,y1: t) ∝at+1(xt,xt+1)ft|t(xt|y1: t).
This is also the basis for the forward-ﬁltering–backward-sampling algorithm;
see , equation (20). From (4), we can derive, in particular, a backward recursion for ft|T .
2.3. Recursive ﬁltering in operator notation.
A compact notation for the ﬁlter
recursion which will be useful later on is
ft|t(·|y1: t) = B
t ft−1|t−1(·|y1: t−1),bt(·,yt)
f (x′)at(x′,x)dµ(x′)
is the Markov transition operator, and
B(f,b)(x) =
 f (x)b(x)dµ(x)
is the Bayes operator that assigns the posterior to a prior f and a likelihood b.
The operators A∗
t and B(·,b) map the space of densities into itself, but they can be
extended to the space of probability distributions.
2.4. Implementation of recursions.
If Xt is discrete with M possible values, integrals are sums and the recursions need O(T M2) operations. In a linear
Gaussian state space model, all ft|s are Gaussian, and their means and variances
are computed with the Kalman ﬁlter and smoother.
In practically all other cases, the recursions are difﬁcult to compute. Analytical
approximations like the extended Kalman ﬁlter are not satisfactory, and numerical
integration is problematic in high dimensions. Much current interest focuses on
Monte Carlo methods. Standard Markov chain Monte Carlo can be used, but it
lacks a recursive implementation. There has been considerable interest in recursive
Monte Carlo methods in recent years; see, for example, .
3. Algorithms for recursive Monte Carlo ﬁltering.
The following is the key
observation: A∗
t f is difﬁcult to compute, but easy to sample from if we can sample
from f and at(x,·). This allows us to generate recursively a sequence of samples
(“particles”) (xj,t;j = 1,...,N,t = 0,1,...) with approximate distribution ft|t
as follows: If (xj,t−1) is available, we can replace
t ft−1|t−1(xt|y1: t−1) =
ft−1|t−1(x|y1: t−1)at(x,xt)dµ(x)
RECURSIVE MONTE CARLO FILTERS
at(xj,t−1,xt).
Therefore, we sample (xj,t) from the distribution with density
t|t (·|y1: t) ∝bt(·,yt) 1
at(xj,t−1,·).
In this section we discuss methods to sample from such a density. We simplify the
notation somewhat and write the target density as
f N(x) ∝f N
u (x) = b(x)
(subscript u for unnormalized). We will call b the likelihood and N−1 
the prior. In the ﬁltering context, the prior is the approximate prediction density.
For later use, we also introduce
a(j,x)b(x)dµ(x),
which is in the ﬁltering context equal to the conditional density of Yt given
Xt−1 = xj,t−1. We assume that we have good methods to generate samples from
a(j,·) for any j. The methods we discuss fall into two categories: accept–reject
and importance sampling with an additional resampling step.
3.1. Accept–reject methods.
The accept–reject method for sampling from the
density (7) produces values X according to a proposal ρ, and if X = x accepts it
with probability
π(x) = f N
Here M is an upper bound for the ratio f N
u (x)/ρ(x):
The most obvious proposal ρ(x) is the prior, that is,
Then the evaluation of the acceptance probabilities π(x) is easy as long as b is
bounded. In order to sample from (9), we ﬁrst choose an index J uniformly from
H. R. KÜNSCH
{1,...,N}, and given J = j, we sample X from a(j,x). Note that, in this case, the
densities a(j,x) need not be available in analytic form; we only have to be able to
sample from them. This is of interest in discretely observed diffusion models.
The average acceptance probability of this algorithm is
 ρ(x)π(x)dµ(x) =
j βj/M. In particular, if ρ is the prior and if we use the smallest value of M, it
is equal to
N supx b(x).
This is low if the likelihood is more informative (concentrated) than the prior, or
if the likelihood and the prior are in conﬂict. We discuss here some modiﬁcations
and tricks that can alleviate this problem in some situations.
3.1.1. The mixture index as auxiliary variable.
Other proposal distributions
than the prediction density can, of course, lead to higher acceptance rates, but
usually it is difﬁcult to compute a good upper bound M, and the evaluation of the
acceptance probability π(x) is complicated due to the sum over j. A way to avoid
at least the last problem is based on an idea by Pitt and Shephard . Namely,
we can generate ﬁrst an index J according to a distribution (τj) and given J = j,
a variable X according to a density ρ(j,x). We then accept the generated pair
(j,x) with probability
π(j,x) = a(j,x)b(x)
Mτjρ(j,x),
a(j,x)b(x)
τjρ(j,x) .
If the pair is accepted, we simply discard j and keep x, and otherwise we generate
a new pair. Because the accepted pairs (J,X) have distribution
a(j,x)b(x)
the marginal distribution of X is the target (7). If we take τj = 1/N and ρ(j,x) =
a(j,x), we obtain the usual algorithm discussed before, but one will try to increase
the acceptance rate by other choices.
Because j runs over a ﬁnite set, we will usually take
where Mj ≥sup
a(j,x)b(x)
For a given choice of densities ρ(j,x) and bounds Mj, the average acceptance probability is less than or equal to βj/ Mj, with equality iff
RECURSIVE MONTE CARLO FILTERS
The average acceptance probability is
π(j,x)τjρ(j,x)µ(dx) = 1
with equality iff Mk/τk is constant.
If ρ(j,x) = a(j,x), the optimal τj’s are thus constant. This is somewhat surprising since one could conjecture that it is better to give higher probability to
those indices j for which the mass of a(j,x) is close to argsupb(x).
The crucial point in implementing this algorithm is the choice of the densities
ρ(j,·). Lemma 1 implies that, for a high acceptance probability, all Mj’s should be
small, that is, each ρ(j,x) should be a good proposal distribution for the density
a(j,x)b(x)/βj. Ideally, we would choose that density itself. But then Mj must
be close to the normalizing constant βj which typically is not available in closed
form. A more practical approach chooses a parametric family (ρ(θ,x)), where we
have available tight upper bounds
M(j,θ) ≥sup
a(j,x)b(x)
We then optimize over θ, that is,
ρ(j,x) = ρ(θj,x),
where θj ≈argmin
Note that it is not necessary to ﬁnd the optimal θ exactly, but M(j,θ) should
be a true upper bound. By choosing the family (ρ(θ,x)) such that it contains all
densities a(j,x), we can make sure that the acceptance probability is at least as
high as with the usual algorithm.
The simplest stochastic volatility model, see, for example, , is
obtained if we take for a(j,·) the normal density with mean mj and variance σ 2
and for b the likelihood of a N (0,exp(x)) random variable Y,
b(x) = b(x,y) = exp
If we choose as ρ(θ,·) the normal density with mean θ and variance σ 2, we can
compute the supremum of
log a(j,x)b(x)
2 exp(−x) −
H. R. KÜNSCH
over x. It is equal to
2 δ2 + mjδ −
(1 + logy2) +
log(1 + 2δ),
provided δ = (θ −mj)/σ 2 ≥−1/2 (otherwise the function is unbounded above).
Minimizing this expression with respect to δ subject to δ ≥−1/2 leads to a nonlinear equation which has no closed form solution. Using log(1 + 2δ) ≤2δ, we
obtain a quadratic upper bound which is minimized by
θj = mj + σ 2
4 + σ 2 (logy2 −mj)
This choice of θj may be slightly suboptimal, but because the bound is sharp for
θ = mj, that is, δ = 0, we still can guarantee a higher acceptance probability than
with the usual method. In practice, the gain can be dramatic if |y| is small.
The above choice of θj is somewhat different from the suggestion
θj = mj + σ 2
y2 exp(−mj) −1
in , page 285. In addition, the choices for τj differ.
3.1.2. Balanced sampling.
Besides reducing the acceptance rate, we can also
try to reduce the variance by using a more balanced sampling: The target f N is a
mixture of N components, and the variance is reduced if the different components
in the mixture are represented with the correct proportions. This idea has received
much attention in the sampling importance sampling context; see Section 3.2.1
below and the references given there. We have not seen this idea in the accept–
reject context. Consider the estimation of
f N(x)ψ(x)dµ(x) =
 ψ(x)a(j,x)b(x)dµ(x)
and ψ is a bounded “test function.” If (Xi) is an i.i.d. sample from f N, the estimator
has variance
N σ 2(ψ) = 1
 (ψ(x) −m(ψ))2a(j,x)b(x)dµ(x)
RECURSIVE MONTE CARLO FILTERS
A method to reduce this variance replaces the random selection of an index J by
a more systematic procedure. Namely, we can propose simultaneously N values,
one each from the density a(j,x), and decide whether to accept each of them
independently. We repeat the procedure until the total of accepted values is at least
N. If we need exactly N values, we can select them at random. We therefore
consider the estimator
j=1 ψ(Xij)1[Uij<b(Xij)]
j=1 1[Uij<b(Xij)]
where (Xij,Uij;1 ≤j ≤N,i = 1,2,...) are independent random variables with
Xij ∼a(j,·), Uij uniform on (0,supb(x)), and T is the smallest integer such that
the denominator is at least N.
In order to compute the variance of ˜m(ψ) approximately, we use
˜m(ψ) −m(ψ) =
j=1(ψ(Xij) −m(ψ))1[Uij<b(Xij)]
j=1 1[Uij<b(Xij)]
For simplicity, we assume that supb(x) = 1. Then, by Wald’s identity the denominator has expected value
In particular, the expected number of random variables that have to be generated is
essentially the same as with basic i.i.d. rejection sampling. Similarly, the numerator
has expectation zero and variance
ψ(X1j) −m(ψ)
1[U1j<b(X1j)]
mj(ψ) −m(ψ)
Assuming the denominator to be approximately constant and equal to N (which is
reasonable if the expected number of accepted values in each round of proposals
is small), we obtain the approximation
E[ ˆm(ψ)] ≈m(ψ),
Var( ˆm(ψ)) ≈1
j (mj(ψ) −m(ψ))2
The second term thus quantiﬁes the gain of the method.
H. R. KÜNSCH
3.2. Sampling importance resampling.
This method generates (zk;1 ≤k ≤R)
according to some proposal ρ and selects from these a sample of size N with inclusion probabilities
j=1 a(j,zk)
The resampling need not be made at random. We will discuss below alternative
methods with reduced variability. The standard proposal is again the prior (9),
leading to the original proposal in .
This method has difﬁculties if the sampling probabilities π(zk) are very unbalanced since this leads to many ties in the ﬁnal sample. Typically, this occurs in
situations where the prior and the likelihood are in conﬂict, that is, when the acceptance rate in rejection sampling is low. Choosing R much bigger than N reduces
the number of ties, but at the expense of longer computations. Note that rejection
sampling is an automatic way of choosing R such that all ties are avoided. There
are also possibilities for reusing the rejected variables for estimating the current
ﬁlter distribution more accurately; see Section 3.3.3 of .
Most of the ideas discussed in connection with rejection sampling can also be
used here. The idea of Pitt and Shephard to include explicitly an index J
was originally developed for this case. It proposes a sample (jk,zk) of size R
with distribution τjρ(j,x) and then selects a sample of size N with inclusion
probabilities
π(zk,jk) ∝b(zk)a(jk,zk)
τjkρ(jk,zk) .
In contrast to rejection sampling, combining ρ(j,·) = a(j,·) with unequal τj’s is
a promising idea here. For instance, we can take τj to be proportional to b(mj),
where mj is the mean or the median of a(j,·). If all a(j,·)’s have a small spread
(relative to the scale at which b varies), then most π(zk,jk)’s will be approximately
equal and therefore R = N is sufﬁcient.
3.2.1. Balanced sampling.
Both in the proposal and in the resampling step,
we have to select indices from a given distribution. In the former case, this distribution is (τk) and in the latter (π(zk)). Balanced sampling is easy to implement
and often can reduce the variance substantially. In the recursive implementation
for ﬁltering, we can combine the resampling step at the end of the current iteration
and the selection of the index at the beginning of the next iteration into a single
selection of indices. In order to keep the notation simple, we discuss the ideas in
the context of the resampling step only. We denote the number of times the index k
is selected by Nk. For random resampling with replacement, these multiplicities
(Nk) are multinomial (N,(π(zk))). Here, we look for more systematic sampling
procedures. We require that Nk ≡N and that sampling is unbiased, that is,
E[Nj|z1,...,zR] = Nπ(zj).
RECURSIVE MONTE CARLO FILTERS
Then the estimator
has the same expected value as the usual importance sampling estimator
ψ(Zj)π(Zj).
Its variance can be written as
Var( ˆm(ψ)) = Var
ψ(Zj)π(Zj)
ψ(Zi)ψ(Zj)CR(i,j)
where CR(i,j) is the conditional covariance of Ni and Nj. The ﬁrst term is the
variance of the usual importance sampling estimator and the second term is the
additional variability due to the resampling step. The advantage of resampling becomes apparent only when we consider several time steps: Without resampling,
the recursive ﬁlter sample would quickly degenerate, that is, practically all the
weights would be given to very few values. Resampling splits the particles with
large weights into several independent ones and kills some of the particles with
very small weights. Nevertheless, we should try to minimize the additional variability introduced by resampling. Because it is not known in advance which functions ψ will be of interest, we consider the supremum over all (bounded) test
functions ψ.
With multinomial Nj’s, we have
ψ(zi)ψ(zj)CR(i,j) = N
ψ(zi)2π(zi) −
ψ(zi)π(zi)
Hence, resampling randomly with replacement can guarantee that the effect of
resampling disappears asymptotically.
Several methods have been proposed which reduce the (conditional) variances
CR(i,i). Residual sampling takes
Ni = [Nπ(zi)] + N′
i) ∼multinomial
N′,(π′(zi))
where [x] denotes the integer part of x and
π′(zi) = Nπ(zi) −[Nπ(zi)]
H. R. KÜNSCH
This reduces 
i,j ψ(zi)ψ(zj)CR(i,j) by the factor N′/N. Intuitively, we expect
the fractional part Nπ(zi) −[Nπ(zi)] to be uniform on (0,1), leading to an average reduction by a factor of two.
The variance CR(k,k) is minimal iff Nk is equal to one of the two integers
closest to Nπ(zk); see . Together with the condition E[Nk] = Nπ(zk), this determines the marginal distribution of Nk. Crisan and Lyons show that then also
the expected relative entropy between the empirical distribution (Nk/N) and the
target (π(zk)) is minimal. There are at least two algorithms such that all Nk’s differ by less than one from Nπ(zk). The following one goes at least back to Whitley
 and has been rediscovered by Carpenter, Clifford and Fearnhead :
∩{1,2,...,N}
where (j1,j2,...,jR) is a random permutation of (1,2,...,R), U is uniform on
[0,1), and the absolute value of a ﬁnite set denotes the number of elements in this
The second algorithm has been proposed by Crisan, Del Moral and Lyons ;
see also . One chooses an arbitrary binary tree with R leaves, labelled as
1,...,R, and one propagates N particles from the root in a speciﬁc way down
the tree. The value of Nj is then the number of particles ending at leaf j. In order
to describe the propagation, we identify a node α with the subset of {1,...,R} that
consists of the leaves connected to α. Furthermore, we denote by Nα the number of
particles that pass through a node α. The expected value of Nα must then be equal
to µα = N 
j∈α πj. The splitting at each node is done such that Nα differs by
less than one from µα and E[Nα] = µα. It is easy to see that this can be achieved:
Each split is either deterministic or chooses between two possibilities with given
probabilities. Decisions at different nodes are made independently.
However, by minimizing CR(k,k), we usually introduce strong dependence
between different Nj’s, and the effects of this are hard to control. Trivially,
|CR(i,j)| ≤1/4, but the bound
ψ(zi)ψ(zj)CR(i,j) ≤N2
contains no useful information because it does not even allow one to conclude that
the additional uncertainty due to resampling disappears asymptotically.
With Whitley’s algorithm , CR(i,j) can be either positive or negative. Since
we know nothing about the sign of ψ(zk), I do not see how one could obtain a
better worst case bound. Still, the following lemma supports the conjecture that,
on average, the algorithm (12) will behave well.
RECURSIVE MONTE CARLO FILTERS
For arbitrary probabilities (πi) and arbitrary N, consider the random variables
∩{1,2,...,N}
where U is uniform on (0,1) [this is the algorithm (12) without the additional
permutation]. Then for any j < k, Cov(Nj,Nk) depends only on rl = Nπj mod1,
ru = Nπk mod1 and rm = N k−1
i=j+1 πi mod1, an explicit expression being given
in the proof. Moreover, the average of this covariance with respect to the uniform
distribution on (0,1) for rm is zero for all values rl and ru.
Because shifting a uniform random variable modulo 1 does not
change the distribution, we may assume that j = 1. Moreover, it is clear that
only the fractional parts rl,rm,ru matter. If we put Mj = Nj −[Nπj] and
Mk = Nk −[Nπk], we obtain therefore
E[MjMk] = P[U ∈(0,rl) ∩(rl + rm −1,rl + rm + ru −1)]
+ P[U ∈(0,rl) ∩(rl + rm −2,rl + rm + ru −2)].
It is easy to evaluate the right-hand side by distinguishing different cases:
(rl + rm + ru −1)+,
(rl + rm ≤1,rm + ru ≤1),
(rl + rm > 1,rm + ru ≤1),
(rl + rm ≤1,rm + ru > 1),
(rl + rm > 1,rm + ru > 1,
rl + rm + ru ≤2),
rl + ru −1,
(rl + rm + ru > 2).
It is also easy to show that, by integrating over rm ∈(0,1), we obtain rlru in all
Although the method is unbiased and has minimal variance without randomizing the order of the values, it seems wise to do so since it is computationally easy
and we expect it to make the values rm approximately uniform.
With the algorithm of Crisan, Del Moral and Lyons we have control over the
sign of CR(j,k).
For the tree-based algorithm, we have, for arbitrary nondecreasing functions f1,...,fR,
E[fj(Nj)].
In particular, the covariances CR(i,j) are negative for i ̸= j.
H. R. KÜNSCH
Denote the two nodes connected directly to the root by α and β. Because the particles are propagated independently in the two subtrees,
fj(Nj)|Nα,Nβ
Furthermore,
fj(Nj)|Nα = [µα]
fj(Nj)|Nα = [µα] + 1
since we can propagate ﬁrst [µα] particles and afterward an additional particle.
Because Nα and Nβ are negatively dependent, we obtain
The proof proceeds now recursively, by considering in the next step each factor
separately and conditioning on the number of particles one level lower.
This lemma implies that the additional variance due to resampling is reduced
by a factor of at least two compared to multinomial sampling:
For the tree-based algorithm described,
ψ(zi)ψ(zj)CR(i,j) ≤1
Write ψ as the difference of positive and negative parts and use
Cauchy–Schwarz for the covariance between positive and negative parts; see
also , page 31.
For later use we formulate and prove the following large deviation inequality:
For the tree-based algorithm,
A⊂{1,...,R}
≤2exp(−4ε2/R).
For any t > 0, we have
(Nj −Nπj) ≥ε
≤exp(−tε)E
RECURSIVE MONTE CARLO FILTERS
By Lemma 3,
t(Nj −Nπj)
Because Nj takes only two values,
t(Nj −Nπj)
 = exp(−trj)
1 −rj + exp(t)rj
where rj = Nπj −[Nπj]. By the standard argument in the proof of Hoeffding’s
inequality, the right-hand side can be bounded by exp(t2/8). Hence,
(Nj −Nπj) ≥ε
≤exp(−tε + |A|t2/8),
which is minimal for t = 4ε/|A|. The probability of a deviation less than or equal
to −ε can be bounded by the same expression. The lemma then follows because
we may assume |A| ≤R/2 (the deviations for A and Ac differ only in sign).
3.3. Accept–reject versus sampling importance resampling.
Generally speaking, the computational effort for rejection sampling is greater than for sampling
importance resampling. By how much depends, however, on the speciﬁc situation. Note that with the auxiliary variable idea of Pitt and Shephard , it is
possible to use the rejection method even in cases where the likelihood b is unbounded, for example, in the stochastic volatility model of Section 3.1.1 with
y = 0. For both methods one needs to ﬁnd proposal densities ρ(j,x) that approximate a(j,x)b(x), but for rejection sampling one needs, in addition, an upper bound
for a(j,x)b(x)/ρ(j,x), which can be difﬁcult in high dimensions.
Usually, a large empirical variance of the inclusion probabilities π(zk) is taken
as an indication that the error of sampling importance resampling is large. However, a low variance does not guarantee a low error. When the true ﬁlter density
is bimodal and if the proposal represents only one mode well, then the inclusion
probabilities are fairly balanced unless the sample size is huge. If we are unable to
compute the modes of the ﬁlter density, then rejection sampling is presumably the
only way to obtain some guarantee for the algorithm in such a case.
The results of the next section allow some theoretical comparison of rejection
and sampling importance resampling methods. We will show in Section 3.1.1 below that rejection sampling has a smaller asymptotic variance than the standard
sampling importance resampling algorithm. Another relevant question is whether
the errors of the methods depend on supx bt(x,yt) (if they do, then it is not clear
how much one gains by an algorithm which does not need a bound on this supremum). For the rejection method, both the exponential bounds in the law of large
numbers and the asymptotic variance do not depend on bt at all as long as the condition (20) is satisﬁed. For sampling importance resampling, our best bound in the
law of large numbers depends on supx bt(x,yt) because of Lemma 9. The bound
on the asymptotic variance does not involve the supremum of bt, but a certain
L2-norm of bt.
H. R. KÜNSCH
3.4. Computation of the likelihood.
Combining (3) and (6), we see that
p(yt|y1: t−1) ≈
N at(xj,t−1,x)bt(x,yt)dµ(x),
which is in the short notation of this section equal to βj/N. If we use τjρ(j,x)
as our proposal, then the usual importance sampling estimator of p(yt|y1: t−1) is
p(yt|y1: t−1) =
b(zk)a(jk,zk)
τjkρ(jk,zk) .
3.5. Monte Carlo backward smoothing.
There is a similar recursive simulation method that generates samples from the conditional distribution of X0: T given
Y1: T = y1: T . At time T , we use the recursive ﬁlter sample xsm
j,T = xj,T . We then
proceed backward in time, using (4) together with an approximation of ft|t. In
order to avoid problems with discreteness, we recommend use of (6) as in ,
instead of replacing ft|t by the empirical distribution of the particles at time t as
in . This means that we generate xsm
j,t from xsm
j,t+1 and (xi,t−1) by simulating
from the density proportional to
at+1(x,xsm
j,t+1)bt(x,yt) 1
at(xi,t−1,x).
[At time t = 0, we will use the density proportional to a1(x,xsm
j,1)a0(x).] Clearly
this has the same structure as (7) and so the same methods as discussed before
apply in principle. However, we need one value from the density (13) for each j
and thus sampling importance resampling does not seem to be useful here. For the
same reason, care is needed when using the mixture index as an auxiliary variable.
Since sampling from (τi) typically involves computing the partial sums of the τi’s,
one should use the same distribution (τi) for all j. Then the computational cost
of the approach is O(T N) and thus at least comparable to a standard MCMC
method. The main disadvantage of this approach is that we have to store all the
ﬁlter samples.
4. Theoretical properties.
In this section we analyze the convergence of the
approximation f N
t|t to the true ﬁltering density ft|t. We will hold the observations
y1: t ﬁxed and drop them from the notation. In particular, we do not make any assumption about how the observations were obtained. The true ﬁltering densities
ft|t are then deterministic, but the approximations f N
t|t are still random since their
computation involves random sampling. All expectations and probabilities in this
section concern the randomness of the Monte Carlo methods, and not the randomness of the state space model. We assume throughout that Xt takes its values in a
complete, separable metric space equiped with the Borel σ-ﬁeld, and we denote
the metric on this state space by d(·,·).
RECURSIVE MONTE CARLO FILTERS
The operator notation for recursive Monte Carlo ﬁlters introduced in Section 2.3
will be used extensively. In addition, we denote by EN(f ) the empirical distribution of a sample of size N from f . Then the approximate ﬁlter density is
t−1|t−1),bt(·,yt)
and by (6) and (5) it has to be compared with
t ft−1|t−1,bt(·,yt)
In the ﬁrst two sections we present two approaches for showing convergence of
t|t to ft|t as N →∞. We measure the error by the L1-distance between densities,
see, for example, , Chapter 1, which can be written in several equivalent forms:
|f (x) −g(x)|dµ(x) = 2
 f (x) −g(x)
|Pf [C] −Pg[C]| = 2
 f (x) −min
f (x),g(x)
(x+ denotes the positive part of x). Clearly, if ∥f N
t|t −ft|t∥1 converges to zero in
probability or almost surely, then for any bounded function ψ on the state space,
the law of large number holds:
ψ(xj,t) −→
ψ(x)ft|t(x)dµ(x)
in probability or almost surely. In the third section we show the corresponding
central limit theorem.
4.1. Stepwise error propagation.
The obvious ﬁrst attempt to show convergence uses the decomposition
t|t −ft|t = B
t−1|t−1),bt
t−1|t−1,bt)
t−1|t−1,bt) −B(A∗
t ft−1|t−1,bt).
The ﬁrst term is the error due to sampling at time t −1 (propagated once) and the
second term is the propagation of the error at time t −1. For a recursive inequality
t|t −ft|t∥1, we have to study the Lipschitz-continuity of Bayes and Markov
operators with respect to the L1-distance and to control the sampling error.
The continuity of Markov operators is well known; see , Section 3.
∥A∗f −A∗g∥1 ≤ρ(A∗)∥f −g∥1,
x,x′ ∥a(x,·) −a(x′,·)∥1 ≤1.
H. R. KÜNSCH
Note that, for a compact state space, the Markov operator is typically contracting.
The continuity of Bayes’ formula with respect to the prior is more problematic.
We have, see , Lemma 3.6(i), the following:
∥B(f,b) −B(g,b)∥1 ≤β(f,b)∥f −g∥1,
 b(x)f (x)dµ(x) ∈
1, supx b(x)
The difﬁculty is that this bound cannot be improved in general. Lemma 3.6(ii)
from shows that the Bayes operator is not contracting for any f , at least for
some “directions” g.
Finally, we have the following bound on sampling errors:
If x →a(x,·) is continuous with respect to the L1-norm, then
under i.i.d. sampling from g,
P[∥A∗EN(g) −A∗g∥1 > ε] N→∞
exponentially fast in N for any ε > 0. The convergence is uniform for all g such
K g dµ ≥1 −ε/6 for some ﬁxed compact set K.
The proof follows closely the arguments in , Chapter 3. We denote
by µN the empirical distribution EN(g) and by µg the distribution g(x)dµ(x).
Let ε > 0 be given. Choose a compact K such that µg(K) ≥1 −ε/6. Next,
choose δ such that ∥a(x,·) −a(x′,·)∥1 ≤ε/6 for all x,x′ ∈K with d(x,x′) ≤δ.
Then choose a partition {B1,...,BJ} of K such that each Bj has diameter at
most δ and choose a point zj in Bj for each j. Finally, put B0 = Kc. Then
µN(Bj)a(zj,·)
1B0(xi)a(xi,x) +
a(xi,x) −a(zj,x)
dµ(x)
|a(xi,x) −a(zj,x)|dµ(x)
≤|µN(B0) −µg(B0)| + ε
RECURSIVE MONTE CARLO FILTERS
Similarly, we obtain
a(x,·)g(x)dµ(x) −
µg(Bj)a(zj,·)
µg(Bj)a(zj,·) −
µN(Bj)a(zj,·)
|µN(Bj) −µg(Bj)|.
Taking these three inequalities together, we obtain
∥A∗EN(g) −A∗g∥1 ≤2ε
|µN(Bj) −µg(Bj)|.
Hence, the large deviation estimate for the multinomial distribution,
|µN(Bj) −µg(Bj)| > ε
≤2J+2 exp(−Nε2/18)
( , Theorem 3.2), implies
P[∥A∗EN(g) −A∗g∥1 > ε] ≤2J+2 exp(−Nε2/18).
From this, the lemma follows (note that, once K is ﬁxed, J depends only on the
transition kernel a and not on g).
THEOREM 1.
If x →at(x,·) is continuous and if for all t, all x and all y,
0 < bt(x,y) ≤C(t,y) < ∞,
then for all t and all y1: t,
t|t −ft|t∥1 −→0
in probability as N →∞.
The proof proceeds by induction on t. For t = 0, there is nothing to
prove because f N
0|0 = f0|0 = a0. From Lemmas 6 and 7, it follows that
t−1|t−1,bt) −B(A∗
t ft−1|t−1,bt)∥1
p(yt|y1: t−1)∥f N
t−1|t−1 −ft−1|t−1∥1 ≤ε
t−1|t−1 −ft−1|t−1∥1 ≤ε p(yt|y1: t−1)
H. R. KÜNSCH
By the induction assumption, there is an N1 such that, for N > N1, (16) holds with
probability at least 1 −η.
In order to bound the ﬁrst term in (15), some care is needed when applying the
bounds provided by Lemmas 7 and 8 with f N
t−1|t−1, which is random. We have to
show that when (16) holds, we can obtain bounds which depend only on ft−1|t−1.
Note ﬁrst that
t−1|t−1(x) −A∗
t ft−1|t−1(x)
2C(t,yt)∥f N
t−1|t−1 −ft−1|t−1∥1.
Hence, if (16) is satisﬁed,
bt(x,yt)A∗
t−1|t−1(x)dµ(x) ≥(1 −ε/2)p(yt|y1: t−1) ≥1
2p(yt|y1: t−1)
and, therefore, by Lemma 7, also
t−1|t−1),bt
t−1|t−1,bt)
p(yt|y1: t−1)∥A∗
t−1|t−1) −A∗
t−1|t−1∥1.
Next we observe that, if K is compact such that
K ft−1|t−1 dµ ≥1 −δ/2 and
if (16) holds, then
t−1|t−1 dµ ≥1 −δ. Therefore, by Lemma 8 we can ﬁnd N2
such that, for N > N2,
t−1|t−1) −A∗
t−1|t−1∥1 ≤6δ
holds with probability at least 1 −η. Collecting all the bounds shows that, for
N > max(N1,N2),
t|t −ft|t∥1 ≤13ε
with probability at least 1 −2η.
The conditions of this theorem are weak. However, the arguments in the proof
require ∥f N
t−1|t−1−ft−1|t−1∥1 to be smaller than ∥f N
t|t −ft|t∥1. This means that the
required sample size N grows with t. It is easy to see that, in general, N has to grow
exponentially with t, and, thus, from a practical point of view, the theorem is not
of great use. Strengthening the assumptions by, for instance, assuming a compact
state space, does not help because by Lemma 3.6(ii) from , the Bayes operator
is expanding. Hence, for a more useful result, we need a different approach which
is provided in the next section.
RECURSIVE MONTE CARLO FILTERS
4.1.1. Sampling errors for sampling importance resampling.
The results so
far have assumed that the Monte Carlo ﬁlter uses i.i.d. samples of f N
t|t , which
means using the accept–reject method (with or without auxiliary variables). It does
not cover sampling importance resampling. In order to extend the results above, we
need to adapt Lemma 8 to the different sampling method.
Let g have the form g = B(h,b) and let (xi,Ni) be a sampling importance resample from g, that is, (xi) is an i.i.d. sample from h and
the Ni’s are the multiplicities in the resampling step which uses probabilities πi ∝b(xi). Assume that x →a(x,·) is continuous for the L1-norm, that
 b(x)h(x)dµ(x) < ∞and that
J⊂{1,2,...,N}
≤c1 exp(−c2Nε2).
P[∥A∗EN(g) −A∗g∥1 > ε] N→∞
exponentially fast in N for any ε > 0.
The assumption of i.i.d. sampling was used in the proof of Lemma 8
only to obtain an exponential bound for
|µN(Bj) −µg(Bj)| > ε
Hence, we have to obtain such a bound by different arguments. By Scheffé’s theorem and Bonferroni’s inequality, we have
|µN(Bj) −µg(Bj)| > ε
|µN(B) −µg(B)| > ε
where the supremum is taken over all sets B in the σ-ﬁeld generated by
(B0,B1,...,BJ). We can decompose via
µN(B) −µg(B)
 b(x)h(x)dµ(x)
b(x)h(x)dµ(x)
i=1 b(xi)1B(xi)
 b(x)h(x)dµ(x)
b(xi)1B(xi) −
B b(x)h(x)dµ(x)
H. R. KÜNSCH
The assumption on the resampling method gives an exponential bound for the
probability that the ﬁrst term is larger than ε/18. Hoeffding’s inequality provides
analogous bounds for the second and third terms.
Applying this lemma with h = N−1 
j a(xj,t−2,·) and b = bt−1(·,yt−1), we
obtain the analogue of Theorem 1. The arguments in the proof of this theorem
show that in this case b(x)/
 b(x)h(x)dµ(x) is bounded.
4.2. Analysis based on considering several steps.
Clearly, we can look at error
propagation over more than one time step. If we deﬁne
Ks,t(f ) = Ks+1,t
s+1f,bs+1)
(s < t), Kt,t(f ) = f,
then, for any s < t, ft|t = Ks,t(fs|s) and, hence,
t|t −ft|t =
r−1|r−1),br
r−1|r−1,br)
+ Ks,t(f N
s|s) −Ks,t(fs|s).
Here the last difference is the error at time s propagated over t −s steps. The other
differences are the errors due to sampling at time r −1, propagated over t −r + 1
This is only useful if we can give a bound on the error propagated over k steps
which is better than the sum over k single steps. It is possible because an alternative
way to get from fs|s to ft|t is to apply ﬁrst the Bayes operator once with likelihood
equal to the conditional density of ys+1: t given xs, followed by t −s Markov operators for the conditional transitions from xr to xr+1 given yr+1: t. The contractivity
of the Markov operators can then beat the expansion of the Bayes operator. It requires, however, a uniform nontrivial upper bound for the contraction coefﬁcient
of the conditional chain given yr+1: t, and for this, we need the following condition: There are probability densities ht and two constants 0 < ca < Ca < ∞such
that, for all x and x′,
ca ht(x) ≤at(x′,x) ≤Ca ht(x).
Condition (19) on at is reasonable when the state space is compact, although it is
slightly stronger than uniform ergodicity. Using (14), we see that the lower bound
of (19) alone implies ρ(A∗
t ) ≤1 −ca and thus also uniform ergodicity. Condition (19) includes even some examples with unbounded state space. For instance,
(19) holds for the model
Xt = g(Xt−1) + Vt
RECURSIVE MONTE CARLO FILTERS
if g is bounded and Vt has a density whose logarithm is uniformly Lipschitz continuous. This is satisﬁed for most heavy-tailed distributions, but not for the Gaussian.
For Gaussian Vt, (19) is false: There is no density ht such that the two bounds in
(19) hold simultaneously. We thus have an example of a uniformly ergodic chain
that we cannot treat with our arguments.
Concerning bt, there is an almost minimal condition, namely,
bt(x,yt)ht(x)dµ(x) < ∞
for all t and all yt. Some arguments become much simpler, however, if we replace (20) by
bt(x′,y) < ∞.
The following lemma shows that, under condition (19), the error propagated
over several steps decreases exponentially. Many versions of this exponential forgetting of the initial conditions of the ﬁlter have appeared in the literature; see,
for example, and the references given there. We use the version of ,
Assume conditions (19) and (20). Then for any two densities f
and g and any s < t we have
∥Ks,t(f ) −Ks,t(g)∥1 ≤1
(1 −γa)t−s∥f −g∥1,
where γa = ca/Ca.
As already mentioned, we write Ks,t as the composition of one Bayes
operator and t −s Markov operators. The likelihood in the Bayes operator is equal
to the conditional density of ys+1: t given xs. It satisﬁes the recursion
p(ys+1: t|xs) =
as+1(xs,xs+1)bs+1(xs+1,ys+1)p(ys+2: t|xs+1)dµ(xs+1).
Hence, by conditions (19) and (20) and an induction argument,
supxs p(ys+1: t|xs)
infxs p(ys+1: t|xs) ≤1
which is, by Lemma 7, the maximal expansion by the Bayes operator. The Markov
operators have transition densities
p(xr|xr−1,yr : t) = ar(xr−1,xr)br(xr,yr)p(yr+1: t|xr)
p(yr : t|xr−1)
H. R. KÜNSCH
which are bounded below by
hr(xr)br(xr,yr)p(yr+1: t|xr)
 hr(xr)br(xr,yr)p(yr+1: t|xr)dµ(xr).
The right-hand side is γa times a density that does not depend on xr−1. Hence, by
Lemma 6 and (14), each Markov operator contracts at least by (1 −γa).
THEOREM 2.
Assume that the transition densities at are the same for all t,
that they are continuous in the L1-norm and satisfy (19), and that (21) holds. Then
to any ε > 0, there are constants c1 and c2 such that, for all t and all N,
t|t −ft|t∥1 > ε] ≤c1 exp(−c2N).
Because at and thus also A∗
t are the same for all t, we drop the time
index during this proof. Let ε > 0 be given. Choose k such that
(1 −γa)k ≤ε.
Assume ﬁrst that k < t. Because the L1-distance between densities is at most 2,
we obtain, in this case from the decomposition (18) with s = t −k and Lemmas 10 and 7,
t|t −ft|t∥1
(1 −γa)t−rB
r−1|r−1),br
r−1|r−1),br
(1 −γa)t−r∥A∗EN(f N
r−1|r−1) −A∗f N
r|r∥1 + ε.
If k > t, we obtain a similar result by considering the decomposition (18) with
s = 0. (Because f N
0|0 = f0|0 = a0, the ε at the end is then absent.) Hence, if
r|r) −A∗f N
holds, then, by the formula for a geometric series,
t|t −ft|t∥1 ≤(Cbγ −2
We are now going to bound the probability that (23) occurs. Note that ε and
thus also k are ﬁxed. Because of Lemma 8, all we need to show is that the set of
RECURSIVE MONTE CARLO FILTERS
distributions (f N
r|r dµ) is tight. By the deﬁnition of f N
r|r and by the conditions (19)
and (21), we have
j=1 a(xj,r−1,x)br(x,yr)
 a(xj,r−1,x)br(x,yr)dµ(x)
≤CbCah(x).
Clearly this implies the desired tightness.
The important feature of the above theorem is that the same N works for all
times t. By Bonferroni’s inequality, we obtain
t|t −ft|t∥1 > ε
≤T c1 exp(−c2N).
Hence, it is sufﬁcient to let N increase logarithmically with the length of the series
to guarantee uniform convergence of the ﬁlter approximation at all time points.
It is not difﬁcult to extend the above theorem to cases where the state transitions
depend on t as long as the continuity is uniform in t.
Condition (21) is used in the proof for bounding
 −B(A∗f N
by applying Lemmas 7 and 8. The following lemma provides a direct way to bound
the above distance by imposing only conditions on a, but assuming a compact state
Let a be a transition density on a compact state space that satis-
ﬁes (19) and
(x′,x) := sup
|a(x,x′′) −a(x′,x′′)|
[d(x,x′) →0]
with the same density h as in (19). Then under i.i.d. sampling from g,
A∗EN(g),b
 −B(A∗g,b)
exponentially fast in N for any ε > 0, uniformly over all densities g and all likelihoods b with 0 <
 h(x′)b(x′)dµ(x′) < ∞.
To make the notation more compact, we introduce
q(x′,x) = a(x′,x)b(x)
a(x,x′)b(x′)dµ(x′).
Then q(x′,x) is again a transition density and we can write
A∗EN(g),b
H. R. KÜNSCH
B(A∗g,b)(x) =
g(x′)β(x′)
 g(x′′)β(x′′)dµ(x′′)q(x′,x)dµ(x′).
The difference between these two expressions can thus be decomposed as
 g(x)β(x)dµ(x)
g(x)β(x)dµ(x)
j=1 β(xj)q(xj,x)
 g(x)β(x)dµ(x)
β(xj)q(xj,x)
g(x′)β(x′)q(x′,x)dµ(x′)
By assumption (19), we have
 g(x)β(x)dµ(x) ≤γ −1
Hence, it follows by Hoeffding’s inequality that
 g(x)β(x)dµ(x) −1
a /(1 −γ 2
Because the L1-norm of 
j β(xj)q(xj,x)/ 
j β(xj) is one, we have the same
bound for the probability that the L1-norm of the ﬁrst term is greater than ε.
Assumption (24) allows us to control the continuity of x →β(x)q(x,·) =
a(x,·)b(·) with respect to the L1-norm:
∥a(x,·)b(·) −a(x′,·)b(·)∥1 ≤(x,x′)
h(x′′)b(x′′)dµ(x′′).
Hence, the same argument as in Lemma 8 can be used to prove an exponential
bound for the probability that the L1-norm of the second term is greater than ε.
By looking at the proof of Theorem 2, this lemma implies immediately the
following:
THEOREM 3.
The claim of Theorem 2 is valid if the state space is compact,
the transition densities do not depend on t and (19), (20) and (24) hold.
RECURSIVE MONTE CARLO FILTERS
4.3. Central limit theorems.
The goal of this section is to show by a simple
induction argument that
ψs(xj,s) −
ψs(x)fs|s(x)dµ(x)
is asymptotically centered normal for any ﬁxed t, any y1: t and functions ψs, 0 ≤
s ≤t, which are square integrable w.r.t. fs|s. Del Moral and Miclo ( , Corollary
20) have obtained a similar result, but we do not assume the ψs’s to be bounded
nor the likelihood bt(·,yt) to be bounded away from zero.
Our argument proceeds by induction on the number t of time steps. For t = 0,
the result is obvious because (xj,0) is an i.i.d. sample from f0|0 = a0. The key idea
for the induction step is to condition on (xj,t−1). We ﬁrst explain the argument
heuristically. Introducing the notation
MN,t(ψ) = 1
t|t (x)dµ(x),
ψ(x)ft|t(x)dµ(x),
we can split
MN,t(ψ) −mt(ψ)
MN,t(ψ) −mN,t(ψ)
mN,t(ψ) −mt(ψ)
We assume that, conditionally on all samples up to time t −1, (xj,t) is an i.i.d.
sample from f N
t|t . Then the ﬁrst term in (25) has the conditional limit distribution
N,t(ψ)), where
 ψ(x) −mN,t(ψ)
t|t (x)dµ(x)
 ψ(x) −mt(ψ)
2ft|t(x)dµ(x)
t|t converges to ft|t. By the recursions for ft|t and f N
t|t , (1)–(2) and (6), respectively,
mN,t(ψ) −mt(ψ)
j Ltψ(xj,t−1)
j Lt1(xj,t−1) −mt−1(Ltψ)
Ltψ(xt−1) =
at(xt−1,xt)bt(xt,yt)ψ(xt)dµ(xt).
H. R. KÜNSCH
Asymptotic normality of the second term of (25) follows therefore from the induction assumption and the delta method.
We now state and prove a rigorous result.
THEOREM 4.
If x →at(x,·) is continuous and if for all t, all x and all y,
0 < bt(x,y) ≤C(t,y) < ∞,
then for all t, all y1: t and all functions ψ with
 ψ(x) −mt(ψ)
2ft|t(x)dµ(x) < ∞,
the recursively deﬁned asymptotic variance
Vt(ψ) = σ 2
p2(yt|y1: t−1)Vt−1
is ﬁnite. Moreover, if σ 2
s (ψs) < ∞for s = 0,1,...,t, then the vector
(MN,s(ψs) −ms(ψs))s=0,...,t converges in distribution to a N (0,(Vr,s(ψr,ψs)))
random vector, where
Vr,t(ψr,ψt) = Vr,t−1
ψt −mt(ψt)
for r < t and Vt,t(ψt,φt) = (Vt(ψt + φt) −Vt(ψt) −Vt(φt))/2.
Using the Cramér–Wold device, it is sufﬁcient to show that
MN,s(ψs) −ms(ψs)
is asymptotically centered normal with variance
Vr,s(ψr,ψs).
For t = 0, the theorem is trivially satisﬁed, and for the induction argument, we
decompose ZN = Z(1)
MN,t(ψt) −mN,t(ψt)
mN,t(ψt) −mt(ψt)
MN,s(ψs) −ms(ψs)
We ﬁrst assume that ψt is bounded. Denoting by Ft the σ-ﬁeld generated by the
(xj,s;1 ≤j ≤N,0 ≤s ≤t), we can write
E[exp(iλZN)] = E
RECURSIVE MONTE CARLO FILTERS
Since conditionally on Ft−1 the xj,t’s are i.i.d., we have
ψt(x1,t) −mN,t(ψt)
Furthermore, by a Taylor expansion of exp(iu),
ψt(x1,t) −mN,t(ψt)
≤|λ|3 sup|ψt(x)|3
Similarly, because 1 −u ≤exp(−u) ≤1 −u + u2 for all u ≥0,
N,t(ψt)/(2N)
 ≤λ4 sup|ψt(x)|4
Because |uN −vN| ≤N|u −v| for |u| ≤1,|v| ≤1, we therefore obtain that, for
converges to zero as N →∞uniformly. By Theorem 1, ∥f N
t|t −ft|t∥1 converges
to zero for N →∞. Because ψt is bounded, this implies that σ 2
N,t(ψt) converges
t (ψt). Therefore,
We now turn to the second term, Z(2)
N . The conditions of the theorem guarantee
mt−1(Lt1) =
ft−1|t−1(xt−1)at(xt−1,xt)bt(xt,yt)dµ(xt−1)dµ(xt)
= p(yt|y1: t−1)
is strictly positive, and Ltψt and Lt1 are easily seen to be bounded if ψt
is bounded. Hence, the conditions for the delta method are satisﬁed, and so
N(mN,t(ψt) −mt(ψt)) is asymptotically equivalent to
Np(yt|y1: t−1)
Ltψt(xj,t−1) −mt−1(Ltψt)
Lt1(xj,t−1) −mt−1(Lt1)
H. R. KÜNSCH
This is equal to
N(MN,t−1(φt−1) −mt−1(φt−1)), where φt−1 = Lt(ψt −
mt(ψt))/p(yt|y1: t−1). Hence, by the induction assumption, E[exp(iλZ(2)
N )] converges to
Vr,s(ψr,ψs)
Vs,t−1(ψs,ψt−1 + φt−1) + Vt−1(ψt−1 + φt−1)
which is equal to exp(−λ2(τ 2 −σ 2
t (ψt))/2) because Vr,t(·,·) is bilinear.
Taking all this together we obtain that, for bounded ψt,
|E[exp(iλZN)] −exp(−λ2τ 2/2)|
−λ2τ 2 −σ 2
converges to zero.
The last part of the proof deals with the case when ψt is unbounded. We show
ﬁrst that σt(ψt) < ∞implies Vt(ψt) < ∞. Again we use induction. For t = 0, this
is clear because σ 2
0 (ψ) = V0(ψ). For the induction step, it is sufﬁcient to show
that σt−1(Lt(ψt −mt(ψt))) < ∞because, by our assumptions, p(yt|y1: t−1) > 0.
By Cauchy–Schwarz, L2
t ψ ≤Lt(ψ2)Lt1, and by our assumption, Lt1 ≤C(t,yt)
is ﬁnite. Hence, by the deﬁnition of Lt and the recursions (1)–(2),
ψt −mt(ψ)
ψt −mt(ψ)
≤C(t,yt)mt−1
ψt −mt(ψ)
= C(t,yt)p(yt|y1: t−1)σ 2
t (ψt) < ∞.
For the asymptotic normality, we use a truncation argument. We set
ψt,c(x) = ψt(x)1{|ψt(x)|≤c},
ψt,c(x) = ψt(x) −ψt,c(x).
Because Vt(ψt) < ∞, it follows by dominated convergence that
Vr,t(ψr,ψt,c) c→∞
−→Vr,t(ψr,ψt).
Next, we are going to show that
N|MN,t(ψt,c) −mt(ψt,c)| ≥ϵ
We ﬁrst condition on Ft−1. By Chebyshev’s inequality,
N|MN,t(ψt,c) −mt(ψt,c)| ≥ϵ|Ft−1
N|mN,t(ψt,c)−mt(ψt,c)|≥ϵ/2} + min
ϵ2 mN,t(ψ2
RECURSIVE MONTE CARLO FILTERS
We therefore have to study the expectations of the two terms on the right. By (26),
mN,t(ψt,c) −mt(ψt,c)
j Ltψt,c(xj,t−1)
j Lt1(xj,t−1)
−mt−1(Ltψt,c)
which by the induction assumption is asymptotically N (0,Vt−1(Lt(ψt,c −
mt(ψt,c))))-distributed. For c →∞, this variance goes to zero, implying the desired behavior of the ﬁrst term. By the recursion for f N
t,c(xj,t−1)
j Lt1(xj,t−1) ,
which, by the induction assumption, converges in probability to
ft|t(x)dµ(x). Hence, by dominated convergence the second term also has the
desired behavior, and, thus, (28) follows.
Now we have all the ingredients to complete the proof. We write
MN,s(ψs) −ms(ψs)
MN,t(ψt,c) −mt(ψt,c)
c for the asymptotic variance of ZN,c. Then
|E[exp(iλZN)] −exp(−λ2τ 2/2)|
≤|E[exp(iλZN,c)] −exp(−λ2τ 2
+ |exp(−λ2τ 2
c /2) −exp(−λ2τ 2/2)|
MN,t(ψt,c) −mt(ψt,c) −MN,t(ψ) + mt(ψ)
By (27), the second term is arbitrarily small if c is large enough. Using
|exp(iu) −1| ≤min(2,|u|) and (28), the same thing holds also for the last term,
uniformly in N. Finally, the ﬁrst term goes to zero for any ﬁxed c as N →∞.
4.3.1. The asymptotic variance.
Similarly as in the case of convergence
t|t , one would like to know whether the asymptotic variances Vt(ψ) stay
bounded as t increases. Using ideas from , we show that this is the case
if ψ is bounded and the condition (19) is satisﬁed. Because mt−1(Ltψ) =
mt(ψ)p(yt|y1: t−1), we have
Hence, by iterating the recursive deﬁnition of Vt(ψ), we obtain
Vt(ψ) = σ 2
s−1(Ls : t(ψ −mt(ψ)))
p2(ys : t|y1: s−1)
H. R. KÜNSCH
Ls : tψ(xs−1) =
ar(xr−1,xr)br(xr,yr)dµ(xr)
= E[ψ(Xt)|xs−1,ys : t]p(ys : t|xs−1).
Here, the expectation is with respect to the state space model and not with respect
to the random sampling in the Monte Carlo ﬁlter. Thus,
E[ψ(Xt)|xs−1,ys : t] −E[ψ(Xt)|y1: t]
p(ys : t|xs−1).
Because p(ys : t|y1: s−1) =
 p(ys : t|xs−1)fs−1|s−1(xs−1|y1: s−1)dµ(xs−1), it follows from (22) that
p(ys : t|xs−1)
p(ys : t|y1: s−1) ≤1
Moreover, condition (19) implies uniform contractivity of the conditional chain
given yt−1
; compare Lemma 10. Hence, under condition (19) we have
E[ψ(Xt)|xs−1,ys : t] −E[ψ(Xt)|y1: t]
x ψ(x) −inf
(1 −γa)t−s+1
and, therefore,
Vt(ψ) ≤γ −3
x ψ(x) −inf
So far, we have dealt with the case where (xj,t) is an i.i.d. sample from f N
t|t , usually generated by an accept–reject method. For sampling importance resampling,
asymptotic normality can be proved by a similar recursive argument; see . However, the formula for the variance Vt changes slightly. Random resampling leads
to the recursion
p2(yt|y1: t−1)Vt−1
p2(yt|y1: t−1)mt−1
(The second term comes from the resampling step and the third from the reweighting.) Using again mt−1(Ltψ) = mt(ψ)p(yt|y1: t−1), we obtain
Vt(ψ) = σ 2
t (ψ) + Vt−1(Lt(ψ −mt(ψ))) −σ 2
t−1(Lt(ψ −mt(ψ)))
p2(yt|y1: t−1)
+ mt(bt(ψ −mt(ψ))2)
p(yt|y1: t−1)
s+1: t(ψ −mt(ψ)))
p(ys|y1: s−1)p2(ys+1: t|y1: s)
RECURSIVE MONTE CARLO FILTERS
[we set Lt+1: t(ψ) = ψ]. Using Cauchy–Schwarz, one can show that each summand in (31) is always greater than or equal to the corresponding term in (29) and,
thus, the additional effort of generating an i.i.d. sampling reduces the variance.
Because of the slightly different form of the asymptotic variance, one also needs
additional conditions in order that Vt(ψ) in (31) remain bounded uniformly in t.
Using the previous bound for (L(s+1): t(ψ −mt(ψ)))/p(ys+1: t|y1: s), one needs,
in addition, a bound for
p(ys|y1: s−1) =
 fs−1|s−1(xs−1)as(xs−1,xs)b2
s (xs,ys)dµ(xs−1)dµ(xs)
 fs−1|s−1(xs−1)as(xs−1,xs)bs(xs,ys)dµ(xs−1)dµ(xs))2 .
Obviously, this is bounded uniformly in s and y under the condition (21). Using
assumption (19), we can replace (21) by a slightly stronger version of (20), namely,
s (xs,ys)dµ(xs) < ∞
for all s and all ys. However, the bound for Vt then depends on y1: t.
4.4. High rate sampling.
So far, we have worked with a ﬁxed sampling rate
which we set equal to one for simplicity. Alternatively, we can consider what
happens when the sampling rate converges to zero. We discuss this case brieﬂy
in this last section. So we let (Xt) be a Markov process in continuous time,
and we assume, for simplicity, that it is time homogeneous with transition kernels P[Xt+s ∈dx|Xt = x′] = a(s,x′,x)dµ(x). We consider the sampling rate
δ = 1/m with m ∈N, and we assume that, for a given m, we have conditionally
independent observations Yjδ,j = 1,2,..., such that Yjδ depends only on Xjδ.
In the previous two subsections we showed how the strong condition (19) allows
one to obtain convergence results that are uniform in t and require essentially no
conditions on the observation densities. Unfortunately, this strategy breaks down
in the high rate sampling limit. In continuous time, the analogue of (19) is
ca(t)h(x) ≤a(t,x′,x) ≤Ca(t)h(x)
for some ﬁxed h and all t,x,x′. It is easy to see that if the lower bound ca(t)
is of larger order than t as t →0, then ∥at(x,·) −at(x′,·)∥1 = 0 for all t > 0.
Hence, except for degenerate cases, the crucial quantity γa diverges at least like
δ−1 for δ →0. Moreover, the continuity module of x →a(δ,x,·) which is used
in Lemmas 8 and 9 also diverges. Because the asymptotic variance Vt(ψ) in (29)
is exact and does not depend on Lemmas 8 and 9, it is slightly easier to study the
behavior of Vt(ψ) as δ →0, and we concentrate on this.
Even this is not trivial. The simplest case occurs if the state space is ﬁnite
and all jump probabilities are positive. Then it is easily seen that (33) holds
with ca(t) = cat, ca > 0 some constant, and Ca(t) ≡Ca. Inserting this into the
H. R. KÜNSCH
bound (30) for the asymptotic variance, we obtain an upper bound for the asymptotic variance of the order m3 which is not satisfactory. Of course, our bounds are
presumably not sharp, but it is not obvious how to improve them in general. We believe that the behavior in the high rate sampling case depends on the properties of
the observation process. If we have a ﬁxed observation density and we increase the
sampling rate, we accumulate more and more information about the state process
in any ﬁxed interval, and the ﬁlter distribution will converge to a point mass except
near the times where a jump occurs. With high rate sampling, it is somewhat more
natural to let the information that is carried by a single observation decrease with
the sampling rate. Then we need additional superscripts for the observations and
their densities. The standard example is
jδ |Xjδ = x ∼N
δg(x),δσ 2,
and we will study this case. Then the partial sum process
converges for m →∞in distribution to the process ηt =
0 g(xs)ds + σWt.
If t is ﬁxed and the sampling rate increases, the formula for Vt contains O(m)
summands. Moreover, with (34), the ﬁlter distributions are not degenerate and the
function xs−δ →E[ψ(Xt)|xs−δ,ys : t] does not converge to a constant. Hence, we
expect that, for ﬁxed t,Vt(ψ) is of the order m. This is not surprising: At each
time step δ we take a new sample even though the ﬁlter distribution changes very
little. The sampling errors accumulate because the ﬁlter does not forget its initial
condition over a ﬁnite time interval. In this setup, it is much better to use sequential
importance sampling, that is, to carry the weights forward by multiplication instead
of resampling at each time step. We thus generate a sample (xj,kδ,k = 0,1,...)
from our model of the state process and compute the weights (m)
1: k(xj,δ : kδ) sequentially, where
i : k(xiδ : kδ) =
b(m)xℓδ,y(m)
is the likelihood. Then
MN,kδ(ψ) =
1: k(xj,δ : kδ)ψ(xj,kδ)
1: k(xj,δ : kδ)
is an asymptotically normal estimator of mkδ(ψ) with asymptotic variance
Vkδ(ψ) = EX[(ψ(Xkδ) −mkδ(ψ))2(m)2
1: k (Xδ : kδ)]
1: k(Xδ : kδ)])2
RECURSIVE MONTE CARLO FILTERS
(EX indicates that the expectation is only with respect to the state variables, the
observations are considered to be ﬁxed). We show ﬁrst that, for a ﬁxed time kδ,
this variance remains bounded as the sampling rate increases to inﬁnity. We assume
the state process to be a diffusion,
dXt = f (Xt)dt + ˜σ(Xt)dBt,
where (Bt) is a Brownian motion.
THEOREM 5.
Consider the state space model (Xjδ,Y (m)
jδ ) deﬁned by
(36) and (34), where f , ˜σ and g, together with their ﬁrst and second derivatives,
are all bounded. Assume, moreover, that the partial sum process (35) converges
in the sup-norm to a continuous function η. Then for δ →0 and kδ →t, t ﬁxed,
the asymptotic variance Vkδ(ψ) of the importance sampling estimator is bounded
by supx ψ2(x) times a constant that depends only on t and the supremum of |η| on
In order to simplify the notation we assume that σ = 1. Moreover, we
b(x,y) = exp
−δg(x)2/2 + g(x)y
(2π)/mexp(my2/2).
Then we can replace the observation density b in the likelihood (m) by b.
Summation by parts gives
1: k(xδ : kδ) = exp
ℓδ + g(xkδ)η(m)
The Itô formula implies
Dg(xt)˜σ(xt)dBt
Dg(xt)f (xt) + 1
2 ˜σ(xt)T D2g(xt)˜σ(xt)
Because f , ˜σ, g, Dg and D2g are all bounded, it follows that
1: k(xδ : kδ) +
Dg(xt)˜σ(xt)dBt
H. R. KÜNSCH
is bounded above and below by constants that depend only on kδ and sup{|η(m)
0 ≤t ≤kδ}. Finally, again by the Itô formula,
Dg(Xt)˜σ(Xt)dBt
Dg(Xt)˜σ(Xt)
which is again bounded above and below by constants that depend only on kδ and
the supremum of |η(m)
|. Using this, the rest of the proof is obvious.
Finally, one can look at the case where both the sampling rate m and the time
index t of the ﬁltering distribution increase. We show that in this situation the
asymptotic variance remains bounded if we resample at ﬁxed time intervals which
for simplicity we take equal to one. Accept–reject methods at a ﬁxed rate cannot
be used in this case because the supremum of (m)
(s−1)m+1: sm(xs−1+δ : s) diverges
as m increases.
By similar arguments as before, the asymptotic variance at time t ∈N of this
version of the particle ﬁlter is
Vt(ψ) = σ 2
(s−1)m+1: sm(Xs−1+δ : s)
(Xs)|xs−1
s−1+δ : t|y(m)
As above, we replace the observation density b in the likelihood (m) by b since
this has no effect on the right-hand side of (38). We deﬁne
j : k(xjδ,xkδ) =
x(j+1)δ : kδ
δ,x(k−1)δ,xkδ
δ,x(i−1)δ,xiδ
We then assume that there exist a probability density h and two functions c and C
+ →R+ such that, for all xjδ,
(j −k)δ,M(m)(jδ,kδ)
j : k(xjδ,xkδ)
(j −k)δ,M(m)(jδ,kδ)
M(m)(s,t) = sup
RECURSIVE MONTE CARLO FILTERS
It follows from the proof of Theorem 3 in that the assumption (39) is satisﬁed in
the case where the state process is a diffusion on a compact Riemannian manifold
with strictly elliptic generator.
Because for j < ℓ< k
j : k(xjδ,xkδ) =
j : ℓ(xjδ,xℓδ)J (m)
ℓ: k (xℓδ,xkδ)dµ(xℓδ),
assumption (39) implies that, for k −j ≥m,
(j+1)δ : kδ|xjδ)
(j+1)δ : kδ|x′
j : k(xjδ,xkδ)dµ(xkδ)
jδ,xkδ)dµ(xkδ)
≤C(1,M(m)(jδ,jδ + 1))
c(1,M(m)(jδ,jδ + 1)) .
Similarly,
xjδ+1|xjδ,y(m)
(j+1)δ : kδ
j : j+m(xjδ,xjδ+1)J (m)
j+m: k(xjδ+1,xkδ)dµ(xkδ)
j : k(xjδ,xkδ)dµ(xkδ)
is bounded below by
c(1,M(m)(jδ,jδ + 1))
C(1,M(m)(jδ,jδ + 1))
j+m: k(xjδ+1,xkδ)dµ(xkδ)
 h(xjδ+1)J (m)
j+m: k(xjδ+1,xkδ)dµ(xjδ+1)dµ(xkδ)
The second ratio on the right-hand side is a probability density which does not
depend on xjδ, and, thus, one minus the ﬁrst ratio is a bound for the contraction
rate of the conditional chain. Therefore, we have
s+δ : t(ψ −mt(ψ))(xs)|
s+δ : t|yδ : s)
≤supx ψ(x) −infx ψ(x)
γ (r) = c(1,M(m)(r,r + 1))
C(1,M(m)(r,r + 1)).
In contrast to the case with ﬁxed sampling rate, these coefﬁcients depend on the
observations, that is, they are random. For nonrandom bounds that hold with high
probability, we would have to assume that the limiting process η has stationary
increments. Finally, we can bound
ms−1(EX[(m)2
(s−1)m+1: sm(Xs−1+δ : s)|xs−1])
s−1+δ : s|y(m)
by similar arguments as before.
Acknowledgments.
I am grateful to Neil Shephard, Eric Moulines and two
referees for helpful comments on earlier versions of this paper. In particular,
I thank Eric Moulines for pointing out to me that Theorem 3 holds.
H. R. KÜNSCH