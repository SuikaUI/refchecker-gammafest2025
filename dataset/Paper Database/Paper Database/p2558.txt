The Annals of Statistics
2005, Vol. 33, No. 5, 1983â€“2021
DOI: 10.1214/009053605000000426
Â© Institute of Mathematical Statistics, 2005
RECURSIVE MONTE CARLO FILTERS: ALGORITHMS
AND THEORETICAL ANALYSIS
BY HANS R. KÃœNSCH
ETH ZÃ¼rich
Recursive Monte Carlo ï¬lters, also called particle ï¬lters, are a powerful tool to perform computations in general state space models. We discuss
and compare the acceptâ€“reject version with the more common sampling importance resampling version of the algorithm. In particular, we show how
auxiliary variable methods and stratiï¬cation can be used in the acceptâ€“reject
version, and we compare different resampling techniques. In a second part,
we show laws of large numbers and a central limit theorem for these Monte
Carlo ï¬lters by simple induction arguments that need only weak conditions.
We also show that, under stronger conditions, the required sample size is independent of the length of the observed series.
1. State space and hidden Markov models.
A general state space or hidden
Markov model consists of an unobserved state sequence (Xt) and an observation
sequence (Yt) with the following properties:
State evolution:
X0,X1,X2,... is a Markov chain with X0 âˆ¼a0(x)dÂµ(x) and
Xt|Xtâˆ’1 = xtâˆ’1 âˆ¼at(xtâˆ’1,x)dÂµ(x).
Generation of observations:
Conditionally on (Xt), the Ytâ€™s are independent
and Yt depends on Xt only with
Yt|Xt = xt âˆ¼bt(xt,y)dÎ½(y).
These models occur in a variety of applications. Linear state space models are
equivalent to ARMA models (see, e.g., ) and have become popular under the
name of structural models (see, e.g., ). Nonlinear state space models occur
in ï¬nance (stochastic volatility; see, e.g., ), in various ï¬elds of engineering
(speech, tracking and control problems; see, e.g., ), in biology (ion channels,
DNA and protein sequences) and in geophysics (rainfall at a network of stations,
data assimilation). A more detailed survey with many references is given in .
In order to apply these models, two kinds of problems have to be solved: Inference about the states based on a stretch of observed values ys : t = (yu,s â‰¤u â‰¤t)
Received January 2003; revised August 2004.
AMS 2000 subject classiï¬cations. Primary 62M09; secondary 60G35, 60J22, 65C05.
Key words and phrases. State space models, hidden Markov models, ï¬ltering and smoothing, particle ï¬lters, auxiliary variables, sampling importance resampling, central limit theorem.
H. R. KÃœNSCH
for a given model, that is, at and bt known (this is called prediction, ï¬ltering and
smoothing), and inference about unknown parameters in at, bt. From a statistical
point of view, the latter problem is maybe of greater interest, but fast and reliable
algorithms for the former are a prerequisite for computing maximum likelihood
or Bayesian estimators. The reason for this is brieï¬‚y mentioned in Section 2.1.
This paper is therefore entirely devoted to algorithms for ï¬ltering, prediction and
smoothing.
Section 2 recalls the basic recursions for ï¬ltering, prediction and smoothing.
Section 3 discusses algorithmic aspects of sequential Monte Carlo methods to implement these recursions. Most algorithms in the literature, beginning with the
pioneering paper by Gordon, Salmond and Smith , use the sampling importance resampling idea of Rubin . An exception is HÃ¼rzeler and KÃ¼nsch 
who use the acceptâ€“reject method instead. Here we show how some ideas like
stratiï¬cation and an auxiliary variable method of Pitt and Shephard can be
adapted to rejection sampling, and we give new results on the performance of systematic resampling methods. In addition, we hope that our view of classifying and
comparing approaches is useful.
Section 4 presents results on the convergence of the method as the number of
Monte Carlo replicates tends to inï¬nity. We discuss both laws of large numbers
and a central limit theorem. Recently, many similar results have been published;
see, for example, . The distinctive features of our presentation here are
the weakness of conditions, the use of the total variation distance to measure the
difference between the approximate and the true ï¬lter density and the simplicity
of the techniques used. We basically show that most results follow by induction,
in accordance with the recursive nature of the algorithm. The complications that
occur are due to a counterintuitive property of Bayesâ€™ formula; see Lemma 3.6(ii)
in . As a consequence, although one can obtain consistency with very few conditions on the model, the required sample size seems to grow exponentially with
the number of time steps. For results that guarantee that the required sample size
is independent of the number of time steps (or grows at most logarithmically), one
has to use induction over several time steps which requires rather strong conditions on the dynamics of the states. At the end, we give some results for the case
where (Xt) is a continuous time process and the sampling rate of the observations
increases.
2. Filtering and smoothing recursions.
In general, we will use the symbol p
as the generic notation for a conditional density of its arguments. However, for the
conditional density of Xt given Y1: s = y1: s, we use the notation ft|s(xt|y1: s). The
three cases s < t, s = t and s > t are called prediction, ï¬ltering and smoothing,
respectively.
RECURSIVE MONTE CARLO FILTERS
The dependence structure of a state space model can be represented by the following directed acyclic graph:
From this, various conditional independence properties follow which are used together with the law of total probability and Bayes theorem to derive recursions
for the ï¬lter, prediction and smoothing densities. These are well known; see, for
example, , Section 3.3, and we state them without proofs.
The most important result is the following recursion for the ï¬lter density:
Propagation.
From the ï¬lter density, we obtain the one-step-ahead prediction
ft|tâˆ’1(xt|y1: tâˆ’1) =
ftâˆ’1|tâˆ’1(x|y1: tâˆ’1)at(x,xt)dÂµ(x).
From the-one-step ahead prediction density, we obtain the ï¬lter density one time step later:
ft|t(xt|y1: t) =
ft|tâˆ’1(xt|y1: tâˆ’1)bt(xt,yt)
 ft|tâˆ’1(x|y1: tâˆ’1)bt(x,yt)dÂµ(x)
âˆft|tâˆ’1(xt|y1: tâˆ’1)bt(xt,yt).
In parts of the literature, for example, in , Yt depends on Xtâˆ’1 and not on Xt.
Then the ï¬lter density is, in our setup, the prediction density which should be kept
in mind when comparing formulae.
2.1. Prediction of observations and likelihood.
The denominator in the update
step (2) is the conditional density of Yt given Y1: tâˆ’1:
p(yt|y1: tâˆ’1) =
ft|tâˆ’1(x|y1: tâˆ’1)bt(x,yt)dÂµ(x).
If ft|tâˆ’1 is available, we thus can obtain the likelihood from
p(y1: T ) =
p(yt|y1: tâˆ’1).
A different representation of the likelihood is obtained by marginalization:
p(y1: T ) =
at(xtâˆ’1,xt)bt(xt,yt)
From this, the likelihood ratio can be expressed as an expectation with respect to
the smoothing distribution; see, for example, .
H. R. KÃœNSCH
2.2. Smoothing.
The ï¬lter densities can also be used for the smoothing problem since conditional on y1: T , (XT ,XT âˆ’1,...,X0) is an inhomogeneous Markov
chain with starting density fT |T and backward transition densities
p(xt|xt+1,y1: T ) = p(xt|xt+1,y1: t) âˆat+1(xt,xt+1)ft|t(xt|y1: t).
This is also the basis for the forward-ï¬lteringâ€“backward-sampling algorithm;
see , equation (20). From (4), we can derive, in particular, a backward recursion for ft|T .
2.3. Recursive ï¬ltering in operator notation.
A compact notation for the ï¬lter
recursion which will be useful later on is
ft|t(Â·|y1: t) = B
t ftâˆ’1|tâˆ’1(Â·|y1: tâˆ’1),bt(Â·,yt)
f (xâ€²)at(xâ€²,x)dÂµ(xâ€²)
is the Markov transition operator, and
B(f,b)(x) =
 f (x)b(x)dÂµ(x)
is the Bayes operator that assigns the posterior to a prior f and a likelihood b.
The operators Aâˆ—
t and B(Â·,b) map the space of densities into itself, but they can be
extended to the space of probability distributions.
2.4. Implementation of recursions.
If Xt is discrete with M possible values, integrals are sums and the recursions need O(T M2) operations. In a linear
Gaussian state space model, all ft|s are Gaussian, and their means and variances
are computed with the Kalman ï¬lter and smoother.
In practically all other cases, the recursions are difï¬cult to compute. Analytical
approximations like the extended Kalman ï¬lter are not satisfactory, and numerical
integration is problematic in high dimensions. Much current interest focuses on
Monte Carlo methods. Standard Markov chain Monte Carlo can be used, but it
lacks a recursive implementation. There has been considerable interest in recursive
Monte Carlo methods in recent years; see, for example, .
3. Algorithms for recursive Monte Carlo ï¬ltering.
The following is the key
observation: Aâˆ—
t f is difï¬cult to compute, but easy to sample from if we can sample
from f and at(x,Â·). This allows us to generate recursively a sequence of samples
(â€œparticlesâ€) (xj,t;j = 1,...,N,t = 0,1,...) with approximate distribution ft|t
as follows: If (xj,tâˆ’1) is available, we can replace
t ftâˆ’1|tâˆ’1(xt|y1: tâˆ’1) =
ftâˆ’1|tâˆ’1(x|y1: tâˆ’1)at(x,xt)dÂµ(x)
RECURSIVE MONTE CARLO FILTERS
at(xj,tâˆ’1,xt).
Therefore, we sample (xj,t) from the distribution with density
t|t (Â·|y1: t) âˆbt(Â·,yt) 1
at(xj,tâˆ’1,Â·).
In this section we discuss methods to sample from such a density. We simplify the
notation somewhat and write the target density as
f N(x) âˆf N
u (x) = b(x)
(subscript u for unnormalized). We will call b the likelihood and Nâˆ’1 
the prior. In the ï¬ltering context, the prior is the approximate prediction density.
For later use, we also introduce
a(j,x)b(x)dÂµ(x),
which is in the ï¬ltering context equal to the conditional density of Yt given
Xtâˆ’1 = xj,tâˆ’1. We assume that we have good methods to generate samples from
a(j,Â·) for any j. The methods we discuss fall into two categories: acceptâ€“reject
and importance sampling with an additional resampling step.
3.1. Acceptâ€“reject methods.
The acceptâ€“reject method for sampling from the
density (7) produces values X according to a proposal Ï, and if X = x accepts it
with probability
Ï€(x) = f N
Here M is an upper bound for the ratio f N
u (x)/Ï(x):
The most obvious proposal Ï(x) is the prior, that is,
Then the evaluation of the acceptance probabilities Ï€(x) is easy as long as b is
bounded. In order to sample from (9), we ï¬rst choose an index J uniformly from
H. R. KÃœNSCH
{1,...,N}, and given J = j, we sample X from a(j,x). Note that, in this case, the
densities a(j,x) need not be available in analytic form; we only have to be able to
sample from them. This is of interest in discretely observed diffusion models.
The average acceptance probability of this algorithm is
 Ï(x)Ï€(x)dÂµ(x) =
j Î²j/M. In particular, if Ï is the prior and if we use the smallest value of M, it
is equal to
N supx b(x).
This is low if the likelihood is more informative (concentrated) than the prior, or
if the likelihood and the prior are in conï¬‚ict. We discuss here some modiï¬cations
and tricks that can alleviate this problem in some situations.
3.1.1. The mixture index as auxiliary variable.
Other proposal distributions
than the prediction density can, of course, lead to higher acceptance rates, but
usually it is difï¬cult to compute a good upper bound M, and the evaluation of the
acceptance probability Ï€(x) is complicated due to the sum over j. A way to avoid
at least the last problem is based on an idea by Pitt and Shephard . Namely,
we can generate ï¬rst an index J according to a distribution (Ï„j) and given J = j,
a variable X according to a density Ï(j,x). We then accept the generated pair
(j,x) with probability
Ï€(j,x) = a(j,x)b(x)
MÏ„jÏ(j,x),
a(j,x)b(x)
Ï„jÏ(j,x) .
If the pair is accepted, we simply discard j and keep x, and otherwise we generate
a new pair. Because the accepted pairs (J,X) have distribution
a(j,x)b(x)
the marginal distribution of X is the target (7). If we take Ï„j = 1/N and Ï(j,x) =
a(j,x), we obtain the usual algorithm discussed before, but one will try to increase
the acceptance rate by other choices.
Because j runs over a ï¬nite set, we will usually take
where Mj â‰¥sup
a(j,x)b(x)
For a given choice of densities Ï(j,x) and bounds Mj, the average acceptance probability is less than or equal to Î²j/ Mj, with equality iff
RECURSIVE MONTE CARLO FILTERS
The average acceptance probability is
Ï€(j,x)Ï„jÏ(j,x)Âµ(dx) = 1
with equality iff Mk/Ï„k is constant.
If Ï(j,x) = a(j,x), the optimal Ï„jâ€™s are thus constant. This is somewhat surprising since one could conjecture that it is better to give higher probability to
those indices j for which the mass of a(j,x) is close to argsupb(x).
The crucial point in implementing this algorithm is the choice of the densities
Ï(j,Â·). Lemma 1 implies that, for a high acceptance probability, all Mjâ€™s should be
small, that is, each Ï(j,x) should be a good proposal distribution for the density
a(j,x)b(x)/Î²j. Ideally, we would choose that density itself. But then Mj must
be close to the normalizing constant Î²j which typically is not available in closed
form. A more practical approach chooses a parametric family (Ï(Î¸,x)), where we
have available tight upper bounds
M(j,Î¸) â‰¥sup
a(j,x)b(x)
We then optimize over Î¸, that is,
Ï(j,x) = Ï(Î¸j,x),
where Î¸j â‰ˆargmin
Note that it is not necessary to ï¬nd the optimal Î¸ exactly, but M(j,Î¸) should
be a true upper bound. By choosing the family (Ï(Î¸,x)) such that it contains all
densities a(j,x), we can make sure that the acceptance probability is at least as
high as with the usual algorithm.
The simplest stochastic volatility model, see, for example, , is
obtained if we take for a(j,Â·) the normal density with mean mj and variance Ïƒ 2
and for b the likelihood of a N (0,exp(x)) random variable Y,
b(x) = b(x,y) = exp
If we choose as Ï(Î¸,Â·) the normal density with mean Î¸ and variance Ïƒ 2, we can
compute the supremum of
log a(j,x)b(x)
2 exp(âˆ’x) âˆ’
H. R. KÃœNSCH
over x. It is equal to
2 Î´2 + mjÎ´ âˆ’
(1 + logy2) +
log(1 + 2Î´),
provided Î´ = (Î¸ âˆ’mj)/Ïƒ 2 â‰¥âˆ’1/2 (otherwise the function is unbounded above).
Minimizing this expression with respect to Î´ subject to Î´ â‰¥âˆ’1/2 leads to a nonlinear equation which has no closed form solution. Using log(1 + 2Î´) â‰¤2Î´, we
obtain a quadratic upper bound which is minimized by
Î¸j = mj + Ïƒ 2
4 + Ïƒ 2 (logy2 âˆ’mj)
This choice of Î¸j may be slightly suboptimal, but because the bound is sharp for
Î¸ = mj, that is, Î´ = 0, we still can guarantee a higher acceptance probability than
with the usual method. In practice, the gain can be dramatic if |y| is small.
The above choice of Î¸j is somewhat different from the suggestion
Î¸j = mj + Ïƒ 2
y2 exp(âˆ’mj) âˆ’1
in , page 285. In addition, the choices for Ï„j differ.
3.1.2. Balanced sampling.
Besides reducing the acceptance rate, we can also
try to reduce the variance by using a more balanced sampling: The target f N is a
mixture of N components, and the variance is reduced if the different components
in the mixture are represented with the correct proportions. This idea has received
much attention in the sampling importance sampling context; see Section 3.2.1
below and the references given there. We have not seen this idea in the acceptâ€“
reject context. Consider the estimation of
f N(x)Ïˆ(x)dÂµ(x) =
 Ïˆ(x)a(j,x)b(x)dÂµ(x)
and Ïˆ is a bounded â€œtest function.â€ If (Xi) is an i.i.d. sample from f N, the estimator
has variance
N Ïƒ 2(Ïˆ) = 1
 (Ïˆ(x) âˆ’m(Ïˆ))2a(j,x)b(x)dÂµ(x)
RECURSIVE MONTE CARLO FILTERS
A method to reduce this variance replaces the random selection of an index J by
a more systematic procedure. Namely, we can propose simultaneously N values,
one each from the density a(j,x), and decide whether to accept each of them
independently. We repeat the procedure until the total of accepted values is at least
N. If we need exactly N values, we can select them at random. We therefore
consider the estimator
j=1 Ïˆ(Xij)1[Uij<b(Xij)]
j=1 1[Uij<b(Xij)]
where (Xij,Uij;1 â‰¤j â‰¤N,i = 1,2,...) are independent random variables with
Xij âˆ¼a(j,Â·), Uij uniform on (0,supb(x)), and T is the smallest integer such that
the denominator is at least N.
In order to compute the variance of Ëœm(Ïˆ) approximately, we use
Ëœm(Ïˆ) âˆ’m(Ïˆ) =
j=1(Ïˆ(Xij) âˆ’m(Ïˆ))1[Uij<b(Xij)]
j=1 1[Uij<b(Xij)]
For simplicity, we assume that supb(x) = 1. Then, by Waldâ€™s identity the denominator has expected value
In particular, the expected number of random variables that have to be generated is
essentially the same as with basic i.i.d. rejection sampling. Similarly, the numerator
has expectation zero and variance
Ïˆ(X1j) âˆ’m(Ïˆ)
1[U1j<b(X1j)]
mj(Ïˆ) âˆ’m(Ïˆ)
Assuming the denominator to be approximately constant and equal to N (which is
reasonable if the expected number of accepted values in each round of proposals
is small), we obtain the approximation
E[ Ë†m(Ïˆ)] â‰ˆm(Ïˆ),
Var( Ë†m(Ïˆ)) â‰ˆ1
j (mj(Ïˆ) âˆ’m(Ïˆ))2
The second term thus quantiï¬es the gain of the method.
H. R. KÃœNSCH
3.2. Sampling importance resampling.
This method generates (zk;1 â‰¤k â‰¤R)
according to some proposal Ï and selects from these a sample of size N with inclusion probabilities
j=1 a(j,zk)
The resampling need not be made at random. We will discuss below alternative
methods with reduced variability. The standard proposal is again the prior (9),
leading to the original proposal in .
This method has difï¬culties if the sampling probabilities Ï€(zk) are very unbalanced since this leads to many ties in the ï¬nal sample. Typically, this occurs in
situations where the prior and the likelihood are in conï¬‚ict, that is, when the acceptance rate in rejection sampling is low. Choosing R much bigger than N reduces
the number of ties, but at the expense of longer computations. Note that rejection
sampling is an automatic way of choosing R such that all ties are avoided. There
are also possibilities for reusing the rejected variables for estimating the current
ï¬lter distribution more accurately; see Section 3.3.3 of .
Most of the ideas discussed in connection with rejection sampling can also be
used here. The idea of Pitt and Shephard to include explicitly an index J
was originally developed for this case. It proposes a sample (jk,zk) of size R
with distribution Ï„jÏ(j,x) and then selects a sample of size N with inclusion
probabilities
Ï€(zk,jk) âˆb(zk)a(jk,zk)
Ï„jkÏ(jk,zk) .
In contrast to rejection sampling, combining Ï(j,Â·) = a(j,Â·) with unequal Ï„jâ€™s is
a promising idea here. For instance, we can take Ï„j to be proportional to b(mj),
where mj is the mean or the median of a(j,Â·). If all a(j,Â·)â€™s have a small spread
(relative to the scale at which b varies), then most Ï€(zk,jk)â€™s will be approximately
equal and therefore R = N is sufï¬cient.
3.2.1. Balanced sampling.
Both in the proposal and in the resampling step,
we have to select indices from a given distribution. In the former case, this distribution is (Ï„k) and in the latter (Ï€(zk)). Balanced sampling is easy to implement
and often can reduce the variance substantially. In the recursive implementation
for ï¬ltering, we can combine the resampling step at the end of the current iteration
and the selection of the index at the beginning of the next iteration into a single
selection of indices. In order to keep the notation simple, we discuss the ideas in
the context of the resampling step only. We denote the number of times the index k
is selected by Nk. For random resampling with replacement, these multiplicities
(Nk) are multinomial (N,(Ï€(zk))). Here, we look for more systematic sampling
procedures. We require that Nk â‰¡N and that sampling is unbiased, that is,
E[Nj|z1,...,zR] = NÏ€(zj).
RECURSIVE MONTE CARLO FILTERS
Then the estimator
has the same expected value as the usual importance sampling estimator
Ïˆ(Zj)Ï€(Zj).
Its variance can be written as
Var( Ë†m(Ïˆ)) = Var
Ïˆ(Zj)Ï€(Zj)
Ïˆ(Zi)Ïˆ(Zj)CR(i,j)
where CR(i,j) is the conditional covariance of Ni and Nj. The ï¬rst term is the
variance of the usual importance sampling estimator and the second term is the
additional variability due to the resampling step. The advantage of resampling becomes apparent only when we consider several time steps: Without resampling,
the recursive ï¬lter sample would quickly degenerate, that is, practically all the
weights would be given to very few values. Resampling splits the particles with
large weights into several independent ones and kills some of the particles with
very small weights. Nevertheless, we should try to minimize the additional variability introduced by resampling. Because it is not known in advance which functions Ïˆ will be of interest, we consider the supremum over all (bounded) test
functions Ïˆ.
With multinomial Njâ€™s, we have
Ïˆ(zi)Ïˆ(zj)CR(i,j) = N
Ïˆ(zi)2Ï€(zi) âˆ’
Ïˆ(zi)Ï€(zi)
Hence, resampling randomly with replacement can guarantee that the effect of
resampling disappears asymptotically.
Several methods have been proposed which reduce the (conditional) variances
CR(i,i). Residual sampling takes
Ni = [NÏ€(zi)] + Nâ€²
i) âˆ¼multinomial
Nâ€²,(Ï€â€²(zi))
where [x] denotes the integer part of x and
Ï€â€²(zi) = NÏ€(zi) âˆ’[NÏ€(zi)]
H. R. KÃœNSCH
This reduces 
i,j Ïˆ(zi)Ïˆ(zj)CR(i,j) by the factor Nâ€²/N. Intuitively, we expect
the fractional part NÏ€(zi) âˆ’[NÏ€(zi)] to be uniform on (0,1), leading to an average reduction by a factor of two.
The variance CR(k,k) is minimal iff Nk is equal to one of the two integers
closest to NÏ€(zk); see . Together with the condition E[Nk] = NÏ€(zk), this determines the marginal distribution of Nk. Crisan and Lyons show that then also
the expected relative entropy between the empirical distribution (Nk/N) and the
target (Ï€(zk)) is minimal. There are at least two algorithms such that all Nkâ€™s differ by less than one from NÏ€(zk). The following one goes at least back to Whitley
 and has been rediscovered by Carpenter, Clifford and Fearnhead :
âˆ©{1,2,...,N}
where (j1,j2,...,jR) is a random permutation of (1,2,...,R), U is uniform on
[0,1), and the absolute value of a ï¬nite set denotes the number of elements in this
The second algorithm has been proposed by Crisan, Del Moral and Lyons ;
see also . One chooses an arbitrary binary tree with R leaves, labelled as
1,...,R, and one propagates N particles from the root in a speciï¬c way down
the tree. The value of Nj is then the number of particles ending at leaf j. In order
to describe the propagation, we identify a node Î± with the subset of {1,...,R} that
consists of the leaves connected to Î±. Furthermore, we denote by NÎ± the number of
particles that pass through a node Î±. The expected value of NÎ± must then be equal
to ÂµÎ± = N 
jâˆˆÎ± Ï€j. The splitting at each node is done such that NÎ± differs by
less than one from ÂµÎ± and E[NÎ±] = ÂµÎ±. It is easy to see that this can be achieved:
Each split is either deterministic or chooses between two possibilities with given
probabilities. Decisions at different nodes are made independently.
However, by minimizing CR(k,k), we usually introduce strong dependence
between different Njâ€™s, and the effects of this are hard to control. Trivially,
|CR(i,j)| â‰¤1/4, but the bound
Ïˆ(zi)Ïˆ(zj)CR(i,j) â‰¤N2
contains no useful information because it does not even allow one to conclude that
the additional uncertainty due to resampling disappears asymptotically.
With Whitleyâ€™s algorithm , CR(i,j) can be either positive or negative. Since
we know nothing about the sign of Ïˆ(zk), I do not see how one could obtain a
better worst case bound. Still, the following lemma supports the conjecture that,
on average, the algorithm (12) will behave well.
RECURSIVE MONTE CARLO FILTERS
For arbitrary probabilities (Ï€i) and arbitrary N, consider the random variables
âˆ©{1,2,...,N}
where U is uniform on (0,1) [this is the algorithm (12) without the additional
permutation]. Then for any j < k, Cov(Nj,Nk) depends only on rl = NÏ€j mod1,
ru = NÏ€k mod1 and rm = N kâˆ’1
i=j+1 Ï€i mod1, an explicit expression being given
in the proof. Moreover, the average of this covariance with respect to the uniform
distribution on (0,1) for rm is zero for all values rl and ru.
Because shifting a uniform random variable modulo 1 does not
change the distribution, we may assume that j = 1. Moreover, it is clear that
only the fractional parts rl,rm,ru matter. If we put Mj = Nj âˆ’[NÏ€j] and
Mk = Nk âˆ’[NÏ€k], we obtain therefore
E[MjMk] = P[U âˆˆ(0,rl) âˆ©(rl + rm âˆ’1,rl + rm + ru âˆ’1)]
+ P[U âˆˆ(0,rl) âˆ©(rl + rm âˆ’2,rl + rm + ru âˆ’2)].
It is easy to evaluate the right-hand side by distinguishing different cases:
(rl + rm + ru âˆ’1)+,
(rl + rm â‰¤1,rm + ru â‰¤1),
(rl + rm > 1,rm + ru â‰¤1),
(rl + rm â‰¤1,rm + ru > 1),
(rl + rm > 1,rm + ru > 1,
rl + rm + ru â‰¤2),
rl + ru âˆ’1,
(rl + rm + ru > 2).
It is also easy to show that, by integrating over rm âˆˆ(0,1), we obtain rlru in all
Although the method is unbiased and has minimal variance without randomizing the order of the values, it seems wise to do so since it is computationally easy
and we expect it to make the values rm approximately uniform.
With the algorithm of Crisan, Del Moral and Lyons we have control over the
sign of CR(j,k).
For the tree-based algorithm, we have, for arbitrary nondecreasing functions f1,...,fR,
E[fj(Nj)].
In particular, the covariances CR(i,j) are negative for i Ì¸= j.
H. R. KÃœNSCH
Denote the two nodes connected directly to the root by Î± and Î². Because the particles are propagated independently in the two subtrees,
fj(Nj)|NÎ±,NÎ²
Furthermore,
fj(Nj)|NÎ± = [ÂµÎ±]
fj(Nj)|NÎ± = [ÂµÎ±] + 1
since we can propagate ï¬rst [ÂµÎ±] particles and afterward an additional particle.
Because NÎ± and NÎ² are negatively dependent, we obtain
The proof proceeds now recursively, by considering in the next step each factor
separately and conditioning on the number of particles one level lower.
This lemma implies that the additional variance due to resampling is reduced
by a factor of at least two compared to multinomial sampling:
For the tree-based algorithm described,
Ïˆ(zi)Ïˆ(zj)CR(i,j) â‰¤1
Write Ïˆ as the difference of positive and negative parts and use
Cauchyâ€“Schwarz for the covariance between positive and negative parts; see
also , page 31.
For later use we formulate and prove the following large deviation inequality:
For the tree-based algorithm,
AâŠ‚{1,...,R}
â‰¤2exp(âˆ’4Îµ2/R).
For any t > 0, we have
(Nj âˆ’NÏ€j) â‰¥Îµ
â‰¤exp(âˆ’tÎµ)E
RECURSIVE MONTE CARLO FILTERS
By Lemma 3,
t(Nj âˆ’NÏ€j)
Because Nj takes only two values,
t(Nj âˆ’NÏ€j)
 = exp(âˆ’trj)
1 âˆ’rj + exp(t)rj
where rj = NÏ€j âˆ’[NÏ€j]. By the standard argument in the proof of Hoeffdingâ€™s
inequality, the right-hand side can be bounded by exp(t2/8). Hence,
(Nj âˆ’NÏ€j) â‰¥Îµ
â‰¤exp(âˆ’tÎµ + |A|t2/8),
which is minimal for t = 4Îµ/|A|. The probability of a deviation less than or equal
to âˆ’Îµ can be bounded by the same expression. The lemma then follows because
we may assume |A| â‰¤R/2 (the deviations for A and Ac differ only in sign).
3.3. Acceptâ€“reject versus sampling importance resampling.
Generally speaking, the computational effort for rejection sampling is greater than for sampling
importance resampling. By how much depends, however, on the speciï¬c situation. Note that with the auxiliary variable idea of Pitt and Shephard , it is
possible to use the rejection method even in cases where the likelihood b is unbounded, for example, in the stochastic volatility model of Section 3.1.1 with
y = 0. For both methods one needs to ï¬nd proposal densities Ï(j,x) that approximate a(j,x)b(x), but for rejection sampling one needs, in addition, an upper bound
for a(j,x)b(x)/Ï(j,x), which can be difï¬cult in high dimensions.
Usually, a large empirical variance of the inclusion probabilities Ï€(zk) is taken
as an indication that the error of sampling importance resampling is large. However, a low variance does not guarantee a low error. When the true ï¬lter density
is bimodal and if the proposal represents only one mode well, then the inclusion
probabilities are fairly balanced unless the sample size is huge. If we are unable to
compute the modes of the ï¬lter density, then rejection sampling is presumably the
only way to obtain some guarantee for the algorithm in such a case.
The results of the next section allow some theoretical comparison of rejection
and sampling importance resampling methods. We will show in Section 3.1.1 below that rejection sampling has a smaller asymptotic variance than the standard
sampling importance resampling algorithm. Another relevant question is whether
the errors of the methods depend on supx bt(x,yt) (if they do, then it is not clear
how much one gains by an algorithm which does not need a bound on this supremum). For the rejection method, both the exponential bounds in the law of large
numbers and the asymptotic variance do not depend on bt at all as long as the condition (20) is satisï¬ed. For sampling importance resampling, our best bound in the
law of large numbers depends on supx bt(x,yt) because of Lemma 9. The bound
on the asymptotic variance does not involve the supremum of bt, but a certain
L2-norm of bt.
H. R. KÃœNSCH
3.4. Computation of the likelihood.
Combining (3) and (6), we see that
p(yt|y1: tâˆ’1) â‰ˆ
N at(xj,tâˆ’1,x)bt(x,yt)dÂµ(x),
which is in the short notation of this section equal to Î²j/N. If we use Ï„jÏ(j,x)
as our proposal, then the usual importance sampling estimator of p(yt|y1: tâˆ’1) is
p(yt|y1: tâˆ’1) =
b(zk)a(jk,zk)
Ï„jkÏ(jk,zk) .
3.5. Monte Carlo backward smoothing.
There is a similar recursive simulation method that generates samples from the conditional distribution of X0: T given
Y1: T = y1: T . At time T , we use the recursive ï¬lter sample xsm
j,T = xj,T . We then
proceed backward in time, using (4) together with an approximation of ft|t. In
order to avoid problems with discreteness, we recommend use of (6) as in ,
instead of replacing ft|t by the empirical distribution of the particles at time t as
in . This means that we generate xsm
j,t from xsm
j,t+1 and (xi,tâˆ’1) by simulating
from the density proportional to
at+1(x,xsm
j,t+1)bt(x,yt) 1
at(xi,tâˆ’1,x).
[At time t = 0, we will use the density proportional to a1(x,xsm
j,1)a0(x).] Clearly
this has the same structure as (7) and so the same methods as discussed before
apply in principle. However, we need one value from the density (13) for each j
and thus sampling importance resampling does not seem to be useful here. For the
same reason, care is needed when using the mixture index as an auxiliary variable.
Since sampling from (Ï„i) typically involves computing the partial sums of the Ï„iâ€™s,
one should use the same distribution (Ï„i) for all j. Then the computational cost
of the approach is O(T N) and thus at least comparable to a standard MCMC
method. The main disadvantage of this approach is that we have to store all the
ï¬lter samples.
4. Theoretical properties.
In this section we analyze the convergence of the
approximation f N
t|t to the true ï¬ltering density ft|t. We will hold the observations
y1: t ï¬xed and drop them from the notation. In particular, we do not make any assumption about how the observations were obtained. The true ï¬ltering densities
ft|t are then deterministic, but the approximations f N
t|t are still random since their
computation involves random sampling. All expectations and probabilities in this
section concern the randomness of the Monte Carlo methods, and not the randomness of the state space model. We assume throughout that Xt takes its values in a
complete, separable metric space equiped with the Borel Ïƒ-ï¬eld, and we denote
the metric on this state space by d(Â·,Â·).
RECURSIVE MONTE CARLO FILTERS
The operator notation for recursive Monte Carlo ï¬lters introduced in Section 2.3
will be used extensively. In addition, we denote by EN(f ) the empirical distribution of a sample of size N from f . Then the approximate ï¬lter density is
tâˆ’1|tâˆ’1),bt(Â·,yt)
and by (6) and (5) it has to be compared with
t ftâˆ’1|tâˆ’1,bt(Â·,yt)
In the ï¬rst two sections we present two approaches for showing convergence of
t|t to ft|t as N â†’âˆ. We measure the error by the L1-distance between densities,
see, for example, , Chapter 1, which can be written in several equivalent forms:
|f (x) âˆ’g(x)|dÂµ(x) = 2
 f (x) âˆ’g(x)
|Pf [C] âˆ’Pg[C]| = 2
 f (x) âˆ’min
f (x),g(x)
(x+ denotes the positive part of x). Clearly, if âˆ¥f N
t|t âˆ’ft|tâˆ¥1 converges to zero in
probability or almost surely, then for any bounded function Ïˆ on the state space,
the law of large number holds:
Ïˆ(xj,t) âˆ’â†’
Ïˆ(x)ft|t(x)dÂµ(x)
in probability or almost surely. In the third section we show the corresponding
central limit theorem.
4.1. Stepwise error propagation.
The obvious ï¬rst attempt to show convergence uses the decomposition
t|t âˆ’ft|t = B
tâˆ’1|tâˆ’1),bt
tâˆ’1|tâˆ’1,bt)
tâˆ’1|tâˆ’1,bt) âˆ’B(Aâˆ—
t ftâˆ’1|tâˆ’1,bt).
The ï¬rst term is the error due to sampling at time t âˆ’1 (propagated once) and the
second term is the propagation of the error at time t âˆ’1. For a recursive inequality
t|t âˆ’ft|tâˆ¥1, we have to study the Lipschitz-continuity of Bayes and Markov
operators with respect to the L1-distance and to control the sampling error.
The continuity of Markov operators is well known; see , Section 3.
âˆ¥Aâˆ—f âˆ’Aâˆ—gâˆ¥1 â‰¤Ï(Aâˆ—)âˆ¥f âˆ’gâˆ¥1,
x,xâ€² âˆ¥a(x,Â·) âˆ’a(xâ€²,Â·)âˆ¥1 â‰¤1.
H. R. KÃœNSCH
Note that, for a compact state space, the Markov operator is typically contracting.
The continuity of Bayesâ€™ formula with respect to the prior is more problematic.
We have, see , Lemma 3.6(i), the following:
âˆ¥B(f,b) âˆ’B(g,b)âˆ¥1 â‰¤Î²(f,b)âˆ¥f âˆ’gâˆ¥1,
 b(x)f (x)dÂµ(x) âˆˆ
1, supx b(x)
The difï¬culty is that this bound cannot be improved in general. Lemma 3.6(ii)
from shows that the Bayes operator is not contracting for any f , at least for
some â€œdirectionsâ€ g.
Finally, we have the following bound on sampling errors:
If x â†’a(x,Â·) is continuous with respect to the L1-norm, then
under i.i.d. sampling from g,
P[âˆ¥Aâˆ—EN(g) âˆ’Aâˆ—gâˆ¥1 > Îµ] Nâ†’âˆ
exponentially fast in N for any Îµ > 0. The convergence is uniform for all g such
K g dÂµ â‰¥1 âˆ’Îµ/6 for some ï¬xed compact set K.
The proof follows closely the arguments in , Chapter 3. We denote
by ÂµN the empirical distribution EN(g) and by Âµg the distribution g(x)dÂµ(x).
Let Îµ > 0 be given. Choose a compact K such that Âµg(K) â‰¥1 âˆ’Îµ/6. Next,
choose Î´ such that âˆ¥a(x,Â·) âˆ’a(xâ€²,Â·)âˆ¥1 â‰¤Îµ/6 for all x,xâ€² âˆˆK with d(x,xâ€²) â‰¤Î´.
Then choose a partition {B1,...,BJ} of K such that each Bj has diameter at
most Î´ and choose a point zj in Bj for each j. Finally, put B0 = Kc. Then
ÂµN(Bj)a(zj,Â·)
1B0(xi)a(xi,x) +
a(xi,x) âˆ’a(zj,x)
dÂµ(x)
|a(xi,x) âˆ’a(zj,x)|dÂµ(x)
â‰¤|ÂµN(B0) âˆ’Âµg(B0)| + Îµ
RECURSIVE MONTE CARLO FILTERS
Similarly, we obtain
a(x,Â·)g(x)dÂµ(x) âˆ’
Âµg(Bj)a(zj,Â·)
Âµg(Bj)a(zj,Â·) âˆ’
ÂµN(Bj)a(zj,Â·)
|ÂµN(Bj) âˆ’Âµg(Bj)|.
Taking these three inequalities together, we obtain
âˆ¥Aâˆ—EN(g) âˆ’Aâˆ—gâˆ¥1 â‰¤2Îµ
|ÂµN(Bj) âˆ’Âµg(Bj)|.
Hence, the large deviation estimate for the multinomial distribution,
|ÂµN(Bj) âˆ’Âµg(Bj)| > Îµ
â‰¤2J+2 exp(âˆ’NÎµ2/18)
( , Theorem 3.2), implies
P[âˆ¥Aâˆ—EN(g) âˆ’Aâˆ—gâˆ¥1 > Îµ] â‰¤2J+2 exp(âˆ’NÎµ2/18).
From this, the lemma follows (note that, once K is ï¬xed, J depends only on the
transition kernel a and not on g).
THEOREM 1.
If x â†’at(x,Â·) is continuous and if for all t, all x and all y,
0 < bt(x,y) â‰¤C(t,y) < âˆ,
then for all t and all y1: t,
t|t âˆ’ft|tâˆ¥1 âˆ’â†’0
in probability as N â†’âˆ.
The proof proceeds by induction on t. For t = 0, there is nothing to
prove because f N
0|0 = f0|0 = a0. From Lemmas 6 and 7, it follows that
tâˆ’1|tâˆ’1,bt) âˆ’B(Aâˆ—
t ftâˆ’1|tâˆ’1,bt)âˆ¥1
p(yt|y1: tâˆ’1)âˆ¥f N
tâˆ’1|tâˆ’1 âˆ’ftâˆ’1|tâˆ’1âˆ¥1 â‰¤Îµ
tâˆ’1|tâˆ’1 âˆ’ftâˆ’1|tâˆ’1âˆ¥1 â‰¤Îµ p(yt|y1: tâˆ’1)
H. R. KÃœNSCH
By the induction assumption, there is an N1 such that, for N > N1, (16) holds with
probability at least 1 âˆ’Î·.
In order to bound the ï¬rst term in (15), some care is needed when applying the
bounds provided by Lemmas 7 and 8 with f N
tâˆ’1|tâˆ’1, which is random. We have to
show that when (16) holds, we can obtain bounds which depend only on ftâˆ’1|tâˆ’1.
Note ï¬rst that
tâˆ’1|tâˆ’1(x) âˆ’Aâˆ—
t ftâˆ’1|tâˆ’1(x)
2C(t,yt)âˆ¥f N
tâˆ’1|tâˆ’1 âˆ’ftâˆ’1|tâˆ’1âˆ¥1.
Hence, if (16) is satisï¬ed,
bt(x,yt)Aâˆ—
tâˆ’1|tâˆ’1(x)dÂµ(x) â‰¥(1 âˆ’Îµ/2)p(yt|y1: tâˆ’1) â‰¥1
2p(yt|y1: tâˆ’1)
and, therefore, by Lemma 7, also
tâˆ’1|tâˆ’1),bt
tâˆ’1|tâˆ’1,bt)
p(yt|y1: tâˆ’1)âˆ¥Aâˆ—
tâˆ’1|tâˆ’1) âˆ’Aâˆ—
tâˆ’1|tâˆ’1âˆ¥1.
Next we observe that, if K is compact such that
K ftâˆ’1|tâˆ’1 dÂµ â‰¥1 âˆ’Î´/2 and
if (16) holds, then
tâˆ’1|tâˆ’1 dÂµ â‰¥1 âˆ’Î´. Therefore, by Lemma 8 we can ï¬nd N2
such that, for N > N2,
tâˆ’1|tâˆ’1) âˆ’Aâˆ—
tâˆ’1|tâˆ’1âˆ¥1 â‰¤6Î´
holds with probability at least 1 âˆ’Î·. Collecting all the bounds shows that, for
N > max(N1,N2),
t|t âˆ’ft|tâˆ¥1 â‰¤13Îµ
with probability at least 1 âˆ’2Î·.
The conditions of this theorem are weak. However, the arguments in the proof
require âˆ¥f N
tâˆ’1|tâˆ’1âˆ’ftâˆ’1|tâˆ’1âˆ¥1 to be smaller than âˆ¥f N
t|t âˆ’ft|tâˆ¥1. This means that the
required sample size N grows with t. It is easy to see that, in general, N has to grow
exponentially with t, and, thus, from a practical point of view, the theorem is not
of great use. Strengthening the assumptions by, for instance, assuming a compact
state space, does not help because by Lemma 3.6(ii) from , the Bayes operator
is expanding. Hence, for a more useful result, we need a different approach which
is provided in the next section.
RECURSIVE MONTE CARLO FILTERS
4.1.1. Sampling errors for sampling importance resampling.
The results so
far have assumed that the Monte Carlo ï¬lter uses i.i.d. samples of f N
t|t , which
means using the acceptâ€“reject method (with or without auxiliary variables). It does
not cover sampling importance resampling. In order to extend the results above, we
need to adapt Lemma 8 to the different sampling method.
Let g have the form g = B(h,b) and let (xi,Ni) be a sampling importance resample from g, that is, (xi) is an i.i.d. sample from h and
the Niâ€™s are the multiplicities in the resampling step which uses probabilities Ï€i âˆb(xi). Assume that x â†’a(x,Â·) is continuous for the L1-norm, that
 b(x)h(x)dÂµ(x) < âˆand that
JâŠ‚{1,2,...,N}
â‰¤c1 exp(âˆ’c2NÎµ2).
P[âˆ¥Aâˆ—EN(g) âˆ’Aâˆ—gâˆ¥1 > Îµ] Nâ†’âˆ
exponentially fast in N for any Îµ > 0.
The assumption of i.i.d. sampling was used in the proof of Lemma 8
only to obtain an exponential bound for
|ÂµN(Bj) âˆ’Âµg(Bj)| > Îµ
Hence, we have to obtain such a bound by different arguments. By ScheffÃ©â€™s theorem and Bonferroniâ€™s inequality, we have
|ÂµN(Bj) âˆ’Âµg(Bj)| > Îµ
|ÂµN(B) âˆ’Âµg(B)| > Îµ
where the supremum is taken over all sets B in the Ïƒ-ï¬eld generated by
(B0,B1,...,BJ). We can decompose via
ÂµN(B) âˆ’Âµg(B)
 b(x)h(x)dÂµ(x)
b(x)h(x)dÂµ(x)
i=1 b(xi)1B(xi)
 b(x)h(x)dÂµ(x)
b(xi)1B(xi) âˆ’
B b(x)h(x)dÂµ(x)
H. R. KÃœNSCH
The assumption on the resampling method gives an exponential bound for the
probability that the ï¬rst term is larger than Îµ/18. Hoeffdingâ€™s inequality provides
analogous bounds for the second and third terms.
Applying this lemma with h = Nâˆ’1 
j a(xj,tâˆ’2,Â·) and b = btâˆ’1(Â·,ytâˆ’1), we
obtain the analogue of Theorem 1. The arguments in the proof of this theorem
show that in this case b(x)/
 b(x)h(x)dÂµ(x) is bounded.
4.2. Analysis based on considering several steps.
Clearly, we can look at error
propagation over more than one time step. If we deï¬ne
Ks,t(f ) = Ks+1,t
s+1f,bs+1)
(s < t), Kt,t(f ) = f,
then, for any s < t, ft|t = Ks,t(fs|s) and, hence,
t|t âˆ’ft|t =
râˆ’1|râˆ’1),br
râˆ’1|râˆ’1,br)
+ Ks,t(f N
s|s) âˆ’Ks,t(fs|s).
Here the last difference is the error at time s propagated over t âˆ’s steps. The other
differences are the errors due to sampling at time r âˆ’1, propagated over t âˆ’r + 1
This is only useful if we can give a bound on the error propagated over k steps
which is better than the sum over k single steps. It is possible because an alternative
way to get from fs|s to ft|t is to apply ï¬rst the Bayes operator once with likelihood
equal to the conditional density of ys+1: t given xs, followed by t âˆ’s Markov operators for the conditional transitions from xr to xr+1 given yr+1: t. The contractivity
of the Markov operators can then beat the expansion of the Bayes operator. It requires, however, a uniform nontrivial upper bound for the contraction coefï¬cient
of the conditional chain given yr+1: t, and for this, we need the following condition: There are probability densities ht and two constants 0 < ca < Ca < âˆsuch
that, for all x and xâ€²,
ca ht(x) â‰¤at(xâ€²,x) â‰¤Ca ht(x).
Condition (19) on at is reasonable when the state space is compact, although it is
slightly stronger than uniform ergodicity. Using (14), we see that the lower bound
of (19) alone implies Ï(Aâˆ—
t ) â‰¤1 âˆ’ca and thus also uniform ergodicity. Condition (19) includes even some examples with unbounded state space. For instance,
(19) holds for the model
Xt = g(Xtâˆ’1) + Vt
RECURSIVE MONTE CARLO FILTERS
if g is bounded and Vt has a density whose logarithm is uniformly Lipschitz continuous. This is satisï¬ed for most heavy-tailed distributions, but not for the Gaussian.
For Gaussian Vt, (19) is false: There is no density ht such that the two bounds in
(19) hold simultaneously. We thus have an example of a uniformly ergodic chain
that we cannot treat with our arguments.
Concerning bt, there is an almost minimal condition, namely,
bt(x,yt)ht(x)dÂµ(x) < âˆ
for all t and all yt. Some arguments become much simpler, however, if we replace (20) by
bt(xâ€²,y) < âˆ.
The following lemma shows that, under condition (19), the error propagated
over several steps decreases exponentially. Many versions of this exponential forgetting of the initial conditions of the ï¬lter have appeared in the literature; see,
for example, and the references given there. We use the version of ,
Assume conditions (19) and (20). Then for any two densities f
and g and any s < t we have
âˆ¥Ks,t(f ) âˆ’Ks,t(g)âˆ¥1 â‰¤1
(1 âˆ’Î³a)tâˆ’sâˆ¥f âˆ’gâˆ¥1,
where Î³a = ca/Ca.
As already mentioned, we write Ks,t as the composition of one Bayes
operator and t âˆ’s Markov operators. The likelihood in the Bayes operator is equal
to the conditional density of ys+1: t given xs. It satisï¬es the recursion
p(ys+1: t|xs) =
as+1(xs,xs+1)bs+1(xs+1,ys+1)p(ys+2: t|xs+1)dÂµ(xs+1).
Hence, by conditions (19) and (20) and an induction argument,
supxs p(ys+1: t|xs)
infxs p(ys+1: t|xs) â‰¤1
which is, by Lemma 7, the maximal expansion by the Bayes operator. The Markov
operators have transition densities
p(xr|xrâˆ’1,yr : t) = ar(xrâˆ’1,xr)br(xr,yr)p(yr+1: t|xr)
p(yr : t|xrâˆ’1)
H. R. KÃœNSCH
which are bounded below by
hr(xr)br(xr,yr)p(yr+1: t|xr)
 hr(xr)br(xr,yr)p(yr+1: t|xr)dÂµ(xr).
The right-hand side is Î³a times a density that does not depend on xrâˆ’1. Hence, by
Lemma 6 and (14), each Markov operator contracts at least by (1 âˆ’Î³a).
THEOREM 2.
Assume that the transition densities at are the same for all t,
that they are continuous in the L1-norm and satisfy (19), and that (21) holds. Then
to any Îµ > 0, there are constants c1 and c2 such that, for all t and all N,
t|t âˆ’ft|tâˆ¥1 > Îµ] â‰¤c1 exp(âˆ’c2N).
Because at and thus also Aâˆ—
t are the same for all t, we drop the time
index during this proof. Let Îµ > 0 be given. Choose k such that
(1 âˆ’Î³a)k â‰¤Îµ.
Assume ï¬rst that k < t. Because the L1-distance between densities is at most 2,
we obtain, in this case from the decomposition (18) with s = t âˆ’k and Lemmas 10 and 7,
t|t âˆ’ft|tâˆ¥1
(1 âˆ’Î³a)tâˆ’rB
râˆ’1|râˆ’1),br
râˆ’1|râˆ’1),br
(1 âˆ’Î³a)tâˆ’râˆ¥Aâˆ—EN(f N
râˆ’1|râˆ’1) âˆ’Aâˆ—f N
r|râˆ¥1 + Îµ.
If k > t, we obtain a similar result by considering the decomposition (18) with
s = 0. (Because f N
0|0 = f0|0 = a0, the Îµ at the end is then absent.) Hence, if
r|r) âˆ’Aâˆ—f N
holds, then, by the formula for a geometric series,
t|t âˆ’ft|tâˆ¥1 â‰¤(CbÎ³ âˆ’2
We are now going to bound the probability that (23) occurs. Note that Îµ and
thus also k are ï¬xed. Because of Lemma 8, all we need to show is that the set of
RECURSIVE MONTE CARLO FILTERS
distributions (f N
r|r dÂµ) is tight. By the deï¬nition of f N
r|r and by the conditions (19)
and (21), we have
j=1 a(xj,râˆ’1,x)br(x,yr)
 a(xj,râˆ’1,x)br(x,yr)dÂµ(x)
â‰¤CbCah(x).
Clearly this implies the desired tightness.
The important feature of the above theorem is that the same N works for all
times t. By Bonferroniâ€™s inequality, we obtain
t|t âˆ’ft|tâˆ¥1 > Îµ
â‰¤T c1 exp(âˆ’c2N).
Hence, it is sufï¬cient to let N increase logarithmically with the length of the series
to guarantee uniform convergence of the ï¬lter approximation at all time points.
It is not difï¬cult to extend the above theorem to cases where the state transitions
depend on t as long as the continuity is uniform in t.
Condition (21) is used in the proof for bounding
 âˆ’B(Aâˆ—f N
by applying Lemmas 7 and 8. The following lemma provides a direct way to bound
the above distance by imposing only conditions on a, but assuming a compact state
Let a be a transition density on a compact state space that satis-
ï¬es (19) and
(xâ€²,x) := sup
|a(x,xâ€²â€²) âˆ’a(xâ€²,xâ€²â€²)|
[d(x,xâ€²) â†’0]
with the same density h as in (19). Then under i.i.d. sampling from g,
Aâˆ—EN(g),b
 âˆ’B(Aâˆ—g,b)
exponentially fast in N for any Îµ > 0, uniformly over all densities g and all likelihoods b with 0 <
 h(xâ€²)b(xâ€²)dÂµ(xâ€²) < âˆ.
To make the notation more compact, we introduce
q(xâ€²,x) = a(xâ€²,x)b(x)
a(x,xâ€²)b(xâ€²)dÂµ(xâ€²).
Then q(xâ€²,x) is again a transition density and we can write
Aâˆ—EN(g),b
H. R. KÃœNSCH
B(Aâˆ—g,b)(x) =
g(xâ€²)Î²(xâ€²)
 g(xâ€²â€²)Î²(xâ€²â€²)dÂµ(xâ€²â€²)q(xâ€²,x)dÂµ(xâ€²).
The difference between these two expressions can thus be decomposed as
 g(x)Î²(x)dÂµ(x)
g(x)Î²(x)dÂµ(x)
j=1 Î²(xj)q(xj,x)
 g(x)Î²(x)dÂµ(x)
Î²(xj)q(xj,x)
g(xâ€²)Î²(xâ€²)q(xâ€²,x)dÂµ(xâ€²)
By assumption (19), we have
 g(x)Î²(x)dÂµ(x) â‰¤Î³ âˆ’1
Hence, it follows by Hoeffdingâ€™s inequality that
 g(x)Î²(x)dÂµ(x) âˆ’1
a /(1 âˆ’Î³ 2
Because the L1-norm of 
j Î²(xj)q(xj,x)/ 
j Î²(xj) is one, we have the same
bound for the probability that the L1-norm of the ï¬rst term is greater than Îµ.
Assumption (24) allows us to control the continuity of x â†’Î²(x)q(x,Â·) =
a(x,Â·)b(Â·) with respect to the L1-norm:
âˆ¥a(x,Â·)b(Â·) âˆ’a(xâ€²,Â·)b(Â·)âˆ¥1 â‰¤(x,xâ€²)
h(xâ€²â€²)b(xâ€²â€²)dÂµ(xâ€²â€²).
Hence, the same argument as in Lemma 8 can be used to prove an exponential
bound for the probability that the L1-norm of the second term is greater than Îµ.
By looking at the proof of Theorem 2, this lemma implies immediately the
following:
THEOREM 3.
The claim of Theorem 2 is valid if the state space is compact,
the transition densities do not depend on t and (19), (20) and (24) hold.
RECURSIVE MONTE CARLO FILTERS
4.3. Central limit theorems.
The goal of this section is to show by a simple
induction argument that
Ïˆs(xj,s) âˆ’
Ïˆs(x)fs|s(x)dÂµ(x)
is asymptotically centered normal for any ï¬xed t, any y1: t and functions Ïˆs, 0 â‰¤
s â‰¤t, which are square integrable w.r.t. fs|s. Del Moral and Miclo ( , Corollary
20) have obtained a similar result, but we do not assume the Ïˆsâ€™s to be bounded
nor the likelihood bt(Â·,yt) to be bounded away from zero.
Our argument proceeds by induction on the number t of time steps. For t = 0,
the result is obvious because (xj,0) is an i.i.d. sample from f0|0 = a0. The key idea
for the induction step is to condition on (xj,tâˆ’1). We ï¬rst explain the argument
heuristically. Introducing the notation
MN,t(Ïˆ) = 1
t|t (x)dÂµ(x),
Ïˆ(x)ft|t(x)dÂµ(x),
we can split
MN,t(Ïˆ) âˆ’mt(Ïˆ)
MN,t(Ïˆ) âˆ’mN,t(Ïˆ)
mN,t(Ïˆ) âˆ’mt(Ïˆ)
We assume that, conditionally on all samples up to time t âˆ’1, (xj,t) is an i.i.d.
sample from f N
t|t . Then the ï¬rst term in (25) has the conditional limit distribution
N,t(Ïˆ)), where
 Ïˆ(x) âˆ’mN,t(Ïˆ)
t|t (x)dÂµ(x)
 Ïˆ(x) âˆ’mt(Ïˆ)
2ft|t(x)dÂµ(x)
t|t converges to ft|t. By the recursions for ft|t and f N
t|t , (1)â€“(2) and (6), respectively,
mN,t(Ïˆ) âˆ’mt(Ïˆ)
j LtÏˆ(xj,tâˆ’1)
j Lt1(xj,tâˆ’1) âˆ’mtâˆ’1(LtÏˆ)
LtÏˆ(xtâˆ’1) =
at(xtâˆ’1,xt)bt(xt,yt)Ïˆ(xt)dÂµ(xt).
H. R. KÃœNSCH
Asymptotic normality of the second term of (25) follows therefore from the induction assumption and the delta method.
We now state and prove a rigorous result.
THEOREM 4.
If x â†’at(x,Â·) is continuous and if for all t, all x and all y,
0 < bt(x,y) â‰¤C(t,y) < âˆ,
then for all t, all y1: t and all functions Ïˆ with
 Ïˆ(x) âˆ’mt(Ïˆ)
2ft|t(x)dÂµ(x) < âˆ,
the recursively deï¬ned asymptotic variance
Vt(Ïˆ) = Ïƒ 2
p2(yt|y1: tâˆ’1)Vtâˆ’1
is ï¬nite. Moreover, if Ïƒ 2
s (Ïˆs) < âˆfor s = 0,1,...,t, then the vector
(MN,s(Ïˆs) âˆ’ms(Ïˆs))s=0,...,t converges in distribution to a N (0,(Vr,s(Ïˆr,Ïˆs)))
random vector, where
Vr,t(Ïˆr,Ïˆt) = Vr,tâˆ’1
Ïˆt âˆ’mt(Ïˆt)
for r < t and Vt,t(Ïˆt,Ï†t) = (Vt(Ïˆt + Ï†t) âˆ’Vt(Ïˆt) âˆ’Vt(Ï†t))/2.
Using the CramÃ©râ€“Wold device, it is sufï¬cient to show that
MN,s(Ïˆs) âˆ’ms(Ïˆs)
is asymptotically centered normal with variance
Vr,s(Ïˆr,Ïˆs).
For t = 0, the theorem is trivially satisï¬ed, and for the induction argument, we
decompose ZN = Z(1)
MN,t(Ïˆt) âˆ’mN,t(Ïˆt)
mN,t(Ïˆt) âˆ’mt(Ïˆt)
MN,s(Ïˆs) âˆ’ms(Ïˆs)
We ï¬rst assume that Ïˆt is bounded. Denoting by Ft the Ïƒ-ï¬eld generated by the
(xj,s;1 â‰¤j â‰¤N,0 â‰¤s â‰¤t), we can write
E[exp(iÎ»ZN)] = E
RECURSIVE MONTE CARLO FILTERS
Since conditionally on Ftâˆ’1 the xj,tâ€™s are i.i.d., we have
Ïˆt(x1,t) âˆ’mN,t(Ïˆt)
Furthermore, by a Taylor expansion of exp(iu),
Ïˆt(x1,t) âˆ’mN,t(Ïˆt)
â‰¤|Î»|3 sup|Ïˆt(x)|3
Similarly, because 1 âˆ’u â‰¤exp(âˆ’u) â‰¤1 âˆ’u + u2 for all u â‰¥0,
N,t(Ïˆt)/(2N)
 â‰¤Î»4 sup|Ïˆt(x)|4
Because |uN âˆ’vN| â‰¤N|u âˆ’v| for |u| â‰¤1,|v| â‰¤1, we therefore obtain that, for
converges to zero as N â†’âˆuniformly. By Theorem 1, âˆ¥f N
t|t âˆ’ft|tâˆ¥1 converges
to zero for N â†’âˆ. Because Ïˆt is bounded, this implies that Ïƒ 2
N,t(Ïˆt) converges
t (Ïˆt). Therefore,
We now turn to the second term, Z(2)
N . The conditions of the theorem guarantee
mtâˆ’1(Lt1) =
ftâˆ’1|tâˆ’1(xtâˆ’1)at(xtâˆ’1,xt)bt(xt,yt)dÂµ(xtâˆ’1)dÂµ(xt)
= p(yt|y1: tâˆ’1)
is strictly positive, and LtÏˆt and Lt1 are easily seen to be bounded if Ïˆt
is bounded. Hence, the conditions for the delta method are satisï¬ed, and so
N(mN,t(Ïˆt) âˆ’mt(Ïˆt)) is asymptotically equivalent to
Np(yt|y1: tâˆ’1)
LtÏˆt(xj,tâˆ’1) âˆ’mtâˆ’1(LtÏˆt)
Lt1(xj,tâˆ’1) âˆ’mtâˆ’1(Lt1)
H. R. KÃœNSCH
This is equal to
N(MN,tâˆ’1(Ï†tâˆ’1) âˆ’mtâˆ’1(Ï†tâˆ’1)), where Ï†tâˆ’1 = Lt(Ïˆt âˆ’
mt(Ïˆt))/p(yt|y1: tâˆ’1). Hence, by the induction assumption, E[exp(iÎ»Z(2)
N )] converges to
Vr,s(Ïˆr,Ïˆs)
Vs,tâˆ’1(Ïˆs,Ïˆtâˆ’1 + Ï†tâˆ’1) + Vtâˆ’1(Ïˆtâˆ’1 + Ï†tâˆ’1)
which is equal to exp(âˆ’Î»2(Ï„ 2 âˆ’Ïƒ 2
t (Ïˆt))/2) because Vr,t(Â·,Â·) is bilinear.
Taking all this together we obtain that, for bounded Ïˆt,
|E[exp(iÎ»ZN)] âˆ’exp(âˆ’Î»2Ï„ 2/2)|
âˆ’Î»2Ï„ 2 âˆ’Ïƒ 2
converges to zero.
The last part of the proof deals with the case when Ïˆt is unbounded. We show
ï¬rst that Ïƒt(Ïˆt) < âˆimplies Vt(Ïˆt) < âˆ. Again we use induction. For t = 0, this
is clear because Ïƒ 2
0 (Ïˆ) = V0(Ïˆ). For the induction step, it is sufï¬cient to show
that Ïƒtâˆ’1(Lt(Ïˆt âˆ’mt(Ïˆt))) < âˆbecause, by our assumptions, p(yt|y1: tâˆ’1) > 0.
By Cauchyâ€“Schwarz, L2
t Ïˆ â‰¤Lt(Ïˆ2)Lt1, and by our assumption, Lt1 â‰¤C(t,yt)
is ï¬nite. Hence, by the deï¬nition of Lt and the recursions (1)â€“(2),
Ïˆt âˆ’mt(Ïˆ)
Ïˆt âˆ’mt(Ïˆ)
â‰¤C(t,yt)mtâˆ’1
Ïˆt âˆ’mt(Ïˆ)
= C(t,yt)p(yt|y1: tâˆ’1)Ïƒ 2
t (Ïˆt) < âˆ.
For the asymptotic normality, we use a truncation argument. We set
Ïˆt,c(x) = Ïˆt(x)1{|Ïˆt(x)|â‰¤c},
Ïˆt,c(x) = Ïˆt(x) âˆ’Ïˆt,c(x).
Because Vt(Ïˆt) < âˆ, it follows by dominated convergence that
Vr,t(Ïˆr,Ïˆt,c) câ†’âˆ
âˆ’â†’Vr,t(Ïˆr,Ïˆt).
Next, we are going to show that
N|MN,t(Ïˆt,c) âˆ’mt(Ïˆt,c)| â‰¥Ïµ
We ï¬rst condition on Ftâˆ’1. By Chebyshevâ€™s inequality,
N|MN,t(Ïˆt,c) âˆ’mt(Ïˆt,c)| â‰¥Ïµ|Ftâˆ’1
N|mN,t(Ïˆt,c)âˆ’mt(Ïˆt,c)|â‰¥Ïµ/2} + min
Ïµ2 mN,t(Ïˆ2
RECURSIVE MONTE CARLO FILTERS
We therefore have to study the expectations of the two terms on the right. By (26),
mN,t(Ïˆt,c) âˆ’mt(Ïˆt,c)
j LtÏˆt,c(xj,tâˆ’1)
j Lt1(xj,tâˆ’1)
âˆ’mtâˆ’1(LtÏˆt,c)
which by the induction assumption is asymptotically N (0,Vtâˆ’1(Lt(Ïˆt,c âˆ’
mt(Ïˆt,c))))-distributed. For c â†’âˆ, this variance goes to zero, implying the desired behavior of the ï¬rst term. By the recursion for f N
t,c(xj,tâˆ’1)
j Lt1(xj,tâˆ’1) ,
which, by the induction assumption, converges in probability to
ft|t(x)dÂµ(x). Hence, by dominated convergence the second term also has the
desired behavior, and, thus, (28) follows.
Now we have all the ingredients to complete the proof. We write
MN,s(Ïˆs) âˆ’ms(Ïˆs)
MN,t(Ïˆt,c) âˆ’mt(Ïˆt,c)
c for the asymptotic variance of ZN,c. Then
|E[exp(iÎ»ZN)] âˆ’exp(âˆ’Î»2Ï„ 2/2)|
â‰¤|E[exp(iÎ»ZN,c)] âˆ’exp(âˆ’Î»2Ï„ 2
+ |exp(âˆ’Î»2Ï„ 2
c /2) âˆ’exp(âˆ’Î»2Ï„ 2/2)|
MN,t(Ïˆt,c) âˆ’mt(Ïˆt,c) âˆ’MN,t(Ïˆ) + mt(Ïˆ)
By (27), the second term is arbitrarily small if c is large enough. Using
|exp(iu) âˆ’1| â‰¤min(2,|u|) and (28), the same thing holds also for the last term,
uniformly in N. Finally, the ï¬rst term goes to zero for any ï¬xed c as N â†’âˆ.
4.3.1. The asymptotic variance.
Similarly as in the case of convergence
t|t , one would like to know whether the asymptotic variances Vt(Ïˆ) stay
bounded as t increases. Using ideas from , we show that this is the case
if Ïˆ is bounded and the condition (19) is satisï¬ed. Because mtâˆ’1(LtÏˆ) =
mt(Ïˆ)p(yt|y1: tâˆ’1), we have
Hence, by iterating the recursive deï¬nition of Vt(Ïˆ), we obtain
Vt(Ïˆ) = Ïƒ 2
sâˆ’1(Ls : t(Ïˆ âˆ’mt(Ïˆ)))
p2(ys : t|y1: sâˆ’1)
H. R. KÃœNSCH
Ls : tÏˆ(xsâˆ’1) =
ar(xrâˆ’1,xr)br(xr,yr)dÂµ(xr)
= E[Ïˆ(Xt)|xsâˆ’1,ys : t]p(ys : t|xsâˆ’1).
Here, the expectation is with respect to the state space model and not with respect
to the random sampling in the Monte Carlo ï¬lter. Thus,
E[Ïˆ(Xt)|xsâˆ’1,ys : t] âˆ’E[Ïˆ(Xt)|y1: t]
p(ys : t|xsâˆ’1).
Because p(ys : t|y1: sâˆ’1) =
 p(ys : t|xsâˆ’1)fsâˆ’1|sâˆ’1(xsâˆ’1|y1: sâˆ’1)dÂµ(xsâˆ’1), it follows from (22) that
p(ys : t|xsâˆ’1)
p(ys : t|y1: sâˆ’1) â‰¤1
Moreover, condition (19) implies uniform contractivity of the conditional chain
given ytâˆ’1
; compare Lemma 10. Hence, under condition (19) we have
E[Ïˆ(Xt)|xsâˆ’1,ys : t] âˆ’E[Ïˆ(Xt)|y1: t]
x Ïˆ(x) âˆ’inf
(1 âˆ’Î³a)tâˆ’s+1
and, therefore,
Vt(Ïˆ) â‰¤Î³ âˆ’3
x Ïˆ(x) âˆ’inf
So far, we have dealt with the case where (xj,t) is an i.i.d. sample from f N
t|t , usually generated by an acceptâ€“reject method. For sampling importance resampling,
asymptotic normality can be proved by a similar recursive argument; see . However, the formula for the variance Vt changes slightly. Random resampling leads
to the recursion
p2(yt|y1: tâˆ’1)Vtâˆ’1
p2(yt|y1: tâˆ’1)mtâˆ’1
(The second term comes from the resampling step and the third from the reweighting.) Using again mtâˆ’1(LtÏˆ) = mt(Ïˆ)p(yt|y1: tâˆ’1), we obtain
Vt(Ïˆ) = Ïƒ 2
t (Ïˆ) + Vtâˆ’1(Lt(Ïˆ âˆ’mt(Ïˆ))) âˆ’Ïƒ 2
tâˆ’1(Lt(Ïˆ âˆ’mt(Ïˆ)))
p2(yt|y1: tâˆ’1)
+ mt(bt(Ïˆ âˆ’mt(Ïˆ))2)
p(yt|y1: tâˆ’1)
s+1: t(Ïˆ âˆ’mt(Ïˆ)))
p(ys|y1: sâˆ’1)p2(ys+1: t|y1: s)
RECURSIVE MONTE CARLO FILTERS
[we set Lt+1: t(Ïˆ) = Ïˆ]. Using Cauchyâ€“Schwarz, one can show that each summand in (31) is always greater than or equal to the corresponding term in (29) and,
thus, the additional effort of generating an i.i.d. sampling reduces the variance.
Because of the slightly different form of the asymptotic variance, one also needs
additional conditions in order that Vt(Ïˆ) in (31) remain bounded uniformly in t.
Using the previous bound for (L(s+1): t(Ïˆ âˆ’mt(Ïˆ)))/p(ys+1: t|y1: s), one needs,
in addition, a bound for
p(ys|y1: sâˆ’1) =
 fsâˆ’1|sâˆ’1(xsâˆ’1)as(xsâˆ’1,xs)b2
s (xs,ys)dÂµ(xsâˆ’1)dÂµ(xs)
 fsâˆ’1|sâˆ’1(xsâˆ’1)as(xsâˆ’1,xs)bs(xs,ys)dÂµ(xsâˆ’1)dÂµ(xs))2 .
Obviously, this is bounded uniformly in s and y under the condition (21). Using
assumption (19), we can replace (21) by a slightly stronger version of (20), namely,
s (xs,ys)dÂµ(xs) < âˆ
for all s and all ys. However, the bound for Vt then depends on y1: t.
4.4. High rate sampling.
So far, we have worked with a ï¬xed sampling rate
which we set equal to one for simplicity. Alternatively, we can consider what
happens when the sampling rate converges to zero. We discuss this case brieï¬‚y
in this last section. So we let (Xt) be a Markov process in continuous time,
and we assume, for simplicity, that it is time homogeneous with transition kernels P[Xt+s âˆˆdx|Xt = xâ€²] = a(s,xâ€²,x)dÂµ(x). We consider the sampling rate
Î´ = 1/m with m âˆˆN, and we assume that, for a given m, we have conditionally
independent observations YjÎ´,j = 1,2,..., such that YjÎ´ depends only on XjÎ´.
In the previous two subsections we showed how the strong condition (19) allows
one to obtain convergence results that are uniform in t and require essentially no
conditions on the observation densities. Unfortunately, this strategy breaks down
in the high rate sampling limit. In continuous time, the analogue of (19) is
ca(t)h(x) â‰¤a(t,xâ€²,x) â‰¤Ca(t)h(x)
for some ï¬xed h and all t,x,xâ€². It is easy to see that if the lower bound ca(t)
is of larger order than t as t â†’0, then âˆ¥at(x,Â·) âˆ’at(xâ€²,Â·)âˆ¥1 = 0 for all t > 0.
Hence, except for degenerate cases, the crucial quantity Î³a diverges at least like
Î´âˆ’1 for Î´ â†’0. Moreover, the continuity module of x â†’a(Î´,x,Â·) which is used
in Lemmas 8 and 9 also diverges. Because the asymptotic variance Vt(Ïˆ) in (29)
is exact and does not depend on Lemmas 8 and 9, it is slightly easier to study the
behavior of Vt(Ïˆ) as Î´ â†’0, and we concentrate on this.
Even this is not trivial. The simplest case occurs if the state space is ï¬nite
and all jump probabilities are positive. Then it is easily seen that (33) holds
with ca(t) = cat, ca > 0 some constant, and Ca(t) â‰¡Ca. Inserting this into the
H. R. KÃœNSCH
bound (30) for the asymptotic variance, we obtain an upper bound for the asymptotic variance of the order m3 which is not satisfactory. Of course, our bounds are
presumably not sharp, but it is not obvious how to improve them in general. We believe that the behavior in the high rate sampling case depends on the properties of
the observation process. If we have a ï¬xed observation density and we increase the
sampling rate, we accumulate more and more information about the state process
in any ï¬xed interval, and the ï¬lter distribution will converge to a point mass except
near the times where a jump occurs. With high rate sampling, it is somewhat more
natural to let the information that is carried by a single observation decrease with
the sampling rate. Then we need additional superscripts for the observations and
their densities. The standard example is
jÎ´ |XjÎ´ = x âˆ¼N
Î´g(x),Î´Ïƒ 2,
and we will study this case. Then the partial sum process
converges for m â†’âˆin distribution to the process Î·t =
0 g(xs)ds + ÏƒWt.
If t is ï¬xed and the sampling rate increases, the formula for Vt contains O(m)
summands. Moreover, with (34), the ï¬lter distributions are not degenerate and the
function xsâˆ’Î´ â†’E[Ïˆ(Xt)|xsâˆ’Î´,ys : t] does not converge to a constant. Hence, we
expect that, for ï¬xed t,Vt(Ïˆ) is of the order m. This is not surprising: At each
time step Î´ we take a new sample even though the ï¬lter distribution changes very
little. The sampling errors accumulate because the ï¬lter does not forget its initial
condition over a ï¬nite time interval. In this setup, it is much better to use sequential
importance sampling, that is, to carry the weights forward by multiplication instead
of resampling at each time step. We thus generate a sample (xj,kÎ´,k = 0,1,...)
from our model of the state process and compute the weights (m)
1: k(xj,Î´ : kÎ´) sequentially, where
i : k(xiÎ´ : kÎ´) =
b(m)xâ„“Î´,y(m)
is the likelihood. Then
MN,kÎ´(Ïˆ) =
1: k(xj,Î´ : kÎ´)Ïˆ(xj,kÎ´)
1: k(xj,Î´ : kÎ´)
is an asymptotically normal estimator of mkÎ´(Ïˆ) with asymptotic variance
VkÎ´(Ïˆ) = EX[(Ïˆ(XkÎ´) âˆ’mkÎ´(Ïˆ))2(m)2
1: k (XÎ´ : kÎ´)]
1: k(XÎ´ : kÎ´)])2
RECURSIVE MONTE CARLO FILTERS
(EX indicates that the expectation is only with respect to the state variables, the
observations are considered to be ï¬xed). We show ï¬rst that, for a ï¬xed time kÎ´,
this variance remains bounded as the sampling rate increases to inï¬nity. We assume
the state process to be a diffusion,
dXt = f (Xt)dt + ËœÏƒ(Xt)dBt,
where (Bt) is a Brownian motion.
THEOREM 5.
Consider the state space model (XjÎ´,Y (m)
jÎ´ ) deï¬ned by
(36) and (34), where f , ËœÏƒ and g, together with their ï¬rst and second derivatives,
are all bounded. Assume, moreover, that the partial sum process (35) converges
in the sup-norm to a continuous function Î·. Then for Î´ â†’0 and kÎ´ â†’t, t ï¬xed,
the asymptotic variance VkÎ´(Ïˆ) of the importance sampling estimator is bounded
by supx Ïˆ2(x) times a constant that depends only on t and the supremum of |Î·| on
In order to simplify the notation we assume that Ïƒ = 1. Moreover, we
b(x,y) = exp
âˆ’Î´g(x)2/2 + g(x)y
(2Ï€)/mexp(my2/2).
Then we can replace the observation density b in the likelihood (m) by b.
Summation by parts gives
1: k(xÎ´ : kÎ´) = exp
â„“Î´ + g(xkÎ´)Î·(m)
The ItÃ´ formula implies
Dg(xt)ËœÏƒ(xt)dBt
Dg(xt)f (xt) + 1
2 ËœÏƒ(xt)T D2g(xt)ËœÏƒ(xt)
Because f , ËœÏƒ, g, Dg and D2g are all bounded, it follows that
1: k(xÎ´ : kÎ´) +
Dg(xt)ËœÏƒ(xt)dBt
H. R. KÃœNSCH
is bounded above and below by constants that depend only on kÎ´ and sup{|Î·(m)
0 â‰¤t â‰¤kÎ´}. Finally, again by the ItÃ´ formula,
Dg(Xt)ËœÏƒ(Xt)dBt
Dg(Xt)ËœÏƒ(Xt)
which is again bounded above and below by constants that depend only on kÎ´ and
the supremum of |Î·(m)
|. Using this, the rest of the proof is obvious.
Finally, one can look at the case where both the sampling rate m and the time
index t of the ï¬ltering distribution increase. We show that in this situation the
asymptotic variance remains bounded if we resample at ï¬xed time intervals which
for simplicity we take equal to one. Acceptâ€“reject methods at a ï¬xed rate cannot
be used in this case because the supremum of (m)
(sâˆ’1)m+1: sm(xsâˆ’1+Î´ : s) diverges
as m increases.
By similar arguments as before, the asymptotic variance at time t âˆˆN of this
version of the particle ï¬lter is
Vt(Ïˆ) = Ïƒ 2
(sâˆ’1)m+1: sm(Xsâˆ’1+Î´ : s)
(Xs)|xsâˆ’1
sâˆ’1+Î´ : t|y(m)
As above, we replace the observation density b in the likelihood (m) by b since
this has no effect on the right-hand side of (38). We deï¬ne
j : k(xjÎ´,xkÎ´) =
x(j+1)Î´ : kÎ´
Î´,x(kâˆ’1)Î´,xkÎ´
Î´,x(iâˆ’1)Î´,xiÎ´
We then assume that there exist a probability density h and two functions c and C
+ â†’R+ such that, for all xjÎ´,
(j âˆ’k)Î´,M(m)(jÎ´,kÎ´)
j : k(xjÎ´,xkÎ´)
(j âˆ’k)Î´,M(m)(jÎ´,kÎ´)
M(m)(s,t) = sup
RECURSIVE MONTE CARLO FILTERS
It follows from the proof of Theorem 3 in that the assumption (39) is satisï¬ed in
the case where the state process is a diffusion on a compact Riemannian manifold
with strictly elliptic generator.
Because for j < â„“< k
j : k(xjÎ´,xkÎ´) =
j : â„“(xjÎ´,xâ„“Î´)J (m)
â„“: k (xâ„“Î´,xkÎ´)dÂµ(xâ„“Î´),
assumption (39) implies that, for k âˆ’j â‰¥m,
(j+1)Î´ : kÎ´|xjÎ´)
(j+1)Î´ : kÎ´|xâ€²
j : k(xjÎ´,xkÎ´)dÂµ(xkÎ´)
jÎ´,xkÎ´)dÂµ(xkÎ´)
â‰¤C(1,M(m)(jÎ´,jÎ´ + 1))
c(1,M(m)(jÎ´,jÎ´ + 1)) .
Similarly,
xjÎ´+1|xjÎ´,y(m)
(j+1)Î´ : kÎ´
j : j+m(xjÎ´,xjÎ´+1)J (m)
j+m: k(xjÎ´+1,xkÎ´)dÂµ(xkÎ´)
j : k(xjÎ´,xkÎ´)dÂµ(xkÎ´)
is bounded below by
c(1,M(m)(jÎ´,jÎ´ + 1))
C(1,M(m)(jÎ´,jÎ´ + 1))
j+m: k(xjÎ´+1,xkÎ´)dÂµ(xkÎ´)
 h(xjÎ´+1)J (m)
j+m: k(xjÎ´+1,xkÎ´)dÂµ(xjÎ´+1)dÂµ(xkÎ´)
The second ratio on the right-hand side is a probability density which does not
depend on xjÎ´, and, thus, one minus the ï¬rst ratio is a bound for the contraction
rate of the conditional chain. Therefore, we have
s+Î´ : t(Ïˆ âˆ’mt(Ïˆ))(xs)|
s+Î´ : t|yÎ´ : s)
â‰¤supx Ïˆ(x) âˆ’infx Ïˆ(x)
Î³ (r) = c(1,M(m)(r,r + 1))
C(1,M(m)(r,r + 1)).
In contrast to the case with ï¬xed sampling rate, these coefï¬cients depend on the
observations, that is, they are random. For nonrandom bounds that hold with high
probability, we would have to assume that the limiting process Î· has stationary
increments. Finally, we can bound
msâˆ’1(EX[(m)2
(sâˆ’1)m+1: sm(Xsâˆ’1+Î´ : s)|xsâˆ’1])
sâˆ’1+Î´ : s|y(m)
by similar arguments as before.
Acknowledgments.
I am grateful to Neil Shephard, Eric Moulines and two
referees for helpful comments on earlier versions of this paper. In particular,
I thank Eric Moulines for pointing out to me that Theorem 3 holds.
H. R. KÃœNSCH