Proceedings of the SIGDIAL 2017 Conference, pages 284–288,
Saarbr¨ucken, Germany, 15-17 August 2017. c⃝2017 Association for Computational Linguistics
Adversarial Evaluation for Open-Domain Dialogue Generation
Elia Bruni and Raquel Fern´andez
Institute for Logic, Language and Computation
University of Amsterdam
 
We investigate the potential of adversarial
evaluation methods for open-domain dialogue generation systems, comparing the
performance of a discriminative agent to
that of humans on the same task. Our results show that the task is hard, both for
automated models and humans, but that a
discriminative agent can learn patterns that
lead to above-chance performance.
Introduction
End-to-end dialogue response generation systems
trained to produce a plausible utterance given
some limited dialogue context are receiving increased attention .
However, for systems dealing with chatbot-style
open-dialogue, where task completion is not applicable, evaluating the quality of their responses remains a challenge. Most current models are evaluated with measures such as perplexity and overlapbased metrics like BLEU, that compare the generated response to the ground-truth response in an
actual dialogue. This kind of measures, however,
correlate very weakly or not at all with human
judgements on response quality .
In this paper, we explore a different approach to
evaluating open-domain dialogue response generation systems, inspired by the classic Turing Test
 : measuring the quality of the generated responses on their indistinguishability from
human output. This approach has been preliminary explored in recent work under the heading
of adversarial evaluation , drawing a parallel with
generative adversarial learning . Here we concentrate on exploring the potential and the limits of such an adversarial evaluation approach by conducting an in-depth analysis. We implement a discriminative model and
train it on the task of distinguishing between actual and “fake” dialogue excerpts and evaluate its
performance, as well as the feasibility of the task
more generally, by conducting an experiment with
human judgements. Results show that the task is
hard not only for the discriminative model, but
also for human judges. We then implement a simple chatbot agent for dialogue generation and test
the discriminator on this data, again comparing
its performance to that of humans on this task.
We show that both humans and the discriminative
model can be fooled by the generator in a signiﬁcant amount of cases.
The Discriminative Agent
Our discriminative agent is a binary classiﬁer
which takes as input a sequence of dialogue utterances and predicts whether the dialogue is real
or fake. The agent treats as positive examples of
coherent dialogue actual dialogue passages and as
negative examples passages where the last utterance has been randomly replaced.
Random replacement has been used in the past to study discourse coherence .
The classiﬁer is modelled as an attention-based
bidirectional LSTM. LSTMs are indeed very effective to model word sequences, and are especially suited for learning on data with long distance dependencies such as multi-turn dialogues.
The bidirectional LSTM includes both a forward function
LSTM, which reads the sentence si from wi1
to wiT ) and a backward function (←−−−−
LSTM, which
reads the sentence si from wiT to wi1):
xit = Wewit, t ∈[1, T]
−→h it = −−−−→
LSTM(xit), t ∈[1, T]
←−h it = ←−−−−
LSTM(xit), t ∈[T, 1]
The words of a dialogue turn do not always contribute equally to determine coherence. We thus
use an attention mechanism to extract words that
are important to detect plausibility or coherence
of a dialogue passage and parametrize their aggregation accordingly. Having an aggregated vector
representation which is adaptive to the content of
each time step allows the classiﬁer to assign large
weights to the most “discriminative” words. Contemporarily, the attention should also have an advantage in modelling long sequences by considering different word locations in the dialogue in a
relatively even manner:
uit = tanh(Wwhit + bw)
We ﬁrst compute the hidden representation of hit
through a one-layer MLP uit; we then weight the
importance of uit by computing its similarity to a
word-level context vector, normalized via a softmax function. The context vector is learned endto-end by the classiﬁer and is meant to represent
a general query about the level of “discriminability” of a word . The output of the attention is
then fed to a sigmoid function, which returns the
probability of the input being real or fake:
p = sigmoid has been created from the Movie-Dic corpus of ﬁlm transcripts and contains 3-utterance passages between two interlocutors who alternate in the conversation.
 is made of 2-utterance passages extracted from movie subtitles. To discourage the pairing of utterances coming from different movie scenes, we selected only those pairs
with a maximum difference of 1 second between
the ﬁrst and the second turn. Switchboard is a corpus of transcribed telephone conversations. We ignored utterances that
consist only of non-verbal acts such as laughter,
and selected sequences of three consecutive utterances. In all cases, we consider the last utterance
of a passage the target response, and the previous
utterances, the context. For the three datasets, we
restrict ourselves to dialogue passages where the
context and the response have a length of 3 to 25
tokens each. We concatenated the three datasets,
obtaining a total of 3,289,835 dialogue passages
(46,499 from MovieTriples, 3,211,899 from Sub-
Tle, and 77,936 from Switchboard).
For training, we limit the vocabulary size to
the top 25K most frequent words.1
mini-batch stochastic gradient descent, shufﬂing
the batches each epoch. We use a bidirectional
layer, with 500 cells, and 500-dimensional embeddings (we tried with more layers and higher number of cells without signiﬁcant improvements).
All model parameters are uniformly initialized in
[−0.1, 0.1] and as optimizer we used Adam with
an initial learning rate of 0.001.
Dropout with
probability 0.3 was applied to the LSTMs.
Human Evaluation
To assess the performance of our discriminative
model, we conduct an experiment with human annotators. To our knowledge, this is the ﬁrst study
of its kind ever conducted. Previous human evaluation experiments of dialogue generation systems
have mostly consisted in asking participants to
choose the better response between two options
generated by different models or to rate a generated dialogue along several dimensions .
In contrast, here we present humans with the same
task faced by the discriminator: We show them a
dialogue passage and ask them to decide whether,
given the ﬁrst one or two utterances of context, the
shown continuation is the actual follow-up utterance in the original dialogue or a random response.
The data for this experiment consists of 900 pas-
1All remaining words are converted into the universal token <unk>.
discriminator
Table 1: Accuracy, Precision, Recall, and F-score of discriminator and humans against ground-truth.
Inter-annotator agreement among humans and between the discriminator and the human majority class.
sages: 300 randomly selected per dataset, with
50% real and 50% fake dialogues.
We use the
CrowdFlower platform to recruit annotators, restricting the pool to English native speakers.2
Each item is classiﬁed as real or random by three
different annotators. A total of 137 annotators participated in the experiment, with each of them annotating between 10 and 150 items.
We test the discriminator on the same data
and compare its performance to the human judgements.
Chance level accuracy for both humans
and the discriminator is 50%, namely when real
and fake passages are indistinguishable from each
other. The results are summarised in Table 1. Let
us ﬁrst consider the performance of humans on
the task. We compute inter-annotator agreement
using Fleiss π , suitable for assessing multi-coder annotation tasks.
Agreement is
low: π = 0.30 across the 3 corpora, indicating
that the task is challenging for humans (there is
limited consensus on whether the shown dialogue
passages are plausible or not). Looking into the
human performance with respect to the ground
truth, we see similar accuracy scores for Switchboard and MovieTriples, while accuracy is lower
for SubTle, where the context consists of one utterance only. Across the three datasets, we observe
slightly higher F-score for positive instances (real)
than negative instances (random). For the positive
instances, recall is higher than precision, while the
opposite is true for negative instances. Arguably,
this indicates that humans tend to accommodate
responses that in fact are random as possible coherent continuations of a dialogue, and will only
ﬂag them as fake if they are utterly surprising.
We compute the agreement of the discriminator’s predictions and the human majority class
over 3 annotators. For Switchboard, agreement is
at chance level (π = .07), while for the other two
2We use strict quality controls, only accepting annotators considered “highly trusted” by CrowdFlower (www.
crowdflower.com) and requiring 90% accuracy on socalled “test questions”. Annotators are paid $4 cents per item.
datasets it is on a par with agreement among humans. As for the discriminator’s performance with
respect to the ground truth, not surprisingly we
obtain low accuracy on Switchboard, but slightly
higher accuracy than humans in the other datasets,
in particular SubTle, possibly due to the larger
amount of training data from this corpus. In what
follows, we investigate what information the discriminator may be exploiting to make its predictions.
To inspect the discriminator’s internal representation of the dialogue turns, at testing time we run
two extra forward passes, inputting context and
target separately, and compute the cosine similarity between the respective LSTM hidden states.
We ﬁnd some clear patterns: The context and response of the dialogue passages classiﬁed as coherent by the discriminator (true and false positives) have signiﬁcantly higher cosine similarity
than the passages classiﬁed as fake (true and false
negatives). This holds across the 3 datasets (p <
.001 on a two-sample Wilcoxon rank sum test)
and indicates that the discriminator is exploiting
this information to make its predictions. We also
observe that, while there is a tendency to higher
cosine similarity in the ground-truth positive instances than in the negative ones in Switchboard
(p = .05) and MovieTriples (p = .03), the effect is
highly signiﬁcant in SubTle (p<.001), which is in
line with the higher performance of the discriminator on this corpus. Since accuracy is higher than
humans in this case, presumably the discriminator
is sensitive to patterns that may not be apparent to
humans. Whether this capacity is useful for developing generative models that interact with humans, however, is an open question.
We ﬁnd another interesting pattern within the
attention mass distribution between context and
target: For true and false positives, higher attention is concentrated on the response (≈90%),
Figure 1: Attention visualization.
while for true and false negatives the attention is
more balanced between the two (≈50%). Figure 1
shows three sample dialogue passages with wordlevel attention weights displayed in different color
intensities. The token <s> separates the context
from the target response. The sample at the top
is a passage from SubTle that humans judged to
be incoherent, but that was rightly classiﬁed by
the discriminator as a positive instance (the passage is real). The sample in the middle (a passage
from MovieTriples where the target is random) illustrates how attention weights are more balanced
in negative instances. Finally, the sample at the
bottom shows a passage from MovieTriples rightly
classiﬁed as coherent by human annotators and by
the discriminative agent. As can be seen, attention is more prominent on the target response, with
particular focus on the pronoun ‘she’ whose antecedent ‘her’ in the context also receives some attention mass. In all cases the token </s> receives
high attention, suggesting that the discriminative
agent is keeping track of turn alternations.
Discriminating Generated Responses
We implement a baseline generative agent to test
the extent to which the discriminator’s ability
to distinguish between generated and actual responses is comparable to humans.
The Generator Agent
The generator directly models the conditional
probability p(y|x) of outputting the subsequent dialogue turn y1, ..., ym given some previous context x1, ..., xn. The model consists of a SEQ2SEQ
model, divided into two components: an encoder
which computes a representation for the dialogue
context and a decoder which generates the subsequent dialogue turn one word at a time. A natural
choice for implementing both the encoder and the
decoder is to use an LSTM (see Section 2). The
decoder is also equipped with an attention system.
We train the generator to predict the next dialogue turn given the preceding dialogue history on
the OpenSubtitles dataset . We
considered each line in the dataset as a target to be
predicted by the model and the concatenation of
the two foregoing lines as the source context. We
opt for OpenSubtitles rather than for the cleaner
datasets used for training the discriminative agent,
because the SEQ2SEQ model requires a very large
amount of data to converge, and with more than 80
million triples, OpenSubtitles is one of the largest
dialogue dataset available.
During training, we ﬁltered out passages with
context or target longer than 25 words. We used
mini-batch stochastic gradient descent, shufﬂing
the batches each epoch. We use stacking LSTM
with 2 bidirectional layers, each with 2048 cells,
and 500-dimensional embeddings. All model parameters are uniformly initialized in [−0.1, 0.1];
we train using SGD, with a start learning rate of
1, and after 5 epochs we start halving the learning
rate at each epoch; the mini-batch size is set to 64
and we rescale the normalized gradients whenever
the norm exceeds 5. We also apply dropout with
probability 0.3 on the LSTMs.
We test our discriminative agent on the task of
distinguishing passages with real responses versus generated responses and, as before, compare
its performance to human performance. For this
evaluation, we selected a random sample of 30
generated instances per corpus, avoiding repeated
generated responses and responses with <unk>
tokens since these would make the human judgements trivial. A summary of results is shown in
Table 2. We can see that human accuracy is at
chance level, while the discriminator’s is above
chance, again suggesting that the discriminator
may pick up on patterns that are not discernible to
humans. The higher performance on SubTle may
again be explained by the larger amount of training data from this dataset. We also observe very
low inter-annotator agreement, with even negative
π for the discriminator with respect to humans in
the case of Switchboard.
Conclusions
In this paper, we investigated the use of an adversarial setting for open domain dialogue eval-
discriminator
.194 –.130
Table 2: Performance of discriminator and humans against ground-truth for generator experiment. Interannotator agreement among humans and between the discriminator and the human majority class.
uation, providing novel results on human performance that are informative of the difﬁculty of the
task and the strategies employed to tackle it. We
found that there is limited consensus among human annotators on what counts as a coherent dialogue passages when only 1 or 2 utterances of context are provided, but that nevertheless a discriminative model is able to learn patterns that lead to
above-chance performance.