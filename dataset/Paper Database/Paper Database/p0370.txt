Network: Computation in Neural Systems 5 517-548. Printed in the UK
The statistics of natural images
Daniel L Rudermant
Physiological Laboratory, University of Cambridge, Downing Street, Cambridge CB2 3EG, UK
Received 2 6 . 3 ~ 1 ~
AbstracL Recently there has been a resurgence of interest in the properties of natural images.
Their statistics are important not only in image compression but also far the study of sensory
processing in biology, which can be viewed as satisfying cettain ‘design criteria’. This review
summarizes previous work on image statistics and presents our own data Perhaps the most
notable property of natural images is an invariance to scale. We present data to support this
claim as well 35 evidence for a hierarchical invariance in natural scenes. These symmetries
provide a powerful description of natunl images as they g ~ d y
resttiit the class of allowed
d i s Vib ut ions.
1. Introduction
We can easily distinguish images of the natural world from man-made pictures or those
created randomly by a computer. Natural images are distinctive, because they contain
particular types of structure. They are far from random: images constructed randomly on a
computer practically never contain a naturalistic scene-or even a tree. Natural images are
thus very rare among the huge space of all possible images. How can we make use of this
Image processing systems such as compression algorithms, analogue storage media, and
our own visual system work with these real-world images. Thus to understand the typical
behaviour of these systems we must first study the structure of natural scenes. We can
then address questions like ‘How much compression should we expect to achieve?’ or
‘How often will the playback distortion be above a critical value?’ or ‘How many bits of
information per second does the optic nerve deliver?’
In this review we begin by summarizing what is known about the characteristics of
natural images and add new findings. The study is motivated by practical examples of the
use of such image statistics, primarily toward biological vision. In analysing images we ask
the following questions:
In which ways do natural images differ from random images?
Do.natural image statistics obey q y simple invariances?
e What implications do these statistics have for image processing in biological visual
In the first section we present an overview of statistical methods in signal processing,
with an emphasis on applications to vision. A framework is described for understanding
visual performance in terms of design criteria which involve the statistics of natural scenes.
t E-mail: 
0954-898) , then
the amount of storage space required on average can be reduced. Most practical algorithms
don’t work at the whole image level, but instead consider subimages such as horizontal scan
lines (predictive coding) or 8 x 8 pixel blocks (JPEC). These are then encoded as independent
entities, ignoring their interdependencies. The fact that such procedures are currently saving
an order of magnitude in disk space [l] speaks for the large amount of redundancy and
predictability contained in real images. Most importanfly, it is the statistical structure of
these images which determines the best compression algorithm. Bot at this stage we do not
even know how much compression could be achieved in principle, since the statistics of
real images have not as yet been well characterized.
Central to the discussion is the concept of an image ensemble. We imagine that each
image Z ( r ) has associated with it a-probability of occurencet, P[l(z)], which defines
the ensemble. Images drawn randomly from this distribution will completely represent the
natural environment at hand. Practical questions might relate to the performance ofa device
or algorithm when acting on images drawn from this ensemble.
To give an illustrative example of the statistical formulation, we ask how well a set
of images is represented by a noisy linear encoding. Let us simplify by considering a
onedimensional random signal @ ( x ) (band-limited and widesense stationary with zero
t Strictly speaking. we should invoke a probability density over the space of images (with an associated measure),
since they comprise a continuum of ‘events’.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
ensemble mean) which is convolved with a filter f ( x ) at each point. Random noise given
by q(x) is added to this signal. The final encoding, y(x), may represent the signal recorded
on analogue tape or the responses of an array of neurons. Specifically, we have
Y(4 = (f *4M) +
and we wish to know how well @ ( x ) is represented by y(x). One way to find out is by
reconstructing the ‘best’ estimate of @(x) from the signal y(x)t.
We define the best estimate, &&),
to be the one with the minimum mean-squared
error, that is
is minimized, where the expectation value is over all signals and noise. It is well known
that this estimator is given by the mean of the posterior distribution[651, P[@lyl:
@e&) = p#J
@(x) P[@lYl
where the integral is over all @(x) in the ensemble.
The statistics of the ensemble enter via Bayes’ theorem as a prior distribution:
and P[yl$] depends on the filter f ( x ) and the noise statistics. So we have
This equation tells us how to go from the measurements y(x) to a best guess of the
signal, &(x).
When~both the signal and the noise probability disbibutions are Gaussian
the estimate is a linear functional of the measurements. .But this is not true in general, and
the estimator can be.quite complicated. In the above formulation it is clear that knowledge
of the ensemble, given by P , is important to understanding how well the system will
In general P must be fully characterized in order to do the above calculations. But
this is impossible when faced with a high-dimensional signal as it would require gathering a
huge number of images from the distribution$. Fortunately there is a regime in which only
a few of the correlation functions of the distribution are needed-when
the signal-to-noise
ratio (SNR) is low.
It can be shown that to lowest order in the SNR the best estimate is given by:
F , and Y are the Fourier transforms of @e,,t, f, and y. respectively, and S(k)
and N(k) are the ensemble power spectral densities of the signal and noise, respectively, at
spatial frequency k. The power spectrum of the signal ensemble can be expressed in terms
of its second-order correlation function as
(@*(k)@(k’)) = 2nS(k)S(k - k’).
t An overview of linear signal analysis and estimation can be found in the pioneering work of Wiener .
$ Just imagine how many 4 x 4 images would need to be seen in order to fill out their probability distribution.
Suppose each pixel is quantized to 16 grey values. Then there are over IO” possible images, and we would need
many limes this many examples in order to make a guess as to how they are distributed.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
At higher SNR equation (6) will include higher-order correlation functions of the image
ensemble. Note that this equation is also the first-order term in a low SNR expansion of the
Wiener filter.
We could now ask a question like: ‘Which choice of f ( x ) (within specified constraints)
minimizes the expected reconshuction error?’ Once the statistical character of the problem
is specified, we are ready not only to quantify its typical behaviour, but also to ask
what the system’s optimal design is with respect to a given criterion. This paradigm
has become popular in recent years, and has given rise to encouraging results when
applied to biological vision, such as predicting neural responses [48, 50, 681, receptive
fields [2,4,5, 10, 54,57,71,72, 80, 83, 84, 861, colour coding [3, 8, 14, 22, 59,641, stereo
coding , the design of compound eyes 147, 78, 791, and even the pupil response 1491.
The ultimate goal of the approach is to predict from first principles a measured response
of a biological system, be it a neuron’s activity, the optimum facet spacing in a compound
eye, or a human psychophysical threshold. In the latter category, Atick and Redlich 141 have
predicted the optimum neural encoding of natural scenes based on the criterion of minimizing
the representation’s redundancy. Their result matches human detection thresholds over many
decades in light level. In constructing an approach they combine a design criterion, system
constraints, and the measured statistics of images. These minimal ingredients allow for
a parameter-free prediction, which implies that basic ideas of efficiency may have wide
application in vision.
Laughlin asks how a visual neuron should best encode contrasts so as to transmit
as much information (in the Shannon measure ) as possible. The answer is to transform
contrast in such a way that the response histogram is uniform. This procedure eliminates
the same type of redundancy present in English text where letters of the alphabet are not
used equally often. His treatment predicts a contrast-response curve which is quite similar
to the response properties of LMC cells in a fly’s visual system. Making this prediction
requires the measured contrast histogram of natural images. We will return to this issue of
response histograms later in the paper.
The basic idea in all of these approaches is that sensory systems are well-adapted
for processing the types of signals present in nature. Many of the proposed critera for
efficient design are based on statistical measures. Attneave and Barlow suggest
that reducing the redundancy of sensory messages is a primary goal of sensory systems.
Linsker has presented examples of ways in which sensory encodings can be maximally
informative [55, 56, 571. Other measures include reconstruction fidelity [57,
711, and the
shape of neural response histograms 132, 481. All of these criteria involve the statistics of
the neural encoding. Only by first studying the properties of natural scenes can we make
predictions as to how best to process them.
In summary, we often want to know how well an image processing system will work
under normal conditions. One generally uses a statistical measure such as the average
reconstruction error, of the amount of information the system delivers. This brings us into a
framework which has a statistical basis and requires knowledge of natural image statistics.
3. Gathering natural images
Up to now we have been providing a motive for the study of natural images. Some important
questions still remain:
0 What should we measure?
Which images should be in the ensemble?
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
What statistics should we compute?
Clearly there is no ‘right’ ensemble. Each environment has its own typical characteristics
and thus its own statistics. In interpreting the differences between these statistics we
might seek creatures whose visual systems differ systematically with the statistics of the
envronments they inhabit [38, 581.
One early study characterized television images in an effort to understand how best
to encode them. More recent work has focused on outdoor images of nature [31, 13, 821.
Our data consist of images of a wooded environment in springtime.
Once an environment is chosen we must decide which images to capturethat is, where
to point the camera. We might also choose to use an angular resolution or speclml sensitivity
which mimics a particular creature’s visual systemt. Of course visual systems function in
real time and so analysing short movies instead of still images,would add another level of
Finally, we must consider the statistical analysis. Ultimately one would like to know
P[Z], the probability of occurence of any image. But, as mentioned earlier, this would
be impossible to achieve due to the required size of the dataset. Another possibility is to
experimentally determine the best parameters for a model P[Z], such as a Markov random
field [33, 431.
Perhaps the most direct approach is to catalog the correlation functions of the
image distribution 1131.
This means using image data to evaluate expressions like
(Z(xl)Z(xz). . . Z(x,,)). But with the exception of the second-order correlation function
(n = 2). these quantities are difficult to interpret (and to visualize, since each spatial index
adds two dimensions to the function’s domain). The correlation functions enter naturally
in a perturbative fashion into statistical calculations at low SNR, as discussed above. So
although higher-order correlation functions may not provide much insight, they do have
straightforward application.
Characterizing an arbitrary distribution can be done through brute force (by measuring
correlation functions, for instance). But we can also seek a simple underlying structure
or ‘invariance’ property in the distribution. That is, the image probability could transform
simply if the image is transformed simply. One such symmetry is translation invariance. For
some ensembles we expect that a given image appears with equal probability regardless of
its positional offset. Such invariances can greatly reduce the complexity of the distribution.
They are commonly sufficient to synthesize the distributions of quantum field theories 1391,
and also play an important role in image processing . If the distribution of natural
scenes contains no such invariance then just collecting statistics will be a useful-but
necessarily interesting-venture.
However, we will find that natural images do indeed
display some rather surprising symmetries.
Since there is no way to collect enough data to fully characterize an image environment,
our statistical description will be far from complete. We will not even be able to reproduce
realistic images with our minimal statistical knowledge. It is interesting to consider just how
much knowledge of this kind is necessary to do something useful. For instance, we have
seen that under conditions of low SNR the only important statistic is the power spectrum,
and it can be used to design the optimal low SNR filter. At high SNR every detail of the
statistics will matter, but how much? Could we limit our knowledge to a few statistics
which allow a nearly optimal performance?
Effectively this is what our visual system does. The development of the mammalian
visual system is strongly dependent on e d y visual stimulation [ll, 611. In particular, it
t Hopefully the characteristics of natural images will be fairly robust to these types of details.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
is known that mammals raised in unusual image environments end up with functionally
modified visual systems as adults . This suggests that the visual system's development
is influenced by the statistics of its environment, through some as yet unknown algorithm.
Contained in the final 'wiring' of the visual system is a set of statistics about the creature's
past visual experience. Knowing which statistics these are might provide great insight
toward the nature of visual processing.
4. Image statistics
In 1952 Kretzrner pioneered the modem analysis of real-world images.
applications to television image coding in mind, he tabulated a set of local image statistics
such as the point histogram and the second-order correlation function.
From these
measurements he placed a lower bound on the image redundancy of about 3 bits per pixel.
The first mention of power-law scaling in image power specbra was in a 1957 paper by
Deriugin , who also measured television signals. This properly was rediscovered in 1978
by Cohen et d (also see [ZO]), and again in 1987 by both Burton and Moorhead 
and Field 1311. This scaling was later studied by Tolhurst etul and by us . The
ensemble power spectrum (averaged over orientations) is found to behave approximately as
S(k) a k-z*n
where k is the modulus of the spatial frequency (in cycles deg-', for instance), and 7~ is
measured to be small. The power spectrum is a function of a single angular scale given by
the spatial frequency, and it changes as a power of that scale. There is no preferred angular
scale in natural images since the form of the power spectrum is invariant to any choice of
basic scale. Doubling the spatial frequency always reduces the power by a factor of 2-z+n.
This is not true, for instance, for a power spectrum of the form S(k) FS e-k/$, where ko
acts as a 'typical' spatial frequency.
These studies provide evidence for a certain symmetry in ensembles of natural images:
scale invariance. Scale invariance implies simply that the image statistics do not change
with the angular scale. Pictures of such an ensemble will have the same ensemble statistics
regardless of the lens' focal length. More generally, the new ensemble may be se[f-uffine
to the original one, meaning that the new images must also be multipliyl by a suitable
constant after rescaling to make the statistics identical to the original ones. If Q[~(Qz)] is
any ensemble statistic of @(z) on scale a, then scale invariance implies that
Q[4(z)l = Q[aV4(az)l
where U is a universal exponent (i.e. it is independent of both (Y and Q). Thus in a scale-
invariant ensemble we can make the replacement @ ( x ) + a"@(ar)
for all instances of @
in any expectation value.
This is a strong statement. It greatly restricts the form of the image distribution. Such
a property also gives us some intuition about natural scenes instead of a mere quantification
of their statistics. For instance, it reinforces the notion that objects in the natural world can
appear at any angular scale in an image (i.e. they can be any distance away), which is one
plausible mechanism for producing scale invariance.
Scale invariance is a widely studied property of critical phenomena, such as the Curie
point of ferromagnets. Physicists study models with local (Markovian) interactions which
give rise to 'long-range' (i.e. power-law) correlations. In order to attain scale-invariance, the
model parameters must be chosen very precisely . Interestingly, in two dimensions local
scale and rotationally-invariant models must also be conformally invariant . But since
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
images in nature are not even isotropic-the
horizon has a definite orientation-we
expect them to be more generally conformally invariant. Thus to model the scale-invariant
statistics while excluding conformal invariance, one must include distant interations.
If natural images form a scale invariant ensemble, then we should find scaling in other
statistics besides the power spectrum. In 1992 William Bialek and I collected natural image
data and began a statistical analysis [70, 73, 741. Aside from noted exceptions, the work
presented below resulted from this collaboration. We were able not only to confirm the
scaling result, but also to find evidence for a novel invariance.
Other work on natural images has included local principal components analysis [34,52,
751, in which the local linear filters which maximally decorrelate the images are sought.
The filters with highest response variance tend to have a resemblance to the oriented
receptive fields found in the cortex, which suggests that some decorrelation process may
be in operation. Decorrelation is commonly considered to be the first step in reducing the
redundancy of a representation.
The spectral reflectance properties of natural scenes have also been studied. In 1947
Krinov measured the spectral reflectances of 337 natural objects, such as grasses and
wood, at 26 wavelengths. Maloney found that all these spectral profiles could be fit closely
using models with 7 free parameters , which represents a redundancy in 19 of the
26 dimensions. Dannemiller found that the noise due to random photon catches
effectively reduces the dimensionality to 3, which is the number of cone types present
in the human retina. Using colourimetric measurements (and thus invoking human colour
vision specifically), Burton and Moorehead found that natural images evoke highly
correlated responses in cones, and showed that power spectra scale approximately as IlkZ
in each of the three cone systems. By studying properties of colour images one might tq
to predict the optimum arrangement of retinal photoreceptors , colour coding , and
the optimal pupil function for chromatic vision .
5. Measuring natural images
As an image ensemble we chose a wooded environment in central New Jersey during
springtime. These woods are the habitat of insects. small mammals. and birds, so it is
an important sensory environment for many different types of visual systems. Since it
constitutes only particular environment our results will not necessarily be characteristic of
others. Another approach would be to gather images from widely varying environments
and analyse them together as a single grand ensemble of natural scenes.
The woods consist of trees, scrub, rocks, and a stream. An image from the ensemble
is presented in figure 1. Translation invariance is built into this ensemble since the camera
was pointed in random directions with small elevation angle, and images of sky or ground
alone were avoided. Details of the measurement process are presented in the appendix.
The luminance of a reflecting scene is proportional to the radiant flux from the
sun. Visual systems, including cameras, adapt to this mean background value, making it
irrelevant. It is removed from our images by considering logarithmic intensity fluctuations
from a background level. We define the the 'log-contrast' @(z) to be
W) = In [~(wzo]
where Z(z) is the measured intensity signal, and ZO is defined for each image such that
= 0. This gives every image histogram zero mean. The definition of ZO is
arbitrary, but many of our statistics are log-contrast differences in which the constant ZO
drops out. The logarithmic measure is convenient in that it covers the whole real axis;
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
Figure 1. Image from the waods: rocks in a stream with background foliage.
intensities, on the other hand, are somewhat difficult to work with since they are non-
negative. We find the use of @ instead of I also seems to improve the observed invariances.
The data set consists of 45 images taken at a 15 mm focal length (I pixel subtends
0.059" of visual angle) and 25 images at an 80 mm focal length (0.01 1" per pixel). The
digitized images are 640 x 480 pixels, from which the central 256 x 256 subimage is taken.
At 15 mm and 80 mm focal lengths the images subtend 15" and 2.8" respectively.
6. Statistical analysis
Previous work has hinted at scaling in natural images. We will try first to substantiate it by
evaluating the power specbum of our images. Digitized images are discretely sampled
continuous images, @(z), which are drawn from a stationary distribution with power
spectrum S(k) given by
(@*(k)@(k'))
= (Zn)2S(k)8(2)(k - k')
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
where @(IC) = I d % @(z)e-'k.". The coordinate system, strictly speaking, consists of the
azimuth and elevation angles. The pixelated image is derived from the continuous image
&.a = $(ma,
where a is the pixel spacing measured in degrees, and m,
n are pixel indices which go from
0 to 255. We want to estimate S(k) from the discretely sampled images.
Spectral estimation is a well explored field, a cogent summary of which can be found
in . The power spec!"
is estimated using
where zm." = @"nu),
IC,, = (2ar/Ma,2as/Maj, M = 256, and i runs over all the
images in the data set. Wm," is called the 'windowing function'; its shape determines how
the estimate relates to the actual spectrum. We use a two-dimensional Bartletf window in
our computations, but :the result is not strongly dependent on this choice.
A contour plot of the power spectrum of the natural scene log-contrast is presented in
figure 2. The contour!; are placed at constant intervals in the logarithm of the power. It
shows a preponderance of power in low spatial frequencies along the horizontal and vertical
orientations, which are clearly special. It is mare illuminating to plot the orientationally
averaged spectrum, which is shown in figure 3. The plot consists of two superposed graphs,
each from a different lens focal length. The scale is logarithmic both in spatial frequency
and power, and the data thus plotted are nearly linear. This means that the power spectrum
is a power-law of the form
with q = 0.19 j, 0.01, and A = (6.47 i
For a given focal length measurement difficulties arise at high spatial frequencies.
Optical blur causes the spectrum to fall, and at the same time noise and aliasing cause
an increase in power. We extend the spatial frequency range by simply using two focal
lengths. The graph shows scaling of the spectrum over nearly 2.5 orders of magnitude
in spatial frequency. The highest frequency for which the scaling is demonstrated is about
30 cycles deg-', which corresponds to about half the acuity of the human eye . These are
thus relevant spatial frequencies for vision. Although our results derive from the logarithm
of intensity, we find the same scaling in the power spectrum that others have found when
using the intensity signal (we find this as well). Scaling in the power spectrum is robust
to such a change, as well as to the obvious differences in choice of ensemble, spectral
sensitivity, and methods of image capture. However, the exponent differs between authors
and environments (e.g. we find 7 % -0.3 for beach images).
This scaling of the power spectrum confirms the findings of others. But we can say
more. First, the power spectrum alone does not tell us whether the distribution is Gaussian.
Also, scaling should be testable through any statistic o$ our choosing. Both issues can be
explored at the same ltime through ,the process of 'coarse-graining.' The original scaling
ideas of Kadanoff I441 propose that a coarse-grained critical system should have the same
statistics as the originall system, aside from a possible rescaling of the field variables.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
Figure 2. Contour plot of ensemble power spectrum Of 45 images taken at focal length of
15 mm. Center of figure is k = 0. Contours are placed at equal intervals in the logarithm of
power. and spatial frequency is ploned on linear axes.
Similarly, we can coarse-grain the images to look for scaling and the possibility of
Gaussian statistics. The average of @ over scale N is given by
An example of this procedure is shown for N = 2 in figure 4.
If @ is a scaling field then the probability PN(QN) should have a shape which is
independent of N . In the theory of critical phenomena, when a field has an anomalous
dimension (i.e. q # 0) it must be ‘renormalized’ when length scales are changed. This
p N ( @ ) = I/@iMs P(@/@iMs)
where @;Ms
and P is the scaling probability distribution. For a scaling field
with anomalous dimension q / Z , @EMs o( N-’”*. In order to compare histogram shapes this
RMS value must be divided out.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
Figure 3. Orientationally averaged power specmm with standard enor bars for 15 mm and
80 mm focal length data (overlapping). along with the regression line fined as discussed in the
Figure 4. Block averaging procedure for N = 2.
If @(n) scales then the probability distribution of QN should always have the same
shape, regardless of the value of the rescaling parameter N. Figure 5 demonstrates scaling
of the log-contrast distribution. The plot shows P(&/@”)
versus @ N / & ~ ’ for N = 1,
2, 4, 8, 16, and 32. These six graphs all lie on top of one another; they have the same
shape A Gaussian distribution would show a parabola in the plots instead of the nearly
linear tails we find. Histogram scaling is a much stronger statement than the scaling of a
two-point function (i.e. the power spectrum), since it means that higher-order correlation
functions must also scale.
The central limit theorem states that when a large number of independent random
variables (with finite variance) are averaged together the resulting distribution becomes
Gaussian. The fact that our histograms do not become Gaussian even after averaging
together nearly 1000 (32 x 32) data points is evidence for what physicists call a non-
Gaussian scaling fixed point. Simply put, the central limit theorem does not apply since the
variables being averaged are highly correlated. Such extended correlations are typical of a
thermodynamic critical point, where the correlation length is infinite.
As another example of both the scaling of statistics and the non-Gaussian character of
the ensemble, consider the distribution of local gradients. We first block the images on a
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
D L Ruderman
-2 Lo&ontras? (Rescaed)
-2 Lo&ontras? (Rescaed)
Figure 5. Scaling of log-conwsr histog” over scales 1, 2, 4, 8, 16, and 32.
Figure 6. Scaling of gradient histograms. Plot shows P ( G N / G ) for N = 1, 2 4, 8, 16, 32
with Rayleigh distribution (solid) shown for comparison.
scale N and then calculate a discrete approximation to the magnitude of the gradient,
IVdN(m,n)l.
If @ is a scaling field then the histoam of G should have a shape which is independent of
N . Figure 6 shows P(GN/&) versus GN/-,
where. & is the mean of the distribution.
If q5 were Gaussian, then this distribution would take the Rayleigh form
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
which is plotted in the figure for comparison.
The distributions of gradients for length scales from 1 to 32 are identical in shape over
nearly four decades in probability. There is a stark contrast between this distribution and the
Rayleigh form. First, the histogram of gradients has a very long exponential tail where the
Rayleigh distribution falls off much more sharply. This means there are far more regions
of large gradient in the images than there would be if they were Gaussian. Also, there is an
excess of small gradients, or uniform patches. These are non-Gaussian signatures of natural
scenes. Such patterns of very large and very small gradients are seen in thermally driven
convective turbulence, which also gives rise to non-Gaussian probability distributions with
exponential tails , and displays scaling .
Scaling allows us to replace q5 (I)
by u"q5(uz) without changing any of the statistics. We
have seen that rescaling images does not change the shape of the log-contrast distribution,
only its width @iMs.
According to the scaling law this width should scale as N-". In figure 7
we plot @iMs
versus N on a log-log scale. The graph is linear with slope -U % -0.2.
All the local quantities we have tested scale with this same exponentt. Had the pixels
been independent of one another, the rescaling factor would have fallen as N-'; this line
is plotted for comparison. The variance in natural images remains characteristically larger
than it would for white noise when images are averaged over large scales.
Figure 7. Standard deviation of 4~ distribution as a function of N (log-log plot). If the images
were made of uncorrelated noise, the standard deviation would scale as 1/N. as shown by the
dotted line.
7. Predictability in natural images
One measure of the non-randomness of images is the amount of predictability they contain.
Suppose we know what some sections of an image looked like, and from them we want
to guess what the missing sections are. How well can this be done? The answer lies, as
t The astute reader may wonder why q # 2v. since lk power spect" involves two powers of the field 6, This
does present something of a mystery, but it can possibly be resolved by considering the fact that the scaling may
not be perfect and isotropic. The way orientations were averaged when computing the power specmm is different
from the way odentations are confounded when averaging over square blocks, which might explain the difference
in exponents.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
one might expect, in the statistical smcture of the images. If they are composed of random
pixels, then there is no predictability. But natural images possess long-range correlations,
and so a large degree of predictability is expected.
Claude Shannon, the inventor of information theoIy, held an interest in statistics of the
English language. In a 1951 paper he used the inherent knowledge of native speakers
to place bounds on the redundancy (or predictability) of written English. He would remove
the last character of a string of n characters from an English text, and the native speaker
would fill in the missing character in as few tries as possible. From the histogram of the
number of guesses until a correct response Shannon placed bounds of between 0.6 and 1.3
bits of entropy per letter for n = 100. This represents a single letter redundancy of about
In 1987 Kersten asked human subjects to perform a similar task on everyday images. A
pixel was removed from an image and a subject was asked to replace it. Using the method
of Shannon, he placed the single pixel redundancy in natural scenes at 65%. This means
that of all the entropy a pixel has, 65% of it is predictable from knowledge of the rest of
the image.
In a collaboration with Horace. Barlow and Chris Wroe in Cambridge, we used the
natural images to make some assessments of predictability. One would ultimately like to
know how much information a patch of image conveys about another patch some distance
away. Such a computation would involve an immense ensemble of images so as to sample
the distribution well. The most that can be practically accomplished is to ask about a few
pixels at a time. For example, how much information does one pixel convey about another
a given distance away?
We compute Shannon's mutual information between the two pixel values, 61 and &
where p2 is the joint distribution of two pixels at a given separation, and p1 is the marginal
distribution of a pixel. For a given displacement vector d all positions in the images
are scanned to create a histogram of joint probabilities. From this distribution a discrete
approximation to equation (19) is computed.
The left graph of figure 8 shows the mutual information between two pixels as a function
of the distance, d, between vertically separated pixels. The graph is very nearly linear on
a log-log scale, meaning the information scales as a power-law in the separation distance:
I(d, 8) zz d-'(e)
where a(@) conveys the dependence of the slope on the angle of the separation axis. The
right graph shows the systematic anisotropy in 01 as a function of 0 (8 = 0 corresponds
to a horizontal separation). The fact that the slope is smallest for 8 = 90" implies that
correlations are strongest vertically, as one might expect in images containing trees. We
compute the single pixel nearest-neighbour redundancy to be about 10%. Kersten's much
larger figure reflects the fact that the redundancy is present in substantially larger areas of
the image.
Interestingly, a similar scaling of information is found in English texts. Ebeling and
Poschel have found that the mutual information between two letters as a function of distance
scales as I zz d-0.37 up to distances of about 100 letters, where finite data-set noise began
to creep into the measurement 1301.
Note that the scaling of information with distance is quite fortuitous, as such a
complicated statistic has no a priori reason to be scale-invariant, even if the image
ensemble is. In a scale-invariant ensemble any single-power correlation function will scale,
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
&%(degrees)
Figure 8. Top: log-log plot of I ( d , e) versus d for R = 900. Bottom: scaling exponents of
I ( d . e) versus e.
such as (@z(0)44(z)). But this is not necessarily true of a mixture of powers, such as
((@(O)@(z)
+q5z(0)~z(z))), since a change in len@ scale rescales each term by a different
exponent (see equation (9)). and so the function changes. The information measure above
is a special quantity which does happen to scale even though it is not a single power of the
scaling field 4.
We can try another measure of correlation, namely the second-order correlation
coefficient, given^ by
= 0. This statistic tms out not to obey a power law; instead it crosses
over from power-law to exponential behaviour at distances larger than about 20 pixels. Since
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruder"
scaling is a basic statistic of natural images, Shannon's mutual information may be a more
'fundamental' measure of correlation than the second-order correlation coefficient.
As a final example of two-point image statistics we examine the degree of
reconstructability that one pixel provides about another. How well can & be estimated from
a pixel @I lying certain distance away? We want a function @?(@I) which is the best guess
for @Z given @I. The estimate which minimizes the mean-squared error, (142 - @F(@dlz),
which is the conditional mean of @z given $1.
Figure 9 shows pst(4) for vertically separated pixels at distances of 1, 4, and 16 pixels.
The relative importance of different regions of the plot is indicated by an overlayed graph
of the pixel probabilities. In the relevant region the estimate is a linear prediction, i.e.
where m(d) is the slope of the line as a function of the separation distance between the two
pixels. Figure 10 shows the functions m(d) and F(d), the reconstruction fidelity defined as
4341) m(d)$l
Both are power-law in form, with exponents -0.31 and -0.72, respectively.
-0.8 -0.6 -0.4
16 (dotted) pixels. The probability distribution of @ is plotted to show the reeion of interest.
Estimator @mt(@) for pixels separated vertically by 1 (solid), 4 (dashed), and
Reconstruction based on the knowledge of a single pixel is not very good. The RMS
prediction error based on knowing a vertical neighbour is nearly 70% of the RMS pixel
variations themselves (i.e. the error we would get if we had no such howledge). The
estimates can improve by using more of the nearby pixels in the form of an optimal
estimator. As more pixets are included the optimal estimator takes into account more
and more of the image statistics in the form of joint pixel densities. Nothing particularly
special about natural images is capturd'in the two-point pixel statistics.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The srarisrics of natural images
Figure 10. ReconsIn~ction fidelity F (dashed), and slope of linear remnsmction m (dotted) as
a function of distance d (log-log).
Even without the complete pixel statistics we can find the optimal linear estimator of a
pixel from its nearest neighbours (NN)
It requires only second-order correlations. The coefficient vector a is calculated to minimize
the expected mean-square error as
where C;j = (@;@j), yi = (@@;), and the indices i and j run over the 8 nearest neighbours.
The coefficients ai are slightly negative along the diagonals and have values of about 0.3
along the horizontal and vertical. This reconstruction provides an RMs error of about 50% of
the RMS pixel fluctuations. A reconstruction which includes next-nearest neighbours brings
the error down to 45%.
8. A new invariance
8.1. Filtering natural images
The images we have been exploring are examples of the signals which the visual system
processes. What do their statistics tell us about how this processing should be done? The
early stages of vision, such as those in the retina, are constrained to process images locally-
no neuron has access to the entire image . The neurons which convey these signals will
have output statistics which are determined by the images. According to various efficiency
criteria the responses of these channels should have certain statistical properties.
For instance, channels with signal variance constraints are optimized for information
transfer by sending Gaussian signals . Neurons have an analogous constraint in their
since their firing rate ca~sahlrateathigh.lev~l~~.ca~ot-go~negative.
optimal encoding statistics thus depend on the imposed constraints.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruder”
Figure 11. A local bandpass filter.
Figure 12. Histoogram of lhc output of a local filter.
Fust we should determine the types of statistics which come from simple linear filtering
of the log-contrasts (filtering the intensities gives similar results). Consider a filter of the
form shown in figure 11. This 2 x 2 filter passes no signal at zero spatial frequency, and
thus lo drops out. The histogram of this filter’s output is shown in figure 12 on a semi-log
scale. It has nearly perfect exponential tails over four decades in probability. In fact my
local linear transformation we try (including Gabor and centmsurround filters) seems to
produce exponential-tailed histograms, though their shapes can differ somewhat.
Long-tailed histograms from natural scenes have also been seen by Daugman ,
Barlow and Tolhurst 191, and Field . Field points out that such ‘sparse coding’ is a
consequence of specific arrangements in the Fourier phases of natural images. He proposes
that long tails have the effect of activating only a very sparse subset of the neurons which
code images. Burr and Morrone 1151 believe that this properly of images is related to the
existence of edges in natural scenes, and ‘signals features of interest to vision.’ Images
drawn from a Gaussian distribution have completely random phases, and show no structural
resemblance to natural scenes 132, 701.
A one-sided exponential distribution maximizes’ information transmission for a given
mean activation level, just as a Gaussian is optimal for fixed variance. If the neurons
encoding visual stimuli=have-a-me?n .firinaweconstraint, ~then.,~eseexponentiaI
histogLaC5
are ideal, we just need to rectify them so that they are one-sided.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
No local linear
transformation does it. But what about local nonlinear transformations? One method
is simply to find a pointwise transformation of the image which produces the distribution
we want, as Laughlin does to predict contrast coding . But his method does not find
the cause of the histogram, it just re-shapes it. We prefer to seek the mechanism which
produces the long tails and systematically 'undoes' it.
Is there a way to transform away the exponential distributions?
A&litude (&its of RdS)
Figure U. Histogram of amplitudes of a 5 min sequence from Strauss' *The Blue Danube'
from a compact disk recording as sampled at 44 kHz using a linear analogue-todigital conveztor
(ADC) with 16-bit resolution. A Gaussian distribution is shown to highlight the excesses in the
peak and the tails of the histogram.
8.2. Ading a nonlinearity
To search for a likely candidate we should think about the possible causes of the excess
histogram tails. Consider an analogy with music, which is an' ensemble with similar
properties to images. The amplitudes of musical sound pressure also have exponential
tailsit (see figure 13). The source of the long tails is the dynamics of the musical score;
some sections are loud and some are quiet for an interval of time. If the quiet passages
were amplified and the loud ones attenuated then the excesses at the tails and the peak of
the distribution would move to more 'typical' values, thus diminishing the peak and tails.
This would give the distribution a more 'rounded' or Gaussian character. Maybe a similar
dynamic occurs in natural scenes, where locally correlated regions are either flat in texture
(i.e. quiet) or very dynamic (loud). This suggests an origin for long exponential tails: The
histogram is a superposition of many distributions of different variance.
Dividing a sound waveform by its recent loudness is a local nonlinear operation. We
can by an analogous procedure on images by normalizing log-contrast fluctuations relative
t Exponential tails in histograms of real-world phenomena~may in fact-be quite~gend- .
t [tis also well known that many forms of Western music have scale-invariant statistics [37. 851
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
Figure 14. Variance modified image. J ( x ) .
to their local standard deviation. This creates a new field
where &(x) is the local mean within the N x N block surrounding position z,
and u ( z )
is the standard deviation of Q within the block. This procedure has the effect of removing
mean displacements from zero log-contrast and normalizing the local variance of the log-
contrast. Patches of small local contrast will be expanded, and high contrast areas will
he toned down. The numerator is like a centre-surround mechanism with the surround N
times as large as the centre (1 pixel). Running the procedure on the image in figure 1
(using N = 5 ) gives a variance modified image, $, shown in figure 14, and a standard
deviation image, U , shown in figure 15. The variance normalized image is much more
homogeneous than the original. Small fluctuations on the rock, for instance, have been
expanded out to higher contrast. The image almost looks like a noise pattern, except for
a few residual object borders. On the other hand, the standard deviation image, U, seems
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
Figure 15. Standard deviation image. ~ ( x ) .
almost to highlight the object borders and to attenuate the object textures. It is very roughly
as if variance normalization separates objects from their textures.
We find that for the value N = 5 , the resulting histogram of +(I) is closest to a
Gaussian, in that its kurtosis is nearest to that of a Gaussian (30~). For smaller N the
kurtosis is greater, and for larger N it is less. The statistics of + are shown in figure 16.
The variance modified images are not exactly Gaussian, but they show Gaussian signatures:
The histogram tails of + fall off rapidly, and its gradients are Rayleigh distributed. This
new signal is thus amenable to transfer down a dynamically limited channel.
What about u(z)? Making images from the variance, uz(z),
averaged and sampled in
5 x 5 blocks, gives us a set of reduced size non-negative images. We can treat these in the
same way as we did the original image data by looking at the log-contrast. Define
((I) =In [u::'l
where U,' is analogous to 10. The statistics of ( are very similar to those of 4, as shown
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L R u d e m
Figure 16. Semi-logarithmic plots of the statistics of variance modified images. *. Top:
histogram of * (rescaled IO unit variance) shown with a Gaussian for comparison . Bottom:
histogram of gradients of
shown with a Rayleigh distribution for comparison .
in figure 17. Both the log-coneast and log-contrast gradient distributions of the original
images and the variance images are quite similar. The power specha are compared between
q5 and the variance images before subsampling, so they are on the same spatial frequency
scale. The slopes of the spectra agree completely at low frequency (they have been shifted
to match vertically). At high frequency the variance image spectrum falls off, as expected
from the.10~-pass nature of the statistic.
8.3. Reiteraring the procedure
For every statistic we measure, the variance images are identical to the original ones. This
means the patterns of local variances in natural images are statistically much like the patterns
of intensity. Does this mean the entire procedure can be reiterated? The answer is yes, but
the results are not quite as clean as before. Start with the full resolution variance images
and produce two datsets, [(x) and E(%), which are the variance normalized and standard
deviation images, respectively, of: (see figure 18). The most Gaussian < statistics are
found by averaging and sampling in 11 x 11 blocks (see figure 19). This is the length scale
at which the kurtosis of 5 crosses zero. Similarly, the distribution of gradients is closest to
Rayleigh when [ is sampled and averaged over 11 x 11 blocks; it is shown in figure 19. This
procedure, iterated a second time, has produced another set of nearly Gaussian data plus
a low resolution set of variances. How do these new variances compare with the original
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statistics of natural images
Figure 17. Statistics of e images compmd to the original images, 4 . Top: histogram of
log-contrasts (sealed to unit variance, semi-log plot). Middle: histogram of aadients (scaled to
unit mean, semi-log plot). Bottom: power spectra of 5 images (falls off nt high frequency) and
original images (arbitrary spatial frequency units, log-log plot).
images? Define a new log-contrast:
Its statistics (averaged over 11 x 11 blocks) are shown in figure 20, along with those of the
original image log-contrasts. Again, the match is nearly perfect. Note that this was not a
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L R u d e m
Figure 18. The iterated variance normalization procedure.
Figure 19. Statistics of < images compared with those of a Gaussian field . Top: histogram of
< (rescaled to unit variance. semi-log plot). Bottom: histogram of gradients of < (rescaled to
unit mean, remi-log plot).
~~~~~~~~~~
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
true re-iteration of the procedure, as that would have meant averaging over 5 x 5 blocks in
reduced images of U . But this latter~method does not reproduce the statistics quite as well.
The statistics of natural images
Figure 20. Statistics of z images compared with the original images, q5 . Top: histogram of
logsontrasts (resealed to unit variance, semi-log plot). Bottom: histogram of gradients of L
(resealed to unit mean, semi-log plot).
Due to a lack of data, the procedure cannot be reasonably performed a third time. If
it could be continued indefinitely then we have found a nonlinear invariance of the image
ensemble. Local variances themselves have local variances with the same statistics. Thus
correlations exist not only between image intensities, but also between local variances. The
variance normalization procedure is analogous to spectral whitening in that it removes the
correlation, though it is done hierarchically instead. The invariance has implications for
coding: one could iterate the procedure to produce at each stage a set of Gaussian signals
which could be communicated efficiently. In the end images could be made completely
Gaussian. Most importantly, this has all been accomplished with local processing. These
results may also imply that naturalistic scenes could be generated from Gaussian noise by
inverting this procedure. Accomplishing this would mean that the structure of P[$l has
been completely understood.
In short, we have discovered a new type of iterative invariance in natural scenes. This
variance normalization pm@ure is remeniscent of the 'contrast gain control' mechanism
found in the visual cortex [35, 631. It may simply serve to limit the variance of neural
responses by reducing the tails of their distribution. Iterating the procedure reproduces the
same statistics again and again, which implies that a universal algorithm may serve ideally
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Rudemuut
to process these image components.
Note that the procedure described above is not particularly advanced or complicated.
The simple square regions we tried may not be optimal at producing this effect-maybe
circulax ones, or weighted averages would perform better. Furthermore, this technique
works to Gaussianize both log-contrast images and the original intensity images (data not
shown). Finetuning of this procedure was not necessary, which suggests that the effects of
variance normalization are both basic and robust.
9. Information in the retina
We have seen that natural scenes are statistically quite different from white noise. They
are highly structured and correlated over large regions. In the space of all possible images
they occupy an infinitesimal volume, a notion which can be quantified using the concept of
entropy. The ensemble of natural scenes is quite restricted and thus has much less entropy
than a white noise ensemble.
The entropy of natural scenes is an important quantity which is related to how much
‘space’ is needed to represent them. If every image consisted of a uniform grey, then just
one number would specify the whole image: its grey level. On the other hand, if the
ensemble really was white noise, then every pixel would need to he retained since they are
all totally uncorrelated. Natural images lie somewhere in between.
In order to put a number on the entropy we must invoke an actual image representation
which includes noise; otherwise the entropy of a continuous distribution is infinite. For
vision the primary representation is at the level of the photoreceptor array, where the scene
is first captured. The question of image entropy can be posed as: ‘Given the encoding of
images by the photoreceptors, what channel capacity is required to transmit ,them to the
brain?’ This question is relevant for the visual system since conveying natural images is
what the optic nerve does. How much information does each nerve fiber need to be able to
information in the encoding is
Suppose the photoreceptors represent an image as a. set of responses {y,].
= H[(y.]] - H[noise]
where H is the entropy of a random variable. The second part of this equation is valid
under the assumption that the photoreceptor noise is additive and independent of the signal.
We use a linear model for the encodingt. First the image is low-pass filtered by the optics
according to the diffraction-limited incoherent point-spread function, whose spatial transfer
function is (approximately) [I21
for Jkl c kc and zero otherwise. Then the photoreceptors sample this’signal, and Gaussian
white noise of variance u2 is added to their responses. Thus
M(k) = 1 - Ikl/$
M(a: - a:+t4d + v.
t The responses are more realistically linear in the image intensity, and not the log-contrast. However we find
that they both have nearly the same power spectrum, ana so the results will be identical. Thus 6 above may be
freely interchanged with I for the purposes of this calculation.
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The statisfics of natural images
where x, is the position of the nth receptor, and (qmq.) = O ~ S , , ~ . This defines the image
The information per receptor is given by
An upper bound to this quantity can be found by assuming that the images, and thus the
responses, have greater entropy than they actually possess. This is'achieved by assigning a
Gaussian distribution of images with the same power spectrum measured for natural scenes.
This distribution has the greatest entropy consistent with that power spectrum, and so the
information is overestimated.
The receptors are placed in a hexagonal arrangement (i.e. on a triangular lattice), as is
present in the fovea ; they are spaced as far apart as possible so that there is no a!iasing,
which is nearly the situation in the fovea? . The spacing is thus
If the receptors were on a square lattice the no aliasing condition would^ require a 13%
increase in the density of receptors, as the spacing would be exactly nlk,. The area of a
unit cell in the hexagonally arranged photoreceptor lattice is A, = (fi/2)a2.
In the limit of an infinite lattice the Fourier components of a stationary Gaussian signal
are independent, and the total information is the sum of the information in each component:
Here Z is the information per receptor, A, is the area of the unit cell in the lattice. and u2
is the variance of the noise.
We use S(k) c( l/k2-o, with q taking its measured value, and we express the noise
level in terms of the SNR in a receptor. It is important to have an information measure to
compare with. Ignoring correlations between receptors gives independent Gaussian signals
in each receptor, which maximizes the information rate at a given SNR. Spatial redundancy
is measured as the difference rind - Z,
which tells us how much information capacity is
effectively wasted due to spatial correlations. For a Gaussian channel at a given SNR (ratio
of signal variance to noise variance),
The quantities are compared in figure 21 for SNR ranging from 1 to 1000. The spatial
redundancy is always greater than a factor of two. Perhaps more interestingly, each receptor
only conveys a few bits of information per image, which seems quite small. In the fovea
there is approximately one ganglion cell fiber leaving the eye for each receptor . If there
are 20 new images per second presented to the eye (an upper bound), then each ganglion cell
would require an information capacity of about 50 baud, which is well within the 300 baud
or so capacity estimated in some neurons . If the one million or so optic nerve fibers
must each convey this much information, then the whole optic nerve should operate at about
50 Mbaud, which is five times the capacity of an Ethernet cable. Of course, new random
images do not appear every 50ms; instead they are highly correlated over time. So the true
figure is certainly much less than this, possibly by many orders of magnitude.
Such structure is not evident outside the central 1' or so ofthe human retina where the situation is complicated
by the presence of rods. Cones become much more sparse and randomly arranged in the periphery. Our model
receptor lattice has theheproperties of a very large fovea.
- 1 log [I -!- SNR] .
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
D L Ruderman
Figure 21. Upper bound to the information per image per receptor as a function of SNR for
natural scenes with 11 = 0.19 (lower curve), and the information capacity of each receptor (upper
10. Conclusions
Previous work has suggested that natural scenes possess scale invariance. We have also
shown this to be the case through the scaling of the power specmm, local histograms, and
pixel information. Our data also demonstrate an approximate invariance to a new symmetry:
variance normalization. Images of local pixel variances seem to have the same statistics
as the image intensities themselves. Furthermore this process may work recursively, as
implied by our data.
Understanding the appearance of scale invariance is straightforward. It seems reasonable
that objects in the image can occur at any distance, and some objects even range in size-
trees and rocks for instance.. When captured in images this means they span many angular
scales, and this can produce a scaleinvariant ensemble. Another possibility is that the
objects and scenes themselves are self-similar, as suggested by the success of fractal image
generation and fractal image compression methods .
The reasons for the variance normalization invariance are not as intuitive, however.
We sought a method for removing the non-Gaussian image statistics. At the same time
an invariance appeared. Either we were lucky or there is something fundamental about
this type of procedure. It may be a very salient property of natural images that they can
be 'Gaussianized' via this iterative scheme. As the nonlinearities present in the visual
system begin to be systematically studied, it is important to understand what effect they
have on the encoding of images. The outcome of variance normalization suggests that these
nonlinearities may prove to be a very rich area of research.
The statistics we have measured are low-dimensional projections of a high-dimensional
probability distribution. Consider the beer foam analogy once again. Suppose it has smaller
bubbles in the middle than elsewhere, and so the fluid density is greater there. A two-
dimensional projection of the foam (i.e. a marginal distribution created by averaging over
one variable) will show this increase in density but not the fact that it is made of bubbles.
Similarly, one may have to search in very high dimensions in order to visualize the true
complexity of the natural image distribution. Finding invariances in the distribution helps
us to reduce its inhinsic dimensionality.
While the image statistics are interesting on their own, they also have practical use.
Image compression algorithms work through a combination of knowledge about image
statistics and psychophysical thresholds . Image restoration procedures ultimately rely
Network Downloaded from informahealthcare.com by University of Waterloo
For personal use only.
The sratisrics of natural images
on prior or ‘Bayesian’ knowledge of the image ensemble . But perhaps the most
exciting application of scene statistics is in understanding sensory processing in vision. In
order to answer the question ‘How well designed is a creature’s visual system?, we need
three things: a criterion of merit, a set of design constraints, and natural image statistics.
Measuring natural scenes is essential to gaining a truly ecological understanding of vision.
Acknowledgments
I am indebted to my collaborators-William Bialek, Horace Barlow, and Chris Wroe-for
inspired interactions. I thank Joseph Atick, Roland Baddeley, David Field, Simon Laughlin,
Albert Libchaber, Adar Pelah, and Yoav Tadmor for enlightening discussions. This work
was supported in part by the Fannie and John Hertz Foundation, the NEC Research Institute,
NSF Grant INT-9301746, and The Physiological Laboratoyat Cambridge.
All images were taken at eye level (about 1.7 m) from random locations in the park. Due
to the presence of deer ticks carrying Lyme ,disease most images were taken from positions
on trails; this may cause a systematic bias in image content. The lens was set to focus at
infinity, and all shots were at an aperture of f/5.6. Due to the optics’ limited depth of field,
nearby objects would appear out of focus. If any were present in a scene, the azimuthal
angle of the camera was changed just enough to remove the offending object from view.
The camera’s elevation angle was no more than about &lo” in any image.
Images were gathered using a Sony Mavica MVC-5500 still video CCD camera equipped
with a 9.5-123.5 mm zoom lens. This device writes analogue video frames onto small
diskettes which are later read off a playback unit (Sony MVR-6500). The video signal is
NTSC format RGB. These three signals (red, green and blue) were digitized to 8 bits (0-255)
using a Silicon Graphics VideoLab board. To reduce the effects of analogue playback noise
we average the result of 32 frame captures to produce floating-point numbers in the range
0-255. Note that since analogue noise is present in the system before digitiiation, repeated
quantization followed by averaging alows us to get around quantization noise.
We use the CIE luminance as a signal. It is derived from the data using the formula 
Y =0.59G+0.3R+0.115.
This quantity was calibrated against grey cards of known reflectance to give an intensity
signal, Z(Y), which is proportional to the illuminant flux through the lens. Since the camera
had a limited dynamic range, saturation was a possibility. We discard images that have
more than about 1% of the pixels saturating. Those pixels that do saturate are set either to
the lowest calibrated luminance value or the highest.