The Random Forest Kernel
and creating other kernels for big data from random partitions
Alex Davies
 
Dept of Engineering, University of Cambridge Cambridge, UK
Zoubin Ghahramani
 
Dept of Engineering, University of Cambridge Cambridge, UK
We present Random Partition Kernels, a new
class of kernels derived by demonstrating a natural connection between random partitions of objects and kernels between those objects.
show how the construction can be used to create kernels from methods that would not normally be viewed as random partitions, such as
Random Forest. To demonstrate the potential of
this method, we propose two new kernels, the
Random Forest Kernel and the Fast Cluster Kernel, and show that these kernels consistently outperform standard kernels on problems involving
real-world datasets. Finally, we show how the
form of these kernels lend themselves to a natural approximation that is appropriate for certain big data problems, allowing O(N) inference
in methods such as Gaussian Processes, Support
Vector Machines and Kernel PCA.
1. Introduction
Feature engineering is often cited as the “the most important factor” for the success of learning
algorithms. In a world of kernelized algorithms this means,
unsurprisingly, that the kernel is the most important factor
in the success of the underlying algorithm. Since there will
likely never be a single best kernel for all tasks , the best hope for kernelized modelling
is to have many available standard kernels and methods to
easily generate new kernels from knowledge and intuition
about the problem domain.
The most commonly used kernels are generally analytically derived. An informal review of kernel machine papers shows that despite a large number of available kernels,
for real valued inputs practitioners overwhelmingly use a
very small subset: Linear, Periodic, Radial Basis Function (RBF), Polynomial, Spline and ANOVA, with RBF
being far and away the most common, and almost exclusively used for high-dimensional data. These kernels are
empirically effective for many tasks and there are instances
where there are compelling arguments that they are accurate models of the data. However, it is unfortunate when
they are used as “default” kernels, encoding at best a bias
towards smoothness, which even then may not be the best
assumption in real-world, high dimensional problems.
These also leave little recourse for constructing new kernels. They can be combined through the use of certain operators such as + and × and some work has been done towards automatically searching these structures , but this is a time consuming and difﬁcult
task. An example of a general purpose method for constructing kernels are Fisher kernels . Fisher kernels provide a method to construct a sensible kernel from a generative model of the data. For cases
where there is a natural generative model of the data, this
is a compelling and intuitive kernel.
Clustering has a long history in machine learning and statistics precisely because it as a very simple and intuitive task.
Originally, clustering methods were designed to ﬁnd “the
best” partition of a datset. More recent probabilistic methods allow uncertainty and instead learn a distribution on
what the clusterings of the data might be. These distributions are known as partition distributions, and samples from
them are random partitions.
In this paper we show how it is very simple to deﬁne a distribution over partitions, and how many existing algorithms
trivially deﬁne such distributions. We show how a kernel
can be constructed from samples from these distributions
(random partitions) and that these kernels outperform common kernels on real-world data. We also show how the
 
The Random Forest Kernel (and creating other kernels for big data from random partitions)
construction of the kernel leads to a simple approximation
scheme that allows for scalable inference in SVMs, Kernel
PCA and GP regression.
2. Background
2.1. Kernel methods
A kernel is a positive semi-deﬁnite (PSD) function
R that is used to indicate the
similarity of two points in a space I.
For a dataset
X = {⃗x1, ..., ⃗xN}, the Gram matrix of X is K ∈RN×N,
where Kab = k(⃗xa, ⃗xb). Many machine learning problems
can be formulated in terms of this Gram matrix, rather
than the more traditional design matrix X. These methods
are collectively refered to as kernel machines (Herbrich,
The choice of k (and thus K) is critical to the
predictive performance of any of these algorithms, in the
same way that the choice of a sensible feature space is
to traditional methods. This is because the choice of the
kernel implicitly sets the feature space being used: the kernel can always be viewed as k(⃗xa, ⃗xb) = ⟨φ(⃗xa), φ(⃗xb)⟩,
where φ is the implicit feature space mapping.
2.2. Iterative methods and pre-conditioning
One issue with kernel machines is that they can be very
computationally expensive. To simply evaluate every element in a Gram matrix requires O(N 2) operations, and
many kernel machine algorithms are O(N 3). Fortunately
in some cases the solutions to these problems can be found
using matrix-free iterative solvers. These solvers are called
matrix-free because they do not require the full Gram matrix K, only the ability to calculate K⃗v for arbitrary ⃗v.
When K can be stored in less that O(N 2) space, and K⃗v
calculated in less than O(N 2) time, these methods can offer great computational savings over direct methods.
Matrix-free iterative methods include conjugate gradient
 , for ﬁnding the minimum of a quadratic
form and power iteration for ﬁnding the
eigenvectors of a matrix. Three common kernel machines
that can use matrix free methods are Gaussian Processes;
calculating the posterior mean requires a solution to K−1⃗y,
which can be solved via conjugate gradient , SVMs, which can be solved using gradient descent
on the objective, where the derivative only depends on Kα
and Kernel PCA, which can be solved with power iteration.
The numerical stability and convergence rate of iterative
methods are directly related to the condition number of the
Gram matrix κ(K). A low value of the condition number
will lead to a numerically stable solution in a small number
of iterations, whereas a problem with a very high condition
number may not converge at all. Conjugate gradient, for
example, has a convergence rate of
If we can construct a matrix B, for which we can efﬁciently
evaluate B⃗v, and where κ(BK) ≪κ(K), we can perform
these iterative methods on the transformed system BK.
This matrix B, known as a pre-conditioning matrix, can
be thought of as an “approximate inverse”, that brings K
closer to the identity matrix (which has the lowest possible
condition number of 1 and would require only one iteration
to solve).
2.3. Random partitions
A cluster C in a dataset D is a non-empty subset of D. A
partition of D is the segmentation of D into one or more
non-overlapping clusters ϱ = {C1, ..., Cj}. ie:
The following is an example dataset and an example partition of that dataset:
{a, b, c, d, e, f}
{{a, e}, {c}, {d, b, f}}
A random partition of D is a sample from a partition
distribution P.
This distribution is a discrete pdf that
represents how likely a given clustering is. We will use
the notation ϱ(a) to indicate the cluster that the partition ϱ
assigns to point a.
Random partitions have been studied extensively in the
ﬁeld of non-parametric Bayesian statistics.
Well-known
Bayesian models on random partitions are the Chinese
Restaurant Process , the Pitman-Yor Process
 and the Mondrian Process , while many other combinatorial models can
be used to deﬁne random partitions.
2.4. Deﬁning distributions with programs
Ordinarily a distribution is deﬁned by its probability density function, or alternatively by some other sufﬁcient description such as the cumulative density function or moment generating function. However, a distribution can also
be deﬁned by a program that maps from a random seed R to
the sample space of the distribution. This program can be
viewed in two ways: ﬁrstly, as a transformation that maps
The Random Forest Kernel (and creating other kernels for big data from random partitions)
from a random variable to a new output space, and alternatively as a program that generates samples from the desired
distribution.
Thus any program that takes a datset as input and outputs a
random partition deﬁnes a partition distribution. This is the
representation that will allow us to easily construct Random Partition Kernels.
3. Random Partition Kernels
A partition distribution naturally leads to a kernel in the
following way:
Deﬁnition 1. Given a partition distribution P, we deﬁne
the kernel
kP(a, b) = E [I [ϱ(a) = ϱ(b)]]ϱ∼P
to be the random partition kernel induced by P, where I is
the indicator function.
That is, the kernel is deﬁned to be the fraction of the time
that two points are assigned to the same cluster.
Lemma 3.1. kP(a, b) constitutes a valid PSD kernel.
Proof. First we deﬁne:
kϱ(a, b) = I [ϱ(a) = ϱ(b)]
To prove that kP is PSD, we decompose the expectation
into the limit of a summation and show that the individual
terms are PSD.
kP(a, b) = E [I [ϱ(a) = ϱ(b)]]ϱ∼P
I [ϱ(a) = ϱ(b)]
For any dataset of size N, the kernel matrix for kϱ will be
an N×N matrix that can be permuted into a block diagonal
matrix of the following form:
where 1 is a matrix with all entries equal to 1, 0 is a matrix
will all entries 0 and Z is a permutation matrix. Each 1
matrix represents a single cluster.
From this we can conclude that ZKϱZ⊺is PSD, as it is a
block matrix of PSD matrices. Further, since a permutation
does not affect the eigenvalues of a matrix, Kϱ is PSD.
Since Kϱ is PSD for any dataset, kϱ must be a valid kernel.
Finally, since a linear combination of kernels with positive
coefﬁcients is also a valid kernel, kP is a valid kernel.
3.1. m-approximate Random Partition Kernel
However in many instances, such as when deﬁning a partition distribution with a program, it is not possible to analytically evaluate this quantity. Fortunately, its structure allows for a simple approximation scheme that only requires
the ability to sample from the distribution P.
Deﬁnition 2. The m-approximate Random Partition Kernel is the fraction of times that ϱ assigns a and b to the same
cluster over m samples.
kP(a, b) ≈˜kP(a, b) = 1
Lemma 3.2. If the samples from P are independent then
the bound on the variance of the approximation to kP(a, b)
Proof. If ϱ are independent samples from P, then
kϱ(a, b) ∼Bernoulli (kP(a, b))
and ˜kP(a, b) is the maximum likelihood estimator for
kP(a, b). The variance of the ML estimator for a Bernoulli
is bounded by
3.2. The Colonel’s Cocktail Party
This process of evaluating the kernel described in Deﬁnition 2 can be described succintly using a metaphor in the
tradition of the CRP and IBP .
We consider The Colonel, who is the host of a cocktail
party. He needs to determine the strength of the afﬁnity
between two of his guests, Alice and Bob. Neither Alice
and Bob, nor the other guests, must suspect the Colonel’s
intentions, so he is only able to do so through surreptuous
observation.
At the beginning of the evening, his N guests naturally
form into different groups to have conversations. As the
evening progresses, these groups naturally evolve as people
The Random Forest Kernel (and creating other kernels for big data from random partitions)
leave and join different conversations. At m opportunities
during the course of the evening, our host notes whether
Alice and Bob are together in the same conversation.
As the Colonel farewells his guests, he has a very good idea
of Alice and Bob’s afﬁnity for one another.
3.3. Efﬁcient evaluation
As stated in Section 2.2, a large class of kernel algorithms
can be efﬁciently solved if we can efﬁciently solve K⃗v for
arbitrary ⃗v. A partition matrix Kϱ can be stored in N space
and multiplied by a vector in 2N operations using the analytic form in Eq 8.
It follows that the kernel matrix for an m-approximate
Random Partition Kernel requires mN space and 2mN
operations for a matrix-vector product.
In an iterative
algorithm this leads to an overall complexity of 2mNI
operations.
For the Random Partition Kernel, we propose the following
pre-conditioning matrix:
(Kϱ + σI)−1
Where σ is set as a small constant to ensure the matrix is
invertible. Due to the simple form of the individual cluster matrices, we can compute (Kϱ + σI)−1 ⃗v in only 2N
operations and B⃗v in 2mN operations using the analytic
form in Eq 10.
(Kϱ + σI)−1 ⃗v
|ϱ(i)| + σ
This small multiplicative overhead to the iterative solver
greatly reduces the condition number and the number of
iterations required for a solution in most cases.
4. Constructing Kernels
To demonstrate the potential of this method, we ﬁrst show
how to obtain random partitions from a large class of existing algorithms. Then we select two example partition
distributions to propose two novel kernels for real-world
data: the Random Forest Kernel and the Fast Cluster Kernel. We also show examples of how some common types
of data can be directly interpreted as random partitions.
4.1. Example random partitions
To demonstrate the ease with which sensible random partitions can be constructed from exisiting machine learning
algorithms, we provide a number of examples. This list is
by no means exhaustive, but serves as an example of how
random partitions can be deﬁned.
4.1.1. STOCHASTIC CLUSTERING ALGORITHMS
Any standard clustering algorithm and DBSCAN being just a couple of examples) generates a partition for a given dataset.
Adding any element of randomness to the algorithms (if
it does not already exist) results in a stochastic clustering
algorithm.
Elements of the clustering algorithm that can be randomized include:
1. Initializations
2. Number of clusters
3. Subsets of the data (Bagging)
4. Subset of the features
5. Projections of the features
The output for a stochastic clustering algorithm is a random
partition. As a simple concrete example, the output of Kmeans with random restarts returns a random partition.
4.1.2. ENSEMBLED TREE METHODS
A large class of random partitions can be derived from
ensembled tree methods.
We use this term to refer to
any method whose output is a collection of random trees.
By far the most well known of these are Random Forest
 and Boosted Decision Trees , though it also includes Bayesian Additive
Regression Trees , Gradient Boosting Machines with decision trees and
many others.
As a tree deﬁnes a hierarchical partition, it can be converted
to a partition by randomly or deterministically choosing a
height of the tree and taking the partition entailed by the
tree at that height. A more detailed example of this is outlined in the Random Forest Kernel algorithm in Section
4.2. Data-deﬁned partitions
There are a number of real-world examples where the data
can be directly interpreted as samples from a random partition.
The Random Forest Kernel (and creating other kernels for big data from random partitions)
Algorithm 1 Random Forest Partition Sampling Algorithm
Input: X ∈RN×D, ⃗y ∈RN, h
T ∼RandomForestTree(X, ⃗y)
d ∼DiscreteUniform(0, h)
for a ∈D do
leafnode = T(a)
ϱ(a) = ancestor(d, leafnode)
4.2.1. COLLABORATIVE FILTERING
For example, in collaborative ﬁltering for movie recommendations, we can cluster users by randomly sampling a
movie and splitting them into two clusters: viewed and not
viewed. Similarly, we can cluster movies but splitting them
into clusters for which were viewed, or not, by a particular
4.2.2. TEXT ANALYSIS
Similarly, when considering text mining applications, we
can view documents as binary partitions of words: those in
the document and those that are not.
4.3. Example kernels
To further understand the kernels and perform experiments,
we select two example random partitions to construct the
Random Forest Kernel and the Fast Cluster Kernel.
4.3.1. RANDOM FOREST KERNEL
To construct the Random Forest Kernel, we deﬁne the
following random partition sampling scheme, summarized
in Algorithm 1.
Firstly a tree T is generated from the Random Forest
algorithm.
A tree generated by Random Forest is a
decision tree that is trained on a subset of the data and
features; however, every datapoint in D is associated with
a leaf node.
For simplicity of explanation, we assume
that T is a binary tree with 2h nodes, but this is not
necessary in general.
Next a height is sampled from
d ∼DiscreteUniform(0, h) which determines at what
level of the tree the partitioning is generated. The nodes of
T at height d of the tree indicate the different clusters, and
each datapoint is associated to the cluster whose node is
the ancestor of the datapoint’s node.
Using this partition sampling scheme in Deﬁnition 2 gives
us the Random Forest Kernel. This kernel has a number of
interesting properties worth higlighting:
(1) When used in Gaussian Processes, it is a prior over
Random Forest
Fast Cluster
Figure 1. GP posterior distribution on toy regression dataset
piece-wise constant functions. This is true of all Random
Partition Kernels. The posterior of a Gaussian Process on
a toy 1-D dataset is shown in Figure 1.
(2) The GP prior is also non-stationary. A property of
real-world datasets that is often not reﬂected in analytically
derived kernels is non-stationarity; where the kernel depends on more than just the distance between two points.
(3) It has no hyper-parameters. This kernel has no hyperparameters, meaning there is no costly hyper-parameter
optimization stage. This normal extra “training” phase in
GPs and SVMs is replaced by the training of the random
forests, which in most cases would be more computationally efﬁcient for large datasets.
(4) It is a supervised kernel. While generally kernels are
unsupervised, there is nothing inherently incorrect about
the use of a supervised kernel such as this, as long as
it is only trained on the training data. While this could
reasonably lead to overﬁtting for regression/classiﬁcation,
the experiments do not show any evidence for this. It can
be used for supervised dimensionality reduction, a plot of
which is shown in Figure 2.
The Random Forest Kernel (and creating other kernels for big data from random partitions)
Fast Cluster PCA
Random Forest PCA
Figure 2. PCA vs FC Kernel PCA vs RF Kernel PCA on ‘bodyfat’
4.3.2. FAST CLUSTER KERNEL
The Fast Cluster Kernel is based around a fast random
partition algorithm inspired by properties of Random
Forest. The algorithm is as follows: First, sample a subset
of the dimensions, then select a random number of cluster
Algorithm 2 Random Clustering Partition Algorithm
Input: X ∈RN×D, h
⃗d ∼Bernoulli(.5, D)
s ∼DiscreteUniform(0, h)
C ∼Sample([1, ..., N], 2s)
for a ∈D do
ϱ(a) = argminc∈C
∥(Xc −Xa) ⊙⃗d∥
Assign every point to the cluster associated
with its nearest center measured only on the subsampled
dimensions. This is outlined in Algorithm 2.
This cluster scheme was constructed largely for it’s simplicity and speed, scaling as O(N). From Figure 1 we
can see that once again it results in a distribution over
piece-wise constant functions that is non-stationary, with
smoother variances than the Random Forest kernel.
is also worth noting that when being used for classiﬁcation/regression the Fast Cluster kernel can easily be used in
a “semi-supervised” manner, by training it on a larger set
of X than is used for the ﬁnal learning task.
5. Experiments
5.1. Kernel Efﬁcacy
We compare the results of using the Random Forest kernel against the Linear kernel and the Radial Basis Function with and without Automatic Relevance Detection using standard GP training methodology1 on 6 real-world regression problems from the UCI Machine Learning repository. Both Random Forest and Fast Cluster were trained
with m = 200.
The evaluation criteria that is most important for a Gaussian Process is the test log-likelihood, as this directly shows
how well the posterior distribution has been learnt. It factors in not only how well the test data has been predicted,
but also the predicted variances and covariances of the test
data. Figure 3 shows that both the Random Forest kernel
and Fast Cluster kernel greatly outperform the standard kernels with respect to this measure on a variety of datasets.
The graph shows the test log-likelihood on a symmetric log
scale as the discrepancy is very large.
As GPs are often used simply as a point estimate regression
method, it is still important to assess the kernels performance with respect to MSE. Figure 4 shows that the Random Forest kernel predicts better in the majority of cases,
though the improvement is not as pronounced as the im-
1Each kernel is optimized for training log-likelihood using a
gradient-based optimizer with 20 random restarts.
The Random Forest Kernel (and creating other kernels for big data from random partitions)
provement in log-likelihood. This shows that while the the
resulting joint predictive posterior for the standard kernels
can be very poorly ﬁt, they may still result in accurate posterior mean predictions.
Figure 3. Test log-likelihood on real-world regression problems
(higher is better)
Figure 4. Test MSE on real-world regression problems (lower is
5.2. Approximation quality
To understand the effect of the kernel approximation on the
resulting predictions, we graph the MSE performance of
both kernels for different values of m. In Figure 5 we can
see that for low samples, the Random Forest kernel performs nearly as well as its optimum, whereas the Fast Cluster kernel improves continually untill around 200 samples.
This appears roughly consistent with what we might expect from the theoretical convergence of the kernel, as this
equates to an error variance of approximately .002, which
is a quite small element-wise error for a well-conditioned
matrix with elements in the range .
Figure 5. Log-likelihood vs m for ‘mpg’ dataset
5.3. Scalability
To show that the scalability of the iterative algorithms is
as we would expect from the computational analysis in
Section 3.3, we show the runtime for Kernel PCA with the
different kernels. Both Random Forest and Fast Cluster
are trained with 100 random partitions. Note that Squared
Exp PCA has a constant advantage, as the code is executed
almost entirely in C, whereas Random Forest and Fast
Cluster are partially executed in python. The experiments
were run on a commidity PC with an Intel Core 2 Duo
E8500 CPU and 8GB RAM.
Figure 6 is a log-log plot, so the exponent of the scaling
can be seen in the gradient of the curves. As predicted, the
scaling of Fast Cluster is O(N), processing around 100,000
points per minute on this machine. In practice, for these
size datasets, Random Forest seems to scale as approximately O(N 1.5), while the RBF remains faithful to its theoretical O(N 3).
As a comment on future scaling, it is worth noting here
that both the Fast Cluster and Random Forest algorithms,
as well as the matrix-vector product, are trivially parallelizable.
6. Conclusion
We have presented the Random Partition Kernel, a novel
method for constructing effective kernels based on a connection between kernels and random partitions. We show
The Random Forest Kernel (and creating other kernels for big data from random partitions)
Figure 6. Log-log plot of dataset size vs execution time for kernel
how this can be used to simply and intuitively deﬁne a large
set of kernels, many trivially from existing algorithms.
The example kernels constructed using this method, the
Random Forest kernel and Fast Cluster kernel, both show
excellent performance in modelling real-world regression
datasets with Gaussian Processes, substantially outperforming other kenels. We also demonstrate that the kernel allows for linearly scaling inference in GPs, SVMs and
kernel PCA.