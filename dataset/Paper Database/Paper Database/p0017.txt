IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 43, NO. 3, MARCH 2005
Investigation of the Random Forest Framework for
Classiﬁcation of Hyperspectral Data
JiSoo Ham, Yangchi Chen, Melba M. Crawford, Senior Member, IEEE, and Joydeep Ghosh, Senior Member, IEEE
Abstract—Statistical classiﬁcation of byperspectral data is
challenging because the inputs are high in dimension and represent multiple classes that are sometimes quite mixed, while the
amount and quality of ground truth in the form of labeled data is
typically limited. The resulting classiﬁers are often unstable and
have poor generalization. This paper investigates two approaches
based on the concept of random forests of classiﬁers implemented
within a binary hierarchical multiclassiﬁer system, with the goal
of achieving improved generalization of the classiﬁer in analysis
of hyperspectral data, particularly when the quantity of training
data is limited. A new classiﬁer is proposed that incorporates bagging of training samples and adaptive random subspace feature
selection within a binary hierarchical classiﬁer (BHC), such that
the number of features that is selected at each node of the tree is
dependent on the quantity of associated training data. Results are
compared to a random forest implementation based on the framework of classiﬁcation and regression trees. For both methods,
classiﬁcation results obtained from experiments on data acquired
by the National Aeronautics and Space Administration (NASA)
Airborne Visible/Infrared Imaging Spectrometer instrument over
the Kennedy Space Center, Florida, and by Hyperion on the NASA
Earth Observing 1 satellite over the Okavango Delta of Botswana
are superior to those from the original best basis BHC algorithm
and a random subspace extension of the BHC.
Index Terms—Binary hierarchical classiﬁer (BHC), classiﬁcation, classiﬁcation and regression trees (CART), Hyperion,
hyperspectral, Okavango Delta, random forests, random subspace
feature selection.
I. INTRODUCTION
HE INCREASING availability of data from hyperspectral
sensors, particularly with the launch of the Hyperion instrument on the National Aeronautics and Space Administration (NASA) Earth Observation 1 (EO-1) satellite, has generated
tremendous interest in the remote sensing community. These instruments characterize spectral signatures with much greater detail than traditional multispectral sensors and thereby can potentially provide improved discrimination of targets . However,
hyperspectral data also present difﬁcult challenges for supervised statistical classiﬁcation, where labeled training data are
Manuscript received March 24, 2004; revised December 7, 2004. This work
was supported in part by the National Aeronautics and Space Administration
Earth Observing 1 Program under Grant NCC5-463, in part by the Terrestrial
Sciences Program of the Army Research Ofﬁce under Grant DAAG55-98-1-
0287, and in part by the National Science Foundation under Grant IIS-0312471.
J. Ham, Y. Chen, and M. M. Crawford are with the Center for Space Research, The University of Texas at Austin, Austin, TX 78759 USA (e-mail:
 ; ; ).
J. Ghosh is with the Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712 USA (e-mail:
 ).
Digital Object Identiﬁer 10.1109/TGRS.2004.842481
used to estimate the parameters of the label-conditional probability density functions . The dimensionality of the data is
; there are often tens of classes C; and the quantity
of training data is often small. Sample statistics of training data
may also not be representative of the true probability distributions of the individual class signatures, particularly for remote,
inaccessible areas where training data are logistically difﬁcult
and expensive to acquire. Generalization of the resulting classiﬁers is often poor, thereby resulting in poor quality mapping
over extended areas.
Various approaches have been investigated to mitigate the impact of small sample sizes and high dimensionality, which are
inherently coupled issues, since the adequacy of a data sample
depends on the data dimensionality, among other factors . For
example, regularization methods try to stabilize the covariance
matrix by weighting the sample covariance matrix and a pooled
covariance matrix or by shrinking the sample covariance matrix toward the identity matrix . While this may reduce the
variance of the parameter estimates, the bias of the estimates
can increase dramatically. Alternatively, the input space can be
transformed into a reduced feature space via feature selection
 or feature extraction. Although these two approaches reduce
the effect of the high-dimensionality problem, feature selection methods are often trapped in a local optimal feature subset,
while feature extraction methods lose the interpretability of the
original features. Another way of dealing with a small training
set is to augment it with unlabeled data and then use semisupervised learning techniques. These methods have been shown
to enhance supervised classiﬁcation , . However, convergence of the updating scheme can be problematic, and it is affected by selection of the initial training samples and by outliers.
In analysis of hyperspectral data, Lee and Landgrebe
proposed methods for feature extraction based on decision
boundaries that maximize separation of data in multiple
two-class problems . These decision boundary feature
extraction (DBFE) methods are often effective for two-class
problems, but do not exploit correlation between sequential
bands. Jia and Richards developed the segmented principal
components transformation (SPCT) whereby the original bands
are grouped into subsets of highly correlated adjacent bands
to which the Karhunen–Loeve transform is applied. The most
signiﬁcant principal components are then selected from each
subset to yield a feature vector with reduced dimension .
The approach treats interband correlation globally and does not
guarantee good discrimination capability because the principal
components transformation preserves variance in the data
rather than maximizing discrimination between classes. Kumar
et al. investigated band-combining techniques, motivated by
0196-2892/$20.00 © 2005 IEEE
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
HAM et al.: INVESTIGATION OF THE RANDOM FOREST FRAMEWORK
best basis functions, as a means of feature extraction in a
pairwise classiﬁer framework . Adjacent bands are selected
for merging (alternate splitting) in a bottom-up (alternate top
down) fashion using the product of a correlation measure and
a Fisher discriminant. Morgan et al. suggested a similar
correlation-based band-combining approach, in conjunction
with a covariance shrinkage method, for both a top-down
and bottom-up hierarchical classiﬁer to ameliorate the small
training data problem.
The theory and practice of classiﬁer ensembles also provide
ways of alleviating sample size and high-dimensionality concerns . Bagging involves bootstrapped sampling of the original data, generating a classiﬁer speciﬁc to each sample, and
then averaging the classiﬁer outputs . This method takes
advantage of data reuse, but when the training dataset in the
(sub-)sample is very small, the potential for improved diversity and reduced impact of outliers is offset by degradation in
individual classiﬁer performance . Boosting also combines
weak individual classiﬁers to develop an improved classiﬁer,
but by reweighting training data to increase sensitivity to incorrectly classiﬁed training observations. While boosting can improve performance for large training samples, it is not useful for
small sample problems, particularly in the presence of outliers.
When the input space is large, random subspace (RS) feature
selection can potentially provide improved classiﬁer diversity,
while stabilizing parameter estimates, by randomly reducing the
number of inputs to each classiﬁer in the ensemble and constructing multiple classiﬁers in the resulting random input space
 , . The method is potentially attractive for problems with
redundant input features (e.g., hyperspectral data) and when outliers exist in the training data. Recently, approaches referred to
as “random forests of classiﬁers” have been proposed. These
involve developing multiple trees from randomly sampled subspaces of input features, then combining the resulting outputs
via voting or a maximum a posteriori rule . These methods
typically achieve superior generalization for small training samples, but are computationally intensive.
Land cover classiﬁcation problems usually involve a large
number of classes, i.e., the output space is large. Output
decomposition using binary classiﬁers in a multiclassiﬁer
framework has been shown to be more successful than traditional 1-ofclassiﬁers for many problems involving large
output spaces . Decomposition methods using pairwise
classiﬁers, error-correcting output codes (ECOC) , and
binary decision trees have all been investigated in this context (see for an overview). Pairwise classiﬁers develop a
separate classiﬁer for each pair of classes, thereby resulting in
classiﬁers that must be combined to determine the ﬁnal
class label. These methods often yield simple classiﬁers with
excellent discrimination for speciﬁc pairs, but are generally
inefﬁcient for problems with a large number of output classes.
In the ECOC, a
-class problem is decomposed into
problems, whereby the original class is then encoded into a
binary vector of a coding matrix. It has been shown that the
ECOC method yields robust, stable classiﬁers. However, since
the code matrix design is not based on the characteristics of the
classes it represents, interpretability of the classiﬁer is limited.
Binary trees, which often provide an attractive approach for
decomposing large output space problems, can be constructed
using a variety of splitting functions involving single or multiple features and output classes. To address the high-dimensional output problem while exploiting the afﬁnity for spectrally
similar classes, Kumar et al. proposed a binary hierarchical classiﬁer (BHC) to decompose a
-class problem into
a binary hierarchy of
simpler two-class problems that
can be solved using a corresponding hierarchy of classiﬁers,
each based on a simple linear discriminant. The method was
extended by Morgan et al. for small training samples using
an adaptive best basis BHC, which exploits the class-speciﬁc
correlation structure between sequential bands of hyperspectral
data and utilizes an adaptive regularization approach to stabilize covariance estimates. An adaptive random subspace feature
selection approach was also investigated within the BHC framework (RS-BHC) as a means of improving classiﬁer performance
when the number of training samples is extremely small .
In this paper, we investigate a random forest of binary
classiﬁers as a means of increasing diversity of hierarchical
classiﬁers. We evaluate the results obtained for trees produced
by our BHC classiﬁer and the original classiﬁcation and regression trees (CART)-based random forest method . For
the BHC, the goal is to exploit the advantages of natural class
afﬁnity, while improving generalization in classiﬁcation of
hyperspectral data when the number of training samples is
small. The CART-based approach is not directly affected by
small sample size statistics and potentially provides greater
diversity within the forest, but typically produces trees of
enormous size if the output space is large. The paper is organized as follows: the best basis (BB-BHC), random subspace
(RS-BHC), and random forest (RF-BHC) implementations of
the BHC method and the CART-based framework (RF-CART)
are all described in Section II; classiﬁcation results using the
random forest approaches obtained for data acquired by the
Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) over
the Kennedy Space Center, Florida, and EO-1 Hyperion over
the Okavango Delta, Botswana, are presented in Section III and
compared to those obtained from the BB-BHC and RS-BHC.
Results from all the methods are evaluated, and new directions
for future work are suggested in Section IV.
II. RANDOM FOREST BINARY HIERARCHICAL
CLASSIFICATION METHOD
The top-down BHC framework recursively decomposes a
-class problem into
two-(meta)class problems via
a deterministic simulated annealing method . The root
classiﬁer tries to optimally partition the original set of classes
into two disjoint metaclasses while simultaneously determining
the Fisher discriminant that separates these two subsets. This
procedure is recursed, i.e., the metaclass
is partitioned into two metaclasses
, until the original
classes are obtained at the leaves. The tree structure, as
shown in Fig. 1, allows the more natural, easier discriminations
to be accomplished earlier. Fewer classes are involved in the
partitioning at lower levels of the BHC hierarchy. Thus, while
the classiﬁcation task typically becomes simpler, the number
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 43, NO. 3, MARCH 2005
Binary hierarchical (multi)-classiﬁer framework for solving a C-class
of relevant training samples also decreases. The BB-BHC
ameliorates this effect by utilizing an ancestor covariance
matrix while exploiting the interband serial correlation through
an adaptive class-dependent band aggregation process . A
band-combining step is performed on highly correlated spectrally adjacent bands prior to the partitioning of metaclasses,
thereby reducing the number of inputs relative to the number of
training data points. Bands are aggregated until a user-deﬁned
between the number of training samples for the respective (meta)classes and input dimension is achieved. Typically,
is selected to be at least 5.
The BHC method is extended in the RS-BHC approach by
utilizing the random subspace method as a postprocessing stage
to tree construction, with the goal of reducing the number of
inputs while reﬁning decision boundaries . The BB-BHC
method is used to ﬁrst construct the hierarchy, and then random
subspace sampling is performed at each node of the tree where
the criterion for
is not satisﬁed. For each (meta)class
vector-valued observations,
a subset of elements of
with dimension
is then randomly selected from the
-dimensional set of features. The resulting modiﬁed training
consists of observation vectors
where the same subset of features is
selected for each element
number of random subspaces selected at each such node is
, where the value of
is a user-supplied
input. A discriminant vector is constructed for each random
subspace, and the
vectors are combined at each node of the
hierarchy via majority voting. Our empirical evidence indicates
that good results are typically achieved for
provides adequate coverage of the feature space. Improvement
in classiﬁcation accuracy is not signiﬁcant for
The random forest implementation of the BHC (RF-BHC) extends the RS-BHC by incorporating random subspace feature
selection in the actual development of the tree. This is particularly advantageous, as random subspace sampling is performed
by the RS-BHC only at nodes where the ratio
is not exceeded. Thus, subsampling of the input features typically occurs only at lower levels of the tree, thereby limiting diversity.
For moderate sized training samples, bagging can increase diversity of the multiclassiﬁer system, so a bootstrap sample of
observations is selected for each tree in the RF-BHC. At each
metaclass node
, a random subspace of features of dimension
is selected to determine the decision
boundary for the classiﬁer at that node, where
is a user-selected input. To guarantee greater diversity, we choose
The tree is then developed using the resulting set of features selected at each node. The process is repeated to grow a forest
of identically, independently distributed random vectors associated with the individual trees.
The fundamental difference between BHC and other decision trees is that the former focuses on decomposing the output
space; partitioning of the input space occurs as a consequence.
Both RF-BHC and RF-CART use the random forest ensemble
method to increase the diversity of each base learning module,
then combine results of the individual modules (trees). While
Breiman’s CART-based random forest follows a typical binary
divide-and-conquer hierarchical scheme, it differs from the
BHC in the base learning module. The BHC uses the generalized framework for associative modular learning systems
(GAMLS) algorithm to split each node into metaclasses
that are separated by the maximum Fisher distance. Using a
sequence of binary tests, CART seeks the split that maximizes
the reduction of the impurity of the parent nodes and its two
child nodes as measured by the Gini index . The most
discriminating feature is selected to perform the split. Used in
the random forest context, a random subspace of the original
features is selected at each node of the tree, and the most
discriminating feature is then selected. Further, unlike the actual CART method, the RF-CART approach does not perform
pruning of nodes, as pruning reduces diversity of trees in the
forest. Analogous to the RF-BHC, each tree is grown using a
bootstrap sample of the training set.
III. RESULTS
Hyperspectral data from the following two sources were analyzed in this paper.
1) Kennedy Space Center, Florida: The NASA AVIRIS instrument acquired data over the Kennedy Space Center
(KSC), Florida, on March 23, 1996. AVIRIS acquires data
in 224 bands of 10-nm width with center wavelengths
from 400–2500 nm. The KSC data, acquired from an altitude of approximately 20 km, have a spatial resolution of
18 m. After removing water absorption and low signal-tonoise (SNR) bands, 176 bands were used for the analysis.
Training data were selected using land cover maps derived
from color infrared photography provided by KSC and
Landsat Thematic Mapper (TM) imagery. The vegetation
classiﬁcation scheme was developed by KSC personnel in
an effort to deﬁne functional types that are discernible at
the spatial resolution of Landsat and these AVIRIS data.
Discrimination of land cover for this environment is difﬁcult due to the similarity of spectral signatures for certain
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
HAM et al.: INVESTIGATION OF THE RANDOM FOREST FRAMEWORK
CLASS CODES, NAMES, AND NUMBER OF TRAINING SAMPLES
FOR KENNEDY SPACE CENTER AVIRIS
vegetation types. For classiﬁcation purposes, 13 classes
representing the various land cover types that occur in this
environment were deﬁned for the site (Table I). Classes 4
and 6 represent mixed classes.
2) Okavango Delta, Botswana: The NASA EO-1 satellite
acquired a sequence of data over the Okavango Delta,
Botswana in 2001–2004. The Hyperion sensor on EO-1
acquires data at 30-m pixel resolution over a 7.7-km strip
in 242 bands covering the 400–2500-nm portion of the
spectrum in 10-nm windows. Preprocessing of the data
was performed by the University of Texas Center for
Space Research to mitigate the effects of bad detectors,
interdetector miscalibration, and intermittent anomalies.
Uncalibrated and noisy bands that cover water absorption
features were removed, and the remaining 145 bands
were included as candidate features: . The data analyzed in this
study, acquired May 31, 2001, consist of observations
from 14 identiﬁed classes representing the land cover
types in seasonal swamps, occasional swamps, and drier
woodlands located in the distal portion of the delta
 . These classes were chosen to reﬂect the impact of
ﬂooding on vegetation in the study area. The class names
and corresponding numbers of ground truth observations
used in the experiments are listed in Table II. Classes 3
and 4 are both ﬂoodplain grasses that are seasonally inundated, but differ in their hydroperiod (the amount of time
inundated). Classes 9–11 represent different mixtures of
acacia woodlands, shrublands, and grasslands and are
named according to the dominant class. Training data
were selected manually using a combination of global
positioning system-located vegetation surveys, aerial
photography from the Aquarap project, and 2.6-m
resolution IKONOS multispectral imagery. The class
CLASS CODES, NAMES, AND NUMBER OF TRAINING AND SPATIALLY
DISJOINT TEST SAMPLES FOR BOTSWANA HYPERION DATA
priors for both datasets, as indicated by the labeled data,
are only moderately skewed. For simplicity, we assume
the class priors to be equal while developing the BHC
classiﬁer. This assumption shall be reconsidered later.
For both datasets, ten randomly sampled partitions of the labeled data were subsampled such that 75% of the data samples
were used for training and 25% for testing. In order to investigate the impact of the quantity of training data on classiﬁer performance, each of the training datasets was then randomly subsampled to create samples whose sizes corresponded to 50%,
30%, and 15% of the original labeled data. All classiﬁers were
evaluated against the same ten testing samples comprised of
25% of the original labeled data in order to isolate the impact of
sample size.
Experiments were performed using the BB-BHC, RS-BHC,
RF-BHC, and RF-CART. Although authors recommend various
values for the dimension of the random subspace and the number
of trees in a random forest, there do not appear to have been any
systematic studies of the issue to date. In the results reported
here, the ratio
was set at 5, and the value of
the RS-BHC method. In our experiments, the dimension of the
random subspace was determined adaptively in the BHC, but
was always selected such that the value of
was at least 5.
For the RF-BHC, the value of
was selected to be 20. In
order to have somewhat comparable inputs, 20 input features
were randomly selected in the RF-CART method. One hundred
trees were grown for each experiment, as our sensitivity studies
showed that larger forests did not provide improved results for
these datasets.
A. Results: Original Training and Test Areas
Kennedy Space Center: The true-color image shown in
Fig. 2(a), along with the classiﬁcation results obtained from
the RF-BHC in Fig. 2(b), shows the spatial distribution of
classes and training sites over the 614
512 pixel study area.
Average classiﬁcation accuracies for test data and associated
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 43, NO. 3, MARCH 2005
(a) AVIRIS data, (Bands 31, 21, 11) acquired over KSC, training sites overlaid. (b) Classiﬁed image of KSC AVIRIS data using RF-BHC classiﬁer.
Average and standard deviation of classiﬁcation accuracies for AVIRIS test data.
standard deviations for the ten experiments conducted with
each classiﬁer are plotted in Fig. 3. The overall trends in
accuracies relative to the quantity of training data are similar
for all methods when applied to the test dataset. At the 75%
sampling rate, the accuracies of all methods are nearly the
same, although the RF-BHC yields somewhat higher accuracies than the other methods. The results obtained by the
BB-BHC and RF-CART methods are very similar over all
sampling rates, with RF-CART yielding slightly lower accuracies. These methods consistently produced the lowest overall
average accuracies. The RS-BHC yielded approximately the
same accuracies as the BB-BHC at the 75% sampling rate, but
improved relative to the BB-BHC and the RF-CART approach
at lower sampling rates. For the BHC-based methods, this
appears to demonstrate the value of reduced redundancy in the
input space and improvements achieved by better tuning of the
decision boundaries, even though the tree structure is identical
to the BB-BHC and random sampling of the feature space
is not required until lower levels of the tree (particularly for
the higher training data fractions). Results were also obtained
using the original RS-BHC and best basis aggregated data. The
BB-RS-BHC consistently yielded slightly lower accuracies
than the RS-BHC because of the reduced diversity of trees, but
results were not statistically different and are not reported here.
The overall average accuracy of the RF-BHC is consistently
the highest and improves relative to other BHC methods and
RF-CART, as the fraction of training data is reduced.
The RF-BHC is also the most stable method over all training
fractions, as measured by the standard deviation of the accuracies. The standard deviations of the accuracies of the randomsubspace-based methods appear to beneﬁt from the diversity of
the input space. The standard deviation of the accuracies obtained by the BB-BHC increased dramatically at the 30% sampling rate because it was necessary to aggregate a large number
of bands to satisfy the ratio
. The problem, which is manifested both in the tree building and in the decision boundary
of the BB-BHC, is offset in the determination of the RS-BHC
decision boundary. It should be noted that although the standard deviation of BB-BHC decreases at the 15% sampling rate,
the associated average classiﬁcation accuracy is also poor, further demonstrating it is uniformly inferior at low sampling rates.
The reduced accuracy of RF-CART at low sampling rates, relative to the BHC-based random forest methods, is attributed to
the value of the inherent exploitation of class afﬁnities by the
BHC approaches. Further, although the standard deviation of
the accuracies for the RF-CART approach is low for high sampling rates, it increases consistently as the sampling rate of the
training data is reduced, likely because the discrimination capability of the single best feature within a small random sample
of inputs may be quite variable. Further, the beneﬁts of bagging
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
HAM et al.: INVESTIGATION OF THE RANDOM FOREST FRAMEWORK
(a) Hyperion data, (Bands 51, 149, 31) acquired over Okavango Delta, training sites overlaid. (b) Classiﬁed image of Hyperion data over Okavango Delta
using RF-BHC classiﬁer.
the training sample occur at the higher sampling rates for both
the RF-BHC and RF-CART methods.
Okavango Delta: The RGB image in Fig. 4(a) and the
classiﬁcation results obtained by the RF-BHC in Fig. 4(b) show
that the spatial distribution of classes is extremely complex over
1476 pixel area. Using the same random sampling
strategy as for the KSC data, results were obtained at each
percentage for all four classiﬁers. Plots of classiﬁcation accuracies at the various sampling rates are shown in Fig. 5. The
overall trends in accuracies relative to the fraction of training
data are similar those of the KSC AVIRIS test data. Among the
classiﬁers, RF-BHC yielded the highest classiﬁcation accuracy
on all training sample fractions, and performance degraded only
slightly at lower sampling rates. The standard deviations of
the accuracies obtained using the RF-BHC are low and remain
nearly constant over the various sampling rates. At the 30%
sampling rate, the standard deviations of the accuracies yielded
by the BB-BHC and RS-BHC are substantially higher. For the
BB-BHC, this again appears to be due to the amount of band
aggregation required to achieve the ratio
. Unlike the KSC
case, the RS-BHC is apparently unable to mitigate problems
associated with band aggregation during the tree construction
phase for the Okavango data.
The difference in results produced by the RF-BHC and
RF-CART methods was unexpected, since both utilize an
ensemble of 100 trees to build a stronger classiﬁer. Previous
research by Tumer and Ghosh indicated that the accuracy
of an ensemble method relies on the diversity of the base classiﬁer. To investigate the performance of the individual trees,
we further analyzed the performance of both random forest
methods at the 75% sampling rate. The average accuracy over
the set of individual trees developed by the RF-BHC is 89.2%,
and the standard deviation is 1.3. The overall accuracy for the
RF-BHC, which is determined by simple voting, increased
by 5.7% to 94.9%. For the RF-CART method, the average
accuracy obtained using 100 trees is 84.2%, with standard
deviation 1.3. The ensemble of these 100 trees, using simple
voting utilized in the original Brieman random forest, yielded a
7.8% increase to 92%. For this type of classiﬁcation problem,
it appears that the BHC is a better base classiﬁer than CART,
although CART realizes substantial improvement when trees
are combined.
B. Generalization to Spatially Disjoint Areas
Traditionally, the training and test data are spatially colocated
and can thus be assumed to be samples from the same distribution. In practice, however, it is also useful to estimate how a classiﬁer will perform in areas that are somewhat different, in order
to indicate how much additional data labeling and retraining is
needed to make the model applicable to much larger areas. With
this goal in mind, a “spatially disjoint” test set was also acquired
from a geographically separate location at the Botswana site and
used to evaluate the classiﬁers developed previously.
These spatially disjoint data have somewhat different characteristics from the training/test data, so the performance of all
classiﬁers is reduced, as expected. Still, as with the test data,
the BB-BHC yielded the lowest overall average accuracy at all
sampling rates. The incremental improvement in average accuracy achieved by the random subspace method increases with
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 43, NO. 3, MARCH 2005
(a) Average and standard deviation of classiﬁcation accuracies for Hyperion test sets. (b) Average and standard deviation of classiﬁcation accuracies for
Hyperion spatially disjoint test sets.
reduced sampling rates, but is not statistically signiﬁcant, as the
standard deviations of the accuracies also increase substantially
with lower sampling rates. The RF-BHC implementation and
the RF-CART method yielded higher accuracies for the spatially
disjoint test data at all sampling rates than both the BB-BHC and
the RS-BHC, thereby demonstrating the greater generalization
of these approaches. Similar to results from the test data, the
RF-BHC consistently produced the highest overall average accuracies for the spatially disjoint test set, indicating the value of
exploiting class afﬁnity, coupled with the increased diversity of
trees achieved by forcing random sampling of the input space
at all nodes. The RF-CART method also achieved good generalization, as indicated by its performance on these spatially
disjoint test data, although results for the original test set were
inferior to the other methods. This is attributed both to the diversity that it achieves and its reduced dependence on the training
sample statistics. Similar to the test data, the performance of
both the RF-BHC and RF-CART methods was further investigated for the 100 trees obtained from the Hyperion spatially disjoint test set at the 75% sampling rate. The average classiﬁcation
accuracy over the set of individual RF-BHC trees is 68.2%, and
the standard deviation is 2.9. The ensemble random forest result
using simple voting is 75.2%, an increase of 7%. For RF-CART
the values are 60.8% and 2.4, respectively. The classiﬁcation
accuracy increased by 9.6% to 70.4% when the 100 trees were
combined using simple voting.
Since RF-BHC and RF-CART use the same random forest
framework, their differences lie both in the tree construction
and the underlying classiﬁer. For the remotely sensed data
in this study, the BHC exploits class afﬁnity, while the good
performance of the CART-like method on the spatially disjoint
ENTROPY-BASED DIVERSITY OF ENSEMBLE MEMBERS OBSERVED
FOR THE SPATIALLY DISJOINT BOTSWANA HYPERION DATA AT
DIFFERENT SAMPLING RATES
test set suggests that it provides more diversity. To further
investigate this issue, we calculated the entropy, a nonpairwise
diversity measure , of trees obtained from both RF-BHC
and RF-CART (Table III). The results indicate that RF-CART
method produces more diverse trees than RF-BHC at all four
sampling rates. RF-CART achieves an 9.6% increase in accuracies via the ensemble, while RF-BHC results improve
only 7%, thereby reinforcing the idea that ensemble methods
beneﬁt more from combining diverse classiﬁers. Further, as
the sampling rate increases, the diversity of RF-BHC trees
decreases. Under the same situation, however, the diversity of
the RF-CART forest remains comparatively consistent. This
means that the RF-BHC inputs become more homogeneous as
the number of samples increases, while RF-CART does not
follow the same trend. Overall, the advantages of an ensemble
approach are clear as the RS-BHC used only one tree structure
rather than an ensemble of potentially different trees, which
signiﬁcantly reduced the generalization of its classiﬁcation
accuracies on the spatially disjoint test set.
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
HAM et al.: INVESTIGATION OF THE RANDOM FOREST FRAMEWORK
(a) Class-dependent accuracies for Hyperion test set. (Left) 15% and (right) 75% sampling rate. (b) Class-dependent accuracies for Hyperion spatially
disjoint test set. (Left) 15% and (right) 75% sampling rate.
The differences between the overall accuracies for the spatially disjoint test set and those for the test set are quite remarkable. As noted earlier, the test data are spatially colocated
with the training data, whereas the spatially disjoint test set is
not. Clearly, either the class priors or the class-conditional feature distributions (or both) are substantially different, at least
for some classes in the more remote area. This motivated us to
further investigate class-speciﬁc results. Class-dependent accuracies for the Hyperion test and spatially disjoint test sets are
provided in Fig. 6, and the detailed confusion matrix for the
RF-BHC is contained in Table IV. Results in Table II indicate
that the priors were indeed somewhat different. In particular,
there were relatively more samples of Classes 2 and 11, and less
of Classes 1 and 9. However, while false negative errors increase
for Classes 2 and 11, there is no overall clear trend. For example,
classiﬁcation accuracies for Class 1 (water) which is spectrally
quite distinct, are unaffected by the change of priors. Moreover,
several class accuracies are now much lower than 80%, while
others are almost unaffected. This leads us to believe that change
in class-conditional distributions in certain classes that are spectrally quite similar is the main cause of the marked degradation in their classiﬁcation accuracies. In particular, the overall
classiﬁcation accuracies of RS-BHC, RF-BHC and RF-CART
methods are strongly inﬂuenced by the performance of Classes
2 and 11. Class 2, hippo grass, which grows within the river
channels, has a small training sample and is spectrally similar
to water, as many pixels are mixed with water. Class 11, acacia
grasslands, is a mixed class that is most often confused with
other grasses or acacia shrubs, which is also a mixed class.
Using Fig. 6 (b), we can also compare the class-speciﬁc
accuracies for the spatially disjoint test set for the RS-BHC,
CONFUSION MATRIX FOR HYPERION SPATIALLY DISJOINT TEST
SET AT 75% SAMPLING RATE, RF-BHC CLASSIFIER
RF-BHC and RF-CART approaches. Consistent with the
overall accuracies, the performance of the RF-BHC is generally better than RF-CART method at both the 75% and 15%
sampling rates. Similarly, the RS-BHC yields consistently
lower accuracies, particularly for Classes 2 and 11. Although
higher classiﬁcation accuracies were achieved for Class 2 by
RF-CART than the two BHC methods, it is not statistically
signiﬁcant, as the standard deviation of the average sample
accuracy is more than 12.
In comparing the overall computational requirements of the
BHC-based and RF-CART methods, there are several tradeoffs. BHC-based trees always solve
binary problems.
Authorized licensed use limited to: University of Texas at Austin. Downloaded on March 25, 2009 at 15:11 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 43, NO. 3, MARCH 2005
At the 75% sampling rate, the average CART decision tree for
Botswana Hyperion data contained 326 nodes (standard deviation
). For this 14-class problem, the BHC tree had only
nodes. For the same experiment, the CPU time
for the RF-CART method was 8 min 42 s, while it was 1 h
4 min 4 s for the RF-BHC. Both experiments were performed
on a 3-GHz Pentium IV CPU machine. The RF-BHC required
more CPU time than the RF-CART method because GAMLS
is a deterministic simulated annealing algorithm. It should be
noted that while neither algorithm was coded as an operational
method, average timing results reﬂect their relative computational requirements.
IV. CONCLUSION AND FUTURE WORK
The primary purpose of the study was to investigate the performance of random feature subset selection methods in terms
of generalization. The secondary goal was to investigate the performance of the methods when applied to data acquired by Hyperion data, which have low SNR. The performance of an implementation that focused on tuning decision boundaries of the
BHC, and that of two random forest approaches was investigated. Classiﬁcation accuracies achieved by ensemble methods
rely heavily on achieving diversity within the ensemble. The
conﬂicting effects of improved SNR and reduced spectral resolution from band aggregation appear to be positively complemented by the improved diversity achieved by the RS-BHC
through random sampling of the original features. We also noted
that the change in classiﬁcation accuracies achieved by using
a forest rather than a single tree indicates that the RF-CART
method actually achieves greater incremental beneﬁt from the
ensemble than the RF-BHC. Thus, the ensemble both exploits
the greater diversity provided by the single feature splits and
mitigates the potential impact of selecting features that are redundant or have poor discrimination capability.
A critical characteristic of the BHC is that it exploits the natural groupings of similar classes, which often occur in remotely
sensed data acquired over natural landscapes. This provides a
natural hierarchy that is often well handled by the simple Fisher
discriminant. The random forest methods all yielded superior
results for both test and spatially disjoint test data at our two
study sites, with the improvement being greater for the spatially disjoint test set, thereby indicating improved generalization to extended areas. For these data, RF-BHC produced stable
results over all sampling rates. Additional study is required to
better characterize this issue. In this context, elimination of irrelevant and possibly redundant input features should also be
considered in the RF-BHC. Other classiﬁers, such as the ECOC
and support vector machines, should also be investigated within
the RF-BHC framework. Overall, the RF-BHC methods appear
to be quite promising in terms of generalization, but should
be applied to many more datasets with different characteristics
in order to better assess their overall performance. Also, much
work remains to be done on determining how to improve performance on extended areas represented by the spatially disjoint
dataset, especially, since both the class mixtures and class-conditional spectral properties can change in such situations. If this
problem can be solved, then one can more conﬁdently label
much larger regions than those directly described by the available labeled data. For mixed classes, the issue may be mitigated
in some cases by determining relative abundances of component
classes via unmixing of hyperspectral data, if representative signatures of pure classes can be obtained , . Approaches
for representing spatially nonstationary spectral signatures may
also be appropriate.
ACKNOWLEDGMENT
The authors thank A. Neuenschwander (UT Center for Space
Research) for help in preprocessing the Hyperion data and interpreting the overall classiﬁcation results.