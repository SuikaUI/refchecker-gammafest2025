RECURRENT CONVOLUTIONAL NEURAL
NETWORKS FOR SCENE PARSING
Pedro H. O. Pinheiro Ronan Collobert
Idiap-RR-22-2013
Centre du Parc, Rue Marconi 19, P.O. Box 592, CH - 1920 Martigny
T +41 27 721 77 11 F +41 27 721 77 12 www.idiap.ch
Recurrent Convolutional Neural Networks for
Scene Parsing
Pedro H. O. Pinheiro and Ronan Collobert
Idiap Research Institute, Rue Marconi 19, 1920 Martigny, Switzerland
 , 
Abstract. Scene parsing is a technique that consist on giving a label to
all pixels in an image according to the class they belong to. To ensure
a good visual coherence and a high class accuracy, it is essential for a
scene parser to capture image long range dependencies. In a feed-forward
architecture, this can be simply achieved by considering a suﬃciently
large input context patch, around each pixel to be labeled. We propose
an approach consisting of a recurrent convolutional neural network which
allows us to consider a large input context, while limiting the capacity of
the model. Contrary to most standard approaches, our method does not
rely on any segmentation methods, nor any task-speciﬁc features. The
system is trained in an end-to-end manner over raw pixels, and models
complex spatial dependencies with low inference cost. As the context size
increases with the built-in recurrence, the system identiﬁes and corrects
its own errors. Our approach yields state-of-the-art performance on both
the Stanford Background Dataset and the SIFT Flow Dataset, while
remaining very fast at test time.
Keywords: scene parsing, convolutional networks, deep learning, image
classiﬁcation, image segmentation
Introduction
In the computer vision ﬁeld, scene parsing is the task of fully labeling an image
pixel-by-pixel with the class of the object the pixel belongs to. This task is very
challenging, as it implies solving a detection, a segmentation and a multi-label
recognition problem all in one.
The parsing problem is most commonly addressed with some kind of local
classiﬁer constrained in its predictions with a graphical model (e.g. Conditional
Random Fields, Markov Random Fields), such that a global decision is made.
These approaches usually consist of segmenting the image into superpixels or segment regions to assure a visible consistency of the labeling and also to take into
account similarities between neighbor segments, giving a high level understanding of the overall structure of the image. Each segment contains a series of input
features describing it and contextual features describing spatial relation between
the label of neighbor segments. These models are then trained to maximize the
likelihood of correct classiﬁcation given the features . The main limitation
Pedro H. O. Pinheiro, Ronan Collobert
of scene parsing approaches based on graphical models is the computational cost
at test time, limiting the model to simple contextual features.
In this work, we consider a neural network-based feed-forward approach which
can take into account long range dependencies on the image while controlling
the capacity of the network, achieving state-of-the-art accuracy while keeping the
computational cost low at test time, thanks to the complete feed-forward design.
Our method relies on a recurrent architecture for convolutional neural networks:
a series of networks sharing the same set of parameters. Each instance consider as
input both an RGB image and the classiﬁcation attempt of the previous instance
of the network. The network learns itself how to smooth its own predicted labels,
improving the estimation as the number of instances increases.
Compared to graphical models approaches relying on image segmentation,
our system has several advantages: (i) it does not require any engineered features,
since deep learning architectures train (hopefully) adequate discriminative ﬁlters
in an end-to-end manner, (ii) the prediction phase does not rely in any label space
searching, since it requires only the forward evaluation of a function.
In the following, Section 2 brieﬂy presents related works, Section 3 describe
the proposed strategy, Section 4 presents the results of our experiments in two
standard datasets: the Stanford Background Dataset (8 classes) and the SIFT
Flow Dataset (33 classes) and compare the performance with other systems. The
paper is ﬁnished with a conclusion and discussion on Section 5.
Related Work
In a preliminary work, proposed an innovative approach to scene parsing
without the use of any graphical model. The authors propose a solution based
on deep convolutional networks relying on a supervised greedy learning strategy.
These network architectures can be fed with raw pixels and are able to capture
texture, shape and contextual information.
 also considered the use of deep learning techniques to deal with scene parsing. Unlike us, the authors consider oﬀ-the-shelf features of segments obtained
from the scenes. They then use a network for recursively merging diﬀerent segments and give them a semantic category label. Our recurrent architecture diﬀers
from theirs in the sense that we use it to parse the scene with a smoother class
annotation.
More recently, also consider the use of convolutional networks, extracting features densely from a multiscale pyramid of images. This solution yields
satisfactory results for the categorization of the pixels, but poor visual coherence.
The authors propose three diﬀerent over-segmentation approaches to produce
the ﬁnal labeling with improved accuracy and better visual coherence: (i) the
scene is segmented in superpixels and a single class is assigned to each of the
superpixels, (ii) a conditional random ﬁeld is deﬁned over a set of superpixels to
model joint probabilities between them and correct aberrant pixel classiﬁcation
(such as “road” pixel surrounded by “sky”) and (iii) the selection of a subset of
tree nodes that maximize the average “purity” of the class distribution, hence
Recurrent Convolutional Neural Networks for Scene Parsing
Table 1. Comparison between diﬀerent methods for scene parsing. The advantage
of our proposed method consists on the simplicity of inference, not relying on any
task-speciﬁc feature extraction nor segmentation method.
task-specific features
Gould et al., 2009 
17-dimensional color and texture features, 9
grid locations around the pixel and the image
row, region segmentation.
Tighe & Lazebnik, 2010 Global, shape, location, texture/SIFT, color,
appearance, MRF.
Munoz et al., 2010 
Gist, pyramid histogram of oriented gradients,
color Histogram CIELab, relative relocation,
hierarchical region representation.
Kumar & Koller, 2010 Color, texture, shape, percentage pixels above
horizontal, region-based segmentation.
Socher et al., 2012 
Same as .
Lempitsky et al., 2011 Histogram of visual SIFT, histogram of RGB, histogram of locations, “contour shape” descriptor.
Farabet et al., 2013 
Laplacian pyramid, superpixels/CRF/tree segmentation.
Our Recurrent CNN
Raw pixels
maximizing the overall likelihood that each segment will contain a single object.
The superpixel-based approach was however order of magnitude faster than the
two other approaches, with slightly lower performance in accuracy. In contrast,
our approach is simpler and completely feed-forward, as it does not require any
image segmentation technique, nor the handling of a multiscale pyramid of input
As in , proposed a similar multiscale convolutional architecture. In
their approach, the authors smooth out the predicted labels with pairwise class
Compared to existing approaches, our method does not rely on any taskspeciﬁc feature (see Table 1). Our scene parsing system is able to extract relevant
contextual information from raw pixels.
Systems Description
We formally introduce convolutional neural networks (CNNs) in Section 3.1. In
Section 3.2 we discuss how to capture long range dependencies with these type of
models, while keeping a tight control on the capacity. Section 3.3 introduces our
recurrent network approach for scene parsing. Finally, in Section 3.4, we show
how to infer the full scene label. Because of pooling operations in a convolutional
neural network architecture, the predicted output label “planes” have a lower
Pedro H. O. Pinheiro, Ronan Collobert
5 conv 4 × 4
pool 2 × 2
2 conv 2 × 2
Fig. 1. A simple convolutional network. Given an image patch providing a context
around a pixel to classify (here blue), a series of convolutions and pooling operations
(ﬁlters slid through input planes) are applied (here 5 4 × 4 convolutions, followed by a
2 × 2 pooling, followed by 2 2 × 2 convolutions. Each 1 × 1 output plane is interpreted
as a score for a given class.
resolution than the input image. Here we introduce a way to obtain the full
resolution in an eﬃcient way.
Convolutional Neural Networks for Scene Parsing
Convolutional neural networks are a natural extension of neural networks
for treating images. Their architecture, vaguely inspired by the biological visual
system, possesses two key properties that make them extremely useful for image
applications: spatially shared weights and spatial pooling. These kind of networks
learn features that are shift-invariant, i.e., ﬁlters that are useful across the entire
image (due to the fact that image statistics are stationary). The pooling layers
are responsible for reducing the sensitivity of the output to slight input shift and
distortions. This type of neural network has proven to be very eﬃcient in many
vision applications, such as object recognition and segmentation ( ).
A typical convolutional network is composed of multiple stages, as shown on
Figure 1. The output of each stage is made of a set of two dimensional arrays
called feature maps. Each feature map is the outcome of one convolutional (or
pooling) ﬁlter applied over the full image. A non-linear squashing function (such
as a hyperbolic tangent) always follows a pooling layer.
In the context of scene parsing we consider a set of images indexed by Ik,
and we are interested in ﬁnding the label of each pixel at location (i, j), for every
image k. To that matter, the network is fed with a squared context patch Ii,j,k
surrounding the pixel at location (i, j) in the k-th image. It can be shown (see
Figure 1) that the output plane size szm of the mth layer is computed as:
szm = szm−1 −kWm
where sz0 is the input patch size, kWm is the size of the convolution (or pooling)
kernels in the mth layer, and dWm is the pixel step size used to slide the convo-
Recurrent Convolutional Neural Networks for Scene Parsing
lution (or pooling) kernels over the input planes.1 Given a network architecture
and an input image, one can compute the output image size by successively
applying Equation 1 on each layer of the network. During the training phase,
the size of the input patch Ii,j,k is chosen carefully such that the output layers
produces 1 × 1 planes, which are then interpreted as scores for each class of
Adopting the same notation as , the output of a network f with M stages
and trainable parameters (W, b), given an input patch Ii,j,k, can be formally
written as:
f(Ii,j,k; (W, b)) = WMHM−1 ,
with the output of the mth hidden layer computed as:
Hm = tanh(pool(WmHm−1 + bm)) ,
for m = {1, ..., M} and denoting H0 = Ii,j,k. bm is the bias vector of layer m
and Wm is the Toeplitz matrix of connection between layer m −1 and layer m.
The pool(·) function is the max-pooling operator and tanh(·) is the point-wise
hyperbolic tangent function applied at each point of the feature map.
In order to train the network by maximizing a likelihood, the network scores
fc(Ii,j,k; (W, b)) (for each class of interest c ∈{1, ..., N}) are transformed into
conditional probabilities, by applying a softmax function:
p(c|Ii,j,k; (W, b)) =
efc(Ii,j,k;(W,b))
d∈{1,...,N}
efd(Ii,j,k;(W,b))
The parameters (W, b) are learned in an end-to-end supervised way, by
minimizing the negative log-likelihood over the training set:
L(W, b) = −
ln p(li,j,k|Ii,j,k; (W, b)) ,
where li,j,k is the correct pixel label class, at position (i, j) in image Ik. The minimization was achieved with the Stochastic Gradient Descent (SGD) algorithm,
with a ﬁxed learning rate λ:
W ←−W −λ ∂L
∂W ; b ←−b −λ∂L
Long Range Dependencies with Convolutional Networks
Existing successful scene parsing systems leverage long range image dependencies
in some way. The most common approach is to add a kind of graphical model
(e.g. a conditional random ﬁeld) over local decisions, such that a certain global
coherence is maintained. An obvious way to capture long range dependencies
1 Most people use dW = 1 for convolutional layers, and dW = kW for pooling layers.
Pedro H. O. Pinheiro, Ronan Collobert
1 instance
2 instance
3 instance
Fig. 2. Context input path of “plain” (a) and recurrent (b) architecture. The size of
the contextual input patch (b) increases as the number of instances in the recurrent
convolutional network increases. The capacity of the model remains the same, since
the parameters over all instances are shared.
would be to consider large input patches when labeling a pixel. Unfortunately,
this approach might face generalization issues, as considering large context often
implies considering large models (i.e. large capacity).
In Table 2, we review possible ways to control the capacity of a convolutional
neural network, assuming a large input context. In a “plain” architecture (as
described in Section 3.1), one can easily control the capacity by increasing the
ﬁlter sizes in pooling layers, reducing the overall number of parameters in the
network. Unfortunately, performing large poolings decreases the network label
output resolution (e.g., if one performs a 1/8 pooling, the label output plane size
will be about 1/8th of the input image size). One can overcome this problem (as
shown in Section 3.4), but at the cost of a slow inference process.
Instead, Farabet et al., 2013 considered the use of a multiscale convolutional network. Large contexts are integrated into local decisions while making
the model still manageable in terms of parameters/dimensionality. Label coherence is then increased by leveraging superpixels.
Another way to consider a large input context size while controlling the
capacity of the model is to make the network recurrent: the output of the model
is fed back to the input of another instance of the same network, which shares
the same parameters (see Figure 3). Given Equation 1, we have
szm−1 = dWm × szm + (kWm −dWm) .
Thus, the required context to label one pixel (i.e. if the network output size is 1×
1), increases with the number of network instances (see an example in Figure 2).
However, the capacity of the system remains constant, since the parameters
of each network instance are simply shared. We will now detail our recurrent
network approach.
Recurrent Convolutional Neural Networks for Scene Parsing
Table 2. Long range dependencies integration in CNN-based scene parsing systems.
Ways to control capacity and speed of each architecture is reported.
Capacity control
local classifier + graphical model
multiscale
scale down input image
large input patches
increase pooling
recurrent architecture
Fig. 3. Recurrent network architecture with two instance of f. The ﬁrst instance f (1)
of the recurrent architecture is fed with a RGB patch and an empty feature map. The
output of the ﬁrst network is coupled with the scaled RGB patch and fed to the same
network (shared parameters (W and b)).
Recurrent Network Approach
The recurrent architecture (see Figure 3) consists of P instances of the “plain”
convolutional network f(·), each of them with identical (shared) trainable parameters (W, b). Each instance f p (1 ≤p ≤P) is fed with an input “image” Fp
of N + 3 features maps
Fp = [f p−1(Ip−1
i,j,k; (W, b)), Ip
i,j,k], F1 = [0, Ii,j,k] ,
which are the output label planes of the previous instance, and the scaled2
version of the raw RGB squared patch surrounding the pixel at location (i, j) of
the training image k. To make the number of inputs equal in all instances of the
network, the input “image” of the ﬁrst instance is simply the patch of raw pixel
coupled with N 0 feature maps. This guarantees the end-to-end characteristics
of the system as well as its fast inference during test.
i,j,k is scaled so that it has the width/height as f p−1.
Pedro H. O. Pinheiro, Ronan Collobert
0 0 0 0 0 0 0
0 0 0 0 0 0 0
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
0 0 0 0 0 0
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
0 0 0 0 0 0 0
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
0 0 0 0 0 0
0 0 0 0 0 0
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
0 0 0 0 0 0 0
0 0 0 0 0 0 0
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
6 7 8 9 10
11 12 13 14 15
16 17 18 19 20
21 22 23 24 25
f(·; (W, b))
f(·; (W, b))
f(·; (W, b))
f(·; (W, b))
Fig. 4. Convolutional neural networks output downscaled label planes (compared to
the input image) due to pooling layers. To alleviate this problem, one can feed several
shifted version of the input image (here represented by pixels 1..25) in the X and Y
axis. In this example the network is assumed to have a single 2 × 2 pooling layer.
Downscaled predicted label planes (here in red) are then merged to get back the full
resolution label plane in an eﬃcient manner. Note that pixels represented by 0 are
adequate padding.
As in the “plain” network, the size of the patch during training is chosen
such that the output layers produces 1 × 1 planes, which are then interpreted as
scores for each class of interest.
Scene Inference
Given a test image I, the pixel at location (i, j) of image k is labeled with the
argmax of the network predictions:
ˆli,j,k = arg max
p(c|Ii,j,k; (W, b)) ,
considering the context patch Ii,j,k. Note that this might imply adding padding
when inferring label of pixels close to the image border. In practice, simply
extracting patches Ii,j,k and then feeding them through the network for all pixels
of a test image is computationally very ineﬃcient. Instead, it is a useful practice
to feed the full test image (also properly padded) to the convolutional network:
applying one convolution to a large image is much faster than applying the same
convolution many times to small patches. When fed with the full input image, the
network will output a plane of label scores. Unfortunately, following Equation 1,
the plane size is smaller than the input image size: this is mainly due to pooling
layers, but also to border eﬀects when applying the convolution. E.g., if the
network includes two 2 × 2 pooling layers, only 1 every 4 pixels of the input
image will be labeled. Most convolutional network users (see ( )) upscale the
label plane to the input image size.
In fact, it is possible to compute eﬃciently the label plane with a ﬁne resolution by feeding to the network several versions of the input image, shifted on
Recurrent Convolutional Neural Networks for Scene Parsing
Table 3. Pixel and averaged per class accuracy and the computing time of other
methods and our proposed approaches on the Stanford Background Dataset.
avg per class
Gould et al., 2009 
Tighe & Lazebnik, 2010 
Munoz et al., 2010 3
Kumar & Koller, 2010 
Socher et al., 2012 
Lempitsky et al., 2011 
Farabet et al., 2013 4
Farabet et al., 2013 5
rCNN1 (2 instances)
rCNN2 (2 instances)
rCNN2 1/2 resolution (3 instances)
rCNN2 full resolution (3 instances)
the X and Y axis. We show an example on Figure 4, for a network which would
have only one 2 × 2 pooling layer, and one output plane: low resolution label
planes (coming out of the network for the input image shifted by (0, 0), (0, 1),
(1, 0) and (1, 1) pixels) are merged to form the high resolution label plane. If
one has several pooling layers which downscale more the input image (e.g. by a
factor of 8), one has to do more forwards (e.g. 8 × 8 = 64), which is still much
faster than forwarding patches at each location of the test image. We will see
in Section 4.3 that having a ﬁner label resolution can increase the classiﬁcation
performance.
Experiments
We tested our proposed method on two diﬀerent datasets for scene parsing: the
Stanford Background and the SIFT Flow Dataset . The Stanford dataset
has 715 images from rural and urban scenes composed of 8 classes. The scenes
have approximately 320 × 240 pixels. As in , we performed a 5-fold crossvalidation with the dataset randomly split into 572 training images and 143 test
images in each fold. The SIFT Flow is a larger dataset composed of 2688 images
of 256 × 256 pixels and 33 semantic labels. All the algorithms and experiments
were implemented in Torch7 .
Each image on the training set was properly padded and normalized such that
they have zero mean and unit variance. All networks were trained by sampling
3 Unpublished improved results have been recently reported by the authors.
4 Multiscale CNN + superpixels.
5 Multiscale CNN + CRF.
Pedro H. O. Pinheiro, Ronan Collobert
Table 4. Pixel and averaged per class accuracy of other methods and our proposed
approaches on the SIFT Flow Dataset.
avg per class
Liu et al., 2009 
Tighe & Lazebnik, 2010 
Farabet et al., 2013 
rCNN2 (2 instances)
rCNN2 (3 instances)
patches surrounding randomly chosen pixel from randomly chosen images from
the training set of images. Contrary to (i) we did not consider any extra
distortion on the images6, and (ii) we did not sample training patches according
to balanced class frequencies.
We considered two diﬀerent accuracy measures to compare the performance
of our proposed methods with other approaches. The ﬁrst one is the accuracy per
pixel of test images. This measure is simply the ratio of correct classiﬁed pixels
of all images in the test set. However, in scene parsing (especially in datasets
with large number of classes), classes which are much more frequent than others
(e.g. the “sky” class is much more frequent than “moon”) have more impact
on this measure. Recent papers also consider the averaged per class accuracy
on the test set (all classes have the same weight in the measure). Note that as
mentioned above, we did not train with balanced class frequencies, which would
have optimized this second measure.
We consider both a “plain” architecture with a large patch and strong number
of pooling and a recurrent architecture with two and three unfolds in time.
Table 3 compares the performance of our architecture with related works on the
Stanford Background Dataset and Table 4 compares the performance on the
SIFT Flow Dataset. In the following, we provide additional technical details for
each architecture used.
Plain Network
The ﬁrst “plain” network was trained with 133×133 input patches. The network
was composed of a 6 × 6 convolution with nhu1 output planes, followed by a
8 × 8 pooling layer, a tanh(·) non-linearity, another 3 × 3 convolutional layer
with nhu2 output planes, a 2 × 2 pooling layer, a tanh(·) non-linearity, and a
ﬁnal 7 × 7 convolution to produce label scores. The hidden units were chosen
to be nhu1 = 25 and nhu2 = 50 for the Stanford dataset, and nhu1 = 50 and
nhu2 = 50 for the SIFT Flow dataset.
6 Which is known to improve the generalization accuracy by few extra percents.
Recurrent Convolutional Neural Networks for Scene Parsing
Recurrent Architectures
We consider two diﬀerent recurrent convolutional network architectures.
The ﬁrst architecture, rCNN1, is composed of two consecutive instances of
the convolutional network CNN1, sharing parameters (as in Figure 3). CNN1
is composed of a 8 × 8 convolution with 25 output planes, followed by a 2 × 2
pooling layer, a tanh(·) non-linearity, another 8 × 8 convolutional layer with 100
output planes, a 2 × 2 pooling layer, a tanh(·) non-linearity, and a ﬁnal 1 × 1
convolution to produce N label scores.
rCNN1 is trained by considering the two network instances simultaneously.
For each training example we randomly choose to perform a “forward” and
“backward” pass through one or two instances of the network. This training
approach allows the network to learn to correct its own mistakes (made by the
ﬁrst network instance). As mentioned in Section 3.2, the input context patch
size depends directly on the number of network instances in the recurrent architecture. In the case of rCNN1, the patch size is of 25 × 25 when considering one
instance and 121 × 121 when considering two network instances.
The second recurrent convolutional neural network, rCNN2, is composed of
three instances of the convolutional network CNN2, with shared parameters.
Each instance of CNN2 is composed of a 8 × 8 convolution with 25 output
planes, followed by a 2 × 2 pooling layer, a tanh(·) non-linearity, another 8 × 8
convolution with 50 planes and a ﬁnal 1 × 1 convolution which outputs the N
label planes.
In rCNN2, the ﬁrst two instances are trained simultaneously7 (as in Figure 3)
through SGD, with input patch of size 67. Then, after the system with two
instances are trained, a third instance of the network is considered (still with
the parameters shared with the others instances) so that it is able to correct
itself from the previous labeling. The input patch is of size 155 in this latter
Figure 5 illustrates inference of the recurrent network with two and three
instances. The network learns itself how to correct its own label prediction.
In all cases, the learning rate in Equation 6 was equal to 10−4. All hyperparameters were tuned with a 10% hold-out for validation.
Compute Time and Scene Inference
In Table 5, we analyze the tradeoﬀbetween compute time and test accuracy,
by running several experiments with diﬀerent output resolutions for recurrent
network rCNN2 (see Section 3.4 and Figure 4). Labeling about 1/4th of the pixels
seemed to be enough to lead to near state-of-the-art performance, while keeping
a very fast inference time.
7 Considering only one instance in this case produces a very small context input patch.
Pedro H. O. Pinheiro, Ronan Collobert
Fig. 5. Recurrent scene parser over original image (a) segments the image as shown in
(b). Due to its recurrent nature, it can be fed again with its own prediction (b) and
the original image (a) which leads to (c): most mistakes are corrected.
Conclusion
This paper presents a feed-forward approach for scene parsing, based on supervised “deep” learning strategies which model in a rather simple way non-local
class dependencies in a scene from raw pixels. We demonstrate that the problem
of scene parsing can be faced without the need of any expensive graphical model
Recurrent Convolutional Neural Networks for Scene Parsing
Table 5. Computation time and performance in pixel accuracy for the recurrent convolutional network rCNN2 with diﬀerent label resolution on the Stanford dataset. Our
algorithms were ran on a 4-core Intel i7.
Resolution
Compute time
nor segmentation tree technique to ensure labeling. The scene labeling is inferred
simply by forward evaluation of a function applied to a RGB image.
In terms of accuracy, our system achieved state-of-the-art results on both
Stanford Background and SIFT Flow dataset. Future work includes investigation of unsupervised or semi-supervised pre-training of the models, as well as
application to larger datasets such as the Barcelona dataset.