Value Function Approximation in
Reinforcement Learning Using the Fourier Basis
George Konidaris1,3
MIT CSAIL1
 
Sarah Osentoski2,3‚àó
Department of Computer Science2
Brown University
 
Philip Thomas3
Autonomous Learning Laboratory3
University of Massachusetts Amherst
 
We describe the Fourier basis, a linear value function approximation scheme based on the Fourier series. We empirically
demonstrate that it performs well compared to radial basis
functions and the polynomial basis, the two most popular
Ô¨Åxed bases for linear value function approximation, and is
competitive with learned proto-value functions.
Introduction
Reinforcement learning (RL) in continuous state spaces requires function approximation. Most work in this area focuses on linear function approximation, where the value
function is represented as a weighted linear sum of a set of
features (known as basis functions) computed from the state
variables. Linear function approximation results in a simple update rule and quadratic error surface, even though the
basis functions themselves may be arbitrarily complex.
RL researchers have employed a wide variety of basis function schemes, most commonly radial basis functions (RBFs) and CMACs . Often,
choosing the right basis function set is criticial for successful
learning. Unfortunately, most approaches require signiÔ¨Åcant
design effort or problem insight, and no basis function set is
both simple and sufÔ¨Åciently reliable to be generally satisfactory. Recent work has
focused on learning basis function sets from experience, removing the need to design an approximator but introducing
the need to gather data to create one.
The most common continuous function approximation
method in the applied sciences is the Fourier series. Although the Fourier series is simple, effective, and has solid
theoretical underpinnings, it is almost never used for value
function approximation. This paper describes the Fourier
basis, a simple linear function approximation scheme using the terms of the Fourier series as basis functions.1 We
Copyright c‚Éù2011, Association for the Advancement of ArtiÔ¨Åcial
Intelligence (www.aaai.org). All rights reserved.
‚àóSarah Osentoski is now with the Robert Bosch LLC Research
and Technology Center in Palo Alto, CA.
1Open-source, RL-Glue compatible Java source code for the Fourier basis can be downloaded
from 
BasisÀô(Java).
demonstrate that it performs well compared to RBFs and the
polynomial basis, the most common Ô¨Åxed bases, and is competitive with learned proto-value functions even though no
extra experience or computation is required.
Background
A d-dimensional continuous-state Markov decision process
(MDP) is a tuple M = (S, A, P, R), where S ‚äÜRd is a
set of possible state vectors, A is a set of actions, P is the
transition model (with P(x, a, x‚Ä≤) giving the probability of
moving from state x to state x‚Ä≤ given action a), and R is the
reward function (with R(x, a, x‚Ä≤) giving the reward obtained
from executing action a in state x and transitioning to state
x‚Ä≤). Our goal is to learn a policy, œÄ, mapping state vectors
to actions so as to maximize return (discounted sum of rewards). When P is known, this can be achieved by learning
a value function, V , mapping state vectors to return, and selecting actions that result in the states with highest V . When
P is not available, the agent will typically either learn it, or
instead learn an action-value function, Q, that maps stateaction pairs to expected return. Since the theory underlying
the two cases is similar we consider only the value function
case. V is commonly approximated as a weighted sum of
a set of basis functions œÜ1, ..., œÜm: ¬ØV (x) = m
i=1 wiœÜi(x).
This is termed linear value function approximation since ¬ØV
is linear in weights w = [w1, ..., wm]; learning entails Ô¨Ånding the w corresponding to an approximate optimal value
function, ¬ØV ‚àó. Linear function approximation is attractive
because it results in simple update rules (often using gradient descent) and possesses a quadratic error surface with a
single minimum (except in degenerate cases). Nevertheless,
we can represent complex value functions since the basis
functions themselves can be arbitrarily complex.
The Polynomial Basis.
Given d state variables x =
[x1, ..., xd], the simplest linear scheme uses each variable
directly as a basis function along with a constant function,
setting œÜ0(x) = 1 and œÜi(x) = xi, 0 ‚â§i ‚â§d. However, most interesting value functions are too complex to be
represented this way. This scheme was therefore generalized to the polynomial basis :
œÜi(x) = d
, where each ci,j is an integer between
0 and n. We describe such a basis as an order n polynomial
basis. For example, a 2nd order polynomial basis deÔ¨Åned
Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence
over two state variables x and y would have feature vector:
x2y2]. Note the
features that are a function of both variables; these features
model the interaction between those variables.
Radial Basis Functions.
Another common scheme
where each basis function is a Gaussian:
2œÄœÉ2 e‚àí||ci‚àíx||2/2œÉ2, for a given collection of
centers ci and variance œÉ2. The centers are typically distributed evenly along each dimension, leading to nd centers
for d state variables and a given order n; œÉ2 can be varied but
is often set to
n‚àí1. RBFs only generalize locally‚Äîchanges
in one area of the state space do not affect the entire state
space. Thus, they are suitable for representing value functions that might have discontinuities. However, this limited
generalization is often reÔ¨Çected in slow initial performance.
Proto-Value Functions. Recent research has focused on
learning basis functions given experience. The most prominent learned basis is proto-value functions or PVFs . In their simplest form, an
agent builds an adjacency matrix, A, from experience and
then computes the Laplacian, L = (D ‚àíA), of A where
D is a diagonal matrix with D(i, i) being the out-degree of
state i. The eigenvectors of L form a set of bases that respect
the topology of the state space (as reÔ¨Çected in A), and can
be used as a set of orthogonal bases for a discrete domain.
Mahadevan et al. extended PVFs to continuous domains, using a local distance metric to construct the graph
and an out-of-sample method for obtaining the values of
each basis function at states not represented in A. Although
the given results are promising, PVFs in continuous spaces
require samples to build A, an eigenvector decomposition to
build the basis functions, and pose several potentially difÔ¨Åcult design decisions.
The Fourier Basis
In this section we describe the Fourier series for one and
multiple variables, and use it to deÔ¨Åne the univariate and
multivariate Fourier bases.
The Univariate Fourier Series
The Fourier series is used to approximate a periodic function; a function f is periodic with period T if f(x + T) =
f(x), ‚àÄx. The nth degree Fourier expansion of f is:
¬Øf(x) = a0
0 f(x) cos( 2œÄkx
) dx and with bk
0 f(x) sin( 2œÄkx
) dx. For the remainder of this paper
we assume for simplicity that T = 2, with the variables of
the function we wish to approximate scaled appropriately.
In the RL setting f is unknown so we cannot compute
a0, ..., an and b1, ..., bn, but we can instead treat them as parameters in a linear function approximation scheme, with:
i > 0, i odd
i > 0, i even.
Thus a full nth order Fourier approximation to a onedimensional value function results in a linear function approximator with 2n + 1 terms. However, as we shall see
below, we can usually use only n + 1 terms.
Even, Odd and Non-Periodic Functions
If f is known to be even (that is, f(x) = f(‚àíx), so that f is
symmetric about the y-axis), then ‚àÄi > 0, bi = 0, so the sin
terms can be dropped. This results in a function guaranteed
to be even, and reduces the terms required for an nth order
Fourier approximation to n + 1. Similarly, if f is known
to be odd (that is, f(x) = ‚àíf(‚àíx), so that f is symmetric
with respect to the origin) then ‚àÄi > 0, ai = 0, so we can
omit the cos terms. These cases are depicted in Figure 1.
Figure 1: Even (a) and odd (b) functions.
However, in general, value functions are not even, odd,
or periodic (or known to be in advance). In such cases, we
can deÔ¨Åne our approximation over [‚àí1, 1] but only project
the input variable to . This results in a function periodic on [‚àí1, 1] but unconstrained on (0, 1]. We are now free
to choose whether or not the function is even or odd over
[‚àí1, 1] and can drop half of the terms in the approximation.
In general, we expect that it will be better to use the ‚Äúhalfeven‚Äù approximation and drop the sin terms because this
causes only a slight discontinuity at the origin. Thus, we
deÔ¨Åne the univariate nth order Fourier basis as:
œÜi(x) = cos (iœÄx) ,
for i = 0, ..., n. Figure 2 depicts a few of the resulting basis functions. Note that frequency increases with i; thus,
high order basis functions will correspond to high frequency
components of the value function.
Figure 2: Univariate Fourier basis functions for i = 1, 2, 3
and 4. The basis function for i = 0 is a constant.
The Multivariate Fourier Series
The nth order Fourier expansion of the multivariate function
F(x) with period T in d dimensions is:
where c = [c1, ..., cd], cj ‚àà[0, ..., n], 1 ‚â§j ‚â§d. This
results in 2(n + 1)d basis functions for an nth order full
Fourier approximation to a value function in d dimensions,
which can be reduced to (n + 1)d if we drop either the sin
or cos terms for each variable as described above. We thus
deÔ¨Åne the nth order Fourier basis for d variables:
œÜi(x) = cos
where ci = [c1, ..., cd], cj ‚àà[0, ..., n], 1 ‚â§j ‚â§d. Each
basis function has a vector c that attaches an integer coefÔ¨Åcient (less than or equal to n) to each variable in x; the basis
set is obtained by systematically varying these coefÔ¨Åcients.
Example basis functions over two variables are shown in
Figure 3. Note that c = results in a constant function. When c = [0, ky] or [kx, 0] for positive integers kx and
ky, the basis function depends on only one of the variables,
with the value of the non-zero component determining frequency. Only when c = [kx, ky] does it depend on both;
this basis function represents an interaction between the two
state variables. The ratio between kx and ky describes the
direction of the interaction, while their values determine the
basis function‚Äôs frequency along each dimension.
Figure 3: A few example Fourier basis functions deÔ¨Åned
over two state variables.
Lighter colors indicate a value
closer to 1, darker colors indicate a value closer to ‚àí1.
This basis is easy to compute accurately even for high orders, since cos is bounded in [‚àí1, 1], and its arguments are
formed by multiplication and summation rather than exponentiation. Although the Fourier basis seems like a natural
choice for value function approximation, we know of very
few instances ) of its use in RL
prior to this work, and no empirical study of its properties.
Scaling Gradient Descent Parameters
When performing stochastic gradient descent with a linear
approximator, each feature need not have the same learning
rate. Intuitively, lower frequency terms should have larger
learning rates so that the agent can rapidly estimate the general shape of the target surface, with slower reÔ¨Ånement by
higher order terms. Many online schemes exist for tuning
each learning rate, though these result in problem speciÔ¨Åc
learning rates . Given a base learning
rate, Œ±1, in practice we Ô¨Ånd that setting the learning rate for
basis function œÜi to Œ±i = Œ±1/||ci||2 (avoiding division by
zero by setting Œ±0 = Œ±1 where c0 = 0) performs best. An
argument for this approach is given in the Appendix.
Variable Coupling
In continuous RL domains it is common to use an approximation scheme that assumes each variable contributes independently to the value function. For example, an order 1
polynomial basis function contains basis functions 1, x1, x2
and x1x2; assuming independent contributions means we
can drop the x1x2 term. We call such a basis uncoupled.
For higher order function approximators, uncoupling removes the curse of dimensionality: for an order n polynomial basis with d variables, the full basis has (n + 1)d
terms, whereas the uncoupled basis has only dn + 1 terms.
Although this can lead to poor approximations (because in
many cases the variables do not contribute independently)
in many continuous domains it does not signiÔ¨Åcantly degrade performance.
However, a more sophisticated approach could use prior knowledge about which variables are
likely to interact to obtain a smaller but still accurate function approximator. The Fourier basis facilitates this through
constraints on c, the coefÔ¨Åcient vector used in Equation 4.
For example, we obtain an uncoupled basis by requiring that
only one element of each c is non-zero. Alternatively, if we
know that the interaction of variables xi and xj is important
then we can constrain c so that only ci and cj can be nonzero simultaneously. We expect that most domains in which
a decoupled basis performs poorly are actually weakly coupled, in that we can use very low-order terms for the interaction between variables and high-order order terms for each
variable independently. The Fourier basis lends itself naturally to implementing such a scheme.
Empirical Evaluation
This section empirically compares the performance of the
Fourier basis2 on standard continuous benchmarks to that of
RBFs, the polynomial basis, and PVFs. Since we are comparing basis functions (as opposed to learning algorithms),
we have selected the learning algorithms most widely used,
rather than those that are most efÔ¨Åcient. While better performance might be obtained using other learning methods, we
expect the relative performance differences to persist.
The Swing-Up Acrobot. The acrobot is an underactuated
two-link robot where the Ô¨Årst link is suspended from a point
and the second can exert torque. The goal is to swing the tip
of the acrobot‚Äôs second joint a segment‚Äôs length above the
level of its suspension, much like a gymnast hanging from
a pole and swinging above it using their waist. Since the
acrobot is underactuated it must swing back and forth to gain
momentum. The resulting task has 4 continuous variables
(an angle and an angular velocity for each joint) and three
actions (exerting a negative, positive or zero unit of torque
on the middle joint). Sutton and Barto contains a
more detailed description.
We employed Sarsa(Œª) (Œ≥ = 1.0, Œª = 0.9, œµ = 0) with
Fourier bases of orders 3 (256 basis functions), 5 .
sis functions) and 7 (4096 basis functions) and RBF, polynomial and PVF bases of equivalent sizes (we did not run PVFs
with 4096 basis functions because the nearest-neighbor calculations for that graph proved too expensive). We systematically varied Œ± (the gradient descent term) to optimize performance for each combination of basis type and order. Table 4(a) shows the resulting Œ± values.3
The PVFs were built using the normalized Laplacian:
L = D‚àí1/2(D ‚àíW)D‚àí1/2, scaling the resulting eigenvectors to the range [‚àí1, 1]. We also rescaled the Acrobot
state variables by [1, 1, 0.05, 0.03] for local distance calculations. Random walks did not sufÔ¨Åce to collect samples that
adequately covered the state space, so we biased the sample
collection to only keep examples where the random walk
actually reached the target within 800 steps. We kept 50 of
these episodes‚Äîapproximately 3200 samples. We subsampled these initial samples to graphs of 1200 or 1400 points,
created using the nearest 30 neighbors. We did not optimize
these settings as well as might have been possible, and thus
the PVF averages are not as good as they could have been.
However, our intention was to consider the performance of
each method as generally used, and a full parameter exploration was beyond the scope of this work.
We ran 20 episodes averaged over 100 runs for each type
of learner. The results, shown in Figure 4, demonstrate that
the Fourier basis learners outperformed all other types of
learning agents for all sizes of function approximators. In
particular, the Fourier basis performs better initially than the
polynomial basis (which generalizes broadly) and converges
to a better policy than the RBFs (which generalize locally).
It also performs slightly better than PVFs, even though the
Fourier basis is a Ô¨Åxed, rather than learned, basis. However,
the value function for Acrobot does not contain any discontinuities, and is therefore not the type of problem that PVFs
are designed to solve. In the next section, we consider a
domain with a discontinuity in the value function.
The Discontinuous Room. In the Discontinuous Room,
shown in Figure 5(a), an agent in a 10 √ó 6 room must reach
a target; the direct path to the target is blocked by a wall,
which the agent must go around. The agent has four actions which move it 0.5 units in each direction. This is a
simple continuous domain with a large discontinuity, which
we use to compare the Fourier basis to PVFs, which were
speciÔ¨Åcally designed to handle discontinuities by modeling
the connectivity of the state space. In order to make a reliable comparison, agents using PVFs were given a perfect
adjacency matrix containing every legal state and transition,
thus avoiding sampling issues and removing the need for an
out-of-sample extension method.
Figure 5(b) shows learning curves for the Discontinuous
Room using Sarsa(Œª) (Œ≥ = 0.9, Œª = 0.9, œµ = 0) using
Fourier bases of order 5 and 7 (Œ± = 0.001 in both cases)
deÔ¨Åned over the two state variables, and agents using 36 and
64 (Œ± = 0.025 in both cases) PVFs. The Fourier basis outperforms PVFs. Figures 5(c) and 5(d) show example value
3Note that Œ± is a gradient descent coefÔ¨Åcient; during learning
it is multiplied by each basis function to modify that function‚Äôs
weight. Hence, its value is not comparable across different bases.
O(3) Fourier
O(3) Polynomial
O(5) Fourier
O(5) Polynomial
O(7) Fourier
O(7) Polynomial
Figure 4: Œ± values used for the Swing-Up Acrobot; learning
curves for agents using (b) order 3 (c) order 5 and (d) order
7 Fourier bases, and RBFs and PVFs with corresponding
number of basis functions. Error bars are standard error.
O(5) Fourier Basis
O(7) Fourier Basis
Figure 5: The Discontinuous Room (a), where an agent must
move from the lower left corner to the upper left corner
by skirting around a wall through the middle of the room.
Learning curves (b) for agents using O(5) and O(7) Fourier
bases and agents using 36 and 64 PVFs.
Error bars are
standard error. Value functions from agents using an O(7)
Fourier basis (c) and 64 PVFs (d).
functions for an agent using an order 7 Fourier basis and a
corresponding agent using 64 PVFs. The PVF value function is clearly a better approximation, very precisely modeling the discontinuity around the wall. In contrast, the Fourier
basis value function is noisier and does not model the discontinuity as cleanly. However, the results in Figure 5(b)
suggest that this does not signiÔ¨Åcantly impact the quality of
the resulting policy; it appears that the Fourier basis agent
has learned to avoid the discontinuity. This suggests that the
Fourier basis is sufÔ¨Åcient for problems with a small number
of discontinuities, and that the extra complexity (in samples
and computation) of PVFs only becomes really necessary
for more complex problems.
Mountain Car. The Mountain Car is an underpowered car stuck in a valley; to escape, it
must Ô¨Årst accelerate up the back of the valley, and then use
the resulting momentum to drive up the other side. This
induces a steep discontinuity in the value function which
makes learning difÔ¨Åcult for bases with global support.
We employed Sarsa(Œª) (Œ≥ = 1.0, Œª = 0.9, œµ = 0) with
Fourier bases of orders 3 and 5, and RBFs and PVFs of
equivalent sizes (we were unable to learn with the polynomial basis). The Œ± values used are in Table 1.
Table 1: Œ± values used for Mountain Car.
O(3) Fourier
O(5) Fourier
Figure 6: Mountain Car learning curves for agents using
(a) order 3 (b) order 5 Fourier bases, and RBFs and PVFs
with corresponding number of basis functions. Error bars
are standard error.
The results (shown in Figure 6) indicate that for low orders, the Fourier basis outperforms RBFs and PVFs. For
higher orders the Fourier basis initially performs worse (because it does not model the discontinuity well) but converges
to a better solution.
Discussion
Our results show that the Fourier basis provides a reliable
basis for value function approximation. We expect that for
many problems, the Fourier basis will be sufÔ¨Åcient to learn a
good value function without requiring transition samples or
an expensive or complex method to create a basis. Additionally, the ability to include prior knowledge about variable
interaction provides a simple and useful tool for inducing
structure in the value function.
Another vein of research has examined multi-scale basis function construction, which create a hierarchy of basis functions at different levels of resolution. One approach
employs multigrid methods to construct basis functions at
multiple levels of resolution . Diffusion wavelets are another approach that compactly represents dyadic powers of
the transition matrix at each level of the hierarchy. We examined the utility of a simple global Ô¨Åxed basis that does
not require sampling to construct; a similar examination of
wavelet basis functions may provide interesting results.
The one area in which we Ô¨Ånd that the Fourier basis has
difÔ¨Åculty is representing Ô¨Çat areas in value functions; moreover, for high order Fourier basis approximations, sudden
discontinuities may induce ‚Äúringing‚Äù around the point of
discontinuity, known as Gibbs phenomenon (Gibbs, 1898).
This may result in policy difÔ¨Åculties near the discontinuity.
Another challenge posed by the Fourier basis (and all
Ô¨Åxed basis function approximators) is how to select basis
functions when a full order approximation of reasonable size
is too large to be useful. PVFs, by contrast, scale with the
size of the sample graph (and thus could potentially take advantage of the underlying complexity of the manifold). One
potential solution is the use of feature selection to obtain a good set of basis functions for learning. The Fourier
basis is particularly well suited to feature selection because
it provides an easy way to specify a Ô¨Åxed yet complete (up
to a given order) set of basis functions, where the important relationships between state variables may be easily interpreted by a human through the c vectors corresponding to
selected basis functions. For example, if basis functions corresponding to high-order interactions between two variables
are consistently selected it would be reasonable to infer that
that interaction is important to the dynamics of the domain.
Our experiences with the Fourier basis show that this simple and easy to use basis reliably performs well on a range of
problems. As such, although it may not perform well for domains with several signiÔ¨Åcant discontinuities, its simplicity
and reliability suggest that linear function approximation using the Fourier basis should be the Ô¨Årst approach tried when
RL is applied to a new problem.
We have described the Fourier basis, a linear function approximation scheme using the terms of the Fourier series as
features. Our experimental results show that this simple and
easy to use basis performs well compared to two popular
Ô¨Åxed bases and a learned set of basis functions in three continuous RL benchmarks, and is competitive with a popular
learned basis.
Acknowledgements
SO was supported in part by the National Science Foundation under grants NSF IIS-0534999 and NSF IIS-0803288.
GDK was supported in part by the AFOSR under grant
AOARD-104135 and the Singapore Ministry of Education
under a grant to the Singapore-MIT International Design
Center. Any opinions, Ô¨Åndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reÔ¨Çect the views of the NSF, the
AFOSR, or the Singapore Ministry of Education.
Appendix: Scaling Œ±
When performing batch gradient descent on an error
function, Armijo and Plagianakos, Vrahatis, and
Magoulas suggested that the learning rate should be
inversely proportional to a Lipschitz constant of the update
term ‚àÇE(w)
‚àÇw , which is deÔ¨Åned in Euclidean space as an L
satisfying
 ‚àÇE(w1)
2 ‚â§L||w1 ‚àíw2||2, for all
w1 and w2, where E is the error function being minimized
(e.g. Bellman error for value estimate with weights w). For
any of the error functions in C2 that are typically used, this
implies that the learning rate should be inversely proportional to the maximum magnitude of the second derivative
of the error function with respect to the weights: Œ± ‚àù1
However, Armijo‚Äôs method for selecting a learning rate
only applies to batch gradient descent with a single learning
rate, and therefore only accounts for the morphology of the
error surface as a function of the weights, and not states or
individual features. Whereas the gradient in batch gradient
descent is only a function of w, when learning a value function with stochastic gradient descent, it also depends on the
current state x. Thus, while Armijo‚Äôs method acts to limit
the difference during batch gradient descent in the update
term ‚àÇE(w)
between consecutive updates, we propose that
the difference in the stochastic gradient descent update term
between consecutive updates should be limited.
We therefore modify Armijo‚Äôs method to the stochastic
gradient descent case by setting each terms‚Äô learning rate
inversely proportional to a Lipschitz constant of ‚àÇE(x,w)
which is deÔ¨Åned in Euclidean space as a Ki satisfying
 ‚àÇE(y1)
2 ‚â§Ki ||y1 ‚àíy2||2, for all y1 and y2
where y = [x w] and E(y) = E(x, w) is the error in
state x with weighting w.
To derive estimates of learning rate proportions, we assume we are performing stochastic gradient descent with the error function E(x, w) =
V (x) ‚àí¬ØV (x)
We assume that the learning rate Œ±1 for a term with unit
coefÔ¨Åcient vector,
c1
2 = 1, is provided, and then scale
the learning rate for each other term as
where Ki is a Lipschitz constant for the term with coefÔ¨Åcient
vector ci and weight wi:
conclusion.
œÄci (V (x) ‚àíw ¬∑ œÜ(x)) +
‚àÇx (V (x) ‚àíw ¬∑ œÜ(x))
ci
functions,
ci
2 + maxx,w (||œÄ (V (x) ‚àíw ¬∑ œÜ(x))||2
‚àÇx (V (x) ‚àíw ¬∑ œÜ(x))
, which satisÔ¨Åes Equation 6
for all i Ã∏= 0. Substituting into Equation 5, we conclude that
the ith term‚Äôs learning rate should be Œ±i = Œ±1/
ci
avoid division by zero for c0 = 0, we select Œ±0 = Œ±1.