XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
Learning Deep Representations of
Appearance and Motion for Anomalous
Event Detection
 
1DISI, University of Trento,
Trento, Italy
Elisa Ricci2,3
 
2Fondazione Bruno Kessler (FBK),
Trento, Italy
Yan Yan1,4, Jingkuan Song1
 , 
3University of Perugia,
Perugia, Italy
Nicu Sebe1
 
4ADSC, UIUC Singapore,
We present a novel unsupervised deep learning framework for anomalous event detection in complex video scenes. While most existing works merely use hand-crafted
appearance and motion features, we propose Appearance and Motion DeepNet (AMDN)
which utilizes deep neural networks to automatically learn feature representations. To
exploit the complementary information of both appearance and motion patterns, we introduce a novel double fusion framework, combining both the beneﬁts of traditional early
fusion and late fusion strategies. Speciﬁcally, stacked denoising autoencoders are proposed to separately learn both appearance and motion features as well as a joint representation (early fusion). Based on the learned representations, multiple one-class SVM
models are used to predict the anomaly scores of each input, which are then integrated
with a late fusion strategy for ﬁnal anomaly detection. We evaluate the proposed method
on two publicly available video surveillance datasets, showing competitive performance
with respect to state of the art approaches.
Introduction
A fundamental challenge in intelligent video surveillance is to automatically detect abnormal
events in long video streams. This problem has attracted considerable attentions from both
academia and industry in recent years . Video anomaly detection is also
important as it is related to other interesting topics in computer vision, such as dominant
behavior detection , visual saliency and interestingness prediction . A typical
approach to tackle the anomaly detection task is to learn a model which describes normal
activities in the video scene and then discovers unusual events by examining patterns which
distinctly diverge from the model. However, the complexity of scenes and the deceptive
nature of abnormal behaviours make anomaly detection still a very challenging task.
c⃝2015. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
Pages 8.1-8.12
DOI: 
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
Image Sequences
Optical Flow Maps
Foreground
Extract multi-scale
image patches and warp
Extract ﬁxed-sized
optical ﬂow patches
Late Fusion
Anomaly Detection
Appearance Representations
Motion Representations
Joint Representations
Pixel-level
Figure 1: Overview of the proposed AMDN method for anomalous event detection.
Among previous works, several anomaly detection approaches are based on analyzing
individual moving objects in the scene. Tracking is usually an initial step for this class of
methods. By using accurate tracking algorithms, trajectory extraction can be carried out to
further perform trajectory clustering analysis or design representative features to
model typical activities and subsequently discover anomalies. In , trajectories which are
spatially close and have similar motion patterns are identiﬁed and used for detecting unusual
events. Vaswani et al. propose a “shape activity” model to describe moving objects and
detect anomalies. However, as tracking performance signiﬁcantly degrades in the presence
of several occluded targets, tracking-based methods are not suitable for analyzing complex
and crowded scenes.
To overcome the aforementioned limitations, researchers address the problem by learning
spatio/temporal activity patterns in a local or a global context from either 2D image cells or
3D video volumes . This category of methods builds the models based
on hand-crafted features extracted from low-level appearance and motion cues, such as color,
texture and optical ﬂow. Commonly used low-level features include histogram of oriented
gradients (HOG), 3D spatio-temporal gradient, histogram of optical ﬂow (HOF), among others. Cong et al. employ multi-scale histograms of optical ﬂow and a sparse coding model
and use the reconstruction error as a metric for outlier detection. Mehran et al. propose
a “social force” model based on optical ﬂow features to represent crowd activity patterns
and identify anomalous activities. In co-occurrence statistics of spatio-temporal events
are employed in combination with Markov Random Fields (MRFs) to discover unusual activities. Kratz et al. introduce a HMMs-based approach for detecting abnormal events
by analyzing the motion variation of local space-time volumes. Kim et al. propose a
method based on local optical ﬂow features and MRFs to spot unusual events. Mahadevan
et al. introduce a mixtures of dynamic textures model to jointly employ appearance
and motion features. However, the adoption of hand-crafted features is a clear limitation of
previous methods, as it implies enforcing some some a priori knowledge which, in case of
complex video surveillance scene, is very difﬁcult to deﬁne.
Recently, deep learning architectures have been successfully used to tackle various computer vision tasks, such as image classiﬁcation , object detection and activity recognition . However, these works are mainly based on Convolutional Neural Networks and
consider a supervised learning scenario. Unsupervised deep learning approaches based on
autoencoder networks have also been investigated to address important tasks such as
object tracking and face alignment . The key of the success is that, using deep
architectures, rich and discriminative features can be learned via multi-layer nonlinear trans-
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
formations. Therefore, it is reasonable to expect that detecting unusual events in videos can
also beneﬁt from deep learning models.
Following this intuition, in this paper we propose a novel Appearance and Motion Deep-
Net (AMDN) framework for discovering anomalous activities in complex video surveillance
scenes. Opposite to previous works , instead of using hand-crafted features to
model activity patterns, we propose to learn discriminative feature representations of both
appearance and motion patterns in a fully unsupervised manner. A novel approach based on
stacked denoising autoencoders (SDAE) is introduced to achieve this goal. An overview
of the proposed AMDN is shown in Fig. 1. Low-level visual information including still image patches and dynamic motion ﬁelds represented with optical ﬂow is used as input of two
separate networks, to ﬁrst learn appearance and motion features, respectively. To further
investigate the correlations between appearance and motion, early fusion is performed by
combining image pixels with their corresponding optical ﬂow to learn a joint representation. Finally, for abnormal event prediction, a late fusion strategy is introduced to combine
the anomaly scores predicted by multiple one-class SVM classiﬁers, each corresponding to
one of the three learned feature representations. The beneﬁts of the proposed double fusion
framework (i.e. combining both early fusion and late fusion strategies) are conﬁrmed by our
extensive experimental evaluation, conducted on two publicly available datasets.
In summary, the main contributions of this paper are: i) As far as we know, we are the
ﬁrst to introduce an unsupervised deep learning framework to automatically construct discriminative representations for video anomaly detection. ii) We propose a new approach
to learn appearance and motion features as well as their correlations. Deep learning methods for combining multiple modalities have been investigated in previous works .
However, to our knowledge, this is the ﬁrst work where multimodal deep learning is applied to anomalous event detection. iii) A double fusion scheme is proposed to combine
appearance and motion features for discovering unusual activities. It is worth noting that the
advantages of combining early and late fusion approaches have been investigated in previous works . However, Lan et al. do not consider a deep learning framework. iv) The
proposed method is validated on challenging anomaly detection datasets and we obtain very
competitive performance compared with the state-of-the-art.
AMDN for Abnormal Event Detection
The proposed AMDN framework for detecting anomalous activities is based on two main
building blocks (Fig.1). First, SDAE are used to learn appearance and motion representations
of visual data, as well as a joint representation capturing the correlation between appearance
and motion features (Sec. 2.1). In the second phase (Sec. 2.2), to detect anomalous events,
we propose to train three separate one-class SVMs based on the three different types
of learned feature representations. Once the one-class SVM models are learned, given a test
sample corresponding to an image patch, three anomaly scores are computed and combined.
The combination of the one-class SVM scores is obtained with a novel late fusion scheme.
In the following we describe the proposed approach in details.
Learning Deep Appearance and Motion Representations
In this subsection we present the proposed AMDN for learning deep representations of appearance and motion. In the following we ﬁrst introduce denoising autoencoders and then
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
describe the details of the structure and the learning approach of the proposed AMDN.
Denoising Autoencoders
A Denoising Auto-Encoder (DAE) is a one-hidden-layer neural network which is trained to
reconstruct a data point xi from its (partially) corrupted version ˜xi . Typical corrupted
inputs are obtained by drawing samples from a conditional distribution p(x|˜x) (e.g. common
choices for corrupting samples are additive Gaussian white noise or salt-pepper noise). A
DAE neural network can be divided into two parts: encoder and decoder, with a single shared
hidden layer. These two parts actually attempt to learn two mapping functions, denoted as
fe(W,b) and fd(W′,b′), where W,b denote the weights and the bias term of the encoder
part, and W′,b′ refer to the corresponding parameters of the decoder. For a corrupted input
˜xi, a compressed hidden layer representation hi can be obtained through hi = fe(˜xi | W,b) =
σ(W˜xi + b). Then, the decoder tries to recover the original input xi from hi computing
ˆxi = fd(hi | W′,b′) = s(W′hi+b′), The function σ(·) and s(·) are activation functions, which
are typically nonlinear transformations such as the sigmoid. Using this encoder/decoder
structure, the network can learn a more stable and robust feature representations of the input.
Given a training set T = {xi}N
i=1, a DAE learns its parameters (W,W′,b,b′) by solving
the following regularized least square optimization problem:
∥xi −ˆxi∥2
where ∥· ∥F denotes the Frobenius norm. The ﬁrst term represents the average reconstruction error, while the weight penalty term is introduced for regularization. The parameter λ
balances the importance of the two terms. Typically, sparsity constraints are imposed on
the output of the hidden units to discover meaningful representations from the data . If
we let µj be the target sparsity level and ˆµ j = 1
i be the average activation values
all over all training samples for the j-th unit, an extra penalty term based on cross-entropy,
ϕ(µµµ|| ˆµµµ) = −∑H
j=1[µ j log( ˆµj) + (1 −µj)log(1 −ˆµj)], can be added to (1) to learn a sparse
representation. Here, H is the number of hidden units. The optimization problem (1) has a
non-convex objective function and gradient descent can be used to compute a local optima.
AMDN Structure
The proposed AMDN structure consists of three SDAE pipelines (Fig.1) corresponding to
different types of low-level inputs. The three SDAE networks learn appearance and motion
features as well as a joint representation of them. We show the basic structures of the proposed SDAE networks in Fig. 2 (a) and (b). Each SDAE consists of two parts: encoder and
decoder. For the encoder part, we use an over-complete set of ﬁlters in the ﬁrst layer to
capture a representative information from the data. Then, the number of neurons is reduced
by half in the next layer until reaching the “bottleneck” hidden layer. The decoder part has
a symmetric structure with respect to the encoder part. We now describe the proposed three
feature learning pipelines in details.
Appearance representation. This SDAE aims at learning mid-level appearance representations from the original image pixels. To capture rich appearance attributes, a multi-scale
sliding-window approach with a stride d is used to extract dense image patches, which are
then warped into equal size wa × ha × ca, where wa,ha are the width and height of each
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
Bottleneck
hidden layer
Figure 2: (a) The structure of the appearance and motion representation learning pipelines.
(b) The structure of the joint representation learning pipeline. (c) Some examples of the
weight ﬁlters extracted from the ﬁrst layer of the joint representation learning pipeline.
patch and ca is the number of the channels (ca = 1 for gray images). The warped patches
i ∈IRwa×ha×ca are used for training. All the patches are linearly normalized into a range
 . We stack 4 encoding layers with νa × wa × ha × ca neurons in the ﬁrst layer, where
νa > 1 is an ampliﬁcation factor for constructing an over-complete set of ﬁlters.
Motion representation. The motion information is computed with optical ﬂow. We use
a sliding window approach with windows of ﬁxed size wm ×hm ×cm (cm = 2 for optical ﬂow
magnitude along x and y axes), to generate dense optical ﬂow patches xm
i ∈IRwm×hm×cm for
motion representation learning. Similar to the appearance feature pipeline, the patches are
normalized into within each channel and 4 encoding layers are used. The number of
neurons of the ﬁrst layer is set to νm ×wm ×hm ×cm.
Joint appearance and motion representation. The above-mentioned SDAE learn appearance and motion features separately. Taking into account the correlations between motion and appearance, we propose to couple these two pipelines to learn a joint representation.
The network training data xJ
i ∈IRw j×h j×(ca+cm) are obtained through a pixel-level early fusion
of the gray image patches and the corresponding optical ﬂow patches.
AMDN Training
We train the AMDN with two steps: pretraining and ﬁne-tuning. The layer-wise pretraining learns one single denoising auto-encoder at a time using (1) with sparsity constraints
(Sec.2.1.1). The input is corrupted to learn the mapping function fe(·), which is then used
to produce the representation for the next layer with uncorrupted inputs. By using a greedy
layer-wise pretraining, the denoising autoencoders can be stacked to build a multi-layer feedforward deep neural network, i.e. a stacked denoising autoencoder. The network parameters
are initialized through pretraining all layers, and then ﬁne-tuning is used to adjust parameters
over the whole network.
Fine-tuning treats all the layers of an SDAE as a single model. Given a training set
i=1 with Nk training samples (k ∈{A,M,J} corresponds to appearance, motion
and joint representation, respectively), the backpropagation algorithm can be used to ﬁnetune the network. The following objective function is used for ﬁne-tuning the SDAE with
2L+1 layers:
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
where λF is a user deﬁned parameter. To speed up the convergence during training, stochastic
gradient descent (SGD) is employed and the training set is divided into mini-batches with
b. Fig. 2 (c) shows some of the learned ﬁlters in the ﬁrst layer after ﬁne-tuning for the
joint representation learning pipeline.
After ﬁne-tuning the whole network, the learned features representation can be computed
to perform video anomaly detection. Theoretically, the output of each layer in an SDAE can
be used as a novel learned feature representation. In this work, we choose the output of the
“bottleneck" hidden layer to obtain a more compact representation. Let xk
i be the i-th input
data sample, and σk
l ) be the mapping function of the l-th hidden layer of the k-th
SDAE pipeline. The learned features, sk
i , can be extracted through a forward pass computing
i = σL(σL−1(···σ1(Wk
1))), where the L-th hidden layer is the “bottleneck" hidden
Abnormal Event Detection with Deep Representations
We formulate the video anomaly detection problem as a patch-based binary categorization
problem, i.e. given a test frame we obtain MI ×NI patches via sliding window with a stride
d and classify each patch as corresponding to a normal or abnormal region. Speciﬁcally,
given each test patch t we compute three anomaly scores Ak(sk
t ), k ∈{A,M,J}, using oneclass SVM models and the computed features representations sk
t . The three scores are then
linearly combined to obtain the ﬁnal anomaly score A(sk
t ) = ∑k∈{A,M,J} αkAk(sk
t ). A preprocessing of dynamic background subtraction can be carried out to improve the computing
efﬁciency during the test phase as the anomalies exist in the foreground region.
One-class SVM Modeling
One-class SVM is a widely used algorithm for outlier detection, where the main idea is
to learn a hypersphere in the feature space and map most of the training data into it. The
outliers of the data distribution correspond to point lying outside the hypersphere. Formally,
given a set of training samples S = {sk
i=1, the underlying problem of one-class SVM can
be formulated as the following quadratic program:
i ) ≥ρ −ξi, ξi ≥0.
where w is the learned weight vector, ρ is the offset, Φ(·) is a feature projection function
which maps feature vector sk
i into a higher dimensional feature space. The user deﬁned
parameter ν ∈(0,1] regulates the expected fraction of outliers distributed outside the hypersphere. Introducing a nonlinear mapping, the projection function Φ(·) can be deﬁned
implicitely by introducing an associated kernel function k(sk
j) and (3)
can be solved in the corresponding dual form . In our experiments we consider a rbf
kernel, k(sk
. Given the optimal w and ρ obtained by solving (3), an outlier score for a test sample sk
t of the k-th SDAE pipeline can be estimated by computing
t ) = ρ −wTΦ(sk
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
Figure 3: Examples of anomaly detection results on Ped1 (top) and Ped2 (bottom) sequences.
Our approach can detect anomalies such as bikes, vehicles and skaters and provides accurate
localization.
Late Fusion for Anomaly Detection
An unsupervised late fusion scheme is designed to automatically learn the weight vector
ααα = [αA,αM,αJ]. The weight learning approach is based on the following optimization:
WskSk WskSkT
+λs ∥ααα∥2
where Sk = [sk
Nk] is the matrix of training set samples, Wsk maps the k-th feature sk
into a new subspace and WskSk  WskSkT represents the covariance of k-th feature type in
the new subspace. The term ∥ααα∥2
2 is introduced to avoid overﬁtting and λs is a user deﬁned
parameter. To solve (4), we ﬁrst get Wsk as the d eigenvectors of SkSkT corresponding to the
d-largest eigenvalues. Then, ααα can be obtained by solving the simplex problem :
2∥ααα −c∥2
with c = [cA,cM,cJ], ck = −1
WskSk WskSkT
. Then for each patch t, we identify if
it corresponds to an abnormal activity by computing the associated anomaly score A(sk
comparing it with a threshold η, i.e. A(sk
Experimental Results
Datasets and Experimental Setup.
The proposed method is mainly implemented in Matlab and C++ based on Caffe framework . The code for optical ﬂow calculation is written in
C++ and wrapped with Matlab mex for computational efﬁciency . For one-class SVMs,
we use the LIBSVM library (version 3.2) . The experiments are carried out on a PC with
a middle-level graphics card (NVIDIA Quadro K4000) and a multi-core 2.1 GHz CPU with
32 GB memory. Two publicly available datasets, the UCSD (Ped1 and Ped2) dataset 
and the Train dataset , are used to evaluate the performance of the proposed approach.
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
False Positive Rate (FPR)
True Positive Rate (TPR)
Social Force + MPPCA 
Social Force 
Sparse reconstruction 
Local statistical 
Detection at 150FPS 
(a) Frame-level ROC curve of PED1 Dataset
False Positive Rate (FPR)
True Positive Rate (TPR)
Social Force + MPPCA 
Social Force 
Sparse Reconstruction 
Detection at 150FPS 
(b) Pixel-level ROC curve of PED1 Dataset
Figure 4: UCSD dataset (Ped1 sequence): comparison of frame-level and pixel-level
anomaly detection results with state of the art methods.
Ped1(frame)
Ped1(pixel)
MPPCA 
Social force 
Social force+MPPCA 
Sparse reconstruction 
Mixture dynamic texture 
Local Statistical Aggregates 
Detection at 150 FPS 
Joint representation (early fusion)
Fusion of appearance and motion pipelines (late fusion)
AMDN (double fusion)
Table 1: UCSD dataset: comparison in terms of EER (Equal Error Rate) and AUC (Area
Under ROC) with the state of the art methods on Ped1 and Ped2.
The UCSD pedestrian dataset is a challenging anomaly detection dataset including
two subsets: Ped1 and Ped2. The video sequences depict different crowded scenes and
anomalies include bicycles, vehicles, skateboarders and wheelchairs. In some frames the
anomalies occur at multiple locations. Ped1 has 34 training and 16 test image sequences
with about 3,400 anomalous and 5,500 normal frames, and the image resolution is 238×158
pixels. Ped2 has 16 training and 12 test image sequences with about 1,652 anomalous and
346 normal frames. The image resolution is 360×240 pixels.
The Train dataset depicts moving people inside a train. This is also a challenging
abnormal event detection dataset due to dynamic illumination changes and camera shake
problems. The dataset consists of 19218 frames, and the anomalous events are mainly due
to unusual movements of people on the train.
Quantitative evaluation.
In the ﬁrst series of experiments we evaluate the performance of
the proposed method on the UCSD dataset. For appearance learning, patches are extracted
using a sliding window approach at three different scales, i.e. 15×15, 18×18 and 20×20
pixels. This generates more than 50 million image patches, 10 million of which are randomly
sampled and warped into the same size (wa ×ha = 15×15 pixels) for training. For learning
the motion representation, the patch size is ﬁxed to wm ×hm = 15×15 pixels, and 6 million
training patches are randomly sampled. In the test phase, we use sliding widow with a size of
15×15 and a stride d = 15. The number of neurons of the ﬁrst layer of the appearance and
motion network is both set to 1024, while for the joint pipeline is 2048. Then the encoder
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
&$"%%*
(%"% 
##&-
&+))"%
Figure 5: Train dataset: (a) a frame depicting typical activities, (b) an example of a detected
anomaly. (c) precision/recall curve.
part can be simply deﬁned as: 1024 →512(1024) →256(512) →128(256), and the
decoder part is a symmetric structure. For the pre-training of the DAE, the corrupted inputs
are produced by adding a Gaussian noise with variance 0.0003. The network training is
based on the SGD with a momentum parameter set to 0.9. We use a ﬁxed learning rates
λ = 0.01, λF = 0.0001 and a mini-batch size Nb = 256. For one-class SVMs, the parameter
ν is tuned with cross validation. The learned late fusion weights [αA,αM,αJ] are obtained
with λs = 0.1 and for Ped1 and Ped2 are [0.2,0.5,0.3] and [0.2,0.4,0.4], respectively.
Some examples of anomaly detection results on the UCSD dataset are shown in Fig. 3.
To perform a quantitative evaluation, we use both a frame-level ground truth and a pixel-level
ground truth. The frame-level ground truth represents whether one or more anomalies occur
in a test frame. The pixel-level ground truth is used to evaluate the anomaly localization
performance. If the detected anomaly region is more than 40% overlapping with the annotated region, it is a true detection. We carry out a frame-level evaluation on both Ped1 and
Ped2. Ped1 provides 10 test image sequences with pixel-level ground truth. The pixel-level
evaluation is performed on these test sequences.
Fig. 4 (a) and (b) show the frame-level and pixel-level detection results on Ped1. The
ROC curve is produced by varying the threshold parameter η. Table 1 shows a quantitative
comparison in terms of Area Under Curve (AUC) and Equal Error Rate (EER) of our method
with several state-of-the-art approaches. From the frame-level evaluation, it is evident that
our method outperforms most previous methods and that its performance are competitive
with the best two baselines and . Moreover, considering pixel-level evaluation,
i.e. accuracy in anomaly localization, our method outperforms all the competing approaches
both in terms of EER and AUC. Table 1 also demonstrate the advantages of the proposed
double fusion strategy. Our AMDN guarantees better performance than early fusion and
late fusion approaches. Speciﬁcally, for early fusion we only consider the learned joint
appearance/motion representation and a single one-class SVM. For late fusion we use the two
separate appearance and motion pipelines and the proposed fusion scheme but we discard
the joint representation pipeline. Interestingly, in this application the late fusion strategy
outperforms early fusion.
In the second series of experiments we consider the Train dataset. For parameters, we
use the same experimental setting of the UCSD experiments, except from the parameters
λF and Nb which are set to 0.00001 and 100, respectively. The learned late fusion weights
are [αA,αM,αJ] = [0.3,0.4,0.3]. We compare the proposed approach with several methods,
XU ET AL.: LEARNING DEEP REPRESENTATIONS OF APPEARANCE AND MOTION...
including dominant behavior learning , spatio-temporal oriented energies , local optical ﬂow , behavior templates and mixture of Gaussian. From the precision/recall
curve shown in Fig. 5 (c), it is clear that our method outperforms all the baselines.
Conclusions
We presented a novel unsupervised learning approach for video anomaly detection based on
deep representations. The proposed method is based on multiple stacked autoencoder networks for learning both appearance and motion representations of scene activities. A double
fusion scheme is designed to combine the learned feature representations. Extensive experiments on two challenging datasets demonstrate the effectiveness of the proposed approach
and show competitive performance with respect to existing methods. Future works include
investigating other network architectures, alternative approaches for fusing multimodal data
in the context of SDAE, and extending our framework using multi-task learning for
detecting anomalies in heterogeneous scenes.
Acknowledgments
This work was partially supported by the MIUR Cluster project Active Ageing at Home, the
EC H2020 project ACANTO and by A*STAR Singapore under the Human-Centered Cyberphysical Systems (HCCS) grant. The authors also would like to thank NVIDIA for GPU