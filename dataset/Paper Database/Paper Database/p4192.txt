Edinburgh Research Explorer
Measuring the Objectness of Image Windows
Citation for published version:
Alexe, B, Deselaers, T & Ferrari, V 2012, 'Measuring the Objectness of Image Windows', IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 34, no. 11, pp. 2189-2202.
 
Digital Object Identifier (DOI):
10.1109/TPAMI.2012.28
Link to publication record in Edinburgh Research Explorer
Document Version:
Peer reviewed version
 
IEEE Transactions on Pattern Analysis and Machine Intelligence
General rights
Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)
and / or other copyright owners and it is a condition of accessing these publications that users recognise and
abide by the legal requirements associated with these rights.
Take down policy
The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer
content complies with UK legislation. If you believe that the public display of this file breaches copyright please
contact providing details, and we will remove access to the work immediately and
investigate your claim.
Download date: 26. Mar. 2025
Measuring the objectness of image windows
Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari
Abstract—We present a generic objectness measure, quantifying how likely it is for an image window to contain an object of any
class. We explicitly train it to distinguish objects with a well-deﬁned boundary in space, such as cows and telephones, from amorphous
background elements, such as grass and road. The measure combines in a Bayesian framework several image cues measuring
characteristics of objects, such as appearing different from their surroundings and having a closed boundary. These include an
innovative cue to measure the closed boundary characteristic. In experiments on the challenging PASCAL VOC 07 dataset, we
show this new cue to outperform a state-of-the-art saliency measure, and the combined objectness measure to perform better than
any cue alone. We also compare to interest point operators, a HOG detector, and three recent works aiming at automatic object
segmentation. Finally, we present two applications of objectness. In the ﬁrst, we sample a small number windows according to their
objectness probability and give an algorithm to employ them as location priors for modern class-speciﬁc object detectors. As we show
experimentally, this greatly reduces the number of windows evaluated by the expensive class-speciﬁc model. In the second application,
we use objectness as a complementary score in addition to the class-speciﬁc model, which leads to fewer false positives. As shown in
several recent papers, objectness can act as a valuable focus of attention mechanism in many other applications operating on image
windows, including weakly supervised learning of object categories, unsupervised pixelwise segmentation, and object tracking in video.
Computing objectness is very efﬁcient and takes only about 4 sec. per image.
Index Terms—Objectness Measure, Object Detection, Object Recognition.
INTRODUCTION
In recent years object class detection has become a major
research area. Although a variety of approaches exist , most state-of-the-art detectors follow the sliding-window
paradigm . A classiﬁer is ﬁrst trained to
distinguish windows containing instances of a given class from
all other windows. The classiﬁer is then used to score every
window in a test image. Local maxima of the score localize
instances of the class.
While object detectors are specialized for one object class,
such as cars or swans, in this paper we deﬁne and train a
measure of objectness generic over classes. It quantiﬁes how
likely it is for an image window to cover an object of any class.
Objects are standalone things with a well-deﬁned boundary
and center, such as cows, cars, and telephones, as opposed to
amorphous background stuff, such as sky, grass, and road (as
in the things versus stuff distinction of ). Fig. 1 illustrates
the desired behavior of an objectness measure. It should score
highest windows ﬁtting an object tight (green), score lower
windows covering partly an object and partly the background
(blue), and score lowest windows containing only stuff (red).
In order to deﬁne the objectness measure, we argue that any
object has at least one of three distinctive characteristics:
• a well-deﬁned closed boundary in space
• a different appearance from its surroundings 
• sometimes it is unique within the image and stands out
as salient 
• B. Alexe is with the Computer Vision Laboratory at ETH Zurich. E-mail:
 
• T. Deselaers is with Google Zurich. E-mail: 
• V. Ferrari is with the Computer Vision Laboratory at ETH Zurich. E-mail:
 
Fig. 1: Desired behavior of an objectness measure. The objectness
measure should score the blue windows, partially covering the
objects, lower than the ground truth windows (green), and score even
lower the red windows containing only stuff.
Many objects have several of these characteristics at the same
time (ﬁg. 2-6). In sec. 2.1 to 2.4 we explore several image
cues designed to measure these characteristics.
Although objects can appear at a variety of locations and
sizes within an image, some windows are more likely to cover
objects than others, even without analyzing the pixel patterns
inside them. A very elongated window in an image corner is
less probable a priori than a square window in the middle of
it. We explore this idea in sec. 2.5 and propose an objectness
cue based on the location and size of a window.
This paper makes several contributions: (a) We design
an objectness measure and explicitly train it to distinguish
windows containing an object from background windows. This
measure combines in a Bayesian framework several cues based
on the above characteristics (secs. 2 and
3). (b) On the
task of detecting objects in the challenging PASCAL VOC
07 dataset , we demonstrate that the combined objectness
measure performs better than any cue alone, and also outperforms traditional salient blob detectors , interest
point detectors , the Semantic Labeling technique of ,
and a HOG detector trained to detect objects of arbitrary
classes. (c) We give an algorithm that employs objectness to
greatly reduce the number of evaluated windows evaluated by
modern class-speciﬁc object detectors (sec. 6).
Different from ESS , our method imposes no restriction on
the class model used to score a window. (d) Analoguosly, we
show how to use objectness for reducing the number of falsepositives returned by class-speciﬁc object detectors (sec. 7).
In addition to the two applications above, objectness is
valuable in others as well. Since the publication of a preliminary version of this work , objectness has already been
used successfully to (a) facilitate learning new classes in a
weakly supervised scenario , where the location
of object instances is unknown. Objectness steers the localization algorithm towards objects rather than backgrounds, e.g.
avoiding to select only grass regions in a set of sheep images;
(b) analoguously, to support weakly supervised pixelwise
segmentation of object classes and unsupervised object
discovery ; (c) learning spatial models of interactions
between humans and objects ; (d) content-aware image
resizing ; (e) object tracking in video, as an additional
likelihood term preventing the tracker from drifting to the
background (ongoing work by Van Gool’s group at ETH
Zurich). The source code of the objectness measure is available
at Computing objectness
is very efﬁcient and takes only about 4 sec. per image
Related work
This paper is related to several research strands, which differ
in how they deﬁne ‘saliency’.
Interest points. Interest point detectors (IPs) respond
to local textured image neighborhoods and are widely used
for ﬁnding image correspondences and recognizing speciﬁc objects . IPs focus on individual points, while our
approach is trained to respond to entire objects. Moreover,
IPs are designed for repeatable detection in spite of changing
imaging conditions, while our objectness measure is trained
to distinguish objects from backgrounds. In sec. 5, we experimentally evaluate IPs on our task.
Class-speciﬁc saliency. A few works deﬁne
as salient the visual characteristics that best distinguish a
particular object class (e.g. cars) from others. This classspeciﬁc saliency is very different from the class-generic task
we tackle here.
Generic saliency. Since , numerous works appeared to measure the saliency of pixels, as the degree
of uniqueness of their neighborhood wrt the entire image or
the surrounding area . Salient pixels form blobs that
‘stand out’ from the image. These works implement selective
visual attention from a bottom-up perspective and are often
inspired by studies of human eye movements .
Liu et al. detect a single dominant salient object in an
image. They formulate the problem as image segmentation.
The method combines local, regional and global pixel-based
saliency measurements in a CRF and derives a binary segmentation separating the object from the background. Valenti
et al. measure the saliency of individual pixels, and then
returns the region with the highest sum of pixel-saliency as
the single dominant object. Achanta et al. ﬁnd salient
regions using a frequency-tuned approach. Their approach
segments objects by adaptive thresholding of fully resolution
saliency maps. The saliency maps are obtained by combining
several band-pass ﬁlters that retain a wider range of spatial
frequencies than . Berg and Berg ﬁnd iconic
images representative for an object class. Their approach
returns images having a large, centered object, which is clearly
separated from the background. These approaches do not seem
suitable for the PASCAL VOC 07 dataset where many
objects are present in an image and they are rarely dominant
(ﬁg. 15, 16).
This paper is related to the above works, as we are looking
for generic objects. We incorporate a state-of-the-art saliency
detector as one cue into our objectness measure. However,
we also include other cues than ‘stand out’ saliency and
demonstrate that our combined measure performs better at
ﬁnding objects (sec. 5).
Our work differs from the above also in other respects. The
unit of analysis is not a pixel, as possibly belonging to an object, but a window, as possibly containing an entire object. This
enables scoring all windows in an image and sampling any
desired number of windows according to their scores. These
can then directly be fed as useful location priors to object
class learning and detection algorithms, rather than making
hard decisions early on. We experimentally demonstrate this
with applications to speeduping object detectors (sec. 6) and
to reducing their false-positive rates (sec. 7).
Analyzing windows also enables evaluating more complex
measures of objectness. For example, in sec. 2.4 we propose
a new image cue and demonstrate it performs better than
traditional saliency cues at ﬁnding entire objects.
Finally, we evaluate our method on a more varied and
challenging dataset (PASCAL VOC 07), where most images
contain many objects and they appear over a wide range of
scales. We explicitly train our objectness measure to satisfy the
strict PASCAL-overlap criterion, and evaluate its performance
using it. This matters because it is the standard criterion for
evaluating the intended clients of our objectness measure, i.e.
object detection algorithms.
Object proposals. Since the publication of the ﬁrst version
of this work , two closely related independent works
appeared . They share with our work the overall idea of
proposing a few hundred class-independent candidates likely
to cover all objects in an image. While both works 
produce rough segmentations as object candidates, they are
computationally very expensive and take 2-7 minutes per
image (compared to a few seconds for our method). In sec. 5
we compare the ability of our method to detect generic objects
to .
Efﬁcient sliding window object detection. State-of-the-art
object detection algorithms often rely on the sliding window paradigm , which scores a large number
of windows in a test image using a classiﬁer. To keep the
computational cost manageable, researchers are careful in
selecting classiﬁers that can be evaluated rapidly (e.g. linear
SVMs ).
Recently, several techniques have been proposed to reduce
the cost of sliding window algorithms. One approach is to
Fig. 2: MS success and failure. Success: the large giraffe in the
original image (a) appears as a blob in the saliency map for a
high scale (b), while the tiny airplane in the map for a low scale
(c). Having multi-scale saliency maps is important for ﬁnding more
objects in challenging datasets. Interestingly, at the low scale the
head of the giraffe is salient, rather than the whole giraffe. Failure:
the numerous cars in the original image (d) are not salient at any
scale. We show the saliency maps for 2 scales in (e) and (f). The
contour of the building appears more salient than the cars.
reduce the cost of evaluating a complex classiﬁer on a window. Excellent approximations to the histogram intersection
kernel and the χ2 kernel have been presented.
Another approach is to reduce the number of times the
complex classiﬁer is evaluated. In this is achieved
by ﬁrst running a linear classiﬁer over all windows, and then
evaluating a non-linear kernel classiﬁer only on a few highly
scored windows. Lampert et al. presented a branch-andbound scheme that exactly ﬁnds the single highest scored
window while minimizing the number of times the classiﬁer
is evaluated.
In sec. 6 we apply objectness to greatly reduce the number
of windows evaluated by the classiﬁer. Our method can be
applied to any window classiﬁer, whereas is restricted to
those for which the user can provide a good bound on the
highest score in a contiguous set of windows. Our speedup is
orthogonal to techniques to reduce the cost of evaluating one
window, and could be combined with them .
Plan of the paper
Sec. 2 describes the cues composing our objectness measure.
Secs. 3 and 4 show how to learn the cue parameters and
how to combine them in a Bayesian framework. In sec. 5 we
evaluate objectness on the very challenging PASCAL VOC
2007 dataset, and then show applications to aiding classspeciﬁc object detectors in secs. 6 and 7.
OBJECTNESS CUES
As mentioned in sec. 1, objects in an image are characterized
by a closed boundary in 3D space, a different appearance from
their immediate surrounding and sometimes by uniqueness. In
secs. 2.1 to 2.4 we present four image cues to measure these
characteristics for an image window. As a complementary
strategy, we deﬁne a last cue based on the location and size of
a window, without analyzing the actual image pixels (sec. 2.5).
Fig. 3: CC success and failure. Success: the windows containing
the objects (cyan) have high color contrast with their surrounding
ring (yellow) in images (a) and (b). Failure: the color contrast for
the object in image (c) is much lower. The surrounding ring drawn
has the optimal size θ∗
CC, as learned in sec. 3.
Multi-scale Saliency (MS)
Hou et al. proposed a global saliency measure based on
the spectral residual of the FFT, which favors regions with
an unique appearance within the entire image f. The saliency
map I of an image f is obtained at each pixel p as
I(p) = g(p) ∗F−1h
 R(f) + P(f)
where F is the FFT, R(f) and P(f) are the spectral residual
and the phase spectrum of the image f, and g is a
Gaussian ﬁlter used for smoothing the output. Since this
measure prefers objects at a certain scale, we extend it to
multiple scales (ﬁg. 2). Moreover, as suggests, we process
the color channels independently as separate images. For each
scale s, we use to obtain a saliency map Is
MS(p). Based
on this, we deﬁne the saliency of a window w at scale s as
MS(p) × |{p ∈w|Is
where the scale-speciﬁc thresholds θs
MS are parameters to be
learned (sec. 3), and | · | indicates the number of pixels.
Saliency is higher for windows with higher density of salient
pixels (second factor), with a bias towards larger windows
(ﬁrst factor). Density alone would score highest windows
comprising just a few very salient pixels. Instead, our measure
is designed to score highest windows around entire blobs
of salient pixels, which correspond better to whole objects
(ﬁg. 2). The need for multiple scales is evident in ﬁg. 2 as
the windows covering the two objects in the image (airplane,
giraffe) score highest at different scales. This MS cue measures
the uniqueness characteristic of objects.
Color Contrast (CC)
CC is a measure of the dissimilarity of a window to its immediate surrounding area (ﬁg. 3). The surrounding Surr(w, θCC)
of a window w is a rectangular ring obtained by enlarging the
window by a factor θCC in all directions, so that
|Surr(w, θCC)|
The CC between a window and its surrounding is computed
as the Chi-square distance between their LAB histograms h
 w, θCC) = χ2(h(w), h(Surr(w, θCC))
Fig. 4: ED success and failure. Success: given images (a) and (b)
the cyan windows covering the bus and the aeroplane score high
as the density of edges is concentrated in these regions. Failure: in
image (c) the cyan window along with many other windows covering
the water score high determining a high rate of false positives. In
particular the windows covering the boats have a low score. We show
the Canny edge maps in (d), (e) and (f) for a value θED = 2 (the
optimal value θ∗
ED goes to inﬁnity, shrinking the inner ring to a
The ring size parameter θCC is learned in sec. 3.
CC is a useful cue because objects tend to have a different
appearance (color distribution) than the background behind
them. In ﬁg. 3a, windows on the grass score lower than
windows half on a sheep and half on the grass. Windows
ﬁtting a sheep tightly score highest. This cue measures the
different appearance characteristic of objects.
CC is related to the center-surround histogram cue of .
However, computes a center-surround histogram centered
at a pixel, whereas CC scores a whole window as whether it
contains an entire object. The latter seems a more appropriate
level of analysis.
Edge Density (ED)
ED measures the density of edges near the window borders.
The inner ring Inn(w, θED) of a window w is obtained by
shrinking it by a factor θED in all directions, so that
|Inn(w, θED)|
The ED of a window w is computed as the density of edgels1
in the inner ring
ED(w, θED) =
p∈Inn(w,θED) IED(p)
Len(Inn(w, θED))
The binary edgemap IED(p) ∈{0, 1} is obtained using the
Canny detector , and Len(·) measures the perimeter of the
inner ring. Note how the expected number of boundary edgels
grows proportionally to the perimeter, not the area, because
edgels have constant thickness of 1 pixel. This normalization
avoids a bias towards very small windows that would otherwise arise. The ring size parameter θED is learned in sec. 3.
The ED cue captures the closed boundary characteristic of
objects, as they tend to have many edgels in the inner ring
1an edgel is a pixel classifﬁed as edge by an edge detector
Fig. 5: The SS cue. Given the segmentation (b) of image (a), for a
window w we compute SS(w, θSS) (eq. 7). In (c), most of the surface
of w1 is covered by superpixels contained almost entirely inside it.
Instead, all superpixels passing by w2 continue largely outside it.
Therefore, w1 has a higher SS score than w2. The window w3 has
an even higher score as it ﬁts the object tightly.
Superpixels Straddling (SS)
A different way of capturing the closed boundary characteristic of objects rests on using superpixels as features.
Superpixels segment an image into small regions of uniform
color or texture. A key property of superpixels is to preserve
object boundaries: all pixels in a superpixel belong to the
same object (ideally). Hence, an object is typically
oversegmented into several superpixels, but none straddles its
boundaries (ﬁg. 5). Based on this property, we propose here a
cue to estimate whether a window covers an object.
A superpixel s straddles a window w if it contains at least
one pixel inside and at least one outside w. Most of the surface
of an object window is covered by superpixels contained
entirely inside it (w1 in ﬁg. 5c). Instead, most of the surface
of a ‘bad’ window is covered by superpixels straddling it (i.e.
superpixels continuing outside the window, w2 in ﬁg. 5c). The
SS cue measures for all superpixels s the degree by which they
straddle w
SS(w, θSS) = 1 −
min(|s \ w|, |s T w|)
where S(θSS) is the set of superpixels obtained using 
with a segmentation scale θSS (learned in sec. 3). For each
superpixel s, eq. (7) computes its area |s T w| inside w and its
area |s \ w| outside w. The minimum of the two is the degree
by which s straddles w and is its contribution to the sum in
Superpixels entirely inside or outside w contribute 0 to the
sum. For a straddling superpixel s, the contribution is lower
when it is contained either mostly inside w, as part of the
object, or mostly outside w, as part of the background (ﬁg. 5c).
Therefore, SS(w, θSS) is highest for windows w ﬁtting tightly
around an object, as desired.
In sec. 5 we show that SS outperforms all other cues we
consider (including MS and its original form ).
Efﬁcient computation of SS.
Naively computing the score
SS(w, θSS) for window w in an image I requires O(|I|)
operations. For each superpixel in the whole image we need
to sum over all the pixels it contains to compute eq. (7).
Fig. 6: SS success and failure. Success: The cyan windows in (a)
and (b) have high SS score computed from segmentations (d) and
(e). Failure: Segmentation produces superpixels (f) not preserving
the boundaries of the small objects in (c), resulting in low SS. All
segmentations are obtained using the optimal segmentation scale θ∗
learned in sec. 3.
Fig. 7: Efﬁciently computing eq. (7). Window w2 from ﬁg. 5c
is straddled by three superpixels. We show here the corresponding
three integral images Is as grayscale maps. The individual superpixel
contributions min(|s\w2|,|s T w2|)
are computed based on them.
Hence, the total cost of computing the score SS(w, θSS) for
all windows w in a set W is O(|W||I|) operations.
We present here an efﬁcient procedure to reduce this complexity. For each superpixel s we build a separate integral
image Is(x, y) giving the number of pixels of s in the rectangle
(0, 0) →(x, y). Then, using Is(x, y), we can compute the
number |s T w| of pixels of s contained in any window w in
constant time, independent of its area (ﬁg. 7). The area |s\w|
outside w is also readily obtained as
|s \ w| = |s| −|s
Therefore, we can efﬁciently compute all elements of
SS(w, θSS) using just 4 operations per superpixel, giving a
a total of 4S operations for a window (with S = |S(θSS)| the
number of superpixels in the image).
Computing all integral images Is takes O(S|I|) operations
and it is done only once (not for each window). Therefore,
our efﬁcient procedure takes only O(S(|W|+|I|)) operations,
compared to O(|W||I|) for the naive algorithm. This is much
faster because S << |I|. In sec. 5 we use this procedure for
computing eq. 7 for 100000 windows in an image.
Location and Size (LS)
Windows covering objects vary in size and location within
an image. However, some windows are more likely to cover
objects than others: an elongated window located at the top of
the image is less probable a priori than a square window in
the image center. Based on this intuition, we propose a new
cue to asses how likely an image window is to cover an object,
based only on its location and size, not on the pixels inside it.
Fig. 8: LS distributions. We visualize here two marginalizations of
the 4D distribution over the training windows, each obtained by
marginalizing over two dimensions. Left: over (x, y) location. Right:
over (width, height). The location of object windows is Gaussian
distributed, with mean at the centre of the image and a rather large
spread. The distribution of object window sizes shows a positive
correlation of width with height, suggesting that square windows have
higher probability.
We compute the probability of an image window to cover
an object using kernel density estimation in the 4D
space W of all possible windows in an image. The space
W is parametrized by the (x, y) coordinates of the center,
the width, and the height of a window. We equip W with a
probability measure pW deﬁning how likely a window is to
cover an object. This probability pW is computed by kernel
density estimation on a large training set of N windows
{w1, w2, . . . , wN} covering objects, which are points in W.
As training images might have different sizes, we normalize
the coordinate frame of all images to 100 × 100 pixels.
We perform kernel density estimation in the space W ⊂
 4 by placing a Gaussian with mean wi and covariance
matrix θLS for each training window wi. The covariance
matrix θLS is learned as explained in sec. 3.3. The probability
of a test window w ∈W is computed as
pW(w, θLS) = 1
(2π)2|θLS|
2 (w−wi)T (θLS)−1(w−wi)
where the normalization constant Z ensures that pW is a probability, i.e. P
w∈W pW(w) = 1. Note that the query window
w can be any point in W, not necessarily corresponding to
a training window. The LS cue assesses the likelihood of a
window w to cover an object as LS(w, θLS) = pW(w, θLS).
Efﬁcient computation of LS.
In practice, we consider a
discretized version of the 4D space W that contains only
windows with integer coordinates in a normalized image of
100 × 100 pixels. We precompute (9) for all these windows,
obtaining a lookup table. For a test window w we ﬁnd its
nearest neighbour wnn in the discretized space W and read
out its value from the lookup table. This procedure is very
efﬁcient, as it takes very few operations for each test window,
compared to N operations in eq. (9).
Implementation details
MS. For every scale s ∈{16, 24, 32, 48, 64} and channel c
we rescale the image to s × s and then compute MS(w, θs
using one integral image (indicating the sum over the
saliency of pixels in a rectangle).
CC. We convert the image to the quantized LAB space 4×8×8
and then compute CC(w, θCC) using one integral image per
quantized color.
ED. We rescale the image to 200 × 200 pixels and then
compute ED(w, θED) using one integral image (indicating the
number of edgels in a rectangle).
SS. We obtain superpixels S(θSS) using the algorithm
of with segmentation scale θSS. We efﬁciently compute
SS(w, θSS) using one integral image per superpixel as detailed
in sec. 2.4.
LEARNING CUE PARAMETERS
We learn the parameters of the objectness cues from a training
dataset T consisting of 1183 images from the PASCAL VOC
07 train+val dataset . In PASCAL VOC 07 each image
is annotated with ground-truth bounding-boxes for all objects
from twenty categories (boat, bicycle, horse, etc). We use for
training all images containing only objects of the 6 classes
bird, car, cat, cow, dog, sheep, for a total of 1587 instances
O (objects marked as difﬁcult are ignored).
The parameters to be learned are θCC, θED, θSS, θLS and
MS (for 5 scales s). The ﬁrst three are learned in a uniﬁed
manner (sec. 3.1), whereas specialized methods are given for
θMS (sec. 3.2) and θLS (sec. 3.3).
Learning the parameters of CC, ED, SS
We learn θCC, θED, θSS in a Bayesian framework. As all three
are learned in the same manner, we restrict the explanation to
θ = θCC. For every image I in T we generate 100000 random
windows uniformly distributed over the entire image. Windows
covering2 an annotated object are considered positive examples
(Wobj), the others negative (Wbg).
For any value of θ we can build the likelihoods for the positive pθ(CC(w, θ)|obj) and negative classes pθ(CC(w, θ)|bg),
as histograms over the positive/negative training windows.
Note how these histograms depend on θ.
We now ﬁnd the optimal θ∗by maximizing the posterior
probability that object windows are classiﬁed as positives
θ∗= arg max
pθ(obj|CC(w, θ)) =
pθ(CC(w, θ)|obj) · p(obj)
c∈{obj,bg} pθ(CC(w, θ)|c) · p(c)
frequency:
p(obj) = |Wobj|/(|Wobj| + |Wbg|), p(bg) = 1 −p(obj).
The advantage of this procedure is that the distribution of
training samples is close to what the method will see when
a new test image is presented, as opposed to just using the
positive ground-truth samples and a few negative samples.
Moreover, it is likely to generalize well, as it is trained from
many variants of the annotated windows in Wobj (i.e. all those
passing the PASCAL criterion, which is also how the method
will be evaluated on test images, sec. 5).
2We follow the widespread PASCAL criterion , and consider a
window w to cover an object o if |w T o|/|w S o| > 0.5.
Fig. 9: Learning parameters for CC, ED, SS. First column: total
sum of posterior log-probabilities over all positive training examples
(eq. (10)) as a function of θ. We set each θ∗to the maximum in each
case. Second column: positive (green) and negative (red) likelihoods,
computed using the optimal θ∗
The learned parameters θ∗
ED deﬁne the optimal outer
ring Surr(w, θCC) and inner ring Inn(w, θED). The parameter
SS deﬁnes the optimal superpixel segmentation scale.
As ﬁg. 9 shows, CC, SS have a well deﬁned optimal value
θ∗. For CC, the optimal size θ∗
CC of the outer ring captures
the amount of context with maximal discriminative power. In
constrast, the posterior (10) for ED continuously increases
with θED, showing that the maximal discriminative power is
found when computing the density of edgels over the entire
Learning the parameters of MS
We learn each threshold θs
MS independently, by optimizing the
localization accuracy of the training object windows O at each
scale s. For every training image I and scale s, we compute the
saliency map Is
MS and the MS score of all windows. We run
non-maximum suppression on this 4D score space using the
efﬁcient technique of , resulting in a set of local maxima
windows Ws
max. We ﬁnd the optimal θs∗
MS by maximizing
MS = arg max
i.e. we seek for the threshold θs∗
MS that leads the local maxima
of MS in the images to most accurately cover the annotated
Fig. 10: Learning parameters for MS. Left: localization accuracy
(eq. (11)) as a function of θs
MS, for each scale s. Right: positive
(green) and negative (red) likelihoods, using the optimal θs∗
objects O. Notice how this procedure is discriminative. Maximizing (11) implicitly entails minimizing the score of windows
not covering any annotated object.
Learning the parameters of LS
The covariance matrix θLS is considered diagonal
θLS = diag(σ1, σ2, σ3, σ4)
We learn the standard deviations σi using a k-nearest neighbours approach. As all four σi are learned in the same manner,
we restrict the explanation to σ1.
For each training window wi ∈O we compute its k-nearest
neighbours in the 4D Euclidian space W, and then derive the
standard deviation of the ﬁrst dimension over these neighbors.
We set σ1 to the median of these standard deviations over all
training windows. Fig. 8 shows marginalizations of the learned
kernel density estimate of the 4D distribution over location and
size dimensions. Clearly, these distributions are not uniform,
which is the root of the discriminative power of the LS cue.
In our experiments we use k = 10.
BAYESIAN CUE INTEGRATION
Since the proposed cues are complementary, using several of
them at the same time appears promising. MS gives only a
rough indication of where an object is as it is designed to ﬁnd
blob-like things (ﬁg. 2). Instead, CC provides more accurate
windows, but sometimes misses objects entirely (ﬁg. 3). ED
provides many false positives on textured areas (ﬁg. 4). SS is
very distinctive but depends on good superpixels, which are
fragile for small objects (ﬁg. 6). LS provides a location-size
prior without analyzing image pixels.
To combine n cues C ⊆{MS, CC, ED, SS, LS} we train
a Bayesian classiﬁer to distinguish between positive and
negative n-uples of values (one per cue). For each training
image, we sample 100000 windows from the distribution given
by the MS cue (thus biasing towards better locations), and then
compute the other cues in C for them. Windows covering an
annotated object are considered as positive examples Wobj,
all others are considered as negative Wbg.
A natural way to combine the cues is to model them
jointly. Unfortunately, integrating many cues C would require
an enormous number of samples to estimate the joint likelihood p(cue1, . . . , cuen|obj), where cuei ∈C. Therefore,
we choose a Naive Bayes approach. We have also tried a
linear discriminant, but it performed worse in our experiments,
probably because it combines cues in a too simplistic manner
(i.e. a weighted sum).
In the Naive Bayes model, the cues are independent, so
training consists of estimating the priors p(obj), p(bg), which
we set by relative frequency, and the individual cue likelihoods
p(cue|c), for cue ∈C and c ∈{obj, bg}, from the large sets
of training windows Wobj, Wbg.
After training, when a test image is given, we can sample
any desired number T of windows from MS and then compute
the other cues for them (as done above for the training
windows). The posterior probability of a test window w is
p(obj|C) = p(C|obj)p(obj)
cue∈C p(cue|obj)
c∈{obj,bg} p(c) Q
cue∈C p(cue|c)
The posterior given by eq. (13) constitutes the ﬁnal objectness score of w. The T test windows and their scores (13)
form a distribution from which we can sample any desired
ﬁnal number F of windows. Note how eq. (13) allows
us to combine any subset C of cues, e.g. pairs of cues
C = {MS, CC}, triplets C = {MS, CC, SS} or all cues
C = {MS, CC, ED, SS, LS}. Function (13) can combine any
subset rapidly without recomputing the likelihoods.
Multinomial sampling. This procedure samples independently windows according to their scores. T window scores
form a multinomial distribution D. Naively sampling F windows from D requires T ·F operations, so we use an efﬁcient
sampling procedure. From the T scores we build the cumulative sum score vector v. Note how the elements of v are sorted
in ascending order and the last vector element v(T) is the sum
of all scores. To sample a window we ﬁrst generate a random
number u uniformly distributed in [0, v(T)]. Then we do a
binary search in v to retrieve the interval [vi−1, vi] containing
u. The chosen sample i has score vi. Hence, sampling F
windows only costs F · log2T operations.
NMS sampling. This procedure samples windows according
to their individual scores and the spatial overlap between
windows. The goal is twofold: sample high scored windows
and cover diverse image locations. This helps detecting more
objects. We start by sampling the single highest scored window. Then we iteratively consider the next highest scored
window and sample it if it does not overlap strongly with
any higher scored window (i.e. intersection-over-union > 0.5).
This is repeated until the desired F samples are obtained.
EXPERIMENTS
We evaluate the performance of the objectness measure on the
popular PASCAL VOC 07 dataset , which is commonly
used to evaluate class-speciﬁc object detectors . We
report results on the test part of the dataset, which consists of
4952 images where all object instances from twenty categories
are annotated by bounding-boxes. The large number of objects
and the variety of classes make this dataset best suited to
our evaluation, as we want to ﬁnd all objects in the image,
irrespective of their classes.
Hou−Direct − 0.059
Itti−Direct − 0.025
Hou−Sampled − 0.404
MS − 0.473
IP − 0.275
OD − 0.344
SL−Direct − 0.134
SL − 0.350
CC − 0.306
ED − 0.275
MS − 0.473
SS − 0.523
LS − 0.330
Fig. 11: DR-#WIN plots. (a) MS vs ; (b) baselines and semantic labeling ; (c) single cues.
MS − 0.473
MS+CC − 0.528
MS+ED − 0.443
SS − 0.523
MS+SS − 0.536
MS+LS − 0.497
MS+SS − 0.536
MS+CC+LS − 0.525
MS+CC+SS − 0.557
Fig. 12: DR-#WIN plots. (a) two cue pair combinations vs MS; (b) another two cue pair combinations vs SS; (c) cue triplet combinations
vs the best pair MS+SS.
We demonstrate that objectness is generic over classes by
testing on images not containing any class used for training
(sec. 3). This results in a test set comprising 2941 images
with 7610 object instances of the classes {aeroplane, bicycle,
boat, bottle, bus, chair, diningtable, horse, motorbike, person,
pottedplant, sofa, train, tvmonitor}. Note how these classes are
of considerably different nature than the training ones. Finally,
this dataset is very challenging: the objects appear against
heavily cluttered backgrounds and vary greatly in location,
scale, appearance, viewpoint and illumination.
DR-#WIN curves. Performance is evaluated with curves measuring the detection-rate vs number of windows (DR-#WIN).
#WIN is the number of windows output by the algorithm
being evaluated. DR is the percentage of ground-truth objects
covered by those windows. An object is considered covered
by a window if the strict PASCAL-overlap criterion is satisﬁed
(intersection-over-union > 0.5 ). For comparing methods,
we summarize a DR-#WIN curve with a single value: the area
under the curve (AUC), after renormalizing the horizontal axis
from to , so the AUC ranges in .
Setup. As there are millions of windows in an image it is
expensive to compute all cues for all windows. Moreover, our
measure is intended as a focus of attention mechanism for
later applications, so it is desirable to output a much smaller
number of windows likely to cover objects.
For ED, MS, SS, LS, for each image we score all the T
windows on a regular 4D grid, then sample 100000 windows
using the multinomial procedure and store them in a table D.
Computing CC is more expensive due to the large number of
integral images involved (one per color bin). Therefore, for
CC we build D by scoring T = 100000 windows sampled
uniformly over the image. We compute each window score as
its posterior probability to cover an object. Finally, to evaluate
each cue we sample 1000 windows out of D using the NMS
procedure.
For a cue combination, we build D by multinomial sampling
T = 100000 windows from the distribution given by MS,
then compute the other cues for these windows, and ﬁnally
rescore them with eq. (13). This is a very efﬁcient and
accurate approximation to sampling directly from (13), as
essentially all objects are covered by 100000 samples from
MS. Finally, to evaluate a cue combination we consider the
ﬁrst 1000 windows sampled from D using the NMS procedure.
Therefore, each individual cue and each cue combination is
evaluated uniformly on 1000 windows per image.
MS vs We compare our MS cue to and 
(ﬁg. 11a). The Hou-Direct curve refers to as originally
proposed3. A single saliency map at scale s = 64 is thresholded at its average intensity4. Each connected component in
the resulting binary map is output as an object. The Itti-Direct
3software from 
4this gives better results than the threshold of , i.e. 3× the average
Fig. 13: Trained HOG template. A visualization of a HOG model
trained on all objects from the same training set T used to train the
objectness measure (sec. 3). Brighter line segments indicate gradient
directions and positions which are given higher weight by the SVM
classiﬁer .
Fig. 14: SL success and failure. Success: The windows covering
the objects in (a) and (b) have high SL score, based on the
semantic segmentations in (d) and (e). The percentage of foreground
pixels (orange) inside the windows (cyan) is much larger than the
percentage in the surrounding ring (yellow), Failure: The semantic
labeling (f) failed to classify the road as background. The resulting
overgrown foreground segment leads to low SL for the window on
the bird. The surrounding ring is drawn for the optimal size θ∗
learned in sec. 3.
curve refers to as originally proposed. The saliency map
is computed and the most salient objects are extracted from it
using Itti’s procedure 5.
Both procedures output only few windows per image (about
5-10). As the curves show, both methods fail on the challenging PASCAL VOC 07 dataset. Hou-Direct performs better
than Itti-Direct, but still reaches only about 16% DR. For
a fair comparison to our method, the Hou-Sampled curve
reports performance when inserting into our framework:
we use their saliency map in our window score (eq. (2)) and
learn the threshold θ64 from our training set as in eq. (11).
Finally, the MS curve uses three color channels and multiple
scales, with separate scale-speciﬁc learned thresholds. DR-
#WIN performance curves improve steadily as we progress
towards our MS cue, showing that all the components we
propose contribute to its success.
Single cues vs baselines. We compare our objectness cues to
two baselines (ﬁg 11b): interest points (IP) and a Histogram
of Oriented Gradients (HOG) detector (OD).
For IP we extract laplacian based multi-scale Harris interest
points and score every window w in a 4D regular grid
5software from 
by the density of IPs it contains
{p∈w} strength(p)
where the summation runs over all IPs inside w, and
strength(p) is the response of the IP detector. For evaluation
we sample 100000 windows from the grid with multinomial
sampling and then retain the ﬁrst 1000 windows sampled with
NMS from them. HOG was shown to perform well for classspeciﬁc object detection. Here we train it to detect objects
of arbitrary classes: for OD we train a single HOG detector
(ﬁg. 13) from all object instances used to train objectness
(sec. 3). For a test image, we score all windows on a dense grid
based on their HOG response, then sample 100000 windows
with multinomial sampling, and ﬁnally apply NMS sampling
to obtain 1000 windows. As ﬁg. 11b shows, OD performs
better than IP, showing it conveys better information about
objects. However, its absolute performance is considerably
lower than MS. We also tried running OD at multiple aspectratios, and we tried using , which extends HOG with parts.
They did not perfom better than plain HOG on this task.
Fig. 11c reports performance for our single cues. All our
cues perform better than the IP baseline, which shows that
ﬁnding entire objects cannot simply be reduced to interest
point detection. Moreover, MS and SS considerably outperform the best baseline OD. This conﬁrms that generic object
detection is different from class-speciﬁc detection. We believe
that no single pattern of gradients (ﬁg. 13) within a window
is characteristic for objects in general, whereas our cues are
designed to capture this.
Our newly proposed SS cue performs best, signiﬁcantly
above the second-best cue MS (and therefore also above
 ). This demonstrates SS is a powerful alternative to
traditional ‘stand out’ saliency cues. Somewhat surprisingly,
LS also performs quite well, about in the range of CC and OD.
This shows that, even in a realistic dataset such as PASCAL
VOC 07, there are locations and sizes at which objects are
more likely to appear. ED is the worst performing cue, as it is
strongly attracted to regions of high edgel density caused by
background clutter.
Cue combinations. Combining cues in our Bayesian framework improve results for most combinations (ﬁg. 12). All cue
pairs combinations, but that including ED, improve over the
performance of the best cue in the pair (ﬁg. 12a and ﬁg. 12b).
Because of ED’s low performance, we exclude it from more
complex combinations. Adding a third cue further raises AUC
for most pairs (ﬁg. 12c): adding SS to MS+CC, or adding CC
to MS+SS, leads to the best performing triplet MS+CC+SS.
The results show these three cues are complementary and
all important for ﬁnding objects in highly challenging images.
In contrast, while LS helps when paired with MS, it makes
no positive contribution to any cue triplet. Starting from two
image cues, objectness is rich enough that it no longer beneﬁts
from the addition of the pure prior LS offers. ED performs
poorly in general and measures a characteristic already captured by the better SS.
Table 1 shows the DR of several cues and combinations
Hou-Direct
MS + CC + SS
MS + CC + SS
MS + CC + SS
MS + CC + SS
MS + CC + SS
Fig. 15: Pascal VOC 07 examples. First 3 columns: example output of Hou-Direct, and the ﬁrst 10 windows sampled from MS and
MS + CC + SS. We mark in yellow windows correctly covering ground-truth object (cyan); if there is more than one correct window, the
best one is shown; all other windows are in red. The windows output by Hou-Direct rarely correctly cover an object. MS ﬁnds some of
the objects but misses others, while MS + SS + CC ﬁnds more objects and its windows also cover the objects more accurately. The last
four columns show the ﬁrst 10 windows sampled from our ﬁnal objectness measure MS + CC + SS. Even with just 10 windows, it can ﬁnd
several objects in these challenging images.
TABLE 1: Detection rate as a function of the number of windows
sampled for various cues and combinations.
MS + CC + SS
when sampling 10, 100 or a 1000 windows. The best combination MS+CC+SS already detects 71% of all objects with just
100 windows per image. Moreover, it reaches 91% detectionrate with 1000 windows per image, which are orders of
magnitude fewer than all windows in the image. This high
recall at such small number of sampled windows is key to the
usefulness of objectness in applications. In comparison, the
class-speciﬁc technique OD only detects 44% and 69%
of the objects with 100 and 1000 windows respectively.
In addition to high recall, MS+CC+SS also localizes objects
accurately. We measure accuracy as the average area of
intersection-over-union of the single window best covering
each detected object, out of the 1000 windows per image
returned by the algorithm. The accuracy for MS+CC+SS
is 70%. In general, sampling more windows leads to more
detected objects but also increases the accuracy of the best
available window (ﬁg. 15, ﬁg. 16).
Computation times. The time needed to compute the objectness measure of an image depends on several factors: the
number of pixels in the image, which cues are combined,
and the number of windows scored on the 4D grid used to
obtain the distribution D. In PASCAL VOC 07 the average
image size is 380 × 470 pixels. In the following we analyze
the computational times for an average image.
As explained in the Setup paragraph, for a cue combination,
the distribution D is built by sampling T = 100000 windows from the distribution given by MS, then computing the
other cues for them and ﬁnally scoring them with eq. (13).
The distribution given by MS is obtained by scoring every
window for 5 scales and 3 channels, for a total of about 11
million windows, which takes about 1.5 seconds. Sampling
T windows takes negligible time (< 0.01 seconds) for the
multinomial sampling and around 0.5 seconds for the NMS
sampling. Scoring the T sampled windows with the other cues
costs: (CC): about 1 second, depending on the quantization of
the LAB space. (ED): about 0.2 seconds, including the time
to compute the edge map using the Canny detector . (SS):
around 1.5 seconds, including the time to compute superpixels
using . (LS): around 0.1 seconds given the precomputed
lookup table from sec. 2.5.
The cue combination MS+CC+SS offers the highest AUC
and takes only 4 seconds to compute (on average per image).
Because of this we consider MS+CC+SS as the objectness
measure and use it to speed up class-speciﬁc object detectors
by greatly reducing the number of windows they evaluate
(sec. 6), or to reduce their false-positive rate (sec. 7).
Generalization over classes. We perform here an additional
experiment to further demonstrate the class-generic nature of
objectness, by showing that its test performance does not
depend on the classes observed during training. For this we
retrain the objectness measure from a different training set,
composed of 50 images randomly sampled from a variety of
existing datasets, including INRIA Person, Pascal VOC 06 and
Caltech 101, showing a total of 291 object instances. These
are the images used in the ﬁrst version of our work and
correspond to our software release.
We evaluate the retrained objectness measure on the same
test set as in the previous paragraphs. Remarkably, the DR-
#WIN performance curve remains essentially identical, changing by less than 1% detection-rate at any number of windows
compared to the MS+CC+SS curve in ﬁg. 12c.
Semantic Labeling . We compare our objectness measure
to the Semantic Labeling technique of (SL). It was
designed for segmenting an image into several background
classes and one generic foreground class. In our experiments
we use the source code6 provided for SL within . The
strength of this technique lies in employing classiﬁers specialized to individual background classes (e.g. grass, road),
which is interesting because there are only a few common
background classes and each has limited appearance variabil-
6 
Fig. 16: Qualitative results for increasing number of windows. We show the accuracy of the objectness measure as a function of the
number of windows it returns. Each row shows the results obtained when sampling 1, 10, 100, and 1000 windows. We mark in yellow
windows correctly covering a ground-truth object (cyan); if there is more than one correct window, the most overlapping one is shown.
As we sample more windows, more objects are detected and the accuracy of the windows covering them continuously increases. Sampling
1000 windows covers nearly all objects with high accuracy. Notice that 1000 is several orders of magnitue smaller than the number of all
windows in the image.
ity. It can be seen as a complementary strategy to ours, which
focuses on properties of objects.
The semantic pixel labeler of divides the image into
background and foreground regions. In a ﬁrst experiment we
directly ﬁt a window on each foreground region and return
them all as objects. Analog to Hou-Direct and Itti-Direct,
this procedure makes hard decisions and returns a moderate
number of disjoint windows. While it performs better than both
Hou-Direct and Itti-Direct, overall it detects less than 20% of
all objects.
In a second experiment, we insert into our framework,
in a manner analoguous to the CC cue. We compute the
score SL(w, Surr(w, θSL)) of a window w as the difference
between the percentage of foreground pixels inside w and the
percentage of foreground pixels in its immediate surrounding
Surr(w, θSL), truncated to positive values
SL(w, θSL) = max(0, pFG(w) −pFG(Surr(w, θSL)))
pFG(w) = |{p ∈w | label(p) = fg}|
and Surr(w, θSL) is deﬁned as in sec. 2.2. This score is highest
when w ﬁts a blob of foreground pixels tightly (ﬁg. 14e). The
truncation is useful to indicate that windows with foreground
density larger outside than inside have 0 probability of containing an object. The ring size θSL is learned as θCC in sec. 3.
As ﬁg. 11b shows, SL performs quite well and it reaches
a higher AUC than the baselines and the CC, ED cues.
This conﬁrms that the technique of is well suited as an
objectness cue, and that sampling a larger set of windows per
image is important for reaching high recall. The DR-#WIN
curve of SL follows a similar trend to OD. Our MS and SS
cues perform better, and our best combination MS+CC+SS
reaches a substantially higher AUC. On the other hand, SL
localizes objects up to their outlines, whereas objectness only
returns rectangular windows. Finally, we note that computing
SL costs about 35 seconds, compared to only 4 seconds for
objectness.
Object proposals based on segmentation . We compare our objectness measure to the two recent works of 
that produce segmentations as object candidates. Both methods
generate a ranked list of segments with the desired goal of
covering all objects in an image. In our experiments we use
the ofﬁcial source code provided by the respective authors 7
8. In order to compare to our method, we ﬁt a bounding-box
on each segment returned by the algorithms.
In ﬁg. 17 we plot the performance of against our
objectness measure (MS+CC+SS). As the plot shows, and
objectness exhibit the same perfomance when considering a
rather low number of windows (up to 100). For a larger
number of windows objectness has a moderate advantage,
detecting 6% more objects with a 1000 windows. The method
of achieves moderately higher recall than objectness until
up to 500 windows, while in the range 500-1000 windows
objectness ﬁnds more objects. Note how both produce
a more detailed output than objectness, offering pixelwise
7 
8 
 − 0.544
 − 0.585
MS+CC+SS − 0.557
Fig. 17: Comparison to . MS+CC+SS vs .
Algorithm 1 Using objectness for class-speciﬁc detectors.
Input: F, D, c
Output: Det
1: I = {w1, . . . , wF }, wi ⇝D, ∀i
2: Is = {(w1, sw1), . . . , (wF , swF )}, swi = c(wi), ∀i
3: Ps = NMS(Is) = {(wn1, swn1 ), . . . , (wnP , swnP )}
4: L = {wlm
n1 , . . . , wlm
nj = maxw∈Vwnj sw
5: Det = NMS(L)
segmentations instead of rectangular windows. In terms of
computation, both are very expensive (about 420
seconds per image for , 120 for ). Depending on the
application, this might be prohibitively slow (e.g. sec. 6).
SPEEDING UP CLASS-SPECIFIC DETECTORS
Many modern class-speciﬁc object detectors are
based on sliding windows. In sec. 6.1 we give a general
algorithm for using our objectness measure as a location prior
for any sliding window detector. In sec. 6.2-6.4 we detail how
this algorithm works for three particular detectors .
We show in sec. 6.5 that objectness greatly reduces the number
of windows evaluated by the detectors.
General algorithm
The general scheme for using our objectness measure as
a location prior for object detectors is algorithm 1. The
algorithm inputs the class-speciﬁc conﬁdence function c which
the detector employs to score a window.
We build an initial set I of F = 1000 windows multinomially sampled from the distribution D of windows scored by
our objectness measure MS + CC + SS (line 1). We use c to
score each window in I (line 2). We then run the non-maxima
suppression of . This results in a set Ps of promising
windows (line 3). For every window wp ∈Ps, we iteratively
move to the local maximum of c in its neighborhood Vwp,
resulting in window wlm
(line 4). Finally, we run NMS on the
local maxima windows L and obtain detections Det (line 5).
In order to use this algorithm one has to specify a window
scoring function c, which is speciﬁc to a particular detector
and object class, and a window neighborhood V.
TABLE 2: For each detector we report its performance
(left column) and that of our algorithm 1 using the same window
scoring function (right column). We show the average number of
windows evaluated per image #win and the detection performance
as the mean average precision (mAP) over all 20 classes.
 OBJ- 
 OBJ- 
ESS-BOW OBJ-BOW
Speeding up 
The detector of models a class by a single window
ﬁlter c on HOG features. In this ﬁlter is applied at all
positions (x, y) and scales s on a grid over image. In our
algorithm instead, we map a window wi ∈I to the best
corresponding location (x, y, s) using the ﬁlter’s aspect-ratio,
and then score it with c. Next, we search for a local maximum
in the neighborhood Vwi on the grid, for up to 5 iterations.
We use the neighborhood Vwi = {x −1, x, x + 1} × {y −
1, y, y + 1} × {s −1, s, s + 1} comprising 27 windows.
Speeding up 
The detector of models a class with a mixtures of
multiscale deformable part models. More complex than ,
the model includes a root ﬁlter and a collection of part ﬁlters
and associated deformation models. The score of a window
at location (x, y, s) combines the score of the root ﬁlter, the
parts and a deformation cost measuring the deviation of the
part from its ideal location.
In our algorithm, we map a window wi ∈I to a location
(x, y, s) using the root’s aspect ratio, and then score it with
c. We use the same neighborhood and number of local search
iterations as in sec. 6.2.
Speeding up Bag-of-words detectors
The window scoring function c can also be a Bag-of-visualwords classiﬁer , which represents a window with a
histogram of local features quantized over a precomputed
codebook. Here we follow the setup of and use a linear
SVM as c .
No mapping to a location (x, y, s) is necessary, as the BOW
descriptor is deﬁned on windows of any aspect-ratio. We score
only the sampled windows wi ∈I and set the neighborhood
Vwi to all windows obtained by translating wi by 2 pixels
and/or scaling wi by factor 1.1. We iterate local search until
convergence (usually about 10 iterations).
Since for this type of window scoring function it is possible to apply the branch-and-bound technique ESS , we
compare to ESS rather than to traditional sliding-windows on
a grid .
Quantitative evaluation
We compare the 3 object detectors to our
algorithm 1 on the entire PASCAL VOC 07 test set (20 object
classes over 4952 images) using the standard PASCAL VOC
protocol .
Our algorithm uses the same window scoring function c
used by the corresponding detector. We train it for using
Fig. 18: Class-speciﬁc detections. We show the output of three object
detectors in yellow (ﬁrst row - , second row - , third row -
ESS). We show in red the output of our algorithm using the same
window scoring function as the corresponding detector. Ground-truth
objects are in cyan.
their source code9. For we train with only the root
ﬁlter (no parts). For we obtained precomputed image
features and SVM hyperplanes from the authors. For detection,
we use the source code of for both and , and the
source code of ESS10 for .
As tab. 2 shows, our algorithm evaluates 15×-60× fewer
windows than the sliding-windows approaches . Interestingly, it also evaluates 60× fewer windows than ESS 
(notice that the implicit search space of is larger than
that of as it contains all windows, not just a grid).
Moreover, our algorithm’s mAP is only slightly below the
original detectors (−0.023 on average), showing this massive
speedup comes at little compromise on performance. The additional cost to compute objectness and sample 1000 windows
I is negligible as it is done only once per image. The same
windows I are reused for all 20 classes, and can in fact be
reused for any class. For example, takes about 4 seconds
for each class, while objectness takes about 4 seconds only
once. Finally, our algorithm is general, as it supports any
window scoring function. ESS instead is restricted to functions
for which the user can provide a (tight) bound on the best
score in a contiguous set of windows (for example, for the
scoring functions of no bound is known and ESS is
not applicable).
FALSE-POSITIVE
CLASS-SPECIFIC DETECTORS
The objectness measure can also be used to reduce the number
of false-positives returned by class-speciﬁc object detectors.
In the following we explain how to use objectness as a
complementary score in addition to the class-speciﬁc scoring
function of any detector. We demonstrate experimentally that
this combination leads to fewer false positives for the detector .
9 
10 
Adding objectness to a class-speciﬁc detector. The classspeciﬁc scoring function of a detector typically returns a high
response to instances of the class, but occasionally also to other
image patterns, leading to false-positive detections. Some of
these false-positives cover background patterns or are partially
on an object and on background. On these windows we expect
objectness to give a low score.
Following this intuition we linearly combine the classspeciﬁc score c(w) of a window w with its objectness score
p(obj|w) (eq. 13): c(w) + α · p(obj|w). This combination can
be used instead of the class-speciﬁc score inside the innermost
loop of any sliding window detector. All other elements of the
standard detector pipeline remain the same (e.g. NMS). The
weight α controls the importance of the objectness score.
Quantitative evaluation for . We present results for
adding objectness to the pipeline of . We set α = 0.2.
In table 3 we show the results obtained on the test set of
PASCAL VOC 07 (all 20 classes) 11. As table 3 shows, adding
objectness improves the mAP for 11 classes. Although the
improvement is not large, it is signiﬁcant given the size of
this dataset. Moreover, note how is a high performance,
state-of-the-art algorithm, which is difﬁcult to further improve
upon. Therefore, we consider the increase in mAP brought by
adding objectness quite exciting (about 1% on average over
all classes).
CONCLUSIONS
We presented an objectness measure trained to distinguish
object windows from background ones. It combines several
image cues, including the innovative SS cue. On the task
of detecting objects of new classes unseen during training, we have demonstrated that objectness outperforms traditional saliency , interest point detectors, Semantic Labeling , and the HOG detector . Moreover,
we have demonstrated algorithms to employ objectness to
greatly reduce the number of windows evaluated by classspeciﬁc detectors, and to reduce their false-positive rates.
Several recent works are increasingly demonstrating the value
of objectness in other applications, such as learning object
classes in weakly supervised scenarios , pixelwise segmentation of objects , unsupervised object
discovery , and learning humans-object interactions .
The source code of the objectness measure is available
at 
Acknowledgements. We thank A. Zisserman for suggesting
the basic intuition behind the LS cue and D. Koller and S.
Gould for helping us using their Semantic Labeling software.