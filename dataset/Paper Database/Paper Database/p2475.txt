Learning to Discover Cross-Domain Relations
with Generative Adversarial Networks
Taeksoo Kim 1 Moonsu Cha 1 Hyunsoo Kim 1 Jung Kwon Lee 1 Jiwon Kim 1
While humans easily recognize relations between data from different domains without any
supervision, learning to automatically discover
them is in general very challenging and needs
many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address
the task of discovering cross-domain relations
given unpaired data. We propose a method based
on generative adversarial networks that learns
to discover relations between different domains
(DiscoGAN). Using the discovered relations, our
proposed network successfully transfers style
from one domain to another while preserving key
attributes such as orientation and face identity.
1. Introduction
Relations between two different domains, the way in which
concepts, objects, or people are connected, arise ubiquitously. Cross-domain relations are often natural to humans.
For example, we recognize the relationship between an English sentence and its translated sentence in French. We
also choose a suit jacket with pants or shoes in the same
style to wear.
Can machines also achieve a similar ability to relate two
different image domains? This question can be reformulated as a conditional image generation problem. In other
words, ﬁnding a mapping function from one domain to the
other can be thought as generating an image in one domain given another image in the other domain. While this
problem tackled by generative adversarial networks (GAN)
 has gained a huge attention recently,
most of today’s training approaches use explicitly paired
data, provided by human or another algorithm.
This problem also brings an interesting challenge from a
learning point of view. Explicitly supervised data is seldom available and labeling can be labor intensive. More-
1SK T-Brain, Seoul, South Korea. Correspondence to: Taeksoo
Kim < >.
(a) Learning cross-domain relations without any extra label
(b) Handbag images (input) & Generated shoe images (output)
(c) Shoe images (input) & Generated handbag images (output)
Figure 1. Our GAN-based model trains with two independently
collected sets of images and learns how to map two domains without any extra label. In this paper, we reduces this problem into
generating a new image of one domain given an image from the
other domain. (a) shows a high-level overview of the training procedure of our model with two independent sets (e.g. handbag images and shoe images). (b) and (c) show results of our method.
Our method takes a handbag (or shoe) image as an input, and
generates its corresponding shoe (or handbag) image. Again, it’s
worth noting that our method does not take any extra annotated
supervision and can self-discover relations between domains.
over, pairing images can become tricky if corresponding
images are missing in one domain or there are multiple best
candidates. Hence, we push one step further by discovering
relations between two visual domains without any explicitly paired data.
 
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Figure 2. Three investigated models. (a) standard GAN , (b) GAN with a reconstruction loss, (c) our proposed
model (DiscoGAN) designed to discover relations between two unpaired, unlabeled datasets. Details are described in Section 3.
In order to tackle this challenge, we introduce a model that
discovers cross-domain relations with GANs (DiscoGAN).
Unlike previous methods, our model can be trained with
two sets of images without any explicit pair labels (see Figure 1a) and does not require any pre-training. Our proposed
model can then take one image in one domain as an input
and generate its corresponding image in another domain
(see Figure 1b). The core of our model is based on two
different GANs coupled together – each of them ensures
our generative functions can map each domain to its counterpart domain. A key intuition we rely on is to constraint
all images in one domain to be representable by images in
the other domain. For example, when learning to generate
a shoe image based on each handbag image, we force this
generated image to be an image-based representation of the
handbag image (and hence reconstruct the handbag image)
through a reconstruction loss, and to be as close to images
in the shoe domain as possible through a GAN loss. We
use these two properties to encourage the mapping between
two domains to be well covered on both directions (i.e. encouraging one-to-one rather than many-to-one or one-tomany). In the experimental section, we show that this simple intuition discovered common properties and styles of
two domains very well.
Both experiments on toy domain and real world image
datasets support the claim that our proposed model is wellsuited for discovering cross-domain relations. When translating data points between simple 2-dimensional domains
and between face image domains, our DiscoGAN model
was more robust to the mode collapse problem compared
to two other baseline models. It also learns the bidirectional
mapping between two image domains, such as faces, cars,
chairs, edges and photos, and successfully apply them in
image translation. Translated images consistently change
speciﬁed attributes such as hair color, gender and orientation while maintaining all other components. Results also
show that our model is robust to repeated application of
translation mappings.
We now formally deﬁne cross-domain relations and present
the problem of learning to discover such relations in two
different domains. Standard GAN model and a similar variant model with additional components are investigated for
their applicability for this task. Limitations of these models are then explained, and we propose a new architecture
based on GANs that can be used to discover cross-domain
relations.
2.1. Formulation
Relation is mathematically deﬁned as a function GAB
that maps elements from its domain A to elements in its
codomain B and GBA is similarly deﬁned. In fully unsupervised setting, GAB and GBA can be arbitrarily deﬁned.
To ﬁnd a meaningful relation, we need to impose a condition on the relation of interest. Here, we constrain relation to be a one-to-one correspondence (bijective mapping).
That means GAB is the inverse mapping of GBA.
The range of function GAB, the complete set of all possible
resulting values GAB(xA) for all xA’s in domain A, should
be contained in domain B and similarly for GBA(xB).
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Figure 3. Illustration of our models on simpliﬁed one dimensional domains. (a) ideal mapping from domain A to domain B in which the
two domain A modes map to two different domain B modes, (b) GAN model failure case, (c) GAN with reconstruction model failure
We now relate these constraints to objective functions. Ideally, the equality GBA ◦GAB(xA) = xA should be satisﬁed, but this hard constraint is difﬁcult to optimize and
relaxed soft constraint is more desirable in the view of
optimization. For this reason, we minimize the distance
d(GBA ◦GAB(xA), xA), where any form of metric function (L1, L2, Huber loss) can be used. Similarly, we also
need to minimize d(GAB ◦GBA(xB), xB).
Guaranteeing that GAB maps to domain B is also very
difﬁcult to optimize. We relax this constraint as follows: we instead minimize generative adversarial loss
−ExA∼PA [log DB(GAB(xA))]. Similarly, we minimize
−ExB ∼PB [log DA(GBA(xB))].
Now, we explore several GAN architectures to learn with
these loss functions.
2.2. Notation and Architecture
We use the following notations in sections below. A generator network is denoted GAB : R64×64×3
the subscripts denote the input and output domains and superscripts denote the input and output image size. The discriminator network is denoted as DB : R64×64×3
and the subscript B denotes that it discriminates images in
domain B. Notations GBA and DA are used similarly.
Each generator takes image of size 64×64×3 and feeds it
through an encoder-decoder pair. The encoder part of each
generator is composed of convolution layers with 4 × 4 ﬁlters, each followed by leaky ReLU . The decoder part is composed of deconvolution layers with 4 × 4 ﬁlters, followed by a ReLU, and outputs a target domain image of size 64×64×3. The number
of convolution and deconvolution layers ranges from four
to ﬁve, depending on the domain.
The discriminator is similar to the encoder part of the generator. In addition to the convolution layers and leaky Re-
LUs, the discriminator has an additional convolution layer
with 4 × 4 ﬁlters, and a ﬁnal sigmoid to output a scalar
output between .
2.3. GAN with a Reconstruction Loss
We ﬁrst consider a standard GAN model for the relation discovery task (Figure 2a).
Originally, a standard GAN takes random Gaussian noise
z, encodes it into hidden features h and generates images
such as MNIST digits. We make a slight modiﬁcation to
this model to ﬁt our task: the model we use takes in image
as input instead of noise.
In addition, since this architecture only learns one mapping
from domain A to domain B, we add a second generator
that maps domain B back into domain A (Figure 2b). We
also add a reconstruction loss term that compares the input
image with the reconstructed image. With these additional
changes, each generator in the model can learn mapping
from its input domain to output domain and discover relations between them.
A generator GAB translates input image xA from domain A
into xAB in domain B. The generated image is then translated into a domain A image xABA to match the original input image (Equation 1, 2). Various forms of distance functions, such as MSE, cosine distance, and hinge-loss, can be
used as the reconstruction loss d (Equation 3). The translated output xAB is then scored by the discriminator which
compares it to a real domain B sample xB.
xAB = GAB(xA)
xABA = GBA(xAB) = GBA ◦GAB(xA)
LCONSTA = d(GBA ◦GAB(xA), xA)
LGANB = −ExA∼PA [log DB(GAB(xA))]
The generator GAB receives two types of losses – a reconstruction loss LCONSTA (Equation 3) that measures how
well the original input is reconstructed after a sequence of
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
two generations, and a standard GAN generator loss LGANB
(Equation 4) that measures how realistic the generated image is in domain B. The discriminator receives the standard
GAN discriminator loss of Equation 6.
LGAB = LGANB + LCONSTA
−ExB ∼PB [log DB(xB)]
−ExA∼PA [log(1 −DB(GAB(xA)))]
During training, the generator GAB learns the mapping
from domain A to domain B under two relaxed constraints:
that domain A maps to domain B, and that the mapping
on domain B is reconstructed to domain A. However, this
model lacks a constraint on mapping from B to A, and these
two conditions alone does not guarantee a cross-domain relation (as deﬁned in section 2.1) because the mapping satisfying these constraints is one-directional. In other words,
the mapping is an injection, not bijection, and one-to-one
correspondence is not guaranteed.
Consider the two possibly multi-modal image domains A
and B. Figure 3 illustrates the two multi-modal data domains on a simpliﬁed one-dimensional representation. Figure 3a shows the ideal mapping from input domain A to domain B, where each mode of data is mapped to a separate
mode in the target domain. Figure 3b, in contrast, shows the
mode collapse problem, a prevalent phenomenon in GANs,
where data from multiple modes of a domain map to a single mode of a different domain. For instance, this case is
where the mapping GAB maps images of cars in two different orientations into the same mode of face images.
In some sense, the addition of a reconstruction loss to a
standard GAN is an attempt to remedy the mode collapse
problem. In Figure 3c, two domain A modes are matched
with the same domain B mode, but the domain B mode can
only direct to one of the two domain A modes. Although
the additional reconstruction loss LCONSTA forces the reconstructed sample to match the original (Figure 3c), this
change only leads to a similar symmetric problem. The reconstruction loss leads to an oscillation between the two
states and does not resolve mode-collapsing.
2.4. Our Proposed Model: Discovery GAN
Our proposed GAN model for relation discovery – Disco-
GAN – couples the previously proposed model (Figure 2c).
Each of the two coupled models learns the mapping from
one domain to another, and also the reverse mapping to for
reconstruction. The two models are trained together simultaneously. The two generators GAB’s and the two generators GBA’s share parameters, and the generated images
xBA and xAB are each fed into separate discriminators LDA
and LDB, respectively.
One key difference from the previous model is that input
images from both domains are reconstructed and that there
are two reconstruction losses: LCONSTA and LCONSTB.
LG = LGAB + LGBA
= LGANB + LCONSTA + LGANA + LCONSTB
LD = LDA + LDB
As a result of coupling two models, the total generator loss
is the sum of GAN loss and reconstruction loss for each
partial model (Equation 7). Similarly, the total discriminator loss LD is a sum of discriminator loss for the two discriminators DA and DB, which discriminate real and fake
images of domain A and domain B (Equation 8).
Now, this model is constrained by two LGAN losses and
two LCONST losses. Therefore a bijective mapping is
achieved, and a one-to-one correspondence, which we de-
ﬁned as cross-domain relation, can be discovered.
3. Experiments
3.1. Toy Experiment
To empirically demonstrate our explanations on the differences between a standard GAN, a GAN with reconstruction loss and our proposed model (DiscoGAN), we designed an illustrative experiment based on synthetic data
in 2-dimensional A and B domains. Both source and target
data samples are drawn from Gaussian mixture models.
In Figure 4, the left-most ﬁgure shows the initial state of toy
experiment where all the A domain modes map to almost
a single point because of initialization of the generator. For
all other plots the target domain 2D plane is shown with target domain modes marked with black ‘x’s. Colored points
on B domain planes represent samples from A domain that
are mapped to the B domain, and each color denotes samples from each A domain mode. In this case, the task is to
discover cross-domain relations between the A and B domain and translate samples from ﬁve A domain modes into
the B domain, which has ten modes spread around the arc
of a circle.
We use a neural network with three linear layers that are
each followed by a ReLU nonlinearity as the generator. For
the discriminator we use ﬁve linear layers that are each
followed by a ReLU, except for the last layer which is
switched out with a sigmoid that outputs a scalar ∈ .
The colored background shows the output value of the
discriminator DB, which discriminates real target domain
samples from synthetic, translated samples from domain
A. The contour lines show regions of same discriminator
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Figure 4. Toy domain experiment results. The colored background shows the output value of the discriminator. ’x’ marks denote different
modes in B domain, and colored circles indicate mapped samples of domain A to domain B, where each color corresponds to a different
mode. (a) ten target domain modes and initial translations, (b) standard GAN model, (c) GAN with reconstruction loss, (d) our proposed
model DiscoGAN
The training was performed for 50,000 iterations, and due
to the domain simplicity our model often converged much
earlier. The results from this experiment match our claim
and illustrations in Figure 4 and the resulting translated
samples show very different behavior depending on the
model used.
In the baseline (standard GAN) case, many translated
points of different colors are located around the same B
domain mode. For example, navy and light-blue colored
points are located together, as well as green and orange colored points. This result illustrates the mode-collapse problem of GANs since points of multiple colors (multiple A
domain modes) are mapped to the same B domain mode.
The baseline model still oscillate around B modes throughout the iterations.
In the case of GAN with a reconstruction loss, the collapsing problem is less prevalent, but navy, green and light-blue
points still overlap at a few modes. The contour plot also
demonstrates the difference from baseline: regions around
all B modes are leveled in a green colored plateau in the
baseline, allowing translated samples to freely move between modes, whereas in the single model case the regions
between B modes are clearly separated.
In addition, both this model and the standard GAN model
fail to cover all modes in B domain since the mapping from
A domain to B domain is injective. Our proposed Disco-
GAN model, on the other hand, is able to not only prevent
mode-collapse by translating into distinct well-bounded regions that do not overlap, but also generate B samples in all
ten modes as the mappings in our model is bijective. It is
noticeable that the discriminator for B domain is perfectly
fooled by translated samples from A domain around B domain modes.
Although this experiment is limited due to its simplicity,
the results clearly support the superiority of our proposed
model over other variants of GANs.
3.2. Real Domain Experiment
To evaluate whether our DiscoGAN successfully learns
underlying relationship between domains, we trained and
tested our model using several image-to-image translation
tasks that require the use of discovered cross-domain relations between source and target domains.
In each real domain experiment, all input images and translated images were of size 64 × 64 × 3. For training, we
used learning rate of 0.0002 and used the Adam optimizer
 with β1 = 0.5 and β2 = 0.999.
We applied Batch Normalization 
to all convolution and deconvolution layers except the ﬁrst
and the last layers, weight decay regularization coefﬁcient
of 10−4 and minibatch of size 200. All computations were
conducted on a single machine with an Nvidia Titan X Pascal GPU and an Intel(R) Xeon(R) E5-1620 CPU.
3.2.1. CAR TO CAR, FACE TO FACE
We used a Car dataset which consists
of rendered images of 3D car models with varying azimuth
angles at 15◦intervals. We split the dataset into train and
test sets and again split the train set into two groups, each
of which is used as A domain and B domain samples. In
addition to training a standard GAN model, a GAN with
a reconstruction model and a proposed DiscoGAN model,
we also trained a regressor that predicts the azimuth angle
of a car image using the train set. To evaluate, we translated
images in the test set using each of the three trained models, and azimuth angles were predicted using the regressor
for both input and translated images. Figure 5 shows the
predicted azimuth angles of input and translated images for
each model. In standard GAN and GAN with reconstruction (5a and 5b), most of the red dots are grouped in a few
clusters, indicating that most of the input images are translated into images with same azimuth, and that these models suffer from mode collapsing problem as predicted and
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Figure 5. Car to Car translation experiment. Horizontal and vertical axes in the plots indicate predicted azimuth angles of input and
translated images, where the angle of input image ranges from -75◦to 75◦. RMSE with respect to ground truth (blue lines) are shown
in each plot. Images in the second row are examples of input car images ranging from -75◦to 75◦at 15◦intervals. Images in the third
row are corresponding translated images. (a) plot of standard GAN (b) GAN with reconstruction (c) DiscoGAN. The angles of input and
output images are highly correlated when our proposed DiscoGAN model is used. Note the angles of input and translated car images are
reversed with respect to 0◦(i.e. mirror images).
shown in Figures 3 and 4. Our proposed DiscoGAN (5c),
on the other hand, shows strong correlation between predicted angles of input and translated images, indicating that
our model successfully discovers azimuth relation between
the two domains. In this experiment, the translated images
either have the same azimuth range (5b), or the opposite
(5a and 5c) of the input images.
Next, we use a Face dataset shown in
Figure 6a, in which the data images vary in azimuth rotation from -90◦to +90◦. Similar to previous car to car experiment, input images in the -90◦to +90◦rotation range generated output images either in the same range, from -90◦to
+90◦, or the opposite range, from +90◦to -90◦when our
proposed model was used (6d). We also trained a standard
GAN and a GAN with reconstruction loss for comparison.
When a standard GAN and GAN with reconstruction loss
were used, the generated images do not vary as much as the
input images in terms of rotation. In this sense, similar to
what has been shown in previous Car to Car experiment,
the two models suffered from mode collapse.
3.2.2. FACE CONVERSION
In terms of the amount of related information between two
domains, we can consider a few extreme cases: two domains sharing almost all features and two domains sharing
only one feature. To investigate former case, we applied the
face attribute conversion task on CelebA dataset , where only one feature, such as gender or hair color,
varies between two domains and all the other facial features
are shared. The results are listed in Figure 7.
In Figure 7a, we can see that various facial features are
well-preserved while a single desired attribute (gender) is
changed. Also, 7b and 7d shows that background is also
well-preserved and images are visually natural, although
the background does change in a few cases such as Figure
7c. An extension to this experiment was sequentially applying several translations – for example, changing the gender
and then the hair color (7e), or repeatedly applying gender
transforms (7f).
Figure 6. Face to Face translation experiment. (a) input face images from -90◦to +90◦(b) results from a standard GAN (c) results
from GAN with a reconstruction loss (d) results from our Disco-
GAN. Here our model generated images in the opposite range,
from +90◦to -90◦.
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
Figure 7. (a,b) Translation of gender in Facescrub dataset and CelebA dataset. (c) Blond to black and black to blond hair color conversion
in CelebA dataset. (d) Wearing eyeglasses conversion in CelebA dataset (e) Results of applying a sequence of conversion of gender and
hair color (left to right) (f) Results of repeatedly applying the same conversions (upper: hair color, lower: gender)
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
(a) Chair to Car
(b) Car to Face
Figure 8. Discovering relations of images from visually very different object classes. (a) chair to car translation. DiscoGAN is trained
on chair and car images (b) car to face translation. DiscoGAN is trained on car and face images. Our model successfully pairs images
with similar orientation.
Figure 9. Edges to photos experiment. Our model is trained on a
set of object sketches and colored images and learns to generate
new sketches or photos. (a) colored images of handbags are generated from sketches of handbags, (b) colored images of shoes are
generated from sketches of shoes, (c) sketches of handbags are
generated from colored images of handbags
3.2.3. CHAIR TO CAR, CAR TO FACE
We also investigated the opposite case where there is a single shared feature between two domains. 3D rendered images of chair and the previously used
car and face datasets were used in this task. All three datasets vary along
the azimuth rotation. Figure 8 shows the results of imageto-image translation from chair to car and from car to face
datasets. The translated images clearly match the rotation
feature of the input images while preserving visual features
of car and face domain, respectively.
3.2.4. EDGES-TO-PHOTOS
Edges-to-photos is an interesting task as it is a 1-to-N problem, where a single edge image of items such as shoes and
handbags can generate multiple colorized images of such
items. In fact, an edge image can be colored in inﬁnitely
many ways. We validated that our DiscoGAN performs
very well on this type of image-to-image translation task
and generate realistic photos of handbags 
and shoes . The generated images
are presented in Figure 9.
3.2.5. HANDBAG TO SHOES, SHOES TO HANDBAG
Finally, we investigated the case with two domains that are
visually very different, where shared features are not explicit even to humans. We trained a DiscoGAN using previously used handbags and shoes datasets, not assuming
any speciﬁc relation between those two. In the translation
results shown in Figure 1, our proposed model discovers
fashion style as a related feature between the two domains.
Note that translated results not only have similar colors and
patterns, but they also have similar level of fashion formality as the input fashion item.
4. Related Work
Recently, a novel method to train generative models named
Generative Adversarial Networks (GANs) was developed. A GAN is composed of two
modules – a generator G and a discriminator D. The generator’s objective is to generate (synthesize) data samples
whose distribution closely matches that of real data sam-
Learning to Discover Cross-Domain Relations with Generative Adversarial Networks
ples while the discriminator’s objective is to distinguish
real ones from generated samples. The two models G and
D, formulated as a two-player minimax game, are trained
simultaneously.
Researchers have studied GANs vigorously in two years:
network models such as LAPGAN 
and DCGAN and improved training
techniques .
More recent GAN works are described in (Goodfellow,
Several methods were developed to generate images
based on GANs. Conditional Generative Adversarial Nets
(cGANs) use MNIST digit class
label as an additional information to both generator and
discriminator and can generate digit images of the speci-
ﬁed class. Similarly, Dosovitskiy et al. showed that
GAN can generate images of objects based on speciﬁed
characteristic codes such as color and viewpoint. Other approaches used conditional features from a completely different domain for image generation. For example, Reed
et al. used encoded text description of images as the
conditional information to generating images that match
the description.
Some researchers have attempted to use multiple GANs in
prior works. proposed to couple two
GANs (coupled generative adversarial networks, CoGAN)
in which two generators and two discriminators are coupled by weight-sharing to learn the joint distribution of images in two different domains without using pair-wise data.
In Stacked GANs (StackGAN) , two
GANs are arranged sequentially where the Stage-I GAN
generates low resolution images given text description and
the Stage-II GAN improves the generated image into high
resolution images. Similarly, Style and Structure GAN (S2-
GAN) used two sequentially connected GANs where the Structure GAN ﬁrst generates surface normal image and the Style GAN transforms it into
natural indoor scene image.
In order to control speciﬁc attributes of an image, T. Kulkarni & P. Kohli proposed a method to disentangle
speciﬁc factors by explicitly controlling target code. Perarnau et al. tackled image generation problems conditioned on speciﬁc attribute vectors by training an attribute
predictor along with latent encoder.
In addition to using conditional information such as class
labels and text encodings, several works in the ﬁeld of
image-to-image translation used images of one domain to
generate images in another domain. 
translated black-and-white images to colored images by
training on paired black-and-white and colored image data.
Similarly, Taigman et al. translated face images to
emojis by providing image features from pre-trained face
recognition module as conditional input to a GAN.
Recently, Tong et al. tackled mode-collapsing and
instability problems in GAN training. They introduced two
ways of regularizing general GAN objective – geometric
metrics regularizer and mode regularizer.
5. Conclusion
This paper presents a learning method to discover crossdomain relations with a generative adversarial network
called DiscoGAN. Our approach works without any explicit pair labels and learns to relate datasets from very different domains. We have demonstrated that DiscoGAN can
generate high-quality images with transferred style. One
possible future direction is to modify DiscoGAN to handle mixed modalities (e.g. text and image).