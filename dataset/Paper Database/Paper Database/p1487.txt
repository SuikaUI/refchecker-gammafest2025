Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432,
Lisbon, Portugal, 17-21 September 2015. c⃝2015 Association for Computational Linguistics.
Document Modeling with Gated Recurrent Neural Network
for Sentiment Classiﬁcation
Duyu Tang, Bing Qin∗, Ting Liu
Harbin Institute of Technology, Harbin, China
{dytang, qinb, tliu}@ir.hit.edu.cn
Document level sentiment classiﬁcation
remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document.
To address this, we introduce a neural network
model to learn vector-based document representation in a uniﬁed, bottom-up fashion. The model ﬁrst learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their
relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classiﬁcation on four
large-scale review datasets from IMDB
and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically
outperforms standard recurrent neural network in document modeling for sentiment
classiﬁcation.1
Introduction
Document level sentiment classiﬁcation is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews . The task calls for identifying the overall sentiment polarity (e.g. thumbs
up or thumbs down, 1-5 stars on review sites) of a
document. In literature, dominant approaches follow and exploit machine learn-
∗Corresponding author.
1 Codes and datasets are publicly available at
 
ing algorithm to build sentiment classiﬁer. Many
of them focus on designing hand-crafted features
 or
learning discriminate features from data, since the
performance of a machine learner is heavily dependent on the choice of data representation .
Document level sentiment classiﬁcation remains a signiﬁcant challenge: how to encode the
intrinsic (semantic or syntactic) relations between
sentences in the semantic meaning of document. This is crucial for sentiment classiﬁcation because relations like “contrast” and “cause” have
great inﬂuences on determining the meaning and
the overall polarity of a document. However, existing studies typically fail to effectively capture
such information. For example, Pang et al. 
and Wang and Manning represent documents with bag-of-ngrams features and build SVM
classiﬁer upon that. Although such feature-driven
SVM is an extremely strong performer and hardly
to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of
side information like relations between sentences.
Recently, Le and Mikolov exploit neural
networks to learn continuous document representation from data. Essentially, they use local ngram
information and do not capture semantic relations
between sentences. Furthermore, a person asked
to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings
of sentences before considering semantic relations between them. This motivates us to develop an
end-to-end and bottom-up algorithm to effectively
model document representation.
In this paper, we introduce a neural network approach to learn continuous document representation for sentiment classiﬁcation. The method is
on the basis of the principle of compositionality
(Frege, 1892), which states that the meaning of
a longer expression (e.g. a sentence or a docu-
Word Representation
Sentence Representation
Document Representation
Document Composition
Sentence Composition
Forward Gated
Neural Network
Backward Gated
Neural Network
Forward Gated
Neural Network
Backward Gated
Neural Network
Forward Gated
Neural Network
Backward Gated
Neural Network
Figure 1: The neural network model for document level sentiment classiﬁcation. wn
i stands for the i-th
word in the n-th sentence, ln is sentence length.
ment) depends on the meanings of its constituents.
Speciﬁcally, the approach models document representation in two steps. In the ﬁrst step, it uses convolutional neural network (CNN) or long
short-term memory (LSTM) to produce sentence
representations from word representations. Afterwards, gated recurrent neural network is exploited to adaptively encode semantics of sentences
and their inherent relations in document representations. These representations are naturally used
as features to classify the sentiment label of each
document. The entire model is trained end-to-end
with stochastic gradient descent, where the loss
function is the cross-entropy error of supervised
sentiment classiﬁcation2.
We conduct document level sentiment classi-
ﬁcation on four large-scale review datasets from
IMDB3 and Yelp Dataset Challenge4. We compare to neural network models such as paragraph
vector , convolutional neural network, and baselines such as feature-based
SVM , recommendation algorithm JMARS . Experimental
results show that: (1) the proposed neural model
shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural
//deeplearning.net/tutorial/lstm.html
3 
4 
network in document modeling. The main contributions of this work are as follows:
• We present a neural network approach to encode relations between sentences in document representation for sentiment classiﬁcation.
• We report empirical results on four large-scale
datasets, and show that the approach outperforms
state-of-the-art methods for document level sentiment classiﬁcation.
• We report empirical results that traditional recurrent neural network is weak in modeling document composition, while adding neural gates dramatically improves the classiﬁcation performance.
The Approach
We introduce the proposed neural model in this
section, which computes continuous vector representations for documents of variable length. These
representations are further used as features to classify the sentiment label of each document.
overview of the approach is displayed in Figure 1.
Our approach models document semantics
based on the principle of compositionality (Frege,
1892), which states that the meaning of a longer
expression (e.g. a sentence or a document) comes
from the meanings of its constituents and the rules
used to combine them. Since a document consists of a list of sentences and each sentence is made
up of a list of words, the approach models document representation in two stages. It ﬁrst produces
continuous sentence vectors from word represen-
tations with sentence composition (Section 2.1).
Afterwards, sentence vectors are treated as inputs
of document composition to get document representation (Section 2.2). Document representations
are then used as features for document level sentiment classiﬁcation (Section 2.3).
Sentence Composition
We ﬁrst describe word vector representation, before presenting a convolutional neural network
with multiple ﬁlters for sentence composition.
Each word is represented as a low dimensional, continuous and real-valued vector, also known
as word embedding .
All the word vectors are stacked in a word embedding matrix Lw ∈Rd×|V |, where d is the dimension of word vector and |V | is vocabulary size.
These word vectors can be randomly initialized
from a uniform distribution ,
or be pre-trained from text corpus with embedding
learning algorithms . We adopt
the latter strategy to make better use of semantic
and grammatical associations of words.
We use convolutional neural network (CNN)
and long short-term memory (LSTM) to compute
continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classiﬁcation . They learn ﬁxed-length vectors for sentences of varying length, captures words order in
a sentence and does not depend on external dependency or constituency parse results. One could
also use tree-based composition method such as
Recursive Neural Tensor Network or Tree-Structured LSTM as alternatives.
Speciﬁcally, we try CNN with multiple convolutional ﬁlters of different widths to produce sentence representation.
Figure 2 displays the method. We use multiple convolutional ﬁlters in order to capture local semantics of n-grams of various granularities, which
have been proven effective for sentiment classiﬁcation. For example, a convolutional ﬁlter with a
width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three
convolutional ﬁlters whose widths are 1, 2 and
3 to encode the semantics of unigrams, bigram-
Convolution
Figure 2: Sentence composition with convolutional neural network.
s and trigrams in a sentence. Each ﬁlter consists
of a list of linear layers with shared parameters. Formally, let us denote a sentence consisting
of n words as {w1, w2, ...wi, ...wn}, let lc be the
width of a convolutional ﬁlter, and let Wc, bc be
the shared parameters of linear layers in the ﬁlter. Each word wi is mapped to its embedding
representation ei ∈Rd. The input of a linear layer is the concatenation of word embeddings in a
ﬁxed-length window size lc, which is denoted as
Ic = [ei; ei+1; ...; ei+lc−1] ∈Rd·lc. The output of
a linear layer is calculated as
Oc = Wc · Ic + bc
where Wc ∈Rloc×d·lc, bc ∈Rloc, loc is the output
length of linear layer. To capture global semantics
of a sentence, we feed the outputs of linear layers
to an average pooling layer, resulting in an output
vector with ﬁxed-length. We further add hyperbolic tangent (tanh) to incorporate pointwise nonlinearity, and average the outputs of multiple ﬁlters
to get sentence representation.
We also try lstm as the sentence level semantic
calculator, the performance comparison between
these two variations is given in Section 3.
Document Composition with Gated
Recurrent Neural Network
The obtained sentence vectors are fed to a document composition component to calculate the document representation. We present a gated recurrent neural network approach for document composition in this part.
Given the vectors of sentences of variable
length as input, document composition produces
a ﬁxed-length document vector as output. To this
end, a simple strategy is ignoring the order of sen-
(a) GatedNN
(b) GatedNN Avg
Figure 3: Document modeling with gated recurrent neural network. GNN stands for the basic computational unit of gated recurrent neural network.
tences and averaging sentence vectors as document vector. Despite its computational efﬁciency,
it fails to capture complex linguistic relations (e.g.
“cause” and “contrast”) between sentences. Convolutional neural network is an
alternative for document composition, which models local sentence relations with shared parameters
of linear layers.
Standard recurrent neural network (RNN) can
map vectors of sentences of variable length to
a ﬁxed-length vector by recursively transforming
current sentence vector st with the output vector
of the previous step ht−1. The transition function
is typically a linear layer followed by pointwise
non-linearity layer such as tanh.
ht = tanh(Wr · [ht−1; st] + br)
where Wr ∈Rlh×(lh+loc), br ∈Rlh, lh and loc are
dimensions of hidden vector and sentence vector,
respectively. Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding
 , where gradients may grow or decay
exponentially over long sequences. This makes
it difﬁcult to model long-distance correlations in
a sequence. To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way
and adaptively encodes sentence semantics in document representations.
The approach is analogous to the recently emerged LSTM and gated neural
network .
Speciﬁcally, the transition function of the gated
RNN used in this work is calculated as follows.
it = sigmoid(Wi · [ht−1; st] + bi)
ft = sigmoid(Wf · [ht−1; st] + bf)
gt = tanh(Wr · [ht−1; st] + br)
ht = tanh(it ⊙gt + ft ⊙ht−1)
where ⊙stands for element-wise multiplication,
Wi, Wf, bi, bf adaptively select and remove history vector and input vector for semantic composition. The model can be viewed as a LSTM whose
output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to
get a better document representation. Figure 3 (a)
displays a standard sequential way where the last
hidden vector is regarded as the document representation for sentiment classiﬁcation. We can
make further extensions such as averaging hidden
vectors as document representation, which takes
considerations of a hierarchy of historical semantics with different granularities. The method is illustrated in Figure 3 (b), which shares some characteristics with . We can go
one step further to use preceding histories and following evidences in the same way, and exploit bidirectional gated RNN as the
calculator. The model is embedded in Figure 1.
Sentiment Classiﬁcation
The composed document representations can be
naturally regarded as features of documents for
sentiment classiﬁcation without feature engineering.
Speciﬁcally, we ﬁrst add a linear layer to
transform document vector to real-valued vector
whose length is class number C. Afterwards, we
add a softmax layer to convert real values to conditional probabilities, which is calculated as follows.
i′=1 exp(xi′)
We conduct experiments in a supervised learning setting, where each document in the training
data is accompanied with its gold sentiment label.
Class Distribution
.09/.09/.14/.33/.36
.10/.09/.15/.30/.36
.10/.09/.14/.30/.37
.07/.04/.05/.05/.08/.11/.15/.17/.12/.18
Table 1: Statistical information of Yelp 2013/2014/2015 and IMDB datasets. #docs is the number of
documents, #s/d and #w/d represent average number of sentences and average number of words contained
in per document, |V | is the vocabulary size of words, #class is the number of classes.
For model training, we use the cross-entropy error between gold sentiment distribution P g(d) and
predicted sentiment distribution P(d) as the loss
i (d) · log(Pi(d))
where T is the training data, C is the number
of classes, d represents a document. P g(d) has
a 1-of-K coding scheme, which has the same
dimension as the number of classes, and only the
dimension corresponding to the ground truth is
1, with all others being 0. We take the derivative of loss function through back-propagation
with respect to the whole set of parameters θ =
[Wc; bc; Wi; bi; Wf; bf; Wr; br; Wsoftmax, bsoftmax],
and update parameters with stochastic gradient
descent. We set the widths of three convolutional
ﬁlters as 1, 2 and 3, output length of convolutional
ﬁlter as 50. We learn 200-dimensional word embeddings with SkipGram 
on each dataset separately, randomly initialize
other parameters from a uniform distribution
U(−0.01, 0.01), and set learning rate as 0.03.
Experiment
We conduct experiments to empirically evaluate
our method by applying it to document level sentiment classiﬁcation. We describe experimental settings and report empirical results in this section.
Experimental Setting
We conduct experiments on large-scale datasets
consisting of document reviews. Speciﬁcally, we
use one movie review dataset from IMDB and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and
2015. Human labeled review ratings are regarded
as gold standard sentiment labels, so that we do
not need to manually annotate sentiment labels of
documents. We do not consider the cases that rating does not match with review texts on IMDB dataset, and split
Yelp datasets into training, development and testing sets with 80/10/10. We run tokenization and
sentence splitting with Stanford CoreNLP on all these datasets. We use
accuracy and MSE 
as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classiﬁcation performance. We use MSE to measure
the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reﬂect sentiment strengths (e.g. one
star means strong negative and ﬁve star means
strong positive).
i (goldi −predictedi)2
Baseline Methods
We compare our methods (Conv-GRNN and
LSTM-GRNN) with the following baseline methods for document level sentiment classiﬁcation.
(1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set
to each document in test set.
(2) In SVM+Ngrams, we use bag-of-unigrams
and bag-of-bigrams as features and train SVM
classiﬁer with LibLinear 5.
(3) In TextFeatures, we implement sophisticated
features including word
ngrams, character ngrams, sentiment lexicon features, cluster features, et al.
5We also try discretized regression 
with ﬁxed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classiﬁer.
SVM + Unigrams
SVM + Bigrams
SVM + TextFeatures
SVM + AverageSG
SVM + SSWE
Paragraph Vector
Convolutional NN
Table 2: Sentiment classiﬁcation on Yelp 2013/2014/2015 and IMDB datasets. Evaluation metrics are
accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
(4) In AverageSG, we learn 200-dimensional
word vectors with word2vec6 , average word embeddings to get document
representation, and train a SVM classiﬁer.
(5) We learn sentiment-speciﬁc word embeddings (SSWE), and use max/min/average pooling
 to get document representation.
(6) We compare with a state-of-the-art recommendation algorithm JMARS ,
which utilizes user and aspects of a review with
collaborative ﬁltering and topic modeling.
(7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis
 .
(8) We implement a state-of-the-art neural network baseline Paragraph Vector because its codes are not ofﬁcially provided.
Window size is tuned on the development set.
Comparison to Other Methods
Experimental results are given in Table 2. We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better).
The best method in each dataset and each evaluation metric is in bold.
From Table 2, we can see that majority is the
worst method because it does not capture any textual semantics. SVM classiﬁers with unigram and
bigram features are extremely
strong, which are almost the strongest performers
6We use Skipgram as it performs slightly better than
CBOW in the experiment. We also try off-the-shell word embeddings from Glove, but its performance is slightly worse
than tailored word embedding from each corpus.
among all baseline methods. Designing complex
features are also effective for document level sentiment classiﬁcation, however, it does not surpass
the bag-of-ngram features signiﬁcantly as on Twitter corpora .
Furthermore, the aforementioned bag-of-features are discrete and sparse. For example, the feature dimension of bigrams and TextFeatures on Yelp 2015
dataset are 899K and 4.81M after we ﬁlter out low
frequent features. Based on them, we try to concatenate several discourse-driven features, but the
classiﬁcation performances remain unchanged.
AverageSG is a straight forward way to compose document representation without feature engineering. Unfortunately, we can see that it does
not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classiﬁcation. We try to make
better use of the sentiment information to learn
a better SSWE , e.g. setting
a large window size. However, its performance
is still worse than context-based word embedding.
This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in
document level reviews, while Tang et al. 
learn SSWE by assigning sentiment label of a text to each phrase it contains. How to learn SSWE
effectively with document level sentiment supervision remains as an interesting future work.
Since JMARS outputs real-valued outputs, we
only evaluate it in terms of MSE. We can see that
sophisticated baseline methods such as JMARS,
paragraph vector and convolutional NN obtain signiﬁcant performance boosts over AverageSG by
Recurrent Avg
Bi Recurrent Avg
GatedNN Avg
Bi GatedNN Avg
Table 3: Sentiment classiﬁcation on IMDB, Yelp 2013/2014/2015 datasets. Evaluation metrics are accuracy (higher is better) and MSE (lower is better). The best method in each setting is in bold.
capturing deeper semantics of texts. Comparing
between CNN and AverageSG, we can conclude
that deep semantic compositionality is crucial for
understanding the semantics and the sentiment of
documents. However, it is somewhat disappointing that these models do not signiﬁcantly outperform discrete bag-of-ngrams and bag-of-features.
The reason might lie in that semantic meanings of
documents, e.g. relations between sentences, are
not well captured. We can see that the proposed
method Conv-GRNN and LSTM-GRNN yield the
best performance on all four datasets in two evaluation metrics. Compared with CNN, Conv-GRNN
shows its superior power in document composition component, which encodes semantics of sentences and their relations in document representation with gated recurrent neural network. We also ﬁnd that LSTM (almost) consistently performs
better than CNN in modeling the sentence representation.
Model Analysis
As discussed before, document composition contributes a lot to the superior performance of Conv-
GRNN and LSTM-GRNN. Therefore, we take
Conv-GRNN as an example and compare different neural models for document composition in this
part. Speciﬁcally, after obtaining sentence vectors
with convolutional neural network as described in
Section 2.1, we carry out experiments in following
(1) Average. Sentence vectors are averaged to
get the document vector.
(2) Recurrent / GatedNN. Sentence vectors are
fed to standard (or gated) recurrent neural network
in a sequential way from the beginning of the input
document. The last hidden vector is regarded as
document representation.
(3) Recurrent Avg / GatedNN Avg. We extend
setting (2) by averaging hidden vectors of recurrent neural network as document vector.
(4) Bi Recurrent Avg / Bi GatedNN Avg. We extend setting (3) by calculating hidden vectors from
both preceding histories and following contexts.
Bi-directional hidden vectors are averaged as document representation.
Table 3 shows the experimental results. We can
see that standard recurrent neural network (RN-
N) is the worst method, even worse than the simple vector average.
This is because RNN suffers from the vanishing gradient problem, stating
that the inﬂuence of a given input on the hidden
layer decays exponentially over time on the network output.
In this paper, it means that document representation encodes rare semantics of
the beginning sentences. This is further justiﬁed
by the great improvement of Recurrent Avg over
Recurrent. Bi Recurrent Avg and Recurrent Avg
perform comparably, but disappointingly both of
them fail to transcend Average. After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and signiﬁcantly outperforms previous settings. The results indicate
that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is
practical to adaptively model sentence semantics
in document representation. GatedNN Avg and Bi
GatedNN Avg obtains comparable performances
with GatedNN.
Related Work
Document level sentiment classiﬁcation is a fundamental problem in sentiment analysis , which aims at identifying
the sentiment label of a document . Pang and Lee 
cast this problem as a classiﬁcation task, and use
machine learning method in a supervised learning
framework. Turney introduces an unsupervised approach by using sentiment words/phrases
extracted from syntactic patterns to determine the
document polarity.
Goldberg and Zhu 
place this task in a semi-supervised setting, and
use unlabelled reviews with graph-based method.
Dominant studies in literature follow Pang et al.
 and work on designing effective features
for building a powerful sentiment classiﬁer. Representative features include word ngrams , text topic ,
bag-of-opinions , syntactic relations , sentiment lexicon features .
Despite the effectiveness of feature engineering,
it is labor intensive and unable to extract and organize the discriminative information from data
 . Recently, neural network emerges as an effective way to learn continuous text
representation for sentiment classiﬁcation. Existing studies in this direction can be divided into two
groups. One line of research focuses on learning
continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way . Since these methods typically map words with similar contexts but opposite polarity (e.g.
“good” and “bad”) to neighboring vectors, several studies learn sentiment-speciﬁc
word embeddings by taking sentiment of texts into account. Another line of research concentrates
on semantic composition . Yessenalina and Cardie represent
each word as a matrix and use iterated matrix multiplication as phrase-level composition function.
Socher et al. introduce a family of recursive neural networks for sentence-level semantic
composition. Recursive neural network is extended with global feedbackward ,
feature weight tuning , deep recursive
layer , adaptive composition functions , combined with
Combinatory Categorial Grammar , and used for opinion relation detection . Glorot et al. use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic composition by automatically capturing local and global semantics. Le
and Mikolov introduce Paragraph Vector to
learn document representation from semantics of
words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also
veriﬁed as strong approaches for semantic composition .
In this work, we represent document with
convolutional-gated
which adaptively encodes semantics of sentences
and their relations. A recent work in also investigate LSTM to model document
meaning. They verify the effectiveness of LSTM
in text generation task.
Conclusion
We introduce neural network models (Conv-
GRNN and LSTM-GRNN) for document level
sentiment classiﬁcation.
The approach encodes
semantics of sentences and their relations in document representation, and is effectively trained
end-to-end with supervised sentiment classiﬁcation objectives. We conduct extensive experiments
on four review datasets with two evaluation metrics. Empirical results show that our approaches
achieve state-of-the-art performances on all these
datasets. We also ﬁnd that (1) traditional recurrent neural network is extremely weak in modeling
document composition, while adding neural gates
dramatically boosts the performance, (2) LSTM
performs better than a multi-ﬁltered CNN in modeling sentence representation.
We brieﬂy discuss some future plans. How to
effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically
without using external discourse analysis results. From one perspective, one could carefully de-
ﬁne a set of sentiment-sensitive discourse relations , such as “contrast”, “condition”, “cause”, etc. Afterwards, relation-speciﬁc
gated RNN can be developed to explicitly model semantic composition rules for each relation
 . However, deﬁning such a
relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document
representation over discourse tree structures rather
than in a sequential way. Accordingly, Recursive
Neural Network and Structured LSTM 
can be used as composition algorithms. However, existing discourse structure learning algorithms are difﬁcult to scale to massive review texts on
the web. How to simultaneously learn document
structure and composition function is an interesting future work.
Acknowledgments
The authors give great thanks to Yaming Sun and
Jiwei Li for the fruitful discussions.
would like to thank three anonymous reviewers for their valuable comments and suggestions.
This work was supported by the National High
Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61133012 and No.
61273321). Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship.