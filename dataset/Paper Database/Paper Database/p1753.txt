Low-shot learning with large-scale diffusion
Matthijs Douze‚Ä†, Arthur Szlam‚Ä†, Bharath Hariharan‚Ä†‚àó, Herv¬¥e J¬¥egou‚Ä†
‚Ä†Facebook AI Research
*Cornell University
This paper considers the problem of inferring image labels from images when only a few annotated examples are
available at training time. This setup is often referred to
as low-shot learning, where a standard approach is to retrain the last few layers of a convolutional neural network
learned on separate classes for which training examples are
abundant. We consider a semi-supervised setting based on
a large collection of images to support label propagation.
This is possible by leveraging the recent advances on largescale similarity graph construction.
We show that despite its conceptual simplicity, scaling
label propagation up to hundred millions of images leads to
state of the art accuracy in the low-shot learning regime.
1. Introduction
Large, diverse collections of images are now commonplace; these often contain a ‚Äúlong tail‚Äù of visual concepts.
Some concepts like ‚Äúperson‚Äù or ‚Äúcat‚Äù appear in many images, but the vast majority of the visual classes do not occur
frequently. Even though the total number of images may
be large, it is hard to collect enough labeled data for most
of the visual concepts. Thus if we want to learn them, we
must do so with few labeled examples. This task is named
low-shot learning in the literature.
In order to learn new classes with little supervision, a
standard approach is to leverage classiÔ¨Åers already learned
for the most frequent classes, employing a so-called transfer learning strategy. For instance, for new classes with few
labels, only the few last layers of a convolutional neural network are re-trained. This limits the number of parameters
that need to be learned and limits over-Ô¨Åtting.
In this paper, we consider the low-shot learning problem described above, where the goal is to learn to detect
new visual classes with only a few annotated images per
class, but we also assume that we have many unlabelled images. This is called semi-supervised learning (con-
‚àóThis work was carried out while B. Hariharan was post-doc at FAIR.
seed images
background images
(unlabeled)
test images
(labels witheld)
Figure 1. The diffusion setup. The arrows indicate the direction
of diffusion. There is no diffusion performed from the test images. For the rest of the graph, the edges are bidirectional (i.e., the
graph matrix is symmetric). Except when mentioned otherwise,
the edges have no weights.
sidered, e.g., for face annotation ). The motivation of
this work is threefold. First we want to show that with modern computational tools, classical semi-supervised learning
methods scale gracefully to hundreds of millions of unlabeled points. A limiting factor in previous evaluations was
that constructing the similarity graph supporting the diffusion was slow. This is no longer a bottleneck: thanks to
advances both in computing architectures and algorithms,
one can routinely compute the similarity graph for 100 millions images in a few hours . Second, we want to answer the question: Does a very large number of images help
for semi-supervised learning? Finally, by comparing the
results of these methods on Imagenet and the YFCC100M
dataset , we highlight how these methods exhibit some
artiÔ¨Åcial aspects of Imagenet that can inÔ¨Çuence the performance of low shot learning algorithms.
In summary, the contribution of our paper is a study of
semi-supervised learning in the scenario where we have a
very large number of unlabeled images. Our main results
are that in this setting, semi-supervised learning leads to
state of the art low-shot learning performance. In more detail, we make the following contributions:
‚Ä¢ We carry out a large-scale evaluation for diffusion
methods for semi-supervised learning and compare
 
it to recent low-shot learning papers.
Our experiments are all carried out on the public benchmark Imagenet and the YFC100M dataset .
‚Ä¢ We show that our approach is efÔ¨Åcient and that the diffusion process scales up to hundreds of millions of images, which is order(s) of magnitude larger than what
we are aware in the literature on image-based diffusion . This is made possible by leveraging the
recent state of the art for efÔ¨Åcient k-nearest neighbor
graph construction .
‚Ä¢ We evaluate several variants and hypotheses involved
in diffusion methods, such as using class frequency
priors .
This scenario is realistic in situations
where this statistic is known a priori. We propose a
simple way to estimate it without this prior knowledge, and extend this assumption to a multiclass setting by introducing a probabilistic projection step derived from Sinkhorn-Knopp algorithm.
‚Ä¢ Our experimental study shows that a simple propagation process signiÔ¨Åcantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i)
the number of annotated images per class is small and
when (ii) the number of unlabeled images is large or
the unlabeled images come form the same domain as
the test images.
This paper is organized as follows. Section 2 reviews
related works and Section 3 describes the label propagation
methods. The experimental study is presented in Section 4.
Our conclusion in section 5 summarizes our Ô¨Åndings.
2. Related work
Low-shot learning
Recently there has been a renewed interest for low-shot learning, i.e., learning with few examples thanks to prior statistics on other classes. Such works
include metric learning , learning kNN , regularization and feature hallucination or predicting parameters of the network . Ravi and Larochelle introduce a
meta-learner to learn the optimization parameters invovled
in the low-shot learning regime . Most of the works
consider small datasets like Omniglot, CIFAR, or a small
subset of Imagenet. In our paper we will focus solely on
large datasets, in particular the Imagenet collection associated with the ILSVRC challenge.
Diffusion methods
We refer the reader to for a
review of diffusion processes and matrix normalization options. Such methods are an efÔ¨Åcient way of clustering images given a matrix of input similarity, or a kNN graph, and
have been successfully used in a semi-supervised discovery
setup . They share some connections with spectral clustering . In , a kNN graph is clustered with spectral
clustering, which amounts to computing the k eigenvectors
associated with the k largest eigenvalues of the graph, and
clustering these eigenvectors. Since the eigenvalues are obtained via Lanczos iterations [15, Chapter 10], the basic operation is similar to a diffusion process. This is also related
to power iteration clustering , as in the work of Cho et
al. to Ô¨Ånd clusters.
Semi-supervised learning
The kNN graph can be used
for transductive and semi-supervised learning (see e.g. for an introduction). In transductive learning, a relatively small number of labels are used to augment a large
set of unlabeled data and the goal is to extend the labeling
to the unlabeled data (which is given at train time). Semisupervised learning is similar, except there may be a separate set of test points that are not seen at train time. In our
work, we consider the simple proposal of Zhu et al. ,
where powers of the (normalized) kNN graph are used to
Ô¨Ånd smooth functions on the kNN graph with desired values at the labeled points. There exist many variations on the
algorithms, e.g., Zhou et al. weight the edges based on
distances and introduce a loss trading a classiÔ¨Åcation Ô¨Åtting
constraint and a smoothness term enforcing consistency of
neighboring nodes.
Label propagation is a transductive method. In order to
evaluate on new data, we need to extend the smooth functions out of the training data. While deep networks have
been used before for out of sample extension, e.g., in 
and , in the speech domain, in this work, we use a
weighted sum of nearest neighbors from the (perhaps unlabeled) training data .
EfÔ¨Åcient kNN-graph construction
The diffusion methods use a matrix as input containing the similarity between
all images of the dataset.
Considering N images, e.g.,
N = 108, it is not possible to store a matrix of size N 2.
However most of the image pairs are not related and have a
similarity close to 0. Therefore diffusion methods are usually implemented with sparse matrices. This means that we
compute a graph connecting each image to its neighbors, as
determined by the similarity metric between image representations. In particular, we consider the k-nearest neighbor graph (kNN-graph) over a set of vectors. Several approximate algorithms have been proposed to
efÔ¨Åciently produce the kNN graph used as input of iterative/diffusion methods, since this operation is of quadratic
complexity in the number of images. In this paper, we employ the Faiss library,which was shown capable to construct
a graph connecting up to 1 billion vectors .
3. Propagating labels
This section describes the initial stage of our proposal,
which estimates the class of the unlabelled images with a
diffusion process. It includes an image description step, the
construction of a kNN graph connecting similar images, and
a label diffusion algorithm.
3.1. Image description
A meaningful semantic image representation and an associated metric is required to match instances of classes that
have not been seen beforehand. While early works on semisupervised labelling were using ad-hoc semantic global
descriptors like GIST , we extract activation maps from
a CNN trained on images from a set of base classes that are
independent from the novel classes on which the evaluation
is performed. See the experimental section for more details
about the training process for descriptors.
The mean class classiÔ¨Åer introduced for low-shot learning is another way to perform dimensionality reduction while improving accuracy thanks to a better comparison metric. We do not consider this approach since it can
be seen as part of the descriptor learning.
3.2. AfÔ¨Ånity matrix: approximate kNN graph
As discussed in the related work, most diffusion processes use as input the kNN graph representing the N √ó N
sparse similarity matrix, denoted by W, which connects
the N images of the collection. We build this graph using approximate k-nearest neighbor search. Thanks to recent advances in efÔ¨Åcient similarity search , trading
some accuracy against efÔ¨Åciency drastically improves the
graph construction time. As an example, with the FAISS
library , building the graph associated with 600k images takes 2 minutes on 1 GPU. In our preliminary experiments, the approximation in the knn-graph does not induce
any sub-optimality, possibly because the diffusion process
compensates the artifacts induced by the approximation.
Different strategies exist to set the weights of the afÔ¨Ånity matrix W. We choose to search the k nearest neighbors
of each image, and set a 1 for each of the neighbors in the
corresponding row of a sparse matrix W0. Then we symmetrize the matrix by adding it to its transpose. We subsequently ‚Ñì1-normalize the rows to produce a sparse stochastic matrix: W = D‚àí1(W‚ä§
0 + W0), with D the diagonal
matrix of row sums.
The handling for the test points is different: test points
do not participate in label propagation because we classify
each of them independently of the others. Therefore, there
are no outgoing edges on test points; they only get incoming
edges from their k nearest neighbors.
3.3. Label propagation
We now give details about the diffusion process itself,
which is summarized in Figure 1. We build on the straightforward label propagation algorithm of . The set of
images on which we perform diffusion is composed of nL
labelled seed images and nB unlabelled background images
(N = nL+nB). DeÔ¨Åne the N √óC matrix L, where C is the
number of classes for which we want to diffuse the labels,
i.e., the new classes not seen in the training set. Each row
li in L is associated with a given image, and represents the
probabilities of each class for that image. A given column
corresponds to a given class, and gives its probabilities for
each image. The method initializes li to a one-hot vector for
the seeds. Background images are initialized with 0 probabilities for all classes. Diffusing from the known labels, the
method iterates as Lt+1 = WLt.
We can optionally reset the L rows corresponding to
seeds to their 1-hot ground-truth at each iteration. When
iterating to convergence, all li would eventually converge
to the eigenvector of W with largest eigenvalue (when not
resetting), or to the harmonic function with respect to W
with boundary conditions given by the seeds (when resetting). Empirically, for low-shot learning, we observe that
resetting is detrimental to accuracy.
Early stopping performs better in both cases, so we cross-validate the number
of diffusion iterations.
ClassiÔ¨Åcation decision & combination with logistic regression
We predict the class of a test example i as the
the column that maximizes the score li. Similar to Zhou
et al. , we have also optimized a loss balancing the Ô¨Åtting constraint with the diffusion smoothing term. However
we found that a simple late fusion (weighted mean of logprobabilities, parametrized by a single cross-validated coefÔ¨Åcient) of the scores produced by diffusion and logistic
regression achieves better results.
3.4. Variations
Using priors
The label propagation can take into account
several priors depending on the assumptions of the problem,
which are integrated by deÔ¨Åning a normalization operator Œ∑
and by modifying the update equation as
Lt+1 = Œ∑(WLt).
Multiclass assumption. For instance, in the ILSVRC challenge built upon the Imagenet dataset , there is only one
label per class, therefore we can deÔ¨Åne Œ∑ as a function that
‚Ñì1-normalizes each row to provide a distribution over labels (by convention the normalization leaves all-0 vectors
unchanged).
Class frequency priors. Additionally, we point out that labels are evenly distributed in Imagenet. If we translate this
setup to our semi-unsupervised setting, it would mean that
we may assume that the distribution of the unlabelled images is uniform over labels. This assumption can be taken
into account by deÔ¨Åning Œ∑ as the function performing a ‚Ñì1
normalization of columns of L.
While one could argue that this is not realistic in general, a more realistic scenario is to consider that we know
the marginal distribution of the labels, as proposed by Zhu
et al. , who show that the prior can be simply enforced
(i.e., apply column-wise normalization to L and multiply
each column by the prior class probability). This arises in
situations such as tag prediction, if we can empirically measure the relative probabilities of tags, possibly regularized
for lowest values.
Combined Multiclass assumption and class frequency priors.
We propose a variant way to use both a multiclass
setting and prior class probabilities by enforcing the matrix
L to jointly satisfy the following properties:
where pC is the prior distribution over labels. For this purpose, we adopt a strategy similar to that of Cuturi in
his work on optimal transport, in which he shows that the
Sinkhorn-Knopp algorithm provides an efÔ¨Åcient and
theoretically grounded way to project a matrix so that it satisÔ¨Åes such marginals. The Sinkhorn-Knopp algorithm iterates by alternately enforcing the marginal conditions, as
L ‚ÜêL diag(L1C)‚àí1diag(pC)
L ‚Üêdiag(1‚ä§
until convergence. Here we assume that the algorithm only
operates on rows and columns whose sum is strictly positive. As discussed by Knight , the convergence of this
algorithm is fast. Therefore we stop after 5 iterations. This
projection is performed after each update by Eqn. 1. Note
that Zhu et al. solely considered the second constraint
in Eqn. 2, which can be obtained by enforcing the prior, as
discussed by Bengio et al. . We evaluate both variants in
the experimental section 4.
Non-linear updates.
The Markov Clustering (MCL) 
is another diffusion algorithm with nonlinear updates originally proposed for clustering. In contrast to the previous
algorithm, MCL iterates directly over the similarity matrix
t ‚ÜêWt ¬∑ Wt
Wt+1 ‚ÜêŒìr(W‚Ä≤
where Œìr is an element-wise raising to power r of the matrix, followed by a column-wise normalization . The
power r ‚àà(1, 2] is a bandwidth parameter: when r is high,
small edges quickly vanish along the iterations. A smaller
r preserves the edges longer. The clustering is performed
by extracting connected components from the Ô¨Ånal matrix.
In Section 4 we evaluate the role of the non-linear update
of MCL by introducing the Œìr non-linearity in the diffusion procedure. More precisely, we modify Equation 1 as
Lt+1 = Œìr (Œ∑(WLt)) .
3.5. Complexity
For the complexity evaluation, we distinguish two
In the off-line stage, (i) the CNN is trained on
the base classes, (ii) descriptors are extracted for the background images, and (iii) a knn-graph is computed for the
background images. In the on-line stage, we receive training and test images from novel classes, (i) compute features
for them, (ii) complement the knn-graph matrix to include
the training and test images, and (iii) perform the diffusion
iterations. Here we assume that the N √ó N graph matrix
W0 is decomposed in four blocks
‚àà{0, 1}(nL+nB)√ó(nL+nB) (6)
The largest matrix WBB ‚àà{0, 1}nB√ónB is computed offline. On-line we compute the three other matrices. We combine WBL and WBB by merging similarity search result
lists, hence each row of W0 contains exactly k non-zero
values, requiring to store the distances along with WBB.
We are mostly interested in the complexity of the online phase. Therefore we exclude the descriptor extraction,
which is independent of the classiÔ¨Åcation complexity, and
the complexity of handling the test images, which is negligible compared to the training operations. We consider the
logistic regression as a baseline for the complexity comparison:
Logistic regression the SGD training entails O(Ilogreg √ó
B √ó C √ó d) multiply-adds, with d denotes the descriptor dimensionality and C the number of classes. The
number of iterations and batch size are Ilogreg and B.
Diffusion the complexity is decomposed into: computing the matrices WLL, WLB and WBL, which involves O(d √ó nL √ó nB) multiply-adds using bruteforce distance computations; and performing Idif iterations of sparse-dense matrix multiplications, which
incurs O(k√óN √óC√óIdif) multiply-adds (note, sparse
matrix operations are more limited by irregular memory access patterns than arithmetic operations). Therefore the diffusion complexity is linear in the number
of background images nB. See the supplemental for
more details.
Memory usage.
One important bottleneck of the algorithm is its memory usage. The sparse matrix W0 occupies 8Nk bytes in RAM, and W almost twice this amount,
because most nearest neighbors are not reciprocal; the L
matrix is 4CN bytes. Fortunately, the iterations can be performed one column of L at a time, reducing this to 2 √ó 4N
bytes for Lt and Lt+1 (in practice, when memory is an issue, we group columns by batches of size C‚Ä≤ < C).
4. Experiments
4.1. Datasets and evaluation protocol
We use Imagenet 2012 and follow a recent
setup previously introduced for low-shot learning. The
1000 Imagenet classes are split randomly into two groups,
each containing base and novel classes. Group 1 (193 base
and 300 novel classes) is used for hyper-parameter tuning
and group 2 (196+311 classes) for testing with Ô¨Åxed hyperparameters. We assume the full Imagenet training data is
available for the base classes. For the novel classes, only n
images per class are available for training. Similar to 
the subset of n images is drawn randomly and the random
selection is performed 5 times with different random seeds.
As a large source of unlabelled images, we use the
YFCC100M dataset . It consists of 99 million representative images from the Flickr photo sharing site1. Note
that some works have used this dataset with tags or GPS
metadata as weak supervision .
Learning the image descriptors.
We use the 50-layer
Resnet trained by Hariharan et al. on all base classes
(group 1 + group 2), to ensure that the description calculation has never seen any image of the novel classes. We
run the CNN on all images, and extract a 2048-dim vector from the 49th layer, just before the last fully connected
layer. This descriptor is used directly as input for the logistic regression. For the diffusion, we PCA-reduce the feature vector to 256 dimensions and L2-normalize it, which
is standard in prior works on unsupervised image matching
with pre-learned image representations .
Performance measure and baseline
In a given group (1
or 2), we classify the Imagenet validation images from both
the base and novel classes, and measure the top-5 accuracy. Therefore the class distribution is heavily unbalanced.
Since the seed images are drawn randomly, we repeat the
random draws 5 times with different random seeds and average the obtained top-5 accuracy (the ¬±xx notation gives
the standard deviation).
The baseline is a logistic regression applied on the labelled points. We employ a per-class image sampling strategy to circumvent the unbalanced number of examples per
class. We optimize the learning rate, batch size and L2 regularization factor of the logistic regression on the group 1
1Of the 100M original Ô¨Åles, some are videos and some are not available
anymore. We replace them with uniform white images.
background
edge weighting
62.7¬±0.68 65.4¬±0.55 73.3¬±0.72
Gaussian weighting*
62.7¬±0.66 65.4¬±0.58 73.6¬±0.71
meaningful neighbors*
62.7¬±0.68 40.0¬±0.20 73.6¬±0.62
Œ∑ operator
40.6¬±0.18 41.1¬±0.10 42.3¬±0.19
61.1¬±0.69 56.8¬±0.50 72.3¬±0.72
column-wise
62.7¬±0.68 65.4¬±0.55 73.3¬±0.72
non-linear transform* Œìr
62.7¬±0.68 65.4¬±0.55 73.3¬±0.72
class frequency prior*
62.7¬±0.66 65.4¬±0.60 73.3¬±0.65
Variations on weighting for edges and normalization
steps on iterates of L. The tests are performed for n = 2 and
k = 30, with 5 runs on the group 1 validation images. Variants
that require a parameter (e.g., the œÉ of the Gaussian weighting) are
indicated with a ‚Äú*‚Äù. In this case we report only the best result,
see the supplementary material for full results. In the rest of the
paper, we use the variants indicated in bold, since they are simple
and do not add any parameter.
images. It is worth noticing that our baseline outperforms
the reported state of the art in this setting.
Background images for diffusion
We consider the following sets of background images:
1. None: the diffusion is directly from the seed images to
the test images;
2. In-domain setting: the background images are the Imagenet training image from the novel classes, but without labels. This corresponds to a use case where all
images are known to belong to a set of classes, but
only a subset of them have been labelled;
3. Out-of-domain setting: the nB background images are
taken from YFCC100M. We denote this setting by
F100k, F1M, F10M or F100M, depending on the number of images we use (e.g., we note F1M for nB =
106). This corresponds to a more challenging setting
where we have no prior knowledge about the image
used in the diffusion.
4.2. Parameters of diffusion
We compare a few settings of the diffusion algorithm as
discussed in section 3.4. In all cases, we set the number of
nearest neighbors to k = 30 and evaluate with n = 2. The
nearest neighbors are computed with Faiss , using the
IVFFlat index. It computes exact distances but occasionally
misses a few neighbors.
Graph edge weighting.
We experimented with different
weightings for W0, that were proposed in the literature. We
compared a constant weight, a Gaussian weighting ,
 


ClassiÔ¨Åcation performance with n = 2, with various
settings of k and nB, ordered by total number of edges (average of
5 test runs, with cross-validated number of iterations).
et al. 
regression
+ logistic
69.76¬±0.88
75.60¬±0.69
81.35¬±0.22
84.56¬±0.12
86.72¬±0.09
Table 2. In-domain diffusion on Imagenet: We compare against
logistic regression and a recent low-shot learning technique 
on this benchmark. Results are reported with k = 30 for diffusion.
(with œÉ a hyper-parameter), and a weighting based on the
‚Äúmeaningful neighbors‚Äù proposal .
Table 1 shows that results are remarkably independent of
the weighting choice, which is why we set it to 12. The best
normalization that can be applied to the L matrix is a simple
column-wise L1 normalization. Thanks to the linear iteration formula, it can be applied at the end of the iterations.
4.3. Large-scale diffusion
Figure 2 reports experiments by varying the number of
background images nB and the number k of neighbors, for
n = 2. All the curves have an optimal point in terms of accuracy vs computational cost at k=30. This may be a intrinsic property of the descriptor manifold. An additional number: before starting the diffusion iterations, with k=1000
and no background images (the best setting) we obtain an
accuracy of 60.5%. This is a knn-classiÔ¨Åer and this is the
fastest setting because the knn-graph does not need to be
constructed nor stored.
4.4. Comparison with low-shot classiÔ¨Åers
We compare the performance of diffusion against the logistic baseline classiÔ¨Åers and a recent method of the state of
the art , using the same features.
In-domain scenario.
For low-shot learning (n ‚â§5), the
in-domain diffusion outperforms the other methods by a
large margin, see Table 2. The combination with logistic
regression is not very effective.
Out-of-domain diffusion.
Table 3 shows that the performance of diffusion is competitive only when 1 or 2 images
are available per class. As stated in Section 3.2, we do not
include the test points in the diffusion, which is standard
for a classiÔ¨Åcation setting. However, if we allow this, as
in a fully transductive setting, we obtain a top-5 accuracy
of 69.6%¬±0.68 with n = 2 with diffusion over F1M, i.e., on
par with diffusion over F100M.
ClassiÔ¨Åer combination.
We experimented with a very
simple late fusion: to combine the scores of the two classiÔ¨Åers, we simply take a weighted average of their predictions (log-probabilities), and cross validate the weight factor. Both in the in-domain (Table 2) and out-of-domain (Table 3) cases, the results are signiÔ¨Åcantly above the best of
the two input classiÔ¨Åers. This shows that the logistic regression classiÔ¨Åer and the diffusion classiÔ¨Åer access different aspects of image collection. We also experimented
with more complicated combination methods, like using the
graph edges as a regularizer during the logistic regression,
which did not improve this result.
Comparison with the state of the art.
With the indomain diffusion, we notice that our method outperforms
the state-of-the-art result of and which, itself, outperforms or is closely competitive with in this setting.
In the out-of-domain setting, out results are better only for
n=1. However, their method is a complementary combination of a speciÔ¨Åc loss and a learned data augmentation
procedure that is speciÔ¨Åcally tailored to the experimental
setup with base and novel classes. In contrast, our diffusion
procedure is generic and has only two parameters (nB and
k). Note that the out-of-domain setting is comparable with
the standard low-shot setting, because the unlabeled images
from F100M are generic, and have nothing to do with Imagenet; and because the neighbor construction and diffusion
are efÔ¨Åcient enough to be run on a single workstation.
2Note that our parametric experiments use the set of baseline image
descriptors used in the arXiv version of , and the table compares all
methods using those underlying features, so the results are not directly
comparable with the rest of the paper.
out-of-domain diffusion
diffusion+logistic
regression
et al. 
Table 3. Out-of-domain diffusion: Comparison of classiÔ¨Åers for different values of n, with k = 30 for the diffusion results. The ‚Äúnone‚Äù
column indicates that the diffusion solely relies on the labelled images. The results of the rightmost column are state-of-the-art on this
benchmark to our knowledge, generally outperforming the results of matching networks and model regression in this setting.
background
F10M F100M
optimal iteration
timing: graph completion
2m57s 8m36s 40m41s 4h08m
timing: diffusion
Table 4. Timings for the different steps on a 24-core 2.5GHz
machine, for a varying number of unlabelled images from
YFCC100M. Note, the timing of 4h08m for graph completion over
F100M takes only 23m when executed on 8 GPUs.
4.5. Complexity: Runtime and memory
We measured the run-times of the different steps involved in diffusion process and report them in Table 4. The
graph construction time is linear in nB, thanks to the precomputation of the graph matrix for the background images
(see Section 3.5). For comparison, training the logistic regression takes between 2m27s and 12m, depending on the
cross-validated parameters.
In terms of memory usage, the biggest F100M experiments need to simultaneously keep in RAM a W matrix
of 5.3 billion non-zero values (39.5 GiB), and Lt and Lt+1
(35.8 GiB, using slices of C‚Ä≤ = 96 columns). This is the
main drawback of using diffusion. However Table 3 shows
that restricting the diffusion to 10 million images already
provides most of the gain, while dividing by an order of
magnitude memory and computational complexity.
4.6. Analysis of the diffusion process
We discuss how fast L ‚ÄúÔ¨Ålls up‚Äù (it is dense after a few
iterations). We consider the rate of nodes reached by the
diffusion process: we consider very large graphs, few seeds
and a relatively small graph degree. Figure 3 measures the
sparsity of the matrix L (on one run of validation), which indicates the rate of (label, image) tuples that have not been attained by the diffusion process at each diffusion step. While
the graph is not necessarily fully connected, we observe that
most images can be reached by all labels in practice.
The fraction of nodes reached by all labeled points grows
rapidly and converges to a value close to 1 in a few iterations when k ‚â•10. In order to relate this observation
with the performance attained along iterations, it is interesting to compare what happens in this plot to the one on the



 
  
 
Figure 3. Statistics over iterations, for n = 2. Top: Rate of nonzero element in the matrix L. Bottom: corresponding accuracy.
right. The plot on the right shows that the iteration number
at which the matrix close to 1 is similar to the iteration at
which accuracy is maximal, as selected by cross-validation.
The maximum occurs later if nB is larger and when k is
smaller. Note also that early stopping is important.
4.7. Qualitative results
Figure 4 shows paths between a seed image and test images, which gives a partial view of the diffusion. Given a
class, we backtrack the path: for a given node (image) and
iteration i, we look up the preceding node that contributed
most to the weight in Li that node at that iteration. At iteration 0, the backtracking process always ends in a source
node. Each row of the Ô¨Ågure is one such paths. For a test
image (right), we show the path for the ground-truth class
triumphal arch
Brian J. Geiger
(triumphal arch)
jack-o‚Äô-lantern
Eric M Martin
jack-o‚Äô-lantern
Urban Sea Star
breathedontbreathe
abroadGuille
(jack-o‚Äô-lantern)
planetarium
planetarium
DexterPerrin
(planetarium)
snarlenarlen
Nika Smetana
(planetarium)
AmberStrocel
teaandcakes
Figure 4. Images visited during the diffusion process from a seed (left) to the test image (right). We give ground-truth class for Imagenet
images (test images marked by parentheses). The Ô¨Årst two rows are classiÔ¨Åed correctly. The two bottom ones are failure cases. Imagenet
images are not shown for copyright reasons, but the labels are shown. For YFCC100M images, we provide the Flickr id of their creators.
and that for the found class, or a single row for both when
the image is classiÔ¨Åed correctly. Note that the preceding
node can be the image itself, since the diagonal of the W
matrix is not set to 0. Thanks to the size of the dataset, the
paths are ‚Äúsmooth‚Äù: they evolve through similar images.
5. Conclusion
We experimented on large-scale label propagation for
low-shot learning. Unsurprisingly, we have found that performing diffusion over images from the same domain works
much better than images from a different domain.
clearly observe that, as the number of images over which
we diffuse grows, the accuracy steadily improve. The main
performance factor is the total number of edges, which also
reasonably reÔ¨Çects the complexity. We also report neutral
results for most sophisticated variants, for instance we show
that edge weights are not useful. Furthermore, labeled images should be included in the diffusion process and not just
used as sources, i.e., not enforced to keep their label.
The main outcome of our study is to show that diffusion
over a large image set is superior to state-of-the-art methods
for low-shot learning when very few labels are available.
Interestingly, late-fusion with a standard classiÔ¨Åer‚Äôs result is
effective. This shows the complementary of the approaches,
and suggests that it could be combined with forthcoming
methods for low-short learning.
When more labels are available, simple logistic regression becomes superior to the methods we describe
(and to other state of the art low-shot learning methods).
However, we note that there are many circumstances
where even a few labels per class are more difÔ¨Åcult to get
than building (and then keeping) a graph over unlabeled
data. For example, if there are a large number of ‚Äútail‚Äù
classes which we will need to classify, a few examples
per class can multiply to many labels.
In these cases
diffusion combined with logistic regression is the best
method. The code to reproduce our results is available at