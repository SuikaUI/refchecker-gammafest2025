A Comparison of Static, Dynamic, and Hybrid Analysis
for Malware Detection
Anusha Damodaran∗Fabio Di Troia† Visaggio Aaron Corrado†
Thomas H. Austin∗Mark Stamp∗‡
In this research, we compare malware detection techniques based on static, dynamic,
and hybrid analysis. Speciﬁcally, we train Hidden Markov Models (HMMs ) on both
static and dynamic feature sets and compare the resulting detection rates over a substantial number of malware families.
We also consider hybrid cases, where dynamic
analysis is used in the training phase, with static techniques used in the detection phase,
and vice versa. In our experiments, a fully dynamic approach generally yields the best
detection rates. We discuss the implications of this research for malware detection based
on hybrid techniques.
Introduction
According to Symantec , more than 317 million new pieces of malware were created
in 2014, which represents a 26% increase over 2013.
Given numbers such as these,
malware detection is clearly a signiﬁcant and worthwhile research topic.
In practice, the most widely used malware detection method is signature scanning,
which relies on pattern matching. While signature scanning is eﬀective for many types
of malware, it is ineﬀective for detecting new malware, or even signiﬁcant variants of
existing malware .
A wide array of advanced detection techniques have been considered in the literature.
Some detection techniques rely only on static analysis , that is, features that can be obtained without executing the software. In addition,
dynamic analysis has been successfully applied to the malware detection problem . Recently, hybrid approaches have been analyzed, where
both static and dynamic features are used .
Here, we compare static analysis with dynamic analysis, and also consider hybrid
schemes that combine elements of both. We use a straightforward training and scoring
∗Department of Computer Science, San Jose State University
†Department of Engineering, Universit`a degli Studi del Sannio
‡ 
technique based on Hidden Markov Models and we consider feature sets consisting of
API call sequences and opcode sequences.
In this research, our goal is to gain some level of understanding of the relative advantages and disadvantages of static, dynamic, and hybrid techniques. In particular,
we would like to determine whether there is any inherent advantage to a hybrid approach. Note that our goal here is not to optimize the detection accuracy, which would
likely require combining a variety of scores and scoring techniques. Instead, we conduct
our analysis in a relatively simple setting, by which we hope to reduce the number of
potentially confounding variables that tend to appear in more highly optimized systems.
The remainder of this paper is organized as follows. In Section 2, we discuss relevant
background information, including related work. Section 3 discusses the experiments
conducted and the datasets used. In Section 4, we present our experimental results.
The paper concludes with Section 5, where we also mention possible future work.
Background
In this section, we ﬁrst provide a brief discussion of malware detection techniques, with
an emphasis on Hidden Markov Models, which are the basis for the research presented in
this paper. We also review relevant related work. Finally, we discuss ROC curves, which
give us a convenient means to quantify the various experiments that we have conducted.
Malware Detection
There are many approaches to the malware detection problem. Here, we brieﬂy consider signature-based, behavior-based, and statistical-based detection, before turning our
attention to a slightly more detailed discussion of HMMs.
Signature Based Detection
Signature based detection is the most widely used anti-virus technique . A signature
is a sequence of bytes that can be used to identify speciﬁc malware. A variety of pattern
matching schemes are used to scan for signatures . Signature based anti-virus software
must maintain a repository of signatures of known malware and such a repository must
be updated frequently as new threats are discovered.
Signature based detection is simple, relatively fast, and eﬀective against most common types malware. A drawback of signature detection is that it requires an up-to-date
signature database—malware not present in the database will not be detected. Also,
relatively simple obfuscation techniques can be used to evade signature detection .
Behavior Based Detection
Behavior based detection focuses on the actions performed by the malware during execution. In behavior based systems, the behavior of the malware and benign ﬁles are
analyzed during a training (learning) phase. Then during a testing (monitoring) phase,
an executable is classiﬁed as either malware or benign, based on patterns derived in the
training phase .
Statistical Based Detection
Malware detection can be based on statistical properties derived from program features.
For example, in , Hidden Markov Models (HMMs) are used to classify metamorphic
malware. This technique has served a benchmark in a variety of other studies . Consequently, we use HMMs as the basis for the malware detection schemes
considered in this research.
Hidden Markov Models
A Hidden Markov Model can be viewed as a machine learning technique, based on a
discrete hill climb . Applications of HMMs are many and varied, ranging from speech
recognition to applications in computational molecular biology, to artiﬁcial intelligence,
to malware detection .
As the name suggests, a Hidden Markov Model includes a Markov process that cannot
be directly observed. In an HMM, we have a series of observations that are related to
the “hidden” Markov process by a set of discrete probability distributions.
We use the following notation for an HMM :
T = length of the observation sequence
N = number of states in the model
M = number of observation symbols
Q = {q0, q1, . . . , qN−1} = distinct states of the Markov process
= {0, 1, . . . , M −1} = set of possible observations
A = state transition probabilities
B = observation probability matrix
π = initial state distribution
O = (O0, O1, . . . , OT−1) = observation sequence.
A generic Hidden Markov Model is illustrated in Figure 1.
A Hidden Markov Model is deﬁned by the matrices π, A and B, and hence we denote
an HMM as λ = (A, B, π). For simplicity, we often refer to λ simply as a “model”.
The practical utility of HMMs derives from the fact that there are eﬃcient algorithms
to solve each of the following three problems .
Problem 1: Given a model λ = (A, B, π) and an observation sequence O, determine P(O | λ). That is, we can score a given observation sequence against a given
model—the better the score, the more closely the observation sequence matches
the observations used to train the model.
Problem 2: Given a model λ = (A, B, π) and an observation sequence O, determine
the optimal state sequence X. That is, we can uncover the “best” hidden state
Here, “best” is in the sense of maximizing the expected number of
Figure 1: Generic Hidden Markov Model
correct states Xi. This is in contrast to a dynamic program, which yields the Xi
corresponding to the highest scoring path.
Problem 3: Given an observation sequence O and parameters N and M, determine
the model λ = (A, B, π) such that P(O | λ) is maximized. That is, we can train a
model to ﬁt a given observation sequence O.
In this research, we use the solution to Problem 3 to train a model based on observation sequences extracted from a given malware family. Then we use the solution
to Problem 1 to score observation sequences extracted from malware ﬁles as well as sequences extracted from benign ﬁles. We use the resulting scores to measure the success
of each technique.
Details on the HMM algorithms are beyond the scope of this paper. For a thorough
discussion of the solutions to HMM Problems 1 through 3, see ; for additional information see or the classic introduction . For the application of HMMs to malware
detection, see, for example, .
Related Work
Here we discuss some relevant examples of previous work. We group the previous work
based on whether it relies on static analysis or dynamic analysis, and we discuss techniques that employ a hybrid approach.
Static Analysis
Static analysis of software is performed without actually executing the program .
Examples of the information we can obtain from static analysis include opcode sequences
(extracted by disassembling the binary ﬁle), control ﬂow graphs, and so on. Such feature
sets can be used individually or in combination for malware detection.
In , the authors presented a malware detection technique that relies on static
analysis and is based on control ﬂow graphs. Their approach focuses on detecting obfuscation patterns in malware and they are able to achieve good accuracy.
Machine learning techniques have been applied to malware detection in the context
of static detection. In , Hidden Markov Models are used to eﬀectively classify metamorphic malware, based on extracted opcode sequences. A similar analysis involving
Proﬁle Hidden Markov Models is considered in , while Principal Component Analysis
is used in and , and Support Vector Machines are used for malware detection
in . The paper employs clustering, based on features derived from static analysis,
for malware classiﬁcation.
In , function call graph analysis is used for malware detection, while analyzes an opcode-based similarity measure that relies on simple substitution cryptanalysis
techniques. API call sequences and opcode sequences are both used in to determine
whether a segment of code has similarity to some particular malware.
The papers analyze ﬁle structure based on entropy variations. The work in
these paper was inspired by the entropy-based score in .
Dynamic Analysis
Dynamic analysis requires that we execute the program, often in a virtual environment . Examples of information that can be obtained by dynamic analysis include
API calls, system calls, instruction traces, registry changes, memory writes, and so on.
In , the authors build ﬁne-grained models that are designed to capture the behavior of malware based on system calls. The resulting behavior models are represented
in the form of graphs, where the vertices denote system calls and the edges denote
dependency between the calls.
The paper presents a run-time monitoring tool that extracts statistical features
based on spatio-temporal information in API call logs. The spatial information consists
of the arguments and return values of the API calls, while the temporal information is
the sequencing of the API calls. This information is used to build formal models that
are fed into standard machine learning algorithms, for use in malware detection.
In , a set of program API calls is extracted and combined with control ﬂow
graphs to obtain a so-called API-CFG model. In a slightly modiﬁed version , n-gram
methods are applied to the API calls.
Some recent work focuses on kernel execution traces as a means of developing a
malware behavior monitor .
In , the authors present an eﬀective method for
malware classiﬁcation using graph algorithms, which relies on dynamically extracted
information. The related work constructs a “kernel object behavioral graph” and
graph isomorphism techniques are used for scoring.
API sequences are again used for malware detection in . Also in the paper ,
malware is analyzed based on frequency analysis of API call sequences.
In , dynamic instruction sequences are logged and converted into abstract assembly blocks. Data mining algorithms are used to build a classiﬁcation model that relies
on feature vectors extracted from this data.
The authors of propose a malware detection technique that uses instruction trace
logs of executables, where this information is collected dynamically. These traces are
then analyzed as graphs, where the instructions are the nodes, and statistics from instruction traces are used to calculate transition probabilities. Support Vector Machines
are used to determine the actual classiﬁcation.
Hybrid Approaches
Hybrid techniques combine aspects of both static and dynamic analysis. In this section,
we discuss two recent examples of work of this type.
In , the authors propose a framework for classiﬁcation of malware using both
static and dynamic analysis. They deﬁne features of malware using an approach that
they call Malware DNA (Mal-DNA). The heart of this technique is a debugging-based
behavior monitor and analyzer that extracts dynamic characteristics.
In the paper , the authors develop and analyze a tool that they call HDM Analyser. This tool uses both static analysis and dynamic analysis in the training phase, but
performs only static analysis in the testing phase. The goal is to take advantage of the
supposedly superior ﬁdelity of dynamic analysis in the training phase, while maintaining
the eﬃciency advantage of static detection in the scoring phase. For comparison, it is
shown that HDM Analyser has better overall accuracy and time complexity than the
static or dynamic analysis methods in . The dynamic analysis in is based on
extracted API call sequences.
Next, we discuss ROC analysis. We use the area under the ROC curve as one of our
measures of success for the experiments reported in Section 4.
ROC Analysis
A Receiver Operating Characteristic (ROC) curve is obtained by plotting the false positive rate against the true positive rate as the threshold varies through the range of
data values. An Area Under the ROC Curve (AUC-ROC) of 1.0 implies ideal detection,
that is, there exists a threshold for which no false positives or false negatives occur.
The AUC-ROC can be interpreted as the probability that a randomly selected positive
instance scores higher than a randomly selected negative instance . Therefore,
an AUC-ROC of 0.5 means that the binary classiﬁer is no better than ﬂipping a coin.
Also, an AUC-ROC that is less than 0.5 implies that we can obtain a classiﬁer with an
AUC-ROC greater than 0.5 by simply reversing the classiﬁcation criteria.
An examples of a scatterplot and the corresponding ROC curve is given in Figure 2.
The red circles in the scatterplot represent positive instances, while the blue squares
represent negative instances. In the context of malware classiﬁcation, the red circles
are scores for malware ﬁles, while the blue squares represent scores for benign ﬁles.
Furthermore, we assume that higher scores are “better”, that is, for this particular
score, positive instances are supposed to score higher than negative instances.
For a given experiment, the true positive rate is also known as the sensitivity, while
the true negative rate is referred to as the speciﬁcity. Then the false positive rate is
1 −speciﬁcity
sensitivity
Figure 2: Scatterplot and ROC Curve
given by 1 −speciﬁcity.
Note that if we place the threshold below the lowest point in the scatterplot in
Figure 2, then
sensitivity = 1 and 1 −speciﬁcity = 1
On the other hand, if we place the threshold above the highest point, then
sensitivity = 0 and 1 −speciﬁcity = 0
Consequently, an ROC curve must always include the points (0, 0) and (1, 1).
intermediate points on the ROC curve are determined as the threshold passes through
the range of values. For example, if we place the threshold at the yellow dashed line in
the scatterplot in Figure 2, the true positive rate (i.e., sensitivity) is 0.7, since 7 of the 10
positive instances are classiﬁed correctly, while the false positive rate (i.e., 1−speciﬁcity)
is 0.2, since 2 of the 10 negative cases lie on the wrong side of the threshold. This implies
that the point (0.2, 0.7) lies on the ROC curve. The point (0.2, 0.7) is illustrated by the
black circle on the ROC graph in Figure 2. The shaded region in Figure 2 represents
the AUC. In this example, we ﬁnd that the AUC-ROC is 0.75.
PR Analysis
Precision Recall (PR) curves oﬀer an alternative to ROC analysis for scatterplot data .
There are many connections between PR curves and ROC curves,1 but in certain cases,
PR curves can be more informative. In particular, when the nomatch set is large relative
to the match set, PR curves may be preferred.
We deﬁne recall to be the fraction of the match cases that are classiﬁed correctly,
and precision is the fraction of elements classiﬁed as positive that actually belong to the
match set. More precisely,
TP + FN and precision =
1For example, if one curve dominates another in ROC space, it also dominates in PR space, and vice versa.
where TP is the number of true positives, FP is the number of false positives, and FN
is the number of false negatives. Note that recall is the true positive rate, which we
referred to as sensitivity in our discussion of ROC curves. However, precision is not the
same as the false positive rate which is used to compute ROC curves. Also note that TN
does not appear in the formula for recall or precision, and hence true negatives play no
(direct) role in computing the PR curve. Again, this may be useful if we want to focus
our attention on the positive set, particularly when we have a relatively large negative
set. As with ROC analysis, we can use the Area Under the PR Curve (AUC-PR) as a
measure of the success of a classiﬁer.
To generate the PR curve, we plot the (recall, precision) pairs as the threshold varies
through the range of values in a given scatterplot. To illustrate the process, we consider the same data as in the ROC curve example in Section 2.4. This data and the
corresponding PR curve is given here in Figure 3.
Figure 3: Scatterplot and PR Curve
For the threshold that appears in the scatterplot in Figure 3, we have TP = 7,
FP = 2, and FN = 3, and hence
7 + 3 = 0.7 and precision =
7 + 2 ≈0.78
This point is plotted on the right-hand side of Figure 3 and the entire PR curve is given.
In this example, the AUC-PR is about 0.69.
Experiments
In this section, we ﬁrst discuss the tools that we use to extract features from code—both
statically and dynamically. Then we give an overview of the malware dataset used in this
project. Finally, we elaborate on the experiments conducted. In Section 4, we provide
the results of our experiments.
Tools for Dynamic and Static Analysis
IDA Pro is a disassembler that generates highly accurate assembly code from an executable.
It can also be used as a debugger.
IDA is a powerful tool that supports
scripting, function tracing, instruction tracing, instruction logging, etc. In this research,
we use IDA Pro for static analysis, speciﬁcally, to generate .asm ﬁles from .exe ﬁles,
from which opcodes and windows API calls can be extracted. We also use IDA Pro for
dynamic analysis, speciﬁcally, to collect instruction traces from executables.
In addition to IDA Pro, for dynamic analysis we use the Buster Sandbox Analyzer
(BSA). BSA is a dynamic analysis tool that has been designed to determine if a process exhibits potentially malicious behavior. In addition to analyzing the behavior of
a process, BSA keeps track of actions taken by a monitored program, such as registry
changes, ﬁle-system changes, and port changes . The tool runs inside a sandbox which
protects the system from infection while executing malware. The sandbox used by BSA
is known as Sandboxie .
For dynamic analysis, we also experimented with Ether . Ether is an open source
tool that resides completely outside of the target OS, which makes it diﬃcult for a
program to detect that emulation is occurring. This is potentially useful for malware
analysis, since viruses can, in principle, detect a debugger or a virtual environment during
execution. However, for the datasets considered in this paper, we found no signiﬁcant
diﬀerences in the API call sequences generated by BSA and Ether. Since BSA is more
user-friendly, in this research, we exclusively used BSA to generate our dynamic API
call sequences.
The following seven malware families were used as datasets in this research .
Harebot is a backdoor that provides remote access to the infected system. Because of
its many features, it is also considered to be a rootkit .
Security Shield is a Trojan that, like Winwebsec, claims to be anti-virus software.
Security Shield reports fake virus detection messages and attempts to coerce the
users into purchasing software .
Smart HDD reports various problems with the hard drive and tries to convince the
user to purchase a product to ﬁx these “errors”.
Smart HDD is named after
S.M.A.R.T., which is a legitimate tool that monitors hard disk drives (HDDs) .
Winwebsec pretends to be anti-virus software. An infected system displays fake messages claiming malicious activity and attempts to convince the user to pay money
for software to clean the supposedly infected system .
Zbot also known as Zeus, is a Trojan horse that compromises a system by downloading
conﬁguration ﬁles or updates. Zbot is a stealth virus that hides in the ﬁle system . The virus eventually vanishes from the processes list and, consequently,
we could only trace its execution for about 5 to 10 minutes.
ZeroAccess is a Trojan horse that makes use of an advanced rootkit to hide itself.
ZeroAccess is capable of creating a new hidden ﬁle system, it can create a backdoor
on the compromised system, and it can download additional malware .
Table 1 gives the number of ﬁles used from each malware family and the benign dataset.
For our benign dataset, we use the set of Windows System 32 ﬁles listed in Table 2.
Table 1: Datasets
Security Shield
ZeroAccess
Table 2: Benign Dataset
driverquery
eventcreate
eventtriggers
Data Collection
For training and scoring, we use opcode sequences and API calls. For both opcode and
API call sequences, we extract the data using both a static and a dynamic approach,
giving us four observation sequences for each program under consideration. As noted
in Section 2.3, opcode sequences and API call traces have been used in many research
studies on malware detection.
Table 3: Example Disassembly
.text:00401017
call sub 401098
.text:0040101C
.text:0040101E
lea ecx, [esp+24h+var 14]
.text:00401022
push offset xyz
.text:00401027
.text:00401028
call sub 401060
.text:0040102D
add esp, 18h
.text:00401030
test eax, eax
.text:00401032
jz short loc 401045
We use IDA Pro to disassemble ﬁles and we extract the static opcode sequences
from the resulting disassembly. For example, suppose we disassemble an exe ﬁle and
obtain the disassembly in Table 3. The static opcode sequence corresponding to this
disassembly is
call, push, lea, push, push, call, add, test, jz
We discard all operands, labels, directives, etc., and only retain the mnemonic opcodes.
For dynamic opcode sequences, we execute the program in IDA Pro using the “tracing” feature. From the resulting program trace, we extract mnemonic opcodes. Note
that the static opcode sequence corresponds to the overall program structure, while
the dynamic opcode sequence corresponds to the actual execution path taken when the
program was traced.
Microsoft Windows provides a variety of API (Application Programming Interface)
calls that facilitate requests for services from the operating system . Each API call
has a distinct name, a set of arguments, and a return value. We only collect API call
names, discarding the arguments and return value. An example of a sequence of API
calls is given by
OpenMutex, CreateFile, OpenProcessToken, AdjustTokenPrivileges,
SetNamedSecurityInfo, LoadLibrary, CreateFile, GetComputerName,
QueryProcessInformation, VirtualAllocEx, DeleteFile
As with opcodes, API calls can be extracted from executables statically or dynamically. Our static API call sequences are obtained from IDA Pro disassembly. As mentioned in Section 3.1, we use Buster Sandbox Analyser (BSA) to dynamically extract
API calls. BSA allows us to execute a program for a ﬁxed amount of time, and it logs
all API calls that occur within this execution window. From these logged API calls, we
form a dynamic API call sequence for each executable.
Training and Scoring
For our experiments, four cases are considered. In the ﬁrst, we use the static observation sequences for both training and scoring. In the second case, we use the dynamically
extracted data for both training and scoring. The third and fourth cases are hybrid situations. Speciﬁcally, in the third case, we use the dynamic data for training, but the static
data for scoring. In the fourth case, we use static training data, but dynamic data for
scoring. We denote these four cases as static/static, dynamic/dynamic, static/dynamic,
and dynamic/static, respectively.
Our static/static and dynamic/dynamic cases can be viewed as representative of
typical approaches used in static and dynamic detection. The dynamic/static case is
analogous to the approach used in many hybrid schemes. This approach seems to oﬀer
the prospect of the best of both worlds. That is, we can have a more accurate model
due to the use dynamic training data, and yet scoring remains eﬃcient, thanks to the
use of static scoring data. Since the training phase is essentially one-time work, it is
acceptable to spend signiﬁcant time and eﬀort in training. And the scoring phase can
be no better than the model generated in the training phase.
On the other hand, the static/dynamic seems to oﬀer no clear advantage. For completeness, we include this case in our opcode experiments.
We conducted a separate experiment for each of the malware datasets listed in Table 1, for each of the various combinations of static and dynamic data mentioned above.
For every experiment, we use ﬁve-fold cross validation. That is, the malware dataset is
partitioned into ﬁve equal subsets, say, S1, S2, S3, S4, and S5. Then subsets S1, S2, S3,
and S4 are used to train an HMM, and the resulting model is used to score the malware
in S5, and to score the ﬁles in the benign set. The process is repeated ﬁve times, with
a diﬀerent subset Si reserved for testing in each of the ﬁve “folds”. Cross validation
serves to smooth out any bias in the partitioning of the data, while also maximizing the
number of scores obtained from the available data.
The scores from a given experiment are used to form a scatterplot, from which an
ROC curve is generated.
The area under the ROC curve serving as our measure of
success, as discussed in Section 2.4.
In this section, we present our experimental results. We performed experiments with
API call sequences and separate experiments using opcode sequences. All experiments
were conducted as discussed in Section 3.4. That is, diﬀerent combinations of static and
dynamic data were used for training and scoring. Also, each experiment is based on
training and scoring with HMMs, using ﬁve-fold cross validation.
As discussed in Section 2.4, the eﬀectiveness of each experiment is quantiﬁed using
the area under the ROC curve (AUC). In this section, we present AUC results, omitting
the scatterplots and ROC curves. For additional details and results, see .
API Call Sequences
We trained HMM models on API call sequences for each of the malware families in
Table 1. The ROC results are given in Table 4, with these same results plotted in the
form of a bar graph in Figure 4.
Table 4: AUC-ROC Results for API Call Sequence
Security Shield
ZeroAccess
Overall, we see that using dynamic training and testing yields the best results, while
static training and testing is as eﬀective in all cases, except for Harebot and Smart HDD.
Perhaps surprisingly, the hybrid approach of dynamic training with static scoring produces worse results than the fully static case for all families. In fact, the dynamic/static
case fares signiﬁcantly worse than the static/static case for all families except Security
Shield and Zbot.
We also computed PR curves for each of the malware families in Table 1. The AUC-
PR results are given in Table 5, with these same results plotted in the form of a bar
graph in Figure 5.
Table 5: AUC-PR Results for API Call Sequence
Security Shield
ZeroAccess
Next, we provide results for analogous experiments using opcode sequences. Then
we discuss the signiﬁcance of these results with respect to static, dynamic, and hybrid
detection strategies.
Security Shield
ZeroAccess
Dynamic/Dynamic
Static/Static
Dynamic/Static
Static/Dynamic
Figure 4: ROC Results for API Call Sequence
Opcode Sequences
In this set of experiments, we use opcode sequences for training and scoring. As in the
API sequence case, we consider combinations of static and dynamic data for training
and scoring. Also as above, we train HMMs and use the resulting models for scoring.
Before presenting our results, we note that the opcode sequences obtained in the
static and dynamic cases diﬀer signiﬁcantly. In Figure 6 we give a bar graph showing
the counts for the number of distinct opcodes in the static and dynamic cases. From
Figure 6, we see that scoring in the dynamic/static case will be complicated by the
fact that, in general, many opcodes will appear when scoring that were not part of the
training set. While there are several ways to deal with such a situation, when scoring,
we simply omit any opcodes that did not appear in the training set.
Our results for training and scoring on opcode sequences are given in Table 6. The
Security Shield
ZeroAccess
Dynamic/Dynamic
Static/Static
Dynamic/Static
Static/Dynamic
Figure 5: PR Results for API Call Sequence
results in Table 6 are given in the form of a bar graph in Figure 7.
These opcode-based results are generally not as strong as those obtained for API call
sequences. But, as with API call sequences, the best results are obtained in the dynamic/dynamic case. However, unlike the API call sequence models, opcode sequences yield
results that are roughly equivalent in the static/static and the hybrid dynamic/static
case. Additional experimental results can be found in .
Imbalance Problem
In statistical-based scoring, we typically have a primary test that is used to ﬁlter suspect
cases, followed by a secondary test that is applied to these suspect cases. For malware
detection, the primary test is likely to have an imbalance, in the sense that the number
of benign samples exceeds the number of malware samples—possibly by a large margin.
Security Shield
ZeroAccess
Distinct Opcodes
Figure 6: Distinct Opcodes
Table 6: AUC-ROC Results for Opcode Sequences
Security Shield
ZeroAccess
In the secondary stage, we would expect the imbalance to be far less signiﬁcant. Due to
their cost, malware detection techniques such as those considered in this paper would
most likely be applied at the secondary stage. Nevertheless, it may be instructive to
consider the eﬀect of a large imbalance between the benign and malware sets. In this
section, we consider the eﬀect of such an imbalance on our dynamic, static, and hybrid
techniques.
Security Shield
ZeroAccess
Dynamic/Dynamic
Static/Static
Dynamic/Static
Static/Dynamic
Figure 7: ROC Results for Opcode Sequences
We can simulate an imbalance by simply duplicating each benign score n times.
Assuming that the original number of scored benign and malware samples are equal,
a duplication factor of n simulates an imbalanced data set where the benign samples
outnumber the malware samples by a factor of n. Provided that our original benign set
is representative, we would expect an actual benign set of the appropriate size to yield
scores that, on average, match this simulated (i.e., expanded) benign set.
However, the AUC-ROC for such an expanded benign set will be the same as for the
original set. To see why this is so, suppose that for a given threshold, we have TP = a,
FN = b, FP = c, and TN = d. Then (x, y) is a point on the ROC curve, where
c + d and y = TPR =
Now suppose that we duplicate each element of the negative (i.e., benign) set n times.
Then for the same threshold used to compute (1), we have TP = a, FN = b, FP = nc,
and TN = nd, and hence we obtain the same point (x, y) on the ROC curve for this
modiﬁed dataset.
In contrast, for PR curves, using the same threshold as above we have
a + b and precision =
When we expand our dataset by duplicating the benign scores n times, this threshold
a + b and precision =
Consequently, we see that simulating an imbalance in this way will tend to ﬂatten the
PR curve, and thereby reduce the AUC-PR. In addition, the precise degree of ﬂattening
will depend on the relative distribution of the malware and benign scores.
We have shown that the AUC-ROC provides no information on the eﬀect of an
imbalance between the malware and benign sets. In some sense, this can be viewed as
a strength of the AUC-ROC statistic, although it does render it useless for analyzing
the eﬀect of imbalanced data. On the other hand, the AUC-PR is a useful statistic for
comparing the eﬀect of an imbalance between these two sets. Consequently, we use the
AUC-PR in this section to determine the eﬀect of a (simulated) imbalance between the
malware and benign sets. We consider the API sequence results, and we duplicate each
benign score by a factor of n = 1, n = 10, n = 100, and n = 1000, and plot the results
on a logarithmic (base 10) scale. The resulting AUC-PR values for each of the four cases
(i.e., dynamic/dynamic, static/static, dynamic/static, and static/dynamic) are plotted
as line graphs in Figure 8.
The results in Figure 8 suggest that we can expect the superiority of the a fully
dynamic approach, to increase as the imbalance between the benign and malware sets
grows. In addition, the advantage of the fully static approach over our hybrid approaches
increases as the imbalance increases. We also see that even in those cases where the dynamic/static approach is initially competitive, it fails to remain so for a large imbalance.
And ﬁnally, the overall weakness of the static/dynamic approach is even more apparent
from this PR analysis.
Discussion
The results in this section show that for API calls and opcode sequences, a fully dynamic
strategy is generally the most eﬀective approach. However, dynamic analysis is generally
costly in comparison to static analysis. At the training phase, this added cost is not a
signiﬁcant issue, since training is essentially one-time work that can be done oﬄine. But,
at the scoring phase, dynamic analysis would likely be impractical, particularly where it
is necessary to scan a large number of ﬁles.
In a hybrid approach, we might attempt to improve the training phase by using dynamic analysis while, for the sake of eﬃciency, using only a static approach in the scoring
phase. However, such a strategy was not particularly successful in the experiments considered here. For API call sequences, we consistently obtained worse results with the
Benign Expansion Factor n
Security Shield
ZeroAccess
Benign Expansion Factor n
Security Shield
ZeroAccess
(a) Dynamic/Dynamic
(b) Static/Static
Benign Expansion Factor n
Security Shield
ZeroAccess
Benign Expansion Factor n
Security Shield
ZeroAccess
(c) Dynamic/Static
(d) Static/Dynamic
Figure 8: AUC-PR and Imbalanced Data (API Calls)
hybrid dynamic/static as compared to a fully static approach. For opcode sequences,
the results were inconsistent—in four of the cases, the hybrid dynamic/static method
was marginally better than the fully static approach, but for one case it was signiﬁcantly
Attempting to optimize a malware detection technique by using hybrid analysis is
intuitively appealing. While such a hybrid approach may be more eﬀective in certain
cases, our results show that this is not likely to be generically the case. Consequently,
when hybrid approaches are proposed, it would be advisable to test the results against
comparable fully dynamic and fully static techniques.
Conclusion and Future Work
In this paper, we tested malware detection techniques based on API call sequences and
opcode sequences. We trained Hidden Markov Models and compared detection rates for
models based on static data, dynamic data, and hybrid approaches.
Our results indicate that a fully dynamic approach based on API calls is extremely
eﬀective across a range of malware families. A fully static approach based on API calls
was nearly as eﬀective in most cases. Our results also show that opcode sequences can
be eﬀective in many cases, but for some families the results are not impressive. These
results likely reﬂect the nature of obfuscation techniques employed by malware writers.
That is, current obfuscation techniques are likely to have a signiﬁcant eﬀect on opcode
sequences, but little attention is paid to API calls. With some additional eﬀort, API
call sequences could likely be obfuscated, in which case the advantage of relying on API
call sequences for detection might diminish signiﬁcantly.
Examples of relatively complex and involved hybrid techniques have recently appeared in the literature. However, due to the use of diﬀerent data sets, diﬀerent measures
of success, and so on, it is often diﬃcult, if not impossible, to compare these techniques
to previous (non-hybrid) work. Further, the very complexity of such detection techniques often makes it diﬃcult to discern the actual beneﬁt of any one particular aspect
of a technique.
The primary goal of this research was to test the tradeoﬀs between
static, dynamic, and hybrid analysis, while eliminating as many confounding variables
as possible.
The experimental results presented in this paper indicate that a straightforward
hybrid approach is unlikely to be superior to fully dynamic detection.
And even in
comparison to fully static detection, our hybrid dynamic/static approach did not oﬀer
consistent improvement. Interestingly, the impractical static/dynamic hybrid approach
was superior in some cases (by some measures). These results are, perhaps, somewhat
surprising given the claims made for hybrid approaches.
Of course, it is certain that hybrid techniques oﬀer signiﬁcant beneﬁts in some cases.
But, the work here suggests that such claims should be subject to careful scrutiny. In
particular, it should be made clear whether improved detection is actually due to a
hybrid model itself, or some other factor, such as the particular combination of scores
used. Furthermore, it should be determined whether these beneﬁts exist over a wide
range of malware samples, or whether they are only relevant for a relatively narrow
range of malware.
Future work could include a similar analysis invovling additional features beyond
API calls and opcodes. A comparison of scoring techniques other than HMMs (e.g.,
graph-based scores, structural scores, other machine learning and statistical scores) and
optimal combinations of static and dynamic scores (e.g., using Support Vector Machines)
would be worthwhile. Finally, a more in-depth analysis of imbalance issues in this context
might prove interesting.