Prediction of Dynamical Systems by Symbolic Regression
Markus Quade and Markus Abel
Universit¨at Potsdam, Institut f¨ur Physik und Astronomie,
Karl-Liebknecht-Straße 24/25, 14476 Potsdam, Germany and
Ambrosys GmbH, David-Gilly Straße 1, 14469 Potsdam, Germany
Kamran Shaﬁand Robert K. Niven
School of Engineering and Information Technology,
The University of New South Wales, Canberra ACT 2600, Australia
Bernd R. Noack
Laboratoire d’Informatique pour la M´ecanique et les Sciences de
l’Ing´enieur LIMSI-CNRS, BP 133, 91403 Orsay cedex, France and
Institut f¨ur Str¨omungsmechanik, Technische Universit¨at Braunschweig,
Hermann-Blenk-Straße 37, 38108 Braunschweig, Germany
 
We study the modeling and prediction of dynamical systems based on conventional models derived from measurements. Such algorithms are highly desirable in situations where the underlying
dynamics are hard to model from physical principles or simpliﬁed models need to be found. We
focus on symbolic regression methods as a part of machine learning. These algorithms are capable
of learning an analytically tractable model from data, a highly valuable property. Symbolic regression methods can be considered as generalized regression methods. We investigate two particular
algorithms, the so-called fast function extraction which is a generalized linear regression algorithm,
and genetic programming which is a very general method. Both are able to combine functions in
a certain way such that a good model for the prediction of the temporal evolution of a dynamical
system can be identiﬁed. We illustrate the algorithms by ﬁnding a prediction for the evolution of a
harmonic oscillator based on measurements, by detecting an arriving front in an excitable system,
and as a real-world application, the prediction of solar power production based on energy production
observations at a given site together with the weather forecast.
INTRODUCTION
The prediction of the behavior of dynamical systems
is of fundamental importance in all scientiﬁc disciplines.
Since ancient times, philosophers and scientists have tried
to formulate observational models and infer future states
of such systems. Applications include topics as diverse
as weather forecasting , the prediction of the motion
of the planets , or the estimation of quantum evolution .
The common ingredient of such systems - at
least in natural sciences - is the existence of an underlying mathematical model which can be applied as the
predictor.
In recent years, the use of artiﬁcial intelligence (AI) or machine learning (ML) methods have complemented the formulation of such mathematical models
through the application of advanced data analysis algorithms that allow accurate estimation of observed dynamics by learning automatically from the given observations and building models in terms of their own modelling languages. Artiﬁcial Neural Networks (ANNs) are
one example of such techniques that are popularly applied to model dynamic phenomena.
ANNs are structured as networks of soft weights organized in layers or
so-called neurons or hidden units. One problem of ANN
type approaches is the diﬃcult-to-interpret black-box nature of the learnt models.
Symbolic regression-based
approaches, such as Genetic Programming (GP), provide alternative ML methods that are recently gaining
increasing popularity. These methods, similar to other
ML counterparts, learn models from observed data and
act as good predictors of the future states of dynamical
systems. Their added advantages over other methods include the interpretable nature of their learnt models and
a ﬂexible and weakly-typed modelling language that
allows them to be applied to a variety of domains and
Undoubtedly, the methods used most often in ML are
neural networks.
These involve deep learning, in the
sense that several layers are used and interpreted as
the organization of patterns, as one imagines the human
brain to work. In the present study, involving deterministic systems, we want to use a certain branch of ML,
namely symbolic regression.
This technique joins the
classical, equation-oriented approach with its computerscientiﬁc upstart. In this publication we do not present
any major improvements in the algorithms; rather we
demonstrate how one can apply symbolic regression to
identify and predict the future state of dynamical systems.
Symbolic regression algorithms work by exploring a
function space, which is generally bounded by a preselected set of mathematical operators and operands (variables, constants, etc.), using a population of randomly
generated candidate solutions. Each candidate solution
encoded as a tree essentially works as a function and is
 
evaluated based on its ﬁtness or in other words its ability
to match the observed output. These candidate solutions
are evolved using a ﬁtness-weighted selection mechanism
and diﬀerent recombination and variation operators. One
common problem in symbolic regression is the bloating
eﬀect which is caused by excessive lengthening of individual solutions or ﬁlling of the population by large number of solutions with low ﬁtness. In this work we use a
multi-objective function evaluation mechanism to avoid
this problem by including minimizing the solution length
as an explicit objective in the ﬁtness function.
Symbolic regression subsumes linear regression, generalized linear regression, and generalized additive models
into a larger class of methods. Such methods have been
used with success to infer equations of dynamical systems directly from data . One problem with deterministic chaotic systems is the sampling of phase space
using embedding. For a high-dimensional system, this
leads to prohibitively long sampling times. Typical reconstruction methods use delay coordinates and the associated diﬀerences, this results in mapping models for
the observed systems. Mathematically, diﬀerential coordinates are better suited for modelingbut they are not
always accessible from data. Both approaches, diﬀerence
and diﬀerential embedding, are discussed in with numerical methods to obtain suitable diﬀerential variables
from data. Modern methods like diﬀusion maps 
or local linear embedding , including the analysis of
stochastic systems, circumvent the curse of dimensionality by working directly on the manifold of the dynamical
Apart from prediction and identiﬁcation of dynamical
systems , the symbolic regression approach has
been used recently for the control of turbulent ﬂow systems . In that application, we demonstrate how
to ﬁnd the symbolic equations in a very general form
combined with subsequent automatic simpliﬁcation and
multiobjective optimization.
This yields interpretable
equations of a complexity that we can select.
open-source Python packages for the analysis. Symbolic
regression is conducted using an elastic net method provided by the fast function extraction package (FFX) for
quick tests, and the more general, but usually slower
method implemented as a genetic programming algorithm (GP) based on the deap package. Subsequent simpliﬁcation is obtained using sympy. Of course, any other
programming framework with similar functionality will
For a systematic study we examine numericallygenerated data from a harmonic oscillator as the simplest
system to be predicted, and a more involved system of
coupled FitzHugh-Nagumo oscillators, which are known
to produce complex behaviour and may serve as a very
simple model for neurons. We investigate the capacity of
the ML approach to detect an incoming front of activity,
and give exact equations for the regression. We compare
diﬀerent sampling and spatio-temporal embedding methods, and discuss the results: it is shown that a space-time
embedding has advantages over time-only and space-only
embedding.
Our ﬁnal example concerns a real-world application,
the short-term and medium-term forecasting of solar
power production. In principle, this could be achieved
trivially by a high-resolution weather forecast and knowledge of the transfer of solar energy to solar cells, a very
well-understood process .
However, such a highly
resolved weather forecast does not exist, because it is
prohibitively expensive: even the largest meteorological
computers are still unable to compute the weather on
small spatial scales, let alone with a long time horizon
at high accuracy. As the dynamical systems community
identiﬁed a long time ago, this is mainly due to uncertainties in the initial conditions, as demonstrated by the
celebrated Lorenz equations . Consequently, we follow a data-based approach and improve upon weather
predictions using local energy production data as a time
series. We are aware that use of the full set of weather
data will improve the reported forecast, but increasing
the resolution is not our interest here, rather the proof of
concept of the ML method and its applicability to realworld problems.
The rest of this paper is organized as follows.
Sec. III we discuss the methods and explain our approach.
This section is followed by a longer section IV where
results are presented for the above-mentioned example
systems. We end the paper with a summary and conclusions, Sec. V.
In the ﬁeld of dynamical systems (DS), and in particular nonlinear dynamical systems, reconstruction of the
characteristics of an observed system from data has been
and is a fundamental scientiﬁc topic.
In this regard, one can distinguish parameter and
structure identiﬁcation. We ﬁrst discuss the existing literature on parameter identiﬁcation which is easier in that
there is an established mathematical framework to ﬁt
coeﬃcients to known curves representing experimental
data, which in turn result from known dynamics. This
can be conducted for linear or non-linear functions. For
deterministic systems, with the advent of modern computers, quantities like fractal dimensions, Lyapunov exponents and entropies can also be computed to make systems comparable in dynamics . These analyses
further allow the rough characterization of the type and
number of orbits of a DS . On the other hand, embedding techniques have been developed to reconstruct
the dynamics of a high-dimensional system from lowerdimensional time series .
These techniques have a number of limitations with
respect to accuracy and the amount of data needed for
making good predictive models. A chaotic system with
positive Lyapunov exponents has a prediction horizon
which depends heavily on accuracy and precision of the
data, since chaos “destroys” information. This can be
seen very clearly by the shift map example . However a system on a regular orbit, even marked with complicated equations, might be predicted accurately. For
high-dimensional systems, one needs a large amount of
data to address the “curse of dimensionality” .
fact it can be shown that for each dimension, the number of data needed increases on a power-law basis .
Eventually, the direct inference of the underlying equations of motion from data can be approached using regression methods, like Kalman ﬁltering, general linear
models (GLM), generalized additive models (GAM), or
more general schemes, see and references therein.
Apart from the equations themselves, partial derivatives
often have to be estimated , which is an additional
problem for low-precision data
We also consider structure identiﬁcation, which as
mentioned above is a more complicated task. In the last
10-15 years, powerful new methods from computer science have been applied to this purpose. This includes numerous studies on diﬀusion maps, local linear embedding,
manifold learning, support vector machines, artiﬁcial
neural networks, and symbolic regression .
Here, we focus on symbolic regression. It must be emphasized that most methods are not unique and their success
can only be tested based on their predictive power.
Symbolic Regression
One drawback of many computational-oriented methods is the lack of equations that can be analyzed mathematically in the neighborhood of analyzed trajectories.
Symbolic regression is a way to produce such equations.
It includes methods that identify the structure or parameters of the searched equation or both of them simultaneously with respect to objective functions Γi.
This means that methods like GLM, or GAM are contained in such a description. A recent implementation
of GLMs is Fast Function Extraction (FFX) , which
is explained brieﬂy below.
Genetic programming, explained in detail below, is another intuitive method and
often used for symbolic regression. Here, the algorithm
searches the function space through random combinations and mutations of functions, chosen from a basic set
of equations.
Symbolic regression is supposed to be form free and
thus unbiased towards human perception. However, human knowledge enters in the meta-rules imposed on the
model through the basic building blocks and rules on how
they can be combined. Thus, the optimal model is always
conditioned on the underlying meta-rules.
Genetic Programming
Genetic programming is an evolutionary algorithm to
ﬁnd an optimal algorithm or program. The term “programming” in optimization is used synonymously with
“plan” or algorithm. It was used ﬁrst by Dantzig, the inventor of linear programming, at a time when computer
programs did not exist as we know them today . The
algorithm seeks an optimal algorithm, in our case a function, using evolutionary, or “genetic” strategies, as explained below. The pioneering work was established by
We can brieﬂy describe it as follows: in GP we
can represent formulae as expression trees, such as that
shown in Fig. 1. Non-terminal nodes are ﬁlled with elements from a basic function set deﬁned by the meta-rules.
Terminal nodes consist of variables or parameters. Given
the optimization problem
f ∗= argopt
we seek the optimal solution f ∗through optimizing (minimizing or maximizing, or for some cost functionals, ﬁnding the supremum or inﬁmum) the ﬁtness (or cost) functional Γ. To ﬁnd the optimal solution, GP uses a whole
population of candidate solutions in parallel which are
evolved iteratively through ﬁtness proportionate selection, recombination and mutation operations. The initial generation is created randomly. Afterwards, the algorithm cycles through the following loop until it reaches
its convergence or stopping criteria:
Based on the current generation Gt, a
new set of size λ of alternative candidate solutions,
the oﬀspring Ot, are selected.
Several problemdependent operators are used for this tweaking
step, e.g. changing parts of a candidate solution
(mutation) or combining two solutions into two new
ones (crossover). These tweaking operations may
include selection pressure, so that the “ﬁtter” solutions are more likely to produce oﬀspring.
• evaluate: The oﬀspring Ot are evaluated, i.e. their
ﬁtness is calculated.
• select: Based on the ﬁtness value, members of the
next generation are selected.
This scheme ﬁts the requirements of symbolic regression.
Mutation is typically conducted by replacing a random
subtree by a new tree.
Crossover takes two trees and
swaps random subtrees between them. This procedure is
illustrated in Fig. 1. The ﬁtness function uses a typical
error metric, e.g. least squares or normalized root mean
squared error.
The random mutations sample the vicinity of their parent solution in function space. As a random mutation
could likely lead to less optimal solution, it does not ensure a bias towards optimality. However, this is achieved
by the selection, because it ensures that favourable mutations are kept in the set while others are not considered
in further iterations.
By design and when based on similar meta-rules, GP
includes other algorithms like GLMs or linear programming .
Figure 1. Illustration of the genetic programming mutation
and crossover. The upper left expression tree describes the
function f(x, y) =
0.981 + sin(x). Mutation is conducted
by picking a random subtree, here the single terminal node
0.981 and replacing it with a new random expression tree.
Similarly, the crossover operator (right) takes two expression
trees and swaps two random subtrees.
Algorithm 1 Top level description of a GP algorithm
procedure main
G0 ←random(λ)
evaluate(G0)
Ot ←breed(Gt−1, λ)
evaluate(Ot)
Gt ←select(Ot, Gt−1, µ)
until t > T or Gt = good()
end procedure
FFX and the Elastic Net
Here we brieﬂy summarize the FFX algorithm of Mc-
Conaghy et al. . This is a symbolic regression algorithm based on a combined generalized linear model and
elastic net approach:
f(⃗x) = a0 +
where {ai} are a set of coeﬃcients to be determined,
and {φi} are an overdetermined set of basis functions
described by an heuristic, simplicity-driven set of rules
highest allowed polynomial exponent, products,
non-linear functions, . . .).
In the elastic method, a least squares criterion is used
to solve the ﬁtting problem.
To avoid overﬁtting, i.e.
high model sensitivity on training data, two regulating
terms are added: The ℓ1, and ℓ2 norms of the coeﬃcient
vector. The ℓ1 norm favors a sparse model (few coeﬃcients) and simultaneously avoids large coeﬃcients. The
ℓ2 norm ensures a more stable convergence as it allows
for several, possibly correlated variables instead of a single one. The resulting objective function written in its
explicit form reads :
⃗a∗= argmin
||y −f(⃗x,⃗a)||2 +λρ||⃗a||1 +(1−ρ)λ||⃗a||2 (3)
where y are the data, λ ≥0 the regularization weight
and ρ ∈ is the mixing between ℓ1 and ℓ2 norms.
A beneﬁt of the regularized objective function is that it
implicitly gives rise to models with diﬀerent complexity,
i.e. diﬀerent number of bases NB.
For large values of λ, the predicted coeﬃcients will
all be zero. Reducing λ will result in more complicated
combinations of non-zero coeﬃcients. For every point on
the (λ, ρ)-grid, the “elastic net”, one can obtain a single
optimal model using a standard solver like coordinate
descent to determine the optimal coeﬃcients ⃗a∗.
A small change in the elastic net parameters leads to
a small change in ⃗a∗such that one can use the already
obtained solution of a neighboring grid point to restart
coordinate descent with the new parameters.
For the obtained models we can calculate the normalized root mean-squared error and model complexity
(number of used basis functions). The FFX algorithm
is based purely on deterministic calculations. Hence its
runtime compared to a similar GP algorithm is signiﬁcantly shorter. However, the meta-rules are more stringent.
Multiobjective Fitness
As mentioned above, the solution of the regression
problem is not unique in general.
A major factor
which motivates symbolic regression is its comprehensible
white-box nature opposed to the black-box nature of, for
example neural networks. Invoking Ockhams razor (lex
parsimoniae), a simple solution is considered superior to
a complicated one as it is more easy to comprehend. In addition, more complicated functions are prone
to overﬁtting. This means that complexity should be a
criterion in the function search, such that more complex
functions are considered less optimal. We therefore seek
a solution which satisﬁes two objectives.
Comparing solutions by more than one metric Γi is
not straightforward. One possible approach is to weight
these metrics into one objective Γ:
making diﬀerent candidate solutions easily comparable.
The elastic net Eq. 3 uses such a composite metric. However, a priori it is assumed that there is a linear trade-oﬀ
between the individual objectives. This has three major
• One needs to determine suitable (problem dependent) wi.
• One does not account for non-linear trade-oﬀs (e.g.
all-or-nothing in one objective).
• Instead of single optimal solution there may be a
set of optimal solutions deﬁning the compromise
between conﬂicting objectives (here error vs complexity).
The optimal set is also called the Pareto-front. This is the
set of non-dominated candidate solutions, i.e. candidate
solutions that are not worse than any other solution in
the population when compared on all objectives.
the FFX algorithm, explained above, one can obtain the
(Pareto-) optimal set of candidate solutions by sorting
the models. The mapping from parameter space to the
Pareto-optimal set is called Pareto-ﬁltering.
Interestingly, the concept of non-domination already
partly solves the sorting problem in higher dimensions as
it maps from RN to M ordered one-dimensional manifolds: Candidate solutions in the Pareto-front are of rank
0. Similarly, one can ﬁnd models of rank 1, i.e. all models
that are dominated only once (or in other words the nondominated models of all models taken out of the original
Pareto-front).
Model 1 f1 can be said to be better than Model 2 f2
if its rank is lower:
f1 ≻f2 ⇐= rank(f1) < rank(f2)
To compare models of the same rank, one has to introduce an additional heuristic criterion, for which there are
several choices . Usually the criterion promotes
uniqueness of a candidate solution to ensure diversity of
the population to avoid becoming trapped in a local minimum. As the uniqueness of a solution may depend on its
representation and is usually costly to compute, often its
projection to ﬁtness space is used. This is conducted to
ensure an eﬀective spread of candidate solutions on the
Pareto-front.
For example, the non-dominated sorting algorithm II
(NSGAII) uses a heuristic metric called crowding
distance or sparsity to compare two models of the same
rank. The scaled Euclidean distance in ﬁtness space to
the neighboring models is used to describe the uniqueness
of a model. For NSGAII we have:
rank(f1) < rank(f2)
rank(f1) = rank(f2) and
sparsity(f1) > sparsity(f2)
Out of the current generation and their oﬀspring Gt ∩Ot
the µ best, in terms of ≻, solutions are chosen for the next
generation Gt+1. This selection method ensures elitism,
i.e. the best solutions found so far are carried forward in
next generations. Looking at the high-level description
in Algorithm 1, Gt can be seen as an archive which keeps
old members as long as they are not dominated by a new
solution from the current oﬀspring Ot.
The diﬀerent selection strategies were ﬁrst studied in
the context of genetic algorithms, but more recently they
have been successfully applied to symbolic regression [38,
OUR GP SETUP
applications
{+, ∗, −, /, sin, cos, exp, log, √, 2}. All discontinuities are
deﬁned as zero. Our terminal set consists of the input
data xi as well as symbolic constants ci which are determined during evaluation. We set up our multiple objectives as follows: the algorithm runs until the error of
the most accurate model is below 0.1%, or for 100 generations. The population size µ as well as the number
of oﬀspring per generation λ is set to 500. The depth
of individuals of the initial populations varies randomly
between 1 and 4.
With equal probability we generate
the corresponding expression trees where each leaf might
have a diﬀerent depth or each leaf is forced to have the
same depth. For mutation we randomly pick a subtree
and replace it with a new tree, again using the half and
half method, with minimum size 0 and maximum size
2. Crossover is conducted by randomly picking a subtree
each and exchanging them. Our breeding step is composed of randomly choosing two individuals from the current population, performing crossover on them with probability p = 0.5 and afterwards always mutating them.
Our multiobjective cost functional has the following components
Γ1 = NRMSE (y, ˆy) =
(yi −ˆyi)2
ymax −ymin
where NRMSE is the normalized root mean-squared error
of the observed data y and its predictor ˆy = f(⃗x), and
Γ2 = size(f)
is simply the total number of nodes in the expression
tree f. Selection is conducted according to NSGAII. In
this paper, a model is called accurate if its error metric
Γ1 is small, where “small” depends on the context. For
example, numerical data might be modeled accurately if
Γ1 ≤0.05 and measured data might be modeled accurately if Γ1 ≤0.20.
Similarly a model is complicated
if its complexity Γ2 is relatively large. “Good” and its
comparatives are to be understood in the sense of ≻.
During the generation of the initial population and selection, we force diversity by prohibiting identical solutions.
It is very unlikely to randomly create identical
solutions. However, oﬀspring may be nearly identical in
structure as well as ﬁtness and consequently a crossover
between parent and child solution may produce an identical grandchild solution. The probability of such an event
grows exponentially with the number of identical solutions in a population and therefore it reduces the diversity of the population in the long-term risking a premature convergence of the algorithm. Thus, by prohibiting identical solutions, the population will have a transient period until it reaches its maximum capacity. This
will also reduce the eﬀective number of oﬀspring per generation. This change reduces the probability of becoming
trapped in a local minimum because of a steady state in
the evolutionary loop.
Our main emphasis is the treatment of the model parameters ci. In standard implementations, e.g. the already mentioned , the parameters are mutated
randomly, like all other nodes. Here, using modern computational power we are able to use traditional parameter
optimization algorithms. Thus, the calculation of Γ1 becomes another optimization task given the current model
Γ1 = NRMSE (y, f(⃗x,⃗c∗))
⃗c∗= argmin
NRMSE (y, f(⃗x,⃗c))
The initial guess for ci is either inherited or set to one.
Thus, we eﬀectively have two combined optimization layers. Each run is conducted using 10 restarts of the algorithm. The Pareto front is the joined front of the individual runs. Finally, we can use algebraic frameworks
to simplify the obtained formulae. This is useful, since a
formula (phenotype, macrostate) may be represented by
many diﬀerent expression trees (genotypes, microstates).
CASE STUDIES
We present here results for three systems with increasing diﬃculty: ﬁrst, we demonstrate the principles using
a very simple system, the harmonic oscillator; second, we
infer a predictive model for a set of coupled oscillators;
and ﬁnally we show how we can predict a very applied
system, namely the power production from a solar panel.
For the ﬁrst two examples we use numerically produced
data, where we have full control over the system, while
for the demonstration of applicability we use data from
a small solar power station .
Harmonic Oscillator
In this subsection we describe the ﬁrst test of our
methodology: an oscillator should be identiﬁed correctly
and a accurate prediction must be possible.
Consequently, we investigate the identiﬁcation of a prediction model, not necessarily using a diﬀerential formalism.
This might be interpreted as ﬁnding an approximation to
the solution of the underlying equation by data analysis.
A deep investigation of the validity of the solution for
certain classes of systems is rather mathematical and is
beyond the scope of this investigation.
Our system reads
where x and y are the state variables and ω is a constant. We use the particular analytical solution x(t) =
x0 sin(ωt), y(t) = x0ω cos(ωt). The prediction target is
x(t + τ), where τ is a time increment.
Since the analytical solution is a linear combination of
the feature inputs, just N = 2 data points are needed
to train the model. This holds for inﬁnite accuracy of
the data and serves as a trivial test for the method. In
general, a learning algorithm is “trained” on some data
and the validity of the result is tested on another set,
that is as independent as possible. That way, overﬁtting
is avoided. For the same reason one needs to deﬁne a
stop criterion for the algorithm, e.g. the data accuracy is
10−5, it is useless and even counterproductive to run an
algorithm until a root mean square error of 10−10 (the
cost function used here) is achieved. For the example under consideration, we stop the training once the training
error is smaller than 1
Typically, a realistic scenario should include the eﬀect
of noise, e.g. in the form of measurement uncertainties.
We consequently add “measurement” Gaussian noise
with mean zero and variance proportional to the signal amplitude: ξ1 ∼N(0, (σx0)2), ξ2 ∼N(0, (σx0ω)2),
hence ˜x = x + ξ1, ˜y = y + ξ2. The training and testing
data sets were created as follows: the data are generated
between [0, tmax]. Out of the ﬁrst half, we chose N values
at random for training. For testing purposes we use the
second half. We study the parameter space (N, τ, σ) and
average the testing errors over 10 realizations for each
parameter set. In Fig. 2 we display the normalized root
mean squared error of the prediction using FFX (measured against the noisy data) as a function of the noise
amplitude. Given x(t) and y(t) the analytical solution
for the non-noisy system is just a linear combination, i.e.
x(t + τ) = cos(ωτ)x(t) + sin(ωτ)
y(t), and has a complexity of two. During training we aim for a NRMSE of 1%.
Thus, we ﬁnd the analytical solution in the limit of small
noise amplitude σ, see Fig. 2 and Fig 4. Strong noise
covers the signal and thus the error saturates.
The length of the analyzed data is another important
parameter: typically one expects convergence of the error ∼
N for more data. A “vertical” cut through the
data in Fig. 2 is shown in Fig. 3. The training set length
N has a much lower impact than the classical scaling
suggests. Crucial for this scaling is the form free structure as well as the heuristic which is used to select the
Average Error
Figure 2. Harmonic oscillator study: NRMSE (6) versus noise
level σ for diﬀerent training set lengths N and ﬁxed τ = 10.
Suﬃciently small noise does not worsen the predictability, i.e.
the prediction algorithm stops at the target training NRMSE
of 1%. After 0.3 the error does not increase further, since the
noise covers the signal completely.
ﬁnal model. For demonstration purposes, we chose the
most accurate model on the testing set, which is of course
vulnerable to overﬁtting. The average complexity, calculated by Eq. (7) of the ﬁnal model as a function of the
noise amplitude, is shown in Fig 4. As evident we can
recover the three regimes of Fig. 2. For small noise, the
analytical and numerical solution agree. In the intermediate regime we ﬁnd on average more complex models
(in comparison to the analytical solution). Very strong
noise hides the signal and a good prediction is impossible. The optimal solution tends to be single constant ,
i.e. for high σ the complexity tends to smaller values as
seen in Fig 4. The prediction error has two components:
1) given a structure, noisy data will lead to uncertain parameters and 2) due to the form-free nature of symbolic
regression, noisy data will also lead to an uncertain structure, increasing the uncertainty in the parameters. Thus,
ﬁnal model selection has to be performed carefully, especially when dealing with noisy data. A detailed study is
presented for the example of coupled oscillators.
Coupled Oscillators
The harmonic oscillator is an easy case to treat with
our methods. Now, we extend the analysis to add a spatial dimension. We study a model of FitzHugh-Nagumo
oscillators on a ring. The oscillators are coupled and
generate traveling pulse solutions. The model was originally derived as a simpliﬁcation of the Hodgkin-Huxley
model to describe spikes in axons , and serves nowadays as a paradigm for excitable dynamics.
spiky behavior is used as an abstraction of a front, observed in real world applications like the human brain,
modeled by connected neurons, or a wind power plant
network where fronts of diﬀerent pressure pass through
the locations of the wind power plants. The aim is to
training set length N
Average Error
Figure 3. Harmonic oscillator study: In solid blue: normalized root mean squared error vs training set length N for
Dashed green:
The error decreases
slightly with N, but the scaling is much less rapid than 1/
Average complexity
Figure 4. Harmonic oscillator study: Average complexity of
the chosen model vs. noise amplitude σ. The form-free structure allows for overﬁtting. For small noise, the true solution is
found with complexity 2, for higher noise levels, the algorithm
starts to ﬁt the noise and more terms are added, reﬂected by
a higher complexity.
show that temporal and/or spatial information on the
state of some network sites enables an increase in predictability of a chosen site or eventually (if there are
waves in the network) to the front detection. The model
for the ith oscillator is:
˙vi = vi −v3
3 −wi + Ii + D
Aij(vj −vi)
˙wi = ε(vi + a −bwi) .
where vi and wi, i, j = 1, . . . , N, denote the fast and
slower state variables, Ii is an external driving force, D
is the coupling strength parameter, and Aij ∈{0, 1} describes the coupling structure between nodes i and j. The
constant parameters ε, a and b determine the dynamics
of the system as ε−1 is the time scale of the slower “recovery variable”, and a, and b set the position of the ﬁxed
point(s). For Aij we choose diﬀusive coupling on a ring,
periodic boundary conditions.
With the external
Figure 5. Space-time plot of the pulse evolution. vi is color
The front velocity is vf = 1.28.
Pulse width (full
width half maximum) τP = 8.4
current Ii we can locally pump energy into the system to
create two pulses which will travel with the same speed
but in opposite directions, annihilating when they meet.
Using diﬀerent spatio-temporal sampling strategies,
the aim is to detect and predict the arrival of a spike
train at a location far enough away from the excitation
center (i.e. farther than the wave train diameter). We
mark this special location with the index zero.
Note that we do not aim to ﬁnd a model for a spatiotemporal diﬀerential equation, since this would involve
the estimation of spatial derivatives, which in turn require a ﬁne sampling. This is deﬁnitely not the scope
here. Rather we focus on the more application-relevant
question to make a prediction based on an equation.
The construction of the data set was similar to the
single oscillator case: sensors were restricted to the vi
variables. We can record the time series of v0 and use
time delayed features for the prediction. Another option
is to use information from non-local sensors.
We prepare and integrate the system as follows: we
consider a ring of N = 200 oscillators. The constants are
chosen as a = 0.7, b = 0.8, τ = 12.5 and D = 1. The system is initialized with vi(0) = 0 and wi(0) = −1.5. With
the characteristic function χT (x) = 1 if x ∈T else 0 we
can write the space and time dependent perturbation as
Ii(t) = 5χt−⌊t⌋≤0.4(t)χt≤40(t)χi∈{−50,−49}(i). This periodic perturbation leads to a pair of traveling waves. The
data were sampled at times tn = n∆t with ∆t = 0.1.
The system has multiple time scales: two are associated
with the on-site FitzHugh-Nagumo oscillator (τfast = 1,
ε), while two more are due to diﬀusive coupling (τDiff = D) and perturbation (τP ert behaves as
Ii(t) described above). The temporal width of the pulse
traveling through a particular site, τP
= 8.4, corresponds to the full width half maximum of the pulse. In
Fig. 5 we show the evolution of the oscillator network.
The state of vi is color-coded. The horizontal width of
the yellow stripe corresponds to the spatial pulse width
ξ ≃10.75. The speed of the spike or front is consequently
vfront ∼ξ/τP = 1.28. An animation of this can be found
in the supplemental material. The training data, denoted
as well feature set, were recorded in three diﬀerent ways:
• site-only: Only v0 is recorded, and time-delayed
features v0,∆n = v0(t = (n −∆n)∆t) are also included with ∆n∆t = −1, −2, −3, −4.
• spatially extended: We record v0 and additionally vi with i = −2, −4, . . . , −10, −20 (upstream
direction).
• mixed: This combines the two approaches above.
For each site we also include the time delayed features.
To avoid introducing additional symbols we use state
variables with double subscripts for discrete times, where
the second index refers to time, and one subscript for continuous time. The respective useage is evident from the
context. We choose to predict the state at time t = 2
given the data described above. In other words, the prediction target is v0(tn + τ) with τ = 20 ≃2.5τP , corresponding to the requirement to be far engouh from the
excitation point.
Of course, this implies a distance of
∆x ∼2.5ξ. The testing and training sets were selected
by using every second point of the recorded time series.
FFX Results
(Sec. II A 2). In Fig. 6 we display the Pareto fronts using the three diﬀerent approaches for the training set.
All curves have one point in common which represents
the best ﬁtting constant (complexity 0). As one would
expect, the site only data do not contain enough information to detect a front. Thus, even high complexity models
cannot reach an error below 4% and the required error
of 1% is never met. In the two other datasets the algorithm has the possibility to ﬁnd a combination of spatial
amd temporal inputs to account for the front velocity.
Note that the shape of the front strongly depends on the
internal ρ parameter of the elastic net Eq. 3. More information should not lead to a decrease in predictability.
Thus, the Pareto front of a data set richer in features
dominate the corresponding Pareto front of a data set
with less features. Counter-intuitively, using ρ = 0.95
 the front for the mixed dataset becomes non-convex
as some good ﬁtting models are hidden by the regularizer.
Thus, we can use ρ to inﬂuence the shape of the front.
Despite that, the most accurate model of the mixed data
set is still the most accurate model overall.
In the following we discuss the results for the best models for each feature set.
If we take the perspective of an observer sitting at i =
0, we see the spike passing: ﬁrst the state is zero, then
a slow increase is observed followed by a rapid increase
and decrease around the spike maximum. Eventually the
state returns slowly to zero. Statistically, the algorithm
Complexity Γ2
spatially extended
Figure 6. Coupled spiking oscillators, method FFX: Pareto
fronts for the diﬀerent spatio-temporal samplings of the network data. For this plot we use ρ = 0.95. This leads to the
non-convex shape of the front based on the most information.
The models are re-evaluated on the testing set.
is trained by long quiet times and a short, complicated
spike form which is hard to model by a reduced set of
state variables. This is illustrated in Fig. 7a where for
any feature set the biggest diﬀerences occur in the spike
region. Apparently, the model with site-only variables
shows worse results than the spatial one, and the spatiotemporal set models best the passing spike. We note that
in a direct confrontation, the true and modeled signal
would hard to be distinguished. In Fig. 7b we confront
the time derivative for the model from mixed variables.
The true and modeled spike are indistinguishable by eye.
The formulae of the most accurate models are shown
in Table I. For site-only features, quadratic combinations
of points at diﬀerent times occur. This reﬂects the approximation of the incoming front by a quadratic term.
If, however only spatial points are used, the dynamics
far away are used to predict the incoming front. If the
small terms are neglected, the model consists of the signal at the target site itself, and the previous site (-2)
which carries the largest weight.
Physically, it means
that despite being far away the front is already felt at 2
sites away. Since the front is stationary in a co-moving
frame, spatio-temporal embedding is best, namely sampling the spike train in space and moving in time with
the train velocity.
Then we have a simple and compact linear dependence as seen in the last row of Table I. Let us inspect the possible physics in the model
approximating the constants a0, a1, a2, a3 roughly as 0,
0.45, 0.35, 0.175 such that a2 = 2a3 . We ﬁrst notice
that τp = 8.4 ≃10.
The last terms can then be recombined to a3v−2,−10 + a3v−2,−10 + v−2,0 as a mean
value of the state with time distance of approximately
one typical time scale. The state at −30 is at the backside of the front and together the most important information, namely the increase and decrease of the incoming signal is selected by the model.
Alternatively,
since v(0, t) = v(−vfτP , t−τP ) the best model in Table I
can be interpreted as the weighted average of the clos-
spatially extended
Figure 7. Coupled spiking oscillators, method FFX. For each
feature set, the most accurate model is used as the predictor
ˆv0. In (a) we show the diﬀerence δv0 = v0 −ˆv0. The upper
two curves are shifted by 0.25 and 0.5 respectively. In (b)
we compare the time derivative (approximated by the ﬁnite
diﬀerence quotient) of the most accurate model overall and
the real data. For details see text.
est combination (∆i, ∆t) to represent the front velocity
This demonstrates how powerful the
algorithm works in selecting important features.
GP Results
We again examine the Pareto-optimal models illustrated in Fig. 8. For each feature set we obtain a nonconvex Pareto front. The shape and the values of the
fronts are broadly similar to the results obtained by FFX.
Because GP is an evolutionary method and relies on random breeding rules, we display averaged results: we initialize the algorithm with diﬀerent seeds of the random
number generator, calculate the Pareto fronts and average the errors for the non dominated models of the
same complexity. Note that not all complexities occur
on each particular front. This way, we obtain a generic
Pareto front and avoid atypical models which may occur
by chance. The speciﬁc model given below in the tables
temporal site-only −0.0273+3.34v0,0 −2.41v0,0v0,−10 −2.09v0,−40v0,−10 +1.64v2
0,−20 −1.53v0,−20 −1.16v0,−10 +0.991v2
0,0 + 0.463v0,−30 + 0.433v0,−20v0,0 + 0.373v0,−20v0,−10 −0.359v2
0,−40 + 0.216v0,−40 + 0.00286v2
spatially extended −0.00247 + 0.897v−2,0 + 0.178v0,0 −0.0650v−4,0 + 0.00280v−10,0 −0.00210v−8,0
temporal spatial
0.00894 + 0.442v−4,−30 + 0.346v−2,−10 + 0.175v−2,0
Table I. Coupled spiking oscillators, method FFX. Formulae of the most accurate models. The spatio-temporal embedding
reproduces the data very well, i.e. an early detection is possible.
Complexity Γ2
spatially extended
Figure 8. Coupled spiking oscillators, method GP. Averaged
Pareto fronts, for each spatio-temporal sampling option, 10
runs are conducted and the resulting complexities and errors
are averaged.
Errorbars represent the standard deviation.
For the spatially extended and mixed data sets the errors
are smaller than the circle size. The models are re-evaluated
on the testing set.
is not averaged, but the best result for one speciﬁc seed
(42). The errors of the models reachable by the diﬀerent sets are again decreasing from site only over spatially
extended to mixed. However, the mixed model reaches
almost zero error which is quite remarkable!
The diﬀerence plots for the method are given in Fig. 9.
While the site only set is not able to give a convincing
model for an incoming front, the spatially extended set
gives a reasonable model with little error.
model is very good with perfect coincidence of model
and true dynamics. This model cannot be distinguished
by eye from the observed signal.
The models provided by the GP algorithm with seed 42
are given in Table II. Due to the very general character of
GP these can be overwhelming at ﬁrst glance. However,
we can simplify them down by using computer algebra
systems like sympy or mathematica (here we use sympy).
The interpretation of the GP results requires a bit
more thinking.
In essence, they follow a logic similar
to the FFX results. The site-only model is complicated,
and instead of a square operator a trigonometric function is used to mimic the incoming pulse. Since the data
do not include directly all information needed, the algorithm tries to ﬁt unphysical functions. This is clearly a
non-deterministic and overﬁtting result, mirrored by the
high complexity of the functions involved. For spatially
extended models, we obtain a linear and sinusoidal components, and the model uses only three features, namely
spatially extended
Figure 9. Coupled spiking oscillators, method GP. For each
feature set, the most accurate model is used as the predictor
ˆv0. In (a) we show the diﬀerence δv0 = v0 −ˆv0. The upper
two curves are shifted by 0.25 and 0.5 respectively. In (b) we
compare the time derivative (approximated by the ﬁnite difference quotient) of the most accurate model overall and the
real data. Prediction and true data cannot be distinguished
the on-site values and the ones at two and four units left
on our site under consideration. Remarkably, a sinusoidal
behavior detected with an exponential decrease, which is
our intuition.
Eventually, the spatio-temporal embedding yields a very simple model which approximates the
front velocity vf to be between 4
3 and 1. The accuracy
of this model is very high.
Summarizing, when given enough input information,
both methods ﬁnd a linear model for the predictor
ˆv0(t + τ) by ﬁnding the most suitable combination of
temporal and spatial shift to mimic the constant front
temporal site-only v2
0,0/(v0,−10 +
( −v0,−10(v0,0 −v0,−30)(v0,−30/ sin(v0,−10 + v0,−20) + exp(v0,−30) −
( sin(v0,−30)) +
(v0,−30)v0,−40))))
spatially extended 0.208v0,0 + 0.792v−2,0 + 0.0274362547430272 exp(−v−4,0) sin(v−2,0)
temporal spatial
0.878v−4,−30 + 0.124496v−4,−40
Table II. Coupled spiking oscillators, method GP. Formulas of the most accurate models for seed 42.
velocity. If this information is not available in the input
data, nonlinear functions are used.
Solar Power Data
In this section, we describe the results obtained for oneday-ahead forecasting of solar power production.
input data used for training are taken from the unisolar
solar panel installation at Potsdam University with about
30 kW installed. Details are found at . We join the
solar power data with meteorological forecast data from
the freely available European Centre for Medium-Range
Weather Forecasts (ECMWF) portal as well as the
actual observed weather data. These public data are of
limited quality and serve for our proof of concept with
real data and all their deﬁciencies.
The solar panel data P(t) were recorded every ﬁve minutes, at geoposition 52.41 latitude, 12.98 longitude. The
information about the weather can be split into two categories: weather observations of a station near the power
source W(t) and the weather forecast ˆW(t + τ), where
τ is the time diﬀerence to the prediction target. We do
not have weather data from the station directly, but can
use data from a weather station nearby (ID: 10379). The
weather forecast data are obtained every six hours at the
closest location publicly accessible, 52.5 latitude and 13
longitude. Typical meteorological data contain, but are
not limited to, the wind speed and direction, pressure at
diﬀerent levels, the irradiation, cloud coverage, temperature and humidity. However, in this example, we only
use temperature and cloudiness as well as their forecasts
as features for our model. The latter is obtained by minimizing
P(t + τ), ˆP
 P(t), W(t), ˆW(t + τ)
with f the model under consideration. Our prediction
target is ˆP(t + τ) with τ = 24, the one-day-ahead power
production. We create our datasets with a sampling of
1h. While additional information from the solar power
data remains unused, the prediction variables have to
be interpolated. The quality of the forecast depends on
quality of the weather measurement and weather forecast.
As we use publicly available data, we can only
demonstrate the procedure and cannot attain errors as
low as those used in commercial products, which will be
discussed elsewhere. The features of the the data set are
listed in Table IV C Furthermore, we scale each feature
Complexity Γ2
Figure 10. Solar power study, Average Pareto front obtained
using GP: with increasing complexity, training and testing
data ﬁrst behave similarly, then testing deviates strongly indicating overﬁtting or too small testing data set, respectively.
The peaks around complexity 20 are due to two reasons: there
are only few models on the fronts (1-3), and one of them is
an extreme outlier.
to have its minimum equal zero and maximum equal to
one. The models are trained with data from June and
July of 2014. Testing is conducted for August 2014. To
obtain ﬁrst impression (assuming no prior knowledge),
we calculate the mutual correlation of the data.
power produced the next day is heavily correlated with
the predicted solar irradiation.
This is a conﬁrmation
that the physics involved is mirrored in the model and
that weather prediction is good in average. Quantitative
statements on the quality of weather prediction is not
easy and can be found in the literature .
GP Results
Let us consider the results of our forecasting with GP
shown in Fig. 10. The Pareto fronts are shown for both
the training and testing set. As above, for the coupled oscillators, we have conducted 10 runs with diﬀerent seeds
and display the averaged result. Of course, for the training set (ﬁlled diamonds), increasing complexity means
decreasing error. We see a strong deviation for very complicated models of the testing data (ﬁlled circles). This
may be an indication of a small testing sample, or indicate overﬁtting. The outlier at Γ = 18 is a result of the
particular realization of the evolutionary optimization.
With a diﬀerent setting, e.g. more iterations, or multiple
runs such outliers are eliminated. To clarify this question,
Sampling Variable
Solar power
direct access
Total cloud coverage
2 meter temperature
Total cloud coverage prediction tccpred(t, τ) ECMWF-TIGGE 6h
2 meter temperature prediction Tpred(t, τ)
ECMWF-TIGGE 6h
Table III. Solar power study: description of the data set. We use a set of 5 features drawn from diﬀerent sources with diﬀerent
we show the functions found as solution of our procedure
with increasing complexity and one speciﬁc seed (42) in
From Table IV we see that GP follows a very reasonable strategy: First, it recognizes that the persistence
method is a very reasonable thing, with production tomorrow being the same as today (x1 = P(t)). Veto a
complexity of 5, the identiﬁed models only depend on
the solar power x1 and describe with increasing accuracy
the conditioned average daily proﬁle. The more complex
models include the weather data and forecast. The geometric mean of current power and predicted temperature
is present. However, due to the low quality weather forecast as well as the seasonal weather diﬀerence between
training and testing data, there is no net gain in prediction quality.
Without any further analysis, the model with the lowest testing error is chosen. In Fig. 11 (a) we confront
the real time series with the prediction from GP for the
model of complexity 4.
One clearly ﬁnds the already
mentioned conditioned average proﬁle. This predicts the
production onset a bit too early. The error distribution is
shown in Fig. 11 (b), where we recognize an asymmetric
error distribution with more probable under- than overprediction.
FFX Results
The results of the FFX method are shown in Fig. 12-13
and the models in Table V. As shown, the FFX method
is less capable of predicting longer inactive periods, such
as at night, where no solar power is produced. This is
clearly visible in Fig. 13.
Analyzing the equations of Table V, we notice that
the best FFX function is a quadratic form with maxima
to limit the signal above zero. This amounts to recover
the mean shape of the signal as a quadratic function.
Unfortunately this seems almost trivial since one could
obtain this mean shape by purely geometrical considerations with a factor for the cloud coverage.
Summarizing the results for the solar power curves,
both methods are able to reproduce the true curve to
approximately 20% which is reasonable for a nonoptimized method. The detection of changes when clear sky
switches to partially or fully clouded one is not entirely
satisfactory and one needs to investigate the improvement of weather predictions for a single location. As said
Time 
Observation
Prediction
Figure 11. Solar power study, method GP: a) Real and predicted time series. We display the results of the ﬁrst week of
August 2015. Prediction used model of complexity 4 which
had lowest error on the test set. b) Histogram of the residuals
ε = P −ˆP. The distribution is asymmetric around zero. The
model tends to underpredict.
in the introduction, a perfect weather prediction with
high resolution would render this work useless for power
production forecast (although not for other questions).
Nevertheless, we note that the results in the form of
analytic models are highly valuable, because interpretations and further mathematical analysis are possible.
Error Γ1 Complexity Γ2 Formula
sin(sin(x1))
(x0 sin(x1))
( sin(x0) sin(x1))
(x0x1 cos(x3))
(x1)(x0 −x3 + 0.649)
(x1) cos(x2x3/x0)
(x1)(x0 −x4(x3 −0.699))
(x1)(x0 −x2(x2x3 −0.592))
(x1)(x0 + (−x2x3 + 0.597) sin(x4 + sin(x1)))
(x1)((sin(x3) −0.641) exp(x4) cos(x3) −1)
(x1) + (−x3 + 0.715)(sin(x1) + sin(x2x4)) cos(x1))
Table IV. Solar power study, method GP: formulae of the Pareto front models for seed 42.
Complexity Γ2 Formula
0.269419547644 0
0.199597415208 1
0.108 + 0.511x1
0.194122751788 2
0.0223 + 0.606x1 + 0.139x0
0.193361252172 3
0.0470 + 0.436x1 + 0.328x0x1 + 0.138x4x0
0.189889358594 4
0.459 −0.458 max(0, 0.200 −x1) −0.339 max(0, 0.333 −x1) −0.134 max(0, 0.733 −x1) −
0.0828 max(0, 0.867 −x1)
0.18135731346
0.301 −1.25 max(0, 0.333 −x1) max(0, 0.200 −x1) −0.810 max(0, 0.467 −x1) max(0, 0.200 −
x1) + 0.457x0x1 −0.252 max(0, 0.333 −x1) −0.0794 max(0, 0.200 −x1)
Table V. Solar power study, method FFX: formulae of the Pareto front models.
Complexity Γ2
Figure 12.
Solar power study, Pareto front obtained using
FFX. The results for FFX are as accurate as the ones obtained
with GP. Test and training set are, however, nicely aligned.
This demonstrates not only consistency of the models, but
less variability of the models found.
CONCLUSION
We have demonstrated the use of symbolic regression
combined with complexity analysis of the resulting models for the future prediction of dynamical systems. More
precisely, we identify a system of equations yielding optimal forecasts in terms of a minimized normalized root
mean squared error of the diﬀerence between model forecast and observation of the system state.
We did not
investigate theoretical aspects such as the underlying
state space, nor what implications of the functions on
the model.
These will be subject of future investigations. Such work is to be carried out carefully to ﬁnd
the limitations of the approach, in particular of genetic
programming, which is rather uncontrolled in the way
the search space is explored.
On the other hand, the
methods stand in line with a large collection of methods
from regression and classiﬁcation and one can use much
of this previous knowledge. In our opinion, the multiobjective analysis is crucial to identify models to a degree
such that they can be used in practice. Probably, this
approach will prove very helpful if used in combination
with scale analysis, e.g. by preﬁltering the data on a selected spatio-temporal scale and then identify equations
for this level.
We have tried to show the possible power by three examples of increasing complexity: a trivial one - the harmonic oscillator with an almost perfect predictive power,
a collection of excitable oscillators where we demonstrated that the methods can perform a kind of multiscale analysis based on the data. Thirdly, examine the
one-day-ahead forecasting of solar power production we
have shown that even for messy data we can improve the
classical methods by a few percent (in NRMSE). For theoretical considerations, this might be negligible, for real
Time 
Observation
Prediction
Figure 13. Solar power study, method FFX: a) Timeseries of
the predicted ( ˆP), and observed (P) data. We display the
results of the ﬁrst week of August 2015. Similar to the GP
prediction extrema are not particularly well predicted. For
the linear model, even the zero values are not well hit. The
reason for this is the regression to mean values and the inability of powers to stay at zero for a suﬃcient time. b) Histogram
of the residuals ε = P −ˆP. Despite diﬀerent formulas, the
histogram of the residuals is asymmetric around zero with a
trend to underpredict as well.
world applications, a few percent might translate into a
considerable advantage, since the usage of rare resources
can be optimized.
A question for further research is how we can use simpliﬁcation during the GP iteration to alter the complexity. It may be even a viable choice to control the complexity growth over time, the so-called bloat, in single
objective genetic programming - a topic of ongoing interest . Additionally, we introduced an intermediate
step to only allow for one of many identical solutions for
further evolution. One could consider to expand the idea
of identical expression trees to include symmetries.
We conclude that symbolic regression is very useful for
the prediction of dynamical systems, based on observations only. Our future research will focus on the use of
equations couple the systems to other macroscopic ones
(e.g. ﬁnance, in the case of wind power), and on the analysis of system stability and other fundamental properties
using the found equations, which is scientiﬁcally a very
crucial point.
ACKNOWLEDGEMENTS
We acknowledge discussion with C. Cornaro and M.
Pierro on solar power forecast, helpful words on machine
learning by M. Segond.
We thank the unisolar initiative for their provision of the data. M. Quade and M.
Abel acknowledge support by the German ministry of
economy, ZIM project ”green energy forecast”, grant ID
KF2768302ED4.
 Edward S. Sarachik and Mark A. Cane. The El Ni˜no-
Southern Oscillation Phenomenon. Cambridge University Press, 2010. Cambridge Books Online.
coelestium in sectionibus conicis solem ambientium auctore Carolo Friderico Gauss. sumtibus Frid. Perthes et
IH Besser, 1809.
 Erwin Schr¨odinger. An undulatory theory of the mechanics of atoms and molecules. Physical Review, 28(6):1049
–1070, 1926.
 Luca Cardelli and Peter Wegner.
On understanding
types, data abstraction, and polymorphism. ACM Computing Surveys (CSUR), 17(4):471–523, 1985.
 H. U. Voss, P. Kolodner, M. Abel, and J. Kurths. Amplitude equations from spatiotemporal binary-ﬂuid convection data. Phys. Rev. Lett., 83(17):3422–3425, 1999.
 Markus Abel, Karsten Ahnert, J¨urgen Kurths, and Simon Mandelj. Additive nonparametric reconstruction of
dynamical systems from time series. Physical Review E,
71(1):015203, 2005.
 M. Abel. Nonparametric modeling and spatiotemporal
dynamical systems. Int. J. Bif. Chaos, 14(6):2027–2039,
 Daniel Rey, Michael Eldridge, Mark Kostuk, Henry D.I.
Abarbanel, Jan Schumann-Bischoﬀ, and Ulrich Parlitz.
Accurate state and parameter estimation in nonlinear
systems with sparse observations.
Physics Letters A,
378(11–12):869 – 873, 2014.
 Steven L Brunton, Joshua L Proctor, and J Nathan Kutz.
Discovering governing equations from data: Sparse identiﬁcation of nonlinear dynamical systems. arXiv preprint
 
 K. Ahnert and M. Abel.
Numerical diﬀerentiation:
global versus local methods. Comput. Phys. Commun.,
177(10):764–774, 2007. doi:10.1016/j.cpc.2007.03.009.
 Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.
 Amit Singer, Radek Erban, Ioannis G Kevrekidis, and
Ronald R Coifman.
Detecting intrinsic slow variables
in stochastic dynamical systems by anisotropic diﬀusion
maps. Proceedings of the National Academy of Sciences,
106(38):16090–16095, 2009.
 Sam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science,
290(5500):2323–2326, 2000.
 Michael Schmidt and Hod Lipson.
Distilling freeform natural laws from experimental data.
324(5923):81–85, 2009.
 K Rodriguez-Vazquez and Peter J Fleming.
Multiobjective genetic programming for nonlinear system identiﬁcation. Electronics Letters, 34(9):930–931, 1998.
 N. Gautier, J.-L. Aider, Duriez, B. R. T., Noack,
M. Segond, and M. W. Abel. Closed-loop separation control using machine learning”. journal of ﬂuid mechanics
770, 242-441. Journal of Fluid Mechanics, 770:242–441,
 S. Brunton and B. R. Noack.
Closed-loop turbulence
Progress and challenges.
Appl. Mech. Rev.,
67(5):050801, 2015.
 E. Lorenzo.
Solar Electricity:
Engineering of Photovoltaic Systems. PROGENSA, 1994.
 E.N. Lorenz. Deterministic nonperiodic ﬂow. J. Athmos.
Sci., 20:130, 1963.
 H. Kantz and T. Schreiber. Nonlinear time series analysis. Cambridge University Press, 1997.
 E. Ott, T. Sauer, and J.A. Yorke. Coping with Chaos.
Series in Nonlinear Science. Wiley, New York, 1994.
 Steven H Strogatz. Nonlinear dynamics and chaos: with
applications to physics, biology, chemistry, and engineering. Westview press, 2014.
 H. Whitney. Diﬀerentiable manifolds. Annals of Math.,
37:645, 1936.
 F. Takens.
Detecting strange attractors in turbulence.
Lecture Notes in Mathematics No.898. Springer, Berlin,
 T. Sauer, J. Yorke, and M. Casdagli. Embeddology. J.
Stat. Phys., 65(3/4):579–615, 1991.
 H. D. I. Abarbanel, M. E. Gilpin, and M. Rotenberg.
Analysis Of Observed Chaotic Data. Springer, New York,
 N.A. Gershenfeld. The Nature of Mathematical Modeling.
Cambridge University Press, 1999.
 Christopher M Bishop. Pattern recognition and machine
learning. springer, 2006.
 Trent McConaghy.
Fast, scalable, deterministic
symbolic regression technology. In Genetic Programming
Theory and Practice IX, pages 235–260. Springer, 2011.
 G.B. Dantzig, R. Cottle, and Y.P. Aneja. Mathematical programming: essays in honor of George B. Dantzig.
Number Bd. 1 in Mathematical Programming: Essays in
Honor of George B. Dantzig. North-Holland, 1985.
 John R Koza. Genetic programming: on the programming
of computers by means of natural selection, volume 1.
MIT press, 1992.
 Hui Zou and Trevor Hastie. Regularization and variable
selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320, 2005.
 Anselm Blumer, Andrzej Ehrenfeucht, David Haussler,
and Manfred K Warmuth. Occam’s razor. Information
processing letters, 24(6):377–380, 1987.
 William H Jeﬀerys and James O Berger. Ockham’s razor
and bayesian analysis. American Scientist, pages 64–72,
 Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm:
Evolutionary Computation,
IEEE Transactions on, 6(2):182–197, 2002.
 Eckart Zitzler, Marco Laumanns, Lothar Thiele, Eckart
Zitzler, Eckart Zitzler, Lothar Thiele, and Lothar Thiele.
Spea2: Improving the strength pareto evolutionary algorithm, 2001.
 Joshua Knowles and David Corne. The pareto archived
evolution strategy: A new baseline algorithm for pareto
multiobjective optimisation. In Evolutionary Computation, 1999. CEC 99. Proceedings of the 1999 Congress
on, volume 1. IEEE, 1999.
 Guido F Smits and Mark Kotanchek. Pareto-front exploitation in symbolic regression.
In Genetic programming theory and practice II, pages 283–299. Springer,
 Ekaterina J Vladislavleva, Guido F Smits, and Dick
Den Hertog. Order of nonlinearity as a complexity measure for models generated by symbolic regression via
pareto genetic programming. Evolutionary Computation,
IEEE Transactions on, 13(2):333–349, 2009.
 Unisolar e.v., 2015.
 Jinichi Nagumo, Suguru Arimoto, and Shuji Yoshizawa.
An active pulse transmission line simulating nerve axon.
Proceedings of the IRE, 50(10):2061–2070, 1962.
 Alan L Hodgkin and Andrew F Huxley. A quantitative
description of membrane current and its application to
conduction and excitation in nerve. The Journal of physiology, 117(4):500, 1952.
 The default value of the package which works well in most
scenarios.
 European Centre for Medium-Range Weather Forecasts.
Ecmwf - european centre for medium-range weather forecasts,.
 Marc-Andr´e
Parizeau. Controlling code growth by dynamically shaping the genotype size distribution. Genetic Programming
and Evolvable Machines, pages 1–44, 2014.