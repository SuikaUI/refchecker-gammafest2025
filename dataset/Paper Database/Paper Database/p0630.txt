Research Highlights (Required)
To create your highlights, please type the highlights against each \item command.
It should be short collection of bullet points that convey the core ﬁndings of the article. It should include 3 to 5 bullet points
(maximum 85 characters, including spaces, per bullet point.)
• We survey deep learning based HAR in sensor modality, deep model, and application.
• We comprehensively discuss the insights of deep learning models for HAR tasks.
• We extensively investigate why deep learning can improve the performance of HAR.
• We also summarize the public HAR datasets frequently used for research purpose.
• We present some grand challenges and feasible solutions for deep learning based HAR.
 
Pattern Recognition Letters
journal homepage: www.elsevier.com
Deep Learning for Sensor-based Activity Recognition: A Survey
Jindong Wanga,b, Yiqiang Chena,b,∗∗, Shuji Haoc, Xiaohui Penga,b, Lisha Hua,b
aBeijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
bUniversity of Chinese Academy of Sciences, Beijing, China
cInstitute of High Performance Computing, A*STAR, Singapore
Sensor-based activity recognition seeks the profound high-level knowledge about human activities
from multitudes of low-level sensor readings.
Conventional pattern recognition approaches have
made tremendous progress in the past years. However, those methods often heavily rely on heuristic
hand-crafted feature extraction, which could hinder their generalization performance. Additionally,
existing methods are undermined for unsupervised and incremental learning tasks. Recently, the recent advancement of deep learning makes it possible to perform automatic high-level feature extraction
thus achieves promising performance in many areas. Since then, deep learning based methods have
been widely adopted for the sensor-based activity recognition tasks. This paper surveys the recent
advance of deep learning based sensor-based activity recognition. We summarize existing literature
from three aspects: sensor modality, deep model, and application. We also present detailed insights
on existing work and propose grand challenges for future research.
Keywords: Deep learning; activity recognition; pattern recognition; pervasive computing
c⃝2017 Elsevier Ltd. All rights reserved.
1. Introduction
Human activity recognition (HAR) plays an important role
in people’s daily life for its competence in learning profound
high-level knowledge about human activity from raw sensor
inputs. Successful HAR applications include home behavior
analysis , video surveillance , gait analysis , and gesture
recognition . There are mainly two
types of HAR: video-based HAR and sensor-based HAR . Video-based HAR analyzes videos or images containing human motions from the camera, while sensor-based
HAR focuses on the motion data from smart sensors such as an
accelerometer, gyroscope, Bluetooth, sound sensors and so on.
Due to the thriving development of sensor technology and pervasive computing, sensor-based HAR is becoming more popular and widely used with privacy well protected. Therefore, in
this paper, our main focus is on sensor-based HAR.
HAR can be treated as a typical pattern recognition (PR)
problem. Conventional PR approaches have made tremendous
∗∗Corresponding author
e-mail: (Yiqiang Chen)
progress on HAR by adopting machine learning algorithms
such as decision tree, support vector machine, naive Bayes, and
hidden Markov models . It is no wonder that in some controlled environments where there are only a
few labeled data or certain domain knowledge is required (e.g.
some disease issues), conventional PR methods are fully capable of achieving satisfying results. However, in most daily
HAR tasks, those methods may heavily rely on heuristic handcrafted feature extraction, which is usually limited by human
domain knowledge . Furthermore, only shallow
features can be learned by those approaches ,
leading to undermined performance for unsupervised and incremental tasks. Due to those limitations, the performances of
conventional PR methods are restricted regarding classiﬁcation
accuracy and model generalization.
Recent years have witnessed the fast development and advancement of deep learning, which achieves unparalleled performance in many areas such as visual object recognition, natural language processing, and logic reasoning . Diﬀerent from traditional PR methods, deep learning
can largely relieve the eﬀort on designing features and can
learn much more high-level and meaningful features by training
an end-to-end neural network. In addition, the deep network
Time domain
Frequency domain
Activity signal
Feature extraction
Model training
Support vector machine
Decision tree
Hidden Markov model
Naïve Bayes
Gaussian Mixture
K nearest neighbor
Riding bike
Having coffee
Watching TV
Activity inference
Fig. 1. An illustration of sensor-based activity recognition using conventional pattern recognition approaches.
structure is more feasible to perform unsupervised and incremental learning. Therefore, deep learning is an ideal approach
for HAR and has been widely explored in existing work .
Although some surveys have been conducted in deep learning and
HAR , respectively, there has been no speciﬁc survey focusing on the intersections of these two areas. To our best knowledge, this is the
ﬁrst article to present the recent advance on deep learning based
HAR. We hope this survey can provide a helpful summary of
existing work, and present potential future research directions.
The rest of this paper is organized as follows. In Section 2,
we brieﬂy introduce sensor-based activity recognition and explain why deep learning can improve its performance. In Section 3, 4 and 5, we review recent advance of deep learning based
HAR from three aspects: sensor modality, deep model, and application, respectively. We also introduce several benchmark
datasets. Section 6 presents summary and insights on existing
work. In Section 7, we discuss some grand challenges and feasible solutions. Finally, this paper is concluded in Section 8.
2. Background
2.1. Sensor-based Activity Recognition
HAR aims to understand human behaviors which enable the
computing systems to proactively assist users based on their
requirement . Formally speaking, suppose
a user is performing some kinds of activities belonging to a
predeﬁned activity set A:
where m denotes the number of activity types. There is a sequence of sensor reading that captures the activity information
s = {d1, d2, · · · , dt, · · · dn}
where dt denotes the sensor reading at time t.
We need to build a model F to predict the activity sequence
based on sensor reading s
ˆA = { ˆAj}n
j=1 = F (s),
while the true activity sequence (ground truth) is denoted as
where n denotes the length of sequence and n ≥m.
The goal of HAR is to learn the model F by minimizing
the discrepancy between predicted activity ˆA and the ground
truth activity A∗. Typically, a positive loss function L(F (s), A∗)
is constructed to reﬂect their discrepancy. F usually does not
directly take s as input, and it usually assumes that there is a
projection function Φ that projects the sensor reading data di ∈
s to a d-dimensional feature vector Φ(di) ∈Rd. To that end, the
goal turns into minimizing the loss function L(F (Φ(di)), A∗).
Fig. 1 presents a typical ﬂowchart of HAR using conventional PR approaches.
First, raw signal inputs are obtained
from several types of sensors (smartphones, watches, Wi-Fi,
Bluetooth, sound etc.). Second, features are manually extracted
from those readings based on human knowledge , such as the mean, variance, DC, and amplitude in
traditional machine learning approaches . Finally, those features serve as inputs to train a PR model to make
activity inference in real HAR tasks.
2.2. Why Deep Learning?
Conventional
approaches
tremendous
progress in HAR . However, there are several drawbacks to conventional PR methods.
Firstly, the features are always extracted via a heuristic and
hand-crafted way, which heavily relies on human experience or
domain knowledge. This human knowledge may help in certain
task-speciﬁc settings, but for more general environments and
tasks, this will result in a lower chance and longer time to build
a successful activity recognition system.
Secondly, only shallow features can be learned according to
human expertise . Those shallow features
often refer to some statistical information including mean, variance, frequency and amplitude etc. They can only be used to
recognize low-level activities like walking or running, and hard
to infer high-level or context-aware activities . For
instance, having coﬀee is more complex and nearly impossible
to be recognized by using only shallow features.
Thirdly, conventional PR approaches often require a large
amount of well-labeled data to train the model. However, most
of the activity data are remaining unlabeled in real applications.
Thus, these models’ performance is undermined in unsupervised learning tasks . In contrast, existing deep
generative networks are able to exploit the
unlabeled samples for model training.
Moreover, most existing PR models mainly focus on learning
from static data; while activity data in real life are coming in
stream, requiring robust online and incremental learning.
Deep learning tends to overcome those limitations. Fig. 2
shows how deep learning works for HAR with diﬀerent types
of networks. Compared to Fig. 1, the feature extraction and
model building procedures are often performed simultaneously
in the deep learning models. The features can be learned automatically through the network instead of being manually designed. Besides, the deep neural network can also extract highlevel representation in deep layer, which makes it more suitable for complex activity recognition tasks. When faced with a
large amount of unlabeled data, deep generative models are able to exploit the unlabeled data for model
Activity signal
Riding bike
Having coffee
Watching TV
Activity inference
Deep feature extraction & model building
Fig. 2. An illustration of sensor-based activity recognition using deep learning approaches.
training. What’s more, deep learning models trained on a largescale labeled dataset can usually be transferred to new tasks
where there are few or none labels.
In the following sections, we mainly summarize the existing work based on the pipeline of HAR: (a) sensor modality,
(b) deep model, and (c) application.
3. Sensor Modality
Although some HAR approaches can be generalized to all
sensor modalities, most of them are only speciﬁc to certain
types. According to , we mainly classify those modalities into three aspects: body-worn sensors, object sensors, and ambient sensors. Table 1 brieﬂy outlines all
the modalities.
3.1. Body-worn Sensor
Body-worn sensors are one of the most common modalities
in HAR. Those sensors are often worn by the users, such as
an accelerometer, magnetometer, and gyroscope. The acceleration and angular velocity are changed according to human
body movements; thus they can infer human activities. Those
sensors can often be found on smart phones, watches, bands,
glasses, and helmets.
Body-worn sensors were widely used in deep learning based
HAR . Among those work, the
accelerometer is mostly adopted. Gyroscope and magnetometer
are also frequently used together with the accelerometer. Those
sensors are often exploited to recognize activities of daily living (ADL) and sports. Instead of extracting statistical and frequency features from the movement data, the original signal is
directly used as inputs for the network.
3.2. Object Sensor
Object sensors are usually placed on objects to detect the
movement of a speciﬁc object . Diﬀerent from body-worn sensors which capture human movements,
object sensors are mainly used to detect the movement of certain objects in order to infer human activities. For instance, the
accelerometer attached to a cup can be used to detect the drinking water activity. Radio frequency identiﬁer (RFID) tags are
typically used as object sensors and deployed in smart home
environment and medical activities . The RFID can provide more ﬁne-grained information for more complex activity recognition.
It should be noted that object sensors are less used than bodyworn sensors due to the diﬃculty in its deployment. Besides,
the combination of object sensors with other types is emerging
in order to recognize more high-level activities .
3.3. Ambient Sensor
Ambient sensors are used to capture the interaction between
humans and the environment. They are usually embedded in
users’ smart environment. There are many kinds of ambient
sensors such as radar, sound sensors, pressure sensors, and temperature sensors. Diﬀerent from object sensors which measure
the object movements, ambient sensors are used to capture the
change of the environment.
Several literature used ambient sensors to recognize daily activities and hand gesture . Most of the work was tested in the
smart home environment. Same as object sensors, the deployment of ambient sensors is also diﬃcult. In addition, ambient
sensors are easily aﬀected by the environment, and only certain
types of activities can be robustly inferred.
3.4. Hybrid Sensor
Some work combined diﬀerent types of sensors for HAR. As
shown in , combining acceleration with
acoustic information could improve the accuracy of HAR. Ambient sensors are also used together with object sensors; hence
they can record both the object movements and environment
state. designed a smart home environment called A-Wristocracy, where a large number of ﬁnegrained and complex activities of multiple occupants can be
recognized through body-worn, object, and ambient sensors. It
is obvious that the combination of sensors is capable of capturing rich information of human activities, which is also possible
for a real smart home system in the future.
4. Deep Model
In this section, we investigate the deep learning models used
in HAR tasks. Table 2 lists all the models.
4.1. Deep Neural Network
Deep neural network (DNN) is developed from artiﬁcial neural network (ANN). Traditional ANN often contains very few
hidden layers (shallow) while DNN contains more (deep). With
more layers, DNN is more capable of learning from large data.
DNN usually serves as the dense layer of other deep models.
For example, in a convolution neural network, several dense
layers are often added after the convolution layers. In this part,
we mainly focus on DNN as a single model, while in other sections we will discuss the dense layer.
 ﬁrst extracted hand-engineered
features from the sensors, then those features are fed into a
DNN model. Similarly, performed PCA
before using DNN. In those work, DNN only served as a classi-
ﬁcation model after hand-crafted feature extraction, hence they
Table 1. Sensor modalities for HAR tasks
Description
Worn by the user to describe the body movements
Smartphone, watch, or band’s accelerometer, gyroscope etc.
Attached to objects to capture objects movements
RFID, accelerometer on cup etc.
Applied in environment to reﬂect user interaction
Sound, door sensor, WiFi, Bluetooth etc.
Crossing sensor boundary
Combination of types, often deployed in smart environments
may not generalize well. And the network was rather shallow.
 used a 5-hidden-layer DNN to perform
automatic feature learning and classiﬁcation with improved performance. Those work indicated that, when the HAR data is
multi-dimensional and activities are more complex, more hidden layers can help the model train well since their representation capability is stronger . However, more details should be considered in certain situations to help the model
ﬁne-tune better.
4.2. Convolutional Neural Network
Convolutional Neural Network (ConvNets, or CNN) leverages three important ideas: sparse interactions, parameter sharing, and equivariant representations . After
convolution, there are usually pooling and fully-connected layers, which perform classiﬁcation or regression tasks.
CNN is competent to extract features from signals and it
has achieved promising results in image classiﬁcation, speech
recognition, and text analysis. When applied to time series classiﬁcation like HAR, CNN has two advantages over other models: local dependency and scale invariance. Local dependency
means the nearby signals in HAR are likely to be correlated,
while scale invariance refers to the scale-invariant for diﬀerent
paces or frequencies. Due to the eﬀectiveness of CNN, most of
the surveyed work focused on this area.
When applying CNN to HAR, there are several aspects to be
considered: input adaptation, pooling, and weight-sharing.
1) Input adaptation. Unlike images, most HAR sensors produce time series readings such as acceleration signal, which is
temporal multi-dimensional 1D readings. Input adaptation is
necessary before applying CNN to those inputs. The main idea
is to adapt the inputs in order to form a virtual image. There are
mainly two types of adaptation: model-driven and data-driven.
• Data-driven approach treats each dimension as a channel,
then performs 1D convolution on them. After convolution
and pooling, the outputs of each channel are ﬂattened to
uniﬁed DNN layers. A very early work is , where each dimension of the accelerometer was
treated as one channel like RGB of an image, then the convolution and pooling were performed separately. further proposed to unify and share weights
in multi-sensor CNN by using 1D convolution in the same
temporal window. Along with this line, resized the convolution kernel to obtain the best kernel for HAR data. Other similar work include combined all dimensions to form an image, while designed a more complex algorithm to transform the time series into an image.
In , pressure sensor data was transformed to the image via modality transformation. Other
similar work include .
This model-driven approach can make use of the temporal
correlation of sensor. But the map of time series to image
is non-trivial task and needs domain knowledge.
2) Pooling. The convolution-pooling combination is common in CNN, and most approaches performed max or average
pooling after convolution . Apart from avoiding overﬁtting, pooling can also speed up the training process on large
data .
3) Weight-sharing.
Weight sharing is an eﬃcient method to speed up
the training process on a new task. utilized
a relaxed partial weight sharing technique since the signal appeared in diﬀerent units may behave diﬀerently. adopted a CNN-pf and CNN-pﬀstructure to investigate
the performance of diﬀerent weight-sharing techniques. It is
shown in those literature that partial weight-sharing could improve the performance of CNN.
4.3. Autoencoder
Autoencoder learns a latent representation of the input values through the hidden layers, which can be considered as an
encoding-decoding procedure. The purpose of autoencoder is
to learn more advanced feature representation via an unsupervised learning schema. Stacked autoencoder (SAE) is the stack
of some autoencoders.
SAE treats every layer as the basic
model of autoencoder.
After several rounds of training, the
learned features are stacked with labels to form a classiﬁer.
 used SAE
for HAR, where they ﬁrst adopted the greedy layer-wise pretraining , then performed ﬁne-tuning. Compared to those works, investigated the sparse autoencoder by adding KL divergence and noise to the cost function, which indicates that adding sparse constraints could improve the performance of HAR. The advantage of SAE is that
it can perform unsupervised feature learning for HAR, which
could be a powerful tool for feature extraction. But SAE depends too much on its layers and activation functions which
may be hard to search the optimal solutions.
Table 2. Deep learning models for HAR tasks
Description
Deep fully-connected network, artiﬁcial neural network with deep layers
Convolutional neural network, multiple convolution operations for feature extraction
Recurrent neural network, network with time correlations and LSTM
Deep belief network and restricted Boltzmann machine
Stacked autoencoder, feature learning by decoding-encoding autoencoder
combination of some deep models
4.4. Restricted Boltzmann Machine
Restricted Boltzmann machine (RBM) is a bipartite, fullyconnected, undirected graph consisting of a visible layer and
a hidden layer .
The stacked RBM is
called deep belief network (DBN) by treating every two consecutive layers as an RBM. DBN/RBM is often followed by
fully-connected layers.
In pre-training, most work applied Gaussian RBM in the ﬁrst
layer while binary RBM for the rest layers . For multi-modal sensors, designed a multi-modal RBM where
an RBM is constructed for each sensor modality, then the output of all the modalities are uniﬁed. added
pooling after the fully-connected layers to extract the important features. used a contrastive gradient
(CG) method to update the weight in ﬁne-tuning, which helps
the network to search and convergence quickly in all directions.
 further implemented RBM on a mobile
phone for oﬄine training, indicating RBM can be very lightweight. Similar to autoencoder, RBM/DBN can also perform
unsupervised feature learning for HAR.
4.5. Recurrent Neural Network
Recurrent neural network (RNN) is widely used in speech
recognition and natural language processing by utilizing the
temporal correlations between neurons.
LSTM (long-short
term memory) cells are often combined with RNN where
LSTM is serving as the memory units through gradient descent.
Few work used RNN for the HAR tasks , where the learning speed and resource consumption are the main concerns for HAR. investigated several model parameters ﬁrst and then proposed a relatively good model which can perform HAR with high throughput. proposed a binarized-BLSTM-
RNN model, in which the weight parameters, input, and output
of all hidden layers are all binary values. The main line of RNN
based HAR models is dealing with resource-constrained environments while still achieve good performance.
4.6. Hybrid Model
Hybrid model is the combination of some deep models.
One emerging hybrid model is the combination of CNN and
RNN. provided
good examples for how to combine CNN and RNN. It is shown
in that the performance of ‘CNN
+ recurrent dense layers’ is better than ‘CNN + dense layers’.
Similar results are also shown in . The reason is that CNN is able to capture the spatial relationship, while
RNN can make use of the temporal relationship. Combining
CNN and RNN could enhance the ability to recognize diﬀerent
activities that have varied time span and signal distributions.
Other work combined CNN with models such as SAE and RBM . In those work, CNN
performs feature extraction, and the generative models can help
in speeding up the training process. In the future, we expect
there will be more research in this area.
5. Applications
HAR is always not the ﬁnal goal of an application, but it
serves as an important step in many applications such as skill
assessment and smart home assistant. In this section, we survey
deep learning based HAR from the application perspective.
5.1. Featured Applications
Most of the surveyed work focused on recognizing activities
of daily living (ADL) and sports . Those
activities of simple movements are easily captured by bodyworn sensors. Some research studied people’s lifestyle such as
sleep and respiration . The detection of such activities
often requires some object and ambient sensors such as WiFi
and sound, which are rather diﬀerent from ADL.
It is a developing trend to apply HAR to health and disease issues.
Some pioneering work has been done with regard to Parkinson’s disease , trauma
resuscitation and paroxysmal atrial ﬁbrillation (PAF) . Disease issues are always
related to the change of certain body movements or functions,
so they can be detected using corresponding sensors.
Under those circumstances, the association between disease
and activity should be given more consideration. It is important
to use the appropriate sensors. For instance, Parkinson’s disease
is often related to the frozen of gait, which can be reﬂected by
some inertial sensors attached to shoes .
Other than health and disease, the recognition of high-level
activities is helpful to learn more resourceful information for
HAR. The movement, behavior, environment, emotion, and
thought are critical parts in recognizing high-level activities.
However, most work only focused on body movements in smart
homes , which
is not enough to recognize high-level activities. For instance,
Table 3. Public HAR datasets (A=accelerometer, G=gyroscope, M=magnetometer, O=object sensor, AM=ambient sensor, ECG=electrocardiograph)
OPPORTUNITY
A, G, M, O, AM
 
Skoda Checkpoint
 
UCI Smartphone
 
 
 
 
 
Ambient kitchen
Food preparation
 
Darmstadt Daily Routines
 
Actitracker
 
 
Heart failure
 
 
Daphnet Gait
 
ActiveMiles
 
 
 
 
Heterogeneous
100-200 Hz
43,930,257
 
 combined activity and environment
signal to recognize activities in a smart home, but the activities
are constrained to body movements without more information
on user emotion and state, which are also important. In the future, we expect there will be more research in this area.
5.2. Benchmark Datasets
We extensively explore the benchmark datasets for deep
learning based HAR. Basically, there are two types of data acquisition schemes: self data collection and public datasets.
• Self data collection: Some work performed their own data
collection ). Very
detailed eﬀorts are required for self data collection, and it
is rather tedious to process the collected data.
• Public datasets: There are already many public HAR
datasets that are adopted by most researchers ). By
summarizing existing literature, we present several widely
used public datasets in Table 3.
6. Summary and Discussion
Table 4 presents all the surveyed work in this article. We can
make several observations based on the table.
1) Sensor deployment and preprocessing. Choosing the
suitable sensors is critical for successful HAR. In surveyed literature, body-worn sensors serve as the most common modalities and accelerometer is mostly used. The reasons are two
folds. Firstly, a lot of wearable devices such as smartphones
or watches are equipped with an accelerometer, which is easy
to access. Secondly, the accelerometer is competent to recognize many types of daily activities since most of them are simple body movements. Compared to body-worn sensors, object
and ambient sensors are better at recognizing activities related
to context and environment such as having coﬀee. Therefore,
it is suggested to use body-worn sensors (mostly accelerometer+gyroscope) for ADL and sports activities. If the activities
are pertaining to some semantic meaning but more than simple
body movements, it is better to combine the object and ambient
sensors. In addition, there are few public datasets for object and
ambient sensors probably because of privacy issues and deployment diﬃculty of the data collecting system. We expect there
will be more open datasets regarding those sensors.
Sensor placement is also important. Most body-worn sensors are placed on the dominant wrist, waist, and the dominant hip pocket. This placement strategy can help to recognize most common daily activities. However, when it comes
to object and ambient sensors, it is critical to deploy them in
a non-invasive way. Those sensors are not usually interacting
with users directly, so it is critical to collect the data naturally
and non-invasively.
Before using deep models, the raw sensor data need to be preprocessed accordingly. There are two important aspects. The
ﬁrst aspect is sliding window. The inputs should be cut into individual inputs according to the sampling rate. This procedure
is similar to conventional PR approaches. The second one is
channels. Diﬀerent sensor modalities can be treated as separate
channels, and each axis of a sensor can also be a channel. Using multi-channel could enhance the representation capability
of the deep model since it can reﬂect the hidden knowledge of
the sensor inputs.
2) Model selection. There are several deep models surveyed
in this article. Then, a natural question arises: which model is
the best for HAR? did an early work by
investigating the performance of DNN, CNN and RNN through
4,000 experiments on some public HAR datasets. We combine their work and our explorations to draw some conclusions:
RNN and LSTM are recommended to recognize short activities
that have natural order while CNN is better at inferring longterm repetitive activities . The reason
is that RNN could make use of the time-order relationship between sensor readings, and CNN is more capable of learning
deep features contained in recursive patterns. For multi-modal
signals, it is better to use CNN since the features can be integrated through multi-channel convolutions 
Literature
Sensor Modality
Deep Model
Application
 
 
ADL, factory, Parkinson
D02, D06, D14
 
Body-worn, ambiemt
Gesture, ADL, transportation
 
 
 
 
D01, D04, Self
 
Object, ambient
 
 
Body-worn, object, ambient
ADL, smart home
D01, D02, D04
 
Factory, health
 
ADL, health
 
 
Body-worn, object, ambient
DNN, CNN, RNN
ADL, smart home, gait
D01, D04, D14
 
 
Body-worn, ambient
ADL, smart home
 
 
D03, D05, D11
 
Respiration
 
Hand gesture
 
 
Body-worn, ambient
ADL, emotion
 
 
 
Patient resuscitation
 
Patient resuscitation
 
 
 
ADL, gesture
 
ADL, smart home
 
ADL, smart home
D01, D02, D05, D14
 
ADL, gesture, posture, factory
 
 
Body-worn, object
ADL, food preparation, factory
D01, D02, D08, D14
 
PAF disease
 
 
ADL, factory
D02, D06, D14, D15
 
ADL, factory, Parkinson
D02, D06, D14, D15
 
 
CNN, RNN, DNN
ADL, sleep
 
 
Body-worn, object, ambient
 
 
Body-worn, ambient
ADL, location
 
Object, ambient
 
Body-worn, object, ambient
ADL, smart home, gesture
 
Body-worn, object
Cartrack, ADL
 
 
Body-worn, ambient, object
ADL, smart home, factory
D01, D02, D10
 
 
 
ADL, smart home
D01, D05, D07
 
 
 
 
ADL, heart failure
Table 5. Performance comparison of existing deep models
 
 
 
DeepConvLSTM
 
 
 
 
 
 
DeepConvLSTM
 
UCI smartphone
 
 
 
 
Zheng et al., 2014; Ha et al., 2015). While adapting CNN, datadriven approaches are better than model-driven approaches as
the inner properties of the activity signal can be exploited better when the input data are transformed into the virtual image.
Multiple convolutions and poolings also help CNN perform better. RBM and autoencoders are usually pre-trained before being
ﬁne-tuned. Multi-layer RBM or SAE is preferred for more accurate recognition.
Technically there is no model which outperforms all the others in all situations, so it is recommended to choose models
based on the scenarios. To better illustrate the performance of
some deep models, Table 5 oﬀers some results comparison of
existing work on public datasets in Table 3 1. In Skoda and UCI
Smartphone protocols, CNN achieves the best performance. In
two OPPORTUNITY protocols, DBN and RNN outperform the
others. This conﬁrms that no models can achieve the best in
all tasks. Moreover, the hybrid models tend to perform better
than single models (DeepConvLSTM in OPPORTUNITY 1 and
Skoda). For a single model, CNN with shifted inputs (Fourier
transform) generates better results compared to shifted kernels.
7. Grand Challenges
Despite the progress in previous work, there are still challenges for deep learning based HAR. In this section, we present
those challenges and propose some feasible solutions.
A. Online and mobile deep activity recognition. Two critical issues are related to deep HAR: online deployment and mobile application. Although some existing work adopted deep
HAR on smartphone and watch , they are still far from online and
mobile deployment. Because the model is often trained oﬄine
on some remote server and the mobile device only utilizes a
trained model. This approach is neither real-time nor friendly
to incremental learning. There are two approaches to tackle this
1OPP 1, OPP 2, Skoda, and UCI smartphone follow the protocols in , , , and , respectively. OPP 1 used weighted f1-score; OPP 2, Skoda, and
UCI smartphone used accuracy.
problem: reducing the communication cost between mobile and
server, and enhancing computing ability of the mobile devices.
B. More accurate unsupervised activity recognition. The
performance of deep learning still relies heavily on labeled samples. Acquiring suﬃcient activity labels is expensive and timeconsuming. Thus, unsupervised activity recognition is urgent.
• Take advantage of the crowd. The latest research indicates
that exploiting the knowledge from the crowd will facilitate the task . Crowd-sourcing takes
advantage of the crowd to annotate the unlabeled activities. Other than acquiring labels passively, researchers
could also develop more elaborate, privacy-concerned way
to collect useful labels.
• Deep transfer learning. Transfer learning performs data
annotation by leveraging labeled data from other auxiliary
domains . There are many factors related to human
activity, which can be exploited as auxiliary information
using deep transfer learning. Problems such as sharing
weights between networks, exploiting knowledge between
activity related domains, and how to ﬁnd more relevant
domains are to be resolved.
C. Flexible models to recognize high-level activities. More
complex high-level activities need to be recognized other than
only simple daily activities. It is diﬃcult to determine the hierarchical structure of high-level activities because they contain
more semantic and context information. Existing methods often
ignore the correlation between signals, thus they cannot obtain
good results.
• Hybrid sensor. Elaborate information provided by the hybrid sensor is useful for recognizing ﬁne-grained activities . Special attention should
be paid to the recognition of ﬁne-grained activities by exploiting the collaboration of hybrid sensors.
• Exploit context information. Context is any information
that can be used to characterize the situation of an entity
 . Context information such as Wi-Fi,
Bluetooth, and GPS can be used to infer more environmental knowledge about the activity. The exploitation of
resourceful context information will greatly help to recognize user state as well as more speciﬁc activities.
D. Light-weight deep models. Deep models often require
lots of computing resources, which is not available for wearable
devices. In addition, the models are often trained oﬀ-line which
cannot be executed in real-time. However, less complex models
such as shallow NN and conventional PR methods could not
achieve good performance. Therefore, it is necessary to develop
light-weight deep models to perform HAR.
• Combination of human-crafted and deep features.
Recent work indicated that human-crafted and deep features
together could achieve better performance . Some pre-knowledge about the activity will greatly
contribute to more robust feature learning in deep models . Researchers should consider the possibility of applying two kinds of features to
HAR with human experience and machine intelligence.
• Collaboration of deep and shallow models. Deep mod-
els have powerful learning abilities, while shallow models
are more eﬃcient. The collaboration of those two models
has the potential to perform both accurate and light-weight
HAR. Several issues such as how to share the parameters
between deep and shallow models are to be addressed.
E. Non-invasive activity sensing. Traditional activity collection strategies need to be updated with more non-invasive
approaches. Non-invasive approaches tend to collect information and infer activity without disturbing the subjects and requires more ﬂexible computing resources.
• Opportunistic activity sensing with deep learning.
Opportunistic sensing could dynamically harness the noncontinuous activity signal to accomplish activity inference . In this scenario, back propagation of deep models should be well-designed.
F. Beyond activity recognition: assessment and assistant.
Recognizing activities is often the initial step in many applications. For instance, some professional skill assessment is required in ﬁtness exercises and smart home assistant plays an
important role in healthcare services. There is some early work
on climbing assessment . With the advancement of deep learning, more applications should be developed
to be beyond just recognition.
8. Conclusion
Human activity recognition is an important research topic
in pattern recognition and pervasive computing.
In this paper, we survey the recent advance in deep learning approaches
for sensor-based activity recognition. Compared to traditional
pattern recognition methods, deep learning reduces the dependency on human-crafted feature extraction and achieves better
performance by automatically learning high-level representations of the sensor data. We highlight the recent progress in
three important categories: sensor modality, deep model, and
application. Subsequently, we summarize and discuss the surveyed research in detail. Finally, several grand challenges and
feasible solutions are presented for future research.
Acknowledgments
This work is supported in part by National Key R & D Program of China (No.2017YFB1002801), NSFC (No.61572471),
and Science and Technology Planning Project of Guangdong
Province (No.2015B010105001). Authors thank the reviewers
for their valuable comments.