IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
On Interpretability of Artiﬁcial
Neural Networks: A Survey
Feng-Lei Fan
, Student Member, IEEE, Jinjun Xiong
, Senior Member, IEEE,
Mengzhou Li
, Graduate Student Member, IEEE, and Ge Wang
, Fellow, IEEE
Abstract—Deep learning as performed by artiﬁcial deep neural
networks (DNNs) has achieved great successes recently in many
important areas that deal with text, images, videos, graphs, and
so on. However, the black-box nature of DNNs has become one
of the primary obstacles for their wide adoption in missioncritical applications such as medical diagnosis and therapy.
Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention.
In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on
interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions,
such as in relation to fuzzy logic and brain science.
Index Terms—Deep learning, interpretability, neural networks,
I. INTRODUCTION
EEP learning has become the mainstream approach
in many important domains targeting common objects,
such as text , images , videos , and graphs .
However, deep learning works as a black box model in the
sense that although deep learning performs quite well in practice, it is difﬁcult to explain its underlying mechanism and
behaviors. Questions are often asked, such as how deep learning makes such a prediction, why some features are favored
over others by a model, and what changes are needed to
improve model performance, etc. Unfortunately, only modest
success has been made to answer these questions.
Interpretability of deep neural networks (DNNs) is essential
to many ﬁelds, and to healthcare , , in particular for the following reasons. First, model robustness is
a vital issue in medical applications. Recent studies suggest that model interpretability and robustness are closely
connected . On the one hand, the improvements in model
Manuscript received October 26, 2020; revised January 9, 2021; accepted
March 10, 2021. Date of publication March 17, 2021; date of current version
November 3, 2021. This work was supported in part by the Rensselaer-IBM
AI Research Collaboration Program ( in part by the IBM
AI Horizons Network ( and in part by NIH under
Grant R01 EB026646, Grant R01 CA233888, Grant R01 CA237267, and
Grant R01 HL151561. (Corresponding author: Ge Wang.)
Feng-Lei Fan, Mengzhou Li, and Ge Wang are with the Department of
Biomedical Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180
USA (e-mail: ).
Jinjun Xiong is with the AI and Hybrid Clouds Systems, IBM Thomas
J. Watson Research Center, Yorktown Heights, NY 10598 USA (e-mail:
 ).
Color versions of one or more ﬁgures in this article are available at
 
Digital Object Identiﬁer 10.1109/TRPMS.2021.3066428
robustness prompt model interpretability. For example, a deep
model trained via adversarial training, a training method that
augments training data with adversarial examples, shows better interpretability (with more accurate saliency maps) than the
same model trained without adversarial examples . On
the other hand, when we understand a model deeply, we can
thoroughly examine its weaknesses because the interpretability can help identify potential vulnerabilities of a complicated
model, thereby improving its accuracy and reliability. Also,
interpretability plays an important role in ethic use of deep
learning techniques . To build patients’ trust in deep learning, interpretability is needed to hold a deep learning system
accountable . If a model builder can explain why a model
makes a particular decision under certain conditions, users
would know whether such a model contributes to an adverse
event or not. It is then possible to establish standards and
protocols to use the deep learning system optimally.
However, the lack of interpretability has become a main
barrier of deep learning in its wide acceptance in missioncritical applications. For example, regulations were proposed
by European Union in 2016 that individuals affected by algorithms have the right to obtain an explanation . Despite
great research efforts made on interpretability of deep learning and availability of several reviews on this topic, we
believe that an up-to-date review is still needed, especially
considering the rapid development of this area. The review of
Zhang and Zhu is mainly on the visual interpretability.
The representative publications from their review fall under
the feature analysis, saliency, and proxy taxonomy in our
review. The review of Chakraborty et al. took opinions
of Lipton on levels of interpretability, and accordingly
structured their review to provide in-depth perspectives but
with limited scope. For example, only 49 references are cited
there. The review of Du et al. has a similar weakness,
only covering 40 papers, which are divided into post-hoc and
ad-hoc explanations, as well as global and local interpretations. Their taxonomy is coarse-grained and neglects a number
of important publications, such as publications on explainingby-text, explaining-by-case, etc. In contrast, our review is much
detailed and comprehensive, with the latest results included.
While publications in are classiﬁed into understanding
the workﬂow of a neural network, understanding the representation of a neural network, and explanation producing,
we cover all these aspects and also discuss the studies on
how to protype an interpretable neural network. Reviews by
Guidotti et al. and Adadi and Berrada cover existing
2469-7311 c⃝2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See for more information.
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
black-box machine learning models instead of focusing on
neural networks. As a result, several hallmark papers on
explaining neural networks are missing in their survey, such
as the interpretation from the perspectives of mathematics and
Arrieta et al. provided an extensive review on explainable AI (XAI), where concepts and taxonomies are clariﬁed,
and challenges are identiﬁed. While that review covers interpretability of AI/ML in general, our review is speciﬁc to DNNs
and offers unique perspectives and insights. Speciﬁcally, our
review is novel in the following senses: 1) We treat posthoc and ad-hoc interpretability separately, because the former explains the existing models, while the latter constructs
interpretable ones; 2) we include widely studied generative
models, advanced mathematical/physical methods that summarize advances in deep learning theory, and the applications
of interpretability in medicine; 3) important methods are illustrated with customized examples and publicly available codes
through GitHub; and 4) interpretability research is a rapidly
evolving ﬁeld, and many research articles are published every
year. Hence, our review should be a valuable and up-to-date
addition to the literature.
Before we start our survey, let us ﬁrst state three essential questions regarding interpretability: 1) What does interpretability mean? 2) Why is interpretability difﬁcult? and
3) How to build a good interpretation method? The ﬁrst question has been well addressed in , and we include their
statements here for completeness. The second question was
partially touched in and , and we incorporate those
comments and complement them with our own views. We
provide our own perspectives on the third question.
A. What Does Interpretability Mean?
Although the word “interpretability” is frequently used,
people do not reach a consensus on the exact meanings
of interpretability, which partially accounts for why current
interpretation methods are so diverse. For example, some
researchers explore post-hoc explanations for models, while
some focus on the interplay mechanism between machineries
of a model. Generally speaking, interpretability refers to the
extent of human’s ability to understand and reason a model.
Based on the categorization of Lipton , we summarize
the implications of interpretability in different levels.
Simulatability is considered as the understanding over the
entire model. In a good sense, we can understand the mechanism of a model at the top level in a uniﬁed theoretical
framework, one example is what was reported in : a class
of radial basis function (RBF) networks can be expressed
by a solution to the interpolation problem with a regularization term, where an RBF network is an artiﬁcial neural
network with RBFs as activation functions. In view of simulatability, the simpler the model, the higher simulatability
the model has. For example, a linear classiﬁer or regressor
is totally understandable. To enhance simulatability, we can
change some facilities of models or use crafted regularization terms.
Decomposability is to understand a model in terms of its
components, such as neurons, layers, blocks, and so on. Such
a modularized analysis is quite popular in engineering ﬁelds.
For instance, the inner working of a complicated system is
factorized as a combination of functionalized modules. A myriad of engineering examples, such as software development
and optical system design have justiﬁed that a modularized
analysis is effective. In machine learning, a decision tree is
a kind of modularized methods, where each node has an
explicit utility to judge if a discriminative condition is satis-
ﬁed or not, each branch delivers an output of a judgement, and
each leaf node represents the ﬁnal decision after computing all
attributes. Modularizing a neural network is advantageous to
the optimization of the network design since we know the role
of each and every component of the entire model.
Algorithmic transparency is to understand the training process and dynamics of a model. The landscape of the objective
function of a neural network is highly nonconvex. The fact that
deep models do not have a unique solution hurts the model
transparency. Nevertheless, it is intriguing that the current
stochastic gradient descent (SGD)-based learning algorithms
still perform efﬁciently and effectively. If we can understand
why learning algorithms work, deep learning research and
applications will be accelerated.
B. Why Is Interpretability Difﬁcult?
After we learn the meanings of interpretability, a question
is what obstructs practitioners to obtain interpretability. This
question was partially addressed in in terms of commercial barrier and data wildness. Here, we complement their
opinion with additional aspects on human limitation and algorithmic complexity. We believe that the hurdles to interpretable
neural networks come from the following four aspects.
Human Limitation: Expertise is often insufﬁcient in many
applications. Nowadays, deep learning has been extensively
used in tackling intricate problems, which even professionals
are unable to comprehend adequately. What is worse is that
these problems are not uncommon. For example, in a recent
study , we proposed to use an artiﬁcial neural network
to predict pseudo-random events. Speciﬁcally, we fed 100 000
binary sequential digits into the network to predict the 100 001
th digit in the sequence. In our prediction, the highly sophisticated hidden relationship was learned to beat a purely random
guess with a 3σ precision. Furthermore, it was conjectured that
high sensitivity and efﬁciency of neural networks may help
discriminate the fundamental differences between pseudorandomness and real quantum randomness. In this case, it is no
wonder that interpretability for neural networks will be missing, because even most talented physicists know little about the
essence of this problem, let alone fully understand predictions
of the neural network.
Commercial Barrier: In the commercial world, there are
strong motives for corporations to hide their models. First and
foremost, companies proﬁt from black-box models. It is not
a common practice that a company makes capital out of totally
transparent models . Second, model opacity helps protect hard work from being reverse engineered. An effective
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
black box is ideal in the sense that customers being served
can obtain satisfactory results while competitors are not able
to steal their intellectual properties easily . Third, prototyping an interpretable model may cost too much in terms of
ﬁnancial, computational, and other resources. Existing opensourced superior models are accessible to easily construct
a well-performed algorithm for a speciﬁc task. However, generating reliable and consistent understanding to the behavior
of the resultant model demands much more endeavors.
Data Wildness: On the one hand, although it is a big
data era, high-quality data are often not accessible in many
domains. For example, in the project of predicting electricity grid failure , the data base involves text documents,
accounting data about electricity dating back to the 1890s,
and data from new manhole inspections. Highly heterogenous and inconsistent data hamper not only the accuracy of
deep learning models but also the construction of interpretability. On the other hand, real-world data have the character of
high dimensionality, which suppresses reasoning. For example, given an MNIST image classiﬁcation problem, the input
image is of size 28 × 28 = 784. Hence, the deep learning
model tackling this problem has to learn an effective mapping
of 784 variables to one of the ten digits. If we consider the
ImageNet dataset, the number of input variables goes up to
512 × 512 × 3 = 768 432.
Algorithmic Complexity: Deep learning is a kind of largescale and highly nonlinear algorithms. Convolution, pooling,
nonlinear activation, shortcut, and so on contribute to the variability of neural networks. The number of trainable parameters
of a deep model can be on the order of hundreds million or
even more. Despite that nonlinearity may not necessarily result
in opacity (for example, a decision tree model is not linear but
interpretable), deep learning’s series of nonlinear operations
indeed prevent us from understanding its inner working. In
addition, recursiveness is another source of difﬁculty. A typical example is the chaos behavior resultant from nonlinear
recursiveness. It is well-known that even a simple recursive
mathematical model can lead to an intractable dynamics .
In , it was proved that there are chaotic behaviors such
as bifurcations even in simple neural networks. In chaotic
systems, tiny changes of initial inputs may lead to huge outcome differences, adding to the complexity of interpretation
C. How to Build Good Interpretation Method?
The third major issue is the criteria for assessing the
quality of a proposed interpretation method. Because existing evaluation methods are still premature, we propose
ﬁve general and well-deﬁned rules-of-thumb: 1) exactness;
2) consistency; 3) completeness; 4) universality; and 5)
reward. Our rules-of-thumb are ﬁne-grained and focus on
the characteristics of interpretation methods, compared to that
described in : application-grounded, human-grounded, and
function-grounded.
Exactness: Exactness means how accurate an interpretation
method is. Is it just limited to a qualitative description or with
a quantitative analysis? Generally, quantitative interpretation
methods are more desirable than qualitative counterparts.
Consistency: Consistency suggests that there is no contradiction in an explanation. For multiple similar samples, a fair
interpretation should produce consistent answers. In addition,
an interpretation method should conform to the predictions of
the authentic model. For example, the proxy-based methods
are evaluated based on how closely they replicate the original
golden model.
Completeness: Mathematically, a neural network is to learn
a mapping that best ﬁts data. A good interpretation method
should show effectiveness in support of the maximal number
of data instances and data types.
Universality: With the rapid development of deep learning,
the deep learning armory has been substantially enriched. Such
diverse deep learning models play important roles in a wide
spectrum of applications. A driving question is whether we can
develop a universal interpreter that deciphers as many models
as possible so as to save labor and time. But, this is technically
challenging due to the high variability among models.
Reward: What are gains from the improved understanding
of neural networks? In addition to the trust from practitioners
and users, fruits of interpretability can be insights into network
design, training, etc. Due to its black-box nature, using neural
networks is largely a trial-and-error process with sometimes
contradictive intuitions. A thorough understanding of deep
learning will be instrumental to the research and applications
of neural networks.
Brieﬂy, our contributions in this review are threefold: 1) we
propose a comprehensive taxonomy for interpretability of neural networks and describe key methods with our insights; 2) we
systematically illustrate interpretability methods as educational
aids, as shown in Figs. 3, 5, 6, 7, 9, 10, 16, and 17; and 3) we
shed light on future directions of interpretability research in
terms of the convergence of neural networks and rule systems,
the synergy between neural networks and brain science, and
interpretability in medicine.
II. SURVEY ON INTERPRETATION METHODS
In this section, we ﬁrst present our taxonomy and then
review interpretability results under each category of our
taxonomy. We enter the search terms “deep learning interpretability,” “neural network interpretability,” “explainable
neural network,” and “explainable deep learning” into the Web
of Science on Sep 22, 2020, with the time range from 2000 to
2019. The number of articles with respect to years is plotted
in Fig. 1, which clearly shows an exponential trend in this
ﬁeld. With the survey, our motive is to cover as many important papers as possible. Therefore, we do not limit ourselves
within Web of Science. We also search related articles using
Google Scholar, PubMed, IEEE Xplore, and so on.
A. Taxonomy Deﬁnition
As shown in Fig. 2, our taxonomy is based on our surveyed
papers and existing taxonomies. We ﬁrst classify the surveyed
papers into Post-hoc interpretability analysis and ad-hoc interpretable modeling. Post-hoc interpretability analysis explains
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
Exponential growth of the number of articles on interpretability.
Taxonomy used for this interpretability review.
the existing models and can be further classiﬁed into feature
analysis, model inspection, saliency, proxy, advanced mathematical/physical analysis, explaining-by-case, and explainingby-text, respectively. Ad-hoc interpretable modeling builds
interpretable models and can be further categorized into interpretable representation and model renovation. In our proposed
taxonomy, the class advanced mathematical/physical analysis
is novel, but it is unfortunately missing in the previous reviews.
We argue that this class is rather essential, because the incorporation of math/physics is critical in placing deep learning on
a solid foundation. In the following, we clarify the taxonomy
deﬁnition and its illustration. We would like to underscore that
one method may fall into different classes, depending on how
one views it.
Post-Hoc Interpretability Analysis: Post-hoc interpretability
is conducted after a model is well learned. A main advantage
of post-hoc methods is that one does not need to compromise interpretability with the predictive performance since
prediction and interpretation are two separate processes without mutual interference. However, a post-hoc interpretation
is usually not completely faithful to the original model. If
an interpretation is 100% accurate compared to the original model, it becomes the original model. Therefore, any
interpretation method in this category is more or less inaccurate. What is worse is that we often do not know the
nuance . Such a nuance makes it hard for practitioners
to have a full trust to an interpretation method, because the
correctness of the interpretation method is not guaranteed.
Feature analysis techniques are centered in comparing, analyzing, and visualizing features of neurons and layers. Through
feature analysis, sensitive features and ways to process them
are identiﬁed such that the rationale of the model can be
explained to some extent.
Feature analysis techniques can be applied to any neural
networks and provide qualitative insights on what kinds of
features are learned by a network. However, these techniques
lack an in-depth, rigorous, and uniﬁed understanding, and
therefore cannot be used to revise a model toward a higher
interpretability.
Model inspection methods use external algorithms to delve
into neural networks by systematically extracting important
structural and parametric information on inner working mechanisms of neural networks.
Methods in this class are more technically accountable
than those in feature analysis because analytical tools such
as statistics are directly involved in the performance analysis. Therefore, the information gained by a model inspection
method is more trustworthy and rewarding. In an exemplary
study , ﬁnding important data routing paths is used as
a way to understand the model. With such data routing paths,
the model can be faithfully compressed to a compact one. In
other words, interpretability improves the trustworthiness of
model compression.
Saliency methods identify which attributes of input data are
most relevant to a prediction or a latent representation of
a model. In this category, human inspection is involved to
decide if a saliency map is plausible. A saliency map is useful, i.e., if a polar bear always appears in a picture coupled
with snow or ice, the model may have misused the information
of snow or ice to detect the polar bear rather than real features
of polar bears for detection. With a saliency map, this issue
can be found and hence avoided.
interpretability
research, however, extensive random tests reported that
some saliency methods might be model independent and
data independent ; i.e., saliency maps offered by some
methods are highly similar to results produced with edge
detectors. This is problematic because it means that those
saliency methods fail to ﬁnd the true attributes of the input
that account for the prediction of the model. Consequently,
a model-relevant and data-relevant saliency method should be
developed in these cases.
Proxy methods construct a simpler and more interpretable
proxy that closely resembles a trained, large, complex, and
black-box model. Proxy methods can be either local in a partial space or global in a whole solution space. The exemplary
proxy models include decision trees, rule systems, and so on.
The weakness of proxy methods is the extra cost needed to
construct a proxy model.
mathematical/physical
a neural network into a theoretical mathematics/physics
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
framework, in which the mechanism of a neural network
is understood with advanced mathematics/physics tools. This
class covers theoretical advances of deep learning including
nonconvex optimization, representational power, and generalization ability.
A concern in this class is that to establish a reasonable
interpretation, unrealistic assumptions are sometimes made to
facilitate a theoretical analysis, which may compromise the
practical validity of the explanation.
Explaining-by-case methods are along the line of casebased reasoning . People favor examples. One may not
be engaged by boring statistic numbers of a product but could
be amazed while listening to other users’ experience of using
such a product. This philosophy wins the heart of many practitioners and intrigues the case-based interpretation for deep
learning. Explaining-by-case methods provide representative
examples that capture the essence of a model.
Methods in this class are interesting and inspiring. However,
this practice is more like a sanity check instead of a general interpretation because not much information regarding the
inner working of a neural network is understood from selected
query cases.
Explaining-by-text methods generate text descriptions in
image-language joint tasks that are conducive to understanding
the behavior of a model. This class can also include methods
that generate symbols for explanation.
Methods in this class are particularly useful in imagelanguage joint tasks such as generating a diagnostic report
from an X-ray radiograph. However, explaining-by-text is not
a general technique for any deep learning model because it
can only work when a language module exists in a model.
Interpretable
interpretable
modeling eliminates the biases that more or less exist in
the post-hoc interpretability analysis. Although it is generally
believed that there is a tradeoff between interpretability and
model expressibility , it is still possible to ﬁnd a model
that is both powerful and interpretable. One notable example
is the work reported in , where an interpretable twolayer additive risk model has won the ﬁrst place in FICO
Recognition Contest.
Interpretable representation methods employ regularization
techniques to steer the optimization of a neural network
toward a more interpretable representation. Properties, such as
decomposability, sparsity, and monotonicity can enhance interpretability. As a result, regularizing features becomes a way
to allow more interpretable models. Correspondingly, the loss
function must contain a regularization term for the purpose of
interpretability, which restricts the original model to perform
its full learning task.
Model renovation methods seek interpretability by the
interpretable
machineries into a network. Those machineries include a neuron with purposely designed activation function, an inserted
layer with a special functionality, a modularized architecture,
and so on. The future direction is to use more and more
explainable components that can at the same time achieve
similar state-of-the-art performance for diverse tasks.
B. Post-Hoc Interpretability Analysis
Feature Analysis: Inverting-based methods , ,
 , crack the representation of a neural network by
inverting feature maps into a synthesized image. For example,
Mahendran and Vedaldi assumed that a representation
of a neural network 0 for an input image x0 was modeled as
0 = (x0), where  is the neural network mapping, usually not invertible. Then, the inverting problem was formulated
as ﬁnding an image x∗, whose neural network representation best matches 0; i.e., arg min x ∥(x) −0∥2 + λR(x),
where R(x) is a regularization term representing prior knowledge about the input image. The goal is to reveal the lost
information by comparing differences between the inverted
image and the original one. Dosovitskiy and Brox directly
trained a new network with features generated by the model of
interest as the input and images as the label, to invert features
of intermediate layers to images. It was found that contours
and colors could still be reconstructed even from deeper layer
features. Zeiler and Fergus designed a deconvolution
network consisting of unpooling, rectiﬁcation, deconvolution
operations, to pair with the original convolutional network so
that features could be inverted without training. In the deconvolution network, an unpooling layer is realized by using
locations of maxima; rectiﬁcation is realized by setting negative values to 0; and deconvolution layers use transposed
convolutional ﬁlters.
Activation maximization methods , , , 
devote to synthesizing images that maximize the output of
a neural network or neurons of interest. The resulting images
are referred as “deep dreams” as these can be regarded as
dream images of a neural network or a neuron.
In , , , , and , it was pointed out
that information about a deep model could be extracted from
each neuron. Yosinski et al. straightforwardly inspected
the activation values of neurons in each layer with respect to
different images or videos. They found that live activation values changes for different inputs are helpful to understand how
a model work. Li et al. contrasted features generated by
different initializations to investigate if a neural network learns
a similar representation when randomly initialized. The receptive ﬁeld (RF) is a spatial extent over which a neuron connects
with an input volume . To investigate the size and shape
of RF of a given input for a neuron, Zhou et al. presented
a network dissection method that ﬁrst selected K images with
high activation values for neurons of interest and constructed
5000 occluded images for each of K images, and then fed them
into a neural network to observe the changes in activation values for a given unit. A large discrepancy signals an important
patch. Finally, the occluded images that have large discrepancy
were recentered and averaged to generate an RF. This network
dissection method has been scaled to generative networks .
In addition, Bau et al. scaled up a low-resolution activation map of a given layer to the same size as the input,
thresholded the map into a binary activation map, and then
computed the overlapping area between the binary activation
map and the ground-truth binary segmentation map as an interpretability measure. Karpathy et al. deﬁned the gate in
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
Based on the inﬂuence function, two harmful images that have the
same label as the test image are identiﬁed.
LSTM to be either left or right saturated depending on
its activation value being either less than 0.1 or more than
0.9. In this regard, neurons that were often right saturated are
interesting because this means that these neurons can remember their values over a long period. Zhang et al. dissected
feature relations in a network with the premise that the feature
map of a ﬁlter in each layer can be activated by part patterns
in the earlier layer. They mined part patterns layer by layer,
discovered activation peaks of part patterns from the feature
map of each layer, and constructed an explanatory graph to
describe the relations of hierarchical features, with each node
representing a part pattern and the edge between neighboring
layers representing a co-activation relation.
Model Inspection: The empirical inﬂuence function is to
measure the dependence of an estimator on a sample .
Koh and Liang applied the concept of the inﬂuence function to address the following question: Given a prediction for
one sample, do other samples in the dataset have positive
effects or negative effects on that prediction? This analysis
could also help identify misannotated labels and outliers existing in the data. As Fig. 3 shows, given a LeNet-5 like network,
two harmful images for a given image are identiﬁed by the
inﬂuence function.
Zhang et al. worked on the detection of failures
or biases in a neural network. For example, Bansal et al. 
developed a model-agnostic algorithm to identify which
instances a neural network is likely to fail to provide any
prediction for. In such a scenario, the model would instead
give a warning like “Do not trust these predictions” as an
alert. Speciﬁcally, they annotated all failed images with
a collection of binary attributes and clustered these images in
the attribute space. As a result, each cluster indicates a failure
mode. To recognize those mislabeled instances with high
predictive scores in the dataset efﬁciently, Lakkaraju et al. 
introduced two basic speculations: the ﬁrst is that mislabeling
an instance with high conﬁdence is due to the systematic
biases instead of random perturbation, while the second is
that each failed example is representative and informative
enough. Then, they clustered the images into several groups
and designed a multiarmed bandit search strategy by taking
each group as a bandit that plans which group should be
queried and sampled in each step. To discover representation
biases, Zhang et al. utilized ground-truth relationships
among attributes according to human’s common knowledge
(ﬁre-hot versus ice-cold) to examine if a mined attribute
relationship by a neural network well ﬁts the ground truth.
Wang et al. demystiﬁed a network by identifying
critical data routes. Speciﬁcally, a gate control binary vector λk ∈{0, 1}nk, where nk is the number of neurons in the
kth layer, was multiplied to the output of the kth layer, and
the problem of ﬁnding control gate values is formulated as
searching λ1, . . . , λK by the formula
d(fθ(x), fθ(x; λ1, . . . , λK)) + γ
where fθ is the mapping represented by a neural network
parameterized by θ, fθ(x; λ1, . . . , λK) is the mapping when
control gates λ1, . . . , λK are enforced, d(·, ·) is a distance measure, γ is a constant controlling the tradeoff between the loss
and regularization, and ∥· ∥1 is the l1 norm such that λk is
sparse. The learned control gates could expose the important
data processing paths of a model. Kim et al. developed the
concept activated vector (CAV) that can quantitatively measure
the sensitivity of the concept C with respect to any layer of
a model. First, a binary linear classiﬁer h was trained to distinguish between layer activations stimulated by two sets of
samples: {fl(x) : x ∈PC} and {fl(x) : x /∈PC}, where fl(x) is the
layer activation at the lth layer, and PC denotes data embodying the concept C. Then, the CAV was deﬁned as the normal
unit vector vl
C to a hyperplane of the linear classiﬁer that separated samples with and without the deﬁned concept. Finally,
C was used to calculate the sensitivity for a concept C in the
lth layer as the directional derivatives
SC,k,l = lim
fl(x) + ϵvl
−hl,k(fl(x))
= ∇hl,k(fl(x))vl
where hl,k denotes the logits of the trained binary linear
classiﬁer for the output class k. You et al. mapped
a neural network into a relational graph and then studied the
relationship between the graph structures of neural networks
and their predictive performance through massive experiments
(transcribed a graph into a network and implemented the
network on a dataset). They discovered that the predictive
performance of a network was correlated with two graph measures: 1) the clustering coefﬁcient and 2) the average path
Saliency: There is a plethora of methods to obtain a saliency
map. Partial dependence plot (PDP) and individual condition
expectation (ICE) , , are model-agnostic statistical
tools to visualize the dependence between the responsible variables and the predictive variables. To compute the PDP, assume
that there are p input dimensions and let S, C ⊆{1, 2, . . . , p}
be two complementary sets, where S is the set one will ﬁx, and
C is the set one will change. Then, the PDP for xS is deﬁned
f(xS, xC)dxC, where f is the model. Compared with
PDP, the deﬁnition of ICE is straightforward. The ICE curve
at xS is obtained by ﬁxing xC and varying xS. Fig. 4 shows
a simple example illustrating how to compute PDP and ICE,
respectively.
A simple approach is to study the change of prediction
leave-oneout
attribution , , , , .
Kádár et al. utilized this idea to deﬁne an omission score:
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
Fig. 4. Toy examples illustrating the deﬁnitions of PDP and ICE, respectively.
On the left, to measure the impact of the brand on the price with the PDP
method, we ﬁx the brand and compute the average of prices as other factors
change, obtaining that the PDP of “Huawei” is 2500 and the PDP of “Apple”
is 4000. On the right, ICE scores regarding brands “Huawei,” “Vivo,” and
Apple are computed by varying brands and ﬁxing other factors.
1 −cosine(h(S), h(S\i)), where cosine(·, ·) is the cosine distance, h is the representation for a sentence, S is the full
sentence, and S\i is the sentence without the ith word, and analyzed the importance of each word. Adler et al. proposed
to measure an indirect inﬂuence for correlated inputs. For
example, in a house loan decision system, race should not
be a factor for decision making. However, solely removing
the race factor is not sufﬁcient to rule out the effect of race
because some remaining factors, such as “zipcode” are highly
concerned with race.
Furthermore, the Shapley value from cooperative game
in , , , , and .
Mathematically, the Shapley value of a set function ˆf
with respect to the feature i is deﬁned as
(N −|S| −1)!|S|!
ˆf(S ∪{i}) −ˆf(S)
where | · | is the size of a set, P is a total player set of N
players, and the set function ˆf maps each subset S ⊆P to
a real number. Furthermore, the deﬁnition of the Shapley value
can be twisted to the neural network function f by replacing the features in the input that are not in S with the zero
value. Motivated by reducing the prohibitive computational
cost incurred by combinatorial explosion, Ancona et al. 
proposed a novel and polynomial-time approximation of the
Shapley values, which basically computed the expectation of
a random coalition rather than enumerated each and every
coalition. Fig. 5 shows a simple example of how the Shapley
values can be computed for a fully connected layer network
trained on the California Housing dataset, which includes eight
attributes, such as house age and room number as the inputs
and the house price as the label.
Instead of removing one or more features, researchers also
resort to gradients. Simonyan et al. , Smilkov et al. ,
Sundararajan et al. , and Singla et al. utilized
the idea of gradients to probe the saliency of an input.
Simonyan et al. calculated the ﬁrst-order Taylor expansion of the class score with respect to image pixels, by
which the ﬁrst-order coefﬁcients produce a saliency map
Positive Shapley value indicates a positive impact on the model
output, and vice versa. Shapley value analysis shows that the model is biased
because the house age has the positive Shapley value on the house price,
which goes against our real experience.
for a class. Smilkov et al. demonstrated that gradients as a saliency map show a correlation between attributes
and labels, however, typically gradients are rather noisy. To
remove noise, they proposed SmoothGrad that adds noise
into the input image multiple times and averages the resultant gradient maps 
Mc(x) = (1/N) N
c (x + N(0, σ 2)),
where M(n)
is a gradient map for a class c, and N(0, σ 2)
is the Gaussian noise with σ
as the standard variance.
Basically, 
Mc(x) is a smoothened version of a salient map.
Sundararajan et al. set two fundamental requirements
for saliency methods: 1) (sensitivity) if only one feature is
different between the input and the baseline, the outputs of
the input and the baseline are different, then this very feature
should be credited by a nonzero attribution; and 2) (implementation invariance) the attributions for the same feature in two
functionally equivalent networks should be identical. Noting
that earlier gradient-based saliency methods did not meet the
above two requirements, they put forth integrated gradients,
which is formulated as (xi−x′
0 [∂F(x′ + α(x −x′))/∂xi]dα,
where F(·) is a neural network mapping, x = (x1, x2, . . . , xN)
is an input, and x′ = (x′
2, . . . , x′
N) is the baseline satisfying (∂/∂x)F(x)|x=x′ = 0. In practice, the integral can
be transformed into a discrete summation [(xi −x′
m=1 [∂F(x′ + (m/M)(x −x′))/∂xi], where M is the number
of steps in the approximation of the integral. Singla et al. 
proposed to use the second-order approximations of a Taylor
expansion to produce a saliency map so as to consider feature
dependencies.
Bach et al. proposed layerwise relevance propagation (LRP) to compute the relevance of one attribute to
a prediction by assuming that a model representation f(x) can
be expressed as the sum of pixelwise relevance Rl
p, where x
is an input image, l is the index of the layer, and p is the
index of the pixel of x. Thus, f(x) =
p, where L is the
ﬁnal layer and RL
p = [(wpxL−1
)]f(x), where wp
is the weight between pixel p of the (L −1)th layer and the
ﬁnal layer. Given a feedforward neural network, the pixelwise
relevance score R1
p of an input is derived by calculating Rl
p′ zp′j)]Rl+1
backwards with zpj = xl
is the weight between pixel p of layer l and the pixel
j of the (l + 1)th layer. Furthermore, Arras et al. extended
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
LRP to recurrent neural networks (RNNs) for sentiment analysis. Montavon et al. employed the whole ﬁrst-order
term of deep Taylor decomposition to produce a saliency
map instead of just gradients. Suppose
x is a well-chosen
root for the function by a model f(x): f(
x) = 0, because
f(x) can be decomposed as f(x) = f(
x) + ([∂f/∂x]|x=
x) + ϵ = 0 +
i (∂f/∂xi)|x=
x(xi −ˆxi) + ϵ, where ϵ
is the high-order terms, the pixel relevance for the pixel i
is expressed as Ri = [∂f/∂xi]|x=
x(xi −ˆxi). Inspired by the
fact that even though a neuron is not ﬁred, it is still likely
to reveal useful information, Shrikumar et al. proposed
DeepLIFT to compute the difference between the activation
of each neuron and its reference, where the reference is the
activation of that neuron when the network is provided a reference input, and then backpropagate the difference to the
image space layer by layer as LRP does. Singh et al. 
introduced contextual decomposition, whose layer propagation
formula is βi = Wβi−1 + [|Wβi−1|/(|Wβi−1| + |Wγi−1|)] · b
and γi = Wγi−1 + [|Wγi−1|/(|Wβi−1| + |Wγi−1|)] · b, where
W is the weight matrix between the ith and (i −1)th layers and b is the bias vector. The restricting condition is
gi(x) = βi(x) + γi(x), where gi(x) is the output of ith layer.
βi(x) is considered as the contextual contribution of the input
and γi(x) implies contribution of the input to gi(x) that is not
included in βi(x).
evaluation
gradients,
SmoothGrad, IntegratedGrad, and Deep Taylor methods with
a LeNet-5-like network. Among them, IntegratedGrad and
Deep Taylor methods perform superbly on ﬁve digits.
A mutual-information measure to quantify the association between inputs and latent representations of a deep
model can also similarly work as saliency , , .
In addition, there are other methods to obtain saliency
maps as well. Ross et al. deﬁned a new loss term
i (Ai(∂/∂xi) K
k=1 log(ˆyk))
2 for training, where i is an index
of a pixel, Ai is the binary mask to be optimized, ˆyk is the
kth digit of the label, and K is the number of class. This
loss is to penalize the sharpness of gradients toward a clearer
interpretation boundary. Fong and Vedaldi explored to
learn the smallest region to delete, which is to ﬁnd the
optimal m∗
m∗= arg min
m∈ n λ∥1 −m∥1 + fc(x0; m)
where m is the soft mask, fc(x0; m) represents the loss of
the network for an image x0 with the soft mask, and n is
the number of pixels. Lei et al. utilized a generator to
specify segments of an original text as the so-called rationales,
which fulﬁll two conditions: 1) rationales should be sufﬁcient
as a replacement for the initial text, and 2) rationales should be
short and coherent. Deriving rationales is actually equivalent
to deriving a binary mask, which can be regarded as a saliency
map. Based on the above two constraints, the penalty term for
a mask is formulated as
(z) = λ1∥z∥1 + λ2
|zt −zt−1|
where z = [z1, z2, . . . ] is a mask, the ﬁrst term penalizes the
number of rationales, and the second term is for smoothness.
Interpreting a LeNet-5-like network by raw gradient, SmoothGrad,
integrated gradient, and deep Taylor methods, respectively. It is seen that integrated gradient and deep Taylor methods have sharper and less noisy saliency
The class activation map method (CAM ) and its
variant utilized global average pooling before a fully
connected layer to derive the discriminative area. Speciﬁcally,
let fk(x, y) represent the kth feature map, for a given class c, the
input to the softmax layer is
x,y fk(x, y),where wc
the weight vector connecting the kth feature map and the class
c. The discriminative area is obtained as
kfk(x, y), which
directly implies the importance of the pixel at (x, y) for class c.
What is more, some weakly supervised learning methods such
as can obtain discriminative areas as well. Speciﬁcally,
they trained a network only with object labels; however, when
they rescaled the feature maps produced by the max-pooling
layer, it was surprisingly found that these feature maps were
consistent with the locations of objects in the input.
Proxy: There are about three ways to prototype a proxy.
The ﬁrst one is direct extraction. The gist of direct extraction
is to construct a new interpretable model, such as a decision tree , or a rule-based system directly from the
trained model. As far as the rule extraction is concerned, both
decompositional and pedagogical methods , 
can be used. Pedagogical approaches extract rules that enjoy
a similar input–output relationship with that of a neural
network. These rules do not correspond to the weights and
structure of the network. For example, the validity interval
analysis (VIA) extracts rules in the following form:
IF (input ∈a hypercube), THEN input belongs to a certain
Setiono and Liu clustered hidden unit activation values based on the proximity of activation values. Then, the
activation values of each cluster were denoted by their average activation values, at the same time kept the accuracy of the
neural network as intact as possible. Next, the input data with
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
Rule extraction process as proposed by Setiono and Liu .
(a) One-hidden-layer network with three hidden neurons is constructed to
classify the Iris dataset. (b) Rules are extracted via discretizing activation
values of hidden units and clustering of inputs, where Petal length and Petal
width are dominating attributes for classiﬁcation of Iris samples. The extracted
rules have the same classiﬁcation performance as that of the original neural
the same average hidden unit activation value were clustered
together to obtain a complete set of rules. In Fig. 7, we illustrate obtained rules from a one-hidden-layer network using
Setiono and Liu’s method over the Iris dataset. In a neural
network for a binary classiﬁcation problem, decision boundaries divide the input space into two parts, corresponding to
two classes, respectively. The explanation system HYPINV
developed in computed for each and every decision
boundary hyperplane a tangent vector. The sign of an inner
product between an input instance and a tangent vector will
imply the position of the input instance relative to the decision boundary. Based on such a fact, a rule system can be
established.
Finally, some specialized networks, such as ANFIS and
RBF networks , straightforwardly correspond to fuzzy
logic systems. For example, an RBF network is equivalent
to the Takagi–Sugeno rule system that comprises rules,
such as “if x ∈set A and y ∈set B, then z = f(x, y)” .
Fuzzy logic interpretation in considers each neuron/ﬁlter
in a network as a generalized fuzzy logic gate. In this
view, a neural network is nothing but a deep fuzzy logic
system. Speciﬁcally, they analyzed a new type of neural
networks, called quadratic networks, in which all the neurons are quadratic neurons that replace the inner product with
the quadratic operation . Their interpretation generalized
fuzzy logic gates implemented by quadratic neurons and then
computed the entropy based on spectral information of fuzzy
operations in a network. It was suggested that such an entropy
could have deep connections with properties of minima and
the complexity of neural networks.
The second one is called knowledge distillation , as
shown Fig. 8. Although knowledge distillation techniques are
mostly used for model compression, their principles can also
be used for interpretability. The motif of knowledge distillation
is that cumbersome models can generate relatively accurate
predictions, assigning probabilities to all the possible classes,
known as soft labels, that are more informative than one-hot
labels. For example, a horse is more likely to be classiﬁed
as a dog instead of a mountain. But, with one-hot labeling,
both the dog class and mountain class have zero probability.
It was shown in that by the means of matching the logits
of the original model, the generalization ability of the original
cumbersome model could be transferred into a simpler model.
Knowledge distillation is to construct an interpretable proxy by the
soft labels from the original complex models.
Breast cancer classiﬁcation task model dissected by LIME. In this
case, the sample is classiﬁed as benign, where the worst concave point, mean
concave point, and so on are contributing forces, while the worst perimeter
is the contributing force to drive the model to predict “malignant.”
Along this direction, an interpretable proxy model, such as
a decision tree , , a decision set , a global additive model , and a simpler network were developed.
For example, Tan et al. used soft labels to train a global
additive model in the form of h0+
i̸=j hij(xi, xj)+
j̸=k hijk(xi, xj, xk)+· · · , where {hi}i≥1 could also work
as a feature saliency map directly.
The last one is to provide a local explainer as a proxy.
Local explainer methods locally mimic the predictive behaviors of a neural network. The basic rationale is that when
a neural network is inspected globally, it looks complex.
However, if we tackle it locally, the picture becomes
One typical local explainer is local interpretable modelagnostic explanation (LIME) , which synthesizes a number of neighbor instances by randomly setting elements of that
sample to 0 and computing the corresponding outcomes. Then,
a linear regressor is used to ﬁt synthesized instances, where
the coefﬁcients of the linear model signify the contributions
of features. As Fig. 9 shows, the LIME method is applied to
a breast cancer classiﬁcation model to identify which attributes
are contributing forces for the model’s benign or malignant
prediction.
Zhang et al. pointed out the lack of robustness in
the LIME explanation, which originates from sampling variance, sensitivity to the choice of parameters, and variation
across different data points. Anchor is an improved
extension of LIME, which is to ﬁnd the most important
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
ODE-Net optimizes the start point and the dynamics to ﬁt the
spiral shape.
segments of an input such that the variability of the rest
segments does not matter. Mathematically, Anchor searches
a set: A = {z|f(z) = f(x), z ∈x}, where f(·) is a black-box
model, x is the input and z is the part of x. Another proposal
local rule-based explanation (LORE) is from . The LORE
takes advantage of the genetic algorithm to generate the balanced neighbors instead of random neighbors, thereby yielding
high-quality training data that alleviates sampling variance
Advanced Mathematical/Physical Analysis: Lu et al. 
showed that many residual networks can be explained as discretized numerical solutions of ordinary differential equations;
i.e., the inner working of a residual block in ResNet can
be modeled as un+1 = un + f(un), where un is the output
of the nth block, and f(un) is the block operation. It was
noted that un+1 = un + f(un) is a one-step ﬁnite difference
approximation of an ordinary differential equation du/dt =
f(u). This idea inspired the invention of ODE-Net . As
Fig. 10 shows, the starting point and the dynamics are tuned
by an ODE-Net to ﬁt a spiral.
Lei et al. constructed an elegant connection between
the Wasserstein generative adversarial network (WGAN )
and the optimal transportation theory. They concluded that
with low dimensionality hypothesis and the intentionally
designed distance function, a generator and a discriminator
can exactly represent each other in a closed form. Therefore,
the competition between a discriminator and a generator in
WGAN in the training is unnecessary.
In , it was proposed that the learning of a neural
network is to extract the most relevant information in the input
random variable X that pertains to an output random variable
Y. Naively, for a feedforward neural network, the following
inequality of mutual information holds:
I(Y; X) ≥I
≥I(Y; hi) ≥I
where I(·; ·) denotes the mutual information, hi and hj are
outputs of hidden layers (i > j means that the ith layer is
deeper), and ˆY is a ﬁnal prediction. Furthermore, Yu and
Principe employed an information bottleneck theory to
gauge the mutual information states of symmetric layers in
a stacked autoencoder as shown in Fig. 11
Application of the information bottleneck theory to compare mutual
information between symmetric layers in an autoencoder.
However, it is tricky to estimate the mutual information since
the probabilistic distribution of data is usually unknown as
Kolouri et al. built an integral geometric explanation
for neural networks with a generalized Radon transform. Let
X be a random variable for the input, which conforms to the
distribution pX, then we can derive a probability distribution
function for the output of a neural network fθ(X) parametrized
with θ: pfθ (z) =
X pX(x)δ(z −fθ(x))dx, which is the generalized Radon transform, and the hypersurface is H(t, θ) = {x ∈
X|fθ(x) = t}. In this regard, the transform by a neural network
is characterized by the twisted hypersurfaces. Huang used
the mean-ﬁeld theory to characterize the mechanism of dimensionality reduction by a deep network that assumes weights in
each layer and input data following a Gaussian distribution.
In his study, the self-covariance matrix of the output of the
lth layer was computed as Cl, then the intrinsic dimensionality was deﬁned as D = [(( N
i )], where
λi is the eigenvalue of Cl, and N is the number of eigenvalues. The quantity D/N was investigated across layers to
analyze how compact representation are learned across layers.
Ye et al. utilized a framelet theory and a low-rank Hankel
matrix to represent signals in terms of their local and nonlocal
bases, corresponding to convolution and generalized pooling
operations. However, in their study, the network structure was
simpliﬁed in concatenating two ReLU units into a linear unit
such that the nonlinearity from ReLU units could be circumvented. As far as advanced physics models are concerned,
Mehta and Schwab built an exact mapping from the
Kadanoff variational renormalized group to the restricted
Boltzmann Machine (RBM) . This mapping is independent of forms of the energy functions and can be scaled to
Theoretical neural network studies are essential to interpretability as well. Currently, theoretical foundations of deep
learning are primarily from three perspectives: 1) representation; 2) optimization; and 3) generalization.
Representation: Let us include two examples here. The
ﬁrst example is to explain why deep networks are superior to the shallow ones. Recognizing successes of deep
Shamir ,
Liang and Srikant , Mhaskar and Poggio , and
Szymanski and McCane justiﬁed that a deep network
is more expressive than a shallow one. The basic idea is to
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
construct a special class of functions that can be efﬁciently
represented by a deep network but hard to be approximated
by a shallow one. The second example is to understand utilities of shortcut connections of deep networks. Veit et al. 
showed that residual connections can render a neural network
to manifest an ensemble-like behavior. Along this direction,
it was reported in that with shortcuts, a network can be
super slim to allow for universal approximation.
Optimization: Generally, optimizing a deep network is
an NP-hard nonconvex problem. The pervasive existence of
saddle points leads to that even ﬁnding a local minimum is also NP-hard . Of particular interest to us is
why an over-parametrized network can still be optimized
well because a deep network is a kind of over-parametrized
networks. The character of an over-parameterized network is
that the number of parameters in a network much exceeds
the number of data instances. Soltanolkotabi et al. 
showed that when data are Gaussian distributed and activation functions of neurons are quadratic, the landscape of
an over-parameterized one-hidden-layer network allows global
optimum to be searched efﬁciently. Nguyen and Hein 
demonstrated that with respect to linearly separable data, under
assumptions on the rank of weight matrices of a feedforward neural network, every critical point of a loss function
is a global minimum. Furthermore, Jacot et al. showed
that when the number of neurons in each layer of a neural
network goes inﬁnitely large, the training only renders small
changes for the network function. As a result, the training of
the network turns into the kernel ridge regression.
Generalization: The conventional generalization theory is
incompetent to explain why a deep network can generalize
well despite that the number of parameters of a deep network
is many more than the number of samples. Recently proposed
generalization bounds that rely on the norm of weight
matrices partially solved this problem. However, these bounds
have an abnormal dependence on data that more data lead to
a larger generalization bound, which apparently contradicts the
common sense. We prospect that more efforts are needed to
resolve the generalization puzzle satisfactorily , .
Explaining-by-Case:
Basically,
case-based
explanations
present a case that is believed by a neural network to be the
most similar to the query case needing an explanation. Finding
a similar case for explanation and selecting a representative
case from data as the prototype are basically the same
thing and just use different metrics for similarity. While prototype selection is to ﬁnd a minimal subset of instances that
can represent the whole dataset, case-based explanations use
the similarity metric based on the closeness of representations
of a neural network, thereby exposing the hidden representation information. In this light, case-based explanations are also
related to deep metric learning .
As shown in Fig. 12, Wallace et al. employed the
k-nearest neighbor algorithm to obtain the most similar cases
for the query case in the feature space and then computed the
percentage of the nearest neighbors belonging to the expected
class as a measure for interpretability, suggesting how much
a prediction is supported by data. Chen et al. constructed
a model that could dissect images by ﬁnding prototypical parts.
Explaining-by-case presents the nearest neighbors in response to
Speciﬁcally, the pipeline of the model splits into multiple channels after convolutional layers, in which the function of each
channel is expected to learn a prototypical part of the input,
such as the head or body of a bird. The decision for an input
image is made based on the similarity of features of channels.
Wachter et al. offered a novel case-based explanation
method by providing a counterfactual case, which is an imaginary case that is close to the query but has a different output
from that of the query. Counterfactual explanation provides
the so-called “closest possible case” or the smallest change to
yield a different outcome. For example, counterfactual explanations may produce the following statement: “If you have
a good striker, your team would have won this soccer game.”
Coincidentally, techniques to generate a counterfactual explanation have been developed for the purpose of “adversarial
perturbation”; i.e., structural attack . Essentially, ﬁnding
the closest possible case x′ to the input x is equivalent to ﬁnding the smallest perturbation to x such that the classiﬁcation
result changes. For example, the following optimization can
−y′2 + d(x, x′)
where λ is a constant, y′ is a different label, and d(·, ·) is
chosen to be the Manhattan distance in hope that the input be
minimally perturbed. Goyal et al. explored an alternative
way to derive a counterfactual visual explanation. Given an
image I with a label c, since the counterfactual visual explanation represents the change for the input that can force the
model to yield a different prediction class c′, they selected an
image I′ with a label c′ and managed to recognize the spatial
region in I and I′ such that the replacement of the recognized
region would alter the model prediction from c to c′.
Explaining-by-Text: Neural image captioning uses a neural network to produce a natural language description for an
image. Despite that neural image captioning is initially not
for network interpretability, descriptive language about images
can tell the information about how a neural network analyzes
an image. One representative method is from that combines a convolutional neural network and a bidirectional RNN
to obtain a bimodal embedding. Due to the hypothesis that
the two embeddings representing similar semantics across two
modalities should share the nearby locations of two spaces, the
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
Image captioning with attention modules provides an explanation
to the features mined by a deep convolutional network.
objective function is deﬁned as
where vi is the ith image fragment in the set gI, and st is
the tth word in a sentence gT. Another representative method
is the attention mechanism , , , , where
deep features are to align the corresponding text descriptions
by a recursive neural network such as LSTM . An explanation for deep features is provided by the corresponding words
in the text and attention maps, which reﬂect which parts of an
image attract the attention of the neural network.
As shown in Fig. 13, in the kth attention module that takes
y0, y1, . . . , yn as input, suppose its output is tk =
sk0, sk1, . . . , skn together form an attention map for tk with
respect to the associated word. However, Jain and Wallace 
argued that an attention map is not qualiﬁed as an explanation because they observed that the attention map was not
correlated with other importance measures of features such as
gradient-based measures, and the change of attention weights
yielded no changes in prediction.
C. Ad-Hoc Interpretable Modeling
Interpretable
Representation:
Traditionally,
regularization techniques for deep learning are primarily designed
to avoid overﬁtting. However, it is also feasible to devise
regularization techniques to enhance an interpretable representation in terms of decomposability , , , ,
monotonicity ,
nonnegativity ,
sparsity ,
human-in-the-loop prior , and so on.
which is a simple but effective way to learn an interpretable representation. Traditionally, a generative adversarial
network (GAN) imposes no restrictions on how a generator utilizes the noise. In contrast, InfoGAN maximizes the
mutual information between the latent codes and observations,
forcing each dimension of noise to encode a semantic concept.
Particularly, the latent codes are made of discrete categorical
codes and continuous style codes. As shown in Fig. 14, two
style codes control the localized part and the digit rotation,
respectively.
In an InfoGAN, two latent codes control the localized parts and
rotation parts, respectively.
Incorporating monotonicity constraints is also useful to enhance interpretability. A monotonical relationship
means when the value of a speciﬁed attribute increases,
predictive
decreases. Such a simplicity promotes interpretability as well.
Chorowski and Zurada imposed non-negativity to weights
of neural networks and argued that it could improve interpretability because it eliminated the cancelation and aliasing
effects among neurons. Subramanian et al. employed a
k-sparse autoencoder for word embedding to promote sparsity
in the embedding and claimed that this enhanced interpretability because a sparse embedding reduced the overlap between
words. Lage et al. proposed a novel human-in-the-loop
evaluation in selecting a model. Speciﬁcally, a diverse set of
models were trained and sent to users for evaluation. Users
were asked to predict the label of a data point that would be
assigned by a model M. The shorter the response time was,
the better a user understood the model. Then, the model with
the lowest response time was chosen.
Model Renovation: Chu et al. proposed to use
activations
network (PLNN); thereby, the decision boundaries of PLNN
could be explicitly deﬁned and further a closed-form solution could be derived for predictions of a network. As
Fig. 15 shows, Fan et al. proposed soft-autoencoder (Soft-
AE) by using adaptable soft-thresholding units in encoding
layers and linear units in decoding layers. Consequently, Soft-
AE can be interpreted as a learned cascaded wavelet adaptation
Fan explained a neural network as a generalized
Hamming network, whose neurons compute the generalized
Hamming distance: h(x, w) = L
l=1 wl + L
l=1 xl −2x · w
for an input x = (x1, . . . , xL) and a weight vector w =
(w1, . . . , wL). The bias term in each neuron is speciﬁed as
b = −(1/2)( L
l=1 wl + L
l=1 xl) so that each neuron is a generalized Hamming neuron. In this regard, the function of the
batch normalization is demystiﬁed as making the bias suitable for computation of the generalized Hamming distance.
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
Soft-AE with soft-thresholding functions as activation functions in
the encoding layers and linear functions as activations in the decoding layers,
thereby admitting a direct correspondence to the wavelet adaptation system.
Kuo et al. proposed a transparent design for constructing
a feedforward convolutional network without the need of backpropagation. Speciﬁcally, ﬁlters in convolutional layers were
built by selecting principal components of PCA for outputs of
earlier pooling layers. A fully connected layer was constructed
by treating it as a linear-squared regressor.
Melis and Jaakkola claimed that a neural network
model f is interpretable if it has the form that f(x) =
g(θ1(x)h1(x), . . . , θk(x)hk(x)), where hi(x) is the prototypical
concept from the input x and θi(x) is the relevance associated
with that concept, g is monotonic and completely additively
separable. Such a model can learn interpretable basis concepts
and facilitate saliency analysis. Similarly, Vaughan et al. 
designed a network structure to compatibly learn the function
formulated as f(x) = μ + γ1h1(βT
1 x) + γ2h2(βT
2 x) + · · · +
Kx), where βk is the projection, hk(·) represents the
nonlinear transformation, μ is the bias, and γk is the weighting factor. Such a model is more interpretable than a general
network, because the function of this model has simpler partial derivatives that can simplify saliency analysis, statistical
analysis, and so on.
Li et al. proposed deep supervision by using
prior hierarchical tasks on features of intermediate layers.
Speciﬁcally, we have a dataset {(x, y1, . . . , ym)}, where labels
y1, . . . , ym are hierarchical that yj, j < i is a strictly necessary condition for the existence of yi, i > 1. Such a scheme
introduces a modularized idea that through the supervision of
a speciﬁc task for an intermediate layer, the learning of that
layer is steered toward the prespeciﬁed task, thereby gaining
interpretability.
Wang proposed to use an interpretable and insertable
substitute on a subset of data which the complex black-box
model overkills. In their work, a rule set was built as an interpretable model to make a decision on the input data ﬁrst.
Those inputs which a rule set was handicapped to classify
were passed into the black-box model for decision making. The logic of this hybrid predictive system is that an
interpretable model for regular cases without compromising
accuracy, a complex black-box model for complicated cases.
Jiang et al. proposed ﬁnite automata-RNN (FA-RNN)
that can be directly transformed into the regular expressions
such that a good interpretability is extracted. The roadmap
is that the constructed FA-RNN can be approximated into
ﬁnite automata, and further transformed into regular expressions because ﬁnite automata and a regular expression are
mutually convertible. In analogy, a regular expression can also
be decoded into an FA-RNN as an initialization. FA-RNN is
a good example to manifest the synergy between a rule system
and a neural network.
III. INTERPRETABILITY IN MEDICINE
These days, reports are often seen in the news that deep
learning-based algorithms outperform experts or classic algorithms in the ﬁeld of medicine . Indeed, given adequate
computational power and well-curated datasets, a properly
designed model can deliver competitive performance in most
well-deﬁned pattern recognition tasks. However, due to the
high stakes of medicine-concerned applications, it is not suf-
ﬁcient to have a deep learning model that produces correct
answers without an explanation. In this section, we focus
on several exemplary papers concerning applications of interpretability methods in medicine and we organize the articles
of relevance in accordance with the aforementioned taxonomy.
A. Post-Hoc Interpretability Analysis
Feature Analysis: Van Molle et al. visualized convolutional neural networks to assist decision making for skin
lesion classiﬁcation. In their work, feature activations generated from the last two convolutional layers were rescaled to the
size of an input image as the activation maps. Where a map has
high activations were inspected. The activation strengths across
different border types, skin colors, skin types, etc., were compared. The activation map exposed a risk that some unexpected
regions had uncommonly high activations.
Bychkov et al. utilized a model that combines a VGG-
16 network and an LSTM network to predict
ﬁve-year survival of colorectal cancer based on digitized tumor
tissue samples. In their work, an RGB pathological image was
split into many tiles. A VGG-16 network extracted a highdimensional feature vector from each tile, which was then
fed into an LSTM network to predict ﬁve-year survival. They
used t-SNE to map features learned by VGG-16 into
a 2-D space for visualization and found that different classes
of features of VGG-16 were well separated.
Saliency: Sturm et al. applied a deep network with
LRP for the single-trial EEG classiﬁcation. The
network entails two linear mean pooling layers before being
activated or normalized. The feature importance score is
assigned by LRP.
Zech et al. developed a deep learning model for chest
radiography to classify patients into having pneumonia or not.
Through interpretability analysis by CAM , they reported
the risk that a deep learning model could make an incorrect
decision by capturing features irrelevant to diseases, such as
metal tokens.
Oktay et al. combined attention gates with the decoder
part of U-Net to cope with interpatient variation in organs’
shapes and sizes. The proposed model can improve model
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
sensitivity and accuracy by inhibiting representations of irrelevant regions. Aided by attention gates, they found that the
model gradually shifted its attention to regions of interest.
Ardila et al. proposed a deep learning algorithm that
considers a patient’s current and previous CT volumes to
predict the risk of lung cancer. They used the integrated
gradient method to derive saliency maps and invited
experienced radiologists to examine the ﬁdelity of these maps.
It turned out that in all cases, the readers strongly agreed that
the model indeed focused on the nodules.
Lee et al. reported an attention-assisted deep learning system for detection and classiﬁcation of acute intracranial
haemorrhage, where an attention map identiﬁed a region relevant to the disease. They evaluated the localization accuracy
of the attention maps by computing the proportion of bleeding points overlapping with the attention maps. Overall, it
was found that 78.1% bleeding points were detected in the
attention maps.
Caicedo-Torres and Gutierrez proposed a multiscale
deep convolutional neural network for the mortality prediction
based on the measurement of 22 different items in ICU, such
as the sodium index, urine output, etc. In their work, three
temporal scales were represented by stacking convolutional
kernels of dimensions 3 × 1, 6 × 1, and 12 × 1. The saliency
map by DeepLIFT was utilized for interpretability.
Guo et al. introduced an effective dual-stream network
that conjugates extracted features from ResNet and clinical prior knowledge to predict the mortality risk of patients
based on low-dose CT images. To further testify the effectiveness of the proposed model, they utilized t-SNE to
reduce the dimensionality of feature maps of malignant and
benign samples and found that malignant and benign features
were well separated. Also, they applied CAM to reveal
that the deceased subjects correctly classiﬁed by the model
were prone to have strong activations.
Proxy: Che et al. applied knowledge distillation into
a deep model to learn a gradient boosting tree (GBT) ,
which provides not only robust prediction performance but
also a good interpretability in the context of electronic health
record prediction. Speciﬁcally, they trained three deep models, respectively, and then used predictions of deep models
as labels to train a GBT model. Experiments on a Pediatric
ICU dataset were reported that the GBT model maintained the
prediction performance of deep models in terms of mortality
and ventilator-free days.
Pereira et al. combined global and local interpretation
efforts for brain tumor segmentation and penumbra estimation
in stroke lesions, where the global interpretability was derived
from mutual information to sense the dependence between an
input sample and the prediction, while the local interpretability
was cast by a variant of LIME .
Explaining-by-Case: Codella et al. employed saliency
and explaining-by-case methods to explain a dermoscopic
image analysis network, which was jointly trained by disease
labels with a triple-let loss. Speciﬁcally, the interpretability
was gained by the discovered neighbors and localized regions
that were most relevant to the distance from queries and
neighbors.
Visualization of feature maps of different arms in PIPO-FAN,
where low-scale subnetworks produce local structural details and high-scale
subnetworks target global morphological information.
Explaining-by-Text: Zhang et al. proposed an allin-one network that reads pathology bladder cancer images,
generated diagnostic reports, retrieved images according to
symptomatic descriptions, and visualized attention maps. They
designed an auxiliary attention sharpening module to improve
the discriminability of attention maps. Pathologists’ feedbacks suggested that the explanatory maps tended to highlight
regions that concern with carcinoma-informative regions.
B. Ad-Hoc Interpretable Modeling
Interpretable Representation: Fang and Yan devised
the pyramid input pyramid output feature abstraction network
(PIPO-FAN) with multiple arms for multiorgan segmentation.
Each of the arm handles the information on one scale. The
total loss is obtained by adding the segmentation loss to each
of these arms such that segmentation-wise features are generated in each arm. Visualization analysis suggested that features
from different arms have hierarchical semantical meanings;
i.e., some are blurry but contain global classwise information,
while the others contain local boundary information. As shown
in Fig. 16, the segmentation loss creates semantically meaningful features, where low-scale arms produce more details
and high-scale arms ﬁnd global morphologies.
Renovation:
a DenseNet model with an LSTM model for
the detection of hip features from pelvic X-ray radiographs.
A radiologist hand-labeled standard descriptive terms to
construct a semantic dataset for these radiographs. Their
model consistently generated informative sentences favored
by doctors over saliency maps. Also, they demonstrated that
the combination of visualization and text interpretation gives
an interpretation superior to either of them alone.
variational
autoencoder (VAE) -based
classiﬁcation of cardiac diseases as well as structurally remodeling
based on cardiovascular images. In their scheme, registered
left ventricular (LV) segmentations at end-diastolic (ED) and
end-systolic (ES) phases were encoded in a low-dimensional
FAN et al.: ON INTERPRETABILITY OF ARTIFICIAL NEURAL NETWORKS: A SURVEY
latent space by VAE. The learned latent low-dimensional
manifold was connected to a multilayer perceptron (MLP)
for disease classiﬁcation. The interpretation was given by
an activation maximization technique. The “deep dream” of
the MLP was derived and inverted to the image space for
visualization.
Shen et al. built an interpretable deep hierarchical
semantic convolutional neural network (HSCNN) to predict
the malignancy of pulmonary nodules in CT images. HSCNN
consists of three modules: 1) a general feature learning
module; 2) a low-level task module that predicts semantic
characteristics, such as sphericity, margin, subtlety, and so
on; and 3) a high-level task module absorbs information from
both general features and low-level task predictions to produce an overall lung nodule malignancy. Due to the semantic
meaning contained in the low-level task, HSCNN has boosted
interpretability.
Zhang et al. developed a deep convolutional network
to automate the whole-slide reading of pathology images for
tumors and the diagnosis process of pathologists. Specially, the
network can generate a clinical pathology report along with
attention-assisted features.
Lei et al. observed that CAM and Grad-
CAM are for interpreting localization tasks and tend
to ignore ﬁne-grained structures. Consequently, they proposed
a shape-and-margin-aware soft activation map (SAM) that
could probe subtle but critical features in a lung nodule classiﬁcation task. The comprehensive experimental comparisons
showed that compared to CAM and Grad-CAM, SAM could
reveal relatively discrete and irregular features around nodules.
IV. PERSPECTIVE
In this section, we suggest a few directions, in hope to
advance the understanding and practice of artiﬁcial neural
Synergy of Fuzzy Logic and Deep Learning:
logic was a buzz phrase in the last nighties. It extends
the Boolean logic from 0–1 judgement to imprecise inference with fuzziness in the interval . The fuzzy theory
can be divided into two branches: 1) fuzzy set theory and
2) fuzzy logic theory. The latter, with an emphasis on “IF-
THEN” rules, has demonstrated effectiveness in dealing with
a plethora of complicated system modeling and control problems. Nevertheless, a fuzzy rule-based system is restricted by
the acquisition of a large number of fuzzy rules, a process
that is tedious and computationally expensive. While a neural network is a data-driven method that extracts knowledge
from data through training, with the knowledge represented by
neurons in a distributed manner. However, a neural network
falls short of delivering a satisfactory result in the context of
small data and suffers from the lack of interpretability. In contrast, a fuzzy logic system employs experts’ knowledge and
represents a system in the form of IF-THEN rules. Although
a fuzzy logic system merits interpretability and accountability, it is incompetent in efﬁcient and effective knowledge
acquisition. It seems that a neural network and a fuzzy logic
system are complementary to each other. Therefore, it is instrumental to combine the strengths of two worlds toward an
enhanced interpretability. In fact, this roadmap is not totally
new. There have been several combinations along this direction: ANFIS model , generic fuzzy perceptron , RBF
networks , and so on.
One suggestion is to build a deep RBF network. Given the
input vector x = [x1, x2, . . . , xn], an RBF network is expressed
as f(x) = n
i wiφi(x −ci), where φi(x−ci) is usually selected
as exp(−[(||x −ci|| ∧2)/(2σ 2)]), where ci is the cluster center of the ith neuron. It was proved the functional equivalence
between an RBF network and a fuzzy inference system under
mild conditions . Also, an RBF network is shown to be
a universal approximator . Hence, an RBF network is
a potentially sound vehicle that can encode fuzzy rules into its
adaptive representation without loss of accuracy. Reciprocally,
rule generation and fuzzy rule representation in an adaptable
RBF network are more straightforward compared to an MLP.
Although the current RBF networks are of one-hidden-layer
structures, it is feasible to develop deep RBF networks, which
can be viewed as a deep fuzzy rule system. A greedy layerwise
training algorithm was developed in , which successfully
solved the training problem for deep networks. It is possible
to translate such success for training of deep RBF networks.
Then, the correspondence between a deep RBF network and
a deep fuzzy logic system can be applied to obtain a deep
fuzzy rule system. We believe that efforts should be made to
synergize fuzzy logic and deep learning techniques aided by
big data along this direction.
Convergence of Neuroscience and Deep Learning: Up to
date, truly intelligent systems are only human. The artiﬁcial
neural networks in their earlier forms were clearly inspired by
biological neural networks . However, subsequent developments of neural networks were, to a much less degree,
pushed by neurological and biological insights. As far as
interpretability is concerned, since biological and artiﬁcial neural networks are deeply connected, advances in neuroscience
should be relevant and even instrumental to the development
and interpretation of deep learning techniques. We believe
that the neuroscience has a great potential of deep learning
interpretability in the following aspects.
Cost Function: The effective use of cost functions is a driving force for the development of deep networks in the past
years; for example, the adversarial loss used in GANs .
In the previous sections, we have highlighted cases that an
appropriate cost function can enable a model to learn an
interpretable representation, such as enhance feature disentanglement. Along this direction, a myriad of cost functions can
be built to reﬂect biologically plausible rationales. Indeed, our
brain can be modeled as an optimization machine , which
has a powerful credit assignment mechanism to form a cost
Optimization Algorithm: Despite the huge success by backpropagation, it is far from ideal in the view of neuroscience.
Truly in many senses, backpropagation fails to manifest how
a human neural system handles the synapses of a neuron. For
example, in a biological neural system, synapses are updated
in a local manner and only depend on the activities of
IEEE TRANSACTIONS ON RADIATION AND PLASMA MEDICAL SCIENCES, VOL. 5, NO. 6, NOVEMBER 2021
Visualization of weights of a network learned by a bio-plausible
algorithm, where prototypes of training images are captured .
presynaptic and postsynaptic neurons. However, connections
in deep networks are tuned through nonlocal backpropagation. Fig. 17 shows a bio-plausible learning algorithm for
a two-layer network on CIFAR-100 . Additionally, a neuromodulator is missing in deep networks in contrast to the
inner working of a human brain, where the state of one neuron can exhibit different input–output patterns controlled by
a global neuromodulator, such as dopamine, serotonin, and
so on . Neuromodulators are believed to be critical due
to their ability to selectively control on and off states of one
neuron, which is equivalent to modifying the involved cost
function .
Considering that there are quite few studies discussing
the interpretability of training algorithms, powerful and interpretable training algorithms will be highly desirable. Just like
what were proved for classic optimization methods, we wish
that future nonconvex optimization algorithms will have some
kinds of uniqueness, stability, and continuous dependency on
data, etc.
Bio-Plausible Architectural Design: In the past decades,
neural networks were designed in diverse architectures from
simple feedforward networks to deep convolutional networks
and other highly sophisticated networks. The structure determines functionality; i.e., a speciﬁc network architecture
regulates the information ﬂow with distinct characteristics.
Therefore, specialized architectures are useful as effective
solutions for intended problems. Currently, the structural differences between deep learning and biological systems are
eminent. A typical network is used and tuned for most tasks
based on big data, while a biological system learns from
a small number of data and generalizes very well. Clearly,
a huge amount of knowledge needs to be learned from biological neural networks so that more desirable and explainable
neural network architectures can be designed.
Interpretability in Medicine:
A majority of interpretability research efforts in medicine are currently for classiﬁcation
tasks, but radiological imaging covers a large variety of tasks,
such as image segmentation, registration, reconstruction, and
so on. Clearly, interpretability is also closely relevant to these
areas and, therefore, it is in need to promote interpretability research in these domains. On one hand, more efforts
should be made to extend the existing interpretation methods
to other tasks that have not been explored. On the other hand,
practitioners can design task-speciﬁc interpretation methods
with their expertise and insights. For example, explaining why
a voxel receives a class label in image segmentation is much
harder than explaining which area in the input image is responsible for a prediction in image classiﬁcation. Similarly, for
image reconstruction, interpretability could be quite complicated. In this regard, our recently proposed ACID framework
allows a synergistic integration of data-driven priors and compressed sensing (CS)-modeled priors, enforcing both of which
iteratively via physics-based analytic mapping . By doing
so, modern CS and state-of-the-art deep networks are united
to overcome the vulnerabilities of existing deep reconstruction
networks, at the same time transferring the interpretability of
the model-based methods to the hybrid DNNs.
In addition to the above referenced publications, gaining
interpretability ultimately also relies on medical doctors, who
have invaluable professional training despite some biases and
errors. As a result, active collaboration among medical doctors, technical experts, and theoretical researchers will be an
important avenue for future development of deep learning
V. CONCLUSION
In conclusion, we have reviewed key ideas, implications,
limitations of the existing interpretability studies, and illustrated some typical interpretation methods through examples. In doing so, we have depicted a holistic landscape
of interpretability research using our proposed taxonomy
and introduced applications of interpretability in medicine.
Figs. 3, 5, 6, 7, 9, 10, 16, and 17 are visualization results from
our own implementations of the chosen interpretation methods. We have publicly shared relevant codes in the GitHub
( There
is no doubt that a uniﬁed and accountable interpretation framework is critical to elevate interpretability research into a new
phase. In the future, more efforts are needed to reveal the
essence of deep learning. Because this ﬁeld is highly interdisciplinary and rapidly evolving, there are great opportunities
ahead that will be both academically and practically rewarding.
ACKNOWLEDGMENT
The authors are grateful for Dr. Hongming Shan’s suggestions (Fudan University) and anonymous reviewers’ advice.