HAL Id: hal-01109284
 
Submitted on 26 Jan 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Describing Textures in the Wild
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed,
Andrea Vedaldi
To cite this version:
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, Andrea Vedaldi.
Describing Textures in the Wild.
IEEE Conference in Computer Vision and Pattern Recognition, Jun
2014, Greater Columbus Convention Center in Columbus, Ohio., United States.
pp.3606 - 3613,
￿10.1109/CVPR.2014.461￿. ￿hal-01109284￿
Describing Textures in the Wild
Mircea Cimpoi
University of Oxford
 
Subhransu Maji
Toyota Technological Institute
 
Iasonas Kokkinos
´Ecole Centrale Paris / INRIA-Saclay
 
Sammy Mohamed
Stony Brook
 
Andrea Vedaldi
University of Oxford
 
Patterns and textures are key characteristics of many natural objects: a shirt can be striped, the wings of a butter-
ﬂy can be veined, and the skin of an animal can be scaly.
Aiming at supporting this dimension in image understanding, we address the problem of describing textures with semantic attributes. We identify a vocabulary of forty-seven
texture terms and use them to describe a large dataset of
patterns collected “in the wild”. The resulting Describable
Textures Dataset (DTD) is a basis to seek the best representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition
the Improved Fisher Vector (IFV) and Deep Convolutionalnetwork Activation Features (DeCAF), and show that surprisingly, they both outperform specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that our describable
attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV
and DeCAF, they signiﬁcantly outperform the state-of-theart by more than 10% on both FMD and KTH-TIPS-2b
benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.
1. Introduction
Recently visual attributes have raised signiﬁcant interest in the community . A “visual attribute”
is a property of an object that can be measured visually and
has a semantic connotation, such as the shape of a hat or the
color of a ball. Attributes allow characterizing objects in far
greater detail than a category label and are therefore the key
to several advanced applications, including understanding
complex queries in semantic search, learning about objects
from textual description, and accounting for the content of
images in great detail. Textural properties have an important
role in object descriptions, particularly for those objects that
are best qualiﬁed by a pattern, such as a shirt or the wing of
Both the man-made and the natural world are
an abundant source of richly textured objects. The textures
of objects shown above can be described (in no particular
order) as dotted, striped, chequered, cracked, swirly, honeycombed, and scaly. We aim at identifying these attributes
automatically and generating descriptions based on them.
a bird or a butterﬂy as illustrated in Fig. 1. Nevertheless, so
far the attributes of textures have been investigated only tangentially. In this paper we address the question of whether
there exists a “universal” set of attributes that can describe a
wide range of texture patterns, whether these can be reliably
estimated from images, and for what tasks they are useful.
The study of perceptual attributes of textures has a
long history starting from pre-attentive aspects and grouping , to coarse high-level attributes , to some
recent work aimed at discovering such attributes by automatically mining descriptions of images from the Internet . However, the texture attributes investigated so
far are rather few or too generic for a detailed description
most “real world” patterns. Our work is motivated by the
one of Bhusan et al. who studied the relationship between commonly used English words and the perceptual
properties of textures, identifying a set of words sufﬁcient
to describing a wide variety of texture patterns. While they
study the psychological aspects of texture perception, the
focus of this paper is the challenge of estimating such properties from images automatically.
Our ﬁrst contribution is to select a subset of 47 describable texture attributes, based on the work of Bhusan
et al., that capture a wide variety of visual properties of
textures and to introduce a corresponding describable tex-
ture dataset consisting of 5,640 texture images jointly annotated with the 47 attributes (Sect. 2).
In an effort to
support directly real world applications, and inspired by
datasets such as ImageNet and the Flickr Material
Dataset (FMD) , our images are captured “in the wild”
by downloading them from the Internet rather than collecting them in a laboratory. We also address the practical issue of crowd-sourcing this large set of joint annotations ef-
ﬁciently accounting for the co-occurrence statistics of attributes and for the appearance of the textures (Sect. 2.1).
Our second contribution is to identify a gold standard texture representation that achieves state-of-the-art
recognition of the describable texture attributes in challenging real-world conditions.
Texture classiﬁcation has
been widely studied in the context of recognizing materials supported by datasets such as CUReT , UIUC ,
UMD , Outex , Drexel Texture Database , and
KTH-TIPS . These datasets address material recognition under variable occlusion, viewpoint, and illumination
and have motivated the creation of a large number of specialized texture representations that are invariant or robust
to these factors . In contrast, generic object
recognition features such as SIFT were shown to work the
best for material recognition in FMD, which, like DTD, was
collected “in the wild”. Our ﬁndings are similar, but we also
ﬁnd that Fisher vectors computed on SIFT features and
certain color features, as well as generic deep features such
as DeCAF , can signiﬁcantly boost performance. Surprisingly, these descriptors outperform specialized state-ofthe-art texture representations not only in recognizing our
describable attributes, but also in a variety of datasets for
material recognition, achieving an accuracy of 65.5% on
FMD and 76.2% on KTH-TIPS2-b (Sect. 3, 4.1).
Our third contribution consists in several applications
of the proposed describable attributes.
These can serve
a complimentary role for recognition and description in
domains where the material is not-important or is known
ahead of time, such as fabrics or wallpapers. However, can
these attributes improve other texture analysis tasks such as
material recognition? We answer this question in the afﬁrmative in a series of experiments on the challenging FMD
and KTH datasets. We show that estimates of these properties when used a features can boost recognition rates even
more for material classiﬁcation achieving an accuracy of
55.9% on FMD and 71.2% on KTH when used alone as
a 47 dimensional feature, and 67.1% on FMD and 77.3%
on KTH when combined with SIFT, simple color descriptors, and deep convolutional network features (Sect. 4.2).
These represent more than an absolute gain of 10% in accuracy over previous state-of the-art. Furthermore, these
attributes are easy to describe and can serve as intuitive dimensions to explore large collections of texture patterns –
for example product catalogs (wallpapers or bedding sets)
or material datasets. We present several such visualizations
in the paper (Sect. 4.3).
2. The describable texture dataset
This section introduces the Describable Textures Dataset
(DTD), a collection of real-world texture images annotated
with one or more adjectives selected in a vocabulary of 47
English words. These adjectives, or describable texture attributes, are illustrated in Fig. 2 and include words such as
banded, cobwebbed, freckled, knitted, and zigzagged.
DTD investigates the problem of texture description,
intended as the recognition of describable texture attributes.
This problem differs from the one of material recognition
considered in existing datasets such as CUReT, KTH, and
FMD. While describable attributes are correlated with materials, attributes do not imply materials (e.g. veined may
equally apply to leaves or marble) and materials do not imply attributes (not all marbles are veined). Describable attributes can be combined to create rich descriptions (Fig. 3;
marble can be veined, stratiﬁed and cracked at the same
time), whereas a typical assumption is that textures are
made of a single material. Describable attributes are subjective properties that depend on the imaged object as well as
on human judgements, whereas materials are objective. In
short, attributes capture properties of textures beyond materials, supporting human-centric tasks where describing textures is important. At the same time, they will be shown to
be helpful in material recognition too (Sect. 3.2 and 4.2).
DTD contains textures in the wild, i.e. texture images
extracted from the web rather than begin captured or generated in a controlled setting. Textures ﬁll the images, so
we can study the problem of texture description independently of texture segmentation. With 5,640 such images,
this dataset aims at supporting real-world applications were
the recognition of texture properties is a key component.
Collecting images from the Internet is a common approach
in categorization and object recognition, and was adopted in
material recognition in FMD. This choice trades-off the systematic sampling of illumination and viewpoint variations
existing in datasets such as CUReT, KTH-TIPS, Outex, and
Drexel datasets for a representation of real-world variations,
shortening the gap with applications. Furthermore, the invariance of describable attributes is not an intrinsic property as for materials, but it reﬂects invariance in the human
judgements, which should be captured empirically.
DTD is designed as a public benchmark, following the
standard practice of providing 10 preset splits into equallysized training, validation and test subsets for easier algorithm comparison (these splits are used in all the experiments in the paper).
DTD is publicly available on
the web at 
data/dtd/, along with standardized code for evaluation
and reproducing the results in Sect. 4.
crosshatched crystalline
honeycombed interlaced
perforated
polka-dotted
Figure 2: The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute
are shown to illustrate the signiﬁcant amount of variability in the data.
Related work. Apart from material datasets, there have
been numerous attempts at collecting attributes of textures
at a smaller scale, or in controlled settings. Our work is
related to the work of , where they analysed images in
the Outex dataset using a subset of the attributes we
consider; differently from them, we demonstrate that our
DTD attributes generalize to new datasets, for example by
helping to establish state-of-the-art performance in material
recognition.
2.1. Dataset design and collection
This section discusses how DTD was designed and collected, including: selecting the 47 attributes, ﬁnding at least
120 representative images for each attribute, and collecting
all the attribute labels for each image in the dataset.
Selecting the describable attributes. Psychological experiments suggest that, while there are a few hundred words
that people commonly use to describe textures, this vocabulary is redundant and can be reduced to a much smaller
number of representative words. Our starting point is the
list of 98 words identiﬁed by Bhusan, Rao and Lohse .
Their seminal work aimed to achieve for texture recognition the same that color words have achieved for describing
color spaces . However, their work mainly focuses on
the cognitive aspects of texture perception, including perceptual similarity and the identiﬁcation of directions of perceptual texture variability. Since we are interested in the
visual aspects of texture, we ignored words such as “corrugated” that are more related to surface shape properties, and
words such as “messy” that do not necessarily correspond to
visual features. After this screening phase we analysed the
remaining words and merged similar ones such as “coiled”,
“spiralled” and “corkscrewed” into a single term. This resulted in a set of 47 words, illustrated in Fig. 2.
Bootstrapping the key images. Given the 47 attributes,
the next step was collecting a sufﬁcient number (120) of example images representative of each attribute. A very large
initial pool of about a hundred-thousand images was downloaded from Google and Flickr by entering the attributes
and related terms as search queries.
Then Amazon Mechanical Turk (AMT) was used to remove low resolution,
poor quality, watermarked images, or images that were not
almost entirely ﬁlled with a texture. Next, detailed annotation instructions were created for each of the 47 attributes,
including a dictionary deﬁnition of each concept and examples of correct and incorrect matches. Votes from three
AMT annotators were collected for the candidate images of
each attribute and a shortlist of about 200 highly-voted images was further manually checked by the authors to eliminate residual errors. The result was a selection of 120 key
representative images for each attribute.
Sequential join annotations. So far only the key attribute
of each image is known while any of the remaining 46 attributes may apply as well. Exhaustively collecting annotations for 46 attributes and 5,640 texture images is fairly expensive. To reduce this cost we propose to exploit the correlation and sparsity of the attribute occurrences (Fig. 3). For
each attribute q, twelve key images are annotated exhaustively and used to estimate the probability p(q′|q) that another attribute q′ could co-exist with q. Then for the remaining key images of attribute q, only annotations for attributes
q′ with non negligible probability – in practice 4 or 5 – are
collected, assuming that the attributes would not apply. This
procedure occasionally misses attribute annotations; Fig. 3
evaluates attribute recall by 12-fold cross-validation on the
12 exhaustive annotations for a ﬁxed budget of collecting
10 annotations per image (instead of 47).
A further reﬁnement is to suggest which attributes q′ to
annotate not just based on q, but also based on the appearance of an image ℓi. This was done by using the attribute
classiﬁer learned in Sect. 4; after Platt’s calibration 
on an held-out test set, the classiﬁer score cq′(ℓi) ∈R is
Occurrences per image
crosshatched
crystalline
honeycombed
interlaced
perforated
polka−dotted
stratified
All occurences
Sequential + CV
Sequential
Figure 3: Quality of joint sequential annotations. Each bar shows the average number of occurrences of a given attribute
in a DTD image. The horizontal dashed line corresponds to a frequency of 1/47, the minimum given the design of DTD
(Sect. 2.1). The black portion of each bar is the amount of attributes discovered by the sequential procedure, using only
10 annotations per image (about one ﬁfth of the effort required for exhaustive annotation). The orange portion shows the
additional recall obtained by integrating CV in the process. Right: co-occurrence of attributes. The matrix shows the joint
probability p(q, q′) of two attributes occurring together (rows and columns are sorted in the same way as the left image).
transformed in a probability p(q′|ℓi) = σ(cq′(ℓ)) where
σ(z) = 1/(1 + e−z) is the sigmoid function.
By construction, Platt’s calibration reﬂects the prior probability
p(q′) ≈p0 = 1/47 of q′ on the validation set. To reﬂect the
probability p(q′|q) instead, the score is adjusted as
p(q′|ℓi, q) ∝σ(cq′(ℓi)) ×
1 −p(q′|q) × 1 −p0
and used to ﬁnd which attributes to annotated for each image. As shown in Fig. 3, for a ﬁxed annotation budged this
method increases attribute recall. Overall, with roughly 10
annotations per images it was possible to recover of all the
attributes for at least 75% of the images, and miss one out
of four (on average) for another 20% while keeping the annotation cost to a reasonable level.
3. Texture representations
Given the DTD dataset developed in Sect. 2, this section
moves on to the problem of designing a system that can
automatically recognize the attributes of textures. Given a
texture image ℓthe ﬁrst step is to compute a representation
φ(ℓ) ∈Rd of the image; the second step is to use a classiﬁer
such as a Support Vector Machine (SVM) ⟨w, φ(ℓ)⟩to score
how strongly the image ℓmatches a given perceptual category. We propose two such representations: a gold-standard
low-level texture descriptor based on the improved Fisher
Vector or DeCAF features (Sect. 3.1) and a mid-level texture descriptor consisting of the describable attributes themselves (Sect. 3.2), discussed in detail in Sect. 4.
3.1. Texture descriptors
This section describes two texture descriptors that we
port to texture from the object recognition: the Improved
Fisher Vector (IFV) and the Deep Convolutional Activation Feature (DeCAF) .
Differently from popular specialized texture descriptors, both representation are
tuned for object recognition. We were therefore somewhat
surprised to discover that these off-the-shelf methods surpass by a large margin the state-of-the-art in several texture
analysis tasks (Sect. 4.1).
IFV. Given an image ℓ, the Fisher Vector (FV) formulation of starts by extracting local SIFT descriptors {d1, . . . , dn} densely and at multiple scales. It then
soft-quantizes the descriptors by using a Gaussian Mixture
Model (GMM) with K modes. The Gaussian covariance
matrices are assumed to be diagonal, but local descriptors
are ﬁrst decorrelated and optionally dimensionality reduced
by PCA. The improved version of the descriptor adds signed
square-rooting and l2 normalization. We are not the ﬁrst to
use SIFT or IFV in texture recognition. For example, SIFT
was used in , and Fisher Vectors were used in .
However, neither work tested the standard IFV formulation , which we found to give excellent results.
DeCAF. The DeCAF features are obtained from an image ℓas the output of the deep convolutional neural network
of . This network, which alternates several layers of linear ﬁltering, rectiﬁcation, max pooling, normalization, and
full linear weighting, is learned to discriminate 1,000 object
classes of the ImageNet challenge. It is used as a texture descriptor by removing the softmax and last fully-connected
layer of the network, resulting in a φ(x) ∈R4096 dimensional descriptor vector which is l2 normalized before use
in an SVM classiﬁer. To the best of our knowledge, we are
the ﬁrst to test these features on texture analysis tasks.
3.2. Describable attributes as a representation
The main motivation for recognizing describable attributes is to support human-centric applications, enriching
the vocabulary of visual properties that machines can understand.
However, once extracted, these attributes may
also be used as texture descriptors in their own right. As
a simple incarnation of this idea, we propose to collect
Local descr.
19.7 ± 0.8 24.1 ± 0.7 30.7 ± 0.7
18.8 ± 0.5 25.8 ± 0.8 31.6 ± 1.1 39.7 ± 1.1
14.6 ± 0.6 22.3 ± 0.7 26.0 ± 0.8 30.7 ± 0.9
18.0 ± 0.4 26.8 ± 0.7 31.6 ± 0.8 37.1 ± 1.0
14.2 ± 0.6 24.8 ± 1.0
21.1 ± 0.8 23.1 ± 1.0 28.5 ± 1.0 34.7 ± 1.3
34.7 ± 0.8 45.5 ± 0.9 49.7 ± 0.8 53.8 ± 0.8
Table 1: Comparison of local descriptors and kernels on the
DTD data, averaged over ten splits.
29.62 blotchy
31.01 grooved
32.72 spiralled
32.86 smeared
33.11 porous
36.03 stained
40.86 bumpy
41.23 meshed
42.75 marbled
44.61 pitted
50.28 grid
51.06 crosshatched
51.64 matted
52.08 sprinkled
52.51 veined
54.14 pleated
58.01 waffled
58.10 wrinkled
61.18 braided
63.89 flecked
64.96 scaly
66.68 perforated
67.44 woven
69.58 honeycombed
69.99 gauzy
70.30 interlaced
72.30 crystalline
72.55 banded
74.65 polka−dotted
74.91 lined
77.73 zigzagged
77.94 bubbly
78.14 swirly
79.07 fibrous
79.45 dotted
79.83 frilly
80.70 knitted
81.90 freckled
83.01 stratified
83.10 potholed
83.90 cracked
84.22 striped
84.36 lacelike
85.17 studded
87.55 cobwebbed
89.69 paisley
95.65 chequered
SIFT IFV on DTD mAP: 64.52
Figure 4: Per-class AP of the 47 describable attribute classiﬁers on DTD using the IFVSIFT representation and linear
classiﬁers.
the response of attribute classiﬁers trained on DTD in a
47-dimensional feature vector φ(ℓ) = (c1(ℓ), . . . , c47(ℓ)).
Sect. 4 shows that this very compact representation achieves
excellent performance in material recognition; in particular,
combined with IFV (SIFT and color) and/or DeCAF it sets
the new state-of-the-art on KTH-TIPS2-b and FMD. In addition to the contribution to the best results, our proposed
attributes generate meaningful descriptions of the materials from KTH-TIPS, e.g. aluminium foil: wrinkled; bread:
4. Experiments
4.1. Object descriptors for textures
This section demonstrates the power of IFV and DeCAF
(Sect. 3.1) as a texture representation by comparing it to established texture descriptors. Most of these representations
can be broken down into two parts: computing local image
descriptors {d1, . . . , dn} and encoding them into a global
image statistics φ(ℓ).
In IFV the local descriptors di are 128-dimensional
SIFT features, capturing a spatial histogram of the local gradient orientations; here spatial bins have an extent of 6 × 6
pixels and descriptors are sampled every two pixels and at
scales 2i/3, i = 0, 1, 2, . . . . We also evaluate as local descriptors the Leung and Malik (LM) (48-D) and MR8
(8-D) ﬁlter banks, the 3 × 3 and 7 × 7 raw image
patches of , and the local binary patterns (LBP) of .
Encoding maps image descriptors {d1, . . . , dn} to a
statistics φ(ℓ)
Rd suitable for classiﬁcation.
Encoding can be as simple as averaging (sum-pooling) descriptors , although this is often preceded by a highdimensional sparse coding step. The most common coding method is to vector quantize the descriptors using an
algorithm such as K-means , resulting in the so-called
bag-of-visual-words (BoVW) representation . Variations
include soft quantization by a GMM in FV (Sect. 3.1),
soft quantization with a kernel in KCB , Localityconstrained Linear Coding (LLC) , or specialized quantization schemes, such as mapping LBPs to uniform patterns (LBPu; we use the rotation invariant multipleradii version of for comparison purposes). For LBP,
we also experiment with a variant (LBP-VQ) where standard LBPu2 is computed in 8 × 8 pixel neighborhoods, and
the resulting local descriptors are further vector quantized
using K-means and pooled as this scheme performs significantly better in our experiments.
For each of the selected features, we experimented
with several SVM kernels K(x′, x′′): linear, Hellinger’s,
additive-χ2, and exponential-χ2 kernels sign-extended as
in . The λ parameter of the exponential kernel is
selected as one over the mean of the kernel matrix on the
training set. The data is normalized so that K(x′, x′′) = 1
as this is often found to improve performance. Learning
uses a standard non-linear SVM solver and validation to select the parameter C. When multiple features are used, the
corresponding kernels are averaged.
Local descriptor comparisons on DTD. This experiment compares local descriptors and kernels on DTD
(Tab. 1). All comparison use the bag-of-visual-word pooling/encoding scheme using K-means for vector quantization the descriptors. The DTD data is used as a benchmark
averaging the results on the ten train-val-test splits. K was
cross-validated, ﬁnding an optimal setting of 1024 visual
words for SIFT and color patches, 512 for LBP-VQ, 470
for the ﬁlter banks. Tab. 1 reports the mean Average Precision (mAP) for 47 SVM attribute classiﬁers. In these experiments, only the key attribute labels for each image are
used; joint annotations are evaluated as DTD-J in Tab. 2,
with similar results. As expected, the best kernel is exp-
χ2, followed by additive χ2 and Hellinger, and then linear.
Dense SIFT (53.8% mAP) outperforms the best specialized
texture descriptor on the DTD data (39.7% mAP for LM).
Fig. 4 shows AP for each attribute: concepts like chequered
achieve nearly perfect classiﬁcation, while others such as
blotchy and smeared are far harder.
Encoding comparisons on DTD. Having established the
excellent performance of SIFT in texture recognition, this
Previous Best
99.4±n/a 
99.7±0.3 
99.4±0.4 
99.4±0.4 
73.0±4.7 
57.1 γ 
Table 2: Comparison of encodings and state-of-the-art texture recognition methods on DTD as well as standard material
recognition benchmarks (in boldface results on par or better than the previous state-of-the-art). All experiments use a linear
SVM. α : three samples for training, one for evaluation; β : one sample for training, three for evaluation. γ : with ground
truth masks ( Sect. 6.5); our results do not use them. δ : DTD considers only the key attribute label of each texture
occurrence and DTD-J includes the joint attribute annotations too (Sect. 2.1), reporting mAP.
experiment compares three encodings: BoVW, VLAD ,
LLC, KCB, and IFV (ﬁrst ﬁve columns of Tab. 2). VLAD
is similar to IFV, but uses K-means for quantization and
stores only ﬁrst-order statistics of the descriptors. Dense
SIFT is used as a baseline descriptor and performance is
evaluated on ten splits of DTD in Tab. 2. IFV (256 Gaussian
modes) and VLAD (512 K-means centers) performs similarly (61-63% mAP) and signiﬁcantly better than BoVW
(54.9% mAP). For BoVW we considered a vocabulary size
of 4096 words, while for LLC and KCB we used vocabularies of size 10k.
As we will see next, however, IFV
signiﬁcantly outperforms VLAD in other texture datasets.
We also experimented with the state-of-the-art descriptor of
 which we did not ﬁnd to be competitive with IFV on
FMD (41.4% acc.) and DTD (40.3% acc.).
State-of-the-art material classiﬁcation.
This experiments evaluates the encodings on several texture recognition benchmarks: CUReT , UMD , UIUC ,
KTH-TIPS , KTH-TIPS2 (a and b) , and material
– FMD . Tab. 2 compares with the existing state-ofthe-art on each of them. For saturated datasets
such as CUReT, UMD, UIUC, KTH-TIPS the performance
of most methods is above to 99% mean accuracy and there
is little difference between them. IFV performs as well or
nearly as well as the state-of-the-art, but DeCAF is not as
good. However, in harder datasets the advantage of IFV and
DeCAF becomes evident: KTH-TIPS-2a (+5%/5% resp.),
KTH-TIPS-2b (+3%/4.3%), and FMD (+1%/+3.6%). Remarkably, DeCAF and IFV appear to capture complementary information as their combination results in signiﬁcant
improvements over each descriptor individually, substantially outperforming any other descriptor in KTH (+11.7%
on the former state-of-the-art), FMD (+9.9%), and DTD
(+8%). In particular, while FMD includes manual segmentations of the textures, these are not used when reporting
our results. Furthermore, IFV and DeCAF are conceptually
simpler than the multiple specialized features used in 
for material recognition.
4.2. Describable attributes as a representation
This section evaluates the 47 describable attributes as a
texture descriptor for material recognition (Tab. 3). The attribute classiﬁers are trained on DTD using the various representations such as IFVSIFT, DeCAF, or combinations and
linear classiﬁers as in the previous section. As explained in
Sect. 3.2, these are then used to form 47-dimensional descriptors of each texture image in FMD and KTH-TIPS2-b.
We call this as DTDfeat
method, denoting the choice of the ﬁnal
classiﬁer (method) and underlying features (feat) used for
DTD attribute estimation.
The best results are obtained when IFVSIFT + DeCAF
features are used as the underlying representation for predicting DTD attributes. When combined with a linear SVM
classiﬁer DTDIFV + DeCAF
1, results are promising: on KTH-
TIPS2-b, the describable attributes yield 71.2% mean accuracy and 55.9% on FMD outperforming the aLDA model
of combining color, SIFT and edge-slice (44.6%).
While results are not as good as the IFVSIFT + DeCAF
representation directly, the dimensionality of this descriptor is three orders of magnitude smaller.
For this reason, using an RBF classiﬁer with the DTD features is relatively cheap. Doing so improves the performance by 1–
2% (DTDIFV + DeCAF
). DTD descriptors constructed out of
IFV alone are also quite competitive achieving 62.9% and
49.8% on KTH-2b and FMD respectively. They also show
a 2–3% improvement when combined with RBF kernels.
Combining the DTD RBF kernels obtained from IFVSIFT
and IFVSIFT + DeCAF improves performance further.
We also investigated combining multiple IFV features
1Note: we drop SIFT in IFVSIFT for brevity
with DTD descriptors: DTDIFV
RBF with IFVSIFT and IFVRGB.
IFVRGB computes the IFV representation on top of all the
3 × 3 RGB patches in the image in the spirit of . The
performance of IFVRGB is notable given the simplicity of
the local descriptors; however, it is not as good as DTDIFV
which is also 26 times smaller. The combination of IFVSIFT
and IFVRGB is already notably better than the previous stateof-the-art results and the addition of DTDIFV
RBF improves by
another signiﬁcant margin. Similarly the DTDIFV
RBF descriptors also provide a signiﬁcant improvement over DeCAF
features alone.
Overall, our best result on KTH-TIPS-2b is 77.3% acc.
(vs. the previous best of 66.3) and on FMD of 67.1% acc.
(vs. 57.1) on FMD, an improvement of more than 10% in
both cases over the previous state of the art.
Finally, we compared the semantic attributes of with
LIN on the Outex data. Using IFVSIFT as an underlying
representation for our attributes, we obtain 49.82% mAP on
the retrieval experiment of , which is is not as good as
their result with LBPu (63.3%). However, LBPu was developed on the Outex data, and it is therefore not surprising that it works so well. To verify this, we retrained our
DTD attributes with IFV using LBPu as local descriptor,
obtaining a score of 64.5% mAP. This is remarkable considering that their retrieval experiment contains the data used
to train their own attributes (target set), while our attributes
are trained on a completely different data source. Tab. 1
shows that LBPu is not competitive on DTD.
4.3. Search and visualization
Fig. 5 shows an excellent semantic correlation between
the ten categories in KTH-TIPS-2b and the attributes in
DTD. For example, aluminium foil is found to be wrinkled,
while bread is found be bumpy, pitted, porous and ﬂecked.
As an additional application of our describable texture
attributes we compute them on a large dataset of 10,000
wallpapers and bedding sets from houzz.com. The 47
attribute classiﬁers are learned as explained in Sect. 4.1 using the IFVSIFT representation and them apply them to the
10,000 images to predict the strength of association of each
attribute and image. Classiﬁers scores are re-calibrated on
the target data and converted to probabilities by examining the extremal statistics of the scores. Fig. 6 shows some
example attribute predictions, selecting for a number of attribute an image that would score perfectly (excluding images used for calibrating the scores), and then including additional top two attribute matches. The top two matches
tend to be very good description of each texture or pattern,
while the third is a good match in about half of the cases.
5. Summary
We introduced a large dataset of 5,640 images collected
“in the wild” jointly labelled with 47 describable texture
62.9±3.8 49.8±1.3
66.0±4.3 52.4±1.3
DTDIFV + DeCAF
71.2±0.6 55.9±2.3
DTDIFV + DeCAF
72.0±0.5 58.0±1.8
RBF + DTDIFV + DeCAF
73.8±1.3 61.1±1.4
70.7±1.6 60.7±2.1
58.8±2.5 47.0±2.7
IFVSIFT + IFVRGB
67.5±3.3 63.3±1.9
RBF + IFVSIFT
70.2±2.4 60.1±1.6
RBF + IFVRGB
70.9±3.5 61.3±2.0
74.6±3.0 65.4±2.0
IFVSIFT + DTDIFV
70.2±2.4 60.0±1.9
IFVSIFT + DTDIFV + DeCAF
75.6±1.8 65.5±1.2
DeCAF + DTDIFV
75.4±1.8 64.6±1.6
DeCAF + DTDIFV + DeCAF
73.7±1.8 64.1±1.5
IFVSIFT +DeCAF + DTDIFV
77.3±2.3 66.7±1.7
IFVSIFT +DeCAF + DTDIFV + DeCAF
76.4±2.8 66.9±1.6
77.1±2.4 67.1±1.5
Prev. best
66.3 57.1 
Table 3: DTD for material recognition. Combined with
IFVSIFT and IFVRGB, the DTDIFV
RBF features achieve a signiﬁcant improvement in classiﬁcation performance on the challenging KTH-TIPS-2b and FMD compared to published
state of the art results. See the text for the details on the
notation and the methods.
attributes and used it to study the problem of extracting semantic properties of textures and patterns, addressing realworld human-centric applications. Looking for the best representation to recognize such describable attributes in natural images, we have ported IFV and DeCAF, object recognition representations, to the texture domain.
they work best in recognizing describable attributes, but
they also outperform specialized texture representations on
a number of challenging material recognition benchmarks.
We have shown that the describable attributes, while not being designed to do so, are good predictors of materials as
well, and that, when combined with IFV, signiﬁcantly outperform the state-of-the-art on FMD and KTH-TIPS2-b.
Acknowledgements. This research is based on work done
at the 2012 CLSP Summer Workshop, and was partially
supported by NSF Grant #1005411, ODNI via the JHU
HLTCOE and Google Research. Mircea Cimpoi was supported by the ERC grant VisRec no. 228180 and Iasonas
Kokkinos by ANR-10-JCJC-0205.