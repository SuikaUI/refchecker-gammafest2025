Neural Collaborative Filtering∗
Xiangnan He
National University of
Singapore, Singapore
 
National University of
Singapore, Singapore
 
Hanwang Zhang
Columbia University
 
Liqiang Nie
Shandong University
 
Texas A&M University
 
Tat-Seng Chua
National University of
Singapore, Singapore
 
In recent years, deep neural networks have yielded immense
success on speech recognition, computer vision and natural
language processing. However, the exploration of deep neural networks on recommender systems has received relatively
less scrutiny. In this work, we strive to develop techniques
based on neural networks to tackle the key problem in recommendation — collaborative ﬁltering — on the basis of
implicit feedback.
Although some recent work has employed deep learning
for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and
acoustic features of musics. When it comes to model the key
factor in collaborative ﬁltering — the interaction between
user and item features, they still resorted to matrix factorization and applied an inner product on the latent features
of users and items.
By replacing the inner product with a neural architecture
that can learn an arbitrary function from data, we present
a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities,
we propose to leverage a multi-layer perceptron to learn the
user–item interaction function.
Extensive experiments on
two real-world datasets show signiﬁcant improvements of our
proposed NCF framework over the state-of-the-art methods.
Empirical evidence shows that using deeper layers of neural
networks oﬀers better recommendation performance.
Collaborative Filtering, Neural Networks, Deep Learning,
Matrix Factorization, Implicit Feedback
∗NExT research is supported by the National Research
Foundation, Prime Minister’s Oﬃce, Singapore under its
IRC@SG Funding Initiative.
c⃝2017 International World Wide Web Conference Committee
(IW3C2), published under Creative Commons CC BY 4.0 License.
WWW 2017, April 3–7, 2017, Perth, Australia.
ACM 978-1-4503-4913-0/17/04.
 
INTRODUCTION
In the era of information explosion, recommender systems
play a pivotal role in alleviating information overload, having been widely adopted by many online services, including
E-commerce, online news and social media sites. The key to
a personalized recommender system is in modelling users’
preference on items based on their past interactions (e.g.,
ratings and clicks), known as collaborative ﬁltering .
Among the various collaborative ﬁltering techniques, matrix
factorization (MF) is the most popular one, which
projects users and items into a shared latent space, using
a vector of latent features to represent a user or an item.
Thereafter a user’s interaction on an item is modelled as the
inner product of their latent vectors.
Popularized by the Netﬂix Prize, MF has become the de
facto approach to latent factor model-based recommendation. Much research eﬀort has been devoted to enhancing
MF, such as integrating it with neighbor-based models ,
combining it with topic models of item content , and extending it to factorization machines for a generic modelling of features. Despite the eﬀectiveness of MF for collaborative ﬁltering, it is well-known that its performance can be
hindered by the simple choice of the interaction function —
inner product. For example, for the task of rating prediction
on explicit feedback, it is well known that the performance
of the MF model can be improved by incorporating user
and item bias terms into the interaction function1. While
it seems to be just a trivial tweak for the inner product
operator , it points to the positive eﬀect of designing a
better, dedicated interaction function for modelling the latent feature interactions between users and items. The inner
product, which simply combines the multiplication of latent
features linearly, may not be suﬃcient to capture the complex structure of user interaction data.
This paper explores the use of deep neural networks for
learning the interaction function from data, rather than a
handcraft that has been done by many previous work . The neural network has been proven to be capable of
approximating any continuous function , and more recently deep neural networks (DNNs) have been found to be
eﬀective in several domains, ranging from computer vision,
speech recognition, to text processing . However, there is relatively little work on employing DNNs for
recommendation in contrast to the vast amount of literature
1 
Recommender.pdf
 
on MF methods. Although some recent advances have applied DNNs to recommendation tasks and shown
promising results, they mostly used DNNs to model auxiliary information, such as textual description of items, audio
features of musics, and visual content of images. With regards to modelling the key collaborative ﬁltering eﬀect, they
still resorted to MF, combining user and item latent features
using an inner product.
This work addresses the aforementioned research problems by formalizing a neural network modelling approach for
collaborative ﬁltering. We focus on implicit feedback, which
indirectly reﬂects users’ preference through behaviours like
watching videos, purchasing products and clicking items.
Compared to explicit feedback (i.e., ratings and reviews),
implicit feedback can be tracked automatically and is thus
much easier to collect for content providers. However, it is
more challenging to utilize, since user satisfaction is not observed and there is a natural scarcity of negative feedback.
In this paper, we explore the central theme of how to utilize
DNNs to model noisy implicit feedback signals.
The main contributions of this work are as follows.
1. We present a neural network architecture to model
latent features of users and items and devise a general framework NCF for collaborative ﬁltering based
on neural networks.
2. We show that MF can be interpreted as a specialization
of NCF and utilize a multi-layer perceptron to endow
NCF modelling with a high level of non-linearities.
3. We perform extensive experiments on two real-world
datasets to demonstrate the eﬀectiveness of our NCF
approaches and the promise of deep learning for collaborative ﬁltering.
PRELIMINARIES
We ﬁrst formalize the problem and discuss existing solutions for collaborative ﬁltering with implicit feedback. We
then shortly recapitulate the widely used MF model, highlighting its limitation caused by using an inner product.
Learning from Implicit Data
Let M and N denote the number of users and items,
respectively.
We deﬁne the user–item interaction matrix
Y ∈RM×N from users’ implicit feedback as,
if interaction (user u, item i) is observed;
otherwise.
Here a value of 1 for yui indicates that there is an interaction between user u and item i; however, it does not mean u
actually likes i. Similarly, a value of 0 does not necessarily
mean u does not like i, it can be that the user is not aware
of the item. This poses challenges in learning from implicit
data, since it provides only noisy signals about users’ preference. While observed entries at least reﬂect users’ interest
on items, the unobserved entries can be just missing data
and there is a natural scarcity of negative feedback.
The recommendation problem with implicit feedback is
formulated as the problem of estimating the scores of unobserved entries in Y, which are used for ranking the items.
Model-based approaches assume that data can be generated
(or described) by an underlying model. Formally, they can
be abstracted as learning ˆyui = f(u, i|Θ), where ˆyui denotes
(a) user–item matrix
(b) user latent space
An example illustrates MF’s limitation.
From data matrix (a), u4 is most similar to u1, followed by u3, and lastly u2.
However in the latent
space (b), placing p4 closest to p1 makes p4 closer to
p2 than p3, incurring a large ranking loss.
the predicted score of interaction yui, Θ denotes model parameters, and f denotes the function that maps model parameters to the predicted score (which we term as an interaction function).
To estimate parameters Θ, existing approaches generally
follow the machine learning paradigm that optimizes an objective function. Two types of objective functions are most
commonly used in literature — pointwise loss and
pairwise loss . As a natural extension of abundant
work on explicit feedback , methods on pointwise
learning usually follow a regression framework by minimizing the squared loss between ˆyui and its target value yui.
To handle the absence of negative data, they have either
treated all unobserved entries as negative feedback, or sampled negative instances from unobserved entries .
pairwise learning , the idea is that observed entries
should be ranked higher than the unobserved ones. As such,
instead of minimizing the loss between ˆyui and yui, pairwise
learning maximizes the margin between observed entry ˆyui
and unobserved entry ˆyuj.
Moving one step forward, our NCF framework parameterizes the interaction function f using neural networks to
estimate ˆyui. As such, it naturally supports both pointwise
and pairwise learning.
Matrix Factorization
MF associates each user and item with a real-valued vector
of latent features. Let pu and qi denote the latent vector for
user u and item i, respectively; MF estimates an interaction
yui as the inner product of pu and qi:
ˆyui = f(u, i|pu, qi) = pT
where K denotes the dimension of the latent space. As we
can see, MF models the two-way interaction of user and item
latent factors, assuming each dimension of the latent space
is independent of each other and linearly combining them
with the same weight. As such, MF can be deemed as a
linear model of latent factors.
Figure 1 illustrates how the inner product function can
limit the expressiveness of MF. There are two settings to be
stated clearly beforehand to understand the example well.
First, since MF maps users and items to the same latent
space, the similarity between two users can also be measured
with an inner product, or equivalently2, the cosine of the
angle between their latent vectors. Second, without loss of
2Assuming latent vectors are of a unit length.
Input Layer (Sparse)
Embedding Layer
Neural CF Layers
Output Layer
User Latent Vector
Item Latent Vector
PM×K = {puk}
QN×K = {qik}
Figure 2: Neural collaborative ﬁltering framework
generality, we use the Jaccard coeﬃcient3 as the groundtruth similarity of two users that MF needs to recover.
Let us ﬁrst focus on the ﬁrst three rows (users) in Figure 1a. It is easy to have s23(0.66) > s12(0.5) > s13(0.4).
As such, the geometric relations of p1, p2, and p3 in the latent space can be plotted as in Figure 1b. Now, let us consider a new user u4, whose input is given as the dashed line
in Figure 1a. We can have s41(0.6) > s43(0.4) > s42(0.2),
meaning that u4 is most similar to u1, followed by u3, and
lastly u2. However, if a MF model places p4 closest to p1
(the two options are shown in Figure 1b with dashed lines),
it will result in p4 closer to p2 than p3, which unfortunately
will incur a large ranking loss.
The above example shows the possible limitation of MF
caused by the use of a simple and ﬁxed inner product to estimate complex user–item interactions in the low-dimensional
latent space. We note that one way to resolve the issue is
to use a large number of latent factors K. However, it may
adversely hurt the generalization of the model (e.g., over-
ﬁtting the data), especially in sparse settings . In this
work, we address the limitation by learning the interaction
function using DNNs from data.
NEURAL COLLABORATIVE FILTERING
We ﬁrst present the general NCF framework, elaborating how to learn NCF with a probabilistic model that emphasizes the binary property of implicit data.
show that MF can be expressed and generalized under NCF.
To explore DNNs for collaborative ﬁltering, we then propose an instantiation of NCF, using a multi-layer perceptron
(MLP) to learn the user–item interaction function. Lastly,
we present a new neural matrix factorization model, which
ensembles MF and MLP under the NCF framework; it uni-
ﬁes the strengths of linearity of MF and non-linearity of
MLP for modelling the user–item latent structures.
General Framework
To permit a full neural treatment of collaborative ﬁltering,
we adopt a multi-layer representation to model a user–item
interaction yui as shown in Figure 2, where the output of one
layer serves as the input of the next one. The bottom input
layer consists of two feature vectors vU
i that describe
user u and item i, respectively; they can be customized to
support a wide range of modelling of users and items, such
3Let Ru be the set of items that user u has interacted with,
then the Jaccard similarity of users i and j is deﬁned as
|Ri|∪|Rj|.
as context-aware , content-based , and neighborbased . Since this work focuses on the pure collaborative
ﬁltering setting, we use only the identity of a user and an
item as the input feature, transforming it to a binarized
sparse vector with one-hot encoding. Note that with such a
generic feature representation for inputs, our method can be
easily adjusted to address the cold-start problem by using
content features to represent users and items.
Above the input layer is the embedding layer; it is a fully
connected layer that projects the sparse representation to
a dense vector.
The obtained user (item) embedding can
be seen as the latent vector for user (item) in the context
of latent factor model. The user embedding and item embedding are then fed into a multi-layer neural architecture,
which we term as neural collaborative ﬁltering layers, to map
the latent vectors to prediction scores. Each layer of the neural CF layers can be customized to discover certain latent
structures of user–item interactions. The dimension of the
last hidden layer X determines the model’s capability. The
ﬁnal output layer is the predicted score ˆyui, and training
is performed by minimizing the pointwise loss between ˆyui
and its target value yui. We note that another way to train
the model is by performing pairwise learning, such as using
the Bayesian Personalized Ranking and margin-based
loss . As the focus of the paper is on the neural network
modelling part, we leave the extension to pairwise learning
of NCF as a future work.
We now formulate the NCF’s predictive model as
ˆyui = f(PT vU
i |P, Q, Θf),
where P ∈RM×K and Q ∈RN×K, denoting the latent factor matrix for users and items, respectively; and Θf denotes
the model parameters of the interaction function f. Since
the function f is deﬁned as a multi-layer neural network, it
can be formulated as
i ) = φout(φX(...φ2(φ1(PT vU
i ))...)),
where φout and φx respectively denote the mapping function
for the output layer and x-th neural collaborative ﬁltering
(CF) layer, and there are X neural CF layers in total.
Learning NCF
To learn model parameters, existing pointwise methods largely perform a regression with squared loss:
(u,i)∈Y∪Y−
wui(yui −ˆyui)2,
where Y denotes the set of observed interactions in Y, and
Y−denotes the set of negative instances, which can be all (or
sampled from) unobserved interactions; and wui is a hyperparameter denoting the weight of training instance (u, i).
While the squared loss can be explained by assuming that
observations are generated from a Gaussian distribution ,
we point out that it may not tally well with implicit data.
This is because for implicit data, the target value yui is
a binarized 1 or 0 denoting whether u has interacted with
i. In what follows, we present a probabilistic approach for
learning the pointwise NCF that pays special attention to
the binary property of implicit data.
Considering the one-class nature of implicit feedback, we
can view the value of yui as a label — 1 means item i is
relevant to u, and 0 otherwise.
The prediction score ˆyui
then represents how likely i is relevant to u. To endow NCF
with such a probabilistic explanation, we need to constrain
the output ˆyui in the range of , which can be easily
achieved by using a probabilistic function (e.g., the Logistic
or Probit function) as the activation function for the output
layer φout.
With the above settings, we then deﬁne the
likelihood function as
p(Y, Y−|P, Q, Θf) =
(1 −ˆyuj).
Taking the negative logarithm of the likelihood, we reach
log ˆyui −
log(1 −ˆyuj)
(u,i)∈Y∪Y−
yui log ˆyui + (1 −yui) log(1 −ˆyui).
This is the objective function to minimize for the NCF methods, and its optimization can be done by performing stochastic gradient descent (SGD). Careful readers might have realized that it is the same as the binary cross-entropy loss, also
known as log loss. By employing a probabilistic treatment
for NCF, we address recommendation with implicit feedback
as a binary classiﬁcation problem.
As the classiﬁcationaware log loss has rarely been investigated in recommendation literature, we explore it in this work and empirically
show its eﬀectiveness in Section 4.3. For the negative instances Y−, we uniformly sample them from unobserved interactions in each iteration and control the sampling ratio
w.r.t. the number of observed interactions. While a nonuniform sampling strategy (e.g., item popularity-biased ) might further improve the performance, we leave the
exploration as a future work.
Generalized Matrix Factorization (GMF)
We now show how MF can be interpreted as a special case
of our NCF framework. As MF is the most popular model
for recommendation and has been investigated extensively
in literature, being able to recover it allows NCF to mimic
a large family of factorization models .
Due to the one-hot encoding of user (item) ID of the input
layer, the obtained embedding vector can be seen as the
latent vector of user (item). Let the user latent vector pu
u and item latent vector qi be QT vI
i . We deﬁne the
mapping function of the ﬁrst neural CF layer as
φ1(pu, qi) = pu ⊙qi,
where ⊙denotes the element-wise product of vectors. We
then project the vector to the output layer:
ˆyui = aout(hT (pu ⊙qi)),
where aout and h denote the activation function and edge
weights of the output layer, respectively. Intuitively, if we
use an identity function for aout and enforce h to be a uniform vector of 1, we can exactly recover the MF model.
Under the NCF framework, MF can be easily generalized and extended. For example, if we allow h to be learnt
from data without the uniform constraint, it will result in
a variant of MF that allows varying importance of latent
dimensions. And if we use a non-linear function for aout, it
will generalize MF to a non-linear setting which might be
more expressive than the linear MF model. In this work, we
implement a generalized version of MF under NCF that uses
the sigmoid function σ(x) = 1/(1 + e−x) as aout and learns
h from data with the log loss (Section 3.1.1). We term it as
GMF, short for Generalized Matrix Factorization.
Multi-Layer Perceptron (MLP)
Since NCF adopts two pathways to model users and items,
it is intuitive to combine the features of two pathways by
concatenating them. This design has been widely adopted
in multimodal deep learning work . However, simply
a vector concatenation does not account for any interactions
between user and item latent features, which is insuﬃcient
for modelling the collaborative ﬁltering eﬀect. To address
this issue, we propose to add hidden layers on the concatenated vector, using a standard MLP to learn the interaction
between user and item latent features. In this sense, we can
endow the model a large level of ﬂexibility and non-linearity
to learn the interactions between pu and qi, rather than the
way of GMF that uses only a ﬁxed element-wise product
on them. More precisely, the MLP model under our NCF
framework is deﬁned as
z1 = φ1(pu, qi) =
φ2(z1) = a2(WT
2 z1 + b2),
φL(zL−1) = aL(WT
LzL−1 + bL),
ˆyui = σ(hT φL(zL−1)),
where Wx, bx, and ax denote the weight matrix, bias vector, and activation function for the x-th layer’s perceptron,
respectively.
For activation functions of MLP layers, one
can freely choose sigmoid, hyperbolic tangent (tanh), and
Rectiﬁer (ReLU), among others.
We would like to analyze each function: 1) The sigmoid function restricts each
neuron to be in (0,1), which may limit the model’s performance; and it is known to suﬀer from saturation, where
neurons stop learning when their output is near either 0 or
2) Even though tanh is a better choice and has been
widely adopted , it only alleviates the issues of sigmoid to a certain extent, since it can be seen as a rescaled
version of sigmoid (tanh(x/2) = 2σ(x) −1).
such, we opt for ReLU, which is more biologically plausible and proven to be non-saturated ; moreover, it encourages sparse activations, being well-suited for sparse data and
making the model less likely to be overﬁtting. Our empirical
results show that ReLU yields slightly better performance
than tanh, which in turn is signiﬁcantly better than sigmoid.
As for the design of network structure, a common solution
is to follow a tower pattern, where the bottom layer is the
widest and each successive layer has a smaller number of
neurons (as in Figure 2). The premise is that by using a
small number of hidden units for higher layers, they can
learn more abstractive features of data . We empirically
implement the tower structure, halving the layer size for
each successive higher layer.
Fusion of GMF and MLP
So far we have developed two instantiations of NCF —
GMF that applies a linear kernel to model the latent feature
interactions, and MLP that uses a non-linear kernel to learn
the interaction function from data. The question then arises:
how can we fuse GMF and MLP under the NCF framework,
Item ( i )
MF User Vector
MF Item Vector
MLP Layer 1
MLP User Vector
MLP Item Vector
Element-wise
Concatenation
MLP Layer 2
MLP Layer X
NeuMF Layer
Concatenation
Figure 3: Neural matrix factorization model
so that they can mutually reinforce each other to better
model the complex user-iterm interactions?
A straightforward solution is to let GMF and MLP share
the same embedding layer, and then combine the outputs of
their interaction functions. This way shares a similar spirit
with the well-known Neural Tensor Network (NTN) .
Speciﬁcally, the model for combining GMF with a one-layer
MLP can be formulated as
ˆyui = σ(hT a(pu ⊙qi + W
However, sharing embeddings of GMF and MLP might
limit the performance of the fused model.
For example,
it implies that GMF and MLP must use the same size of
embeddings; for datasets where the optimal embedding size
of the two models varies a lot, this solution may fail to obtain
the optimal ensemble.
To provide more ﬂexibility to the fused model, we allow
GMF and MLP to learn separate embeddings, and combine
the two models by concatenating their last hidden layer.
Figure 3 illustrates our proposal, the formulation of which
is given as follows
φMLP = aL(WT
L(aL−1(...a2(WT
+ b2)...)) + bL),
ˆyui = σ(hT
denote the user embedding for GMF
and MLP parts, respectively; and similar notations of qG
for item embeddings. As discussed before, we use
ReLU as the activation function of MLP layers. This model
combines the linearity of MF and non-linearity of DNNs for
modelling user–item latent structures. We dub this model
“NeuMF”, short for Neural Matrix Factorization. The derivative of the model w.r.t. each model parameter can be calculated with standard back-propagation, which is omitted
here due to space limitation.
Pre-training
Due to the non-convexity of the objective function of NeuMF,
gradient-based optimization methods only ﬁnd locally-optimal
solutions. It is reported that the initialization plays an important role for the convergence and performance of deep
learning models . Since NeuMF is an ensemble of GMF
and MLP, we propose to initialize NeuMF using the pretrained models of GMF and MLP.
We ﬁrst train GMF and MLP with random initializations
until convergence. We then use their model parameters as
the initialization for the corresponding parts of NeuMF’s
parameters. The only tweak is on the output layer, where
we concatenate weights of the two models with
(1 −α)hMLP
where hGMF and hMLP denote the h vector of the pretrained GMF and MLP model, respectively; and α is a
hyper-parameter determining the trade-oﬀbetween the two
pre-trained models.
For training GMF and MLP from scratch, we adopt the
Adaptive Moment Estimation (Adam) , which adapts
the learning rate for each parameter by performing smaller
updates for frequent and larger updates for infrequent parameters. The Adam method yields faster convergence for
both models than the vanilla SGD and relieves the pain of
tuning the learning rate. After feeding pre-trained parameters into NeuMF, we optimize it with the vanilla SGD, rather
than Adam. This is because Adam needs to save momentum
information for updating parameters properly. As we initialize NeuMF with pre-trained model parameters only and
forgo saving the momentum information, it is unsuitable to
further optimize NeuMF with momentum-based methods.
EXPERIMENTS
In this section, we conduct experiments with the aim of
answering the following research questions:
RQ1 Do our proposed NCF methods outperform the stateof-the-art implicit collaborative ﬁltering methods?
RQ2 How does our proposed optimization framework (log
loss with negative sampling) work for the recommendation task?
RQ3 Are deeper layers of hidden units helpful for learning
from user–item interaction data?
In what follows, we ﬁrst present the experimental settings,
followed by answering the above three research questions.
Experimental Settings
Datasets. We experimented with two publicly accessible
datasets: MovieLens4 and Pinterest5. The characteristics of
the two datasets are summarized in Table 1.
MovieLens.
This movie rating dataset has been
widely used to evaluate collaborative ﬁltering algorithms.
We used the version containing one million ratings, where
each user has at least 20 ratings.
While it is an explicit
feedback data, we have intentionally chosen it to investigate
the performance of learning from the implicit signal of
explicit feedback. To this end, we transformed it into implicit data, where each entry is marked as 0 or 1 indicating
whether the user has rated the item.
2. Pinterest. This implicit feedback data is constructed
by for evaluating content-based image recommendation.
4 
5 
academic-projects
Table 1: Statistics of the evaluation datasets.
Interaction#
The original data is very large but highly sparse. For example, over 20% of users have only one pin, making it diﬃcult
to evaluate collaborative ﬁltering algorithms. As such, we
ﬁltered the dataset in the same way as the MovieLens data
that retained only users with at least 20 interactions (pins).
This results in a subset of the data that contains 55, 187
users and 1, 500, 809 interactions. Each interaction denotes
whether the user has pinned the image to her own board.
Evaluation Protocols.
To evaluate the performance of
item recommendation, we adopted the leave-one-out evaluation, which has been widely used in literature .
For each user, we held-out her latest interaction as the test
set and utilized the remaining data for training. Since it is
too time-consuming to rank all items for every user during
evaluation, we followed the common strategy that
randomly samples 100 items that are not interacted by the
user, ranking the test item among the 100 items. The performance of a ranked list is judged by Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) . Without special mention, we truncated the ranked list at 10 for
both metrics. As such, the HR intuitively measures whether
the test item is present on the top-10 list, and the NDCG
accounts for the position of the hit by assigning higher scores
to hits at top ranks. We calculated both metrics for each
test user and reported the average score.
Baselines. We compared our proposed NCF methods (GMF,
MLP and NeuMF) with the following methods:
- ItemPop. Items are ranked by their popularity judged
by the number of interactions. This is a non-personalized
method to benchmark the recommendation performance .
- ItemKNN . This is the standard item-based collaborative ﬁltering method. We followed the setting of 
to adapt it for implicit data.
- BPR .
This method optimizes the MF model of
Equation 2 with a pairwise ranking loss, which is tailored
to learn from implicit feedback. It is a highly competitive
baseline for item recommendation. We used a ﬁxed learning
rate, varying it and reporting the best performance.
- eALS . This is a state-of-the-art MF method for
item recommendation. It optimizes the squared loss of Equation 5, treating all unobserved interactions as negative instances and weighting them non-uniformly by the item popularity.
Since eALS shows superior performance over the
uniform-weighting method WMF , we do not further report WMF’s performance.
As our proposed methods aim to model the relationship
between users and items, we mainly compare with user–
item models. We leave out the comparison with item–item
models, such as SLIM and CDAE , because the performance diﬀerence may be caused by the user models for
personalization (as they are item–item model).
Parameter Settings. We implemented our proposed methods based on Keras6.
To determine hyper-parameters of
NCF methods, we randomly sampled one interaction for
6 
collaborative_filtering
each user as the validation data and tuned hyper-parameters
on it. All NCF models are learnt by optimizing the log loss
of Equation 7, where we sampled four negative instances
per positive instance.
For NCF models that are trained
from scratch, we randomly initialized model parameters with
a Gaussian distribution (with a mean of 0 and standard
deviation of 0.01), optimizing the model with mini-batch
Adam . We tested the batch size of ,
and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since
the last hidden layer of NCF determines the model capability, we term it as predictive factors and evaluated the
factors of . It is worth noting that large factors
may cause overﬁtting and degrade the performance. Without special mention, we employed three hidden layers for
MLP; for example, if the size of predictive factors is 8, then
the architecture of the neural CF layers is 32 →16 →8, and
the embedding size is 16. For the NeuMF with pre-training,
α was set to 0.5, allowing the pre-trained GMF and MLP to
contribute equally to NeuMF’s initialization.
Performance Comparison (RQ1)
Figure 4 shows the performance of HR@10 and NDCG@10
with respect to the number of predictive factors. For MF
methods BPR and eALS, the number of predictive factors
is equal to the number of latent factors. For ItemKNN, we
tested diﬀerent neighbor sizes and reported the best performance. Due to the weak performance of ItemPop, it is
omitted in Figure 4 to better highlight the performance difference of personalized methods.
First, we can see that NeuMF achieves the best performance on both datasets, signiﬁcantly outperforming the stateof-the-art methods eALS and BPR by a large margin (on
average, the relative improvement over eALS and BPR is
4.5% and 4.9%, respectively).
For Pinterest, even with a
small predictive factor of 8, NeuMF substantially outperforms that of eALS and BPR with a large factor of 64. This
indicates the high expressiveness of NeuMF by fusing the
linear MF and non-linear MLP models. Second, the other
two NCF methods — GMF and MLP — also show quite
strong performance.
Between them, MLP slightly underperforms GMF. Note that MLP can be further improved by
adding more hidden layers (see Section 4.4), and here we
only show the performance of three layers. For small predictive factors, GMF outperforms eALS on both datasets;
although GMF suﬀers from overﬁtting for large factors, its
best performance obtained is better than (or on par with)
that of eALS. Lastly, GMF shows consistent improvements
over BPR, admitting the eﬀectiveness of the classiﬁcationaware log loss for the recommendation task, since GMF and
BPR learn the same MF model but with diﬀerent objective
functions.
Figure 5 shows the performance of Top-K recommended
lists where the ranking position K ranges from 1 to 10. To
make the ﬁgure more clear, we show the performance of
NeuMF rather than all three NCF methods.
seen, NeuMF demonstrates consistent improvements over
other methods across positions, and we further conducted
one-sample paired t-tests, verifying that all improvements
are statistically signiﬁcant for p < 0.01. For baseline methods, eALS outperforms BPR on MovieLens with about 5.1%
relative improvement, while underperforms BPR on Pinterest in terms of NDCG. This is consistent with ’s ﬁnding
that BPR can be a strong performer for ranking performance
(a) MovieLens — HR@10
(b) MovieLens — NDCG@10
(c) Pinterest — HR@10
(d) Pinterest — NDCG@10
Figure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.
(a) MovieLens — HR@K
(b) MovieLens — NDCG@K
(c) Pinterest — HR@K
(d) Pinterest — NDCG@K
Figure 5: Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets.
owing to its pairwise ranking-aware learner. The neighborbased ItemKNN underperforms model-based methods. And
ItemPop performs the worst, indicating the necessity of modeling users’ personalized preferences, rather than just recommending popular items to users.
Utility of Pre-training
To demonstrate the utility of pre-training for NeuMF, we
compared the performance of two versions of NeuMF —
with and without pre-training.
For NeuMF without pretraining, we used the Adam to learn it with random initializations.
As shown in Table 2, the NeuMF with pretraining achieves better performance in most cases; only
for MovieLens with a small predictive factors of 8, the pretraining method performs slightly worse. The relative improvements of the NeuMF with pre-training are 2.2% and
1.1% for MovieLens and Pinterest, respectively.
This result justiﬁes the usefulness of our pre-training method for
initializing NeuMF.
Table 2: Performance of NeuMF with and without
pre-training.
With Pre-training
Without Pre-training
Log Loss with Negative Sampling (RQ2)
To deal with the one-class nature of implicit feedback,
we cast recommendation as a binary classiﬁcation task. By
viewing NCF as a probabilistic model, we optimized it with
the log loss.
Figure 6 shows the training loss (averaged
over all instances) and recommendation performance of NCF
methods of each iteration on MovieLens. Results on Pinterest show the same trend and thus they are omitted due to
space limitation. First, we can see that with more iterations,
the training loss of NCF models gradually decreases and
the recommendation performance is improved.
eﬀective updates are occurred in the ﬁrst 10 iterations, and
more iterations may overﬁt a model (e.g., although the training loss of NeuMF keeps decreasing after 10 iterations, its
recommendation performance actually degrades). Second,
among the three NCF methods, NeuMF achieves the lowest
training loss, followed by MLP, and then GMF. The recommendation performance also shows the same trend that
NeuMF > MLP > GMF. The above ﬁndings provide empirical evidence for the rationality and eﬀectiveness of optimizing the log loss for learning from implicit data.
An advantage of pointwise log loss over pairwise objective
functions is the ﬂexible sampling ratio for negative
instances. While pairwise objective functions can pair only
one sampled negative instance with a positive instance, we
can ﬂexibly control the sampling ratio of a pointwise loss. To
illustrate the impact of negative sampling for NCF methods,
we show the performance of NCF methods w.r.t. diﬀerent
negative sampling ratios in Figure 7. It can be clearly seen
that just one negative sample per positive instance is insuf-
ﬁcient to achieve optimal performance, and sampling more
negative instances is beneﬁcial. Comparing GMF to BPR,
we can see the performance of GMF with a sampling ratio
of one is on par with BPR, while GMF signiﬁcantly betters
Training Loss
(a) Training Loss
(c) NDCG@10
Figure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations
on MovieLens (factors=8).
Number of Negatives
(a) MovieLens — HR@10
Number of Negatives
(b) MovieLens — NDCG@10
Number of Negatives
(c) Pinterest — HR@10
Number of Negatives
(d) Pinterest — NDCG@10
Figure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (factors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a
positive instance for learning.
BPR with larger sampling ratios.
This shows the advantage of pointwise log loss over the pairwise BPR loss. For
both datasets, the optimal sampling ratio is around 3 to 6.
On Pinterest, we ﬁnd that when the sampling ratio is larger
than 7, the performance of NCF methods starts to drop. It
reveals that setting the sampling ratio too aggressively may
adversely hurt the performance.
Is Deep Learning Helpful? (RQ3)
As there is little work on learning user–item interaction
function with neural networks, it is curious to see whether
using a deep network structure is beneﬁcial to the recommendation task. Towards this end, we further investigated
MLP with diﬀerent number of hidden layers. The results
are summarized in Table 3 and 4.
The MLP-3 indicates
the MLP method with three hidden layers (besides the embedding layer), and similar notations for others. As we can
see, even for models with the same capability, stacking more
layers are beneﬁcial to performance. This result is highly
encouraging, indicating the eﬀectiveness of using deep models for collaborative recommendation. We attribute the improvement to the high non-linearities brought by stacking
more non-linear layers. To verify this, we further tried stacking linear layers, using an identity function as the activation
The performance is much worse than using the
ReLU unit.
For MLP-0 that has no hidden layers (i.e., the embedding
layer is directly projected to predictions), the performance is
very weak and is not better than the non-personalized Item-
Pop. This veriﬁes our argument in Section 3.3 that simply
concatenating user and item latent vectors is insuﬃcient for
modelling their feature interactions, and thus the necessity
of transforming it with hidden layers.
Table 3: HR@10 of MLP with diﬀerent layers.
RELATED WORK
While early literature on recommendation has largely focused on explicit feedback , recent attention is increasingly shifting towards implicit data .
collaborative ﬁltering (CF) task with implicit feedback is
usually formulated as an item recommendation problem, for
which the aim is to recommend a short list of items to users.
In contrast to rating prediction that has been widely solved
by work on explicit feedback, addressing the item recommendation problem is more practical but challenging . One
key insight is to model the missing data, which are always
ignored by the work on explicit feedback . To tailor
latent factor models for item recommendation with implicit
feedback, early work applies a uniform weighting
where two strategies have been proposed — which either
treated all missing data as negative instances or sampled negative instances from missing data . Recently, He
et al. and Liang et al. proposed dedicated models
to weight missing data, and Rendle et al. developed an
Table 4: NDCG@10 of MLP with diﬀerent layers.
implicit coordinate descent (iCD) solution for feature-based
factorization models, achieving state-of-the-art performance
for item recommendation. In the following, we discuss recommendation works that use neural networks.
The early pioneer work by Salakhutdinov et al. proposed a two-layer Restricted Boltzmann Machines (RBMs)
to model users’ explicit ratings on items. The work was been
later extended to model the ordinal nature of ratings .
Recently, autoencoders have become a popular choice for
building recommendation systems . The idea of
user-based AutoRec is to learn hidden structures that
can reconstruct a user’s ratings given her historical ratings
as inputs. In terms of user personalization, this approach
shares a similar spirit as the item–item model that
represents a user as her rated items. To avoid autoencoders
learning an identity function and failing to generalize to unseen data, denoising autoencoders (DAEs) have been applied
to learn from intentionally corrupted inputs . More
recently, Zheng et al. presented a neural autoregressive
method for CF. While the previous eﬀort has lent support
to the eﬀectiveness of neural networks for addressing CF,
most of them focused on explicit ratings and modelled the
observed data only. As a result, they can easily fail to learn
users’ preference from the positive-only implicit data.
Although some recent works have explored deep learning models for recommendation based on
implicit feedback, they primarily used DNNs for modelling
auxiliary information, such as textual description of items ,
acoustic features of musics , cross-domain behaviors
of users , and the rich information in knowledge bases .
The features learnt by DNNs are then integrated with MF
for CF. The work that is most relevant to our work is ,
which presents a collaborative denoising autoencoder (CDAE)
for CF with implicit feedback. In contrast to the DAE-based
CF , CDAE additionally plugs a user node to the input
of autoencoders for reconstructing the user’s ratings.
shown by the authors, CDAE is equivalent to the SVD++
model when the identity function is applied to activate the hidden layers of CDAE. This implies that although
CDAE is a neural modelling approach for CF, it still applies
a linear kernel (i.e., inner product) to model user–item interactions. This may partially explain why using deep layers for
CDAE does not improve the performance (cf. Section 6 of
 ). Distinct from CDAE, our NCF adopts a two-pathway
architecture, modelling user–item interactions with a multilayer feedforward neural network. This allows NCF to learn
an arbitrary function from the data, being more powerful
and expressive than the ﬁxed inner product function.
Along a similar line, learning the relations of two entities has been intensively studied in literature of knowledge
graphs . Many relational machine learning methods
have been devised . The one that is most similar to our
proposal is the Neural Tensor Network (NTN) , which
uses neural networks to learn the interaction of two entities
and shows strong performance. Here we focus on a diﬀerent problem setting of CF. While the idea of NeuMF that
combines MF with MLP is partially inspired by NTN, our
NeuMF is more ﬂexible and generic than NTN, in terms of
allowing MF and MLP learning diﬀerent sets of embeddings.
More recently, Google publicized their Wide & Deep learning approach for App recommendation . The deep component similarly uses a MLP on feature embeddings, which has
been reported to have strong generalization ability. While
their work has focused on incorporating various features
of users and items, we target at exploring DNNs for pure
collaborative ﬁltering systems. We show that DNNs are a
promising choice for modelling user–item interactions, which
to our knowledge has not been investigated before.
CONCLUSION AND FUTURE WORK
In this work, we explored neural network architectures
for collaborative ﬁltering. We devised a general framework
NCF and proposed three instantiations — GMF, MLP and
NeuMF — that model user–item interactions in diﬀerent
ways. Our framework is simple and generic; it is not limited
to the models presented in this paper, but is designed to
serve as a guideline for developing deep learning methods for
recommendation. This work complements the mainstream
shallow models for collaborative ﬁltering, opening up a new
avenue of research possibilities for recommendation based
on deep learning.
In future, we will study pairwise learners for NCF models and extend NCF to model auxiliary information, such
as user reviews , knowledge bases , and temporal signals . While existing personalization models have primarily focused on individuals, it is interesting to develop models
for groups of users, which help the decision-making for social
groups . Moreover, we are particularly interested in
building recommender systems for multi-media items, an interesting task but has received relatively less scrutiny in the
recommendation community . Multi-media items, such as
images and videos, contain much richer visual semantics that can reﬂect users’ interest. To build a multi-media
recommender system, we need to develop eﬀective methods
to learn from multi-view and multi-modal data . Another emerging direction is to explore the potential of recurrent neural networks and hashing methods for providing
eﬃcient online recommendation .
Acknowledgement
The authors thank the anonymous reviewers for their valuable comments, which are beneﬁcial to the authors’ thoughts
on recommendation systems and the revision of the paper.