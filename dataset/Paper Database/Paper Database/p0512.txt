Journal of Econometrics 75 {1996) 79-97
JOURNAL OF
Econometrics
Calculating posterior distributions and modal estimates
in Markov mixture models
Siddhartha Chib
John M. Olin Sdlool of Busine.~'s, Washington University, St. Louis, MO 6313 01770-E
S. Chib/Journal of Ecommr'trics 75 79 97
generalizes the situation in which the populations are selected hulependently ac-
cording to a discrete probability mass function. A complete discussion of the
latter class of models is provided by Everitt and Hand and Titterington,
Smith, and Makov .
In contrast to the classical mixture model, the Markov mixture model (MMM)
is especially useful for modeling persistence, i.e., serial correlation in time series
data. The general model can be described in terms of a sequence of unobservabh,
finite state random variables, st E { I,..., m}, which evolve according to a Markov
stlst-I ~ Markov(P, rt0 ),
where P--{ P,i } is the one-step transition probability matrix of the chain, i.e., P0--
Pr(st=j J st-i =i), and nl is the probability distribution at t= !. For identifiability
reasons, assume that this chain is time-homogeneous, irreducible, and aperiodic.
At each observation point t, a realization of the state occurs. Then, given that st--
k, the observation Yt is drawn from the population given by the conditional density
k = I,...,m,
where Yt-I = ()'l,..., )'t-n ), .f is a density (or mass) function with respect to a
a finite measure, and 0~ is the parameter vector of the kth population. Thus, the
observation ~,t t is drawn fi'om the finite mixture distribution
.I'(.~',IY, .... i,s,
..... 1, 0)
~.f(YtlY, vo~,O~)rcm(st=k),
~.f()',JY, ........ ~,0~ ) l,(s, =~: ~..!.~'.: t),
It is possible to obtain a number of important models as special cases of
this structure. For exarnple, the classical finite mixture model is obtained if st is
distributed independently and identically across time. Single and multiple change-
point models can be also be obtained it" suitable restrictions are placed on the off
diagonal elements of P. The Markov switching regression model of Goldfeid and
Quandt and Markov switching autoregressive time series models are also
particular cases of (3). Even the atttoregressive models considered by ttamilton
 and Albert and Chib , in which the conditional density in (2)
depends on lagged values of st, can be put in tiffs thmily by redelining the
states . However, in the lattcr case the support of the
distribution of the states can become quite large.
in recent years, such models have attracted considerable attention in economet-
rics, biometrics, and engin.,:cring. These models are referred to as hidden Markov
S. ChihlJournal ~71 Econometrics 75 79 97
models although the terminology Markov mixture models is more appropriate.
A major problem with MMM's is that the likelihood function of the parameters
is not available in simple form. Much earlier. Baum, Petrie, Soules, and Weiss
 addressed this problem and proposed a recursive algorithm, now called
the forward-backward algorithm, to compute the likelihood function. Even so,
Leroux and Puterman note that the algorithm is often not stable to small
perturbations of the dat~. Leroux in an important paper has established
the asymptotic properties of the maximum likelihood estimator.
A quite different approach to the estima.ion of mixture models is possible
from the Bayesian Markov chain simulation perspective. Basically, the point is
that the computation of the likelihood function can be avoided if the population
index variable {& } is treated as an unknown parameter and simulated along side
the other parameters of the model by Gibbs sampling methods. Such an approach
is used by Diebolt and Robert to estimate the classical mixture model.
For the Marker mixture model, Albert and Chib and McCulloch and Tsay
 , beth in the context of Gaussian lime series models, exploit this idea to
simulate the posterior distribution.
The first main contribution of this paper is to show that it is possible to
simulate the latent data S,, = (sl,s2,...,s,,)
from the joint distribution
sl.s2, ..... s,, I },,, O,
&, E .'1' = { 1,2,..., m}",
rather than the sequence of full conditional distributions st Y,,,si, .j 74. t. This
new result is extremely signiticant. Instead of n additiot:ai blocks in the Gibbs
sample' (the number required if each state is sampled f,o:~.~ its full conditional
dislribulior~), truly one addJ~.ional block is required. This dramatically improves
the convergcnce of the Markov chain induced by the Gibbs sampling algorithm.
Second, it is shown how the Markov chain Monte Carlo approach can bc
moditied slightly to obtain modal estimates, or alternatively maximum likelihood
estimates if diffuse priors are adopted. These modal estimates are obtained using
stochastic versions of the EM algorithm such as the stochastic EM (SEM), and
the Monte Carlo EM. The pertbrmance of these procedures is contrasted with the
lull Bayesian approach.
Third, the ideas are applied to both Gaussian and non-Gaussian discrete data,
and more than two component problems. The examples involve the Poisson dis-
tribution, multivariate Gaussian distributions, and autoregressive time series.
1.2. Gihbs sanq~ling
The approach taken in this paper is motivated by the Gibbs sampling aigo-
rithn3. The idea in Gibbs sampling is to simulate, in turn, the distribution of
each parameter vector conditioned on the data and the remaining parameters (the
so-called full conditional distribution). This process generates a Markov chain,
which under mild conditions converges under the L i norm to the desired posterior
S. 79.97
distribution. The output of the Markov chain, once it has passed its transient stage,
is taken as a sample from the posterior distribution for purposes of computing mo-
ments and marginal densities. Briefly, the systematic form of the Gibbs sampler
for a parameter vector tp (which may include the missing data), with blocking
(~t,...,~d) and full conditional distributions {~,j ] (Y,,,I]/_j), 1 ~< j ~< d}, is given
by the following steps:
Step I" Specify starting values q/0= (q to .... ~,~) and set/= 1.
Step 2: Simulate
~'31 y,,, ~ i,+ , ~ i,+ , ~]' ;4
Step 3: Set i = i + !, and i,,o to 2.
The above cycle is repea'cd a large number of times and the simulated
values {~'. i I> T}, where T is a number sufficiently large so that the sampler has
converged, is used as a sample fi'om the joint distribution ~'1Y,,. Full details are
provided in Gelland and Smith . If the full conditional distributions are
readily sampled, this method is quite easy to implement. Note, that the sampler
is dcti~cd b) ~hc c',,,icc ~;[" ~p amt the cl:oice of b!ockip.g (i,e., the cb,~ice of ,/,,i)
Due to the tact that we include {st } in lit, there is a considerable prolit'cration in
the number ot" parameters if each st is treated individually. A technique to avoid
this problem, by treating all the states as one block and sampling the states from
their joint distribution, is developed next.
2. Full conditional distributions
2. I. S#mdation of the states
The key feature of the new Bayesian Markov chain Monte Carlo approach is
the simulation of the states (tile population index) l'ronl the distribution p(S,,] Y,,,
0), which is the joint posterior mass function of all the states given 0. This
silnulation might seem to be intractable because the range space is .'/, the n-fold
product of the set { !, 2, .... m}. However, it is possible to develop a quite simple
expression ['or the joint distribution that leads to a recursive simulation procedure.
At each step, starting with the terminal state, s,,, only a single state has to be
drawn. To simplit~, the notation and the discussion it is convenient to adopt the
S. Chih/Journal o,f Ecommwtrics 75 79 97
following conventions:
S, = (,¢,,... ,s,),
S H i = (st+t,..., s,,),
with a similar convention applying to Yt and y,+l. In words, St is the history of
the states up to time t, and S t+~ is the future from t + 1 to n. Now write the
joint density (4) in the following manner:
p(S,,IY,,,O)-- p(s,, Y,,,O) x ... x p(s, lY,,,S'+~,O) × ... × p(s~lY,,,sZ, o),
in which the typical term, excluding the terminal point, is given by
p(s, I Y,,,S'+I, 0).
By Bayes theorem,
p(s, I Y,,,S'+l,0) ~ p(s, Y,,O) x .f(Y'~,S'+~lYt,s,,O)
p(s, lY,,O) x p(s,+l st, O) x .f(Y'+l,St+2 l y,,st,sHl,O)
,:x p(s, t Y,,O) x p(s,~l s,,O),
since the term .f(Yt~l,St ~2ly,,s,,st~l,O) is independent of s,. Thus, the required
mass function in (6) is the product of two terms, one of which is the mass
ftmction of st, given (Y,,O), and the other is the transition probability of going
from st to St,l, given 0. The normalizing constant of this mass function is the
sum ot' the numbers obtained in (7) as st runs from I 1o m.
The rest of the calculation is concerned with determining the lirst mass function
in (7). it can be determined recursively for all t starting with period !. The
objective is to lind p(s, lYt, O) and this is obtained as foilows. Assume that the
function p(s,_ I I Y,-i, O) is available. Then, repeat the following steps.
Prediction stcT: Determination of p(s, I Y,_ I, 0).
By the law of tolal probability,
p(s, I Y,_I,O)= ~ p(s, Is,_l = k,O) x p(st-I = k l Yt-i,O),
where the fact that p(stlYt-l,s,-i,O)=
p(s, Is,-i,O) has been utilized.
Ulnhm' sttT" Determination of p(stj )), 0).
of the state given information up to time t is now
By Bayes theorem, the mass function
where the nom]alizing constant is the sum or all the terms over st from 1 to m,
p(s, I Y,,O),-x p(s, I Y, ..... .,0) × .f(y, I Y,_.,O.,., ),
S. 79-97
These steps can be initialized at t = 1 by setting p(sl I Yo, O) to be the stationary
distribution of the chain (the left eigenvector corresponding to the eigenvalue
of !); the prediction step is thus not required at the start of these recursions.
Based on these results, the states can be simulated from their joint distribution
(5) in a very simple manner. (Note that if the prior on all the parameters is
proper, it is not necessary to reject a particular S;, that does not ascribe at least
one observation to each population).
First run the prediction and updale steps recursively to comoute the mass func-
tions p(stl Yt, 0). [These mass functions are obtained by defining a n × m storage
matrix, say F. Given the t- 1 row Ft-i, the next row is Ft which is proportional
to (Ftt_lP) ', dr, where dt is a row vector consisting of.f(YtiYt-i,O,.,) and ~-~ is
the eiement-by-eie~,el:~ n~ultiplication operator.] The last row of F i~, d~n used
to simulate s,,. After s, is simulated, the remaining ';rates (beginning with s,,_l)
are simulated using the probability mass function that emerges from (7). Note
that the calculation of the latter distribution requires the numbers in the tth row
of F, and those in the column of P corresponding to the simulated value of st+l.
2.2. S#mthttion o.f P
Given the states, it is rather straightl'orward to determine the full conditional
distribution of the unique element of the transition matrix P. This is because
P becomes independent of()",, I1'"
0/,), given S,,. Thus, the full ,'onditiona!
tiistriixmon of tl~e transition matrix can be derived without regard to the sampling
Suppose the ith row ot'P is denoted by p, = (pil ..... Pi,,,)', and let the prior
distribution of Pi, independently of Pi, .I ~ i, be a Dirichlet on the m-dimensional
simplex, i.e.,
Pi "" ~(:zil,...,
Then, multiplying the prior by the likelihood function of PIS,, immediately gives
the result that tire updated distribution is also Dirichlet. In particular,
p; I S,, '~ ~(~;i
+ n;l ..... :¢;I + hi,,,).
i --= I ..... m,
where n,~ is the total nunlber of om,-stt7~ transitions from state i ~o state k in the
vector S,. The vector p, (! :~. i <~ m) can now be simulated from (l 0) by letting
x i ~, Gamma(2u + n!j, ! ).
S. C'hihlJournal of Ecommwtrics 75 79 97
3. Modal estimates
An important implication of the above result on the simulation of the states
is that it can be directly used to compute the maximizer of the likelihood fianc-
tion, or the maximizer of the posterior, through the Monte Carlo EM (MCEM)
algorithm proposed by Wei and Tanner . The latter algorithm is a stochas-
tic modification of the original Dcrnpster, Laird, and Rubin EM
algorithm.
Suppose that, given the current guess of the maximizer, it is of interest to eval-
uate the E-step of the EM algorithm. In the Bayesian formulation that amounts
to an evaluation of the integral
= ./" log (rr 01 }",,, s,, )) d[S,, I Y,,, 0 1,
where the integral is a sum with integrating measure given by the mass function
in (5). As this is an intractable calculation, consider the evaluation of the Q
function by Monte Carlo. Given the current pararneter value 0 ~, one can take a
large number of draws of S,, as per the approach described above. Suppose the
draws are denoted by S,,.i, .j = !,..., N. Then the Q function can be approximated
via the average
Q( 0, 0 i ) = N - I ~--~ log( rt( 0 1 Y,,. S,,.i ) ).
In the M-step, the Q timction can be maximized over 0 to obtain ~he new
is negligible. In producing the iterate sequence {01,0 2 .... ,0 i ...} via the above
strategy, it is best to begin with a small value ot'N and then let the num-
replications
maximizer.
This procedure provides a straightforward device to locate the modal estimates
due to the fact that the Q function is additive in the respective parameters. For
example, to obtain the updated estimate of { p~l} under the Dirichlet prior (9),
each row can be treated separately of all the other rows and tile 0/,'s. From
~-~,i log(n( pklY,, S,.i), which is proportional to
j:::-:l k I::I
(m,/.i + :z/s -
!) log(px/)
+ (nk,,,i + ~,,, - I) log(l - Pill ..... p~,,-I )},
S. ChihlJournai ol Ecommu'trics 75 79-97
where nkt, j is the number of transitions from state k to state / in the simulation
S,,.j, the next iterate is given by
E ~l~=l(9~kl+tlkl,.J)
A modification of the MCEM algorithm leads to another version of the EM
algorithm . Suppose that instead of taking N draws
of S,, for each value of 0, only one draw is made. As before, the updated or
new value of 0 is found by maximizing the posterior density of 0 given (Y,,,S,,).
However, unlike the MCEM which generates a deterministic sequence of param-
eter updates, the iterates in this algorithm [bilows a aperiodic, irreducible Markov
4. Examples
4. !. Poisson fetal &tta
We begin by considering the fetal movement data analyzed in Leroux and
Puterman . The data consists of number of movements by a fiztal lamb
(obscrvcd by ultra sound) in 240 consecutive live-second intervals. The number ot'
co~tnts is modeled as a Poisson process in which the unknown rate parameter, 2,
can vary fronl one interval to the next according to a Markov chain described by
(I). In particular, given the state at time t, the count (the number ot' movew,mts)
during interval t is given by
" t'r ,~, o- }.,;
.f(Ytl/-k ) -
t = I,,.,...,9 240,
k = I,..., m.
The data used in the study is given in Fig. I.
Two models are tit to this data set, one with two components and the second
with three components. Note that it is convenient to take independent Ganama
priors on 21, due to the fact that such a distribution is conjugate to the Poisson
likelihood. Then, under the assumption that 2~ ~ ~.6'(at.b~ ), the full conditional
distribution of,:.t is
2~ I Y,,, S,,, /" ,,- ~!,' a~ + ~y,
I Is, = kl, l,t + Nt
where /[st = k] is the indicator thnction that takes the value i if s, = k and 0
otherwise, and Nt is the total nunlber of observations fi'om the kth population.
Thus, given S,, all the ,;.~'s are simulated t'rom Gamma distributions. The MCMC
S. 79 97
algorithm is then based on iterating between the simulation of S, from (7), P
from (10). and ,;.~ from ( 15 ).
For the MCEM algorithm, the updated iterate of ,:.t is also obtained quitc
Due to the fact ,--,,=S[N i log(n(;.t, lY,,,S,,, j) is proportional to
(Uk.j + ak- 1)log(;.k) - Y](bt. + Nk, i);,h,
where Ua.j = +..~,=l Y, I [s,,j = k] is the sum of the v values in state k in the ./th
draw of &,, the next iterate is obtained as
;4, = ~ (Uk, j + aa-- !)
(bk + Nk, i).
The update value of P is obtained from (13). Finally, the estimates for the SEM
algorithm are obtained by dropping the summation over .j in (16) and (17).
Consider the case of two populations. Suppose the prior parameters of ,:,k are
given by (al,bl)=(!,2) and (a2,b2)=(2, I), which specifies the belief that the
first population has the lower mean. Also suppose that in the Dirichlet prior on
P, (~li,:~12)= (3, !) and (~21,~22)=(0.5,0.5). The implied prior moments are
given in Table I. After initializing the iterations with values chosen from the
prior distribution, we rtm the MCMC a!gorithl.n in one long streanl till we have
approximate convergence. Thcn, the first 200 sampled values are discarded and
the next 6,000 are used to summarize the posterior distributions. The results arc
reported in Table I, l:ig. ! (the poslerior probability that s, = k) and Fig. 2
(posterior densities of parameters ).
Note that here and in examples later, the design of the Gibbs sampler algorithm
(the number of iterations discarded and the choice of Gibbs sample size) is
governed by inspecting the autocorrelation function of the sampled draws and
the numerical standard errors of the estimates. In most cases, the autocorrelations
in the sampled values decayed to zero by about the I Oth lag. "l-he numerical
standard errors being small are not reported, in addition, the box plots ,'eproduce
the minimum and maximum values, and the 25th, 50lh and 751h percentiles.
ML and M 79-97
......... ........ ,, ................
.............
I.,,,,,,,,..,.,,.I,..;
........................................
interval #
O IiHiittllll
ll!llllll......lllllNilUlllllllllllli,,
IlItlIIII!HI!H
interval #
FiB. I. T~o-population I~oi.~n~m mixlur¢
l'op: data )',,. middl¢: ih'(st
I I );,). boltom: Ih'(.~, ..... 2 I )',,).
FiB. 2. Po,slcrior box plots in two-population Poisson mixlur¢
TOP ;., bottom: P.
S. ChihlJournal of Ecommtetrics 75 79 97
Final five iterates in combined SEM-MCEM algorithm for Poisson two-population Markov model;
in computing (12), N = 1000
p__11 p__12 p__13 p__21 p__22 p__23 p__31 p__32 p__33
Fig. 3. Posterior box plots in thrcc-i~Ol~tdalion Poisson mixture q'op: ,;,, bottum: P.
The results indicate that obscrvations in excess ot'2 are classified as belonging
to the high mean population. For two observations that are exactly 2 (.1'23, y24),
either population is about as likely. Parameter estimates are precise and tile model
appears to be a good lit to tile data. Note that tile maximunl likelihood estimates
are taken fi'om Leroux and Puterman . They are similar to the Bayes point
estimates and seem to differ mainl) in tile case ~,~ ;.2.
Point estimates are also obtained by the Monte kar!o EM. We decided to
combine the SEM and MCEM algorithms in the following manner. First, during
the burn-in period, the SEM algorithm was employed, and then after the values
appeared to settle down, a switch was made to the MCEM algorithm. Specifically,
the SEM algorithm was used tbr the first 100 iterations, then the MCEM for the
last live iterations. The Q function in the MCEM steps was approximated using
I000 d~a~. The evolution of the iterate ,sequence in those last live iterations is
contained in Table 2. It appears that the algorithm has converge, ' to thc posterior
mode. An average of the estimates, or the final iterate values, c~l be used as thc
output of the Monte Carlo algorithm.
Rc.~ult;; are also obtained for a three-pepulafion mixt~'.r¢. Tile re.~-;u!|ing [~n~te-
rior densities are summarized in Fig. 3 while the posterior probabilities of the
S. 79 97
interval #
,' ll[I,I III I
...................
,..i,I,.....,,,
lllllllll,,iL
uillllllllhI
interval #
..............................
interval #
II . . . . . . .
, . . . . . . .
,.I . . . . . . . . . . . . . . . . . . .
, ........... ,..II .
interval #
Fig. 4. Hlrec-populati,~m Poisson mixture
Top: dala )",,. second: Pr(s, -:: I I )',,). third: Pr(st .... 211',).
I'r(s, = 31 )',, ).
populations are presented in Fig. 4. In this case only six obse,'vations appeal to
arise from the third population while the rest of the data is evenly distribu:ed
among the first two populations. We have presented the Ml estimates and the tall
Bayes results in Table 3 but have suppressed the MCEM estimates to conser~'e
space. They are all in close agreement with the lull Bayes results.
4.2..,lutorc~lressive GNI' ~hota
Now consider tl~c data set oll quarterly U.S. real GNP that has been analyzed
earlier by Hamilton and Albert and Chib using a two-population
model with a fourth-order stationary autoregression. Tlic variable of" interest is the
percentage change (multiplied by 100) in the postwar real GNP tbr the period
S. ChiblJournal o/Ecomnnetrics 75:1996) 79 97
ML and MCMC estimates for Poisson three-population Markov model.
1951.2 to 1984.4. The objective is to fit autoregressive models in which the
intercept can be drawn from one of four populations but all other parameters are
constant across the populations. This is flexible structure that can capture Markov
shifts in the level of the r~rocess. McCulloch and Tsay consider a similar
model but they restrict attention to two populations and use a Gibbs sampler in
which the population indices are not drawn jointly.
Specitically, let the conditional density of yt, given J)--i and st-i, be given
.l'(.~,lyt .... i,S,_l,~,,)= ~p(s,=ks,
...... t).l'(v, Y~-I,zq,,,
where ~ = (~l
~4), ~' = (~'l
f(y, I Y,_ ~. ~k.~,.a" ) = 4~(.v, I~, + 7~y,- ~ + .... F 7,y,_p,a-
Hence, at time t, the data is drawn t'ronl one of four Gaussian populations with
(respective) conditional mean E(yt I Yt--I,2,7,a 2) - ~¢t + 713'l-t + .... t- 7p.Vt-t,
and conditional variance that is constant across the populations. Note that this
specification differs from that used by Hamilton and Albert and Chib
 , where in the context of two populations, the conditional mean of y~
depends on realizations of the states at previous time points.
The MCMC algorithm is again quite easily implemented, provided the analysis
is conditioned on the first p observations. As before, the states and the transition
nrobabilitv matrix are simulated according ~o (8) and (il). Then, given S,,, the
other parameters, namely (2, [I, :r ~-), arc simulated fi'om distributions that are easily
derived based on results presented in Chib . In particular, under" the prior
~ .I i(~ok,A~-~)), ~- is simulated from the distribution
~k l Y,,.S,,. 7. :."
V~(Ao~oA + 'r "~z, 1 is, = k]), Vk
S. 79 97
where :, = y, - 5'1 yt-i ......
7t, Y,- v and I,% = (A0~. + Nk a-"
. Next, under
a . t t,(;.'0. F~ -I ) prior on 7 restricted to the stationary region, ,' is simulated from
the distribution
:.]Y,,,S,,,x,a-
V(Coz'o+a-"
x,(y,-x,)),
where x, =(v,_i ..
y,-p)', ~, =~k, when s,=k
and V =(F~, +a-2~
A drawing from this distribution is accepted only if all the roots of the
polynomial I -",'IL .....
;'nL t' lie outside the unit circle. Finally, under the
inverse-gamma prior .¢~/;(v0/2, ~'i0,/2), a-' is simulated fi'om
These results are applied to the GNP data set for different values of p under
weak priors on the parameters. For brevity, consider the case of a fourth-order
autoregression (p = 4). The Gibbs sampler is run for 7,000 iterations and the last
6,000 draws are used for purposes of summarizing the posterior distribution. The
results on (~,,',a 2) are presented in Table 4 while those on P are in Table 5.
The posterior moments o1"~ relative to the prior moments appears to provide
support tbr more than two populations, in addition, it is noted that the marginal
posterior distributions ot" ,'~ and 74 are quite concentrated around 0. This suggests
that an AR(2) specilicalion with I'our populations is a parsimonious description
Ibr the data.
From the posterior distribution of the elements ot" tile transition matrix it may
be noted that tl~e data is not inlormattv¢ about some elements of tl~e i,la,tlx but
that there is considerable evidence for switching between the populations.
M('M(." csmnatcs of (~,;',a:) in the AR(4) four-population Markov model
l)o,st~rior
...... 0.153
..... 11.522
....... 0.133
S. Chil~ I Journal o~ Ecommtetri~'s 75 79 97
MCMC estimates of P in AR(4) four-population Markov model
4.3. Bivarhtte Gausshm ~ktta
Next the model in the previous two sections is generalized to a three-component
mixture of multivariate normal distributions, in particular, consider a bivariate
normal distribution and let
.f contain a treatment of the work done on such models in the classical
mixture set-up.
We generate 300 observations li"om tiffs model under the Ibllowing specilica-
tions: Itl = ( I, 2 )', f12 = (3, 0 )', It3 =
(5, 4)', vec(~2t ) = ( 1.5, 0.5, 0.5, I )', vec( 02 ) =
(2.0, 0.6, 0.6, !.0 )', vec(f23 ) = (I.5, --0.5, -0.5, 2.0)'. For the transition probability
matrix, values are specified that imply some persistence in the choice or the
populations:
It should be noted that each of the components of yt satisl~, different order
relations in the mean and variance. The data used in the study is reproduced ill
S. 79 97
t IHilI ! II ilU !!imlfflJlfil
l llillIl I
l tkL JJrltmJ
Fig. 5. Ili~,m'ialc ihrcc°l~Ol'~ulation (iau~ian mi,,turc
lop: vi~, middle: t':t, boltOlll: trtlc ~.
The t'ull conditional distribution of ll~ is casi!y derived. Under a bivariate
normal prior given by lt~ '~ t ~.(lt~,Aiix I ), the updated distribution Ibr It~ is
I "~ (,'1o~ -t- N~ ~2-I )-I
.4o1~o~ -t- ~.-,) ..... I ~-:.y,l [s, =/~1
. (.4(v,. + N/,
I ) ..... I
it' ~'2~ I is given a Wishart
prior, say /1 (v.~,Do~ ), ~2 .... I is simulated
rq~/, + N,~,
Dc;X I + ~
(.1',/I,,', -/,l
:::::/~l ..... m
Tile SEM and MCEM updates are tile sample mean and sample covariance it"
the prior is fully diffuse, l"hcsc are easy to modil'y lbr the above priors. The
ffdi Bayes results tbr this model are obtained under a fairly diffuse specilication.
The resuils in Table 6 (relating to l~t), are not sensitive to the specilication of
S. ('hih/Jmtrnal ol Economelric.~" 75 (199(~) 79 97
M('M(" estimates for lO, in bivariatc (iattssian thrcc-r)opulatiorl Markov rnodel
the prior, it should be noted that the Bayes estimates are more accurate than the
SEM estimates.
The posterior probabilities in Fig. 6 are able to correctly tmcover the mem-
bership tbr most of the observations. An interesting feature is observed in the
simulation. Since there are no order relations between the population parameters
and the numbering of the states is arbitrary, we find that st = 2 corresponds to
the third population as detined above. The same feature is observed with the
SEM results. In summary, we find that the Bayes results are very accurate, and
they show clearly that even in this quite difficult problem, the MCMC approach
developed in this paper is able to learn about the component densities and the
component parameters.
5. Concluding remarks
This paper has developed a new Markov chain Monte Carlo method to es-
timate an important class o1' linite mixture distributions. For models described
by (I)(3), a approach is developed that relies, lirst, on data atlgmentation and,
second, on the simulation of the unobserved population index fi'om its joint dis-
tribution given the data and the remaining parameters. The paper also shows
the value of stochastic versions of the EM algorithm in tinding modal estimates
and includes comparisons with results obtained from the full Bayesian approach.
The ideas arc illustrated with Poisson data, bivariate Gaussian data, and an auto-
regressive time series model applied to U.S. GNP data. In all the examples, the
methods perform extremely well.
In future work, it will be of interest to consider the issue of model selection in
this setting. Recently, Carlin and Chib have developed simulation based
approaches to model selection in regression models and classical finite mixture
models. Similar results on the model selection problem in Markov mixture models
will be presented elsewhere.
S. 79 97
0 ll Jlltl Ir [l'l ill ,Jl] l. ,r II I[I JLll].[IJLlJl,rt
r'l, ]rr l l, ldf I ,II.IE
ll, l, IL[I I r,. r tl[... ,l r. 1 r, l J;
0 I,J ..... Ut] 1 II 111 k I 151 t L J l ll' J ILl I't 111 l k I]tll
Fig. 6. Three-population bivariatc Gaussian mixture ...... Top: trite s~, second: Prts, = II E,), third:
Prts, = 21 Y,, ), bottom: Pr(st ~:: 31 Y,, ).