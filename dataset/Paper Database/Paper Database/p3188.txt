Taming Transformers for High-Resolution Image Synthesis
Patrick Esser*
Robin Rombach*
Bj¨orn Ommer
Heidelberg Collaboratory for Image Processing, IWR, Heidelberg University, Germany
*Both authors contributed equally to this work
Figure 1. Our approach enables transformers to synthesize high-resolution images like this one, which contains 1280x460 pixels.
Designed to learn long-range interactions on sequential
data, transformers continue to show state-of-the-art results
on a wide variety of tasks. In contrast to CNNs, they contain
no inductive bias that prioritizes local interactions. This
makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We
demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution
images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize
transformers to efﬁciently model their composition within
high-resolution images. Our approach is readily applied
to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information,
such as segmentations, can control the generated image.
In particular, we present the ﬁrst results on semanticallyguided synthesis of megapixel images with transformers and
obtain the state of the art among autoregressive models on
class-conditional ImageNet. Code and pretrained models
can be found at 
1. Introduction
Transformers are on the rise—they are now the de-facto
standard architecture for language tasks 
and are increasingly adapted in other areas such as audio
 and vision . In contrast to the predominant vision architecture, convolutional neural networks (CNNs),
the transformer architecture contains no built-in inductive
prior on the locality of interactions and is therefore free
to learn complex relationships among its inputs. However,
this generality also implies that it has to learn all relationships, whereas CNNs have been designed to exploit prior
knowledge about strong local correlations within images.
Thus, the increased expressivity of transformers comes with
quadratically increasing computational costs, because all
pairwise interactions are taken into account. The resulting energy and time requirements of state-of-the-art transformer models thus pose fundamental problems for scaling
them to high-resolution images with millions of pixels.
Observations that transformers tend to learn convolutional structures thus beg the question: Do we have
to re-learn everything we know about the local structure
and regularity of images from scratch each time we train
a vision model, or can we efﬁciently encode inductive image biases while still retaining the ﬂexibility of transformers? We hypothesize that low-level image structure is well
described by a local connectivity, i.e. a convolutional architecture, whereas this structural assumption ceases to be
effective on higher semantic levels. Moreover, CNNs not
only exhibit a strong locality bias, but also a bias towards
spatial invariance through the use of shared weights across
 
all positions. This makes them ineffective if a more holistic
understanding of the input is required.
Our key insight to obtain an effective and expressive
model is that, taken together, convolutional and transformer
architectures can model the compositional nature of our visual world : We use a convolutional approach to efﬁciently learn a codebook of context-rich visual parts and,
subsequently, learn a model of their global compositions.
The long-range interactions within these compositions require an expressive transformer architecture to model distributions over their consituent visual parts. Furthermore, we
utilize an adversarial approach to ensure that the dictionary
of local parts captures perceptually important local structure to alleviate the need for modeling low-level statistics
with the transformer architecture. Allowing transformers to
concentrate on their unique strength—modeling long-range
relations—enables them to generate high-resolution images
as in Fig. 1, a feat which previously has been out of reach.
Our formulationgives control over the generated images by
means of conditioning information regarding desired object
classes or spatial layouts. Finally, experiments demonstrate
that our approach retains the advantages of transformers by
outperforming previous codebook-based state-of-the-art approaches based on convolutional architectures.
2. Related Work
The Transformer Family
The deﬁning characteristic of
the transformer architecture is that it models interactions between its inputs solely through attention 
which enables them to faithfully handle interactions between inputs regardless of their relative position to one another. Originally applied to language tasks, inputs to the
transformer were given by tokens, but other signals, such as
those obtained from audio or images , can be used.
Each layer of the transformer then consists of an attention
mechanism, which allows for interaction between inputs at
different positions, followed by a position-wise fully connected network, which is applied to all positions independently. More speciﬁcally, the (self-)attention mechanism
can be described by mapping an intermediate representation with three position-wise linear layers into three representations, query Q ∈RN×dk, key K ∈RN×dk and value
V ∈RN×dv, to compute the output as
Attn(Q, K, V ) = softmax
When performing autoregressive maximum-likelihood
learning, non-causal entries of QKt, i.e.
all entries below its diagonal, are set to −∞and the ﬁnal output of the
transformer is given after a linear, point-wise transformation to predict logits of the next sequence element. Since
the attention mechanism relies on the computation of inner
products between all pairs of elements in the sequence, its
computational complexity increases quadratically with the
sequence length. While the ability to consider interactions
between all elements is the reason transformers efﬁciently
learn long-range interactions, it is also the reason transformers quickly become infeasible, especially on images, where
the sequence length itself scales quadratically with the resolution. Different approaches have been proposed to reduce
the computational requirements to make transformers feasible for longer sequences. and restrict the receptive ﬁelds of the attention modules, which reduces the expressivity and, especially for high-resolution images, introduces assumptions on the independence of pixels. and
 retain the full receptive ﬁeld but can reduce costs for
a sequence of length n only from n2 to n√n, which makes
resolutions beyond 64 pixels still prohibitively expensive.
Convolutional Approaches
The two-dimensional structure of images suggests that local interactions are particularly important. CNNs exploit this structure by restricting
interactions between input variables to a local neighborhood
deﬁned by the kernel size of the convolutional kernel. Applying a kernel thus results in costs that scale linearly with
the overall sequence length (the number of pixels in the case
of images) and quadratically in the kernel size, which, in
modern CNN architectures, is often ﬁxed to a small constant
such as 3×3. This inductive bias towards local interactions
thus leads to efﬁcient computations, but the wide range of
specialized layers which are introduced into CNNs to handle different synthesis tasks suggest that
this bias is often too restrictive.
Convolutional architectures have been used for autoregressive modeling of images but, for lowresolution images, previous works demonstrated that transformers consistently outperform their convolutional counterparts.
Our approach allows us to ef-
ﬁciently model high-resolution images with transformers
while retaining their advantages over state-of-the-art convolutional approaches.
Two-Stage Approaches
Closest to ours are two-stage approaches which ﬁrst learn an encoding of data and afterwards learn, in a second stage, a probabilistic model of this
encoding. demonstrated both theoretical and empirical
evidence on the advantages of ﬁrst learning a data representation with a Variational Autoencoder (VAE) ,
and then again learning its distribution with a VAE. 
demonstrate similar gains when using an unconditional normalizing ﬂow for the second stage, and when using
a conditional normalizing ﬂow. To improve training efﬁciency of Generative Adversarial Networks (GANs), 
learns a GAN on representations of an autoencoder and
 on low-resolution wavelet coefﬁcients which are then
Figure 2. Our approach uses a convolutional VQGAN to learn a codebook of context-rich visual parts, whose composition is subsequently
modeled with an autoregressive transformer architecture. A discrete codebook provides the interface between these architectures and a
patch-based discriminator enables strong compression while retaining high perceptual quality. This method introduces the efﬁciency of
convolutional approaches to transformer based high resolution image synthesis.
decoded to images with a learned generator.
 presents the Vector Quantised Variational Autoencoder (VQVAE), an approach to learn discrete representations of images, and models their distribution autoregressively with a convolutional architecture. extends
this approach to use a hierarchy of learned representations.
However, these methods still rely on convolutional density
estimation, which makes it difﬁcult to capture long-range
interactions in high-resolution images. models images
autoregressively with transformers in order to evaluate the
suitability of generative pretraining to learn image representations for downstream tasks. Since input resolutions of
32 × 32 pixels are still quite computationally expensive ,
a VQVAE is used to encode images up to a resolution of
192 × 192. In an effort to keep the learned discrete representation as spatially invariant as possible with respect to
the pixels, a shallow VQVAE with small receptive ﬁeld is
employed. In contrast, we demonstrate that a powerful ﬁrst
stage, which captures as much context as possible in the
learned representation, is critical to enable efﬁcient highresolution image synthesis with transformers.
3. Approach
Our goal is to exploit the highly promising learning capabilities of transformer models and introduce them to
high-resolution image synthesis up to the megapixel range.
Previous work which applied transformers to image
generation demonstrated promising results for images up to
a size of 64 × 64 pixels but, due to the quadratically increasing cost in sequence length, cannot simply be scaled
to higher resolutions.
High-resolution image synthesis requires a model that
understands the global composition of images, enabling it to
generate locally realistic as well as globally consistent patterns. Therefore, instead of representing an image with pixels, we represent it as a composition of perceptually rich image constituents from a codebook. By learning an effective
code, as described in Sec. 3.1, we can signiﬁcantly reduce
the description length of compositions, which allows us to
efﬁciently model their global interrelations within images
with a transformer architecture as described in Sec. 3.2.
This approach, summarized in Fig. 2, is able to generate
realistic and consistent high resolution images both in an
unconditional and a conditional setting.
3.1. Learning an Effective Codebook of Image Constituents for Use in Transformers
To utilize the highly expressive transformer architecture for
image synthesis, we need to express the constituents of an
image in the form of a sequence. Instead of building on individual pixels, complexity necessitates an approach that uses
a discrete codebook of learned representations, such that
any image x ∈RH×W ×3 can be represented by a spatial
collection of codebook entries zq ∈Rh×w×nz, where nz is
the dimensionality of codes. An equivalent representation
is a sequence of h · w indices which specify the respective
entries in the learned codebook. To effectively learn such
a discrete spatial codebook, we propose to directly incorporate the inductive biases of CNNs and incorporate ideas
from neural discrete representation learning . First, we
learn a convolutional model consisting of an encoder E and
a decoder G, such that taken together, they learn to represent images with codes from a learned, discrete codebook
k=1 ⊂Rnz (see Fig. 2 for an overview). More
precisely, we approximate a given image x by ˆx = G(zq).
We obtain zq using the encoding ˆz = E(x) ∈Rh×w×nz
and a subsequent element-wise quantization q(·) of each
spatial code ˆzij ∈Rnz onto its closest codebook entry zk:
zq = q(ˆz) :=
∥ˆzij −zk∥
The reconstruction ˆx ≈x is then given by
ˆx = G(zq) = G (q(E(x))) .
Backpropagation through the non-differentiable quantization operation in Eq. (3) is achieved by a straight-through
gradient estimator, which simply copies the gradients from
the decoder to the encoder , such that the model and
codebook can be trained end-to-end via the loss function
LVQ(E, G, Z) = ∥x −ˆx∥2 + ∥sg[E(x)] −zq∥2
+ ∥sg[zq] −E(x)∥2
Here, Lrec = ∥x−ˆx∥2 is a reconstruction loss, sg[·] denotes
the stop-gradient operation, and ∥sg[zq]−E(x)∥2
2 is the socalled “commitment loss” .
Learning a Perceptually Rich Codebook
Using transformers to represent images as a distribution over latent image constituents requires us to push the limits of compression and learn a rich codebook. To do so, we propose VQ-
GAN, a variant of the original VQVAE, and use a discriminator and perceptual loss to keep good
perceptual quality at increased compression rate. Note that
this is in contrast to previous works which applied pixelbased and transformer-based autoregressive models on top of only a shallow quantization model. More
speciﬁcally, we replace the L2 loss used in for Lrec by
a perceptual loss and introduce an adversarial training procedure with a patch-based discriminator D that aims to
differentiate between real and reconstructed images:
LGAN({E, G, Z}, D) = [log D(x) + log(1 −D(ˆx))] (5)
The complete objective for ﬁnding the optimal compression
model Q∗= {E∗, G∗, Z∗} then reads
Q∗= arg min
LVQ(E, G, Z)
+λLGAN({E, G, Z}, D)
where we compute the adaptive weight λ according to
∇GL[LGAN] + δ
where Lrec is the perceptual reconstruction loss , ∇GL[·]
denotes the gradient of its input w.r.t. the last layer L of
the decoder, and δ = 10−6 is used for numerical stability.
To aggregate context from everywhere, we apply a single
attention layer on the lowest resolution. This training procedure signiﬁcantly reduces the sequence length when unrolling the latent code and thereby enables the application
of powerful transformer models.
3.2. Learning the Composition of Images with
Transformers
Latent Transformers
With E and G available, we can
now represent images in terms of the codebook-indices of
their encodings. More precisely, the quantized encoding of
an image x is given by zq = q(E(x)) ∈Rh×w×nz and
is equivalent to a sequence s ∈{0, . . . , |Z|−1}h×w of indices from the codebook, which is obtained by replacing
each code by its index in the codebook Z:
sij = k such that (zq)ij = zk.
By mapping indices of a sequence s back to their corresponding codebook entries, zq =
is readily recovered and decoded to an image ˆx = G(zq).
Thus, after choosing some ordering of the indices in
s, image-generation can be formulated as autoregressive
next-index prediction: Given indices s<i, the transformer
learns to predict the distribution of possible next indices,
i.e. p(si|s<i) to compute the likelihood of the full representation as p(s) = Q
i p(si|s<i). This allows us to directly
maximize the log-likelihood of the data representations:
LTransformer = Ex∼p(x) [−log p(s)] .
Conditioned Synthesis
In many image synthesis tasks a
user demands control over the generation process by providing additional information from which an example shall be
synthesized. This information, which we will call c, could
be a single label describing the overall image class or even
another image itself. The task is then to learn the likelihood
of the sequence given this information c:
p(si|s<i, c).
If the conditioning information c has spatial extent, we ﬁrst
learn another VQGAN to obtain again an index-based representation r ∈{0, . . . , |Zc|−1}hc×wc with the newly obtained codebook Zc Due to the autoregressive structure of
the transformer, we can then simply prepend r to s and
restrict the computation of the negative log-likelihood to
entries p(si|s<i, r). This “decoder-only” strategy has also
been successfully used for text-summarization tasks .
Generating
High-Resolution
mechanism of the transformer puts limits on the sequence
Figure 3. Sliding attention window.
length h · w of its inputs s. While we can adapt the number
of downsampling blocks m of our VQGAN to reduce
images of size H × W to h = H/2m × w = W/2m, we
observe degradation of the reconstruction quality beyond
a critical value of m, which depends on the considered
dataset. To generate images in the megapixel regime, we
therefore have to work patch-wise and crop images to
restrict the length of s to a maximally feasible size during
training. To sample images, we then use the transformer
in a sliding-window manner as illustrated in Fig. 3. Our
VQGAN ensures that the available context is still sufﬁcient
to faithfully model images, as long as either the statistics of
the dataset are approximately spatially invariant or spatial
conditioning information is available. In practice, this is
not a restrictive requirement, because when it is violated,
i.e. unconditional image synthesis on aligned data, we can
simply condition on image coordinates, similar to .
4. Experiments
This section evaluates the ability of our approach to retain the advantages of transformers over their convolutional
counterparts (Sec. 4.1) while integrating the effectiveness
of convolutional architectures to enable high-resolution image synthesis (Sec. 4.2). Furthermore, in Sec. 4.3, we investigate how codebook quality affects our approach. We
close the analysis by providing a quantitative comparison
to a wide range of existing approches for generative image synthesis in Sec. 4.4. Based on initial experiments, we
usually set |Z|= 1024 and train all subsequent transformer
models to predict sequences of length 16 · 16, as this is the
maximum feasible length to train a GPT2-medium architecture (307 M parameters) on a GPU with 12GB VRAM.
More details on architectures and hyperparameters can be
found in the appendix (Tab. 7 and Tab. 8).
4.1. Attention Is All You Need in the Latent Space
Transformers show state-of-the-art results on a wide variety of tasks, including autoregressive image modeling.
However, evaluations of previous works were limited to
transformers working directly on (low-resolution) pixels
 , or to deliberately shallow pixel encodings .
This raises the question if our approach retains the advantages of transformers over convolutional approaches.
To answer this question, we use a variety of conditional
and unconditional tasks and compare the performance between our transformer-based approach and a convolutional
approach. For each task, we train a VQGAN with m = 4
downsampling blocks, and, if needed, another one for the
Negative Log-Likelihood (NLL)
Transformer
P-SNAIL steps
Transformer
P-SNAIL time
PixelSNAIL
LSUN-CT / 310M
D-RIN / 180 M
S-FLCKR / 310 M
Table 1. Comparing Transformer and PixelSNAIL architectures
across different datasets and model sizes. For all settings, transformers outperform the state-of-the-art model from the PixelCNN
family, PixelSNAIL in terms of NLL. This holds both when comparing NLL at ﬁxed times (PixelSNAIL trains roughly 2 times
faster) and when trained for a ﬁxed number of steps. See Sec. 4.1
for the abbreviations.
conditioning information, and then train both a transformer
and a PixelSNAIL model on the same representations,
as the latter has been used in previous state-of-the-art twostage approaches . For a thorough comparison, we vary
the model capacities between 85M and 310M parameters
and adjust the number of layers in each model to match one
another. We observe that PixelSNAIL trains roughly twice
as fast as the transformer and thus, for a fair comparison,
report the negative log-likelihood both for the same amount
of training time (P-SNAIL time) and for the same amount of
training steps (P-SNAIL steps).
Tab. 1 reports results for unconditional image
modeling on ImageNet (IN) , Restricted ImageNet
(RIN) , consisting of a subset of animal classes from
ImageNet, LSUN Churches and Towers (LSUN-CT) ,
and for conditional image modeling of RIN conditioned on
depth maps obtained with the approach of (D-RIN) and
of landscape images collected from Flickr conditioned on
semantic layouts (S-FLCKR) obtained with the approach
Note that for the semantic layouts, we train the
ﬁrst-stage using a cross-entropy reconstruction loss due to
their discrete nature. The results shows that the transformer
consistently outperforms PixelSNAIL across all tasks when
trained for the same amount of time and the gap increases
even further when trained for the same number of steps.
These results demonstrate that gains of transformers carry
over to our proposed two-stage setting.
4.2. A Uniﬁed Model for Image Synthesis Tasks
The versatility and generality of the transformer architecture makes it a promising candidate for image synthesis. In
the conditional case, additional information c such as class
labels or segmentation maps are used and the goal is to learn
the distribution of images as described in Eq. (10). Using
the same setting as in Sec. 4.1 (i.e. image size 256 × 256,
latent size 16 × 16), we perform various conditional image
synthesis experiments:
conditioning
Figure 4. Transformers within our setting unify a wide range of
image synthesis tasks.
We show 256 × 256 synthesis results
across different conditioning inputs and datasets, all obtained with
the same approach to exploit inductive biases of effective CNN
based VQGAN architectures in combination with the expressivity of transformer architectures. Top row: Completions from unconditional training on ImageNet. 2nd row: Depth-to-Image on
RIN. 3rd row: Semantically guided synthesis on ADE20K. 4th
row: Pose-guided person generation on DeepFashion.
row: Class-conditional samples on RIN.
(i): Semantic image synthesis, where we condition on
semantic segmentation masks of ADE20K , a webscraped landscapes dataset (S-FLCKR) and COCO-Stuff
 . Results are depicted in Figure 4, 5 and Fig. 6.
(ii): Structure-to-image, where we use either depth or edge
information to synthesize images from both RIN and IN
(see Sec. 4.1). The resulting depth-to-image and edge-toimage translations are visualized in Fig. 4 and Fig. 6.
(iii): Pose-guided synthesis: Instead of using the semantically rich information of either segmentation or depth maps,
Fig. 4 shows that the same approach as for the previous experiments can be used to build a shape-conditional generative model on the DeepFashion dataset.
(iv): Stochastic superresolution, where low-resolution images serve as the conditioning information and are thereby
upsampled. We train our model for an upsampling factor of
8 on ImageNet and show results in Fig. 6.
(v): Class-conditional image synthesis: Here, the conditioning information c is a single index describing the class
label of interest. Results for the RIN and IN dataset are
demonstrated in Fig. 4 and Fig. 8, respectively.
All of these examples make use of the same methodology.
Instead of requiring task speciﬁc architectures or modules,
the ﬂexibility of the transformer allows us to learn appropriate interactions for each task, while the VQGAN — which
can be reused across different tasks — leads to short sequence lengths. In combination, the presented approach can
be understood as an efﬁcient, general purpose mechanism
for conditional image synthesis. Note that additional results
for each experiment can be found in the appendix, Sec. D.
High-Resolution Synthesis
The sliding window approach introduced in Sec. 3.2 enables image synthesis beyond a resolution of 256 × 256 pixels. We evaluate this
approach on unconditional image generation on LSUN-CT
and FacesHQ (see Sec. 4.3) and conditional synthesis on D-
RIN, COCO-Stuff and S-FLCKR, where we show results
in Fig. 1, 6 and the supplementary (Fig. 29-39). Note that
this approach can in principle be used to generate images
of arbitrary ratio and size, given that the image statistics
of the dataset of interest are approximately spatially invariant or spatial information is available. Impressive results
can be achieved by applying this method to image generation from semantic layouts on S-FLCKR, where a strong
VQGAN can be learned with m = 5, so that its codebook together with the conditioning information provides
the transformer with enough context for image generation
in the megapixel regime.
4.3. Building Context-Rich Vocabularies
How important are context-rich vocabularies?
To investigate this question, we ran experiments where the transformer architecture is kept ﬁxed while the amount of context encoded into the representation of the ﬁrst stage is varied through the number of downsampling blocks of our VQ-
GAN. We specify the amount of context encoded in terms
of reduction factor in the side-length between image inputs and the resulting representations, i.e. a ﬁrst stage encoding images of size H × W into discrete codes of size
H/f × W/f is denoted by a factor f. For f = 1, we reproduce the approach of and replace our VQGAN by a
k-means clustering of RGB values with k = 512.
During training, we always crop images to obtain inputs of
size 16 × 16 for the transformer, i.e. when modeling images with a factor f in the ﬁrst stage, we use crops of size
16f × 16f. To sample from the models, we always apply
them in a sliding window manner as described in Sec. 3.
Results Fig. 7 shows results for unconditional synthesis of
faces on FacesHQ, the combination of CelebA-HQ and
Figure 5. Samples generated from semantic layouts on S-FLCKR.
Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 ×
240 pixels. Best viewed zoomed in. A larger visualization can be
found in the appendix, see Fig 29.
FFHQ . It clearly demonstrates the beneﬁts of powerful VQGANs by increasing the effective receptive ﬁeld of
the transformer. For small receptive ﬁelds, or equivalently
small f, the model cannot capture coherent structures. For
an intermediate value of f = 8, the overall structure of
images can be approximated, but inconsistencies of facial
features such as a half-bearded face and of viewpoints in
different parts of the image arise. Only our full setting of
f = 16 can synthesize high-ﬁdelity samples. For analogous
results in the conditional setting on S-FLCKR, we refer to
the appendix (Fig. 13 and Sec. C).
To assess the effectiveness of our approach quantitatively,
we compare results between training a transformer directly
on pixels, and training it on top of a VQGAN’s latent code
with f = 2, given a ﬁxed computational budget. Again, we
follow and learn a dictionary of 512 RGB values on CI-
FAR10 to operate directly on pixel space and train the same
transformer architecture on top of our VQGAN with a latent
code of size 16 × 16 = 256. We observe improvements of
18.63% for FIDs and 14.08× faster sampling of images.
Figure 6. Applying the sliding attention window approach (Fig. 3)
to various conditional image synthesis tasks. Top: Depth-to-image
on RIN, 2nd row: Stochastic superresolution on IN, 3rd and 4th
row: Semantic synthesis on S-FLCKR, bottom: Edge-guided synthesis on IN. The resulting images vary between 368 × 496 and
1024 × 576, hence they are best viewed zoomed in.
SPADE 
Pix2PixHD (+aug) 
COCO-Stuff
22.6/23.9(*)
111.5 (54.2)
33.9/35.7(*)
81.8 (41.5)
Table 2. FID score comparison for semantic image synthesis
(256 × 256 pixels). (*): Recalculated with our evaluation protocol
based on on the validation splits of each dataset.
4.4. Benchmarking Image Synthesis Results
In this section we investigate how our approach quantitatively compares to existing models for generative image
synthesis. In particular, we assess the performance of our
model in terms of FID and compare to a variety of established models (GANs, VAEs, Flows, AR, Hybrid).
results on semantic synthesis are shown in Tab. 2, where
we compare to , and the results on unconditional face synthesis are shown in Tab. 3. While some
task-specialized GAN models report better FID scores, our
approach provides a uniﬁed model that works well across
a wide range of tasks while retaining the ability to encode
and reconstruct images. It thereby bridges the gap between
purely adversarial and likelihood-based approaches.
downsampling factor
Figure 7. Evaluating the importance of effective codebook for HQ-Faces (CelebA-HQ and FFHQ) for a ﬁxed sequence length |s|= 16·16 =
256. Globally consistent structures can only be modeled with a context-rich vocabulary (right). All samples are generated with temperature
t = 1.0 and top-k sampling with k = 100. Last row reports the speedup over the f1 baseline which operates directly on pixels and takes
7258 seconds to produce a sample on a NVIDIA GeForce GTX Titan X.
CelebA-HQ 256 × 256
FFHQ 256 × 256
VDVAE (t = 0.7) 
VDVAE (t = 1.0)
PIONEER (B.) 
39.2 (25.3)
VDVAE (t = 0.8)
NCPVAE 
VDVAE (t = 0.9)
VAEBM 
VQGAN+P.SNAIL
Style ALAE 
DC-VAE 
ours (k=300)
ours (k=400)
U-Net GAN (+aug) 
10.9 (7.6)
PGGAN 
StyleGAN2 (+aug) 
Table 3. FID score comparison for face image synthesis. CelebA-
HQ results reproduced from , FFHQ from .
Autoregressive models are typically sampled with a decoding strategy such as beam-search, top-k or nucleus
sampling. For most of our results, including those in Tab. 2,
we use top-k sampling with k = 100 unless stated otherwise. For the results on face synthesis in Tab. 3, we computed scores for k ∈{100, 200, 300, 400, 500} and report
the best results, obtained with k = 400 for CelebA-HQ and
k = 300 for FFHQ. Fig. 10 in the supplementary shows
FID and Inception scores as a function of k.
Class-Conditional Synthesis on ImageNet
To address a
direct comparison with the previous state-of-the-art for autoregressive modeling of class-conditional image synthesis
on ImageNet, VQVAE-2 , we train a class-conditional
ImageNet transformer on 256 × 256 images, using a VQ-
GAN with dim Z = 16384 and f = 16, and additionally compare to BigGAN , IDDPM , DCTransformer
 and ADM in Tab. 4. Note that our model uses
≃10× less parameters than VQVAE-2, which has an estimated parameter count of 13.5B (estimate based on ).
Samples of this model for different ImageNet classes are
shown in Fig. 8. We observe that the adversarial training
of the corresponding VQGAN enables sampling of highquality images with realistic textures, of comparable or
higher quality than existing approaches such as BigGAN
and VQVAE-2, see also Fig. 14-17 in the supplementary.
acceptance rate
mixed k, p = 1.0
70.6 ± 1.8
k = 973, p = 1.0
47.3 ± 1.3
k = 250, p = 1.0
78.6 ± 1.1
k = 973, p = 0.88
74.3 ± 1.8
k = 600, p = 1.0
280.3 ± 5.5
mixed k, p = 1.0
125.5 ± 2.4
mixed k, p = 1.0
188.6 ± 3.3
mixed k, p = 1.0
304.8 ± 3.6
mixed k, p = 1.0
402.7 ± 2.9
DCTransformer 
VQVAE-2 
BigGAN 
168.6 ± 2.5
BigGAN-deep
203.6 ± 2.6
IDDPM 
ADM-G, no guid. 
ADM-G, 1.0 guid.
ADM-G, 10.0 guid.
234.0 ± 3.9
FID score comparison for class-conditional synthesis
on 256 × 256 ImageNet, evaluated between 50k samples and the
training split. Classiﬁer-based rejection sampling as in VQVAE-2
uses a ResNet-101 classiﬁer. BigGAN(-deep) evaluated via
 truncated at 1.0. “Mixed”
k refers to samples generated with different top-k values, here k ∈
{100, 200, 250, 300, 350, 400, 500, 600, 800, 973}.
Quantitative results are summarized in Tab. 4. We report
FID and Inception Scores for the best k/p in top-k/top-p
sampling. Following , we can further increase quality
via classiﬁer-rejection, which keeps only the best m-outof-n samples in terms of the classiﬁer’s score, i.e. with an
acceptance rate of m/n. We use a ResNet-101 classiﬁer .
We observe that our model outperforms other autoregressive approaches (VQVAE-2, DCTransformer) in terms of
FID and IS, surpasses BigGAN and IDDPM even for low
rejection rates and yields scores close to the state of the art
for higher rejection rates, see also Fig. 9.
How good is the VQGAN?
Reconstruction FIDs obtained
via the codebook provide an estimate on the achievable FID
of the generative model trained on it. To quantify the per-
Figure 8. Samples from our class-conditional ImageNet model trained on 256 × 256 images.
Figure 9. FID and Inception Score as a function of top-k, nucleus and rejection ﬁltering.
Codebook Size
64 × 64 & 32 × 32
DALL-E 
64 × 64 & 32 × 32
Table 5. FID on ImageNet between reconstructed validation split
and original validation (FID/val) and training (FID/train) splits.
∗trained with Gumbel-Softmax reparameterization as in .
formance gains of our VQGAN over discrete VAEs trained
without perceptual and adversarial losses (e.g. VQVAE-2,
DALL-E ), we evaluate this metric on ImageNet and
report results in Tab. 5.
Our VQGAN outperforms nonadversarial models while providing signiﬁcantly more compression (seq. length of 256 vs. 5120 = 322 + 642 for
VQVAE-2, 256 vs 1024 for DALL-E). As expected, larger
versions of VQGAN (either in terms of larger codebook
sizes or increased code lengths) further improve performance. Using the same hierarchical codebook setting as in
VQVAE-2 with our model provides the best reconstruction
FID, albeit at the cost of a very long and thus impractical
sequence. The qualitative comparison corresponding to the
results in Tab. 5 can be found in Fig. 12.
5. Conclusion
This paper adressed the fundamental challenges that previously conﬁned transformers to low-resolution images. We
proposed an approach which represents images as a composition of perceptually rich image constituents and thereby
overcomes the infeasible quadratic complexity when modeling images directly in pixel space. Modeling constituents
with a CNN architecture and their compositions with a
transformer architecture taps into the full potential of their
complementary strengths and thereby allowed us to represent the ﬁrst results on high-resolution image synthesis
with a transformer-based architecture. In experiments, our
approach demonstrates the efﬁciency of convolutional inductive biases and the expressivity of transformers by synthesizing images in the megapixel range and outperforming
state-of-the-art convolutional approaches. Equipped with a
general mechanism for conditional synthesis, it offers many
opportunities for novel neural rendering approaches.
This work has been supported by the German Research Foundation
(DFG) projects 371923335, 421703927 and a hardware donation from
NVIDIA corporation.
Taming Transformers for High-Resolution
Image Synthesis
Supplementary Material
The supplementary material for our work Taming Transformers for High-Resolution Image Synthesis is structured as follows:
First, Sec. A summarizes changes to a previous version of this paper. In Sec. B, we present hyperparameters and architectures
which were used to train our models. Next, extending the discussion of Sec. 4.3, Sec. C presents additional evidence for the
importance of perceptually rich codebooks and its interpretation as a trade-off between reconstruction ﬁdelity and sampling
capability. Additional results on high-resolution image synthesis for a wide range of tasks are then presented in Sec. D, and
Sec. E shows nearest neighbors of samples. Finally, Sec. F contains results regarding the ordering of image representations.
A. Changelog
We summarize changes between this version 1 of the paper and its previous version 2.
In the previous version, Eq. (4) had a weighting term β on the commitment loss, and Tab. 8 reported a value of β = 0.25
for all models. However, due to a bug in the implementation, β was never used and all models have been trained with β = 1.0.
Thus, we removed β in Eq. (4).
We updated class-conditional synthesis results on ImageNet in Sec. 4.4. The previous results, included here in Tab. 6
for completeness, were based on a slightly different implementation where the transformer did not predict the distribution
of the ﬁrst token but used a histogram for it. The new model has been trained for 2.4 million steps with a batch size of
16 accumulated over 8 batches, which took 45.8 days on a single A100 GPU. The previous model had been trained for
1.0 million steps. Furthermore, the FID values were based on 50k (18k) samples against 50k (18k) training examples (to
compare with MSP). For better comparison with other works, the current version reports FIDs based on 50k samples against
all training examples of ImageNet using torch-fidelity . We updated all qualitative ﬁgures showing samples from
this model and added visualizations of the effect of tuning top-k/p or rejection rate in Fig. 14-26.
To provide a better overview, we also include results from works that became available after the previous version of our
work. Speciﬁcally, we include results on reconstruction quality of the VQVAE from in Tab. 5 and Fig. 12 (which replaces
the previous qualitative comparison), and results on class-conditional ImageNet sampling from in Tab. 4. Note
that with the exception of BigGAN and BigGAN-deep , no models or sampling results are available for the methods we
compare to in Tab. 4. Thus, we can only report the numbers from the respective papers but cannot re-evaluate them with the
same code. We follow the common evaluation protocol for class-conditional ImageNet synthesis from and evaluate 50k
samples from the model against the whole training split of ImageNet. However, it is not clear how different implementations
resize the training images. In our code, we use the largest center-crop and resize it bilinearly with anti-aliasing to 256 × 256
using Pillow . FID and Inception Scores are then computed with torch-fidelity .
We updated face-synthesis results in Tab. 3 based on a slightly different implementation as in the case of class-conditional
ImageNet results and improve the previous results slightly. In addition, we evaluate the ability of our NLL-based training to
detect overﬁtting. We train larger models (FFHQ (big) and CelebA-HQ (big) in Tab. 8) on the face datasets, and show nearest
neighbors of samples obtained from checkpoints with the best NLL on the validation split and the training split in Sec. E. We
also added Fig. 10, which visualizes the effect of tuning k in top-k sampling on FID and IS.
B. Implementation Details
The hyperparameters for all experiments presented in the main paper and supplementary material can be found in Tab. 8.
Except for the c-IN (big), COCO-Stuff and ADE20K models, these hyperparameters are set such that each transformer model
can be trained with a batch-size of at least 2 on a GPU with 12GB VRAM, but we generally train on 2-4 GPUs with an
accumulated VRAM of 48 GB. If hardware permits, 16-bit precision training is enabled.
1 
2 
ours-previous (+R)
BigGAN (-deep)
IN 256, 50K
19.8 (11.2)
IN 256, 18K
ours-previous
CelebA-HQ 256
Results from a previous version of this paper, see also Sec. A. Left: Previous results on class-conditional ImageNet synthesis
with a slightly different implementation and evaluated against 50k and 18k training examples instead of the whole training split. See Tab. 4
for new, improved results evaluated against the whole training split. Right: Previous results on face-synthesis with a slightly different
implementation compared to the new implementation. See also Tab. 3 for comparison with other methods.
Figure 10. FID and Inception Score as a function of top-k for CelebA-HQ (left) and FFHQ (right).
x ∈RH×W ×C
zq ∈Rh×w×nz
Conv2D →RH×W ×C′
Conv2D →Rh×w×C′′
m× { Residual Block, Downsample Block} →Rh×w×C′′
Residual Block →Rh×w×C′′
Residual Block →Rh×w×C′′
Non-Local Block →Rh×w×C′′
Non-Local Block →Rh×w×C′′
Residual Block →Rh×w×C′′
Residual Block →Rh×w×C′′
m× { Residual Block, Upsample Block} →RH×W ×C′
GroupNorm, Swish, Conv2D →Rh×w×nz
GroupNorm, Swish, Conv2D →RH×W ×C
Table 7. High-level architecture of the encoder and decoder of our VQGAN. The design of the networks follows the architecture presented
in with no skip-connections. For the discriminator, we use a patch-based model as in . Note that h =
2m and f = 2m.
VQGAN Architecture
The architecture of our convolutional encoder and decoder models used in the VQGAN experiments
is described in Tab. 7. Note that we adopt the compression rate by tuning the number of downsampling steps m. Further note
that λ in Eq. 5 is set to zero in an initial warm-up phase. Empirically, we found that longer warm-ups generally lead to better
reconstructions. As a rule of thumb, we recommend setting λ = 0 for at least one epoch.
Transformer Architecture
Our transformer model is identical to the GPT2 architecture and we vary its capacity
mainly through varying the amount of layers (see Tab. 8). Furthermore, we generally produce samples with a temperature
t = 1.0 and a top-k cutoff at k = 100 (with higher top-k values for larger codebooks).
C. On Context-Rich Vocabularies
Sec. 4.3 investigated the effect of the downsampling factor f used for encoding images. As demonstrated in Fig. 7, large
factors are crucial for our approach, since they enable the transformer to model long-range interactions efﬁciently. However,
since larger f correspond to larger compression rates, the reconstruction quality of the VQGAN starts to decrease after a
certain point, which is analyzed in Fig. 11. The left part shows the reconstruction error (measured by LPIPS ) versus the
negative log-likelihood obtained by the transformer for values of f ranging from 1 to 64. The latter provides a measure of the
ability to model the distribution of the image representation, which increases with f. The reconstruction error on the other
hand decreases with f and the qualitative results on the right part show that beyond a critical value of f, in this case f = 16,
reconstruction errors become severe. At this point, even when the image representations are modeled faithfully, as suggested
by a low negative log-likelihood, sampled images are of low-ﬁdelity, because the reconstruction capabilities provide an upper
bound on the quality that can be achieved.
Hence, Fig. 11 shows that we must learn perceptually rich encodings, i.e. encodings with a large f and perceptually faithful
reconstructions. This is the goal of our VQGAN and Fig. 12 compares its reconstruction capabilities against the VQVAE 
Experiment
# params [M]
c-IN (big)
S-FLCKR, f = 4
S-FLCKR, f = 16
S-FLCKR, f = 32
(FacesHQ, f = 1)∗
FacesHQ, f = 2
FacesHQ, f = 4
FacesHQ, f = 8
FacesHQ∗∗, f = 16
FFHQ∗∗, f = 16
CelebA-HQ∗∗, f = 16
FFHQ (big)
CelebA-HQ (big)
COCO-Stuff
DeepFashion
Table 8. Hyperparameters. For every experiment, we set the number of attention heads in the transformer to nh = 16. nlayer denotes the
number of transformer blocks, # params the number of transformer parameters, nz the dimensionality of codebook entries, |Z| the number
of codebook entries, dropout the dropout rate for training the transformer, length(s) the total length of the sequence, ne the embedding
dimensionality and m the number of downsampling steps in the VQGAN. D-RINv1 is the experiment which compares to Pixel-SNAIL in
Sec. 4.1. Note that the experiment (FacesHQ, f = 1)∗does not use a learned VQGAN but a ﬁxed k-means clustering algorithm as in 
with K = 512 centroids. A preﬁx “c” refers to a class-conditional model. The models marked with a ‘∗∗‘ are trained on the same VQGAN.
used in DALL-E . We observe that for f = 8 and 8192 codebook entries, both the VQVAE and VQGAN capture the
global structure faithfully. However, the textures produced by the VQVAE are blurry, whereas those of the VQGAN are crisp
and realistic looking (e.g. the stone texture and the fur and tail of the squirrel). When we increase the compression rate of the
VQGAN further to f = 16, we see that some reconstructed parts are not perfectly aligned with the input anymore (e.g. the
paw of the squirrel), but, especially with slightly larger codebooks, the reconstructions still look realistic. This demonstrates
how the VQGAN provides high-ﬁdelity reconstructions at large factors, and thereby enables efﬁcient high-resolution image
synthesis with transformers.
To illustrate how the choice of f depends on the dataset, Fig. 13 presents results on S-FLCKR. In the left part, it shows,
analogous to Fig. 7, how the quality of samples increases with increasing f. However, in the right part, it shows that
reconstructions remain faithful perceptually faithful even for f32, which is in contrast to the corresponding results on faces
in Fig. 11. These results might be explained by a higher perceptual sensitivity to facial features as compared to textures, and
allow us to generate high-resolution landscapes even more efﬁciently with f = 32.
D. Additional Results
Qualitative Comparisons
The qualitative comparison corresponding to Tab. 4 and Tab. 6 can be found in Fig. 14, 15, 16
and 17. Since no models are available for VQVAE-2 and MSP, we extracted results directly from the supplementary3 and
3 
from the provided samples4, respectively. For BigGAN, we produced the samples via the provided model5. Similarly, the
qualitative comparison with the best competitor model (SPADE) for semantic synthesis on standard benchmarks (see Tab. 2)
can be found in Fig. 40 (ADE20K) and Fig. 41 (COCO-Stuff)6.
Comparison to Image-GPT
To further evaluate the effectiveness of our approach, we compare to the state-of-the-art
generative transformer model on images, ImageGPT . By using immense amounts of compute the authors demonstrated
that transformer models can be applied to the pixel-representation of images and thereby achieved impressive results both in
representation learning and image synthesis. However, as their approach is conﬁned to pixel-space, it does not scale beyond
a resolution of 192 × 192. As our approach leverages a strong compression method to obtain context-rich representations
of images and then learns a transformer model, we can synthesize images of much higher resolution. We compare both
approaches in Fig. 27 and Fig. 28, where completions of images are depicted. Both plots show that our approach is able
to synthesize consistent completions of dramatically increased ﬁdelity. The results of are obtained from https://
openai.com/blog/image-gpt/.
Additional High-Resolution Results
Fig. 29, 30, 31 and Fig. 32 contain additional HR results on the S-FLCKR dataset
for both f = 16 (m = 4) and f = 32 (m = 5) (semantically guided). In particular, we provide an enlarged version of Fig. 5
from the main text, which had to be scaled down due to space constraints. Additionally, we use our sliding window approach
(see Sec. 3) to produce high-resolution samples for the depth-to-image setting on RIN in Fig. 33 and Fig. 34, edge-to-image
on IN in Fig. 35, stochastic superresolution on IN in Fig. 36, more examples on semantically guided landscape synthesis
on S-FLCKR in Fig. 37 with f = 16 and in Fig. 38 with f = 32, and unconditional image generation on LSUN-CT (see
Sec. 4.1) in Fig. 39. Moreover, for images of size 256 × 256, we provide results for generation from semantic layout on
(i) ADE20K in Fig. 40 and (ii) COCO-Stuff in Fig. 41, depth-to-image on IN in Fig. 42, pose-guided person generation in
Fig. 43 and class-conditional synthesis on RIN in Fig. 44.
E. Nearest Neighbors of Samples
One advantage of likelihood-based generative models over, e.g., GANs is the ability to evaluate NLL on training data and
validation data to detect overﬁtting. To test this, we trained large models for face synthesis, which can easily overﬁt them,
and retained two checkpoints on each dataset: One for the best validation NLL (at the 10th and 13th epoch for FFHQ and
CelebA-HQ, respectively), and another for the best training NLL (at epoch 1000). We then produced samples from both
checkpoints and retrieved nearest neighbors from the training data based on the LPIPS similarity metric . The results
are shown in Fig. 45, where it can be observed that the checkpoints with best training NLL (best train NLL) reproduce the
training examples, whereas samples from the checkpoints with best validation NLL (best val. NLL) depict new faces which
are not found in the training data.
Based on these results, we can conclude that early-stopping based on validation NLL can prevent overﬁtting. Furthermore,
the bottleneck for our approach on face synthesis is given by the dataset size since it has the capacity to almost perfectly ﬁt
the training data. Unfortunately, FID scores cannot detect such an overﬁtting. Indeed, the best train NLL checkpoints achieve
FID scores of 3.86 on CelebA-HQ and 2.68 on FFHQ, compared to 10.2 and 9.6 for the best val. NLL checkpoints. While
validation NLL provides a way to detect overﬁtting for likelihood-based models, it is not clear if early-stopping based on it
is optimal if one is mainly interested in the quality of samples. To address this and the evaluation of GANs, new metrics will
be required which can differentiate between models that produce new, high-quality samples and those that simply reproduce
the training data.
Our class-conditional ImageNet model does not display overﬁtting according to validation NLL, and the nearest neighbors
shown in Fig. 46 also provide evidence that the model produces new, high-quality samples.
F. On the Ordering of Image Representations
For the “classical” domain of transformer models, NLP, the order of tokens is deﬁned by the language at hand. For images
and their discrete representations, in contrast, it is not clear which linear ordering to use. In particular, our sliding-window
approach depends on a row-major ordering and we thus investigate the performance of the following ﬁve different permutations of the input sequence of codebook indices: (i) row major, or raster scan order, where the image representation is
4 
5 
6samples were reproduced with the authors’ ofﬁcial implementation available at 
unrolled from top left to bottom right. (ii) spiral out, which incorporates the prior assumption that most images show a
centered object. (iii) z-curve, also known as z-order or morton curve, which introduces the prior of preserved locality when
mapping a 2D image representation onto a 1D sequence. (iv) subsample, where preﬁxes correspond to subsampled representations, see also . (v) alternate, which is related to row major, but alternates the direction of unrolling every row. (vi)
spiral in, a reversed version of spiral out which provides the most context for predicting the center of the image. A graphical
visualization of these permutation variants is shown in Fig. 47. Given a VQGAN trained on ImageNet, we train a transformer
for each permutation in a controlled setting, i.e. we ﬁx initialization and computational budget.
Fig.47 depicts the evolution of negative log-likelihood for each variant as a function of training iterations, with
ﬁnal values given by (i) 4.767, (ii) 4.889, (iii) 4.810, (iv) 5.015, (v) 4.812, (vi) 4.901. Interestingly, row major performs best
in terms of this metric, whereas the more hierarchical subsample prior does not induce any helpful bias. We also include
qualitative samples in Fig. 48 and observe that the two worst performing models in terms of NLL (subsample and spiral in)
tend to produce more textural samples, while the other variants synthesize samples with much more recognizable structures.
Overall, we can conclude that the autoregressive codebook modeling is not permutation-invariant, but the common row major
ordering outperforms other orderings.
reconstruction error
negative log-likelihood
high-ﬁdelity
low-ﬁdelity
reconstruction
rec. error
0.11 ± 0.02
0.20 ± 0.03
0.23 ± 0.04
0.38 ± 0.07
0.63 ± 0.08
0.66 ± 0.11
5.66 · 104
1.29 · 104
4.10 · 103
2.32 · 103
2.28 · 102
6.75 · 101
Figure 11. Trade-off between negative log-likelihood (nll) and reconstruction error. While context-rich encodings obtained with large
factors f allow the transformer to effectively model long-range interactions, the reconstructions capabilities and hence quality of samples
suffer after a critical value (here, f = 16). For more details, see Sec. C.
Figure 12. Comparing reconstruction capabilities between VQVAEs and VQGANs. Numbers in parentheses denote compression factor and
codebook size. With the same compression factor and codebook size, VQGANs produce more realistic reconstructions compared to blurry
reconstructions of VQVAEs. This enables increased compression rates for VQGAN while retaining realistic reconstructions. See Sec. C.
reconstructions
Figure 13. Samples on landscape dataset (left) obtained with different factors f, analogous to Fig. 7. In contrast to faces, a factor of f = 32
still allows for faithful reconstructions (right). See also Sec. C.
VQVAE-2 
BigGAN 
Figure 14. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 28: spotted
salamander (top) and 97: drake (bottom). We report class labels as in VQVAE-2 .
VQVAE-2 
BigGAN 
Figure 15. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 108: sea
anemone (top) and 141: redshank (bottom). We report class labels as in VQVAE-2 .
VQVAE-2 
BigGAN 
Figure 16. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 11: goldﬁnch
(top) and 22: bald eagle (bottom).
VQVAE-2 
BigGAN 
Figure 17. Qualitative assessment of various models for class-conditional image synthesis on ImageNet. Depicted classes: 0: tench (top)
and 9: ostrich (bottom).
933: cheeseburger
acc. rate 1.0
acc. rate 0.5
acc. rate 0.1
992: agaric
acc. rate 1.0
acc. rate 0.5
acc. rate 0.1
200: tibetian terrier
acc. rate 1.0
acc. rate 0.5
acc. rate 0.1
Figure 18.
Visualizing the effect of increased rejection rate (i.e. lower acceptance rate) by using a ResNet-101 classiﬁer trained on
ImageNet and samples from our class-conditional ImageNet model. Higher rejection rates tend to produce images showing more central,
recognizable objects compared to the unguided samples. Here, k = 973, p = 1.0 are ﬁxed for all samples. Note that k = 973 is the
effective size of the VQGAN’s codebook, i.e. it describes how many entries of the codebook with dim Z = 16384 are actually used.
933: cheeseburger
992: agaric
200: tibetian terrier
Figure 19.
Visualizing the effect of varying k in top-k sampling (i.e. truncating the probability distribution per image token) by using
a ResNet-101 classiﬁer trained on ImageNet and samples from our class-conditional ImageNet model. Lower values of k produce more
uniform, low-entropic images compared to samples obtained with full k. Here, an acceptance rate of 1.0 and p = 1.0 are ﬁxed for all
samples. Note that k = 973 is the effective size of the VQGAN’s codebook, i.e. it describes how many entries of the codebook with
dim Z = 16384 are actually used.
933: cheeseburger
992: agaric
200: tibetian terrier
Figure 20. Visualizing the effect of varying p in top-p sampling (or nucleus sampling ) by using a ResNet-101 classiﬁer trained on
ImageNet and samples from our class-conditional ImageNet model. Lowering p has similar effects as decreasing k, see Fig. 19. Here, an
acceptance rate of 1.0 and k = 973 are ﬁxed for all samples.
Figure 21.
Random samples on 256 × 256 class-conditional ImageNet with k ∈ ,
p = 1.0, acceptance rate 1.0. FID: 17.04, IS: 70.6 ± 1.8. Please see for an uncompressed version.
Figure 22.
Random samples on 256 × 256 class-conditional ImageNet with k = 600, p = 1.0, acceptance rate 0.05. FID: 5.20, IS:
280.3 ± 5.5. Please see for an uncompressed version.
Figure 23.
Random samples on 256 × 256 class-conditional ImageNet with k = 250, p = 1.0, acceptance rate 1.0. FID: 15.98, IS:
78.6 ± 1.1. Please see for an uncompressed version.
Figure 24.
Random samples on 256 × 256 class-conditional ImageNet with k = 973, p = 0.88, acceptance rate 1.0. FID: 15.78, IS:
74.3 ± 1.8. Please see for an uncompressed version.
Figure 25.
Random samples on 256 × 256 class-conditional ImageNet with k ∈ ,
p = 1.0, acceptance rate 0.005. FID: 6.59, IS: 402.7 ± 2.9. Please see for an uncompressed version.
Figure 26.
Random samples on 256 × 256 class-conditional ImageNet with k ∈ ,
p = 1.0, acceptance rate 0.05. FID: 5.88, IS: 304.8 ± 3.6. Please see for an uncompressed version.
conditioning
ours (top) vs iGPT (bottom)
Figure 27. Comparing our approach with the pixel-based approach of . Here, we use our f = 16 S-FLCKR model to obtain high-ﬁdelity
image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and
three of (bottom).
conditioning
ours (top) vs iGPT (bottom)
Figure 28. Comparing our approach with the pixel-based approach of . Here, we use our f = 16 S-FLCKR model to obtain high-ﬁdelity
image completions of the inputs depicted on the left (half completions). For each conditioning, we show three of our samples (top) and
three of (bottom).
Figure 29. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 × 240
Figure 30. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1536 × 512, 1840 × 1024, and 1536 × 620
Figure 31. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 2048 × 512, 1460 × 440, 2032 × 448 and
2016 × 672 pixels.
Figure 32. Samples generated from semantic layouts on S-FLCKR. Sizes from top-to-bottom: 1280 × 832, 1024 × 416 and 1280 × 240
conditioning
Figure 33. Depth-guided neural rendering on RIN with f = 16 using the sliding attention window.
conditioning
Figure 34. Depth-guided neural rendering on RIN with f = 16 using the sliding attention window.
conditioning
Figure 35. Intentionally limiting the receptive ﬁeld can lead to interesting creative applications like this one: Edge-to-Image synthesis on
IN with f = 8, using the sliding attention window.
conditioning
Figure 36. Additional results for stochastic superresolution with an f = 16 model on IN, using the sliding attention window.
conditioning
Figure 37. Samples generated from semantic layouts on S-FLCKR with f = 16, using the sliding attention window.
conditioning
Figure 38. Samples generated from semantic layouts on S-FLCKR with f = 32, using the sliding attention window.
Figure 39. Unconditional samples from a model trained on LSUN Churches & Towers, using the sliding attention window.
conditioning
ground truth
SPADE 
Figure 40. Qualitative comparison to on 256 × 256 images from the ADE20K dataset.
conditioning
ground truth
SPADE 
Figure 41. Qualitative comparison to on 256 × 256 images from the COCO-Stuff dataset.
conditioning
conditioning
Figure 42. Conditional samples for the depth-to-image model on IN.
conditioning
conditioning
Figure 43. Conditional samples for the pose-guided synthesis model via keypoints on DeepFashion.
class exemplar
class exemplar
Figure 44. Samples produced by the class-conditional model trained on RIN.
nearest neighbors
CelebA-HQ (best val. NLL)
CelebA-HQ (best train NLL)
FFHQ (best val. NLL)
FFHQ (best train NLL)
Figure 45. Nearest neighbors for our face-models trained on FFHQ and CelebA-HQ (256 × 256 pix), based on the LPIPS distance.
The left column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding
class (increasing distance) in the training dataset. We evaluate two different model checkpoints for each dataset: Best val. NLL denotes
the minimal NLL over the course of training, evaluated on unseen testdata. For this checkpoint, both models generate crisp, high-quality
samples not present in the training data. However, when drastically overﬁtting the model, it reproduces samples from the training data
(best train NLL). Although not an ideal measure of image quality, NLL thus provides a proxy on model selection, whereas FID does not.
See also Sec. E.
k = 250, p = 1.0, a = 1.0
k = 973, p = 0.88, a = 1.0
mixed k, p = 1.0, a = 0.05
mixed k, p = 1.0, a = 0.005
Figure 46.
Nearest neighbors for our class-conditional ImageNet model (256 × 256 pix), based on the LPIPS distance. The left
column shows a sample from our model, while the 10 examples to the right show the nearest neighbors from the corresponding class
(increasing distance) in the training dataset. Our model produces new, unseen high-quality images, not present in the training data.
spiral out
12 13 14 15
10 11 14 15
10 14 11 15
15 14 13 12
training step
negative log-likelihood
Batch-wise Training Loss
training step
negative log-likelihood
Validation Loss
Figure 47. Top: All sequence permutations we investigate, illustrated on a 4 × 4 grid. Bottom: The transformer architecture is permutation
invariant but next-token prediction is not: The average loss on the validation split of ImageNet, corresponding to the negative log-likelihood,
differs signiﬁcantly between different prediction orderings. Among our choices, the commonly used row-major order performs best.
Spiral Out
Alternating
Figure 48. Random samples from transformer models trained with different orderings for autoregressive prediction as described in Sec. F.