Machine Learning, 24, 123–140 
c⃝1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.
Bagging Predictors
LEO BREIMAN
 
Statistics Department, University of California, Berkeley, CA 94720
Editor: Ross Quinlan
Abstract. Bagging predictors is a method for generating multiple versions of a predictor and using these to get
an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and
does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of
the learning set and using these as new learning sets. Tests on real and simulated data sets using classiﬁcation and
regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy.
The vital element is the instability of the prediction method. If perturbing the learning set can cause signiﬁcant
changes in the predictor constructed, then bagging can improve accuracy.
Keywords: Aggregation, Bootstrap, Averaging, Combining
Introduction
A learning set of L consists of data {(yn, xn), n = 1, . . . , N} where the y’s are either class
labels or a numerical response. Assume we have a procedure for using this learning set to
form a predictor ϕ(x, L) — if the input is x we predict y by ϕ(x, L). Now, suppose we
are given a sequence of learnings sets {Lk} each consisting of N independent observations
from the same underlying distribution as L. Our mission is to use the {Lk} to get a better
predictor than the single learning set predictor ϕ(x, L). The restriction is that all we are
allowed to work with is the sequence of predictors {ϕ(x, Lk)}.
If y is numerical, an obvious procedure is to replace ϕ(x, L) by the average of ϕ(x, Lk)
over k, i.e. by ϕA(x) = ELϕ(x, L) where EL denotes the expectation over L, and the
subscript A in ϕA denotes aggregation. If ϕ(x, L) predicts a class j ∈{1, . . . , J}, then
one method of aggregating the ϕ(x, Lk) is by voting. Let Nj = nr{k; ϕ(x, Lk) = j} and
take ϕA(x) = argmaxjNj, that is, the j for which Nj is maximum.
Usually, though, we have a single learning set L without the luxury of replicates of L.
Still, an imitation of the process leading to ϕA can be done. Take repeated bootstrap samples
{L(B)} from L, and form {ϕ(x, L(B))}. If y is numerical, take ϕB as
ϕB(x) = avBϕ(x, L(B)).
If y is a class label, let the {ϕ(x, L(B))} vote to form ϕB(x). We call this procedure
“bootstrap aggregating” and use the acronym bagging.
The {L(B)} form replicate data sets, each consisting of N cases, drawn at random, but
with replacement, from L. Each (yn, xn) may appear repeated times or not at all in any
particular L(B). The {L(B)} are replicate data sets drawn from the bootstrap distribution
approximating the distribution underlying L. For background on bootstrapping, see Efron
L. BREIMAN
and Tibshirani . A critical factor in whether bagging will improve accuracy is the
stability of the procedure for constructing ϕ. If changes in L, i.e. a replicate L, produces
small changes in ϕ, then ϕB will be close to ϕ. Improvement will occur for unstable
procedures where a small change in L can result in large changes in ϕ. Instability was
studied in Breiman where it was pointed out that neural nets, classiﬁcation and
regression trees, and subset selection in linear regression were unstable, while k-nearest
neighbor methods were stable.
For unstable procedures bagging works well. In Section 2 we bag classiﬁcation trees on
a variety of data sets. The reduction in test set misclassiﬁcation rates ranges from 6% to
77%. In Section 3 regression trees are bagged with reduction in test set mean squared error
on data sets ranging from 21% to 46%. Section 4 goes over some theoretical justiﬁcation
for bagging and attempts to understand when it will or will not work well. This is illustrated
by the results of Section 5 on subset selection in linear regression using simulated data.
Section 6 gives concluding remarks. These discuss how many bootstrap replications are
useful, bagging nearest neighbor classiﬁers and bagging class probability estimates. The
Appendix gives brief descriptions of the data sets.
The evidence, both experimental and theoretical, is that bagging can push a good but
unstable procedure a signiﬁcant step towards optimality. On the other hand, it can slightly
degrade the performance of stable procedures. There has been recent work in the literature
with some of the ﬂavor of bagging. In particular, there has been some work on averaging
and voting over multiple trees. Buntine gave a Bayesian approach, Kwok and Carter
 used voting over multiple trees generated by using alternative splits, and Heath et al.
 used voting over multiple trees generated by alternative oblique splits. Dietterich
and Bakiri showed that a method for coding many class problems into a large number
of two class problems increases accuracy. There is some commonality of this idea with
Bagging Classiﬁcation Trees
Results for Moderate Sized Data Sets
Bagging was applied to classiﬁcation trees using the following data sets
waveform (simulated)
breast cancer (Wisconsin)
ionosphere
All of these except the heart data are in the UCI repository (ftp to ics.uci.edu/pub/machinelearning-databases). Table 1 gives a numerical summary of the data sets and brief descriptions are contained in the Appendix:
BAGGING PREDICTORS
Table 1. Data Set Summary
# Variables
breast cancer
ionosphere
In all runs the following procedure was used:
The data set is randomly divided into a test set T and a learning set L. In the real data
sets T is 10% of the data. In the simulated waveform data, 1800 samples are generated.
L consists of 300 of these, and T the remainder.
ii) A classiﬁcation tree is constructed from L using 10-fold cross-validation. Running the
test set T down this tree gives the misclassiﬁcation rate eS(L, T ).
iii) A bootstrap sample LB is selected from L, and a tree grown using LB. The original
learning set L is used as test set to select the best pruned subtree (see Section 4.3). This
is repeated 50 times giving tree classiﬁers φ1(x), . . . , φ50(x).
iv) If (jn, xn) ∈T , then the estimated class of xn is that class having the plurality in
φ1(xn), . . . , φ50(xn). If there is a tie, the estimated class is the one with the lowest
class label. The proportion of times the estimated class differs from the true class is the
bagging misclassiﬁcation rate eB(L, T ).
v) The random division of the data into L and T is repeated 100 times and the reported
¯eS, ¯eB are the averages over the 100 iterations. For the waveform data, 1800 new cases
are generated at each iteration. Standard errors of ¯eS and ¯eB over the 100 iterations are
also computed.
Table 2 gives the values of ¯eS, ¯eB, and Table 3 their estimated standard errors.
Table 2. Misclassiﬁcation Rates (%)
breast cancer
ionosphere
L. BREIMAN
Table 3. Standard Errors of Misclassi-
breast cancer
ionosphere
For the waveform data, its known that the lowest possible error rate is 14%. Bagging
reduces the excess error by about two-thirds. We conjecture that the small decrease in the
diabetes data set is because bagging is pushing close to the minimal attainable error rate.
For instance, in the comparison by Michie et al. of 22 classiﬁers on this data set,
the smallest error rate achieved (estimated by 12-fold cross-validation) was 22.3%.
Statlog Comparisons for Larger Data Sets
The Statlog Project [Michie et al., 1994] compared 22 classiﬁcation methods over a wide
variety of data sets. For most of these data sets, error rates were estimated using a single
cross-validation. Without knowing the random subdivisions used in these cross-validations,
the variability in the resulting error estimates makes comparisons chancey.
However, there were larger data sets used in the project which were divided into training
and test sets. Four are publically available, and we used these as a basis for comparison.
They can be accessed by ftp to ftp.strath.ac.uk and are described both in the Michie et al.
 book and in the data repository. Their numerical characteristics are given in Table
4 with brief descriptions in the Appendix.
Table 4. Statlog Data Set Summary
#Variables
In each data set, a random 10% of the training set was set aside and a tree grown on the
other 90%. The set aside 10% was then used to select the best pruned subtree. In bagging,
50 bootstrap replicates of the training set were generated and a large tree grown on each
one. The original training set is used to select the best pruned subtree (see Section 4.3).
The test set errors are listed in Table 5.
BAGGING PREDICTORS
Table 5. Test Set Misclassiﬁcation Rates
Compared to the 22 classiﬁers in the Statlog Project, bagged trees ranked 2nd in accuracy
on the DNA data set, 1st on the shuttle, 2nd on the satellite and 1st on letters. Following
the Statlog method of ordering classiﬁers by their average rank, bagged trees was the top
classiﬁer on these four data sets with an average rank of 1.8. The next highest of the 22 has
an average rank of 6.3. Average ranks for well-known classiﬁers are given in Table 6.
Table 6. Average Ranks of Classiﬁers
Average Rank
Radial Basis Functions
Quad. Discriminant
Neural Net
Some of the misclassiﬁcation rates for the CART algorithm in Table 5 differ from those
listed in the Statlog results. Possible sources for these differences are:
Different strategies may have been used to grow and prune the tree. I used the 90%-10%
method speciﬁed above. Its not clear what was done in the Statlog project.
ii) An important setting is the minimum node size. This setting is not speciﬁed in the
Statlog project. We used a minimum node size of one throughout.
iii) In the DNA data, different preprocessing of the input variable was used (see Appendix).
Bagging Regression Trees
Bagging trees was used on ﬁve data sets with numerical responses.
Boston Housing
Friedman #1
(simulated)
Friedman #2
(simulated)
Friedman #3
(simulated)
L. BREIMAN
Table 7. Summary of Data Sets
# Variables
# Test Set
Boston Housing
Friedman #1
Friedman #2
Friedman #3
A summary of these data sets is given in Table 7.
Brief descriptions of the data are in the Appendix. A procedure similar to that used in
classiﬁcation was followed:
Each real data set is randomly divided into a test set of consisting of 10% of the data
and a learning set L consisting of the other 90%. In the 3 simulated data sets, 1200
cases are generated, 200 used as learning and 1000 as test.
ii) A regression tree is constructed from L using 10-fold cross-validation. Running the
test set down this tree gives the squared error eS(L, T ).
iii) AbootstrapsampleLB isselectedfromLandatreegrownusingLB andLusedtoselect
the pruned subtree. This is repeated 25 times giving tree predictors φ1(x), . . . , φ25(x).
iv) For (yn, xn) ∈T , the bagged predictor is ˆyn = avkφk(xn), and the squared error
eB(L, T ) is avn(yn −ˆyn)2
v) The random division of the data into L and T is repeated 100 times and the errors
averaged to give ¯eS, ¯eB. For the simulated data, the 1200 cases are newly generated
for each repetition.
Table 8 lists the values of ¯eS, ¯eB, and Table 9 gives their estimated standard errors.
Table 8. Mean Squared Test Set Error
Boston Housing
Friedman #1
Friedman #2
Friedman #3
BAGGING PREDICTORS
Table 9. Standard Errors
Boston Housing
Friedman #1
Friedman #2
Friedman #3
Why Bagging Works
Numeric Prediction
Let each (y, x) case in L be independently drawn from the probability distribution P.
Suppose y is numerical and φ(x, L) the predictor. Then the aggregated predictor is the
average over L of φ(x, L), i.e.
φA(x) = ELφ(x, L).
Take x to be a ﬁxed input value and y an output value. Then
EL(y −φ(x, L))2 = y2 −2yELφ(x, L) + ELφ2(x, L).
Using ELφ(x, L) = φA(x) and applying the inequality EZ2 ≥(EZ)2 to the third term
in (4.1) gives
EL(y −φ(x, L))2 ≥(y −φA(x))2.
Integrating both sides of (4.2) over the joint y, x output-input distribution, we get that the
mean-squared error of φA(x) is lower than the mean-squared error averaged over L of
How much lower depends on how unequal the two sides of
[ELϕ(x, L)]2 ≤ELϕ2(x, L)
are. The effect of instability is clear. If ϕ(x, L) does not change too much with replicate L
the two sides will be nearly equal, and aggregation will not help. The more highly variable
the ϕ(x, L) are, the more improvement aggregation may produce. But ϕA always improves
Now φA depends not only on x but also the underlying probability distribution P from
which L is drawn, i.e. φA = φA(x, P). But the bagged estimate is not ϕA(x, P), but
ϕB(x) = ϕA(x, PL),
where PL is the distribution that concentrates mass 1/N at each point (yn, xn) ∈L, (PL
is called the bootstrap approximation to P). Then ϕB is caught in two currents: on the one
hand, if the procedure is unstable, it can give improvement through aggregation. On the
L. BREIMAN
other side, if the procedure is stable, then ϕB = ϕA(x, PL) will not be as accurate for data
drawn from P as ϕA(x, P) ≃ϕ(x, L).
There is a cross-over point between instability and stability at which ϕB stops improving
on ϕ(x, L) and does worse. This has a vivid illustration in the linear regression subset
selection example in the next section. There is another obvious limitation of bagging. For
some data sets, it may happen that ϕ(x, L) is close to the limits of accuracy attainable on
that data. Then no amount of bagging will do much improving. This is also illustrated in
the next section.
Classiﬁcation
In classiﬁcation, a predictor ϕ(x, L) predicts a class label j ∈{1, . . . , J}. Denote
Q(j|x) = P(φ(x, L) = j).
The interpretation of Q(j|x) is this: over many independent replicates of the learning set
L, φ predicts class label j at input x with relative frequency Q(j|x). Let P(j|x) be the
probability that input x generates class j. Then the probability that the predictor classiﬁes
the generated state at x correctly is
Q(j|x)P(j|x).
The overall probability of correct classiﬁcation is
Q(j|x)P(j|x)]PX(dx)
where PX(dx) is the x probability distribution.
Looking at (4.3) note that for any Q(j|x),
Q(j|x)P(j|x) ≤max
with equality only if
½ 1 if P(j|x) = maxi P(i|x)
The predictor φ∗(x) = argmaxjP(j|x) (known as the Bayes predictor) leads to the above
expression for Q(j|x) and gives the highest attainable correct classiﬁcation rate:
P(j|x)PX(x).
Call φ order-correct at the input x if
argmaxjQ(j|x) = argmaxjP(j|x).
BAGGING PREDICTORS
This means that if input x results in class j more often than any other class, then φ also
predicts class j at x more often than any other class. An order-correct predictor is not necessarily an accurate predictor. For instance, in a two class problem, suppose P(1|x) = .9,
P(2|x) = .1 and Q(1|x) = .6, Q(2|x) = .4. Then the probability of correct classiﬁcation
by φ at x is .58, but the Bayes predictor gets correct classiﬁcation with probability .90.
The aggregated predictor is: φA(x) = argmaxjQ(j|x). For the aggregated predictor
the probability of correct classiﬁcation at x is
I(argmaxiQ(i|x) = j)P(j|x)
whereI(·)istheindicatorfunction. Ifφisorder-correctatx, then(4.4)equalsmaxj P(j|x).
Letting C be the set of all inputs x at which φ is order-correct, we get for the correct
classiﬁcation probability of φA the expression
P(j|x)PX(dx) +
I(φA(x) = j)P(j|x)]PX(x).
Even if φ is order-correct at x its correct classiﬁcation rate can be far from optimal. But
φA is optimal. If a predictor is good in the sense that it is order-correct for most inputs
x, then aggregation can transform it into a nearly optimal predictor. On the other hand,
unlike the numerical prediction situation, poor predictors can be transformed into worse
ones. The same behavior regarding stability holds. Bagging unstable classiﬁers usually
improves them. Bagging stable classiﬁers is not a good idea.
Using the Learning Set as a Test Set
In bagging trees, the training set LB is generated by sampling from the distribution PL.
Using LB a large tree T is constructed. The standard CART methodology ﬁnds the sequence
ofminimumcostprunedsubtreesofT. The“best”prunedsubtreeinthissequenceisselected
using either cross-validation or a test set.
The idea of a test set is that it is formed by independently sampling from the same
underlying distribution that gave rise to the learning set. In the present context, a test set
is formed by independently sampling from PL, i.e. we can get a test set by sampling with
replacement, from the original learning set L.
Consider sampling, with replacement, a large number of times N ′ from L. If k(n) is
the number of times that (yn, xn) is selected, then the intuitive content of my argument is
that k(n)/k(n′) ≃1, i.e. each case (yn, xn) will be selected about the same number of
times (ratio-wise) as any other case. Thus, using a very large test set sampled from PL is
equivalent to just using L as a test set.
A somewhat more convincing argument is this: if there are N cases (yn, xn) then the
number of times any particular (yn, xn) is selected has a binomial distribution with p =
1/N, and N ′ trials. The expected number of times (yn, xn) is selected is N ′p = N ′/N.
The standard deviation of the number of times (yk, xn) is selected is √N ′pq ≃
L. BREIMAN
Thus, for all n,
k(n)/(N ′/N) ≃1 + o(1).
The fact that L can be used as a test set for predictors grown on a bootstrap sample LB
is more generally useful than just in a tree predictor context. For instance, in neural nets
early stopping depends on the use of a test set. Thus, in bagging neural nets, the optimal
point of early stopping can be estimated using the original learning set as a test set.
A Linear Regression Illustration
Forward Variable Selection
Subset selection in linear regression gives an illustration of the points made in the previous
section. With data of the form L = {(yn, xn), n = 1, . . . , N} where x = (x1, . . . , xM)
consistsofM predictorvariables, apopularpredictionmethodconsistsofformingpredictors
ϕ1(x), . . . , ϕM(x) where ϕm is linear in x and depends on only m of the M x-variables.
Then one of the {ϕm} is chosen as the designated predictor. For more background, see
Breiman and Spector .
A common method for constructing the {ϕm}, and one that is used in our simulation,
is forward variable entry. If the variables used in ϕk are xm1, . . . , xmk, then for each
m ̸∈{m1, . . . , mk} form the linear regression of y on (xm1, . . . , xmk, xm), compute the
residual sum-of-squares RSS(m) and take xmk+1 such that mk+1 minimizes RSS(m) and
ϕk+1(x) the linear regression based on (xm1, . . . , xmk+1).
There are other forms of variable selection e.g. best subsets, backwards variable selection,
and variants thereof. What is clear about all of them is that they are unstable procedures
(see Breiman ). The variables are competing for inclusion in the {ϕm} and small
changes in the data can cause large changes in the {ϕm}.
Simulation Structure
The simulated data used in this section are drawn from the model.
where ϵ is N(0, 1) (normally distributed with mean zero and variance one). The number
of variables M = 30 and the sample size is 60. The {xm} are drawn from a mean-zero
joint normal distribution with EXiXj = ρ|i−j| and at each iteration, ρ is selected from a
uniform distribution on .
It is known that subset selection is nearly optimal if there are only a few large non-zero
βm, and that its performance is poor if there are many small but non-zero βm. To bridge
the spectrum, three sets of coefﬁcients are used. Each set of coefﬁcients consists of three
clusters; one is centered at m = 5, one at m = 15 and the other at m = 25. Each cluster is
of the form
βm = c[(h −|m −k|)+]2,
m = 1, . . . , 30
BAGGING PREDICTORS
where k is the cluster center, and h = 1, 3, 5 for the ﬁrst, second and third set of coefﬁcients
respectively. Thus, for h = 1, there are only three non-zero {βm}. For h = 3 there are
15 non-zero {βm}, and for h = 5, there are 27 non-zero {βm}, all relatively small. The
normalizing constant c is taken so that the R2 for the data is ≃.75 where R is the correlation
between the output variable y and the full least squares predictor.
For each set of coefﬁcients, the following procedure was replicated 250 times:
Data L = {(yn, xn), n = 1, . . . , 60} was drawn from the model
where the {xm} were drawn from the joint normal distribution described above.
ii) Forward entry of variables was done using L to get the predictors ϕ1(x), . . . , ϕM(x).
The mean-squared prediction error of each of these was computed giving e1, . . . , eM.
iii) Fifty bootstrap replicates {L(B)} of L were generated. For each of these, forward stepwise regression was applied to construct predictors {ϕ1(x, L(B)), . . . , ϕM(x, L(B))}.
These were averaged over the L(B) to give the bagged sequence ϕ(B)
(x), . . . , ϕ(B)
The prediction errors e(B)
, . . . , e(B)
for this sequence were computed.
These computed mean-squared-errors were averaged over the 250 repetitions to give two
sequences {¯e(S)
m }, {¯e(B)
m }. For each set of coefﬁcients, these two sequences are plotted vs.
m in Figure 1a,b,c.
Discussion of Simulation Results
Looking at Figures 1a,b,c, an obvious result is that the most accurate bagged predictor is
at least as good as the most accurate subset predictor. When h = 1 and subset selection is
nearly optimal, there is no improvement. For h = 3 and 5 there is substantial improvement.
This illustrates the obvious: bagging can improve only if the unbagged is not optimal.
The second point is less obvious. Note that in all three graphs there is a point past which
the bagged predictors have larger prediction error than the unbagged. The explanation is
this: linear regression using all variables is a fairly stable procedure. The stability decreases
as the number of variables used in the predictor decreases. As noted in Section 4, for a
stable procedure ϕB = ϕA(x, PL) is not as accurate as ϕ ≃ϕ(x, P). The higher values
for large m reﬂect this fact. As m decreases, the instability increases and there is
a cross-over point at which ϕ(B)
becomes more accurate than ϕm.
Concluding Remarks
Bagging Class Probability Estimates
Some classiﬁcation methods estimate probabilities ˆp(j|x) that an object with prediction vector x belongs to class j. Then the class corresponding to x is estimated as arg maxj ˆp(j|x).
L. BREIMAN
Figure 1. Prediction error for subset selection and bagged subset selection.
BAGGING PREDICTORS
For such methods, a natural competitor to bagging by voting is to average the ˆp(j|x) over all
bootstrap replications, getting ˆpB(j|x), and then use the estimated class arg maxj ˆpB(j|x).
This estimate was computed in every classiﬁcation example we worked on. The resulting
misclassiﬁcation rate was always virtually identical to the voting misclassiﬁcation rate.
In some applications, estimates of class probabilities are required instead of, or along
with, the classiﬁcations. The evidence so far indicates that bagged estimates are likely to
be more accurate than the single estimates. To verify this, it would be necessary to compare
both estimates with the true values p∗(j|x) over the x in the test set. For real data the true
values are unknown. But they can be computed for the simulated waveform data, where
they reduce to computing an expression involving error functions.
Using the waveform data, we did a simulation similar to that in Section 2 with learning
and test sets both of size 300, and 25 bootstrap replications. In each iteration, we computed
the average over the test set and classes of |ˆp(j|x) −p∗(j|x)| and |ˆpB(j|x) −p∗(j|x)|.
This was repeated 50 times and the results averaged. The single tree estimates had an error
of .189. The error of the bagged estimates was .124, a decrease of 34%.
How Many Bootstrap Replicates Are Enough?
In our experiments, 50 bootstrap replicates was used for classiﬁcation and 25 for regression.
This does not mean that 50 or 25 were necessary or sufﬁcient, but simply that they seemed
reasonable. My sense of it is that fewer are required when y is numerical and more are
required with an increasing number of classes.
The answer is not too important when procedures like CART are used, because running
times, even for a large number of bootstraps, are very nominal. But neural nets progress
much more slowly and replications may require many days of computing. Still, bagging is
almost a dream procedure for parallel computing. The construction of a predictor on each
L(B) proceeds with no communication necessary from the other CPU’s.
To give some ideas of what the results are as connected with the number of bootstrap
replicates we ran the waveform data using 10, 25, 50 and 100 replicates using the same
simulation scheme as in Section 2. The results appear in Table 10.
Table 10. Bagged Misclassiﬁcation Rates (%)
No. Bootstrap Replicates
Misclassiﬁcation Rate
The unbagged rate is 29.1, so its clear that we are getting most of the improvement using
only 10 bootstrap replicates. More than 25 bootstrap replicates is love’s labor lost.
L. BREIMAN
How Big Should the Bootstrap Learning Set Be?
In all of our runs we used bootstrap replicates L(B) of the same size as the initial learning
set L. While a bootstrap replicate may have 2, 3, . . . duplicates of a given instance, it also
leaves out about .37 of the instances. A reader of the technical report on which this paper is
based remarked that this was an appreciable loss of data, and that accuracy might improve
if a larger bootstrap set was used. We experimented with bootstrap learning sets twice the
size of L. These left out about e−2 = .14 of the instances. There was no improvement in
Bagging Nearest Neighbor Classiﬁers
Nearest neighbor classiﬁers were run on all the data sets described in Section 2 except for
the soybean data whose variables were nominal. The same random division into learning
and test sets was used with 100 bootstrap replicates, and 100 iterations in each run. A
Euclidean metric was used with each coordinate standardized by dividing by its standard
deviation over the learning set. See Table 11 for the results.
Misclassiﬁcation
Rates for Nearest Neighbor
breast cancer
ionosphere
Nearest neighbor is more accurate than single trees in 3 of the 6 data sets, but bagged trees
are more accurate in all of the 6 data sets.
Cycles did not have to be expended to ﬁnd that bagging nearest neighbors does not change
things. Some simple computations show why. Given N possible outcomes of a trial (the
N cases (yn, xn) in the learning set) and N trials, the probability that the nth outcome is
selected 0, 1, 2, . . . times is approximately Poisson distributed with λ = 1 for large N. The
probability that the nth outcome will occur at least once is 1 −(1/e) ≃.632.
If there are NB bootstrap repetitions in a 2-class problem, then a test case may change
classiﬁcation only if its nearest neighbor in the learning set is not in the bootstrap sample
in at least half of the NB replications. This probability is given by the probability that the
number of heads in NB tosses of a coin with probability .632 of heads is less than .5NB.
As NB gets larger, this probability gets very small. Analogous results hold for J-class
The stability of nearest neighbor classiﬁcation methods with respect to perturbations of
the data distinguishes them from competitors such as trees and neural nets.
BAGGING PREDICTORS
Conclusions
Bagging goes a ways toward making a silk purse out of a sow’s ear, especially if the sow’s
ear is twitchy. It is a relatively easy way to improve an existing method, since all that needs
adding is a loop in front that selects the bootstrap sample and sends it to the procedure
and a back end that does the aggregation. What one loses, with the trees, is a simple and
interpretable structure. What one gains is increased accuracy.
Descriptions of Data Sets
A. Classiﬁcation Data Sets
Waveform This is simulated 21 variable data with 300 cases and 3 classes each having
probability 1/3. It is described in Breiman et al (a C subroutine for generating the
data is in the UCI repository subdirectory/waveform).
Heart This is data from the study referred to in the opening paragraphs of the CART book
(Breiman et. al. ). To quote:
At the University of California, San Diego Medical Center, when a heart attack patient
is admitted, 19 variables are measured during the ﬁrst 24 hours. These include blood
pressure, age, and 17 other ordered and binary variables summarizing the medical
symptoms considered as important indicators of the patient’s condition.
The goal of a recent medical study (see Chapter 6) was the development of a method
to identify high risk patients (those who will not survive at least 30 days) on the basis
of the initial 24-hour data.
The data base has also been studied in Olshen et al . It was gathered on a project
(SCOR) headed by John Ross Jr. Elizabeth Gilpin and Richard Olshen were instrumental in
my obtaining the data. The data used had 18 variables. Two variables with high proportions
of had missing data were deleted, together with a few other cases that missing values. This
left 779 complete cases — 77 deaths and 702 survivors. To equalize class sizes, each case
of death was replicated 9 times giving 693 deaths for a total of 1395 cases.
Breast Cancer This is data given to the UCI repository by Willian H. Wolberg, University
of Wisconsin Hospitals, Madison (see Wolberg and Mangasarian ). It is two class
data with 699 cases (458 benign and 241 malignant). It has 9 variables consisting of cellular
characteristics.
Ionosphere This is radar data gathered by the Space Physics Group at Johns Hopkins
University (see Sigillito et. al. ). There are 351 cases with 34 variables, consisting
of 2 attributes for each at 17 pulse numbers. There are two classes: good = some type of
structure in the ionosphere (226); bad = no structure (125).
L. BREIMAN
Diabetes This is a data base gathered among the Pima Indians by the National Institute of
Diabetes and Digestive and Kidney Diseases. (See Smith et. al. ). The data base
consists of 768 cases, 8 variables and two classes. The variables are medical measurements
on the patient plus age and pregnancy information. The classes are: tested positive for
diabetes (268) or negative (500).
Glass This data base was created in the Central Research Establishment, Home Ofﬁce
Forensic Science Service Aldermaston, Reading, Berkshire. Each case consists of 9 chemical measurements on one of 6 types of glass. There are 214 cases.
Soybean The soybean data set consists of 683 cases, 35 variables and 19 classes. The
classes are various types of soybean diseases. The variables are observations on the plants
together with some climatic variables. All are nominal. Some missing values were ﬁlled
in by their modal values.
Letters This data set was constructed by David J. Slate, Odesta Corporation.
pixel displays of the 26 capital English letters were created using 20 different fonts and
then randomly distorted to create 20,000 images. Sixteen features, consisting of statistical
moments and edge counts, were extracted from each image.
Satellite This is data extracted from Landsat images. For the 3 × 3 pixel area examined,
intensity readings are given in 4 spectral bands for each pixel. The middle pixel is classiﬁed
as one of 6 different soil types. The multiple authorship of the data set is explained in the
documentation in the repository.
Shuttle This data set involves shuttle controls concerning the position of radiators within
the Space Shuttle. The data originated from NASA and was provided to the archives by J.
DNA The classes in this data set are types of boundaries in a spliced DNA sequence. The
input variables consists of a window of 60 nucleotides each having one of 4 categorical
values (A, G, C, T). The problem is to classify the middle point of the window as one of
two types of boundaries or neither. The data is part of the Genbank and was donated by G.
Towell, M. Noordewier and J. Shavlik.
Because some classiﬁers in the Statlog project could accept only numeric inputs, each of
the 60 categoricals was coded into 3 binary variables, resulting in 180 input variables. For
reasons not explained, some of the tree algorithms run on the DNA data were given the 60
categoricals as input while the CART algorithm was given the 180 binary inputs. In my
runs the 60 categorical inputs were used.
B. Regression Data Sets
Boston Housing This data became well-known through its use in the book by Belsley,
Kuh, and Welsch . It has 506 cases corresponding to census tracts in the greater
Boston area. The y-variable is median housing price in the tract. There are 12 predictor
BAGGING PREDICTORS
variables, mainly socio-economic. The data has since been used in many studies. (UCI
repository/housing).
Ozone The ozone data consists of 366 readings of maximum daily ozone at a hot spot
in the Los Angeles basin and 9 predictor variables — all meteorlogical, i.e. temperature,
humidity, etc. It is described in Breiman and Friedman and has also been used in
many subsequent studies. Eliminating one variable with many missing values and a few
other cases leaves a data set with 330 complete cases and 8 variables.
Friedman #1 All three Friedman data sets are simulated data that appear in the MARS
paper (Friedman ). In the ﬁrst data set, there are ten independent predictor variables
x1, . . . , x10 each of which is uniformly distributed over . The response is given by
y = 10 sin(πx1x2) + 20(x3 −.5)2 + 10x4 + 5x5 + ϵ
where ϵ is N(0, 1). Friedman gives results for this model for sample sizes 50, 100, 200.
We use sample size 200.
Friedman #2, #3 These two examples are taken to simulate the impedance and phase shift
in an alternating current circuit. They are 4 variable data with
1 + (x2x3 −(1/x2x4))2)1/2 + ϵ2
x2x3 −(1/x2x4)
where x1, x2, x3, x4 are uniformly distributed over the ranges
0 ≤x1 ≤100
20 ≤(x2/2π) ≤280
The noise ϵ2, ϵ3 are distributed as N(0, σ2
2), N(0, σ2
3) with σ2, σ3 selected to give 3:1
signal/noise ratios.