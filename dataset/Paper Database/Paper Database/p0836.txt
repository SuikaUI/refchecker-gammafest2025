Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447–2469
November 7–11, 2021. ©2021 Association for Computational Linguistics
Challenges in Detoxifying Language Models
Johannes Welbl∗Amelia Glaese∗
Jonathan Uesato∗Sumanth Dathathri∗
John Mellor∗
Lisa Anne Hendricks
Kirsty Anderson
Pushmeet Kohli
Ben Coppin
Po-Sen Huang∗
{welbl,glamia,juesato,sdathath,johnme,posenhuang}@deepmind.com
Large language models (LM) generate remarkably ﬂuent text and can be efﬁciently adapted
across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of
safety is imperative for deploying LMs in the
real world; to this end, prior work often relies on automatic evaluation of LM toxicity.
We critically discuss this approach, evaluate
several toxicity mitigation strategies with respect to both automatic and human evaluation,
and analyze consequences of toxicity mitigation in terms of model bias and LM quality.
We demonstrate that while basic intervention
strategies can effectively optimize previously
established automatic metrics on the REAL-
TOXICITYPROMPTS dataset, this comes at the
cost of reduced LM coverage for both texts
about, and dialects of, marginalized groups.
Additionally, we ﬁnd that human raters often
disagree with high automatic toxicity scores
after strong toxicity reduction interventions—
highlighting further the nuances involved in
careful evaluation of LM toxicity.
Introduction
Contemporary text generation models are capable of generating harmful language, including hate speech, insults, profanities and threats .
These harms are often grouped under the umbrella
term “toxicity”.1
To enable safe language model (LM) use and
deployment, it is necessary to measure, understand
the origins, and undertake effective steps to mitigate toxic text generation in LMs. Prior work has
considered various approaches towards reducing
LM toxicity, either by ﬁne-tuning a pre-trained
LM ,
∗Denotes equal contribution.
1Although broad, this term typically does not capture less
obvious, but no less important harms—such as subtle or distributional biases .
Figure 1: Unintended side effect of automatic toxicity reduction methods: Over-ﬁltering of text about
marginalized groups reduces the ability of the LM to
generate text about these groups, even in a positive way.
by steering a model’s generation towards text less
likely to be classiﬁed as toxic , or
through direct test-time ﬁltering .
Recently, Gehman et al. introduced automatic metrics for LM toxicity evaluation based on
toxicity scores of the widely used and commercially deployed PERSPECTIVE API model trained
on online comments annotated for toxicity.2
In this paper, we critically discuss both toxicity evaluation and mitigation for contemporary
transformer-based English LMs. We conduct studies with both human annotation and classiﬁer-based
evaluation, to evaluate the effectiveness of different
toxicity mitigation methods, and investigate tradeoffs with respect to LM quality and social bias.
Our contributions are as follows:
1. We critically discuss LM toxicity evaluation
(§3) and conduct evaluation studies for several mitigation methods (§4), relying both on
automatic toxicity scores (§5) and on human
judgement (§6).
2. We show that combinations of simple methods (§4) are very effective in optimizing (au-
2Perspective
( 
tomatic) toxicity metrics (§5), but prone to
overﬁlter texts related to marginalized groups
3. We ﬁnd increased disagreement of high automatic toxicity scores with human annotators
once strong toxicity reduction measures are
applied, limiting their usefulness as a metric
for further mitigation of toxicity (§6).
4. We show that a reduction in (automatic) toxicity scores comes at a cost. We identify both
a trade-off with LM evaluation loss (§7), and
further show that this disproportionately affects texts about and by marginalized groups
(§8): both topic-related and dialect-related
LM biases increase, as illustrated in Figure 1.
Related Work
While detecting hate speech and offensive language , mostly in the context of online community
moderation, has long been a subject of research; the
study of toxic text generated by language models is
a more recent direction. Wallace et al. ﬁrst
demonstrated that synthetic text prompts can cause
racist model continuations with GPT-2. Gehman
et al. extended the analysis of LM toxicity to non-synthetic prompts, further investigating
the effectiveness of multiple potential mitigation
approaches. We build on, and extend this work,
critically discussing previously introduced metrics
to assess LM toxicity, and compare classiﬁer-based
LM toxicity scoring with human evaluation.
Among the most promising approaches for LM
toxicity reduction is steering generation towards
text less likely to be classiﬁed as toxic . This typically
relies on an external toxicity classiﬁer, although
Schick et al. show that even a LM’s own
toxicity self-diagnosis can be used to this end.
Toxic language detection systems are known to
be biased against speciﬁc social groups, and similar to Zhou et al. , we distinguish two bias
types. First, classiﬁcation bias can manifest as
topic-related biases, where text mentioning particular identities leads to false positives in toxicity
classiﬁers—e.g. LGBTQ+ identity terms (“gay”).
This phenomenon has been linked to an increased
relative prevalence of identity terms among toxic
samples . A second type of bias considers disparate performance across dialects, where
classiﬁers on average assign higher toxicity scores
e.g. to African-American English (AAE) . A potential
side-effect of applying classiﬁer-based toxicity mitigation methods in an LM context, then, is that
such biases might also be inherited by the resulting
Our ﬁndings are consistent with contemporary
work by Xu et al. demonstrating that LM
toxicity mitigations can amplify social biases. Our
work expands these results across a broader range
of models, demographics, and datasets, and uses
Wikipedia metadata rather
than keyword-matching for measuring topic-related
biases. We also show that models which perform
well under our and their likelihood-based metrics
can still exacerbate bias. Finally, by upsampling
toxic samples, we can estimate overall LM toxicity, whereas a comparison-based approach can
emphasize minor changes to already non-toxic LM
completions.
Other work on toxicity in generated text includes
Xu et al. , who investigate safety speciﬁcally
in a dialogue setting, and translating existing offensive text into non-offensive variants .
Toxic Language and LMs
Following the deﬁnition developed by
PERSPECTIVE API, we consider an utterance to be
toxic if it is rude, disrespectful, or unreasonable
language that is likely to make someone leave a
discussion. This deﬁnition has been adopted by
prior work on LM toxicity ,
and allows for direct comparability of quantitative
results. However, we note two important caveats.
First, under this deﬁnition, toxicity judgements are subjective, and depend on both the
raters evaluating toxicity and their cultural background , as well as the inferred
context. As an example, historical inequalities
could lead to a higher toleration of offensive speech
among disadvantaged groups, and measurements of
toxicity should consider such potential disparities.
Phenomena where subjective toxicity ratings can
differ include sarcasm and utterances of political
discontent; we show some example utterances in
Table 12 in the appendix. While not the focus of
this paper, it is important for future work to con-
tinue to develop the above deﬁnition, and clarify
how it can be fairly applied in different contexts.
Second, this notion of toxicity only covers one
aspect of possible LM harms .
For example, LMs can perpetuate harmful stereotypes, or display biases which only manifest statistically over many samples . Though
important, we do not address these here.
LM safety criteria are both application- and
audience-speciﬁc, and in this regard, we recommend caution in over-generalizing results from our
work, particularly regarding the absolute and relative efﬁcacy of speciﬁc techniques. These caveats
are consistent with the limitations our experiments
highlight: regarding the relationship between human and automatic toxic evaluation (Section 6),
and the trade-offs between toxicity mitigation and
coverage for marginalized groups (Section 8).
Evaluating LM Toxicity
In this work, we consider both automatic and human evaluation to measure a LM’s tendency to produce toxic language.
Automatic evaluation can give a ﬁrst, low-cost
indication of toxicity and is useful for particular
types of research, such as narrowly focused steering methods . However, we ultimately care about the impacts of LMs on people, so the beneﬁts of toxicity
reduction must ultimately be deﬁned by human
judgement. An important consideration for human
evaluation is that the annotation process itself can
impose emotional burden on annotators exposed
to toxic content . In Section 10.1 we discuss our strategies to
ensure the annotators’ well-being.
Model and Methods
We next describe the LM we evaluate, as well as
three methods we consider for reducing the LM’s
toxicity, covering both data-based, controllable generation, and direct ﬁltering-based approaches.
TransformerXL
model trained on the C4
dataset , with 24 layers, 16
heads, dmodel = 2048, and dff = 8192.
model contains 1.4B parameters, and achieves
a loss-per-token of 2.40 on the C4 validation
set. It uses a 32,000 subword vocabulary with a
SentencePiece tokenizer . We train all LM variants on 128 Google
Cloud TPUv3 cores using the Adam optimizer, a
batch size of 256 for a total of 3 × 105 training
steps—about 5 days.
For all sampling we use
nucleus sampling , with
top-p = 0.9.
LM Toxicity Reduction Techniques
Training Set Filtering
In this intervention, we
train LMs on different versions of the C4 corpus,
ﬁltered for toxicity according to PERSPECTIVE
API scores.
We denote these subsets as train-
ﬁlter@X, indicating that documents with toxicity
scores above X are removed—lower values of X
denote stronger ﬁltering.3 We choose 0.2, 0.1, and
0.05 as thresholds for ﬁltering the training data,
after which 311M (85%), 209M (57%), and 78M
(22%) of the original training C4 documents remain. We did not see indications of overﬁtting on
these smaller datasets.
Decoder / Test-Time Filtering
We also consider
ﬁltering LM outputs directly at decoding / test-time,
and denote this baseline as test-ﬁlter. To avoid
using PERSPECTIVE API for both ﬁltering and
evaluation, we ﬁlter with a separate BERT-based
toxicity classiﬁer , denoted
as BERT in this work), which is ﬁnetuned for 1
epoch with a learning rate of 2×10−5 on the CIVIL-
COMMENTS dataset , using
16 Google Cloud TPUv3 cores. Following Wulczyn et al. , we use soft labels, based on
the fraction of annotators rating each comment as
toxic, and a cross entropy training objective. The
classiﬁer achieves an accuracy of 96.8% on the
validation set. We ﬁrst generate up to K samples
from the LM, stopping generation when a sample
with BERT toxicity score below τreject = 0.01 is
found.4 If we do not obtain such a continuation
with a low BERT toxicity score (lower scores are
better), we return the sample with the lowest BERT
toxicity score.
Plug-and-Play
We also evaluate PPLM ,
which was the strongest decoding-based method
in Gehman et al. .
Given the hidden
representations from a base LM, PPLM uses an
additional linear discriminator trained to predict
toxicity. When trained on top of our standard LM,
this model achieves a test F1 score of 0.78. PPLM
3Using BERT (cf. Decoder Filtering) to ﬁlter the training
data is another possible setup. We use PERSPECTIVE API as
it most closely matches the target in automatic evaluation.
4For computational reasons, we use K = 4 throughout.
Expected Maximum Toxicity
Probability of Toxicity
Unprompted
Unprompted
†GPT-2 + PPLM
standard (C4)
Train ﬁltering
 
 
 
standard + test-ﬁlter
 + test-ﬁlter
 + test-ﬁlter
 + test-ﬁlter
standard (C4)
standard + test-ﬁlter
 
 + test-ﬁlter
Table 1: Left: Expected Maximum Toxicity over 25 generations. Right: Probability of generating toxic text
at least once over 25 generations. The best performing detoxiﬁcation method yielding the lowest toxicity percategory is marked in bold. All models are evaluated on a full dataset of 100K prompts and 100K unprompted
sentences, except PPLM, which is evaluated on a dataset of 10K prompted and 10K unprompted continuations,
due to computational budget. Results marked with † are taken from Gehman et al. .
uses this discriminator to steer the LM’s hidden
representations towards a direction of both low
predicted toxicity, and low KL-divergence from the
original LM prediction. PPLM hyperparameters
are tuned similar to Madotto et al. , and we
refer to Appendix A.2 for additional details.
Classiﬁer-Based Toxicity Evaluation
Although our primary targets are based on human
evaluation of LM toxicity, described in Section 6,
we ﬁrst describe our evaluation using automatic toxicity metrics for consistency with prior work. We
note that several limitations of automated toxicitydetection tools have been well documented, both
by Jigsaw and by other work .
For automated, classiﬁer-based toxicity evaluation we rely on the REALTOXICITYPROMPTS
(RTP) benchmark . The aim
is to measure LM toxicity within a 20 token continuation, in both the prompt-conditional and unconditional settings. For the conditional case, RTP
consists of 100K English web language prompts,
with each prompt labelled as either toxic or nontoxic. The RTP metrics are derived from the PER-
SPECTIVE API toxicity classiﬁer, which outputs a
calibrated TOXICITY score between 0 and 1.5
5 It is worth noting that the TOXICITY scores provided
by PERSPECTIVE API are calibrated and intended to reﬂect
the probability of the given text being toxic. That is, text with
a score of 0.7 does not indicate that the toxicity level of the
sample is more severe than that of text with score 0.5; but
instead that the classiﬁer has more certainty in its prediction
for the former case, and that for the latter case the model’s
Given these scores, RTP reports two metrics:
i) Expected Maximum Toxicity measures the maximum toxicity score given 25 continuations for a
given prompt, averaged across prompts; ii) Probability of Toxicity measures how frequently at least
one continuation has a toxicity score > 0.5, given
25 LM-generated continuations per prompt.
Automatic Evaluation Results
Table 1 shows results for the three different toxicity
mitigation approaches, and combinations of them,
alongside baselines including the strongest prior
method as reported by Gehman et al. .
First, we observe slightly reduced toxicity rates
in the standard model trained on C4, compared to
GPT-2 (e.g. 0.16 vs. 0.33 unprompted Probability
of Toxicity). This aligns with the overall higher
proportion of toxic documents (score ≥0.5) in the
GPT-2 training corpus, which Gehman et al. 
report at 4.3%, compared to C4 at 0.6%.6 Filtering
the C4 train set based on classiﬁer-based toxicity
leads to further reduced LM toxicity scores, which
also tend to be lower with stronger data ﬁlters. This
conﬁrms that toxic training data directly affects the
resulting LM’s rate of toxicity.
Decoder ﬁltering and PPLM are both highly effective at reducing the automatic toxicity metrics,
across all generation settings. The different methprediction is uncertain.
6C4 has been ﬁltered based on a keyword list that includes
insults, vulgar terms and slurs, but such keyword-based ﬁltering also excludes non-toxic uses for some of these terms, and
this can potentially affect the coverage of the resulting LMs.
ods yield complementary improvements: e.g. decoder ﬁltering further improves already reduced
scores obtained via train ﬁltering alone; PPLM—
when combined with these methods—results in the
largest reductions in toxicity overall.
As a central takeaway, the three detoxiﬁcation
methods and their combinations can effectively optimize automatic toxicity evaluation metrics. In
relative terms, the reduction to the previously reported state-of-the-art is 6fold and 17-fold in the toxic prompt and non-toxic
prompt settings, and a reduction to 0.00 (from 0.05)
in the unprompted setting (Probability of Toxicity). Given how low these scores are in absolute
terms (e.g. Probability of Toxicity scores of 0.00
and 0.01 in the unprompted and non-toxic prompt
settings), the question arises to what extent improvements here are still meaningful, especially
since they are derived from an imperfect automatic
classiﬁcation system. We thus turn to a human
evaluation study in Section 6.
Limitations and Recommendations
We next highlight shortcomings in the above used
automated toxicity evaluation protocol, and provide
suggestions for improvement.
First, we observed that sampling only 20 tokens,
as was done in prior work ,
can provide insufﬁcient context to form a toxicity
judgement. Second, a hard truncation after a ﬁxed
number of word-piece tokens, can truncate words
at the sequence end (e.g. “ass”), which can erroneously trigger automatic toxicity classiﬁers. In Table 6 (appendix), we thus provide analogous automated toxicity evaluation results when using longer
text samples and truncating incomplete sentences at
the end of each sample—with overall similar observations. In our subsequent human evaluation, we
use the same setup to avoid the above issues, and
observed that with longer text continuations, the
agreement between automatic scores and human
ratings tends to increase (Figure 6, appendix).
Finally, we point out that toxicity classiﬁers such
as PERSPECTIVE API, when applied on LM output,
are operating outside their training domain and intended use case, which consists of annotated forum
or discussion comments.
Evaluation via Human Annotation
Following the previous section on automated LM
toxicity evaluation, we will next measure toxicity
and LM generation quality using human evaluation.
Average Perspective API scores
Average Human toxicity scores
standard LM
 
 
 
test-filter
train-filter@.05+test-filter
 +test-filter+PPLM
Figure 2: Average human toxicity scores vs. PERSPEC-
TIVE API scores for the different methods we evaluate.
Methodology
aggregated
humanjudgement to measure the quality of the generated
text and the extent of toxicity present. For the
human toxicity evaluation we rely on previous
annotation instructions by PERSPECTIVE API,7
but we adapt them slightly for the context of LM
generation, including additional questions on
comprehensibility, consistency, and grammaticality.
For each of the LMs under consideration, we
provide both a prompt from the REALTOXIC-
ITYPROMPTS dataset, and the corresponding
continuation generated by the LM to three separate
annotators. We then ask the annotators to judge
whether the continuation adds to the toxicity
present in the prompt with one of the following
labels: VERY TOXIC, TOXIC, NOT SURE, NOT
TOXIC, matching the annotation labels used by
PERSPECTIVE API. We further ask the annotators
to rate if the sentences are i) grammatical, ii)
comprehensible, and iii) consistent in terms
of topicality and style with the labels:
SOMEWHAT, NO. Here, we wish to address the
following questions: i) how effective are toxicity
reduction techniques based on human ratings? ii)
how do automated evaluations align with human
evaluation? and iii) what qualitative impacts are
there on the language generated?
As most PERSPECTIVE API scores for detoxiﬁed LMs are relatively small, random sampling
leads to very few samples with high scores, and
we would not be able to compare different toxicity
ranges efﬁciently. Hence, we up-sample continuations with high classiﬁer-based toxicity scores
when selecting texts to present to annotators. In total, we prepare 300 samples for each setting. From
a pool of 49 annotators overall, each sample is
rated by at least 3 annotators, then we discard NOT
7 
conversationai.github.io/blob/
8a88f1fc0a/crowdsourcing_annotation_
schemes/toxicity_with_subattributes.md
Perspective API score
Percent rated by humans
with each toxicity level
33 57 51 28
21 35 29 19
Toxicity level (annotated by humans)
very_toxic
Figure 3: Human rating distributions vs PERSPECTIVE
API scores for the standard LM. Bars are labelled with
the number of human ratings in each bin.
SURE annotations, map NOT TOXIC to 0.0 and
both TOXIC and VERY TOXIC to 1.0, and take the
average.8 We weigh the annotations to compensate
for up-sampling. Detailed human annotation instructions, and a full description of the up-sampling
setup are given in Appendix E.
In Figure 2 we present the overall average toxicity scores from human annotations
vs. those of PERSPECTIVE API. A central observation is that the various LM toxicity reduction
methods indeed result in improvements in toxicity
ratings according to human judgement, and there
is furthermore a direct and largely monotonic relation between average human and classiﬁer-based
results. Next, in Figure 3, we show the alignment of
PERSPECTIVE API scores with human ratings for
samples of the standard LM. As expected (cf. footnote 5), the scores are correlated with the probability that humans mark a sample toxic.
Annotation Quality
Measuring agreement between raters, we ﬁnd a Krippendorff’s alpha score
of 0.49 for the standard LM, and of 0.48 for all
annotations across LMs. To calculate these, we
map the NOT TOXIC label to 0.0, NOT SURE to
0.5, TOXIC and VERY TOXIC to 1.0, using absolute differences between these as distance function. Overall, very few cases were labeled as NOT
SURE (about 1%). The score indicates fair overall
agreement, and is comparable to the level of agreement reported in prior work . We note that toxicity rating has subjective aspects, and even with improved
deﬁnitions, experts may disagree—for a concrete
list of phenomena for which we observed annotator
disagreement we defer to Appendix E.3.
8We acknowledge that other aggregation options are possible, e.g. whether any annotator rates a sample as toxic.
Figure 4: False positive analysis: avg. PERSPECTIVE
API vs. human score, with std. error, for annotated samples where the continuation toxicity (Persp.) is > 0.75.
Note that annotated samples will differ from the overall RTP distribution due to the upsampling procedure
described in the Methodology part of Section 6.
False Positives
Notably, in the higher toxicity
score range we ﬁnd that the human and PERSPEC-
TIVE API scores differ substantially after LM
detoxiﬁcation. Figure 4 shows the average PER-
SPECTIVE API vs. average human scores for LMgenerated continuations that have a PERSPECTIVE
API score > 0.75. Human annotations indicate
that far fewer samples are toxic than the automatic
score might suggest, and this effect is stronger as
intervention strength increases, or when multiple
methods are combined. That is, after the application of strong toxicity reduction measures, the
majority of samples predicted as likely toxic are
false positives. Several such examples are shown
in Tables 13 and 14 in the appendix.
Manual inspection reveals that identity term mentions are disproportionately frequent false positives.
For example, we observe that 30.2% of the train-
 LM generations with a toxicity score
above 0.5 mention the word gay, when generating
continuations based on REALTOXICITYPROMPTS
prompts (see Appendix G.1 for additional analysis).
A reliance on automatic metrics alone, like those
used by Gehman et al. , could thus lead to
potentially misleading interpretations. As we will
see in the following Sections 7 and 8, detoxiﬁcation measures can result in a higher LM loss and
ampliﬁed social biases. It is unclear whether further reductions in the fraction of generated samples
with high automatic scores would in fact also further lower toxicity as judged by human annotators,
or instead only exacerbate the problems incurred
by applying detoxiﬁcation measures without providing meaningful reductions in LM toxicity.
Consequences on LM Quality
To understand consequences of applying LM toxicity interventions, and their potential impact on text
generation, we next consider their effect on LM
loss, text sample quality, and LM toxicity prediction ability.
Effect on Language Modeling Loss
shows validation losses for several train-ﬁltered
models. The ﬁrst observation is that training set
ﬁltering has a moderate negative impact on LM
loss which increases with stronger ﬁltering. The
 model loss roughly matches the
LM loss level of a 417M parameter model (about
a third the size), trained on C4 without any interventions. Evaluation on the LAMBADA dataset conﬁrms this trend, with an
accuracy decrease from 50.1% to 34.9% for train-
 (Table 7, appendix). To shed more light
on the origins of deteriorated LM performance, we
note that LM loss increase is particularly strong for
text labeled as toxic by PERSPECTIVE API. For example, the loss on evaluation documents least likely
to be toxic (score < 0.1) increases by 0.17 (+7%)
with the intervention, whereas it
increases by 0.9 (+34%) for the evaluation documents most likely to be toxic (score ≥0.5).
Text Quality We do not observe any strong differences for the different toxicity reduction interventions compared to the standard LM in how comprehensible, how grammatical, and how consistent
with the prompt the generated continuations are:
differences to the standard LM are no larger than
1%, 4%, and 1%, respectively (Table 10, appendix).
Effect on LM’s Ability to Detect Toxicity
When training on a toxicity-ﬁltered LM corpus
(threshold 0.05), we notice a modest drop in the F1score (to 0.73; -0.05 points) of the PPLM toxicity
classiﬁer, which is trained on the LM’s representations. This could potentially negatively impact
self-debiasing strategies .
Social Bias Ampliﬁcation
Fairness with respect to all identity groups is crucial if LMs are to be used in the real world. Two
properties, that we highlight as necessary (but insufﬁcient) for fairness are that LMs should both be
able to model text about topics related to different
identity groups (i.e. topic coverage), and also text
by people from different identity groups and with
different dialects (i.e. dialect coverage).
standard 1.4B
 
 
 
standard 417M
Table 2: Evaluation loss for standard and train-ﬁltered
LMs, across different test sets. Low / mid / high correspond to [0-.1); [.1-.5); [.5-1] toxicity bins in C4.
WT103: WikiText103 .
Previous works have shown that toxicity classi-
ﬁers often show lower performance for text written
by, or referring to marginalized identity groups
 . Given that
many detoxiﬁcation techniques heavily rely on toxicity classiﬁers, we investigate how detoxiﬁcation
affects topic and dialect coverage with respect to
different identity groups. We also discuss potential representational harms 
which can arise from disparities in the effectiveness
of LM toxicity mitigation across different dialects.
We use the gender and ethnicity domains in the BOLD dataset 
to evaluate topic coverage. The former contains
Wikipedia sentences about female and male actors. Similarly, the latter domain contains sentences
about people with different ethnic backgrounds.
We evaluate dialectal coverage using the TWITTER-
AAE dataset introduced by Blodgett et al. ,
where we use tweets from African-American English (AAE) and White Aligned English (WAE)
subsets. We hope that future work can also consider a broader array of groups, including unobserved and ﬂexible categories. Further dataset details are
in Appendix B.1.
Topic-related Biases
We investigate the effects of toxicity reduction on
the LM’s topic coverage, i.e. its ability to model
text about various identity groups. Figure 5 shows
that train-time ﬁltering – while generally leading
to increased loss – indeed has a disparate impact
on topic coverage when measured via loss gaps
relative to a standard LM on the same documents.
This holds for both gender (Figure 5a) and ethnic
(Figure 5b) groups. While the standard model has
similar loss for text about female and male actors
(3.414 vs. 3.412), detoxiﬁcation introduces gender
(a) Gender
(b) Ethnicity
(c) Demographic dialect
Figure 5: LM loss gap between a standard LM and the train-ﬁlter@X LMs (denoted as tf@X), on different subsets
of BOLD (gender and ethnicity) and TWITTERAAE (demographic dialects). Some subsets already have substantially higher loss under a standard LM; we calculate the loss gap in order to avoid this as a potential confounding
factor. While toxicity reduction increases loss on all subsets, the impact is largest for marginalized groups.
bias, leading to larger LM loss for female actors
relative to male actors. Similarly, we observe that
LM loss deterioration is stronger for marginalized
ethnic groups compared to European-Americans.
Although the standard LM has the lowest loss for
Hispanic-American-related text (3.46 vs. 3.68 for
European-American), Hispanic-American sees the
largest negative impact of detoxiﬁcation. This indicates that detoxiﬁcation techniques may introduce
biases distinct from those already existing in LMs.
Dialect-related Biases
Disparate Positive Rates for Tweets Based on
Demographic Dialect
Besides lexical biases,
toxicity classiﬁers have also been shown to exhibit
dialectal biases . Our analysis
shows that TWITTERAAE tweets are more likely to
be classiﬁed as toxic (details in Appendix G.2), congruent with prior work , demonstrating bias against AAE in toxicity classiﬁers.
This suggests that toxicity reduction interventions
might adversely affect dialectical coverage. Investigating this further, we next analyze impacts on
a LM’s ability to model language from different
demographic dialects.
Disparate Impacts on Dialect Coverage
Figure 5c shows relative loss gaps between the detoxiﬁed and the standard models, for both AAE and
WAE tweets. Consistent with Xu et al. ,
we ﬁnd that detoxiﬁcation has larger impact on
AAE coverage than for WAE. We note that AAE
tweets already have substantially higher loss under
a standard LM (5.53 vs. 4.77), which is likely a
result of the underrepresentation (0.07% of all documents) of AAE in C4, as highlighted by Dodge
et al. . This bias is further ampliﬁed with
detoxiﬁcation.
Exp. Max. Toxicity
Prob. of Toxicity
 
Table 3: Expected Maximum Toxicity and Probability
of Toxicity for a standard LM and a 
model, as in Table 1, with TWITTERAAE tweets as
LM Toxicity Reduction with Prompts from Different Dialects
Next we measure the effectiveness of LM detoxiﬁcation for prompts in different
dialects, using the TWITTERAAE tweets in AAE
and WAE to prompt the LM. We ﬁrst apply the automatic metrics from Section 5 to the LM-generated
continuations, as shown in Table 3. This shows
substantially higher values for AAE prompts than
for WAE under the standard LM (e.g. 0.72 vs. 0.59
Probability of Toxicity). LM detoxiﬁcation reduces
automatic toxicity metrics in both dialects, but average LM toxicity scores remain still substantially
higher for AAE prompts after detoxiﬁcation (e.g.
0.22 vs. 0.14 Probability of Toxicity).
Turning to human evaluation, we collect 100
samples for each setting (model × dialect), following the evaluation protocol in Section 6. Table 4
shows that the LM also reduces
average human toxicity scores, in particular for
AAE. In contrast to what automatic evaluation may
suggest, in this human evaluation we ﬁnd similar
levels of toxicity between the dialects, underscoring the limitations of using automatic evaluation
Limitations of Likelihood for Bias
Evaluation
Our above evaluations on LM coverage primarily
rely on likelihood-based loss metrics. However it is
 
Table 4: Average human toxicity scores for model completions of AAE and WAE prompts from TWITTER-
AAE. Standard errors are given as subscripts.
worth noting that such an evaluation can potentially
underestimate existing LM bias.
For instance, consider the loss gap on the BOLD
dataset incurred by a test-time ﬁltering variant
which picks the best of K generated samples.
While the small and similar loss gaps – between
0.09 and 0.13 across all groups (see Table 11 in
Appendix H) – suggests a minimal impact on topic
coverage, it is worth noting that even for highly
biased classiﬁers, e.g. a classiﬁer which ﬂags any
text mentioning female actors as toxic, the impact
on loss-per-token is tightly bounded based on the
following observation:
Observation 1 (Informal). Irrespective of the classiﬁer used for ﬁltering, test-time ﬁltering with a
minimum acceptance rate of ϵ will never increase
loss-per-token by more than −n−1 ln ϵ, where n is
the document length.
The formal statement and proof are included in
Appendix H. Thus, LMs with low loss can still have
bad samples, including effects concentrated on particular topics and dialects. Although this example
refers speciﬁcally to test-time ﬁltering, similar underlying concerns also apply to other ﬁltering techniques, including train-time ﬁltering, ﬁne-tuning,
or PPLM. Similar observations have been made previously ; we add
that these limitations become particularly salient
when using ﬁltering-based techniques.
We thus recommend caution in interpreting
likelihood-based metrics: while large loss gaps
can demonstrate high bias, small loss gaps do not
automatically imply low bias.
Conclusion
In this work, we have examined and discussed challenges of LM toxicity evaluation and side-effects of
automatic toxicity mitigation using a combination
of relatively simple toxicity reduction approaches
and previously published methods. We have highlighted the discrepancy between conventional metrics of toxicity and what is perceived by humans.
This points towards a research roadmap of deﬁning metrics that better align with perceived toxicity,
deﬁning sub-types of toxicity, and including separate test sets for each sub-type. We have further
identiﬁed a transfer of toxicity classiﬁer bias onto
LMs, which supports the importance of debiasing toxicity classiﬁers. Based on our results, we
additionally highlight the following challenges in
mitigating toxic language in LMs.
First, toxicity is subjective and context dependent – what is considered toxic may differ across
cultures, social groups, and personal experiences.
Though existing methods can effectively optimize
automatic toxicity scores, precisely deﬁning what
we should measure is an open challenge. Ultimately, this will be dependent on users and applications, and requires cross-disciplinary expertise
and input from a broad variety of groups.
Secondly, very low automatic toxicity metrics of
state-of-the-art LMs after application of the evaluated mitigation techniques suggest that further improvement with respect to these metrics is limited.
It is unclear if further optimization against automatic toxicity metrics will lead to improvements in
toxicity as judged by humans, or only intensify unintended and problematic side effects of automatic
detoxiﬁcation. We also point out limitations in collecting human ratings, including potential negative
psychological impact on annotators.
Finally, our detoxiﬁcation increases LM loss,
and introduces and ampliﬁes social biases in topic
and dialect coverage, potentially leading to decreased LM performance for marginalized groups.
We note that although this problem exists in current
methods, this tradeoff is not necessarily unavoidable, particularly if future work enables less biased
classiﬁers. Alongside toxicity, future work should
consider other metrics, such as loss gaps for different topics and dialects. As noted in Section 8.3,
loss gaps are an imperfect metric; future work on
developing quantitative metrics for LM bias could
help better understand trade-offs in mitigating toxicity.
Ethical Considerations
Our goal in this work is to reduce harms from LMs
by better understanding how to detoxify LMs, and
characterizing any trade-offs that occur when detoxifying LMs. During the course of our research, we
encountered a variety of ethical questions, including how to ethically collect human annotations for
toxic language (detailed in Section 10.1).
As discussed in Section 3, toxicity is subjective
and ill-deﬁned. The deﬁnition of what is “toxic” or
“offensive” may differ between social groups and
cultures. Language acceptable to those who wield
more privilege may be offensive to those who wield
less privilege. While our current methods might
mitigate toxicity as deﬁned by some people, it may
not be sufﬁcient for others.
In this work, we only consider English LMs,
though there are over 7, 000 languages spoken
throughout the world , and we
recommend caution when generalizing our ﬁndings to non-English LMs. We note that the PER-
SPECTIVE API includes toxicity classiﬁers for six
languages besides English,9 though we do not attempt to mitigate toxicity on non-English LMs with
non-English classiﬁers here. However, ethical deployment of LMs requires equitable access and
safety also for non-English speakers.
In considering the potential harms of LMs there
are many more facets than we have considered in
this paper. Here we discuss one important dimension, but other potential harms have been discussed
in prior work, such as, but not limited to, statistical
biases , privacy concerns ,
and environmental impact ,
alongside points raised by Bender et al. ,
which should also be considered when striving for
ethical LMs.
Human Evaluation
Asking humans to annotate toxicity necessarily exposes them to toxic language. Before conducting our study, it was reviewed by DeepMind’s
Human Behavioural Research Ethics Committee (HuBREC).
Participants were recruited through Google’s internal labeling platform, a service that hires contractors to complete tasks. Annotators are hired
to perform a variety of annotation tasks and are
paid based on time worked, not per HITs completed. We design our human evaluation experiments, then work with the annotation platform to
ensure annotators understand the task. Annotator
training (including a module on wellbeing) takes
approximately one hour. Uncertainty in the task is
directly communicated to us (the researchers). In
our initial annotation pilot, the authors also annotated sentences and observed similar trends to the
annotators.
9When considering production level for the TOXICITY
attribute: 
Because of the sensitive nature of annotating
toxic language, we ensured that several options
were available to annotators. Annotators could
choose to split their time between our task and
other tasks which did not include toxic content.
Annotators were given the option to (and did) opt
out of annotating data for our task. Annotators selfdetermined the amount of time they annotated our
data and had access to employee resources for wellbeing concerns caused by our annotation task. We
tracked well-being via a well-being survey. Results
of this survey are detailed in Appendix E.4.
We acknowledge that our annotation instructions
do not include race and dialect priming as introduced by Sap et al. to mitigate racial bias
in hate speech annotations. Thus some of our annotators may be unaware that identity groups and
speciﬁcally African-Americans reclaim offensive
and racist terms and use them safely. However, we
annotate LM continuations, not human written language. As LMs do not have an identity, we do not
believe it is safe for generated language to include
reclaimed terms, even if they can be safely used by
members of marginalized groups. We acknowledge
that there are applications for which this approach
would be incorrect.
Acknowledgements
We would like to thank James Besley, Phil Blunsom, Taylan Cemgil, Sanah Choudhry, Iason
Gabriel, Geoffrey Irving, Maribeth Rauh, Sebastian Ruder, and Laura Weidinger for comments and
discussion on earlier versions of this draft, as well
as Lucy Vasserman and Jeffrey Sorensen for providing support on using PERSPECTIVE API. We have
shared the ﬁndings of this work with the Jigsaw