Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–672,
Gothenburg, Sweden, April 26-30 2014. c⃝2014 Association for Computational Linguistics
Encoding Semantic Resources in Syntactic Structures
for Passage Reranking
Kateryna Tymoshenko
Trento RISE
38123 Povo (TN), Italy
 
Alessandro Moschitti
Qatar Computing Research Instit.
5825 Doha, Qatar
 
Aliaksei Severyn
University of Trento
38123 Povo (TN), Italy
 
In this paper, we propose to use semantic knowledge from Wikipedia and largescale structured knowledge datasets available as Linked Open Data (LOD) for
the answer passage reranking task.
represent question and candidate answer
passages with pairs of shallow syntactic/semantic trees, whose constituents are
connected using LOD. The trees are processed by SVMs and tree kernels, which
can automatically exploit tree fragments.
The experiments with our SVM rank algorithm on the TREC Question Answering
(QA) corpus show that the added relational
information highly improves over the state
of the art, e.g., about 15.4% of relative improvement in P@1.
Introduction
Past work in TREC QA, e.g. ,
and more recent work in
QA has shown that, to achieve human performance, semantic resources, e.g., Wikipedia1,
must be utilized by QA systems. This requires
the design of rules or machine learning features
that exploit such knowledge by also satisfying
syntactic constraints, e.g., the semantic type of
the answer must match the question focus words.
The engineering of such rules for open domain
QA is typically very costly.
For instance, for
automatically deriving the correctness of the
answer passage in the following question/answer
passage (Q/AP) pair (from the TREC corpus2):
Q: What company owns the soft drink brand “Gatorade”?
A: Stokely-Van Camp bought the formula and started
marketing the drink as Gatorade in 1967. Quaker Oats Co.
took over Stokely-Van Camp in 1983.
1 
2It will be our a running example for the rest of the paper.
we would need to write the following complex
is(Quaker Oats Co.,company),
own(Stokely-Van Camp,Gatorade),
took over(Quaker Oats Co.,Stokely-Van Camp),
took over(Y, Z)→own(Z,Y),
and carry out logic uniﬁcation and resolution.
Therefore,
approaches that can automatically
generate patterns (i.e., features) from syntactic
and semantic representations of the Q/AP are
needed. In this respect, our previous work, e.g.,
 , has shown that tree
kernels for NLP, e.g., , can
exploit syntactic patterns for answer passage
reranking signiﬁcantly improving search engine
baselines. Our more recent work, , has shown that using automatically
produced semantic labels in shallow syntactic
trees, such as question category and question
focus, can further improve passage reranking and
answer extraction .
However, such methods cannot solve the class
of examples above as they do not use background
knowledge, which is essential to answer complex questions.
On the other hand, Kalyanpur
et al. and Murdock et al. showed
that semantic match features extracted from largescale background knowledge sources, including
the LOD ones, are beneﬁcial for answer reranking.
In this paper, we tackle the candidate answer
passage reranking task. We deﬁne kernel functions that can automatically learn structural patterns enriched by semantic knowledge, e.g., from
LOD. For this purpose, we carry out the following steps: ﬁrst, we design a representation for the
Q/AP pair by engineering a pair of shallow syntactic trees connected with relational nodes (i.e.,
Annotators
Question classifiers
Annotators
Question classifiers
syntactic/semantic
syntactic/semantic
train/test
Kernel-based
Kernel-based
Evaluation
Evaluation
UIMA pipeline
Search engine
Search engine
q/a similarity
q/a similarity
Wikipedia link
Wikipedia link
LOD type annotator
LOD type annotator
LOD datasets
LOD datasets
Figure 1: Kernel-based Answer Passage Reranking System
those matching the same words in the question and
in the answer passages).
Secondly, we use YAGO , DBpedia and Word-
Net to match constituents from
Q/AP pairs and use their generalizations in our
syntactic/semantic structures.
We employ word
sense disambiguation to match the right entities in
YAGO and DBpedia, and consider all senses of an
ambiguous word from WordNet.
Finally, we experiment with TREC QA and several models combining traditional feature vectors
with automatic semantic labels derived by statistical classiﬁers and relational structures enriched
with LOD relations.
The results show that our
methods greatly improve over strong IR baseline,
e.g., BM25, by 96%, and on our previous stateof-the-art reranking models, up to 15.4% (relative
improvement) in P@1.
Reranking with Tree Kernels
In contrast to ad-hoc document retrieval, structured representation of sentences and paragraphs
helps to improve question answering . Typically, rules considering syntactic and
semantic properties of the question and its candidate answer are handcrafted. Their modeling is in
general time-consuming and costly. In contrast,
we rely on machine learning and automatic feature engineering with tree kernels. We used our
state-of-the-art reranking models, i.e., as a baseline.
Our major difference with such approach is that
we encode knowledge and semantics in different
ways, using knowledge from LOD. The next sections outline our new kernel-based framework, although the detailed descriptions of the most innovative aspects such as new LOD-based representations are reported in Section 3.
Framework Overview
Our QA system is based on a rather simple reranking framework as displayed in Figure 1: given a
question Q, a search engine retrieves a list of candidate APs ranked by their relevancy. Next, the
question together with its APs are processed by
a rich NLP pipeline, which performs basic tokenization, sentence splitting, lemmatization, stopword removal.
Various NLP components, embedded in the pipeline as UIMA3 annotators, perform more involved linguistic analysis, e.g., POStagging, chunking, NE recognition, constituency
and dependency parsing, etc.
Each Q/AP pair is processed by a Wikipedia
link annotator.
It automatically recognizes ngrams in plain text, which may be linked to
Wikipedia and disambiguates them to Wikipedia
URLs. Given that question passages are typically
short, we concatenate them with the candidate answers to provide a larger disambiguation context
to the annotator.
These annotations are then used to produce
computational structures (see Sec. 2.2) input to the
reranker. The semantics of such relational structures can be further enriched by adding links between Q/AP constituents.
Such relational links
can be also generated by: (i) matching lemmas
as in ; (ii) matching the question focus type derived by the question classiﬁers with the type of the target NE as
in ; or (iii) by matching the
constituent types based on LOD (proposed in this
paper). The resulting pairs of trees connected by
semantic links are then used to train a kernel-based
reranker, which is used to re-order the retrieved
answer passages.
Relational Q/AP structures
We use the shallow tree representation that we
proposed in as a
baseline structural model. More in detail, each Q
and its candidate AP are encoded into two trees,
where lemmas constitute the leaf level, the partof-speech (POS) tags are at the pre-terminal level
and the sequences of POS tags are organized into
the third level of chunk nodes. We encoded structural relations using the REL tag, which links the
related structures in Q/AP, when there is a match
3 
Figure 2: Basic structural representations using a shallow chunk tree structure for the Q/AP in the running example. Curved
line indicates the tree fragments in the question and its answer passage linked by the relational REL tag.
between the lemmas in Q and AP. We marked the
parent (POS tags) and grand parent (chunk) nodes
of such lemmas by prepending a REL tag.
However, more general semantic relations, e.g.,
derived from the question focus and category, can
be encoded using the REL-FOCUS-<QC> tag,
where <QC> stands for the question class. In
 , we
used statistical classiﬁers to derive question focus
and categories of the question and of the named
entities in the AP. We again mark (i) the focus
chunk in the question and (ii) the AP chunks containing named entities of type compatible with the
question class, by prepending the above tags to
their labels. The compatibility between the categories of named entities and questions is evaluated with a lookup to a manually predeﬁned mapping ). We
also prune the trees by removing the nodes beyond
a certain distance (in terms of chunk nodes) from
the REL and REL-FOCUS nodes. This removes
irrelevant information and speeds up learning and
classiﬁcation. We showed that such model outperforms bag-of-words and POS-tag sequence models .
An example of a Q/AP pair encoded using shallow chunk trees is given in Figure 2. Here, for example, the lemma “drink” occurs in both Q and AP
(we highlighted it with a solid line box in the ﬁgure). “Company” was correctly recognized as a focus4, however it was misclassiﬁed as “HUMAN”
As no entities of the matching type
“PERSON” were found in the answer by a NER
system, no chunks were marked as REL-FOCUS
on the answer passage side.
We slightly modify the REL-FOCUS encoding into the tree.
Instead of prepending REL-
FOCUS-<QC>, we only prepend REL-FOCUS
to the target chunk node, and add a new node
QC as the rightmost child of the chunk node, e.g,
in Figure 2, the focus node would be marked as
REL-FOCUS and the sequence of its children
would be [WP NN HUM]. This modiﬁcation in-
4We used the same approach to focus detection and question classiﬁcation used in 
tends to reduce the feature sparsity.
LOD for Semantic Structures
We aim at exploiting semantic resources for building more powerful rerankers. More speciﬁcally,
we use structured knowledge about properties of
the objects referred to in a Q/AP pair. A large
amount of knowledge has been made available as
LOD datasets, which can be used for ﬁnding additional semantic links between Q/AP passages.
In the next sections, we (i) formally deﬁne novel
semantic links between Q/AP structures that we
introduce in this paper; (ii) provide basic notions
of Linked Open Data along with three of its most
widely used datasets, YAGO, DBpedia and Word-
Net; and, ﬁnally, (iii) describe our algorithm to
generate linked Q/AP structures.
Matching Q/AP Structures: Type Match
We look for token sequences (e.g., complex nominal groups) in Q/AP pairs that refer to entities and
entity classes related by isa (Eq. 1) and isSubclassOf (Eq. 2) relations and then link them in the
structural Q/AP representations.
isa : entity × class →{true, false}
isSubclassOf : class × class →{true, false}
Here, entities are all the objects in the world
both real or abstract, while classes are sets of entities that share some common features. Information about entities, classes and their relations can
be obtained from the external knowledge sources
such as the LOD resources. isa returns true if an
entity is an element of a class (false otherwise),
while isSubclassOf(class1,class2) returns true if
all elements of class1 belong also to class2.
We refer to the token sequences introduced
above as to anchors and the entities/classes they
refer to as references. We deﬁne anchors to be in
a Type Match (TM) relation if the entities/classes
they refer to are in isa or isSubclassOf relation.
More formally, given two anchors a1 and a2 belonging to two text passages, p1 and p2, respectively, and given an R(a, p) function, which returns a reference of an anchor a in passage p, we
deﬁne TM (r1, r2) as
isa (r1, r2) : if isEntity (r1) ∧isClass (r2)
subClassOf (r1, r2) : if isClass (r1) ∧isClass (r2)(3)
where r1 = R(a1, p1), r2 = R(a2, p2) and isEntity(r) and isClass(r) return true if r is an entity or
a class, respectively, and false otherwise. It should
be noted that, due to the ambiguity of natural language, the same anchor may have different references depending on the context.
LOD for linking Q/A structures
LOD consists of datasets published online according to the Linked Data (LD) principles5 and available in open access.
LOD knowledge is represented following the Resource Description Framework (RDF)6 speciﬁcation as a set of statements.
A statement is a subject-predicate-object triple,
where predicate denotes the directed relation, e.g.,
hasSurname or owns, between subject and object.
Each object described by RDF, e.g., a class or
an entity, is called a resource and is assigned a
Unique Resource Identiﬁer (URI).
LOD includes a number of common schemas,
i.e., sets of classes and predicates to be reused
when describing knowledge.
For example, one
of them is RDF Schema (RDFS)7, which contains
predicates rdf:type and rdfs:SubClassOf
similar to the isa and subClassOf functions above.
LOD contains a number of large-scale crossdomain datasets, e.g., YAGO and DBpedia . Datasets
created before the emergence of LD, e.g., Word-
Net, are brought into correspondence with the LD
principles and added to the LOD as well.
Algorithm for detecting TM
Algorithm 1 detects n-grams in the Q/AP structures that are in TM relation and encodes TM
knowledge in the shallow chunk tree representations of Q/AP pairs. It takes two text passages, P1
and P2, and a LOD knowledge source, LODKS,
as input. We run the algorithm twice, ﬁrst with
AP as P1 and Q as P2 and then vice versa. For
example, P1 and P2 in the ﬁrst run could be, according to our running example, Q and AP candidate, respectively, and LODKS could be YAGO,
DBpedia or WordNet.
Detecting anchors. getAnchors(P2,LODKS)
in line 1 of Algorithm 1 returns all anchors in the
5 
LinkedData.html
6 
7 
Algorithm 1 Type Match algorithm
Input: P1, P2 - text passages; LODKS - LOD knowledge
1: for all anchor ∈getAnchors(P2,LODKS) do
for all uri ∈getURIs(anchor,P2,LODKS) do
for all type ∈getTypes(uri,LODKS) do
for all ch ∈getChunks(P1) do
matchedTokens
checkMatch(ch,
type.labels)
if matchedTokens ̸= ∅then
markAsTM(anchor,P2.parseTree)
markAsTM(matchedTokens,
P1.parseTree)
given text passage, P2. Depending on LODKS
one may have various implementations of this procedure.
For example, when LODKS is Word-
Net, getAnchor returns token subsequences of the
chunks in P2 of lengths n-k, where n is the number
of tokens in the chunk and k = [1, .., n −1).
In case when LODKS is YAGO or DBpedia,
we beneﬁt from the fact that both YAGO and DBpedia are aligned with Wikipedia on entity level by
construction and we can use the so-called wikiﬁcation tools, e.g., , to detect
the anchors. The wikiﬁcation tools recognize ngrams that may denote Wikipedia pages in plain
text and disambiguate them to obtain a unique
Wikipedia page.
Such tools determine whether
a certain n-gram may denote a Wikipedia page(s)
by looking it up in a precomputed vocabulary created using Wikipedia page titles and internal link
network .
Obtaining references. In line 2 of Algorithm 1
for each anchor, we determine the URIs of entities/classes it refers to in LODKS. Here again,
we have different strategies for different LODKS.
In case of WordNet, we use the all-senses strategy, i.e., getURI procedure returns a set of URIs
of synsets that contain the anchor lemma.
In case when LODKS is YAGO or DBpedia,
we use wikiﬁcation tools to correctly disambiguate
an anchor to a Wikipedia page. Then, Wikipedia
page URLs may be converted to DBpedia URIs by
substituting the en.wikipedia.org/wiki/
preﬁx to the dbpedia.org/resource/; and
YAGO URIs by querying it for subjects of the
RDF triples with yago:hasWikipediaUrl8
as a predicate and Wikipedia URL as an object.
For instance, one of the anchors detected in
the running example AP would be “Quaker oats”,
8yago: is a shorthand for the http preﬁx http://
yago-knowledge.org/resource/
a wikiﬁcation tool would map it to wiki:
Quaker_Oats_Company9, and the respective
YAGO URI would be yago:Quaker_Oats_
Obtaining type information. Given a uri, if it
is an entity, we look for all the classes it belongs
to, or if it is a class, we look for all classes for
which it is a subclass. This process is incorporated in the getTypes procedure in line 3 of Algorithm 1. We call such classes types. If LODKS
is WordNet, then our types are simply the URIs of
the hypernyms of uri. If LODKS is DBpedia or
YAGO, we query these datasets for the values of
the rdf:type and rdfs:subClassOf properties of the uri (i.e., objects of the triples with uri
as subject and type/subClassOf as predicates) and
add their values (which are also URIs) to the types
set. Then, we recursively repeat the same queries
for each retrieved type URI and add their results to
the types. Finally, the getTypes procedure returns
the resulting types set.
The extracted URIs returned by getTypes are
HTTP ids, however, frequently they have humanreadable names, or labels, speciﬁed by the rdfs:
label property.
If no label information for a
URI is available, we can create the label by removing the technical information from the type
URI, e.g., http preﬁx and underscores. type.labels
denotes a set of type human-readable labels for
a speciﬁc type.
For example, one of the types
extracted for yago:Quaker_Oats_Company
would have label ”company”.
Checking for TM. Further, the checkMatch
procedure checks whether any of the labels in the
type.labels matches any of the chunks in P1 returned by getChunks, fully or partially (line 5 of
Algorithm 1). Here, getChunks procedure returns
a list of chunks recognized in P1 by an external
More speciﬁcally, given a chunk, ch, and a type
label, type.label, checkMatch checks whether the
ch string matches10 type.label or its last word(s).
If no match is observed, we remove the ﬁrst token from ch and repeat the procedure. We stop
when the match is observed or when no tokens
in ch are left. If the match is observed, check-
Match returns all the tokens remaining in ch as
matchedTokens. Otherwise, it returns an empty
set. For example, the question of the running ex-
9wiki: is a shorthand for the http preﬁx 
wikipedia.org/wiki/
10case-insensitive exact string match
ample contains the chunk “what company”, which
partially matches the human readable “company”
label of one of the types retrieved for the “Quaker
oats” anchor from the answer.
Our implementation of the checkMatch procedure would return “company” from the question as one of the
matchedTokens.
If the matchedTokens set is not empty,
 anchor, P2
 matchedTokens, P1
in Eq. 3 returns true.
Indeed, a1 is an anchor and a2 is the matched-
Tokens sequence (see Eq. 3), and their respective
references, i.e., URI assigned to the anchor and
URI of one of its types, are either in subClassOf
or in isa relation by construction. Naturally, this
is only one of the possible ways to evaluate the
TM function, and it may be noise-prone.
Marking TM in tree structures.
if the TM match is observed, i.e., matchedTokens is not an empty set, we mark tree substructures corresponding to the anchor in the structural representation of P2 (P2.parseTree) and
those corresponding to matchedTokens in that of
P1 (P1.parseTree) as being in a TM relation. In
our running example, we would mark the substructures corresponding to ”Quaker oats” anchor in the
answer (our P2) and the “company” matchedToken in the question (our P1) shallow syntactic tree
representations. We can encode TM match information into a tree in a variety of ways, which we
describe below.
Encoding TM knowledge in the trees
a1 and a2 from Eq. 3 are n-grams, therefore they
correspond to the leaf nodes in the shallow syntactic trees of p1 and p2. We denote the set of
their preterminal parents as NTM.
We considered the following strategies of encoding TM relation in the trees: (i) TM node (TMN). Add leaf
sibling tagged with TM to all the nodes in NTM.
(ii) Directed TM node (TMND). Add leaf sibling tagged with TM-CHILD to all the nodes in
NTM corresponding to the anchor, and leaf siblings tagged with TM-PARENT to the nodes corresponding to matchedTokens.
(iii) Focus TM
(TMNF ). Add leaf siblings to all the nodes in
NTM. If matchedTokens is a part of a question
focus label them as TM-FOCUS. Otherwise, label them as TM. (iv) Combo TMNDF . Encode
using the TMND strategy. If matchedTokens is a
part of a question focus label then also add a child
labeled FOCUS to each of the TM labels. Intu-
Figure 3: Fragments of a shallow chunk parse tree annotated in TMND mode.
itively, TMND, TMNF , TMNDF are likely to result in more expressive patterns. Fig. 3 shows an
example of the TMND annotation.
Wikipedia-based matching
Lemma matching for detecting REL may result in
low coverage, e.g., it is not able to match different variants for the same name. We remedy this
by using Wikipedia link annotation. We consider
two word sequences (in Q and AP, respectively)
that are annotated with the same Wikipedia link
to be in a matching relation. Thus, we add new
REL tags to Q/AP structural representations as described in Sec. 2.2.
Experiments
We evaluated our different rerankers encoding several semantic structures on passage retrieval task,
using a factoid open-domain TREC QA corpus.
Experimental Setup
TREC QA 2002/2003. In our experiments, we
opted for questions from years 2002 and 2003,
which totals to 824 factoid questions.
AQUAINT corpus11 is used for searching the supporting passages.
Following we prune the shallow trees by removing the
nodes beyond distance of 2 from the REL, REL-
FOCUS or TM nodes.
LOD datasets. We used the core RDF distribution of YAGO212, WordNet 3.0 in RDF13, and the
datasets from the 3.9 DBpedia distribution14.
Feature Vectors.
We used a subset of the similarity functions between Q and AP described in
 .
These are used along
with the structural models. More explicitly: Termoverlap features:
i.e., a cosine similarity over
question/answer, simCOS(Q, AP), where the input vectors are composed of lemma or POS-tag
11 
LDC2002T31
12 
yago1_yago2/download/yago2/yago2core_
20120109.rdfs.7z
13 
14 
n-grams with n = 1, .., 4. PTK score: i.e., output of the Partial Tree Kernel (PTK), deﬁned in
 , when applied to the structural
representations of Q and AP, simPTK(Q, AP) =
PTK(Q, AP) in SVM-light
 . We use default parameters and
the preference reranking model described in .
We used PTK and the polynomial kernel of degree
3 on standard features.
Pipeline. We built the entire processing pipeline
on top of the UIMA framework.We included many
off-the-shelf NLP tools wrapping them as UIMA
annotators to perform sentence detection, tokenization, NE Recognition, parsing, chunking and
lemmatization.
Moreover, we used annotators
for building new sentence representations starting
from tools’ annotations and classiﬁers for question
focus and question class.
Search engines. We adopted Terrier16 using the
accurate BM25 scoring model with default parameters. We trained it on the TREC corpus (3Gb),
containing about 1 million documents. We performed indexing at the paragraph level by splitting
each document into a set of paragraphs, which are
then added to the search index. We retrieve a list of
50 candidate answer passages for each question.
annotators.
Wikipedia Miner (WM) 17 tool and the Machine Linking (ML)18
web-service to annotate Q/AP pairs with links to
Wikipedia.
Both tools output annotation conﬁdence. We use all WM and ML annotations with
conﬁdence exceeding 0.2 and 0.05, respectively.
We obtained these ﬁgures heuristically, they are
low because we aimed to maximize the Recall of
the Wikipedia link annotators in order to maxi-
15 
Tree-Kernel.htm
16 
17 
wikipedia-miner/files/wikipedia-miner/
wikipedia-miner_1.1, we use only topic detector
module which detects and disambiguates anchors
18 
28.02±2.94
18.17±3.79
CH+V 
CH+V+QC+TFC
 
36.82±2.68
26.34±2.17
CH + V+ QC+TFC
40.20±1.84
30.85±2.35
CH+V+QC+TFC*
40.50±2.32
31.46±2.42
Table 1: Baseline systems
mize the number of TMs. In all the experiments,
we used a union of the sets of the annotations provided by WM and ML.
Metrics. We used common QA metrics: Precision
at rank 1 (P@1), i.e., the percentage of questions
with a correct answer ranked at the ﬁrst position,
and Mean Reciprocal Rank (MRR). We also report
the Mean Average Precision (MAP). We perform
5-fold cross-validation and report the metrics averaged across all the folds together with the std.dev.
Baseline Structural Reranking
In these experiments, we evaluated the accuracy
of the following baseline models: BM25 is the
BM25 scoring model, which also provides the initial ranking; CH+V is a combination of tree structures encoding Q/AP pairs using relational links
with the feature vector; and CH+V+QC+TFC is
CH+V extended with the semantic categorial links
introduced in .
Table 1 reports the performance of our baseline systems.
The lines marked with contain the results we reported in
 .
Lines four and ﬁve report
the performance of the same systems, i.e., CH+V
and CH+V+QC+TFC, after small improvement
and changes.
Note that in our last version, we
have a different set of V features than in . Finally, CH+V+QC+TFC* refers to the
performance of CH+V+QC+TFC with question
type information of semantic REL-FOCUS links
represented as a distinct node (see Section 2.2).
The results show that this modiﬁcation yields a
slight improvement over the baseline, thus, in
the next experiments, we add LOD knowledge to
CH+V+QC+TFC*.
Impact of LOD in Semantic Structures
These experiments evaluated the accuracy of the
following models (described in the previous sections): (i) a system using Wikipedia to establish
the REL links; and (ii) systems which use LOD
knowledge to ﬁnd type matches (TM).
The ﬁrst header line of the Table 2 shows which
baseline system was enriched with the TM knowledge. Type column reports the TM encoding strategy employed (see Section 3.2.2). Dataset column
reports which knowledge source was employed to
ﬁnd TM relations. Here, yago is YAGO2, db is
DBpedia, and wn is WordNet 3.0. The ﬁrst result line in Table 2 reports the performance of
the strong CH+V and CH+V+QC+TFC* baseline systems.
Line with the “wiki” dataset reports on CH+V and CH+V+QC+TFC* using
both Wikipedia link annotations provided by ML
and MW and hard lemma matching to ﬁnd the related structures to be marked by REL (see Section 3.3 for details of the Wikipedia-based REL
matching). The remainder of the systems is built
on top of the baselines using both hard lemma and
Wikipedia-based matching. We used bold font to
mark the top scores for each encoding strategy.
The tables show that all the systems exploiting LOD knowledge, excluding those using DBpedia only, outperform the strong CH+V
and CH+V+QC+TFC* baselines.
CH+V enriched with TM tags performs comparably to, and in some cases even outperforms, CH+V+QC+TFC*. Compare, for example, the outputs of CH+V+TMNDF using YAGO,
WordNet and DBpedia knowledge and those of
CH+V+QC+TFC* with no LOD knowledge.
Adding TM tags to the top-performing baseline system, CH+V+QC+TFC*, typically results in further increase in performance.
best-performing system in terms of MRR and
P@1 is CH+V+QC+TFC*+TMNF system using the combination of WordNet and YAGO2 as
source of TM knowledge and Wikipedia for RELmatching. It outperforms the CH+V+QC+TFC*
baseline by 3.82% and 4.15% in terms of MRR
and P@1, respectively. Regarding MAP, a number of systems employing YAGO2 in combination with WordNet and Wikipedia-based RELmatching obtain 0.37 MAP score thus outperforming the CH+V+QC+TFC* baseline by 4%.
We used paired two-tailed t-test for evaluating
the statistical signiﬁcance of the results reported in
Table 2. ‡ and † correspond to the signiﬁcance levels of 0.05 and 0.1, respectively. We compared (i)
the results in the wiki line to those in the none line;
and (ii) the results for the TM systems to those in
the wiki line.
The table shows that we typically obtain better results when using YAGO2 and/or WordNet.
In our intuition this is due to the fact that these
resources are large-scale, have ﬁne-grained class
CH + V + QC + TFC*
36.82±2.68
26.34±2.17
40.50±2.32
31.46±2.42
39.17±1.29‡
0.31±0.01‡
28.66±1.43‡
41.33±1.17
31.46±1.40
40.60±1.88
0.33±0.01‡
31.10±2.99†
40.80±1.01
30.37±1.90
41.39±1.96‡
0.33±0.01‡
31.34±2.94
42.43±0.56
32.80±0.67
40.85±1.52‡
0.33±0.01‡
30.37±2.34
42.37±1.12
32.44±2.64
40.71±2.07
0.33±0.03†
30.24±2.09‡
43.28±1.91†
0.36±0.01†
33.90±2.75
41.25±1.57‡
0.34±0.02‡
31.10±1.88‡
42.39±1.83
32.93±3.14
42.01±2.26‡
0.34±0.02‡
32.07±3.04‡
43.98±1.08‡
0.36±0.01‡
35.24±1.46‡
yago+wn+db
41.52±1.85‡
0.34±0.02‡
30.98±2.71‡
43.13±1.38
33.66±2.77
40.67±1.94†
0.33±0.01‡
30.85±2.22†
41.43±0.70
31.22±1.09
40.95±2.27‡
0.33±0.01‡
30.98±3.74
42.37±0.98
32.56±1.76
40.84±2.18†
0.34±0.01‡
30.73±3.04
43.08±0.83†
0.36±0.01†
33.54±1.29†
42.01±2.44†
0.34±0.02‡
32.07±3.01‡
43.82±2.36†
0.36±0.02‡
34.88±3.35
41.32±1.70†
0.34±0.02‡
31.10±2.48‡
43.19±1.17†
0.36±0.01†
33.90±1.86
41.69±1.66‡
0.34±0.02‡
31.10±2.44‡
44.32±0.70‡
0.36±0.01‡
35.61±1.11‡
yago+wn+db
41.56±1.41‡
0.34±0.02‡
30.85±2.22†
43.79±0.73‡
0.37±0.01†
34.88±1.69‡
40.37±1.87
0.33±0.01‡
30.37±2.17
41.58±1.02
0.35±0.01†
31.46±1.59
41.13±2.14‡
0.33±0.01‡
30.73±2.75
42.19±1.39
32.32±1.36
41.28±1.03‡
0.34±0.01‡
30.73±0.82‡
42.37±1.16
32.44±2.71
42.11±3.24‡
0.34±0.02‡
32.07±4.06†
44.04±2.05‡
0.36±0.01‡
34.63±2.17‡
42.28±2.01‡
0.35±0.01‡
32.44±1.99‡
43.77±2.02†
0.37±0.01†
34.27±2.42
42.96±1.45‡
0.35±0.01‡
33.05±2.04‡
44.25±1.32‡
0.37±0.00‡
34.76±1.61‡
yago+wn+db
42.56±1.25‡
0.35±0.01‡
32.56±1.91‡
43.91±1.01‡
0.37±0.01†
34.63±1.32‡
40.40±1.93†
0.33±0.01‡
30.49±1.78‡
41.85±1.05
0.35±0.01‡
31.83±0.80
40.84±1.69‡
0.33±0.01‡
30.49±2.24
41.89±0.99
31.71±0.86
41.14±1.29‡
0.34±0.01‡
30.73±1.40‡
42.31±0.92
32.32±2.36
42.31±2.57‡
0.35±0.02‡
32.68±3.01‡
44.22±2.38‡
0.37±0.02‡
35.00±2.88‡
41.96±1.82‡
0.35±0.01‡
32.32±2.24‡
43.82±1.95‡
0.37±0.01‡
34.51±2.39†
42.80±1.19‡
0.35±0.01‡
33.17±1.86‡
43.91±0.98‡
0.37±0.01‡
34.63±0.90‡
yago+wn+db
43.15±0.93‡
0.35±0.01‡
33.78±1.59‡
43.96±0.94‡
0.37±0.01‡
34.88±1.69‡
Table 2: Results in 5-fold cross-validation on TREC QA corpus
taxonomy and contain many synonymous labels
per class/entity thus allowing us to have a good
coverage with TM-links. DBpedia ontology that
we employed in the db experiments is more shallow and contains fewer labels for classes, therefore the amount of discovered TM matches is
not always sufﬁcient for increasing performance.
YAGO2 provides better coverage for TM relations
between entities and their classes, while Word-
Net contains more relations between classes19.
Note that in , we
also used supersenses of WordNet (unsuccessfully) whereas here we use hypernymy relations
and a different technique to incorporate semantic
match into the tree structures.
Different TM-knowledge encoding strategies,
TMN, TMND, TMNF , TMNDF produce small
changes in accuracy. We believe, that the difference between them would become more signiﬁcant when experimenting with larger corpora.
Conclusions
This paper proposes syntactic structures whose
nodes are enriched with semantic information
from statistical classiﬁers and knowledge from
LOD. In particular, YAGO, DBpedia and Word-
Net are used to match and generalize constituents
from QA pairs: such matches are then used in
19We consider the WordNet synsets to be classes in the
scope of our experiments
syntactic/semantic structures.
The experiments
with TREC QA and the above representations
also combined with traditional features greatly improve over a strong IR baseline, e.g., 96% on
BM25, and on previous state-of-the-art reranking models, up to 15.4% (relative improvement)
in P@1. In particular, differently from previous
work, our models can effectively use semantic
knowledge in statistical learning to rank methods.
These promising results open interesting future
directions in designing novel semantic structures
and using innovative semantic representations in
learning algorithms.
Acknowledgments
This research is partially supported by the EU’s 7th
Framework Program (#288024
LIMOSINE project) and by a Shared University
Research award from the IBM Watson Research
Center - Yorktown Heights, USA and the IBM
Center for Advanced Studies of Trento, Italy. The
third author is supported by the Google Europe
Fellowship 2013 award in Machine Learning.