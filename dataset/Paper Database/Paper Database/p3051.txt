MIT Open Access Articles
TSM: Temporal Shift Module for Efficient Video Understanding
The MIT Faculty has made this article openly available. Please share
how this access benefits you. Your story matters.
Citation: Lin, Ji, Gan, Chuang and Han, Song. 2019. "TSM: Temporal Shift Module for Efficient
Video Understanding." Proceedings of the IEEE International Conference on Computer Vision,
2019-October.
As Published: 10.1109/ICCV.2019.00718
Publisher: IEEE
Persistent URL: 
Version: Author's final manuscript: final author's manuscript post peer review, without
publisher's formatting or copy editing
Terms of Use: Article is made available in accordance with the publisher's policy and may be
subject to US copyright law. Please refer to the publisher's site for terms of use.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
TSM: Temporal Shift Module for Efﬁcient and
Scalable Video Understanding on Edge Devices
Ji Lin, Chuang Gan, Kuan Wang and Song Han
Abstract—The explosive growth in video streaming requires video understanding at high accuracy and low computation cost.
Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good
performance but are computationally intensive. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys
both high efﬁciency and high performance. The key idea of TSM is to shift part of the channels along the temporal dimension, thus
facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero
computation and zero parameters. TSM offers several unique advantages. Firstly, TSM has high performance; it ranks the ﬁrst on the
Something-Something leaderboard upon submission. Secondly, TSM has high efﬁciency; it achieves a high frame rate of 74fps and 29fps
for online video recognition on Jetson Nano and Galaxy Note8. Thirdly, TSM has higher scalability compared to 3D networks, enabling
large-scale Kinetics training on 1,536 GPUs in 15 minutes. Lastly, TSM enables action concepts learning, which 2D networks cannot
model; we visualize the category attention map and ﬁnd that spatial-temporal action detector emerges during the training of classiﬁcation
tasks. The code is publicly available at 
Index Terms—Temporal Shift Module, Video Recognition, Video Object Detection, Distributed Training, Edge Device, Network Dissection.
INTRODUCTION
ARDWARE-EFFICIENT video understanding is an important
step towards real-world deployment, both on the cloud and on
the edge. For example, there are over 105 hours of videos uploaded
to YouTube every day to be processed for recommendation and
ads ranking; tera-bytes of sensitive videos in hospitals need to be
processed locally on edge devices to protect privacy. All these
industry applications require both accurate and efﬁcient video
understanding.
Deep learning has become the standard for video understanding
over the years , , , , , , . One key
difference between video recognition and image recognition is the
need for temporal modeling. For example, to distinguish between
opening and closing a box, reversing the order will give opposite
results, so temporal modeling is critical. Existing efﬁcient video
understanding approaches directly use 2D CNN , , ,
 . However, 2D CNN on individual frames could not capture
the temporal information very well. 3D CNNs , can jointly
learn spatial and temporal features but the computation cost is
large, making the deployment on edge devices difﬁcult; it cannot
be applied to real-time online video recognition. There are works to
trade off between temporal modeling and computation, such as posthoc fusion , , , and mid-level temporal fusion ,
 , . Such methods sacriﬁce the low-level temporal modeling
for efﬁciency, but much of the useful information is lost during the
feature extraction before the temporal fusion happens.
In this paper, we propose a new perspective for efﬁcient
temporal modeling in video understanding by proposing a novel
Temporal Shift Module (TSM). Concretely, an activation in a video
model can be represented as A ∈RN×C×T ×H×W , where N is
J. Lin, K. Wang, S. Han are with the Department of Electrical Engineering
and Computer Science, Massachusetts Institute of Technology.
E-mail: {jilin, kuanwang, songhan}@mit.edu
C. Gan is with MIT-IBM Watson AI Lab.
E-mail: 
Temporal T
(a) The original tensor
without shift.
temporal shift
shift (bi-direction).
(c) Online temporal shift
(uni-direction).
Fig. 1. Temporal Shift Module (TSM) performs efﬁcient temporal
modeling by moving the feature map along the temporal dimension.
It is computationally free on top of a 2D convolution, but achieves
strong temporal modeling ability. TSM efﬁciently supports both ofﬂine
and online video recognition. Bi-directional TSM mingles both past
and future frames with the current frame, which is suitable for highthroughput ofﬂine video recognition. Uni-directional TSM mingles
only the past frame with the current frame, which is suitable for
low-latency online video recognition.
the batch size, C is the number of channels, T is the temporal
dimension, H and W are the spatial resolutions. Traditional 2D
CNNs operate independently over the dimension T; thus no
temporal modeling takes effects (Figure 1a). In contrast, our
Temporal Shift Module (TSM) shifts the channels along the
temporal dimension, both forward and backward. As shown in
Figure 1b, the information from neighboring frames is mingled with
the current frame after shifting. Our intuition is: the convolution
operation consists of shift and multiply-accumulate. We shift in
the time dimension by ±1 and fold the multiply-accumulate from
time dimension to channel dimension. For real-time online video
understanding, future frames can’t get shifted to the present, so
we use a uni-directional TSM (Figure 1c) to perform online video
understanding.
Despite the zero-computation nature of the shift operation, we
empirically ﬁnd that simply adopting the spatial shift strategy 
 
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
used in image classiﬁcations introduces two major issues for video
understanding: (1) it is not efﬁcient: shift operation is conceptually
zero FLOP but incurs data movement. The additional cost of data
movement is non-negligible and will result in latency increase. This
phenomenon has been exacerbated in the video networks since
they usually have a large memory consumption (5D activation).
(2) It is not accurate: shifting too many channels in a network
will signiﬁcantly hurt the spatial modeling ability and result
in performance degradation. To tackle the problems, we make
two technical contributions. (1) We use a temporal partial shift
strategy: instead of shifting all the channels, we shift only a small
portion of the channels for efﬁcient temporal fusion. Such strategy
signiﬁcantly cuts down the data movement cost (Figure 2a). (2) We
insert TSM inside residual branch rather than outside so that the
activation of the current frame is preserved, which does not harm
the spatial feature learning capability of the 2D CNN backbone.
To verify the effectiveness of TSM, we carried out comprehensive experiments: (1) we show that TSM consistently improve
the video recognition performance compared to 2D model without
incurring extra computation or parameters; it also achieves stateof-the-art performance on multiple action recognition dataset; (2)
TSM has much better accuracy-computation trade-off compared
to prior works. The contributions of our paper are summarized as
• We provide a new perspective for efﬁcient video model design
by temporal shift, which is computationally free but has strong
spatio-temporal modeling ability.
• We observed that naive shift cannot achieve high efﬁciency or
high performance. We then proposed two technical modiﬁcations partial shift and residual shift to realize a high efﬁciency
model design.
• We propose bi-directional TSM for ofﬂine video understanding
that achieves state-of-the-art performance. It ranks the ﬁrst on
Something-Something leaderboard upon publication.
• We propose uni-directional TSM for online real-time video
recognition with strong temporal modeling capacity at low
latency on edge devices.
• With the efﬁcient design of TSM, we scale up the training
of video network to 1,536 GPUs, and ﬁnish the training on
Kinetics dataset in 15 minutes, without losing accuracy. To
the best of our knowledge, we are the ﬁrst to systematically
investigate the large-scale training on video recognition.
• We provide an in-depth analysis to understand the learned
knowledge inside the TSM module, and ﬁnd that spatialtemporal action detector automatically emerges during training
using only classiﬁcation labels.
RELATED WORK
In this section, we brieﬂy review four related topics: 1) Deep
Video Recognition, 2) Temporal Modeling, and 3) Efﬁcient Neural
Deep Video Recognition
Using the 2D CNN is a straightforward way to conduct video
recognition , , , , , , . For example,
Simonyan et al. designed a two-stream CNN for RGB input
(spatial stream) and optical ﬂow input (temporal stream)
respectively. Temporal Segment Networks (TSN) extracted
averaged features from strided sampled frames. Such methods are
more efﬁcient compared to 3D counterparts but cannot infer the
temporal order or more complicated temporal relationships.
3D convolutional neural networks can jointly learn spatio-temporal
features. Tran et al. proposed a 3D CNN based on VGG
models, named C3D, to learn spatio-temporal features from a
frame sequence. Carreira and Zisserman proposed to inﬂate
all the 2D convolution ﬁlters in an Inception V1 model into
3D convolutions. However, 3D CNNs are computationally heavy,
making the deployment difﬁcult. They also have more parameters
than 2D counterparts, thus are more prone to over-ﬁtting. On the
other hand, our TSM has the same spatial-temporal modeling ability
as 3D CNN while enjoying the same computation and parameters
as the 2D CNNs.
Trade-offs.
There have been attempts to trade off expressiveness and computation costs. Lee et al. proposed a motion ﬁlter to generate
spatio-temporal features from 2D CNN. Tran et al. and Xie et
al. proposed to study mixed 2D and 3D networks, either ﬁrst
using 3D and later 2D (bottom-heavy) or ﬁrst 2D and later 3D
(top-heavy) architecture. ECO also uses a similar top-heavy
architecture to achieve a very efﬁcient framework. Another way
to save computation is to decompose the 3D convolution into a
2D spatial convolution and a 1D temporal convolution , ,
 . For mixed 2D-3D CNNs, they still need to remove low-level
temporal modeling or high-level temporal modeling. Compared
to decomposed convolutions, our method completely removes the
computation cost of temporal modeling has enjoys better hardware
efﬁciency.
Temporal Modeling
A direct way for temporal modeling is to use 3D CNN based
methods as discussed above. Wang et al. proposed a spatialtemporal non-local module to capture long-range dependencies.
Wang et al. proposed to represent videos as space-time region
graphs. An alternative way to model the temporal relationships
is to use 2D CNN + post-hoc fusion , , , . Some
works use LSTM to aggregate the 2D CNN features , ,
 , , . Attention mechanism also proves to be effective
for temporal modeling , , . Zhou et al. proposed
Temporal Relation Network to learn and reason about temporal
dependencies. The former category is computational heavy, while
the latter cannot capture the useful low-level information that is lost
during feature extraction. Our method offers an efﬁcient solution at
the cost of 2D CNNs, while enabling both low-level and high-level
temporal modeling, just like 3D-CNN based methods.
Efﬁcient Neural Networks
The efﬁciency of 2D CNN has been extensively studied. Some
works focused on designing an efﬁcient model , , ,
 . Recently neural architecture search , , has
been introduced to ﬁnd an efﬁcient architecture automatically ,
 . Another way is to prune, quantize and compress an existing
model for efﬁcient deployment , , , , , .
Address shift, which is a hardware-friendly primitive, has also
been exploited for compact 2D CNN design on image recognition
tasks , . Nevertheless, we observe that directly adopting
the shift operation on video recognition task neither maintains
efﬁciency nor accuracy, due to the complexity of the video data.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
TEMPORAL SHIFT MODULE (TSM)
We ﬁrst explain the intuition behind TSM: data movement and
computation can be separated in a convolution. However, we
observe that such naive shift operation neither achieves high
efﬁciency nor high performance. To tackle the problem, we propose
two techniques minimizing the data movement and increasing the
model capacity, which leads to the efﬁcient TSM module.
Let us ﬁrst consider a normal convolution operation. For brevity,
we used a 1-D convolution with the kernel size of 3 as an example.
Suppose the weight of the convolution is W = (w1, w2, w3), and
the input X is a 1-D vector with inﬁnite length. The convolution
operator Y = Conv(W, X) can be written as: Yi = w1Xi−1 +
w2Xi + w3Xi+1. We can decouple the operation of convolution
into two steps: shift and multiply-accumulate: we shift the input X
by −1, 0, +1 and multiply by w1, w2, w3 respectively, which sum
up to be Y . Formally, the shift operation is:
and the multiply-accumulate operation is:
Y = w1X−1 + w2X0 + w3X+1
The ﬁrst step shift can be conducted without any multiplication.
While the second step is more computationally expensive, our
Temporal Shift module merges the multiply-accumulate into the
following 2D convolution, so it introduces no extra cost compared
to 2D CNN based models.
The proposed Temporal Shift module is described in Figure 1.
In Figure 1a, we describe a tensor with C channels and T frames.
The features at different time stamps are denoted as different colors
in each row. Along the temporal dimension, we shift part of the
channels by −1, another part by +1, leaving the rest un-shifted
(Figure 1b). For online video recognition setting, we also provide
an online version of TSM (Figure 1c). In the online setting, we
cannot access future frames, therefore, we only shift from past
frames to future frames in a uni-directional fashion.
Naive Shift Does Not Work
Despite the simple philosophy behind the proposed module,
we ﬁnd that directly applying the spatial shift strategy to
the temporal dimension cannot provide high performance nor
efﬁciency. To be speciﬁc, if we shift all or most of the channels,
it brings two disasters: (1) Worse efﬁciency due to large data
movement. The shift operation enjoys no computation, but it
involves data movement. Data movement increases the memory
footprint and inference latency on hardware. Worse still, such
effect is exacerbated in the video understanding networks due
to large activation size (5D tensor). When using the naive shift
strategy shifting every map, we observe a 13.7% increase in
CPU latency and 12.4% increase in GPU latency, making the
overall inference slow. (2) Performance degradation due to
worse spatial modeling ability. By shifting part of the channels
to neighboring frames, the information contained in the channels
is no longer accessible for the current frame, which may harm the
spatial modeling ability of the 2D CNN backbone. We observe a
2.6% accuracy drop when using the naive shift implementation
compared to the 2D CNN baseline (TSN).
Module Design
To tackle the two problem from naive shift implementation, we
discuss two technical contributions.
Naive shift:
large overhead
Latency Overhead
Shift Proportion
Our Choice
(a) Overhead vs. proportion.
In-place TSM
Residual TSM
Naive shift:
Shift Proportion
Our Choice
2D baseline
(b) Residual vs. in-place.
Fig. 2. (a) Latency overhead of TSM due to data movement. (b)
Residual TSM achieve better performance than in-place shift. We
choose 1/4 proportion residual shift as our default setting. It achieves
higher accuracy with a negligible overhead.
Reducing Data Movement.
To study the effect of data movement, we ﬁrst measured the
inference latency of TSM models and 2D baseline on different
hardware devices. We shifted different proportion of the channels
and measured the latency. We measured models with ResNet-50
backbone and 8-frame input using no shift (2D baseline), partial
shift (1/8, 1/4, 1/2) and all shift (shift all the channels). The
timing was measure on server GPU (NVIDIA Tesla P100), mobile
GPU (NVIDIA Jetson TX2) and CPU (Intel Xeon E5-2690). We
report the average latency from 1000 runs after 200 warm-up runs.
We show the overhead of the shift operation as the percentage of
the original 2D CNN inference time in 2a. We observe the same
overhead trend for different devices. If we shift all the channels, the
latency overhead takes up to 13.7% of the inference time on CPU,
which is deﬁnitely non-negligible during inference. On the other
hand, if we only shift a small proportion of the channels, e.g., 1/8,
we can limit the latency overhead to only 3%. Therefore, we use
partial shift strategy in our TSM implementation to signiﬁcantly
bring down the memory movement cost.
Keeping Spatial Feature Learning Capacity.
We need to balance the model capacity for spatial feature learning
and temporal feature learning. A straight-forward way to apply
TSM is to insert it before each convolutional layer or residual
block, as illustrated in Figure 3a. We call such implementation
in-place shift. It harms the spatial feature learning capability of
the backbone model, especially when we shift a large amount of
channels, since the information stored in the shifted channels is
lost for the current frame.
To address such issue, we propose a variant of the shift module.
Instead of inserting it in-place, we put the TSM inside the residual
branch in a residual block. We denote such version of shift as
residual shift as shown in 3b. Residual shift can address the
degraded spatial feature learning problem, as all the information in
the original activation is still accessible after temporal shift through
identity mapping.
To verify our assumption, we compared the performance of
in-place shift and residual shift on Kinetics dataset. We studied
the experiments under different shift proportion setting. The results
are shown in 2b. We can see that residual shift achieves better
performance than in-place shift for all shift proportion. Even we
shift all the channels to neighboring frames, due to the shortcut
connection, residual shift still achieves better performance than the
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
(a) In-place TSM.
(b) Residual TSM.
Fig. 3. Residual shift is better than in-place shift. In-place shift happens
before a convolution layer (or a residual block). Residual shift fuses
temporal information inside a residual branch.
2D baseline. Another ﬁnding is that the performance is related to
the proportion of shifted channels: if the proportion is too small,
the ability of temporal reasoning may not be enough to handle
complicated temporal relationships; if too large, the spatial feature
learning ability may be hurt. For residual shift, we found that the
performance reaches the peak when 1/4 (1/8 for each direction)
of the channels are shifted. Therefore, we use this setting for the
rest of the paper.
TSM VIDEO NETWORK
Ofﬂine Models with Bi-directional TSM
We insert bi-directional TSM to build ofﬂine video recognition
models. Given a video V , we ﬁrst sample T frames Fi, F1, ..., FT
from the video. After frame sampling, 2D CNN baselines process
each of the frames individually, and the output logits are averaged
to give the ﬁnal prediction. Our proposed TSM model has exactly
the same parameters and computation cost as 2D model. During
the inference of convolution layers, the frames are still running
independently just like the 2D CNNs. The difference is that
TSM is inserted for each residual block, which enables temporal
information fusion at no computation. For each inserted temporal
shift module, the temporal receptive ﬁeld will be enlarged by 2,
as if running a convolution with the kernel size of 3 along the
temporal dimension. Therefore, our TSM model has a very large
temporal receptive ﬁeld to conduct highly complicated temporal
modeling. In this paper, we used ResNet-50 as the backbone
unless otherwise speciﬁed.
A unique advantage of TSM is that it can easily convert any
off-the-shelf 2D CNN model into a pseudo-3D model that can
handle both spatial and temporal information, without adding
additional computation. Thus the deployment of our framework
is hardware friendly: we only need to support the operations in
2D CNNs, which are already well-optimized at both framework
level (CuDNN , MKL-DNN, TVM ) and hardware level
(CPU/GPU/TPU/FPGA).
Online Models with Uni-directional TSM
Video understanding from online video streams is important in reallife scenarios. Many real-time applications requires online video
recognition with low latency, such as AR/VR and self-driving. In
this section, we show that we can adapt TSM to achieve online
video recognition while with multi-level temporal fusion.
As shown in Figure 1, ofﬂine TSM shifts part of the channels
bi-directionally, which requires features from future frames to
replace the features in the current frame. If we only shift the
feature from previous frames to current frames, we can achieve
online recognition with uni-directional TSM.
The inference graph of uni-directional TSM for online video
recognition is shown in Figure 4. During inference, for each frame,
we save the ﬁrst 1/8 feature maps of each residual block and cache
it in the memory. For the next frame, we replace the ﬁrst 1/8 of
the current feature maps with the cached feature maps. We use the
Fig. 4. Uni-directional TSM for online video recognition.
combination of 7/8 current feature maps and 1/8 old feature maps
to generate the next layer, and repeat. Using the uni-directional
TSM for online video recognition shares several unique advantages:
1. Low latency inference. For each frame, we only need to
replace and cache 1/8 of the features, without incurring any extra
computations. Therefore, the latency of giving per-frame prediction
is almost the same as the 2D CNN baseline. Existing methods
like use multiple frames to give one prediction, which may
leads to large latency.
2. Low memory consumption. Since we only cache a small
portion of the features in the memory, the memory consumption is
low. For ResNet-50, we only need 0.9MB memory cache to store
the intermediate feature.
3. Multi-level temporal fusion. Most of the online method
only enables late temporal fusion after feature extraction like 
or mid level temporal fusion , while our TSM enables all levels
of temporal fusion. Through experiments (Table 2) we ﬁnd that
multi-level temporal fusion is very important for complex temporal
EXPERIMENTS
We ﬁrst show that TSM can signiﬁcantly improve the performance
of 2D CNN on video recognition while being computationally
free and hardware efﬁcient. It further demonstrated state-of-the-art
performance on temporal-related datasets, arriving at a much better
accuracy-computation pareto curve. TSM models achieve an order
of magnitude speed up in measured GPU throughput compared
to conventional I3D model from . Finally, we leverage unidirectional TSM to conduct low-latency and real-time online
prediction on both video recognition and object detection.
Training & Testing.
We conducted experiments on video action recognition tasks. The
training parameters for the Kinetics dataset are: 100 training epochs,
initial learning rate 0.01 (decays by 0.1 at epoch 40&80), weight
decay 1e-4, batch size 64, and dropout 0.5. For other datasets,
we scale the training epochs by half. For most of the datasets,
the model is ﬁne-tuned from ImageNet pre-trained weights; while
HMDB-51 and UCF-101 are too small and prone to
over-ﬁtting , we followed the common practice , to
ﬁne-tune from Kinetics pre-trained weights and freeze the
Batch Normalization layers. For testing, when pursue high
accuracy, we followed the common setting in , to sample
multiple clips per video (10 for Kinetics, 2 for others) and use the
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Less Temporal
More Temporal
TABLE 1. Our method consistently outperforms 2D counterparts on
multiple datasets at zero extra computation (protocol: ResNet-50 8f
input, 10 clips for Kinetics, 2 for others, full-resolution).
full resolution image with shorter side 256 for evaluation, so that
we can give a direct comparison; when we consider the efﬁciency
(e.g., as in Table 2), we used just 1 clip per video and the center
224×224 crop for evaluation. We keep the same protocol for the
methods compared in the same table.
To have an apple-to-apple comparison with the state-of-the-art
method , we used the same backbone (ResNet-50) on the
dataset ( Something-Something-V1 ).This dataset focuses on
temporal modeling. The difference is that used 3D ResNet-
50, while we used 2D ResNet-50 as the backbone to demonstrate
efﬁciency.
Kinetics dataset is a large-scale action recognition dataset
with 400 classes. As pointed in , , datasets like Something-
Something (V1&V2) , Charades , and Jester are
more focused on modeling the temporal relationships , while
UCF101 , HMDB51 , and Kinetics are less sensitive to
temporal relationships. Since TSM focuses on temporal modeling,
we mainly focus on datasets with stronger temporal relationships
like Something-Something. Nevertheless, we also observed strong
results on the other datasets and reported it.
Improving 2D CNN Baselines
We can seamlessly inject TSM into a normal 2D CNN and
improve its performance on video recognition. In this section,
we demonstrate a 2D CNN baseline can signiﬁcantly beneﬁt from
TSM with double-digits accuracy improvement. We chose TSN 
as the 2D CNN baseline. We used the same training and testing
protocol for TSN and our TSM. The only difference is with or
without TSM.
Comparing Different Datasets.
We compare the results on several action recognition datasets in
Table 1. The chart is split into two parts. The upper part contains
datasets Kinetics , UCF101 , HMDB51 , where temporal relationships are less important, while our TSM still consistently
outperforms the 2D TSN baseline at no extra computation. For
the lower part, we present the results on Something-Something V1
and V2 and Jester , which depend heavily on temporal
relationships. 2D CNN baseline cannot achieve a good accuracy,
but once equipped with TSM, the performance improved by double
Scaling over Backbones.
TSM scales well to backbones of different sizes. We show
the Kinetics top-1 accuracy with MobileNet-V2 , ResNet-
50 , ResNext-101 and ResNet-50 + Non-local module 
backbones in Table 3. TSM consistently improves the accuracy
over different backbones, even for NL R-50, which already has
temporal modeling ability.
Comparison with State-of-the-Arts
TSM not only signiﬁcantly improves the 2D baseline but also
outperforms state-of-the-art methods, which heavily rely on 3D
convolutions. We compared the performance of our ofﬂine (bidirectional) TSM model with state-of-the-art methods on both
Something-Something V1&V2 because these two datasets focus
on temporal modeling.
Something-Something-V1.
Something-Something-V1 is a challenging dataset, as activity
cannot be inferred merely from individual frames (e.g., pushing
something from right to left). We compared TSM with current
state-of-the-art methods in Table 2. We only applied center crop
during testing to ensure the efﬁciency unless otherwise speciﬁed.
TSM achieves the ﬁrst place on the leaderboard upon publication.
We ﬁrst show the results of the 2D based methods TSN and
TRN . TSN with different backbones fails to achieve decent
performance (<20% Top-1) due to the lack of temporal modeling.
For TRN, although late temporal fusion is added after feature
extraction, the performance is still signiﬁcantly lower than state-ofthe-art methods, showing the importance of temporal fusion across
all levels.
The second section shows the state-of-the-art efﬁcient video
understanding framework ECO . ECO uses an early 2D + late
3D architecture which enables medium-level temporal fusion.
Compared to ECO, our method achieves better performance at a
smaller FLOPs. For example, when using 8 frames as input, our
TSM achieves 45.6% top-1 accuracy with 33G FLOPs, which is
4.2% higher accuracy than ECO with 1.9× less computation. The
ensemble versions of ECO (ECOEnLite and ECOEnLiteRGB+Flow,
using an ensemble of {16, 20, 24, 32} frames as input) did achieve
competitive results, but the computation and parameters are too
large for deployment. While our model is much more efﬁcient: we
only used {8, 16} frames model for ensemble (TSMEn), and the
model achieves better performance using 2.7× less computation
and 3.1× fewer parameters.
The third section contains previous state-of-the-art methods:
Non-local I3D + GCN , that enables all-level temporal fusion.
The GCN needs a Region Proposal Network trained on
MSCOCO object detection dataset to generate the bounding
boxes, which is unfair to compare since external data (MSCOCO)
and extra training cost is introduced. Thus we compared TSM to
its CNN part: Non-local I3D. Our TSM (8f) achieves 1.2% better
accuracy with 10× fewer FLOPs on the validation set compared
to the Non-local I3D network. Note that techniques like Non-local
module are orthogonal to our work, which could also be added
to our framework to boost the performance further.
1. We reported the performance of NL I3D described in , which is a
variant of the original NL I3D . It uses fewer temporal dimension pooling
to achieve good performance, but also incur larger computation.
2. Includes parameters and FLOPs of the Region Proposal Network.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
FLOPs/Video
Test Top-1
BNInception
TSN (our impl.)
TRN-Multiscale 
BNInception
TRN-Multiscale (our impl.)
Two-stream TRNRGB+Flow 
BNInception
BNIncep+3D Res18
BNIncep+3D Res18
ECOEnLite 
BNIncep+3D Res18
ECOEnLiteRGB+Flow 
BNIncep+3D Res18
I3D from 
3D ResNet-50
Non-local I3D from 
3D ResNet-50
Non-local I3D + GCN 
3D ResNet-50+GCN
CorrNet-50 
R(2+1)D-50
ip-CSN-152 
3D ResNet-152
TSMRGB+Flow
TABLE 2. Comparing TSM against other methods on Something-Something dataset (center crop, 1 clip/video unless otherwise speciﬁed).
TABLE 3. TSM can consistently improve the performance over
different backbones on Kinetics dataset.
We further include two recent state-of-the-art methods that
achieve state-of-the-art performance: CorrNet and CSN .
For CorrNet, we compare to CorrNet-50 which has a similar
backbone shape; For CSN, we compare to ip-CSN-152, which is
the largest model and achieves the highest accuracy. Both methods
achieve high accuracy on Something-Something dataset. However,
they still require sampling 10 clips to get the average prediction.
The total computation is larger than 800G FLOPs, which is not
practical for edge deployment.
Generalize to Other Modalities.
We also show that our proposed method can generalize to other
modalities like optical ﬂow. To extract the optical ﬂow information
between frames, we followed to use the TVL1 optical ﬂow
algorithm implemented in OpenCV with CUDA. We conducted
two-stream experiments on both Something-Something V1 and V2
datasets, and it consistently improves over the RGB performance:
introducing optical ﬂow branch brings 5.4% and 2.6% top-1
improvement on V1 and V2.
Something-Something-V2.
We also show the result on Something-Something-V2 dataset,
which is a newer release to its previous version. The results
compared to other state-of-the-art methods are shown in Table 4.
On Something-Something-V2 dataset, we achieved state-of-the-art
performance while only using RGB input.
TSN (our impl.)
MultiScale TRN 
2-Stream TRN 
TSMRGB+Flow
TABLE 4. Results on Something-Something-V2. Our TSM achieves
state-of-the-art performance.
I3D from [ ]
FLOPs/Video (G)
Accuracy (%)
NL I3D+GCN
# Parameters
Fig. 5. TSM enjoys better accuracy-cost trade-off than I3D family and
ECO family on Something-Something-V1 dataset. (GCN includes
the cost of ResNet-50 RPN to generate region proposals.)
Cost vs. Accuracy.
Our TSM model achieves very competitive performance while
enjoying high efﬁciency and low computation cost for fast inference.
We show the FLOPs for each model in Table 2. Although GCN
itself is light, the method used a ResNet-50 based Region Proposal
Network to extract bounding boxes, whose cost is also
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Latency/Clip
I3D NL R-50 
SlowFast R-50 
TSM NL R-50
ir-CSN-152 
TSM RX-101
TABLE 5. Compare to state-of-the-art methods on Kinetics. TSM can
achieve higher or comparable performance at a lower inference latency.
considered in the chart. Note that the computation cost of optical
ﬂow extraction is usually larger than the video recognition model
itself. Therefore, we do not report the FLOPs of two-stream based
We show the accuracy, FLOPs, and number of parameters
trade-off in Figure 5. The accuracy is tested on the validation set of
Something-Something-V1 dataset, and the number of parameters is
indicated by the area of the circles. We can see that our TSM based
methods have a better Pareto curve than both previous state-of-theart efﬁcient models (ECO based models) and high-performance
models (non-local I3D based models). TSM models are both
efﬁcient and accurate. It can achieve state-of-the-art accuracy at
high efﬁciency: it achieves better performance while consuming
3× less computation than the ECO family . Considering that ECO
is already an efﬁciency-oriented design, our method enjoys highly
competitive hardware efﬁciency.
Although Kinetics does not focus on temporal modeling (Table 1),
we compare TSM with state-of-the-art methods on Kinetics to give
a comprehensive comparison. The results are show in Table 5.
We compare to several state-of-the-art methods: Non-local
networks (I3D NL R-50), SlowFast (SlowFast R-50),
X3D (X3D-M), and CSN (ir-CSN-152). We report the
latency and accuracy trade-off of different methods. The latency
is measured on NVIDIA RTX 2080 Ti GPU using batch size 1.
We ﬁrst warm-up the inference for 100 iterations and measure
the average latency of the next 200 iterations. TSM can achieve
higher or comparable performance at a lower inference latency.
TSM (TSM NL R-50) achieves same-level of accuracy compared
to SlowFast network (SlowFast R-50) at 1.3× lower latency. TSM
(TSM RX-101) also outperforms X3D (X3D-M) at 1.8× lower
latency. Notice that though X3D has a small computation FLOPs,
its inferior hardware efﬁciency leads to the slow inference speed.
ir-CSN-152 achieves slightly higher accuracy than TSM, but TSM
runs 3.4× faster. TSM is very competitive for accuracy-speed
trade-off.
Latency and Throughput Speedup
The measured inference latency and throughput are important for
the large-scale video understanding. TSM has low latency and high
throughput. We performed measurement on a single NVIDIA Tesla
P100 GPU. We used batch size of 1 for latency measurement; batch
size of16 for throughput measurement. We made two comparisons:
(1) Compared with the I3D model from , our method is
faster by an order of magnitude at 1.8% higher accuracy (Table 6).
We also compared our method to the state-of-the-art efﬁcient model
ECO : Our TSM model has 1.75× lower latency (17.4ms vs.
30.6ms), 1.7× higher throughput, and achieves 2% better accuracy.
Efﬁciency Statistics
FLOPs Param. Latency Thrput.
I3D from 
ECO16F 
I3D from 
I3Dreplace
39.5V/s 47.2%
TABLE 6. TSM enjoys low GPU inference latency and high throughput. V/s means videos per second, higher the better (Measured on
NVIDIA Tesla P100 GPU).
Latency Kinetics UCF101 HMDB51 Something
TABLE 7. Comparing the accuracy of ofﬂine TSM and online TSM
on different datasets. Online TSM brings negligible latency overhead.
ECO has a two-branch (2D+3D) architecture, while TSM only
needs the in-expensive 2D backbone.
(2) We then compared TSM to efﬁcient 3D model designs. One
way is to only inﬂate the ﬁrst 1 × 1 convolution in each of the
block as in , denoted as ”I3D from ” in the table. Although
the FLOPs are similiar due to pooling, it suffers from 1.5× higher
latency and only 55% the throughput compared with TSM, with
worse accuracy. We speculate the reason is that TSM model only
uses 2D convolution which is highly optimized for hardware. To
exclude the factors of backbone design, we replace every TSM
primitive with 3 × 1 × 1 convolution and denote this model as
I3Dreplace. It is still much slower than TSM and performs worse.
Online Recognition with TSM
Online vs. Ofﬂine
Online TSM models shift the feature maps uni-directionally so that
it can give predictions in real time. We compare the performance
of ofﬂine and online TSM models to show that online TSM can
still achieve comparable performance. Follow , we use the
prediction averaged from all the frames to compare with ofﬂine
models, i.e., we compare the performance after observing the
whole videos. The performance is provided in Table 7. We can see
that for less temporal related datasets like Kinetics, UCF101 and
HMDB51, the online models achieve comparable and sometimes
even better performance compared to the ofﬂine models. While
for more temporal related datasets Something-Something, online
model performs worse than ofﬂine model by 1.0%. Nevertheless,
the performance of online model is still signiﬁcantly better than
the 2D baseline.
We also compare the per-frame prediction latency of pure 2D
backbone (TSN) and our online TSM model. We compile both
models with TVM on GPU. Our online TSM model only adds
to less than 0.1ms latency overhead per frame while bringing up
to 25% accuracy improvement. It demonstrates online TSM is
hardware-efﬁcient for latency-critical real-time applications.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Fig. 6. TSM improves detection results with the help of temporal cues. For the left video, 2D baseline R-FCN generates false positive due to the
glare of car headlight on frame 2/3/4, while TSM does not have such issue by considering the temporal information. For the right video, R-FCN
generates false positive surrounding the bus due to occlusion by the trafﬁc sign on frame 2/3/4. Also, it fails to detect motorcycle on frame 4 due
to occlusion. TSM model addresses such issues with the help of temporal information.
Accuracy %
ECO (s=12)
ECO (s=20)
Video Observation %
Fig. 7. Early recognition on UCF101. TSM gives high prediction
accuracy after only observing a small portion of the video.
Accuracy %
Video Observation %
Video Observation %
(a) Something-Something
(b) Jester
Fig. 8. Early recognition on Something-Something and Jester datasets.
TSM consistently outperforms TRN at various portions by a large
Early Recognition
Early recognition aims to classify the video while only observing a
small portion of the frames. It gives fast response to the input video
stream. Here we compare the early video recognition performance
with ECO on UCF101 (Figure 7) and TRN on Something-
Something and Jester (Figure 8). Compared to ECO, TSM gives
Jetson Nano Jetson TX2 Rasp. Note8 Pixel1
27.5 117.6
Power (watt)
TABLE 8. TSM efﬁciently runs on edge devices with low latency.
much higher accuracy, especially when only observing a small
portion of the frames. For example, when only observing the ﬁrst
10% of video frames, TSM model can achieve 90% accuracy, which
is 6.6% higher than the best ECO model. TSM also consistently
outperforms TRN at various observation percentages by a large
Edge Deployment
TSM is mobile device friendly. We build an online TSM model
with MobileNet-V2 backbone, which achieves 69.5% accuracy on
Kinetics. The latency and energy on NVIDIA Jetson Nano & TX2,
Raspberry Pi 4B, Samsung Galaxy Note8, Google Pixel-1 is shown
in Table 8. The models are compiled using TVM . Power is
measured with a power meter, subtracting the static power. TSM
achieves low latency and low power on edge devices.
ONLINE OBJECT DETECTION
Real-time online video object detection is an important application
in self-driving vehicles, robotics, etc. Most exiting methods treat
video detection as image detection per frame, which is not robust
since temporal information is not considered. Other methods on
video object detection fuses information along temporal
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Online Need
Flow Latency
Overall Slow Medium Fast
Online TSM
TABLE 9. Video detection results on ImageNet-VID.
dimension after the 2D feature is extracted by the backbone, which
results in high latency, and also loses low-level temporal cues.
Here we show that we can enable temporal fusion in online
video object detection by injecting our uni-directional TSM into
the backbone. We show that we can signiﬁcantly improve the
performance of video detection by simply modifying the backbone
with online TSM, without changing the detection module design or
using optical ﬂow features.
We conducted experiments with R-FCN detector on
ImageNet-VID dataset. Following the setting in , we used
ResNet-101 as the backbone for R-FCN detector. For TSM
experiments, we inserted uni-directional TSM to the backbone,
while keeping other settings the same. We used the ofﬁcial training
code of to conduct the experiments, and the results are shown
in Table 9. Compared to 2D baseline R-FCN , our online TSM
model signiﬁcantly improves the performance, especially on the fast
moving objects, where TSM increases mAP by 4.6%. FGFA 
is a strong baseline that uses optical ﬂow to aggregate the temporal
information from 21 frames (past 10 frames and future 10 frames)
for ofﬂine video detection. Compared to FGFA, TSM can achieve
similar or higher performance while enabling online recognition
(using information from only past frames) at much smaller latency
per frame. The latency overhead of TSM module itself is less than
1ms per frame, making it a practical tool for real deployment.
We visualize the detection results of two video clips in Figure 6.
In the left video clip, 2D baseline R-FCN generates false positive
due to the glare of car headlight on frame 2/3/4, while TSM
suppresses false positive. In the right video clip, R-FCN generates
false positive surrounding the bus due to occlusion by the trafﬁc
sign on frame 2/3/4. Also, it fails to detect motorcycle on frame 4
due to occlusion. TSM model addresses such issues with the help
of temporal information.
SCALABILITY IN DISTRIBUTED TRAINING
In this section, we study how the design of TSM helps to improve
the scalability in distributed training of video models.
Factor of Video Network Design
To study the distributed training scalability, we ﬁrst discuss
the factors that might affect the scalability of video network
training .
Temporal modeling unit.
3D convolution is the most widely used operator for spatialtemporal modeling. However, it suffers from two problems: (1)
large computation and large parameter size, which slows down
training and communication; (2) low hardware efﬁciency compared
to 2D convolution. Give the same amount of FLOPs, 3D kernels
run 1.2 to 3 times slower than 2D on cuDNN . On the other
hand, our TSM module is a highly efﬁcient alternative.
Backbone topology.
Existing video networks usually sample many frames as input
(32 frames or 64 frames ), and perform temporal pooling
later to progressively reduce the temporal resolution (Figure 10a).
Another way is to sample fewer frames (e.g. 8 frames ) as
input while keeping the same temporal resolution to keep the
information (Figure 10b). Although the overall computation of the
two designs are similar, the former signiﬁcantly increases the data
loading trafﬁc, making the system I/O heavy, which could be quite
challenging in a distributed system considering the limited disk
bandwidth.
Design Guidelines to Video Model Architecture
To tackle the challenge in a distributed training systems, we
propose three video model design guidelines: (1) To increase
the computation efﬁciency, use operators with lower FLOPs and
higher hardware efﬁciency; (2) To reduce data loading trafﬁc, use
a network topology with higher FLOPs/data ratio; (3) To reduce
the networking trafﬁc, use operators with fewer parameters.
We show the advantage of the above three design guidelines by
experimenting on three models in Table 10. All the models use the
ResNet-50 backbone to exclude the inﬂuence of spatial modeling.
The model architectures are introduced as follows.
(1) The ﬁrst model is an I3D model from . The model
takes 16 frames as input and inﬂate all the 3 × 3 convolutions to
3×3×3. It performs temporal dimension pooling by four times to
reduce the temporal resolution. We denote the model as I3D3×3×3.
(2) The second model is an I3D model from , taking 32
frames as input and inﬂating the ﬁrst 1 × 1 convolution in every
other ResBlock. It applies temporal dimension pooling by three
times. We denote this more computation and parameter efﬁcient
design as I3D3×1×1.
(3) The third model is built with TSM. The TSM operator is
inserted into every ResBlock. The model takes 8 frames as input
and performs no temporal pooling. We denote this model as TSM.
Computation Efﬁciency.
Computation efﬁciency is the most direct factor that inﬂuence the
training time. As shown in Table 10, TSM8f has 1.2× fewer FLOPs
compared to I3D3×3×3 and roughly the same FLOPs compared to
I3D3×1×1. However, the actual inference throughput also depends
on the hardware utilization. We measure the inference throughput
(deﬁned as videos per second) of the three models on a single
NVIDIA Tesla P100 GPU using batch size 16. We also measured
the hardware utilization, deﬁned as achieved FLOPs/second over
peak FLOPs/second. The inference throughput and the hardware
efﬁciency comparison is shown in Figure 9a. We can ﬁnd that the
model is more hardware efﬁcient if it has more 2D convolutions
than 3D: TSM is a fully 2D CNN, therefore it has the best hardware
utilization (2.0×); while the last several stage of I3D3×3×3 (res4,
res5) have few temporal resolution (as shown in Table 11), it is
more similar to 2D convolution and thus is 1.8× more hardware
efﬁcient than I3D3×1×1 (1.0×).
Data Loading Efﬁciency.
Video datasets are usually very large. For a distributed system
like the Summit supercomputer, the data is usually stored in
High Performance Storage System (HPSS) shared across all the
worker nodes. Such ﬁle systems usually have great sequential I/O
performance but inferior random access performance. Therefore,
large data trafﬁc could easily become the system bottleneck.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Input size↓
Throughput↑
Compute/IO↑
I3D3×3×3 
63.1V/s (1.5×)
16.6k (2.4×)
I3D3×1×1 
41.9V/s (1.0×)
6.85k (1×)
84.8V/s (2.0×)
27.4k (4×)
TABLE 10. Efﬁciency statistics of different models. Arrows show the better direction.
Throughput Hardware Utilization
(a) TSM has fewer FLOPs, better throughput and
utilization.
I/O Time (not hidden)
Compute Time
I3D3x3x3 I3D3x1x1 TSM8f
Batch Training Time (s)
(b) TSM is I/O light, decreasing the total batch
Number of GPUs
Scalability
(c) TSM has better scalability due to smaller
model size.
Fig. 9. Analyzing how different design aspects inﬂuence the distributed training scalability of video recognition models: (a) computation
efﬁciency; (b) data loading efﬁciency; (c) networking efﬁciency.
global pool
TABLE 11. The temporal resolution of output feature map for each block. TSM is a fully 2D structure, enjoying the best hardware efﬁciency.
The last several stages of I3D3×3×3 have fewer temporal resolution, making it more similar to 2D CNN, thus enjoying better hardware efﬁciency
compared to I3D3×1×1.
Feature F1
Input data
(a) Pooled-up
Input data
Feature F1
(b) Straight-up (ours)
Fig. 10. Two kinds of video backbone design. Straight-up backbone
does not perform temporal pooling and is more data efﬁcient. Pooledup version requires many input frames and drains I/O.
Previous popular I3D models , takes many frames per
video (16 or 32) as input and perform down-sample over temporal
dimension. We argue that such design is a waste of disk bandwidth:
a TSM8f only takes 8 frames as input while achieving better
accuracy. The intuition is that nearby frames are similar; loading
too many similar frames is redundant. We empirically test the data
loading bottleneck on Summit. To exclude the communication cost
from the experiments, we perform timing on single-node training.
We measure the total time of one-batch training and the time for
data loading (that is not hidden by the computation). As shown
in Figure 9b, for I3D3×1×1, it takes 32 frames as input. The data
loading time cannot be hidden by the computation, therefore data
I/O becomes the bottleneck. I3D3×3×3 that takes 16 frame as input
has less problem on data loading, while TSM8f can fully hide the
data loading time with computation. We also compute the model
FLOPs divided by the input data size as a measurement of data
efﬁciency. The value is denoted as ”Compute/IO” as in Table 10.
For scalable video recognition models, we want a model with larger
Compute/IO ratio.
Networking Efﬁciency.
In distributed training system, the communication time can be
modelled as:
communication time = latency + model size
The latency and bandwidth is determined by the network condition,
which cannot be optimized through model design. However, we can
reduce the model size to reduce the communication cost. Both I3D
models inﬂate some of the 2D convolution kernels to 3D, which
will increase the number of parameters by kT . While TSM module
does not introduce extra parameters. Therefore, it has the same
model size as the 2D counterpart. For example, I3D3×3×3 has 1.9×
larger model size than TSM8f, which introduces almost two times
of network communication during distributed training. To test the
inﬂuence of model size on scalability, we measure the scalability
on a 8 node cluster. Each computer has 4 NVIDIA TESLA P100
GPUs. We deﬁne the scalability as the actual training speed divided
by the ideal training speed (single machine training speed * number
of nodes). The results are shown in Figure 9c. Even with the highspeed connection, the scalability of I3D3×3×3 quickly drops as the
number of training nodes increase: the scalability is smaller than
85% when applied to 8 nodes. While TSM8f model still has over
98% of scalability thanks to the smaller model size thus smaller
networking trafﬁc.
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
Fig. 11. Kinetics top-1 validation accuracy vs. mini-batch size. The
performance of the model does not degrade when we scale up the
mini-batch size to 12k. The mean and standard deviation (the scale of
the STD is hardly visible) are shown in the ﬁgure.
#Node #GPU Batch #Frames Accuracy
74.12±0.11 49h 55m Baseline
74.12±0.08 7h 7m
same level
74.18±0.14 3h 38m
74.14±0.10 1h 50m
74.10±0.08 55m 56s
73.99±0.04 28m 14s
12,288 98,304
73.99±0.07 14m 13s
18,432 147,456
72.52±0.07 10m 9s
24,576 196,608
69.80±0.13 -
1024* 6144
49,152 393,216
62.22±0.17 -
TABLE 12. Detailed statistics of different mini-batch size (* indicates
simulated performance).
Large-scale Distributed Training on Summit
We scale up the training of video recognition model on Summit
supercomputer. With the help of above hardware-aware model
design techniques, we can scale up the training to 1536 GPUs,
ﬁnishing the training of Kinetics in 15 minutes.
Summit or OLCF-4 is a supercomputer at Oak Ridge
National Laboratory, which as of September 2019 is the fastest
supercomputer in the world. It consists of approximately 4,600
compute nodes, each with two IBM POWER9 processors and
six NVIDIA Volta V100 accelerators. The POWER9 processor is
connected via dual NVLINK bricks, each capable of a 25GB/s
transfer rate in each direction. Nodes contain 512 GB of DDR4
memory for use by the POWER9 processors and 96 GB of High
Bandwidth Memory (HBM2) for use by the accelerators 3.
We used PyTorch and Horovod for distributed training. The
framework uses ring-allreduce algorithm to perform synchronized
SGD. The training is accelerated by CUDA and cuDNN. We used
NVIDIA Collective Communication Library (NCCL) 4 for most of
the communication.
For training on Kinetics, we used the same hyper-parameter at
the same batch size, applying a linear scaling rule .
Experiments
Baseline. For the baseline, we trained a ResNet-50 TSM8f model
on a single Summit node with 6 GPUs, each GPU contains 16
video clips, resulting in a total batch size of kn = 96. We evaluate
3. 
summit-user-guide
4. 
Fig. 12. The training speed and scalability of distributed synchronous
SGD training. TSM8f achieves a good scalability (>80%) even when
using 1536 GPUs. TSM8f can achieve 1.6× higher training speed
compared to I3D3×3×3 and 2.9× compared to I3D3×1×1, showing the
effectiveness of the proposed design guidelines.
the performance of last 5 checkpoints, it achieves a top-1 accuracy
of 74.12 ± 0.11%.
Performance vs. Batch Size. We ﬁrst compare the training error
vs. the batch size. As shown in , the accuracy will not degrade
when the batch size is relatively small. Therefore, our experiments
start from 8 computing nodes (48 GPUs, 384 video clips, 3072
frames) to 1024 computing nodes (6144 GPUs, 49152 video clips,
393216 frames) . Note that each sample in a video recognition
model is a video clip consisting of several frames/images (in our
case, 8). Therefore, the actual number of images used in one batch
could be much larger than ImageNet training (e.g., 98k vs. 8k ).
We ﬁrst plot the error vs. batch size trade-off in Figure 11. The
error does not increase when we scale the number of computing
nodes up to 256 (1536 GPU), where the batch size is 12288, the
total frame number is 98304. The detailed statistics are shown in
Table 12. The scalability of TSM model is very close to the ideal
case. Note that due to quota limitation, the largest physical nodes
we used is 384 with 2304 GPUs. For 512 and 1024 nodes, we used
gradient accumulation to simulate the training process (denoted by
We also provide the training and testing convergence curves
using 768, 1536 and 3072 GPUs in Figure 13. For 768 GPUs and
1536 GPUs, although the convergence of large-batch distributed
training is slower than single-machine training baseline, the ﬁnal
converged accuracy is similar, so that the model does not lose
accuracy. For 3072 GPUs, the accuracy degrades for both training
and testing.
Scalabilty. We test the scalability of distributed training on Summit.
According to the results from last section, we can keep the accuracy
all the way to 256 computing nodes. Therefore, we sweep the
number of computing nodes from 1 to 256 to measure the scalability.
We keep a batch size of 8 for each GPU and each node has 6
GPUs. So the batch sizes change from 48 to 18,432. Each video
clips contains 8 frames in our model, resulting a total number
of frames from 384 to 147,456. We measure the training speed
(videos/second) to get the actual speed-up. We calculate the ideal
training speed using the single node training speed multiplied by
number of nodes. The comparison of different models is provided
in Figure 12. The actual training speed is just marginally below the
ideal scaling, achieving > 80% scaling efﬁciency. We also provide
the detailed overall training time in Table 12. With 1536 GPUs,
we can ﬁnish the Kinetics training with TSM within 14 minutes
and 13 seconds, achieving a top-1 accuracy of 74.0%. The overall
training speed of TSM8f is 1.6× larger than I3D3×3×3 and 2.9×
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
(a) Mini-batch size 6k.
(b) Mini-batch size 12k.
(c) Mini-batch size 25k (degrade).
Fig. 13. The learning curve for baseline training and large-batch distributed training (batch size 6144, 12228, 24576). The performance does not
degrade for batch size 6144 and 12228, while degrades for a batch size of 24576.
Plugging something into something
Hitting something with something
Removing something, revealing something behind
Fig. 14. Spatial-temporal action detector emerges in TSM video classiﬁcation models, while single-frame baseline (TSN) cannot localize the
action. Italic title indicates the action category. In the ﬁrst example, our TSM model precisely localize the “hitting” action, while TSN can only
highlight the object. In the second example, TSM localizes the “plugging” action but not the other hand motion. Finally, TSM accurately locates
the temporal region where the “removing” action happens.
larger than I3D3×1×1, showing the advantage of hardware-aware
model design.
VIDEO NETWORK DISSECTION
In this section, we dissect the trained TSM model to understand
how it learns temporal information compared to 2D networks.
To investigate what the action recognition network is learning,
we adopt a similar method as in to get the Class Activation
Mapping (CAM), which shows the salience of each class over the
input image. Take ResNet backbone as an example, the original
output for the video network is:
logit = fc(pool(xconv))
where xconv is the output activation of the last convolutional
layer , pool is the global
average pooling over both spatial and temporal dimension , and fc is the last fully connected layer
for classiﬁcation. To get CAM, we remove the global average
pooling layer, and change the fc layer to a 1 × 1 × 1 convolution
using the same weights, which results in a output tensor of shape
1 × #class × 8 × 7 × 7. We use the CAM map of the predicted
Journal of LATEX Class Files, Vol. 14, No. 8, August 2015
category (highest probability) as the attention of the network.
For visualization, we used a similar method as in . Speciﬁcally, we ﬁrst resize the spatial resolution of CAM feature map to
the size of the input video clip (1 × #class × 8 × 224 × 224) with
bilinear interpolation and use a threshold to divide the attention
foreground and background. We set the threshold to preserve 20%
of the pixels over the validation set.
We perform experiments on Something-Something V2 
dataset. And some results are shown in Figure 14. We compare
the attention distribution between our TSM model and 2D baseline
TSN. The background of the category-aware attention map is
darkened. We ﬁnd that spatial-temporal action detector emerges in
TSM video network, even though we only provide classiﬁcation
label during the training. TSM models can accurately localize the
“action”, instead of the “object”. For example, in the ﬁrst video
clip labeled as “Hitting something with something”, TSM model
only highlights the region where a pen is hitting the card box,
i.e., when and where the action is happening. However, for the
2D baseline, since it does not have the temporal information, it
only highlights the object box. The same situation happens for the
following two video clips. Note that in the third clip (“Removing
something, revealing something behind”), the 5-th frame and the 6th frame look exactly the same, while with the help of the temporal
modeling, TSM model can tell that the 5-th frame is part of the
action while the 6-th frame not.
CONCLUSION
We propose Temporal Shift Module for hardware-efﬁcient video
recognition. It can be inserted into 2D CNN backbone to enable
joint spatial-temporal modeling at no additional cost. The module
shifts part of the channels along temporal dimension to exchange
information with neighboring frames. Our framework is both
efﬁcient and accurate, enabling low-latency video recognition on
edge devices. It has better scalability than 3D networks, enabling
large-scale training on video recognition. We also show that spatialtemporal action detector emerges in TSM network.