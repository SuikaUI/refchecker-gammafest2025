Machine Learning 2:285 318, 1988
@ 1988 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands
Quickly When Irrelevant Attributes
A New Linear-threshold Algorithm
NICK LITTLESTONE
( )
Department of Computer and Information Sciences, University of California,
Santa Cruz, CA 95063, U.S.A.
 
 
Keywords: Learning from examples, prediction, incremental learning, mistake bounds,
learning Boolean functions, linear-threshold algorithms
Valiant and others have studied the problem of learning various
classes of Boolean functions from examples.
Here we discuss incremental learning of
these functions. We consider a setting in which the learner responds to each example
according to a current hypothesis. Then the learner updates the hypothesis, if necessary,
based on the correct classification of the example. One natural measure of the quality of
learning in this setting is the number of mistakes the learner makes. For suitable classes
of functions, learning algorithms are available that make a bounded number of mistakes,
with the bound independent of the number of examples seen by the learner. We present
one such algorithm that learns disjunctive Boolean functions, along with variants for
learning other classes of Boolean functions. The basic method can be expressed as a
linear-threshold algorithm. A primary advantage of this algorithm is that the number
of mistakes grows only logarithmically with the number of irrelevant attributes in the
examples. At the same time, the algorithm is computationally efficient in both time and
1. Introduction
In this paper, we consider learning from examples in a situation in which
the goal of the learner is simply to make few mistakes. The task is to induce
a concept that can be described by a Boolean function; that is, the informa-
tion received in each example is a list of Boolean attributes and the correct
response is a Boolean function of the attributes. We are interested in cases
where the correct-response function depends on only a small proportion of
the attributes present in each example. For example, this case may occur
in pattern recognition tasks; feature detectors may extract a large number
of features for the learner's consideration, not knowing which few will prove
useful. For another example, consider an environment in which the learner
N. LITTLESTONE
builds new concepts as Boolean functions of old concepts . Here the learner may need to sift through a large library of
available concepts to find the suitable ones to use in expressing each new
concept. In a special case of this situation, one may design a library of con-
cepts specifically to ease learning of a certain class of complex functions.
In this case one chooses concepts for the library that allow representation
of any function in the class as a simple function of the library concepts. In
the context of this paper, the concepts in the library will just be Boolean
functions themselves. For example, consider k-DNF, the class of Boolean
functions that can be represented in disjunctive normal form with no more
than k literals per term . If one has available intermediate
concepts that include all conjunctions of no more than k literals, then any
k-DNF function can be represented as a simple disjunction of these con-
cepts. We will return to this idea at the end of the paper, presenting an
algorithm for learning k-DNF.
Our main result is an algorithm that deals efficiently with large num-
bers of irrelevant attributes. If desired, it can be implemented within a
neural net framework as a simple linear-
threshold algorithm. The method learns certain classes of functions that
can be computed by a one-layer linear-threshold network; these include,
among other functions, disjunctions, conjunctions, and r-of-k threshold
flmctions .
(The latter functions are true if at least r out of k designated variables are
true.) Preproeessing techniques can be used to extend the algorithm to
classes of Boolean functions that are not linearly separable, such as k-DNF
(for fixed k). When our algorithm is applied to k-DNF formulas with few
terms, it makes significantly fewer mistakes than the algorithm presented
by Valiant . The algorithm is similar to classical perceptron
algorithms, but it uses a multiplieative weight-update scheme that permits
it to do much better than classical perceptron training algorithms when
many attributes are irrelevant.
We study learning in an on-line setting. By this we mean that there
is no separate set of training examples. The learner attempts to predict
the appropriate response for each example, starting with the first exam-
ple received. After making this prediction, the learner is told whether the
prediction was correct, and then uses this information to improve its hy-
pothesis. The learner continues to learn as long as it receives examples;
that is, it continues to examine the information it receives in an effort to
improve its hypothesis. In this setting, it is advantageous to use an algo-
rithm that computes successive hypotheses incrementally, saving work that
would be required to calculate every hypothesis from scratch from stored
input examples. Our algorithm is incremental in this sense.
LEARNING QUICKLY
We evaluate the algorithm's learning behavior by counting the worst-
case number of mistakes that it will make while learning a function from
a specified class of flmctions. We also consider computational complexity.
We will prove that the mistake bound of our algorithm is within a con-
stant factor of optimal when the algorithm is applied to certain classes of
functions. The method is also computationally time and space efficient.
Before we present the algorithm we will discuss some properties of ntis-
take bounds for concept classes, including general lower bounds. We will
also demonstrate a close relationship between exact identification with
equivalence queries, as presented by Anghfin . and learning with
a bounded nmnber of mistakes.
The mistake bounds that we present are strong in the sense that they
do not depend on any assumption about which examples the learner sees
or the order in which it sees them: the selection and ordering can be done
by an adversary. However, due to the freedom given the adversary, we
cannot say how early the learner will make the mistakes. For example, a
single instance could be repeated arbitrarily many times at the beginning
of the sequence of trials and then followed by other instances for which the
learner does not yet know how to respond.
One and discussed by Blumer, Ehrenfeucht.
Haussler, and Warmuth and Angluin . Starting with
a lnistake-bounded algorithm, one can derive an algorithm that does well
under the criteria of this probabilistie model. We mention one indirect way
to do this, using Anghfin's results. Kearns, Li, Pitt, and Valiant
 have mentioned a related technique.
Another change that one might make to the learning model involves
keeping the on-line setting, but analyzing it with probabilistic instead of
worst-case assumptions. One can use the probabilistic model mentioned
above to this end. Haussler, Littlestone, and Warmuth discuss a
related model developed particularly for this setting.
It is interesting to compare our main algorithm to similar classical meth-
ods for perceptron training. Hampson and Volper present empirical
evidence that, for one classical perceptron algorithm, the number of mis-
takes grows linearly with the number of irrelevant attributes. This is in
keeping with theoretical bounds from the perceptron convergence theorem
N. LITTLESTONE
 . We know
of no evidence that any other standard perceptron algorithm does better.
In contrast, we will prove that the number of mistakes that our algorithm
makes grows only logarithmically with the number of irrelevant attributes.
Others have looked at the problem of dealing efficiently with irrelevant
attributes in the context of learning Boolean functions. Haussler 
mentions two algorithms for learning disjunctive functions in the context
of Valiant's learning model. One of them is designed to learn rapidly in the
presence of irrelevant attributes. However, that algorithm is not naturally
incremental, and thus is significantly less time and space efficient than
ours when used in an on-line setting. Valiant introduces a
mechanism by which a friendly and knowledgeable teacher can help the
learner by indicating which attributes are relevant. Hampson and Volper
 , in addition to their study of classical perceptron algorithms, have
experimented with new algorithms that use conditional probabilities in
an effort to reduce the cost of irrelevant attributes. They do not present
theoretical bounds for these algorithms.
The mistake-counting model that we use is essentially the same as a
model discussed in Barzdin and Freivald . See Angluin and Smith
 for a survey that compares a number of learning models.
2. The setting
In this section we will describe in more detail the learning environment
that we consider and the classes of functions that our algorithm can learn.
We assume that learning takes place in a sequence of trials. The order of
events in a trial is as follows:
(1) The learner receives some information about the world, correspond-
ing to a single example. This information consists of the values of n
Boolean attributes, for some n that remains fixed. We think of the in-
formation received as a point in {0, 1} n. We call this point an instance
and we call {0, 1} n the instance space.
(2) The learner makes a response. The learner has a choice of two re-
sponses, labeled 0 and 1. We call this response the learner's prediction
of the correct value.
(3) The learner is told whether or not the response was correct.
information is called the reinforcement.
Each trial begins after the previous trim has ended.
We assume that for the entire sequence of trials, there is a single function
f : {0, 1} n --* {0, 1} which maps each instance to the correct response to
that instance. We call this function the target function or target concept.
LEARNING QUICKLY
We call an algorithm for learning in this setting an algorithm for on-line
learning from examples. When we speak of learning algorithms without fur-
ther qualification we refer to algorithms for on-line learning from examples.
For this paper we restrict our attention to deterministic algorithms.
We will present mistake bounds as worst case bounds over some class of
possible target functions, which we will call the target class.
3. The nature of absolute mistake bounds
In this section we give some general results about mistake bounds for on-
line learning from examples. We present upper and lower bounds on the
number of mistakes in the case where one ignores issues of computational
efficiency. The instance space can be any finite space X, and the target
class is assumed to be a collection of functions, each with domain X and
range {0, 1}. The results also apply to infinite X, provided that the target
class remains finite. However, computability issues may arise in this case,
and we do not consider them here.
For any learning algorithm A and any target function f, let MA(f) be
the maximum over all possible sequences of instances of the number of
mistakes that algorithm A makes when the target function is f. For any
learning algorithm A and any non-empty target class C, let MA(C) =
maxfec MA(f). 1 Define MA(C) = -1 if C is empty. Any number greater
than or equal to MA(C) will be called a mistake bound for algorithm A
applied to class C.
Definition 1 The optimal mistake bound for a target class C, denoted
opt(C), is the minimum over all learning algorithms A of MA(C). This
minimum is taken over all algorithms regardless of their computational
efficiency. An algorithm A is called optimal for class C if MA(C) = opt(C).
Thus opt(C) represents the best possible worst case mistake bound for any
algorithm learning C.
If computational resources are no issue, there is a straightforward learn-
ing algorithm that has excellent mistake bounds for many classes of func-
tions. This algorithm uses the idea of repeated halving of the set of plausi-
ble hypotheses. This idea appears in various forms in Barzdin and Freivald
 , Mitchell , and Angluin . We restate it in the current
context because it gives an upper limit on the mistake bound and because it
suggests strategies that one might explore in searching for computationally
efficient algorithms.
1Some algorithms that we will describe are general algorithms whose functioning
depends on knowledge of the particular target class for which they are being used. For
such an algorithm A, we will use MA(C) to denote max fee MA(f) when A is told that
the target class is C.
N. LITTLESTONE
Algorithm 1 (halving algorithm)
The halving algorithm can be applied to any finite class C of functions
taking values in {0, 1}. It maintains a list of all of the functions in the
class that agree with the target function on all past instances. We will call
the functions on this list the consistent functions. In the terminology of
Mitchell , the consistent flmctions form the current version space of
the algorithm, hfitially the list contains all of the flmctions in the class.
To respond to a new instance, the algorithm computes the values of all
consistent fimctions at the new instance, and makes the prediction that
agrees with the majority (or either possibility in case of a tie). Following
each trial, the algorithm updates the list of consistent functions.
We will now give a second description of the halving algorithm to intro-
duce notation that we will use later. Given a target class C and a point
z in the associated instance space X, let (0(C, z) denote the subset of C
containing those functions that are 0 at z, and let ~1(C. z) denote those
fimctions in C that arc 1 at z.
The halving algorithm maintains a variable CONSIST whose value is
a set containing all functions in C that are consistent with all past in-
stances. Initially CONSIST = C. When the halving algorithm receives an
instance, it determines the sets @(CONSIST, x) and (~(CONSIST, x). If
1~1 (CONSIST, x)l > I@ (CONSIST. x)l then the algorithm predicts 1; oth-
erwise it predicts 0. When the algorithm receives the reinforcement, it sets
CONSIST accordingly: if the correct response to z is 0 then it sets CON-
SISTto (o( CONSIST, z); otherwise it sets CONSIST to (~(CONSIST, "z).
Let M~rALWNC(C) denote the maximum number of mistakes that the al-
gorithm will make when it is run for the target class C (i.e., its initial list
of functions consists of C) and the target flmction in fact comes from C.
Theorem 1 For any non-empty target cla,v8 C, MHALVING(C) < log2 IC].
PROOF: Since there are only two possible predictions, the learner will
always be able to choose a prediction agreed to by at least half of the
current list of consistent functions.
Whenever a mistake occurs, those
functions that agree with the prediction of the learner will be eliminated
from the list of consistent functions; these functions constitute at least half
of the list. Thus at each mistake the size of tile list will be divided by at
least two. Since we have assumed that the target function is in the initial
class of functions, there will always be at least one consistent function.
Thus the method can make at most log 2 IC[ mistakes.
The theorem above also holds for a modified version of the halving algo-
rithm in which CONSIST is only changed following trials in which mistakes
occur. The same proof applies in this case. The halving algorithm imme-
diately gives us the following theorem:
LEARNING QUICKLY
Theorem 2 For any finite target class C,
opt(C) < log 2 1(71.
Example 1 Note that for some classes of functions this bound is not tight.
For example, for x e {0, 1} ~ let g, : {0, 1} ~ + {0, 1} be the function that
is l at z and 0 elsewhere. Then one can easily verify that the halving
algorithm applied to the class of functions {g,},~{o,1}" will make at most
one mistake.
Now we will study opt(C) more closely. To do this we need the following
definitions.
Definition 2 A mistake tree for a target class C over an instance space X
is a binary tree each of whose nodes is a non-empty subset of C and each
of whose internal nodes is labeled with a point of X. which satisfies tile
following:
(1) The root of the tree is C.
(2) Given any internal node C' labeled with x, the left child of (;, if
present, is ~0(C', z), and the right child, if present, is ~1(C', x).
For example, Figure 1 shows the mistake tree for C when X = {0, 1} 5 and
C consists of the functions fi(xl .... , x~) = zi, for i = 1 ..... 5.
A complete k-mistake tree is a mistake tree that is a complete binary tree
of height k. We define the height of a tree to be the length in edges of the
longest path from the root. The tree above is a complete 2-mistake tree.
These trees provide a way to characterize the number of mistakes made by
an optimal learning algorithm. We will present an optimal algorithm, and
then discuss the number of mistakes that it makes.
For any non-empty finite target class C, let K(C) equal the largest inte-
ger k such that there exists a complete k-mistake tree for C. The definition
of mistake trees guarantees a finite upper bound to k. Let K(O) = -1.
Algorithm 2 (standard optimal algorithm)
The standard optimal algorithm is similar to tile halving algorithm. It
maintains the variable CONSIST in the same manner, and like the halving
algorithm examines ~o(CONSIST, x) and E1 (CONSIST, z) to determine its
prediction. The only difference from the halving algorithm lies in the rule
it uses to choose its prediction. Instead of predicting according to which
of these sets of flmetions is larger, it compares K(@(CONSIST, x)) with
K(~(CONSIST, x)). If K(~I(CONSIST, x)) > K(~o(CONSIST, x)) then
the algorithm responds 1; otherwise it responds 0. Thus whenever a mis-
take occurs, the remaining consistent functions have the smaller maximal
complete mistake tree.
N, LITTLESTONE
label (0,0,1,1,1)
{f3,f4,fb}
label (0,1,0,0,0)
label (0,0,0,1,1)
Figure 1. A complete 2-mistake tree.
Theorem 3 Let X be any instance space. Let SOA denote the standard
optimal algorithm defined above, and let C be any finite class of functions
with domain X and range {01 1}. Then
opt(C) : MSoA(C ) : K(C).
We will prove this theorem using the following two lemmas:
Lemma 1 For any target class C,
opt(C) > K(c).
PROOF: This follows trivially from the definition if C is empty. Assume
C is non-empty, and let k = K(C). Saying that opt(C) > k is equivalent
to saying that for any deterministic learning algorithm A, there exists a
function f C C and a sequence of instances such that A makes at least
k mistakes when presented with that sequence of instances. Given an al-
gorithm A, we will show how an adversary can choose a function and a
sequence of instances such that A makes at least k mistakes. The adver-
sary keeps track of a current mistake tree. Initially this is a complete k
mistake tree for C. If k = 0, the lemma follows trivially. Otherwise, the
first instance chosen by the adversary is the label of the root of the tree.
Whatever the algorithm predicts, the adversary tells the algorithm that its
prediction is wrong. This response of the adversary eliminates some func-
tions as possible target functions. The remaining candidate functions are
LEARNING QUICKLY
either the class @(C, x) or the class ~(C, x), depending on the algorithm's
prediction and the adversary's response to it. One of the two subtrees of
the root of the adversary's current mistake tree is a complete k - 1 mistake
tree for the remaining candidate functions. The adversary sets its current
mistake tree to that subtree. It chooses the next instance to be the label of
the root of the new current tree. The adversary continues in this manner,
forcing the algorithm to be wrong at each instance. After j mistakes, the
adversary's current tree is a complete k - j mistake tree for the remaining
candidate target functions. As long as j < k, the root of the current tree
has two children corresponding to non-empty subclasses of C; thus the ad-
versary can choose a point (the label of the root) at which it can force the
algorithm to make a mistake. When j = k, k mistakes have been made, as
desired. The target function chosen by the adversary can be any candidate
remaining after the last mistake was made.
Lemma 2 Let C be a finite non-empty target class. Suppose that SOA
is run to learn some function in C and that the sequence of instances it
receives is xl,..., xt. Consider the variable CONSIST maintained by SOA.
Let CONSISTi denote the value of CONSIST at the start of trial i. For
any k > 0 and i in {1,..., t}, if K(CONSISTi) = k, then SOA will make
at most k mistakes during trials i, . . . , t.
PROOF: We prove this by induction on k, taking k = 0 to be the base
case. By the construction of SOA, the target function will always be in
CONSISTi. If K(CONSISTi) = 0 then CONSISTi can contain only the
target function. (If there are two fmlctions in CONSISTi, then any instance
on which they differ is the label of the root of a complete l-mistake tree
for CONSISTi.) The definition K(0) = -1 ensures that SOA will always
respond correctly when CONSISTi contains only the target function. This
proves the base case of the induction.
Now we will prove the lemma for arbitrary k > 0, assuming that it holds
for k - 1. If SOA makes no mistakes during trials i,..., t - 1 then we are
done. Otherwise, let j be the number of the first trial among trials i,..., t-
1 at which SOA makes a mistake. If there are complete k-mistake trees
for both ~o(CONSISTj, xj) and ~I(CONSISTj, xj), then we can combine
them into a complete k+l mistake tree for CONSISTj; we add a root node
labeled with xj. Since CONSISTj C_ CONSISTi it is easy to transform this
into a complete k + l-mistake tree for CONSISTi. But we have assumed
that there does not exist a complete k+l-mistake tree for CONSISTi. Thus
at least one of K(@(CONSISTj,xj)) and K(~I(CONSISTj, xj)) must be
less than k. Since the response of SOA corresponded to the larger of these
two values for K, and since SOA was wrong, CONSISTj+I will have the
property that K(CONSISTj+I) < k. By the induction hypothesis, SOA
N. LITTLESTONE
Table I. Values of nine functions in Example 2,
will make at most k - 1 mistakes during trials j + 1,..., t. This gives the
desired result.
PROOF OF THEOREM 3:
If we set k = K(C) and i = 1 in Lemma 2
we get MSoA(C ) < K(C). Lemma 1 states K(C) <_ opt(C). From the
definition of opt(C) we have opt(C) <_ MSoA(C ). The theorem follows. •
One of the consequences of this theorem is that we could use opt instead
of K in the description of SOA and obtain the same algorithm.
Note that Example 1 shows that there are arbitrarily large target classes
C for which opt(C) = 1. Using this, one can construct a target class C for
which there is some point x such that
I,Io(C, x)l >
opt( o(C, x)) < opt(6(c,
For such a target class, if the point x is the first instance, then the standard
optimal algorithm and the halving algorithm will make different predictions
for x. Let us consider an example of such a target class for which the halving
algorithm is not optimal,
Example 2 Let the instance space X be an eight element set {al,.. •, as}.
Let the target class C consist of nine functions fl,. • •, f9, with values shown
in Table 1. If the first three instances received by the halving algorithm
are a6, aT, as in that order, then there is some target function for which
the halving algorithm will make three mistakes. (If we use the version
of the halving algorithm that chooses 0 in case of a tie, then the halving
algorithm will make three mistakes for target function fg-) On tile other
hand, there is no sequence of points and target function for which SOA will
make more than 2 mistakes. One can see this by considering each point
of the instance space in turn. For every x E X either opt(@(C,x)) 5 1
LEARNING QUICKLY
or opt(~l(C, x)) < 1. Thus no matter on which instance SOA makes its
first mistake, its prediction will have been chosen so that the remaining
consistent functions have an optimal mistake bound of at most one. Hence
the halving algorithm is not optimal for this target class.
Now we give a lower bound for opt(C) in terms of the Vapnik-Chervonen-
kis dimension of C, which is a combinato-
rial parameter that has proven useful in other studies of learning . 2 To
define the Vapnik-Chervonenkis dimension, we use the notion of a shattered
Definition 3 A set S _c X is shattered by a target class C if for every
U c S there exists a function f E C such that f is 1 on U and 0 on S - U.
Definition 4 The Vapnik-Chervonenkis dimension of a non-empty target
class C is the cardinality of the largest set that is shattered by C. We will
denote this VCdim(C). We will define VCdim(O) = -1.
Theorem 4 For any target class C, VCdim(C) <_ opt(C).
PROOF: Let k = VCdim(C). Choose any set {Vl,...,vk} C_ X that is
shattered by C. Then we can construct a complete k-mistake tree for C
with all internal nodes at depth j labeled with vj+l for j = 0, 1,..., k - 1.
The nodes are chosen to be subclasses of C as required in the definition of
a mistake tree. These subclasses will be all non-empty (as required by the
definition) by virtue of the fact that {Vl,..., vk} is shattered by C.
The Vapnik-Chervonenkis dimension will prove to be a useful lower
bound on opt(C) for concept classes that we will consider in later sec-
tions of the paper. However, there are also concept classes for which the
Vapnik-Chervonenkis dimension is a very weak lower bound. In fact, as
the following example shows, opt(C) can be arbitrarily large for classes for
which VCdim(C) = 1.
Example 3 For n > 0, take X = {1,...,2 n- 1}. For eachj E {1,...,2 n}
let fj: X --~ {0, 1} be the function such that fj(x) = 1 if and only if x < j.
Let C = {fj: 1 < j < 2n}. Then YCdim(C) = 1 but opt(C) = n. To see
this, first note that for any f E C if f(x) = 1 then for all y < x, f(y) = 1.
Thus no set of size 2 is shattered and VCdim(C) = 1. Also, by Theorem
2, opt(C) < log 21CI = n. To see that opt(C) > n we can construct a
complete n-mistake tree. Label the root with the point 2 n-1. We have
~0(C, 2 n-l) = {fl,...,f2~-l} and ~1(C, 2 n-l) = {f2,~-1+1,..., f2~}. Each
of these two subclasses is similar to the original class but half as large.
2In Vapnik "1982), the Vapnik-Chervonenkis dimension is called the capacity.
N. LITTLESTONE
It is easy to see that points can be found to be the labels of the children
of the root that will split each of the subclasses exactly in two. This line
of reasoning can be formalized to yield an inductive construction of the
mistake tree.
4. General transformations
There is a close relationship between learning algorithms of the type that
we have been considering and those that exactly identify a target function
using a bounded number of equivalence queries, as described by Angluin
 . An equivalence query is a request by an algorithm that asks if
the target function matches some function described in the query. When-
ever an algorithm receives a negative answer to an equivalence query, it
also receives a counterexample, i.e., a point at which the target function
and the proposed function disagree.
The equivalence query algorithms
that we consider here receive no examples as input other than the coun-
terexamples to the queries. In this section, we will use the term "query
algorithm" to refer to an algorithm that learns using equivalence queries,
and the terms "on-line learning algorithm", "mistake-bounded algorithm",
and "algorithm for learning from examples" to refer to algorithms of the
type discussed elsewhere in this paper.
To describe the relationship between equivalence query algorithms and
our model, we must define the notion of the current hypothesis of an algo-
rithm for on-line learning from examples. The current hypothesis is defined
initially and between trials, and is a function from the instance space to
{0, 1}. Its value at any instance x is defined to be the response that the
algorithm would give at the next trial if the instance received in the next
trial were x. This is well-defined for any deterministic algorithm. If we
copy the state of an algorithm at the conclusion of a trial, then we can use
the copy of the state to determine (by simulating the algorithm) what pre-
diction the algorithm would make for any new instance, without sending
that instance to the running version of the algorithm. Thus the state can
be considered a representation of the current hypothesis of the algorithm.
(Often a portion of the state will suffice.) Using this representation to
represent the functions appearing in queries, an algorithm that learns from
examples can be transformed into a query algorithm. We will show that
the number of queries needed will be at most one more than the number
of mistakes that the learning-from-examples algorithm would make. a
SNore that for most of Angluin's results, the queries are restricted to use only functions
from the target class in question. For the conversion here, the flmctious used in the
queries must be allowed to come from the class of flmctions that the original algorithm
uses for its hypotheses. Also note that with this transformation, the functions used in
LEARNING QUICKLY
The inverse transformation is also possible: a query algorithm can be
transformed into an algorithm that learns from examples making a bounded
number of mistakes. The efficiency of the transformed algorithm will de-
pend on the difficulty of evaluating the functions given in the queries. The
number of mistakes made by the transformed algorithm is bounded by the
number of queries used by the query algorithm. We now give the details
of these transformations.
Algorithm transformation 1 Given a mistake-bounded learning algo-
rithm A, this transformation yields a query algorithm B for the same target
class. The first query of the derived algorithm B is the initial hypothesis
of algorithm A. Algorithm B waits for a response to this query and then
repeats the following for the first response and the response to each subse-
quent query: if the response indicates that the query specified the correct
target function, then algorithm B halts and reports the correct target func-
tion; otherwise, the response to the latest query includes a counterexample.
The derived algorithm gives this instance to algorithm A. After receiving
A's prediction, it tells A that the prediction was incorrect. (Algorithm B
knows that A will be wrong here, since the last query was just the current
hypothesis of A, and by definition the current hypothesis tells how A will
respond to the next instance. ) Algorithm B takes the new hypothesis of
algorithm A and uses it as the next query, continuing in this fasl~ion until
it determines the correct target function.
Since every query after the first results from a mistake of A, we have the
following theorem:
Theorem 5 The number of queries needed by the derived algorithm to
exactly identify target function f is bounded by MA(f) + 1.
Algorithm transformation 2 Now suppose we are given a query algo-
rithm A that achieves exact identification of every function in some target
class C with a bounded number of queries. This transformation yields
a mistake-bounded learning algorithm B for the same target class. The
initial hypothesis of algorithm B is the hypothesis output by the query
algorithm as its initial query. Algorithm B uses this hypothesis to respond
to all instances that are received until it is told that it has made a mistake.
Until the first mistake, algorithm A receives no response to its first query.
At the time of the first mistake, algorithm B gives algorithm A a response
to its query: it tells A that its hypothesis was wrong, and reports that the
the queries will not necessarily be given a compact symbolic representation. However,
if the query algorithm is derived from a computationally efficient algorithm for on-line
learning from examples, then the query functions will be represented in a form that can
be efficiently evaluated.
N. LITTLESTONE
instance at which a mistake was made is a counterexample. Algorithm B
now waits to make any further predictions until A either makes another
query or halts and reports the correct target function. Since A achieves
exact identification, one of these events will occur. The hypothesis given
in the Pnew query (or the reported target function) becomes the new cur-
rent hypothesis of algorithm B. The derived algorithm B proceeds in this
manner indefinitely.
The next theorem follows immediately.
Theorem 6 For any target function f E C, the number of mistakes made
by the derived algorithm in learning f is bounded by the number of queries
needed by algorithm A to exactly identify f.
One can also convert a mistake-bounded algorithm into an algorithm that
learns effectively in the probabilistic model introduced by Valiant 
and described by Blumer et al. . Angluin refers to
this model as pac-learning, where pac stands for "probably approximately
correct." One way to perform the conversion essentially follows a method
discussed by Kearns, Li, Pitt, and Valiant for using failure bounds
to derive probabitistic learning results. Alternatively, one can use an indi-
rect route: one can convert a mistake-bounded algorithm into an algorithm
for exact identification using equivalence queries, and then use a conversion
described by Angluin to obtain an algorithm for the probabilistic
Other general algorithm transformations are possible. Sometimes it is
useful to have an algorithm that changes its hypothesis only when a mistake
occurs; Haussler has referred to such methods as conservative. One
can transform a mistake-bounded algorithm into a conservative algorithm
with the same mistake bound. Haussler has referred to such methods
as failure-bounded. One way to convert a mistake-bounded algorithm to a
conservative algorithm is to use the above transformations to convert it first
to an equivalence query algorithm and thence back to a mistake-bounded
algorithm. The mistake bound increases by one if the above theorems
about the transformations are applied as they stand. With more careful
analysis of the double conversion, the increase disappears. The conversion
to a conservative algorithm is also straightforward to perform directly.
5. The linear-threshold algorithm
Now we describe our main algorithm, first describing the classes of target
functions.
We will consider linearly-separable Boolean functions, which
are those functions that can be computed by a one-layer linear-threshold
network such as a perceptron. A function from {0, 1} n to {0, 1} is said to
LEARNING QUICKLY
be linearly separable if there is a hyperplane in R n separating the points
on which the function is 1 from the points on which it is 0. Monotone
disjunctions constitute one class of linearly-separable functions.
Definition 5 A monotone disjunction is a disjunction in which no literal
appears negated, that is, a function of the form
f(xl .... ,xn) = xil V... V xik.
The hyperplane given by xil + "" + xik = 1/2 is a separating hyperplane
for f(xl,..., xn) = xil V... V xik. We will present two variants of our algo-
rithm. The first variant, which we now present, is specialized for learning
monotone disjunctions. We will later describe a simple transformation to
remove the monotone restriction.
Algorithm 3 (WINNOW1)
We call this algorithm "WINNOW" because it has been designed for
efficiency in separating relevant from irrelevant attributes. We will present
the algorithm as a linear-threshold algorithm. The instance space is X =
{0, 1 }". The algorithm maintains non-negative real-valued weights w~,...,
Wn, each having 1 as its initial value. The algorithm also makes use of a
real number 0, which we call the threshold. When the learner receives an
instance (xl,..., Xn), the learner responds as follows:
• If ~ wix i > 0, then it predicts 1;
• If ~ wixi <_ 0, then it predicts 0.
The choice of prediction when ~ wixi = 0 is not critical for our results.
The weights are changed only if the learner makes a mistake, and then
only the weights corresponding to non-zero xi are changed. The amount
by which the weights are changed depends on a fixed parameter a > 1.
Good bounds are obtained if 0 is set to n/2 and a is set to 2. We will
say more about the values of a and 0 later. Table 2 describes the changes
made to the weights in response to different combinations of prediction and
reinforcement. The threshold is left fixed.
Note in Table 2 that we have given each type of update action a name;
each mistake corresponds to a single promotion step or to a single elimi-
nation step. The space needed (without counting bits per weight) and the
sequential time needed per trial are both clearly linear in n. Note that the
non-zero weights are powers of a. We will prove that the weights are at
most a0. Thus if the logarithms (base a) of the weights are stored, only
N. LITTLESTONE
Table 2. WINNOW 1 's response to mistakes.
prediction
wi := 0 if x~ = 1
elimination
wi unchanged if xi = 0
wi := c~ • wi if xi = 1
wi unchanged if xi = 0
O(logz log~ O) bits per weight are needed. The running time needed to cal-
culate predictions and changes to the weights could be reduced greatly by
parallel implementation, such as with an appropriately constructed neural
net. For a mistake bound we give the following theorem.
Theorem 7 Suppose that the target function is a k-literal monotone dis-
junction given by f(xl,..., Xn) = xil V"" V xik. If WINNOW1 is run with
c~ > 1 and 0 > 1ICY, then for any sequence of instances the total number of
mistakes will be bounded by c~k(log a 0 + 1) + 9"
For example, if 0 = n and c~ = 2 then the mistake bound is 2k(log 2 n +
n the bound simplifies to c~k log a n + a. For c~ = 2
1)+1. If we set0=~,
this gives a bound of 2k log 2 n + 2. The dominating first term is minimized
for c~ = e; the bound then becomes lo-~2 e k log 2 n + e < 1.885k log 2 n + e.
We will prove this theorem by finding bounds on the number of promo-
tion and elimination steps that occur. First we give three lemmas used in
tile proof.
Let u be the number of promotion steps that have occurred by the end
of some sequence of trials and let v be the number of elimination steps that
have occurred by the end of the same sequence of trials.
v<~+(a-1)u.
PROOF: Consider how the sum ~i=1 wi changes over time. Initially the
sum is n; promotion and elimination steps cause it to change. Each pro-
motion steps increases this sum by at most (a - 1)0, since when a promo-
tion step occurs we have ~ilz~=l wi <_ O. Each elimination step decreases
~=1 wi by at least 0. Since the sum is never negative we have
giving the desired result.
LEARNING QUICKLY
Lemma 4 For all i, wi G c~O.
PROOF: Since 0 > 1/c~, the weights are initially less than or equal to c~0.
For any j, the value of wj is only increased during a trial in which xj = 1
and ~] WiX i ~ O. These conditions can only occur together if wj < 0
immediately prior to the promotion. Thus wj <_ c~O after the promotion. •
Lemma 5 After u promotion steps and an arbitrary number of elimination
steps, there exists some i for which logc~ wi >_ u/k.
PROOF: Let R = {ia,...,ik}.
We look at how the product yIiffRV.;i is
changed by elimination and promotion steps. Note that f(xl,..., xn) = 0
if and only if xi -= 0 for all i E R. Elimination steps occur only when
f(x~,...,Xn) = 0; promotion steps occur only when f(xl,...,xn)
Thus l-IieR wi is unchanged by elimination steps and is increased by a
factor of at least c~ by each promotion step. Initially l-[ieR wi = 1. Thus
after u promotion steps ]-IieR wi >_ a ~', giving ~ieR l°ga wi >_ u. Since
IR[ = k, for some i E R we have log awi > u/k, as desired.
Note that only the last of these lemmas depends on the form of the target
function and it is only there that k appears.
PROOF OF THEOREM 7: The total number of mistakes made during a
run of the algorithm is equal to the number of promotion steps, u, plus
the number of elimination steps, v. We bound u using the last two lemmas
and then use the first lemma to bound v. Combining lemmas 4 and 5 we
u/k < log~ wi <_ logo 0 + l,
Lemma 3 now gives
u < k(log 0 + 1),
Adding the bounds on u and v leads to the desired bound on the total
number of mistakes.
Note that the above algorithm does not depend on k. Thus the algorithm
can learn the entire target class of monotone disjunctions without modifi-
cation. The mistake bound depends on the number of literals in the actual
target concept. Now for 1 _< k _< n, let Ok denote the class of k-literal
monotone disjunctions, and let Ck denote the class of all those monotone
disjunctions that have at most k-literals. Suppose one wants to specialize
N. LITTLESTONE
the algorithm to learn the target class Cko efficiently for a particular k0. If
one chooses 0 = 5~ko, then the mistake bound becomes
.k log. V0 + ~k0 _< -k0(1 + log~ V00 )
when the target function is a k-literal monotone disjunction in Cko. For
c~ = 2 this gives a bound of 2ko(1 + log 2 Vo)" For c~ = e we obtain the
bound ko(e + 1.8851og 2 n)ko"
We now give a lower bound on the number of mistakes needed to learn
Ck and Ck.
Theorem 8 (lower bound)
For 1 <_ k < n, opt(Ck)
k[log 2 ~J. For n > 1 we also have opt(Ck) >_ ~(1 + log 2 -~).
>_ opt(dl~) >
The second form gives a formula directly comparable to the upper bound
above. When the above algorithm is specialized for a particular Ck and
when n > 1, the algorithm is within a constant factor of being optimal.
PROOF: Since Ca C Ck, it is clear that opt(Ck) >_ opt(Ok). By Theorem
4, any algorithm to learn a concept class will have a mistake bound at. least
equal to the Vapnik-Chervonenkis dimension of the concept class. In the
following lemma we show that the Vapnik-Chervonenkis dimension of Ck
is bounded below by k[log 2 ~J. This gives the first part of the theorem.
We will split the derivation of the second formula from the first into two
cases, depending on whether or not k < ~ If k < n then log s ~ > 1. Thus
n -1) > k(2+log2 n -1) > ~(l+log 2 n
k[log2 ~J > ~(2Llog2 ~J +log2 ~
as desired. If n > k > ~, then opt(Ck) > opt(C[_}j ) since CI~ j _c Ck. (Here
we use the assumption that n > 1.) We have
opt(dt~ ~) >_ L2JLlog2 k-~-J >~ [~]Llog22J
We also have
-k(1 + log2- ) <
log 2) = -
n _ 1 giving the desired result. •
For n > 2, this is less than or equal to
We will prove a more general lemma than is needed here, since it will give
us results that will be useful later. Note that k-literal monotone conjunc-
tions are just 1-term monotone k-DNF formulas. Also/-literal monotone
disjunctions are/-term monotone 1-DNF formulas.
LEARNING QUICKLY
Lemma 6 For 1 < k < n and 1 < l < (~), let C be the class of functions
expressible as l-term monotone k-DNF formulas and let m be any integer,
such that ('k > l. Then VCdim(C) > kl[log2
Note in particular that if l is 1 then we can take m to be k and if k is 1
then we can take m to be I.
PROOF: Let r = [log 2 nj. If r = 0, then the theorem follows trivially,
since C is non-empty. Assume r > 0. Let s = 2 r. Note that ms ~_ n. We
will construct a set S c {0, 1} n containing klr points that is shattered by
C. To describe the construction we will need to refer to an enumeration
of the (~) ways to choose k distinct integers from the set {1,..., m}. Let
{ (~;jl,..., ~Cjk)}, where j runs from 1 through (~n), be such an enumeration.
(The values of the aji are the chosen integers.) We will construct S as the
union of sets @i for i = 1,..., k and J' = 1,...,l. Each set Sji contains
r points and each point has n coordinates. Split these coordinates into
groups so that there are m disjoint groups of s coordinates each. There
may be some coordinates left over; we will not make them a part of any
group. Number the groups from 1 through m. Fix attention on some i and
j. Let all coordinates of each point in Sji be 0 except for coordinates in
the groups numbered tgjl through njk. Let the coordinates in groups ~;j~
through n3k be 1 except for those in group ~ji. The coordinates in group
gji are used to distinguish the points within set @i. Set the coordinates in
this group to 0 or 1 in such a manner that for each subset. V _ @i, there
is a corresponding coordinate in group gji that is 1 at points in V and 0
at points in Sji - V. This is possible since there are 2 r subsets of S~i and
there are 2 r coordinates in the group. For example, suppose n = 24, k = 2,
and l = 3. If we take m -- 3, then we get r = 3. Picking (1, 2), (1, 3), (2, 3)
as the enumeration of the three ways to choose two integers from {1, 2, 3},
we can take the sets Sji as shown in Table 3.
Now we show how to construct an/-term k-DNF formula that is 1 exactly
on some arbitrary subset U C_ S. Each of the I terms of this formula will
have length exactly k, and no literal will be negated. Let Uji = U ~ Sji.
We will express the formula in terms of n variables, with one variable
corresponding to each coordinate. This gives m groups of variables, corre-
sponding to the m groups of coordinates. The jth term will contain one
variable from each of the groups ~jl,..., ~jk. We choose the ith variable
in the jth term from group ~cji so that it is 1 at all of the points in Uji and
0 at points in Sji - Uji. This is possible due to the way the sets @i were
constructed.
To see that this formula is 1 on U and 0 on S - U, consider any point
x E S. This point will be in Sji for some i and j. The coordinates of
the point, will be 0 except in groups ~;jl,-.-, ~cj~. Thus every term but the
N. LITTLESTONE
Table 3. An example of the sets Sji.
Su = {(o,o,o,0,1,1,1,1,
(0,0,1,1,0,0,1,1,
(0,1,0,1,0,1,0,1,
S12 = {(1,1,1,1,1,1,1,1,
(1,1,1,1,1,1,1,1,
(1,1,1,1,1,1,1,1,
$2~ = {(0,0,0,0,1,1,1,1,
(0,0,1,1,0,0,1,1,
(0,1,0,1,0,1,0,1,
$22 = {(1,1,1,1,1,1,1,1,
(1,1,1,1,1,1,1,1,
(I,I,I,I,i,i,I,1,
$3~ = {(o,o,o,0,0,0,0,0,
(0,0,0,0,0,0,02,
(0,0,o,o,0,0,o,o,
832 = {(0,o,o,o,0,0,0,0,
(0,0,0,0,0,0,0,0,
(o,o,o,o,o,o,o,o,
1,1,1,1,1,1,1,1,
1,1,1,1,1,1,1,1,
1,1,1,1,1,1,1,1,
0,0,0,0,1,1,1,1,
0,0,1,1,0,0,1,1,
0,1,0,1,0,1,0,1,
0,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,
0,0,0,0,0,0,0,0,
0,0,0,0,1,1,1,1,
0,0,1,1,0,0,1,1,
0,1,0,1,0,1,0,1,
1,1,1,1,1,1,1,1,
1,1,1,1,1,1,1,1,
1,1,1,1,1,1,1,1,
o,o,o,o,o,o,o,o),
o,o,o,o,o,o,o,o),
o,o,o,o,o,o,o,o)}
o,o,o,o,o,o,0,o),
o,o,o,o,o,o,o,o),
o,o,o,o,o,o,o,o)}
1,1,1,1,1,1,1,1),
1,1,1,1,1,1,1,1),
1,1,1,1,1,1,1,1)}
0,0,0,0,1,1,1,1),
o,o,1,1,o,o,1,1),
O,l,O,l,O,l,O,1)}
1,1,1,1,1,1,1,1),
1,1,1,1,1,1,1,1),
1,1,1,1,1,1,1,1)}
0,0,0,0,1,1,1,1),
o,o,1,1,o,o,1,1),
0,1,0,1,0,1,0,1)}
jth will contain at least one variable that is 0 at x. Therefore the formula
will be 1 if and only if the jth term is 1. The coordinates of x will be 1
in groups Kjl,..., ~;jk, except possibly in group gji. Hence all variables in
the jth term will be 1, except possibly for the ith variable. Therefore the
value of the formula at x will match the value of the ith variable of the jth
term. This variable will be 1 if z E Uji and 0 if x E Sji - Uii, as desired. •
The algorithm can be modified to work on larger classes of Boolean
functions. For any instance space X C_ {0, 1} n, and for any 6 satisfying
0 < 6 _< 1 let F(X, 6) be the class of functions from X to {0, 1} with the
following property: for each f E F(X, 6) there exist /~,..., #,~ > 0 such
that for all (Zl,...,xn) E X
if f(x~,..., Xn) = 1
In other words, the inverse images of 0 and 1 are linearly separable with a
minimum separation that depends on & We will present a second variant
of WINNOW that can handle target classes of this form.
LEARNING QUICKLY
Table 4. WINNOW2's response to mistakes.
prediction
wi := wi/a if xi = 1
wi unchanged if xi = 0
wi := a • wi if xi = 1
wi unchanged if xi = 0
The mistake bound that we derive will be practical only for those linearly-
separable functions for which 5 is sufficiently large. For example, these
include the Boolean r-of& threshold functions. Let X = {0, 1} '~. An r-of&
threshold function f(xl,...,
xn) is defined by selecting a set of k significant
variables. The value of f is 1 whenever at least r of these k variables are
1. If the k selected variables are xi,,...,xi~,
then f is 1 exactly when
xi~ + • •. + xik > r. Equivalently, f is 1 when
--~il -t- "'" -'~ -xi k >_ 1.
The value of f is 0 when no more than r - 1 of the selected variables are
1. In this case
+ ' " + - xik < 1--.
Thus the r-of& threshold functions are contained in F({0, l} n, ~).
There exist other classes of linearly-separable Boolean functions for which
! grows exponentially with n when the instance space is {0, 1} n . One example of a set of functions with
exponentially small 5 consists of
f(xl,...,x~) = m, V (x~ A (xa V (m4 A...x~))...)
as n varies. For such functions, the mistake bound that we will derive grows
exponentially with n. We now give a description of the second variant of
Algorithm 4 (WINNOW2)
The only change to WINNOW 1 involves the amount by which the weights
are changed when a mistake is made. In a promotion step, as before, we
multiply the weights by a fixed a > 1. But now, instead of setting weights
to zero in an elimination step, we divide them by a. (We now call this a
demotion step.) We must now be more careful in our choice of c~. For the
N. LITTLESTONE
mistake bound that we derive below, we use a = 1 + 5/2 for learning a
target function in F(X, 8).
Table 4 describes WlNNOW2's responses to different types of mistakes.
Space and time requirements for WINNOW2 are similar to those for WIN-
NOW 1. However, more bits will be needed to store each weight, perhaps as
many as the logarithm of the mistake bound. The following theorem gives
a mistake bound for WINNOW2.
Theorem 9 For 0 < 5 < 1, if the target function is in the class F(X, 8)
for X C_ {0, 1} ~, if pl,...,#n have been chosen so that the target function
satisfies the inequalities (1) and (2), and if Algorithm 4 i8 run with a =
1 + ~ and 0 > 1 and the algorithm receives instances from X, then the
number of mistakes will be bounded by
141n0).-~.
Before proving this theorem, we will state and prove three lemmas anal-
ogous to the lemmas used to prove Theorem 7. We define u and v in the
same manner as for those lemmas. The current lemmas do not depend on
the particular choice of a given in Theorem 9.
v<_ ---+au.
PROOF: We will examine how the weights are changed by promotion and
demotion steps. We will use Wi,bef tO denote weights at the beginning of
a trial in which a promotion or demotion occurs, and Wi,af t to denote the
weights resulting from the promotion or demotion. For a promotion step,
we can write the update rule as
Wi,aft = Wi,bef + (OZ -- 1)XiWi,bef
for i = 1,..., n.
Since a promotion step only occurs when ~int Wi,befX i <~ O, we have
for a promotion step. For a demotion step, we have
Wi,bef -- (1 - 1--)xiWi,bd
for i = 1,..., n.
LEARNING QUICKLY
A demotion step only occurs when ~=1 Wi,befZi > O. Thus
Wi,aft ~ ~
Wi,bef -- (1 - l/s)0.
Initially, the sum of the weights is n; hence after u promotions and v
demotions,
Since the weights are never negative, we must have n + u(a - 1)0 - v(1 -
1), giving v < h-~-l~ + su, as
l/s)0 > 0. Thus v(1
Lemma 8 For all i, wi <_ sO.
PROOF: Since 0 > 1 and s > 1, the weights are initially less than or
equal to sO. For any j, the value of wj is only increased during a trial in
which xj = 1 and ~n wixi <_ O. These conditions can only occur together
if wj <_ 0 immediately prior to the promotion. Thus wj < sO after the
promotion.
Lemma 9 After u promotion step8 and v elimination steps, there exi2ts
8ome i for which
log wi > u - (1 - ~i)v log s.
PROOF: We will use the symbols Wi,be f and wi,~ as in the proof of Lemma
7. This time we look at what happens to ~i~__1 #i log wi. We can write the
promotion update rule as
Wi,af t =- sXiwi,bef.
Taking the logarithm and multiplying by #i, we get
~i log Wi,af t = ~i log Wi,be f + ]AiX i log s.
A promotion step only occurs when
~i=l #iXi ~ 1. Thus, at a promotion
step we have
/~i log wi,aft ~ ~ #i log Wi,be f + log a.
N. LITTLESTONE
At a demotion step we have
Wi,aft ~ O~-xiWi,bef.
#i log Wi,aft = #i log Wi,be f -- ~iXi log a.
For a demotion step to occur, we must have ~i=; #ixi <_ 1 - 6. Thus, at a
demotion step we have
#~ log ~,~ > ~ #~ log ~,bof - (1 - ~) log ~.
Initially,
~i=1 #i log wi = 0. After u promotion steps and v demotion steps,
~ #i log wi >_ ulog~-
(1 - 6 )v log a.
Since the #i are non-negative, we get
( max log wi) E
#¢ > [u - (1 - 5)v] log c~,
and dividing by ~ pi gives the desired result.
PROOF OF THEOREM 9" From lemmas 8 and 9 we get
u - (1 - 5)v log a <_ log a + log O.
Since c~ > 1 and the #i are non-negative, we can rewrite this inequality as
A second inequality involving u - (1 - 6)v results from using Lemma 7 to
eliminate v from the expression. This gives us
- (a - ~)v >_ ~ - (1 - ~)(7c-~- 1 ~ + a~)'
and using the value for c¢ given in the theorem, we get
> ~-(1-6) 2 + 6 n
(1-6)(1+~)~
(1- 6)(2 + 6)
LEARNING QUICKLY
Combining the two inequalities involving u - (1 - 5)v, we get
(1-5)(2 +5) n < (1 +
)~--~. pi,
(5 + 5 .> -
log(~ + ~) ,.=~
and therefore
(5 < (1-5)(2+5)~
0 + (1 + log(1 + ~
From Taylor's formula with remainder, we get
In(~ + 5) > 2
and since 5 < 1 we get
ln(1 + 5) > 3(5/8.
Thus, since we have assumed that 0 _> 1,
(1 - 6)(2 + (5) n
+ (1 + a77g/s/~ ui.
From Lemma 7, we have a bound on the total number of mistakes:
+ (a+ 1)u.
(i- a)(2 +5) n
~+v<_ -g-~ +-5-~[
ln0 . ,--L,
=((2+5)6+(4+5)(1-5)(2+5)5
)~n + ---~--
(14+5 + -~)~_..,#i.8
Using 0 < 5 <_ 1, we can simplify the upper bound to get
14 In 0.,2-,
as desired.
N. LITTLESTONE
For the earlier example involving r-of-k threshold functions, we have
6= 71 and ~=1 #i -7"-
k Thus we get a mistake bound for r-of-k threshold
functions for a = 1 + ~r and 0 = n of 8r 2 + 5k + 14kr Inn. We do not know
lower bounds for this concept class which are comparable to this upper
bound. Note that 1-of-k threshold functions are just k-literal monotone
disjunctions. Thus if a = 3/2, WINNOW2 will learn monotone disjunctions.
The mistake bound is similar to the bound for WINNOW1, though with
larger constants.
6. Transformations to other target classes
Various transformations are possible that let one apply the above al-
gorithms to other classes of functions. One can think of these transfor-
mations as letting one derive a new learning algorithm from an existing
one. The transformations that we will describe here take the form of map-
pings applied to the instances and predictions. If the instance space of
the derived algorithm is X1 and that of the original algorithm is Xs, then
the transformations will take the form of functions Ti : X1 ~ X2 and
Tp : {0, 1} ~ (0, 1}. We will always take Tp to be either the identity or
the function that interchanges 0 and 1 (negation); thus Tp will be invert-
ible. When the derived algorithm receives an instance x E X1, it sends the
instance T/(x) to the original algorithm, which generates the prediction
y. The derived algorithm then generates the prediction Tp(y). Finally, to
conclude the trial, when a reinforcement is received, the derived algorithm
sends it to the original algorithm. (The reinforcement is passed along with-
out transformation since we view it as a message saying "right" or "wrong"
rather than as a message containing the value of the correct response).
Suppose we start with an original algorithm A and we want to derive
an algorithm to learn some target class C. What we seek is a target class
Co that can be learned by A and mappings T~- and Tp such that for every
g c C, there exists an f E Co such that TpofoTi
= g. We have the
following theorem.
Theorem 10 Suppose we are given transformation Ti : X1 --~ Xs, in-
vertible transformation Tp : (0, 1} ~ (0, 1}, an original algorithm A that
can accept instances from X2, and a derived algorithm B constructed from
these a8 described above. Suppose that we wish algorithm B to learn a tar-
get function g : X1 ~ (0, 1}. If f : X2 ~ (0, 1} is a function that can be
learned by A with a bounded number of mistakes, and if Tp o f o Ti = g,
then algorithm B will learn g making at most MA(f) mistakes.
PROOF: Let y be the prediction that the derived algorithm B makes in
response to some instance x. For algorithm B to make this prediction,
algorithm A must have made the prediction Tp-l(y) in response to the
LEARNING QUICKLY
instance Ti(x).
We have Tp-l(y) = f(Ti(x)) if and only if y = g(x).
Algorithm A is told that it has made a mistake when the derived algorithm
makes a mistake. From the above we see that this happens exactly when
the response of A to an instance Ti(x) is not equal to f(Ti(x)). This can
happen at most MA(f) times.
These transformations are similar in effect to the substitutions described
by Kearns, Li, Pitts, and Valiant .
Now we consider some examples of ways that these transformations can
be used to extend the classes of flmctions learnable using WINNOWl and
WINNOW2. For each example, we show that the transformation satisfies
the condition given in Theorem 10, namely that for any desired target
function g, there exists a function f in a target class that can be learned
by W1NNOWl or WINNOW2 and for which Tp o f o Ti = g. Note that in
any case in which we use WINNOW1, WINNOW2 could also be used.
Example 4 Learning arbitrary disjunctions.
This is an example of one
way to learn disjunctions that are not necessarily monotone. Arbitrary
disjunctions are also special cases of tile classes discussed in Examples 6
and 7 below. We will use W/NNOWl, but the learner does not send the
first instances to WINNOW1. Instead, the learner just responds 1 until
the first mistake is made.
This will be an extra mistake, not counted
in the bound for WINNOW l. Then the learner starts using WINNOW1,
using transformations defined as follows. Suppose (Zl .... , z~) is the first
instance on which a mistake is made. Then we let Ti : {0, 1} n ~ {0, l} n
be the function given by
Ti(Xl,..., Xn) ~- (Xl -]- Zl ..... Xn ~- Zn),
where the addition is modulo 2. We let Tp be the identity.
To construct the function f of Theorem 10, write the target function g
g(zl,...,Xn) = zi~ V'" V xi~ V 2j~ V'" V 2j,,
for some l and m. Since g(zl,..., zn) = 0 we must have zi~ = ....
and zj, ....
= zj,~ = 1. Let
f(xl,...,xn) = xi~ V... Vxi~ Vxj, V... Vxj,,~.
f oTi(xl .... , X,n) :
X~I V'." V Zil V (Xjl -~ 1) V'" V (:cjm 4- 1) = g(xl,..., xn),
as desired. The mistake bound for learning non-monotone disjunctions
with this method is one more than the corresponding mistake bound for
monotone disjunctions.
N. LITTLESTONE
Example 5 Learning k-literal monotone conjunctions. We use WINNOW 1.
Let Ti(xi,...,xn) = (1-xl,...,1-xn)
and Tp(r) = 1-r. If one thinks of
0 and 1 as false and true, then the transformations Ti and Tp just negate all
of their arguments. Thus if the target function g(xl,..., Xn) = xi~ "'" xik
(i.e., the conjunction of these k variables) and if we let f(xl,...,Xn) =
xi~ V ... V xik, then Tp o f o Ti = g by de Morgan's law. Using WIN-
and (~ = 2, the nmnber of mistakes will be bounded by
NOW 1 with 0 =
2k log2 n + 2.
Example 6 Learning linearly-separable Boolean functions with weights that
vary in sign. For X C {0,1} n and 0 < 6 _< 1, let G(X, 6) be the class
of functions g • X ~
{0,1} for which there exist vl,...,L'n _> 0 and
t)l,... ,~n >_ 0 depending on g such that for all (xl,... ,xn) E X,
~(L'ixi + 5i(1 - xi)) >__ 1
if g(xl,...,x~) = 1
We will first give a transformation to learn G(X, 5), and then demonstrate
that any linearly-separable Boolean function with domain X is in G(X, 5)
for some 6. To learn functions in G(X, 6), we use WINNOW2 and the
transformation 7~ : {0, 1} n --* {0, 1} 2" given by
Ti(xl,...,Xn) -- (Xl,X2,...,Zn, l-Xl,1
We let Tp be the identity. For any function g ~ G(X, 5) we can find a
function f E F(Ti(X), 5) for which Tp o f o Ti = g, satisfying the condition
of Theorem 10. To define f, let ~,~,..., ~'n and 51,..., 9n be as above. Let
pi = L,i for i = 1,..., n, and let #i = ~i-n for i = n + 1,..., 2n. Then the
function f that is 1 if and only if
~i=1 PtiZi ~ 1 is the desired function.
The mistake bound of Theorem 9 applies, except that n must be replaced
with 2n, and the sum ~i~l Pi with ~i';l(~,i + ;'i).
Now we show that any linearly-separable Boolean function f is in G(X, 5)
for some 6. To see this, first observe that the function that is identically
1 on X is in G(X, 1); we can take vi = ~i = 1 for i = 1,...,n. Now take
g to be any linearly-separable Boolean function which is not identically 1.
We can find Pl,...,#~, 0 and 0' < 0 such that for all (xl,...,x~) E X,
ttixi >_ 6
if g(xl,...,xn) = 1
ifg(:cl .... ,xr~)=0.
Here we allow the ¢i to vary in sign. Now for each i choose ~+, #~- > 0
such that #i = #+ - #[ and either p+ or #[ is 0. Then
if g(xt, .... xn)=l
if g(xl,... ,x~) = O.
We will next divide each of these inequalities by 0 + ~in=l #[. Note that
since g is not identically 1, we have
Hence 0' + ~n
i=1 ]Ai ~ 0 and 0 + ~i=1 ~? ~> 0. We obtain the inequalities
~=1 o + EL, ~/- x,: + o + E?:I ~-
if g(xl,...,
i=1 0 +~,:=~
O+E '~ i--1 v,[
if g(xl, .. ., Xn) = 0. Thus g is in G(X, ~+~i=1°-°"~ ~ ).
Example 7 Learning k-DNF for fixed k.
This transformation demon-
strates the use of WINNOW 1 to learn functions that are not linearly sep-
arable. The class k-DNF consists of functions that can be expressed in
N. LITTLESTONE
disjunctive normal form with at most k literals per term. Valiant 
and Kearns et al. have studied this class. To learn k-DNF, we let
n2 = Ei=0 2 (~). Let
Ti(Xl,...,Xnl
(Cl(Xl,...,Xnl),c2(xl,...,Xnl),...,Cn2(Xl
.... ,Xnl) )
where tile ci(xl,..., xn~ ) range over all conjunctions that form valid terms
of a k-DNF formula, i.e., all conjunctions of no more than k literals. We let
Tp be the identity. For any k-DNF target function g with 1 terms, there exist
il,...,it such that g(xl,...,xn) is the disjunction of cil(zl,...,Xn),...,
Ci~(Xl,...,Xn). Let f" {0,1} n2 --* {0,1} be defined by I(Yl,...,Yn2) =
Yil V'" V yi~. Then g = f o Ti as desired.
One can show that n2 _< (2n) k + 1. To WINNOWl, it will appear that
the function being learned is an/-literal monotone disjunction. Thus if the
target concept has l terms, WINNOW 1 will make O(l log n k) = O(kl log n)
mistakes. By contrast, the algorithm for learning k-DNF and similar classes
presented by Valiant can be forced to make (~) - 1 mistakes,
which is roughly n k mistakes when 1 is small. Lemma 6 gives a lower
bound on the Vapnik-Chervonenkis dimension of the class of /-term k-
DNF formulas. This is also a lower bound on the mistake bound. In that
lower bound, take m = [kll/k]. We have
as required. Thus a lower bound on the mistake bound, in the case that
kl 1/k < n, is
If we know 1 and run WINNOW1 with c~ = 2 and a = ~, then the number
of mistakes made by the derived algorithm will be bounded by
(2n) k + 1
2l(1 + log2
) <-2l(2+1°g2 ~ )
=41+2kll°g211/t:"
For fixed k, this is similar in form to the lower bound.
7. Conclusion
This paper divides into two parts. The first part contains general results
about how many mistakes an effective learner might make if computational
complexity were not an issue. The second portion describes an efficient
algorithm for learning specific target classes. The general results of the
LEARNING QUICKLY
first part lead to the inequalities:
VCdim(C) <_ opt(C) <_ MHALVING(C) <_ log2(JCI)
for any non-empty target class C. Examples in Section 3 demonstrate
that VCdim(C) can be 1 for classes for which opt(C) is large, and that
MHALVING(C ) can be 1 for classes for which iog~(ICI) is large. Example 2
also shows that opt and MHALV[N G can differ. There also exist classes C
for which VCdim(C) = log2(ICI), making all of the above inequalities into
equalities. (This is true of any class that contains exactly those concepts
required to shatter some particular finite subset of the domain.)
When we turn to efficient algorithms, we find that WINNOW1, WIN-
NOW2, and their transformations do very well for certain concept classes.
These include disjunctions, conjunctions, r-of-k threshold flmctions, other
classes of linearly-separable functions with sufficiently large separation, and
some classes of non-linearly-separable functions, such as k-DNF for fixed
The results here contrast with those of Kearns et al. , who have
demonstrated that if P ¢ NP and if the learner is required to choose
hypotheses from the target class, then r-of-k threshold functions are not
polynomially learnable in the Valiant learning model. Using methods that
we mentioned in Section 3, WINNOW2 (which learns r-of-k threshold func-
tions) can be converted into a polynomial learning algorithm in the Valiant
model. This algorithm succeeds in efficiently learning r-of-k threshold
functions by choosing hypotheses from a larger class of Boolean functions.
WINNOW 1 and WINNOW2 are natural algorithms for parallel implementa-
tion; Slade has implemented WINNOW on the Connection Machine.
A key advantage of WINNOWl and WINNOW2 is their performance
when few attributes are relevant. If we define the number of relevant
variables needed to express a function in the class F({0, 1} n, ~) to be the
least nmnber of strictly positive weights needed to describe a separating
hyperplane, then the bounds for WINNOWl and WINNOW2 tell us that
the target class F({0, 1} ~, 6) for n > 1 can be learned with a number of
mistakes bounded by a constant times ~
when the target function can
be expressed with k relevant variables. (This follows from the bound given
in Theorem 9 using the observation that, in the inequalities (1) and (2)
in the definition of F(X, 5), any #i larger than 1 can be set to 1 without
changing the function.)
Note that WINNOWl (for the target class F(X, 1)) and WINNOW:2,
achieve this bound without necessarily producing a hypothesis expressed
with few significant weights. For example, if several attributes match each
other in every instance, then their weights will always match, and a hy-
pothesis making significant use of any of them will make use of all.
N. LITTLESTONE
One theme that recurs in this paper is transformation from one algorithm
to another. We have discussed transformations of several kinds, including:
• transformations from algorithms for one target class to algorithms for
another target class;
• transformations between mistake-bounded algorithms and query algo-
• transformations from mistake-bounded algorithms to algorithms that
provably perform well in a probabilistic learning model;
• transformations from arbitrary mistake-bounded algorithms to "nor-
realized" nlistake-bounded algorithms (e.g., the transformation to a
conservative algorithm).
Transformations can also be used with the hope of improving the behavior
of an algorithm for a target class it is already capable of learning. In this
regard, notice that monotone conjunctions can be learned by WINNOW2
with or without the use of the transformation described in Example 5. A
k-literal monotone conjunction is just a k-of-k threshold flmction. Using
WINNOW2 to learn a k-of-k threshold function, we have a mistake bound
of 5k + (8 + 141nn)k 2, which applies when c~ = 1 + ~ and 0 = n. If
we use the transformation of Example 5, we end up using WIBINOW2 to
learn a derived 1-of-k threshold function. In this case, the mistake bound
is 8 + (5 + 141nn)k with c~ = 3 and 0 = n, which is better by a factor
of k than the bound without the transformation. It is not clear to what
extent this difference is an artifact of our analysis. However, note that to
express a 1-of-k threshold function, any set of weights will work, as long
as the weight of each relevant variable is above the threshold and the sum
of all other weights is below the threshold. There are tighter constraints
on weights used to represent a k-of-k threshold function. The sum of all
weights, omitting only that of any single relevant variable, must be below
the threshold, whereas the weights of the relevant variables must have a
sum above the threshold. This suggests that a k-of-k threshold function
might indeed be harder for WINNOW2 to learn than a 1-of-k threshold
Though we have shown that WINNOW2 is within a constant factor of
optimal for some classes of functions, the ratio of the mistake bound to the
optimum grows as 5 shrinks. The number of Iinearly-separable Boolean
functions of n attributes is at most 2 '~2 for n > 1 . Thus the halving algorithm would make no more than n 2
mistakes learning any such function. The bound for WINNOW2 grows in
proportion to 1/52, and there exist classes for which 1/5 grows exponen-
tially with n. Are there efficient algorithms that close this gap?
LEARNING QUICKLY
One advantage of WINNOWl and WINNOW2 is that they perform well
for functions with few relevant attributes without needing to know the
number of relevant attributes in advance. This is not true with respect to
the separation parameter ~ which affects the choice of the multiplier used
by WINNOW2. For practical problems, it would be useful to have a version
of WINNOW2 that could function without needing to know 5.
We have mentioned that any mistake-bounded algorithm can be trans-
formed into an algorithm that provably performs well in a probabilistic
learning model. One can also run a mistake-bounded algorithm without
transformation but assume that the instances are chosen randomly, and
then examine its behavior in probabilistic terms. It would be interesting
to understand the behavior of WINNOW 1 and WINNOW2 in such a setting.
Finally, when the input data to the learner contains errors, WINNOW 1 is
not robust: if a weight is mistakenly set to zero, the mistake will never be
undone. WINNOW2 can learn all concept classes learnable by WINNOW l,
and it is more robust. We are currently studying its performance when
there are errors in the input data.
Acknowledgements
This research was supported by Contract N00014-86-K-0454 from the
Office of Naval Research. I would like to acknowledge the inspiration pro-
vided by many valuable discussions with David Haussler and Manfred War-
muth, and by the many key questions that they asked. I also benefited
from discussions with Sally Floyd, Ron Rivest, Dana Angluin, and Eli Up-
fal. Dick Karp raised the question of lower bounds for learning monotone
disjunctions, and Larry Stoekmeyer and Manfred Warmuth subsequently
developed ideas relating to these lower bounds. Mike Paterson and Man-
fred Warmuth suggested improvements to the upper bounds. Ideas leading
to complete k-mistake trees were worked out in conjunction with David