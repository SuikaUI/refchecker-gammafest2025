Deeper Insights into Graph Convolutional Networks
for Semi-Supervised Learning
Qimai Li1, Zhichao Han12, Xiao-Ming Wu1∗
1The Hong Kong Polytechnic University
2ETH Zurich
 , , 
Many interesting problems in machine learning are being
revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph
convolutional networks (GCNs), which nicely integrate local
vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other
state-of-the-art methods, its mechanisms are not clear and it
still requires considerable amount of labeled data for validation and model selection.
In this paper, we develop deeper insights into the GCN model
and address its fundamental limits. First, we show that the
graph convolution of the GCN model is actually a special
form of Laplacian smoothing, which is the key reason why
GCNs work, but it also brings potential concerns of oversmoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures,
we propose both co-training and self-training approaches to
train GCNs. Our approaches signiﬁcantly improve GCNs in
learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on
benchmarks have veriﬁed our theory and proposals.
Introduction
The breakthroughs in deep learning have led to a paradigm
shift in artiﬁcial intelligence and machine learning. On the
one hand, numerous old problems have been revisited with
deep neural networks and huge progress has been made in
many tasks previously seemed out of reach, such as machine
translation and computer vision. On the other hand, new
techniques such as geometric deep learning are being developed to generalize deep neural models
to new or non-traditional domains.
It is well known that training a deep neural model typically requires a large amount of labeled data, which cannot
be satisﬁed in many scenarios due to the high cost of labeling
training data. To reduce the amount of data needed for training, a recent surge of research interest has focused on fewshot learning – to learn a classiﬁcation model with
very few examples from each class. Closely related to fewshot learning is semi-supervised learning, where a large
∗Corresponding author.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
amount of unlabeled data can be utilized to train with typically a small amount of labeled data.
Many researches have shown that leveraging unlabeled
data in training can improve learning accuracy signiﬁcantly
if used properly . The key issue is
to maximize the effective utilization of structural and feature information of unlabeled data. Due to the powerful feature extraction capability and recent success of deep neural networks, there have been some successful attempts to
revisit semi-supervised learning with neural-network-based
models, including ladder network ,
semi-supervised embedding , planetoid
 , and graph convolutional networks .
The recently developed graph convolutional neural networks (GCNNs) is a successful attempt of generalizing the powerful convolutional neural networks (CNNs) in dealing with
Euclidean data to modeling graph-structured data. In their
pilot work , Kipf and Welling proposed a simpliﬁed type of GCNNs, called graph convolutional networks (GCNs), and applied it to semi-supervised
classiﬁcation. The GCN model naturally integrates the connectivity patterns and feature attributes of graph-structured
data, and outperforms many state-of-the-art methods significantly on some benchmarks. Nevertheless, it suffers from
similar problems faced by other neural-network-based models. The working mechanisms of the GCN model for semisupervised learning are not clear, and the training of GCNs
still requires considerable amount of labeled data for parameter tuning and model selection, which defeats the purpose
for semi-supervised learning.
In this paper, we demystify the GCN model for semisupervised learning. In particular, we show that the graph
convolution of the GCN model is simply a special form of
Laplacian smoothing, which mixes the features of a vertex
and its nearby neighbors. The smoothing operation makes
the features of vertices in the same cluster similar, thus
greatly easing the classiﬁcation task, which is the key reason why GCNs work so well. However, it also brings potential concerns of over-smoothing. If a GCN is deep with
many convolutional layers, the output features may be oversmoothed and vertices from different clusters may become
indistinguishable. The mixing happens quickly on small
 
Figure 1: Performance comparison of GCNs, label propagation, and our method for semi-supervised classiﬁcation on
the Cora citation network.
datasets with only a few convolutional layers, as illustrated
by Fig. 2. Also, adding more layers to a GCN will make it
much more difﬁcult to train.
However, a shallow GCN model such as the two-layer
GCN used in has its own limits.
Besides that it requires many additional labels for validation,
it also suffers from the localized nature of the convolutional
ﬁlter. When only few labels are given, a shallow GCN cannot
effectively propagate the labels to the entire data graph. As
illustrated in Fig. 1, the performance of GCNs drops quickly
as the training size shrinks, even for the one with 500 additional labels for validation.
To overcome the limits and realize the full potentials of
the GCN model, we propose a co-training approach and a
self-training approach to train GCNs. By co-training a GCN
with a random walk model, the latter could complement the
former in exploring global graph topology. By self-training a
GCN, we can exploit its feature extraction capability to overcome its localized nature. Combining both the co-training
and self-training approaches can substantially improve the
GCN model for semi-supervised learning with very few
labels, and exempt it from requiring additional labeled data
for validation. As illustrated in Fig. 1, our method outperforms GCNs by a large margin.
In a nutshell, the key innovations of this paper are: 1)
providing new insights and analysis of the GCN model for
semi-supervised learning; 2) proposing solutions to improve
the GCN model for semi-supervised learning. The rest of
the paper is organized as follows. Section 2 introduces the
preliminaries and related works. In Section 3, we analyze
the mechanisms and fundamental limits of the GCN model
for semi-supervised learning. In Section 4, we propose our
methods to improve the GCN model. In Section 5, we
conduct experiments to verify our analysis and proposals.
Finally, Section 6 concludes the paper.
Preliminaries and Related Works
First, let us deﬁne some notations used throughout this
paper. A graph is represented by G = (V, E), where V
is the vertex set with |V| = n and E is the edge set.
In this paper, we consider undirected graphs. Denote by
A = [aij] ∈Rn×n the adjacency matrix which is nonnegative. Denote by D = diag(d1, d2, . . . , dn) the degree matrix
of A where di = P
j aij is the degree of vertex i. The graph
Laplacian is deﬁned as L := D −A, and the
two versions of normalized graph Laplacians are deﬁned as
Lsym := D−1
2 and Lrw := D−1L respectively.
Graph-Based Semi-Supervised Learning
The problem we consider in this paper is semi-supervised
classiﬁcation on graphs. Given a graph G = (V, E, X),
= [x1, x2, · · · , xn]⊤∈Rn×c is the feature
matrix, and xi ∈Rc is the c-dimensional feature vector of
vertex i. Suppose that the labels of a set of vertices Vl are
given, the goal is to predict the labels of the remaining vertices Vu.
Graph-based semi-supervised learning has been a popular research area in the past two decades. By exploiting the
graph or manifold structure of data, it is possible to learn
with very few labels. Many graph-based semi-supervised
learning methods make the cluster assumption , which assumes that nearby vertices on a
graph tend to share the same label. Researches along this
line include min-cuts and randomized min-cuts , spectral graph transducer , label propagation and its variants , modiﬁed adsorption
 , and iterative classiﬁcation
algorithm .
But the graph only represents the structural information of data. In many applications, data instances come
with feature vectors containing information not present in
the graph. For example, in a citation network, the citation
links between documents describe their citation relations,
while the documents are represented as bag-of-words vectors which describe their contents. Many semi-supervised
learning methods seek to jointly model the graph structure
and feature attributes of data. A common idea is to regularize a supervised learner with some regularizer. For example, manifold regularization (LapSVM) regularizes a support vector machine with
a Laplacian regularizer. Deep semi-supervised embedding
 regularizes a deep neural network with
an embedding-based regularizer. Planetoid also regularizes a neural network
by jointly predicting the class label and the context of an
Graph Convolutional Networks
Graph convolutional neural networks (GCNNs) generalize traditional convolutional neural networks to the graph
domain. There are mainly two types of GCNNs : spatial GCNNs and spectral GCNNs. Spatial
GCNNs view the convolution as “patch operator” which
constructs a new feature vector for each vertex using its
neighborhood information. Spectral GCNNs deﬁne the convolution by decomposing a graph signal s ∈Rn (a scalar
for each vertex) on the spectral domain and then applying a spectral ﬁlter gθ (a function of eigenvalues of Lsym)
on the spectral components . However this model
requires explicitly computing the Laplacian eigenvectors,
which is impractical for real large graphs. A way to circumvent this problem is by approximating the spectral ﬁlter gθ
with Chebyshev polynomials up to Kth order . In , Defferrard et al. applied this
to build a K-localized ChebNet, where the convolution is
deﬁned as:
kTk(Lsym)s,
where s ∈Rn is the signal on the graph, gθ is the spectral
ﬁlter, ⋆denotes the convolution operator, Tk is the Chebyshev polynomials, and θ′ ∈RK is a vector of Chebyshev
coefﬁcients. By the approximation, the ChebNet is actually
spectrum-free.
In , Kipf and Welling simpliﬁed
this model by limiting K = 1 and approximating the largest
eigenvalue λmax of Lsym by 2. In this way, the convolution
where θ is the only Chebyshev coefﬁcient left. They further
applied a normalization trick to the convolution matrix:
where ˜A = A + I and ˜D = P
Generalizing the above deﬁnition of convolution to a
graph signal with c input channels, i.e., X ∈Rn×c (each
vertex is associated with a c-dimensional feature vector), and
using f spectral ﬁlters, the propagation rule of this simpli-
ﬁed model is:
H(l+1) = σ
2 H(l)Θ(l)
where H(l) is the matrix of activations in the l-th layer, and
H(0) = X, Θ(l) ∈Rc×f is the trainable weight matrix
in layer l, σ is the activation function, e.g., ReLU(·) =
max(0, ·).
This simpliﬁed model is called graph convolutional networks (GCNs), which is the focus of this paper.
Semi-Supervised Classiﬁcation with GCNs
In , the GCN model was applied for
semi-supervised classiﬁcation in a neat way. The model used
is a two-layer GCN which applies a softmax classiﬁer on the
output features:
Z = softmax
where ˆA = ˜D−1
2 , softmax(xi) =
Z exp(xi) with
i exp(xi). The loss function is deﬁned as the crossentropy error over all labeled examples:
Yif ln Zif,
Table 1: GCNs vs. Fully-connected networks
where Vl is the set of indices of labeled vertices and F is the
dimension of the output features, which is equal to the number of classes. Y ∈R|Vl|×F is a label indicator matrix. The
weight parameters Θ(0) and Θ(1) can be trained via gradient
The GCN model naturally combines graph structures and
vertex features in the convolution, where the features of
unlabeled vertices are mixed with those of nearby labeled
vertices, and propagated over the graph through multiple
layers. It was reported in that GCNs
outperformed many state-of-the-art methods signiﬁcantly on
some benchmarks such as citation networks.
Despite its promising performance, the mechanisms of the
GCN model for semi-supervised learning have not been
made clear. In this section, we take a closer look at the GCN
model, analyze why it works, and point out its limitations.
Why GCNs Work
To understand the reasons why GCNs work so well, we
compare them with the simplest fully-connected networks
(FCNs), where the layer-wise propagation rule is
H(l+1) = σ
Clearly the only difference between a GCN and a FCN is the
graph convolution matrix ˆA = ˜D−1
2 (Eq. (5)) applied
on the left of the feature matrix X. To see the impact of the
graph convolution, we tested the performances of GCNs and
FCNs for semi-supervised classiﬁcation on the Cora citation network with 20 labels in each class. The results can be
seen in Table 1. Surprisingly, even a one-layer GCN outperformed a one-layer FCN by a very large margin.
Laplacian Smoothing. Let us ﬁrst consider a one-layer
GCN. It actually contains two steps. 1) Generating a new
feature matrix Y from X by applying the graph convolution:
Y = ˜D−1/2 ˜A ˜D−1/2X.
2) Feeding the new feature matrix Y to a fully connected
layer. Clearly the graph convolution is the key to the huge
performance gain.
Let us examine the graph convolution carefully. Suppose
that we add a self-loop to each vertex in the graph, then the
adjacency matrix of the new graph is ˜A = A+I. The Laplacian smoothing on each channel of the input
features is deﬁned as:
ˆyi = (1 −γ)xi + γ
(for 1 ≤i ≤n), (9)
(a) 1-layer
(b) 2-layer
(c) 3-layer
(d) 4-layer
(e) 5-layer
Figure 2: Vertex embeddings of Zachary’s karate club network with GCNs with 1,2,3,4,5 layers.
where 0 < γ ≤1 is a parameter which controls the weighting between the features of the current vertex and the features of its neighbors. We can write the Laplacian smoothing
in matrix form:
ˆY = X −γ ˜D−1 ˜LX = (I −γ ˜D−1 ˜L)X,
where ˜L = ˜D −˜A. By letting γ = 1, i.e., only using the
neighbors’ features, we have ˆY = ˜D−1 ˜AX, which is the
standard form of Laplacian smoothing.
Now if we replace the normalized Laplacian ˜D−1 ˜L with
the symmetrically normalized Laplacian ˜D−1
γ = 1, we have ˆY = ˜D−1/2 ˜A ˜D−1/2X, which is exactly the
graph convolution in Eq. (8). We thus call the graph convolution a special form of Laplacian smoothing – symmetric Laplacian smoothing. Note that here the smoothing still
includes the current vertex’s features, as each vertex has a
self-loop and is its own neighbor.
The Laplacian smoothing computes the new features of a
vertex as the weighted average of itself and its neighbors’.
Since vertices in the same cluster tend to be densely connected, the smoothing makes their features similar, which
makes the subsequent classiﬁcation task much easier. As we
can see from Table 1, applying the smoothing only once has
already led to a huge performance gain.
Multi-layer Structure. We can also see from Table 1 that
while the 2-layer FCN only slightly improves over the 1layer FCN, the 2-layer GCN signiﬁcantly improves over the
1-layer GCN by a large margin. This is because applying
smoothing again on the activations of the ﬁrst layer makes
the output features of vertices in the same cluster more similar and further eases the classiﬁcation task.
When GCNs Fail
We have shown that the graph convolution is essentially
a type of Laplacian smoothing. A natural question is how
many convolutional layers should be included in a GCN?
Certainly not the more the better. On the one hand, a GCN
with many layers is difﬁcult to train. On the other hand,
repeatedly applying Laplacian smoothing may mix the features of vertices from different clusters and make them indistinguishable. In the following, we illustrate this point with a
popular dataset.
We apply GCNs with different number of layers on the
Zachary’s karate club dataset , which has
34 vertices of two classes and 78 edges. The GCNs are
untrained with the weight parameters initialized randomly as
in . The dimension of the hidden
layers is 16, and the dimension of the output layer is 2. The
feature vector of each vertex is a one-hot vector. The outputs
of each GCN are plotted as two-dimensional points in Fig. 2.
We can observe the impact of the graph convolution (Laplacian smoothing) on this small dataset. Applying the smoothing once, the points are not well-separated (Fig. 2a). Applying the smoothing twice, the points from the two classes are
separated relatively well. Applying the smoothing again and
again, the points are mixed (Fig. 2c, 2d, 2e). As this is a
small dataset and vertices between two classes have quite a
number of connections, the mixing happens quickly.
In the following, we will prove that by repeatedly applying Laplacian smoothing many times, the features of vertices
within each connected component of the graph will converge to the same values. For the case of symmetric Laplacian smoothing, they will converge to be proportional to the
square root of the vertex degree.
Suppose that a graph G has k connected components
i=1, and the indication vector for the i-th component
is denoted by 1(i) ∈Rn. This vector indicates whether a
vertex is in the component Ci, i.e.,
0, vj ̸∈Ci
Theorem 1. If a graph has no bipartite components, then
for any w ∈Rn, and α ∈(0, 1],
m→+∞(I −αLrw)mw = [1(1), 1(2), . . . , 1(k)]θ1,
m→+∞(I −αLsym)mw = D−1
2 [1(1), 1(2), . . . , 1(k)]θ2,
where θ1 ∈Rk, θ2 ∈Rk, i.e., they converge to a linear
combination of {1(i)}k
i=1 and {D−1
i=1 respectively.
Proof. Lrw and Lsym have the same n eigenvalues (by multiplicity) with different eigenvectors . If
a graph has no bipartite components, the eigenvalues all fall
in [0,2) . The eigenspaces of Lrw and Lsym
corresponding to eigenvalue 0 are spanned by {1(i)}k
i=1 respectively . For
α ∈(0, 1], the eigenvalues of (I −αLrw) and (I −αLsym)
all fall into (-1,1], and the eigenspaces of eigenvalue 1 are
spanned by {1(i)}k
i=1 and {D−1
i=1 respectively. Since
the absolute value of all eigenvalues of (I −αLrw) and
(I −αLsym) are less than or equal to 1, after repeatedly
multiplying them from the left, the result will converge to the
linear combination of eigenvectors of eigenvalue 1, i.e. the
linear combination of {1(i)}k
i=1 and {D−1
i=1 respectively.
Note that since an extra self-loop is added to each vertex, there is no bipartite component in the graph. Based on
the above theorem, over-smoothing will make the features
indistinguishable and hurt the classiﬁcation accuracy.
The above analysis raises potential concerns about stacking many convolutional layers in a GCN. Besides, a deep
GCN is much more difﬁcult to train. In fact, the GCN used
in is a 2-layer GCN. However, since
the graph convolution is a localized ﬁlter – a linear combination of the feature vectors of adjacent neighbors, a shallow
GCN cannot sufﬁciently propagate the label information to
the entire graph with only a few labels. As shown in Fig. 1,
the performance of GCNs (with or without validation) drops
quickly as the training size shrinks. In fact, the accuracy of
GCNs decreases much faster than the accuracy of label propagation. Since label propagation only uses the graph information while GCNs utilize both structural and vertex features, it reﬂects the inability of the GCN model in exploring
the global graph structure.
Another problem with the GCN model in is that it requires an additional validation set
for early stopping in training, which is essentially using the
prediction accuracy on the validation set for model selection.
If we optimize a GCN on the training data without using the
validation set, it will have a signiﬁcant drop in performance.
As shown in Fig. 1, the performance of the GCN without validation drops much sharper than the GCN with validation. In
 , the authors used an additional set
of 500 labeled data for validation, which is much more than
the total number of training data. This is certainly undesirable as it defeats the purpose of semi-supervised learning.
Furthermore, it makes the comparison of GCNs with other
methods unfair as other methods such as label propagation
may not need the validation data at all.
We summarize the advantages and disadvantages of the
GCN model as follows. The advantages are: 1) the graph
convolution – Laplacian smoothing helps making the classi-
ﬁcation problem much easier; 2) the multi-layer neural network is a powerful feature extractor. The disadvantages are:
1) the graph convolution is a localized ﬁlter, which performs
unsatisfactorily with few labeled data; 2) the neural network
needs considerable amount of labeled data for validation and
model selection.
We want to make best use of the advantages of the GCN
model while overcoming its limits. This naturally leads to a
co-training idea.
Co-Train a GCN with a Random Walk Model
We propose to co-train a GCN with a random walk model
as the latter can explore the global graph structure, which
complements the GCN model. In particular, we ﬁrst use a
random walk model to ﬁnd the most conﬁdent vertices – the
nearest neighbors to the labeled vertices of each class, and
then add them to the label set to train a GCN. Unlike in , we directly optimize the parameters of a
GCN on the training set, without requiring additional labeled
data for validation.
Algorithm 1 Expand the Label Set via ParWalks
1: P := (L + αΛ)−1
2: for each class k do
Find the top t vertices in p
Add them to the training set with label k
6: end for
We choose to use the partially absorbing random walks
(ParWalks) as our random walk model. A
partially absorbing random walk is a second-order Markov
chain with partial absorption at each state. It was shown in
 that with proper absorption settings, the absorption probabilities can well capture the global
graph structure. Importantly, the absorption probabilities can
be computed in a closed-form by solving a simple linear system, and can be fast approximated by random walk sampling
or scaled up on top of vertex-centric graph engines .
The algorithm to expand the training set is described in
Algorithm 1. First, we calculate the normalized absorption
probability matrix P = (L + αΛ)−1 (the choice of Λ may
depend on data). Pi,j is the probability of a random walk
from vertex i being absorbed by vertex j, which represents
how likely i and j belong to the same class. Second, we
need to measure the conﬁdence of a vertex belonging to class
k. We partition the labeled vertices into S1, S2, ..., where
Sk denotes the set of labeled data of class k. For each class
k, we calculate a conﬁdence vector p =
P:,j, where
p ∈Rn and pi is the conﬁdence of vertex i belonging to
class k. Finally, we ﬁnd the t most conﬁdent vertices and
add them to the training set with label k to train a GCN.
GCN Self-Training
Another way to make a GCN “see” more training examples
is to self-train a GCN. Speciﬁcally, we ﬁrst train a GCN with
given labels, then select the most conﬁdent predictions for
each class by comparing the softmax scores, and add them
to the label set. We then continue to train the GCN with the
expanded label set, using the pre-trained GCN as initialization. This is described in Algorithm 2.
The most conﬁdent instances found by the GCN are supposed to share similar (but not the same) features with the
labeled data. Adding them to the labeled set will help training a more robust and accurate classiﬁer. Furthermore, it
complements the co-training method in the situation that a
graph has many isolated small components and it is not possible to propagate labels with random walks.
Combine Co-Training and Self-Training. To improve
the diversity of labels and train a more robust classiﬁer, we
Algorithm 2 Expand the Label Set via Self-Training
1: Z := GCN(X) ∈Rn×F , the output of GCN
2: for each class k do
Find the top t vertices in Zi,k
Add them to the training set with label k
5: end for
propose to combine co-training and self-learning. Speciﬁcally, we expand the label set with the most conﬁdent predictions found by the random walk and those found by the
GCN itself, and then use the expanded label set to continue
to train the GCN. We call this method “Union”. To ﬁnd more
accurate labels to add to the labeled set, we also propose to
add the most conﬁdent predictions found by both the random
walk and the GCN. We call this method “Intersection”.
Note that we optimize all our methods on the expanded
label set, without requiring any additional validation data.
As long as the expanded label set contains enough correct
labels, our methods are expected to train a good GCN classiﬁer. But how much labeled data does it require to train a
GCN? Suppose that the number of layers of the GCN is τ,
and the average degree of the underlying graph is ˆd. We propose to estimate the lower bound of the number of labels
η = |Vl| by solving ( ˆd)τ ∗η ≈n. The rationale behind this
is to estimate how many labels are needed to for a GCN with
τ layers to propagate them to cover the entire graph.
Experiments
In this section, we conduct extensive experiments on real
benchmarks to verify our theory and the proposed methods,
including Co-Training, Self-Training, Union, and Intersection (see Section 4).
We compare our methods with several state-of-the-art
methods, including GCN with validation (GCN+V); GCN
without validation (GCN-V); GCN with Chebyshev ﬁlter
(Cheby) ; label propagation using
ParWalks (LP) ; Planetoid ; DeepWalk ; manifold regularization (ManiReg) ; semi-supervised embedding
(SemiEmb) ; iterative classiﬁcation
algorithm (ICA) .
Experimental Setup
We conduct experiments on three commonly used citation
networks: CiteSeer, Cora, and PubMed .
The statistics of the datasets are summarized in Table 2.
On each dataset, a document is described by a bag-ofwords feature vector, i.e., a 0/1-valued vector indicating
the absence/presence of a certain word. The citation links
between documents are described by a 0/1-valued adjacency
matrix. The datasets we use for testing are provided by
the authors of and
 .
For ParWalks, we set Λ = I, and α = 10−6, following
Wu et al.. For GCNs, we use the same hyper-parameters as
Table 2: Dataset statistics
in : a learning rate of 0.01, 200 maximum epochs, 0.5 dropout rate, 5 × 10−4 L2 regularization
weight, 2 convolutional layers, and 16 hidden units, which
are validated on Cora by Kipf and Welling. For each run,
we randomly split labels into a small set for training, and a
set with 1000 samples for testing. For GCN+V, we follow
 to sample additional 500 labels for
validation. For GCN-V, we simply optimize the GCN using
training accuracy. For Cheby, we set the polynomial degree
K = 2 (see Eq. (1)). We test these methods with 0.5%, 1%,
2%, 3%, 4%, 5% training size on Cora and CiteSeer, and
with 0.03%, 0.05%, 0.1%, 0.3% training size on PubMed.
We choose these labeling rates for easy comparison with
 , , and other methods. We report the mean accuracy of
50 runs except for the results on PubMed , which are averaged over 10 runs.
Results Analysis
The classiﬁcation results are summarized in Table 3, 4 and 5,
where the highest accuracy in each column is highlighted in
bold and the top 3 are underlined. Our methods are displayed
at the bottom half of each table.
We can see that the performance of Co-Training is closely
related to the performance of LP. If the data has strong manifold structure, such as PubMed, Co-Training performs the
best. In contrast, Self-Training is the worst on PubMed, as it
does not utilize the graph structure. But Self-Training does
well on CiteSeer where Co-Training is overall the worst.
Intersection performs better when the training size is relatively large, because it ﬁlters out many labels. Union performs best in many cases since it adds more diverse labels to
the training set.
Comparison with GCNs. At a glance, we can see that on
each dataset, our methods outperform others by a large margin in most cases. When the training size is small, all our
methods are far better than GCN-V, and much better than
GCN+V in most cases. For example, with labeling rate 1%
on Cora and CiteSeer, our methods improve over GCN-V by
23% and 28%, and improve over GCN+V by 12% and 7%.
With labeling rate 0.05% on PubMed, our methods improve
over GCN-V and GCN+V by 37% and 18% respectively.
This veriﬁes our analysis that the GCN model cannot effectively propagate labels to the entire graph with small training size. When the training size grows, our methods are still
better than GCN+V in most cases, demonstrating the effectiveness of our approaches. When the training size is large
enough, our methods and GCNs perform similarly, indicating that the given labels are sufﬁcient for training a good
GCN classiﬁer. Cheby does not perform well in most cases,
which is probably due to overﬁtting.
Table 3: Classiﬁcation Accuracy On Cora
Label Rate
Co-training
Self-training
Intersection
Table 4: Classiﬁcation Accuracy on CiteSeer
Label Rate
Co-training
Self-training
Intersection
Comparison with other methods. We compare our
methods with other state-of-the-art methods in Table 6. The
experimental setup is the same except that for every dataset,
we sample 20 labels for each class, which corresponds to
the total labeling rate of 3.6% on CiteSeer, 5.1% on Cora,
and 0.3% on PubMed. The results of other baselines are
copied from . Our methods perform
similarly as GCNs and outperform other baselines significantly. Although we did not directly compare with other
baselines, we can see from Table 3, 4 and 5 that our methods
with much fewer labels already outperform many baselines.
For example, our method Union on Cora (Table 3) with 2%
labeling rate (54 labels) beats all other baselines with 140
labels (Table 6).
Inﬂuence of the Parameters. A common parameter of
our methods is the number of newly added labels. Adding
too many labels will introduce noise, but with too few labels
we cannot train a good GCN classiﬁer. As described in the
end of Section 4, we can estimate the lower bound of the
total number of labels η needed to train a GCN by solving
( ˆd)τ ∗η ≈n. We use 3η in our experiments. Actually, we
found that 2η, 3η and 4η perform similarly in the experiments. We follow Kipf and Welling to set the number of
convolutional layers as 2. We also observed in the experiments that 2-layer GCNs performed the best. When the number of convolutional layers grows, the classiﬁcation accuracy
decreases drastically, which is probably due to overﬁtting.
Computational Cost. For Co-Training, the overhead is
the computational cost of the random walk model, which
requires solving a sparse linear system. In our experiments,
the time is negligible on Cora and CiteSeer as there are only
a few thousand vertices. On PubMed, it takes less than 0.38
Table 5: Classiﬁcation Accuracy On PubMed
Label Rate
Co-training
Self-training
Intersection
Table 6: Accuracy under 20 Labels per Class
Co-training
Self-training
Intersection
seconds in MatLab R2015b. As mentioned in Section 4, the
computation can be further speeded up using vertex-centric
graph engines , so the scalability of our
method is not an issue. For Self-Training, we only need to
run a few epochs in addition to training a GCN. It converges
fast as it builds on a pre-trained GCN. Hence, the running
time of Self-Training is comparable to a GCN.
Conclusions
Understanding deep neural networks is crucial for realizing their full potentials in real applications. This paper contributes to the understanding of the GCN model and its application in semi-supervised classiﬁcation. Our analysis not
only reveals the mechanisms and limitations of the GCN
model, but also leads to new solutions overcoming its limits.
In future work, we plan to develop new convolutional ﬁlters
which are compatible with deep architectures, and exploit
advanced deep learning techniques to improve the performance of GCNs for more graph-based applications.
Acknowledgments
This research received support from the grant 1-ZVJJ funded
by the Hong Kong Polytechnic University. The authors
would like to thank the reviewers for their insightful comments and useful discussions.