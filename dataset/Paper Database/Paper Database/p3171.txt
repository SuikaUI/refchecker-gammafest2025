Rethinking ImageNet Pre-training
Kaiming He
Ross Girshick
Piotr Doll´ar
Facebook AI Research (FAIR)
We report competitive results on object detection and instance segmentation on the COCO dataset using standard
models trained from random initialization.
The results
are no worse than their ImageNet pre-training counterparts
even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for ﬁne-tuning pretrained models, with the sole exception of increasing the
number of training iterations so the randomly initialized
models may converge. Training from random initialization
is surprisingly robust; our results hold even when: (i) using only 10% of the training data, (ii) for deeper and wider
models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide
regularization or improve ﬁnal target task accuracy.
push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data—a result on
par with the top COCO 2017 competition results that used
ImageNet pre-training. These observations challenge the
conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage
people to rethink the current de facto paradigm of ‘pretraining and ﬁne-tuning’ in computer vision.
1. Introduction
Deep convolutional neural networks revolutionized computer vision arguably due to the discovery that feature representations learned on a pre-training task can transfer useful information to target tasks . In recent
years, a well-established paradigm has been to pre-train
models using large-scale data (e.g., ImageNet ) and then
to ﬁne-tune the models on target tasks that often have less
training data. Pre-training has enabled state-of-the-art results on many tasks, including object detection ,
image segmentation , and action recognition .
A path to ‘solving’ computer vision then appears to be
paved by pre-training a ‘universal’ feature representation
on ImageNet-like data at massive scale . Attempts
along this path have pushed the frontier to up to 3000×
 the size of ImageNet. However, the success of these
iterations (105)
bbox AP: R50-FPN, GN
fine-tuning
random init
w/ pre-train
Figure 1. We train Mask R-CNN with a ResNet-50 FPN 
and GroupNorm backbone on the COCO train2017 set
and evaluate bounding box AP on the val2017 set, initializing
the model by random weights or ImageNet pre-training. We explore different training schedules by varying the iterations at which
the learning rate is reduced (where the accuracy leaps). The model
trained from random initialization needs more iterations to converge, but converges to a solution that is no worse than the ﬁnetuning counterpart. Table 1 shows the resulting AP numbers.
experiments is mixed: although improvements have been
observed, for object detection in particular they are small
and scale poorly with the pre-training dataset size. That this
path will ‘solve’ computer vision is open to doubt.
This paper questions the paradigm of pre-training even
further by exploring the opposite regime: we report that
competitive object detection and instance segmentation accuracy is achievable when training on COCO from random
initialization (‘from scratch’), without any pre-training.
More surprisingly, we can achieve these results by using
baseline systems and their hyper-parameters
that were optimized for ﬁne-tuning pre-trained models. We
ﬁnd that there is no fundamental obstacle preventing us
from training from scratch, if: (i) we use normalization
techniques appropriately for optimization, and (ii) we train
the models sufﬁciently long to compensate for the lack of
pre-training (Figure 1).
 
We show that training from random initialization on
COCO can be on par with its ImageNet pre-training counterparts for a variety of baselines that cover Average Precision (AP, in percentage) from 40 to over 50. Further, we ﬁnd
that such comparability holds even if we train with as little
as 10% COCO training data. We also ﬁnd that we can train
large models from scratch—up to 4× larger than a ResNet-
101 —without overﬁtting. Based on these experiments
and others, we observe the following:
(i) ImageNet pre-training speeds up convergence, especially early on in training, but training from random initialization can catch up after training for a duration that is
roughly comparable to the total ImageNet pre-training plus
ﬁne-tuning computation—it has to learn the low-/mid-level
features (such as edges, textures) that are otherwise given by
pre-training. As the cost of ImageNet pre-training is often
ignored when studying the target task, ‘controlled’ comparisons with a short training schedule can veil the true behavior of training from random initialization.
(ii) ImageNet pre-training does not automatically give
better regularization.
When training with fewer images (down to 10% of COCO), we ﬁnd that new hyperparameters must be selected for ﬁne-tuning (from pretraining) to avoid overﬁtting.
Then, when training from
random initialization using these same hyper-parameters,
the model can match the pre-training accuracy without any
extra regularization, even with only 10% COCO data.
(iii) ImageNet pre-training shows no beneﬁt when the
target tasks/metrics are more sensitive to spatially welllocalized predictions. We observe a noticeable AP improvement for high box overlap thresholds when training from
scratch; we also ﬁnd that keypoint AP, which requires ﬁne
spatial localization, converges relatively faster from scratch.
Intuitively, the task gap between the classiﬁcation-based,
ImageNet-like pre-training and localization-sensitive target
tasks may limit the beneﬁts of pre-training.
Given the current literature, these results are surprising
and challenge our understanding of the effects of ImageNet
pre-training. These observations hint that ImageNet pretraining is a historical workaround (and will likely be so for
some time) for when the community does not have enough
target data or computational resources to make training on
the target task doable.
In addition, ImageNet has been
largely thought of as a ‘free’ resource, thanks to the readily
conducted annotation efforts and wide availability of pretrained models. But looking forward, when the community
will proceed with more data and faster computation, our
study suggests that collecting data and training on the target tasks is a solution worth considering, especially when
there is a signiﬁcant gap between the source pre-training
task and the target task. This paper provides new experimental evidence and discussions for people to rethink the
ImageNet-like pre-training paradigm in computer vision.
2. Related Work
Pre-training and ﬁne-tuning. The initial breakthrough of
applying deep learning to object detection (e.g., R-CNN
 and OverFeat ) were achieved by ﬁne-tuning networks that were pre-trained for ImageNet classiﬁcation.
Following these results, most modern object detectors and
many other computer vision algorithms employ the ‘pretraining and ﬁne-tuning’ paradigm. Recent work pushes
this paradigm further by pre-training on datasets that are
6× (ImageNet-5k ), 300× (JFT ), and even 3000×
(Instagram ) larger than ImageNet. While this body of
work demonstrates signiﬁcant improvements on image classiﬁcation transfer learning tasks, the improvements on object detection are relatively small (on the scale of +1.5 AP
on COCO with 3000× larger pre-training data ). The
marginal beneﬁt from the kind of large-scale pre-training
data used to date diminishes rapidly.
Detection from scratch. Before the prevalence of the ‘pretraining and ﬁne-tuning’ paradigm, object detectors were
trained with no pre-training (e.g., )—a fact that
is somewhat overlooked today. In fact, it should not be surprising that object detectors can be trained from scratch.
Given the success of pre-training in the R-CNN paper
 , later analysis found that pre-training plays an important role in detector accuracy when training data is limited,
but also illustrated that training from scratch on more detection data is possible and can achieve 90% of the ﬁne-tuning
accuracy, foreshadowing our results.
As modern object detectors 
evolved under the pre-training paradigm, the belief that
training from scratch is non-trivial became conventional
wisdom. Shen et al. argued for a set of new design
principles to obtain a detector that is optimized for the accuracy when trained from scratch. They designed a specialized detector driven by deeply supervised networks and
dense connections . DetNet and CornerNet 
also present results when training detectors from scratch.
Similar to , these works focus on designing
detection-speciﬁc architectures. However, in 
there is little evidence that these specialized architectures
are required for models to be trained from scratch.
Unlike these papers, our focus is on understanding the
role of ImageNet pre-training on unspecialized architectures (i.e., models that were originally designed without the
consideration for training from scratch). Our work demonstrates that it is often possible to match ﬁne-tuning accuracy
when training from scratch even without making any architectural specializations. Our study is on the comparison between ‘with vs. without pre-training’, under controlled settings in which the architectures are not tailored.
3. Methodology
Our goal is to ablate the role of ImageNet pretraining via controlled experiments that can be done without
ImageNet pre-training. Given this goal, architectural improvements are not our purpose; actually, to better understand what impact ImageNet pre-training can make, it is
desired to enable typical architectures to be trained from
scratch under minimal modiﬁcations. We describe the only
two modiﬁcations that we ﬁnd to be necessary, related to
model normalization and training length, discussed next.
3.1. Normalization
Image classiﬁer training requires normalization to help
optimization.
Successful forms of normalization include
normalized parameter initialization and activation
normalization layers . When training object
detectors from scratch, they face issues similar to training
image classiﬁers from scratch . Overlooking the
role of normalization can give the misperception that detectors are hard to train from scratch.
Batch Normalization (BN) , the popular normalization method used to train modern networks, partially makes
training detectors from scratch difﬁcult. Object detectors
are typically trained with high resolution inputs, unlike image classiﬁers. This reduces batch sizes as constrained by
memory, and small batch sizes severely degrade the accuracy of BN . This issue can be circumvented if
pre-training is used, because ﬁne-tuning can adopt the pretraining batch statistics as ﬁxed parameters ; however,
freezing BN is invalid when training from scratch.
We investigate two normalization strategies in recent
works that help relieve the small batch issue:
(i) Group Normalization (GN) : as a recently proposed alternative to BN, GN performs computation
that is independent of the batch dimension. GN’s accuracy is insensitive to batch sizes .
(ii) Synchronized Batch Normalization (SyncBN) :
this is an implementation of BN with batch statistics computed across multiple devices (GPUs). This
increases the effective batch size for BN when using
many GPUs, which avoids small batches.
Our experiments show that both GN and SyncBN can enable detection models to train from scratch.
We also report that using appropriately normalized initialization , we can train object detectors with VGG
nets from random initialization without BN or GN.
3.2. Convergence
It is unrealistic and unfair to expect models trained from
random initialization to converge similarly fast as those initialized from ImageNet pre-training. Overlooking this fact
ImageNet pre-train
1.28M im × 100 ep
COCO fine-tune
115k im × 24 ep
COCO from random init
115k im × 72 ep
Figure 2. Total numbers of images, instances, and pixels seen during all training iterations, for pre-training + ﬁne-tuning (green
bars) vs. from random initialization (purple bars). We consider
that pre-training takes 100 epochs in ImageNet, and ﬁne-tuning
adopts the 2× schedule (∼24 epochs over COCO) and random initialization adopts the 6× schedule (∼72 epochs over COCO). We
count instances in ImageNet as 1 per image (vs. ∼7 in COCO), and
pixels in ImageNet as 224×224 and COCO as 800×1333.
one can draw incomplete or incorrect conclusions about the
true capability of models that are trained from scratch.
Typical ImageNet pre-training involves over one million
images iterated for one hundred epochs. In addition to any
semantic information learned from this large-scale data, the
pre-training model has also learned low-level features (e.g.,
edges, textures) that do not need to be re-learned during
ﬁne-tuning.1 On the other hand, when training from scratch
the model has to learn low- and high-level semantics, so
more iterations may be necessary for it to converge well.
With this motivation, we argue that models trained from
scratch must be trained for longer than typical ﬁne-tuning
schedules. Actually, this is a fairer comparison in term of
the number of training samples provided. We consider three
rough deﬁnitions of ‘samples’—the number of images, instances, and pixels that have been seen during all training
iterations (e.g., one image for 100 epochs is counted as 100
image-level samples). We plot the comparisons on the numbers of samples in Figure 2.
Figure 2 shows a from-scratch case trained for 3 times
more iterations than its ﬁne-tuning counterpart on COCO.
Despite using more iterations on COCO, if counting imagelevel samples, the from-scratch case still sees considerably
fewer samples than its ﬁne-tuning counterpart—the 1.28
million ImageNet images for 100 epochs dominate. Actually, the sample numbers only get closer if we count pixellevel samples (Figure 2, bottom)—a consequence of object
detectors using higher-resolution images. Our experiments
show that under the schedules in Figure 2, the from-scratch
detectors can catch up with their ﬁne-tuning counterparts.
This suggests that a sufﬁciently large number of total samples (arguably in terms of pixels) are required for the models
trained from random initialization to converge well.
1In fact, it is common practice to freeze the convolutional ﬁlters
in the ﬁrst few layers when ﬁne-tuning.
4. Experimental Settings
We pursue minimal changes made to baseline systems
for pinpointing the keys to enabling training from scratch.
Overall, our baselines and hyper-parameters follow Mask
R-CNN in the publicly available code of Detectron
 , except we use normalization and vary the number of
training iterations. The implementation is as follows.
Architecture. We investigate Mask R-CNN with
ResNet or ResNeXt plus Feature Pyramid Network (FPN) backbones.
We adopt the end-to-end
fashion of training Region Proposal Networks (RPN)
jointly with Mask R-CNN. GN/SyncBN is used to replace
all ‘frozen BN’ (channel-wise afﬁne) layers. For fair comparisons, in this paper the ﬁne-tuned models (with pretraining) are also tuned with GN or SyncBN, rather than
freezing them. They have higher accuracy than the frozen
ones .
Learning rate scheduling. Original Mask R-CNN models in Detectron were ﬁne-tuned with 90k iterations
(namely, ‘1× schedule’) or 180k iterations (‘2× schedule’).
For models in this paper, we investigate longer training and
we use similar terminology, e.g., a so-called ‘6× schedule’
has 540k iterations. Following the strategy in the 2× schedule, we always reduce the learning rate by 10× in the last
60k and last 20k iterations respectively, no matter how many
total iterations (i.e., the reduced learning rates are always
run for the same number of iterations). We ﬁnd that training
longer for the ﬁrst (large) learning rate is useful, but training
for longer on small learning rates often leads to overﬁtting.
Hyper-parameters. All other hyper-parameters follow
those in Detectron . Specially, the initial learning rate
is 0.02 (with a linear warm-up ). The weight decay is
0.0001 and momentum is 0.9. All models are trained in 8
GPUs using synchronized SGD, with a mini-batch size of 2
images per GPU.
By default Mask R-CNN in Detectron uses no data augmentation for testing, and only horizontal ﬂipping augmentation for training. We use the same settings. Also, unless
noted, the image scale is 800 pixels for the shorter side.
5. Results and Analysis
5.1. Training from scratch to match accuracy
Our ﬁrst surprising discovery is that when only using the
COCO data, models trained from scratch can catch up in
accuracy with ones that are ﬁne-tuned.
In this subsection, we train the models on the COCO
train2017 split that has ∼118k (118,287) images, and
evaluate in the 5k COCO val2017 split.
We evaluate
bounding box (bbox) Average Precision (AP) for object detection and mask AP for instance segmentation.
iterations (105)
bbox AP: R101-FPN, GN
random init
w/ pre-train
Figure 3. Learning curves of APbbox on COCO val2017 using
Mask R-CNN with R101-FPN and GN. Table 1 shows the resulting AP numbers.
iterations (105)
bbox AP: R50-FPN, SyncBN
random init
w/ pre-train
Figure 4. Learning curves of APbbox on COCO val2017 using
Mask R-CNN with R50-FPN and SyncBN (that synchronizes batch statistics across GPUs). The results of the 6× schedule
are 39.3 (random initialization) and 39.0 (pre-training).
Baselines with GN and SyncBN. The validation bbox AP
curves are shown in Figures 1 and 3 when using GN for
ResNet-50 (R50) and ResNet-101 (R101) backbones and in
Figure 4 when using SyncBN for R50. For each ﬁgure, we
compare the curves between models trained from random
initialization vs. ﬁne-tuned with ImageNet pre-training.
We study ﬁve different schedules for each case, namely,
2× to 6× iterations (Sec. 4). Note that we overlay the ﬁve
schedules of one model in the same plot. The leaps in the
AP curves are a consequence of reducing learning rates, illustrating the results of different schedules.
random init
w/ pre-train
random init
w/ pre-train
Table 1. Object detection APbbox on COCO val2017 of training
schedules from 2× (180k iterations) to 6× (540k iterations). The
model is Mask R-CNN with FPN and GN (Figures 1 and 3).
APbbox APbbox
APmask APmask
random init
w/ pre-train
random init
w/ pre-train
Table 2. Training from random initialization vs. with ImageNet
pre-training (Mask R-CNN with FPN and GN, Figures 1, 3), evaluated on COCO val2017. For each model, we show its results
corresponding to the schedule (2 to 6×) that gives the best APbbox.
Similar phenomena, summarized below, are consistently
present in Figures 1, 3, and 4:
(i) Typical ﬁne-tuning schedules (2×) work well for the
models with pre-training to converge to near optimum (see
also Table 1, ‘w/ pre-train’). But these schedules are not
enough for models trained from scratch, and they appear to
be inferior if they are only trained for a short period.
(ii) Models trained from scratch can catch up with their
ﬁne-tuning counterparts, if a 5× or 6× schedule is used—
actually, when they converge to an optimum, their detection
AP is no worse than their ﬁne-tuning counterparts.
In the standard COCO training set, ImageNet pretraining mainly helps to speed up convergence on the target
task early on in training, but shows little or no evidence of
improving the ﬁnal detection accuracy.
Multiple detection metrics. In Table 2 we further compare different detection metrics between models trained
from scratch and with pre-training, including box-level
and segmentation-level AP of Mask R-CNN, under
Intersection-over-Union (IoU) thresholds of 0.5 (AP50) and
0.75 (AP75).
Table 2 reveals that models trained from scratch and with
pre-training have similar AP metrics under various criteria,
suggesting that the models trained from scratch catch up not
only by chance for a single metric.
Moreover, for the APbbox
metric (using a high overlap
threshold), training from scratch is better than ﬁne-tuning
by noticeable margins (1.0 or 0.8 AP).
R50 baseline
cascade + train aug
cascade + train/test aug
bbox AP, random init
bbox AP, w/ pre-train
mask AP, random init
mask AP, w/ pre-train
R101 baseline
cascade + train aug
cascade + train/test aug
bbox AP, random init
bbox AP, w/ pre-train
mask AP, random init
mask AP, w/ pre-train
Figure 5. Comparisons between from random initialization vs.
with pre-training on various systems using Mask R-CNN, including: (i) baselines using FPN and GN, (ii) baselines with trainingtime multi-scale augmentation, (iii) baselines with Cascade R-
CNN and training-time augmentation, and (iv) plus test-time
multi-scale augmentation. Top: R50; Bottom: R101.
Enhanced baselines. The phenomenon that training with
and without pre-training can be comparable is also observed
in various enhanced baselines, as compared in Figure 5. We
ablate the experiments as follows:
– Training-time scale augmentation: Thus far all models are trained with no data augmentation except horizontal
ﬂipping. Next we use the simple training-time scale augmentation implemented in Detectron: the shorter side of images is randomly sampled from pixels. Stronger
data augmentation requires more iterations to converge, so
we increase the schedule to 9× when training from scratch,
and to 6× when from ImageNet pre-training.
Figure 5 (‘train aug’) shows that in this case models
trained with and without ImageNet pre-training are still
comparable. Actually, stronger data augmentation relieves
the problem of insufﬁcient data, so we may expect that models with pre-training have less of an advantage in this case.
– Cascade R-CNN : as a method focusing on improving localization accuracy, Cascade R-CNN appends two ex-
tra stages to the standard two-stage Faster R-CNN system.
We implement its Mask R-CNN version by simply adding
a mask head to the last stage. To save running time for the
from-scratch models, we train Mask R-CNN from scratch
without cascade, and switch to cascade in the ﬁnal 270k iterations, noting that this does not alter the fact that the ﬁnal
model uses no ImageNet pre-training. We train Cascade R-
CNN under the scale augmentation setting.
Figure 5 (‘cascade + train aug’) again shows that
Cascade R-CNN models have similar AP numbers with
and without ImageNet pre-training.
Supervision about
localization is mainly provided by the target dataset and
is not explicitly available from the classiﬁcation-based
ImageNet pre-training. Thus we do not expect ImageNet
pre-training to provide additional beneﬁts in this setting.
– Test-time augmentation: thus far we have used no testtime augmentation. Next we further perform test-time augmentation by combining the predictions from multiple scaling transformations, as implemented in Detectron .
Again, the models trained from scratch are no worse than
their pre-training counterparts.
Actually, models trained
from scratch are even slightly better in this case—for example, mask AP is 41.6 (from scratch) vs. 40.9 for R50,
and 42.5 vs. 41.9 for R101.
Large models trained from scratch. We have also trained
a signiﬁcantly larger Mask R-CNN model from scratch using a ResNeXt-152 8×32d (in short ‘X152’) backbone
with GN. The results are in Table 3.
This backbone has ∼4× more FLOPs than R101. Despite being substantially larger, this model shows no noticeable overﬁtting. It achieves good results of 50.9 bbox AP
and 43.2 mask AP in val2017 when trained from random
initialization.
We submitted this model to COCO 2018
competition, and it has 51.3 bbox AP and 43.6 mask AP
in the test-challenge set. Our bbox AP is at the level
of the COCO 2017 winners (50.5 bbox AP, ), and is by
far the highest number of its kind (single model, without
ImageNet pre-training).
We have trained the same model with ImageNet pretraining. It has bbox/mask AP of 50.3/42.5 in val2017
(vs. from-scratch’s 50.9/43.2). Interestingly, even for this
large model, pre-training does not improve results.
vs. previous from-scratch results. DSOD reported
29.3 bbox AP by using an architecture specially tailored for
results of training from scratch. A recent work of CornerNet
 reported 42.1 bbox AP (w/ multi-scale augmentation)
using no ImageNet pre-training. Our results, of various versions, are higher than previous ones. Again, we emphasize
that previous works reported no evidence that models without ImageNet pre-training can be comparably good
as their ImageNet pre-training counterparts.
APbbox APbbox
APmask APmask
R101 w/ train aug 45.0
X152 w/ train aug 46.4
+ test aug
Table 3. Mask R-CNN with ResNeXt-152 trained from random
initialization (w/ FPN and GN), evaluated on COCO val2017.
iterations (105)
keypoint AP: R50-FPN GN
random init
w/ pre-train
Figure 6. Keypoint detection on
COCO using Mask R-CNN with
R50-FPN and GN. We show keypoint AP on COCO val2017.
ImageNet pre-training has little
beneﬁt, and training from random
initialization can quickly catch up
without increasing training iterations. We only need to use 2× and
3× schedules, unlike the object detection case. The result is 65.6 vs.
65.5 (random initialization vs. pretraining) with 2× schedules.
Keypoint detection. We also train Mask R-CNN for the
COCO human keypoint detection task. The results are in
Figure 6. In this case, the model trained from scratch can
catch up more quickly, and even when not increasing training iterations, it is comparable with its counterpart that uses
ImageNet pre-training. Keypoint detection is a task more
sensitive to ﬁne spatial localization. Our experiment suggests that ImageNet pre-training, which has little explicit
localization information, does not help keypoint detection.
Models without BN/GN — VGG nets. Thus far all of our
experiments involve ResNet-based models, which require
some form of activation normalization (e.g., BN or GN).
Shallower models like VGG-16 can be trained from
scratch without activation normalization as long as a proper
initialization normalization is used . Our next experiment tests the generality of our observations by exploring
the behavior of training Faster R-CNN from scratch using
VGG-16 as the backbone.
We implement the model following the original Faster
R-CNN paper and its VGG-16 architecture; no FPN is
used. We adopt standard hyper-parameters with a learning
rate of 0.02, learning rate decay factor of 0.1, and weight
decay of 0.0001. We use scale augmentation during training.
Following previous experiments, we use the exact
same hyper-parameters when ﬁne-tuning and training from
scratch. When randomly initializing the model, we use the
same MSRA initialization for ImageNet pre-training
and for COCO from scratch.
The baseline model with pre-training is able to reach a
maximal bbox AP of 35.6 after an extremely long 9× training schedule (training for longer leads to a slight degradation in AP). Here we note that even with pre-training,
iterations (105)
bbox AP: 35k training images
w/ pre-train
iterations (105)
bbox AP: 35k training images
random init
w/ pre-train
iterations (105)
bbox AP: 10k training images
random init
w/ pre-train
Figure 7. Training with fewer COCO images (left/middle: 35k; right: 10k). The model is Mask R-CNN with R50-FPN and GN,
evaluated by bbox AP in val2017. Left: training with 35k COCO images, using the default hyper-parameters that were chosen for
the 118k train2017. It shows overﬁtting before and after the learning rate changes. Middle: training with 35k COCO images, using
hyper-parameters optimized for ‘w/ pre-train’ (the same hyper-parameters are then applied to the model from random initialization). Right:
training with 10k COCO images, using hyper-parameters optimized for ‘w/ pre-training’.
full convergence for VGG-16 is slow. The model trained
from scratch reaches a similar level of performance with a
maximal bbox AP of 35.2 after an 11× schedule (training
for longer resulted in a lower AP, too). These results indicate that our methodology of ‘making minimal/no changes’
(Sec. 3) but adopting good optimization strategies and training for longer are sufﬁcient for training comparably performant detectors on COCO, compared to the standard ‘pretraining and ﬁne-tuning’ paradigm.
5.2. Training from scratch with less data
Our second discovery, which is even more surprising,
is that with substantially less data (e.g., ∼1/10 of COCO),
models trained from scratch are no worse than their counterparts that are pre-trained.
35k COCO training images. We start our next investigation with ∼1/3 of COCO training data (35k images from
train2017, equivalent to the older val35k). We train
models with or without ImageNet pre-training on this set.
Figure 7 (left) is the result using ImageNet pre-training
under the hyper-parameters of Mask R-CNN that were chosen for the 118k COCO set. These hyper-parameters are
not optimal, and the model suffers from overﬁtting even
with ImageNet pre-training. It suggests that ImageNet pretraining does not automatically help reduce overﬁtting.
To obtain a healthy baseline, we redo grid search for
hyper-parameters on the models that are with ImageNet pretraining.2 The gray curve in Figure 7 (middle) shows the
results. It has optimally 36.3 AP with a 6× schedule.
2Our new recipe changes are: training-time scale augmentation range
of (vs. baseline’s no scale augmentation), a starting learning rate
of 0.04 (vs. 0.02), and a learning rate decay factor of 0.02 (vs. 0.1).
Then we train our model from scratch using the exact
same new hyper-parameters that are chosen for the pretraining case. This obviously biases results in favor of the
pre-training model. Nevertheless, the model trained from
scratch has 36.3 AP and catches up with its pre-training
counterpart (Figure 7, middle), despite less data.
10k COCO training images. We repeat the same set of
experiments on a smaller training set of 10k COCO images
(i.e., less than 1/10th of the full COCO set). Again, we
perform grid search for hyper-parameters on the models that
use ImageNet pre-training, and apply them to the models
trained from scratch. We shorten the training schedules in
this small training set (noted by x-axis, Figure 7, right).
The model with pre-training reaches 26.0 AP with 60k
iterations, but has a slight degradation when training more.
The counterpart model trained from scratch has 25.9 AP at
220k iterations, which is comparably accurate.
Breakdown regime: 1k COCO training images. That
training from scratch in 10k images is comparably accurate
is surprising. But it is not reasonable to expect this trend
will last for arbitrarily small target data, as we report next.
In Figure 8 we repeat the same set of experiments
using only 1k COCO training images (∼1/100th of full
COCO, again optimizing hyper-parameters for the pretraining case) and show the training loss. In terms of optimization (i.e., reducing training loss), training from scratch
is still no worse but only converges more slowly, as seen
previously. However, in this case, the training loss does
not translate into a good validation AP: the model with
ImageNet pre-training has 9.9 AP vs. the from scratch
model’s 3.5 AP.
For one experiment only we also performed a grid search to optimize the from-scratch case: the
iterations (104)
Loss: 1k training images
random init
w/ pre-train
Figure 8. Training with 1k COCO images (shown as the loss
in the training set). The model is Mask R-CNN with R50-FPN
and GN. As before, we use hyper-parameters optimized for the
model with pre-training, and apply the same hyper-parameters to
the model from random initialization. The randomly initialized
model can catch up for the training loss, but has lower validation
accuracy (3.4 AP) than the pre-training counterpart (9.9 AP).
result improves to 5.4 AP, but does not catch up. This is a
sign of strong overﬁtting due to the severe lack of data.
We also do similar experiments using 3.5k COCO training images. The model that uses pre-training has a peak
of 16.0 bbox AP vs. the trained from scratch counterpart’s
9.3 AP. The breakdown point in the COCO dataset is somewhere between 3.5k to 10k training images.
Breakdown regime: PASCAL VOC. Lastly we report the
comparison in PASCAL VOC object detection . We train
on the set of trainval2007+train2012, and evaluate
on val2012. Using ImageNet pre-training, our Faster R-
CNN baseline (with R101-FPN, GN, and only training-time
augmentation) has 82.7 mAP at 18k iterations. Its counterpart trained from scratch in VOC has 77.6 mAP at 144k
iterations and does not catch up even training longer.
There are 15k VOC images used for training.
these images have on average 2.3 instances per image (vs.
COCO’s ∼7) and 20 categories (vs. COCO’s 80). They are
not directly comparable to the same number of COCO images. We suspect that the fewer instances (and categories)
has a similar negative impact as insufﬁcient training data,
which can explain why training from scratch on VOC is not
able to catch up as observed on COCO.
6. Discussions
We summarize the main observations from our experiments as follows:
- Training from scratch on target tasks is possible
without architectural changes.
- Training from scratch requires more iterations to sufﬁciently converge.
- Training from scratch can be no worse than its
ImageNet pre-training counterparts under many circumstances, down to as few as 10k COCO images.
- ImageNet pre-training speeds up convergence on the
target task.
- ImageNet pre-training does not necessarily help reduce
overﬁtting unless we enter a very small data regime.
- ImageNet pre-training helps less if the target task is
more sensitive to localization than classiﬁcation.
Based on these observations, we provide our answers to
a few important questions that may encourage people to rethink ImageNet pre-training:
Is ImageNet pre-training necessary? No—if we have
enough target data (and computation).
Our experiments
show that ImageNet can help speed up convergence, but
does not necessarily improve accuracy unless the target
dataset is too small (e.g., <10k COCO images). It can be
sufﬁcient to directly train on the target data if its dataset
scale is large enough.
Looking forward, this suggests
that collecting annotations of target data (instead of pretraining data) can be more useful for improving the target
task performance.
Is ImageNet helpful? Yes. ImageNet pre-training has
been a critical auxiliary task for the computer vision community to progress. It enabled people to see signiﬁcant improvements before larger-scale data was available (e.g., in
VOC for a long while). It also largely helped to circumvent optimization problems in the target data (e.g., under the
lack of normalization/initialization methods).
ImageNet pre-training reduces research cycles, leading to
easier access to encouraging results—pre-trained models
are widely and freely available today, pre-training cost does
not need to be paid repeatedly, and ﬁne-tuning from pretrained weights converges faster than from scratch.
believe that these advantages will still make ImageNet undoubtedly helpful for computer vision research.
Do we need big data?
But a generic largescale, classiﬁcation-level pre-training set is not ideal if we
take into account the extra effort of collecting and cleaning data—the cost of collecting ImageNet has been largely
ignored, but the ‘pre-training’ step in the ‘pre-training +
ﬁne-tuning’ paradigm is in fact not free when we scale out
this paradigm. If the gain of large-scale classiﬁcation-level
pre-training becomes exponentially diminishing , it
would be more effective to collect data in the target domain.
Shall we pursuit universal representations? Yes. We
believe learning universal representations is a laudable goal.
Our results do not mean deviating from this goal. Actually, our study suggests that the community should be more
careful when evaluating pre-trained features (e.g., for selfsupervised learning ), as now we learn that
even random initialization could produce excellent results.
In closing, ImageNet and its pre-training role have been
incredibly inﬂuential in computer vision, and we hope that
our new experimental evidence about ImageNet and its role
will shed light into potential future directions for the community to move forward.