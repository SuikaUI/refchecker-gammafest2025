Estimating the Support of a High-Dimensional Distribution
Bernhard Sch¬®olkopf
John C. Platt
John Shawe-Taylor
Alex J. Smola
Robert C. Williamson
? Microsoft Research Ltd, 1 Guildhall Street, Cambridge CB2 3NH, UK
z Microsoft Research, 1 Microsoft Way, Redmond, WA, USA
y Royal Holloway, University of London, Egham, UK
x Department of Engineering, Australian National University,
Canberra 0200, Australia
 , , 
 , 
27 November 1999; revised 18 September 2000
Technical Report
MSR-TR-99-87
Microsoft Research
Microsoft Corporation
One Microsoft Way
Redmond, WA 98052
An abridged version of this document will appear in Neural Computation.
Suppose you are given some dataset drawn from an underlying probability
distribution
P and you want to estimate a ‚Äúsimple‚Äù subset
S of input space such
that the probability that a test point drawn from
P lies outside of
S equals some
a priori speciÔ¨Åed value between
We propose a method to approach this problem by trying to estimate a function
f which is positive on
S and negative on the complement. The functional
f is given by a kernel expansion in terms of a potentially small subset of
the training data; it is regularized by controlling the length of the weight vector
in an associated feature space. The expansion coefÔ¨Åcients are found by solving
a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of
the statistical performance of our algorithm.
The algorithm is a natural extension of the support vector algorithm to the
case of unlabelled data.
Keywords. Support Vector Machines, Kernel Methods, Density Estimation,
Unsupervised Learning, Novelty Detection, Condition Monitoring, Outlier Detection
Introduction
During recent years, a new set of kernel techniques for supervised learning has been
developed . SpeciÔ¨Åcally, support vector (SV)
algorithms for pattern recognition, regression estimation and solution of inverse problems have received considerable attention.
There have been a few attempts to transfer the idea of using kernels to compute
inner products in feature spaces to the domain of unsupervised learning. The problems
in that domain are, however, less precisely speciÔ¨Åed. Generally, they can be characterized as estimating functions of the data which tell you something interesting about the
underlying distributions. For instance, kernel PCA can be characterized as computing functions which on the training data produce unit variance outputs while having
minimum norm in feature space . Another kernel-based unsupervised learning technique, regularized principal manifolds ,
computes functions which give a mapping onto a lower-dimensional manifold minimizing a regularized quantization error. Clustering algorithms are further examples of
unsupervised learning techniques which can be kernelized .
An extreme point of view is that unsupervised learning is about estimating densities. Clearly, knowledge of the density of
P would then allow us to solve whatever
problem can be solved on the basis of the data.
The present work addresses an easier problem: it proposes an algorithm which
computes a binary function which is supposed to capture regions in input space where
the probability density lives (its support), i.e. a function such that most of the data will
live in the region where the function is nonzero . In doing so,
it is in line with Vapnik‚Äôs principle never to solve a problem which is more general
than the one we actually need to solve. Moreover, it is applicable also in cases where
the density of the data‚Äôs distribution is not even well-deÔ¨Åned, e.g. if there are singular
components.
The article is organized as follows. After a review of some previous work in Sec. 2,
we propose SV algorithms for the considered problem. Sec. 4 gives details on the implementation of the optimization procedure, followed by theoretical results characterizing the present approach. In Sec. 6, we apply the algorithm to artiÔ¨Åcial as well as
real-world data. We conclude with a discussion.
Previous Work
In order to describe some previous work, it is convenient to introduce the following
deÔ¨Ånition of a (multi-dimensional) quantile function, introduced by Einmal and Mason
 . Let
` be i.i.d. random variables in a set
X with distribution
be a class of measurable subsets of
 be a real-valued function deÔ¨Åned on
The quantile function with respect to
In the special case where
P is the empirical distribution (P
we obtain the empirical quantile function. We denote by
) the (not
necessarily unique)
C that attains the inÔ¨Åmum (when it is achievable). The most
common choice of
 is Lebesgue measure, in which case
) is the minimum volume
C that contains at least a fraction
of the probability mass. Estimators of the form
) are called minimum volume estimators.
Observe that for
C being all Borel measurable sets,
() is the support of the
p corresponding to
P, assuming it exists. (Note that
() is well deÔ¨Åned even
p does not exist.) For smaller classes
() is the minimum volume
containing the support of
Turning to the case where
, it seems the Ô¨Årst work was reported by Sager
 and then Hartigan who considered
C being the class of
closed convex sets in
X. (They actually considered density contour clusters; cf. Appendix A for a deÔ¨Ånition.) Nolan considered higher dimensions with
the class of ellipsoids. Tsybakov has studied an estimator based on piecewise
polynomial approximation of
) and has shown it attains the asymptotically minimax rate for certain classes of densities
p. Polonik has studied the estimation
). He derived asymptotic rates of convergence in terms of various
measures of richness of
C. He considered both VC classes and classes with a log
covering number with bracketing of order
0. More information on
minimum volume estimators can be found in that work, and in Appendix A.
A number of applications have been suggested for these techniques. They include
problems in medical diagnosis , marketing , condition monitoring of machines , estimating manufacturing yields , econometrics and generalized nonlinear principal curves , regression
and spectral analysis , tests for multimodality and clustering and others .
Most of these papers, in particular those with a theoretical slant, do not go all
the way in devising practical algorithms that work on high-dimensional real-worldproblems. A notable exception to this is the work of Tarassenko et al. .
Polonik has shown how one can use estimators of
) to construct density estimators. The point of doing this is that it allows one to encode a range of prior
assumptions about the true density
p that would be impossible to do within the traditional density estimation framework. He has shown asymptotic consistency and rates
of convergence for densities belonging to VC-classes or with a known rate of growth
of metric entropy with bracketing.
Let us conclude this section with a short discussion of how the present work relates
to the above. The present paper describes an algorithm which Ô¨Ånds regions close to
). Our class
C is deÔ¨Åned implicitly via a kernel
k as the set of half-spaces in a
SV feature space. We do not try to minimize the volume of
C in input space. Instead,
we minimize a SV style regularizer which, using a kernel, controls the smoothness
of the estimated function describing
C. In terms of multi-dimensional quantiles, our
approach can be thought of as employing
) are a weight vector and an offset parametrizing a hyperplane in the
feature space associated with the kernel.
The main contribution of the present work is that we propose an algorithm that
has tractable computational complexity, even in high-dimensional cases. Our theory,
which uses very similar tools to those used by Polonik, gives results that we expect
will be of more use in a Ô¨Ånite sample size setting.
Algorithms
We Ô¨Årst introduce terminology and notation conventions. We consider training data
N is the number of observations, and
X is some set. For simplicity, we think
of it as a compact subset of
 be a feature map
F, i.e. a map into an
inner product space
F such that the inner product in the image of
 can be computed
by evaluating some simple kernel is determined by evaluating which side of the hyperplane it
falls on, in feature space. Via the freedom to utilize different types of kernel functions,
this simple geometric picture corresponds to a variety of nonlinear estimators in input
To separate the data set from the origin, we solve the following quadratic program:
subject to
] is a parameter whose meaning will become clear later.
Since nonzero slack variables
i are penalized in the objective function, we can
expect that if
 solve this problem, then the decision function
will be positive for most examples
i contained in the training set,1 while the SV type
regularization term
k will still be small. The actual trade-off between these two
goals is controlled by
Using multipliers
0, we introduce a Lagrangian
and set the derivatives with respect to the primal variables
 equal to zero, yielding
In (8), all patterns
0g are called Support Vectors. Together with (2),
the SV expansion transforms the decision function (6) into a kernel expansion
Substituting (8) ‚Äì (9) into
L (7), and using (2), we obtain the dual problem:
) subject to
One can show that at the optimum, the two inequality constraints (5) become equalities
i are nonzero, i.e. if
`). Therefore, we can recover
exploiting that for any such
i, the corresponding pattern
i satisÔ¨Åes
1We use the convention that sgn(z
  otherwise.
Note that if
 approaches
0, the upper boundaries on the Lagrange multipliers tend
to inÔ¨Ånity, i.e. the second inequality constraint in (11) becomes void. The problem then
resembles the corresponding hard margin algorithm, since the penalization of errors
becomes inÔ¨Ånite, as can be seen from the primal objective function (4). It is still a
feasible problem, since we have placed no restriction on the offset
, so it can become
a large negative number in order to satisfy (5). If we had required
0 from the start,
we would have ended up with the constraint
 instead of the corresponding
equality constraint in (11), and the multipliers
i could have diverged.
It is instructive to compare (11) to a Parzen windows estimator. To this end, suppose we use a kernel which can be normalized as a density in input space, such as
the Gaussian (3). If we use
, then the two constraints only allow the solution
=`. Thus the kernel expansion in (10) reduces to a Parzen windows estimate of the underlying density. For
, the equality constraint in (11) still
ensures that the decision function is a thresholded density, however, in that case, the
density will only be represented by a subset of training examples (the SVs) ‚Äî those
which are important for the decision (10) to be taken. Sec. 5 will explain the precise
meaning of
To conclude this section, we note that one can also use balls to describe the data
in feature space, close in spirit to the algorithms of Sch¬®olkopf et al. , with hard
boundaries, and Tax and Duin , with ‚Äúsoft margins.‚Äù Again, we try to put most
of the data into a small ball by solving, for
subject to
This leads to the dual
subject to
and the solution
corresponding to a decision function of the form
Similar to the above,
 is computed such that for any
argument of the sgn is zero.
For kernels
) which only depend on
x) is constant. In this case,
the equality constraint implies that the linear term in the dual target function is constant, and the problem (14‚Äì15) turns out to be equivalent to (11). It can be shown that
the same holds true for the decision function, hence the two algorithms coincide in that
case. This is geometrically plausible: for constant
x), all mapped patterns lie on a
sphere in feature space. Therefore, Ô¨Ånding the smallest sphere (containing the points)
really amounts to Ô¨Ånding the smallest segment of the sphere that the data live on. The
segment, however, can be found in a straightforward way by simply intersecting the
data sphere with a hyperplane ‚Äî the hyperplane with maximum margin of separation
to the origin will cut off the smallest segment.
Optimization
The last section has formulated quadratic programs (QPs) for computing regions that
capture a certain fraction of the data. These constrained optimization problems can be
solved via an off-the-shelf QP package to compute the solution. They do, however,
possess features that set them apart from generic QPs, most notably the simplicity of
the constraints. In the present section, we describe an algorithm which takes advantage of these features and empirically scales better to large data set sizes than a standard
QP solver with time complexity of order
) . The algorithm is a
modiÔ¨Åed version of SMO (Sequential Minimal Optimization), an SV training algorithm originally proposed for classiÔ¨Åcation , and subsequently adapted to
regression estimation .
The strategy of SMO is to break up the constrained minimization of (11) into the
smallest optimization steps possible. Due to the constraint on the sum of the dual
variables, it is impossible to modify individual variables separately without possibly
violating the constraint. We therefore resort to optimizing over pairs of variables.
Elementary optimization step.
For instance, consider optimizing over
with all other variables Ô¨Åxed. Using the shorthand
), (11) then reduces
ij, subject to
We discard
C, which is independent of
, and eliminate
 to obtain
with the derivative
Setting this to zero and solving for
 is found,
 can be recovered from
. If the new point
is outside of
`)], the constrained optimum is found by projecting
 from (22)
into he region allowed by the constraints, and the re-computing
The offset
 is recomputed after every such step.
Additional insight can be obtained by rewriting the last equation in terms of the
outputs of the kernel expansion on the examples
 before the optimization
 denote the values of their Lagrange parameter before the step. Then
the corresponding outputs (cf. (10)) read
Using the latter to eliminate the
i, we end up with an update equation for
does not explicitly depend on
which shows that the update is essentially the fraction of Ô¨Årst and second derivative of
the objective function along the direction of
-constraint satisfaction.
Clearly, the same elementary optimization step can be applied to any pair of two
variables, not just
. We next brieÔ¨Çy describe how to do the overall optimization.
Initialization of the algorithm.
We start by setting a fraction
i, randomly
chosen, to
` is not an integer, then one of the examples is set to a value in
`)) to ensure that
. Moreover, we set the initial
Optimization algorithm.
We then select a Ô¨Årst variable for the elementary optimization step in one of the two following ways. Here, we use the shorthand
nb for the indices of variables which are not at bound, i.e.
At the end, these correspond to points that will sit exactly on the hyperplane, and that
will therefore have a strong inÔ¨Çuence on its precise position.
(i) We scan over the entire data set2 until we Ô¨Ånd a variable violating a KKT
condition , i.e. a point such that
0. Once we have found one, say
i, we pick
according to
(ii) Same as (i), but the scan is only performed over
In practice, one scan of type (i) is followed by multiple scans of type (ii), until there
are no KKT violators in
nb, whereupon the optimization goes back to a single scan
of type (i). If the type (i) scan Ô¨Ånds no KKT violators, the optimization terminates.
In unusual circumstances, the choice heuristic (25) cannot make positive progress.
Therefore, a hierarchy of other choice heuristics is applied to ensure positive progress.
2This scan can be accelerated by not checking patterns which are on the correct side of the hyperplane
by a large margin, using the method of Joachims .
These other heuristics are the same as in the case of pattern recognition, cf. , and have been found to work well in our experiments to be reported below.
In our experiments with SMO applied to distribution support estimation, we have
always found it to converge. However, to ensure convergence even in rare pathological
conditions, the algorithm can be modiÔ¨Åed slightly, cf. .
We end this section by stating a trick which is of importance in practical implementations. In practice, one has to use a nonzero accuracy tolerance when checking
whether two quantities are equal. In particular, comparisons of this type are used in determining whether a point lies on the margin. Since we want the Ô¨Ånal decision function
to evaluate to
 for points which lie on the margin, we need to subtract this constant
from the offset
 at the end.
In the next section, it will be argued that subtracting something from
 is actually
advisable also from a statistical point of view.
We now analyse the algorithm theoretically, starting with the uniqueness of the hyperplane (Proposition 2). We then describe the connection to pattern recognition (Proposition 3), and show that the parameter
 characterizes the fractions of SVs and outliers
(Proposition 4). Following that, we give a robustness result for the soft margin (Proposition 5) and Ô¨Ånally we present a theoretical result on the generalization error (Theorem
7). The proofs are given in the Appendix.
In this section, we will use italic letters to denote the feature space images of the
corresponding patterns in input space, i.e.
DeÔ¨Ånition 1 A data set
is called separable if there exists some
F such that
If we use a Gaussian kernel (3), then any data set
` is separable after it is
mapped into feature space: to see this, note that
j, thus all inner
products between mapped patterns are positive, implying that all patterns lie inside the
same orthant. Moreover, since
i, they all have unit length. Hence
they are separable from the origin.
Proposition 2 (Supporting Hyperplane) If the data set (27) is separable, then there
exists a unique supporting hyperplane with the properties that (1) it separates all data
from the origin, and (2) its distance to the origin is maximal among all such hyperplanes. For any
0, it is given by
 subject to
The following result elucidates the relationship between single-class classiÔ¨Åcation
and binary classiÔ¨Åcation.
Proposition 3 (Connection to Pattern Recognition) (i) Suppose
) parametrizes
the supporting hyperplane for the data (27). Then
0) parametrizes the optimal separating hyperplane for the labelled data set
(ii) Suppose
0) parametrizes the optimal separating hyperplane passing through
the origin for a labelled data set
aligned such that
) is positive for
. Suppose, moreover, that
is the margin of the optimal hyperplane. Then
) parametrizes the supporting
hyperplane for the unlabelled data set
Note that the relationship is similar for nonseparable problems. In that case, margin
errors in binary classiÔ¨Åcation (i.e. points which are either on the wrong side of the
separating hyperplane or which fall inside the margin) translate into outliers in singleclass classiÔ¨Åcation, i.e. into points which fall on the wrong side of the hyperplane.
Proposition 3 then holds, cum grano salis, for the training sets with margin errors and
outliers, respectively, removed.
The utility of Proposition 3 lies in the fact that it allows us to recycle certain results
proven for binary classiÔ¨Åcation for use in the single-class
scenario. The following, explaining the signiÔ¨Åcance of the parameter
, is such a case.
Proposition 4 (-Property) Assume the solution of (4),(5) satisÔ¨Åes
0. The following statements hold:
 is an upper bound on the fraction of outliers.
 is a lower bound on the fraction of SVs.
(iii) Suppose the data (27) were generated independently from a distribution
which does not contain discrete components. Suppose, moreover, that the kernel is analytic and non-constant. With probability 1, asymptotically,
 equals both the fraction
of SVs and the fraction of outliers.
Note that this result also applies to the soft margin ball algorithm of Tax and Duin
 , provided that it is stated in the
-parameterization given in Sec. 3.
Proposition 5 (Resistance) Local movements of outliers parallel to
w do not change
the hyperplane.
Note that although the hyperplane does not change, its parametrization in
We now move on to the subject of generalization. Our goal is to bound the probability that a novel point drawn from the same underlying distribution lies outside of
the estimated region. We present a ‚Äúmarginalised‚Äù analysis which in fact provides a
bound on the probability that a novel point lies outside the region slightly larger than
the estimated one.
DeÔ¨Ånition 6 Let
f be a real-valued function on a space
(x)g: Similarly for a training sequence
In the following,
log denotes logarithms to base 2 and
ln denotes natural logarithms.
Theorem 7 (Generalization Error Bound) Suppose we are given a set of
` examples
` generated i.i.d. from an unknown distribution
P which does not contain discrete components. Suppose, moreover, that we solve the optimisation problem
(4),(5) (or equivalently (11)) and obtain a solution
w given explicitly by (10). Let
g denote the induced decision region. With probability
over the draw of the random sample
`, for any
 is given by (12).
The training sample
X deÔ¨Ånes (via the algorithm) the decision region
expect that new points generated according to
P will lie in
;. The theorem gives a
probabilistic guarantee that new points lie in the larger region
The parameter
 can be adjusted when running the algorithm to trade off incorporating outliers versus minimizing the ‚Äúsize‚Äù of
;. Adjusting
 will change the
D. Note that since
D is measured with respect to
 while the bound applies
, any point which is outside of the region that the bound applies to will make a
contribution to
D which is bounded away from
0. Therefore, (32) does not imply that
asymptotically, we will always estimate the complete support.
The parameter
allows one to trade off the conÔ¨Ådence with which one wishes
the assertion of the theorem to hold against the size of the predictive region
one can see from (33) that
k and hence the RHS of (32) scales inversely with
fact, it scales inversely with
, i.e. it increases with
w. This justiÔ¨Åes measuring the
complexity of the estimated region by the size of
w, and minimizing
 in order to
Ô¨Ånd a region that will generalize well. In addition, the theorem suggests not to use the
 returned by the algorithm, which would correspond to
0, but a smaller
frac. SVs/OLs
0.54, 0.43
0.59, 0.47
0.24, 0.03
0.65, 0.38
Figure 1: First two pictures: A single-class SVM applied to two toy problems;
0:, domain:
. Note how in both cases, at least a fraction of
all examples is in the estimated region (cf. table). The large value of
 causes the
additional data points in the upper left corner to have almost no inÔ¨Çuence on the decision function. For smaller values of
, such as
0: (third picture), the points cannot
be ignored anymore. Alternatively, one can force the algorithm to take these ‚Äòoutliers‚Äô (OLs) into account by changing the kernel width (3): in the fourth picture, using
0:, the data is effectively analyzed on a different length scale which
leads the algorithm to consider the outliers as meaningful points.
We do not claim that using Theorem 7 directly is a practical means to determine
the parameters
explicitly. It is loose in several ways. We suspect
c is too large
by a factor of more than 50. Furthermore, no account is taken of the smoothness of the
kernel used. If that were done ), then the Ô¨Årst term in (33)
would increase much more slowly when decreasing
. The fact that the second term
would not change indicates a different tradeoff point. Nevertheless, the theorem gives
one some conÔ¨Ådence that
are suitable parameters to adjust.
Experiments
We apply the method to artiÔ¨Åcial and real-world data. Figure 1 displays 2-D toy examples, and shows how the parameter settings inÔ¨Çuence the solution. Figure 2 shows
a comparison to a Parzen windows estimator on a 2-D problem, along with a family of
estimators which lie ‚Äúin between‚Äù the present one and the Parzen one.
Figure 3 shows a plot of the outputs
(x)) on training and test sets of the US
postal service database of handwritten digits. The database contains
  digit images
; the last
00 constitute the test set. We used a Gaussian kernel
(3), which has the advantage that the data are always separable from the origin in
feature space (cf. the comment following DeÔ¨Ånition 1). For the kernel parameter
. This value was chosen a priori, it is a common value for SVM classiÔ¨Åers
on that data set, cf. Sch¬®olkopf et al. ).3 We fed our algorithm with the training
3In , the following procedure is used to determine a value of
c. For small
training points will become SVs ‚Äî the algorithm just memorizes the data, and will not generalize well.
c increases, the number of SVs drops. As a simple heuristic, one can thus start with a small value of
c and increase it until the number of SVs does not decrease any further.
Figure 2: A single-class SVM applied to a toy problem;
0:, domain:
various settings of the offset
. As discussed in Sec. 3,
 yields a Parzen windows
expansion. However, to get a Parzen windows estimator of the distribution‚Äôs support,
we must in that case not use the offset returned by the algorithm (which would allow
all points to lie outside of the estimated region). Therefore, in this experiment, we
adjusted the offset such that a fraction
0: of patterns would lie outside. From
left to right, we show the results for
g. The rightmost picture
corresponds to the Parzen estimator which utilizes all kernels; the other estimators use
roughly a fraction of
 kernels. Note that as a result of the averaging over all kernels,
the Parzen windows estimate does not model the shape of the distribution very well
for the chosen parameters.
instances of digit
0 only. Testing was done on both digit
0 and on all other digits. We
present results for two values of
, one large, one small; for values in between, the
results are qualitatively similar. In the Ô¨Årst experiment, we used
0%, thus aiming
for a description of ‚Äú0-ness‚Äù which only captures half of all zeros in the training set.
As shown in Ô¨Ågure 3, this leads to zero false positives (i.e. even though the learning
machine has not seen any non-0-s during training, it correctly identiÔ¨Åes all non-0-s as
such), while still recognizing
% of the digits
0 in the test set. Higher recognition
rates can be achieved using smaller values of
%, we get
% correct
recognition of digits
0 in the test set, with a fairly moderate false positive rate of
Whilst leading to encouraging results, this experiment did not really address the
actual task the algorithm was designed for. Therefore, we next focussed on a problem
of novelty detection. Again, we utilized the USPS set; however, this time we trained
the algorithm on the test set and used it to identify outliers ‚Äî it is folklore in the
community that the USPS test set (Fig. 4) contains a number of patterns which are
hard or impossible to classify, due to segmentation errors or mislabelling . In this experiment, we augmented the input patterns by ten extra dimensions
corresponding to the class labels of the digits. The rationale for this is that if we disregarded the labels, there would be no hope to identify mislabelled patterns as outliers.
With the labels, the algorithm has the chance to identify both unusual patterns and
usual patterns with unusual labels. Fig. 5 shows the 20 worst outliers for the USPS
test set, respectively. Note that the algorithm indeed extracts patterns which are very
hard to assign to their respective classes. In the experiment, we used the same kernel
width as above, and a
 value of
%. The latter was chosen roughly to reÔ¨Çect our expectations as to how many ‚Äúbad‚Äù patterns there are in the test set: most good learning
algorithms achieve error rates of
% on the USPS benchmark ).
Figure 3: Experiments on the US postal service OCR dataset. Recognizer for digit
output histogram for the exemplars of
0 in the training/test set, and on test exemplars
of other digits. The
x-axis gives the output values, i.e. the argument of the sgn function
in (10). For
0% (top), we get
0% SVs and
 % outliers (consistent with Proposition 4),
% true positive test examples, and zero false positives from the ‚Äúother‚Äù
class. For
% (bottom), we get
% for SVs and outliers, respectively.
In that case, the true positive rate is improved to
%, while the false positive rate
increases to
%. The offset
 is marked in the graphs.
Note, Ô¨Ånally, that the plots show a Parzen windows density estimate of the output histograms. In reality, many examples sit exactly at the offset value (the non-bound SVs).
Since this peak is smoothed out by the estimator, the fractions of outliers in the training
set appear slightly larger than it should be.
fraction of OLs
fraction of SVs
training time
Table 1: Experimental results for various values of the outlier control constant
test set, size
00. Note that
 bounds the fractions of outliers and support
vectors from above and below, respectively (cf. Proposition 4). As we are not in the
asymptotic regime, there is some slack in the bounds; nevertheless,
 can be used to
control the above fractions. Note, moreover, that training times (CPU time in seconds
on a Pentium II running at 450 MHz) increase as
 approaches
. This is related to
the fact that almost all Lagrange multipliers will be at the upper bound in that case (cf.
Sec. 4). The system used in the outlier detection experiments is shown in bold face.
Figure 4: A subset of
0 examples randomly drawn from the USPS test set, with class
Figure 5: Outliers identiÔ¨Åed by the proposed algorithm, ranked by the negative output
of the SVM (the argument of (10)). The outputs (for convenience in units of
written underneath each image in italics, the (alleged) class labels are given in bold
face. Note that most of the examples are ‚ÄúdifÔ¨Åcult‚Äù in that they are either atypical or
even mislabelled.
In the last experiment, we tested the runtime scaling behaviour of the proposed
SMO solver which is used for training the learning machine (Fig. 6). It was found
to depend on the value of
 utilized. For the small values of
 which are typically
used in outlier detection tasks, the algorithm scales very well to larger data sets, with
a dependency of training times on the sample size which is at most quadratic.
In addition to the experiments reported above, the present algorithm has since been
applied in several other domains, such as the modelling of parameter regimes for the
control of walking robots , and condition monitoring of jet
engines .
Discussion
One could view the present work as an attempt to provide a new algorithm which is in
line with Vapnik‚Äôs principle never to solve a problem which is more general than the
one that one is actually interested in. E.g., in situations where one is only interested
in detecting novelty, it is not always necessary to estimate a full density model of the
data. Indeed, density estimation is more difÔ¨Åcult than what we are doing, in several
Mathematically speaking, a density will only exist if the underlying probability
measure possesses an absolutely continuous distribution function. However, the general problem of estimating the measure for a large class of sets, say the sets measureable in Borel‚Äôs sense, is not solvable . Therefore we need to restrict ourselves to making a statement about the measure of some
sets. Given a small class of sets, the simplest estimator which accomplishes this task
is the empirical measure, which simply looks at how many training points fall into the
region of interest. Our algorithm does the opposite. It starts with the number of training points that are supposed to fall into the region, and then estimates a region with
the desired property. Often, there will be many such regions ‚Äî the solution becomes
unique only by applying a regularizer, which in our case enforces that the region be
small in a feature space associated to the kernel.
Therefore, we must keep in mind that the measure of smallness in this sense de-
training time
training time
Figure 6: Training times vs. data set sizes
` (both axes depict logs at base 2; CPU time
in seconds on a Pentium II running at 450 MHz, training on subsets of the USPS test
. As in Table 1, it can be seen that larger values of
 generally
lead to longer training times (note that the plots use different y-axis ranges). However,
they also differ in their scaling with the sample size. The exponents can be directly
read off from the slope of the graphs, as they are plotted in log scale with equal axis
spacing: for small values of
%), the training times were approximately linear
in the training set size. The scaling gets worse as
 increases. For large values of
training times are roughly proportional to the sample size raised to the power of
(right plot). The results should be taken only as an indication of what is going on:
they were obtained using fairly small training sets, the largest being
00, the size of
the USPS test set. As a consequence, they are fairly noisy, and they strictly speaking
only refer to the examined regime. Encouragingly, the scaling is better than the cubic
one that one would expect when solving the optimization problem using all patterns
at once, cf. Sec. 4. Moreover, for small values of
, i.e. those typically used in outlier
detection (in Fig. 5, we used
%), our algorithm was particularly efÔ¨Åcient.
pends on the kernel used, in a way that is no different to any other method that regularizes in a feature space. A similar problem, however, appears in density estimation
already when done in input space. Let
p denote a density on
X. If we perform a (nonlinear) coordinate transformation in the input domain
X, then the density values will
change; loosely speaking, what remains constant is
dx is transformed,
too. When directly estimating the probability measure of regions, we are not faced
with this problem, as the regions automatically change accordingly.
An attractive property of the measure of smallness that we chose to use is that it can
also be placed in the context of regularization theory, leading to an interpretation of the
solution as maximally smooth in a sense which depends on the speciÔ¨Åc kernel used.
More speciÔ¨Åcally, let us assume that
k is Green‚Äôs function of
P for an operator
mapping into some inner product space , and take a
look at the dual objective function that we minimize,
x). The regularization operators of common kernels can be
shown to correspond to derivative operators ‚Äî therefore,
minimizing the dual objective function corresponds to maximizing the smoothness of
the function
f (which is, up to a thresholding operation, the function we estimate).
This, in turn, is related to a prior
 on the function space.
Interestingly, as the minimization of the dual objective function also corresponds
to a maximization of the margin in feature space, an equivalent interpretation is in
terms of a prior on the distribution of the unknown other class (the ‚Äúnovel‚Äù class in a
novelty detection problem) ‚Äî trying to separate the data from the origin amounts to
assuming that the novel examples lie around the origin.
The main inspiration for our approach stems from the earliest work of Vapnik
and collaborators. In 1962, they proposed an algorithm for characterizing a set of
unlabelled data points by separating it from the origin using a hyperplane . However, they quickly moved on
to two-class classiÔ¨Åcation problems, both in terms of algorithms and in terms of the
theoretical development of statistical learning theory which originated in those days.
From an algorithmic point of view, we can identify two shortcomings of the original approach which may have caused research in this direction to stop for more than
three decades. Firstly, the original algorithm in was
limited to linear decision rules in input space; secondly, there was no way of dealing
with outliers. In conjunction, these restrictions are indeed severe ‚Äî a generic dataset
need not be separable from the origin by a hyperplane in input space.
The two modiÔ¨Åcations that we have incorporated dispose of these shortcomings.
First, the kernel trick allows for a much larger class of functions by nonlinearly mapping into a high-dimensional feature space, and thereby increases the chances of a
separation from the origin being possible. In particular, using a Gaussian kernel (3),
such a separation is always possible, as shown in Sec. 5. The second modiÔ¨Åcation
directly allows for the possibility of outliers. We have incorporated this ‚Äòsoftness‚Äô of
the decision rule using the
-trick and thus obtained a direct
handle on the fraction of outliers.
We believe that our approach, proposing a concrete algorithm with well-behaved
computational complexity (convex quadratic programming) for a problem that so far
has mainly been studied from a theoretical point of view has abundant practical applications. To turn the algorithm into an easy-to-use black-box method for practicioners,
questions like the selection of kernel parameters (such as the width of a Gaussian kernel) have to be tackled. It is our hope that the theoretical results which we have brieÔ¨Çy
outlined in this paper will provide a solid foundation for this formidable task. This,
alongside with algorithmic extensions such as the possibility of taking into account
information about the ‚Äúabnormal‚Äù class Sch¬®olkopf et al. , is subject of current
Acknowledgement.
This work was supported by the ARC, the DFG (# Ja 379/9-
1 and Sm 62/1-1), and by the European Commission under the Working Group Nr.
27150 (NeuroCOLT2). Parts of it were done while BS and AS were with GMD FIRST,
Berlin. Thanks to S. Ben-David, C. Bishop, P. Hayton, N. Oliver, C. Schn¬®orr, J. Spanjaard, L. Tarassenko, and M. Tipping for helpful discussions.