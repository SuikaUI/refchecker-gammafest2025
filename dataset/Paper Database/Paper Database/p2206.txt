Informer: Beyond Efﬁcient Transformer for Long Sequence
Time-Series Forecasting
Haoyi Zhou, 1 Shanghang Zhang, 2 Jieqi Peng, 1 Shuai Zhang, 1 Jianxin Li, 1
Hui Xiong, 3 Wancai Zhang 4
1 SCSE and BDBC, Beihang University, Beijing, China
2 UC Berkeley, California, US
3 Rutgers University, New Jersey, US
4 Beijing Guowang Fuda Science & Technology Development Company
{zhouhy, pengjq, zhangs, lijx}@act.buaa.edu.cn, , {xionghui,zhangwancaibuaa}@gmail.com
Many real-world applications require the prediction of long
sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands
a high prediction capacity of the model, which is the ability
to capture precise long-range dependency coupling between
output and input efﬁciently. Recent studies have shown the
potential of Transformer to increase the prediction capacity.
However, there are several severe issues with Transformer
that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efﬁcient transformer-based
model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which
achieves O(L log L) in time complexity and memory usage,
and has comparable performance on sequences’ dependency
alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efﬁciently
handles extreme long input sequences. (iii) the generative
style decoder, while conceptually simple, predicts the long
time-series sequences at one forward operation rather than
a step-by-step way, which drastically improves the inference
speed of long-sequence predictions. Extensive experiments
on four large-scale datasets demonstrate that Informer signiﬁcantly outperforms existing methods and provides a new
solution to the LSTF problem.
Introduction
Time-series forecasting is a critical ingredient across many
domains, such as sensor network monitoring , energy and smart grid management, economics and ﬁnance , and disease
propagation analysis . In these scenarios, we can leverage a substantial amount of time-series
data on past behavior to make a forecast in the long run,
namely long sequence time-series forecasting (LSTF). However, existing methods are mostly designed under short-term
problem setting, like predicting 48 points or less . All rights reserved.
Ground truth
Prediction
(a) Sequence Forecasting.
The predict sequence length
The predictions/sec
(in log scale)
Inference speed
(b) Run LSTM on sequences.
Figure 1: (a) LSTF can cover an extended period than
the short sequence predictions, making vital distinction in
policy-planning and investment-protecting. (b) The prediction capacity of existing methods limits LSTF’s performance. E.g., starting from length=48, MSE rises unacceptably high, and the inference speed drops rapidly.
et al. 2019; Qin et al. 2017; Wen et al. 2017). The increasingly long sequences strain the models’ prediction capacity
to the point where this trend is holding the research on LSTF.
As an empirical example, Fig.(1) shows the forecasting results on a real dataset, where the LSTM network predicts the
hourly temperature of an electrical transformer station from
the short-term period (12 points, 0.5 days) to the long-term
period (480 points, 20 days). The overall performance gap
is substantial when the prediction length is greater than 48
points (the solid star in Fig.(1b)), where the MSE rises to
unsatisfactory performance, the inference speed gets sharp
drop, and the LSTM model starts to fail.
The major challenge for LSTF is to enhance the prediction capacity to meet the increasingly long sequence demand, which requires (a) extraordinary long-range alignment ability and (b) efﬁcient operations on long sequence inputs and outputs. Recently, Transformer models have shown
superior performance in capturing long-range dependency
than RNN models. The self-attention mechanism can reduce the maximum length of network signals traveling paths
into the theoretical shortest O(1) and avoid the recurrent
structure, whereby Transformer shows great potential for
the LSTF problem. Nevertheless, the self-attention mechanism violates requirement (b) due to its L-quadratic computation and memory consumption on L-length inputs/outputs.
Some large-scale Transformer models pour resources and
The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)
yield impressive results on NLP tasks ,
but the training on dozens of GPUs and expensive deploying
cost make theses models unaffordable on real-world LSTF
problem. The efﬁciency of the self-attention mechanism and
Transformer architecture becomes the bottleneck of applying them to LSTF problems. Thus, in this paper, we seek to
answer the question: can we improve Transformer models to
be computation, memory, and architecture efﬁcient, as well
as maintaining higher prediction capacity?
Vanilla Transformer has three signiﬁcant limitations when solving the LSTF problem:
1. The quadratic computation of self-attention. The atom
operation of self-attention mechanism, namely canonical
dot-product, causes the time complexity and memory usage per layer to be O(L2).
2. The memory bottleneck in stacking layers for long inputs. The stack of J encoder/decoder layers makes total
memory usage to be O(J · L2), which limits the model
scalability in receiving long sequence inputs.
3. The speed plunge in predicting long outputs. Dynamic
decoding of vanilla Transformer makes the step-by-step
inference as slow as RNN-based model (Fig.(1b)).
There are some prior works on improving the efﬁciency of
self-attention. The Sparse Transformer ,
LogSparse Transformer , and Longformer
 all use a heuristic method
to tackle limitation 1 and reduce the complexity of selfattention mechanism to O(L log L), where their efﬁciency
gain is limited . Reformer also achieves O(L log L) with locallysensitive hashing self-attention, but it only works on extremely long sequences. More recently, Linformer claims a linear complexity O(L), but the project
matrix can not be ﬁxed for real-world long sequence input, which may have the risk of degradation to O(L2).
Transformer-XL and Compressive Transformer use auxiliary hidden states to capture long-range dependency, which could amplify limitation
1 and be adverse to break the efﬁciency bottleneck. All these
works mainly focus on limitation 1, and the limitation 2&3
remains unsolved in the LSTF problem. To enhance the prediction capacity, we tackle all these limitations and achieve
improvement beyond efﬁciency in the proposed Informer.
To this end, our work delves explicitly into these three issues. We investigate the sparsity in the self-attention mechanism, make improvements of network components, and conduct extensive experiments. The contributions of this paper
are summarized as follows:
• We propose Informer to successfully enhance the prediction capacity in the LSTF problem, which validates the
Transformer-like model’s potential value to capture individual long-range dependency between long sequence
time-series outputs and inputs.
• We propose ProbSparse self-attention mechanism to ef-
ﬁciently replace the canonical self-attention. It achieves
the O(L log L) time complexity and O(L log L) memory
usage on dependency alignments.
• We propose self-attention distilling operation to privilege dominating attention scores in J-stacking layers and
Masked Multi-head
ProbSparse
Self-attention
Multi-head
Inputs: Xen
Concatenated Feature Map
Inputs: Xde={Xtoken, X0}
0 0 0 0 0 0 0
Fully Connected Layer
Multi-head
ProbSparse
Self-attention
Multi-head
ProbSparse
Self-attention
Figure 2: Informer model overview. Left: The encoder receives massive long sequence inputs (green series). We replace canonical self-attention with the proposed ProbSparse
self-attention. The blue trapezoid is the self-attention distilling operation to extract dominating attention, reducing the
network size sharply. The layer stacking replicas increase robustness. Right: The decoder receives long sequence inputs,
pads the target elements into zero, measures the weighted
attention composition of the feature map, and instantly predicts output elements (orange series) in a generative style.
sharply reduce the total space complexity to be O((2 −
ϵ)L log L), which helps receiving long sequence input.
• We propose generative style decoder to acquire long sequence output with only one forward step needed, simultaneously avoiding cumulative error spreading during the
inference phase.
Preliminary
We ﬁrst provide the LSTF problem deﬁnition. Under the
rolling forecasting setting with a ﬁxed size window, we have
the input X t = {xt
1, . . . , xt
i ∈Rdx} at time t,
and the output is to predict corresponding sequence Yt =
1, . . . , yt
i ∈Rdy}. The LSTF problem encourages
a longer output’s length Ly than previous works and the feature dimension is not limited to univariate case (dy ≥1).
Encoder-decoder architecture Many popular models are
devised to “encode” the input representations X t into a hidden state representations Ht and “decode” an output representations Yt from Ht = {ht
1, . . . , ht
Lh}. The inference
involves a step-by-step process named “dynamic decoding”,
where the decoder computes a new hidden state ht
the previous state ht
k and other necessary outputs from k-th
step then predict the (k + 1)-th sequence yt
Input Representation A uniform input representation is
given to enhance the global positional context and local temporal context of the time-series inputs. To avoid trivializing
description, we put the details in Appendix B.
Methodology
Existing methods for time-series forecasting can be roughly
grouped into two categories1. Classical time-series models serve as a reliable workhorse for time-series forecast-
1Related work is in Appendix A due to space limitation.
ing , and deep learning techniques
mainly develop an encoder-decoder prediction paradigm by
using RNN and their variants . Our proposed Informer
holds the encoder-decoder architecture while targeting the
LSTF problem. Please refer to Fig.(2) for an overview and
the following sections for details.
Efﬁcient Self-attention Mechanism
The canonical self-attention in is de-
ﬁned based on the tuple inputs, i.e, query, key and value,
which performs the scaled dot-product as A(Q, K, V) =
Softmax(QK⊤/
d)V, where Q ∈RLQ×d, K ∈RLK×d,
V ∈RLV ×d and d is the input dimension. To further discuss
the self-attention mechanism, let qi, ki, vi stand for the i-th
row in Q, K, V respectively. Following the formulation in
 , the i-th query’s attention is deﬁned as a
kernel smoother in a probability form:
A(qi, K, V) =
l k(qi, kl)vj = Ep(kj|qi)[vj] , (1)
where p(kj|qi) = k(qi, kj)/P
l k(qi, kl) and k(qi, kj)
selects the asymmetric exponential kernel exp(qik⊤
The self-attention combines the values and acquires outputs
based on computing the probability p(kj|qi). It requires
the quadratic times dot-product computation and O(LQLK)
memory usage, which is the major drawback when enhancing prediction capacity.
Some previous attempts have revealed that the distribution
of self-attention probability has potential sparsity, and they
have designed “selective” counting strategies on all p(kj|qi)
without signiﬁcantly affecting the performance. The Sparse
Transformer incorporates both the row
outputs and column inputs, in which the sparsity arises
from the separated spatial correlation. The LogSparse Transformer notices the cyclical pattern in selfattention and forces each cell to attend to its previous one
by an exponential step size. The Longformer extends previous two works to more
complicated sparse conﬁguration. However, they are limited
to theoretical analysis from following heuristic methods and
tackle each multi-head self-attention with the same strategy,
which narrows their further improvement.
To motivate our approach, we ﬁrst perform a qualitative
assessment on the learned attention patterns of the canonical self-attention. The “sparsity” self-attention score forms
a long tail distribution (see Appendix C for details), i.e., a
few dot-product pairs contribute to the major attention, and
others generate trivial attention. Then, the next question is
how to distinguish them?
Query Sparsity Measurement From Eq.(1), the i-th
query’s attention on all the keys are deﬁned as a probability p(kj|qi) and the output is its composition with values v.
The dominant dot-product pairs encourage the corresponding query’s attention probability distribution away from the
uniform distribution. If p(kj|qi) is close to a uniform distribution q(kj|qi) = 1/LK, the self-attention becomes a
trivial sum of values V and is redundant to the residential
input. Naturally, the “likeness” between distribution p and
q can be used to distinguish the “important” queries. We
measure the “likeness” through Kullback-Leibler divergence
KL(q||p) = ln PLK
ln LK. Dropping the constant, we deﬁne the i-th query’s
sparsity measurement as
M(qi, K) = ln
where the ﬁrst term is the Log-Sum-Exp (LSE) of qi on
all the keys, and the second term is the arithmetic mean on
them. If the i-th query gains a larger M(qi, K), its attention probability p is more “diverse” and has a high chance to
contain the dominate dot-product pairs in the header ﬁeld of
the long tail self-attention distribution.
ProbSparse Self-attention Based on the proposed measurement, we have the ProbSparse self-attention by allowing
each key to only attend to the u dominant queries:
A(Q, K, V) = Softmax(QK⊤
where Q is a sparse matrix of the same size of q and it
only contains the Top-u queries under the sparsity measurement M(q, K). Controlled by a constant sampling factor c,
we set u = c · ln LQ, which makes the ProbSparse selfattention only need to calculate O(ln LQ) dot-product for
each query-key lookup and the layer memory usage maintains O(LK ln LQ). Under the multi-head perspective, this
attention generates different sparse query-key pairs for each
head, which avoids severe information loss in return.
However, the traversing of all the queries for the measurement M(qi, K) requires calculating each dot-product pairs,
i.e., quadratically O(LQLK), besides the LSE operation has
the potential numerical stability issue. Motivated by this, we
propose an empirical approximation for the efﬁcient acquisition of the query sparsity measurement.
Lemma 1. For each query qi ∈Rd and kj ∈Rd in the
keys set K, we have the bound as ln LK ≤M(qi, K) ≤
d} + ln LK. When
qi ∈K, it also holds.
From the Lemma 1 (proof is given in Appendix D.1), we
propose the max-mean measurement as
M(qi, K) = max
The range of Top-u approximately holds in the boundary relaxation with Proposition 1 (refers in Appendix D.2).
Under the long tail distribution, we only need to randomly
sample U = LK ln LQ dot-product pairs to calculate the
M(qi, K), i.e., ﬁlling other pairs with zero. Then, we select sparse Top-u from them as Q. The max-operator in
M(qi, K) is less sensitive to zero values and is numerical stable. In practice, the input length of queries and keys
are typically equivalent in the self-attention computation, i.e
LQ = LK = L such that the total ProbSparse self-attention
time complexity and space complexity are O(L ln L).
T = t + Dx
Attention Block 1
MaxPool1d,
Attention Block 2
MaxPool1d,
Attention Block 3
Figure 3: The single stack in Informer’s encoder. (1) The horizontal stack stands for an individual one of the encoder replicas
in Fig.(2). (2) The presented one is the main stack receiving the whole input sequence. Then the second stack takes half slices
of the input, and the subsequent stacks repeat. (3) The red layers are dot-product matrixes, and they get cascade decrease by
applying self-attention distilling on each layer. (4) Concatenate all stacks’ feature maps as the encoder’s output.
Encoder: Allowing for Processing Longer
Sequential Inputs under the Memory Usage
Limitation
The encoder is designed to extract the robust long-range dependency of the long sequential inputs. After the input representation, the t-th sequence input X t has been shaped into
a matrix Xt
en ∈RLx×dmodel. We give a sketch of the encoder
in Fig.(3) for clarity.
Self-attention Distilling As the natural consequence of
the ProbSparse self-attention mechanism, the encoder’s feature map has redundant combinations of value V. We use
the distilling operation to privilege the superior ones with
dominating features and make a focused self-attention feature map in the next layer. It trims the input’s time dimension
sharply, seeing the n-heads weights matrix (overlapping red
squares) of Attention blocks in Fig.(3). Inspired by the dilated convolution , our “distilling” procedure forwards from
j-th layer into (j + 1)-th layer as:
j+1 = MaxPool
 ELU( Conv1d([Xt
where [·]AB represents the attention block. It contains the
Multi-head ProbSparse self-attention and the essential operations, where Conv1d(·) performs an 1-D convolutional
ﬁlters (kernel width=3) on time dimension with the ELU(·)
activation function . We add a max-pooling layer with stride 2 and downsample Xt into its half slice after stacking a layer, which
reduces the whole memory usage to be O((2 −ϵ)L log L),
where ϵ is a small number. To enhance the robustness of
the distilling operation, we build replicas of the main stack
with halving inputs, and progressively decrease the number
of self-attention distilling layers by dropping one layer at a
time, like a pyramid in Fig.(2), such that their output dimension is aligned. Thus, we concatenate all the stacks’ outputs
and have the ﬁnal hidden representation of encoder.
Decoder: Generating Long Sequential Outputs
Through One Forward Procedure
We use a standard decoder structure in
Fig.(2), and it is composed of a stack of two identical multihead attention layers. However, the generative inference is
employed to alleviate the speed plunge in long prediction.
We feed the decoder with the following vectors as
de = Concat(Xt
0) ∈R(Ltoken+Ly)×dmodel
∈RLtoken×dmodel is the start token, Xt
RLy×dmodel is a placeholder for the target sequence (set
scalar as 0). Masked multi-head attention is applied in the
ProbSparse self-attention computing by setting masked dotproducts to −∞. It prevents each position from attending
to coming positions, which avoids auto-regressive. A fully
connected layer acquires the ﬁnal output, and its outsize dy
depends on whether we are performing a univariate forecasting or a multivariate one.
Generative Inference Start token is efﬁciently applied in
NLP’s “dynamic decoding” , and we extend it into a generative way. Instead of choosing speciﬁc
ﬂags as the token, we sample a Ltoken long sequence in the
input sequence, such as an earlier slice before the output sequence. Take predicting 168 points as an example (7-day
temperature prediction in the experiment section), we will
take the known 5 days before the target sequence as “starttoken”, and feed the generative-style inference decoder with
Xde = {X5d, X0}. The X0 contains target sequence’s time
stamp, i.e., the context at the target week. Then our proposed
decoder predicts outputs by one forward procedure rather
than the time consuming “dynamic decoding” in the conventional encoder-decoder architecture. A detailed performance
comparison is given in the computation efﬁciency section.
Loss function We choose the MSE loss function on prediction w.r.t the target sequences, and the loss is propagated
back from the decoder’s outputs across the entire model.
0.098 0.247
0.092 0.246
0.103 0.259
0.222 0.389
0.114 0.272
0.107 0.280
0.108 0.284
0.115 0.275
0.158 0.319
0.161 0.322
0.167 0.328
0.284 0.445
0.193 0.358
0.162 0.327
0.175 0.424
0.168 0.330
0.183 0.346
0.187 0.355
0.207 0.375
1.522 1.191
0.236 0.392
0.239 0.422
0.396 0.504
1.224 0.763
0.222 0.387
0.215 0.369
0.230 0.398
1.860 1.124
0.590 0.698
0.445 0.552
0.468 0.593
1.549 1.820
0.269 0.435
0.257 0.421
0.273 0.463
2.112 1.436
0.683 0.768
0.658 0.707
0.659 0.766
2.735 3.253
0.093 0.240
0.099 0.241
0.102 0.255
0.263 0.437
0.155 0.307
0.098 0.263
3.554 0.445
0.199 0.381
0.155 0.314
0.159 0.317
0.169 0.348
0.458 0.545
0.190 0.348
0.163 0.341
3.190 0.474
0.304 0.462
0.232 0.389
0.235 0.390
0.246 0.422
1.029 0.879
0.385 0.514
0.255 0.414
2.800 0.595
2.145 1.068
0.263 0.417
0.258 0.423
0.267 0.437
1.668 1.228
0.558 0.606
0.604 0.607
2.753 0.738
2.096 2.543
0.277 0.431
0.285 0.442
0.303 0.493
2.030 1.721
0.640 0.681
0.429 0.580
2.878 1.044
3.355 4.664
0.030 0.137
0.034 0.160
0.065 0.202
0.095 0.228
0.121 0.233
0.091 0.243
0.090 0.206
0.120 0.290
0.069 0.203
0.066 0.194
0.078 0.220
0.249 0.390
0.305 0.411
0.219 0.362
0.179 0.306
0.133 0.305
0.194 0.372
0.187 0.384
0.199 0.386
0.920 0.767
0.287 0.420
0.364 0.496
0.272 0.399
0.194 0.396
0.401 0.554
0.409 0.548
0.411 0.572
1.108 1.245
0.524 0.584
0.948 0.795
0.462 0.558
0.452 0.574
0.512 0.644
0.519 0.665
0.598 0.702
1.793 1.528
1.064 0.873
2.437 1.352
0.639 0.697
2.747 1.174
0.117 0.251
0.119 0.256
0.136 0.279
0.231 0.401
0.131 0.254
0.128 0.274
0.219 0.355
0.302 0.433
0.178 0.318
0.185 0.316
0.206 0.356
0.328 0.423
0.190 0.334
0.203 0.353
0.273 0.409
0.445 0.536
0.266 0.398
0.269 0.404
0.309 0.439
0.654 0.634
0.341 0.448
0.293 0.451
0.503 0.599
2.441 1.142
0.297 0.416
0.310 0.422
0.359 0.484
1.792 1.093
0.456 0.554
0.585 0.644
0.728 0.730
1.987 2.468
0.359 0.466
0.361 0.471
0.388 0.499
2.087 1.534
0.866 0.809
0.499 0.596
1.062 0.943
3.859 1.144
0.239 0.359
0.238 0.368
0.280 0.429
0.971 0.884
0.493 0.539
0.204 0.357
0.879 0.764
0.524 0.595
0.447 0.503
0.442 0.514
0.454 0.529
1.671 1.587
0.723 0.655
0.315 0.436
1.032 0.833
2.725 1.273
0.489 0.528
0.501 0.552
0.514 0.563
3.528 2.196
1.212 0.898
0.414 0.519
1.136 0.876
2.246 3.077
0.540 0.571
0.543 0.578
0.558 0.609
4.891 4.047
1.511 0.966
0.563 0.595
1.251 0.933
4.243 1.415
0.582 0.608
0.594 0.638
0.624 0.645
7.019 5.105
1.545 1.006
0.657 0.683
1.370 0.982
6.901 4.264
Table 1: Univariate long sequence time-series forecasting results on four datasets (ﬁve cases).
Experiment
We extensively perform experiments on four datasets, including 2 collected real-world datasets for LSTF and 2 public benchmark datasets.
ETT (Electricity Transformer Temperature)2: The ETT is
a crucial indicator in the electric power long-term deployment. We collected 2-year data from two separated counties
in China. To explore the granularity on the LSTF problem,
we create separate datasets as {ETTh1, ETTh2} for 1-hourlevel and ETTm1 for 15-minute-level. Each data point consists of the target value ”oil temperature” and 6 power load
features. The train/val/test is 12/4/4 months.
ECL (Electricity Consuming Load)3: It collects the electricity consumption (Kwh) of 321 clients. Due to the missing
data , we convert the dataset into hourly consumption of 2 years and set ‘MT 320’ as the target value.
The train/val/test is 15/3/4 months.
Weather 4: This dataset contains local climatological data
for nearly 1,600 U.S. locations, 4 years from 2010 to 2013,
where data points are collected every 1 hour. Each data point
2We collected the ETT dataset and published it at https://
github.com/zhouhaoyi/ETDataset.
3ECL dataset was acquired at 
datasets/ElectricityLoadDiagrams20112014.
4Weather dataset was acquired at 
data/local-climatological-data/.
consists of the target value “wet bulb” and 11 climate features. The train/val/test is 28/10/10 months.
Experimental Details
We brieﬂy summarize basics, and more information on network components and setups are given in Appendix E.
Baselines: We have selected ﬁve time-series forecasting methods as comparison, including ARIMA , Prophet , LSTMa , LSTnet and DeepAR . To better explore the ProbSparse selfattention’s performance in our proposed Informer, we incorporate the canonical self-attention variant (Informer†),
the efﬁcient variant Reformer and the most related work LogSparse self-attention
 in the experiments. The details of network
components are given in Appendix E.1.
Hyper-parameter tuning: We conduct grid search over
the hyper-parameters, and detailed ranges are given in Appendix E.3. Informer contains a 3-layer stack and a 1-layer
stack (1/4 input) in the encoder, and a 2-layer decoder. Our
proposed methods are optimized with Adam optimizer, and
its learning rate starts from 1e−4, decaying 0.5 times smaller
every epoch. The total number of epochs is 8 with proper
early stopping. We set the comparison methods as recommended, and the batch size is 32. Setup: The input of each
dataset is zero-mean normalized. Under the LSTF settings,
Table 2: Multivariate long sequence time-series forecasting results on four datasets (ﬁve cases).
we prolong the prediction windows size Ly progressively,
i.e., {1d, 2d, 7d, 14d, 30d, 40d} in {ETTh, ECL, Weather},
{6h, 12h, 24h, 72h, 168h} in ETTm. Metrics: We use two
evaluation metrics, including MSE = 1
i=1(y −ˆy)2 and
i=1 |y −ˆy| on each prediction window (averaging for multivariate prediction), and roll the whole set with
stride = 1. Platform: All the models were trained/tested on
a single Nvidia V100 32GB GPU. The source code is available at 
Results and Analysis
Table 1 and Table 2 summarize the univariate/multivariate
evaluation results of all the methods on 4 datasets. We gradually prolong the prediction horizon as a higher requirement
of prediction capacity, where the LSTF problem setting is
precisely controlled to be tractable on one single GPU for
each method. The best results are highlighted in boldface.
Univariate Time-series Forecasting Under this setting,
each method attains predictions as a single variable over
time series. From Table 1, we can observe that: (1) The proposed model Informer signiﬁcantly improves the inference
performance (wining-counts in the last column) across all
datasets, and their predict error rises smoothly and slowly
within the growing prediction horizon, which demonstrates
the success of Informer in enhancing the prediction capacity
in the LSTF problem. (2) The Informer beats its canonical
degradation Informer† mostly in wining-counts, i.e., 32>12,
which supports the query sparsity assumption in providing
a comparable attention feature map. Our proposed method
also out-performs the most related work LogTrans and Reformer. We note that the Reformer keeps dynamic decoding
and performs poorly in LSTF, while other methods beneﬁt
from the generative style decoder as nonautoregressive predictors. (3) The Informer model shows signiﬁcantly better
results than recurrent neural networks LSTMa. Our method
has a MSE decrease of 26.8% (at 168), 52.4% (at 336) and
60.1% (at 720). This reveals a shorter network path in the
self-attention mechanism acquires better prediction capacity than the RNN-based models. (4) The proposed method
outperforms DeepAR, ARIMA and Prophet on MSE by decreasing 49.3% (at 168), 61.1% (at 336), and 65.1% (at 720)
in average. On the ECL dataset, DeepAR performs better
on shorter horizons (≤336), and our method surpasses on
longer horizons. We attribute this to a speciﬁc example, in
which the effectiveness of prediction capacity is reﬂected
with the problem scalability.
Multivariate Time-series Forecasting Within this setting, some univariate methods are inappropriate, and LSTnet
is the state-of-art baseline. On the contrary, our proposed Informer is easy to change from univariate prediction to multivariate one by adjusting the ﬁnal FCN layer. From Table 2,
we observe that: (1) The proposed model Informer greatly
outperforms other methods and the ﬁndings 1 & 2 in the univariate settings still hold for the multivariate time-series. (2)
The Informer model shows better results than RNN-based
LSTMa and CNN-based LSTnet, and the MSE decreases
Prolong Input Length (Lx, Ltoken)
Encoder Input (horizon=48)
Decoder Token (horizon=48)
Encoder Input (horizon=168)
Decoder Token (horizon=168)
(a) Input length.
Encoder Input Length (Lx)
Informer, factor c=3
Informer, factor c=5
Informer, factor c=8
Informer, factor c=10
(b) Sampling Factor.
Encoder Input Length (Lx)
L-scale Dependency
L/2-scale Dependency
L/4-scale Dependency
Informer Dependency
(c) Stacking Combination.
Figure 4: The parameter sensitivity of three components in Informer.
Prediction length
Encoder’s input
1 Informer† uses the canonical self-attention mechanism.
2 The ‘-’ indicates failure for the out-of-memory.
Table 3: Ablation study of the ProbSparse self-attention mechanism.
O(L log L)
O(L log L)
Transformer
O(L log L)
O(L log L)
O(L log L)
1 The LSTnet is hard to present in a closed form.
2 The ⋆denotes applying our proposed decoder.
Table 4: L-related computation statics of each layer.
26.6% (at 168), 28.2% (at 336), 34.3% (at 720) in average.
Compared with the univariate results, the overwhelming performance is reduced, and such phenomena can be caused by
the anisotropy of feature dimensions’ prediction capacity. It
is beyond the scope of this paper, and we will explore it in
the future work.
LSTF with Granularity Consideration We perform an
additional comparison to explore the performance with various granularities. The sequences {96, 288, 672} of ETTm1
(minutes-level) are aligned with {24, 48, 168} of ETTh1
(hour-level). The Informer outperforms other baselines even
if the sequences are at different granularity levels.
Parameter Sensitivity
We perform the sensitivity analysis of the proposed Informer model on ETTh1 under the univariate setting. Input
Length: In Fig.(4a), when predicting short sequences (like
48), initially increasing input length of encoder/decoder degrades performance, but further increasing causes the MSE
to drop because it brings repeat short-term patterns. However, the MSE gets lower with longer inputs in predicting long sequences (like 168). Because the longer encoder
input may contain more dependencies, and the longer decoder token has rich local information. Sampling Factor:
The sampling factor controls the information bandwidth of
ProbSparse self-attention in Eq.(3). We start from the small
factor (=3) to large ones, and the general performance increases a little and stabilizes at last in Fig.(4b). It veriﬁes
our query sparsity assumption that there are redundant dotproduct pairs in the self-attention mechanism. We set the
sample factor c = 5 (the red line) in practice. The Combination of Layer Stacking: The replica of Layers is complementary for the self-attention distilling, and we investigate
each stack {L, L/2, L/4}’s behavior in Fig.(4c). The longer
stack is more sensitive to the inputs, partly due to receiving
more long-term information. Our method’s selection (the red
line), i.e., joining L and L/4, is the most robust strategy.
Ablation Study: How well Informer works?
We also conducted additional experiments on ETTh1 with
ablation consideration.
The performance of ProbSparse self-attention mechanism In the overall results Table 1 & 2, we limited the
problem setting to make the memory usage feasible for the
canonical self-attention. In this study, we compare our methods with LogTrans and Reformer, and thoroughly explore
their extreme performance. To isolate the memory efﬁcient
problem, we ﬁrst reduce settings as {batch size=8, heads=8,
dim=64}, and maintain other setups in the univariate case.
In Table 3, the ProbSparse self-attention shows better performance than the counterparts. The LogTrans gets OOM
in extreme cases because its public implementation is the
Prediction length
Encoder’s input
1 Informer‡ removes the self-attention distilling from Informer†.
2 The ‘-’ indicates failure for the out-of-memory.
Table 5: Ablation study of the self-attention distilling.
Prediction length
Prediction offset
1 Informer§ replaces our decoder with dynamic decoding one in Informer‡.
2 The ‘-’ indicates failure for the unacceptable metric results.
Table 6: Ablation study of the generative style decoder.
Encoder Input length (Lx)
Train time (day)
Decoder predict length (Ly)
Inference time (day)
Figure 5: The total runtime of training/testing phase.
mask of the full-attention, which still has O(L2) memory
usage. Our proposed ProbSparse self-attention avoids this
from the simplicity brought by the query sparsity assumption in Eq.(4), referring to the pseudo-code in Appendix E.2,
and reaches smaller memory usage.
The performance of self-attention distilling In this
study, we use Informer† as the benchmark to eliminate
additional effects of ProbSparse self-attention. The other
experimental setup is aligned with the settings of univariate Time-series. From Table 5, Informer† has fulﬁlled
all the experiments and achieves better performance after
taking advantage of long sequence inputs. The comparison method Informer‡ removes the distilling operation and
reaches OOM with longer inputs (> 720). Regarding the
beneﬁts of long sequence inputs in the LSTF problem, we
conclude that the self-attention distilling is worth adopting,
especially when a longer prediction is required.
The performance of generative style decoder In this
study, we testify the potential value of our decoder in acquiring a “generative” results. Unlike the existing methods, the
labels and outputs are forced to be aligned in the training and
inference, our proposed decoder’s predicting relies solely on
the time stamp, which can predict with offsets. From Table 6, we can see that the general prediction performance
of Informer‡ resists with the offset increasing, while the
counterpart fails for the dynamic decoding. It proves the decoder’s ability to capture individual long-range dependency
between arbitrary outputs and avoid error accumulation.
Computation Efﬁciency
With the multivariate setting and all the methods’ current ﬁnest implement, we perform a rigorous runtime
comparison in Fig.(5). During the training phase, the Informer (red line) achieves the best training efﬁciency among
Transformer-based methods. During the testing phase, our
methods are much faster than others with the generative
style decoding. The comparisons of theoretical time complexity and memory usage are summarized in Table 4. The
performance of Informer is aligned with the runtime experiments. Note that the LogTrans focus on improving the selfattention mechanism, and we apply our proposed decoder in
LogTrans for a fair comparison (the ⋆in Table 4).
Conclusion
In this paper, we studied the long-sequence time-series forecasting problem and proposed Informer to predict long sequences. Speciﬁcally, we designed the ProbSparse selfattention mechanism and distilling operation to handle the
challenges of quadratic time complexity and quadratic memory usage in vanilla Transformer. Also, the carefully designed generative decoder alleviates the limitation of traditional encoder-decoder architecture. The experiments on
real-world data demonstrated the effectiveness of Informer
for enhancing the prediction capacity in LSTF problem.
Acknowledgments
This work was supported by grants from the Natural Science
Foundation of China (U20B2053, 61872022 and 61421003)
and State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12). Thanks for computing infrastructure provided by Beijing Advanced Innovation Center for Big Data and Brain Computing. This work was also
sponsored by CAAI-Huawei MindSpore Open Fund. The
corresponding author is Jianxin Li.
Ethics Statement
The proposed Informer can process long inputs and make
efﬁcient long sequence inference, which can be applied
to the challenging long sequence times series forecasting
(LSTF) problem. The signiﬁcant real-world applications include sensor network monitoring , energy and smart grid management, disease propagation analysis , economics and ﬁnance forecasting , evolution of agriecosystems, climate change forecasting, and variations in air
pollution. As a speciﬁc example, online sellers can predict
the monthly product supply, which helps to optimize longterm inventory management. The distinct difference from
other time series problems is its requirement on a high degree of prediction capacity. Our contributions are not limited to the LSTF problem. In addition to acquiring long sequences, our method can bring substantial beneﬁts to other
domains, such as long sequence generation of text, music,
image, and video.
Under the ethical considerations, any time-series forecasting application that learns from the history data runs the
risk of producing biased predictions. It may cause irreparable losses to the real owners of the property/asset. Domain
experts should guide the usage of our methods, while the
long sequence forecasting can also beneﬁt the work of the
domain experts. Taking applying our methods to electrical
transformer temperature prediction as an example, the manager will examine the results and decide the future power
deployment. If a long enough prediction is available, it will
be helpful for the manager to prevent irreversible failure in
the early stage. In addition to identifying the bias data, one
promising method is to adopt transfer learning. We have donated the collected data (ETT dataset) for further research
on related topics, such as water supply management and 5G
network deployment. Another drawback is that our method
requires high-performance GPU, which limits its application
in the underdevelopment regions.