Rethinking Knowledge Graph Propagation for Zero-Shot Learning
Michael Kampffmeyer∗1, Yinbo Chen∗2, Xiaodan Liang†3, Hao Wang4, Yujia Zhang5, and Eric P. Xing6
1UiT The Arctic University of Norway, 2Tsinghua University, 3Sun Yat-sen University, 4Massachusetts
Institute of Technology, 5Institute of Automation, Chinese Academy of Sciences,
6Carnegie Mellon University
Graph convolutional neural networks have recently
shown great potential for the task of zero-shot learning.
These models are highly sample efﬁcient as related concepts in the graph structure share statistical strength allowing generalization to new classes when faced with a
lack of data. However, multi-layer architectures, which are
required to propagate knowledge to distant nodes in the
graph, dilute the knowledge by performing extensive Laplacian smoothing at each layer and thereby consequently
decrease performance. In order to still enjoy the beneﬁt
brought by the graph structure while preventing dilution of
knowledge from distant nodes, we propose a Dense Graph
Propagation (DGP) module with carefully designed direct
links among distant nodes. DGP allows us to exploit the hierarchical graph structure of the knowledge graph through
additional connections. These connections are added based
on a node’s relationship to its ancestors and descendants. A
weighting scheme is further used to weigh their contribution
depending on the distance to the node to improve information propagation in the graph. Combined with ﬁnetuning
of the representations in a two-stage training approach our
method outperforms state-of-the-art zero-shot learning approaches.
1. Introduction
With the ever-growing supply of image data, from an
ever-expanding number of classes, there is an increasing
need to use prior knowledge to classify images from unseen classes into correct categories based on semantic relationships between seen and unseen classes. This task is
called zero-shot image classiﬁcation. Crucial to this task is
precise modeling of class relationships based on prior class
∗Indicates equal contribution.
†Corresponding Author.
knowledge. Previously, prior knowledge has been incorporated in form of semantic descriptions of classes, such as
attributes or word embeddings , or by
using semantic relations such as knowledge graphs . Approaches that use knowledge graphs are lessexplored and generally assume that unknown classes can
exploit similarity to known classes. Recently, the beneﬁt of
hybrid approaches that combine knowledge graph and semantic class descriptions has been illustrated .
The current state-of-the-art by Wang et al. processes
the unweighted knowledge graph by exploiting recent developments in neural networks for non-Euclidean spaces,
such as graph and manifold spaces .
A deep graph
convolutional neural network (GCN) is used and the
problem is phrased as a regression task, where the GCN
is trained to output a classiﬁer for each class by regressing real-valued weight vectors. These weight vectors correspond to the last layer weights of a pretrained convolutional neural network (CNN) and can be viewed as logistic
regression classiﬁers on top of the feature extraction produced by the CNN. GCNs balance model complexity and
expressiveness with a simple scalable model relying on the
idea of message passing, i.e. nodes pass knowledge to their
neighbors. However, these models were originally designed
for classiﬁcation tasks, albeit semi-supervised, an arguably
simpler task than regression. In recent work, it has been
shown that GCNs perform a form of Laplacian smoothing,
where feature representations will become more similar as
depth increases leading to easier classiﬁcation . In the
regression setting, instead, the aim is to exchange information between nodes in the graph and extensive smoothing is
not desired as it dilutes information and does not allow for
accurate regression. For instance, in a connected graph all
features in a GCN with n layers will converge to the same
representation as n →∞under some conditions, hence
washing out all information .
Therefore, we argue that this approach is not ideal for
the task of zero-shot learning and that the number of lay-
(a) Graph Propagation
Descendants
Propagation
Descendent
Propagation
(b) Dense Graph Propagation
Figure 1: a) Illustration of graph propagation in a GCN for node ’Cat’. Here, graph propagation represents the knowledge
that a node receives in a single layer for previous approaches. b) Proposed dense graph propagation for node ’Cat’. The node
receives knowledge from all its descendants during the descendant phase (blue arrows) and its ancestors during the ancestor
phase (red arrows). This leads to a densely connected graph where knowledge can directly propagate between related nodes.
The learned weights αa
k are used to weigh nodes that are k-hops away from a given node in the ancestor and the
descendants phase, respectively.
ers in the GCN should be small in order to avoid smoothing. We illustrate this phenomenon in practice, by showing
that a shallow GCN consistently outperforms previously reported results. Choosing a small number of layers, however, has the effect that knowledge will not propagate well
through the graph. A 1-layer GCN for instance only considers neighbors that are two hops away in the graph such that
only immediate neighbors inﬂuence a given node. Thus,
we propose a dense connectivity scheme, where nodes are
connected directly to descendants/ancestors in order to include distant information. These new connections allow us
to propagate information without over-smoothing but remove important structural information in the graph since
all descendants/ancestors would be included in the one-hop
neighborhood and would be weighed equally when computing the regression weight vector for a given class. To
address this issue, we further propose a weighting scheme
that considers the distance between nodes in order to weigh
the contribution of different nodes. This allows the model
to not only recover the original structure in the graph but
further provides an additional degree of ﬂexibility that enhances the inference capabilities of our model. Introducing
distance-based shared weights also has the beneﬁt that it
only adds a minimal amount of parameters, is computationally efﬁcient, and balances model ﬂexibility and restrictiveness to allow good predictions for the nodes of the unseen
classes. Fig. 1 illustrates the difference in the way knowledge is propagated in this proposed Dense Graph Propagation (DGP) module compared to a GCN layer.
To allow the feature extraction stage of the pre-trained
CNN to adjust to the newly learned classiﬁers we propose
a two-phase training scheme. In the ﬁrst step, the DGP is
trained to predict the last layer CNN weights. In the second
phase, we replace the last layer weights of the CNN with
the weights predicted by the DGP, freeze the weights and
ﬁnetune the remaining weights of the CNN by optimizing
the cross entropy classiﬁcation loss on the seen classes.
Our main contributions are the following:
• An analysis of our intuitions for zero-shot learning and
an illustration of how these intuitions can be combined
to design a DGP that outperforms previous state-ofthe-art approaches.1
• Our DGP module, which explicitly exploits the hierarchical structure of the knowledge graph to perform
zero-shot learning by efﬁciently propagating knowledge through the proposed dense connectivity structure.
• A novel weighting scheme for DGP where weights are
learned based on the distance between nodes.
• Experimental results on various splits of the 21K ImageNet dataset, a popular large-scale dataset for zeroshot learning.
We obtain relative improvements of
more than 50% over previously reported best results.
2. Related Work
Graph convolutional networks are a class of graph neural networks, based on local graph operators .
1The source code for the experiments performed in this paper is available at: 
Their advantage is that their graph structure allows the
sharing of statistical strength between classes making these
methods highly sample efﬁcient. After being introduced in
Bruna et al. , they were extended with an efﬁcient ﬁltering approach based on recurrent Chebyshev polynomials,
reducing their computational complexity to the equivalent
of the commonly used CNNs in image processing operating
on regular grids . Kipf et al. further proposed simpli-
ﬁcations to improve scalability and robustness and applied
their approach to semi-supervised learning on graphs. Their
approach is termed graph convolutional network (GCN) and
provides the foundation for the model in this paper.
Zero-shot learning has in recent years been considered from various set of viewpoints such as manifold alignment , linear auto-encoder , and low-rank embedded dictionary learning approaches , using semantic relationships based on attributes and relations in knowledge graphs .
One of the
early works proposed a method based on the idea of
a model-of-models approach, where a model is trained to
predict class models based on their description. Each class
is modeled as a function of its description. This idea has
recently been used in another work in Wang et al. , the
work most similar to our own, where a graph convolutional
neural network is trained to predict logistic regression classiﬁers on top of pre-trained CNN features in order to predict
unseen classes. Their approach has yielded impressive performance on a set of zero-shot learning tasks and can, to the
author’s knowledge, be considered the current state-of-theart.
3. Approach
Here we ﬁrst formalize the problem of zero-shot learning and provide information on how a GCN model can be
utilized for the task. We then describe our proposed model
Let C denote the set of all classes and Cte and Ctr the
test and training classes, respectively. Further, assume that
the training and test classes are disjoint Cte ∩Ctr = ∅and
that we are given a S dimensional semantic representation
vector z ∈RS for all classes and a set of training data points
Dtr = {( ⃗Xi, ci) i = 1, ..., N}, where ⃗Xi denotes the i-th
training image and ci ∈Ctr the corresponding class label.
In this setting, zero-shot classiﬁcation aims to predict the
class labels of a set of test data points to the set of classes
Cte. Note that, unlike traditional classiﬁcation, the test data
set points have to be assigned to previously unseen classes.
3.1. Graph Convolutional Networks for Zero-Shot
In this work, we perform zero-shot classiﬁcation by using the word embedding of the class labels and the knowledge graph to predict classiﬁers for each unknown class
in form of last layer CNN weights. Our zero-shot learning framework is illustrated in Fig. 2. The last layer CNN
weights are interpreted as a class-speciﬁc classiﬁer for a
given output class on top of the extracted CNN features.
The zero-shot task can then be expressed as predicting a
new set of weights for each of the unseen classes in order to
extend the output layer of the CNN. Our DGP takes as input the combined knowledge graph for all seen and unseen
classes, where each class is represented by a word embedding vector that encodes the class name. It is then trained to
predict the last layer CNN weights for all (seen and unseen)
classes in a semi-supervised manner. Exploiting the knowledge graph allows us to capture semantic relationships between classes, while the word embedding provides a semantic description of each speciﬁc class. During inference, the
predicted weights can then be used to extend the set of output classes in the original CNN to enable classiﬁcation of
datapoints from unseen classes.
More speciﬁcally, given a graph with N nodes and S input features per node, X ∈RN×S denotes the feature matrix. Here each node represents one distinct concept/class
in the classiﬁcation task and each concept is represented by
the word vector of the class name. The connections between
the classes in the knowledge graph are encoded in form of
a symmetric adjacency matrix A ∈RN×N, which also includes self-loops. We employ a simple propagation rule to
perform convolutions on the graph
H(l+1) = σ
D−1AH(l)Θ(l)
where H(l) represents the activations in the lth layer and
Θ ∈RS×F denotes the trainable weight matrix for layer l
with F corresponding to the number of learned ﬁlters. For
the ﬁrst layer, H(0) = X. σ(·) denotes a nonlinear activation function, in our case a Leaky ReLU. Dii = P
a degree matrix D ∈RN×N, which normalizes rows in A
to ensure that the scale of the feature representations is not
modiﬁed by A. Similarly to previous work done on graph
convolutional neural networks, this propagation rule can be
interpreted as a spectral convolution .
The model is trained to predict the classiﬁer weights for
the seen classes by optimizing the loss
W ∈RM×P denotes the prediction of the GCN
for the known classes and therefore corresponds to the M
rows of the GCN output, which correspond to the training classes.
M denotes the number of training classes
and P denotes the dimensionality of the weight vectors.
The ground truth weights are obtained by extracting the
last layer weights of a pre-trained CNN and denoted as
Extraction
Descendant
Propagation
Classifier
Propagation
classifier
classifier
Figure 2: DGP is trained to predict classiﬁer weights W for each node/class in a graph. The weights for the training classes
are extracted from the ﬁnal layer of a pre-trained ResNet. The graph is constructed from a knowledge graph and each
node is represented by a vector that encodes semantic class information, in our experiments the classes word embedding.
The network consists of two phases, a descendant phase where each node receives knowledge form its descendants and an
ancestor phase, where it receives knowledge from its ancestors.
W ∈RM×P . During the inference phase, the features of
new images are extracted from the CNN and the classiﬁers
predicted by the GCN are used to classify the features.
However, the Laplacian smoothing operation in matrix
form can be written as (I−γD−1L)H, as also noted in Li et
al. . Substituting the graph Laplacian with its deﬁnition
L = D−A the operation simpliﬁes for γ = 1 (looking only
at the immediate neighbors) to D−1AH. This corresponds
in parts to the graph convolution operation in Eq. 1.
ˆY = H −γD−1LH = (I −γD−1L)H
ˆY = D−1AH
Thus, repeatedly applying Eq. 1 in a multi-layer GCN
architecture will lead to repeated Laplacian smoothing, thus
diluting the information. Empirical evidence is provided in
the model analysis section (Sec. 4.4).
3.2. Dense Graph Propagation Module
Our DGP for zero-shot learning aims to use the hierarchical graph structure for the zero-shot learning task and
avoids dilution of knowledge by intermediate nodes. This
is achieved using a dense graph connectivity scheme consisting of two phases, namely descendant propagation and
ancestor propagation. This two-phase approach further enables the model to learn separate relations between a node
and its ancestors and a node and its descendants. Table 6 in
the model analysis section provides empirical evidence for
this choice. Unlike the GCN, we do not use the knowledge
graph relations directly as an adjacency graph to include
information from neighbors further away. We do therefore
not suffer from the problem of knowledge being washed out
due to averaging over the graph. Instead, we introduce two
separate connectivity patterns, one where nodes are connected to all their ancestors and one where nodes are connected to all descendants. We use two adjacency matrices:
Aa ∈RN×N denotes the connections from nodes to their
ancestors, whereas Ad denotes the connections from nodes
to their descendants. Note, as a given node is the descendant
of its ancestors, the difference between the two adjacency
matrices is a reversal of their edges Ad = AT
a . Unlike previous approaches, this connectivity pattern allows nodes direct access to knowledge in their extended neighborhood as
opposed to knowledge that has been modiﬁed by intermediate nodes. Note that both these adjacency matrices include
self-loops. The connection pattern is illustrated in Fig. 1.
The same propagation rule as in Eq. 1 is applied consecutively for the two connectivity patterns leading to the overall
DGP propagation rule
Distance weighting scheme In order to allow DGP to
weigh the contribution of various neighbors in the dense
graph, we propose a weighting scheme that weighs a given
node’s neighbors based on the graph distance from the node.
Note, the distance is computed on the knowledge graph
and not the dense graph.
We use wa = {wa
i=0 to denote the learned weights for the ancestor and the descendant propagation phase, respectively.
i correspond to weights for nodes that are i hops
away from the given node. wa
0 correspond to self-loops
K correspond to the weights for all nodes further
than K −1 hops away. We normalize the weights using
a softmax function αa
k = softmax(wa
i=0 exp(wa
Similarly, αd
k = softmax(wd
k). The weighted propagation
rule in Eq. 3 becomes
k denote the parts of the adjacency matrices that only contain the k-hop edges for the ancestor and
descendant propagation phase, respectively. Da
are the corresponding degree matrices for Aa
weights are shared across the graph, the proposed weighting scheme only adds 2×(K +1) parameters to the model,
where K tends to be small (K = 4 in our experiments).
Our proposed weighting scheme is related to the attention mechanisms in graph convolutional neural networks .
However, unlike attention approaches, our
weighting scheme adds only a negligible amount of parameters and does not add the potentially considerable memory overhead of attention approaches. Further, in our zeroshot learning setting, we observed a drop in performance
when including the attention approach proposed in . We
hypothesize that this is due to the fact that a more complex model will be more prone to overﬁt given the limited
amount of labeled data (sparsely labeled graph). Results are
provided in the supplementary material.
3.3. Finetuning
Training is done in two stages, where the ﬁrst stage trains
the DGP to predict the last layer weights of a pre-trained
CNN using Eq. 2. Note, f
W, in this case, contains the M
rows of H, which correspond to the training classes. In order to allow the feature representation of the CNN to adapt
to the new class classiﬁers, we train the CNN by optimizing
the cross-entropy classiﬁcation loss on the seen classes in a
second stage. During this stage, the last layer weights are
ﬁxed to the predicted weights of the training classes in the
DGP and only the feature representation is updated. This
can be viewed as using the DGP as a constraint for the CNN,
as we indirectly incorporate the graph information to constrain the CNN output space.
4. Experiments
We perform a comparative evaluation of the DGP against
previous state-of-the-art on the ImageNet dataset , the
largest commonly used dataset for zero-shot learning 2. In
our work, we follow the train/test split suggested by Frome
2Additional experiments have been performed on the AWA2 dataset
and can be found in the supplementary material.
et al. , who proposed to use the 21K ImageNet dataset
for zero-shot evaluation. They deﬁne three tasks in increasing difﬁculty, denoted as ”2-hops”, ”3-hops” and ”All”.
Hops refer to the distance that classes are away from the
ImageNet 2012 1K classes in the ImageNet hierarchy and
thus is a measure of how far unseen classes are away from
seen classes. ”2-hops” contains all the classes within two
hops from the seen classes and consists of roughly 1.5K
classes, while ”3-hops” contains about 7.8K classes. ”All”
contains close to 21K classes.
None of the classes are
contained in the ImageNet 2012 dataset, which was used
to pre-train the ResNet-50 model. Mirroring the experiment setup in we further evaluate the performance when training categories are included as potential
labels. Note that since the only difference is the number
of classes during the inference phase, the model does not
have to be retrained. We denote the splits as ”2-hops+1K”,
”3-hops+1K”, ”All+1K”.
4.1. Training details
We use a ResNet-50 model that has been pre-trained
on the ImageNet 2012 dataset. Following Wang et al. ,
we use the GloVe text model trained on the Wikipedia
dataset as the feature representation of our concepts in the
graph. The DGP model consists of two layers as illustrated
in Eq. 3 with feature dimensions of 2048 and the ﬁnal output
dimension corresponds to the number of weights in the last
layer of the ResNet-50 architecture, 2049 for weights and
bias. Following the observation of Wang et al. , we
perform L2-Normalization on the outputs as it regularizes
the outputs into similar ranges. Similarly, we also normalize
the ground truth weights produced by the CNN. We further
make use of Dropout with a dropout rate of 0.5 in each
layer. The model is trained for 3000 epochs with a learning
rate of 0.001 and weight decay of 0.0005 using Adam .
We make use of leaky ReLUs with a negative slope of 0.2.
The number of values per phase K was set to 4 as additional
weights had diminishing returns. The proposed DGP model
is implemented in PyTorch and training and testing are
performed on a GTX 1080Ti GPU. Finetuning is done for
20 epochs using SGD with a learning rate of 0.0001 and
momentum of 0.9.
4.2. Comparing approaches
We compare our DGP to the following approaches: Devise linearly maps visual information in form of features extracted by a convolutional neural network to the
semantic word-embedding space.
The transformation is
learned using a hinge ranking loss. Classiﬁcation is performed by assigning the visual features to the class of the
nearest word-embedding. ConSE projects image features into a semantic word embedding space as a convex
combination of the T closest seen classes semantic embed-
Table 1: Top-k accuracy for the different models on the
ImageNet dataset. Accuracy when only testing on unseen
classes. Results indicated with ∗, †, and ‡ are taken from
 , , and , respectively.
DGP (ours)
DGP (ours)
DGP (ours)
Table 2: Top-k accuracy for the different models on the ImageNet dataset. Accuracy when testing on seen and unseen
classes. Results indicated with ††, ‡‡, and ‡ are taken from
 , , and , respectively.
DGP (ours)
DGP (ours)
DGP (ours)
ding weighted by the probabilities that the image belongs
to the seen classes. The probabilities are predicted using
a pre-trained convolutional classiﬁer.
Similar to Devise,
ConSE assigns images to the nearest classes in the embedding space. EXEM creates visual class exemplars by
averaging the PCA projections of images belonging to the
same seen class. A kernel-based regressor is then learned
to map a semantic embedding vector to the class exemplar. For zero-shot learning visual exemplars can be predicted for the unseen classes using the learned regressor and
images can be assigned using nearest neighbor classiﬁcation. SYNC aligns a semantic space (e.g., the wordembedding space) with a visual model space, adds a set of
phantom object classes in order to connect seen and unseen
classes, and derives new embeddings as a convex combination of these phantom classes. GCNZ represents the
current state of the art and is the approach most related to
our proposed DGP. A GCN is trained to predict last layer
weights of a convolutional neural network.
Guided by experimental evidence (see our analysis in Table 5 in the model analysis section) and our intuition that
extensive smoothing is a disadvantage for the weight regression in zero-shot learning, we add a single-hidden-layer
GCN (SGCN) with non-symmetric normalization (D−1A)
(as deﬁned in Eq. 1) as another baseline.
Note, GCNZ
made use of a symmetric normalization (D−1/2AD−1/2)
but our experimental evaluation indicates that the difference is negligible. For the interested reader, an analysis of
the effect of the changes between GCN and SGCN is included in the supplementary material. SGCN further yields
a better baseline since our proposed DGP also utilizes the
non-symmetric normalization. As DGP, our SGCN model
makes use of the proposed two-stage ﬁnetuning approach.
4.3. Comparison to state-of-the-art methods
Quantitative results for the comparison on the ImageNet datasets are shown in Table 1. Compared to previous results such as ConSE , EXEM , and GCNZ 
our proposed methods outperform the previous results with
a considerable margin, achieving, for instance, more than
50% relative improvement for Top-1 accuracy on the 21K
ImageNet ”All” dataset. We observe that our methods especially outperform the baseline models on the ”All” task,
illustrating the potential of our methods to more efﬁciently
propagate knowledge. DGP also achieves consistent improvements over the SGCN model. We observed that ﬁnetuning consistently improved performance for both models
in all our experiments. Ablation studies that highlight the
impact of ﬁnetuning and weighting of neighbors for the 2hop scenario can be found in Table 3. DGP(-wf) is used to
denote the accuracy that is achieved after training the DGP
model without weighting (adding no weights in Eq. 4) and
without ﬁnetuning. DGP(-w) and DGP(-f) are used to denote the results for DGP without weighting and DGP without ﬁnetuning, respectively. We further report the accuracy
achieved by the SGCN model without ﬁnetuning (SGCN(f)). We observe that the proposed weighting scheme, which
allows distant neighbors to have less impact, is crucial for
the dense approach. Further, ﬁnetuning the model consistently leads to improved results.
plane, shoe shop, hook,
sundial, electric fan
fastener, block plane, jointer,
dovetail plane, scrub plane
dovetail plane, beading plane,
jointer, circular plane, block plane
circular plane, dovetail plane,
opener, jointer, router plane
sea lion, oystercatcher, king penguin,
ruddy turnstone, meerkat
pelagic bird, wandering albatross, penguin,
black-footed albatross, california sea lion
penguin, california sea lion, steller sea lion,
south american sea lion, australian sea lion
penguin, california sea lion, south american
sea lion, hoary marmot, yellowbelly marmot
bookcase, entertainment center, library,
file, comic book
wall unit, furniture, secretary,
davenport, writing desk
furniture, office furniture, dining-room
table, wall unit, writing desk
furniture, office furniture, chest of
drawers, cabinet, wall unit
baboon, langur, koala,
macaque, madagascar cat
phalanger, kangaroo, lemur,
marsupial, tree squirrel
phalanger, kangaroo,tree
squirrel, lemur, tree wallaby
tree squirrel, kangaroo,
phalanger, lemur, tree wallaby
Figure 3: Qualitative result comparison. The correct class is highlighted in bold. We report the top-5 classiﬁcation results.
Qualitative results of DGP and the SGCN are shown in
Fig. 3. Example images from unseen test classes are displayed and we compare the results of our proposed DGP
and the SGCN to results produced by a pre-trained ResNet.
Note, ResNet can only predict training classes while the
others predict classes not seen in training. For comparison, we also provide results for our re-implementation of
GCNZ. We observe that the SGCN and DGP generally provide coherent top-5 results. All methods struggle to predict
the opener and tend to predict some type of plane instead,
however, DGP does include opener in the top-5 results.
We further observe that the prediction task on this dataset
for zero-shot learning is difﬁcult as it contains classes of
ﬁne granularity, such as many different types of squirrels,
planes, and furniture. Additional examples are provided in
the supplementary material.
Testing including training classiﬁers. Following the
example of , we also report the results when including both training labels and testing labels as potential
labels during classiﬁcation of the zero-shot examples. Results are shown in Table 2. For the baselines, we include
two implementations of ConSE, one that uses AlexNet as
a backbone and one that uses ResNet-50 . Compared to Table 1, we observe that the accuracy is considerably lower, but the SGCN and DGP still outperform the previous state-of-the-art approach GCNZ. SGCN outperforms
DGP for low k in the Top-k accuracy measure especially
for the 2-hops setting, while DGP outperforms SGCN for
larger k. We observe that DGP tends to favor prediction to
the closest training classes for its Top-1 prediction (see Table 4). However, this is not necessarily a drawback and is a
well-known tradeoff between performing well on the unseen classes and the seen classes, which are not considered
in this setting. This tradeoff can be controlled by including
a novelty detector, which predicts if an image comes from
the seen or unseen classes as done in and then assign it
to the zero-shot classiﬁer or a classiﬁer trained on the seen
Table 3: Results of the ablation experiments on the 2-hops
dataset. (-f), (-w), and (-wf) indicate models without ﬁnetuning, weighting and without both weighting and ﬁnetuning, respectively.
SGCN (ours)
DGP (ours)
classes. Another approach is calibrated stacking , which
rescales the prediction scores of the known classes.
To put the zero-shot performance into perspective, we
perform experiments where we analyze how the model’s
performance on the original 1000 seen classes is affected by
domain shift as additional unseen classes (all 2-hop classes)
are introduced. Table 4 shows the results when the model
is tested on the validation dataset from ImageNet 2012. We
compare the performance to our re-implementation of the
GCNZ model with ResNet-50 backbone and also the performance from the original ResNet-50 model, which is trained
only on the seen classes. It can be observed that both our
methods outperform GCNZ.
4.4. Model analysis
Analysis of weighting scheme. To validate our intuition
that weighting allows our approach to weigh distant neighbors less, we inspect the learned weights. For the ﬁrst stage
the weights are 0.244, 0.476, 0.162, 0.060, 0.058 and for
the second (ﬁnal) stage they are 0.493, 0.322, 0.097, 0.047,
0.041. Note, the ﬁrst value corresponds to self-weighting,
the second to the 1-hop neighbors, and so forth. It can be
observed, that ancestors aggregate information mainly from
their immediate descendants in the ﬁrst phase and later dis-
Table 4: Performance on the seen ImageNet classes. ResNet
represents ideal performance as it only predicts known
classes. GCNZ is our reimplementation of .
DGP (ours)
Table 5: Results for 2-hops for SGCN without ﬁnetuning
when increasing the depth.
Table 6: Results for 2-hops with/without separating the adjacency matrix into ancestors and descendants for DGP.
tribute it to their descendants in the second phase. Further,
we observe that distant neighbors have far less impact in the
ﬁnal stage. This means that the model learns to preserve the
overall graph structure imposed by the knowledge graph,
where importance is governed by the distance in the graph.
Analysis of number of layers. We perform an empirical
evaluation to verify that our intuition is correct and that additional hidden layers indeed cause a drop in performance
when employing a GCN. Table 5 illustrates the performance
when adding additional layers to the GCN for the 2-hops
experiment. These results are reported without ﬁnetuning
the model. In order to perform this ablation study we ﬁx
all hidden layers to have a dimensionality of 2048 with 0.5
dropout. We want to stress that there is a fundamental difference in our experimental setting and the study in Wang et
al. , as their ablation study does not only consider a different number of layers in the network but also a different
number of neurons per layer at the same time.
Analysis of two-phase propagation. We further, perform an ablation study to analyze the beneﬁt of a two-phase
directed propagation rule where ancestors and descendants
are considered individually. We compared this to two consecutive updates using the full adjacency matrix in the dense
method and illustrate the results in Table 6. Consistent improvements are obtained using our proposed two-phase directed propagation rule.
Robustness of results. Table 7 shows the mean and stan-
Table 7: Mean and standard deviation for 3 runs. More
stable as the number of class increases.
26.17±0.03
40.41±0.03
26.67±0.09
40.74±0.04
dard deviation for 3 runs for the 2-hops and All datasets.
The results are stable over multiple runs and it can clearly
be observed that as the number of classes increases (2-hops
to all), results become more stable.
Scalability. To obtain good scalability it is important
that the adjacency matrix A is a sparse matrix so that
the complexity of computing D−1AXΘ is linearly proportional to the number of edges present in A. Our approach
exploits the structure of knowledge graphs, where entities
only have few ancestors and descendants, to ensure this.
The adjacency matrix for the ImageNet hierarchy used in
our experiments, for instance, has a density of 9.3 × 10−5,
while our dense connections only increase the density of the
adjacency matrix to 19.1 × 10−5.
With regards to the number of parameters, the SGCN
consists of 4,810,752 weights. DGP increases the number
of trainable parameters by adding 2 × (K + 1) additional
weights. However, as K = 4 in our experiments, this difference in the number of parameters is negligible. Overall the
number of trainable parameters is considerably lower than
that in the GCNZ model (9,527,808 weights).
5. Conclusion
In contrast to previous approaches using graph convolutional neural networks for zero-shot learning, we illustrate
that the task of zero-shot learning beneﬁts from shallow
networks. Further, to avoid the lack of information propagation between distant nodes in shallow models, we propose DGP, which exploits the hierarchical structure of the
knowledge graph by adding a weighted dense connection
scheme. Experiments illustrate the ability of the proposed
methods, outperforming previous state-of-the-art methods
for zero-shot learning. In future work, we aim to investigate the potential of more advanced weighting mechanisms
to further improve the performance of DGP compared to
the SGCN. The inclusion of additional semantic information for settings where these are available for a subset of
nodes is another future direction.
Acknowledgments:
This work was partially funded by the
Norwegian Research Council FRIPRO grant no. 239844.
work was supported by the Sun Yat-sen University start-up foundation grant no. 76160-18841201.