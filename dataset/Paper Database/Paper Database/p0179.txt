STOCHASTIC VOLATILITY: LIKELIHOOD INFERENCE
AND COMPARISON WITH ARCH MODELS
Sangjoon Kim
Salomon Brothers Asia Limited, 5-2-20 Akasaka, Minato-ku, Tokyo 107, JAPAN
Neil Shephard
Nuï¬ƒeld College, Oxford University, Oxford OX1 1NF, UK
Siddhartha Chib
John M. Olin School of Business, Washington University, St Louis, MO 63130, USA
July 14, 1997
In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a
uniï¬ed, practical likelihood-based framework for the analysis of stochastic volatility models.
A highly eï¬€ective method is developed that samples all the unobserved volatilities at once
using an approximating oï¬€set mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The
paper also develops simulation-based methods for ï¬ltering, likelihood evaluation and model
failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes
factors is also investigated. These methods are used to compare the ï¬t of stochastic volatility
and GARCH models. All the procedures are illustrated in detail.
 
Some key words: Bayes estimation, Bayes factors, Factor stochastic volatility, GARCH, Gibbs
sampler, Heteroscedasticity, Maximum likelihood, Likelihood ratio, Markov chain Monte Carlo,
Marginal likelihood, Quasi-maximum likelihood, Simulation, Stochastic volatility, Stock returns.
INTRODUCTION
The variance of returns on assets tends to change over time. One way of modelling this feature of
the data is to let the conditional variance be a function of the squares of previous observations
and past variances.
This leads to the autoregressive conditional heteroscedasticity (ARCH)
based models developed by Engle and surveyed in Bollerslev, Engle, and Nelson .
An alternative to the ARCH framework is a model in which the variance is speciï¬ed to follow
some latent stochastic process. Such models, referred to as stochastic volatility (SV) models,
appear in the theoretical ï¬nance literature on option pricing in their work generalizing the Black-Scholes option pricing formula to allow for stochastic
volatility). Empirical versions of the SV model are typically formulated in discrete time. The
canonical model in this class for regularly spaced data is:
Î²eht/2Îµt , t â‰¥1
Âµ + Ï†(ht âˆ’Âµ) + ÏƒÎ·Î·t , t â‰¥2
where yt is the mean corrected return on holding the asset at time t, ht is the log volatility at time
t which is assumed to follow a stationary process (|Ï†| < 1) with h1 drawn from the stationary
distribution, Îµt and Î·t are uncorrelated standard normal white noise shocks and N(., .) is the
normal distribution. The parameter Î² or exp(Âµ/2) plays the role of the constant scaling factor
and can be thought of as the modal instantaneous volatility, Ï† as the persistence in the volatility,
and ÏƒÎ· the volatility of the log-volatility. For identiï¬ability reasons either Î² must be set to one
or Âµ to zero. We show later that the parameterization with Î² equal to one in preferable and so
we shall leave Âµ unrestricted when we estimate the model but report results for Î² = exp(Âµ/2)
as this parameter has more economic interpretation.
This model has been used as an approximation to the stochastic volatility diï¬€usion by Hull
and White and Chesney and Scott . Its basic econometric properties are discussed
in Taylor , the review papers by Taylor , Shephard and Ghysels, Harvey, and
Renault and the paper by Jacquier, Polson, and Rossi . These papers also review
the existing literature on the estimation of SV models.
In this paper we make advances in a number of diï¬€erent directions and provide the ï¬rst
complete Markov chain Monte Carlo simulation-based analysis of the SV model (1) that covers
eï¬ƒcient methods for Bayesian inference, likelihood evaluation, computation of ï¬ltered volatility
estimates, diagnostics for model failure, and computation of statistics for comparing non-nested
volatility models. Our study reports on several interesting ï¬ndings. We consider a very simple
Bayesian method for estimating the SV model (based on one-at-a-time updating of the volatilities). This sampler is shown to quite ineï¬ƒcient from a simulation perspective. An improved
(multi-move) method that relies on an oï¬€set mixture of normals approximation to a log-chisquare distribution coupled with a importance reweighting procedure is shown to be strikingly
more eï¬€ective. Additional reï¬nements of the latter method are developed to reduce the number of blocks in the Markov chain sampling.
We report on useful plots and diagnostics for
detecting model failure in a dynamic (ï¬ltering) context. The paper also develops formal tools
for comparing the basic SV and Gaussian and t-GARCH models. We ï¬nd that the simple SV
model typically ï¬ts the data as well as more heavily parameterized GARCH models. Finally,
we consider a number of extensions of the SV model that can be ï¬tted using our methodology.
The outline of this paper is as follows. Section 2 contains preliminaries. Section 3 details
the new algorithms for ï¬tting the SV model. Section 4 contains methods for simulation-based
ï¬ltering, diagnostics and likelihood evaluations. The issue of comparing the SV and GARCH
models is considered in Section 5. Section 6 provides extensions while Section 7 concludes. A
description of software for ï¬tting these models that is available through the internet is provided
in Section 8. Two algorithms used in the paper are provided in the Appendix.
PRELIMINARIES
Quasi-likelihood method
A key feature of the basic SV model in (1) is that it can be transformed into a linear model by
taking the logarithm of the squares of the observations
t = ht + log Ïµ2
where E(log Ïµ2
t) = âˆ’1.2704 and V ar(log Ïµ2
t) = 4.93. Harvey, Ruiz, and Shephard have
employed Kalman ï¬ltering to estimate the parameters Î¸ = (Ï†, Ïƒ2
Î·, Âµ) âˆˆ(âˆ’1, 1) Ã— â„œ+ Ã— â„œby
maximizing the quasi likelihood
log LQ(y|Î¸) = âˆ’n
2 log 2Ï€ âˆ’1
where y = (y1, ..., yn), vt is the one-step-ahead prediction error for the best linear estimator of
t and Ft is the corresponding mean square error 1. It turns out that this quasi-likelihood
estimator is consistent and asymptotically normally distributed but is sub-optimal in ï¬nite
samples because log Ïµ2
t is poorly approximated by the normal distribution, as shown in Figure
1. As a consequence, the quasi-likelihood estimator under the assumption that log Ïµ2
t is normal
1The Kalman ï¬lter algorithms for computing vt and Ft are given in the Appendix.
has poor small sample properties, even though the usual quasi-likelihood asymptotic theory is
Ratio of densities
Figure 1: Log-Normal approximation to Ï‡2
1 density. Left is the Ï‡2
1 density and the log-normal
approximation which is used in the quasi-likelihood approach. Right is the log of the ratio of the
1 density to the approximation.
Markov chain Monte Carlo
An alternative, exact approach to inference in the SV model is based on Markov chain Monte
Carlo (MCMC) methods, namely the Metropolis-Hastings and Gibbs sampling algorithms.
These methods have had a widespread inï¬‚uence on the theory and practice of Bayesian inference. Early work on these methods appears in Metropolis, Rosenbluth, Rosenbluth, Teller,
and Teller , Hastings , Ripley and Geman and Geman while some
of the more recent developments, spurred by Tanner and Wong and Gelfand and Smith
 , are included in Chib and Greenberg , Gilks, Richardson, and Spiegelhalter 
and Tanner . Chib and Greenberg provide a detailed exposition of the
Metropolis-Hastings algorithm and include a derivation of the algorithm from the logic of reversibility.
The idea behind MCMC methods is to produce variates from a given multivariate density
(the posterior density in Bayesian applications) by repeatedly sampling a Markov chain whose
invariant distribution is the target density of interest. There are typically many diï¬€erent ways
of constructing a Markov chain with this property and one goal of this paper is to isolate those
that are simulationâ€“eï¬ƒcient in the context of SV models. In our problem, one key issue is that
the likelihood function f(y|Î¸) =
R f(y|h, Î¸)f(h|Î¸)dh is intractable. This precludes the direct
analysis of the posterior density Ï€(Î¸|y) by MCMC methods. This problem can be overcome
by focusing instead on the density Ï€(Î¸, h|y), where h = (h1, ..., hn) is the vector of n latent
volatilities.
Markov chain Monte Carlo procedures can be developed to sample this density
without computation of the likelihood function f(y|Î¸). It should be kept in mind that sample
variates from a MCMC algorithm are a high-dimensional (correlated) sample from the target
density of interest. These draws can be used as the basis for making inferences by appealing
to suitable ergodic theorems for Markov chains. For example, posterior moments and marginal
densities can be estimated (simulation consistently) by averaging the relevant function of interest
over the sampled variates. The posterior mean of Î¸ is simply estimated by the sample mean
of the simulated Î¸ values. These estimates can be made arbitrarily accurate by increasing the
simulation sample size. The accuracy of the resulting estimates (the so called numerical standard
error) can be assessed by standard time series methods that correct for the serial correlation in
the draws. The serial correlation can be quite high for badly behaved algorithms.
An initial Gibbs sampling algorithm for the SV model
For the problem of simulating a multivariate density Ï€(Ïˆ|y), the Gibbs sampler is deï¬ned by a
blocking scheme Ïˆ = (Ïˆ1, ..., Ïˆd) and the associated full conditional distributions Ïˆi|y, Ïˆ\i, where
Ïˆ\i denotes Ïˆ excluding the block Ïˆi. The algorithm proceeds by sampling each block from the
full conditional distributions where the most recent values of the conditioning blocks are used
in the simulation. One cycle of the algorithm is called a sweep or a scan. Under regularity
conditions, as the sampler is repeatedly swept, the draws from the sampler converge to draws
from the target density at a geometric rate. For the SV model the Ïˆ vector becomes (h, Î¸). To
sample Ïˆ from the posterior density, one possibility and Shephard ) is to update each of the elements of the Ïˆ vector one at a time.
1. Initialize h and Î¸.
2. Sample ht from ht|h\t, y, Î¸ , t = 1, ..., n.
3. Sample Ïƒ2
Î·|y, h, Ï†, Âµ, Î².
4. Sample Ï†|h, Âµ, Î², Ïƒ2
5. Sample Âµ|h, Ï†, Ïƒ2
6. Goto 2.
Cycling through 2 to 5 is a complete sweep of this (single move) sampler. The Gibbs sampler
will require us to perform many thousands of sweeps to generate samples from Î¸, h|y.
The most diï¬ƒcult part of this sampler is to eï¬€ectively sample from ht|h\t, yt, Î¸ as this operation has to be carried out n times for each sweep. However,
f(ht|h\t, Î¸, y) âˆf(ht|h\t, Î¸)f(yt|ht, Î¸),
t = 1, ..., n.
We sample this density by developing a simple accept/reject procedure. 2 Let fN(t|a, b) denote
the normal density function with mean a and variance b. It can be shown (ignoring end conditions
to save space) that
f(ht|h\t, Î¸) = f(ht|htâˆ’1, ht+1, Î¸) = fN(ht|hâˆ—
t = Âµ + Ï† {(htâˆ’1 âˆ’Âµ) + (ht+1 âˆ’Âµ)}
(1 + Ï†2) .
Next we note that exp(âˆ’ht) is a convex function and can be bounded by a function linear in ht.
Let log f(yt|ht, Î¸) = const + log f âˆ—(yt, ht, Î¸). Then
log f âˆ—(yt, ht, Î¸)
2 {exp(âˆ’ht)}
2 {exp(âˆ’hâˆ—
t )(1 + hâˆ—
t ) âˆ’ht exp(âˆ’hâˆ—
log gâˆ—(yt, ht, Î¸, hâˆ—
f(ht|h\t, Î¸) f âˆ—(yt, ht, Î¸) â‰¤fN(ht|hâˆ—
t , v2) gâˆ—(yt, ht, Î¸) .
The terms on the right hand side can be combined and shown to be proportional to fN(ht|Âµt, v2)
2Five previous MCMC algorithms for simulating from ht|htâˆ’1, ht+1, yt; Î¸ have been given in the literature by
Shephard , Jacquier, Polson, and Rossi , Shephard and Kim , Geweke and Shephard
and Pitt . The closest to our suggestion is Geweke who also bounded log f âˆ—, but by âˆ’0.5ht. This
suï¬€ers from the property of having a high rejection rate for slightly unusual observations (for example, 0.9 for
|yt|/Î² exp(ht/2) > 3). Shephard and Pitt , on the other hand, used a quadratic expansion of log f âˆ—about
t . This increases the generality of the procedure but it involves a Metropolis rejection step and so is more
involved. Shephard approximated f âˆ—by a normal distribution with the same moments as log Ïµ2
Geweke and Shephard and Kim independently suggested the use of the Gilks and Wild 
procedure for sampling from log concave densities such as log f(ht|h\t, Î¸, y). This is generalizable to non-logconcave densities using the Gilks, Best, and Tan sampler. Typically these routines need about 10 to 12
evaluations of log f(ht|h\t, Î¸, y) to draw a single random variable. Hence they are about 10 times less eï¬ƒcient
than the simple accept/reject algorithm given above.
Jacquier, Polson, and Rossi â€™s Metropolis algorithm uses a very diï¬€erent approach. They approximate
the density of ht|h\t and so use a non-Gaussian proposal based on f âˆ—. Typically this procedure is considerably
slower than the use of the Gilks and Wild methods suggested above.
With these results, the accept-reject procedure ) to sample ht from f(ht|h\t, Î¸, y)
can now be implemented. First, propose a value ht from fN(ht|Âµt, v2). Second, accept this value
with probability f âˆ—/gâˆ—; if rejected return to the ï¬rst step and make a new proposal. 3
Sampling Ïƒ2
Sampling the Ïƒ2
Î· and Ï† one at a time is straightforward. If we assume a
conjugate prior Ïƒ2
Î·|Ï†, Âµ âˆ¼IG(Ïƒr
2 ), then Ïƒ2
Î· is sampled from
Î·|y, h, Ï†, Âµ âˆ¼IG
, SÏƒ + (h1 âˆ’Âµ)2(1 âˆ’Ï†2) + Pnâˆ’1
t=1 ((ht+1 âˆ’Âµ) âˆ’Ï†(ht âˆ’Âµ))2
where IG denotes the inverse-gamma distribution. Throughout we set Ïƒr = 5 and SÏƒ = 0.01Ã—Ïƒr.
For Ï†, sampling from the full conditional density is also easy. Let Ï† = 2Ï†âˆ—âˆ’1 where Ï†âˆ—is
distributed as Beta with parameters (Ï†(1), Ï†(2)). Hence, our prior on Ï† is
Ï†(1)âˆ’1 (1 âˆ’Ï†)
Ï†(1), Ï†(2) > 1
and has support on the interval (âˆ’1, 1) with a prior mean of
Ï†(1) + Ï†(2)
work we will select Ï†(1) = 20 and Ï†(2) = 1.5, implying a prior mean of 0.86. Alternative priors
could also be used. For example, the ï¬‚at prior Ï€(Ï†) âˆ1 is attractive in that it leads to an
analytically tractable full conditional density. But this prior can cause problems when the data
are close to being non-stationary and Schotman and Van Dijk ). Chib
and Greenberg and Marriott and Smith discuss other priors (restricted to the
stationary region) for autoregressive models. We feel that it is important from a data-analytic
view to impose stationarity in the SV model. Further, if Ï† = 1 then the Âµ terms cancel in (1)
and so Âµ becomes unidentiï¬ed from the data. The prior we select avoids these two problems
rather well.
Under the speciï¬ed prior, the full conditional density of Ï† is proportional to
Ï€(Ï†)f(h|Âµ, Ï†, Ïƒ2
log f(h|Âµ, Ï†, Ïƒ2
Î·) âˆâˆ’(h1 âˆ’Âµ)2  1 âˆ’Ï†2
{(ht+1 âˆ’Âµ) âˆ’Ï† (ht âˆ’Âµ)}2
This function is concave in Ï† for all values of Ï†(1), Ï†(2). This means that Ï† can be sampled using
an acceptance algorithm. Employ a ï¬rst order Taylor expansion of the prior about
(ht+1 âˆ’Âµ) (ht âˆ’Âµ) /
(ht âˆ’Âµ)2 ,
3This proposal has an average acceptance rate of approximately 1âˆ’y2
. A typical situation
is where v2
t = 0.01. Usually y2
t /Î²2 will not be very large as hâˆ—
t is the smoothed log-volatility of yt
and so reï¬‚ects the variation in yt. An extreme case is where y2
t /Î²2 = 100, which leads to an average
acceptance rate of approximately 0.75 . In our experience an average acceptance rate of over 0.995 seems usual
for real ï¬nancial datasets.
and combine with f(h|Âµ, Ï†, Ïƒ2). The resulting density provides a good suggestion density. Alternatively, one can specialize the method of Chib and Greenberg (which is based on the
Metropolis-Hastings algorithm). Given the current value Ï†(iâˆ’1) at the (iâˆ’1)-st iteration, sample
a proposal value Ï†âˆ—from N(Ë†Ï†, VÏ†) where VÏ† = Ïƒ2
t=1 (ht âˆ’Âµ)2oâˆ’1. Then, provided Ï†âˆ—is in
the stationary region, accept this proposal value as Ï†(i) with probability exp{g(Ï†âˆ—) âˆ’g(Ï†(iâˆ’1))}
g(Ï†) = log Ï€(Ï†) âˆ’(h1 âˆ’Âµ)2  1 âˆ’Ï†2
If the proposal value is rejected, set Ï†(i) to equal Ï†(iâˆ’1) . Both these approaches can be used
with alternative priors on Ï†.
Sampling Âµ
Suppose we work with a diï¬€use prior4 on Âµ, then Âµ is sampled from the full
conditional density
Âµ|h, Ï†, Ïƒ2
h1 + (1 âˆ’Ï†)
(ht+1 âˆ’Ï†ht)
(n âˆ’1) (1 âˆ’Ï†)2 +
1 âˆ’Ï†2oâˆ’1 .
In our work we sample Âµ and record the value Î² = exp(Âµ/2).
Illustration
To illustrate this algorithm we analyze the daily observations of weekday close
exchange rates for the UK Sterling/US Dollar exchange rate from 1/10/81 to 28/6/85. The
sample size is n = 946. Later in the paper we will also use the corresponding series for the
German Deutschemark (DM), Japanese Yen and Swiss Franc (SwizF), all against the US Dollar.
This data set has been previously analysed using quasi-likelihood methods in Harvey, Ruiz, and
Shephard . The mean-corrected returns will be computed as
yt = 100 Ã—
(log rt âˆ’log rtâˆ’1) âˆ’1
(log ri âˆ’log riâˆ’1)
where rt denotes the exchange rate at time t. The MCMC sampler was initialized by setting all
the ht = 0 and Ï† = 0.95 , Ïƒ2
Î· = 0.02 and Âµ = 0. We iterated the algorithm on the log-volatilities
for 1, 000 iterations and then the parameters and log-volatilities for 50, 000 more iterations, before
recording the draws from a subsequent 1, 000, 000 sweeps. The burn-in period is thus much larger
than what is customary in the literature and is intended to ensure that the eï¬€ect of the starting
4Occassionally, for technical reasons, we take a slightly informative prior such as Âµ âˆ¼N(0, 10). In this paper,
this prior was used for the computation of Bayes factors.
250000 500000 750000
(a) phi|y against iteration
250000 500000 750000
(b) sigma_eta|y against iteration
250000 500000 750000
(c) beta|y against iteration
(d) Histogram of phi|y
(e) Histogram of sigma_eta|y
(f) Histogram of beta|y
(g) Correlogram for phi|y
(h) Correlogram for sigma_eta|y
(i) Correlogram for beta|y
Figure 2: Single move Gibbs sampler for the Sterling series. Graphs (a)-(c): simulations against
iteration. Graphs (d)-(f): histograms of marginal distribution. Graphs (g)-(i): corresponding
correlograms for simulation. In total 1,000,000 iterations were drawn, discarding the ï¬rst 50,000.
values becomes insigniï¬cant. As a result, there is likely to be no additional information from
running multiple chains from dispersed starting values. The complete 1, 000, 000 iterations5 are
graphed in Figure 2 and summarized in Table 1. 6
The summary statistics of Table 1 report the simulation ineï¬ƒciency factors of the sampler.
These are estimated as the variance of the sample mean from the MCMC sampling scheme
(the square of the numerical standard error) divided by the variance of the sample mean from a
hypothetical sampler which draws independent random variables from the posterior (the variance
divided by the number of iterations). We think that the simulation ineï¬ƒciency statistic is a
useful diagnostic (but by no means the only one) for measuring how well the chain mixes. The
numerical standard error of the sample mean is estimated by time series methods uniform random number as the
basis of all our random numbers. This has a period of 231 âˆ’1, which allows us to draw around 2.1 billion random
numbers. In these experiments we are drawing approximately n Ã— 2 Ã— 1.05 random numbers per sweep of the
sampler, where 5% is a very conservative estimate of the overall rejection rate. For this dataset this is 1984 draws
per sweep. Given that we employ 1, 000, 000 sweeps, we are close, but not beyond, the period of our random
number generator.
6Timings will be given for all the computations given in this paper. These are made using the authors C++
code which has been linked to Ox. The single move algorithm is optimised to this special case and so is about as
fast as it is possible to make it. The latter algorithms are much more general and so it is not completely fair to
compare the computed time reported here to their times.
for the serial correlation in the draws) as
bRBM = 1 +
where bÏ(i) is an estimate of the autocorrelation at lag i of the MCMC sampler, BM represents
the bandwidth and K the Parzen kernel ) given by
1 âˆ’6z2 + 6z3,
elsewhere.
The correlogram (autocorrelation function) indicates important autocorrelations for Ï† and
ÏƒÎ· at large lag lengths.
If we require the Monte Carlo error in estimating the mean of the
posterior to be no more than one percentage of the variation of the error due to the data, then
this Gibbs sampler would have to be run for around 40, 000 iterations. This seems a reasonably
typical result: see Table 2.
Ineï¬ƒciency
Covariance & Correlation
0.00013754
0.00011062
0.00063273
-0.00022570
0.00098303
0.00036464
0.00021196
-0.00040183
Table 1: Daily returns for Sterling: summaries of Figure 2. The Monte Carlo S.E. of simulation
is computed using a bandwidth of 2,000, 4,000 and 2,000 respectively. Italics are correlations
rather than covariances of the posterior. Computer time is seconds on a Pentium Pro/200. The
other time is the number of seconds to perform 100 sweeps of the sampler.
Ineï¬ƒciency
Ineï¬ƒciency
Ineï¬ƒciency
Table 2: Bandwidth was 2,000, 4,000 and 2,000, respectively for the parameters, for all series.
In all cases 1,000,000 sweeps were used.
Parameterization
An alternative to this sampler is to replace the draw for Âµ|h, Ï†, Ïƒ2
that resulting from the alternative parameterisation Î²|y, h. Such a move would be a mistake.
Table 3 reports the ineï¬ƒciency factor for this sampler using 1,000,000 draws of this sampler.
There is a small deterioration in the sampler for Ï†|y and a very signiï¬cant reduction in eï¬ƒciency
for Î²|y. The theoretical explanation for the inadequacies of the Î² parameterization is provided
by Pitt and Shephard .
Ineï¬ƒciency
Ineï¬ƒciency
Ineï¬ƒciency
Table 3: Bandwidth was 4,000, 4,000 and 15,000, respectively for the parameters. 1,000,000
sweeps were used.
Reason for slow convergence
The intuition for the slow convergence reported in Table 1 is
that the components of h|y, Î¸ are highly correlated and in such cases sampling each component
from the full conditional distribution produces little movement in the draws, and hence slowly
decaying autocorrelations ). For analytical results, one can think
of the Gaussian equivalent of this problem.
Under the Gaussian assumption and the linear
approximation (2) and (1), the sampler in the simulation of h from h|y, Î¸ has an analytic
convergence rate of )
1 + Ï†2 + Ïƒ2
Î·/Var(log Îµ2
where Î¸ is taken as ï¬xed at the expected values given in the results for the Sterling series. If
Var(log Îµ2
t ) is set equal to 4.93, then this result implies a geometric convergence rate of ÏA =
0.9943 and an ineï¬ƒciency factor of (1 + ÏA) / (1 âˆ’ÏA) = 350 which is in the range reported in
In order to improve the above sampler it is necessary to try to sample the log-volatilities in a
diï¬€erent way. One method is to sample groups of consecutive log volatilities using a Metropolis
algorithm. This is investigated in Shephard and Pitt . In this paper we detail a more
ambitious model speciï¬c approach. This approach is described next.
OFFSET MIXTURE METHOD
In this section we design an oï¬€set mixture of normals distribution (deï¬ned below) to accurately
approximate the exact likelihood. This approximation helps in the production of an eï¬ƒcient
(adapted Gibbs sampler) Monte Carlo procedure that allows us to sample all the log-volatilities
We then show how one can make the analysis exact by correcting for the (minor)
approximation error by reweighting the posterior output.
Our approximating parametric model for the linear approximation (2) will be an oï¬€set mixture
time series model
t = ht + zt ,
t = log(y2
t + c) and
qifN in order
to robustify the QML estimator of the SV model to y2
t being very small. Throughout we will set
c = 0.001 (although it is possible to let c depend on the actual value taken by y2
t ). It should be
noted that the mixture density can also be written in terms of a component indicator variable
st such that
i âˆ¼N(mi âˆ’1.2704, v2
This representation will be used below in the MCMC formulation.
We are now in a position to select K and
mi, qi, v2
(i â‰¤K) to make the mixture approximation â€œsuï¬ƒciently goodâ€. In our work, following for instance Titterington, Smith, and Makov
 , we matched the ï¬rst four moments of fexp(Z)(r) (the implied log-normal distribution) and f(zt) to those of a Ï‡2
1 and log Ï‡2
1 random variable respectively, and required that
the approximating densities lie within a small distance of the true density. This was carried out
by using a non-linear least squares program to move the weights, means and variances around
until the answers were satisfactory. It is worth noting that this nonlinear optimisation incurs
only a one-time cost, as there are no model-dependent parameters involved. We found what we
judged to be satisfactory answers by setting K = 7 . The implied weights, means and variances
are given in Table 4, while the approximating and the true density are drawn in Figure 3. It
would be easy to improve the ï¬t by increasing the value of K, however further experiments that
we have conducted suggest that increasing K has little discernible eï¬€ect on our main results.
Mixture simulator
In the MCMC context, mixture models are best estimated by exploiting the representation in
(10). The general algorithm for state space models was suggested independently by Shephard
Table 4: Selection of the Mixing Distribution to be log Ï‡2
 and Carter and Kohn .
The posterior density of interest is Ï€(s, h, Ï†, Ïƒ2
where s = (s1, ..., sn). In this case, both h and s can be sampled separately in one block and
the sampler takes the form
1. Initialize s, Ï†, Ïƒ2
2. Sample h from h|yâˆ—, s, Ï†, Ïƒ2
3. Sample s from s|yâˆ—, h.
4. Update Ï†, Ïƒ2
Î·, Âµ according to (6 ), (4) and (7).
5. Goto 2.
Note that we are using yâˆ—=
1 + c), ..., log(y2
in the conditioning set above as a
pointer to the mixture model. The vectors yâˆ—and y, of course, contain the same information.
The important improvement over the methods in section 2 is that it is now possible to
eï¬ƒciently sample from the highly multivariate Gaussian distribution h|yâˆ—, s, Ï†, ÏƒÎ·, Âµ because
yâˆ—|s, Ï†, ÏƒÎ·, Âµ is a Gaussian time series which can be placed into the state-space form associated
with the Kalman ï¬lter. The time series literature calls such models partially non-Gaussian or
conditionally Gaussian. This particular model structure means we can sample from the entire
h|yâˆ—, s, Ï†, ÏƒÎ·, Âµ using the Gaussian simulation signal smoother detailed in the Appendix. As
for the sampling of s from s|yâˆ—, h, this is done by independently sampling each st using the
probability mass function
Pr(st = i|yâˆ—
t , ht) âˆqifN(yâˆ—
t |ht + mi âˆ’1.2704, v2
i ) , i â‰¤K .
The results from 750,000 sweeps of this mixture sampler are given in Table 5 and Figure
4. This sampler has less correlation than the single move sampler and suggests that generating
20,000 simulations from this sampler would probably be suï¬ƒcient for inferential purposes.
Ratio of densities
Figure 3: Mixture approximation to Ï‡2
1 density. Left: Ï‡2
1 density and mixture approximation.
Right: the log of the ratio of the Ï‡2
1 density to the mixture approximation.
Integrating out the log-volatilities
Although this mixture sampler improves the correlation behaviour of the simulations, the gain
is not very big as there is a great deal of correlation between the volatilities and parameters.
However, we can use the Gaussian structure of yâˆ—|s, Ï†, Ïƒ2
Î· to overcome this. We can sample the
joint distribution Ï€(Ï†, Ïƒ2
Î·, h, Âµ|yâˆ—, s) by sampling (Ï†, Ïƒ2
n) from Ï€(Ï†, Ïƒ2
Î·|yâˆ—, s) âˆf(yâˆ—|s, Ï†, Ïƒ2
and then sampling (h, Âµ) from Ï€(h, Âµ|yâˆ—, s, Ï†, Ïƒ2
Î·). We are able to sample the former distribution
because the density f(yâˆ—|s, Ï†Ïƒ2
Î·) can be evaluated using an augmented version of the Kalman
Ineï¬ƒciency
Covariance & Correlation
6.6811e-005
0.00011093
0.00046128
-0.00023141
0.00024217
0.00021441
-0.00040659
Table 5: Daily returns for Sterling against Dollar. Summaries of Figure 2. The Monte Carlo
S.E. of simulation is computed using a bandwidth of 2000, 2000 and 100 respectively. Italics are
correlations rather than covariances of the posterior. Computer time is seconds on a Pentium
Pro/200. The other time is the number of seconds to perform 100 complete passes of the sampler.
(a) phi|y against iteration
(b) sigma_eta|y against iteration
(c) beta|y against iteration
(d) Histogram of phi|y
(e) Histogram of sigma_eta|y
(f) Histogram of beta|y
(g) Correlogram for phi|y
(h) Correlogram for sigma_eta|y
(i) Correlogram for beta|y
Figure 4: Mixture sampler for Sterling series. Graphs (a)-(c): simulations against iteration.
Graphs (d)-(f): histograms of marginal distribution. Graphs (g)-(i): corresponding correlograms
for simulation. In total 750,000 iterations were drawn, discarding the ï¬rst 10,000.
ï¬lter (analytically integrating out Âµ and h).7 Then, writing Âµ|yâˆ—, s, Ï†, Ïƒ2
Î· âˆ¼N(eÂµ, Ïƒ2
eÂµ) we have
Î·)f(yâˆ—|s, Ï†, Ïƒ2
Î·) = Ï€(Ï†)Ï€(Ïƒ2
Î·)f(yâˆ—|s, Ï†, Ïƒ2
Î·, Âµ = 0)Ï€(Âµ = 0)
Ï€(Âµ = 0|yâˆ—, s, Ï†, Ïƒ2Î·)
where vt is the one-step-ahead prediction error for the best mean square estimator of yâˆ—
Ft is the corresponding mean square error. The quantities vt, Ft, eÂµ, Ïƒ2
eÂµ are computed from the
augmented Kalman ï¬lter provided in the Appendix, conditional on s.
This implies that we can sample from Ï†, Ïƒ2
Î·|yâˆ—, s directly by making the proposal
Ï†(i), Ïƒ2(i)
given the current value
Ï†(iâˆ’1), Ïƒ2(iâˆ’1)
, by drawing from some density g(Ï†, Ïƒ2
Î·) and then accepting them using the Metropolis-Hastings probability of move
Ï€(Ï†(i), Ïƒ2(i)
Ï€(Ï†(iâˆ’1), Ïƒ2(iâˆ’1)
g(Ï†(iâˆ’1), Ïƒ2(iâˆ’1)
g(Ï†(i), Ïƒ2(i)
If the proposal value is rejected, we then set
Ï†(i), Ïƒ2(i)
Ï†(iâˆ’1), Ïƒ2(iâˆ’1)
. We call this an
â€˜integration samplerâ€™ as it integrates out the log-volatilities.
7Augmented Kalman ï¬lters and simulation smoothers are discussed in the Appendix.
The structure of the integration sampler is then generically:
1. Initialize (s, Ï†, ÏƒÎ·, Âµ).
2. Sample (Ï†, Ïƒ2
Î·) from Ï€(Ï†, Ïƒ2
Î·|yâˆ—, s) using a Metropolis-Hastings suggestion based on g(Ïƒ2
accepting with probability (11).
3. Sample h, Âµ|yâˆ—, s, Ï†, Ïƒ2
Î· using the augmented simulation smoother given in the Appendix.
4. Sample s|yâˆ—, h as in the previous algorithm.
5. Goto 2.
An important characteristic of this sampler is that the simulation smoother can jointly draw
h and Âµ. The scheme allows a free choice of the proposal density g(Ï†, Ïƒ2
Î·). We have employed
a composite method which ï¬rst draws 200 samples (discarding the ï¬rst ten samples) from the
posterior density Ï€(Ï†, Ïƒ2
Î·|y) using a Metropolis-Hastings sampler based on Gilks, Best, and Tan
 which only requires the coding of the function yâˆ—|s, Ï†, Ïƒ2
Î· and the prior. These 200 draws
are used to estimate the posterior mean and covariance. The mean and twice the covariance are
then used to form a Gaussian proposal density g(Ï†, Ïƒ2
Î·) for the Metropolis-Hastings algorithm in
(11). As an alternative, one could also use a multivariate Student t proposal distribution instead
of the Gaussian. See Chib and Greenberg for further discussion on the issues involved in
choosing a proposal density for the Metropolis-Hastings algorithm.
The output from the resulting sampler is reported in Figure 5 and Table 6. These suggest
that 2,000 samples from this generator would be suï¬ƒcient for this problem. This result seems
reasonably robust to the data set.
Ineï¬ƒciency
Covariance & Correlation
6.7031e-005
0.00011297
0.00025965
-0.00023990
0.00023753
0.00021840
-0.00042465
Table 6: Daily returns for Sterling against Dollar. Summaries of Figure 5. The Monte Carlo
S.E. of simulation is computed using a bandwidth of 100, 100 and 100 respectively. Italics are
correlations rather than covariances of the posterior. Computer time is seconds on a Pentium
Pro/200. The other time is the number of seconds to perform 100 complete passes of the sampler.
Reweighting
The approach based on our (very accurate) oï¬€set mixture approximation provides a neat connection to conditionally Gaussian state space models and leads to elegant and eï¬ƒcient sampling
(a) phi|y against iteration
(b) sigma_eta|y against iteration
(c) beta|y against iteration
(d) Histogram of phi|y
(e) histogram of sigma_eta|y
(f) Histogram of beta|y
(g) Correlogram for phi|y
(h) Correlogram for sigma_eta|y
(i) Correlogram for beta|y
Figure 5: The integration sampler for Sterling series.
Graphs (a)-(c): simulations against
iteration. Graphs (d)-(f): histograms of marginal distribution. Graphs (g)-(i): corresponding
correlograms for simulation. In total 250,000 iterations were drawn, discarding the ï¬rst 250.
procedures, as shown above. We now show that it is possible to correct for the minor approximation error by appending a straightforward reweighting step at the conclusion of the above
procedures. This step then provides a sample from the exact posterior density of the parameters
and volatilities. The principle we describe is quite general and may be used in other simulation
problems as well.
First write the mixture approximation as making draws from k(Î¸, h|yâˆ—), and then deï¬ne
w(Î¸, h) = log f(Î¸, h|y) âˆ’log k(Î¸, h|y) = const + log f(y|h) âˆ’log k(yâˆ—|h),
fN{yt|0, exp(ht)}
t |ht + mi âˆ’1.2704, v2
Both these functions involve Gaussian densities and are straightforward to evaluate for any value
of h. Then,
R g(Î¸)f(Î¸|y)dÎ¸
R g(Î¸) exp {w(Î¸, h)} k(Î¸, h|yâˆ—)dÎ¸dh/
R exp {w(Î¸, h)} k(Î¸, h|yâˆ—)dÎ¸dh.
Thus we can estimate functionals of the posterior by reweighting the MCMC draws according
where the weights are
As the mixture approximation is very good, we would expect that the weights cj would have a
small variance.
To see the dispersion of the weights, we recorded the weights from the sampler which generated Figure 5 and plotted the resulting log-weights in Figure 6. The log-weights are close to
being normally distributed with a standard deviation of around one.
log-weights
Normal approx
Figure 6: Histogram of the log of the M Ã— cj for 250,000 sweeps for the integration sampler and
a corresponding approximating normal density with ï¬tted mean and standard deviation. All the
weights around zero would indicate a perfect sampler.
To see the eï¬€ect of the weights on the parameters estimates, we reweighted the 250,000
samples displayed in Figure 5. This produced the estimates which are given in Table 7. These
Monte Carlo estimates of the posterior means are statistically insigniï¬cantly diï¬€erent from Monte
Carlo estimated values given in Table 1. However, the Monte Carlo precision has improved
dramatically. Further, the Monte Carlo standard errors indicate that this data set could be
routinely analysed using around 1,500 sweeps.
Ineï¬ƒciency
Covariance & Correlation
7.0324e-005
0.00010973
0.00024573
-0.00022232
0.00096037
0.00025713
0.00021181
-0.00039768
Table 7: Daily returns for Sterling against Dollar. Summaries of reweighted sample of 250,000
sweeps of the integration sampler. The Monte Carlo S.E. of simulation is computed using a
block one tenth of the size of the simulation. Italics are correlations rather than covariances of
the posterior. Computer time is seconds on a Pentium Pro/200. The other time is the number
of seconds to perform 100 complete passes of the sampler.
This conclusion seems to hold up for some other exchange rate series. Table 8 reports the
estimates of the parameters and simulation ineï¬ƒciency measures for the DM, Yen and Swiss
Franc series. This table is the exact analog of Table 2 for the single move algorithm.
Ineï¬ƒciency
Ineï¬ƒciency
Ineï¬ƒciency
Table 8: Bandwidth for each parameter was 100 on all series. In all cases 250,000 sweeps were
FILTERING, DIAGNOSTICS AND LIKELIHOOD EVALU-
Introduction
There has been considerable recent work on the development of simulation based methods to
perform ï¬ltering, that is computing features of ht|Yt, Î¸, for each value of Yt = (y1, ..., yt). Leading
papers in this ï¬eld include Gordon, Salmond, and Smith , Kitagawa , Isard and
Blake , Berzuini, Best, Gilks, and Larizza , West and Muller . We
work with a simple approach which is a special case of a suggestion made by Pitt and Shephard
 . Throughout we will assume Î¸ is known. In practice Î¸ will be set to some estimated
value, such as the maximum likelihood estimator or the Monte Carlo estimator of the posterior
The objective is to obtain a sample of draws from ht|Yt, Î¸ given a sample of draws h1
tâˆ’1, ..., hM
from htâˆ’1|Ytâˆ’1, Î¸. Such an algorithm is called a particle ï¬lter in the literature. We now show
how this may be done. From Bayes theorem,
f(ht|Yt, Î¸) âˆf(yt|ht, Î¸)f(ht|Ytâˆ’1, Î¸)
f(ht|Ytâˆ’1, Î¸) =
f(ht|htâˆ’1, Î¸)f(htâˆ’1|Ytâˆ’1, Î¸)dhtâˆ’1
and f(ht|htâˆ’1, Î¸) = fN(ht|Âµ+Ï†(htâˆ’1âˆ’Âµ), Ïƒ2
Î·) is the normal evolution density. The latter integral
can be estimated from the sample h1
tâˆ’1, ..., hM
tâˆ’1 leading to the approximations
f(ht|Ytâˆ’1, Î¸) â‰ƒ1
f(ht|Yt, Î¸)
.âˆf(yt|ht, Î¸) 1
The question now is to sample ht from the latter density. The obvious importance sampling
procedure of producing a sample {hj
t} from f(ht|hj
tâˆ’1, Î¸) and then resampling these draws with
weights proportional to {f(yt|hj
t, Î¸)} is not eï¬ƒcient. An improved procedure runs as follows.
Let ht|tâˆ’1 = Âµ + Ï†(Mâˆ’1 P hj
tâˆ’1 âˆ’Âµ) and log f(yt|ht, Î¸) = const + log f âˆ—(yt, ht, Î¸). Now expand
log f âˆ—(yt, ht, Î¸) in a Taylor series around the point ht|tâˆ’1as
log f âˆ—(yt, ht, Î¸)
2 {exp(âˆ’ht)}
exp(âˆ’ht|tâˆ’1)(1 + ht|tâˆ’1) âˆ’ht exp(âˆ’ht|tâˆ’1)
log gâˆ—(ht, ht|tâˆ’1, Î¸) .
Also, after some algebra it can be shown that
gâˆ—(ht, ht|tâˆ’1, Î¸)f(ht|hj
tâˆ’1, Î¸) âˆÏ€jfN(ht|hj
tâˆ’1 âˆ’Âµ) + Ïƒ2
t exp(âˆ’ht|tâˆ’1) âˆ’1
Hence, the kernel of the target density in (14) can be bounded as
f âˆ—(yt, ht, Î¸) 1
tâˆ’1, Î¸) â‰¤gâˆ—(ht, ht|tâˆ’1, Î¸) 1
where the right hand side terms are proportional to
j=1 Ï€jfN(ht|hj
Î·) due to (15).
These results suggest a simple accept-reject procedure for drawing ht.
First, we draw a
proposal value ht from the mixture density
j fN(ht|hj
Î·), where Ï€âˆ—
Second, we accept this value with probability f âˆ—(yt, ht, Î¸)/gâˆ—(ht, ht|tâˆ’1, Î¸) . If the value is rejected, we return to the ï¬rst step and draw a new proposal.
By selecting a large M this ï¬ltering sampler will become arbitrarily accurate.
Application
To illustrate this, we apply these methods to the Sterling/Dollar series, ï¬ltering the volatility.
Throughout we will employ M = 2, 500. Similar results were obtained when M fell to 1, 000,
although reducing M below that ï¬gure created important biases. The results are made conditional of the estimated parameters, which are taken from Table 9 and based on 2, 500 sweeps of
the integration sampler.
Ineï¬ƒciency
Covariance & Correlation
0.00014783
-0.00033148
0.00030503
-0.00074971
Table 9: Daily returns for Sterling series. Summaries of reweighted sample of 2,500 sweeps of
the integration sampler. The Monte Carlo S.E. of simulation is computed using a block one tenth
of the size of the simulation. Italics are correlations rather than covariances of the posterior.
Computer time is seconds on a Pentium Pro/200. The other time is the number of seconds to
perform 100 complete passes of the sampler.
The resulting ï¬ltered and smoothed estimates of the volatility are given in Figure 7, together
with a graph of the absolute values of the returns. The graph shows the expected feature of the
ï¬ltered volatility lagging the smoothed volatility. Throughout the sample, the ï¬ltered volatility
is slightly higher than the smoothed values due to the gradual fall in volatility observed for these
series during this period.
Diagnostics
Having designed a ï¬ltering algorithm it is a simple matter to sample from the one-step-ahead
prediction density and distribution function. By deï¬nition the prediction density is
f(yt+1|Yt, Î¸) =
f(yt+1|Yt, ht+1, Î¸) f(ht+1|Yt, ht, Î¸) f(ht|Yt, Î¸) dht+1dht
Filtered and smoothed volatility
Figure 7: Top: ï¬ltered and smoothed estimate of the volatility exp(ht/2), computed using M =
2000. Bottom: |yt|, the absolute values of the returns.
which can be sampled by the method of composition as follows. For each value hj
t (j = 1, 2, ..., M)
from the ï¬ltering algorithm, one samples hj
Based on these M draws on ht+1 from the prediction density, we can estimate the probability
t+1 will be less than the observed yo2
t+1|Yt, Î¸) = uM
For each t = 1, . . . , n, under the null of a correctly speciï¬ed model uM
converges in distribution
to independent and identically distributed uniform random variables as M â†’âˆ ). This provides a valid basis for diagnostic checking. These variables can be mapped into
the normal distribution, by using the inverse of the normal distribution function nM
to give a standard sequence of independent and identically distributed normal variables, which
are then transformed one-step-ahead forecasts normed by their correct standard errors. These
can be used to carry out Box-Ljung, normality, and heteroscedasticity tests, among others.
The computed forecast uniforms and resulting correlograms and QQ plots are given in Figure
8. The results suggest the model performs quite well, although it reveals some outliers. However,
(a) Correlogram of y_t^2
(b) Normalized innovations
(c) Correlogram of normalized innovations
(d) QQ plot of normalized innovations
Figure 8: Diagnostic checks. Graph (a): correlogram of y2
t . Graph (b): normalised innovations.
Graph (c): the corresponding correlogram. Graph (d): associated QQ-plot.
closer inspection shows that the outliers correspond to small values of y2
t . This suggests that
the SV model fails to accommodate some of the data values that have limited daily movements.
On the other hand it appears to perform well when the movements in the data are large. This
will be made more formal in the next sub-section.
Likelihood estimation
The one-step-ahead predictions can also be used to estimate the likelihood function since the
one-step-ahead prediction density, f(yt+1|Yt), can be estimated as:
using drawings from the ï¬ltering simulator. The same argument gives a ï¬ltered estimate of ht+1
using the information up to time t.
Table 10 shows the results from some standard diagnostic checks on the nM
1 , ..., nM
n produced
by the ï¬tted model. Under the correctness of the model, the diagnostics should indicate that
the variables are Gaussian white noise. We report the skewness and kurtosis coeï¬ƒcients,
Skew = nb3
Kurtosis = n (b4 âˆ’3)2
where b3 and b4 denote the standardized estimators of the third and fourth moment of
about the mean, an overall Bowman and Shenton normality statistic which combines
these two measures and the Box-Ljung statistic using 30 lags. The Table also gives the simulation standard error for these statistics, based on repeating the simulation ten times with
diï¬€erent random draws but with the data ï¬xed. Finally, for comparison the Table gives the
same diagnostics for the N(0, Ïƒ2) and scaled Student t iid models. The results suggest that
there are no straightforward failures in the way the model has been ï¬tted.
Table 10: Diagnostics of the SV model using M = 2, 500. BL(l) denotes a Box-Ljung statistic
on l lags. The ï¬gures in brackets are simulation standard errors using 10 replications. The
two other models are ï¬tted using ML. The estimated degrees of the Student t model is given in
COMPARISON OF NON-NESTED MODELS VIA SIMU-
GARCH model
In this section we compare the ï¬t of basic SV models with the GARCH models commonly used
in the literature. Two approaches are used in this non-nested model comparison â€” one based
on likelihood ratios and another based on ratios of marginal likelihoods resulting in what are
called Bayes factors.
The notation we use for the Gaussian GARCH(1,1) model is:
yt|Ytâˆ’1 âˆ¼N(0, Ïƒ2
t ), where Ïƒ2
t = Î±0 + Î±1y2
tâˆ’1 + Î±2Ïƒ2
while the equivalent Student - t model introduced by Bollerslev is denoted as t-GARCH
with Î½ as the notation for the positive degrees of freedom.
The diagnostic statistics given in Table 11 suggest that the Gaussian GARCH model does not
ï¬t the data very well, suï¬€ering from positive skewness and excess kurtosis. This suggests that
the model cannot accommodate the extreme positive observations in the data. The t-GARCH
model is better, with much better distributional behaviour.
Again its diagnostics for serial
dependence are satisfactory. The ï¬tted likelihood is very slightly better than the SV model,
although it has one more parameter.
t-GARCH (8.44)
Table 11: Diagnostics of the ML estimators of the Gaussian and Student t distributed GARCH
models. BL(l) denotes a Box-Ljung statistic on l lags. Above the line are the answers of the real
data, the ones below are the corrected observations. Figures in brackets for the t-GARCH model
are the estimated degrees of freedom.
Likelihood ratio statistics
There is an extensive literature on the statistical comparison of non-nested models based on
likelihood ratio statistics. Much of the econometric literature on this topic is reviewed in Gourieroux and Monfort . The approach we suggest here relies on simulation and is based on
Atkinson . Related ideas appear in, for instance, Pesaran and Pesaran and Hinde
Let M1 denote the SV model and M0 the GARCH model. Then, the likelihood ratio test
statistic for comparative ï¬t that is investigated here is given by
log bf(y|M1, Ë†Î¸1) âˆ’log f(y|M0, eÎ¸0)
where log bf(y|M1, Ë†Î¸1) and log f(y|M0, eÎ¸0) denote the respective estimates of the log likelihoods,
the former estimated by simulation as described above 8 , bÎ¸1 is the estimated posterior mean of SV
model parameters and eÎ¸0 the MLE of the GARCH model parameters. The sampling variation of
LRy under the hypothesis that the SV model is true or under the alternative that the GARCH
model is true is approximated by simulation, following Atkinson .
Clearly, analytical
derivations of the sampling distribution are diï¬ƒcult given the unconventional estimators of the
log-likelihood.
Under the assumption that the SV model is true and the true values of its parameters are
1 , we generate simulations yi, i = 1, ..., M from the true model. For each simulated series we
estimate the parameters of the GARCH and SV models and record the value of LRy, which
we denote as LRi
y. The resulting scatter of values LR1
y, ..., LRM
are a sample from the exact
distribution of LRy under the SV null.
The fact that we estimated the likelihood and the
parameters of the SV model for each yi does not alter this result. Hence we could use these
simulations LRi
y as inputs into a trivial Monte Carlo test ) of the hypothesis that the GARCH model is true. Unfortunately Î¸(0)
is unknown and so
8The GARCH process has to be initialized by setting Ïƒ2
0. The choice of this term eï¬€ects the likelihood function.
In our calculations we set Ïƒ2
0 = Î±0/ (1 âˆ’Î±1 âˆ’Î±2) .
it is estimated from the data and chosen to be bÎ¸1. This introduces an additional approximation
error into the sampling calculation which falls as the sample size n â†’âˆ.
The estimated approximate sampling distributions of LRy under each hypothesis based on
99 simulations plus the realization from the data are given in Figure 9. This ï¬gure shows that if
the null of the SV model is true, then LRy can be expected to be positive when the alternative is
a Gaussian GARCH, while it is expected to be around zero when the alternative is a t-GARCH.
For the Sterling series the observed LRy is 19.14 for the SV model against GARCH and
-2.68 for the SV model against t-GARCH. This suggests that the SV model ï¬ts the data better
than the GARCH model but slightly worse than the t-GARCH model (which has one more
parameter). These results are conï¬rmed by looking at the simulated LRy. Table 12 records the
ranking of the observed LRy amongst the 99 simulations conducted under the assumption that
the SV model is true. Hence if the observed LRy is the 96th largest, then it is ranked as being
96th. If the ranking is either close to zero or 100 then this would provide evidence against the
The recorded rankings under the SV hypothesis are not very extreme, with about 20% of
the simulations generating LR tests against the GARCH model which are higher than that
observed, while 30% of the simulations were lower than that observed for the t-GARCH LR
test. Although suggestive, neither of these tests are formally signiï¬cant. This implies that they
are both consistent with the SV model being true.
A more decisive picture is generated when the Gaussian GARCH model is the null hypothesis.
No value is as extreme as the observed LR test against the SV model, rejecting the Gaussian
GARCH model for these data. The evidence of the test against the t-GARCH model is less
In summary, the observed non-nested LRy tests give strong evidence against the use of
Gaussian GARCH models. The two remaining models are the t-GARCH and SV models. The
statistics show a slight preference for the t-GARCH model, but this model is less parsimonious
than the SV model and so it would be fairer to argue for the statement that they ï¬t the data
more or less equally well. These results carry over to the other three exchange rates. The results
from the non-nested tests are given in Table 12, although there is a considerable evidence that
the t-GARCH model is preferable to the SV model for the Yen series.
Bayes factors
An alternative to likelihood ratio statistics is the use of Bayes factors, which are symmetric in
the models and extremely easy to interpret. The approach adopted here for the computation of
Bayes factors relies on the method developed by Chib . From the basic marginal likelihood
(a) null: SV, alternative: Gaussian GARCH
(b) null: SV, alternative: t-GARCH
(c) null: Gaussian GARCH, alternative: SV
(d) null: Gaussian GARCH, alternative: t-GARCH
Figure 9: Non-nested testing. Graphs (a)-(b) LRy computed when SV is true. Graph (a): SV
against a GARCH model. Graph (b): SV against a t-GARCH. The observed values are 19.14
and -2.68 respectively, which are 80th and 29th out of the 100 samples. Graphs (c)-(d): LRy
computed when GARCH model is true. Graph (c): GARCH against SV. Graph (d): GARCH
against t-GARCH. The observed values are 19.14 and -2.68 respectively, which ranks them 100th
and 79th out of the 100 samples.
identity in Chib , the log of the Bayes factor can be written as
log f(y|M1) âˆ’log f(y|M0)
{log f(y|M1, Î¸âˆ—
1) + log f(Î¸âˆ—
1|M1) âˆ’log f(Î¸âˆ—
1|M1, y), }
âˆ’{log f(y|M0, Î¸âˆ—
0) + log f(Î¸âˆ—
0) âˆ’log f(Î¸âˆ—
for any values of Î¸âˆ—
1. Here f(Î¸âˆ—
0) is the GARCH prior density, while f(Î¸âˆ—
1|M1) is the prior
for the SV parameters. The likelihood for the GARCH model is known, while that of the SV
model is estimated via simulation as described above. Next, the posterior densities f(Î¸âˆ—
1|M1, y) are estimated at the single points Î¸âˆ—
1 using a Gaussian kernel applied to
the posterior sample of the parameters. We follow the suggestion in Chib and use the
posterior means of the parameters as Î¸âˆ—
1 since the choice of these points is arbitrary.
To perform a Bayes estimation of the GARCH model we have to write down some priors for
the GARCH parameters. This is most easily done by representing the model in its ARMA(1,1)
form for squared data:
t = Î±0 + (Î±1 + Î±2) y2
tâˆ’1 + vt âˆ’Î±2vtâˆ’1,
SV verses GARCH
SV against t-GARCH
rank GARCH
rank GARCH
Table 12: Non-nested LR tests of the SV model against the ARCH models. In each case the 99
simulations were added to the observed LRy to form the histograms. The reported r-th rankings
are the r-th largest of the observed LR test out of the 100 LRy tests conducted under SV or
GARCH model.
Hence Î±1 + Î±2 is the persistence parameter, Î±2 (which has to be positive) is the negative of
the moving average coeï¬ƒcient, while Î±0/ (1 âˆ’Î±1 âˆ’Î±2) is the unconditional expected value of
t . We will place the same prior on Î±1 + Î±2 as was placed on the persistence parameter Ï† in
the SV model (see ( 5)). This will force the GARCH process to be covariance stationary. The
prior speciï¬cation is completed by assuming that Î±2/ (Î±1 + Î±2) |Î±1 + Î±2 = rÎ± follows a Beta
distribution with
log f {Î±2/ (Î±1 + Î±2) |Î±1 + Î±2 = rÎ±} = const +
Since we would expect that Î±2/ (Î±1 + Î±2) to be closer to one than zero, we will take Ï†(1) = 45
and let Ï†(2) = 2. This gives a mean of 0.957. The scale parameter Î±0/ (1 âˆ’Î±1 âˆ’Î±2) |Î±1, Î±2 are
given a standard diï¬€use inverse chi-squared prior distribution. Finally, for the t-GARCH model,
v âˆ’2 was given in chi-squared prior with a mean of ten.
log fGARCH
log fGARCH
Table 13: Estimated Bayes factors for SV model against GARCH model and t-GARCH. All the
densities were evaluate at the estimated posterior mean.
In order to carry out the MCMC sampling we used the Gilks, Best, and Tan procedure
which just requires the programming of the priors and the GARCH likelihood.
The results of the calculations are given in Table 13.
They are very much in line with
the likelihood ratio analysis given in Table 12. Again the SV model dominates the Gaussian
GARCH model, while it suï¬€ers in comparison with the t-GARCH model, especially for the
Yen data series. It should be mentioned, however, that these conclusions are in relation to the
simplest possible SV model. The performance of the SV model can be improved by considering
other versions of the model, for example, one that relaxes the Gaussian assumption. We discuss
this and other extensions next.
EXTENSIONS
More complicated dynamics
This paper has suggested three ways of performing Bayesian analysis of the SV model: single
move, oï¬€set mixture and integration sampling.
All three extend to the problem where the
volatility follows a more complicated stochastic process than an AR(1). A useful framework is:
ht = ct + ZtÎ³t,
Î³t+1 = dt + TtÎ³t + Htut,
âˆ¼N(0, I), ct and dt are assumed to be strictly exogenous, and Zt, Tt and Ht are
selected to represent the log-volatility appropriately.
With this framework the log volatility
process can be speciï¬ed to follow an ARMA process.
In the single move Gibbs algorithm, it is tempting to work with the Î³t as
f(Î³t|yt, Î³tâˆ’1, Î³t+1) âˆf(yt|ct + ZtÎ³t)f(Î³t|Î³tâˆ’1)f(Î³t+1|Î³t),
has a simple structure. However, this would suï¬€er from the problems of large MCMC simulation
ineï¬ƒciency documented above especially if Î³t is high dimensional or if the {Î³t} process displayed
considerable memory ). Alternatively, one
could sample ht using
f(ht|h\t, y) âˆf(yt|ht)f(ht|h\t),
as we can evaluate ht|h\t using the de Jong scan sampler. This is uniformly superior to
the algorithms built using (20). Neither of these choices would be competitive, however, with
versions of the multi-move and integration sampler which rely on the state space form and can
thus be trivially extended to cover these models.
More sophisticated dynamics for the volatility could be modeled by exploiting factor type
models. An example of this is
ht = h1t + h2t,
h1t+1 = Ï†1h1t + Î·1t,
h2t+1 = Ï†2h2t + Î·2t,
where Ï†1 > Ï†2 and Î·1t, Î·2t are independent Gaussian white noise processes. Here h1t and h2t
would represent the longer-term and shorter-term ï¬‚uctuations in log-volatility. The introduction
of such components, appropriately parametrized, produce volatility versions of the long memory
models advocated by Cox .
Missing observations
The framework described above can also be extended to handle missing data. Suppose that
the exchange rate r34 at time 34 is missing. Then the returns y34 and y35 would be missing.
We could complete the data by adding in r34 to the list of unknowns in the sampling. Given
r34 we could generate y and then sweep h, Î¸|y. Having carried this out we could update r34
by drawing it given h, Î¸ and y. Iterating this procedure gives a valid MCMC algorithm and so
would eï¬ƒciently estimate Î¸ from the non-missing data.
This argument generalizes to any amount of missing data. Hence this argument also generalizes to the experiment where we think of the SV model (1) holding at a much ï¬ner discretization
than the observed data. Think of the model holding at intervals of 1/d-th of a day, while suppose that the exchange rate rt is available daily. Then we can augment the â€˜missingâ€™ intra-daily
data ert = (rt1, ..., rtdâˆ’1) to the volatilities eht =
 ht1, ..., htdâˆ’1, ht
 and design a simple MCMC
algorithm to sample from
er1, ..., ern, eh1, ..., ehn, Î¸|r0, ..., rn.
This will again allow eï¬ƒcient estimation of Î¸ from the â€˜coarseâ€™ daily data even though the
model is true at the intra-daily level.
This type of argument is reminiscent of the indirect
inference methods which have recently been developed for diï¬€usions by Gourieroux, Monfort,
and Renault and Gallant and Tauchen , however our approach has the advantage
of not depending on the ad hoc choice of an auxiliary model and is automatically fully eï¬ƒcient.
Heavy-tailed SV models
The discrete time SV model can be extended to allow Îµt in ( 1) to be more heavy-tailed than
the normal distribution. This would help in overcoming the comparative lack of ï¬t indicated
by Table 12 for the Yen series. One approach, suggested in Harvey, Ruiz, and Shephard 
amongst others, is to use an ad hoc scaled Student t distribution, so that
and the Î¶t and Ï‡2
t,v are independent of one another. The single move and oï¬€set mixture algorithms immediately carry over to this problem if we design a Gibbs sampler for Ï‡2
1,v, ..., Ï‡2
n,v, h, Î¸|y
1,v, ..., Ï‡2
n,v, h, Î¸, Ï‰|y respectively.
An alternative to this, which can be carried out in the single move algorithm, would be
to directly integrate out the Ï‡2
t,v, which would mean f(yt|ht, Î¸) would be a scaled Student t
distribution.
This has the advantage of reducing the dimension of the resulting simulation.
However, the conditional sampling becomes more diï¬ƒcult.
This is because f(yt|ht, Î¸) is no
longer log-concave in ht and the simple accept/reject algorithm will no longer work. However,
one could adopt the pseudo-dominating accept/reject procedure that is discussed in Tierney
 and Chib and Greenberg . This version of the algorithm incorporates a Metropolis
step in the accept/reject method and does not require a bounding function. The same ideas can
also be extended for multivariate models and models with correlated Îµt, Î·t errors.
Semi-parametric SV
The oï¬€set mixture representation of the SV model naturally leads to a semi-parametric version
of the SV model. Suppose we select the â€œparametersâ€ m1, ..., mK, v2
1, ..., v2
K, q1, ..., qK freely from
the data. Then, this procedure is tantamount to the estimation of the density of the shocks
Îµt. The constraint that V ar(Îµt) = 1 is automatically imposed if Âµ is incorporated into these
mixture weights.
This generic approach to semi-parametric density estimation along with MCMC type algorithms for the updating of the mixture parameters has been suggested by Escobar and West
 and Richardson and Green . Mahieu and Schotman use a simulated EM
approach to estimate a small numbers of mixtures inside an SV model.
Prior sensitivity
The methods developed above can be easily modiï¬ed to assess the consequences of changing
the prior. Instead of rerunning the entire samplers with the alternative prior, one can reweight
the simulation output so that it corresponds to the new prior - in much the same way as the
simulation was reweighted to overcome the bias caused by the oï¬€set mixture. Since the posterior
f(Î¸, h|y) âˆf(y|h, Î¸)f(h|Î¸)f(Î¸) = f(y|h, Î¸)f(h|Î¸)f âˆ—(Î¸) f(Î¸)
where f(Î¸) denotes the new prior and f âˆ—(Î¸) the prior used in the simulations, the reweighting
follows the form of (12) where wj = log f(Î¸j) âˆ’log f âˆ—(Î¸j). This is particularly attractive as the
reweighting is a smooth function of the diï¬€erence between the old prior f âˆ—and the new prior f.
Rerunning the sampler will not have this property.
Multivariate factor SV models
The basis of the N dimensional factor SV model will be
yt = Bft+Îµt,
âˆ¼N âŸ¨0, diag {exp(h1t), ..., exp(hNt), exp(hN+1t), ..., exp(hN+Kt)}âŸ©,
where ft is K dimensional and
(ht+1 âˆ’Âµ) =
(ht âˆ’Âµ) + Î·t,
As it stands the model is highly overparameterized. This basic structure was suggested in the
factor ARCH models analyzed9 by Diebold and Nerlove and reï¬ned by King, Sentana,
and Wadhwani , but replaces the unobserved ARCH process for ft by SV processes. It
was mentioned as a possible multivariate model by Shephard and discussed by Jacquier,
Polson, and Rossi .
Jacquier, Polson, and Rossi discussed using MCMC methods on a simpliï¬ed version10
of this model, by exploiting the conditional independence structure of the model to allow the
repeated use of univariate MCMC methods to analyse the multivariate model. This method
requires the diagonality of Ï†Îµ, Ï†f, Î£ÎµÎ· and Î£fÎ· to be successful. However, their argument can
be generalized in the following way for our oï¬€set mixture approach.
Augment the unknown h, Î¸ with the factors f, for then h|f, y, Î¸ has a very simple structure.
In our case we can transform each fjt using
= hN+jt + zjt,
zjt|sjt = i âˆ¼N so that these procedures can be easily used by
non-experts.
In the case of the single move Gibbs sampler and the diagnostics routines the software
is unfortunately specialized to the SV model with AR(1) log-volatility.
However, the other
procedures for sampling h|y, s, Î¸ and the resampling weights are general.
ACKNOWLEDGEMENTS
This paper is an extensively revised version of a paper with the same title by Sangjoon
Kim and Neil Shephard. That version of the paper did not have the section on the use of the
reweighting which corrects for the mixture approximation, nor the formal non-nested testing
procedures for comparison with GARCH models. Neil Shephard would like to thank the ESRC
for their ï¬nancial support through the project â€˜Estimation via Simulation in Econometricsâ€™ and
some computational help from Michael K. Pitt and Jurgen A. Doornik. All the authors would
like to thank the referees for their comments on the previous version of the paper.
Sangjoon Kimâ€™s work was carried out while he was a Ph.D. student at Department of Economics, Princeton University, under the supervision of John Campbell. Sangjoon Kim would like
to thank Princeton University for their ï¬nancial support. The comments of the participants of
the â€˜Stochastic volatilityâ€™ conference of October 1994, held at HEC (UniversitÂ´e de Montreal), are
gratefully acknowledged. Finally, Neil Shephard would like to thank A.C.Atkinson and D.R.Cox
for various helpful conversations on non-nested likelihood ratio testing.
This appendix contains various algorithms which allow the eï¬ƒcient computations of some of the
quantities required in the paper.
Basic Gaussian state space results
We discuss general ï¬ltering and simulation smoothing results which are useful for a general
Gaussian state space model. We analyse the multivariate model:
ct + ZtÎ³t + Gtut,
dt + TtÎ³t + Htut,
N(a1|0, P1|0),
For simplicity we assume that GtHâ€²
t = 0 and we write the non-zero rows of Ht as Mt, GtGâ€²
t = Î£Î·t. Throughout ct and dt are assumed known.
In the context of our paper we have mostly worked with the simplest of models were, putting
Î² = 1 and writing rd
t to denote daily returns computed as (8),
+ const) = ht + Îµt,
ht+1 = Âµ(1 âˆ’Ï†) + Ï†ht + Î·t
where we condition on the mixture st such that
Îµt|st = i âˆ¼N(mi, v2
Î·t âˆ¼N(0, Ïƒ2
So this puts yt = log(rd2
+ const), ct = Âµi, Gt = (Ïƒi 0), Î³t = ht and Zt = 1.
dt = Âµ(1 âˆ’Ï†), Tt = Ï† and Ht = (0 ÏƒÎ·). Finally, for a stationary initial condition, a1|0 = Âµ
and P1|0 = Ïƒ2
 1 âˆ’Ï†2. This means that Îµt = Gtut, Î·t = Htut and ut is a bivariate standard
The Kalman ï¬lter is run for t = 1, ..., n,
dt + TtÎ³t|tâˆ’1 + Ktvt,
TtPt|tâˆ’1Lâ€²
yt âˆ’ZtÎ³t|tâˆ’1 âˆ’ct,
ZtPt|tâˆ’1Zâ€²
TtPt|tâˆ’1Zâ€²
Here Î³t+1|t = E(Î³t+1|y1, ..., yt) while Pt+1|t is the corresponding mean square error. More detailed
discussion of the state space form and the Kalman ï¬lter is given in Harvey .
The simulation signal smoother ) draws from the multivariate
normal posterior
(c1 + Z1Î³1, ..., cn + ZnÎ³n) |y, Î¸ ,
where Î¸ denotes the parameters of the model. Setting rn = 0 and Nn = 0, and writing Dt =
tNtKt, nt = F âˆ’1
trt, we run for t = n, ..., 1,
Î£t âˆ’Î£tDtÎ£t,
Îºt âˆ¼N(0, Ct),
Vt = Î£t , provides
easy to use functions which perform Kalman ï¬ltering and the simulation signal smoothing for
an arbitrary state space model.
Augmented state space
Suppose that we write
Î² âˆ¼N(0, Î›),
where Î² is independent of the ut process. Then we can estimate the states and the regression
parameter Î² at the same time using the augmented Kalman ï¬lter and simulation smoother. The
ï¬rst of these ideas is due to de Jong , the second to de Jong and Shephard .
The augmented Kalman ï¬lter adds two equations to the Kalman ï¬lter (22) which is run with
ct = 0, dt = 0, additionally computing
t|tâˆ’1 âˆ’Xt,
t+1|t = Wt + TtÎ³a
t|tâˆ’1 + KtV a
1|0 = W1. Here V a
t is a dim(yt)Ã—dim(Î²) matrix. The augmented innovations V a
innovations resulting from running âˆ’Xt through the Kalman ï¬lter (22) with dt = Wt. Hence
we can compute the posterior of Î²|y by looking at the weighted least squares regression of vt on
t Î² with prior information Î² âˆ¼N(0, Î›) and variances Ft. If we set S1 = Î›âˆ’1 and s1 = 0 (the
notation of st is local to this discussion) then recursively calculating
st+1 = st + V aâ€²
St+1 = St + V aâ€²
we have that Î²|y, Î¸ âˆ¼N(âˆ’Sâˆ’1
n ), where Î¸ now denotes the remaining parameters in the
The random regression eï¬€ect Î² can be analytically integrated out of the joint density of y
and Î² (given Î¸) as
f(y|Î², Î¸)Ï€(Î²)dÎ² = f(y|Î² = 0, Î¸)Ï€(Î² = 0)
Ï€(Î² = 0|y, Î¸)
|Î›|âˆ’1/2 exp
using the terms from the augmented Kalman ï¬lter. This result is due to de Jong .
If we draw from b âˆ¼Î²|y, Î¸ we can calculate a new set of innovations vs
t = vt + V a
t b, which
are the innovations from running the Kalman ï¬lter on a state space with known Î² = b. Hence
we can use the simulation signal smoother which draws c1 + Z1Î³1, ..., cn + ZnÎ³n|y, Î² = b, Î¸ using
the simulation signal smoother (23) just by plugging in the vs
t instead of the vt. By using both
of these draws we are actually sampling directly from the distribution of
(Î², c1 + Z1Î³1, ..., cn + ZnÎ³n)|y, Î¸.