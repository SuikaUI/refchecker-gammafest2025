Patch SVDD: Patch-level SVDD
for Anomaly Detection and Segmentation
Jihun Yi and Sungroh Yoon
Department of Electrical and Computer Engineering,
Seoul National University, Seoul, South Korea
{ t080205, sryoon }@snu.ac.kr
Abstract. In this paper, we address the problem of image anomaly
detection and segmentation. Anomaly detection involves making a binary
decision as to whether an input image contains an anomaly, and anomaly
segmentation aims to locate the anomaly on the pixel level. Support vector
data description (SVDD) is a long-standing algorithm used for an anomaly
detection, and we extend its deep learning variant to the patch-based
method using self-supervised learning. This extension enables anomaly
segmentation and improves detection performance. As a result, anomaly
detection and segmentation performances measured in AUROC on MVTec
AD dataset increased by 9.8% and 7.0%, respectively, compared to the
previous state-of-the-art methods. Our results indicate the eﬃcacy of the
proposed method and its potential for industrial application. Detailed
analysis of the proposed method oﬀers insights regarding its behavior,
and the code is available online1.
Introduction
Anomaly detection is a binary classiﬁcation problem to determine whether an
input contains an anomaly. Detecting anomalies is a critical and long-standing
problem faced by the manufacturing and ﬁnancial industries. Anomaly detection
is usually formulated as a one-class classiﬁcation because abnormal examples
are either inaccessible or insuﬃcient to model distribution during the training.
When concentrating on image data, detected anomalies can also be localized, and
anomaly segmentation problem is to localize the anomalies at the pixel level. In
this study, we tackle the problems of image anomaly detection and segmentation.
One-class support vector machine (OC-SVM) and support vector data
description (SVDD) are classic algorithms used for one-class classiﬁcation.
Given a kernel function, OC-SVM seeks a max-margin hyperplane from the origin
in the kernel space. Likewise, SVDD searches for a data-enclosing hypersphere in
the kernel space. These approaches are closely related, and Vert et al. showed
their equivalence in the case of a Gaussian kernel. Ruﬀet al. proposed a deep
learning variant of SVDD, Deep SVDD, by deploying a deep neural network
in the place of the kernel function. The neural network was trained to extract
1 
 
Anomaly map
Anomaly map
Anomaly map
aggregated
Fig. 1: Proposed method localizes defects in an MVTec AD image.
Patch SVDD performs multi-scale inspection and combines the results. As a
result, the anomaly map pinpoints the defects (contoured with a red line).
a data-dependent representation, removing the need to choose an appropriate
kernel function by hand. Furthermore, Ruﬀet al. re-interpreted Deep SVDD
in an information-theoretic perspective and applied to semi-supervised scenarios.
In this paper, we extend Deep SVDD to a patch-wise detection method,
thereby proposing Patch SVDD. This extension is rendered nontrivial by the
relatively high level of intra-class variation of the patches and is facilitated by
self-supervised learning. The proposed method enables anomaly segmentation
and improves anomaly detection performance. Fig. 1 shows an example of the
localized anomalies using the proposed method. In addition, the results in previous
studies show that the features of a randomly initialized encoder might be
used to distinguish anomalies. We detail the more in-depth behavior of random
encoders and investigate the source of separability in the random features.
Background
Anomaly detection and segmentation
Problem formulation Anomaly detection is a problem to make a binary
decision whether an input is an anomaly or not. The deﬁnition of anomaly ranges
from a tiny defect to an out-of-distribution image. We focus here on detecting
a defect in an image. A typical detection method involves training a scoring
function, Aθ, which measures the abnormality of an input. At test time, inputs
with high Aθ(x) values are deemed to be an anomaly. A de facto standard metric
for the scoring function is the area under the receiver operating characteristic
curve (AUROC), as expressed in Eq. 1 .
AUROC [Aθ] = P [Aθ(Xnormal) < Aθ(Xabnormal)] .
A good scoring function is, thus, one that assigns a low anomaly score to normal
data and a high anomaly score to abnormal data. Anomaly segmentation problem
is similarly formulated, with the generation of an anomaly score for every pixel
(i.e., an anomaly map) and the measurement of AUROC with the pixels.
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Patch SVDD
Fig. 2: Comparison of Deep SVDD and the proposed method. Patch
SVDD performs inspection on every patch to localize a defect. In addition, the
self-supervised learning allows the features to form multi-modal clusters, thereby
enhancing anomaly detection capability. The image is from MVTec AD dataset.
Auto encoder-based methods Early deep learning approaches to anomaly
detection used auto encoders . These auto encoders were trained with the
normal training data and did not provide accurate reconstruction of abnormal
images. Therefore, the diﬀerence between the reconstruction and the input
indicated abnormality. Further variants have been proposed to utilize structural
similarity indices , adversarial training , negative mining , and iterative
projection . Certain previous works utilized the learned latent feature of the
auto encoder for anomaly detection. Akcay et al. deﬁned the reconstruction
loss of the latent feature as an anomaly score, and Yarlagadda et al. trained
OC-SVM using the latent features. More recently, several methods have made
use of factors other than reconstruction loss, such as restoration loss and an
attention map .
Classiﬁer-based methods After the work of Golan et al. , discriminative
approaches have been proposed for anomaly detection. These methods exploit
an observation that classiﬁers lose their conﬁdence for the abnormal input
images. Given an unlabeled dataset, a classiﬁer is trained to predict synthetic
labels. For example, Golan et al. randomly ﬂip, rotate, and translate an
image, and the classiﬁer is trained to predict the particular type of transformation
performed. If the classiﬁer does not provide a conﬁdent and correct prediction,
the input image is deemed to be abnormal. Wang et al. proved that such
an approach could be extended to an unsupervised scenario, where the training
data also contains a few anomalies. Bergman et al. adopted an open-set
classiﬁcation method and generalized the method to include non-image data.
SVDD-based methods SVDD is a classic one-class classiﬁcation algorithm.
It maps all the normal training data into a predeﬁned kernel space and seeks the
smallest hypersphere that encloses the data in the kernel space. The anomalies
are expected to be located outside the learned hypersphere. As a kernel function
determines the kernel space, the training procedure is merely deciding the radius
and center of the hypersphere.
Ruﬀet al. improved this approach using a deep neural network. They
adopted the neural network in place of the kernel function and trained it along
Anomaly map
using SVDD
Anomaly map
using Patch SVDD
Anomaly map
using SVDD
Anomaly map
using Patch SVDD
Fig. 3: Comparison of anomaly maps generated using two diﬀerent
losses. For a relatively simple image (leather), the encoders trained with either LSVDD or LPatch SVDD both localize the defect (contoured with a red line)
well. By contrast, when the image has high complexity (cable), LSVDD fails to
localize the defect. The image is from MVTec AD dataset.
with the radius of the hypersphere. This modiﬁcation allows the encoder to
learn a data-dependent transformation, thus enhancing detection performance
on high-dimensional and structured data. To avoid a trivial solution (i.e., the
encoder outputs a constant), they removed the bias terms in the network. Ruﬀ
et al. further applied this method to a semi-supervised scenario.
Self-supervised representation learning
Learning a representation of an image is a core problem of computer vision. A
series of methods have been proposed to learn a representation of an image without
annotation. One branch of research suggests training the encoder by learning with
a pretext task, which is a self-labeled task to provide synthetic learning signals.
When a network is trained to solve the pretext task well, the network is expected
to be able to extract useful features. The pretext tasks include predicting relative
patch location , solving a jigsaw puzzle , colorizing images , counting
objects , and predicting rotations .
Patch-wise Deep SVDD
Deep SVDD trains an encoder that maps the entire training data to features
lying within a small hypersphere in the feature space. The encoder, fθ, is trained
to minimize the Euclidean distances between the features and the center of the
hypersphere using the following loss function:
∥fθ(xi) −c∥2 ,
where x is an input image. At test time, the distance between the representation
of the input and the center is used as an anomaly score. The center c is calculated
in advance of the training, as shown in Eq. 3, where N denotes the number of the
training data. Therefore, the training pushes the features around a single center.
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Classifier
Fig. 4: Self-supervised learning . The encoder is trained to extract informative features so that the following classiﬁer can correctly predict the relative
positions of the patches. Once the training is complete, the classiﬁer is discarded.
Note that the two encoders share their weights, as in Siamese Network . The
image is from MVTec AD dataset.
In this study, we extend this approach to patches; the encoder encodes each
patch, not the entire image, as illustrated in Fig. 2. Accordingly, inspection is
performed for each patch. Patch-wise inspection has several advantages. First,
the inspection result is available at each position, and hence we can localize
the positions of defects. Second, such ﬁne-grained examination improves overall
detection performance.
A direct extension of Deep SVDD to a patch-wise inspection is straightforward. A patch encoder, fθ, is trained using LSVDD with x replaced with a
patch, p. The anomaly score is deﬁned accordingly, and the examples of the
resulting anomaly maps are provided in Fig. 3. Unfortunately, the detection
performance is poor for the images with high complexity. This is because patches
have high intra-class variation; some patches correspond to the background, while
the others contain the object. As a result, mapping all the features of dissimilar
patches to a single center and imposing a uni-modal cluster weaken the connection
between the representation and the content. Therefore, using a single center c is
inappropriate, yet deciding on the appropriate number of multiple centers and
the allocation of patches to each center are cumbersome.
To bypass the above issues, we do not explicitly deﬁne the center and allocate
the patches. Instead, we train the encoder to gather semantically similar patches
by itself. The semantically similar patches are obtained by sampling spatially
adjacent patches, and the encoder is trained to minimize the distances between
their features using the following loss function:
∥fθ(pi) −fθ(pi′)∥2 ,
Fig. 5: Hierarchical encoding. An input patch is divided into a 2 × 2 grid of
sub-patches, and the sub-patches are independently encoded using the smaller
encoder (fsmall). The output features are aggregated to produce a single feature.
The image is from MVTec AD dataset.
where pi′ is a patch near pi. Furthermore, to enforce the representation to capture
the semantics of the patch, we append the following self-supervised learning.
Self-supervised learning
Doersch et al. trained an encoder and classiﬁer pair to predict the relative
positions of two patches, as depicted in Fig. 4. A well-performing pair implies that
the trained encoder extracts useful features for location prediction. Aside from
this particular task, previous research reported that the self-supervised
encoder functions as a powerful feature extractor for downstream tasks.
For a randomly sampled patch p1, Doersch et al. sampled another patch
p2 from one of its eight neighborhoods in a 3 × 3 grid. If we let the true
relative position be y ∈{0, ..., 7}, the classiﬁer Cφ is trained to predict y =
Cφ(fθ(p1), fθ(p2)) correctly. The size of the patch is the same as the receptive ﬁeld
of the encoder. To prevent the classiﬁer from exploiting shortcuts (e.g., color
aberration), we randomly perturb the RGB channels of the patches. Following
the approach by Doersch et al. , we add a self-supervised learning signal by
adding the following loss term:
LSSL = Cross-entropy (y, Cφ (fθ(p1), fθ(p2))) .
As a result, the encoder is trained using a combination of two losses with the
scaling hyperparameter λ, as presented in Eq. 6. This optimization is performed
using stochastic gradient descent and Adam optimizer .
LPatch SVDD = λLSVDD’ + LSSL.
Hierarchical encoding
As anomalies vary in size, deploying multiple encoders with various receptive ﬁelds
helps in dealing with variation in size. The experimental results in Section 4.2
show that enforcing a hierarchical structure on the encoder boosts anomaly
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Nearest normal patches Anomaly Map
feature space
Fig. 6: Overall ﬂow of the proposed method. For a given test image, Patch
SVDD divides the image into patches of size K with strides S and extracts their
features using the trained encoder. The L2 distance to the nearest normal patch
in the feature space becomes the anomaly score of each patch. The resulting
anomaly map localizes the defects (contoured with a red line). The image is from
MVTec AD dataset.
detection performance as well. For this reason, we employ a hierarchical encoder
that embodies a smaller encoder; the hierarchical encoder is deﬁned as
fbig(p) = gbig(fsmall(p)).
An input patch p is divided into a 2 × 2 grid, and their features are aggregated
to constitute the feature of p, as shown in Fig. 5. Each encoder with receptive
ﬁeld size K is trained with the self-supervised task of patch size K. Throughout
the experiment, the receptive ﬁeld sizes of the large and small encoders are 64
and 32, respectively.
Generating anomaly maps
After training the encoders, the representations from the encoder are used to
detect the anomalies. First, the representation of every normal train patch,
{fθ(pnormal)|pnormal}, is calculated and stored. Given a query image x, for every
patch p with a stride S within x, the L2 distance to the nearest normal patch in
the feature space is then deﬁned to be its anomaly score using Eq. 8. To mitigate
the computational cost of the nearest neighbor search, we adopt its approximate
algorithm2. As a result, the inspection of a single image from MVTec AD 
dataset for example, requires approximately 0.48 second.
(p) .= min
pnormal ∥fθ(p) −fθ(pnormal)∥2 .
Patch-wise calculated anomaly scores are then distributed to the pixels. As a
consequence, pixels receive the average anomaly scores of every patch to which
they belong, and we denote the resulting anomaly map as M.
The multiple encoders discussed in Section 3.3 constitute multiple feature
spaces, thereby yielding multiple anomaly maps. We aggregate the multiple
2 
Anomaly map
Anomaly map
Anomaly map
Fig. 7: Anomaly maps generated by the proposed method. Patch SVDD
generates anomaly maps of each image in ﬁfteen classes of MVTec AD dataset.
The ground truth defect annotations are depicted as red contours in the image,
and the darker heatmap indicates higher anomaly scores.
maps using element-wise multiplication, and the resulting anomaly map, Mmulti,
provides the answer to the problem of anomaly segmentation:
Mmulti .= Msmall ⊙Mbig,
where Msmall and Mbig are the generated anomaly maps using fsmall and fbig,
respectively. The pixels with high anomaly scores in the map Mmulti are deemed
to contain defects.
It is straightforward to address the problem of anomaly detection. The
maximum anomaly score of the pixels in an image is its anomaly score, as
expressed in Eq. 10. Fig. 6 illustrates the overall ﬂow of the proposed method,
and its pseudo-code is provided in Appendix A1.
(x) .= max
i,j Mmulti(x)ij.
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Table 1: Anomaly detection (left) and segmentation (right) performances on MVTec AD dataset. The proposed method, Patch SVDD,
achieves the state-of-the-art performances on both tasks.
Task: Anomaly Detection
Deep SVDD 
NeurIPS’ 18
GANomaly 
Patch SVDD (Ours)
Task: Anomaly Segmentation
CVPR’ 20 
ICLR’ 20 
Patch SVDD (Ours)
Results and Discussion
To verify the validity of the proposed method, we applied it to MVTec AD 
dataset. The dataset consists of 15-class industrial images, each class categorized
as either an object or texture. Ten object classes contain regularly positioned objects, whereas the texture classes contain repetitive patterns. The implementation
details used throughout the study are provided in Appendix A3, and please refer
to for more details on the dataset.
Anomaly detection and segmentation results
Fig. 7 shows anomaly maps generated using the proposed method, indicating
that the defects are properly localized, regardless of their size. Table 1 shows the
detection and segmentation performances for MVTec AD dataset compared
with state-of-the-art baselines in AUROC. Patch SVDD provides state-of-theart performance over the powerful baselines including auto encoder-based and
classiﬁer-based methods and outperforms Deep SVDD by 55.6% improvement.
More numerical results are provided in Appendix A2.1.
Detailed analysis
t-SNE visualization Fig. 8 shows t-SNE visualizations of the learned
features of multiple train images. Patches located at the points shown in Fig. 8(b)
are mapped to the points with the same color and size in Fig. 8(a) and Fig. 8(c).
In Fig. 8(a), the points with similar color and size form clusters in the feature
space. Since the images in the cable class are regularly positioned, the patches
from the same position have similar content, even if they are from diﬀerent images.
Likewise, for the regularly positioned object classes, the points with similar color
and size in t-SNE visualization (i.e., the patches with similar positions) can be
regarded to be semantically similar. By contrast, features of the leather class
in Fig. 8(c) show the opposite tendency. This is because the patches in texture
classes are analogous, regardless of their position in the image; the positions of
the patches are not quite related to their semantics for the texture images.
(a) t-SNE for cable
(b) Points at each position
(c) t-SNE for leather
Fig. 8: t-SNE visualizations of the learned features. The color and size
of each point represent the position (θ and r of the polar coordinates) within an
image (b). From its color and size, we can infer the positions of the corresponding
patches of the features in (a, c).
Table 2: The eﬀect of the losses.
Modifying LSVDD to LSVDD’ and
adopting LSSL both improve the
anomaly detection (Det.) and segmentation (Seg.) performances.
LSVDD LSVDD’ LSSL
0.921 0.957
transistor
Anomaly detection
performance (AUROC)
The eﬀects of the losses vary
among classes. LSSL is particularly beneﬁciary to the object classes.
Eﬀect of self-supervised learning Patch SVDD trains an encoder using two
losses: LSVDD’ and LSSL, where LSVDD’ is a variant of LSVDD. To compare the
roles of the proposed loss terms, we conduct an ablation study. Table 2 suggests
that the modiﬁcation of LSVDD to LSVDD’ and the adoption of LSSL improve
the anomaly detection and segmentation performances. Fig. 9 shows that the
eﬀects of the proposed loss terms vary among classes. Speciﬁcally, the texture
classes (e.g. tile and wood) are less sensitive to the choice of loss, whereas the
object classes, including cable and transistor, beneﬁt signiﬁcantly from LSSL.
To investigate the reason behind these observations, we provide (in Fig. 10)
t-SNE visualizations of the features of an object class (the transistor) for the
encoders trained with LSVDD, LSVDD’, and LSVDD’ + LSSL. When training is
performed with LSVDD (Fig. 10(a)) or LSVDD’ (Fig. 10(b)), the features form a
uni-modal cluster. In contrast, LSSL results in multi-modal feature clusters on
the basis of their semantics (i.e., color and size), as shown in Fig. 10(c). The
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
(a) t-SNE using SVDD
(b) t-SNE using SVDD’
(c) t-SNE using SVDD’ + SSL
Fig. 10: Features of the multiple train images in the transistor class by
the encoders trained with diﬀerent losses. Adopting LSSL (c) enables the
representations to form clusters on the basis of their semantics.
Intrinsic dimension
Fig. 11: Intrinsic dimensions of the features
under diﬀerent losses.
Hierarchical
Aggregated
Nonhierarchical
Anomaly inspection
performances (AUROC)
Fig. 12: The eﬀect of hierarchical encoding.
Aggregating the results from multi-scale inspection
boosts the performance, and adopting hierarchical
structure to the encoder is helpful as well.
multi-modal property of the features is particularly beneﬁcial to the object classes,
which have high intra-class variation among the patches. Features of the patches
with dissimilar semantics are separated, and hence anomaly inspection using
those features becomes more deliberate and accurate.
The intrinsic dimensions (ID) of the features also indicate the eﬀectiveness
of LSSL. The ID is the minimal number of coordinates required to describe the
points without signiﬁcant information loss . A larger ID denotes that the
points are spreaded in every direction, while a smaller ID indicates that the
points lie on low-dimensional manifolds with high separability. In Fig. 11, we
show the average IDs of features in each class trained with three diﬀerent losses.
If the encoder is trained with the proposed LPatch SVDD, features with the lowest
ID are yielded, implying that these features are neatly distributed.
Hierarchical encoding In Section 3.3, we proposed the use of hierarchical
encoders. Fig. 12 shows that aggregating multi-scale results from multiple encoders improves the inspection performances. In addition, an ablation study
Anomaly map
Anomaly map
Anomaly map
aggregated
Fig. 13: Multi-scale inspection. Patch SVDD performs multi-scale inspection
and aggregates the results. The image is from MVTec AD dataset.
with a non-hierarchical encoder shows that the hierarchical structure itself also
boosts performance. We postulate that the hierarchical architecture provides
regularization for the feature extraction. Note that the non-hierarchical encoder
has a number of parameters similar to that of the hierarchical counterpart.
We provide an example of multi-scale inspection results, together with an
aggregated anomaly map, in Fig. 13. The anomaly maps from various scales
provide complementary inspection results; the encoder with a large receptive ﬁeld
coarsely locates the defect, whereas the one with a smaller receptive ﬁeld reﬁnes
the result. Therefore, an element-wise multiplication of the two maps localizes
the accurate position of the defect.
Hyperparameters As shown in Eq. 6, the hyperparameter λ balances LSVDD’
and LSSL. A large λ emphasizes gathering of the features, while a small λ promotes
their informativeness. Interestingly, the most favorable value of λ varies among
the classes. Anomalies in the object classes are well detected under a smaller
λ, while the texture classes are well detected with a larger λ. Fig. 14 shows
an example of this diﬀerence; the anomaly detection performance for the cable
class (object) improves as λ decreases, while the wood class (texture) shows the
opposite trend. As discussed in the previous sections, this occurs because the
self-supervised learning is more helpful when the patches show high intra-class
variation, which is the case for the object classes. The result coincides with that
shown in Fig. 9 because using LSVDD’ as a loss is equivalent to using LPatch SVDD
with λ >> 1.
The number of feature dimensions, D, is another hyperparameter of the
encoder. The anomaly inspection performance for varying D is depicted in
Fig. 15(a). A larger D signiﬁes improved performance—a trend that has been
discussed in a self-supervised learning venue . Fig. 15(b) indicates that the
ID of the resulting features increases with increasing D. The black dashed line
represents the y = x graph, and it is the upper bound of ID. The average ID of
features among the classes saturates as D = 64; therefore, we used a value of
D = 64 throughout our study.
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
Anomaly detection
performance (AUROC)
Fig. 14: The
λ. The anomaly detection
performances for the two
classes show the opposite
trends as λ varies.
Intrinsic dimension
Anomaly inspection
performances (AUROC)
Fig. 15: The eﬀect of the embedding dimension, D. Larger D yields better inspection results
(a) and larger intrinsic dimensions (b).
Random encoder Doersch et al. showed that randomly initialized encoders
perform reasonably well in image retrieval; given an image, the nearest images
in the random feature space look similar to humans as well. Inspired by this
observation, we examined the anomaly detection performance of the random
encoders and provided the results in Table 3. As in Eq. 8, the anomaly score
is deﬁned to be the distance to the nearest normal patch, but in the random
feature space. In the case of certain classes, the features of the random encoder
are eﬀective in distinguishing between normal and abnormal images. Some results
even outperform the trained deep neural network model (L2-AE).
Here, we investigate the reason for the high separability of the random features.
For simplicity, let us assume the encoder to be a one-layered convolutional layer
parametrized by a weight W ̸= 0 and a bias b followed by a nonlinearity, σ. Given
two patches p1 and p2, their features h1 and h2 are provided by Eq. 11, where ∗
denotes a convolution operation.
h1 = σ(W ∗p1 + b)
h2 = σ(W ∗p2 + b).
As suggested by Eq. 12, when the features are close, so are the patches,
and vice versa. Therefore, retrieving the nearest patch in the feature space is
analogous to doing so in the image space.
∥h1 −h2∥2 ≈0 ⇔(W ∗p1 + b) −(W ∗p2 + b) ≈0
⇔W ∗(p1 −p2) ≈0
⇔∥p1 −p2∥2 ≈0.
In Table 3, we also provide the results for anomaly detection task using the
nearest neighbor algorithm using the raw patches (i.e., fθ(p) = p in Eq. 8). For
certain classes, the raw patch nearest neighbor algorithm works surprisingly well.
Table 3: Nearest neighbor algorithm using the random encoders and
raw patches for MVTec AD dataset. For certain classes, the nearest
neighbor algorithm using the random features shows good anomaly detection
performance. For those classes, using the raw patches also yields high performance.
Random Encoder
toothbrush
transistor
The eﬀectiveness of the raw patches for anomaly detection can be attributed to
the high similarity among the normal images.
Furthermore, the well-separated classes provided by the random encoder
are well-separated by the raw patch nearest neighbor algorithm, and vice versa.
Together with the conclusion of Eq. 12, this observation implies the strong
relationship between the raw image patch and its random feature. To summarize,
the random features of anomalies are easily separable because they are alike the
raw patches, and the raw patches are easily separable.
Conclusion
In this paper, we proposed Patch SVDD, a method for image anomaly detection
and segmentation. Unlike Deep SVDD , we inspect the image at the patch
level, and hence we can also localize defects. Moreover, additional self-supervised
learning improves detection performance. As a result, the proposed method
achieved state-of-the-art performance on MVTec AD industrial anomaly
detection dataset.
In previous studies , images were featurized prior to the subsequent
downstream tasks because of their high-dimensional and structured nature.
However, the results in our analysis suggest that a nearest neighbor algorithm
with a raw patch often discriminates anomalies surprisingly well. Moreover, since
Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation
the distances in random feature space are closely related to those in the raw
image space, random features can provide distinguishable signals.