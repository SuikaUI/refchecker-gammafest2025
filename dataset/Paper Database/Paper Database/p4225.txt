ViViT: A Video Vision Transformer
Anurag Arnab*
Mostafa Dehghani*
Georg Heigold
Mario Luˇci´c†
Cordelia Schmid†
Google Research
{aarnab, dehghani, heigold, chensun, lucic, cordelias}@google.com
We present pure-transformer based models for video
classiﬁcation, drawing upon the recent success of such models in image classiﬁcation.
Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we
propose several, efﬁcient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show
how we can effectively regularise the model during training
and leverage pretrained image models to be able to train on
comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple
video classiﬁcation benchmarks including Kinetics 400 and
600, Epic Kitchens, Something-Something v2 and Moments
in Time, outperforming prior methods based on deep 3D
convolutional networks. To facilitate further research, we
release code at 
1. Introduction
Approaches based on deep convolutional neural networks have advanced the state-of-the-art across many standard datasets for vision problems since AlexNet . At
the same time, the most prominent architecture of choice in
sequence-to-sequence modelling (e.g. in natural language
processing) is the transformer , which does not use convolutions, but is based on multi-headed self-attention. This
operation is particularly effective at modelling long-range
dependencies and allows the model to attend over all elements in the input sequence. This is in stark contrast to
convolutions where the corresponding “receptive ﬁeld” is
limited, and grows linearly with the depth of the network.
The success of attention-based models in NLP has recently inspired approaches in computer vision to integrate
transformers into CNNs , as well as some attempts to
replace convolutions completely . However, it is
*Equal contribution
†Equal advising
only very recently with the Vision Transformer (ViT) ,
that a pure-transformer based architecture has outperformed
its convolutional counterparts in image classiﬁcation. Dosovitskiy et al. closely followed the original transformer
architecture of , and noticed that its main beneﬁts
were observed at large scale – as transformers lack some
of the inductive biases of convolutions (such as translational equivariance), they seem to require more data 
or stronger regularisation .
Inspired by ViT, and the fact that attention-based architectures are an intuitive choice for modelling longrange contextual relationships in video, we develop several transformer-based models for video classiﬁcation. Currently, the most performant models are based on deep 3D
convolutional architectures which were a natural extension of image classiﬁcation CNNs . Recently, these models were augmented by incorporating selfattention into their later layers to better capture long-range
dependencies .
As shown in Fig. 1, we propose pure-transformer models for video classiﬁcation. The main operation performed
in this architecture is self-attention, and it is computed on
a sequence of spatio-temporal tokens that we extract from
the input video. To effectively process the large number of
spatio-temporal tokens that may be encountered in video,
we present several methods of factorising our model along
spatial and temporal dimensions to increase efﬁciency and
scalability. Furthermore, to train our model effectively on
smaller datasets, we show how to reguliarise our model during training and leverage pretrained image models.
We also note that convolutional models have been developed by the community for several years, and there are
thus many “best practices” associated with such models.
As pure-transformer models present different characteristics, we need to determine the best design choices for such
architectures. We conduct a thorough ablation analysis of
tokenisation strategies, model architecture and regularisation methods. Informed by this analysis, we achieve stateof-the-art results on multiple standard video classiﬁcation
benchmarks, including Kinetics 400 and 600 , Epic
Kitchens 100 , Something-Something v2 and Moments in Time .
 
Position + Token
Factorised
Self-Attention
Transformer Encoder
Layer Norm
Layer Norm
Multi-Head
Dot-Product
Factorised
Self-Attention
Factorised
Dot-Product
Figure 1: We propose a pure-transformer architecture for video classiﬁcation, inspired by the recent success of such models for images .
To effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components
of the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different
attention patterns over space and time.
2. Related Work
Architectures for video understanding have mirrored advances in image recognition.
Early video research used
hand-crafted features to encode appearance and motion
information .
The success of AlexNet on ImageNet initially led to the repurposing of 2D image convolutional networks (CNNs) for video as “twostream” networks .
These models processed
RGB frames and optical ﬂow images independently before
fusing them at the end. Availability of larger video classi-
ﬁcation datasets such as Kinetics subsequently facilitated the training of spatio-temporal 3D CNNs 
which have signiﬁcantly more parameters and thus require
larger training datasets. As 3D convolutional networks require signiﬁcantly more computation than their image counterparts, many architectures factorise convolutions across
spatial and temporal dimensions and/or use grouped convolutions . We also leverage factorisation
of the spatial and temporal dimensions of videos to increase
efﬁciency, but in the context of transformer-based models.
Concurrently, in natural language processing (NLP),
Vaswani et al. achieved state-of-the-art results by replacing convolutions and recurrent networks with the transformer network that consisted only of self-attention, layer
normalisation and multilayer perceptron (MLP) operations.
Current state-of-the-art architectures in NLP remain transformer-based, and have been scaled to web-scale
datasets . Many variants of the transformer have also
been proposed to reduce the computational cost of selfattention when processing longer sequences and to improve parameter efﬁciency .
Although self-attention has been employed extensively in
computer vision, it has, in contrast, been typically incorporated as a layer at the end or in the later stages of
the network or to augment residual
blocks within a ResNet architecture .
Although previous works attempted to replace convolutions in vision architectures , it is only very recently that Dosovitisky et al. showed with their ViT architecture that pure-transformer networks, similar to those
employed in NLP, can achieve state-of-the-art results for
image classiﬁcation too.
The authors showed that such
models are only effective at large scale, as transformers lack
some of inductive biases of convolutional networks (such
as translational equivariance), and thus require datasets
larger than the common ImageNet ILSRVC dataset to
train. ViT has inspired a large amount of follow-up work
in the community, and we note that there are a number
of concurrent approaches on extending it to other tasks in
computer vision and improving its dataefﬁciency . In particular, have also proposed
transformer-based models for video.
In this paper, we develop pure-transformer architectures
for video classiﬁcation. We propose several variants of our
model, including those that are more efﬁcient by factorising the spatial and temporal dimensions of the input video.
We also show how additional regularisation and pretrained
models can be used to combat the fact that video datasets
are not as large as their image counterparts that ViT was
originally trained on. Furthermore, we outperform the stateof-the-art across ﬁve popular datasets.
3. Video Vision Transformers
We start by summarising the recently proposed Vision
Transformer in Sec. 3.1, and then discuss two approaches for extracting tokens from video in Sec. 3.2. Finally, we develop several transformer-based architectures
for video classiﬁcation in Sec. 3.3 and 3.4.
3.1. Overview of Vision Transformers (ViT)
Vision Transformer (ViT) adapts the transformer
architecture of to process 2D images with minimal
changes. In particular, ViT extracts N non-overlapping image patches, xi ∈Rh×w, performs a linear projection and
then rasterises them into 1D tokens zi ∈Rd. The sequence
of tokens input to the following transformer encoder is
z = [zcls, Ex1, Ex2, . . . , ExN] + p,
where the projection by E is equivalent to a 2D convolution.
As shown in Fig. 1, an optional learned classiﬁcation token
zcls is prepended to this sequence, and its representation at
the ﬁnal layer of the encoder serves as the ﬁnal representation used by the classiﬁcation layer . In addition, a
learned positional embedding, p ∈RN×d, is added to the
tokens to retain positional information, as the subsequent
self-attention operations in the transformer are permutation
invariant. The tokens are then passed through an encoder
consisting of a sequence of L transformer layers. Each layer
ℓcomprises of Multi-Headed Self-Attention , layer normalisation (LN) , and MLP blocks as follows:
yℓ= MSA(LN(zℓ)) + zℓ
zℓ+1 = MLP(LN(yℓ)) + yℓ.
The MLP consists of two linear projections separated by a
GELU non-linearity and the token-dimensionality, d,
remains ﬁxed throughout all layers. Finally, a linear classi-
ﬁer is used to classify the encoded input based on zL
if it was prepended to the input, or a global average pooling
of all the tokens, zL, otherwise.
As the transformer , which forms the basis of
ViT , is a ﬂexible architecture that can operate on any
sequence of input tokens z ∈RN×d, we describe strategies
for tokenising videos next.
3.2. Embedding video clips
We consider two simple methods for mapping a video
RT ×H×W ×C to a sequence of tokens ˜z
Rnt×nh×nw×d. We then add the positional embedding and
reshape into RN×d to obtain z, the input to the transformer.
Uniform frame sampling
As illustrated in Fig. 2, a
straightforward method of tokenising the input video is to
uniformly sample nt frames from the input video clip, embed each 2D frame independently using the same method
as ViT , and concatenate all these tokens together. Concretely, if nh · nw non-overlapping image patches are extracted from each frame, as in , then a total of nt·nh·nw
tokens will be forwarded through the transformer encoder.
Intuitively, this process may be seen as simply constructing
a large 2D image to be tokenised following ViT. We note
that this is the input embedding method employed by the
concurrent work of .
Figure 2: Uniform frame sampling: We simply sample nt frames,
and embed each 2D frame independently following ViT .
Figure 3: Tubelet embedding. We extract and linearly embed nonoverlapping tubelets that span the spatio-temporal input volume.
Tubelet embedding
An alternate method, as shown in
Fig. 3, is to extract non-overlapping, spatio-temporal
“tubes” from the input volume, and to linearly project this to
Rd. This method is an extension of ViT’s embedding to 3D,
and corresponds to a 3D convolution. For a tubelet of dimension t × h × w, nt = ⌊T
t ⌋, nh = ⌊H
h ⌋and nw = ⌊W
tokens are extracted from the temporal, height, and width
dimensions respectively. Smaller tubelet dimensions thus
result in more tokens which increases the computation.
Intuitively, this method fuses spatio-temporal information
during tokenisation, in contrast to “Uniform frame sampling” where temporal information from different frames is
fused by the transformer.
3.3. Transformer Models for Video
As illustrated in Fig. 1, we propose multiple transformerbased architectures. We begin with a straightforward extension of ViT that models pairwise interactions between all spatio-temporal tokens, and then develop more
efﬁcient variants which factorise the spatial and temporal
dimensions of the input video at various levels of the transformer architecture.
Model 1: Spatio-temporal attention
This model simply forwards all spatio-temporal tokens extracted from the
video, z0, through the transformer encoder. We note that
this has also been explored concurrently by in their
“Joint Space-Time” model. In contrast to CNN architectures, where the receptive ﬁeld grows linearly with the
number of layers, each transformer layer models all pair-
Positional + Token
Temporal + Token
Embed to tokens
Temporal Transformer Encoder
Spatial Transformer
Spatial Transformer
Spatial Transformer
Figure 4: Factorised encoder (Model 2). This model consists of
two transformer encoders in series: the ﬁrst models interactions
between tokens extracted from the same temporal index to produce
a latent representation per time-index. The second transformer
models interactions between time steps. It thus corresponds to a
“late fusion” of spatial- and temporal information.
wise interactions between all spatio-temporal tokens, and it
thus models long-range interactions across the video from
the ﬁrst layer.
However, as it models all pairwise interactions, Multi-Headed Self Attention (MSA) has
quadratic complexity with respect to the number of tokens.
This complexity is pertinent for video, as the number of tokens increases linearly with the number of input frames, and
motivates the development of more efﬁcient architectures
Model 2: Factorised encoder
As shown in Fig. 4, this
model consists of two separate transformer encoders. The
ﬁrst, spatial encoder, only models interactions between tokens extracted from the same temporal index. A representation for each temporal index, hi ∈Rd, is obtained after Ls
layers: This is the encoded classiﬁcation token, zLs
cls if it was
prepended to the input (Eq. 1), or a global average pooling
from the tokens output by the spatial encoder, zLs, otherwise. The frame-level representations, hi, are concatenated
into H ∈Rnt×d, and then forwarded through a temporal
encoder consisting of Lt transformer layers to model interactions between tokens from different temporal indices.
The output token of this encoder is then ﬁnally classiﬁed.
This architecture corresponds to a “late fusion” of temporal information, and the initial spatial encoder is identical to the one used for image classi-
ﬁcation. It is thus analogous to CNN architectures such
 which ﬁrst extract per-frame features, and then aggregate them into a ﬁnal representation
before classifying them.
Although this model has more
transformer layers than Model 1 (and thus more parameters), it requires fewer ﬂoating point operations (FLOPs),
as the two separate transformer blocks have a complexity
of O((nh · nw)2 + n2
t) compared to O((nt · nh · nw)2) of
Transformer Block x L
Layer Norm
Layer Norm
Multi-Head
Layer Norm
Multi-Head
Temporal Self-Attention Block
Spatial Self-Attention Block
Token embedding
Positional embedding
Figure 5: Factorised self-attention (Model 3). Within each transformer block, the multi-headed self-attention operation is factorised into two operations (indicated by striped boxes) that ﬁrst
only compute self-attention spatially, and then temporally.
Model 3: Factorised self-attention
This model, in contrast, contains the same number of transformer layers as
However, instead of computing multi-headed
self-attention across all pairs of tokens, zℓ, at layer l, we
factorise the operation to ﬁrst only compute self-attention
spatially (among all tokens extracted from the same temporal index), and then temporally (among all tokens extracted from the same spatial index) as shown in Fig. 5.
Each self-attention block in the transformer thus models
spatio-temporal interactions, but does so more efﬁciently
than Model 1 by factorising the operation over two smaller
sets of elements, thus achieving the same computational
complexity as Model 2. We note that factorising attention
over input dimensions has also been explored in ,
and concurrently in the context of video by in their “Divided Space-Time” model.
This operation can be performed efﬁciently by reshaping
the tokens z from R1×nt·nh·nw·d to Rnt×nh·nw·d (denoted
by zs) to compute spatial self-attention. Similarly, the input
to temporal self-attention, zt is reshaped to Rnh·nw×nt·d.
Here we assume the leading dimension is the “batch dimension”. Our factorised self-attention is deﬁned as
s = MSA(LN(zℓ
t = MSA(LN(yℓ
zℓ+1 = MLP(LN(yℓ
We observed that the order of spatial-then-temporal selfattention or temporal-then-spatial self-attention does not
make a difference, provided that the model parameters are
initialised as described in Sec. 3.4. Note that the number
of parameters, however, increases compared to Model 1, as
there is an additional self-attention layer (cf. Eq. 7). We do
not use a classiﬁcation token in this model, to avoid ambiguities when reshaping the input tokens between spatial and
temporal dimensions.
Model 4: Factorised dot-product attention
Finally, we
develop a model which has the same computational complexity as Models 2 and 3, while retaining the same number
of parameters as the unfactorised Model 1. The factorisation of spatial- and temporal dimensions is similar in spirit
Self-Attention Block
Layer Norm
Multi-Head
Dot-product Attention
Concatenate
Scaled Dot-Product Attention
Spatial Heads
Scaled Dot-Product Attention
Temporal Heads
Figure 6: Factorised dot-product attention (Model 4). For half of
the heads, we compute dot-product attention over only the spatial
axes, and for the other half, over only the temporal axis.
to Model 3, but we factorise the multi-head dot-product attention operation instead (Fig. 6). Concretely, we compute
attention weights for each token separately over the spatialand temporal-dimensions using different heads. First, we
note that the attention operation for each head is deﬁned as
Attention(Q, K, V) = Softmax
In self-attention, the queries Q = XWq, keys K = XWk,
and values V = XWv are linear projections of the input X
with X, Q, K, V ∈RN×d. Note that in the unfactorised
case (Model 1), the spatial and temporal dimensions are
merged as N = nt · nh · nw.
The main idea here is to modify the keys and values for
each query to only attend over tokens from the same spatialand temporal index by constructing Ks, Vs ∈Rnh·nw×d
and Kt, Vt ∈Rnt×d, namely the keys and values corresponding to these dimensions. Then, for half of the attention heads, we attend over tokens from the spatial dimension by computing Ys = Attention(Q, Ks, Vs), and for
the rest we attend over the temporal dimension by computing Yt = Attention(Q, Kt, Vt). Given that we are only
changing the attention neighbourhood for each query, the
attention operation has the same dimension as in the unfactorised case, namely Ys, Yt ∈RN×d. We then combine
the outputs of multiple heads by concatenating them and
using a linear projection , Y = Concat(Ys, Yt)WO.
3.4. Initialisation by leveraging pretrained models
ViT has been shown to only be effective when
trained on large-scale datasets, as transformers lack some of
the inductive biases of convolutional networks . However, even the largest video datasets such as Kinetics ,
have several orders of magnitude less labelled examples
when compared to their image counterparts . As
a result, training large models from scratch to high accuracy
is extremely challenging. To sidestep this issue, and enable
more efﬁcient training we initialise our video models from
pretrained image models. However, this raises several practical questions, speciﬁcally on how to initialise parameters
not present or incompatible with image models. We now
discuss several effective strategies to initialise these largescale video classiﬁcation models.
Positional embeddings
A positional embedding p is
added to each input token (Eq. 1).
However, our video
models have nt times more tokens than the pretrained image model. As a result, we initialise the positional embeddings by “repeating” them temporally from Rnw·nh×d to
Rnt·nh·nw×d. Therefore, at initialisation, all tokens with
the same spatial index have the same embedding which is
then ﬁne-tuned.
Embedding weights, E
When using the “tubelet embedding” tokenisation method (Sec. 3.2), the embedding ﬁlter
E is a 3D tensor, compared to the 2D tensor in the pretrained model, Eimage. A common approach for initialising
3D convolutional ﬁlters from 2D ﬁlters for video classiﬁcation is to “inﬂate” them by replicating the ﬁlters along the
temporal dimension and averaging them as
t [Eimage, . . . , Eimage, . . . , Eimage].
We consider an additional strategy, which we denote as
“central frame initialisation”, where E is initialised with zeroes along all temporal positions, except at the centre ⌊t
E = [0, . . . , Eimage, . . . , 0].
Therefore, the 3D convolutional ﬁlter effectively behaves
like “Uniform frame sampling” (Sec. 3.2) at initialisation,
while also enabling the model to learn to aggregate temporal
information from multiple frames as training progresses.
Transformer weights for Model 3
The transformer
block in Model 3 (Fig. 5) differs from the pretrained ViT
model , in that it contains two multi-headed self attention (MSA) modules. In this case, we initialise the spatial
MSA module from the pretrained module, and initialise all
weights of the temporal MSA with zeroes, such that Eq. 5
behaves as a residual connection at initialisation.
4. Empirical evaluation
We ﬁrst present our experimental setup and implementation details in Sec. 4.1, before ablating various components
of our model in Sec. 4.2. We then present state-of-the-art
results on ﬁve datasets in Sec. 4.3.
4.1. Experimental Setup
Network architecture and training
Our backbone architecture follows that of ViT and BERT . We consider ViT-Base (ViT-B, L=12, NH=12, d=768), ViT-Large
(ViT-L, L=24, NH=16, d=1024), and ViT-Huge (ViT-H,
L=32, NH=16, d=1280), where L is the number of transformer layers, each with a self-attention block of NH heads
Table 1: Comparison of input encoding methods using ViViT-B
and spatio-temporal attention on Kinetics. Further details in text.
Top-1 accuracy
Uniform frame sampling
Tubelet embedding
Random initialisation 
Filter inﬂation 
Central frame
and hidden dimension d. We also apply the same naming
scheme to our models (e.g., ViViT-B/16x2 denotes a ViT-
Base backbone with a tubelet size of h×w×t = 16×16×2).
In all experiments, the tubelet height and width are equal.
Note that smaller tubelet sizes correspond to more tokens at
the input, and thus more computation.
We train our models using synchronous SGD and momentum, a cosine learning rate schedule and TPU-v3 accelerators.
We initialise our models from a ViT image
model trained either on ImageNet-21K (unless otherwise speciﬁed) or the larger JFT dataset. We implement our method using the Scenic library and have released our code and models.
We evaluate the performance of our proposed
models on a diverse set of video classiﬁcation datasets:
Kinetics consists of 10-second videos sampled at
25fps from YouTube. We evaluate on both Kinetics 400
and 600, containing 400 and 600 classes respectively. As
these are dynamic datasets (videos may be removed from
YouTube), we note our dataset sizes are approximately 267
000 and 446 000 respectively.
Epic Kitchens-100 consists of egocentric videos capturing daily kitchen activities spanning 100 hours and 90 000
clips . We report results following the standard “action
recognition” protocol. Here, each video is labelled with a
“verb” and a “noun” and we therefore predict both categories using a single network with two “heads”. The topscoring verb and action pair predicted by the network form
an “action”, and action accuracy is the primary metric.
Moments in Time consists of 800 000, 3-second
YouTube clips that capture the gist of a dynamic scene involving animals, objects, people, or natural phenomena.
Something-Something v2 (SSv2) contains 220 000
videos, with durations ranging from 2 to 6 seconds. In contrast to the other datasets, the objects and backgrounds in
the videos are consistent across different action classes, and
this dataset thus places more emphasis on a model’s ability
to recognise ﬁne-grained motion cues.
The input to our network is a video clip of 32
frames using a stride of 2, unless otherwise mentioned, similar to . Following common practice, at inference
time, we process multiple views of a longer video and aver-
Table 2: Comparison of model architectures using ViViT-B as the
backbone, and tubelet size of 16×2. We report Top-1 accuracy on
Kinetics 400 (K400) and action accuracy on Epic Kitchens (EK).
Runtime is during inference on a TPU-v3.
Model 1: Spatio-temporal
Model 2: Fact. encoder
Model 3: Fact. self-attention
Model 4: Fact. dot product
Model 2: Ave. pool baseline
Table 3: The effect of varying the number of temporal transformers, Lt, in the Factorised encoder model (Model 2). We report the
Top-1 accuracy on Kinetics 400. Note that Lt = 0 corresponds to
the “average pooling baseline”.
age per-view logits to obtain the ﬁnal result. Unless otherwise speciﬁed, we use a total of 4 views per video (as this
is sufﬁcient to “see” the entire video clip across the various
datasets), and ablate these and other design choices next.
4.2. Ablation study
Input encoding
We ﬁrst consider the effect of different
input encoding methods (Sec. 3.2) using our unfactorised
model (Model 1) and ViViT-B on Kinetics 400. As we pass
32-frame inputs to the network, sampling 8 frames and extracting tubelets of length t = 4 correspond to the same
number of tokens in both cases. Table 1 shows that tubelet
embedding initialised using the “central frame” method
(Eq. 9) performs well, outperforming the commonly-used
“ﬁlter inﬂation” initialisation method by 1.6%, and
“uniform frame sampling” by 0.7%. We therefore use this
encoding method for all subsequent experiments.
Model variants
We compare our proposed model variants (Sec. 3.3) across the Kinetics 400 and Epic Kitchens
datasets, both in terms of accuracy and efﬁciency, in Tab. 2.
In all cases, we use the “Base” backbone and tubelet size of
16 × 2. Model 2 (“Factorised Encoder”) has an additional
hyperparameter, the number of temporal transformers, Lt.
We set Lt = 4 for all experiments and show in Tab. 3 that
the model is not sensitive to this choice.
The unfactorised model (Model 1) performs the best
on Kinetics 400. However, it can also overﬁt on smaller
datasets such as Epic Kitchens, where we ﬁnd our “Factorised Encoder” (Model 2) to perform the best. We also
consider an additional baseline (last row), based on Model
2, where we do not use any temporal transformer, and simply average pool the frame-level representations from the
spatial encoder before classifying.
This average pooling
baseline performs the worst, and has a larger accuracy drop
Table 4: The effect of progressively adding regularisation (each
row includes all methods above it) on Top-1 action accuracy on
Epic Kitchens. We use a Factorised encoder model with tubelet
size 16 × 2.
Top-1 accuracy
Random crop, ﬂip, colour jitter
+ Kinetics 400 initialisation
+ Stochastic depth 
+ Random augment 
+ Label smoothing 
+ Mixup 
Input tubelet size
Top-1 Accuracy
Input tubelet size
(a) Accuracy
(b) Compute
Figure 7: The effect of the backbone architecture on (a) accuracy
and (b) computation on Kinetics 400, for the spatio-temporal attention model (Model 1).
Input tubelet size
Top-1 Accuracy
Input tubelet size
Spatio-temporal
Factorised encoder
Factorised self-attention
Factorised dot-product
(a) Accuracy
(b) Compute
Figure 8: The effect of varying the number of temporal tokens on
(a) accuracy and (b) computation on Kinetics 400, for different
variants of our model with a ViViT-B backbone.
on Epic Kitchens, suggesting that this dataset requires more
detailed modelling of temporal relations.
As described in Sec. 3.3, all factorised variants of our
model use signiﬁcantly fewer FLOPs than the unfactorised
Model 1, as the attention is computed separately over
spatial- and temporal-dimensions. Model 4 adds no additional parameters to the unfactorised Model 1, and uses the
least compute. The temporal transformer encoder in Model
2 operates on only nt tokens, which is why there is a barely
a change in compute and runtime over the average pooling baseline, even though it improves the accuracy substantially (3% on Kinetics and 4.9% on Epic Kitchens). Finally, Model 3 requires more compute and parameters than
the other factorised models, as its additional self-attention
block means that it performs another query-, key-, valueand output-projection in each transformer layer .
regularisation
Pure-transformer
architectures
such as ViT are known to require large training
datasets, and we observed overﬁtting on smaller datasets
like Epic Kitchens and SSv2, even when using an ImageNet
pretrained model. In order to effectively train our models
Table 5: The effect of spatial resolution on the performance of
ViViT-L/16x2 and spatio-temporal attention on Kinetics 400.
on such datasets, we employed several regularisation strategies that we ablate using our “Factorised encoder” model
in Tab. 4. We note that these regularisers were originally
proposed for training CNNs, and that have recently
explored them for training ViT for image classiﬁcation.
Each row of Tab. 4 includes all the methods from the
rows above it, and we observe progressive improvements
from adding each regulariser. Overall, we obtain a substantial overall improvement of 5.3% on Epic Kitchens. We
also achieve a similar improvement of 5% on SSv2 by using all the regularisation in Tab. 4. Note that the Kineticspretrained models that we initialise from are from Tab. 2,
and that all Epic Kitchens models in Tab. 2 were trained
with all the regularisers in Tab. 4. For larger datasets like
Kinetics and Moments in Time, we do not use these additional regularisers (we use only the ﬁrst row of Tab. 4),
as we obtain state-of-the-art results without them. The appendix contains hyperparameter values and additional details for all regularisers.
Varying the backbone
Figure 7 compares the ViViT-
B and ViViT-L backbones for the unfactorised spatiotemporal model. We observe consistent improvements in
accuracy as the backbone capacity increases. As expected,
the compute also grows as a function of the backbone size.
Varying the number of tokens
We ﬁrst analyse the performance as a function of the number of tokens along the
temporal dimension in Fig. 8. We observe that using smaller
input tubelet sizes (and therefore more tokens) leads to consistent accuracy improvements across all of our model architectures.
At the same time, computation in terms of
FLOPs increases accordingly, and the unfactorised model
(Model 1) is impacted the most.
We then vary the number of tokens fed into the model
by increasing the spatial crop-size from the default of 224
to 320 in Tab. 5. As expected, there is a consistent increase
in both accuracy and computation. We note that when comparing to prior work we consistently obtain state-of-the-art
results (Sec. 4.3) using a spatial resolution of 224, but we
also highlight that further improvements can be obtained at
higher spatial resolutions.
Varying the number of input frames
In our experiments
so far, we have kept the number of input frames ﬁxed at 32.
We now increase the number of frames input to the model,
thereby increasing the number of tokens proportionally.
Table 6: Comparisons to state-of-the-art across multiple datasets. For “views”, x × y denotes x temporal crops and y spatial crops. We
report the TFLOPs to process all spatio-temporal views. “FE” denotes our Factorised Encoder model.
(a) Kinetics 400
blVNet 
TSM-ResNeXt-101 
I3D NL 
CorrNet-101 
ip-CSN-152 
LGD-3D R101 
SlowFast R101-NL 
X3D-XXL 
TimeSformer-L 
ViViT-L/16x2 FE
ViViT-L/16x2 FE
Methods with large-scale pretraining
ip-CSN-152 (IG )
ViViT-L/16x2 FE (JFT)
ViViT-H/14x2 (JFT)
(b) Kinetics 600
AttentionNAS 
LGD-3D R101 
SlowFast R101-NL 
X3D-XL 
TimeSformer-L 
ViViT-L/16x2 FE
ViViT-L/16x2 FE (JFT)
ViViT-H/14x2 (JFT)
(c) Moments in Time
blVNet 
AssembleNet-101 
ViViT-L/16x2 FE
(d) Epic Kitchens 100 Top 1 accuracy
SlowFast 
ViViT-L/16x2 FE
(e) Something-Something v2
SlowFast 
TimeSformer-HR 
blVNet 
ViVIT-L/16x2 FE
Number of views
Top-1 Accuracy
32 stride 2
64 stride 2
128 stride 2
Figure 9: The effect of varying the number of frames input to the
network and increasing the number of tokens proportionally. We
use ViViT-L/16x2 Factorised Encoder on Kinetics 400. A Kinetics
video contains 250 frames (10 seconds sampled at 25 fps) and the
accuracy for each model saturates once the number of equidistant
temporal views is sufﬁcient to “see” the whole video clip. Observe how models processing more frames (and thus more tokens)
achieve higher single- and multi-view accuracy.
Figure 9 shows that as we increase the number of frames
input to the network, the accuracy from processing a single view increases, since the network incorporates longer
temporal context. However, common practice on datasets
such as Kinetics is to average results over multiple, shorter “views” of the same video clip. Figure 9 also
shows that the accuracy saturates once the number of views
is sufﬁcient to cover the whole video. As a Kinetics video
consists of 250 frames, and we sample frames with a stride
of 2, our model which processes 128 frames requires just a
single view to “see” the whole video and achieve its maximum accuarcy.
Note that we used ViViT-L/16x2 Factorised Encoder
(Model 2) here. As this model is more efﬁcient it can process more tokens, compared to the unfactorised Model 1
which runs out of memory after 48 frames using tubelet
length t = 2 and a “Large” backbone. Models processing
more frames (and thus more tokens) consistently achieve
higher single- and multi-view accuracy, in line with our observations in previous experiments (Tab. 5, Fig. 8). Moroever, observe that by processing more frames (and thus
more tokens) with Model 2, we are able to achieve higher
accuracy than Model 1 (with fewer total FLOPs as well).
Finally, we observed that for Model 2, the number of
FLOPs effectively increases linearly with the number of input frames as the overall computation is dominated by the
initial Spatial Transformer. As a result, the total number
of FLOPs for the number of temporal views required to
achieve maximum accuracy is constant across the models.
In other words, ViViT-L/16x2 FE with 32 frames requires
995.3 GFLOPs per view, and 4 views to saturate multi-view
accuracy. The 128-frame model requires 3980.4 GFLOPs
but only a single view. As shown by Fig. 9, the latter model
achieves the highest accuracy.
4.3. Comparison to state-of-the-art
Based on our ablation studies in the previous section,
we compare to the current state-of-the-art using two of our
model variants. We primarily use our Factorised Encoder
model (Model 2), as it can process more tokens than Model
1 to achieve higher accuracy.
Tables 6a and 6b show that our spatio-temporal
attention models outperform the state-of-the-art on Kinetics
400 and 600 respectively. Following standard practice, we
take 3 spatial crops (left, centre and right) 
for each temporal view, and notably, we require signiﬁcantly fewer views than previous CNN-based methods.
We surpass the previous CNN-based state-of-the-art using ViViT-L/16x2 Factorised Encoder (FE) pretrained on
ImageNet, and also outperform who concurrently proposed a pure-transformer architecture. Moreover, by initialising our backbones from models pretrained on the larger
JFT dataset , we obtain further improvements.
Although these models are not directly comparable to previous work, we do also outperform who pretrained on
the large-scale, Instagram dataset . Our best model uses
a ViViT-H backbone pretrained on JFT and signiﬁcantly advances the best reported results on Kinetics 400 and 600 to
84.9% and 85.8%, respectively.
Moments in Time
We surpass the state-of-the-art by a
signiﬁcant margin as shown in Tab. 6c. We note that the
videos in this dataset are diverse and contain signiﬁcant label noise, making this task challenging and leading to lower
accuracies than on other datasets.
Epic Kitchens 100
Table 6d shows that our Factorised
Encoder model outperforms previous methods by a signiﬁcant margin. In addition, our model obtains substantial improvements for Top-1 accuracy of “noun” classes, and the
only method which achieves higher “verb” accuracy used
optical ﬂow as an additional input modality . Furthermore, all variants of our model presented in Tab. 2 outperformed the existing state-of-the-art on action accuracy.
We note that we use the same model to predict verbs and
nouns using two separate “heads”, and for simplicity, we do
not use separate loss weights for each head.
Something-Something v2 (SSv2)
Finally, Tab. 6e shows
that we achieve state-of-the-art Top-1 accuracy with our
Factorised encoder model (Model 2), albeit with a smaller
margin compared to previous methods. Notably, our Factorised encoder model signiﬁcantly outperforms the concurrent TimeSformer method by 2.9%, which also proposes
a pure-transformer model, but does not consider our Factorised encoder variant or our additional regularisation.
SSv2 differs from other datasets in that the backgrounds
and objects are quite similar across different classes, meaning that recognising ﬁne-grained motion patterns is necessary to distinguish classes from each other. Our results suggest that capturing these ﬁne-grained motions is an area of
improvement and future work for our model. We also note
an inverse correlation between the relative performance of
previous methods on SSv2 (Tab. 6e) and Kinetics (Tab. 6a)
suggesting that these two datasets evaluate complementary
characteristics of a model.
5. Conclusion and Future Work
We have presented four pure-transformer models for
video classiﬁcation, with different accuracy and efﬁciency
proﬁles, achieving state-of-the-art results across ﬁve popular datasets.
Furthermore, we have shown how to effectively regularise such high-capacity models for training
on smaller datasets and thoroughly ablated our main design choices. Future work is to remove our dependence on
image-pretrained models. Finally, going beyond video classiﬁcation towards more complex tasks is a clear next step.