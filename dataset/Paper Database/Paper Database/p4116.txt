Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1–17
December 7-11, 2022 ©2022 Association for Computational Linguistics
Generative Knowledge Graph Construction: A Review
Hongbin Ye1,2, Ningyu Zhang1,2 ∗, Hui Chen 3, Huajun Chen1,2
1 Zhejiang University & AZFT Joint Lab for Knowledge Engine
2 Hangzhou Innovation Center, Zhejiang University
3 Alibaba Group
{yehongbin,zhangningyu,huajunsir}@zju.edu.cn, 
Generative Knowledge Graph Construction
(KGC) refers to those methods that leverage the
sequence-to-sequence framework for building
knowledge graphs, which is flexible and can be
adapted to widespread tasks. In this study, we
summarize the recent compelling progress in
generative knowledge graph construction. We
present the advantages and weaknesses of each
paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest
promising research directions for the future.
Our contributions are threefold: (1) We present
a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical
and empirical analysis of the generative KGC
methods; (3) We propose several research directions that can be developed in the future.
Introduction
Knowledge Graphs (KGs) as a form of structured
knowledge have drawn significant attention from
academia and the industry . However, high-quality KGs rely almost exclusively on
human-curated structured or semi-structured data.
To this end, Knowledge Graph Construction (KGC)
is proposed, which is the process of populating (or
building from scratch) a KG with new knowledge
elements (e.g., entities, relations, events). Conventionally, KGC is solved by employing task-specific
discriminators for the various types of information
in a pipeline manner , typically including (1) entity discovery or named entity recognition , (2) entity linking ,
(3) relation extraction and (4)
event extraction . However,
this presents limitations of error population and
poor adaptability for different tasks.
∗Corresponding author.
Tags:    O    B-CP-1  E-CP-1      O     B-CP-2    E-CP-2     O            O        O
(a) Classification Model
The [United States]E-loc President [Joe Biden]E-per visited [Samsung]E-Org .
(b) Tagging Model
(c) Generation Model
Country-President
Extracted Results
{United states, Country-President, Joe Biden}
Input Text: The    United States President  Joe       Biden visited Samsung   .
Final Results:
{United states, Country-President, Joe Biden}
Input Text: The    United States President  Joe Biden visited Samsung   .
Seq2Seq Text: <triplet> United States <subj> Joe Biden <obj> Country-President.
{United states, Country-President, Joe Biden}
Delinearization
Figure 1: Discrimination and generation methodologies
for relation extraction. “Country-President” is the relation, and “CP” is short for “Country-President.”
Generative Knowledge Graph Construction.
Some generative KGC methods based on the
sequence-to-sequence (Seq2Seq) framework are
proposed to overcome this barrier. Early work
 has explored using the generative paradigm to solve different entity and relation extraction tasks. Powered by fast advances
of generative pre-training such as T5 , and BART , Seq2Seq
paradigm has shown its great potential in unifying
widespread NLP tasks. Hence, more generative
KGC works have been proposed, showing appealing performance in benchmark datasets. Figure 1 illustrates an example of generative KGC for
relation extraction. The target triple is preceded
by the tag <triple>, and the head entity, tail entity,
and relations are also specially tagged, allowing the
structural knowledge (corresponding to the output)
to be obtained by inverse linearization. Despite the
success of numerous generative KGC approaches,
these works scattered among various tasks have not
been systematically reviewed and analyzed.
Present work
In this paper, we summarize recent progress in generative KGC (An timeline of
generative KGC can be found in Appendix A) and
maintain a public repository for research convenience1. We propose to organize relevant work by
the generation target of models and also present the
axis of the task level (Figure 3):
• Comprehensive review with new taxonomies. We conduct the first comprehensive
review of generative KGC together with new
taxonomies. We review the research with different generation targets for KGC with a comprehensive comparison and summary (§3).
• Theoretical insight and empirical analysis.
We provide in-depth theoretical and empirical analysis for typical generative KGC methods, illustrating the advantages and disadvantageous of different methodologies as well as
remaining issues (§4).
• Wide coverage on emerging advances and
outlook on future directions. We provide
comprehensive coverage of emerging areas,
including prompt-based learning. This review
provides a summary of generative KGC and
highlights future research directions (§5).
Related work
As this topic is relatively nascent,
only a few surveys exist. Closest to our work, Ji
et al. covers methods for knowledge graph
construction, representation learning, and applications, which mainly focus on general methods for
KGC. Zhu et al. provides a systematic survey for multi-modal knowledge graph construction
and review the challenges, progress, and opportunities. For general NLP, Min et al. survey
recent work that uses these large language models to solve tasks via text generation approaches,
which has overlaps in generation methodologies
for information extraction. Different from those
surveys, in this paper, we conduct a literature review on generative KGC, hoping to systematically
understand the methodologies, compare different
methods and inspire new ideas.
1 
Construction_Papers
Knowledge Graph Completion
Entity Linking
Event Extraction
Relation Extraction
Named Entity Recognition
Blank-based Sequence
Indice-based Sequence
Label-based Sequence
Structure-based Sequence
Copy-based Sequence
Figure 2: Sankey diagram of knowledge graph construction tasks with different generative paradigms.
Preliminary on Knowledge Graph
Construction
Knowledge Graph Construction
Knowledge Graph Construction mainly aims to
extract structural information from unstructured
texts, such as Named Entity Recognition (NER)
 , Relation Extraction (RE)
 , Event Extraction (EE) , Entity Linking (EL) , and Knowledge Graph Completion .
Generally, KGC can be regarded as structure prediction tasks, where a model is trained to approximate a target function F(x) →y, where x ∈X
denotes the input data and y ∈Y denotes the output
structure sequence. For instance, given a sentence,
"Steve Jobs and Steve Wozniak co-founded Apple
in 1977.":
Named Entity Recognition aims to identify the
types of entities, e.g., ‘Steve Job’, ‘Steve Wozniak’
⇒PERSON, ‘Apple’ ⇒ORG;
Relation Extraction aims to identify the relationship of the given entity pair ⟨Steve Job, Apple⟩as
Event Extraction aims to identify the event type
as Business Start-Org where ‘co-founded’ triggers the event and (Steve Jobs, Steve Wozniak) are
participants in the event as AGENT and Apple as
ORG respectively.
Entity Linking aims to link the mention Steve Job
to Steven Jobs (Q19837) on Wikidata, and Apple
to Apple (Q312) as well.
Knowledge Graph Completion aims to complete
incomplete triples ⟨Steve Job, create, ?⟩for blank
entities Apple, NeXT Inc. and Pixar.
Generative KGC Taxonomy
Generation
Copy-based
CopyRE , CopyRRL , CopyMTL , TEMPGEN ,
Seq2rel 
Structure-based
Seq2Seq4ATE , Nested-seq , CGT , PolicyIE
 , Text2Event , HySPA , REBEL , SQUIRE
 , GenKGC , EPGEL , HuSe-Gen , UIE ,
DEEPSTRUCT , De-Bias , KGT5 , KG-S2S 
Label-based
ANL , GENRE , TANL 
Indice-based
PNDec , SEQ2SEQ-PTR , GRIT , UGF for NER
 , UGF for ABSA 
Blank-based
COMET , BART-Gen , GTT , DEGREE , ClarET
 , GTEE , X-GEAR , PAIE 
Generation
Named Entity
Recognition
Nested-seq , ANL , TANL , HySPA ,
UGF for NER , DEEPSTRUCT , De-Bias , UIE 
Relation Extraction
CopyRE , CopyRRL , PNDec , CopyMTL ,
CGT , TANL , HySPA , TEMPGEN
 , REBEL , DEEPSTRUCT , UIE ,
Seq2rel 
Event Extraction
CGT , TANL , BART-Gen , GTT ,
GRIT , Text2Event , DEGREE , ClarET , GTEE
 , X-GEAR , DEEPSTRUCT , PAIE , UIE
 
Entity Linking
GENRE , EPGEL 
Knowledge Graph
Completion
COMET , SQUIRE , GenKGC ,, HuSe-Gen ,
ClarET , KGT5 , KG-S2S 
Figure 3: Taxonomy of Generative Knowledge Graph Construction.
Discrimination and Generation
Methodologies
In this section, we introduce the background of
discrimination and generation methodologies for
KGC. The goal of the discrimination model is to
predict the possible label based on the characteristics of the input sentence. As shown in Figure 1,
given annotated sentence x and a set of potentially
overlapping triples tj = {(s, r, o)} in x, we aim to
maximize the data likelihood during the training
pcls(t|x) =
(s,r,o)∈tj
p ((s, r, o) | xj)
Another method of discrimination is to output
tags using sequential tagging for each position i
 . As shown in
Figure 1, for an n-word sentence x, n different tag
sequences are annotated based on "BIESO" (Begin,
Inside, End, Single, Outside) notation schema. The
size of a set of pre-defined relations is |R|, and
the related role orders are represented by "1" and
"2". During the training model, we maximize the
log-likelihood of the target tag sequence using the
hidden vector hi at each position i:
ptag(y | x) =
exp(hi, yi)
y′∈R exp (exp(hi, y′
For the generation model, if x is the input sentence and y the result of linearized triplets, the target for the generation model is to autoregressively
generate y given x:
pgen(y | x) =
pgen (yi | y<i, x)
By fine-tuning seq2seq model , T5 , and BART
 ) on such a task, using the crossentropy loss, we can maximize the log-likelihood
of the generated linearized triplets.
Advantages of the Generation Methods
While the previous discriminative methods extracts relational
triples from unstructured text according to a predefined schema to efficiently construct large-scale
knowledge graphs, these elaborate models focus on
solving a specific task of KGC, such as predicting
relation and event information from a segment of
input text which often requires multiple models to
process. The idea of formulating KGC tasks as
sequence-to-sequence problems 
will be of great benefit to develop a universal architecture to solve different tasks, which can be free
from the constraints of dedicated architectures, isolated models, and specialized knowledge sources.
News of the list’s existence unnerved
officials in Khartoum, Sudan ’s capital.
capital, Sudan, Khartoum, contains, Sudan, Khartoum
Copied entity
Predicted relation
Generation Model
Figure 4: Copy-based Sequence.
In addition, generative models can be pre-trained in
multiple downstream tasks by structurally consistent linearization of the text, which facilitates the
transition from traditional understanding to structured understanding and increases knowledge sharing . In contexts with nested
labels in NER , the proposed
generative method implicitly models the structure
between named entities, thus avoiding the complex multi-label mapping. Extracting overlapping
triples in RE is also difficult to handle for traditional discriminative models, Zeng et al. 
introduce a fresh perspective to revisit the RE task
with a general generative framework that addresses
the problem by end-to-end model. In short, new
directions can be explored for some hard-to-solve
problems through paradigm shifts.
Note that the discriminative and generative methods are not simply superior or inferior due to the
proliferation of related studies. The aim of this paper is to summarize the characteristics of different
generative paradigms in KGC tasks and provide a
promising perspective for future research.
Taxonomy of Generative Knowledge
Graph Construction
In this paper, we mainly consider the following five
paradigms that are widely used in KGC tasks based
on generation target, i.e. copy-based Sequence,
structure-linearized Sequence, label-augmented Sequence, indice-based Sequence, and blank-based
Sequence. As shown in Figure 2, these paradigms
have demonstrated strong dominance in many
mainstream KGC tasks. In the following sections,
we introduce each paradigm as shown in Figure 3.
Copy-based Sequence
This paradigm refers to developing more robust
models to copy the corresponding token (entity) directly from the input sentence during the generation
process. Zeng et al. designs an end-to-end
model based on a copy mechanism to solve the
The man returned to Los Angeles from Mexico following
his capture Tuesday by bounty hunters.
Generation Model
((Transport returned (Artifact The man) (Destination Los
Angeles) (Origin Mexico))
(Arrest-Jail capture (Person The man) (Time Tuesday)
(Agent bounty hunters))
Arrest-Jail
Destination
Los Angeles
Event Schema
Figure 5: Structure-linearized Sequence.
triple overlapping problem. As shown in Figure 4,
the model copies the head entity from the input sentence and then the tail entity. Similarly, relations
are generated from target vocabulary, which is restricted to the set of special relation tokens. This
paradigm avoids models generating ambiguous or
hallucinative entities. In order to identify a reasonable triple extraction order, Zeng et al. 
converts the triplet generation process into a reinforcement learning process, enabling the copy
mechanism to follow an efficient generative order.
Since the entity copy mechanism relies on unnatural masks to distinguish between head and tail
entities, Zeng et al. maps the head and tail
entities to fused feature space for entity replication
by an additional nonlinear layer, which strengthens
the stability of the mechanism. For document-level
extraction, Huang et al. proposes a TOPk copy mechanism to alleviate the computational
complexity of entity pairs.
Structure-linearized Sequence
This paradigm refers to utilizing structural knowledge and label semantics, making it prone to handling a unified output format. Lu et al. proposes an end-to-end event extraction model based
on T5, where the output is a linearization of the
extracted knowledge structure as shown in Figure 5.
In order to avoid introducing noise, it utilizes the
event schema to constrain decoding space, ensuring
the output text is semantically and structurally legitimate. Lou et al. reformulates event detection as a Seq2Seq task and proposes a Multi-Layer
Bidirectional Network (MLBiNet) to capture the
document-level association of events and semantic
information simultaneously. Besides, Zhang et al.
 ; Ye et al. introduce a contrastive
learning framework with a batch dynamic attention
masking mechanism to overcome the contradiction
in meaning that generative architectures may produce unreliable sequences . Similarly, Cabot and Navigli employs a simple
Tolkien’s epic novel The Lord of the Rings was published
in 1954-1955, years after the book was completed.
Generation Model
[ Tolkien | person ]’s epic novel [ The Lord of the Rings |
book | author = Tolkien ] was published in 1954-1955,
years after the book was completed.
The Lord of the Rings
Figure 6: Label-augmented Sequence.
triplet decomposition method for the relation extraction task, which is flexible and can be adapted
to unified domains or longer documents.
In the nested NER task, Straková et al. 
proposes a flattened encoding algorithm, which
outputs multiple NE tags following the BILOU
scheme. The multi-label of a word is a concatenation of all intersecting tags from highest priority
to lowest priority. Similarly, Zhang et al. 
eliminates the incorrect biases in the generation
process according to the theory of backdoor adjustment. In EL task, Cao et al. proposes
Generative ENtity REtrieval (GENRE) in an autoregressive fashion conditioned on the context, which
captures fine-grained interactions between context
and entity name. Moreover, Wang et al. ;
Lu et al. extends the domain to structural
heterogeneous information extraction by proposing
a unified task-agnostic generation framework.
Label-augmented Sequence
This paradigm refers to utilizing the extra markers to indicate specific entities or relationships. As
shown in Figure 6, Athiwaratkun et al. 
investigates the label-augmented paradigm for various structure prediction tasks. The output sequence
copies all words in the input sentence, as it helps
to reduce ambiguity. In addition, this paradigm
uses square brackets or other identifiers to specify the tagging sequence for the entity of interest.
The relevant labels are separated by the separator
"|" within the enclosed brackets. Meanwhile, the
labeled words are described with natural words
so that the potential knowledge of the pre-trained
model can be leveraged . Similarly, Athiwaratkun et al. naturally combines tag semantics and shares knowledge across
multiple sequence labeling tasks. To retrieve entities by generating their unique names, Cao et al.
 extends the autoregressive framework to
capture the relations between context and entity
Anti-Ethiopia riots erupted in Mogadishu , the capital of
Somalia , on Friday , while masked gunmen emerged ...
Generation Model
Somalia Mogadishu
Somalia Mogadishu
/location/location/contains /location/country/capital
/location/location/contains
/location/country/capital
Figure 7: Indice-based Sequence.
name by effectively cross-encoding both. Since the
length of the gold decoder targets is often longer
than the corresponding input length, this paradigm
is unsuitable for document-level tasks because a
great portion of the gold labels will be skipped.
Indice-based Sequence
This paradigm generates the indices of the words
in the input text of interest directly and encodes
class labels as label indices. As the output is strictly
restricted, it will not generate indices that corresponding entities do not exist in the input text,
except for relation labels. Nayak and Ng 
apply the method to the relation extraction task, enabling the decoder to find all overlapping tuples
with full entity names of different lengths. As
shown in Figure 7, given the input sequence x,
the output sequence y is generated via the indices:
y = [b1, e1, t1, . . . , bi, ei, ti, . . . , bk, ek, tk] where
bi and ei indicates the begin and end indices of a
entity tuple, ti is the index of the entity type, and k
is the number of entity tuples. The hidden vector
is computed at decoding time by the pointer network to get the representation
of the tuple indices. Besides, Yan et al. 
explores the idea of generating indices for NER,
which can be applied to different settings such as
flat, nested, and discontinuous NER. In addition,
Du et al. applies the method to a role-filler
entity extraction task by implicitly capturing noun
phrase coreference structure.
Blank-based Sequence
This paradigm refers to utilizing templates to
define the appropriate order and relationship for
the generated spans. Du et al. explores a
blank-based form for event extraction tasks which
includes special tokens representing event information such as event types. Li et al. frames
document-level event argument extraction as con-
Generation Model
Document: Elliott testified that on April 15, McVeigh came into the
body shop and <tgr> reserved <tgr> the truck, to be picked up at 4pm
two days later ...
Template: <arg1> bought, sold, or traded <arg3> to <arg2> in
exchange for <arg4> for the benefit of <arg5> at <arg6> place.
Elliott bought, sold or traded truck to McVeigh in exchange for $280.32 for
the benefit of at body shop place.
Blank Extraction
Figure 8: Blank-based Sequence.
ditional generation given a template and introduces
the new document-level informative to aid the generation process. As shown in Figure 8, the template
refers to a text describing an event type, which adds
blank argument role placeholders. The output sequences are sentences where the blank placeholders
are replaced by specific event arguments. Besides,
Hsu et al. focuses on low-resource event extraction and proposes a data-efficient model called
DEGREE, which utilizes label semantic information.
Huang et al. designs a languageagnostic template to represent the event argument
structures, which facilitate the cross-lingual transfer. Instead of conventional heuristic threshold
tuning, Ma et al. proposes an effective yet
efficient model PAIE for extracting multiple arguments with the same role.
Comparison and Discussion
Recently, the literature on generative KGC has
been growing rapidly. A unifying theme across
many of these methods is that of end-to-end architecture or the idea that the knowledge extraction
can be redefined as text sequence to structure generation task. Generative models can decode and
control extraction targets on demand for different
specific tasks, scenarios, and settings (i.e., different
schema). However, due to the different forms of
specific KGC tasks, there is still some disagreement
in the utilization of the generation paradigms.
As shown in Table 1, we make a comprehensive
comparison among the paradigms mentioned above
via rating based on different evaluation scopes: 1)
Semantic utilization refers to the degree to which
the model leverages the semantics of the labels.
In principle, we believe that the closer the output form is to natural language, the smaller the
gap between the generative model and the training
task. We observe that the blank-based paradigm
has a clear advantage in this scope, which uses
manually constructed templates to make the output
close to natural language fluency. 2) Search space
refers to the vocabulary space searched by the decoder. Due to the application of the constraint decoding mechanism, some structure-based methods
can be reduced to the same decoding space as the
copy-based methods. In addition, the indice-based
paradigm uses a pointer mechanism that constrains
the output space to the length of the input sequence.
3) Application scope refers to the range of KGC
tasks that can be applied. We believe that architectures with the ability to organize information more
flexibly have excellent cross-task migration capabilities such as structure-based, label-based and
blank-based paradigms. 4) Template cost refers to
the cost of constructing the input and golden output text. We observe that most paradigms do not
require complex template design and rely only on
linear concatenation to meet the task requirement.
However, the blank-based paradigm requires more
labor consumption to make the template conform
to the semantic fluency requirement.
Totally in line with recent trends in NLP, a growing number of unified generation strategies require
more universal architectures , as they allow a remarkable degree
of output flexibility. We think that future research
should focus on unifying cross-task models and
further improving decoding efficiency.
Theoretical Insight
This section provides theoretical insight into optimization and inference for generative KGC. For optimization, NLG are normally modeled by parameterized probabilistic models pgen over text strings
y = ⟨y1, y2, . . . ⟩decomposed by words yt:
pgen(y | x) =
pgen (yi | y<i, x)
where y consists of all possible strings that can
be constructed from words in the model’s vocabulary V. Note that the output y can take on a variety of forms depending on the task, e.g., entities,
relational triples, or an event structure. Usually,
the model will limit the target set by pre-defined
schema as YT ⊂Y. The optimization procedure
will be taken to estimate the parameters with loglikelihood maximization as follows:
L(θ; T ) = −
Generative Strategy
Representative Model
Evaluation Scope
Copy-based (§ 3.1)
Directly copy entity
CopyRE 
Restricted target vocabulary
Seq2rel 
Structure-based (§ 3.2)
Per-token tag encoding
Nested-seq 
Faithful contrastive learning
CGT 
Prefix tree constraint decoding
TEXT2EVENT 
Triplet linearization
REBEL 
Entity-aware hierarchical decoding
GenKGC 
Unified structure generation
UIE 
Reformulating triple prediction
DEEPSTRUCT 
Query Verbalization
KGT5 
Label-based (§ 3.3)
Augmented natural language
TANL 
Indice-based (§ 3.4)
Pointer mechanism
PNDec 
Pointer selection
GRIT 
Blank-based (§ 3.5)
Template filling as generation
GTT 
Prompt semantic guidance
DEGREE 
Language-agnostic template
X-GEAR 
Table 1: Comparison of generation methods from different evaluation scopes. "SU" indicates semantic utilization,
"SS" indicates search space, "AS" indicates application scope, and "TS" indicates template cost. We divide the
degree into three grades: L (low), M (middle), and H (high), and the ↑indicates that the higher grade performance is
better while the ↓is the opposite.
where θ are the model parameters. Notably, with
small output space (e.g., methods with the indicebased sequence in §3.4), the model can converge
faster. However, the model with a small output
space may fail to utilize rich semantic information
from labels or text (like models in §3.5). In short,
the design of output space is vital for generative
KGC, and it is necessary to balance parametric
optimization as well as semantic utilization.
For inference, we argue that sequence decoding
in the generation is an essential procedure for generative KGC. Given the probabilistic nature of q,
the decoding process will select words that maximize the probability of the resulting string. Vanilla
decoding solutions such as beam search or greedy
have been investigated in generative KGC. On the
one hand, knowledge-guided (or schema-guided)
decoding has become the mainstay for many generative KGC tasks. For example, Lu et al. 
proposes Text2Event in which words are decoded
through a prefix tree based on pre-defined schema.
On the other hand, non-autoregressive parallel decoding has also been leveraged for generative KGC.
Sui et al. formulates end-to-end knowledge
base population as a direct set generation problem,
avoiding considering the order of multiple facts.
Note that the decoding mechanism plays a vital
role in inference speed and quality. We argue that
it is necessary to develop sophisticated, efficient
decoding strategies (e.g., with guidance from KG)
for generative KGC.
Empirical Analysis
To investigate the effect of different generation
methods, we conduct an analysis of the experimental results of existing generative KGC work.
Due to space limitations of the article, we only
select two representative tasks of entity/relation extraction and event extraction with NYT and ACE
datasets2. Table 2 shows the performance of discrimination models and generative models on the
NYT datasets. We can observe that: 1) Structurebased and label-based methods both achieve similar extraction performance compared with all discrimination models on NYT datasets. We believe
this is because they can better utilize label semantics and structural knowledge than other generation
methods. 2) Although the discrimination methods obtain good performance, the performance of
the generation methods has been improved more
vastly in recent years, so we have reason to believe
that they will have greater application scope in the
near future. In addition, we also show the performance of the non-autoregressive method on two
datasets, and we discuss the promising value of this
method in § 5. We observe that parallel generation
of the unordered triple set can obtain comparable
2Results are taken from existing papers.
Discrimination
CasRel 
TPLinker 
OneRel 
Copy-based
CopyRE 
CopyRRL 
CopyMTL 
Structure-based
CGT 
REBEL 
UIE 
DEEPSTRUCT 
Label-based
TANL 
Indice-based
PNDec 
SPN 
Seq2UMTree 
Table 2: Main results of NYT dataset. The top section refers to the discrimination models, and the bottom section
indicates generation models. "*" refers to the non-autoregressive models.
Discrimination
JMEE 
DYGIE++ 
OneIE 
QAEE 
MQAEE 
RCEE 
Structure-based
TEXT2EVENT 
UIE 
DEEPSTRUCT 
Label-based
TANL 
Blank-based
BART-Gen 
DEGREE 
GTEE 
PAIE 
Table 3: F1 results (%) of ACE-2005. The top section refers to the discrimination models, and the bottom section
indicates the generation models. Id is Identification, and Cl is Classification. "*" refers to experiments only in
argument extraction tasks with the golden trigger.
performance with advanced discriminative models,
noting that non-autoregressive methods have better
decoding efficiency and training efficiency.
From Table 3, we observe that generation methods can obtain comparable performance compared
with discrimination models on event extraction
tasks. Since the framework of event extraction has
a hierarchical structure (i.e., it is usually decomposed into two subtasks: trigger extraction and argument extraction), structure-based methods have
a supervised learning framework for the sequenceto-structure generation, while schema constraints
guarantee structural and semantic legitimacy. In
addition, owing to the complete template design of
the Blank-based approach, PLMs can understand
complex task knowledge, structural knowledge of
the extraction framework, and label semantics in a
natural language manner.
Future Directions
Though lots of technical solutions have been proposed for generative KGC as surveyed, there remain some potential directions:
Generation Architecture. Most of the recent
generative KGC frameworks face serious homogenization with Transformer. For enhancing interpretability, we argue that neuro-symbolic models
(i.e., a reasoning system that integrates neural and
symbolic) can be designed for generative
KGC. In addition, some cutting-edge technologies
such as spiking neural network , dynamic neural networks , ordinary differential equations and diffusion models can also provide promising architectures.
Generation Quality. Considering the target reliability of generation methods, more sophisticated
strategies can be leveraged to control the quality of
generative KGC, including: 1) Control code construction ;
2) Decoding strategy such as introducing external
feedback and generative
discriminator ; 3) Loss function design ; 4) Prompt design
 ; 5) Retrieval
augmentation ; 6) Write-then-Edit
strategy ; 7) Diffusion process .
Training Efficiency. In practical applications, it
is essential to reduce data annotation and training
costs. One idea is to freeze most of the generation model parameters or leverage prompt
learning . Another idea is that
knowledge decoupling intervention training models can reduce parameter redundancy .
Universal Deployment. Inspired by the T5 , which transforms all NLP tasks
into Text-to-Text tasks, generation models can be
generalized to the multi-task and multi-modal domain. Therefore, instead of improvements being
prone to be exclusive to a single task, domain, or
dataset, we argue that it is beneficial to study the
framework to advocate for a unified view of KGC,
such as the wonderful work UIE .
Furthermore, it is efficient for real-world deployment when we can provide a single model to support widespread KGC tasks .
Inference Speed. To be noted, although previous work has treated KGC as end-to-end generative
tasks, they are still limited by auto-regressive decoders. However, the autoregressive decoder generates each token based on previously generated
tokens during inference, and this process is not parallelizable. Therefore, it is beneficial to develop
a fast inference model for generative KGC. Previously, Sui et al. utilizes the transformerbased non-autoregressive decoder 
as a triple set generator that can predict all triples
at once. Sui et al. also formulates end-toend knowledge base population as a direct set generation problem. Zhang et al. proposes
a two-dimensional unordered multitree allowing
prediction deviations not to aggregate and affect
other triples. To sum up, the non-autoregressive
approach applied to KGC proves to be effective in
solving the exposure bias and overfitting problems.
Likewise, the semi-autoregressive decoding preserves the autoregressive approach
within the block to ensure consistency while improving the tuple output efficiency. Additionally,
pathways can dynamically
assign competencies to different parts of the neural
network, which is faster and more efficient as it
does not activate the entire network for each task.
Conclusion and Vision
In this paper, we provide an overview of generative
KGC with new taxonomy, theoretical insight and
empirical analysis, and several research directions.
Note that the generative paradigm for KGC has
the potential advantages of unifying different tasks
and better utilizing semantic information. In the
future, we envision a more potent synergy between
the methodologies from the NLG and knowledge
graph communities. We hope sophisticated and
efficient text generation models to be increasingly
contributed to improving the KGC performance.
On the converse, we expect symbolic structure in
KG can have potential guidance for text generation.
Limitations
In this study, we provide a review of generative
KGC. Due to the page limit, we cannot afford the
technical details for models. Moreover, we only
review the works within five years, mainly from the
ACL, EMNLP, NAACL, COLING, AAAI, IJCAI,
etc. We will continue adding more related works
with more detailed analysis.
Acknowledgment
We want to express gratitude to the anonymous
reviewers.
This work was supported by the
National Natural Science Foundation of China
(No.62206246, 91846204 and U19B2027), Zhejiang Provincial Natural Science Foundation of
China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), and Yongjiang Talent
Introduction Programme (2021A-156-G).