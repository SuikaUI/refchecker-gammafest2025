Communicated by Terrence Sanger
Probabilistic Interpretation of Population Codes
Richard S. Zemel
Departments of Psychology and Computer Science, University of Arizona,
Tucson, AZ 85721, U.S.A.
Peter Dayan
Department of Brain and Cognitive Sciences, MIT, Cambridge, MA 02139, U.S.A.
Alexandre Pouget
Georgetown Institute for Cognitive and Computational Sciences,
Georgetown University, Washington, DC 20007-2197, U.S.A.
We present a general encoding-decoding framework for interpreting the
activity of a population of units. A standard population code interpretation method, the Poisson model, starts from a description as to how a
single value of an underlying quantity can generate the activities of each
unit in the population. In casting it in the encoding-decoding framework,
we ﬁnd that this model is too restrictive to describe fully the activities of
units in population codes in higher processing areas, such as the medial
temporal area. Under a more powerful model, the population activity
can convey information not only about a single value of some quantity
but also about its whole distribution, including its variance, and perhaps
even the certainty the system has in the actual presence in the world of the
entity generating this quantity. We propose a novel method for forming
such probabilistic interpretations of population codes and compare it to
the existing method.
1 Introduction
Populationcodes,whereinformationisrepresentedintheactivitiesofwhole
populations of units, are ubiquitous in the brain. There has been substantial
work on how animals should or actually do extract information about the
underlying encoded quantity . With the exception of Anderson
 , most of this work has concentrated on the case of extracting a single
value for this quantity. In this article, we study ways of characterizing the
joint activity of a population as coding a whole probability distribution over
the underlying quantity.
We will use two main motivating examples throughout this article, both
Neural Computation 10, 403–430 
c⃝1998 Massachusetts Institute of Technology
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
of them well-studied, classic examples of population codes. The ﬁrst is place
cells in hippocampus of freely moving rats ,
which tend to ﬁre when the animal is at a particular part of an environment.
The second example is that of motion-selective cells in the medial temporal
(MT) area of monkeys that are reporting aspects of the motion in a stochastic
stimulus made up of dots moving in various directions . MT cells are selective for particular directions of motion
and are well activated by such random displays provided that some of the
dots are moving in the directions that they prefer.
In these cases, treating the activity of such populations of cells as reporting on a single value of the variables they code (e.g., direction of motion) is
inadequate. Instead, these are examples of two general situations in which
the population must be interpreted as coding probability distributions over
these variables:
1. Insufﬁcient information exists to deﬁne a single value with certainty.
The rat may not have enough information from visual or vestibular
cues to know exactly where it is. This article discusses how to make
statistical sense of the obvious notion that if the animal is confused
whether it is in place x1 or place x2, then the place cells that prefer both
places should be activated.1 Hinton pointed out that one should
be able to use patterns of activity over such population codes to report
not only a single place in the world, but also variance and uncertainty
about that place and other aspects of a probability distribution.
2. Multiple values underlie the input. The monkey may have to report
the direction of coherent motion embedded in a ﬁeld of random motion noise. MT cells cannot be characterized as only reporting on the
direction of coherent motion, since cells that prefer directions opposite to this are activated by the noise. The population must therefore
be characterized as reporting something about the entire distribution
of inputs.
In this article, we ﬁrst provide a general statistical framework that can
be used to understand how the activity of a population of neurons can be
consideredasencodinginformationabouttheworldand,concomitantly,the
way that this information can be decoded from the activity. We illustrate the
framework by casting the standard model for population codes in its terms,
use it to show why this model is inadequate for representing probability
distributions even though it does contain some distributional information,
and describe an existing model for probabilistic interpretation in terms of the framework.
In section 2 we deﬁne the encoding and decoding framework and exam-
1 As far as we are aware, there are as yet no data on how the place cells actually ﬁre in
such ambiguous cases.
Probabilistic Population Codes
ine existing models for population codes in its terms. In section 3 we propose
a novel method for encoding and decoding probability distributions from
population codes. In section 4 we present empirical comparisons between
the alternative methods.
2 Population Code Interpretations
2.1 The Encoding-Decoding Framework. The starting point for almost
all work on neural population codes is the neurophysiological ﬁnding that
many neurons respond to a particular variable underlying a stimulus (such
as the orientation of a visually presented bar) according to a unimodal tuning function. This function is read out as the mean ﬁring rate of the cell
and is often reasonably well characterized as a gaussian. The value of the
underlying variable at which the peak of the tuning function occurs (the
mean of the gaussian) is called the preferred value for the cell. This form of
response characterizes not only cells near the sensory periphery, but also
cells that report the results of more complex processing, including receiving
information from groups of cells that themselves have these tuning properties (in MT, for instance). A major caveat with almost all work on population
codes, including that in this article, is that the responses of cells vary with
many quantities in the world other than the particular one that is usually
studied. For example, many MT cells are also selective for disparity and can
be affected by spatial frequency, making it difﬁcult to ascribe weak ﬁring
to nonpreferred motion, or an incorrect disparity, or something else. In our
theory, we assume that we know the (collection of) variables with respect
to which a cell’s response is systematically determined; all other variables
are treated as noise, so if the response depends on some unknown variable,
interpretation will be hampered.
By deﬁnition, for each of these populations, the activities of the cells
can be interpreted as conveying information about some underlying lowdimensional space. Interpreting population codes thus involves two spaces.
First, there is an explicit space that consists of the activities r = {ri} of the
cells in the population. Second, these activities are described in terms of an
implicit space , which contains the small number
of underlying dimensions (e.g., direction in the MT example above) that
the activities of the cells encode and in which they are described as having
tuning functions. The implicit space plays no explicit role, even though any
processing on the basis of the activities r has to be referred to the implicit
space. For instance, if the activities across the population are combined in
some particular way (as in generating a single value), then the implicit space
can be used to work out how much information is lost, and consequently
to work out the best method of combination.
This notion of explicit and implicit spaces underlies our framework,
which is depicted in Figure 1 in the context of a single experiment. At the
top are the measured activities of a population of cells. There are two key
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
Figure 1: (Left) Standard models of population codes, such as the Poisson
model, assume an underlying encoding model such as the one illustrated here.
The output of the encoding process is shown at the top: the activities of units,
corresponding to the explicit, observable representation. These are assumed
to have been generated by the independent application of each cell’s tuning
function and additive noise to the implicit representation shown at the bottom,
which in this case is a single value x∗in the space of underlying variables. (Right)
Bayesian decoding models describe how to extract P[x|r] from the observed activities, through knowledge of the tuning functions f(x). An estimate of the true
value of x can then be formed according to some loss function. Uppercase letters
label operations.
questions to ask about this ﬁring:
1. What is the relationship between the activities r of the cells and the
underlying quantity x in the world that is represented? (encoding)
2. What information about the quantity x can be extracted from the activities? (decoding)
Although it is of active interest ,
we do not consider constraints that come from the neural instantiation of
the decoding algorithms and pose the decoding question as an abstract
Probabilistic Population Codes
problem. Since neurons are generally noisy, it is often convenient to characterize encoding (see Figure 1, operations A and B) in a probabilistic way,
by specifying:
The simplest models make a further assumption of conditional independence of the different units given the underlying quantity,
although others characterize the degree of correlation between the units
 .
If the encoding model in equation 2.1 is true, then a Bayesian decoding
model speciﬁes that the information that r carries about x can be characterized precisely as
P[x|r] ∝P[r|x]P[x],
where P[x] is the prior distribution about x and the constant of proportionality is set so that
P[x|r]dx = 1.
Note that starting with a deterministic quantity x in the world, encoding
it in the ﬁring rates r, and decoding it (operation C) using equation 2.3
results in a probability distribution over x. This uncertainty arises from the
stochasticity represented by P[r|x]. Given a loss function, we could then go
on to extract a single value from this distribution (operation D).
For most real cases of population codes, encoding cannot be described
so crisply. This article describes the inadequacy of one particularly pervasive assumption: that the underlying quantity is a single value for instance,
the single position of a rat in an environment, or the single coherent direction of motion of a set of dots in a direction discrimination task. The
assumption is pervasive since this is how one typically works out what a
population of cells is encoding and how each responds to some particular
x. It is inadequate because it cannot capture the subtleties of other experiments, such as those in which rats can be made to be uncertain about
their position , or in which one direction of motion
predominates yet there are several simultaneous motion directions . In many cases, the natural characterization is actually
a whole probability distribution P[x|ω] over the value of the variable x,
where ω represents all the available information. For instance, for the rat,
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
this distribution might be the distribution over its possible locations in the
environment.
The rat can clearly expect to be in exactly one position at any one time,
and it therefore makes sense to consider the distribution of uncertainty
P[x|ω] as to that position. This is not quite true for the monkey; it could be
observing many different directions of motion simultaneously. In this article, we characterize this multiplicity by considering a separation between
the direction of motion of a single randomly chosen dot (which gives the
equivalent of P[x|ω]) and the actual number of dots present at any one time.
Following Hinton we consider the sum total activity over the population as reporting the latter and the distribution of that activity across the
population as reporting the former. This imposes the important constraint
that there is some independent standard for how much activity there should
be (with which to work out the number of dots) and is clearly not the only
possibility.2
Note also that there is no requirement that the animal perform decoding
as in equation 2.3, or, indeed, that it explicitly perform decoding at all. That
Wilson and McNaughton can extract the (x, y) coordinates of a rat in
a room on the basis of the activities of 25 of its place cells does not mean that
the stages of rodent processing subsequent to the hippocampus actually do
We can now cast two existing classes of proposals for population codes
in terms of this framework.
2.2 The Poisson Model. Under the Poisson encoding model, the quantity encoded is indeed one particular value, and the activities of the individual units are independent, with the terms in equation 2.2 speciﬁed as
P[ri |x] = e−fi(x) ( fi(x))ri
The activity ri could, for example, be the number of spikes the cell emits
in a ﬁxed time interval following the stimulus onset. A typical form for the
tuning function fi(x) is gaussian:
fi(x) ∝e−(x−xi)2/2σ 2,
about a preferred value xi for cell i. In terms of Figure 1, turning the quantity
x into a set of mean ﬁring rates fi(x) for the units is operation A; sampling
2 We propose that the magnitude of ﬁring can be used to suggest the multiplicity of
inputs as well as their properties (i.e., doubling the ﬁring rate could indicate that perhaps
there are two stimuli present). Then one could interpret the remaining pattern of activity as
implying multiple distributions, one for each possible stimulus. It is likely, however, that
nonlinear processes affect the ﬁring rate under multiple stimuli. For example, Snowden
 showed that an MT cell’s response to motion in its preferred direction can be
suppressed by adding motion in an orthogonal direction.
Probabilistic Population Codes
the activities ri from these means according to a Poisson distribution is operation B. These operations are descriptive models; they capture the essence
of the results of a collection of experiments rather than being based on a
biophysical understanding of how x actually causes the activities r.
Several authors have examined maximum likelihood (ML) decoding under the Poisson encoding model and analyzed the performance of other
decoding methods relative to ML. These methods all focus on extracting
a single value for the underlying parameter. The full probability distribution over the quantity x from this Poisson model is given by :
P[x|r] ∝P[x]
e−fi(x) ( fi(x))ri
Although the Poisson model is simple and straightforward, it suffers from
the assumption criticized above: that there is just a single value x. If the rat
is really uncertain about whether it is at location x1 or location x2 in the
world, then the standard Poisson model has no formal way of turning that
uncertainty into activities. In this case, we argued that the natural characterization of the quantity in the world that the activities of the cells encode
is now P[x|ω].
We describe below a method of encoding that takes exactly this approach.
However, one might argue thateventhoughthereisnoformalwayofencoding uncertainty in the activities, there is a formal way of decoding activities
to produce a probability distribution over x. Perhaps one could form P[x|r]
in equation 2.6 as a decoding of r to model a diffuse or multimodal P[x|ω].
We now show that this approach turns out to be inadequate.
Consideraone-dimensionalcasewithtuningfunctionsasinequation2.5.
Imagine that activities r are speciﬁed directly in some manner on the basis
of a whole probability distribution P[x|ω] over x. The goal is to decode
the activities r using equation 2.6 and actually represent P[x|ω] adequately.
From now on, we will use ˆPr(x) as the decoding distribution over x that is
speciﬁed by r. This is no longer P[x|r] unless it is a true Bayesian inverse.
From equation 2.6, and assuming a uniform prior over x, we have:
log ˆPr(x) = K −
ri(x −xi)2
by completing the square, if, as in most reasonable cases, there are sufﬁcient
units such that P
i fi(x) is constant in x. But the distribution in equation 2.8
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
is then gaussian, with mean µ and variance ξ2, where:
If we extract a single value from this decoded distribution by simply taking its mean, then this value matches the center-of-gravity estimate . However, if we are interested in the entire distribution, then this standard model cannot capture the range of input distributions P[x|ω] under
consideration. First, it will clearly not be possible to match multimodal
P[x|ω] with any ﬁdelity, since this decoded gaussian is unimodal. Second,
remembering that ri ≥0 should reasonably be integers, then unless they
are all 0, the variance of ˆPr(x) is bound to be less than σ 2, and so there is
no setting for the r that will match P[x|ω] having widths greater than that
of the tuning functions themselves. Thus this Poisson model is incapable of
representing distributions that are broader than the tuning functions.
In fact, even though it is not actually being used to specify the activities of
the cells, the underlying assumption in the encoding of the Poisson model
(as embodied in equation 2.4) is that there is just one value of x that results in
the activities of the cells, and thus the Poisson model has trouble encoding
anything other than delta function P[x|ω]. This analysis also applies to the
gaussian encoding model. It is not strictly true if the tuning functions are
not gaussian or the units have some baseline activity . We see later (in Figure 9) a case in which allowing baseline
activities permits the Poisson decoding model to produce a multimodal
distribution. However, each of the modes is much too narrow. To reinforce
this point: if the ﬁring rate were stationary for long enough so that one could
collect arbitrary numbers of spikes, then one could estimate the true mean
activities fi(x) for the cells. Decoding using the Poisson model if one knows
the means will almost always lead to a delta function estimate.
2.3 The KDE Model. Anderson and Anderson and Van Essen
 deﬁned a new interpretation for population codes in which the notion
ofrepresentingprobabilitydistributionsoverxratherthanjustasinglevalue
is essential. This method represents the distribution ˆPr(x) in terms of kernel
density estimates (KDEs), forming a linear combination of simple kernel or
basis functions ψi(x) associated with each cell, weighted by a normalized
function of its activity ri:
Here the r′
i are normalized such that ˆPr(x) is a probability distribution. If
the ψi(x) are probability distributions themselves and ri are all positive, a
Probabilistic Population Codes
natural choice is to have
Note that the kernel functions ψi(x) are not the tuning functions fi(x) of the
cells that would commonly be measured in an experiment. They need have
no neural instantiation; instead, they form part of the interpretive structure
for the population code. If the ψi(x) are probability distributions, and so
are positive, then the range of spatial frequencies in P[x|ω] that they can
reproduce in ˆPr(x) is likely to be severely limited.
In terms of our framework, whereas the Poisson model makes decoding a corollary of (that is, the Bayesian inverse of) the encoding model, the
KDE model makes encoding a corollary of the decoding model. Evaluating the KDE model requires us to consider encoding—taking a probability
distribution P[x|ω] over x and producing a set of ﬁring rates {ri} such that
ˆPr(x) in equation 2.9 approximates P[x|ω] closely. It is the encoding process
that standard experiments probe. Presenting single, unambiguous stimuli
(the experimental procedure that led to the characterization in the Poisson
model) amounts to asking the system to encode delta function distributions
P[x|ω] = δ(x −x∗) for various x∗. The measured responses are then the
mean resulting activities fi(x∗) ∼⟨ri⟩.
One natural way to do encoding is to use the Kullback-Leibler divergence
as a measure of the discrepancy between P[x|ω] and P
iψi(x) and use the
expectation-maximization (EM) algorithm to ﬁt the {r′
i}, treating them as
mixing proportions in a mixture model .
This relies on {ψi(x)} being probability distributions themselves.
The projection method is an alternative encoding
scheme that does not require the iterations of EM but instead uses the L2
distance. This allows ri to be computed as a projection of P[x|ω] onto the
tuning functions:
P[x|ω] fi(x) dx.
The kernel functions are again assumed to be a ﬁxed implicit property of
the cells in this formulation and the optimal L2 tuning functions fi(x) are
derived as
ψi(x)ψj(x) dx.
These tuning functions are likely to need smoothing or regularizing , particularly if the ψi(x) overlap substantially. In this case,
with purely linear encoding and decoding operations, the overall scheme
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
is a particular sort of linear ﬁlter, and many of its properties can be derived
from this perspective.
There is a further aspect of P[x|ω] that we might wish to represent in
a population code: certainty. Consider reducing the contrast of the moving
random dots near to threshold. Then the absolute activity levels of MT cells
might represent the certainty that there is actually a stimulus at all. In this
case, one might characterize P[x|ω] as a mixture model, with one mixing
proportion for the absence of a stimulus and one mixing proportion for a
distribution over directions for the presence of a stimulus. The normalization step in equation 2.10 prevents the KDE from representing this form of
certainty, since P
i = 1. However, certainty could easily be captured. For
instance, if there is a maximum value Rmax for the summed actual activities
i ri, then one could have an indicator variable z ∈{0, 1} representing the
presence (1) or absence (0) of the underlying object and:
P[z = 1|{ri}] =
ˆP(x; {ri}, z) =
where P[x] is the prior distribution over x. Note that under this formulation,
as the probability of the underlying object’s presence approaches zero, the
marginalized estimate of the original distribution approaches the prior:
ˆPr(x) = ˆP(x; {ri}, z = 1)P[z = 1|{ri}]
+ ˆP(x; {ri}, z = 0)(1 −P[z = 1|{ri}]).
Of course, as P[z = 1|{ri}] tends to 0, the question of the true underlying
distribution becomes moot.
3 The Extended Poisson Model
Given its linear decoding method and a limited number of decoding kernel
functions ψi(x), we might expect the KDE model to have difﬁculty capturing
in ˆPr(x) probability distributions P[x|ω] that have high frequencies, such
as delta functions. We also saw that the standard Poisson model has the
problem of decoding almost any pattern of activities r into something that
rapidly approaches a delta function as the activities increase. Is there any
middle ground?
We argued that the problem for the standard Poisson model comes from
its encoding model (see equation 2.4), which is based on there being a single underlying value x. We can extend this encoding model to allow the
Probabilistic Population Codes
recorded activities r to depend explicitly on general P[x|ω]. The extended
Poisson model is based on an encoding model in which the activity of cell i
is Poisson about a mean, which, in the continuous version, is
P[x|ω] fi(x)dx.
Note that this equation is identical to the encoding model for the kernel density model (see equation 2.11), except that here the ﬁring rates are stochastic.
In the kernel density model, there is no variability in the activities {ri} that
encode a particular P[x|ω]. Under the extended Poisson model, the activity of each cell is a ﬁltered, sampled version of the underlying probability
distribution and this implies strong constraints on the potential quality of
reconstructions.
This model is the most straightforward extension of the conventional
Poisson model and makes roughly similar predictions about observable
activities when P[x|ω] is a delta function. However, it offers a much more
powerful model for representing P[x|ω] that are not delta functions.
Given {ri} generated using Poisson noise from equation 3.1, how should
one infer P[x|ω]? Recall that for the standard Poisson model, encoding a
single location in a population (P[ri|x]) leads, through decoding, to a probability distribution over possible locations (P[x|r]). Here the encoding model
takes a whole function (P[x|ω]) and stochastically produces a set of numbers ({ri}) that provide partial information about that function. The general
Bayesian inverse to this is a model that takes the numbers and produces a
probability distribution over the functions that could have generated them.
In our case, this means that decoding should really produce a probability
distribution over probability distributions over the implicit variable x, that
is, P[P[x|ω]|r]. Rather than do this, we choose to summarize this distribution over distributions by an approximation to its most likely member;
we perform an approximate form of maximum a posteriori (MAP) decoding, not in the value of x but in distributions over x. Figure 2 illustrates the
entire encoding and decoding process for the case of implicit probability
distributions.
We approximate P[x|ω] as a piece-wise constant histogram that takes the
value ˆφj in (xj, xj+1], and fi(x) by a piece-wise constant histogram that take
the values fij in (xj, xj+1]. Then we model activities {ri} as being independent
Poisson random variables whose means are (see equation 3.1)
Then the true inverse distribution is:
{ ˆφj}|{ri}
ij ˆφj fij Y
where P[{ ˆφj}] is the prior over the { ˆφj}.
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
P[P(x|w)|{r}]
P[r|P(x|w)]
Figure 2: A set of ﬁring rates may also be interpreted as encoding a probability distribution in implicit space. Decoding the rates now involves forming a
probability distribution over possible implicit distributions, P [P[x|ω]|r]. The
decoding distribution ˆPr(x) may be formed from this distribution over distributions by integrating or through a maximum a posteriori computation. The
extended Poisson model forms ˆPr(x) using an approximate form of ML in distributions over x.
If the system is translation invariant then P
i fij = f is constant for
j ˆφj = 1, because it represents a probability distribution, therefore
ij ˆφj fij = e−f , which is constant. Taking logs,
{ ˆφj}|{ri}
= K + log P
where K is a constant. Maximum a posteriori decoding in this context requires ﬁnding the set of { ˆφj} that sum to 1 and maximize this expression. If
log P[{ ˆφj}] is dominated by a smoothness prior such as
ˆφj −ˆφj+1
Probabilistic Population Codes
then we are left with the following expression:
AP({ ˆφj}) =
ˆφj −ˆφj+1
where ϵ is a weighting coefﬁcient on the smoothness prior.
Thus, the extended Poisson model creates a decoding distribution ˆPr(x)
that approximates MAP inference from the distribution over distributions
P [P[x|ω]|{ri}].
The values of this decoding distribution can be found in a number of
ways. One simple method involves adjusting { ˆφj} via simple gradient ascent in MAP({ ˆφj}). Alternatively, one can use a version of EM to ﬁnd the
MAP values. For this, one interprets P
j ˆφj fij as a mixture model for case i,
where { ˆφj} are the mixing proportions and { fij} are the ﬁxed values of the
underlying distributions. ri is then the weighting for case i, and the resulting
expression in equation 3.3 therefore acts just like a likelihood. In practice,
a smoothness prior such as equation 3.4 is required for regularization. For
the experiments described below, we implemented a crude approximation
by averaging adjacent ˆφj after each EM iteration.
With this method of decoding in mind, we now see how the extended
Poisson model competes with the KDE model as a way of representing
probability distributions. By comparison with the linear equation 2.9, equation 3.5 offers a nonlinear way of combining a set of activities {ri} to give a
probability distribution ˆPr(x) over the underlying variable x. The computational complexities of equation 3.5 are irrelevant, since decoding is only
an implicit operation that the system need never actually perform.
Another way of looking at the difference between the models is that the
extended Poisson model is a Bayesian decoding method, and thus involves
a multiplication of tuning functions (assuming the units are independent),
while the KDE model is a basis function method, in which decoding entails
a summation of kernels. We will see the consequences of this difference in
the simulations below.
Finally, just as for the KDE model, this form of decoding does not capture
well the certainty in the presence of a stimulus. A natural extension to the
model in equation 3.5 is to estimate ˆφj just as before, but set the approximation to ˆPr(x) to be ˆφ × ˆφj in the interval (xj, xj+1], where
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
4 Comparing the Models
We now compare the ability of these interpretation methods to extract the
probabilistic information contained in a population code. The primary question of interest is whether we can ﬁnd a set of deterministic activities {ri}
that make ˆPr(x) a close approximation to an interesting implicit distribution
P[x|ω]. We take three instances of implicit distributions intended to model
the primary cases in which interpreting a population code as a probability
distribution is essential:
1. Some uncertainty exists about the location of the target. The aim is to
approximate correctly the mean and the uncertainty about this mean.
2. The target could be in one of two locations. A bimodal distribution
could arise due to insufﬁcient information, as in the case of the rat, or
due to the actual existence of multiple values.
3. Uncertainty exists about the presence of the target. Here the magnitude of the integral under the implicit distribution is intended to
capture the degree of certainty in the presence of the target.
Finally, we also examine the noise robustness of the methods.
In each of these cases, we consider a simpliﬁed situation in which the
location of the object varies along only one dimension. This eases visualization of network activity and decoded distributions, but each model readily
could be extended to include other dimensions. For each model, the population code consisted of 50 units, with preferred values xi spaced uniformly
in the range of x = [−10, 10]. Associated with each unit was a gaussian
distribution: N (xi, σ = 0.3). In the KDE model, these 50 gaussians were
the kernels ψi(x), while in the Poisson and extended Poisson models, they
represented the tuning functions fi(x).
For the KDE model, we used two methods to ﬁnd the set of activities that
provided the best ﬁt between the true implicit distribution P[x|ω] and the
estimate ˆPr(x) (see equation 2.9). The projection method computes the rates
{ri} as a projection of P[x|ω] onto the tuning functions, where the optimal
tuning functions are derived from the ﬁxed kernels (see equations 2.11 and
2.12). The EM method adapts {ri} to minimize the Kullback-Leibler divergence between the estimated and true distributions.
For the Poisson and extended Poisson models, we again computed the
rates{ri}asaprojectionofP[x|ω]ontothetuningfunctions,wherethetuning
functions are now the ﬁxed { fi(x)}. In the Poisson model, we decode directly
into the ML values of equation 2.8, while in the extended Poisson model,
we decode using a version of EM to ﬁnd the MAP values of equation 3.5.
In all of these experiments, we limited the activities {ri} to assume integer
values. Note that in the case of the KDE model, these activities are then
normalized as part of the decoding process (see equation 2.10). Finally, in
Probabilistic Population Codes
true P[x|w]
KDE (Proj.)
true P[x|w]
true P[x|w]
Figure 3: All three methods can reconstruct the original implicit gaussian distribution over a range of values of τ, the standard deviation of this gaussian.
Here τ = 1.0.
all simulations presented here, Rmax = 50, and the number of histogram
bins in the extended Poisson model was 500. Table 1 summarizes these
We also require some quantitative way of comparing the reconstructions
of the various models. Although the Kullback-Leiber distance (the implicit
metric for both extended Poisson and KDE-EM methods) is the natural
measure of the difference between two probability distributions, it cannot
be used here, since the reconstructions from the KDE-projection method are
not strict distributions (they are not nonnegative). We therefore used the
somewhat less informative squared error measure:
h ˆPr(xj) −P[xj|ω]
4.1 Uncertainty in Target Location. For these simulations, the implicit
distribution is a gaussian:
P[x|ω] = N (0, τ).
All three methods form good estimates for a range of values of τ, the width
of the true implicit distribution (for example, see Figure 3). However, as
predicted, both KDE methods are unable to represent narrow distributions
accurately for example, τ = 0.2 (see Figure 4). In general, the critical variable in the ﬁdelity of the KDE estimate is the ratio of the widths σ of the
decoding kernels and the widths τ in the true implicit distribution. The extended Poisson method is able to capture both narrow and wide implicit
distributions, so it can form accurate interpretations of distributions with a
relatively high variance, as well as delta functions.
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
Table 1: Summary of the Key Operations of the Three Interpretation Methods.
Extended Poisson
KDE (Projection)
x P[x|ω] fi(x)dx¤
x P[x|ω] fi(x)dx¤
fi(x) = RmaxN(xi, σ)
i to max L
x ψi(x)ψj(x)dx
Pr(x) to max L
Pr(x) fi(x)dx ≈P
Likelihood
L = log P £
{ ˆφj}|{ri}¤
i ri log ˆri
x P[x|ω] log ˆ
i ri log(ri/ ˆri)
Pr(x) −P[x|ω]
x P[x|ω] log P[x|ω]
Note: h[ ] is a rounding operator to ensure integer ﬁring rates, and ψi(x) = N(xi, σ) are the kernel functions for the KDE
method. The certainty calculation is not included in this table.
Probabilistic Population Codes
true P[x|w]
KDE (Proj.)
true P[x|w]
true P[x|w]
Figure 4: The KDE method has difﬁculty capturing the high-frequency information in the implicit gaussian distribution, when its standard deviation, τ = 0.2,
is smaller than the kernel widths (σ = 0.3).
4.2 Multiple Locations. A multimodal implicit distribution can be described as a mixture of two gaussians:
P[x|ω] = 0.5 ∗N (x1, τ) + 0.5 ∗N (x2, τ).
Here we can model the situation in which the rat is uncertain whether the
target is in location x1 or x2. The variable τ describes an additional degree
of uncertainty about the exact location of each potential target. For these
simulations, we let x1 = 2 and x2 = −2.
To get a sense of the different encoding models, the expected activities
of the 50 units in the population code are plotted in Figure 5. For τ = 1.0,
the KDE-EM method has converged to a local minimum, which is why
one of the ﬁring rates looks odd. However, because the distribution being
modeled is so broad, this makes very little difference to the net quality of
reconstruction.
Applying the respective decoding models to these expected ﬁring rates,
we ﬁnd results similar to those of the previous section. Both KDE methods
can capture a bimodal implicit distribution where the width exceeds that
of the kernels (see Figure 6), yet they cannot accurately reconstruct narrow
distributions (see Figure 7). The extended Poisson model matches both
types of implicit distributions with high ﬁdelity.
For the sake of comparison, Figure 8 shows the decoded distribution
formedbythesimplePoissonmodel(seeequation2.8).Hereweusethesame
encoding model as in the extended Poisson method, in order to allow the
unit activities to convey information about the entire implicit distribution.
Nonetheless, the simple decoding model always produces a delta function
estimate. An estimated delta function ﬁts the case of the narrow unimodal
implicit distribution shown in Figure 4, but it cannot match the bimodal
implicit distribution here.
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
true P[x|w]
true P[x|w]
Figure 5: Each method’s encoding model speciﬁes how the expected ﬁring rates
of units in the population are based on the implicit distribution. The implicit
distribution here is a bimodal gaussian with standard deviation τ. The ﬁring
rates on the left are for τ = 0.2 and on the right for τ = 1.0. In both cases,
Rmax = 50.
true P[x|w]
KDE (Proj.)
true P[x|w]
true P[x|w]
Figure 6: All three methods provide a good ﬁt to the bimodal gaussian distribution when its variance is sufﬁciently large (τ = 1.0).
Probabilistic Population Codes
true P[x|w]
KDE (Proj.)
true P[x|w]
true P[x|w]
Figure 7: The KDE method again has difﬁculty capturing the high-frequency
information in the implicit bimodal gaussian distribution when its variance
τ = 0.2 is smaller than the kernel widths (σ = 0.3).
true P[x|w]
std. Poisson
Figure 8: The simple Poisson decoding model leads to a delta function for the
estimated implicit distribution. Here the implicit distribution is the bimodal
gaussian, τ = 1.0. The unit activities r are the same as for the extended Poisson
model, and the decoding is done using equation 2.8.
This result is predicted based on the the analysis presented in section 2.2.
This analysis, however, applied to the case of gaussian tuning functions in
the absence of any baseline activity. Using simulations, we examine what
happens when the units have some baseline activity. Here it is necessary to
change the encoding model so that the unit’s expected ﬁring rate is the sum
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
w/ baseline
true P[x|w]
std. Poisson
Figure 9: When we extend the simple Poisson decoding model to include baseline ﬁring rates (Rb = 5.0 here), the estimated implicit distribution can be multimodal. Yet this decoded distribution does not contain any variance about these
values. The implicit distribution is again the bimodal gaussian, τ = 1.0, and the
unit activities r are the same as for the extended Poisson model.
of a gaussian tuning function and a baseline ﬁring rate, Rb:
fi(x) ∝e−(x−xi)2/2σ 2 + Rb.
Again we form the decoding distribution as the Bayesian inverse of this
new encoding model:
P[x|r] ∝P[x]
e−fi(x) ( fi(x))ri
For the simulations, we again use the encoding model of the extended
Poisson model, but now use this modiﬁed simple Poisson model for decoding. An example of the results is shown in Figure 9. The decoded distribution is able to take on multiple modes, yet it is always highly peaked,
due to the likelihood being a product of Poisson probabilities. Thus, this
decoding method can recover multiple implicit values but cannot capture
any uncertainty in these values.
4.3 Uncertainty in Object Presence. The next set of simulations addresses the issue of object presence. We use a dampened gaussian to model
Probabilistic Population Codes
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
extended Poisson
KDE - Projection
Figure 10: This plot shows the squared error (see equation 4.1) for different values of c, the total integral under the implicit distribution, for both the extended
Poisson and KDE projection methods. In these simulations, Rmax = 50.
the situation in which uncertainty exists in the actual presence of the target:
cN (0, τ), 0 ≤c ≤1.
Ideally, c would not be restricted to be less than 1. Instead, it could take on
any positive value and thereby describe the actual number of instances of
the implicit variable. Here we consider the simpler situation in which only
one instance of the object is represented.
In these simulations, we compare the KDE model using the projection
method to the extended Poisson model. We set τ to be 1.0, because both
methods are able to match this distribution with high ﬁdelity when c = 1.0.
The primary empirical result is that while both methods have difﬁculty as
the presence c gets close to 0, both are able to recover a variety of nonnormalized gaussian implicit distributions, as shown in Figure 10. The main
reason for the poor performance as c decreases is that the ﬁring rates are
forced to be integers. Clearly, increasing Rmax would extend the range of
good reconstructions.
4.4 Noise Robustness. In the previous simulations, the activities of the
units in the population code were equal to their expected responses according to the respective encoding models. We now examine the robustness of
the interpretation methods by treating the unit activities as Poisson random variables. Each simulation involves a stochastic sampling of the unit
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
KDE (Projection)
Extended Poisson
Figure 11: The expected error ⟨E⟩in the decoded distribution, averaged over 50
samples from the ﬁring rate distributions, is plotted against different values of
τ, the width of the bimodal gaussian implicit distribution. The results for only
one of the KDE methods are shown because both KDE methods involve the
same decoding operation.
responses. The robustness of a method is estimated by computing the expected error in the decoding distribution with respect to the true implicit
distribution, that is, averaging the squared-error metric (see equation 4.1)
over 50 stochastic trials.
The results of this set of simulations again match the predictions based
on the contrast between the methods, as shown in Figure 11. Because the
decoding in the extended Poisson model is nonlinear, we predict that this
model will be more robust to noise than the KDE model, in which decoding
is linear.
Figure 11 makes it appear that both methods produce perfect reconstructions for larger values of τ. This is in fact not true and is largely due to
the magnitude of the expected error for small values of τ. For the extended
Poisson model, the regularization removes one component of the error: inaccuracies in the shape of the reconstructed distribution. This process is
more effective as τ gets larger. However, the smoothing does not remove
the second component: the bias that is present if the centers of the two gaussians in the reconstructed distribution are incorrect. This bias component of
the error is slightly higher for low values of τ but relatively constant and
nonzero for τ > 0.4.
Probabilistic Population Codes
5 Discussion
We have presented a theoretical framework for understanding population
codes that generalizes naturally to the important case in which the population provides information about a whole probability distribution over an
underlying quantity rather than just a single value. We used the framework
to analyze two existing models and to suggest and evaluate a third model
for encoding such probability distributions.
More informally, we have tried to examine the consequences of the seemingly obvious step of saying that if a rat, for instance, is uncertain about
whether it is at one of two places, then place cells representing both places
could be activated. The complications come because the structure of the interpretation changes; for instance, one can no longer think of ML methods
to extract a single value from the code directly.
We are not suggesting that the uncertainty is generated at any particular
step in the processing system. Rather, it is a type of information that is
potentially contained in the population activity, about which inferences can
be made from one level to the next. So the rat need not be aware that it
is uncertain; we are not positing anything about “conscious” uncertainty.
Instead, different ﬁring patterns corresponding to different distributions
over the underlying implicit variables are all that is needed to infer the
uncertainty at the level above. This leads to the prediction that place cells
corresponding to multiple locations will be active when the rat is uncertain
as to its location. This uncertainty may be present even at the sensor level. A
population of orientation-selective cells should have a different pattern of
ﬁring to a bar at one orientation and a blurred image of the same bar in which
the orientation is “uncertain.” This fuzzier bar (as well as the sharper bar)
can be represented in terms of a probability distribution over orientation.
One main result of our framework is a method for encoding and decoding probability distributions that is the natural extension of the standard
Poisson model for encoding and decoding single values. We also showed
that this standard model is inadequate for coding probability distributions.
Under the new encoding model, the activity of a cell has Poisson statistics about a mean that is dependent on the integral of the whole encoded
probability distribution, weighted by the tuning function of the cell. The
behavior of this encoding model is appropriate in circumstances such as the
positional uncertainty of the rat. We suggested a particular decoding model,
based on an approximation to ML decoding to a discretized version of the
whole probability distribution. The resulting calculations require a form
of the EM algorithm, regularized by a smoothing operation. We showed
that this nonlinear decoding method works well in a variety of circumstances, reconstructing broad, narrow, and multimodal distributions more
accurately than either the standard Poisson model or the kernel density
model. Stochasticity is built into our method, since the units are supposed
to have Poisson statistics, and it is therefore also quite robust to noise.
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
Various aspects of this scheme merit discussion. First, we readily acknowledge that the decoding model is quite nonbiological, involving an
implausible iterative computation. The point of our particular decoding
model was to show explicitly a lower bound to the veracity with which a
set of activities can code a distribution. One might expect the subsequent
stages of processing in the brain to do one of two things with the information
in the population:
1. Integrate it with information represented in other population codes
to form a combined population code (e.g., combining uncertain information about the relative position of two landmarks to generate the
activity of a population code formed of place cells).
2. Extract a single value from it to control behavior (e.g., pull a lever
to report the best-supported direction for the motion of the dots or
choose where to explore for food in a maze).
In both cases, the extraction is presumably performed through standard
neural operations such as taking nonlinear weighted sums and, possibly,
products of the activities. We are interested in how much information is
preserved by such operations, as measured against the nonbiological standard of our decoding method.
The ﬁrst issue—how to integrate two or more population codes to generate the output in the form of another population code—was stressed by
Hinton , who noted that it directly relates to Ballard’s notion
of generalized Hough transforms. This question is particularly important
because of the apparent ubiquity of population coding in the brain. It is not
at all obvious how simple and local combination methods could be capable
of preserving and combining probabilistic information in the population,
and we are studying this question, using the EM-based decoder to generate
targets and using local learning rules.
One interesting theoretical concern is that the population code output of
such a combination might not have exactly the same form as the population
code inputs. For instance, it might not be completely accurate to characterize
the cells as having Poisson statistics based on a gaussian tuning function.
In this case, one could formally calculate the true statistical interpretation
of the combined code. However, in the brain, there does not appear to be
a great difference between the population codes near to the input and the
population codes in deeper areas, such as MT. This actually places a strong
constraint on the method of combination.
One special concern for combination is how to understand noise. For
instance, the visual system can be behaviorally extraordinarily sensitive,
detecting just a handful of photons. However, the outputs of real cells at
various stages in the system are quite noisy, with apparent Poisson statistics.
If noise is added at every stage of processing and combination, then the ﬁnal
population code will not be very faithful to the input. There is much current
Probabilistic Population Codes
and confusing research on the issue of the creation and elimination of noise
in cortical synapses and neurons . Correlated noise presents extra and
different concerns.
A further concern for combination is the basis function strategy apparently adopted in parietal cortex, in which the position of an object in space is
reported by neurons that have a retinotopic visual receptive ﬁeld and multiplicative modulation from the position of the eyes in their orbits . Multiplicative modulation based on the locus of
attention has also been found in V4 , and it has been suggested as being a general computational strategy . The statistical effects of the
multiplicative modulation remain to be investigated.
The second issue—extracting a single value from the population—is also
important, particularly at the interface into the motor system. Some empirical data about how this is accomplished come from the work on biasing
the choices of the monkeys as to the directions of motion of the random dot
stimuli through electrical microstimulation in MT . If the dots were moving in one direction but the electrical stimulation
favored a different direction, then the monkeys would typically choose one
or other of the two directions, rather than something like the mean direction.
In our framework, we would regard the simultaneous activity of the MT
cells that prefer the two directions as encoding a distribution and postulate
an output extraction process that chooses a single value on the basis of this
distribution. Short of knowing exactly the effects of the electrical stimulation
on the activity of the MT cells, it is hard to use the experiment to conﬁrm or
reject this hypothesis.
A ﬁnal issue that we have addressed is that of certainty or magnitude.
Hinton’s idea of using the sum total activity of a population to code
the certainty in the existence of the quantity they represent is attractive,
provided that there is some independent way of knowing what the scale
is for this total. We used this scaling idea for both the kernel density and
the extended Poisson models. In fact, we can go one stage further and interpret greater activity still as representing information about the existence
of multiple objects or multiple motions. However, this treatment seems less
appropriate for the place cell system; the rat presumably is always certain
that it is somewhere. There, it has been suggested that the absolute level
of activity could be coding something different, such as the familiarity of a
An entire collection of cells is a terrible thing to waste on representing
just a single value of some quantity. Representing a whole probability distribution, at least with some ﬁdelity, is not more difﬁcult, provided that the
Richard S. Zemel, Peter Dayan, and Alexandre Pouget
interpretation of the encoding and decoding is clear. Here we have suggested some steps in this direction.
Acknowledgments
We thank Charlie Anderson, Terry Sanger, and Larry Abbott for helpful
discussions and an anonymous reviewer for useful comments. R. Z. was
supported by the McDonnell Foundation, grant JSMF 95-1; P. D. by NIMH
R29 MH 55541-01 and the Surdna Foundation; and A. P. by a grant from the
DOD, grant DAMD17-93-V-3018.