Generating Visual Explanations
Lisa Anne Hendricks1
Zeynep Akata2
Marcus Rohrbach1,3
JeﬀDonahue1
Bernt Schiele2
Trevor Darrell1
1UC Berkeley EECS, CA, United States
2Max Planck Institute for Informatics, Saarbr¨ucken, Germany
3ICSI, Berkeley, CA, United States
Abstract. Clearly explaining a rationale for a classiﬁcation decision
to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not
output any justiﬁcation text; contemporary vision-language models can
describe image content but fail to take into account class-discriminative
image aspects which justify visual predictions. We propose a new model
that focuses on the discriminating properties of the visible object, jointly
predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling
and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class speciﬁcity. Our results on
a ﬁne-grained bird species classiﬁcation dataset show that our model is
able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing
captioning methods.
Introduction
Explaining why the output of a visual system is compatible with visual evidence
is a key component for understanding and interacting with AI systems . Deep
classiﬁcation methods have had tremendous success in visual recognition ,
but their predictions can be unsatisfactory if the model cannot provide a consistent justiﬁcation of why it made a certain prediction. In contrast, systems which
can justify why a prediction is consistent with visual elements to a user are more
likely to be trusted .
We consider explanations as determining why a certain decision is consistent
with visual evidence, and diﬀerentiate between introspection explanation systems
which explain how a model determines its ﬁnal output (e.g., “This is a Western
Grebe because ﬁlter 2 has a high activation...”) and justiﬁcation explanation
systems which produce sentences detailing how visual evidence is compatible
with a system output (e.g., “This is a Western Grebe because it has red eyes...”).
We concentrate on justiﬁcation explanation systems because such systems may
be more useful to non-experts who do not have detailed knowledge of modern
computer vision systems .
We argue that visual explanations must satisfy two criteria: they must both
be class discriminative and accurately describe a speciﬁc image instance. As
 
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
Description: This is a large bird with a white neck and a black back in the water.
Class Definition: The Western Grebe is a waterbird with a yellow pointy beak, white neck and belly,
and black back.
Explanation: This is a Western Grebe because this bird has a long white neck, pointy yellow beak
and red eye.
Image Relevance
Class Relevance
Description
Explanation
Class Definition
Laysan Albatross
Description: This is a large flying bird with black wings and a white belly.
Class Definition: The Laysan Albatross is a large seabird with a hooked yellow beak, black back
and white belly.
Visual Explanation: This is a Laysan Albatross because this bird has a large wingspan, hooked
yellow beak, and white belly.
Description: This is a large bird with a white neck and a black back in the water.
Class Definition: The Laysan Albatross is a large seabird with a hooked yellow beak, black back
and white belly.
Visual Explanation: This is a Laysan Albatross because this bird has a hooked yellow beak white
neck and black back.
Laysan Albatross
Western Grebe
Fig. 1. Our proposed model generates visual explanations. Visual explanations are both
image relevant and class relevant. In contrast, image descriptions are image relevant,
but not necessarily class relevant, and class deﬁnitions are class relevant but not necessarily image relevant. In the visual explanations above, class discriminative visual
features that are also present in the image are discussed.
shown in Figure 1, explanations are distinct from descriptions, which provide
a sentence based only on visual information, and deﬁnitions, which provide a
sentence based only on class information. Unlike descriptions and deﬁnitions,
visual explanations detail why a certain category is appropriate for a given image
while only mentioning image relevant features. As an example, let us consider
an image classiﬁcation system that predicts a certain image belongs to the class
“western grebe” (Figure 1, top). A standard captioning system might provide a
description such as “This is a large bird with a white neck and black back in the
water.” However, as this description does not mention discriminative features, it
could also be applied to a “laysan albatross” (Figure 1, bottom). In contrast, we
propose to provide explanations, such as “This is a western grebe because this
bird has a long white neck, pointy yellow beak, and a red eye.” The explanation
includes the “red eye” property, e.g., when crucial for distinguishing between
“western grebe” and “laysan albatross”. As such, our system explains why the
predicted category is the most appropriate for the image.
We outline our approach in Figure 2. We condition language generation on
both an image and a predicted class label which allows us to generate classspeciﬁc sentences. Unlike other caption models, which condition on visual features from a network pre-trained on ImageNet , our model also includes a
ﬁne-grained recognition pipeline to produce strong image features . Like many
contemporary description models , our model learns to generate a
sequence of words using an LSTM . However, we design a novel loss function
which encourages generated sentences to include class discriminative information. One challenge in designing a loss to optimize for class speciﬁcity is that
class speciﬁcity is a global sentence property: e.g., whereas a sentence “This is
an all black bird with a bright red eye” is class speciﬁc to a “Bronzed Cowbird”,
words and phrases in the sentence, such as “black” or “red eye” are less class
discriminative on their own. Our proposed generation loss enforces that generated sequences fulﬁll a certain global property, such as category speciﬁcity. Our
ﬁnal output is a sampled sentence, so we backpropagate the discriminative loss
Generating Visual Explanations
Deep Finegrained Classifier
Compact Bilinear
Recurrent explanation generator model
This is a cardinal because ...
Fig. 2. Generation of explanatory text with our joint classiﬁcation and language model.
Our model extracts visual features using a ﬁne-grained classiﬁer before language generation. Additionally, unlike description models we also condition sentence generation
on the predicted class label.
through the sentence sampling mechanism via a technique from the reinforcement learning literature. While typical sentence generation losses optimize the
alignment between generated and ground truth sentences, our discriminative loss
speciﬁcally optimizes for class-speciﬁcity.
To the best of our knowledge, ours is the ﬁrst method to produce deep visual
explanations using natural language justiﬁcations. We describe below a novel
joint vision and language explanation model which combines classiﬁcation and
sentence generation and incorporates a loss function operating over sampled
sentences. We show that this formulation is able to focus generated text to be
more discriminative and that our model produces better explanations than a
description-only baseline. Our results also conﬁrm that generated sentence quality improves with respect to traditional sentence generation metrics by including
a discriminative class label loss during training. This result holds even when class
conditioning is ablated at test time.
Related Work
Explanation. Automatic reasoning and explanation has a long and rich history
within the artiﬁcial intelligence community . Explanation
systems span a variety of applications including explaining medical diagnosis ,
simulator actions , and robot movements . Many of these systems
are rule-based or solely reliant on ﬁlling in a predetermined template .
Methods such as require expert-level explanations and decision processes.
In contrast, our visual explanation method is learned directly from data by
optimizing explanations to fulﬁll our two proposed visual explanation criteria.
Our model is not provided with expert explanations or decision processes, but
rather learns from visual features and text descriptions. In contrast to systems
like which aim to explain the underlying mechanism behind
a decision, authors in concentrate on why a prediction is justiﬁable to a user.
Such systems are advantageous because they do not rely on user familiarity with
the design of an intelligent system in order to provide useful information.
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
A variety of computer vision methods have focused on discovering visual
features which can help “explain” an image classiﬁcation decision . Importantly, these models do not attempt to link discovered discriminative features
to natural language expressions. We believe methods to discover discriminative
visual features are complementary to our proposed system, as such features could
be used as additional inputs to our model and aid producing better explanations.
Visual Description. Early image description methods rely on ﬁrst detecting
visual concepts in a scene (e.g., subject, verb, and object) before generating
a sentence with either a simple language model or sentence template .
Recent deep models have far outperformed such systems and
are capable of producing ﬂuent, accurate descriptions of images. Many of these
systems learn to map from images to sentences directly, with no guidance on
intermediate features (e.g., prevalent objects in the scene). Likewise, our model
attempts to learn a visual explanation given only an image and predicted label with no intermediate guidance, such as object attributes or part locations.
Though most description models condition sentence generation only on image
features, propose conditioning generation on auxiliary information, such as
the words used to describe a similar image in the train set. However, does not
explore conditioning generation on category labels for ﬁne-grained descriptions.
The most common loss function used to train LSTM based sentence generation models is a cross-entropy loss between the probability distribution of predicted and ground truth words. Frequently, however, the cross-entropy
loss does not directly optimize for properties that are desired at test time. 
proposes an alternative training scheme for generating unambiguous region descriptions which maximizes the probability of a speciﬁc region description while
minimizing the probability of other region descriptions. In this work, we propose
a novel loss function for sentence generation which allows us to specify a global
constraint on generated sentences.
Fine-grained Classiﬁcation. Object classiﬁcation, and ﬁne-grained classiﬁcation in particular, is attractive to demonstrate explanation systems because
describing image content is not suﬃcient for an explanation. Explanation models
must focus on aspects that are both class-speciﬁc and depicted in the image.
Most ﬁne-grained zero-shot and few-shot image classiﬁcation systems use
attributes as auxiliary information that can support visual information.
Attributes can be thought of as a means to discretize a high dimensional feature space into a series of simple and readily interpretable decision statements
that can act as an explanation. However, attributes have several disadvantages.
They require ﬁne-grained object experts for annotation which is costly. For each
additional class, the list of attributes needs to be revised to ensure discriminativeness so attributes are not generalizable. Finally, though a list of image attributes could help explain a ﬁne-grained classiﬁcation, attributes do not provide
a natural language explanation like the user expects. We therefore, use natural
language descriptions collected in which achieved superior performance on
zero-shot learning compared to attributes.
Generating Visual Explanations
p(w1|w0,I,C)
Classifier
Deep Finegrained Classifier
Relevance Loss
p(w1|w0,I,C)
p(w2|w0:1,I,C)
p(wT|w0:T-1,I,C)
p(w2|w0:1,I,C)
p(wT|w0:T-1,I,C)
wT-1: beak
Discriminative Loss
Sampled Sentence:
“a red bird with black
Target Sentence
“a bright red bird with an
orange beak.”
Compact Bilinear
Classifier
Image Category:
Fig. 3. Training our explanation model. Our explanation model diﬀers from other
caption models because it (1) includes the object category as an additional input and
(2) incorporates a reinforcement learning based discriminative loss
Reinforcement Learning in Computer Vision. Vision models which incorporate algorithms from reinforcement learning, speciﬁcally how to backpropagate
through a sampling mechanism, have recently been applied to visual question
answering and activity detection . Additionally,
 use a sampling
mechanism to attend to speciﬁc image regions for caption generation, but use
the standard cross-entropy loss during training.
Visual Explanation Model
Our visual explanation model (Figure 3) aims to produce an explanation which
(1) describes visual content present in a speciﬁc image instance and (2) contains appropriate information to explain why an image instance belongs to a
speciﬁc category. We ensure generated descriptions meet these two requirements
for explanation by including both a relevance loss (Figure 3, bottom right) and
discriminative loss (Figure 3, top right). Our main technical contribution is the
inclusion of a loss which acts on sampled word sequences during training. Our
proposed loss enables us to enforce global sentence constraints on sentences and
by applying our loss to sampled sentences, we ensure that the ﬁnal output of
our system fulﬁlls our criteria for an explanation. In the following sections we
consider a sentence to be a word sequence comprising either a complete sentence
or a sentence fragment.
Relevance Loss
Image relevance can be accomplished by training a visual description model.
Our model is based on LRCN , which consists of a convolutional neural network, which extracts powerful high level visual features, and two stacked recurrent networks (speciﬁcally LSTMs), which learn how to generate a description
conditioned on visual features. During inference, the ﬁrst LSTM receives the
previously generated word wt−1 as input (at time t = 0 the model receives
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
a “start-of-sentence” token), and produces an output lt. The second LSTM, receives the output of the ﬁrst LSTM lt as well as an image feature f and produces
a probability distribution p(wt) over the next word. At each time step, the word
wt is generated by sampling from the distribution p(wt). Generation continues
until an “end-of-sentence” token is generated.
We propose two modiﬁcations to the LRCN framework to increase the image relevance of generated sequences (Figure 3, top left). First, our explanation
model uses category predictions as an additional input to the second LSTM in
the sentence generation model. Intuitively, category information can help inform
the caption generation model which words and attributes are more likely to occur
in a description. For example, if the caption generation model conditioned only
on images mistakes a red eye for a red eyebrow, category level information could
indicate the red eye is more likely for a given class. We experimented with a few
methods to represent class labels, but found a vector representation in which we
ﬁrst train a language model, e.g., an LSTM, to generate word sequences conditioned on images, then compute the average hidden state of the LSTM across
all sequences for all classes in the train set worked best. Second, we use rich
category speciﬁc features to generate relevant explanations.
Each training instance consists of an image, category label, and a ground
truth sentence. During training, the model receives the ground truth word wt
for each time step t ∈T. We deﬁne the relevance loss as:
log p(wt+1|w0:t, I, C)
where wt is a ground truth word, I is the image, C is the category, and N is
the batch size. By training the model to predict each word in a ground truth
sentence, the model is trained to produce sentences which correspond to image
content. However, this loss does not explicitly encourage generated sentences to
discuss discerning visual properties. In order to generate sentences which are
both image relevant and category speciﬁc, we include a discriminative loss to
focus sentence generation on discriminative visual properties of an image.
Discriminative Loss
Our discriminative loss is based on a reinforcement learning paradigm for learning with layers which require intermediate activations of a network to be sampled. In our formulation, we ﬁrst sample a sentence and then input the sampled
sentence into a discriminative loss function. By sampling the sentence before
computing the loss, we ensure that sentences sampled from our model are more
likely to be class discriminative. We ﬁrst overview how to backpropagate through
the sampling mechanism, then discuss how we calculate the discriminative loss.
The overall function we minimize in the explanation network weights W
is LR −λE ˜
w∼p(w) [RD( ˜w)], a linear combination of the relevance loss LR and
the expectation of the negative discriminator reward −RD( ˜w) over descriptions
˜w ∼p(w|I, C), where p(w|I, C) is the model’s estimated conditional distribution
Generating Visual Explanations
over descriptions w given the image I and category C. Since this expectation
over descriptions is intractable, we estimate it at training time using Monte
Carlo sampling of descriptions from the categorical distribution given by the
model’s softmax output at each timestep. As a discrete distribution, the sampling
operation for the categorical distribution is non-smooth in the distribution’s
parameters {pi}, so the gradient ∇W RD( ˜w) of the reward RD for a given sample
˜w with respect to the weights W is undeﬁned.
Following REINFORCE , we make use of the following equivalence property of the expected reward gradient:
w∼p(w) [RD( ˜w)] = E ˜
w∼p(w) [RD( ˜w)∇W log p( ˜w)]
In the reformulation on the right-hand side, the gradient ∇W log p( ˜w) is welldeﬁned: log p( ˜w) is the log-likelihood of the sampled description ˜w, just as LR
was the log-likelihood of the ground truth description. In this case, however, the
sampled gradient term is weighted by the reward RD( ˜w), pushing the weights
to increase the likelihood assigned to the most highly rewarded (and hence most
discriminative) descriptions.
Therefore, the ﬁnal gradient we compute to update the weights W, given a
description ˜w sampled from the model’s softmax distribution, is:
∇W LR −λRD( ˜w)∇W log p( ˜w).
RD( ˜w) should be high when sampled sentences are discriminative. We deﬁne
our reward simply as RD( ˜w) = p(C| ˜w), or the probability of the ground truth
category C given only the generated sentence ˜w. By placing the discriminative
loss after the sampled sentence, the sentence acts as an information bottleneck.
For the model to produce an output with a large reward, the generated sentence
must include enough information to classify the original image properly. For the
sentence classiﬁer, we train a single layer LSTM-based classiﬁcation network
to classify ground truth sentences. Our sentence classiﬁer correctly predicts the
class of unseen validation set sentences 22% of the time. This number is possibly
low because descriptions in the dataset do not necessarily contain discriminative
properties (e.g., “This is a white bird with grey wings.” is a valid description
but can apply to multiple bird species). Nonetheless, we ﬁnd that this classiﬁer
provides enough information to train our explanation model. We do not update
the sentence classiﬁer weights when training our explanation model.
Experimental Setup
Dataset. In this work, we employ the Caltech UCSD Birds 200-2011 (CUB)
dataset which contains 200 classes of North American bird species and 11,788
images in total. A recent extension to this dataset collected 5 sentences for
each of the images. These sentences do not only describe the content of the
image, e.g., “This is a bird”, but also gives a detailed description of the bird,
e.g., “that has a cone-shaped beak, red feathers and has a black face patch”.
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
Unlike other image-sentence datasets, every image in the CUB dataset belongs
to a class, and therefore sentences as well as images are associated with a single
label. This property makes this dataset unique for the visual explanation task,
where our aim is to generate sentences that are both discriminative and classspeciﬁc. We stress that sentences collected in were not collected for the task
of visual explanation. Consequently, they do not explain why an image belongs
to a certain class, but rather include discriptive details about each bird class.
Implementation. For image features, we extract 8,192 dimensional features
from the penultimate layer of the compact bilinear ﬁne-grained classiﬁcation
model which has been pre-trained on the CUB dataset and achieves an accuracy of 84%. We use one-hot vectors to represent input words at each time step
and learn a 1, 000-dimensional embedding before inputting each word into the a
1000-dimensional LSTM. We train our models using Caﬀe , and determine
model hyperparameters using the standard CUB validation set before evaluating
on the test set. All reported results are on the standard CUB test set.
Baseline and Ablation Models. In order to investigate our explanation
model, we propose two baseline models: a description model and a deﬁnition
model. Our description baseline is trained to generate sentences conditioned
only on images and is equivalent to LRCN except we use features from a
ﬁne-grained classiﬁer. Our deﬁnition model is trained to generate sentences using only the image label as input. Consequently, this model outputs the same
sentence for diﬀerent image instances of the same class. By comparing these
baselines to our explanation model, we demonstrate that our explanation model
is both more image and class relevant, and thus generates superior explanations.
Our explanation model diﬀers from a description model in two key ways.
First, in addition to an image, generated sentences are conditioned on class predictions. Second, our explanations are trained with a discriminative loss which
enforces that generated sentences contain class speciﬁc information. To understand the importance of these two contributions, we compare our explanation
model to an explanation-label model which is not trained with the discriminative
loss, and to an explanation-discriminative model which is not conditioned on the
predicted class. By comparing our explanation model to the explanation-label
model and explanation-discriminative model, we demonstrate that both class information and the discriminative loss are important in generating descriptions.
Metrics. To evaluate our explanation model, we use both automatic metrics
and a human evaluation. Our automatic metrics rely on the common sentence
evaluation metrics, METEOR and CIDEr . METEOR is computed by
matching words in generated and reference sentences, but unlike other common
metrics such as BLEU , uses WordNet to also match synonyms. CIDEr
measures the similarity of a generated sentence to reference sentence by counting
common n-grams which are TF-IDF weighted. Consequently, the metric rewards
sentences for correctly including n-grams which are uncommon in the dataset.
A generated sentence is image relevant if it mentions concepts which are
mentioned in ground truth reference sentences for the image. Thus, to mea-
Generating Visual Explanations
sure image relevance we simply report METEOR and CIDEr scores, with more
relevant sentences producing higher METEOR and CIDEr scores.
Measuring class relevance is considerably more diﬃcult. We could use the
LSTM sentence classiﬁer used to train our discriminative loss, but this is an unfair metric because some models were trained to directly increase the accuracy as
measured by the LSTM classiﬁer. Instead, we measure class relevance by considering how similar generated sentences for a class are to ground truth sentences for
that class. Sentences which describe a certain bird class, e.g., “cardinal”, should
contain similar words and phrases to ground truth “cardinal” sentences, but not
ground truth “black bird” sentences. We compute CIDEr scores for images from
each bird class, but instead of using ground truth image descriptions as reference
sentences, we use all reference sentences which correspond to a particular class.
We call this metric the class similarity metric.
More class relevant sentences should result in a higher CIDEr scores, but it
is possible that if a model produces better overall sentences it will have a higher
CIDEr score without generating more class relevant descriptions. To further
demonstrate that our sentences are class relevant, we also compute a class rank
metric. To compute this metric, we compute the CIDEr score for each generated
sentence and use ground truth reference sentences from each of the 200 classes
in the CUB dataset as references. Consequently, each image is associated with
a CIDEr score which measures the similarity of the generated sentences to each
of the 200 classes in the CUB dataset. CIDEr scores computed for generated
sentences about cardinals should be higher when compared to cardinal reference
sentences than when compared to reference sentences from other classes.
We choose to emphasize the CIDEr score when measuring class relevance
because it includes the TF-IDF weighting over n-grams. Consequently, if a bird
includes a unique feature, such as “red eyes”, generated sentences which mention this attribute should be rewarded more than sentences which just mention
attributes common across all bird classes.
The ultimate goal of an explanation system is to provide useful information
to a human. We therefore also consulted experienced bird watchers to rate our
explanations against our two baseline and ablation models. We provided a random sample of images in our test set with sentences generated from each of our
ﬁve models and asked the bird watchers to rank which sentence explained the
classiﬁcation best. Consulting experienced bird watchers is important because
some sentences may list correct, but non-discriminative, attributes. For example, a sentence “This is a Geococcyx because this bird has brown feathers and a
brown crown.” may be a correct description, but if it does not mention unique
attributes of a bird class, it is a poor explanation. Though it is diﬃcult to expect
an average person to infer or know this information, experienced bird watchers
are aware of which features are important in bird classiﬁcation.
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
Table 1. Comparison of our explanation model to our deﬁnition and description baseline, as well as the explanation-label and explanation-discriminative (explanation-dis. in
the table) ablation models. We demonstrate that our generated explanations are image
relevant by computing METEOR and CIDEr scores (higher is better). We demonstrate
class relevance using a class similarity metric (higher is better) and class rank metric
(lower is better) (see Section 4 for details). Finally, we ask experienced bird watchers
to rank our explanations. On all metrics, our explanation model performs best.
Image Relevance Class Relevance Best Explanation
METEOR CIDEr Similarity
Bird Expert Rank
Description
Explanation-Label
Explanation-Dis.
Explanation
We demonstrate that our model produces visual explanations by showing that
our generated explanations fulﬁll the two aspects of our proposed deﬁnition of
visual explanation and are image relevant and class relevant. Furthermore, we
demonstrate that by training our model to generate class speciﬁc descriptions, we
generate higher quality sentences based on common sentence generation metrics.
Quantitative Results
Image Relevance. Table 5, columns 2 & 3, record METEOR and CIDEr scores
for our generated sentences. Importantly, our explanation model has higher ME-
TEOR and CIDEr scores than our baselines. The explanation model also outperforms the explanation-label and explanation-discriminative model suggesting
that both label conditioning and the discriminative loss are key to producing
better sentences. Furthermore, METEOR and CIDEr are substantially higher
when including a discriminative loss during training (compare rows 2 and 4 and
rows 3 and 5) demonstrating that including this additional loss leads to better
generated sentences. Surprisingly, the deﬁnition model produces more image relevant sentences than the description model. Information in the label vector and
image appear complimentary as the explanation-label model, which conditions
generation both on the image and label vector, produces better sentences.
Class Relevance. Table 5, columns 4 & 5, record the class similarity and class
rank metrics (see Section 4 for details). Our explanation model produces a higher
class similarity score than other models by a substantial margin. The class rank
for our explanation model is also lower than for any other model suggesting
that sentences generated by our explanation model more closely resemble the
correct class than other classes in the dataset. We emphasize that our goal is to
produce reasonable explanations for classiﬁcations, not rank categories based on
our explanations. We expect the rank of sentences produced by our explanation
Generating Visual Explanations
Fig. 4. Visual explanations generated by our system. Our explanation model produces
image relevant sentences that also discuss class discriminative attributes.
model to be lower, but not necessarily rank one. Our ranking metric is quite
diﬃcult; sentences must include enough information to diﬀerentiate between
very similar bird classes without looking at an image, and our results clearly
show that our explanation model performs best at this diﬃcult task. Accuracy
scores produced by our LSTM sentence classiﬁer follow the same general trend,
with our explanation model producing the highest accuracy (59.13%) and the
description model producing the lowest accuracy (22.32%).
Explanation. Table 5, column 6 details the evaluation of two experienced bird
watchers. The bird experts evaluated 91 randomly selected images and answered
which sentence provided the best explanation for the bird class. Our explanation
model has the best mean rank (lower is better), followed by the description
model. This trend resembles the trend seen when evaluating class relevance.
Additionally, all models which are conditioned on a label (lines 1, 3, and 5) have
lower rank suggesting that label information is important for explanations.
Qualitative Results
Figure 4 shows sample explanations produced by ﬁrst outputing a declaration
of the predicted class label (“This is a warbler...”) and then a justiﬁcation conjunction (e.g., “because”) followed by the explantory text sentence fragment
produced by the model described above in Section 3. Qualitatively, our explanation model performs quite well. Note that our model accurately describes ﬁne
detail such as “black cheek patch” for “Kentucky warbler” and “long neck” for
“pied billed grebe”. For the remainder of our qualitative results, we omit the
class declaration for easier comparison.
Comparison of Explanations, Baselines, and Ablations. Figure 5 compares sentences generated by our deﬁnition and description baselines, explanationlabel and explanation-discriminative ablations and explanation model. Each
model produces reasonable sentences, however, we expect our explanation model
to produce sentences which discuss class relevant attributes. For many images,
the explanation model mentions attributes that not all other models mention. For
example, in Figure 5, row 1, the explanation model speciﬁes that the “bronzed
cowbird” has “red eyes” which is a rarer bird attribute than attributes mentioned
correctly by the deﬁnition and description models (“black”, “pointy bill”). Similarly, when explaining the “White Necked Raven” (Figure 5 row 3), the explanation model identiﬁes the “white nape”, which is a unique attribute of that bird.
Based on our image relevance metrics, we also expect our explanations to be
more image relevant. An obvious example of this is in Figure 5 row 7 where the
explanation model includes only attributes present in the image of the “hooded
merganser”, whereas all other models mention at least one incorrect attribute.
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
This is a Bronzed Cowbird because ...
Definition:
this bird is black with blue on its wings and has a long pointy beak.
Description:
this bird is nearly all black with a short pointy bill.
Explanation-Label:
this bird is nearly all black with bright orange eyes.
Explanation-Dis.:
this is a black bird with a red eye and a white beak.
Explanation:
this is a black bird with a red eye and a pointy black beak.
This is a Black Billed Cuckoo because ...
Definition:
this bird has a yellow belly and a grey head.
Description:
this bird has a yellow belly and breast with a gray crown and green wing.
Explanation-Label:
this bird has a yellow belly and a grey head with a grey throat.
Explanation-Dis.:
this is a yellow bird with a grey head and a small beak.
Explanation:
this is a yellow bird with a grey head and a pointy beak.
This is a White Necked Raven because ...
Definition:
this bird is black in color with a black beak and black eye rings.
Description:
this bird is black with a white spot and has a long pointy beak.
Explanation-Label:
this bird is black in color with a black beak and black eye rings.
Explanation-Dis.:
this is a black bird with a white nape and a black beak.
Explanation:
this is a black bird with a white nape and a large black beak.
This is a Northern Flicker because ...
Definition:
this bird has a speckled belly and breast with a long pointy bill.
Description:
this bird has a long pointed bill grey throat and spotted black and white mottled crown.
Explanation-Label:
this bird has a speckled belly and breast with a long pointy bill.
Explanation-Dis.:
this is a grey bird with black spots and a red spotted crown.
Explanation:
this is a black and white spotted bird with a red nape and a long pointed black beak.
This is a American Goldfinch because ...
Definition:
this bird has a yellow crown a short and sharp bill and a black wing with a white breast.
Description:
this bird has a black crown a yellow bill and a yellow belly.
Explanation-Label:
this bird has a black crown a short orange bill and a bright yellow breast and belly.
Explanation-Dis.:
this is a yellow bird with a black wing and a black crown.
Explanation:
this is a yellow bird with a black and white wing and an orange beak.
This is a Yellow Breasted Chat because ...
Definition:
this bird has a yellow belly and breast with a white eyebrow and gray crown.
Description:
this bird has a yellow breast and throat with a white belly and abdomen.
Explanation-Label:
this bird has a yellow belly and breast with a white eyebrow and gray crown.
Explanation-Dis.:
this is a bird with a yellow belly and a grey back and head.
Explanation:
this is a bird with a yellow breast and a grey head and back.
This is a Hooded Merganser because ...
Definition:
this bird has a black crown a white eye and a large black bill.
Description:
this bird has a brown crown a white breast and a large wingspan.
Explanation-Label:
this bird has a black and white head with a large long yellow bill and brown tarsus and feet.
Explanation-Dis.:
this is a brown bird with a white breast and a white head.
Explanation:
this bird has a black and white head with a large black beak.
Fig. 5. Example sentences generated by our baseline models, ablation models, and
proposed explanation model. Correct attributes are highlighted in green, mostly correct
attributes are highlighted in yellow, and incorrect attributes are highlighted in red. The
explanation model consistently discusses image relevant and class relevant features.
Comparing Deﬁnitions and Explanations. Figure 6 directly compares explanations to deﬁnitions for three bird categories. Explanations in the left column
include an attribute about an image instance of a bird class which is not present
in the image instance of the same bird class in the right column. Because the
deﬁnition remains constant for all image instances of a bird class, the deﬁnition
can produce sentences which are not image relevant. For example, in the second
row, the deﬁnition model indicates that the bird has a “red spot on its head”.
Though this is true for the image on the left and for many “Downy Woodpecker”
images, it is not true for the image on the right. In contrast, the explanation
model produces image relevant sentences for both images.
Training with the Discriminative Loss. To illustrate how the discriminative loss impacts sentence generation we directly compare the description model
to the explanation-discriminative model in Figure 7. Neither of these models
Generating Visual Explanations
Definition: this bird is brown and white in color
with a skinny brown beak and brown eye rings.
Explanation: this is a small brown bird with a
long tail and a white eyebrow.
Definition: this bird is brown and white in color
with a skinny brown beak and brown eye rings.
Explanation: this is a small bird with a long bill
and brown and black wings.
Definition: this bird has a white breast black
wings and a red spot on its head.
Explanation: this is a white bird with a black wing
and a black and white striped head.
Definition: this bird has a white breast black
wings and a red spot on its head.
Explanation: this is a black and white bird with
a red spot on its crown.
Definition: this bird is black with a long tail and
has a very short beak.
Explanation: this is a black bird with a small black
Definition: this bird is black with a long tail and
has a very short beak.
Explanation: this is a black bird with a long tail
feather and a pointy black beak.
This is a Marsh Wren because...
This is a Downy Woodpecker because...
This is a Shiny Cowbird because...
This is a Marsh Wren because...
This is a Downy Woodpecker because...
This is a Shiny Cowbird because...
Fig. 6. We compare generated explanations and descriptions. All explanations on the
left include an attribute which is not present on the image on the right. In contrast to
deﬁnitions, our explanation model can adjust its output based on visual evidence.
receives class information at test time, though the explanation-discriminative
model is explicitly trained to produced class speciﬁc sentences. Both models can
generate visually correct sentences. However, generated sentences trained with
our discriminative loss contain properties speciﬁc to a class more often than the
ones generated using the image description model, even though neither has access
to the class label at test time. For instance, for the class “black-capped vireo”
both models discuss properties which are visually correct, but the explanationdiscriminative model mentions “black head” which is one of the most prominent
distinguishing properties of this vireo type. Similarly, for the “white pelican” image, the explanation-discriminative model mentions the properties “long neck”
and “orange beak”, which are ﬁne-grained and discriminative.
Class Conditioning. To qualitatively observe the relative importance of image features and label features in our explanation model, we condition explanations for a “baltimore oriole”, “cliﬀswallow”, and “painted bunting” on the
correct class and incorrect classes (Figure 8). When conditioning on the “painted
bunting”, the explanations for “cliﬀswallow” and “baltimore oriole” both include colors which are not present suggesting that the “painted bunting” label
encourages generated captions to include certain color words. However, for the
“baltimore oriole” image, the colors mentioned when conditioning on “painted
bunting” (red and yellow) are similar to the true color of the oriole (yelloworange) suggesting that visual evidence informs sentence generation.
Conclusion
Explanation is an important capability for deployment of intelligent systems.
Visual explanation is a rich research direction, especially as the ﬁeld of computer vision continues to employ and improve deep models which are not easily
interpretable. Our work is an important step towards explaining deep visual
L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell
Description: this bird is black and white in color with a
orange beak and black eye rings.
Explanation-Dis.: this is a black bird with a white eye
and an orange beak.
Description: this bird has a bright blue crown and a
bright yellow throat and breast.
Explanation-Dis.: this is a yellow bird with a blue
head and a black throat.
Description: this bird has a long black bill a white
throat and a brown crown.
Explanation-Dis.: this is a black and white spotted bird
with a long tail feather and a pointed beak.
Description: this bird is blue and black in color with a
stubby beak and black eye rings.
Explanation-Dis.: this is a blue bird with a red eye and
a blue crown.
Description: this bird has a white belly and breast
black and white wings with a white wingbar.
Explanation-Dis: this is a bird with a white belly yellow
wing and a black head.
Description: this bird is white and black in color with a
long curved beak and white eye rings.
Explanation: this is a large white bird with a long
neck and a large orange beak.
This is a Black-Capped Vireo because...
This is a Crested Auklet because...
This is a Green Jay because...
This is a White Pelican because...
This is a Geococcyx because...
This is a Cape Glossy Starling because...
Fig. 7. Comparison of sentences generated using description and explanationdiscriminative models. Though both are capable of accurately describing visual
attributes, the explanation-discriminative model captures more “class-speciﬁc” attributes.
This is a Baltimore Oriole because this is a small orange bird with a black head and a small orange beak.
This is a Cliff Swallow because this is a black bird with a red throat and a white belly.
This is a Painted Bunting because this is a colorful bird with a red belly green head and a yellow throat.
This is a Baltimore Oriole because this is a small bird with a black head and a small beak.
This is a Cliff Swallow because this bird has a black crown a brown wing and a white breast.
This is a Painted Bunting because this is a small bird with a red belly and a blue head.
This is a Baltimore Oriole because this is a small bird with a black head and orange body with black wings and tail.
This is a Cliff Swallow because this bird has a black crown a black throat and a white belly.
This is a Painted Bunting because this is a colorful bird with a red belly green head and a yellow throat.
Fig. 8. We observe how explanations change when conditioning on diﬀerent classes.
Some bird categories, like “painted bunting” carry strong class information that heavily
inﬂuence the explanation.
models. We anticipate that future models will look “deeper” into networks to
produce explanations and perhaps begin to explain the internal mechanism of
deep models.
To build our explanation model, we proposed a novel reinforcement learning
based loss which allows us to inﬂuence the kinds of sentences generated with
a sentence level loss function. Though we focus on a discriminative loss in this
work, we believe the general principle of including a loss which operates on a
sampled sentence and optimizes for a global sentence property is potentially
beneﬁcial in other applications. For example, propose introducing new
vocabulary words into a captioning system. Though both models aim to optimize
a global sentence property (whether or not a caption mentions a certain concept),
neither optimizes for this property directly.
In summary, we have presented a novel framework which provides explanations of a visual classiﬁer. Our quantitative and qualitative evaluations demonstrate the potential of our proposed model and eﬀectiveness of our novel loss
Generating Visual Explanations
function. Our explanation model goes beyond the capabilities of current captioning systems and eﬀectively incorporates classiﬁcation information to produce convincing explanations, a potentially key advance for adoption of many
sophisticated AI systems.
Acknowledgements. This work was supported by DARPA, AFRL, DoD MURI
award N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Vision and Learning Center. Marcus Rohrbach was supported by a fellowship
within the FITweltweit-Program of the German Academic Exchange Service
(DAAD). Lisa Anne Hendricks is supported by an NDSEG fellowship. We thank
our experienced bird watchers, Celeste Riepe and Samantha Masaki, for helping
us evaluate our model.