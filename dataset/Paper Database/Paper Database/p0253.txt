Article type: Advanced Review
Anomaly Detection in Dynamic Networks:
Stephen Ranshous1,2, Shitian Shen1,2, Danai Koutra3, Christos Faloutsos3,
and Nagiza F. Samatova1,2,+
1Department of Computer Science, North Carolina State University
2Computer Science and Mathematics Division, Oak Ridge National Laboratory
3Computer Science Department, Carnegie Mellon University
+Corresponding author: 
Anomaly detection is an important problem with multiple applications, and thus
has been studied for decades in various research domains.
However, for
dynamic networks, no comprehensive survey exists covering the richness of
methods proposed and their differences, as well as the variety of problems
they solve. In light of this, the current survey focuses on anomaly detection
in time-evolving networks. The methods covered are classiﬁed into ﬁve categories based on their technical approaches, and subsequently broken down by
their output, i.e., the type of anomalies they detect—nodes, edges, subgraphs,
and events. Some of the methods discussed cannot easily be placed into a
single category, leveraging multiple approaches. For such cases, the method
has been classiﬁed based on the predominant approach utilized. We cover a
total of 46 methods: 9 based on communities, 2 on compression, 15 on decomposition, 11 on distance metrics, and 9 probabilistic based. Within these
categories we highlight the similarities and differences showing the wealth of
different techniques derived from similar conceptual approaches.
Introduction
Anomaly detection, or outlier detection, is the well-known problem of “ﬁnding
patterns in data that do not conform to expected behavior” . This area has been
well-covered for a variety of domains including sequence data , discrete sequence
data , and temporal data ; see also a general comprehensive overview .
However, anomaly detection in dynamic networks1 has been barely touched in existing
works . No extensive survey exists, despite the popularity and the growing
importance of the topic.
A surge for researching novel methods for real world networks that are constantly
evolving has been motivated by a number of applications. Dozens of methods specifically deisgned for dynamic environments have emerged over the last ﬁve years (see
1Throughout this paper we will use the terms networks and graphs interchangeably.
summary in Table 1 and Table 2). These techniques complement methods for static
graphs , as the latter cannot be easily adopted for dynamic networks.
They ﬁnd real world applications in numerous domains, including but not limited to:
• Detection of ecological disturbances such as wildﬁres and cyclones 
by analyzing the ecological evolution.
• Intrusion detection for individual systems and network systems 
by observing past normal behaviors and comparing new actions to the expected
• Identifying abnormal users and events in communication networks , such
as days with an abnormally high number of messages.
We present in this survey techniques for performing anomaly detection in dynamic networks in hopes to bridge the gap between the increasing number of of emerging algorithms for this important graph mining task and the lack of their comprehensive analysis. We start by introducing four different types of anomalies that these algorithms
detect, namely, anomolous nodes, edges, subgraphs, and events. We then continue
with an extensive overview of the existing methods based on the proposed taxonomy
that takes into account their underlying design principles, such as those based on graph
communities, compression, decomposition, distance metrics, and probabilistic modeling of graph features. Finally, we subcategorize each taxonomic group further based
on the types of anomalies detected.
Types of anomalies
In this section we identify and formalize four types of anomalies that arise in dynamic
networks. These categories represent the output of the methods, not the implementation
details of how they detect the anomalies, e.g., comparing consecutive time points, using
a sliding window technique, etc.
Because graphs are assumed to be dynamic, their vertices and edges can be inserted
or removed at every time step; however, for the sake of simplicity, we assume that
the node correspondence and the edge correspondence across different time steps is
resolved due to unique labeling of nodes and edges, respectively. We deﬁne a graph
series as an ordered set of graphs with a ﬁxed number of time steps, and a graph stream
as an ordered set of graphs where new time steps are continuously added. The majority of techniques we discuss assume that the input graphs are simple, undirected, and
unweighted. Hence, we assume that all graphs are of this type unless otherwise noted.
Type 1: Anomalous nodes
Problem 1. Given a ﬁxed graph series G or graph stream G, ﬁnd a subset of the
vertices such that every vertex in the subset has an “irregular” evolution compared to
the other nodes in the graph. Optionally, identify the time point(s) where the nodes are
determined to be anomalous.
Applications. Consider a graph series G where each graph Gt is a single day of network IP trafﬁc. Assume that it is known that at time t a virus spreads throughout the
system causing network congestion. By comparing the activity of each node on the
days surrounding the attack to their activity on the day of the attack, one can detect the
nodes from which the virus originated, as, at time t, those nodes should have been more
active than usual. Typical applications of this type of anomaly detection are identifying nodes that contribute the most to a discovered anomaly, such as in communication
networks (also known as attribution), and observing the shifts in community involvement .
Example. An example of nodes that do not share the same evolution pattern as the
community they initially belong to is shown in Figure 1. In the ﬁrst time step, all of
the nodes are found to be in the same community according to some extracted feature
values, which is indicated by the blue circle. In time steps two and three, there is a shift
in the values of the community as a whole, altering the distribution of the feature values. Some nodes, however, do not follow this trend and end up as outliers (anomalous
nodes), indicated by the red crosses.
Type 2: Anomalous edges
Problem 2. Given a ﬁxed graph series G or graph stream G, ﬁnd a subset of the edges
such that every edge in the subset has an “irregular” evolution. Optionally, identify
the time point(s) where they are determined to be anomalous.
There are two main types of irregular evolution for edges: (i) abnormal edge weight
evolution, where the weight of a single edge ﬂuctuates over time and has inconsistent
spikes in value, and (ii) appearance of unlikely links in a graph , where two
nodes that are not typically connected or part of the same community have an edge
added between them.
Applications. Consider a dynamic communication network where the nodes are people, the edges represent communication between two people, and the edge weight corresponds to the number of messages they exchange. Imagine persons a and b typically
message each other 10 times each day, represented by the weighted edge (a, b, 10).
Then, suddenly, they have a day where they message each other 25 times. It is natural to assume that something happened between them because of the sharp increase
in communication. This is the intuition behind edge detection. Anomaly detection of
edges appears in many areas, such as vehicle trafﬁc patterns and ﬁnding unlikely
social interactions .
Example. An example of edges that have abnormal weight evolution is shown in Figure 2,
where the anomalous edge weights are highlighted. By examining the edge weights,
one can observe that they tend to hover around a certain value, changing by ±0.05 at
most each time step. However, at time t = 2, edge (3, 4) suddenly has a weight of
0.77, radically different from it’ average weight, 0.21, in the rest of the time points.
Similarly, edge (1, 4) at t = 3 has an edge weight of 0.93, which is roughly double its
typical value (0.48).
Type 3: Anomalous subgraphs
Problem 3. Given a ﬁxed graph series G or graph stream G, ﬁnd all subgraphs that
have “unusual” behavior. Optionally, identify the time point(s) where they are determined to be anomalous.
Applications. Given a dynamic network representing vehicle trafﬁc, where nodes are
street intersections, edges are the streets, and the edge weights are the travel times for
that particular road, how can trafﬁc accidents be identiﬁed? Accidents typically block
trafﬁc on the roads surrounding the intersection where they occur, increasing the travel
time by a substantial amount for those roads. If we were able to identify regions of the
graph that exhibited this type of behavior, we would be able to identify such accidents
automatically. This is just one possible application area , others include detecting
the change in communities in the food web , and detecting changes and threats in
social networks .
Example. Figure 3 shows an example of two types of community based anomalous
subgraphs. The shrunken community shown in Figure 3a is when a single community
loses a considerable number of its members. Closely related is the idea of a split
community shown in Figure 3b, when a single community divides into several distinct
communities.
Type 4: Event and change detection
Problem 4 (Event detection). Given a ﬁxed graph series G or graph stream G, ﬁnd a
time point at which the graph exhibits behavior sufﬁciently different from the others.
Applications. In the example scenario presented in the node anomaly section, we assumed that the time point that marked an event was known. However, in real world
applications the event points are seldom known, requiring methods to discover them
from the data. Event detection has been used to detect when the collective motions
in molecular dynamics simulations suddenly change , to ﬁnd frames in a video
that are unlike the others , and to detect disturbances in the ecosystem (e.g.,
wildﬁres) .
Example. A simple example is given in Figure 4a. At time t = 3, there is clearly a large
structural change in the graph, increasing the connectivity to almost a clique. However,
at the next time step, the graph is back to normal, having only sparse connections
between the nodes.
Problem 5 (Change detection). Given a ﬁxed graph series G or graph stream G, ﬁnd
a time point at which a substantial change in the graph persists until the next point of
It is important to note the distinction between event and change detection. While events
represent isolated incidents, change points mark a point in time where the entire behavior of the graph changes and the difference is maintained until the next change point.
Applications. Consider the DBLP network where authors are linked to the venues they
publish per year. By proﬁling the authors, one can spot that most of them publish in
speciﬁc conferences consistently, while few authors act as “bridges” and switch areas
of interest and venues over the course of their career . Change detection has been
also applied to mobile communication graphs and trafﬁc networks .
Example. Figure 4b illustrates the concept of change detection. Time steps t = 1, 2
have approximately the same structure, with sparse connectivity between the nodes.
However, at time t = 3 there is a large structural change, similar to that in Figure 4a,
except the new structure is maintained.
Community based methods
Community based methods track the evolution of communities and their associated
nodes in the graphs over time . Various community-based approaches differ in two main points: (i) in the aspects of the community structure they analyze, e.g.,
the connectivity within each community vs. how the individual nodes are assigned to
the communities at each time step; and (ii) in the deﬁnitions of communities they use,
e.g., soft communities where each node has a probability of belonging to each community vs. disjoint communities where each node is placed into at most one community
in the graph. Moreover, based on how the community evolution is viewed, it can be
applied for the detection of different anomaly types. For example, a rapid expansion or
contraction of a community could indicate that the speciﬁc subgraph for that community is undergoing drastic changes, whereas a drop in the number of communities by
two corresponds to an abnormal event.
Node detection
Main Idea. A group of nodes that belong to the same community are expected to
exhibit similar behavior. Intuitively this means that if at consecutive time steps one
node in the community has a signiﬁcant number of new edges added, the other nodes
in the community would also have a signiﬁcant number of new edges. If the rest of the
nodes in the community did not have new edges added, the node that did is anomalous.
Based on this logic, using soft community matching and looking at each community
individually, the average change in belongingness (the probability the node is part of
the community) for each node can be found for consecutive time steps. Nodes whose
change in belongingness is different from the average change of the nodes in the community are said to be evolutionary community outliers .
The changes in a node’s community belongingness may form a pattern over time, called
a soft temporal pattern. In , a Non-Negative Matrix Factorization approach in combination with the Minimum Description Length principle are used to automatically detect node roles and build transition models. The community memberships are found
slightly differently in , where Xmeans is used. However, in both cases, the patterns
common across all of the nodes in the graph are extracted, then each node’s patterns
are compared to the extracted ones. If a node’s patterns are not similar to any of the
extracted common patterns, then the node is anomalous. Later extended to networks
derived from heterogeneous data sources , the two step approach in is modiﬁed to an alternating iterative approach. Instead of extracting patterns ﬁrst and then
identifying outliers, the patterns and outliers are found in an alternating fashion (pattern
extraction →outlier detection →pattern extraction →· · · ) until the outliers discovered
do not change on consecutive iterations. By alternating back and forth between pattern
extraction and outlier detection, the algorithm accounts for the affect the outliers have
on the communities discovered .
If the edge weights are considered as the connection strength between two nodes, then
the removal of a node with a high edge weight impacts the node it is connected to more
than the removal of a node with a low edge weight . The “closeness” of nodes is
deﬁned as a function of the weighted path between them instead of strictly the length
of the path (e.g., one-hop paths). A node’s “corenet” is built based on the closeness
values of nodes. Neighboring nodes within two hops and with a weighted path higher
than a threshold value are part of the corenet. At each time step every node is given an
outlier score based on the change in its corenet, and the top outlier scores are declared
anomalous.
Subgraph detection
Main Idea. Instead of looking at individual nodes and their community belongingness,
entire subgraphs that behave abnormally can be found by observing the behavior of
communities themselves over time.
Six different types of community-based anomalies have been proposed : shrink,
grow, merge, split, born, and vanish. To detect such anomalies, time steps are compared
using graph and community representatives. Conversely, instead of ﬁnding changes,
communities that are conserved, or stable, can be identiﬁed. Constructing multiple
networks at each time step based on different information sources, communities can be
conserved across time and networks. Networks which behave similarly can be grouped
using clustering or prior knowledge. If a community is conserved across time and the
networks within its group, but has no corresponding community in any other group of
networks, then the community is anomalous ; two communities are considered corresponding communities if they have a certain percentage of their vertices in common.
Unlike which consider only the structure of the network, in social networks
there is often more information available. For example, in the Twitter user network,
clusters can be found based on the content of tweets (edges), as well as the users (vertices) involved. If the fraction of the tweets (edges) added during the recent time window for a cluster is much larger than the fraction of tweets (edges) added anytime
before the window, then this inﬂux is declared as an evolution event for that cluster .
A cluster that experiences an evolution event is marked as an anomalous subgraph at
the time when the evolution event occurs.
Change detection
Main Idea. Changes are detected by partitioning the streaming graphs into coherent
segments. The beginning of each segment represents a detected change.
The segments are found online by comparing the node partitioning of the newest graph
to the partitioning found for the graphs in the current segment. Node partitioning can be
achieved with many methods, but in it is done using the relevance matrix computed
by random walks with restarts and modularity maximization. When the partitioning of
the new graph is much different than the current segment’s, a new segment begins and
the time point for the new graph is output as a detected change. The similarity of two
partitions is computed as their Jaccard coefﬁcient.
Compression based methods
The methods discussed in this section are all based on the minimum description length
(MDL) principle . The minimum description length principle, and compression
techniques based on this principle, exploit patterns and regularity in the data to achieve
a compact graph representation . Applying this principle to graphs is done by
viewing the adjacency matrix of a graph as a single binary string, ﬂattened in row or
column major order. If the rows and columns of the matrix can be rearranged such
that the entropy of the binary string representation of the matrix is minimized, then
the compression cost (also known as encoding cost ) is minimized. Anomalies can
then be deﬁned as graphs or edges that inhibit compressibility.
Edge detection
Main Idea. An edge is considered anomalous if the compression of a subgraph has
higher encoding cost when the edge is included than when it is omitted.
Node partitioning can be done by rearranging the rows and columns in the adjacency
matrix. In , a two step alternating iterative method is used for automatic partitioning of the nodes. In the ﬁrst step, for a ﬁxed number of partitions, the nodes are
assigned to them so that the encoding cost is minimized. The second step iteratively
splits the partitions with high entropy into two. Any node whose removal from the
original partition would result in a decrease in entropy is removed from that partition
and placed into the new one. Once the method has converged, meaning steps 1 and 2
are unable to ﬁnd an improvement, the edges can be given outlierness scores. The score
for each edge is computed by comparing the encoding cost of the matrix including the
edge to the encoding cost if the edge is removed.
Change detection
Main Idea. The main idea is that consecutive time steps that are very similar can
be grouped together leading to low compression cost. Increase in the compression
cost means that the new time step differs signiﬁcantly from the previous ones, and thus
signiﬁes a change.
To detect changes in a graph stream, similar consecutive time steps can be grouped
into segments. When considering the next graph in the stream, it can either be grouped
with the graphs in the current segment, or it can be the beginning of a new one. The
decision to start a new segment is made by comparing the encoding cost of the current
segment without the next graph to the encoding cost of the segment if the next graph
were included. If the node partitioning for the new graph is very similar to the node
partitioning of the graphs in the segment then the encoding cost will not change much.
However, if the partitions are very different, the encoding cost would increase because
of the increase in entropy. Changes in the graph stream are the time points when a new
segment begins . This method is also parameter-free.
Decomposition based methods
These techniques represent the set of graphs as a tensor, most easily thought of as a
multidimensional array, and perform its factorization or dimensionality reduction. The
most straightforward method of modeling a dynamic graph as a tensor is to create a
dimension for each graph aspect of interest, e.g., a dimension for time, source nodes,
destination nodes, etc. For example, modeling the Enron email dataset can be done
using a 4-mode tensor, with dimensions for sender, recipient, keyword, and date. The
element (i, j, k, l) is 1 if there exists an email that is sent from sender i to recipient j
with keyword k on day l, otherwise it is 0.
Similar to compression techniques, decomposition techniques search for patterns or
regularity in the data to exploit. One of the most popular methods for matrices (2-mode
tensors) is singular value decomposition (SVD) , and for higher order tensors (≥3
modes) is PARAFAC , a generalization of SVD. The main differences between the
decomposition based methods are whether they use a matrix or a higher order tensor,
how the tensor is constructed, and the method of decomposition.
Node detection
Main Idea. Matrix decomposition is used to obtain activity vectors per node. A node
is characterized as anomalous if its activity changes signiﬁcantly between consecutive
time steps.
Due to the computational complexity of performing principal component analysis on
the entire graph, it is advantageous to apply it locally. One approach is to have each
node maintain an edge correlation matrix M, which has one row and column for every
neighbor of the node. The value of an entry in the matrix for node i, M(j, k), is
the correlation between the weighted frequencies of the edges (i, j) and (i, k). The
weighted frequencies are found using a decay function, where edges that occured more
recently have a higher weight. The largest eigenvalue and its corresponding vector
obtained by performing PCA on M are summaries of the activity of the node and the
correlation of its edges, respectively. The time series formed by ﬁnding the changes in
these values are used to compute a score for each node at each time step. Nodes that
have a score above a threshold value are output as anomalies at that time .
Event detection
Main Idea. There are two main approaches: (a) Tensor decomposition approximates
the original data in a reduced dimensionality, and the reconstruction error is an indicator of how well the original data is approximated. Sub-tensors, slices, or individual
ﬁbers in the tensor that have high reconstruction error do not exhibit behavior typical
of the surrounding data, and reveal anomalous nodes, subgraphs, or events. (b) Singular values and vectors, as well as eigenvalues and eigenvectors are tracked over time
in order to detect signiﬁcant changes that showcase anomalous nodes.
Using the reconstruction error as an indicator for anomalies has been employed for
detecting times during molecular dynamics simulations where the collective motions
suddenly change , ﬁnding frames in a video which are unlike the others , and
identifying data that do not ﬁt any concepts .
To address the intermediate blowup problem—when the input tensor and output tensors
exceed memory capacity during the computation,—Memory-Efﬁcient Tucker (MET)
decomposition was proposed , based on the original Tucker decomposition .
The Tucker decomposition approximates a higher order tensor using a smaller core
tensor (thought of as a compressed version of the original tensor) and a matrix for
every mode (dimension) of the original tensor. Similarly, methods have been developed for ofﬂine, dynamic, and streaming tensor analysis , in addition to static and
sliding window based methods . These extensions allow the method to operate on
continuous graph streams as well as those with a ﬁxed number of time points. Compact Matrix Decomposition (CMD) computes a sparse low rank approximation of a
given matrix. By applying CMD to every adjacency matrix in the stream, a time series
of reconstruction values is created and used for event detection . Colibri and
ParCube can be used in the same fashion and provide a large increase in efﬁciency.
The PARAFAC decomposition has been shown to be effective at spotting anomalies in
tensors as well .
A probabilistic model that represents the graph expected at a given time step can be
created using the Chung-Lu random graph model . Taking the difference between
the real graph’s adjacency matrix and the expected graphs forms a residual matrix.
Anomalous time windows are found by performing SVD on the residual matrices—
on which a linear ramp ﬁlter has been applied—and by analyzing the change in the
top singular values . The responsible nodes are identiﬁed via inspection of the
right singular vectors. More accurate graph models that also consider attributes are
proposed in .
Performing PCA on the data, the calculated eigenvectors can be separated into “normal” and “anomalous” sets by projecting the data onto each eigenvector, and ﬂagging
as anomalous the projections that contain data points outside 3 standard deviations of
the values. At each time step, the components are found by projecting the data onto
its normal and anomalous subspace. Changes are detected when modiﬁcations in the
anomalous components are above a threshold in trafﬁc ﬂow data . Expanding on
this method, joint sparse PCA and graph-guided joint sparse PCA were developed to
localize the anomalies . The responsible nodes are more easily identiﬁed by using
a sparse set of components for the anomalous set. Nodes are given an anomaly score
based on the values of their corresponding row in the abnormal subspace. As a result
of the anomalous components being sparse, the nodes that are not anomalous receive
a score of 0. Due to the popularity of PCA in trafﬁc anomaly detection, a study was
performed identifying and evaluating the main challenges of using PCA .
Change detection
Main Idea. The activity vector of a graph is the principal component, the left singular
vector corresponding to the largest singular value obtained by performing SVD on the
weighted adjacency matrix. A change point is when an activity vector is substantially
different from the “normal activity” vector.
The normal activity vector is the left singular vector obtained by performing SVD
on the matrix formed by the activity vectors for the last W time steps. Each time
point is given a score based on the difference between the normal vector and activity
vector. Anomalies can be found online using a dynamic thresholding scheme, where
time points with a score above the threshold are output as changes . The nodes
responsible are found by calculating the ratio of change between the normal and activity
vectors. The nodes that correspond to the indexes with the largest change are labeled
anomalous. Similar approaches have used the activity vector of a node-to-node feature
correlation matrix , and a node-to-node correlation matrix based on the similarity
between node’s neighbors .
Distance based methods
Using the notion of distance as a metric to measure change is natural. Two objects
that have a small difference in a measured metric can be said to be similar. The metrics
measured in graphs are typically structural features, such as the number of nodes. Once
the summary metrics are found for each graph, the difference or similarity, which are
inversely related, can be calculated. The variation in the algorithms lies in the metrics
chosen to extract and compare, and the methods they use to determine the anomalous
values and corresponding graphs.
Edge detection
Main Idea. If the evolution of some edge attribute (e.g., edge weight) differs from the
“normal” evolution, then the corresponding edge is characterized as anomalous.
In , a dynamic road trafﬁc network whose edge weights vary over time is studied.
The similarity between the edges over time is computed using a decay function that
weighs the more recent patterns higher. At each time step, an outlierness score is
calculated for each edge based on the sum of the changes in similarity scores.
Viewing the network at a stream of edges, meaning the network does not have a ﬁxed
topology as the road trafﬁc network did, the frequency and persistence of an edge can
be measured and used as an indicator of its novelty. The persistence of an edge is how
long it remains in the graph once it is added. Set system discrepancy is one way
to measure the persistence and frequency of the edges. When a new edge arrives, its
discrepancy is calculated and compared to the mean discrepancy value of the active
edge set. If the weighted discrepancy of the edge is more than a threshold level greater
than the mean discrepancy, the edge is declared novel (anomalous) . This method
also allows anomalous nodes and subgraphs to be detected.
Subgraph detection
Main Idea. A subgraph with many “anomalous” edges is deemed anomalous.
Fixed subgraphs over a time interval are scored by the sum of their edge weights over
that time interval. Signiﬁcant anomalous regions (SARs) are the ﬁxed subgraphs that
have the highest scores. Every edge at every time stamp is given its own anomaly score,
which is a function of the probability of seeing that particular edge weight on that particular edge given the distribution of weights for that edge over all graphs in the series.
Mining SARs is analogous to ﬁnding the heaviest dynamic subgraphs (HDS) . A
similar approach mines weighted frequent subgraphs in network trafﬁc, where the edge
weights correspond to the anomaly contribution of that edge .
Event detection
Main Idea. Provided a function f(Gi, Gj) that measures the distance between two
graphs, a time series of distance values can be constructed by applying the function on
consecutive time steps in the series. Anomalous values can then be extracted from this
time series using a number of different heuristics, such as selecting the top k or using
a moving average threshold.
Extracting features from the graphs is a common technique to create a summary of the
graph in a few scalar values, its signature. Local features are speciﬁc to a single node
and its egonet (the subgraph induced by itself and its one-hop neighbors), such as the
node or egonet degree. Global features are derived from the graph as a whole, such as
the graph radius. The local features of every node in the graph can be agglomerated
into a single vector, the signature vector, of values that describe the graph using the
moments of distribution (such as mean and standard deviation) of the feature values.
In , the similarity between two graphs is the Canberra distance, a weighted version of the L1 distance, between the two signature vectors. A similar approach is used
in to detect abnormal times in trafﬁc dispersion graphs. Instead of an agglomeration of local features, it extracts global features from each graph, and any graph with a
feature value above a threshold is anomalous.
As an alternative to extracting multiple features from the graph, the pairwise node
afﬁnity scores may be used. Pairwise node afﬁnity scores are a measure of how much
each node inﬂuences another node, and can be found using fast belief propagation .
In the scores are calculated for two consecutive time steps, and the similarity between the two graphs is the rooted Euclidean distance (Matusita distance) between the
score matrices. A moving threshold is set on the time series of similarity scores using
quality control with individual moving range. The exponential weighted moving average has also been used as a way to dynamically set the threshold, tested on distribution
features extracted from Wikipedia revision logs .
Complementary to feature similarity, one can look at the structural differences between
graphs to identify the magnitude of change. These methods focus on the function that
deﬁnes the distance between graphs instead of ﬁnding the optimal features to use as
summaries. Many metrics have been developed and tested to quantify the differences
between graphs. Ten different distance functions (e.g., weight distance, maximum
common subgraph (MCS) weight difference, graph edit distance) were evaluated on
TCP/IP trafﬁc graphs with known anomalies (ground truth). Box-Jenkins autoregressive moving average (ARMA) modeling was used to set the threshold and identify
anomalies. Of the ten distance functions tested, the MCS based methods performed the
best . In , ﬁve different distance scoring functions were tested on web graphs
with speciﬁc types of anomalies: missing subgraph, missing vertices, and connectivity
change. The best method was the signature similarity, which is done by extracting a
signature vector from each graph and ﬁnding the distance between them. The features
used were each vertex and its PageRank value, and each edge (u, v) with a weight of
u’s PageRank divided by the number of outlinks from u. A ﬁxed threshold was set to
ﬁnd graphs with abnormally low similarity scores.
Instead of ﬁnding the difference between two consecutive graphs, events can be detected using the time series of robustness values for each graph. Robustness is a measure of the connectivity of the graph. A graph with a high robustness will retain the
same general structure and connectivity even when some nodes or edges are removed.
Finding events is then ﬁnding when the robustness value changes signiﬁcantly .
Probabilistic model based methods
With a foundation in probability theory, distributions, and scan statistics, these methods typically construct a model of what is considered “normal” or expected. Deviations from this model are ﬂagged as anomalous. The type of model used, how it is
constructed, what it is modeling, and the method for determining outliers is what differentiates these approaches.
Anomalous node detection
Main Idea. There are three main approaches: (a) Modeling some node features using
probabilistic approaches, and tracking the nodes that do not ﬁt the models; (b) Building
scan statistics time series and detecting points that are several standard deviations
away from the mean; (c) Node classication.
One way to model the relationship between nodes is considering the communication
between them as a Bayesian discrete time counting process. A model is ﬁt to the expected number of communications (weight) for each edge and is continuously updated
as new graphs are considered. At any time point when the actual number of communications is different from the expected number by a statistically signiﬁcant amount both
nodes are added to the set of anomalies . Statistical signiﬁcance is determined by
a predictive p-value dropping below a threshold.
Scan statistics are often called “moving window analysis,” where the local maximum
or minimum of a measured statistic is found in speciﬁc regions of the data. In a graph
a scan statistic can be considered as the maximum of a graph invariant feature, such as
the number of edges, found for each node and its neighborhood in the graph. The local
statistic for each node is normalized using the mean and standard deviation of its recent
values. The scan statistic of a graph is the maximum of all of the local scan statistics.
Normalizing the values accounts for the history of each node, meaning the statistic for
each node is relative to its own past instead of the values of the other nodes. Building
a standardized time series of the scan statistic values, any value that is ﬁve standard
deviations away from the mean of the series is considered an event. The node most
responsible is identiﬁed as the one that was chosen for the scan statistic value for the
entire graph .
The Markov Random Field Model (MRF) is used to uncover the states for nodes and
infer the maximum likelihood assignment by belief propagation algorithm. In ,
anomalies (fraudsters) are uncovered in an online auction network by discovering bipartite cores, which are posited to be the interaction behavior between fraudsters and
accomplices. It incrementally updates the model as new edges are added, taking advantage of the fact that an edge insertion or removal will affect only a local subgraph.
Nodes that are assigned the label of fraudster are anomalous.
Anomalous subgraph detection
Main Idea. Fixed subgraphs (e.g., like paths and stars), multigraphs, and cumulative
graphs are used to constuct models on the expected behaviors. Deviations from the
models signify an anomalous subgraph.
To identify hacker behaviors in a network, scan statistics are combined with a Hidden
Markov Model (HMM) for edge behavior. Unlike that used neighborhoods, the
local scan statistics are based on two graph shapes, the k-path and star. Comparing
the scan statistics of a sliding window to its past values, and using an online thresholding system, local anomalies are identiﬁed . The local anomaly is the subgraph
representing the k-path or star used to derive the statistic.
Another method to model a dynamic network, instead of using a series of graphs, is to
have a single multigraph where parallel edges correspond to communication between
nodes at two different time steps. The initial multigraph is decomposed into telescoping
graphs for each time window . Telescoping graphs that are large in size, but have a
low probability of appearing, are output as anomalies.
Likewise, a cumulative graph, which included every edge seen up until the current time
step, could be used. Edge weights are calculated using a decay function where more
recent edges weigh more. The connected components in the cumulative graph for the
entire time series are found and monitored for unusual behavior. Unusual behavior
is deﬁned as a deviation from the model constructed using the cumulative graph for
expected behavior at a given time step .
Event detection
Main Idea. Deviations from the models of the graph likelihood or the distribution of
the eigenvalues reveal when an event occurs.
Maintaining a structural summary of the network by keeping multiple partitionings of
the graph can be used to detect graph outliers online . The structural summary and
partitionings allow an edge likelihood value to be calculated for every edge in the new
graph. After calculating the edge likelihoods, the graph likelihood is derived by taking
the geometric mean of the edge likelihoods. If the graph likelihood is several standard
deviations below the average of all the graphs up until that point, the graph is an outlier.
In , a similar edge likelihood approach is to construct an edge probability matrix
for each time window in the series, where each element in the matrix (i, j) is the probability of having an edge between node i and j. Using Expectation Maximization to
calculate a potential score for every sender-recipient pair, the probability of all possible source-destination node tuples can be estimated. Tuples represent an email, where
there is one sender and multiple recipients, while edges represent a link between two
individual nodes. Time windows that are anomalous are found by comparing the average log-likelihood scores for the windows. Individual emails that are abnormal can
also be found by looking at the emails with the lowest likelihood.
Based on the assumption that each node has a time series of feature values, a nodeto-node correlation matrix can be generated for each time step. The eigen equation of
the correlation matrix is compressed by keeping the largest eigenvalues and a set of
low dimension matrices (one matrix for each node). By learning the distributions of
the eigenvalues and matrices, both events and the nodes responsible can be identiﬁed.
When the eigenvalues deviate from the expected distribution an event has occured.
The nodes whose matrices deviate from the matrix distribution the most are considered
responsible .
Conclusion
In this survey we ﬁrst introduced four types of anomalies that occur in a dynamic graph (node, edge, subgraph, and events) and an example of each. Individual nodes in the graph are considered anomalous if their evolution is unlike
the rest of the nodes, see Figure 1 for an example. Anomalous edges typically
relate to irregular weight evolution or the insertion/removal of an unlikely edge,
Figure 2 shows an example of an irregular edge weight. Entire subgraphs that
behave unlike the rest of the graph, for example, in terms of community evolution or communication patterns, are anomalous. Figure 3 shows two types
of anomalous community evolutions. The last type of anomaly is points in time
which represent a large change in the graph or an isolated abnormal ﬂuctuation
in the graph behavior (event and change detection, respectively, see Figure 4).
We partitioned the methods into ﬁve main categories derived from the intuition
behind each: community, compression, decomposition, distance, and probabilistic model based. Community based methods track the evolution of communities over time, using the changes in the communities to identify anomalies.
Different methods have different deﬁnitions of a community and may look at
the evolution of a node’s community belongingness instead of the behavior of
communities themselves. Decomposition based methods try to ﬁnd a low dimensional approximation of the data represented as a tensor or a sequence
of matrices. When the approximation is poor and there is a high reconstruction error it indicates a lack of regularity, hence abnormal behavior. Performing the decomposition online and the target of the decomposition is the major difference between the methods presented. Distance based methods use
graph invariant summaries and ﬁnd the difference between consecutive values.
They differ mainly in their choice of summary values and method of declaring
a value to be abnormal. Probabilistic methods create a model of “normal” or
“expected” behavior and declare deviations from this model anomalous. The
type of model used and the method for determining how closely the model is
being followed is what separates these methods. Table 1 and Table 2 show a
summary of the methods presented in this survey, the type of anomalies they
detect, and the year the paper was published. The ﬁeld of anomaly detection
in dynamic graphs is relatively young and is rapidly growing in popularity, as
indicated by the number of papers published in the past ﬁve years. For further reading on anomaly detection in graphs and other domains we direct the
reader to .
Acknowledgements
This work was supported in part by the DOE SDAVI Institute and the U.S. National
Science Foundation (Expeditions in Computing program).