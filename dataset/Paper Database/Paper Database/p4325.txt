Human Action Recognition by Learning Bases of Action Attributes and Parts
Bangpeng Yao1, Xiaoye Jiang2, Aditya Khosla1, Andy Lai Lin3, Leonidas Guibas1, and Li Fei-Fei1
1Computer Science Department, Stanford University, Stanford, CA
2Institute for Computational & Mathematical Engineering, Stanford University, Stanford, CA
3Electrical Engineering Department, Stanford University, Stanford, CA
{bangpeng,aditya86,guibas,feifeili}@cs.stanford.edu
{xiaoye,ydna}@stanford.edu
In this work, we propose to use attributes and parts for
recognizing human actions in still images. We deﬁne action
attributes as the verbs that describe the properties of human
actions, while the parts of actions are objects and poselets
that are closely related to the actions. We jointly model
the attributes and parts by learning a set of sparse bases
that are shown to carry much semantic meaning.
the attributes and parts of an action image can be reconstructed from sparse coefﬁcients with respect to the learned
This dual sparsity provides theoretical guarantee
of our bases learning and feature reconstruction approach.
On the PASCAL action dataset and a new “Stanford 40 Actions” dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in
human actions while achieving state-of-the-art classiﬁcation performance.
1. Introduction
Recognizing human actions in still images has many potential applications in image indexing and retrieval. One
straightforward solution for this problem is to use the whole
image to represent an action and treat action recognition as
a general image classiﬁcation problem . Such
methods have achieved promising performance on the recent PASCAL challenge using spatial pyramid or
random forest based methods. These methods do not,
however, explore the semantically meaningful components
of an action, such as human poses and the objects that are
closely related to the action.
There is some recent work which uses objects interacting with the person or human poses to
build action classiﬁers. However, these methods are prone
to problems caused by false object detections or inaccurate pose estimations. To alleviate these issues, some methods rely on labor-intensive annotations of objects and

 
Figure 1. We use attributes (verb related properties) and parts (objects and poselets ) to model action images. Given a large number of image attributes and parts, we learn a number of sparse
action bases, where each basis encodes the interactions between
some highly related attributes, objects, and poselets. The attributes
and parts of an image can be reconstructed from a sparse weighted
summation of those bases. The colored bars indicate different attributes and parts, where the color code is: green - attribute, red object, blue - poselet. The height of a bar reﬂects the importance
of this attribute or part in the corresponding basis.
human body parts during training time, posing a serious
concern towards large scale action recognition.
Inspired by the recent work on using objects and body
parts for action recognition as well as global and local attributes for object recognition, in this paper,
we propose an attributes and parts based representation of
human actions in a weakly supervised setting. The action
attributes are holistic image descriptions of human actions,
usually associated with verbs in the human language such
as “riding” and “sitting” (as opposed to “repairing” or “lifting”) for the action “riding bike”. The action parts include
objects that are related to the corresponding action (e.g.
“bike”, “helmet”, and “road” in “riding bike”) as well as
different conﬁgurations of local body parts (we use poselet described in ). Given an image of a human action,
many attributes and parts1 contribute to the recognition of
the corresponding action.
Given an image collection of many different actions,
there is a large number of possible attributes, objects and
poselets. Furthermore, there is a large number of possible
interactions among these attributes and parts in terms of cooccurrence statistics. For example, the “riding” attribute is
likely to co-occur with objects such as “horse” and “bike”,
but not “laptop”, while the “right arm extended upward”
poselet is more likely to co-occur with objects such as “volleyball” and the attribute “hitting”. We formulate these interactions of action attributes and parts as action bases for
expressing human actions. A particular action in an image
can therefore be represented as a weighted summation of a
subset of these bases, as shown in Fig.1.
This representation can be naturally formulated as a reconstruction problem. Our challenge is to: 1) represent each
image by using a sparse set of action bases that are meaningful to the content of the image, 2) effectively learn these
bases given far-from-perfect detections of action attributes
and parts without meticulous human labeling as proposed
in previous work . To resolve these challenges, we propose a dual sparsity reconstruction framework to simultaneously obtain sparsity in terms of both the action bases as
well as the reconstruction coefﬁcients for each image. We
show that our method has theoretical foundations in sparse
coding and compressed sensing . On the PASCAL
action dataset and a new “Stanford 40 Actions” dataset,
our attributes and parts representation signiﬁcantly outperforms state-of-the-art methods. Furthermore, we visualize
the bases obtained by our framework and show semantically
meaningful interpretations of the images.
The remaining part of this paper is organized as follows.
Related work are described in Sec.2. The attributes and
parts based representation of actions and the method to learn
action bases are elaborated in Sec.3 and Sec.4 respectively.
Experiment results are shown and discussed in Sec.5.
2. Related Work
Most of the action recognition approaches for
still images treat the problem as a pure image classiﬁcation
problem. There are also algorithms which model the objects
1Our deﬁnition of action attributes and parts are different from the attributes and parts in common object recognition literature. Please refer to
Sec.2 for details. In this work we use “action attribute” and “attribute”,
“action part” and “part” interchangeably, if not explicitly speciﬁed.
or human poses for action classiﬁcation, such as the mutual context model and poselets . However, the
mutual context model requires supervision of the bounding
boxes of objects and human body parts, which are expensive to obtain especially when there is a large number of
images. Also, we want to put the objects and human poses
in a more discriminative framework so that the action recognition performance can be further improved. While poselets have achieved promising performance on action recognition , it is unclear how to jointly explore the semantic
meanings of poselets and the other concepts such as objects
for action recognition.
In this paper, we propose to use attributes and parts for
action classiﬁcation. Inspired by the recent work of learning attributes for object recognition and action
recognition in videos , the attributes we use are linguistically related description of the actions. We use a global
image based representation to train a classiﬁer for each attribute. Compared to the attributes for objects which are
usually adjectives or shape related, the attributes we use to
describe actions are mostly related to verbs. The parts based
models have been successfully used in object detection 
and recognition . However unlike these approaches that
use low-level descriptors, the action parts we use are objects
and poselets with pre-trained detectors as in . The
discriminative information in those detectors can help us alleviate the problem of background clutter in action images
and give us more semantic information of the images .
In the attributes and parts based representation, we learn
a set of sparse action bases and estimate a set of coefﬁcients
on these bases for each image. This dual sparsity makes our
problem different from traditional dictionary learning and
sparse coding problems , given that our action
bases are sparse (in the large set of attributes and parts, only
a small number of them are highly related in each basis)
and far from being mutually orthogonal (consider the two
bases “riding - sitting - bike” and “riding - sitting - horse”).
In this work, we solve this dual sparsity problem using the
elastic-net constrained set , and show that our approach
has theoretical foundations in the compressed network theorem .
3. Action Recognition with Attributes & Parts
3.1. Attributes and Parts in Human Actions
Our method jointly models different attributes and parts
of human actions, which are deﬁned as follows.
Attributes: The attributes are linguistically related descriptions of human actions. Most of the attributes we use
are related to verbs in human language. For example, the
attributes for describing “riding a bike” can be “riding” and
“sitting (on a bike seat)”. It is possible for one attribute to
correspond to more than one action. For instance, “riding”
can describe both “riding a bike” and “riding a horse”, while
this attribute can differentiate the intentions and human gestures in the two actions with the other ones such as “drinking water”. Inspired by the previous work on attributes for
object recognition , we train a discriminative classiﬁer for each attribute.
Parts: The parts we use are composed of objects and
human poses.
We assume that an action image consists
of the objects that are closely related to the action and the
descriptive local human poses. The objects are either manipulated by the person (e.g. “bike” in “riding a bike”) or
related to the scene context of the action (e.g. “road” in
“riding a bike”, “reading lamp” in “reading a book”). The
human poses are represented by poselets , where the human body parts in different images described by the same
poselet are tightly clustered in both appearance space and
conﬁguration space. In our approach, each part is modeled
by a pre-trained object detector or poselet detector.
To obtain our features, we run all the attribute classiﬁers
and part detectors on a given image. A vector of the normalized conﬁdence scores obtained from these classiﬁers
and detectors is used to represent this image.
3.2. Action Bases of Attributes and Parts
Our method learns high-order interactions of image attributes and parts. Each interaction corresponds to the cooccurrence of a set of attributes and parts with some speciﬁc
conﬁdence values (Fig.1). These interactions carry richer
information about human actions and are thus expected to
improve recognition performance. Furthermore, the components in each high-order interaction can serve as context
for each other, and therefore the noise in the attribute classiﬁers and part detectors can be reduced. In our approach,
the high-order interactions are regarded as the bases of the
representations of human actions, and each image is represented as a sparse distribution with respect to all the bases.
Examples of the learned action bases are shown in Fig.4.
We can see that the bases are sparse in the whole space of
attributes and parts, and many of the attributes and parts are
closely correlated in human actions, such as “riding - sitting
- bike” and “using - keyboard - monitor - sitting” as well as
the corresponding poselets.
Now we formalize the action bases in a mathematical
framework. Assume we have 𝑃attributes and parts, and let
a ∈ℝ𝑃be the vector of conﬁdence scores obtained from
the attribute classiﬁers and part detectors. Denoting the set
of action bases as Φ = [𝝓1, ⋅⋅⋅, 𝝓𝑀] where each 𝝓𝑚∈
ℝ𝑃is a basis, the vector a can be represented as
where w = {𝑤1, ⋅⋅⋅, 𝑤𝑀} are the reconstruction coefﬁcients of the bases, and 𝜺∈ℝ𝑃is a noise vector. Note that
in our problem, the vector w and {𝝓𝑚}𝑀
𝑚=1 are all sparse.
This is because on one hand, only a small number of attributes and parts are highly related in each basis of human
actions; on the other hand, a small proportion of the action bases are enough to reconstruct the set of attributes and
parts in each image.
3.3. Action Classiﬁcation Using the Action Bases
From Eqn.1, we can see that the attributes and parts representation a of an action image can be reconstructed from
the sparse factorization coefﬁcients w. w reﬂects the distribution of a on all the action bases Φ, each of which encodes a speciﬁc interaction between action attributes and
parts. The images that correspond to the same action should
have high coefﬁcients on the similar set of action bases. In
this paper, we use the coefﬁcients vector w to represent an
image, and train an SVM classiﬁer for action classiﬁcation.
The above classiﬁcation approach resolves the two challenges of using attributes and parts (objects and poselets)
for action recognition that we proposed in Sec.1. Since we
only use the learned action bases to reconstruct the feature
vector, our method can correct some false detections of objects and poselets by removing the noise component 𝜀in
Eqn.1. Also, those action bases correspond to some highorder interactions in the features, and therefore they jointly
model the complex interactions between different attributes,
objects, and poselets.
4. Learning the Dual-Sparse Action Bases and
Reconstruction Coefﬁcients
Given a collection of training images represented as
𝒜= {a1, a2, ⋅⋅⋅, a𝑁} as described in Sec.3.2, where each
a𝑖is the vector of conﬁdence scores of attribute classiﬁcations and part detections computed from image 𝑖. Intuitively, there exists a latent dictionary of bases where each
basis characterizes frequent co-occurrence of attributes, objects, and poselets involved in an action, e.g. “cycling” and
“bike”, such that each observed data a𝑖can be sparsely reconstructed with respect to the dictionary. Our goal is to
identify a set of sparse bases Φ = [𝝓1, ⋅⋅⋅, 𝝓𝑀] such that
each a𝑖has a sparse representation with respect to the dictionary, as shown in Eqn.1.
During the bases learning stage, we need to learn the
bases Φ and ﬁnd the reconstruction coefﬁcients w𝑖for each
a𝑖. Given a new image represented by a, we want to ﬁnd a
sparse w such that a can be reconstructed from the learned
Φ. Therefore our bases learning and action reconstruction
can be achieved by the following two optimization problems respectively,
Φ∈𝒞,W∈ℝ𝑀×𝑁
2∥a𝑖−Φw𝑖∥2
2 + 𝜆∥w𝑖∥1
2 + 𝜆∥w∥1,
where W = [w1, ⋅⋅⋅, w𝑁] ∈ℝ𝑀×𝑁, 𝜆is a regularization
parameter, and 𝒞is the convex set that Φ belongs to. The
𝑙1-norm in Eqn.2 makes the reconstruction coefﬁcients w𝑖
tend to be sparse. In our setting, the bases Φ should also be
sparse, even though the given 𝒜might be quite noisy due
to the error-prone object detectors and poselet detectors. To
address this issue, we construct the convex set 𝒞as:
𝒞= {Φ ∈ℝ𝑃×𝑀, s.t. ∀𝑗, ∥Φ𝑗∥1 + 𝛾
2 ≤1}. (4)
where 𝛾is another regularization parameter.
Including both 𝑙1-norm and 𝑙2-norm to deﬁne the convex
set 𝒞, the sparsity requirement of the bases are encoded.
This is called the elastic-net constraint set . Furthermore, the sparsity on Φ implies that different action bases
have small overlaps, therefore the coefﬁcients learned from
Eqn.2 are guaranteed to generalize to the testing case in
Eqn.3 according to the compressed network theorem .
Please refer to the supplementary document2 for details.
In our two optimization problems, Eqn.3 is convex while
Eqn.2 is non-convex. However Eqn.2 is convex with respect
to each of the two variables Φ and W when the other one
is ﬁxed. We use an online learning algorithm which
scales up to large datasets to solve this problem.
5. Experiments and Results
5.1. Dataset and Experiment Setup
We test the performance of our proposed method on
the PASCAL action dataset and a new larger scale
dataset collected by us. The new dataset, called Stanford
40 Actions, contains 40 diverse daily human actions, such
as “brushing teeth”, “cleaning the ﬂoor”, “reading book”,
“throwing a frisbee”, etc. All the images are obtained from
Google, Bing, and Flickr. We collect 180∼300 images for
each class. The images within each class have large variations in human pose, appearance, and background clutter. The comparison between our dataset and the existing
still image action datasets are summarized in Table 1. As
there might be multiple people in a single image, we provide bounding boxes for the humans who are doing one of
the 40 actions in each image, similar to . Examples of
the images in our dataset3 are shown in Fig.2.
On the PASCAL dataset, we use the training and validation set speciﬁed in for training, and use the same testing
set. On the Stanford 40 Action dataset, we randomly select
2The supplementary document can be found on the author’s website.
3Please refer to for
more details of the Stanford 40 Actions dataset.
Figure 2. Example
images of the Stanford
Visibility
Ikizler 
Gupta 
PASCAL 
Stanford 40
Table 1. Comparison of our Stanford 40 Action dataset and other
existing human action datasets on still images. “Visibility” variation refers to the variation of visible human body parts, e.g. in
some images the full human body is visible, while in some other
images only the head and shoulder are visible. Bold font indicate
relatively larger scale datasets or larger image variations.
100 images in each class for training, and the remaining images for testing. For each dataset, we annotate the attributes
that can be used to describe the action in each image, and
then train a binary classiﬁer for each attribute. We take a
global representation of the attributes as in , and use the
Locality-constrained Linear Coding (LLC) method on
dense SIFT features to train the classiﬁer for each attribute. As in , the classiﬁers are trained by concatenating
the features from both the foreground bounding box of the
action and the whole image. We extend and normalize the
bounding boxes in the same way as in . For objects, we
use the ImageNet dataset with provided bounding boxes
to train the object detectors by using the Deformable Parts
Model , instead of annotating the positions of objects in
instrument
UCLEAR DOSP
WILLOW LSVM
Ours Conf Score
Ours Sparse Bases
Table 2. Comparison of our method and the other action classiﬁcation approaches evaluated using the percentage of average precision.
“Overall” indicates the mean Average Precision (mAP) on all the nine classes. The bold fonts indicate the best performance. SURREY MK
UCLEAR DOSP, WILLOW SVMSIFT, and POSELETS are the approaches presented in the PASCAL challenge .
the action data. For poselets, we use the pre-trained poselet
detectors in . For each object or poselet detector, we use
the highest detection score in the response map of each image to measure the conﬁdence of the object or poselet in the
given image. We linearly normalize the conﬁdence scores
of all the attribute classiﬁers and part detectors so that all
the feature values are between 0 and 1.
We use 14 attributes and 27 objects for the PASCAL
data, 45 attributes and 81 objects for the Stanford 40 Action data. We only use the attributes and objects that we believe are closely related to the actions in each dataset. Also
some useful objects are not included, e.g. cigarette which
is helpful for recognizing the action of “smoking cigarette”
but there is no cigarette bounding box in ImageNet images.
Please refer to the supplementary document for the list of attributes and objects that we use. We use 150 poselets as provided in on both datasets. The number of action bases
are set to 400 and 600 respectively. The 𝜆and 𝛾values in
Eqn.2, 3, and 4 are set to 0.1 and 0.15.
In the following experiment, we consider two approaches of using attributes and parts for action recognition. One is to simply concatenate the normalized conﬁdence scores of attributes classiﬁcation and parts detection
as feature representation (denoted as “Conf Score”), the
other is to use the reconstruction coefﬁcients on the learned
sparse bases as feature representation (denoted as “Sparse
Bases”). We use linear SVM classiﬁers for both feature
representations. As in , we use mean Average Precision
(mAP) to evaluate the performance on both datasets.
5.2. Results on the PASCAL Action Dataset
On the PASCAL dataset, we compare our methods with
four approaches from the PASCAL challenge :
SURREY MK and UCLEAR DOSP which mainly rely on
general image classiﬁcation methods and achieve the best
performance in the challenge, WILLOW LSVM which is
a parts based model, and POSELETS which also uses the
poselet features for classiﬁcation.
The average precision of different approaches is shown
in Table 2. We can see that by simply concatenating the con-
ﬁdence scores of attributes classiﬁcation and parts detection, our method outperforms the best result in the PASCAL
challenge in terms of the mean Average Precision (mAP).
The performance can be further improved by learning highorder interactions of attributes and parts, from which the
feature noise can be reduced. A visualization of the learned
bases of our method is shown in Fig.4. We observe that almost all the bases are very sparse, and many of them carry
useful information for describing speciﬁc human actions.
However due to the large degree of noise in both object detectors and poselet detectors, some bases contain noise, e.g.
“guitar” in the basis of “calling - cell phone - guitar”. In
Fig.4 we also show some action images with the annotations
of attributes and objects that have high conﬁdence score in
the feature representation reconstructed from the bases.
Our approach considers three concepts: attributes, parts
as objects, and parts as poselets. To analyze the contribution of each concept, we remove the conﬁdence scores of attribute classiﬁers, part detectors, and poselet detectors from
our feature set, one at a time. The classiﬁcation results are
shown in Fig.3. We observe that using the reconstruction
coefﬁcients consistently outperform the methods that simply concatenating the conﬁdence scores of classiﬁers and

Figure 3. Comparison of the methods by removing the conﬁdence
scores obtained from attributes (A), objects (O), and poselets (P)
from the feature vector, one at a time. The performance are evaluated using mean Average Precision on the PASCAL dataset.
Figure 4. Visualization of the 400 learned bases from the PASCAL action dataset. Each row in the left-most matrix corresponds to one
basis. Red color indicates large magnitude in the action bases while blue color indicates low magnitude. We observe that the bases are
indeed very sparse. We also show some semantically meaningful action bases learned by our results, e.g. “riding - grass - horse”. By using
the learned action bases to reconstruct the attributes and parts representation, we show the attributes and objects that have high conﬁdence
scores on some images. Magenta color indicates wrong tags.
detectors. We can also see that attributes make the biggest
contribution to the performance, because removing the attribute features makes the performance much worse. This
is due to the large amount of noise produced from objects
and poselets detectors which are pre-trained from the other
datasets. However, objects and poselets do contain complementary information with the attributes, and the effect of
the noise can be alleviated by the bases learned from our
approach. We observe that in the case of only considering
objects and poselets, learning the sparse bases signiﬁcantly
improves the performance. By combining attributes, objects and poselets and learning the action bases, our method
achieves state-of-the-art classiﬁcation performance.
Our learning method (Eqn.2) has the dual sparsity on
both action bases Φ and reconstruction coefﬁcients W.
Here we compare our method with a simple 𝑙1-norm method
- 𝑙1 logistic regression based on the concatenation of the
conﬁdence scores of attributes and parts. The mAP result
of 𝑙1 logistic regression is 47.9%, which is lower than our
results. This shows that a simple 𝑙1-norm logistic regression cannot effectively learn the information from the noisy
attributes classiﬁcation and parts detection features. Furthermore, in order to demonstrate the effectiveness of the
two sparsity constraints, we remove the constraints one at
To remove the sparsity constraint on the reconstruction weight W, we simply change ∥w𝑖∥1 in Eqn.2 and
Eqn.3 to ∥w𝑖∥2. To remove the sparsity constraint on the
bases Φ, we change the convex set 𝒞in Eqn.4 to be:
𝒞= {Φ ∈ℝ𝑃×𝑀, s.t. ∀𝑗, ∥Φ𝑗∥2
In the ﬁrst case, where we do not have sparsity constraint on
W, the mAP result drops to 64.0%, which is comparable to
directly concatenating all attributes classiﬁcation and parts
detection conﬁdence scores. This shows that the sparsity
on W helps to remove noise from the original data. In the
second case where we do not have sparsity constraint on Φ,
the performance becomes 64.7% which is very close to that
of having sparsity constraint on Φ. The reason might be
that although there is much noise in the parts detections and
attribute classiﬁcations, the original vector of conﬁdence
scores already has some level of sparsity. However, by explicitly imposing the sparsity on Φ, we can guarantee the
sparsity of the bases, so that our method can explicitly extract more semantic information and its performance is also
theoretically guaranteed. Please refer to the supplementary
document of this paper for more details.
5.3. Results on the Stanford 40 Actions Dataset
We next show the performance of our proposed method
on the new Stanford 40 Actions dataset. We setup two baselines on this dataset: LLC method with densely sampled SIFT features, and object bank . Comparing
these two algorithms with our approach, the mAP is shown
in Table 3. The results show that compared to the baselines which uses image classiﬁers or object detectors only,
combining attributes and parts (objects and poselets) signiﬁcantly improved the recognition performance by more
than 10%. The reason might be that, on this relatively large
dataset, more attributes are used to describe the actions and
more objects are related to the actions, which contains a lot
of complementary information.
As done in Sec.5.2, we also remove the features that are
related to attributes, objects, and poselets from our feature
set, one at a time. The results are shown in Fig.5. On this
dataset, the contribution of objects is larger than that on the
PASCAL dataset. This is because more objects are related
to the actions on this larger scale dataset, and therefore we
can extract more useful information for recognition from
the object detectors.
The average precision obtained from LLC and our
method by using reconstruction coefﬁcients as feature representation for each of the 40 classes is shown in Fig.6.
Using a sparse representation on the action bases of at-
Conf Score
Sparse Bases
Table 3. Comparison of our attributes and parts based action recognition methods with the two baselines: object bank and
LLC . The performance is evaluated with AP. The bold font
indicates the best performance.

Figure 5. Comparison of the methods by removing the conﬁdence
scores obtained from attributes (A), objects (O), and poselets (P)
from the feature vector, one at a time. The performance is evaluated using mean Average Precision on the Stanford 40 Actions
*)'!"
-" (*
#!
Figure 6. Average precision of our method (Sparse Bases) on each
of the 40 classes of the Stanford 40 Actions dataset. We compare
our method with the LLC algorithm.
tributes and parts, our method outperforms LLC on all the
40 classes. Furthermore, the classiﬁcation performance on
different actions varies a lot, ranging from 89.2% on “riding
a horse” to only 6.2% on “texting message”. It is interesting
to observe that the result shown in Fig.6 is somewhat similar to that on the PASCAL dataset in Table 2. The classes
“riding a horse” and “riding a bike” have high classiﬁcation performance on both datasets while the classes “calling”, “reading a book” and “taking photos” have low classi-
ﬁcation performance, showing that the two datasets capture
similar image statistics of human actions. The classes “riding a horse” and “riding a bike” can be easily recognized
in part because the human poses do not vary much within
each action, and the objects (horse and bike) are easy to detect. However, the performance on “feeding a horse” and
“repairing a bike” is not as good as that on “riding a horse”
and “riding a bike”. One reason is that the body parts of
horses in most of the images of “feeding a horse” are highly
occluded, and therefore the horse detector is difﬁcult to detect them. From the images of “repairing a bike”, we can
see that the human pose changes a lot and the bikes are
also occluded or disassembled, making them difﬁcult to be
recognized by bike detectors. There are some classes on
which the recognition performance is very low, e.g. “taking photos”. The reason is that the cameras are very small,
which makes it difﬁcult to distinguish “taking photos” and
the other actions.
6. Discussion
In this work, we use attributes and parts for action recognition. The attributes are verbs related description of human
actions, while the parts are composed of objects and poselets. We learn a set of sparse bases of the attributes and parts
based image representation, allowing an action image to be
reconstructed by a set of sparse coefﬁcients with respect
to the bases. Experimental results show that our method
achieves state-of-the-art performance on two datasets. One
direction of our future work is to use the learned action
bases for image tagging, so that we can explore more detailed semantic understanding of human actions in images.
Acknowledgement.
L.F-F. is partially supported by an
NSF CAREER grant (IIS-0845230), an ONR MURI grant,
the DARPA VIRAT program and the DARPA Mind’s Eye
program. X.J. and L.G. is partially supported by an NSF
grant (IIS-101632), an ARO grant (W911NF-10-1-0037)
and an ONR MURI grant. B.Y. is partially supported by
the SAP Stanford Graduate Fellowship.