Human Action Recognition by Learning Bases of Action Attributes and Parts
Bangpeng Yao1, Xiaoye Jiang2, Aditya Khosla1, Andy Lai Lin3, Leonidas Guibas1, and Li Fei-Fei1
1Computer Science Department, Stanford University, Stanford, CA
2Institute for Computational & Mathematical Engineering, Stanford University, Stanford, CA
3Electrical Engineering Department, Stanford University, Stanford, CA
{bangpeng,aditya86,guibas,feifeili}@cs.stanford.edu
{xiaoye,ydna}@stanford.edu
In this work, we propose to use attributes and parts for
recognizing human actions in still images. We deï¬ne action
attributes as the verbs that describe the properties of human
actions, while the parts of actions are objects and poselets
that are closely related to the actions. We jointly model
the attributes and parts by learning a set of sparse bases
that are shown to carry much semantic meaning.
the attributes and parts of an action image can be reconstructed from sparse coefï¬cients with respect to the learned
This dual sparsity provides theoretical guarantee
of our bases learning and feature reconstruction approach.
On the PASCAL action dataset and a new â€œStanford 40 Actionsâ€ dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in
human actions while achieving state-of-the-art classiï¬cation performance.
1. Introduction
Recognizing human actions in still images has many potential applications in image indexing and retrieval. One
straightforward solution for this problem is to use the whole
image to represent an action and treat action recognition as
a general image classiï¬cation problem . Such
methods have achieved promising performance on the recent PASCAL challenge using spatial pyramid or
random forest based methods. These methods do not,
however, explore the semantically meaningful components
of an action, such as human poses and the objects that are
closely related to the action.
There is some recent work which uses objects interacting with the person or human poses to
build action classiï¬ers. However, these methods are prone
to problems caused by false object detections or inaccurate pose estimations. To alleviate these issues, some methods rely on labor-intensive annotations of objects and

 
Figure 1. We use attributes (verb related properties) and parts (objects and poselets ) to model action images. Given a large number of image attributes and parts, we learn a number of sparse
action bases, where each basis encodes the interactions between
some highly related attributes, objects, and poselets. The attributes
and parts of an image can be reconstructed from a sparse weighted
summation of those bases. The colored bars indicate different attributes and parts, where the color code is: green - attribute, red object, blue - poselet. The height of a bar reï¬‚ects the importance
of this attribute or part in the corresponding basis.
human body parts during training time, posing a serious
concern towards large scale action recognition.
Inspired by the recent work on using objects and body
parts for action recognition as well as global and local attributes for object recognition, in this paper,
we propose an attributes and parts based representation of
human actions in a weakly supervised setting. The action
attributes are holistic image descriptions of human actions,
usually associated with verbs in the human language such
as â€œridingâ€ and â€œsittingâ€ (as opposed to â€œrepairingâ€ or â€œliftingâ€) for the action â€œriding bikeâ€. The action parts include
objects that are related to the corresponding action (e.g.
â€œbikeâ€, â€œhelmetâ€, and â€œroadâ€ in â€œriding bikeâ€) as well as
different conï¬gurations of local body parts (we use poselet described in ). Given an image of a human action,
many attributes and parts1 contribute to the recognition of
the corresponding action.
Given an image collection of many different actions,
there is a large number of possible attributes, objects and
poselets. Furthermore, there is a large number of possible
interactions among these attributes and parts in terms of cooccurrence statistics. For example, the â€œridingâ€ attribute is
likely to co-occur with objects such as â€œhorseâ€ and â€œbikeâ€,
but not â€œlaptopâ€, while the â€œright arm extended upwardâ€
poselet is more likely to co-occur with objects such as â€œvolleyballâ€ and the attribute â€œhittingâ€. We formulate these interactions of action attributes and parts as action bases for
expressing human actions. A particular action in an image
can therefore be represented as a weighted summation of a
subset of these bases, as shown in Fig.1.
This representation can be naturally formulated as a reconstruction problem. Our challenge is to: 1) represent each
image by using a sparse set of action bases that are meaningful to the content of the image, 2) effectively learn these
bases given far-from-perfect detections of action attributes
and parts without meticulous human labeling as proposed
in previous work . To resolve these challenges, we propose a dual sparsity reconstruction framework to simultaneously obtain sparsity in terms of both the action bases as
well as the reconstruction coefï¬cients for each image. We
show that our method has theoretical foundations in sparse
coding and compressed sensing . On the PASCAL
action dataset and a new â€œStanford 40 Actionsâ€ dataset,
our attributes and parts representation signiï¬cantly outperforms state-of-the-art methods. Furthermore, we visualize
the bases obtained by our framework and show semantically
meaningful interpretations of the images.
The remaining part of this paper is organized as follows.
Related work are described in Sec.2. The attributes and
parts based representation of actions and the method to learn
action bases are elaborated in Sec.3 and Sec.4 respectively.
Experiment results are shown and discussed in Sec.5.
2. Related Work
Most of the action recognition approaches for
still images treat the problem as a pure image classiï¬cation
problem. There are also algorithms which model the objects
1Our deï¬nition of action attributes and parts are different from the attributes and parts in common object recognition literature. Please refer to
Sec.2 for details. In this work we use â€œaction attributeâ€ and â€œattributeâ€,
â€œaction partâ€ and â€œpartâ€ interchangeably, if not explicitly speciï¬ed.
or human poses for action classiï¬cation, such as the mutual context model and poselets . However, the
mutual context model requires supervision of the bounding
boxes of objects and human body parts, which are expensive to obtain especially when there is a large number of
images. Also, we want to put the objects and human poses
in a more discriminative framework so that the action recognition performance can be further improved. While poselets have achieved promising performance on action recognition , it is unclear how to jointly explore the semantic
meanings of poselets and the other concepts such as objects
for action recognition.
In this paper, we propose to use attributes and parts for
action classiï¬cation. Inspired by the recent work of learning attributes for object recognition and action
recognition in videos , the attributes we use are linguistically related description of the actions. We use a global
image based representation to train a classiï¬er for each attribute. Compared to the attributes for objects which are
usually adjectives or shape related, the attributes we use to
describe actions are mostly related to verbs. The parts based
models have been successfully used in object detection 
and recognition . However unlike these approaches that
use low-level descriptors, the action parts we use are objects
and poselets with pre-trained detectors as in . The
discriminative information in those detectors can help us alleviate the problem of background clutter in action images
and give us more semantic information of the images .
In the attributes and parts based representation, we learn
a set of sparse action bases and estimate a set of coefï¬cients
on these bases for each image. This dual sparsity makes our
problem different from traditional dictionary learning and
sparse coding problems , given that our action
bases are sparse (in the large set of attributes and parts, only
a small number of them are highly related in each basis)
and far from being mutually orthogonal (consider the two
bases â€œriding - sitting - bikeâ€ and â€œriding - sitting - horseâ€).
In this work, we solve this dual sparsity problem using the
elastic-net constrained set , and show that our approach
has theoretical foundations in the compressed network theorem .
3. Action Recognition with Attributes & Parts
3.1. Attributes and Parts in Human Actions
Our method jointly models different attributes and parts
of human actions, which are deï¬ned as follows.
Attributes: The attributes are linguistically related descriptions of human actions. Most of the attributes we use
are related to verbs in human language. For example, the
attributes for describing â€œriding a bikeâ€ can be â€œridingâ€ and
â€œsitting (on a bike seat)â€. It is possible for one attribute to
correspond to more than one action. For instance, â€œridingâ€
can describe both â€œriding a bikeâ€ and â€œriding a horseâ€, while
this attribute can differentiate the intentions and human gestures in the two actions with the other ones such as â€œdrinking waterâ€. Inspired by the previous work on attributes for
object recognition , we train a discriminative classiï¬er for each attribute.
Parts: The parts we use are composed of objects and
human poses.
We assume that an action image consists
of the objects that are closely related to the action and the
descriptive local human poses. The objects are either manipulated by the person (e.g. â€œbikeâ€ in â€œriding a bikeâ€) or
related to the scene context of the action (e.g. â€œroadâ€ in
â€œriding a bikeâ€, â€œreading lampâ€ in â€œreading a bookâ€). The
human poses are represented by poselets , where the human body parts in different images described by the same
poselet are tightly clustered in both appearance space and
conï¬guration space. In our approach, each part is modeled
by a pre-trained object detector or poselet detector.
To obtain our features, we run all the attribute classiï¬ers
and part detectors on a given image. A vector of the normalized conï¬dence scores obtained from these classiï¬ers
and detectors is used to represent this image.
3.2. Action Bases of Attributes and Parts
Our method learns high-order interactions of image attributes and parts. Each interaction corresponds to the cooccurrence of a set of attributes and parts with some speciï¬c
conï¬dence values (Fig.1). These interactions carry richer
information about human actions and are thus expected to
improve recognition performance. Furthermore, the components in each high-order interaction can serve as context
for each other, and therefore the noise in the attribute classiï¬ers and part detectors can be reduced. In our approach,
the high-order interactions are regarded as the bases of the
representations of human actions, and each image is represented as a sparse distribution with respect to all the bases.
Examples of the learned action bases are shown in Fig.4.
We can see that the bases are sparse in the whole space of
attributes and parts, and many of the attributes and parts are
closely correlated in human actions, such as â€œriding - sitting
- bikeâ€ and â€œusing - keyboard - monitor - sittingâ€ as well as
the corresponding poselets.
Now we formalize the action bases in a mathematical
framework. Assume we have ğ‘ƒattributes and parts, and let
a âˆˆâ„ğ‘ƒbe the vector of conï¬dence scores obtained from
the attribute classiï¬ers and part detectors. Denoting the set
of action bases as Î¦ = [ğ“1, â‹…â‹…â‹…, ğ“ğ‘€] where each ğ“ğ‘šâˆˆ
â„ğ‘ƒis a basis, the vector a can be represented as
where w = {ğ‘¤1, â‹…â‹…â‹…, ğ‘¤ğ‘€} are the reconstruction coefï¬cients of the bases, and ğœºâˆˆâ„ğ‘ƒis a noise vector. Note that
in our problem, the vector w and {ğ“ğ‘š}ğ‘€
ğ‘š=1 are all sparse.
This is because on one hand, only a small number of attributes and parts are highly related in each basis of human
actions; on the other hand, a small proportion of the action bases are enough to reconstruct the set of attributes and
parts in each image.
3.3. Action Classiï¬cation Using the Action Bases
From Eqn.1, we can see that the attributes and parts representation a of an action image can be reconstructed from
the sparse factorization coefï¬cients w. w reï¬‚ects the distribution of a on all the action bases Î¦, each of which encodes a speciï¬c interaction between action attributes and
parts. The images that correspond to the same action should
have high coefï¬cients on the similar set of action bases. In
this paper, we use the coefï¬cients vector w to represent an
image, and train an SVM classiï¬er for action classiï¬cation.
The above classiï¬cation approach resolves the two challenges of using attributes and parts (objects and poselets)
for action recognition that we proposed in Sec.1. Since we
only use the learned action bases to reconstruct the feature
vector, our method can correct some false detections of objects and poselets by removing the noise component ğœ€in
Eqn.1. Also, those action bases correspond to some highorder interactions in the features, and therefore they jointly
model the complex interactions between different attributes,
objects, and poselets.
4. Learning the Dual-Sparse Action Bases and
Reconstruction Coefï¬cients
Given a collection of training images represented as
ğ’œ= {a1, a2, â‹…â‹…â‹…, ağ‘} as described in Sec.3.2, where each
ağ‘–is the vector of conï¬dence scores of attribute classiï¬cations and part detections computed from image ğ‘–. Intuitively, there exists a latent dictionary of bases where each
basis characterizes frequent co-occurrence of attributes, objects, and poselets involved in an action, e.g. â€œcyclingâ€ and
â€œbikeâ€, such that each observed data ağ‘–can be sparsely reconstructed with respect to the dictionary. Our goal is to
identify a set of sparse bases Î¦ = [ğ“1, â‹…â‹…â‹…, ğ“ğ‘€] such that
each ağ‘–has a sparse representation with respect to the dictionary, as shown in Eqn.1.
During the bases learning stage, we need to learn the
bases Î¦ and ï¬nd the reconstruction coefï¬cients wğ‘–for each
ağ‘–. Given a new image represented by a, we want to ï¬nd a
sparse w such that a can be reconstructed from the learned
Î¦. Therefore our bases learning and action reconstruction
can be achieved by the following two optimization problems respectively,
Î¦âˆˆğ’,Wâˆˆâ„ğ‘€Ã—ğ‘
2âˆ¥ağ‘–âˆ’Î¦wğ‘–âˆ¥2
2 + ğœ†âˆ¥wğ‘–âˆ¥1
2 + ğœ†âˆ¥wâˆ¥1,
where W = [w1, â‹…â‹…â‹…, wğ‘] âˆˆâ„ğ‘€Ã—ğ‘, ğœ†is a regularization
parameter, and ğ’is the convex set that Î¦ belongs to. The
ğ‘™1-norm in Eqn.2 makes the reconstruction coefï¬cients wğ‘–
tend to be sparse. In our setting, the bases Î¦ should also be
sparse, even though the given ğ’œmight be quite noisy due
to the error-prone object detectors and poselet detectors. To
address this issue, we construct the convex set ğ’as:
ğ’= {Î¦ âˆˆâ„ğ‘ƒÃ—ğ‘€, s.t. âˆ€ğ‘—, âˆ¥Î¦ğ‘—âˆ¥1 + ğ›¾
2 â‰¤1}. (4)
where ğ›¾is another regularization parameter.
Including both ğ‘™1-norm and ğ‘™2-norm to deï¬ne the convex
set ğ’, the sparsity requirement of the bases are encoded.
This is called the elastic-net constraint set . Furthermore, the sparsity on Î¦ implies that different action bases
have small overlaps, therefore the coefï¬cients learned from
Eqn.2 are guaranteed to generalize to the testing case in
Eqn.3 according to the compressed network theorem .
Please refer to the supplementary document2 for details.
In our two optimization problems, Eqn.3 is convex while
Eqn.2 is non-convex. However Eqn.2 is convex with respect
to each of the two variables Î¦ and W when the other one
is ï¬xed. We use an online learning algorithm which
scales up to large datasets to solve this problem.
5. Experiments and Results
5.1. Dataset and Experiment Setup
We test the performance of our proposed method on
the PASCAL action dataset and a new larger scale
dataset collected by us. The new dataset, called Stanford
40 Actions, contains 40 diverse daily human actions, such
as â€œbrushing teethâ€, â€œcleaning the ï¬‚oorâ€, â€œreading bookâ€,
â€œthrowing a frisbeeâ€, etc. All the images are obtained from
Google, Bing, and Flickr. We collect 180âˆ¼300 images for
each class. The images within each class have large variations in human pose, appearance, and background clutter. The comparison between our dataset and the existing
still image action datasets are summarized in Table 1. As
there might be multiple people in a single image, we provide bounding boxes for the humans who are doing one of
the 40 actions in each image, similar to . Examples of
the images in our dataset3 are shown in Fig.2.
On the PASCAL dataset, we use the training and validation set speciï¬ed in for training, and use the same testing
set. On the Stanford 40 Action dataset, we randomly select
2The supplementary document can be found on the authorâ€™s website.
3Please refer to for
more details of the Stanford 40 Actions dataset.
Figure 2. Example
images of the Stanford
Visibility
Ikizler 
Gupta 
PASCAL 
Stanford 40
Table 1. Comparison of our Stanford 40 Action dataset and other
existing human action datasets on still images. â€œVisibilityâ€ variation refers to the variation of visible human body parts, e.g. in
some images the full human body is visible, while in some other
images only the head and shoulder are visible. Bold font indicate
relatively larger scale datasets or larger image variations.
100 images in each class for training, and the remaining images for testing. For each dataset, we annotate the attributes
that can be used to describe the action in each image, and
then train a binary classiï¬er for each attribute. We take a
global representation of the attributes as in , and use the
Locality-constrained Linear Coding (LLC) method on
dense SIFT features to train the classiï¬er for each attribute. As in , the classiï¬ers are trained by concatenating
the features from both the foreground bounding box of the
action and the whole image. We extend and normalize the
bounding boxes in the same way as in . For objects, we
use the ImageNet dataset with provided bounding boxes
to train the object detectors by using the Deformable Parts
Model , instead of annotating the positions of objects in
instrument
UCLEAR DOSP
WILLOW LSVM
Ours Conf Score
Ours Sparse Bases
Table 2. Comparison of our method and the other action classiï¬cation approaches evaluated using the percentage of average precision.
â€œOverallâ€ indicates the mean Average Precision (mAP) on all the nine classes. The bold fonts indicate the best performance. SURREY MK
UCLEAR DOSP, WILLOW SVMSIFT, and POSELETS are the approaches presented in the PASCAL challenge .
the action data. For poselets, we use the pre-trained poselet
detectors in . For each object or poselet detector, we use
the highest detection score in the response map of each image to measure the conï¬dence of the object or poselet in the
given image. We linearly normalize the conï¬dence scores
of all the attribute classiï¬ers and part detectors so that all
the feature values are between 0 and 1.
We use 14 attributes and 27 objects for the PASCAL
data, 45 attributes and 81 objects for the Stanford 40 Action data. We only use the attributes and objects that we believe are closely related to the actions in each dataset. Also
some useful objects are not included, e.g. cigarette which
is helpful for recognizing the action of â€œsmoking cigaretteâ€
but there is no cigarette bounding box in ImageNet images.
Please refer to the supplementary document for the list of attributes and objects that we use. We use 150 poselets as provided in on both datasets. The number of action bases
are set to 400 and 600 respectively. The ğœ†and ğ›¾values in
Eqn.2, 3, and 4 are set to 0.1 and 0.15.
In the following experiment, we consider two approaches of using attributes and parts for action recognition. One is to simply concatenate the normalized conï¬dence scores of attributes classiï¬cation and parts detection
as feature representation (denoted as â€œConf Scoreâ€), the
other is to use the reconstruction coefï¬cients on the learned
sparse bases as feature representation (denoted as â€œSparse
Basesâ€). We use linear SVM classiï¬ers for both feature
representations. As in , we use mean Average Precision
(mAP) to evaluate the performance on both datasets.
5.2. Results on the PASCAL Action Dataset
On the PASCAL dataset, we compare our methods with
four approaches from the PASCAL challenge :
SURREY MK and UCLEAR DOSP which mainly rely on
general image classiï¬cation methods and achieve the best
performance in the challenge, WILLOW LSVM which is
a parts based model, and POSELETS which also uses the
poselet features for classiï¬cation.
The average precision of different approaches is shown
in Table 2. We can see that by simply concatenating the con-
ï¬dence scores of attributes classiï¬cation and parts detection, our method outperforms the best result in the PASCAL
challenge in terms of the mean Average Precision (mAP).
The performance can be further improved by learning highorder interactions of attributes and parts, from which the
feature noise can be reduced. A visualization of the learned
bases of our method is shown in Fig.4. We observe that almost all the bases are very sparse, and many of them carry
useful information for describing speciï¬c human actions.
However due to the large degree of noise in both object detectors and poselet detectors, some bases contain noise, e.g.
â€œguitarâ€ in the basis of â€œcalling - cell phone - guitarâ€. In
Fig.4 we also show some action images with the annotations
of attributes and objects that have high conï¬dence score in
the feature representation reconstructed from the bases.
Our approach considers three concepts: attributes, parts
as objects, and parts as poselets. To analyze the contribution of each concept, we remove the conï¬dence scores of attribute classiï¬ers, part detectors, and poselet detectors from
our feature set, one at a time. The classiï¬cation results are
shown in Fig.3. We observe that using the reconstruction
coefï¬cients consistently outperform the methods that simply concatenating the conï¬dence scores of classiï¬ers and

Figure 3. Comparison of the methods by removing the conï¬dence
scores obtained from attributes (A), objects (O), and poselets (P)
from the feature vector, one at a time. The performance are evaluated using mean Average Precision on the PASCAL dataset.
Figure 4. Visualization of the 400 learned bases from the PASCAL action dataset. Each row in the left-most matrix corresponds to one
basis. Red color indicates large magnitude in the action bases while blue color indicates low magnitude. We observe that the bases are
indeed very sparse. We also show some semantically meaningful action bases learned by our results, e.g. â€œriding - grass - horseâ€. By using
the learned action bases to reconstruct the attributes and parts representation, we show the attributes and objects that have high conï¬dence
scores on some images. Magenta color indicates wrong tags.
detectors. We can also see that attributes make the biggest
contribution to the performance, because removing the attribute features makes the performance much worse. This
is due to the large amount of noise produced from objects
and poselets detectors which are pre-trained from the other
datasets. However, objects and poselets do contain complementary information with the attributes, and the effect of
the noise can be alleviated by the bases learned from our
approach. We observe that in the case of only considering
objects and poselets, learning the sparse bases signiï¬cantly
improves the performance. By combining attributes, objects and poselets and learning the action bases, our method
achieves state-of-the-art classiï¬cation performance.
Our learning method (Eqn.2) has the dual sparsity on
both action bases Î¦ and reconstruction coefï¬cients W.
Here we compare our method with a simple ğ‘™1-norm method
- ğ‘™1 logistic regression based on the concatenation of the
conï¬dence scores of attributes and parts. The mAP result
of ğ‘™1 logistic regression is 47.9%, which is lower than our
results. This shows that a simple ğ‘™1-norm logistic regression cannot effectively learn the information from the noisy
attributes classiï¬cation and parts detection features. Furthermore, in order to demonstrate the effectiveness of the
two sparsity constraints, we remove the constraints one at
To remove the sparsity constraint on the reconstruction weight W, we simply change âˆ¥wğ‘–âˆ¥1 in Eqn.2 and
Eqn.3 to âˆ¥wğ‘–âˆ¥2. To remove the sparsity constraint on the
bases Î¦, we change the convex set ğ’in Eqn.4 to be:
ğ’= {Î¦ âˆˆâ„ğ‘ƒÃ—ğ‘€, s.t. âˆ€ğ‘—, âˆ¥Î¦ğ‘—âˆ¥2
In the ï¬rst case, where we do not have sparsity constraint on
W, the mAP result drops to 64.0%, which is comparable to
directly concatenating all attributes classiï¬cation and parts
detection conï¬dence scores. This shows that the sparsity
on W helps to remove noise from the original data. In the
second case where we do not have sparsity constraint on Î¦,
the performance becomes 64.7% which is very close to that
of having sparsity constraint on Î¦. The reason might be
that although there is much noise in the parts detections and
attribute classiï¬cations, the original vector of conï¬dence
scores already has some level of sparsity. However, by explicitly imposing the sparsity on Î¦, we can guarantee the
sparsity of the bases, so that our method can explicitly extract more semantic information and its performance is also
theoretically guaranteed. Please refer to the supplementary
document of this paper for more details.
5.3. Results on the Stanford 40 Actions Dataset
We next show the performance of our proposed method
on the new Stanford 40 Actions dataset. We setup two baselines on this dataset: LLC method with densely sampled SIFT features, and object bank . Comparing
these two algorithms with our approach, the mAP is shown
in Table 3. The results show that compared to the baselines which uses image classiï¬ers or object detectors only,
combining attributes and parts (objects and poselets) signiï¬cantly improved the recognition performance by more
than 10%. The reason might be that, on this relatively large
dataset, more attributes are used to describe the actions and
more objects are related to the actions, which contains a lot
of complementary information.
As done in Sec.5.2, we also remove the features that are
related to attributes, objects, and poselets from our feature
set, one at a time. The results are shown in Fig.5. On this
dataset, the contribution of objects is larger than that on the
PASCAL dataset. This is because more objects are related
to the actions on this larger scale dataset, and therefore we
can extract more useful information for recognition from
the object detectors.
The average precision obtained from LLC and our
method by using reconstruction coefï¬cients as feature representation for each of the 40 classes is shown in Fig.6.
Using a sparse representation on the action bases of at-
Conf Score
Sparse Bases
Table 3. Comparison of our attributes and parts based action recognition methods with the two baselines: object bank and
LLC . The performance is evaluated with AP. The bold font
indicates the best performance.

Figure 5. Comparison of the methods by removing the conï¬dence
scores obtained from attributes (A), objects (O), and poselets (P)
from the feature vector, one at a time. The performance is evaluated using mean Average Precision on the Stanford 40 Actions
*)'!"
-" (*
#!
Figure 6. Average precision of our method (Sparse Bases) on each
of the 40 classes of the Stanford 40 Actions dataset. We compare
our method with the LLC algorithm.
tributes and parts, our method outperforms LLC on all the
40 classes. Furthermore, the classiï¬cation performance on
different actions varies a lot, ranging from 89.2% on â€œriding
a horseâ€ to only 6.2% on â€œtexting messageâ€. It is interesting
to observe that the result shown in Fig.6 is somewhat similar to that on the PASCAL dataset in Table 2. The classes
â€œriding a horseâ€ and â€œriding a bikeâ€ have high classiï¬cation performance on both datasets while the classes â€œcallingâ€, â€œreading a bookâ€ and â€œtaking photosâ€ have low classi-
ï¬cation performance, showing that the two datasets capture
similar image statistics of human actions. The classes â€œriding a horseâ€ and â€œriding a bikeâ€ can be easily recognized
in part because the human poses do not vary much within
each action, and the objects (horse and bike) are easy to detect. However, the performance on â€œfeeding a horseâ€ and
â€œrepairing a bikeâ€ is not as good as that on â€œriding a horseâ€
and â€œriding a bikeâ€. One reason is that the body parts of
horses in most of the images of â€œfeeding a horseâ€ are highly
occluded, and therefore the horse detector is difï¬cult to detect them. From the images of â€œrepairing a bikeâ€, we can
see that the human pose changes a lot and the bikes are
also occluded or disassembled, making them difï¬cult to be
recognized by bike detectors. There are some classes on
which the recognition performance is very low, e.g. â€œtaking photosâ€. The reason is that the cameras are very small,
which makes it difï¬cult to distinguish â€œtaking photosâ€ and
the other actions.
6. Discussion
In this work, we use attributes and parts for action recognition. The attributes are verbs related description of human
actions, while the parts are composed of objects and poselets. We learn a set of sparse bases of the attributes and parts
based image representation, allowing an action image to be
reconstructed by a set of sparse coefï¬cients with respect
to the bases. Experimental results show that our method
achieves state-of-the-art performance on two datasets. One
direction of our future work is to use the learned action
bases for image tagging, so that we can explore more detailed semantic understanding of human actions in images.
Acknowledgement.
L.F-F. is partially supported by an
NSF CAREER grant (IIS-0845230), an ONR MURI grant,
the DARPA VIRAT program and the DARPA Mindâ€™s Eye
program. X.J. and L.G. is partially supported by an NSF
grant (IIS-101632), an ARO grant (W911NF-10-1-0037)
and an ONR MURI grant. B.Y. is partially supported by
the SAP Stanford Graduate Fellowship.