Bagging and Boosting Negatively Correlated
Neural Networks
言語: English
公開日: 2011-01-25
キーワード (Ja):
キーワード (En):
作成者: ISLAM, Md.Monirul, YAO, Xin, NIRJON,
S.M.Shahriar, ISLAM, Muhammad Asiful, MURASE,
 
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
Bagging and Boosting Negatively
Correlated Neural Networks
Md. Monirul Islam, Xin Yao, Fellow, IEEE, S. M. Shahriar Nirjon, Muhammad Asiful Islam, and Kazuyuki Murase
Abstract—In this paper, we propose two cooperative ensemble
learning algorithms, i.e., NegBagg and NegBoost, for designing
neural network (NN) ensembles. The proposed algorithms incrementally train different individual NNs in an ensemble using the
negative correlation learning algorithm. Bagging and boosting
algorithms are used in NegBagg and NegBoost, respectively, to
create different training sets for different NNs in the ensemble.
The idea behind using negative correlation learning in conjunction
with the bagging/boosting algorithm is to facilitate interaction and
cooperation among NNs during their training. Both NegBagg and
NegBoost use a constructive approach to automatically determine
the number of hidden neurons for NNs. NegBoost also uses the
constructive approach to automatically determine the number
of NNs for the ensemble. The two algorithms have been tested
on a number of benchmark problems in machine learning and
NNs, including Australian credit card assessment, breast cancer,
diabetes, glass, heart disease, letter recognition, satellite, soybean,
and waveform problems. The experimental results show that Neg-
Bagg and NegBoost require a small number of training epochs to
produce compact NN ensembles with good generalization.
Index Terms—Bagging, boosting, constructive approach, diversity, generalization, negative correlation learning, neural network
(NN) ensemble design.
I. INTRODUCTION
HE GENERALIZATION ability of neural networks
(NNs) is an important property for their practical applications to many real-world problems. This generalization ability
can signiﬁcantly be improved through NN ensembles, i.e.,
training several individual NNs and combining their outputs
 – . An ensemble approach has two major components,
i.e., a method for creating individual NNs and a method for
combining NNs. Theoretical and experimental studies have
shown that when the NNs in the ensemble are accurate and their
errors are negatively correlated, an improved generalization
Manuscript received August 17, 2007; revised February 11, 2008. The work
of M. M. Islam was supported by the Japanese Society for the Promotion of
Sciences (JSPS). The work of X. Yao was supported in part by the Engineering
and Physical Sciences Research Council (U.K.) under Grant GR/T10671/01
and by the Fund for Foreign Scholars in University Research and Teaching
Programs (China) under Grant B07033. This paper was recommended by
Associate Editor N. Chawla.
M. M. Islam is with the Bangladesh University of Engineering and Technology (BUET), Dhaka 1000, Bangladesh, and also with the University of Fukui,
Fukui 910-8507, Japan.
X. Yao is with the University of Birmingham, B15 2TT Birmingham, U.K.,
and also with the University of Science and Technology of China, Hefei
230026, China.
S. M. Shahriar Nirjon is with the Bangladesh University of Engineering and
Technology (BUET), Dhaka 1000, Bangladesh.
M. A. Islam is with the State University of New York at Stony Brook, Stony
Brook, NY 11794-4400 USA.
K. Murase is with the University of Fukui, Fukui 910-8507, Japan.
Digital Object Identiﬁer 10.1109/TSMCB.2008.922055
ability can be obtained by voting or averaging the outputs of
NNs – . There is little to be gained by combining NNs
whose errors are positively correlated.
There are a number of alternative approaches that can be
used for producing negatively correlated NNs for the ensemble.
These include varying the initial random weights of NNs,
varying the topology of NNs, varying the algorithm employed
for training NNs, and varying the training sets of NNs. It is
argued that training NNs using different training sets is likely
to produce more uncorrelated errors than other approaches .
This is because it is the training data on which a network is
trained that determine the function it approximates. Data sampling or variation of examples in training data is a common and
effective technique for producing negatively correlated NNs.
The two most popular algorithms for constructing ensembles that independently and sequentially train individual NNs,
respectively, using different training sets are the bagging 
and boosting algorithms. Recently, Liu and Yao – 
proposed negative correlation learning that simultaneously
trains NNs in the ensemble. While bagging and boosting explicitly create different training sets for different NNs by probabilistically changing the distribution of the original training
data, negative correlation learning implicitly creates different
training sets by encouraging different NNs to learn different
parts or aspects of the training data.
As explicit and implicit approaches for creating training sets
have complementary strengths and limitations (as described in
Section II), the question arises as to whether their integration
could lead to an improved and more powerful ensemble learning scheme. We studied this question and propose two new
ensemble learning algorithms, i.e., NegBagg and NegBoost,
for designing NN ensembles. The NegBagg and NegBoost
algorithms incorporate negative correlation learning in bagging
and boosting, respectively, for incrementally training NNs. The
reason for using negative correlation learning in conjunction
with bagging and boosting algorithms is to improve the interaction and cooperation among NNs in ensembles. This approach is quite different from others, which independently ,
sequentially , , or simultaneously – train NNs
in ensembles by explicitly or implicitly dividing the training
data. It also differs from these other approaches in its automatic
construction of the ensemble architecture.
In , an ensemble learning algorithm is proposed, which
sequentially trains NNs in the ensemble. The algorithm attempts not only to minimize the NN error but also to decorrelate
the error from previously trained NNs. Since this algorithm
does not provide any mechanism for interaction among NNs,
sequentially training an NN cannot affect the previously trained
1083-4419/$25.00 © 2008 IEEE
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
NNs. The errors of the NNs are, therefore, not necessarily
negatively correlated . Our algorithms, i.e., NegBagg and
NegBoost, incrementally train each NN in the ensemble using
a different training set and negative correlation learning. The
negatively correlated NNs can easily be obtained by using
NegBagg and NegBoost. Such a training strategy has produced
a signiﬁcant improvement in the NN ensemble’s performance,
as shown by our study.
The NegBagg and NegBoost algorithms are different from
the mixtures-of-experts (ME) architecture – , which
consists of a gating network and a number of expert networks.
Although the ME architecture can also produce biased NNs
whose estimates are negatively correlated, it uses a gating
network. The NegBagg and NegBoost algorithms do not require
such a gating network. They use different training sets and
negative correlation learning to ensure negative correlation
among NNs in an ensemble. The advantage of this approach is
that the λ parameter of negative correlation learning provides a
convenient way to balance the bias–variance–covariance tradeoff. The ME architecture does not provide such control over this
Recently, a constructive algorithm for training cooperative
NN ensembles (CNNEs) has been proposed . The algorithm
automatically determines the ensemble architecture and uses
incremental training based on negative correlation learning
for training NNs. However, it does not use different data for
training NNs. One major disadvantage of such an approach is
that CNNE solely relies on negative correlation learning for
producing negatively correlated NNs. Since all the NNs in an
ensemble are trained using the same training data, there will be
competitions during the training of NNs . We extend CNNE
by incrementally training individual NNs using different training sets that are created using bagging and boosting algorithms.
Such an extension has produced a signiﬁcant performance gain
for the NN ensembles.
The rest of this paper is organized as follows. Section II
discusses various methods to create NNs for ensembles and
explains why we combine negative correlation learning with
bagging and boosting algorithms. Section III describes our Neg-
Bagg and NegBoost algorithms in detail. Section IV presents
the results of our experimental study. Finally, Section V
concludes this paper with a brief summary and a few remarks.
II. PREVIOUS WORK
The idea of designing an ensemble learning system can be
traced back to as early as 1958 , . Since the early
1990s, algorithms based on similar ideas have been developed
in many different but related forms such as NN ensembles and
ME. An NN ensemble approach can be viewed as comprised
of two different methods, i.e., a method for creating individual
NNs and a method for combining the outputs of NNs. One
important feature in creating NNs is that the networks should
be accurate yet diverse, i.e., produce uncorrelated errors. The
accuracy of NNs is dependent on their architectures, whereas
the diversity is dependent on the error correlation among NNs.
This section brieﬂy describes some work toward creating NNs
for an ensemble.
The most prevalent methods for creating NNs for an ensemble are the bagging and boosting algorithms. Bagging 
creates M NNs for the ensemble by independently training
these M NNs on M different training sets, which are generated
by forming bootstrap replicates of the original training data.
This approach is particularly attractive when there is not much
training data available. Another advantage of bagging is that all
individual NNs in the ensemble can independently be trained in
parallel. The boosting algorithm was proposed by Schapire 
and improved by Freund and Schapire . This algorithm
generates a series of NNs whose training sets are determined
by the performance of the former ones. Training examples
that were wrongly predicted by former NNs will play more
important roles in the training of later NNs. The main advantage
of boosting is that the number of NNs in the ensemble can easily
be automatically determined because the NNs are trained one
after another. However, the main problem of both bagging and
boosting is that the NNs created by them are not necessarily
negatively correlated , .
The negative correlation learning algorithm proposed by
Liu and Yao – encourages different NNs in an ensemble
to learn different parts or aspects of the training data, so that
the ensemble can efﬁciently learn the entire training data. The
algorithm trains NNs simultaneously and interactively rather
than independently or sequentially by introducing a correlation
penalty term into the error function of NNs. The advantage of
negative correlation learning is that the algorithm can produce
biased NNs whose errors tend to be negatively correlated.
However, NNs may engage in competition during their training
because all NNs are concerned with the same remaining ensemble error in the training scheme of negative correlation learning
 . There is no chance for a such competition in a bagging or
boosting algorithm because the algorithm independently trains
each NN in the ensemble on a separate training set.
There are also many other approaches to create NNs for
ensembles. Raviv and Intrator present a method for creating NNs that uses a combination of bootstrap sampling of
data, variable amounts of noise to inputs, and weight decay.
Cherkauer creates NNs with different numbers of hidden
nodes. Maclin and Shavlik initialize NNs at different points
in the weight space. Krogh and Vedelsby employ cross
validation to create NNs. Opitz and Shavlik exploit the
genetic algorithm to train diverse knowledge-based NNs. Oza
and Tumer present a method that seeks to reduce the correlations among NNs using different subsets of input features.
Recently, the DECORATE algorithm was proposed to
create diverse NNs for an ensemble in a simple and straightforward manner. The algorithm creates different training sets for
different NNs in the ensemble by adding different artiﬁcially
produced examples to the training sets. Granitto et al. 
proposed a stepwise ensemble construction algorithm that seeks
a new NN that should be at least partially anticorrelated with
the previous NNs in the ensemble. This is achieved by applying
a late-stopping method in the learning phase of NNs, leading
to a controlled level of overtraining of the ensemble members.
The algorithm retains the simplicity of independent network
training, although, if necessary, it can avoid the computational
burden by saving intermediate NNs.
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS
Liu uses the bagging algorithm to create different training sets for ensembles. In , each training set is used once for
training all the NNs in an ensemble using negative correlation
learning. The aim of creating separate training sets is to use
them for the cross-validation purpose. One basic difference
between NegBagg and the work in is that NegBagg uses
negative correlation learning to establish training time interaction among NNs in the ensemble trained by different training
sets. In addition, NegBagg considers the accuracy and diversity
of NNs in the ensemble, whereas the work in considers
diversity among NNs. Both accuracy and diversity are important for improving the performance of ensembles. These are
also basic differences between NegBagg/NegBoost and most
existing algorithms in the literature. More developments in NN
ensembles can be found in the survey paper .
III. NEGBAGG AND NEGBOOST
This section describes our NegBagg and NegBoost algorithms in detail. Both algorithms incrementally train individual
NNs in an ensemble using different training sets. The algorithms use a constructive approach to automatically determine
the number of hidden nodes in NNs. Furthermore, NegBoost
also uses the constructive approach to automatically determine
the number of individual NNs in the ensemble. To facilitate
training time interactions among NNs, the two algorithms use
negative correlation learning , to train the NNs in the
In comparison with bagging, boosting, and negative correlation, the major advantages of the proposed algorithms include
the following: 1) the use of a single-stage ensemble learning
approach, i.e., the creation and combination of individual NNs
are accomplished in the same learning phase; 2) the use of
incremental training based on negative correlation learning and
different training sets; and 3) maintaining both accuracy and
diversity among NNs in ensembles.
Three-layered feedforward networks are used as individual
NNs for constructing ensembles in NegBagg and NegBoost.
The sigmoid transfer function is employed for nodes in the
hidden and output layers of NNs. This is, however, not an
inherent constraint. In fact, any type of NNs (e.g., CCA ,
CNNDA , and RBF ) and transfer functions (e.g.,
threshold function and Hermite polynomial ) can be
used in the proposed algorithms.
A. NegBagg Algorithm
The major steps of NegBagg can be summarized as
Step 1) Create an ensemble architecture consisting of M
individual NNs. Here, M is a user-speciﬁed parameter, and it is a positive integer number. The number
of nodes in the input and output layers of NNs is
deﬁned by the problem. Since NegBagg uses a constructive approach to determine the architecture of
NNs, the hidden layer of NNs initially contains only
one node, similar to other constructive approaches
(see the review paper ). If the initial architecture
contains more than one node, it is necessary to know
how many nodes will be for the initial architecture,
which is problem dependent and difﬁcult to determine in advance. All the connection weights of each
NN in the ensemble are initialized at random within
a small range.
Step 2) Create M training sets by forming bootstrap replicates of the original training data as used in
bagging .
Step 3) Partially train M NNs in the ensemble, each with
a different set, for a certain number of training
epochs using negative correlation learning .
The number of training epochs τ is speciﬁed by the
user. The term partial training was ﬁrst introduced
in . It means that an NN is to be trained for a
ﬁxed number of epochs regardless of whether it has
converged or not.
Step 4) Compute the ensemble error on the validation set.
If the termination criterion is not satisﬁed, go to
the next step. It is assumed here that the NNs in
the ensemble are not sufﬁciently trained or the architectures of NNs are too small. Otherwise, stop
the ensemble training and determine the accuracy of
the existing ensemble architecture using the testing
set of the given problem. According to , the
ensemble error E is deﬁned as
Fi(n) −d(n)
where N is the number of patterns in the validation
set, Fi(n) is the output of the network i for the nth
example in the validation set, and d(n) is the desired
output of the nth validation example.
Step 5) Add one hidden node to any NN in the ensemble if
the NN satisﬁes the node addition criterion, and go
to Step 3).
It is now clear that NegBagg can automatically add hidden
nodes to NNs during the training process of the ensemble. The
algorithm adds a hidden node by splitting an existing node in
an NN. The process of a node splitting is called “cell division”
by Odri et al. . Two nodes created by splitting an existing
node have the same number of connections as the existing
node. According to , the weights of the new nodes are
calculated as
i = (1 + β)wi
where wi represents the ith weight of the existing node, and w1
i are the corresponding weights of the two new nodes. β
is a mutation parameter whose value may be a ﬁxed or random
one. The advantage of this addition process is that it does not
require random initialization of the weight vector of the newly
added node. As a result, the new NN can better maintain the
behavioral link with its predecessor .
Although the design of NN ensembles is generally formulated as a two-stage process , NegBagg trains and combines NNs in
the same learning process using a simple cost function. The
idea behind NegBagg is simple and straightforward. NegBagg
minimizes the ensemble error ﬁrst by training NNs and then by
adding hidden nodes to existing NNs. Details about the node addition criterion used in NegBagg will be given in a later section.
B. NegBoost Algorithm
The major steps of NegBoost can be summarized as
Step 1) Initialize an ensemble architecture with one NN.
The number of nodes in the input and output layers
of the NN is deﬁned by the problem. Initially, the
hidden layer contains only one node. All connection
weights of the NN are initialized at random within a
small range. Label the individual NN with I.
Step 2) Create a new training set for the newly added
NN in the ensemble by changing the distribution of examples in the training data as used in
boosting .
Step 3) Partially train the I-labeled NN in the ensemble on
the new training set for a certain number of training epochs using negative correlation learning. The
number of training epochs τ is speciﬁed by the user.
Step 4) Compute the ensemble error on the validation set.
If the termination criterion is not satisﬁed, it is
assumed that the I-labeled NN is not sufﬁciently
trained or the ensemble architecture is too small, and
go to the next step. Otherwise, stop the ensemble
training and determine the accuracy of the existing
ensemble architecture using the testing set of the
given problem.
Step 5) Check
construction and node addition. If the I-labeled NN
does not satisfy the criterion for stopping network
construction or node addition, it is assumed that
the NN is not sufﬁciently trained, and go to Step 3)
for further training. Otherwise, go to the next
step for the modiﬁcation of the existing ensemble
architecture.
Step 6) Replace the labeling of the I-labeled NN by F
if it satisﬁes the stopping criterion for network
construction.
Step 7) If all the individual NNs in the ensemble have been
marked with the label F, add one new individual NN
to the ensemble. Initialize the new NN in the same
way as that described in Step 1) and go to Step 2).
Otherwise, go to the next step.
Step 8) Add one hidden node to the I-labeled NN and go
to Step 3). The hidden node is also added here, as
in NegBagg, by splitting an existing node of the
I-labeled NN.
Although NegBoost uses the original boosting algorithm in
creating different training sets for NNs in ensembles, it can also
use adaboost.M1 or adaboost.M2 . This is because
the boosting algorithm is used here only for creating training
sets. Similar to NegBagg, NegBoost also trains and combines
individual NNs in the same learning process using a simple
cost function, i.e., the ensemble error E. However, NegBoost
automatically determines not only the number of hidden nodes
in NNs but also the number of NNs in the ensemble. The
following section gives more details about our algorithms.
1) Criterion for Node Addition: Both NegBagg and Neg-
Boost use the same criterion for deciding when to add hidden
nodes to existing NNs in an ensemble. The criterion is based on
the contribution of individual NNs to the ensemble. According
to , the contribution Cm of the mth NN to the ensemble at
any training epoch is
where E is the ensemble error including the network m, and
Em is the ensemble error excluding the network m. The errors
are computed according to (1). The criterion is very simple and
does not require any extra computational cost because Em is a
part of E. Thus, one could extract the value of Em during the
computation of E and save it for future use.
According to the node addition criterion, NegBagg and Neg-
Boost add one node to any network m when its contribution Cm
to the ensemble does not improve by a threshold ϵ after a certain
number of training epochs τ. The parameters ϵ and τ are speci-
ﬁed by the user. The addition criterion is tested in NegBagg and
NegBoost for every τ epochs and is described as 
Cm(t + τ) −Cm(t) < ϵ,
t = τ, 2τ, 3τ, . . . .
2) Criterion for Stopping Network Construction: Another
simple criterion is used in NegBoost for deciding when to stop
the construction of individual NNs. The NegBoost algorithm
stops the construction of an I-labeled NN when its contribution
C to the ensemble, measured after the addition of each hidden
node, failed to improve after the addition of a certain number of
hidden nodes nh. The parameter nh is speciﬁed by the user, and
it is a positive integer number. In other words, the construction
of the I-labeled NN is stopped when the following is true:
C(i + nh) ≤C(i),
i = 1, 2, . . . .
If nh = 0, then all individual NNs can consist of only one
hidden node. In NegBoost, no nodes are added to the NNs after
their construction processes have been stopped. The salient
feature of using the individual NN’s contribution as a criterion
for node addition and stopping network construction reﬂects
our emphasis on the cooperative training of NNs for designing
ensembles. This is because the contribution of any NN in an
ensemble will increase only when the NN produces accurate but
uncorrelated outputs with respect to other NNs in the ensemble.
As with NegBoost, NegBagg does not stop the construction
process of NNs during the training process of the ensembles.
Instead, the algorithm rather continuously adds hidden nodes
to an NN one after another if the node addition criterion is
satisﬁed and the problem is remain unsolved. The following is
the reason for such a continuous addition of hidden nodes in
the training scheme of NegBagg. It is clear from Section III-A
that NegBagg does not add NNs during the training process of
an ensemble. Hence, when the performance of the ensemble
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS
does not improve after training, the algorithm can only try to
improve the ensemble performance by increasing its computational power, i.e., by adding hidden nodes to existing NNs.
3) Termination Criterion: It can be seen from Sections III-A
and B that both NegBagg and NegBoost use the validation error
for stopping ensemble training. Since the two algorithms use
a constructive approach in designing ensembles, the training
error of the ensembles will gradually decrease as their construction processes progress. However, at some point, usually in the
later stages of training, ensembles may start to take advantage
of idiosyncrasies in the training data, and their generalization
performance, i.e., performance on a testing set, may start to
deteriorate although the training error continues to decrease.
This type of overﬁtting due to overtraining is described in .
One common approach to avoid overﬁtting is to estimate the
validation error during training and stop the training process
when the error begins to increase. The simplest method to
achieve this goal is to divide the training data into a training
set and a validation set. The training set is used to modify the
weights of NNs and the validation set to determine when the
training process is to be stopped. In other words, the validation
set is used to anticipate the behavior of the testing set, assuming
that the error on both validation and testing sets is similar.
However, the real situation is a lot more complex because the
real generalization curves almost always have more than one
local minima. It is shown for linear networks with K inputs
and K outputs that up to K such local minima are possible; for
multilayer NNs, the situation is even worse . In general, it is
impossible to know whether an increase in the validation error
indicates real overﬁtting or is just intermittent.
In NegBagg or NegBoost, a very simple criterion that relies
on the sign of the changes in the validation error is used for
deciding when the ensemble training is to be stopped. The
criterion stops the training process of the ensemble when its
validation error increases for N successive times. The idea behind this deﬁnition is that when the validation error is increased
not just once but during N consecutive times, it can be assumed
that such increases indicate the beginning of ﬁnal overﬁtting,
independent of how large the increases actually are. A similar
criterion is described in for automatically stopping the
training process of a single NN.
C. NegBagg/NegBoost and Variance Reduction
One of the main explanations for the improvements achieved
by ensembles is based on separating the expected error into
a bias term and a variance term. Several methods have been
suggested for decomposing the error into bias and variance. The
bias term basically measures how closely a learning algorithm’s
average guess (over all the possible training sets of the given
training set size) matches the target. The variance term measures how much the learning algorithm’s guess ﬂuctuates for
the different training sets of the given size.
The bias of an ensemble is simply the average of the biases
of individual NNs. The introduction of negative correlation
learning in bagging and boosting helps reduce the ensemble
bias. When an NN in the ensemble fails to solve some parts
or aspects of a given task, other NNs become aware of this
situation in the training scheme of negative correlation learning, and the NNs can try to solve the unsolved parts of the
task. Eventually, the unsolved parts of the task get solved. If
the interaction among NNs in the ensemble were completely
missing as in pure bagging or boosting , it might be
the case that some portion of the task remain unsolved, and the
bias term does not signiﬁcantly reduce with training. However,
NegBagg or NegBoost overcomes this problem by facilitating
interaction through negative correlation learning , and
can successfully keep the bias of the error signiﬁcantly reduced.
The variance V (x) of the ensemble can be deﬁned in terms of
the deviations of individual NN predictions fk(x). According to
 , V (x) can be expressed as
V (x) = ED
where wk is the weight for the kth NN, which are positive
constants and sum to 1, and ∆fk(x) denotes the deviation
of fk(x). The expectation is taken over all training sets D.
The ensemble variance in (7) can be related to the variance of
the individual NN, which is ED[(∆fk)2]. Since V (x) cannot
be more than the average of the variances of all NNs in the
ensemble, we get the inequality denoting the upper bound of
V (x), which is
This bound is saturated when the ﬂuctuations of the networks’ outputs are fully correlated and have equal variance
 . This situation occurs when all NNs in the ensemble are
identical, which means each NN in the ensemble reacts very
similarly to different training sets. On the other hand, when the
ﬂuctuations of NNs are completely uncorrelated, the variance
of the ensemble can be expressed as
This shows that, when NNs are given equal weights (i.e.,
wk = 1/k), one can have a reduction of variance by a factor of
O(1/k). It is therefore proved that the ensemble variance can be
reduced by a large factor when individual NNs are uncorrelated.
Now the question is whether the introduction of negative correlation learning in bagging or boosting can reduce the correlation
among NNs in the ensemble. The answer is afﬁrmative, and
the following few paragraphs explain how negative correlation
learning reduces the correlation among NNs.
The bagging algorithm creates separate training sets for the
different NNs of an ensemble by forming bootstrap replicates of
the original training data T. Let T consist of N training examples. The algorithm constructs training sets by uniformly drawing N examples at random (with replacement) from T. It can
easily be shown that there is correlation among NNs although
they are trained with separate training sets created by bagging.
Suppose that two new training sets Ti and Tj of size N are
created from T using bagging. Let Di and Dj be the set of distinct examples inTi andTj,respectively.WheneverTi
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
two NNs trained by Ti and Tj will produce correlated outputs
for any example x ∈(Ti
Tj). If (|Di| + |Dj|) > N, then
surely such x exists. Let |Di| = riN and |Dj| = rjN, where ri
and rj are positive fractions. We now investigate the probability
P((|Di| + |Dj|) <= N). By inserting the value of Di and Dj,
and replacing the value of ri and rj with their expected value
E[r], the probability term becomes P(E[r] ≤1/2). For an
ensemble consisting of M NNs, the probability term becomes
P(E[r] ≤1/M). This probability decreases as M increases. It
is therefore proved that bagging cannot completely eliminate
the correlation among NNs in the ensemble.
The introduction of negative correlation learning in bagging
can completely remove the correlation. This is because negative
correlation learning facilitates interaction among NNs by introducing a penalty term into the error function of NNs. In negative
correlation learning, the error Ei(n) of the ith NN for the nth
training example is expressed by the following equation :
2 (Fi(n) −d(n))2 + λpi(n)
where Fi(n) and d(n) are the actual and desired outputs of
the ith NN for the nth training example, respectively. The ﬁrst
term in (10) is the empirical risk function of the ith NN. The
second term pi is the correlation penalty function, which can
be expressed as
pi(n) = (Fi(n) −F(n))
(Fj(n) −F(n)) .
The interaction among NNs of the ensemble is controlled by
the value of λ. The penalty function keeps the NNs negatively
correlated. Once a training example x ∈T is learned by an NN,
negative correlation learning inherently forces other NNs in
the ensemble to learn from T −{x}. The NegBagg algorithm,
being a hybrid of negative correlation learning and bagging,
uses negative correlation learning to keep the NNs in the
ensemble negatively correlated. Since bagging is incorporated,
each NN in an ensemble of NegBagg has a different training
set. The common examples (i.e., Ti
Tj ̸= φ) were a cause for
correlation among NNs in bagging. Negative correlation learning works on these common examples and implicitly divides
the learning task (Ti
Tj) among NNs. It thereby removes the
correlations that were present even after bagging. The explicit
division of task by bagging is therefore complemented by the
negative correlation’s implicit task division, and hence, the
correlation is eliminated. Since the ensemble variance is related
to the correlation among NNs, NegBagg reduces the ensemble
variance, and it thereby reaches an ensemble variance given by
(9). A similar analysis is also applicable for NegBoost.
IV. EXPERIMENTAL STUDIES
In this section, we evaluate the performance of NegBagg and
NegBoost on several classiﬁcation problems from the University of California, Irvine, Machine Learning Repository and one
regression problem used in . Nine classiﬁcation problems
are considered for evaluating the performance. They are the
Australian credit card, breast cancer, diabetes, glass, heart disease, letter recognition, satellite image, soybean, and waveform
CHARACTERISTICS OF NINE CLASSIFICATION DATA SETS
problems. Detailed descriptions of the classiﬁcation problems
are available at ics.uci.edu in directory/pub/machine-learningdatabases. The characteristics of the classiﬁcation problems
are summarized in Table I, which shows considerable diversity in the number of examples, attributes, and classes among
A. Experimental Setup
The data sets representing nine classiﬁcation problems were
preprocessed by rescaling the input attribute values to between
0 and 1. The outputs were encoded by 1 −of −c for c classes,
where the output node with the highest activation was designated as the network output. The ﬁrst 50%, the following 25%,
and the ﬁnal 25% examples in all data sets except the letter were
used for the training, validation, and testing sets. Such partition
follows previous suggestions on benchmarking , . For
the letter data set, 16 000 and 2000 examples were randomly
selected for the training and validation sets, and the remaining
2000 examples in the data set were used for the testing set.
For each individual NN, one bias node with a ﬁxed input 1
was used for the hidden and output layers. The logistic sigmoid
function f(y) = 1/(1 + exp(−y)) was used as an activation
function for all nodes in the hidden and output layers. The
initial connection weights for individual NNs in an ensemble
were randomly chosen in the range of −0.3 to 0.3. The learning
rate and momentum of backpropagation were chosen in the
range of 0.10–0.50 and 0.5–0.9, respectively. The values used
for τ and nh were 10 and 2, respectively. The threshold value
ϵ was chosen between 0.10 and 0.25. The value of M, i.e., the
number of individual NNs in NegBagg, was chosen between 5
and 10 for all problems except for the letter, where it was chosen
between 10 and 15. The value of N used in the termination
criterion to stop the ensemble construction process was set to
2. The same experimental settings were used in our previous
studies . It has been shown in that the values of τ, nh,
and ϵ do not appreciably affect the performance of ensembles
when a constructive approach is used for designing ensembles.
Two sets of experiments were performed in our study. In the
ﬁrst set of experiments, the value of the correlation parameter λ
was set to 1 in both NegBagg and NegBoost. In the second set of
experiments, it was set to 0, which is equivalent to using plain
bagging and boosting in association with optimization of the
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS
PERFORMANCE OF NEGBAGG WITH λ = 0 AND λ = 1 FOR
DIFFERENT CLASSIFICATION PROBLEMS. THE RESULTS
WERE AVERAGED OVER 30 INDEPENDENT RUNS
ensemble architecture. Other parameters, such as the training
algorithm, learning rate, and the heuristics for adding hidden
nodes, were chosen the same in both sets of experiments. The
use of the same experimental settings will provide a fair chance
to compare the results of the two sets of experiments. The
comparison may give the answer to a crucial question: whether
is it beneﬁcial to use NegBagg and NegBoost or is it sufﬁcient
to use bagging and boosting in association with optimization of
the ensemble architecture?
B. Experimental Results
Tables II and III summarize the average results of NegBagg
and NegBoost, respectively, over 30 independent runs on nine
classiﬁcation problems. The error rate in the tables refers to
the percentage of wrong classiﬁcations produced by the trained
ensemble on the testing set. The number of epochs refers to the
total number of iterations required for training all individual
NNs in an ensemble.
It is clear from Tables II and III that the ensemble architectures produced and the number of training epochs required
by both NegBagg and NegBoost were inﬂuenced by the value
of λ. For example, for the card problem, the average numbers
of hidden nodes in the NNs of an ensemble produced by
NegBagg with λ = 0 and λ = 1 were 5.2 and 4.3, respectively.
The average numbers of epochs required by NegBagg with
λ = 0 and λ = 1 were 50.3 and 42.5, respectively. These
results indicate that a value of unity for λ in NegBagg helps
produce compact NN architectures that require a small number
of training epochs. This is reasonable because when λ = 1,
NNs in the ensemble can communicate with each other during
training. Thus, different NNs in the ensemble can cooperatively
learn separate parts or aspects of training data. In contrast,
PERFORMANCE OF NEGBOOST WITH λ = 0 AND λ = 1 FOR
DIFFERENT CLASSIFICATION PROBLEMS. THE RESULTS
WERE AVERAGED OVER 30 INDEPENDENT RUNS
when λ = 0, NNs cannot communicate with each other during
training. Consequently, different NNs in the ensemble may
learn the same parts or aspects of the training data because there
are overlaps among examples in different training sets created
by bagging . This means that the ensemble will learn
redundant information when λ = 0. It is natural to require more
computational resources, i.e., large architectures and training
epochs, to learn redundant information. Similar phenomena
were also observed for NegBoost.
The positive effect of using λ = 1 in NegBagg and NegBoost
is also seen from the classiﬁcation accuracies. For the card
problem, for example, the average testing error rates of Neg-
Bagg with λ = 0 and λ = 1 were 0.129 and 0.112, respectively.
For the diabetes problem, the average error rates of NegBoost
with λ = 0 and λ = 1 were 0.208 and 0.187, respectively. As
shown in Tables II and III, both NegBagg and NegBoost with
λ = 0 produced large ensemble architectures, i.e., NNs with
more hidden nodes. It is well known that small NN architectures
are more likely to have better generalization ability, i.e., smaller
testing error rates , . In summary, when the same
architectural optimization procedure and experimental setup
were used, NegBagg and NegBoost with λ = 1 were found
better than their counterparts with λ = 0 for producing compact ensemble architectures with small testing error rates and
epochs. This indicates the essence of using negative correlation
learning in conjunction with bagging or boosting for producing
better ensembles.
In this paper, the t-test was used to determine whether
the performance difference between two different sets of experiments is statistically signiﬁcant or not. It was found that
NegBagg or NegBoost with λ = 1 was signiﬁcantly better than
its counterpart with λ = 0 at 95% conﬁdence level, with the
exception of the cancer problem. The t-test based on error rate
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
indicated that the performance of NegBagg (or NegBoost) with
λ = 0 and λ = 1 was similar for the cancer problem, which is
the easiest problem in the realm of machine learning.
The t-test between NegBoost and NegBagg with λ = 1 indicated that the error rate of NegBagg is signiﬁcantly lower
than that of NegBoost for the glass and heart problems. It is
shown in Table I that the number of training examples for these
two problems is small. Furthermore, the heart problem contains
many noisy examples or missing attributes. It is known that
boosting is not very effective for small and noisy data – .
Neither NegBagg nor NegBoost was found superior for the
soybean problem. For the other problems, when the number of
training examples was large or the examples were nonnoisy,
the error rate of NegBoost was signiﬁcantly better than that of
NegBagg. However, NegBagg required a signiﬁcantly smaller
number of epochs than that of NegBoost for all problems. This
is because NegBagg trained all the NNs of an ensemble in
parallel, whereas NegBoost sequentially trained them one after
another. The difference in average training epochs between
NegBagg and NegBoost is at least an order of magnitude three
for all problems (Tables II and III). The order is high for
problems with a large number of training examples, e.g., the
letter problem. The experimental results indicate that NegBagg
could be a good choice when the training time is an important
issue, or there are few examples in the data set, or the examples
in the data set contains noise. It is known that both the error rate
and the training time are important in many application areas;
improving one at the expense of the other becomes a crucial
decision . It would be highly desirable to devise some
efﬁcient parallel implementation mechanism of NegBoost.
Although it was found that NegBoost is better than boosting,
an interesting question arises as to whether NegBoost is better
than adaboost.M1 in terms of the ability to deal with noise.
To investigate this issue, these two algorithms were applied on
the following regression function :
10 sin(πx1x2) + 20
+ 10x4 + 5x5
where x = [x1, . . . , x5] is an input vector whose components
lie between 0 and 1. The value of f(x) lies in the interval
[1, −1]. The training and testing sets consisted of 500 and 1024
input–output examples, respectively. Here, the components of
the input vectors were independently sampled from a uniform
distribution over the interval (0, 1). The target outputs were
created by adding moderate noise (σ2 = 0.1) and large noise
(σ2 = 0.2) sampled from a Gaussian distribution with a mean
of 0 and a variance of σ2 to the function. The same noise
condition is used in to determine the effect of their algorithm on noisy data. To make a fair comparison, adaboost
used the same ensemble architectures and epochs that were
obtained by NegBoost with λ = 1. It is important to note here
that the suggestions provided in were used for conducting
experiments on the regression problem using NegBoost and
adaboost.M1.
COMPARISON BETWEEN NEGBOOST AND ADABOOST WITH NO,
MODERATE, AND HIGH NOISE CONDITIONS
Table IV compares the performance of NegBoost and adaboost for the regression function with no, moderate, and high
noise conditions. The results show that NegBoost outperformed
adaboost in terms of the mean squared error. It is seen that
the performance of adaboost is more affected by noise when
compared with NegBoost. Since adaboost puts more weights on
training examples that were misclassiﬁed, this leads adaboost to
overﬁt very badly in noisy conditions. In contrast, NegBoost
does not put more weights on misclassiﬁed examples so its
performance is not appreciably affected due to noise.
C. Analyses
A number of measures have been proposed in the literature
with the aim of gaining a better understanding of the ensemble
performance. The three most widely used measures are correlation, margin, and bias–variance decomposition. The purpose
of this section is to analyze the performance of our proposed
algorithms based on these three measures.
1) Correlation: Correlation is one of the most common and
most useful statistics that describes the degree of relationship between two variables. A number of criteria have been
proposed in the literature to estimate the correlation between
different pairs of NNs in an ensemble . However, there has
not been a conclusive study showing which measure is the best.
In our study, the correlation between two networks i and j is
calculated as follows :
Cor(i, j)=
(n)−F i(n)
(n)−F i(n)
(n)−F j(n)
(n)−F j(n)
where F (k)(n) and F (k)
(n) are the outputs of the ensemble and
network i, respectively, of the nth pattern in the testing set from
the kth simulation, and K is the number of simulations. When
the ensemble consisted of M NNs, there are CM
correlations
in total between different pairs of NNs.
Twenty-ﬁve simulations of an ensemble architecture obtained by NegBagg and NegBoost were conducted for three
different problems. The same number of simulations was
conducted as in the previous study to determine correlations
among NNs in the ensemble . In each simulation, the same
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS
Correlation among individual NNs produced by NegBagg with λ = 0 and λ = 1 for three different classiﬁcation problems.
ensemble architecture was trained on a different training set
from the same random initial weights. The training algorithm
employed in each simulation was also the same. In other words,
different simulations of the same architecture could yield different performances only due to the use of different training sets.
Such an experimental setup follows the suggestion from .
The architectures of ensembles for different problems were
chosen from our previous experiments. There were ﬁve, seven,
and eight NNs, respectively, in the ensemble architectures for
the cancer, satellite, and waveform problems. These values
were the same for both NegBagg and NegBoost. The reason
for choosing the same value for NegBagg and NegBoost is to
make a fair comparison between the two algorithms.
Figs. 1 and 2 summarize our experimental results for representing correlations between NNs in ensemble for three different problems. It is observed that NegBagg and NegBoost with
λ = 1 tended to produce more negatively correlated NNs for
the ensembles than their counterparts with λ = 0. The number
of negatively correlated NNs and the summation of negative
correlations were large for NegBagg and NegBoost with λ = 1.
In contrast, the number of positively correlated NNs and the
summation of positive correlations were large for NegBagg and
NegBoost with λ = 0. The t-test indicated that NegBagg and
NegBoost with λ = 1 were superior to their counterparts with
λ = 0 for producing negative correlation. However, NegBagg
and NegBoost with λ = 1 were inferior to their counterparts
with λ = 0 for producing positive correlation, which is not
beneﬁcial for the performance of ensembles.
It is also shown in Figs. 1 and 2 that NegBoost with λ = 1
produces a larger number of negatively correlated NNs than
NegBagg with λ = 1. In addition, the summation of negative
correlations produced by NegBoost with λ = 1 was larger than
that produced by NegBagg with λ = 1. A possible reason for
such difference might be a nonoptimal M (i.e., number of NNs
in an ensemble) we used in NegBagg. The t-test indicated that
NegBoost with λ = 1 produced a signiﬁcantly larger amount
of negative correlation than that of NegBagg with λ = 1 with
95% conﬁdence level for the cancer, satellite, and waveform
problems. However, when the t-test was conducted based on
the number of negatively correlated NNs, NegBoost with λ = 1
was found to be signiﬁcantly better than that of NegBagg with
λ = 1 for the satellite and waveform problems. For the cancer
problem, the performance of the two algorithms was found to
be similar.
2) Margin: The margin of an example is deﬁned as the
difference between the weight assigned to the correct label
and the maximal weight assigned to any single incorrect label.
Schapire et al. proposed an analysis of the generalization
performance of bagging , , and boosting ,
 using margin. Margin is generally expressed as a number
between −1 and +1, and is positive if the example is correctly
classiﬁed. Furthermore, the absolute value of the margin represents the conﬁdence of classiﬁcation.
It is customary to plot the margin as a cumulative distribution
graph, that is, f(z) versus z, where z is the margin, and f(z)
is its cumulative value. A margin distribution curve that moves
to the right is indicative of a more conﬁdent classiﬁcation. It
has also been shown that large margins are associated with
superior upper bounds on the generalization error . The idea
that maximizing the margin could improve the generalization
error of a classiﬁer was also studied earlier , . However, other more recent results show that improving the whole
margin distribution can also result in a degraded generalization .
The margin distribution curve of NegBagg and NegBoost on
cancer and waveform problems is shown in Fig. 3. It is clear
from the ﬁgure that the number of examples with high margin
increased when λ = 1 was used in NegBagg and NegBoost. For
example, for the cancer problem, all examples had a margin
greater than 0.5 for NegBoost with λ = 1. In contrast, some
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
Correlation among individual NNs produced by NegBoost with λ = 0 and λ = 1 for three different classiﬁcation problems.
Margin distributions of NegBagg and NegBoost for (top row) cancer and (bottom row) waveform problems.
examples had a margin below 0.5 when λ = 0 was used in
NegBoost. The achievement of high margin could be attributed
to the fact that when λ = 1, NNs in an ensemble need to learn
a smaller number of examples. This is because NegBagg or
NegBoost with λ = 1 can produce more negatively correlated
NNs for ensembles than NegBagg or NegBoost with λ = 0
(Figs. 1 and 2). It is natural that an NN can concentrate more
on increasing the margins when it learns a smaller number of
examples. The margin distribution curve also indicates that the
use of negative correlation learning in bagging or boosting did
not change the nature of the curve.
3) Bias and Variance: In the ensemble literature, a number
of ideas have been suggested for decomposing the classiﬁcation
error into bias and variance. The decomposition proposed by
Kohavi and Wolport is used in this paper to estimate the
bias and variance. A two-stage sampling procedure is used in
 . First, a test set is split from the training set. Then, the
remaining data D are repeatedly sampled to estimate the bias
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS
RESULTS OF BIAS–VARIANCE EXPERIMENTS OF NEGBAGG AND
NEGBOOST ON THREE DIFFERENT CLASSIFICATION PROBLEMS
and variance on the testing set. According to , D is to be
chosen in such a way that it is twice the size of the desired
training set. We create ten training sets from D using uniform
random sampling without replacement. The whole process is
repeated three times to get stable estimates.
The results of bias–variance experiments on three different
problems are summarized in Table V. Both NegBagg and
NegBoost show a reduced variance with an increased λ. For
example, for the satellite problem, the variance of NegBagg
was 0.056 when λ = 0, and it was reduced to 0.038 when
λ = 1. The effect of using λ = 1 for producing compact NN
architectures is evident from the results in Tables II and III.
These two results are related in the sense that NNs with small
architectures could reduce the variance of the ensemble. The
reduction of variance can also be explained by the fact that with
increased λ, the NNs in the ensemble are more communicative
to reduce correlation (Figs. 1 and 2). It is theoretically shown in
Section III-C that the reduction of correlation between NNs in
the ensemble is beneﬁcial for reducing the variance.
The effect of λ on the bias of an ensemble is also shown in
Table V. The bias is reduced when the NNs in the ensemble
are more communicative in learning the task. For example, the
bias is reduced from 0.131 (λ = 0) to 0.112 (λ = 1) when
NegBagg is applied to the waveform problem. Since NNs in
the ensemble focus on solving different portions of a task when
λ = 1, they are now more specialized and will make less error
on their portions of the task. The overall effect is that the bias
is reduced. Similar results were also observed for NegBoost.
D. Comparison With Other Work
There are many ensemble learning algorithms in the literature that could be compared. However, it is not feasible and
not necessary to conduct an exhaustive comparison with all the
ensemble learning algorithms. The goal of our experimental
comparison here is to evaluate and understand the strengths and
weaknesses of NegBagg and NegBoost as applied to different
problems. The proposed algorithms are compared here against
nine most relevant works. They are bagging , arcboosting
 , and adaboosting.M1 tested by Opitz and Maclin
 ; bagging and adaboosting.M2 tested by Schwenk and
Bengio ; bagging and adaboosting.M1 tested by Dietterich
 ; CNNE ; and evolutionary ensemble with negative
correlation learning (EENCL) .
COMPARISON BETWEEN NEGBAGG, CNNE , BAGGED NN ,
BAGGED NN , BAGGED C4.5 , AND EENCL . NN AND C4.5
INDICATE THE TYPE OF CLASSIFIER USED WITH DIFFERENT
APPROACHES. “—” MEANS NOT AVAILABLE
The aforementioned algorithms represent a wide range of
ensemble approaches. CNNE and EENCL, for example, employ constructive and evolutionary approaches, respectively, for
automatically designing ensembles. However, the remaining algorithms do not design the ensembles, but rather use predeﬁned
and ﬁxed ensemble architectures. Two types of base classiﬁers
are used in different algorithms. Dietterich used C4.5 as a
base classiﬁer with bagging and adaboosting.M1, whereas NN
was used as a base classiﬁer in all the other algorithms. A tenfold cross validation was used in all the algorithms except 
and the proposed algorithms. The data sets were partitioned into
training and testing sets in , whereas they were partitioned
into training, validation, and testing sets in this paper.
Table VI compares the average testing error rates of Neg-
Bagg with those of CNNE, bagged NN, or C4.5 and EENCL.
Table VI shows that NegBagg achieved the smallest error rates
for six out of nine problems and the second smallest (next
to CNNE) for two problems. For one problem, the bagged
NN tested in achieved the smallest error rate, whereas
NegBagg achieved the third smallest error rate.
The improved performance of NegBagg could be attributed
to a couple of factors. First, bagged NN or C4.5 independently
trained individual NNs or constructed decision trees for an
ensemble. The NNs or decision trees could not communicate
with each other during their training or construction process.
In contrast, NegBagg with λ = 1 cooperatively trains NNs in
the ensemble using negative correlation. It is clear from our
previous experiments that the communication among NNs in
an ensemble is helpful for achieving small testing error rates
(Tables II and III).
Second, EENCL and bagged NN manually determined the
architecture of individual NNs, whereas NegBagg automatically determined the architecture. It is well known that the
performance of any NN is greatly dependent on its architecture.
While manual determination of NN architectures might be
appropriate for problems where rich prior knowledge and an
experienced expert exist, it is inappropriate when such knowledge and expertise are unavailable.
The reason that NegBagg was outperformed by CNNE and
bagged NN on two and one problems, respectively, might
be its nonoptimal setting of M, i.e., the number of NNs in the
IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS—PART B: CYBERNETICS, VOL. 38, NO. 3, JUNE 2008
COMPARISON BETWEEN NEGBOOST, CNNE , ADABOOSTED.M1 C4.5
 , ARCBOOSTED , AND ADABOOSTED.M1 NN BASED ON THE
AVERAGE TESTING ERROR RATE. NN AND C4.5 INDICATE THE
TYPE OF CLASSIFIER USED WITH DIFFERENT APPROACHES.
“—” MEANS NOT AVAILABLE
ensemble. CNNE can automatically determine the number of
NNs in the ensemble, whereas NegBagg cannot. In , a large
ensemble architecture was used for the letter problem, which
might be suitable for this problem. Furthermore, the training
strategies used in CNNE, bagged NN , and NegBagg
were different, and they might contribute to the performance
difference.
Table VII compares the average testing error rates of Neg-
Boost with those of CNNE, arcboosted, adaboosted.M1, and
adaboosted.M2. It is shown in Table VII that NegBoost was
able to achieve the smallest average testing error rate among all
the algorithms on six out of nine problems. For two problems,
the error rate of NegBoost and CNNE was similar. There are
three possible reasons why NegBoost performed so well. First,
NegBoost automatically determined the ensemble architecture,
whereas adaboosted.M1 NN or C4.5, arcboosted NN, and adaboosted.M2 NN manually determined the architecture, which
implies that NegBoost could search for more appropriate and
different ensemble architectures for different problems. Second,
NegBoost trained all NNs in the ensemble for an appropriate
(and potentially different) number of training cycles, automatically determined by the algorithm, whereas arcboosted NN,
adaboosted.M1 NN, and adaboosted.M2 NN trained NNs with
a user-speciﬁed and ﬁxed number of training cycles. Third,
NegBoost used both negative correlation learning and boosting
for training NNs in the ensemble, whereas the others used only
one of the two algorithms for training NNs.
It is observed in Tables VI and VII that the error rate
of NegBagg and NegBoost was worse than that of bagging
and boosting tested in for the letter problem, which was
the largest problem among all the problems considered in
this paper. One reason might be that adaboost.M2 was used
in , which is superior to the boosting algorithm used
in NegBoost. Furthermore, the ensemble architecture used in
 for the letter problem was much larger than that automatically obtained by NegBagg and NegBoost. This raises
a question as to whether the proposed algorithms scale well
for large real-world problems. Since NegBagg and NegBoost
used a nonevolutionary constructive approach for determining
ensemble architectures, the architecture determination process
of these algorithms might trap into the local optima. However,
this is not an inherent problem because NegBagg and NegBoost
can easily be modiﬁed to accommodate any kind of evolutionary or nonevolutionary approach (e.g., , – ) for
determining ensemble architectures. One of our future research
projects will be to study algorithms and techniques suitable for
designing ensembles for large problems.
The use of the constructive approach in this paper
gives us an opportunity for making a direct comparison with
our previous work CNNE . The effect of using negative
correlation in conjunction with bagging or boosting is therefore
clearly understood. It is straightforward for an inexperienced
user to specify the initial conditions for the constructive approach , , . Furthermore, the constructive algorithm
is computationally more efﬁcient because it always searches
small solutions ﬁrst – , so an algorithm can be tested
on several problems within a reasonable amount of time.
V. CONCLUSION
Bagging, boosting, and negative correlation learning are
popular approaches to train NN ensembles. Bagging , ,
 and boosting , rely on the construction of different
training sets for different NNs in an ensemble to encourage
error decorrelation among NNs. Negative correlation learning
 , , , incorporates a penalty term in NN’s error
function to explicitly promote an error negative correlation.
In this paper, we have proposed and studied two algorithms
(i.e., NegBagg and NegBoost) that combine the complementary
strengths of both negative correlation and bagging/boosting.
Our experimental results on nine different problems have shown
that NegBagg and NegBoost are able to produce compact
ensembles that generalize better than either negative correlation
or bagging/boosting by itself.
Our algorithms NegBagg and NegBoost also have the added
advantage that the algorithms can automatically determine individual NN architectures during ensemble learning. The number
of epochs needed for training each individual NN can also be
automatically determined. Furthermore, NegBoost is able to
automatically determine the number of individual NNs in an
ensemble, relieving the user from a potentially tedious trialand-error process of manually selecting an optimal number of
NNs in the ensemble.
There are several future research directions that logically
follow this study. First, an automatic technique for deciding the
number of NNs in an ensemble for NegBagg will be very useful
in improving NegBagg’s performance and its practical use.
Second, a more in-depth analysis of NegBagg and NegBoost is
needed to gain more insights into when NegBagg and NegBoost
are most likely to perform well and for what kind of problems.
Third, while NegBagg has concentrated on the classiﬁcation
problems, it would be interesting to study how well NegBagg
would perform on regression problems.
ACKNOWLEDGMENT
The authors would like to thank the associate editor and
anonymous reviewers for their constructive comments.
ISLAM et al.: BAGGING AND BOOSTING NEGATIVELY CORRELATED NEURAL NETWORKS