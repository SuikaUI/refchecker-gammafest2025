Can AI-Generated Text be Reliably Detected?
Vinu Sankar Sadasivan
 
Department of Computer Science, University of Maryland
Aounon Kumar
 
Department of Computer Science, Harvard University
Sriram Balasubramanian
 
Department of Computer Science, University of Maryland
Wenxiao Wang
 
Department of Computer Science, University of Maryland
Soheil Feizi
 
Department of Computer Science, University of Maryland
Large Language Models (LLMs) can perform impressively well in various applications, such
as document completion and question-answering. However, the potential for misuse of these
models in activities such as plagiarism, generating fake news, and spamming has raised
concerns about their responsible use. Consequently, the reliable detection of AI-generated text
has become a critical area of research. Recent works have attempted to address this challenge
through various methods, including the identification of model signatures in generated
text outputs and the application of watermarking techniques to detect AI-generated text.
These detectors have shown to be effective under their specific settings. In this paper, we
stress-test the robustness of these AI text detectors in the presence of an attacker. We
introduce recursive paraphrasing attack to stress test a wide range of detection schemes,
including the ones using the watermarking as well as neural network-based detectors, zeroshot classifiers, and retrieval-based detectors. Our experiments conducted on passages, each
approximately 300 tokens long, reveal the varying sensitivities of these detectors to our
attacks. We also observe that these paraphrasing attacks add slight degradation to the text
quality. We analyze the trade-offs between our attack strength and the resulting text quality,
measured through human studies, perplexity scores, and accuracy on text benchmarks. Our
findings indicate that while our recursive paraphrasing method can significantly reduce
detection rates, it only slightly degrades text quality in many cases, highlighting potential
vulnerabilities in current detection systems in the presence of an attacker. Additionally, we
investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying
human-written text as AI-generated. We demonstrate that an attacker can infer hidden AI
text signatures without white-box access to the detection method, potentially leading to
reputational risks for LLM developers. Finally, we provide a theoretical framework connecting
the AUROC of the best possible detector to the Total Variation distance between human
and AI text distributions. This analysis offers insights into the fundamental challenges of
reliable detection as language models continue to advance. Our code is publicly available at
 
Introduction
Artificial Intelligence (AI) has made tremendous advances in recent years, from generative models in computer
vision to generative models in natural language processing
 
Figure 1: An illustration of vulnerabilities of existing AI-text detectors. We consider both watermarking-based
and non-watermarking-based detectors and show that they are not reliable in practical scenarios. Colored
arrow paths show the potential pipelines for adversaries to avoid detection. In red: an attacker can use
a paraphraser to remove the LLM signatures from an AI-generated text to avoid detection. In blue: an
adversary can query the watermarked LLM multiple times to learn its watermarking scheme. This information
can be used to spoof the watermark detector.
(NLP) . Large Language Models (LLMs) can now
generate texts of supreme quality with the potential in many applications. For example, the recent model of
ChatGPT can generate human-like texts for various tasks such as writing codes for computer
programs, lyrics for songs, completing documents, and question answering; its applications are endless. The
trend in NLP shows that these LLMs will even get better with time. However, this comes with a significant
challenge in terms of authenticity and regulations. AI tools have the potential to be misused by users for
unethical purposes such as plagiarism, generating fake news, spamming, generating fake product reviews, and
manipulating web content for social engineering in ways that can have negative impacts on society . Some news articles rewritten by AI have led to many fundamental errors in them
 . Hence, there is a need to ensure the responsible use of these generative AI tools. In order
to aid this, a lot of recent research focuses on detecting AI-generated texts.
Recent works propose several ways, such as using trained neural network-based detectors, zero-shot detectors,
watermarking, and retrieval-based detectors for flagging AI-generated text. These detectors, especially
watermarking, have shown to be effective in various settings. Neural network-based detectors approach the
detection problem as a binary classification task . For example, OpenAI fine-tunes RoBERTa-based
 GPT-2 detector models to distinguish between non-AI generated and GPT-2 generated
texts . This requires such a detector to be fine-tuned with supervision on each newly released
LLM for reliable detection. Zero-shot detectors address this downside of trained detectors by performing
the detection task without any additional training overhead . These works evaluate the expected per-token log probability of texts and perform
thresholding to detect AI-generated texts. Mitchell et al. observe that AI-generated passages tend to
lie in negative curvature of log probability of texts. To leverage this observation, they propose DetectGPT, a
zero-shot LLM text detection method. However, zero-shot detectors require access to the original model used
to generate the AI text to achieve the best performance. Additionally, neural network-based and zero-shot
detectors rely on a deep net for their detection, and they can be vulnerable to adversarial and poisoning
attacks . In comparison
to these methods, watermarking significantly eases the detection of AI-generated text by imprinting specific
patterns on them that are imperceptible to humans . Soft watermarking proposed in
Kirchenbauer et al. partitions tokens into “green” and “red” lists, as they define, to help create these
patterns. A watermarked LLM samples a token, with high probability, from the green list determined by a
pseudo-random generator seeded by its prefix token. The watermarking detector would classify a passage
with a large number of tokens from the green list as AI-generated. The soft watermarking approach of
Kirchenbauer et al. has been shown to be effective in various settings and remains one of the popular
approaches for detecting AI-generated text. For example, their watermarking scheme can achieve a high true
positive rate of 99.8% at a false positive rate of 1% on a task for classifying AI text against human-written
news articles. However, for watermarking to be an effective tool for preventing AI misuse, it has to be enforced
on all the major LLM generators. Otherwise, an adversary could use a non-watermarking LLM for their
purposes. Krishna et al. introduces an information retrieval-based detector by storing the outputs of
the LLM in a database. For a candidate passage, their algorithm searches this database for semantically
similar matches for detection. However, storing user-LLM conversations might cause serious privacy concerns.
Several recent news show that some of
these popular AI-text detectors fail in practical settings. In this paper, through several experiments, we
stress-test state-of-the-art AI-text detectors to evaluate their robustness in the presence of an attacker . In §2, we have developed
a recursive paraphrasing attack that use neural network-based paraphrasing to recursively paraphrase the
source LLM’s output text. We perform experiments with our automated recursive paraphrasing to show the
sensitivity of a range of AI text detectors to type-II error (detecting AI text as human text). For instance,
recursive paraphrasing attack on watermarked texts, even over relatively long passages of 300
tokens in length, can drop the detection rate (true positive rate at 1% false positive rate or
TPR@1%FPR) from 99.3% to 9.7%. We find that our attack can add slight degradations to the AI text
quality. Hence, we analyze the trade-offs between our attack and the resulting text quality, measured through
human studies, perplexity scores, and accuracy of text benchmarks. Our attack differs from the relatively
weaker attack from Kirchenbauer et al. where they perform span replacement by replacing random
tokens (in-place) using an LLM. Thus, our experiments show the sensitivity of the watermarking scheme
against paraphrasing attacks in the presence of a stronger attacker.
Zhang et al. and Lu et al. 
are also substitution-based attacks. Zhang et al. have a different attack objective when compared to
our attack. Their attack is performed to maintain the quality of the text alone and not semantic similarity.
Hence, their attack can change the original content. Lu et al. employs in-context optimization through
iterative generation of word- or sentence-level substitutions using a proxy AI text detector to guide the
substitutions. This positions their attack as an adversarial algorithm for evading text detection. In contrast,
our approach focuses on non-adversarial iterative or recursive text paraphrasing attacks.
After paraphrasing, the area under the receiver operating characteristic (AUROC) curves of zero-shot
detectors drops from 96.5% to 25.2%. We also observe that the performance of neural
network-based trained detectors deteriorates significantly after our paraphrasing attack. For
instance, the TPR@1%FPR of the RoBERTa-Large-Detector from OpenAI drops from 100% to 60% after
paraphrasing. In addition, we show that the retrieval-based detector by Krishna et al. designed to
evade paraphrase attacks is vulnerable to our recursive paraphrasing. In fact, the accuracy of their detector
falls from 100% to below 60% with our recursive paraphrase attack.
To quantify the drop in text quality after recursive paraphrasing, we perform MTurk human evaluation
studies and measure other automated metrics such as perplexity and text benchmark accuracy. Our human
evaluation study shows that 77% of the recursively paraphrased passages are rated high quality
in terms of content preservation, and 89% of them are rated high quality in terms of grammar or
text quality. We also show that our recursive paraphrasing, when applied to a text benchmark such
as a question-answering dataset, does not affect the performance, providing additional evidence
that recursive paraphrasing does not hurt the content of the original text. Though an attacker may further
improve the text quality with manual interventions, paraphrasing attacks can be sufficient for an adversary
to perform social engineering tasks such as spamming, phishing, or spreading propaganda.
Moreover, we show the possibility of spoofing attacks on various AI text detectors in §3. In this setting, an
attacker generates a non-AI text that is detected to be AI-generated, thus adversarially increasing type-I error
(falsely detecting human text as AI text). An adversary can potentially launch spoofing attacks to produce
derogatory texts that are detected to be AI-generated to affect the reputation of the target LLM’s developers.
In particular, we show that an adversary can infer hidden AI text signatures without having white-box
access to the detection method. For example, though the pseudo-random generator used for generating
watermarked text is private, we develop an attack that adaptively queries the target LLM multiple times
to learn its watermarking scheme. An adversarial human can then use this information to compose texts
that are detected to be watermarked. Figure 1 shows an illustration of some of the vulnerabilities of the
existing AI-text detectors. Gu et al. build upon our spoofing attacks by employing watermarked data
distillation to train a student model to replicate the next-token distribution.
Finally, in §4, we present a theoretical result regarding the hardness of AI-text detection. Our main result in
Theorem 1 states that the AUROC of the best possible detector differentiating two distributions H (e.g.,
human text) and M (e.g., AI-generated text) reduces as the total variation distance TV(M, H) between
them decreases. Note that this result is true for any two arbitrary distributions H and M. For example,
H could be the text distribution for a person or group and M could be the output text distribution of a
general LLM or an LLM trained by an adversary to mimic the text of a particular set of people. Essentially,
adversaries can train LLMs to mimic human text as they get more sophisticated, potentially reducing the TV
distance between human and AI text, leading to an increasingly more difficult detection problem according
to our Theorem 1. Although estimating the exact TV between text distributions from a finite set of samples
is a challenging problem, we provide some empirical evidence, over simulated data or via TV estimations,
showing that more advanced LLMs can potentially lead to smaller TV distances. Thus, our Theorem 1
would indicate an increasingly more difficult reliable detection problem in such cases. Our theory
also indicates that if a detector becomes more robust to type-I errors, type-II errors will increase, revealing a
fundamental tradeoff between type-I and type-II errors for the AI text detection problem. Similar tradeoffs
have been explored in other domains as well. For example, Khajavi & Kuh study the relationship
between detection performance and KL divergence between input distributions in the context of covariance
selection. Thapliyal & Hwang show that undetectable cyberattacks can be generated by mimicking
the input-output data distribution of network control systems. Although not a surprising result, Theorem 1
is the first to link this tradeoff to the detection of AI-generated content to our knowledge.
Identifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes such as
plagiarism, generating fake news, and spamming. However, blindly relying on these detectors may not be the
right solution to tackle this issue since it can cause its own damages, such as falsely accusing a human of
plagiarism. Our results highlight the sensitivities of a wide range of detectors to both evasion and spoofing
attacks and indicate the difficulty of developing reliable detectors in the presence of an attacker. We hope to
reveal the sensitivity of AI text detectors to various attacks in our stress tests experiments.
In summary, we make the following contributions in this work.
• Our work is the first to comprehensively analyze the robustness of four different classes of detectors,
including watermarking-based, neural network-based, zero-shot, and retrieval-based detectors, and
stress-test them in the presence of an attacker (in §2). In particular, the recursive paraphrasing
attack that we develop is the first method that can break watermarking 
and retrieval-based detectors. We also provide experiments to analyze the
resulting text quality after our attack to find that recursive paraphrasing only leads to a slight text
quality degradation in many cases.
• Our work is the first to show that existing detectors are vulnerable against spoofing attacks where an
adversarial human aims to write a (potentially derogatory) passage falsely detected as AI-generated
without having a white-box access to the detection methods (in §3). For instance, as proof of concept,
we show that an adversary can infer the watermarking signatures by probing the watermarked LLM
and analyzing the statistics of the generated tokens.
preservation
Avg. rating
Ratings 5&4
Grammar or
text quality
Avg. rating
4.28 ± 0.67
4.12 ± 0.50
4.12 ± 0.53
4.11 ± 0.64
4.07 ± 0.53
4.14 ± 0.58
Ratings 5&4
Table 1: Summary of the MTurk human evaluation study on content preservation and grammar or text
quality of the recursive paraphrases with DIPPER that we use for our attacks. Ratings are on a Likert scale
of 1 to 5. See Appendix B.1 for details.
preservation
Avg. rating
4.37 ± 0.63
4.18 ± 0.67
3.93 ± 0.71
3.9 ± 0.75
3.85 ± 0.78
4.05 ± 0.2
Ratings 5&4
Grammar or
text quality
Avg. rating
4.62 ± 0.55
4.28 ± 0.73
4.26 ± 0.65
4.22 ± 0.64
4.17 ± 0.74
4.31 ± 0.35
Ratings 5&4
Table 2: Summary of the MTurk human evaluation study of the recursive paraphrases with LLaMA-2-7B-Chat.
• Our work is the first to establish a theoretical connection between the AUROC of the best possible
detector and the TV distance between human and AI-text distributions that can be used to study
the hardness of the reliable text detection problem (in §4). Our theory also reveals a fundamental
tradeoff between type-I and type-II errors for the AI text detection problem.
Evading AI-Detectors using Paraphrasing Attacks
In this section, we first present the experimental setup for our paraphrasing attacks in §2.1. We also provide
experiments in §2.2 to study the trade-off between evasion success and text quality after the attack. §2.3
and §2.4 show the effect of the paraphrasing attacks on watermarking and non-watermarking detectors,
respectively. In Appendix A.1, we provide attack experiments with Llama-2-13B as the target model on
additional detectors.
Attack Setup and Paraphrasing Methods
For evasion attacks, we consider a scenario where an adversary takes an AI response generated by a target
model and then modifies the AI response in an automated and scalable fashion to evade detection. In this work,
we propose the adversary modify the AI text from model L using an AI paraphraser P to evade detection.
Figure 2: Recursive paraphrasing
Note that the adversary might be incentivized to use a detectable
model L (say, watermarked) if L is powerful or might have been
fine-tuned for specific applications. In these cases where L could
answer a user prompt better, an adversary would still prefer to use the
watermarked L to generate a response and then use a less detectable
AI model P for paraphrasing to evade detection. We quantify the
text quality using automated metrics such as perplexity and human
studies. As shown in §2.2, our evasion attacks only lead to a slight
degradation in text quality while successfully evading detectors most
of the time. Note that the amount of degradation that can be tolerated
is application-specific. For example, an adversary could tolerate more
quality degradation when generating a social media post than when
generating a news article.
We use the “document” features of the XSum dataset containing 1000 long news articles
(∼300 tokens in length) for our experiments. In Appendix A, we perform experiments with additional datasets
– a medical text dataset PubMedQA and a dataset with articles from 10 different domains
Kafkai . As target LLMs, we use OPT-1.3B and OPT-13B language models
with 1.3B and 13B parameters, respectively. In Appendix A, we also evaluate our attacks with GPT-2 Medium
 as the target model. We use three different neural network-based paraphrasers – DIPPER
with 11B parameters , LLaMA-2-7B-Chat with 7B parameters ,
and T5-based paraphraser with 222M parameters. Suppose a passage S = (s1, s2, ..., sn)
where si is the ith sentence. DIPPER and LLaMA-2-7B-Chat paraphrase S to be S′ = fstrong(S) in one-shot
while the light-weight T5-based paraphraser would output S′ = (fweak(s1), fweak(s2), ..., fweak(sn)) where
they can only paraphrase sentence-by-sentence. DIPPER and LLaMA-2-7B-Chat also have the ability to
input a context prompt text C to generate higher-quality paraphrasing S′ = fstrong(S, C). We can also vary
two different hyperparameters of DIPPER to generate a diverse number of paraphrases for a single input
Paraphraser
Evaluation
Mean perplexity
QA performance
LLaMA-2-7B-Chat
Mean perplexity
QA Performance
Table 3: Automated evaluation of the text quality of recursive
paraphrases using perplexity measures with respect to OPT-13B
and question-answering benchmark accuracy.
We use DIPPER and LLaMA-2-7B-Chat
for recursive paraphrasing attacks since
they provide high-quality paraphrasing
when compared to the 222M parameter T5 model. Let an LLM L generate
AI text output S = L(C) for an input
DIPPER or LLaMA-2-7B-
Chat can be used to generate a paraphrase pp1(S) = fstrong(S, C).
paraphrasing can be performed in recursion (see Figure 2). That is, pp2(S) = fstrong(pp1(S), C) and so on.
While DIPPER is explicitly trained to be a paraphraser, LLaMA-2-7B-Chat is an instruction-tuned model
for chat purposes. We design a system prompt (see Appendix B.2) with the LLaMA-2-7B-Chat model to use
it as a paraphraser. In §2.3 and §2.4, we show that recursive paraphrasing is effective in evading the strong
watermark and retrieval-based detectors when compared to a single round of paraphrasing. Using human
and other automated evaluation techniques in §2.2, we show that our recursive paraphrasing method only
degrades the text quality slightly most of the time.
Quality of the Paraphrases
In order to reliably study the quality of the recursive paraphrases we use in our experiments using DIPPER
and LLaMA-2-7B-Chat, we perform human evaluations using MTurk and other automated techniques. The
AI-text used in this study is generated using a watermarked OPT-13B model. Tables 1 and 2 provide a
summary of the study. We investigate the content preservation and text quality or grammar of the recursive
paraphrases with respect to the AI-generated texts (see Tables 7-10 in Appendix B.1 for more details). In
terms of content preservation with DIPPER, 70% of the paraphrases were rated high quality
and 23% somewhat equivalent. In terms of text quality or grammar, 89% of the paraphrases
were rated high quality. On a Likert scale of 1 to 5, the DIPPER paraphrases that we use received an
average rating of 4.14 ± 0.58 for text quality or grammar and 4.0 ± 0.9 for content preservation. Similarly,
83% of the recursive paraphrases we obtain with LLaMA-2-7B-Chat were rated high quality.
See Appendix B.1 for more details on the human study.
For automated text quality evaluations, we use perplexity measures and a question-answering (QA) benchmark
in Table 3. We measure the perplexity scores using OPT-13B. As shown in the table, the perplexity scores
degrade from 5.5 to 8.7 and 10.5, respectively, for DIPPER and LLaMA-2-7B-Chat after 5 rounds of
paraphrasing. We also use a QA benchmark SQuAD-v2 to evaluate the effect of
recursive paraphrasing. For this, we use two hundred data points from SQuAD-v2, which have context text
length of at least 300 tokens. The length of context text passages we use in the study is 328 ± 28 tokens.
Each data point consists of a context passage, a question, and an answer. We evaluate a QA model on the
SquAD-v2 benchmark to observe that it achieves 97% accuracy in the QA task. For the QA model, we use the
LLaMA-2-13B-Chat model with a carefully written system prompt (see Appendix B.2). To evaluate the quality
of paraphrases, we paraphrase the context passages recursively and use these to evaluate the QA accuracy with
the QA model. If the QA model can answer the question correctly based on the paraphrased context, then the
information is preserved in the paraphrase. As we observe, the QA performance with recursive paraphrasing
(a) Recursive paraphrasing with DIPPER
(b) Recursive paraphrasing with LLaMA-2-7B-Chat
Figure 3: ROC plots for soft watermarking with recursive paraphrasing attacks. AUROC, TPR@1%FPR,
and perplexity scores measured using OPT-13B are given in the legend. The target LLM OPT-13B is used to
generate watermarked output that are 300 tokens in length.
is similar to that with the clean context passage. These results substantiate that AI text detectors can be
effectively attacked using recursive paraphrasing with only a slight degradation in text quality.
We note that the amount of acceptable quality degradation can be application-specific. For example, an
adversary might be okay with a higher quality drop when writing a social media post than when writing a
fake news article. Our human studies rate our recursive paraphrases to have a score of either 5 or 4 over
70% of the time. Though this might be acceptable for some adversaries, others might not tolerate a score
of 4 for their applications. Since a score of 4 denotes minor degradation, we presume that the adversaries
could manually edit them for their attacks. Nevertheless, our paraphrases get a perfect score 35% of the time,
indicating that it is still practical for adversaries to use it to perform their attacks successfully. However,
we believe this tradeoff in text quality degradation and detection evasion would diminish as paraphrasers
improve in the future.
Paraphrasing Attacks on Watermarked AI Text
In this section, we evaluate our recursive paraphrasing attacks on the soft watermarking scheme proposed in
Kirchenbauer et al. . Soft watermarking encourages LLMs to output token s(t) at time-step t that
belongs to a “green list”. The green list for s(t) is created using a private pseudo-random generator that is
seeded with the prior token s(t−1). A watermarked output from the LLM is designed to have tokens that are
majorly selected from the green list. Hence, a watermark detector with the pseudo-random generator checks
the number of green tokens in a candidate passage to detect whether it is watermarked or not. Here, we
target watermarked OPT-13B with 13B parameters in Figure 3 and watermarked OPT-1.3B in Figure 4 for
our experiments. In Appendix A.2, we also evaluate our attacks on GPT-2 Medium 
and other datasets – PubMedQA and Kafkai .
Dataset. We perform our experiments on 2000 text passages that are around 300 tokens in length (1000
passages per human and AI text classes). We pick 1000 long news articles from the XSum “document” feature.
For each article, the first ∼300 tokens are input to the target OPT-1.3B to generate 1000 watermarked AI
text passages that are each ∼300 tokens in length. The second 300 tokens from the 1000 news articles in
the dataset are treated as baseline human text. We note that our considered dataset has more and longer
passages compared to the experiments in Kirchenbauer et al. .
Detection results after paraphrasing attack. Weaker paraphrasing attacks discussed in Kirchenbauer
et al. are not effective in removing watermarks. They perform “span replacement” by replacing
random tokens (in-place) using a language model. However, after a single round of paraphrasing (pp1) with
(a) Watermarked text with mean token length 300
(b) Watermarked text with varying token lengths
Figure 4: ROC plots for soft watermarking with recursive paraphrasing attacks. AUROC, TPR@1%FPR,
and perplexity scores measured using OPT-13B are given in the legend. The target LLM is OPT-1.3B.
(a) Even for 300 tokens long watermarked passages, recursive paraphrasing is effective. As paraphrasing
rounds proceed, detection rates degrade significantly with a slight trade-off in text quality. (b) Attacking
watermarked passages become easier as their length reduces.
random (0.01, 0.01, 0.01)
Figure 5: ROC curves for various trained and zero-shot detectors. Left: Without attack. Middle: After
paraphrasing attack using T5-based paraphraser. The performance of zero-shot detectors drops significantly.
Right: Here, we assume we can query the detector ten times for the paraphrasing attack. We generate
ten paraphrasings for each passage and query multiple times to evade detection. Notice how all detectors
have low TPR@1%FPR. In the plot legend – perturbation refers to the zero-shot methods in Mitchell et al.
 ; threshold refers to the zero-shot methods in Solaiman et al. ; Gehrmann et al. ; Ippolito
et al. ; roberta refers to OpenAI’s trained detectors . The TPR@1%FPR scores of
different detectors before the attack, after the attack, and after the attack with multiple queries, respectively,
are provided in the plot legend.
a watermarked OPT-13B as the target LLM, TPR@1%FPR of watermark detector degrades from 99.8% to
80.7% and 54.6%, respectively, with DIPPER and LLaMA-2-7B-Chat paraphrasers. Though watermarking is
a worthwhile endeavor to prevent AI plagiarism, with our stress test, we show that an adversary can find their
way to evade detection via paraphrasing. As shown in Figures 3-4, the recursive paraphrase attack further
degrades the detection rate of the detector to below 20% after 5 rounds of paraphrasing (pp5). Note that in all
the settings pp2 or 2 rounds of paraphrasing is sufficient to degrade TPR@1%FPR to below 50%. As shown in
Figure 3, DIPPER shows a clearer and more consistent trend in improving attack performance over recursions
of paraphrasing in comparison to LLaMA-2. This is because DIPPER is trained explicitly to be a paraphraser
with hyperparameters that can control the quality of paraphrasing. Therefore, we mainly employ DIPPER for
our recursive paraphrase attacks. Best of ppi in the figure refers to the method where, for each passage, we
select the paraphrase out of all the ppi’s that has the worst detector score. For Best of ppi with OPT-1.3B,
the detection rate reduces drastically from 99.8% to 4.0% with a trade-off of 1.5 in the perplexity score
(Figure 4a). Best of ppi, unlike the ppi attacks, assume black box query access to the detector. Figure
4b shows that the watermarking detector becomes weaker as the length of the watermarked text reduces.
Note that for watermarked texts that are 50 or 100 tokens long, the detection performance after the recursive
paraphrasing attack is similar to that of a random detector. As the plot indicates, watermarking could be
more reliable for preventing AI plagiarism in tasks that require longer texts. However, this does not guarantee
that watermarking will be a foolproof defense in such settings. This requires more investigation, and we leave
this for future work. We provide examples of paraphrased text that we use for our attacks in Appendix B.3.
Paraphrasing Attacks on Non-Watermarked AI Text
Neural network-based trained detectors such as RoBERTa-Large-Detector from OpenAI are
trained or fine-tuned for binary classification with datasets containing human and AI-generated texts. Zero-shot
classifiers leverage specific statistical properties of the source LLM outputs for their detection. Retrieval-based
methods search for a candidate passage in a database that stores the LLM outputs. Here, we perform
experiments on these non-watermarking detectors to show they are vulnerable to our paraphrasing attack.
Trained and Zero-shot detectors.
We use a pre-trained GPT-2 Medium model with 355M parameters as the target LLM to evaluate our attack on 1000 long passages from the
XSum dataset . We use the T5-based paraphrasing model with
222M parameters to rephrase the 1000 output texts generated using the target GPT-2 Medium model.
Figure 6: Recursive paraphrasing breaks the retrievalbased detector with only slight
degradation in text quality. ppi refers to i recursion(s)
of paraphrasing. Numbers next to markers denote the
perplexity scores of the paraphraser output.
Figure 5 shows the effectiveness of the paraphrasing
attack over these detectors. The AUROC scores
of DetectGPT drop
from 96.5% (before the attack) to 59.8% (after
the attack). Note that AUROC of 50% corresponds
to a random detector.
The rest of the zero-shot
detectors also perform poorly after
our attack. Though the performance of the trained
neural network-based detectors is
better than that of zero-shot detectors, they are also
not reliable. For example, TPR@1%FPR of OpenAI’s RoBERTa-Large-Detector drops from 100% to
around 92% after our attack.
In another setting, we assume the attacker may have
multiple access to the detector. That is, the attacker
can query the detector with an input AI text passage,
and the detector would reveal the detection score
to the attacker. For this scenario, we generate ten
different paraphrases for an input passage and query
the detector for the detection scores. For each AI text passage, we then select the paraphrase with the worst
detection score for evaluating the ROC curves. As shown in Figure 5, with multiple queries to the
detector, an adversary can paraphrase more efficiently to bring down TPR@1%FPR of the
RoBERTa-Large-Detector from 100% to 80%. In Appendix A.3, we show more experiments with more
datasets and target LLMs.
As seen in the results, the detection of the entropy threshold detector improves with paraphrasing. LLMs are
trained on human-written texts, and for this reason, they might have low entropy scores on human-written
samples we use in our experiments due to memorization. Therefore, the entropy detector might have poor
detection scores before the paraphrasing attack. However, after paraphrasing with a different AI model, the
entropy scores for these human-written samples might increase, improving the detection scores. Despite this,
the entropy threshold detector has poor detection rates before and after the attack.
Though the performance of performance of trained detectors degrades after each round of paraphrasing,
they seem to be more robust to paraphrase attacks than the other detectors we study. We hypothesize that
this might be due to these detectors being trained on human-written samples we use for our study. For
example, the MAGE dataset includes passages from the XSum dataset we use. Gameiro
et al. argues that while trained detectors can generalize better to unseen LLMs, they may overfit to
this training distribution of human text. They also show that some of these detectors fail to generalize to
out-of-distribution human-written text. This is an aspect that we do not consider in our work, but would
still make these detectors unreliable for real-world applications.
Retrieval-based detectors. Detector in Krishna et al. is designed to be robust against paraphrase
attacks. However, we show that they can suffer from the recursive paraphrase attacks that we develop
using DIPPER. We use 2000 passages (1000 generated by OPT-1.3B and 1000 human passages) from the
XSum dataset. AI outputs are stored in the AI database by the detector. As shown in Figure 6, this
detector detects almost all of the AI outputs even after a round of paraphrasing. However, the detection
accuracy drops below ∼60% after five rounds of recursive paraphrasing. As marked in the plot,
the perplexity score of the paraphrased text only degrades by 1.7 at a detection accuracy of ∼60%. Moreover,
retrieval-based detectors are concerning since they might lead to serious privacy issues from storing users’
LLM conversations. In Appendix A.4, we show more experiments with more datasets and target LLMs.
Spoofing Attacks on Generative AI-text Models
An AI language detector without a low type-I error can cause harm as it might wrongly accuse a human
of plagiarizing using an LLM. Moreover, an attacker (adversarial human) can generate a non-AI text to be
detected as AI-generated. This is called the spoofing attack. An adversary can potentially launch spoofing
attacks to produce derogatory texts to damage the reputation of the target LLM’s developers. In this section,
as a proof-of-concept, we show that current text detectors can be spoofed to detect texts composed by
adversarial humans as AI-generated. More details on the spoofing experiments are presented in Appendix D.
Soft watermarking. As discussed in §2, soft watermarked LLMs generate
tokens from the “green list” that are determined by a pseudo-random generator seeded by the prefix token.
Though the pseudo-random generator is private, an attacker can estimate the green lists by observing
multiple token pairs in the watermarked texts from the target LLM. An adversarial human can then leverage
the estimated green lists to compose texts by themselves that are detected to be watermarked. In our
experiments, we estimate the green lists for 181 most commonly used words in the English vocabulary. We
query the target watermarked OPT-1.3B model one million times to observe the token pair distributions
within this smaller vocabulary subset we select. Note that this attack on a watermarked model only needs
to be performed once to learn the watermarking pattern or the proxy green list to spoof it thereafter.
Figure 7: ROC curve of a soft watermarkingbased detector 
after our spoofing attack.
Based on the frequency of tokens that follow a prefix token
in the observed generative outputs, we estimate green lists for
each of the 181 common words. We build a tool that helps
adversarial humans create watermarked sentences by providing
them with the proxy green list that we learn with only access to a
watermarked text corpora obtained from the target watermarked
LLM. We observe that the soft watermarking scheme can
be spoofed to degrade its detection AUROC from 99.8%
to 1.3% (see Figure 7).
Retrieval-based detectors.
Krishna et al. use a
database to store LLM outputs to detect AI-text by retrieval.
We find in our experiments (see Figure 13) that an adversary
can spoof this detector 100% of the time, even if the
detector maintains a private database. Suppose an adversary, say a teacher, has access to a human written document S, say a student’s essay. The adversary
can prompt the target LLM to paraphrase S to get S′. This results in the LLM, by design, storing its
output S′ in its private database for detection purposes. Now, the detector would classify the original human
text S as AI-generated since a semantically similar copy S′ is present in its database. In this manner, a
teacher can purposefully allege an innocent student to have plagiarised using the retrieval-based detector.
Note that manipulating retrieval-based detectors is easier using this approach compared to watermarking
techniques. This observation implies a practical tradeoff between type-I and type-II errors. When a detector
is strengthened against type-II errors, it tends to result in a deterioration of its performance in terms of
type-I errors.
Zero-shot and neural network-based detectors. In this setting, a malicious adversary could write a
short text in a collaborative work, which may lead to the entire text being classified as AI-generated. To
simulate this, we prepend a human-written text marked as AI-generated by the detector to all the other
human-generated text for spoofing. In other words, from 200 long passages in the XSum dataset, we pick the
human text with the worst detection score for each detector considered in §2.4. We then prepend this text to
all the other human texts, ensuring that the length of the prepended text does not exceed the length of the
original text. Our experiments show that the AUROC of all these detectors drops after spoofing (see
plots in Appendix D). After this naïve spoofing attack, the TPR@1%FPR of most of these detectors drop
significantly.
Hardness of Reliable AI Text Detection
In this section, we formally upper bound the AUROC of an arbitrary detector in terms of the TV between
the distributions for M (e.g., AI text) and H (e.g., human text) over the set of all possible text sequences Ω.
We note that this result holds for any two arbitrary distributions H and M. For example, H could be the
text distribution for a person or group, while M could be the output text distribution of a general LLM or
an LLM trained by an adversary to mimic the text of a particular set of people.
We use TV(M, H) to denote the TV between these two distributions and model a detector as a function
D : Ω→R that maps every sequence in Ωto a real number. Sequences are classified into AI-generated or
human-generated by applying a threshold γ on this number. By adjusting the parameter γ, we can tune the
sensitivity of the detector to AI and human-generated texts to obtain an ROC curve.
Theorem 1. The area under the ROC of any detector D is bounded as
AUROC(D) ≤1
2 + TV(M, H) −TV(M, H)2
Figure 8: Comparing the performance, in
terms of AUROC, of the best possible detector to that of the baseline performance
corresponding to a random classifier.
The proof is deferred to Appendix C.1. Figure 8 shows how
the above bound grows as a function of the TV distance. This
theorem states that as the TV distance between AI and human
text distributions reduces, the AUROC of the best possible
detector decreases. Based on our theory, an adversary can
use advanced LLMs to mimic human text to reduce the TV
distance between human and AI text distributions to evade text
detection systems.
For a detector to have a good performance (say, AUROC > 0.9),
the distributions of human and AI-generated texts must be very
different from each other (TV > 0.5 based on the figure). As
M gets more similar to H (say, TV < 0.2), the performance
of even the best-possible detector becomes unreliable (AUROC
< 0.7). For some applications, say AI-text plagiarism, reliable
detection should have a low false positive rate (say, < 0.01)
and a high true positive rate (say, > 0.9). Based on our theory,
this cannot be achieved even when the overlap between the
distributions is relatively low, say 11% (or TV = 0.9 −0.01 = 0.89, based on equation 1 in Appendix C.1).
Note that, for a watermarked model, the above bound can be close to one as the TV between the watermarked
distribution and human-generated distribution can be high. Corollary 1 in Appendix C.2 discusses how
Figure 9: Increasing model size reduces the exact
TV between the true synthetic data distribution
and the learned distribution. Error bars report
standard deviations after 5 independent trials.
Figure 10: Estimated TV distances of GPT-
2 output datasets from the WebText dataset
using meta-token sequences of varying lengths.
TV decreases with model size for each length.
paraphrasing attacks can be effective in evading watermarks using Theorem 1. In Appendix C.3, we also
present a tightness analysis of the bound in Theorem 1, where we show that for any distribution H there
exists M and a detector D for which the bound holds with equality. We also discuss general trade-offs
between true positive and false positive rates of detection in Corollaries 2 and 3 in Appendix C.2. Theorem 2
in Appendix C.4 extends Theorem 1 to bound the AUROC of the best possible detector by a function of the
TV distance between LLM outputs generated using pseudorandomness and human text distributions.
In studying the hardness of the detection problem, we consider the following assumption that for a given
human-text distribution H, more advanced LLMs mimicking H can lead to smaller TV. Thus, using Theorem
1, the detection problem becomes increasingly more difficult. This is the core argument of our hardness result
on AI text detection. Although the underlying assumption seems to be intuitive given the capabilities of LLMs
such as GPT-4 , a precise analysis of this assumption is quite difficult because estimating the
true TV of the text distributions from a finite set of samples is extremely challenging. Nevertheless, we provide
some empirical evidence supporting this assumption using two sets of experiments. In all the experiments,
we consistently observe that the TV distance estimates between human and AI text distributions reduce
as language models get more advanced, indicating the increasing difficulty associated with AI text detection.
(i) Using synthetic text data. We perform experiments on a toy synthetic text dataset where the exact
TV distance can be calculated. We use the Markov assumption to generate the synthetic text data with
sequence length 3 using a randomly generated token transition matrix for varying vocabulary sizes. We
use single-layer LSTMs of different hidden unit sizes to train on a dataset of size 20,000 sampled from this
synthetic data distribution using a default AdamW optimizer . We compute
the learned token transition matrix for the LSTM output distribution using the softmax logit values of the
trained model. Using transition matrices of both distributions, we compute the exact TV. Figure 9 shows
that the exact TV distances between the learned and true synthetic distributions reduce as the LSTM model
size increases.
(ii) Using projection. For discrete distributions, the TV distance can be computed as 1/2 of the sum of the
point-wise differences between their probability density functions (PDFs). While this is mathematically simple
since texts can be considered as token sequences with bounded length, it is not practical to compute true TV
distances directly through estimating PDFs due to the size of the sample space, which is approximately the
size of the token set to the power of sequence length. To tackle this issue, we split the original token set into
five roughly equal partitions and assign a meta-token to each partition. Given a sequence of tokens from the
original set, we construct a new sequence by replacing each token with the corresponding meta-token. We
estimate the PDFs of the sequences of meta-tokens created using texts from the WebText and GPT-2 output
datasets. Since the set of meta-tokens is significantly smaller than the original token set, estimating PDFs
becomes much more tractable. We then use these PDFs to estimate the total variation distances of the output
distributions of different GPT-2 models (GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL) from
the WebText dataset. Figure 10 plots these TV estimates for different sequence lengths, averaged over 30
runs of the experiment. We observe that the TV distance consistently decreases with increasing model size
for all sequence lengths.
These experiments provide empirical evidence that more advanced LLMs can lead to smaller TV distances.
Thus, based on Theorem 1, reliable AI text detection would become increasingly difficult.
Acknowledgments and Disclosure of Funding
This project was supported in part by NSF CAREER AWARD 1942230, ONR YIP award N00014-22-1-2271,
NIST 60NANB20D134, Meta award 23010098, HR001119S0026 (GARD), Army Grant No. W911NF2120076,
a capital one grant, and the NSF award CCF2212458 and an Amazon Research Award. Sadasivan is also
supported by the Kulkarni Research Fellowship. The authors would like to thank Keivan Rezaei and Mehrdad
Saberi for their insights on this work. The authors also acknowledge the use of OpenAI’s ChatGPT to
improve clarity and readability.