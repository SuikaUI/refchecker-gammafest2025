IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
Automatic Foveation for Video Compression Using
a Neurobiological Model of Visual Attention
Laurent Itti
Abstract—We evaluate the applicability of a biologically-motivated algorithm to select visually-salient regions of interest in video
streams for multiply-foveated video compression. Regions are selected based on a nonlinear integration of low-level visual cues,
mimicking processing in primate occipital, and posterior parietal
cortex. A dynamic foveation ﬁlter then blurs every frame, increasingly with distance from salient locations. Sixty-three variants of
the algorithm (varying number and shape of virtual foveas, maximum blur, and saliency competition) are evaluated against an outdoor video scene, using MPEG-1 and constant-quality MPEG-4
(DivX) encoding. Additional compression radios of 1.1 to 8.5 are
achieved by foveation. Two variants of the algorithm are validated
against eye ﬁxations recorded from four to six human observers
on a heterogeneous collection of 50 video clips (over 45 000 frames
in total). Signiﬁcantly higher overlap than expected by chance is
found between human and algorithmic foveations. With both variants, foveated clips are, on average, approximately half the size of
unfoveated clips, for both MPEG-1 and MPEG-4. These results
suggest a general-purpose usefulness of the algorithm in improving
compression ratios of unconstrained video.
Index Terms—Bottom up, eye movements, foveated, saliency,
video compression, visual attention.
I. INTRODUCTION AND BACKGROUND
N INCREASINGLY popular approach to reduce the size
of compressed video streams is to select a small number of
interesting regions in each frame and to encode them in priority.
This spatial prioritization scheme relies on the highly nonuniform distribution of photoreceptors on the human retina, by
which only a small region of 2 – 5 of visual angle (the fovea)
around the center of gaze is captured at high resolution, with
logarithmic resolution falloff with eccentricity . Thus, the rationale is that it may not be necessary or useful to encode each
video frame with uniform quality, since human observers will
crisply perceive only a very small fraction of each frame, dependent upon their current point of ﬁxation. In a simple approach
(used here), priority encoding of a small number of image regions may decrease overall compressed ﬁle size by tolerating
additional degradation in exchange for increased compression
outside the priority regions. In more sophisticated approaches,
priority encoding may serve to temporally sequence content delivery (deliver priority regions ﬁrst), or to continuously scale
Manuscript received June 24, 2003; revised June 24, 2004. This work was
supported by NSF, NEI, NIMA, the Zumberge Fund, and the Charles Lee Powell
Foundation. The associate editor coordinating the review of this manuscript and
approving it for publication was Dr. Fernando M. B. Pereira.
The author is with the Departments of Computer Science, Psychology and
Neuroscience Graduate Program, University of Southern California, Los Angeles, CA 90089-2520 USA (e-mail: ).
Digital Object Identiﬁer 10.1109/TIP.2004.834657
video quality depending on available transmission bandwidth
(encode priority regions as the core of a compressed stream,
with additional details transmitted only as additional bandwidth
is available – ).
The selection of priority regions remains an open problem.
Recently, key advances have been achieved in at least two contexts. First, real-time interactive gaze-contingent foveation for
video transmission over a bandwidth-limited communication
channel, and, second, priority encoding for general-purpose
noninteractive video compression. Gaze-contingent video
transmission typically uses an eye-tracking device to record
eye position from a human observer on the receiving end, and
applies in real time a foveation ﬁlter to the video contents at the
source – . Thus, most of the communication bandwidth
is allocated to high-ﬁdelity transmission of a small region
around the viewer’s current point of regard, while peripheral
image regions are highly degraded and transmitted over little
remaining bandwidth. This approach is particularly effective,
with observers often not noticing any degradation of the signal,
if well matched to their visual system and viewing conditions.
Even in the absence of eye tracking, this interactive approach
has demonstrated usefulness, for example when there exists
a set of ﬁxed priority regions, or when observers explicitly
point to priority regions . Further, online analysis of the
observer’s patterns of eye movements may allow more sophisticated interactions than simple foveation (e.g., zooming-in and
other computer interface controls ). However, extending
this approach to general-purpose noninteractive video compression presents severe limitations.
In the context of general-purpose video compression, indeed,
it is assumed that a single compressed video stream will be
viewed by many observers, at variable viewing distances, and
in the absence of any eye tracking or user interaction. Very high
inter-observer variability then precludes recording a single eye
movement scanpath from a reference observer and using it to determine priority regions in the video clip of interest. Recording
from several observers and using the union of their scanpaths
partially overcomes this limitation , but at a prohibitive cost:
An eye-tracking setup, population of observers, and time-consuming experiments are required for every new clip to be compressed.
Algorithmic methods, requiring no human testing have the
potential of making the process practical and cost-effective ,
 . Computer vision algorithms have, thus, been proposed to
automatically select regions of high encoding priority. Of particular interest here, several techniques rely on known properties of the human visual system to computationally deﬁne perceptually important image regions (e.g., based on object size,
contrast, shape, color, motion, or novelty – ). This type
1057-7149/04$20.00 © 2004 IEEE
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
of approach has been particularly successful, and those properties which are well deﬁned (e.g., contrast sensitivity function,
importance of motion, and temporal masking effects) are already implemented in modern video and still-image codecs ,
 . A limitation of these approaches, however, is that the remaining properties of human vision are difﬁcult to algorithmically implement (e.g., evaluating object size and shape requires
that ﬁrst object segmentation be solved in a general manner).
In contrast, an important contribution of the present study is to
evaluate the applicability of a computational model that mimics
the well-known response characteristics of low-level visual processing neurons in the primate brain, rather than attempting to
implement less well-deﬁned, higher-level visual properties of
objects and scenes. In addition, many of the existing computational algorithms have typically been developed for speciﬁc
video content (e.g., giving preference to skin color or facial features, under the assumption that human faces should always be
present and given high priority , , or learning image processing algorithms that maximize overlap with human scanpaths
on a speciﬁc set of images ) and, thus, are often not universally applicable. Instead, our model makes no assumption on
video contents, but is strongly committed to the type of neuronal
response properties found in early visual areas of monkeys and
humans. Finally, computational algorithms have thus far typically been demonstrated on a small set of video clips and often
lack validation against human eye movement data. Another important contribution of our study is, hence, to widely validate
our algorithm against eye movements of human observers ,
Validating an algorithm against human behavior is a challenging test, as human eye movements are inﬂuenced by many
factors, often tied to higher cognitive understanding of semantic
and affective scene contents rather than low-level image properties – . These factors include recognizing the scene’s
gist (broad semantic category, such as indoors or outdoors) and
layout, which may provide priors on the probable locations of
objects of interest and facilitate their recognition – . In
addition, when speciﬁc objects are actively searched for or ignored, low-level visual processing may be biased for or against
the visual features of these objects – , resulting in a topdown guidance of visual attention toward objects of interest
 , . Task, training, and general expertise also affect eye
movements, in part as the recognition of one object may provide clues as to the location of another , – . Similarly, memorizing objects found along an initial exploratory
scanpath may allow observers to later efﬁciently orient back to
some of these objects , . Finally, different observers exhibit different eye movement idiosyncrasies, possibly resulting
from different internal world representations , search strategies, personal preferences, culture, and other factors . Thus,
a last important contribution of the present study is to investigate whether a simple visual processing algorithm, which does
not attempt to understand the semantic contents of a given video
clip, may reasonably well correlate with human eye movements.
In the following, we start by describing our neurobiological
model of visual attention, which automatically selects regions
of high saliency (conspicuity) in unconstrained video inputs,
without requiring any per-clip tuning. As the low-level visual
processing at the basis of this model has been previously described in details , – , we focus here on speciﬁc
new model additions for the prediction of priority regions in
video streams. We then proceed with a systematic investigation of 63 variants of the algorithm, demonstrating a range of
achievable tradeoffs between compressed ﬁle size and visual
quality. We then validate the algorithm, for two of the 63 settings, on a heterogeneous collection of 50 video clips, including
synthetic stimuli, outdoors daytime and nighttime scenes, video
games, television commercials, newscast, sports, music video,
and other content. Using eye movement recordings from eight
human subjects watching the unfoveated clips (four to six subjects for each clip), we show that subjects preferentially ﬁxate
locations which the model also determines to be of high priority,
in a highly signiﬁcant manner. We ﬁnally compute the additional
compression ratios obtained on the 50 clips using the foveation
centers determined by our model, demonstrating the usefulness
of our approach to the fully automatic determination of priority
regions in unconstrained video clips.
II. ATTENTION MODEL AND FOVEATION
The model computes a topographic saliency map (Fig. 1),
which indicates how conspicuous every location in the input
image is , . Retinal input is processed in parallel by
multiscale low-level feature maps, which detect local spatial
discontinuities using simulated center-surround neurons ,
 . Twelve neuronal features are implemented, sensitive to
color contrast (red/green and blue/yellow, separately), temporal
ﬂicker (onset and offset of light intensity, combined), intensity
contrast (light-on-dark and dark-on-light, combined), four
orientations (0 , 45 , 90 , 135 ), and four oriented motion
energies (up, down, left, and right), as previously described
 , , . The extent to which the low-level features
used here attract attention in humans and monkeys has been
previously investigated in details , , . Center and
surround scales are obtained using dyadic pyramids with nine
scales (from scale 0, the original image, to scale 8, the image
reduced by a factor 256). Center-surround differences are then
computed as pointwise differences across pyramid scales, for
combinations of three center scales (
center-surround scale differences (
); thus, six feature
maps are computed for each of the 12 features, yielding a
total of 72 feature maps. Each feature map is endowed with
internal dynamics that operate a strong spatial within-feature and within-scale competition for activity, followed by
within-feature, across-scale competition . Resultingly,
initially possibly very noisy feature maps are reduced to sparse
representations of only those locations which strongly stand
out from their surroundings. All feature maps are then summed
 into the unique scalar saliency map that guides attention
The basic operation of the algorithm is as follows in the context of video compression: A dynamic saliency map is computed as described above, over the entire duration of each video
clip. In one variant of the algorithm, a snapshot of the saliency
map at each frame directly determines the priority to be given
to every spatial location in the frame, after normalization by
a squashing function and temporal smoothing [Fig. 2(a)]. For
every pixel with image coordinates
and instanta-
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
Overview of the model. Inputs are decomposed into multiscale analysis channels sensitive to low-level visual features (two color contrasts, temporal
ﬂicker, intensity contrast, four orientations, and four directional motion energies). After strong nonlinear competition for saliency, all channels are combined into
a unique saliency map. This map either directly modulates encoding priority (higher priority for more salient pixels), or guides several virtual foveas toward the
most salient locations (highest priority given to fovea centers).
neous saliency value
at frame , the saliency value is ﬁrst
squashed by a remapping function
, to downplay values below
average and give more dynamic range to values above average.
is a fourth-order sigmoidal interpolation polynomial satisfying
has horizontal slope at both ends of an input range of values
and remaps that range to a new range
also remapping a midpoint
. Adjustment of this
midpoint allows to give more emphasis to higher or lower values
in the remapping. The functional expression for
is derived
from the above constraints, yielding
In our implementation, for a given , denoting by
the minimum saliency value over the current frame, by
the maximum and by
the average
Encoding priority
of an image location
ﬁnally computed as the averaged squashed saliency value over
successive frames
Alternatively, a small number of discrete virtual foveas endowed with mass, spring (stiffness
), and ﬂuid friction (coef-
) dynamics attempt to track a collection of most salient
objects, with each fovea modeled as a unit mass at one end of
a spring with zero rest length, and a salient location serving
as an anchor point at the other end. Given the current location
of the -th fovea and an anchor location
, at every time step
with, in our implementation,
N s/pixel, and an implicit mass of 1 kg at
(hence the large value for
, so that signiﬁcant motion may be
achieved in a few milliseconds). At every frame, a new spring
anchor point
is set for every fovea
at a salient image
location that maximizes a correspondence score for that fovea.
Correspondence between the
most salient locations on a given
frame and the
foveas from the previous frame is established
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
Examples of predicted priority maps for two settings of our model. (a) Maxnorm feature normalization, continuous priority map (0 virtual foveas),
foveation pyramid depth 4. (b) Fancynorm, three foveas, depth 6. Top left: original frame. Top right: foveated frame (with fovea centers marked by yellow squares
when using discrete foveas). Bottom left: saliency map (brighter for higher saliency). Bottom right: priority map (darker for higher priority).
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
through an exhaustive scoring of all
possible pairings between a new salient location
and an old foveation
(typically,
is ﬁxed and
ensure robustness against varying saliency ordering from frame
to frame). The correspondence score
to be maximized
combines four criteria
is the Euclidean spatial distance between locations
the Euclidean distance between 72-component feature vectors
constructed by reading out (with bilinear interpolation) values
from the 72 feature maps at the locations of
a penalty term that discourages permuting previous pairings
by encouraging a ﬁxed ordered pairing, and
saliency of . In our implementation,
are the image width and height,
(center-surround features vary between
255 and 255),
. Combined, these criteria tend
to assign foveas to salient locations in decreasing order of
saliency, unless feature similarity is strong enough to warrant
a different ordering (e.g., when a tracked object switches from
being the second most salient in one frame to the fourth most
salient in the following frame). Video compression priority
at every location
is then derived from the squashed
distance to the closest fovea center [Fig. 2(b)], as computed
using a 3/4 chamfer distance transform 
is computed as in . For
further implementation details, we refer the reader to the source
code of our algorithm, freely available online upon request .
The dynamics of the virtual foveas do not attempt to emulate
human saccadic (rapid ballistic) eye movements , as those
rapid and often jerky motions would create highly visible artifacts in the compressed streams. Rather, the virtual foveas attempt to track salient objects in a smooth and damped manner
so that the foveation ﬁlter does not change too abruptly from
frame to frame. Also, perfect tracking of a given set of objects
was not desirable in our context where it is very important to
rapidly focus onto new salient objects, even though that often
means losing track of some current object. This is why we have
not used stronger trackers like particle ﬁlters . Thus, our correspondence and tracking algorithm compromises between reliably tracking the few most salient objects, and time-sharing
remaining foveas among a larger set of less salient objects. A
key aspect of our approach is that is makes no assumption on
the type of video streams to be processed or their contents (e.g.,
presence of human faces). Instead, our low-level features maps
are computed according to electrophysiological evidence for
certain types of neural feature detectors and spatial interactions
in the primate retina, lateral geniculate nucleus of the thalamus,
primary visual cortex, and posterior parietal cortex . Similarly, our foveation technique relies on spatial and feature-space
distances rather than, for example, predeﬁned object templates.
To evaluate the algorithm, we here simply use it as a front
end, applied before standard video compression [both MPEG-1
and MPEG-4 (DivX) were tested]: a spatially-variable blur is
applied to the input frames such that lower-priority regions are
more strongly blurred. Although this is not optimal in terms
of expected ﬁle size gains, it has the advantage of producing
compressed streams that are compatible with existing decoders
and to render the spatial prioritization computed by the algorithm obvious upon visual inspection. This method should be
regarded as a worst-case scenario for two reasons. First, as
virtual foveas move, the appearance of static objects far from
the foveas changes, requiring continuous re-encoding of those
changes. Second, even when the foveas remain static for some
time, peripheral moving objects receive variable amounts of
blur, depending on distance to the closest fovea. This defeats
motion compensation in the encoder, yielding continuous
re-encoding of these moving objects. Video codecs have been
proposed to address these problems inherent to any foveated
video compression technique (e.g., encode high-priority regions
ﬁrst, then lower-priority regions, in a continuously-variable-bitrate encoding scheme ). To simplify the visual evaluation
of our algorithm and to evaluate whether our technique might
prove useful even with standard codecs, however, here we
use standard MPEG-1 and MPEG-4 encoding and simple
spatially-variable blur of the video stream prior to compression.
Any ﬁle size gain obtained despite these limitations would,
hence, represent the promise that even better size gains should
be obtained with a video codec that would truly use the model’s
priority maps to prioritize encoding.
On one example clip, we explore variations of the algorithm
(Table I); namely, we vary the following.
1) Number of foveas: When using virtual foveas.
2) Saliency interactions: We explore three methods by
which conspicuous regions spatially compete with each
other for saliency, previously described in details ,
 : Maxnorm (normalization in each feature channel
by the squared difference between global maximum
and average of all other local maxima), FancyOne (one
iteration of a detailed model of nonclassical surround
interactions in primary visual cortex), and Fancy (ten
iterations of the former). Maxnorm yields smoother, more
continuous saliency maps [Fig. 2(a)], while the other two
yield increasingly sparser saliency maps, with only a few
sharp peaks of activity [Fig. 2(b)].
3) Object segmentation: When using virtual foveas, their
centers are represented either by a disk of ﬁxed radius (30 pixels) or are dynamically shaped to coarsely
match the shape of the attended object, thus resulting in
so-called “object-shaped foveas.” The object segmentation technique used for this purpose relies on a simple
region-growing algorithm that is applied in the feature
map that is the most active at the attended location and,
thus, is predominantly responsible for making that location salient. Additional details on this technique have
been previously described .
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
COMPRESSED FILE SIZE OF FOVEATED RELATIVE TO ORIGINAL CLIP FOR BEVERLY03 (FIG. 2), VARYING FOVEATION TECHNIQUE (CONTINUOUS BLUR
BASED ON ENTIRE SALIENCY MAP, OR 1–5 DISCRETE FOVEAS) AND SALIENCY COMPETITION TECHNIQUE (MAXNORM, FANCYONE OF FANCY)
4) Blur pyramid depth: Blurring is achieved through computation of a Gaussian pyramid from each input frame
and subsequent readout of every pixel from a pyramid
level that depends on the priority assigned to that pixel
(with trilinear interpolation). The deeper the pyramid,
the higher the amount of blur applied at locations of low
The encoder settings are shown in Fig. 3. For MPEG-1, we
simply used the default settings of the mpeg_encode program
 . For MPEG-4, the default settings of the mencoder program are for constant-bitrate, variable-quality encoding,
which is not appropriate for size comparisons since the encoder
would vary quality to always produce streams with approximately same bitrate. Thus, we used constant-quality, variablebitrate and otherwise default settings for this encoder. We used
a medium MPEG-4 quality setting, to approximately match the
MPEG-1 ﬁlesizes on the unfoveated clips.
Overall, the results in Table I indicate a high degree of ﬂexibility of the approach, with additional compression ratios for
the foveated clips ranging from 1.1 to 8.5 compared to the unfoveated clips. With pyramid depths 2 and 4, the compressed
clips are pleasant to watch, especially in the continuous mode
(using the whole saliency map to derive blur rather than discrete foveas). With depth 2, it is often difﬁcult to notice any
obvious blurring, though additional compression ratios of up to
2.8 with MPEG-1 and 4.2 with DivX are achieved. With depth 6,
size gains are maximized but blurring artifacts are obvious. This
suggests an application to long-term archival, where apparent
artifacts may be tolerated as long as the main contents of the
clip can be understood. Object-shaped foveas resulted in better
clarity, at the cost of lower size gains compared to the circular
foveas (since object shapes often extended to much larger regions than the ﬁxed-size circular apertures). The dependence of
size gain on model parameter settings followed the same trends
for both encoders. The resulting clips can be visually inspected
online .
III. HUMAN EYE TRACKING
To validate our approach, we compared the computed locations of high priority from our algorithm to the gaze locations
from eight human observers watching the unfoveated clips.
Subjects were naïve to the purpose of the experiment and
were USC students and staff (three females, ﬁve males, mixed
ethnicities, ages 23–32, normal or corrected-to-normal vision).
They were instructed to watch the video clips, and to attempt
to follow the main actors and actions, as they would be later
asked some general questions about what they had watched. It
was emphasized that the questions would not pertain to small
details (e.g., speciﬁc small objects, colors of clothing, identities
of persons, or contents of text messages) but would instead help
us evaluate their general understanding of the contents of the
clips. The choice of task instructions given to our subjects was
motivated by two factors: ﬁrst, we avoided instructionless free
viewing, since we believe that it often yields largely idiosyncratic patterns of eye movements, as subjects may attempt to
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
Video grabbing and encoding.
ﬁnd what the true undisclosed purpose of the experiment may
be, or simply lose interest and disengage from the experiment
over time. Second, our purpose was to evaluate the applicability
of our model to predicting regions that should be given high priority during compression, that is, regions of high cognitive importance for scene understanding, such as those which subjects
would consider were the main actors and actions in a scene. We
believe that our instructions did not explicitly bias subjects toward low-level salient image locations. During the discussions
that led subjects to sign an informed consent to participate to the
experiments (as approved by USC’s Internal Review Board), we
were careful to avoid words including “saliency,” “attention,”
“center-surround features,” or “image processing,” and to never
mention our model. Nevertheless, one should be aware that our
decision to give explicit task instructions to our subjects may
limit the general applicability of our results, to the extent that
biasing subjects toward the main actors and actions may have
implicitly biased them toward low-level salient image locations.
Given all the factors that contribute to guiding eye movements in
humans (see Introduction), however, we believe that our results
in the following section evaluate the extent to which a task-independent, pixel-based image processing algorithm, devoid of
explicit notions of actors or actions, may yield a selection of
scene locations that is comparable to the selection operated by
human subjects trying to build a cognitive interpretation of the
main contents of a video clip. A set of calibration points and
clips not part of the experiment were shown to familiarize the
subjects with the displays.
Stimuli were presented on a 22” computer monitor (LaCie
480, 60.27 Hz double-scan, mean screen luminance 30 cd/m , room 4 cd/m). Subjects were seated on an
adjustable chair at a viewing distance of 80 cm (28
usable ﬁeld-of-view) and rested on a chin-rest. A nine-point
eye-tracker calibration was performed every ﬁve clips .
Each calibration point consisted of ﬁxating ﬁrst a central cross,
then a blinking dot at a random point on a 3
3 matrix. The experiment was self-paced and subjects could stretch before any
nine-point calibration. Subjects ﬁxated a central cross, pressed
a key to start, at which point the eye-tracker was triggered, the
cross blinked for 1 206 ms, and the clip started. Stimuli were
presented on a Linux computer, under SCHED_FIFO scheduling (process would keep 100% of the CPU as long as needed
 ). Each unfoveated clip (MPEG-1 encoded) was entirely
preloaded into memory. Frame displays were hardware-locked
to the vertical retrace of the monitor (one movie frame was
shown for two screen retraces, yielding a playback rate of 30.13
fps). Microsecond-accurate timestamps were stored in
memory as each frame was presented, and later saved to disk to
check for dropped frames. No frame drop ever occurred and all
timestamps were spaced by
Eye position was tracked using a 240-Hz infrared-videobased eye tracker (ISCAN, Inc., model RK-464). This machine
estimates point of regard (POR) in real time from comparative
tracking of both the center of the pupil and the specular reﬂection of the infrared light source on the cornea. This technique
renders POR measurements immune to small head translations
(tested up to
10 mm in our laboratory). All analysis was
performed off line. Linearity of the machine’s POR-to-stimulus
coordinate mapping was excellent, as previously tested using
5 calibration matrix in our laboratory, justifying a 3
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
matrix here. The eye-tracker calibration traces were ﬁltered
for blinks and segmented into two ﬁxation periods (the central
cross, then the ﬂashing point), or discarded if that segmentation failed a number of quality control criteria. An afﬁne
POR-to-stimulus transform was computed in the least-square
sense, outlier calibration points were eliminated, and the afﬁne
transform was recomputed. If fewer than six points remained
after outlier elimination, recordings were discarded until the
next calibration. A thin-plate-spline nonlinear warping was
then applied to account for any small residual nonlinearity .
Data was discarded until the next calibration if residual errors
greater than 20 pixels on any calibration point or 10 pixels
overall remained. Eye traces for the ﬁve clips following a calibration were remapped to screen coordinates, or discarded if
they failed some quality control criteria (excessive eye-blinks,
motion, eye wetting, or squinting). Calibrated eye traces were
visually inspected when superimposed with the clips, but none
was discarded based upon that subjective inspection. Although
we had no external reference to quantify the accuracy of the
calibrated traces, overall the quality seemed remarkable with
this eye-tracker (e.g., subjects tracking the ten-pixel-wide head
of a person running at a distance, in clip beverly08).
Fifty video clips were selected from a database of 85, with
as only selection criterion to maximize diversity. All clips had
been digitized from their analog interlaced NTSC video source
(Fig. 3) using a consumer-grade framegrabber (WinTV Go,
Hauppage, Inc.) and no attempt was made at de-interlacing or
color-correcting them. The clips included:
daytime outdoors scenes ﬁlmed at a park in
Beverly Hills;
various video games (ﬁrst person, racing, etc);
outdoors day/night scenes at the Santa Monica
Promenade;
saccadetest
a synthetic disk drifting on a textured background;
daylight scenes ﬁlmed at a crowded open-air
rooftop bar;
an action scene from a television movie;
television advertisements;
tv announce
a television program’s announce;
a music video interleaved with some football
various television newscasts;
televised basketball and football games;
television talk-shows and interviews.
All clips and the corresponding algorithmic multifoveation
results and human eye movements may be examined online .
Clips had between 164 and 2 814 frames (5.5 s to 93.9 s). Subjects viewed each clip at most once, to ensure that they were
naïve to its contents. Five subjects viewed all clips and three
only viewed a few; after our quality control criteria were applied, calibrated eye movement data was available for four to
six subjects on each clip (Table II). Note that the ﬁgures for
beverly03 slightly differ between Tables I and II due to different
playback frame rates (29.97 fps versus 30.13 fps).
To interpret our results, it is useful to note that (not unexpectedly) the average recommended blur (over the extent of
each frame, then over all frames) closely matched a compound
measure of local blur over a random scanpath and also closely
matched a compound measure of blur over a human scanpath
if the priority map was randomly scrambled (not shown). Thus,
in the %avg at eye columns of Table II, a value of 100% or
more would indicate that humans did not look at regions of
high model priority (low suggested blur) more than a random
scanpath (or, equivalently, that a randomly scrambled priority
map would predict human ﬁxations as well as the model’s priority map). Conversely, a value of 0% would indicate that humans always gazed exactly at regions of maximum priority (no
suggested blur). Remarkably, for all but two clips, we found a
highly signiﬁcant agreement between model priority and human
ﬁxations. Agreement was independently evaluated for each clip
using a one-tailed t-test for the null hypothesis that the %avg
at eye ﬁgures could have been drawn from a distribution with
mean 100% (i.e., human scanpaths correlated with model priority no better than random scanpaths). When using Maxnorm
feature normalization and continuously-variable blur, the hypothesis was rejected for every clip, with
or better.
When using Fancy normalization and three circular foveas, the
hypothesis was also rejected with
or better for 48 of
the 50 clips. Figs. 4 and 5 show sample frames.
A breakdown by visual features in Table III indicated that
ﬂicker and motion energy were the most elevated at human eye
ﬁxations compared to on average over the entire frame, in agreement with the widely recognized importance of motion in capturing attention . However, all features were signiﬁcantly
elevated at human eye locations and, thus, none was useless
for each feature on a one-tailed t-test that the mean
ratio of feature values at eye to the average was greater than
It is interesting to compare our model to a much simpler,
center-weighted model, since previous studies have indicated
that the distributions of ﬁxations typically observed as humans
inspect visual scenes show a strong bias for the center of the display . Fig. 6(a) indicates that indeed, overall, the distribution
of ﬁxations over all observers and clips was strongly biased toward the display center. On individual clips, however, this was
not always true, as exempliﬁed in Fig. 6(b)–(d). Thus, while a
center-weighted model may perform well overall, it will fail on
some clips, while our model performed well on all clips, except
for two with one model variant. This, however, suggests an interesting extension of our model, by which the distributions of
human eye ﬁxations could be used as prior biasing maps, if the
model was to be applied to speciﬁc video content of ﬁxed layout
(e.g., the highly structured tv news clips) .
IV. DISCUSSION
Our main focus in this study is not a speciﬁc video compression technique, but the evaluation of our biological model of
attention to compute priority maps. Simply blurring the frames
before compression is a worst-case scenario in terms of expected
size gains, but is useful for inspecting our results.
In Table I, we have shown a high degree of ﬂexibility of
our paradigm, yielding a wide range of additional compression
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
AGREEMENT BETWEEN HUMAN EYE MOVEMENTS AND MODEL PRIORITY MAPS, FOR TWO ALGORITHM VARIANTS (FIG. 2). NSUBJ: FOUR TO SIX HUMAN
SUBJECTS VIEWED EACH CLIP. %AVG AT EYE: COMPOUNDED RATIOS BETWEEN MODEL-SUGGESTED BLUR AT HUMAN EYE FIXATIONS AND AVERAGE BLUR
OVER ENTIRE FRAME. MPG1, DIVX: RATIO (%) OF COMPRESSED SIZES FOR FOVEATED VERSUS UNFOVEATED CLIPS, USING MPEG-1 AND DIVX ENCODING
ratios (from 1.1 to 8.5) and associated visual degradation, depending on settings. In Table II, foveated clip size was approximately half of the unfoveated size on average. Smallest sizes
were obtained for the simplest clips, where only one small object
moved on a static background (e.g., saccadetest, gamecube05).
There was no evidence for any systematic covariation of size
gains between the two variants of the algorithm. Typically, using
a continuous priority map yielded higher compression when a
small number of objects moved on a static background (e.g.,
saccadetest, beverly07). Indeed, the amount of blur applied to
the background would remain fairly unchanged in this case, but
would greatly vary when discrete foveas focused on the moving
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
Additional examples of model-predicted priority maps for the ﬁrst variant of the model [continuously variable blur, Maxnorm, blur pyramid depth 4, as
in Fig. 2(a)]. Current eye position of one human observer watching the original unfoveated clip is shown (arrow) for comparison.
objects (but see below, for the more complex scenario where
additional objects enter and leave the ﬁeld of view). When signiﬁcant full-ﬁeld motion was present, however, three discrete
foveas often performed better (e.g., panning camera motion in
some “standard” clips), because they would select at most three
regions while the continuous map would select many more.
Note that our low-level motion detectors, however, are naturally
insensitive to full-ﬁeld translational motion, due to the global
spatial competition for saliency operated in each feature map
 , . A possible future extension of our algorithm consists
of allowing the number of foveas to vary from frame to frame,
although one difﬁculty will be to ensure that the appearance and
disappearance of additional foveas does not create highly visible
artifacts in the compressed stream.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
Additional examples of model-predicted priority maps for the second variant of the model [three discrete foveas, Fancynorm, blur pyramid depth 6, as in
Fig. 2(b)]. Current eye position of one human observer watching the original unfoveated clip is shown (arrow) for comparison.
An interesting feature of the results in Table II was that
both variants of the algorithm performed equally well in terms
of average additional compression ratios, yet one used a blur
pyramid of depth 4 (and continuous blur) while the other used
stronger depth 6 (and three foveas). For general application,
when it is unknown whether only small objects or the full
ﬁeld of view will substantially change over time, the variant
with depth 4 yields lower maximum possible blur and overall
more visually pleasant results. Hence, our results argue somewhat against the traditional foveation paradigm and the binary
selection of a small number of regions of interest , .
Instead, we suggest that a continuous measure of interest for
every pixel in the frame is a more efﬁcient strategy, since depth
4 in that case yielded on average same compression gains but
substantially less-visible artifacts than depth 6 with the traditional approach.
ITTI: AUTOMATIC FOVEATION FOR VIDEO COMPRESSION
BREAKDOWN BY FEATURE CATEGORIES. RATIOS (MEAN  STANDARD ERROR OF THE MEAN) OF THE FEATURE VALUES AT HUMAN EYE POSITION TO THE
AVERAGE VALUE OVER THE ENTIRE FRAME, COMPOUNDED OVER ALL 1 658 161 VALID EYE MOVEMENT SAMPLES, FOR EACH FEATURE CATEGORY (COLOR,
RED/GREEN AND BLUE/YELLOW MAPS COMBINED; FLICKER; INTENSITY, ORIENTATION, 0 , 45 , 90 , 135 COMBINED; AND MOTION ENERGIES, UP, DOWN,
LEFT AND RIGHT COMBINED). ALL FEATURES WERE SIGNIFICANTLY HIGHER AT HUMAN EYE POSITIONS THAN ON AVERAGE OVER THE DISPLAY (RATIOS
LARGER THAN UNITY, p < 0:001 ON A ONE-TAILED T-TEST FOR EVERY FEATURE), WITH FLICKER AND MOTION ENERGY BEING THE MOST ELEVATED. RATIOS
ARE OVERALL HIGHER AND MORE VARIABLE FOR THE FANCY MODEL VARIANT, WHICH HAS SPARSER FEATURE MAPS THAN THE MAXNORM VARIANT (FIG. 2)
When using three discrete foveas, sometimes the motion of
the foveas induced so much change in textured backgrounds
that any size gain due to foveation was offset by the need to
continuously re-encode the background (e.g., saccadetest and
some gamecube clips). This is a clear limitation of the fairly simplistic blurring technique used here to visualize model predictions. When using continuous blur, sometimes a similar problem
existed, as, inherently, the saliency of an object depends on
every other object in the scene (e.g., a bright red object will
not be salient if many other bright red objects are present in the
image). Consequently, the saliency of a static object typically
varied as other salient objects entered or left the ﬁeld of view,
requiring re-encoding of the static object to reﬂect changes in
its appearance with varying saliency and blur. A simple solution
to this problem could be to take the maximum, at each frame,
between previous and current priority maps, in regions where
no signiﬁcant change is detected. Thus, once a static object has
been encoded crisply, it would remain crisp as long as it remains
static (at very little cost in the compressed stream, since current
appearance would be accurately predicted by previous frames).
However, it may be more useful to instead replace our blurring
scheme used for testing by a more sophisticated prioritization
scheme and dedicated video codec . These limitations in the
applicability of blur-based foveation to video compression may
be somewhat reduced by the use of a biresolutional foveation
blur instead of the continuously-variable blur used here .
Overall, we found surprisingly strong agreement between our
model’s predictions and the scanpaths recorded from human observers, with motion and ﬂicker cues providing the best agreement, but other cues in the model showing strong responses at
human eye ﬁxations as well. It is important to note that our measure of agreement required that humans ﬁxated a given location
at the same time as the model did, in order for this ﬁxation to
increase the measure of agreement between humans and model.
Our use of multiple foveas certainly helped reduce the number
of cases where humans and model were essentially interested in
the same objects but in a different temporal order, but did not
entirely eliminate them.
Another difference which lowered the agreement between humans and model was that humans often ﬁxated small details
(e.g., ﬁneprint text tickers in the tv-news clips), while other actors or actions were more salient (e.g., the anchor). This is perfectly reasonable and indicates that sometimes there was a mismatch between regions that were low-level salient as computed
by our model, but largely irrelevant to cognitive scene understanding as attempted by our observers (e.g., subjects guessed
that the salient anchor would just speak, with a low probability
of making interesting faces, and decided that the text might be
more informative although far less salient, in particular since our
clips had no soundtrack). Given this, it is remarkable that such
good agreement between humans and model was obtained on
our wide variety of clips. Our results indicate that throughout
each clip, bottom-up saliency based on low-level analysis of
pixels strongly correlated with the cognitive decision of what
constituted the main actors and actions. This contrasts with a
more commonly agreed view in which bottom-up attentional
guidance is believed to be active only for a short period after
the presentation of a new (static) scene, with more cognitive
top-down guidance taking over afterwards . With rapidly
changing video stimuli, our results reinforce the idea of a continual contribution of bottom-up cues to the guidance of attention. Thus, although our model is bottom-up and has no explicit
notion of actor or action, its applicability to the selection of regions that are important for cognitive scene understanding is
well supported by our data.
One last difference between humans and model was the often
predictive nature of human gaze shifts, while our model only
follows salient objects. In several gamecube clips, indeed, humans focused on the empty path before a fast-running hero,
probably to ensure that it remained clear of obstacles. This is another limitation of our model, which may be solved using more
sophisticated trackers for our virtual foveas, with predictive capabilities. Note, however, that since usually the path was empty
and often featureless, its representation in the foveated clips was
not much degraded by blurring.
Our model performed generally well on every clip, with
%avg at eye ﬁgures in Table II signiﬁcantly below 100%
on every clip for the ﬁrst model variant and on 48/50 clips
for the second. In contrast, we saw in Fig. 6 that a simpler
center-weighted model would be expected to perform well
overall, but not necessarily on every clip. Indeed, our model
does not make any a priori assumption as to the spatial distribution of priority regions. As mentioned in introduction, however,
it is known that the rapid recognition of the gist of a scene
may impose such prior distribution as to the likely locations of
objects of interest . Such distribution may also be learned
over time in the form of a “task-relevance map” that may
ﬁlter out salient locations known to be of little task relevance
(e.g., in the gamecube clips, salient health indicators strongly
attracted the model but were largely ignored by subjects). A
promising direction for future research is to attempt to learn
the association between scene gists and eye movement density
maps like those in Fig. 6, and to use those as modulators to the
model’s saliency map.
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 13, NO. 10, OCTOBER 2004
Distribution of the number n of our 1 658 161 valid human eye movement samples (recorded at 240 Hz) over the (x; y) extent of the video frames,
histogrammed over 32  32 image tiles. (a) The overall distribution for all subjects and clips shows a strong bias toward the center of the display. However,
individual clips may exhibit very different distributions, in a manner not predictive of how well our algorithm will perform. (b) Distribution for clip beverly03
(see sample frames in Fig. 2) where very good agreement was found between the three-fovea model and humans. (c) Distribution for clip saccadetest (see sample
frames in Fig. 5), also in excellent agreement. (d) Distribution for clip standard01 (see sample frames in Fig. 4), where worse agreement was found between model
and humans.
Overall, our study demonstrates that our biological model of
bottom-up attention highly signiﬁcantly correlates with human
eye movements on unconstrained video inputs. Importantly, this
was observed although the model contains nothing in its design or implementation that specializes it to any speciﬁc video
content or pre-deﬁned objects of interest. Both with MPEG-1
and MPEG-4 encoding, substantial compressed ﬁle size reductions were achieved by foveation. Our study thus argues that our
model is applicable to automatic spatiotemporal prioritization of
arbitrary video contents for compression.