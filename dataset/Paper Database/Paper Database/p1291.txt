Machine learning phases of matter
Juan Carrasquilla1 and Roger G. Melko2, 1
1Perimeter Institute for Theoretical Physics,
Waterloo, Ontario N2L 2Y5, Canada
2Department of Physics and Astronomy,
University of Waterloo, Ontario, N2L 3G1, Canada
Neural networks can be used to identify phases and phase transitions in condensed
matter systems via supervised machine learning. Readily programmable through
modern software libraries, we show that a standard feed-forward neural network
can be trained to detect multiple types of order parameter directly from raw state
conﬁgurations sampled with Monte Carlo. In addition, they can detect highly nontrivial states such as Coulomb phases, and if modiﬁed to a convolutional neural
network, topological phases with no conventional order parameter. We show that this
classiﬁcation occurs within the neural network without knowledge of the Hamiltonian
or even the general locality of interactions. These results demonstrate the power
of machine learning as a basic research tool in the ﬁeld of condensed matter and
statistical physics.
 
Condensed matter physics is the study of the collective behavior of massively complex
assemblies of electrons, nuclei, magnetic moments, atoms or qubits . This complexity is
reﬂected in the size of the classical or quantum state space, which grows exponentially with
the number of particles. This exponential growth is reminiscent of the “curse of dimensionality” commonly encountered in machine learning. That is, a target function to be learned
requires an amount of training data that grows exponentially in the dimension (e.g. the
number of image features). Despite this curse, the machine learning community has developed a number of techniques with remarkable abilities to recognize, classify, and characterize
complex sets of data. In light of this success, it is natural to ask whether such techniques
could be applied to the arena of condensed-matter physics, particularly in cases where the
microscopic Hamiltonian contains strong interactions, where numerical simulations are typically employed in the study of phases and phase transitions . We demonstrate that
modern machine learning architectures, such as fully-connected and convolutional neural
networks , can provide a complementary approach to identifying phases and phase transitions in a variety of systems in condensed matter physics. The training of neural networks
on data sets obtained by Monte Carlo sampling provides a particularly powerful and simple
framework for the supervised learning of phases and phase boundaries in physical models,
and can be easily built from readily-available tools such as Theano or TensorFlow 
libraries.
Conventionally, the study of phases in condensed matter systems is performed with the
help of tools that have been carefully designed to elucidate the underlying physical structures
of various states. Among the most powerful are Monte Carlo simulations, which consist of
two steps: a stochastic importance sampling over state space, and the evaluation of estimators for physical quantities calculated from these samples . These estimators are constructed based on a variety of physical impetuses; e.g. the ready availability of an analogous
experimental measure like a speciﬁc heat; or, the encoding of some more abstract theoretical
device, like an order parameter . However, unique and technologically important states of
matter may not be straightforwardly identiﬁed with standard estimators. Indeed, for some
highly-coveted phases such as topologically-ordered states , positive identiﬁcation may
require prohibitively expensive (and experimentally challenging ) measures such as the
entanglement entropy .
Machine learning, already explored as a tool in condensed matter and materials research
Input layer
Hidden layer
Output layer
Output layer
FIG. 1. Machine learning the ferromagnetic Ising model. (A) The trained neural network learns
representations of the low- and high-temperature Ising states. The average output layer (B) and
accuracy (C) over the test sets vs. temperature. (D) Toy model of a neural network for the Ising
model. (E) The average output layer and accuracy of the toy model are displayed in (E) and
(F), respectively. The orange lines signal Tc of the model in the thermodynamic limit, Tc/J =
 , provides an alternate paradigm to the above approach. The ability of modern machine learning techniques to classify, identify, or interpret massive data sets like images,
videos, genome sequences, internet traﬃc statistics, natural language recordings, etc. foreshadows their suitability to provide physicists with similar analyses on the exponentially
large data sets embodied in the state space of condensed matter systems. We ﬁrst demonstrate this on the prototypical example of the square-lattice ferromagnetic Ising model,
j. We set the energy scale J = 1; the Ising variables σz
i = ±1 so that for
N lattice sites, the state space is of size 2N. Standard Monte Carlo techniques can eﬃciently
provide samples of conﬁgurations for any temperature T, weighted by the Boltzmann distribution. The existence of a well-understood phase transition at temperature Tc , between
a high-temperature paramagnetic phase and a low-temperature ferromagnetic phase, allows
us the opportunity to attempt to classify the two diﬀerent types of conﬁgurations without
the use of Monte Carlo estimators (e.g. the magnetization). Instead, we construct a fully
connected feed-forward neural network, implemented with TensorFlow , to perform supervised learning directly on the raw conﬁgurations sampled by a Monte Carlo simulation
(see Figure 1). The neural network is composed of an input layer with values determined by
the spin conﬁgurations, 100-unit hidden layer of sigmoid neurons, and an analogous output
layer. We use a cross-entropy cost function supplemented with an L2 regularization term
to prevent overﬁtting. The neural network is trained using the Adam method for stochastic
optimization . As Illustrated in Figure 1(A) through (C), when trained on a broad range
of temperatures above and below Tc, the neural network is able to correctly classify 94% of
uncorrelated data provided in a test set, at the same temperatures as in the training set,
for a system of N = L2 = 102 spins. The classiﬁcation accuracy improves as the system size
is increased (as high as 99% for L = 40), as inferred from Figure 1(C), indicating that this
training/testing paradigm is capable of systematically narrowing in on the true thermodynamic value of Tc in a way analogous to the direct measurement of the magnetization in a
conventional Monte Carlo simulation. In fact, due to the simplicity of the underlying order
parameter (a bulk polarization of Ising spins below Tc), one can understand the training of
the network through a simple toy model involving a hidden layer of only three analytically
“trained” perceptrons, representing the possible combinations of high and low-temperature
magnetic states exclusively based on their magnetization.
As illustrated in Figure 1(D)
through (F), it performs the classiﬁcation task with remarkably high accuracy. We em-
phasize that the toy model has no a priori knowledge of the critical temperature. Further
details about the toy model, as well as a low-dimensional visualization of the training data
to gain intuition for how these neural networks operate, are discussed in the supplementary materials. Similar results and success rates occur if the model is modiﬁed to have
anti-ferromagnetic couplings, H = J P
j, illustrating that the neural network is not
only useful in identifying a global spin polarization, but an order parameter with a more
complicated ordering wave vector ( here q = (π/a, π/a), where a is the lattice spacing).
Clearly, such a framework does not provide the same quantitive understanding as a direct
Monte Carlo measurement of the order parameter, which sits on a solid bedrock of decades
of statistical mechanics theory. However, the power of neural networks lies in their ability
to generalize to tasks beyond their original design. For example, what if one was presented
with a data set of Ising conﬁgurations from an unknown Hamiltonian, where the lattice
structure (and therefore its Tc) is not known? We illustrate this scenario by taking our
above feed-forward neural network, already trained on conﬁgurations for the square-lattice
ferromagnetic Ising model, and feed it a test set produced by Monte Carlo simulations of
the triangular lattice ferromagnetic Ising Hamiltonian. The network has no information
about the Hamiltonian, the lattice structure, or even the general locality of interactions.
In Figure 2 we present the output layer neurons averaged over the test set as a function
of temperature for L = 30. We estimate the critical temperature based on the crossing
point of the low- and high-temperature outputs to be Tc/J = 3.63581, which is close to
the exact thermodynamic Tc/J = 4/ ln 3 ≈3.640957 – a discrepancy easily attributed
to ﬁnite-size eﬀects.
Further, the same strategy can be repeated, using instead our toy
neural network. Again, without any knowledge of the critical temperatures on the square
or triangular lattices, we estimate Tc/J = 3.63403, diﬀering from the true thermodynamic
critical Tc by less than 1%.
We turn to the application of such techniques to problems of greater interest in modern
condensed matter, such as disordered or topological phases, where no conventional order
parameter exists. Coulomb phases, for example, are states of frustrated lattice models where
local energetic constraints lead to extensively degenerate classical ground states, which are
highly-correlated “spin liquids” without a bulk magnetization or other local order parameter.
We consider a two-dimensional square ice Hamiltonian given by H = J P
v where the
charge at vertex v is Qv = P
i , and the Ising variables located in the lattice bonds as
Output layer
FIG. 2. Detecting the critical temperature of the triangular Ising model through the crossing of the
values of the output layer vs T. The orange line signals the triangular Ising model Tc/J = 4/ ln 3,
while the blue dashed line represents our estimate Tc/J = 3.63581.
shown in Figure 3. In a conventional condensed-matter approach, the ground states and the
high-temperature states are distinguished by their spin-spin correlation functions: powerlaw decay in the Coulomb phase at T = 0, and exponential decay at high temperature.
Instead we use supervised learning, feeding raw Monte Carlo conﬁgurations to train a fullyconnected neural network (Figure 1(A)) to distinguish ground states from high-temperature
states. Figure 3(A) and Figure 3(B) display high- and low-temperature snapshots of the
conﬁgurations used in the training of the model. For a square ice system with N = 2 ×
16 × 16 spins, we ﬁnd that a standard fully-connected neural network with 100 hidden units
successfully distinguishes the states with a 99% accuracy. The network does so solely based
on spin conﬁgurations, with no information about the underlying lattice – a feat diﬃcult for
the human eye, even if supplemented with a layout of the underlying Hamiltonian locality.
These results indicate that the learning capabilities of neural networks go beyond the
simple ability to encode order parameters, extending to the detection of subtle diﬀerences
in higher-order correlations functions. As a ﬁnal demonstration of this, we examine an Ising
High temperature state
B Ising square ice ground state
Ising lattice gauge theory
FIG. 3. Typical conﬁgurations of square ice and Ising gauge models. (A) A high-temperature state.
(B) A ground state of the square ice Hamiltonian. (C) A ground state conﬁguration of the Ising
lattice gauge theory. The vertices and plaquettes deﬁning the square ice and Ising gauge theory
Hamiltonians are shown in the insets of (B) and (C).
lattice gauge theory, one of the most prototypical examples of a topological phase of matter
 . The Hamiltonian is given by H = −J P
i where the Ising spins live on
the bonds of a two-dimensional square lattice with plaquettes p, as shown in the inset of
Figure 3(C). The ground state is again a degenerate manifold (Figure 3(C)), with
exponentially-decaying spin-spin correlations that makes it much more diﬃcult to distinguish
from the high temperature phase.
Just as in the square ice model, we have made an attempt to use the neural network in
Figure 1(A) to classify the high- and low- temperature states in the Ising gauge theory. A
straightforward implementation of supervised training fails to classify a test set containing
samples of the two states to an accuracy over 50% – equivalent to simply guessing. Such
failures typically occur because the neural network overﬁts to the training set. To overcome this diﬃculty we consider a convolutional neural network (CNN) which readily
takes advantage of the two-dimensional structure of the input conﬁgurations, as well as the
translational invariance of the model. The CNN in Figure 4 is detailed in the supplementary materials. We optimize the CNN using Monte Carlo conﬁgurations drawn from the
partition function of the Ising gauge theory at T = 0 and T = ∞. Using this setting, the
CNN successfully discriminates high-temperature from ground states with an accuracy of
100% on a test set with 1 × 104 conﬁgurations, in spite of the lack of an order parameter
or qualitative diﬀerences in the spin-spin correlations. Through the generation of new test
(64 per sublattice)
Fully connected
layer (64)
regularization
FIG. 4. Illustrating the convolutional neural network. The ﬁrst hidden layer convolves 64 2 × 2
ﬁlters with the spin conﬁguration on each sublattice, followed by rectiﬁed linear units (ReLu). The
outcome is followed by fully-connected layer with 64 units and a softmax output layer. The green
line represents the sliding of the maps across the conﬁguration.
sets that violate an extensive fraction of the local energetic constraints of the theory, we
conclude that the discriminative power of the CNN relies on the detection of these satis-
ﬁed constraints. Furthermore, test sets with defects that retain most local constraints but
disrupt non-local features, like the extended closed-loop gas picture or the associated topological degeneracy , indicate that local constraints are the only features that the CNN
relies on for classiﬁcation of the ground state. In view of these observations, we construct
a simpliﬁed analytical toy model of our original CNN designed to explicitly exploit local
constraints in the classiﬁcation task. Such a model discriminates high-temperature from
ground states with an accuracy of 100%. Details of the behavior of the CNN with various
test sets, as well as the details of the analytical model, are contained in the supplementary
We have shown that neural network technology, developed for engineering applications
such as computer vision and natural language processing, can be used to encode phases of
matter and discriminate phase transitions in correlated many-body systems. In particular,
we have argued that neural networks encode information about conventional ordered phases
by learning the order parameter of the phase, without knowledge of the energy or locality
conditions of Hamiltonian. Furthermore, we have shown that neural networks can encode
basic information about the ground states of unconventional disordered models, such as
square ice model and the Ising lattice gauge theory, where they learn local constraints satis-
ﬁed by the spin conﬁgurations in the absence of an order parameter. These results indicate
that neural networks have the potential to faithfully represent ground state wave functions.
For instance, ground states of the toric code can be represented by convolutional neural
networks akin to the one in Figure 4 (see the supplementary materials for details). We thus
anticipate adoption to the ﬁeld of quantum technology , such as quantum error correction
protocols and quantum state tomography . The ability of machine learning algorithms
to generalize to situations beyond their original design anticipates future applications such
as the detection of phases and phase transitions in models vexed with the Monte Carlo sign
problem , as well as in experiments with single-site resolution capabilities such as the
modern quantum gas microscopes . As in all other areas of “big data”, we expect the
rapid adoption of machine learning techniques as a basic research tool in condensed matter
and statistical physics in the near future.
Acknowledgments. We would like to thank Ganapathy Baskaran, Claudio Castelnovo,
Anushya Chandran, Lauren E. Hayward Sierens, Bohdan Kulchytskyy, David Schwab, Miles
Stoudenmire, Giacomo Torlai, Guifre Vidal, and Yuan Wan for discussions and encouragement. We thank Adrian Del Maestro for a careful reading of the manuscript. This research
was supported by NSERC of Canada, the Perimeter Institute for Theoretical Physics, the
John Templeton Foundation, and the Shared Hierarchical Academic Research Computing
Network (SHARCNET). R.G.M. acknowledges support from a Canada Research Chair. Research at Perimeter Institute is supported through Industry Canada and by the Province of
Ontario through the Ministry of Research & Innovation.
Appendix A: Details of the toy model
The analytical model encodes the low- and high-temperature phases of the Ising model
through their magnetization. The hidden layer contains 3 perceptrons (a neuron with a
Heaviside step nonlinearity); the ﬁrst two perceptrons activate when the input states are
mostly polarized, while the third one activate if the states are polarized up or unpolarized.
Notice that the third neuron can also be choosen to activate if the states are polarized down
or unpolarized. The resulting outcomes are recombined in the output layer and produce the
desired classiﬁcation of the state. The hidden layer is parametrized through a weight matrix
FIG. 5. Hidden layer arguments as a function of the magnetization of the Ising state m(x). (A)
Hidden layer arguments for our toy model. (B) and (C) show the arguments for a neural network
with 3 sigmoid neurons before and after training, respectively.
and bias vector given by
−1 −1 · · · −1
, and b =
where 0 < ϵ < 1 is the only free parameter of the model. The arguments of the three hidden
layer neurons, in terms of the weight matrix, bias vector, and a particular Ising conﬁguration
x = (σ1σ2, ..., σN)T, are given by
where m(x) =
σi is the magnetization of the Ising conﬁguration. In Figure 5(A) we
display the components of the Wx + b vector as a function of the magnetization of the
Ising state m(x). The ﬁrst and second neuron activate when the state is predominantly
polarized, i.e., when m(x) > ϵ or m(x) < −ϵ. The third neuron activates if the state has
a magnetization m(x) > −ϵ, which means that, in the limit where 0 < ϵ ≪1, it activates
when the state is either polarized or unpolarized. The parameter ϵ is thus a threshold value
of the magnetization that helps deciding whether the state is considered polarized or not.
The output layer is parametrized through a weight matrix and bias vector given by
, and b2 =
where these arbitrary choices ensure that the ordered, low-T output neuron OLow-T = 1
is active when either the spins polarize mostly ↑or ↓. On the other hand, when the ↑∥0
neuron is active but the ↑is not, then the high-temperature output neuron OHigh-T = 1,
symbolizing a high-temperature state.
To illustrate what the eﬀects of the training on the parameters W and b are, we consider
the numerical training of a fully-connected neural network with only 3 neurons using the
same setup and training/test data used in our ferromagnetic model (Figure 1) for the L = 30
system. In Figure 5(B) we display the argument W0x + b0 of the input layer at training
iteration t = 0 for conﬁgurations x in the test set as a function of the magnetization of the
conﬁgurations m(x). The weights have been randomly initialized at t = 0 from a normal
distribution with zero mean and unit standard deviation. As the training proceeds, the
parameters adjust such that the components of the vector Wtx + bt approximately become
linear functions of the magnetization m(x), as shown in Figure 5(C), in agreement with the
assumptions of our toy model. These results clearly support our claim about the neural
network’s ability encode and learn the magnetization in the hidden layer.
Appendix B: Visualizing the action of a neural network on the Ising ferromagnet
A strategy to gain intuition for how these neural networks operate is to produce a lowdimensional visualization of data used in the training. We consider the t-distributed stochastic neighbor embedding (t-SNE) technique where high-dimensional data is embedded in
two or three dimensions so that data points close to each other in the original space are also
positioned close to each other in the embedded low-dimensional space. Figure 6 displays a
t-SNE visualization of the Ising conﬁgurations used the training of our ferromagnetic model.
The two low-temperature blue regions correspond to the two ordered states with spins polarized either up or down. The high-temperature red region identiﬁes the paramagnetic state.
The resulting neural networks, which are functions deﬁned over the high-dimensional state
space, become trained so that the low-temperature output neuron takes a high value in the
Temperature
FIG. 6. Two-dimensional t-SNE visualization of the training set used in the Ising model for L = 30
colored according to temperature. The orange line represents a hyperplane separating the low- from
high-temperatures states.
cool region (and vice versa), crossing over to a low value as the system is warmed through
the orange hyperplane. This allows the classiﬁcation of a state in terms of the neuron values.
Appendix C: Details of the convolutional neural network of the Ising lattice gauge
The exact architecture of the convolutional neural network (CNN) , schematically
described in Figure 4, is as follows. The input layer is a two-dimensional Ising spin conﬁguration with N = 16 × 16 × 2 spins, where σi = ±1. The ﬁrst hidden layer convolves 64
2 × 2 ﬁlters on each of the two sublattices of the model with a unit stride, no padding, with
periodic boundary conditions, followed by rectiﬁed linear unit (ReLu). The ﬁnal hidden
layer is a fully-connected layer with 64 ReLu units, while the output is a softmax layer with
two outputs (correponding to T = 0 and T = ∞states). To prevent overﬁtting, we apply a
dropout regularization in the fully-connected layer . Our model has been implemented
using TensorFlow .
Since our CCN correctly classiﬁes T = 0 and T = ∞states with 100% accuracy, we
would like to scrutinize the origin of its discriminiative power by asking whether it discerns
the states by the presence (or absence) of the local Hamiltonian constraints or the extended
closed-loop structure. Our strategy consists in the construction of new test sets with modiﬁed
low temperature states, as detailed below. First, we consider transformations that do not
destroy the topological order of the T = 0 state but change the local constraints of
the original Ising lattice gauge theory. As shown in the conﬁgurations in Figure 7(A) and
(B), we consider transformations where a spin is ﬂipped every m = 2 (A) (m = 8 (B))
plaquettes. The positions of the ﬂipped spins are marked with red crosses. After optimizing
the CNN using the original training set, the neural network classiﬁes most of the transformed
T = 0 states as high-temperature ones, resulting in an overall test accuracy of 50% and
55% for m = 2 and m = 8, respectively. This reveals that the neural network relies on
the presence of satisﬁed local constraints of the original Ising lattice gauge theory, and
not on the topological order of the state, in deciding whether a state is considered low
or high temperature. Second, we consider a new test set where the T = 0 states retain
most local constraints but disrupt non-local features like the extended closed-loop structure.
We consider dividing the original states into 4 pieces as shown in Figure 7(C) and then
reshuﬄing the 4 pieces among diﬀerent states, subsequently stitching them to form new
“low” temperature conﬁgurations. The new conﬁgurations will contain defects along the
dashed lines in Figure 7(C), thus disrupting the extended closed-loop picture, but preserving
the local constraints everywhere else in the conﬁguration. We ﬁnd that the trained CNN
recognizes such states as ground states with high conﬁdence, suggesting that the CNN does
not use the extended closed-loop structure and indicating that local constraints are the only
features that the CNN relies on for classiﬁcation of the ground state.
In view of the conclusion above, we now present a toy model that uses a streamlined version of our original CNN constructed to explicitly detect satisﬁed energetic local constraints.
The convolutional layer contains 16 2×2 ﬁlters per sublattice with unit stride in both directions and periodic boundary conditions. The convolutional layer is fully connected to two
perceptron neurons in the output layer, as described below. A schematic representation of
the toy CNN is presented in Figure 8. The values of the ﬁlters Wyxsf are presented in Table
C, where x and y represent the spatial indices of the convolution, and s and f label the
FIG. 7. Investigating the source of discriminative power of the convolutional neural network. New
test sets with sublattice rotations where the local constrains are such that 1/2 spins per plaquette
(A) and 1/8 spins per plaquette (B) are ﬂipped. The red crossess symbolize the ﬂipped spins.
(C) Cuts/stitches (yellow dashed lines) performed on the ground state conﬁgurations in order to
produce a new test set from mixing the 4 resulting pieces among diﬀerent ground states.
16 2x2 maps
sublattice
Fully connected
output layer
FIG. 8. Toy model of a convolutional neural network to classify states of the Ising gauge theory.
sublattice and the ﬁlter, respectively. The purpose of the ﬁlters is to individually process
each plaquette in the spin conﬁguration and determine whether its energetic constraints
are satisﬁed or not. The Ising gauge theory contains 16 diﬀerent spin conﬁgurations per
plaquette, of which 8 satisfy the energetic constraints of the Hamiltonian. The ﬁrst group
of 16 ﬁlters Wyxsf (f = 1 thorugh f = 8, left blue column in Table
C) detect satisﬁed
plaquettes, while the remaining 16 (f = 9 through f = 16, right red column in Table C)
detect unsatisﬁed plaquettes. The bias of the convolutional layer is a 16-dimensional vector
given by bc = −(2+δ) (1 · · · 1)T, where 0 < δ ≪1 is a small parameter. The outcome of the
TABLE I. Specifying the tensor Wyxsf ﬁlters in the CNN. y and x specify the spatial indices of
the ﬁlter, while s and f specify the sublattice and the ﬁlter, respectively.
convolutional layer consists of 16 two-dimensional arrays of size L × L processed through
perceptrons. Here the total number of spins in the conﬁguration is N = 2 × L × L, where
L is the linear size of the system. The outcome of the convolutional layer is reshaped into a
16 × L2-dimensional vector such that the ﬁrst 8L2 entries correspond to the outcome of the
ﬁrst group of ﬁlters (f = 1 thorugh f = 8) while the remaining 8L2 correspond to the last
group of ﬁlters (f = 9 through f = 16). The output layer, which is fully connected to the
the convolutional layer, contains two perceptron neurons denoted by O0 and O∞for zeroand high-temperature states, respectively. It is parametrized through a weight matrix and
a bias vector given by
−L2 + L −1 . . . −L2 + L −1
−1 . . . −1
L2 −L + 1 . . . L2 −L + 1
These choices ensure that whenever an unsatisﬁed plaquette is encountered by the convolutional layer, the zero-temperature neuron is O0 = 0 and the high-temperature O∞= 1,
while only if all energetic constraints are satisﬁed O0 = 1 and O∞= 0, thus allowing the
classiﬁcation of the states. When used on our test sets, the model performs the classiﬁcation
task with a 100% accuracy, which means that all the high temperature states in the test
set contain least one unsatisﬁed plaquette. Note that the classiﬁcation error for this task is
expected to be exponentially small in the volume of the system, since at inﬁnte temperature
the ground states appear with exponentially small probability. Having distilled the model’s
basic ingredients, we proceed to train an analogue model numerically starting from random
weights and biases Wyxsf, Wo, bc, and bo. Further, we replace the perceptron nonlinearities
by ReLu units and a softmax output layer to enable a reliable numerical training. After the
training, the model performs the classiﬁcation task with a 100% accuracy on the test sets,
as expected.
As a consequence of the classiﬁcation scheme provided by the analytical toy model, we
observe that the values of the zero-temperature neuron O0 behave exactly like the amplitudes of one of the ground states of the toric code written in the σz basis .
ground state described by O0 is a linear combination of all 4 ground states with well
deﬁned parity on the torus.
More precisely, such a state can be written as |ψtoric⟩=
σz1,...,σzN O0(σz1...σzN)|σz1...σzN⟩, where the spin conﬁgurations σzi = ±1, and O0(σz1...σzN)
corresponds to the value of O0 after a feed-forward pass of the neural network for a given
a input conﬁguration σz 1, ..., σz N. Our model bears resemblance with the construction of
the ground state of the toric code in terms of projected entangled pair states in that local
tensors project out states containing plaquettes with odd parity . These observations
suggest that convolutional neural networks have the potential to represent ground states
with topological order.
 X. Wen, Quantum Field Theory of Many-Body Systems:From the Origin of Sound to an Origin
of Light and Electrons: From the Origin of Sound to an Origin of Light and Electrons, Oxford
Graduate Texts .
 A. Avella, F. Mancini, Strongly Correlated Systems: Numerical Methods, Springer Series in
Solid-State Sciences .
 A. W. Sandvik, AIP Conference Proceedings 1297 .
 Y. B. Ian Goodfellow, A. Courville, Deep learning . Book in preparation for MIT Press.
 J. Bergstra, et al., Proceedings of the Python for Scientiﬁc Computing Conference (SciPy)
 . Oral Presentation.
 M. Abadi, et al., TensorFlow: Large-scale machine learning on heterogeneous systems .
Software available from tensorﬂow.org.
 A. Kitaev, Annals of Physics 303, 2 .
 R. Islam, et al., Nature 528, 77 . Article.
 M. Levin, X.-G. Wen, Phys. Rev. Lett. 96, 110405 .
 A. Kitaev, J. Preskill, Phys. Rev. Lett. 96, 110404 .
 L. Onsager, Phys. Rev. 65, 117 .
 L.-F. m. c. Arsenault, A. Lopez-Bezanilla, O. A. von Lilienfeld, A. J. Millis, Phys. Rev. B 90,
155136 .
 A. G. Kusne, et al., Scientiﬁc Reports 4, 6367 EP . Article.
 S. V. Kalinin, B. G. Sumpter, R. K. Archibald, Nat Mater 14, 973 . Progress Article.
 L. M. Ghiringhelli, J. Vybiral, S. V. Levchenko, C. Draxl, M. Scheﬄer, Phys. Rev. Lett. 114,
105503 .
 S. S. Schoenholz, E. D. Cubuk, D. M. Sussman, E. Kaxiras, A. J. Liu, Nat Phys 12, 469
 . Letter.
 P. Mehta, D. J. Schwab, arXiv:1410.3831 .
 D. Kingma, J. Ba, arXiv:1412.6980 .
 G. F. Newell, Phys. Rev. 79, 876 .
 J. B. Kogut, Rev. Mod. Phys. 51, 659 .
 C. Castelnovo, C. Chamon, Phys. Rev. B 76, 174416 .
 Y. Lecun, L. Bottou, Y. Bengio, P. Haﬀner, Proceedings of the IEEE , pp. 2278–2324.
 J. R. B. K. R. M. Mohammad H. Amin, Evgeny Andriyash, arXiv:1601.02036 .
 O. Landon-Cardinal, D. Poulin, New Journal of Physics 14, 085004 .
 W. S. Bakr, J. I. Gillen, A. Peng, S. Folling, M. Greiner, Nature 462, 74 .
 L. W. Cheuk, et al., Phys. Rev. Lett. 114, 193001 .
 L. van der Maaten, G. Hinton, Journal of Machine Learning Research 9: 25792605 .
 C. Fern´andez-Gonz´alez, N. Schuch, M. M. Wolf, J. I. Cirac, D. P´erez-Garc´ıa, Phys. Rev. Lett.
109, 260401 .