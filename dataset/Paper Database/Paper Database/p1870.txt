Free-Form Image Inpainting with Gated Convolution
Jiahui Yu1
Jimei Yang2
Xiaohui Shen3
Thomas Huang1
1University of Illinois at Urbana-Champaign
2Adobe Research
3ByteDance AI Lab
Figure 1: Free-form image inpainting results by our system built on gated convolution. Each triad shows original image,
free-form input and our result from left to right. The system supports free-form mask and guidance like user sketch. It helps
user remove distracting objects, modify image layouts and edit faces in images.
We present a generative image inpainting system to complete images with free-form mask and guidance. The system is based on gated convolutions learned from millions of
images without additional labelling efforts. The proposed
gated convolution solves the issue of vanilla convolution
that treats all input pixels as valid ones, generalizes partial
convolution by providing a learnable dynamic feature selection mechanism for each channel at each spatial location
across all layers. Moreover, as free-form masks may appear
anywhere in images with any shape, global and local GANs
designed for a single rectangular mask are not applicable.
Thus, we also present a patch-based GAN loss, named SN-
PatchGAN, by applying spectral-normalized discriminator
on dense image patches. SN-PatchGAN is simple in formulation, fast and stable in training. Results on automatic
image inpainting and user-guided extension demonstrate
that our system generates higher-quality and more ﬂexible results than previous methods. Our system helps user
quickly remove distracting objects, modify image layouts,
clear watermarks and edit faces. Code, demo and models
are available at: 
generative_inpainting.
1. Introduction
Image inpainting (a.k.a. image completion or image
hole-ﬁlling) is a task of synthesizing alternative contents in
missing regions such that the modiﬁcation is visually realistic and semantically correct. It allows to remove distracting
objects or retouch undesired regions in photos. It can also
be extended to tasks including image/video un-cropping, rotation, stitching, re-targeting, re-composition, compression,
super-resolution, harmonization and many others.
In computer vision, two broad approaches to image inpainting exist: patch matching using low-level image features and feed-forward generative models with deep convolutional networks. The former approach can synthesize plausible stationary textures, but usually makes critical failures in non-stationary cases like complicated scenes,
faces and objects. The latter approach can exploit semantics learned
from large scale datasets to synthesize contents in nonstationary images in an end-to-end fashion.
However, deep generative models based on vanilla conarXiv:1806.03589v2 [cs.CV] 22 Oct 2019
volutions are naturally ill-ﬁtted for image hole-ﬁlling because the spatially shared convolutional ﬁlters treat all
input pixels or features as same valid ones.
ﬁlling, the input to each layer are composed of valid pixels/features outside holes and invalid ones in masked regions. Vanilla convolutions apply same ﬁlters on all valid,
invalid and mixed (for example, the ones on hole boundary)
pixels/features, leading to visual artifacts such as color discrepancy, blurriness and obvious edge responses surrounding holes when tested on free-form masks .
To address this limitation, recently partial convolution is proposed where the convolution is masked and
normalized to be conditioned only on valid pixels. It is then
followed by a rule-based mask-update step to update valid
locations for next layer. Partial convolution categorizes all
input locations to be either invalid or valid, and multiplies a
zero-or-one mask to inputs throughout all layers. The mask
can also be viewed as a single un-learnable feature gating
channel1. However this assumption has several limitations.
First, considering the input spatial locations across different layers of a network, they may include (1) valid pixels in
input image, (2) masked pixels in input image, (3) neurons
with receptive ﬁeld covering no valid pixel of input image,
(4) neurons with receptive ﬁeld covering different number
of valid pixels of input image (these valid image pixels may
also have different relative locations), and (5) synthesized
pixels in deep layers. Heuristically categorizing all locations to be either invalid or valid ignores these important
information. Second, if we extend to user-guided image inpainting where users provide sparse sketch inside the mask,
should these pixel locations be considered as valid or invalid? How to properly update the mask for next layer?
Third, for partial convolution the “invalid” pixels will progressively disappear layer by layer and the rule-based mask
will be all ones in deep layers. However, to synthesize pixels in hole these deep layers may also need the information of whether current locations are inside or outside the
hole. The partial convolution with all-ones mask cannot
provide such information. We will show that if we allow
the network to learn the mask automatically, the mask may
have different values based on whether current locations are
masked or not in input image, even in deep layers.
We propose gated convolution for free-form image inpainting.
It learns a dynamic feature gating mechanism
for each channel and each spatial location (for example,
inside or outside masks, RGB channels or user-guidance
channels). Speciﬁcally we consider the formulation where
the input feature is ﬁrstly used to compute gating values
g = σ(wgx) (σ is sigmoid function, wg is learnable param-
1Applying mask before convolution or after is equivalent when convolutions are stacked layer-by-layer in neural networks. Because the output
of current layer is the input to next layer and the masked region of input
image is already ﬁlled with zeros.
eter). The ﬁnal output is a multiplication of learned feature
and gating values y = φ(wx)⊙g where φ can be any activation function. Gated convolution is easy to implement and
performs signiﬁcantly better when (1) the masks have arbitrary shapes and (2) the inputs are no longer simply RGB
channels with a mask but also have conditional inputs like
sparse sketch. For network architectures, we stack gated
convolution to form an encoder-decoder network following . Our inpainting network also integrates contextual
attention module within same reﬁnement network to
better capture long-range dependencies.
Without compromise of performance, we also signiﬁcantly simplify training objectives as two terms: a pixelwise reconstruction loss and an adversarial loss. The modiﬁcation is mainly designed for free-form image inpainting. As the holes may appear anywhere in images with
any shape, global and local GANs designed for a single rectangular mask are not applicable. Instead, we propose a variant of generative adversarial networks, named
SN-PatchGAN, motivated by global and local GANs ,
MarkovianGANs , perceptual loss and recent work
on spectral-normalized GANs . The discriminator of
SN-PatchGAN directly computes hinge loss on each point
of the output map with format Rh×w×c, formulating h ×
w × c number of GANs focusing on different locations and
different semantics (represented in different channels). SN-
PatchGAN is simple in formulation, fast and stable in training and produces high-quality inpainting results.
Comparison of different approaches including
PatchMatch , Global&Local , ContextAttention ,
PartialConv and our approach. The comparison of image inpainting is based on four dimensions: Semantic Understanding, Non-Local Algorithm, Free-Form Masks and
User-Guided Option.
User-guided
For practical image inpainting tools, enabling user interactivity is crucial because there could exist many plausible solutions for ﬁlling a hole in an image. To this end,
we present an extension to allow user sketch as guided input. Comparison to other methods is summarized in Table 1. Our main contributions are as follows: (1) We introduce gated convolution to learn a dynamic feature selection
mechanism for each channel at each spatial location across
all layers, signiﬁcantly improving the color consistency and
inpainting quality of free-form masks and inputs. (2) We
present a more practical patch-based GAN discriminator,
SN-PatchGAN, for free-form image inpainting. It is simple,
fast and produces high-quality inpainting results. (3) We
extend our inpainting model to an interactive one, enabling
user sketch as guidance to obtain more user-desired inpainting results. (4) Our proposed inpainting system achieves
higher-quality free-form inpainting than previous state of
the arts on benchmark datasets including Places2 natural
scenes and CelebA-HQ faces. We show that the proposed
system helps user quickly remove distracting objects, modify image layouts, clear watermarks and edit faces in images.
2. Related Work
2.1. Automatic Image Inpainting
A variety of approaches have been proposed for image
inpainting. Traditionally, patch-based algorithms progressively extend pixels close to the hole boundaries based
on low-level features (for example, features of mean square
difference on RGB space), to search and paste the most
similar image patch. These algorithms work well on stationary textural regions but often fail on non-stationary images. Further, Simakov et al. propose bidirectional similarity synthesis approach to better capture and summarize
non-stationary visual data. To reduce the high cost of memory and computation during search, tree-based acceleration
structures of memory and randomized algorithms 
are proposed. Moreover, inpainting results are improved
by matching local features like image gradients and
offset statistics of similar patches . Recently, image inpainting systems based on deep learning are proposed to
directly predict pixel values inside masks. A signiﬁcant advantage of these models is the ability to learn adaptive image features for different semantics. Thus they can synthesize more visually plausible contents especially for images
like faces , objects and natural scenes .
Among all these methods, Iizuka et al. propose a fully
convolutional image inpainting network with both global
and local consistency to handle high-resolution images on
a variety of datasets . This approach, however,
still heavily relies on Poisson image blending with traditional patch-based inpainting results . Yu et al. 
propose an end-to-end image inpainting model by adopting stacked generative networks to further ensure the color
and texture consistence of generated regions with surroundings. Moreover, for capturing long-range spatial dependencies, contextual attention module is proposed and integrated into networks to explicitly borrow information from
distant spatial locations. However, this approach is mainly
trained on large rectangular masks and does not generalize
well on free-form masks. To better handle irregular masks,
partial convolution is proposed where the convolution
is masked and re-normalized to utilize valid pixels only. It
is then followed by a rule-based mask-update step to recompute new masks layer by layer.
2.2. Guided Image Inpainting and Synthesis
To improve image inpainting, user guidance is explored
including dots or lines , structures , transformation or distortion information and image exemplars . Notably, Hays and Efros ﬁrst
utilize millions of photographs as a database to search for
an example image which is most similar to the input, and
then complete the image by cutting and pasting the corresponding regions from the matched image.
Recent advances in conditional generative networks empower user-guided image processing, synthesis and manipulation learned from large-scale datasets. Here we selectively review several related work. Zhang et al. propose colorization networks which can take user guidance
as additional inputs.
Wang et al. propose to synthesize high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks. The Scribbler explore a deep generative network conditioned on sketched boundaries and sparse color
strokes to synthesize cars, bedrooms, or faces.
2.3. Feature-wise Gating
Feature-wise gating has been explored widely in vision , language , speech and many
other tasks. For examples, Highway Networks utilize
feature gating to ease gradient-based training of very deep
Squeeze-and-Excitation Networks re-calibrate
feature responses by explicitly multiplying each channel
with learned sigmoidal gating values.
WaveNets 
achieve better results by employing a special feature gating
y = tanh(w1x)·sigmoid(w2x) for modeling audio signals.
3. Approach
In this section, we describe our approach from bottom to
top. We ﬁrst introduce the details of the Gated Convolution,
SN-PatchGAN, and then present the overview of inpainting
network in Figure 3 and our extension to allow optional user
3.1. Gated Convolution
We ﬁrst explain why vanilla convolutions used in are ill-ﬁtted for the task of free-form image inpainting.
We consider a convolutional layer in which a bank of ﬁlters
are applied to the input feature map as output. Assume input
is C−channel, each pixel located at (y, x) in C′−channel
output map is computed as
h+i,k′w+j · Iy+i,x+j,
where x, y represents x-axis, y-axis of output map, kh and
kw is the kernel size (e.g. 3 × 3), k′
, W ∈Rkh×kw×C′×C represents convolutional ﬁlters,
Iy+i,x+j ∈RC and Oy,x ∈RC′ are inputs and outputs. For
simplicity, the bias in convolution is ignored.
The equation shows that for all spatial locations (y, x),
the same ﬁlters are applied to produce the output in vanilla
convolutional layers. This makes sense for tasks such as image classiﬁcation and object detection, where all pixels of
input image are valid, to extract local features in a slidingwindow fashion. However, for image inpainting, the input
are composed of both regions with valid pixels/features outside holes and invalid pixels/features (in shallow layers) or
synthesized pixels/features (in deep layers) in masked regions. This causes ambiguity during training and leads to
visual artifacts such as color discrepancy, blurriness and obvious edge responses during testing, as reported in .
Recently partial convolution is proposed which
adapts a masking and re-normalization step to make the
convolution dependent only on valid pixels as
P P W · (I ⊙
if sum(M) > 0
in which M is the corresponding binary mask, 1 represents
pixel in the location (y, x) is valid, 0 represents the pixel
is invalid, ⊙denotes element-wise multiplication.
each partial convolution operation, the mask-update step
is required to propagate new M with the following rule:
y,x = 1, iff sum(M) > 0.
Partial convolution improves the quality of inpainting on irregular mask, but it still has remaining issues: (1) It
heuristically classiﬁes all spatial locations to be either valid
or invalid. The mask in next layer will be set to ones no
matter how many pixels are covered by the ﬁlter range in
previous layer (for example, 1 valid pixel and 9 valid pixels
are treated as same to update current mask). (2) It is incompatible with additional user inputs. We aim at a user-guided
image inpainting system where users can optionally provide
sparse sketch inside the mask as conditional channels. In
this situation, should these pixel locations be considered as
valid or invalid? How to properly update the mask for next
layer? (3) For partial convolution the invalid pixels will progressively disappear in deep layers, gradually converting all
mask values to ones. However, our study shows that if we
allow the network to learn optimal mask automatically, the
network assigns soft mask values to every spatial locations
even in deep layers. (4) All channels in each layer share the
same mask, which limits the ﬂexibility. Essentially, partial
convolution can be viewed as un-learnable single-channel
feature hard-gating.
We propose gated convolution for image inpainting network, as shown in Figure 2. Instead of hard-gating mask
Binary Mask
Binary Mask
Rule-based
Rule-based
Soft Gating
Soft Gating
Learnable Gating/Feature
Rule-based Binary Mask
Figure 2: Illustration of partial convolution (left) and gated
convolution (right).
updated with rules, gated convolutions learn soft mask automatically from data. It is formulated as:
Gatingy,x =
Featurey,x =
Oy,x = φ(Featurey,x) ⊙σ(Gatingy,x)
where σ is sigmoid function thus the output gating values
are between zeros and ones. φ can be any activation functions (for examples, ReLU, ELU and LeakyReLU). Wg and
Wf are two different convolutional ﬁlters.
The proposed gated convolution learns a dynamic feature selection mechanism for each channel and each spatial
Interestingly, visualization of intermediate gating values show that it learns to select the feature not only
according to background, mask, sketch, but also considering semantic segmentation in some channels. Even in deep
layers, gated convolution learns to highlight the masked regions and sketch information in separate channels to better
generate inpainting results.
3.2. Spectral-Normalized Markovian Discriminator (SN-PatchGAN)
For previous inpainting networks which try to ﬁll a single rectangular hole, an additional local GAN is used on
the masked rectangular region to improve results .
However, we consider the task of free-form image inpainting where there may be multiple holes with any shape at any
location. Motivated by global and local GANs , MarkovianGANs , perceptual loss and recent work on
spectral-normalized GANs , we present a simple and
effective GAN loss, SN-PatchGAN, for training free-form
image inpainting networks.
A convolutional network is used as the discriminator
where the input consists of image, mask and guidance channels, and the output is a 3-D feature of shape Rh×w×c (h,
w, c representing the height, width and number of channels
respectively). As shown in Figure 3, six strided convolutions with kernel size 5 and stride 2 is stacked to captures
Coarse Result
Coarse Network (Stage I)
: Gated Convolution
: Dilated Gated Convolution
RGB Channels
Mask Channel
Sketch Channel
Two Branch Refinement Network with
Contextual Attention (Stage II)
: Contextual Attention
Inpainting Result
Fully Convolutional Spectral-Normalized Markovian Discriminator
Real or Fake
Real or Fake
Real or Fake
GAN Loss on Each Neuron
: Convolution
Figure 3: Overview of our framework with gated convolution and SN-PatchGAN for free-form image inpainting.
the feature statistics of Markovian patches . We then
directly apply GANs for each feature element in this feature map, formulating h × w × c number of GANs focusing
on different locations and different semantics (represented
in different channels) of input image. It is noteworthy that
the receptive ﬁeld of each neuron in output map can cover
entire input image in our training setting, thus a global discriminator is not necessary.
We also adapt the recently proposed spectral normalization to further stabilize the training of GANs. We use
the default fast approximation algorithm of spectral normalization described in SN-GANs . To discriminate if the
input is real or fake, we also use the hinge loss as objective
function for generator LG = −Ez∼Pz(z)[Dsn(G(z))] and
discriminator LDsn = Ex∼Pdata(x)[ReLU(1 −Dsn(x))] +
Ez∼Pz(z)[ReLU(1 + Dsn(G(z)))] where Dsn represents
spectral-normalized discriminator, G is image inpainting
network that takes incomplete image z.
With SN-PatchGAN, our inpainting network trains faster
and more stable than baseline model .
Perceptual
loss is not used since similar patch-level information is already encoded in SN-PatchGAN. Compared with Partial-
Conv in which 6 different loss terms and balancing
hyper-parameters are used, our ﬁnal objective function for
inpainting network is only composed of pixel-wise ℓ1 reconstruction loss and SN-PatchGAN loss, with default loss
balancing hyper-parameter as 1 : 1.
3.3. Inpainting Network Architecture
We customize a generative inpainting network with
the proposed gated convolution and SN-PatchGAN loss.
Speciﬁcally, we adapt the full model architecture in 
with both coarse and reﬁnement networks. The full framework is summarized in Figure 3.
For coarse and reﬁnement networks, we use a simple
encoder-decoder network instead of U-Net used in PartialConv .
We found that skip connections in a U-
Net have no signiﬁcant effect for non-narrow mask.
This is mainly because for center of a masked region, the
inputs of these skip connections are almost zeros thus cannot propagate detailed color or texture information to the
decoder of that region. For hole boundaries, our encoderdecoder architecture equipped with gated convolution is
sufﬁcient to generate seamless results.
We replace all vanilla convolutions with gated convolutions . One potential problem is that gated convolutions
introduce additional parameters. To maintain the same ef-
ﬁciency with our baseline model , we slim the model
width by 25% and have not found obvious performance
drop both quantitatively and qualitatively. The inpainting
network is trained end-to-end and can be tested on free-form
holes at arbitrary locations. Our network is fully convolutional and supports different input resolutions in inference.
3.4. Free-Form Mask Generation
The algorithm to automatically generate free-form
masks is important and non-trivial. The sampled masks,
in essence, should be (1) similar to masks drawn in real
use-cases, (2) diverse to avoid over-ﬁtting, (3) efﬁcient in
computation and storage, (4) controllable and ﬂexible. Previous method collects a ﬁxed set of irregular masks
from an occlusion estimation method between two consecutive frames of videos. Although random dilation, rotation
and cropping are added to increase its diversity, the method
does not meet other requirements listed above.
We introduce a simple algorithm to automatically generate random free-form masks on-the-ﬂy during training. For
the task of hole ﬁlling, users behave like using an eraser to
brush back and forth to mask out undesired regions. This
behavior can be simply simulated with a randomized algorithm by drawing lines and rotating angles repeatedly. To
ensure smoothness of two lines, we also draw a circle in
joints between the two lines. More details are included in
the supplementary materials due to space limit.
3.5. Extension to User-Guided Image Inpainting
We use sketch as an example user guidance to extend our
image inpainting network as a user-guided system. Sketch
(or edge) is simple and intuitive for users to draw. We show
both cases with faces and natural scenes. For faces, we extract landmarks and connect related landmarks. For natural
scene images, we directly extract edge maps using the HED
edge detector and set all values above a certain threshold (i.e. 0.6) to ones. Sketch examples are shown in the
supplementary materials due to space limit.
For training the user-guided image inpainting system, intuitively we will need additional constraint loss to enforce
the network generating results conditioned on the user guidance. However with the same combination of pixel-wise
reconstruction loss and GAN loss (with conditional channels as input to the discriminator), we are able to learn
conditional generative network in which the generated results respect user guidance faithfully.
We also tried to
use additional pixel-wise loss on HED output features
with the raw image or the generated result as input to enforce constraints, but the inpainting quality is similar. The
user-guided inpainting model is separately trained with a
5-channel input (R,G,B color channels, mask channel and
sketch channel).
4. Results
We evaluate the proposed free-form image inpainting
system on Places2 and CelebA-HQ faces . Our
model has totally 4.1M parameters, and is trained with TensorFlow v1.8, CUDNN v7.0, CUDA v9.0.
For testing,
it runs at 0.21 seconds per image on single NVIDIA(R)
Tesla(R) V100 GPU and 1.9 seconds on Intel(R) Xeon(R)
CPU @ 2.00GHz for images of resolution 512 × 512 on
average, regardless of hole size.
4.1. Quantitative Results
As mentioned in , image inpainting lacks good quantitative evaluation metrics. Nevertheless, we report in Table 2 our evaluation results in terms of mean ℓ1 error and
mean ℓ2 error on validation images of Places2, with both
center rectangle mask and free-form mask. As shown in the
table, learning-based methods perform better than Patch-
Match in terms of mean ℓ1 and ℓ2 errors. Moreover, partial convolution implemented within the same framework
obtains worse performance, which may due to un-learnable
rule-based gating.
Table 2: Results of mean ℓ1 error and mean ℓ2 error on validation images of Places2 with both rectangle masks and
free-form masks. Both PartialConv* and ours are trained
on same random combination of rectangle and free-form
masks. No edge guidance is utilized in training/inference
to ensure fair comparison. * denotes our implementation
within the same framework due to unavailability of ofﬁcial
implementation and models.
rectangular mask
free-form mask
PatchMatch 
Global&Local 
ContextAttention 
PartialConv* 
4.2. Qualitative Comparisons
Next, we compare our model with previous state-of-theart methods . Figure 4 and Figure 5 shows automatic and user-guided inpainting results with several representative images. For automatic image inpainting, the result of PartialConv is obtained from its online demo2. For
user-guided image inpainting, we train PartialConv* with
the exact same setting of GatedConv, expect the convolution types (sketch regions are treated as valid pixels for rulebased mask updating). For all learning-based methods, no
post-processing step is performed to ensure fairness.
2 
Figure 4: Example cases of qualitative comparison on the Places2 and CelebA-HQ validation sets. More comparisons are
included in supplementary materials due to space limit. Best viewed (e.g., shadows in uniform region) with zoom-in.
Figure 5: Comparison of user-guided image inpainting.
As reported in , simple uniform region (last row
of Figure 4 and Figure 5) are hard cases for learningbased image inpainting networks. Previous methods with
vanilla convolution have obvious visual artifacts and edge
responses in/surrounding holes. PartialConv produces better results but still exhibits observable color discrepancy.
Our method based on gated convolution obtains more visually pleasing results without noticeable color inconsistency.
In Figure 5, given sparse sketch, our method produces realistic results with seamless boundary transitions.
4.3. Object Removal and Creative Editing
Moreover, we study two important real use cases of image inpainting: object removal and creative editing.
Object Removal.
In the ﬁrst example, we try to remove the distracting person in Figure 6.
We compare
our method with commercial product Photoshop (based on
PatchMatch ) and the previous state-of-the-art generative inpainting network (ofﬁcial released model trained on
Places2) . The results show that Content-Aware Fill
function from Photoshop incorrectly copies half of face
from left.
This example reﬂects the fact that traditional
methods without learning from large-scale data ignore the
semantics of an image, which leads to critical failures
in non-stationary/complicated scenes. For learning-based
methods with vanilla convolution , artifacts exist near
hole boundaries.
Creative Editing. Next we study the case where user interacts with the inpainting system to produce more desired
results. The examples on both faces and natural scenes are
shown in Figure 7. Our inpainting results nicely follow the
user sketch, which is useful for creatively editing image lay-
Figure 6: Object removal case study with comparison.
Figure 7: Examples of user-guided inpainting/editing of
faces and natural scenes.
outs, faces and many others.
4.4. User Study
We performed a user study by ﬁrst collecting 30 test images (with holes but no sketches) from Places2 validation
dataset without knowing their inpainting results on each
We then computed results of the following four
methods for comparison: (1) ground truth, (2) our model,
(3) re-implemented PartialConv within same framework, and (4) ofﬁcial PartialConv . We did two types
of user study. (A) We evaluate each method individually
to rate the naturalness/inpainting quality of results (from
1 to 10, the higher the better), and (B) we compare our
model and the ofﬁcial PartialConv model to evaluate which
method produces better results. 104 users ﬁnished the user
study with the results shown as follows.
(A) Naturalness: (1) 9.89, (2) 7.72, (3) 7.07, (4) 6.54
(B) Pairwise comparison of (2) our model vs. (4) ofﬁcial
PartialConv model: 79.4% vs. 20.6% (the higher the better).
4.5. Ablation Study of SN-PatchGAN
Figure 8: Ablation study of SN-PatchGAN. From left to
right, we show original image, masked input, results with
one global GAN and our results with SN-PatchGAN.
SN-PatchGAN is proposed for the reason that free-form
masks may appear anywhere in images with any shape. Previously introduced global and local GANs designed
for a single rectangular mask are not applicable. We provide ablation experiments of SN-PatchGAN in the context
of image inpainting in Figure 8. SN-PatchGAN leads to signiﬁcantly better results, which veriﬁes that (1) one vanilla
global discriminator has worse performance , and (2)
GAN with spectral normalization has better stability and
performance . Although introducing more loss functions may help in training free-form image inpainting networks , we demonstrate that a simple combination of
SN-PatchGAN loss and pixel-wise ℓ1 loss, with default loss
balancing hyper-parameter as 1:1, produces photo-realistic
inpainting results. More comparison examples are shown in
the supplementary materials.
5. Conclusions
We presented a novel free-form image inpainting system
based on an end-to-end generative network with gated convolution, trained with pixel-wise ℓ1 loss and SN-PatchGAN.
We demonstrated that gated convolutions signiﬁcantly improve inpainting results with free-form masks and user
guidance input.
We showed user sketch as an exemplar
guidance to help users quickly remove distracting objects,
modify image layouts, clear watermarks, edit faces and
interactively create novel objects in photos. Quantitative
results, qualitative comparisons and user studies demonstrated the superiority of our proposed free-form image inpainting system.