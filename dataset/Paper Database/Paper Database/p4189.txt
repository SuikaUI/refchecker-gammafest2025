Joint Deep Modeling of Users and Items Using Reviews for
Recommendation
Department of Computer
University of Illinois at Chicago
Chicago, U.S.
 
Vahid Noroozi
Department of Computer
University of Illinois at Chicago
Chicago, U.S.
 
Philip S. Yu
Department of Computer
University of Illinois at Chicago
Chicago, U.S.
 
A large amount of information exists in reviews written by
users. This source of information has been ignored by most
of the current recommender systems while it can potentially
alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to
learn item properties and user behaviors jointly from review
text. The proposed model, named Deep Cooperative Neural
Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses
on learning user behaviors exploiting reviews written by the
user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on
the top to couple these two networks together. The shared
layer enables latent factors learned for users and items to
interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate
that DeepCoNN signiﬁcantly outperforms all baseline recommender systems on a variety of datasets.
CCS Concepts
•Information systems →Collaborative ﬁltering; Recommender systems; •Computing methodologies →
Neural networks;
Recommender Systems, Deep Learning, Convolutional Neural Networks, Rating Prediction
INTRODUCTION
The variety and number of products and services provided
by companies have increased dramatically during the last
Companies produce a large number of products
to meet the needs of customers. Although this gives more
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
WSDM 2017, February 06-10, 2017, Cambridge, United Kingdom
⃝2017 ACM. ISBN 978-1-4503-4675-7/17/02. . . $15.00
DOI: 
options to customers, it makes it harder for them to process the large amount of information provided by companies.
Recommender systems help customers by presenting products or services that are likely of interest to them based on
their preferences, needs, and past buying behaviors. Nowadays, many people use recommender systems in their daily
life such as online shopping, reading articles, and watching
Many of the prominent approaches employed in recommender systems are based on Collaborative Filtering
(CF) techniques. The basic idea of these techniques is that
people who share similar preferences in the past tend to have
similar choices in the future. Many of the most successful
CF techniques are based on matrix factorization . They
ﬁnd common factors that can be the underlying reasons for
the ratings given by users. For example, in a movie recommender system, these factors can be genre, actors, or director of movies that may aﬀect the rating behavior of users.
Matrix factorization techniques not only ﬁnd these hidden
factors, but also learn their importance for each user and
how each item satisﬁes each factor.
Although CF techniques have shown good performance
for many applications, the sparsity problem is considered as
one of their signiﬁcant challenges . The sparsity problem
arises when the number of items rated by users is insigniﬁcant to the total number of items. It happens in many real
applications. It is not easy for CF techniques to recommend
items with few ratings or to give recommendations to the
users with few ratings.
One of the approaches employed to address this lack of
data is using the information in review text .
many recommender systems, other than the numeric ratings, users can write reviews for the products. Users explain
the reasons behind their ratings in text reviews.
The reviews contain information which can be used to alleviate
sparsity problem. One of the drawbacks of most current CF
techniques is that they model users and items just based on
the numeric ratings provided by users and ignore the abundant information existed in the review text. Recently, some
studies have shown that using review text can improve the prediction accuracy of recommender systems, in
particular for the items and users with few ratings .
In this paper, we propose a neural network (NN) based
model, named Deep Cooperative Neural Networks (Deep-
CoNN), to model users and items jointly using review text
for rating prediction problems. The proposed model learns
hidden latent features for users and items jointly using two
coupled neural networks such that the rating prediction accuracy is maximized. One of the networks models user behavior using the reviews written by the user, and the other
network models item properties using the written reviews
for the item. The learned latent features for user and item
are used to predict the corresponding rating in a layer introduced on the top of both networks. This interaction layer
is motivated by matrix factorization techniques to let
latent factors of users and items interact with each other.
To the best of our knowledge, DeepCoNN is the ﬁrst deep
model that represents both users and items in a joint manner using reviews.
It makes the model scalable and also
suitable for online learning scenarios where the model needs
to get updated continuously with new data. Another key
contribution is that DeepCoNN represents review text using pre-trained word-embedding technique to extract semantic information from the reviews. Recently, this
representation has shown excellent results in many Natural Language Processing (NLP) tasks . Moreover, a
signiﬁcant advantage of DeepCoNN compared to most other
approaches which beneﬁt from reviews is that it
models users and items in a joint manner with respect to
prediction accuracy. Most of the similar algorithms perform
the modeling independently of the ratings. Therefore, there
is no guarantee that the learned factors can be beneﬁcial to
the rating prediction.
The experiments on real-world datasets including Yelp,
Amazon , and Beer show that DeepCoNN outperforms all the compared baselines in prediction accuracy. Also,
the proposed algorithm increases the performance for users
and items with fewer ratings more than the ones with a
higher number of ratings. It shows that DeepCoNN alleviates the sparsity problem by leveraging review text.
Our contributions and also advantages of DeepCoNN can
be summarized as follows:
• The proposed Deep Cooperative Neural Networks (Deep-
CoNN) jointly model user behaviors and item properties using text reviews. The extra shared layer at the
top of two neural networks connects the two parallel
networks such that user and item representations can
interact with each other to predict ratings.
best of our knowledge, DeepCoNN is the ﬁrst one that
jointly models both user and item from reviews using
neural networks.
• It represents review text as word-embeddings using
pre-trained deep models.
The experimental results
demonstrate that the semantic meaning and sentimental attitudes of reviews in this representation can increase the accuracy of rating prediction. All competing
techniques which are based on topic modeling use the traditional bag of words techniques.
• It does not only alleviate the problem of sparsity by
leveraging reviews, but also improves the overall performance of the system signiﬁcantly. It outperforms
state-of-the-art techniques in terms of
prediction accuracy on all of the evaluated datasets including Yelp, 21 categories of Amazon, and Beer (see
Section 3).
The rest of the paper is organized as follows. In Section
2, we describe DeepCoNN in detail. Experiments are presented in Section 3 to analyze DeepCoNN and demonstrate
its eﬀectiveness compared to the state-of-the-art techniques
for recommendation systems. In Section 4, we give a short
review of the works related to our study. Finally, conclusions
are presented in Section 5.
METHODOLOGY
The proposed model, DeepCoNN, is described in detail in
this section.
DeepCoNN models user behaviors and item
properties using reviews.
It learns hidden latent factors
for users and items by exploiting review text such that the
learned factors can estimate the ratings given by users. It
is done with a CNN based model consisting of two parallel
neural networks, coupled to each other with a shared layer
at the top. The networks are trained in a joint manner to
predict the ratings with minimum prediction error. We ﬁrst
describe notations used throughout this paper and formulate the deﬁnition of our problem. Then, the architecture of
DeepCoNN and the objective function to get optimized is
explained. Finally, we describe how to train this model.
Deﬁnition and Notation
A set of training set T consists of N tuples. Each tuple
(u, i, rui, wui) denotes a review written by user u for item i
with rating rui and text review of wui. The mathematical
notations used in this paper are summarized in Table 1.
Table 1: Notations
Deﬁnitions and Descriptions
user or item u’s review text consisting of n
word vectors of user or item u
a review text written by user u for item i
the output of jth neuron in the convolutional
the number of neurons in the layer i
the jth kernel in the convolutional layer
the bias of jth convolutional kernel
the bias of the fully connected layer
the jth feature map in the convolutional layer
the weight matrix of the fully connected layer
the window size of convolutional kernel
the dimension of word embedding
the output of Netu
the output of Neti
the learning rate
Architecture
The architecture of the proposed model for rating prediction is shown in Figure 1. The model consists of two parallel
neural networks coupled in the last layer, one network for
users (Netu) and one network for items (Neti). User reviews
and item reviews are given to Netu and Neti respectively
as inputs, and corresponding rating is produced as the output. In the ﬁrst layer, denoted as look-up layer, review text
for users or items are represented as matrices of word embeddings to capture the semantic information in the review
text. Next layers are the common layers used in CNN based
models to discover multiple levels of features for users and
items, including convolution layer, max pooling layer, and
fully connected layer. Also, a top layer is added on the top
of the two networks to let the hidden latent factors of user
and item interact with each other. This layer calculates an
Loss Function
User Review Text
Item Review Text
Fully Connected
Convolution
Max-pooling
Convolution
Max-pooling
Fully-connected
Figure 1: The architecture of the proposed model
objective function that measures the rating prediction error using the latent factors produced by Netu and Neti. In
the following subsections, since Netu and Neti only diﬀer
in their inputs, we focus on illustrating the process for Netu
in detail. The same process is applied for Neti with similar
Word Representation
A word embedding f : M →ℜn, where M represents the
dictionary of words, is a parameterized function mapping
words to n-dimensional distributed vectors. Recently, this
approach has boosted the performance in many NLP applications .
DeepCoNN uses this representation technique to exploit the semantics of reviews. In the look-up
layer, reviews are represented as a matrix of word embeddings to extract their semantic information. To achieve it,
all the reviews written by user u, denoted as user reviews,
are merged into a single document du
1:n, consisting of n words
in total. Then, a matrix of word vectors, denoted as V u
built for user u as follows:
1:n = φ(du
k indicates the kth word of document du
1:n, lookup function φ(du
k) returns the corresponding c-dimensional
word vector for the word du
k, and ⊕is the concatenation
operator. It should be considered that the order of words is
preserved in matrix V u
1:n that is another advantage of this
representation comparing to bag-of-words techniques.
CNN Layers
Next layers including convolution layer, max pooling, and
fully connected layer follow the CNN model introduced in
 . Convolution layer consists of m neurons which produce
new features by applying convolution operator on word vectors V u
1:n of user u. Each neuron j in the convolutional layer
uses ﬁlter Kj ∈ℜc×t on a window of words with size t. For
1:n, we perform a convolution operation regarding each kernel Kj in the convolutional layer.
zj = f(V u
1:n ∗Kj + bj)
Here symbol ∗is convolution operator, bj is a bias term and
f is an activation function. In the proposed model, we use
Rectiﬁed Linear Units (ReLUs) .
It is deﬁned as Eq.
Deep convolutional neural networks with ReLUs train
several times faster than their equivalents with tanh units
f(x) = max{0, x}
Following the work of , we then apply Eq. 4, a max pooling operation, over the feature map and take the maximum
value as the feature corresponding to this particular kernel.
The most important feature of each feature map, which has
the highest value, has been captured. This pooling scheme
can naturally deal with the varied length of the text. After
the max pooling operation, convolutional results are reduced
to a ﬁxed size vector.
oj = max{z1, z2, ..., z(n−t+1)}
We have described the process by which one feature is extracted from one kernel. The model uses multiple ﬁlters to
obtain various features and the output vector of the convolutional layer is as Eq. 5.
O = {o1, o2, o3, ..., on1},
where n1 denotes the number of kernel in the convolutional
xu = f(W × O + g)
The results from the max-pooling layer are passed to a
fully connected layer with weight matrix W . As shown in
Eq. 6, the output of the fully connected layer xu ∈ℜn2×1
is considered as features for user u. Finally, the outputs of
both user and item CNN xu and yi can be obtained.
The Shared Layer
Although these outputs can be viewed as features of users
and items, they can be in diﬀerent feature space and not
comparable. Thus, to map them into the same feature space,
we introduce a shared layer on the top to couple Netu and
Neti. First, let us concatenate xu and yi into a single vector ˆz = (xu, yi). To model all nested variable interactions
in ˆz, we introduce Factorization Machine (FM) as the
estimator of the corresponding rating. Therefore, given a
batch of N training examples T , we can write down its cost
⟨ˆvi, ˆvj⟩ˆziˆzj,
where ˆw0 is the global bias, ˆwi models the strength of the ith
variable in ˆz and ⟨ˆvi, ˆvj⟩= P|ˆz|
f=1 ˆvi,f ˆvj,f. ⟨ˆvi, ˆvj⟩models
the second order interactions.
Network Training
Our network is trained by minimizing Eq. 7. We take
derivatives of J with respect to z, as shown in Eq. 8.
∂ˆzi = ˆwi +
⟨ˆvi, ˆvj⟩ˆzj
The derivatives of other parameters in diﬀerent layers can
be computed by applying diﬀerentiation chain rule.
Given a set of training set T consisting of N tuples, we optimize the model through RMSprop over shuﬄed minibatches. RMSprop is an adaptive version of gradient descent
which adaptively controls the step size with respect to the
absolute value of the gradient.
It does it by scaling the
update value of each weight by a running average of its gradient norm. The updating rules for parameter set θ of the
networks are as the following:
rt ←0.9(∂J
√rt + ǫ)∂J
where λ is the learning rate, ǫ is a small value added for
numerical stability. Additionally, to prevent overﬁtting, the
dropout strategy has also been applied to the fully connected layers of the two networks.
Some Analysis on DeepCoNN
Word Order Preservation
Most of the recommender systems which use reviews in
the modeling process employ topic modeling techniques to
model users or items . Topic modeling techniques infer
latent topic variables using the bag-of-words assumption, in
which word order is ignored. However, in many text modeling applications, word order is crucial . DeepCoNN is
not based on topic modeling and uses word embeddings to
create a matrix of word vectors where the order of words are
preserved. In this way, convolution operations make use of
the internal structure of data and provide a mechanism for
eﬃcient use of words’ order in text modeling .
Online Learning
Scalability and handling dynamic pools of items and users
are considered as critical needs of many recommender systems. The time sensitivity of recommender systems poses
a challenge in learning latent factors in an online fashion.
DeepCoNN is scalable to the size of the training data, and
also it can easily get trained and updated with new data because it is based on NN. Updating latent factors of items or
users can get performed independently from historical data.
All the approaches which employ topic modeling techniques
do not beneﬁt from these advantages to this extent.
EXPERIMENTS
We have performed extensive experiments on a variety
of datasets to demonstrate the eﬀectiveness of DeepCoNN
compared to other state-of-the-art recommender systems.
We ﬁrst present the datasets and the evaluation metric used
in our experiments in Section 3.1. The baseline algorithms
selected for comparisons are explained in Section 3.2. Experimental settings are given in Section 3.3. Performance
evaluation and some analysis of the model are discussed in
sections 3.4 and 3.5 respectively.
Datasets and Evaluation Metric
In our experiments, we have selected the following three
datasets to evaluate our model.
• Yelp: It is a large-scale dataset consisting of restaurant reviews, introduced in the 6th round of Yelp Challenge 1 in 2015. It contains more than 1M reviews and
• Amazon: Amazon Review dataset contains product reviews and metadata from Amazon website2. It
includes more than 143.7 million reviews spanning from
May 1996 to July 2014. It has 21 categories of items,
and as far as we know, this is the largest public available rating dataset with text reviews.
• Beer: It is a beer review dataset extracted from ratebeer.com.
The data span a period of more than 10
years, including almost 3 million reviews up to November 2011 .
As we can see in Table 2, all datasets contain more than
half a million of reviews.
However, in Yelp and Amazon,
customers provide less than six pair of reviews and ratings
on average which shows these two datasets are extremely
This sparsity can largely deteriorate the performance of recommender systems.
Besides, in all datasets,
each review consists of less than 150 words on average.
In our experiments, we adopt the well-known Mean Square
Error (MSE) to evaluate the performance of the algorithms.
It is selected because most of the related works have used
the same evaluation metric . MSE can be deﬁned
as follows:
(rn −ˆrn)2,
1 
2 
Table 2: The Statistics of the datasets
#reviews per user
#words per review
34,686,770
where rn is the nth observed value, ˆrn is the nth predicted
value and N is the total number of observations.
To validate the eﬀectiveness of DeepCoNN, we have selected three categories of algorithms for evaluations:
purely rating based models.
We chose Matrix Factorization (MF) and Probabilistic Matrix Factorization (PMF) to
validate that review information is helpful for recommender
systems, (ii) topic modeling based models which use review
information. Most of the recommender systems which take
reviews into consideration are based on topic modeling techniques. To compare our model with topic modeling based
recommender systems, we select three representative models:
Latent Dirichlet Allocation (LDA) , Collaborative
Topic Regression (CTR) and Hidden Factor as Topic
(HFT) , and (iii) deep recommender systems. In ,
authors have proposed a state-of-the-art deep recommender
system named Collaborative Deep Learning (CDL). Note
that all the baselines except MF and PMF have incorporated
review information into their models to improve prediction.
• MF: Matrix Factorization is the most popular
CF-based recommendation method. It only uses rating
matrix as input and estimates two low-rank matrices
to predict ratings. In our implementation, Alternating
Least Squares (ALS) technique is adopted to minimize
its objective function.
• PMF: Probabilistic Matrix Factorization is introduced
in . It models latent factors of users and items by
Gaussian distributions.
• LDA: Latent Dirichlet Allocation is a well-known topic
modeling algorithm presented in . In , it is proposed to employ LDA to learn a topic distribution from
a set of reviews for each item. By treating the learned
topic distributions as latent features for each item, latent features for each user is estimated by optimizing
rating prediction accuracy with gradient descent.
• CTR: Collaborative Topic Regression has been proposed by . It showed very good performance on
recommending articles in a one-class collaborative ﬁltering problem where a user is either interested or not.
• HFT: Hidden Factor as Topic proposed in employs topic distributions to learn latent factors from
user or item reviews.
The authors have shown that
item speciﬁc topic distributions produce more accurate
predictions than user speciﬁc ones. Thus, we report
the results of HFT learning from item reviews.
• CDL: Collaborative Deep Learning tightly couples a
Bayesian formulation of the stacked denoising autoencoders and PMF. The middle layer of auto-encoders
serves as a bridge between auto-encoders and PMF.
number of latent factors
100 150 200 250 300 350 400
number of convolutional kernels
Figure 2: The impact of the number of latent factors and
convolutional kernels on the performance of DeepCoNN in
terms of MSE (Yelp Dataset).
Experimental Settings
We divided each dataset shown in Table 2 into three sets of
training set, validation set, and test set. We use 80% of each
dataset as the training set, 10% is treated as the validation
set to tune the hyper-parameters, and the rest is used as
the test set. All the hyper-parameters of the baselines and
DeepCoNN are selected based on the performance on the
validation set.
For MF and PMF, we used grid search to ﬁnd the best values for the number of latent factors from {25, 50, 100, 150, 200},
and regularization parameter from {0.001, 0.01, 0.1, 1.0}.
For LDA, CTR and HFT, the number of topics K is selected from {5, 10, 20, 50, 100} using the validation set. We
set K = 10 for LDA and CTR. The CTR model solves the
one-class collaborative ﬁltering problem by using two
diﬀerent values for the precision parameter c of a Gaussian
distribution. Following the work of , in our experiments,
we set precision c as the same for all the observed ratings
for rating prediction. HFT-k (k = 10, 50) are included to
show the impact of the number of latent factors for HFT.
By performing a grid search on the validation set, we set
hyper-parameters α = 0.1, λu = 0.02 and λv = 10 for CTR
and HFT. To optimize the performance of CDL, we performed a grid search on the hyper-parameters λu, λv, λn,
λw and L. Similar with CTR, the conﬁdence parameter cij
of CDL is set as the same for all observed ratings.
We empirically studied the eﬀects of two important parameters of DeepCoNN: the number of latent factors(|xu|
and |yi|) and the number of convolutional kernels: n1. In
Figure 2, we show the performance of DeepCoNN on the
validation set of Yelp with varying |xu| and |yi| from 5 to
100 and n1 from 10 to 400 to investigate its sensitivity. As it
can be seen, it does not improve the performance when the
number of latent factors and number of kernels is greater
than 50 and 100 respectively. Thus, we set |xu| = |yi| = 50
and n1 = 100. Other hyper-parameters: t, c, λ and batch
size are set as 3, 300, 0.002 and 100, respectively. These values were chosen through a grid search on the validation sets.
We used a pre-trained word embeddings which are trained
Table 4: Comparing variants of the proposed model.
Best results are indicated in bold.
Music Instruments
DeepCoNN-User
DeepCoNN-Item
DeepCoNN-TFIDF
DeepCoNN-Random
DeepCoNN-DP
on more than 100 billion words from Google News 3.
Our models are implemented in Theano , a well-known
Python library for machine learning and deep learning. The
NVIDIA CUDA Deep Neural Network4 (cuDNN v4) accelerated our training process. All models are trained and tested
on an NVIDIA Tesla K40 GPU.
Performance Evaluation
The performance of DeepCoNN and the baselines (see Section 3.2) are reported in terms of MSE in Tables 3. Table 3
shows the results on the three datasets including the performance averaged on all 21 categories of Amazon. The experiments are repeated 3 times, and the averages are reported
with the best performance shown in bold. The last column
indicates the percentage of improvements gained by Deep-
CoNN compared to the best baseline in the corresponding
In Table 3, all models perform better on Beer dataset than
on Yelp and Amazon. It is mainly related to the sparsity of
Yelp and Amazon.
Although PMF performs better than
MF on Yelp, Beer, and most categories of Amazon, both
techniques do not show good performance compared to the
ones which use reviews.
It validates our hypothesis that
review text provides additional information, and considering
reviews in models can improve rating prediction.
Although simply employing LDA to learn features from
item reviews can help the model to achieve improvements,
LDA models reviews independent of ratings.
Therefore,
there is no guarantee that the learned features can be beneﬁcial to rating prediction. Therefore, by modeling ratings and
reviews together, CTR and HFT attain additional improvements. Among those topic modeling based models (LDA,
CTR and HFT), both HFT-10 and HFT-50 perform better
in all three datasets.
With the capability of extracting deep eﬀective features
from item review text, as we can see in Table 3, CDL outperforms all topic modeling based recommender systems and
advances the state-of-the-art. However, in beneﬁting from
joint modeling capacity and semantic meaning existing from
review text, DeepCoNN beats the best baseline in Yelp, Beer
and Amazon and gains 8.3% improvement on average.
Model Analysis
Are the two parallel networks really cooperate to learn effective features from reviews? Does the proposed model beneﬁt from the use of word embedding to exploit the semantic
information in the review text? How much does the shared
layer help in improving the predcition accuracy comparing to
3 
a simpler coupling approach? To answer these questions, we
compare the DeepCoNN with its ﬁve variants: DeepCoNN-
User, DeepCoNN-Item, DeepCoNN-TFIDF, DeepCoNN-Ra
ndom and DeepCoNN-DP. These ﬁve variants are summarized in the following:
• DeepCoNN-User: The Neti of DeepCoNN is substituted with a matrix. Each row of the matrix is the
latent factors of one item. This matrix is randomly
initialized and optimized during the training.
• DeepCoNN-Item: Similar with DeepCoNN-User, the
Netu of DeepCoNN is replaced with a matrix. Each
row of the matrix is the latent factors of one user. This
matrix is randomly initialized and optimized during
the training.
• DeepCoNN-TFIDF: Instead of using word embedding, the TFIDF scheme is employed to represent review text as input to DeepCoNN.
• DeepCoNN-Random: Our baseline model where all
word representations are randomly initialized as ﬁxedlength vectors.
• DeepCoNN-DP: The factorization machine in the
objective function is substitued with a simple dot product of xu and yi.
The performance of DeepCoNN and its variants on Yelp,
Beer and one category of the Amazon dataset: Music Instruments are given in Table 4.
To demonstrate that the two deep CNNs can cooperate
with each other to learn eﬀective latent factors from user
and item reviews, DeepCoNN-User and DeepCoNN-Item are
trained with only one CNN with review text as input and
the other CNN is substituted with a list of latent variables
as the parameters to get learned. In this manner, latent factors of users or items are learned without considering their
corresponding review text. As it can be seen in Table 4,
while DeepCoNN-User and DeepCoNN-Item achieve similar
results, DeepCoNN delivers the best performance by modeling both users and items. It veriﬁes that review text is necessary for modeling latent factors of both users and items.
Also, it shows that review text has informative information
that can help to improve the performance of recommendation.
Furthermore, to validate the eﬀectiveness of word representation, we compare DeepCoNN with DeepCoNN-TFIDF
and DeepCoNN-Random. The DeepCoNN-TFIDF and DeepCoNN-Random are trained to show that word embedding
is helpful to capture semantic meaning existed in the review text. While the performance of DeepCoNN-TFIDF is
slightly better than DeepCoNN-Random, they both perform
considerably weaker than DeepCoNN. It shows the eﬀectiveness of representing review text in semantic space for modeling the latent factors of items or users.
At last, to investigate the eﬃciency of the shared layer,
DeepCoNN-DP is introduced that couples the two networks
with a simpler objective function. The comparison shows
the superiority of the factorization machine coupling. It can
be the result of not only modeling the ﬁrst order interactions
but also the second order interactions between xu and yi.
Table 3: MSE Comparison with baselines. Best results are indicated in bold.
Improvement
of DeepCoNN
Average on all datasets
number of training reviews
reduction in MSE
number of training reviews
reduction in MSE
Amazon (Music Instruments)
number of training reviews
reduction in MSE
Figure 3: MSE improvement achieved by DeepCoNN compared to MF. For users and items with diﬀerent number of training
reviews, DeepCoNN gains diﬀerent MSE reductions.
The Impact of the Number of Reviews
The cold start problem is prevalent in recommender
systems. In particular, when a new user joins or a new item
is added to the system, their available ratings are limited.
It would not be easy for the system to learn preferences
of such users just from their ratings.
It has been shown
in some of the previous works that exploiting review text
can help to alleviate this problem especially for users or
items with few ratings . In this section, we conduct a
set of experiments to answer the following questions. Can
DeepCoNN help to tackle the cold start problem? What is
the impact of the number of reviews on the eﬀectiveness of
the proposed algorithm?
In Fig. 3, we have illustrated the reductions in MSE resulted from DeepCoNN compared to MF technique on three
datasets of Yelp, Beer, and a group of Amazon (Music Instruments). By reduction in MSE, we mean the diﬀerence
between the MSE of MF and the MSE of DeepCoNN. Users
and items are categorized based on the number of their reviews, and reductions are plotted for both users and items
groups. It can be seen that in all three datasets, reductions
are positive, and DeepCoNN can achieve RMS reduction on
all groups of users and items with few number of ratings. A
more important advantage of DeepCoNN is that higher reductions are gained for groups with fewer ratings. It shows
that DeepCoNN can alleviate the sparsity problem and help
on the cold start problem.
It can also be seen that there exists a relation between
the eﬀectiveness of DeepCoNN and the number of ratings
for a user or item. For users or items with a lower number
of ratings, DeepCoNN reduction in MSE is higher. It shows
that review text can be valuable information especially when
we have limited information on the users or items.
RELATED WORKS
There are two categories of studies related to our work:
techniques that model users and/or items by exploiting the
information in online review text, and deep learning techniques employed for recommender systems. In this section,
we give a short review of these two research areas and distinguish our work from the existing approaches.
The ﬁrst studies that used online review text in rating
prediction tasks were mostly focused on predicting ratings
for an existing review , while in our paper, we predict
the ratings from the history of review text written by a user
to recommend desirable products to that user.
One of the pioneer works that explored using reviews to
improve the rating prediction is presented in . It found
that reviews are usually related to diﬀerent aspects, e.g.,
price, service, positive or negative feelings, that can be exploited for rating prediction. In , the authors proposed
Hidden Factors as Topics (HFT) to employ topic modeling techniques to discover latent aspects from either item or
user reviews. This method achieves signiﬁcant improvement
compared to models which only use ratings or reviews. A
similar approach is followed in with the main diﬀerence
that it models user’s and items’ reviews simultaneously. In
 , a probabilistic model is proposed based on collaborative
ﬁltering and topic modeling. It uncovers aspects and sentiments of users and items, but it does not incorporate ratings
during modeling reviews. Ratings Meet Reviews (RMR) 
also tries to harness the information of both ratings and reviews. One diﬀerence between HFT and RMR is that RMR
applies topic modeling techniques on item review text and
aligns the topics with the rating dimensions to improve prediction accuracy.
Overall, one limitation of the above studies is that their
textual similarity is solely based on lexical similarity. The
vocabulary in English is very diverse, and two reviews can
be semantically similar even with low lexical overlapping.
The semantic meaning is of particular importance and has
been ignored in these works. Additionally, reviews are represented by using bag-of-words, and words’ order exists in
reviews has not been preserved.
At last, the approaches
which employ topic modeling techniques suﬀer from a scalability problem and also cannot deal with new coming users
and items.
Recently, several studies have been done to use neural
network based models including deep learning techniques
for recommendation tasks. Several works model
users and/or items from the rating matrix using neural networks like denoising auto-encoders or Restricted Boltzmann
Machines (RBM). They are considered as collaborative based
techniques because they just utilize the rating matrix and
ignore review text unlike our approach.
In and , deep models of CNN and Deep Belief
Network (DBN) are introduced to learn latent factors from
music data for music recommendation. In both models, initially, they ﬁnd user and item latent factors using matrix factorization techniques. Then, they train a deep model such
that it can reconstruct these latent factors for the items from
the music content. A similar approach is followed in for
movie recommendation by using a generalized Stacked Auto
Encoder (SAE) model. In all these works , an
item’s latent factors are learned from item’s content and review text is ignored.
In , a multi-view deep model is built to learn the user
and item latent factors in a joint manner and map them
to a common space. The general architecture of the model
seems to have some similarities to our proposed model, but it
diﬀers from ours in some aspects. Their model is a contentbased recommender system and does not use review text.
Moreover, their outputs are coupled with a cosine similarity
objective function to produce latent factors with high similarity. In this way, user and item factors are not learned
explicitly in relation to the rating information, and there is
no guarantee that the learned factors can help the recommendation task.
All the above NN based approaches diﬀer from DeepCoNN
because they ignore review text. To the best of our knowledge, the only work which has utilized deep learning techniques to use review text to improve recommendation is presented in . To use the information exists in reviews, they
proposed a model consisting of a matrix factorization technique and a Recurrent Neural Network (RNN). The matrix
factorization is responsible for learning the latent factors of
users and items, and the RNN models the likelihood of a
review using the item’s latent factors. The RNN model is
combined with the MF simply via a trade-oﬀterm as some
sort of a regularization term to tame the curse of data sparsity.
Due to the matrix factorization technique, handling
new users and items is not trivial in this model unlike Deep-
CoNN that handles them easily. Their proposed algorithm
does not model users and items explicitly in a joint manner from their reviews, and it just uses reviews to regularize
their model. In addition, since item text is represented by
using bag-of-words, semantic meaning existing in words has
not been explored.
CONCLUSION
It is shown that reviews written by users can reveal some
info on the customer buying and rating behavior, and also
reviews written for items may contain info on their features
and properties. In this paper, we presented Deep Cooperative Neural Networks (DeepCoNN) which exploits the information exists in the reviews for recommender systems.
DeepCoNN consists of two deep neural networks coupled together by a shared common layer to model users and items
from the reviews. It makes the user and item representations
mapped into a common feature space. Similar to MF techniques, user and item latent factors can eﬀectively interact
with each other to predict the corresponding rating.
In comparison with state-of-the-art baselines, DeepCoNN
achieved 8.5% and 7.6% improvements on datasets of Yelp
and Beer, respectively. On Amazon, it outperformed all the
baselines and gained 8.7% improvement on average. Overall, 8.3% improvement is attained by the proposed model
on all three datasets.
Additionally, in the experiments by limiting modeling to
just one of the users and items, we demonstrated that the
two networks could not only separately learn user and item
latent factors from review text but also cooperate with each
other to boost the performance of rating prediction. Furthermore, we showed that word embedding could be helpful
to capture semantic meaning of review text by comparing it
with a variant of DeepCoNN which uses random or TF-IDF
representations for reviews.
At last, we conducted experiments to investigate the impact of the number of reviews. Experimental results showed
that for the users and items with few reviews or ratings,
DeepCoNN obtains more reduction in MSE than MF. Especially, when only one review is available, DeepCoNN gains
the greatest MSE reduction. Thus, it validates that Deep-
CoNN can e¨ıˇn˘Aectively alleviate the sparsity problem.
ACKNOWLEDGEMENTS
This work is supported in part by NSF through grants
IIS-1526499, and CNS-1626432. We gratefully acknowledge
the support of NVIDIA Corporation with the donation of
the Titan X GPU used for this research.