Modeling Textual Cohesion for Event Extraction
Ruihong Huang and Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
{huangrh,riloff}@cs.utah.edu
Event extraction systems typically locate the role ﬁllers for an
event by analyzing sentences in isolation and identifying each
role ﬁller independently of the others. We argue that more accurate event extraction requires a view of the larger context
to decide whether an entity is related to a relevant event. We
propose a bottom-up approach to event extraction that initially identiﬁes candidate role ﬁllers independently and then
uses that information as well as discourse properties to model
textual cohesion. The novel component of the architecture
is a sequentially structured sentence classiﬁer that identiﬁes
event-related story contexts. The sentence classiﬁer uses lexical associations and discourse relations across sentences, as
well as domain-speciﬁc distributions of candidate role ﬁllers
within and across sentences. This approach yields state-ofthe-art performance on the MUC-4 data set, achieving substantially higher precision than previous systems.
Introduction
The aim of event extraction systems is to identify noun
phrases that represent role ﬁllers for a speciﬁc type of event.
Role ﬁllers are the participants, objects, and properties associated with an event. For example, event extraction systems
have been created to identify role ﬁllers for management
succession events (e.g., the names of people being hired or
ﬁred, and the companies involved), corporate acquisitions
(e.g., purchased companies, and the purchasers), terrorism
events (e.g., perpetrators, victims, and targets), and many
Most event extraction systems use patterns or classiﬁers
to decide which noun phrases are role ﬁllers based on the local context around them. However, each sentence in a story
is usually processed independently, ignoring the rest of the
document. Processing sentences in isolation can cause several problems. False hits can occur due to ambiguity and
metaphor. For example, “Obama was attacked” may lead to
Obama being extracted as the victim of a physical attack,
even if the preceding sentences describe a presidential debate and the verb “attacked” is being used metaphorically.
Many sentences also contain phrases that are role ﬁllers only
if the preceding context describes a relevant event. For example, people can be injured or killed in many ways . All rights reserved.
vehicle accidents, military engagements, natural disasters,
and civilian crime). Someone who is injured or killed should
be characterized as a victim of terrorism only if the discourse
is describing a terrorist event.
Role ﬁllers can also be overlooked because a sentence
does not appear to be relevant when viewed in isolation. For
example, consider the sentence “He used a gun”. Without
considering the surrounding story context, event extraction
systems will either extract from these types of sentences,
which improves recall but lowers precision (e.g., false hits
will occur when non-perpetrators use weapons, such as police and soldiers), or they will ignore them, which means that
some legitimate role ﬁllers will be missed. In this paper, we
argue that event extraction systems need to incorporate contextual inﬂuence across sentences in order to achieve better
performance.
We propose a bottom-up approach for event extraction
that aggressively identiﬁes candidate role ﬁllers based on
local (intra-sentential) context, and then uses distributional
properties of the candidate role ﬁllers as well as other discourse features to model textual cohesion across sentences.
Our event extraction architecture has two components: (1)
a set of local role ﬁller extractors, and (2) a sequential sentence classiﬁer that identiﬁes event-related story contexts.
The novel component is the sentence classiﬁer, which uses
a structured learning algorithm, conditional random ﬁelds
(CRFs), and features that capture lexical word associations
and discourse relations across sentences, as well as distributional properties of the candidate role ﬁllers within and
across sentences. The sentence classiﬁer sequentially reads
a story and determines which sentences contain event information based on both the local and preceding contexts. The
two modules are combined by extracting only the candidate
role ﬁllers that occur in sentences that represent event contexts, as determined by the sentence classiﬁer.
Related Work
Most event extraction systems, both pattern-based and classiﬁer-based , scan a text
and search for individual role ﬁllers based on local contexts around noun phrases. However, recent work has begun to explore the use of additional context to improve
Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence
performance. used discourse
trees and local syntactic dependencies within sentences in
a pattern-based framework. created HMMs to ﬁrst identify relevant sentences and then
trained another set of HMMs to extract individual role ﬁllers
from the relevant sentences. 
learned to recognize event sentences in a weakly-supervised
learning paradigm and then extracted role ﬁllers from the
event sentences using patterns. GLACIER jointly considered sentential evidence and local
phrasal evidence in a probabilistic framework to extract role
Only a few event extraction models have gone beyond individual sentences to make extraction decisions. calculated document-level role ﬁller statistics and used the co-occurrence information of different
types of events and role ﬁllers to train better extraction models. Ji and Grishman enforced event role consistency
across different documents. TIER 
used a document genre classiﬁer to recognize event narrative
stories and then identiﬁed event sentences as well as rolespeciﬁc sentences in the event narratives, but each sentence
was classiﬁed and used independently.
Structured models have been applied in several areas
of natural language processing, including event extraction.
But previous event extraction research has used structured
models to sequentially label noun phrases, not sentences
 ). Our research is the ﬁrst to sequentially label sentences to identify domain-speciﬁc event contexts.
Our work is related to the document-level content models introduced by , which utilized a
novel adaptation of the generative sequential model HMMs
 to capture the topics that the texts address
and the transitions between topics. The learned topic sequences improved two applications, information ordering
and extractive summarization. Recently, incorporates the latent content structure
directly into two text analysis tasks, extractive summarization and sentiment analysis, in a joint learning framework.
Our research also learns a structured sequential model for
the sentences in a document. However, we are not aiming to
model the content ﬂow between all sentences. Our goal is to
capture content transitions and discourse relations that can
recognize event-related story contexts for a speciﬁc domain.
Improving Event Extraction by Modeling
Textual Cohesion
Our event extraction model involves two processes that each
focus on a different aspect of the problem. The left side of
Figure 1 shows the two components and illustrates how they
interact. The top component on the left is a set of traditional
role ﬁller detectors, one for each event role. This component identiﬁes candidate role ﬁllers based on the immediate
context surrounding a noun phrase. These role ﬁllers tend to
be overly aggressive on their own, producing many correct
extractions but also many false hits.
Figure 1: A Bottom-up Architecture for Event Extraction
The bottom component on the left side of Figure 1 is
a structured sentence classiﬁer that identiﬁes event-related
story contexts. This classiﬁer determines whether a sentence
is discussing a domain-relevant event based on two types
of information. The structured learning algorithm explicitly
models whether the previous sentence is an event context,
which captures discourse continuity across sentences. We
also provide the learner with features representing other textual cohesion properties, including lexical associations and
discourse relations between adjacent sentences. In addition,
the bottom-up design of the architecture provides information about candidate role ﬁllers found by the local detectors.
This domain-speciﬁc information is incorporated into features that represent the number, types, and distribution of
the candidate role ﬁllers both within and across sentences.
The two components provide different sources of evidence that are both considered when making ﬁnal extraction
decisions. The right side of Figure 1 illustrates how the two
components are used. The event extraction system only produces a role ﬁller if the noun phrase was hypothesized to
be a candidate role ﬁller based on local context and it appears in an event-related story context, as determined by the
sequential sentence classiﬁer. In the following sections, we
describe each of these components in more detail.
Candidate Role Filler Detectors
The local role ﬁller detectors are support vector machine
(SVM) classiﬁers that label noun phrases with respect to an
event role. We create a set of binary classiﬁers, one for each
event role. If multiple classiﬁers assign positive scores to the
same noun phrase, then the event role that receives the highest score is assigned.
Three types of features represent the local context surrounding a noun phrase (NP). Lexical features consist of
four words to the left and and four words to the right of the
targeted NP, as well as its head and premodiﬁers. Semantic
features include named entity tags produced by the Stanford
named entity recognizer and semantic class labels assigned by the Sundance
parser . Finally, lexico-syntactic
pattern features are produced by AutoSlog , which automatically generates
patterns for expressions in which the NP participates as a
syntactic subject, direct object, or prepositional phrase.1 The
candidate role ﬁller detectors only consider the local context
surrounding a NP, so they tend to overgenerate role ﬁller hypotheses when they see any context that could be relevant.
The classiﬁers are trained using the gold standard MUC-4
answer key templates. For each event role, the noun phrases
matching a document’s answer key strings for that event role
are positive training instances. All other noun phrases in the
document are negative instances. Since the number of negative instances is far greater than the number of positive instances, we randomly choose among the negative instances
to create a 10:1 ratio of negative to positive instances.
Our candidate role ﬁller detectors are identical to the local
role ﬁller extractors used by TIER ,
which allows for direct comparisons between TIER and our
new model. They are also very similar to the plausible role
ﬁller detectors used by GLACIER (the other system we compare against in Section 6),
except for small differences in the lexical features and the
positive/negative training ratios.
Structured Sentence Classiﬁcation to
Identify Event Contexts
The sequential sentence classiﬁer is responsible for determining which sentences are related to domain-relevant
events. We utilize conditional random ﬁelds (CRFs) to carry out this sequential labeling task. A sequential CRF is a structured discriminative learning model that produces a sequence of labels
using features derived from the input sequence. This component will sequentially read the sentences in a story and determine whether each sentence is discussing a relevant event
based on direct evidence from both the current sentence and
the previous sentence. All other sentences only affect the results indirectly through label transitions.
We used the CRF++ 2 toolkit to create our structured sentence classiﬁer. CRF++ performs sequential labeling tasks
and requires each unit in the input to have a ﬁxed number of
raw features. Since the length of sentences can vary, affecting the number of n-grams and other features accordingly,
we expand the feature vector for each sentence with pseudotokens3 as needed to ensure that every sentence has the same
number of features. The toolkit was modiﬁed not to generate
real features from the pseudo-tokens.
We provide the classiﬁer with four types of features to
represent individual sentences and textual cohesion properties linking adjacent sentences: basic features, lexical
bridges, discourse bridges and role ﬁller distributions. The
following sections describe each of these feature sets.
1These patterns are similar in spirit to the relations produced by
dependency parsers.
2 [crfpp.sourceforge.net]
3We deﬁne a special token for this purpose.
Basic Features
As the basic representation of a sentence, we use unigram
and bigram features. We create features for every unigram
and bigram, without stemming or stopword lists. In addition, we found it beneﬁcial to create ﬁve additional features
representing the ﬁrst ﬁve bigrams in the sentence. We deﬁne
features for positions 1 through 5 of a sentence to represent
the bigrams that begin in each of these positions. We hypothesize that these positional bigram features help to recognize
expressions representing discourse cue phrases at the beginning of a sentence, as well as the main subject of a sentence.
Lexical Bridge Features
An important aspect of textual cohesion is lexical word associations across sentences. This idea has been explored in
 to model the intuition that the use
of certain words in a discourse unit (e.g., sentence) tends to
trigger the use of other words in subsequent discourse units.
In the context of event extraction, a pair of related event keywords may occur in consecutive sentences. For example, it
is common to see “bombed” in one sentence and “killed”
in the next sentence because bombing event descriptions are
often followed by casualty reports. Similarly, we may see
“attacked” and “arrested” in adjacent sentences because a
mention of an attack is often followed by news of the arrest
of suspected perpetrators.
To capture lexical associations between sentences, we create lexical bridge features that pair each verb in the current
sentence (V erbi) with each verb in the preceding sentence
sentence (V erbi−1):
< V erbi−1, V erbi >
To obtain better generalization, we stem the verbs before
creating the bridge features using the Porter stemmer . For example, a sentence that mentions a bombing followed by a sentence containing “killed” would generate the
following lexical bridge feature:
< bomb, kill >
Event keywords could also appear as nouns, such as “assassination” and “death”. Therefore, we also create lexical
bridge features by pairing nouns from the current sentence
and the preceding sentence:
< Nouni−1, Nouni >
For example, if we see the word “explosion” in the preceding
sentence and the nouns “people” and “ofﬁces” in the current
sentence, then two features will be created as follows:
< explosion, people >
< explosion, offices >
We also tried including associations between nouns and
verbs in adjacent sentences (i.e. < V erbi−1, Nouni > and
< Nouni−1, V erbi >), but they did not improve performance. To focus on event recognition, the lexical bridges are
only created between sentences that each contains at least
one candidate role ﬁller.
Discourse Bridge Features
We also represent two types of discourse relations between
consecutive sentences: discourse relations produced by a
Penn Discourse Treebank (PDTB) trained discourse parser,
and syntactic discourse focus relations. We hypothesized
that these features could provide additional evidence for
event label transitions between sentences by recognizing explicit discourse connectives or a shared discourse focus.
PDTB-style discourse relations are
organized hierarchically in three levels based on different
granularities. We use the discourse relation output produced
by a PDTB-style discourse parser .
Given a text, the discourse parser generates both explicit
(triggered by cue phrases such as “if” or “because”) and implicit level-2 PDTB discourse relations, such as cause, condition, instantiation, and contrast. A discourse relation may
exist within a sentence or between two adjacent sentences
in the same paragraph. We create features representing the
intra-sentential discourse relations found in the current sentence, as well as the inter-sentential discourse relations connecting the current sentence with the previous one. Each discourse relation produced by the parser yields a feature for its
discourse relation type:
< DiscRelType >
We also create features designed to (approximately) recognize shared discourse focus. We consider the noun phrases
in three syntactic positions: subject, direct object, and the
objects of “by” prepositional phrases (PP-by). Sentences in
active voice constructions are typically focused on the entities in the subject and direct object positions as the central entities of the discourse. Sentences in passive voice constructions are usually focused on the entities in the subject
and PP-by positions as the most central entities. We use the
Stanford parser 
to identify these syntactic constituents.
The motivation for this type of feature is that sentences
which have a shared discourse focus probably should be
assigned the same event label (i.e., if one of the sentences is
discussing a domain-relevant event, then the other probably
is too). To capture the intuition behind this idea, consider
the following two sentences:
(1) A customer in the store was shot by masked men.
(2) The two men used 9mm semi-automatic pistols.
Because the same entity (the men) appears in both the “by”
PP of sentence (1) and the subject position of sentence (2),
the classiﬁer should recognize that the second sentence is
connected to the ﬁrst. Recognizing this connection may enable the extraction system to correctly identify the pistols as
instruments used in the shooting event, even though sentence
(2) does not explicitly mention the shooting.
We create a discourse focus feature for each shared noun
phrase that occurs in two adjacent sentences in one of the
designated syntactic positions. We consider any two noun
phrases that have the same head word to match. We encode
each feature as a triple consisting of the head word of the
shared noun phrase (NPHead), the NP’s position in the
current sentence (SynPosi), and the NP’s position in the
preceding sentence (SynPosi−1):
< NPHead, SynPosi, SynPosi−1 >
For example, sentences (1) and (2) would produce the following discourse focus feature:
< men, subject, PP-by >
Role Filler Distribution Features
The motivation for the bottom-up design of our event extraction architecture is that the sentence classiﬁer can bene-
ﬁt from knowledge of probable role ﬁllers hypothesized by
the local detectors. Intuitively, the presence of multiple role
ﬁllers within a sentence or in the preceding sentence is a
strong indication that a domain-relevant event is being discussed. The local detectors are not perfect, but they provide
valuable clues about the number, types, and density of probable role ﬁllers in a region of text.
First, we create features that capture information about the
candidate role ﬁllers within a single sentence. We create features for the event role type and the head noun of each candidate role ﬁller in the sentence. We also encode two types of
features that capture properties of the set of candidate role
ﬁllers. For each event role, we deﬁne a binary feature that
indicates whether there are multiple candidate role ﬁllers for
that role. For example, if we see multiple victims in a sentence, this is more evidence than seeing a single victim. The
second type of feature represents combinations of different
event role types detected in the same sentence. We deﬁne 10
binary features that represent the presence of pairs of distinct
event roles occurring in the same sentence.4 For example, if
we see both a perpetrator and a victim in a sentence, we may
be more conﬁdent that the sentence is describing a crime.
We also create several types of features that represent role
ﬁller distributions across sentences. Intuitively, the presence
of a particular type of role ﬁller in one sentence may predict
the presence of a role ﬁller in the next sentence. For example, a gun is more likely to be an instrument used in a crime
if the preceding sentences mention perpetrators and victims
than if they only mention other weapons. To capture domainspeciﬁc distributional properties of the candidate role ﬁllers,
we create features for the role ﬁllers found in adjacent sentences. We use both the head word of the noun phrase as
well as the type of the event role. If the local detectors produce a candidate role ﬁller of type RFTypei−1 with head
RFHeadi−1 in the previous sentence, and a role ﬁller of
type RFTypei with head RFHeadi in the current sentence,
then two features are generated:
< RFHeadi−1, RFTypei >
< RFHeadi−1, RFTypei−1, RFTypei >
4Since there are 5 event roles, there are 10 pairs of distinct roles
because the order of them doesn’t matter.
For example, assuming that three candidate role ﬁllers have
been detected for the example sentences in Section 5.3 (Victim(customer) and Perpetrator(men) from sentence (1) and
Weapon(pistols) from sentence (2)), the following features
will be created:
< customer, Weapon >
< customer, V ictim, Weapon >
< men, Weapon >
< men, Perpetrator, Weapon >
We also create features to represent role ﬁllers that
occur in adjacent sentences and share a discourse relation. If two adjacent sentences share a discourse relation
(DiscRelType), then we represent the types of role ﬁllers
found in those sentences, coupled with the discourse relation. For example, if two sentences are in a causal relation
and the candidate role ﬁller detectors found a candidate victim in the previous sentence and a candidate perpetrator in
the current sentence, then the causal relation provides further
evidence that the victim and perpetrator are likely correct.
These types of features are represented as:
< RFTypei−1, DiscRelType, RFTypei >
For the example above, the feature would be:
< V ictim, cause, Perpetrator >
Finally, verbs often provide valuable clues that a sentence
is discussing an event, so the presence of a speciﬁc verb in
the previous sentence may bolster a role ﬁller hypothesis in
the current sentence. We create an additional feature that
links each verb in the previous sentence to each candidate
role ﬁller in the current sentence:
< V erbi−1, RFTypei >
For example, a sentence containing a candidate victim preceded by a sentence containing the word “bombed” would
produce the following feature:
< bombed, V ictim >
When generating these features during training, the gold
standard role ﬁllers are not suitable because gold role ﬁllers
will not be available in new texts. A model trained with gold
role ﬁllers would probably not be effective when applied to
new documents that have system-generated candidate role
ﬁllers. To obtain realistic values for the candidate role ﬁller
distributions, we used 5-fold cross-validation on the training
data. To get the candidate role ﬁllers for one fold, we trained
the role ﬁller detectors using the other four folds and then
applied the detectors to the selected fold.
Evaluation
We evaluated our approach on a standard benchmark collection for event extraction research, the MUC-4 data set
 . The MUC-4 corpus consists of
1700 documents with associated answer key templates. To
be consistent with previously reported results on this data
set, we use the 1300 DEV documents for training, 200 documents (TST1+TST2) as a tuning set, and 200 documents
(TST3+TST4) as the test set.
Table 1: # of Role Fillers in the MUC-4 Test Set
Following previous studies, we evaluate our system on
the ﬁve MUC-4 “string-ﬁll” event roles: perpetrator individuals, perpetrator organizations, physical targets, victims
and weapons. These event roles (essentially) represent the
agents, patients, and instruments associated with terrorism
events. Table 1 shows the distribution of gold role ﬁllers
in the MUC-4 test set. The complete IE task involves template generation, which requires event segmentation because
many documents discuss multiple events. Our work focuses
on extracting individual role ﬁllers and not template generation per se, so we follow the same evaluation paradigm of recent research and evaluate the accuracy of the role ﬁllers directly (i.e., if the role ﬁller has the correct label and appears
in any event template for the document, then it is correct).
We use head noun matching against the answer key strings
(e.g., “armed guerrillas” is considered to match “guerrillas”)5. Our results are reported as Precision/Recall/F(1)score for each event role separately. We also show the macro
average over all ﬁve event roles.
Experimental Results
Table 2 shows the evaluation results on the ﬁve event roles
for the MUC-4 task, and the macro-average over all ﬁve
roles. Each cell in the table shows the precision (P), recall
(R), and F scores, written as P/R/F. The ﬁrst row of numbers
shows the results for the candidate role ﬁller detectors when
used by themselves. These local role ﬁller extractors produce relatively high recall, but consistently low precision.
The next set of rows in Table
2 shows the effect of
adding the structured sentence classiﬁer to create the complete bottom-up event extraction model. We incrementally
add each set of of textual cohesion features to assess the impact of each one separately. The Basic feature set row uses
only the N-gram features. Even with just these simple features, incorprating the structured sentence classiﬁer into the
model yields a large improvement in precision (+25) but at
the expense of substantial recall (-19).
The + Candidate RF features row shows the impact of
providing the candidate role ﬁller information to the sentence classiﬁer (see Section 5.4). Compared with the previous row, the role ﬁller features produce an average recall
gain of +3, with only a one point loss of precision. When
looking at the event roles individually, we see that recall improves for all of the event roles except Targets.
The + Lexical Bridge features row shows the impact of
the lexical bridge features (Section 5.2). These features produced a two point gain in precision, yielding a one point gain
5Pronouns were discarded since we do not perform coreference
resolution. Duplicate extractions with the same head noun were
counted as one hit or one miss.
Local Extraction Only
Candidate RF Detectors
with Structured Sentence Classiﬁer
Basic feature set
+ Candidate RF features
+ Lexical Bridge features
+ Discourse features
Previous Systems
TIER 
GLACIER 
Table 2: Experimental results, reported as Precision/Recall/F-score.
in F-score. Two of the event roles (PerpOrg and Weapon)
showed improvement in both precision and recall.
The + Discourse features row shows the performance after adding the discourse bridge features (Section 5.3). The
discourse features improve precision for three of the ﬁve
event roles (PerpInd, PerpOrg, and Victim). Weapons also
gain two points of recall. Overall, the discourse features
yield a two point increase in the F score.
Together, all of the textual cohesion features yield a 3
point gain in precision and a 4 point gain in recall relative
to the basic feature set (N-grams), achieving an F-score improvement of 3 points.
Comparison with Other Systems
We compare the performance of our event extraction model
with two relatively recent event extraction systems that have
been evaluated on the same MUC-4 data set: TIER and GLACIER . TIER is a multi-layered architecture for event extraction. Documents pass through a pipeline where they
are analyzed at different levels of granularity: document
level, sentence level and phrase level. TIER is designed
to identify secondary role ﬁller contexts in the absence of
event keywords by using a document genre classiﬁer, a
set of role-speciﬁc sentence classiﬁers, one per event role,
in addition to an event sentence classiﬁer ). TIER has produced the best results reported to date on the MUC-4 event extraction data
set for learning-based role ﬁller
extraction systems.
As a second baseline, we also compare our results with
GLACIER . GLACIER uses a
uniﬁed probabilistic model for event extraction that jointly
considers sentential evidence and phrasal evidence when extracting each role ﬁller. It consists of an sentential event recognizer and a set of plausible role ﬁller recognizers, one for
each role. The ﬁnal extraction decisions are based on the
product of the normalized sentential and the phrasal probabilities.
The last two rows in Table 2 show the results for TIER
and GLACIER, using the same evaluation criteria as our
system. We compare their results with the performance of
our complete event extraction system using all of the feature sets, which is shown in the + Discourse Features row
2. Compared with TIER, our model achieves 7
points higher precision, although with slightly lower recall
(-2). Overall, our model yields a 3 point higher F score
than TIER. If we look at the individual event roles, our
model produces substantially higher precision across all ﬁve
event roles. Recall is comparable for PerpInd, Victim, and
Weapon, but is several points lower on the PerpOrg and Target roles. Compared with GLACIER, our model also shows
signiﬁcant gains in precision over all ﬁve event roles. Furthermore, the average recall is 3 points higher, with Weapons
showing the largest beneﬁt (+11 recall gain).
In summary, our bottom-up event extraction model yields
substantially higher precision than previous event extraction
systems on the MUC-4 data set, with similar levels of recall.
Conclusions
We have presented a bottom-up architecture for event extraction that demonstrates how textual cohesion properties
can be integrated into an event extraction model to improve
its accuracy. Our event extraction system has two components: (1) local role ﬁller detectors that identify candidate
role ﬁllers, and (2) a structured sentence classiﬁer that identiﬁes event-related story contexts. The main contribution of
our work is the integration of a sequential sentence classiﬁer
that utilizes features representing several types of properties
associated with textual cohesion, including lexical associations and discourse relations across sentences. In addition,
the bottom-up design of the architecture allows the sentence
classiﬁer to consider distributional properties of the domainspeciﬁc candidate role ﬁllers, both within and across sentences. This model yields state-of-the-art performance on
the MUC-4 data set for learning-based systems, achieving
substantially higher precision than previous models. In future work, we hope to explore additional textual cohesion
properties and discourse issues associated with event descriptions to further improve event extraction performance.
Acknowledgments
We gratefully acknowledge the support of the National
Science Foundation under grant IIS-1018314 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any
opinions, ﬁndings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not
necessarily reﬂect the view of the DARPA, AFRL, or the
U.S. government.