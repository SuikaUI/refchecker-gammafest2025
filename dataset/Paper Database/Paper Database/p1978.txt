Detection of Non-Technical Losses in Smart Meter Data
based on Load Curve Proﬁling and Time Series Analysis
Esther Villar-Rodrigueza, Javier Del Sera,b,c,∗, Izaskun Oregia,
Miren Nekane Bilbaob, and Sergio Gil-Lopeza
aTECNALIA, 48160 Derio, Bizkaia, Spain.
bUniversity of the Basque Country (EHU/UPV), 48013 Bilbao, Bizkaia, Spain.
cBasque Center for Applied Mathematics (BCAM), 48009 Bilbao, Bizkaia, Spain.
The advent and progressive deployment of the so-called Smart Grid has unleashed a proﬁtable portfolio of new possibilities for an eﬃcient management of the low-voltage distribution network supported by the introduction
of information and communication technologies to exploit its digitalization.
Among all such possibilities this work focuses on the detection of anomalous energy consumption traces: disregarding whether they are due to malfunctioning metering equipment or fraudulent purposes, strong eﬀorts are
invested by utilities to detect such outlying events and address them to optimize the power distribution and avoid signiﬁcant income costs.
context this manuscript introduce a novel algorithmic approach for the identiﬁcation of consumption outliers in Smart Grids that relies on concepts from
probabilistic data mining and time series analysis. A key ingredient of the
proposed technique is its ability to accommodate time irregularities – shifts
and warps – in the consumption habits of the user by concentrating on the
shape of the consumption rather than on its temporal properties. Simulation results over real data from a Spanish utility are presented and discussed,
from where it is concluded that the proposed approach excels at detecting
diﬀerent outlier cases emulated on the aforementioned consumption traces.
Smart Grids; Smart Meter Data; Non-Technical Losses; Outlier
Detection.
∗Corresponding author: (Prof. Dr. Javier Del Ser). TEC-
NALIA. P. Tecnologico Bizkaia, Ed. 700, 48160 Derio, Spain. Tl: +34 946 430 50. Fax:
+34 901 760 009. E-mail: .
 
June 26, 2017
1. Introduction
According to the oﬃcial deﬁnition introduced by the Energy Independence and Security Act of 2007 , Smart Grids can be understood in the
wide sense as the technological eﬀorts to modernize and digitize the electricity
distribution system of a nation to ensure, in an scalable manner, the improved
reliability, security and eﬃciency of the grid, as well as to guarantee an optimized management of its resources and operation. This deﬁnition describes
such a term as a comprehensive set of operational resources established to
guarantee an eﬃcient power transmission and electricity distribution, paying
an special attention to reliability and security.
The advent and progressive deployment of the so-called Smart Grid has
unleashed a proﬁtable portfolio of new possibilities for an eﬃcient management of the low and medium voltage distribution network, supported by the
introduction of information and communication technologies to exploit its
digitalization. In this context, the deployment of the Advanced Metering
Infrastructures (AMIs) allows the utilities to acquire ﬁne-grained data about
the real consumption of end-users (not based on estimations or monthly measurements), which results essential to acquire deeper insights on how, when
and where energy is distributed and consumed through the network .
This fact is particularly crucial in regards to the traceability and characterization of electrical losses, which account for the diﬀerence between the
amount of energy distributed by the electrical distribution company and the
amount of energy paid by the consumers. Such losses may be due to two
main contributing causes: 1) losses inherent to the transformation and distribution of energy, which are proportional to the squared of electrical current
and widely referred to as Technical Losses (TL); and 2) non-technical losses
(NTLs), associated to erroneous readings, defected smart meters or fraud .
This work focuses on the detection of energy consumption traces which
contribute to NTLs: disregarding whether they are due to malfunctioning
metering equipment or fraudulent purposes. As mentioned by the
amount of energy loss in the distribution grids varies between 7 – 50 % of the
total delivered energy (depending of the country and the characteristics of
the distribution network), which undoubtedly justiﬁes the strong eﬀorts that
utilities are investing towards detecting and inspecting atypical consumption
traces to ultimately avoid signiﬁcant economical losses. As stated in , only
in US between 1 and 10 billion worth of electricity was stolen in the late 90s,
showing an increment between 5-10% in the last two decades with a remark-
able 40% and beyond in Southeast Asia . In addition, the identiﬁcation of
atypicalities provides further proﬁtable advantages beyond fraud assessment:
by properly characterizing the statistics of the consumption traces registered
over the power grid, the power distribution can be optimized by matching
generation to consumption, thereby avoiding network under-dimensioning
and electrical surge.
Interestingly for the scope of this work, energy theft accounts for the
majority of reasons for the aforementioned non-technical losses. There are
indeed very diverse methods by which malicious consumers reduce illegally
the consumption monitored by the installed smart equipment, particularly
in the last stage of the distribution network. One of the most usual forms of
electricity theft is fraud, by which the user deliberately attempts at deceiving the energy supplier (utility) at hand. This can be achieved by diverse
means such as meter tampering, by which the meter is forced to register
a lower power reading than the real consumption of the user. While other
forms of electricity theft prevail across diﬀerent countries and cultures (e.g.
billing irregularities), this work revolves around those fraudulent cases when
the non-technical loss may be reﬂected in a behavioral change of the energy
consumption trace registered by the metering device. In this regard, both
tampering and electricity theft fall within the scope of this work: they constitute a prioritized target of most utility companies around the world, due to
the severe consequences of these phenomena (i.e. higher electricity rates for
paying consumers, increased risk of ﬁre or electrocution due to improperly
installed bypasses and in general, a reduced grid reliability).
From a data based perspective, a change in the energy consumption proﬁle
of a user contributing to NTLs can be understood as a deviating observation in the time series that models such a proﬁle, whose statistics make it
quite likely to be generated by another diﬀerent underlying behavior .
However, normal load proﬁling in the low-voltage network can be produced
by the aggregation of diﬀerent, yet related behavioral components (seasonality, daily and weekly statistical variability, habits changes, among others)
that diﬀer from each other in both, amplitude (i.e. amount of energy consumed from the power grid due to diﬀerent load consumptions) and time
domains (correspondingly, the statistical consumption schedule of the set of
users’ loads along the day or week). It is the dissimilarity of any new consumption trace to any of those previously learned behavioral patterns (load
proﬁling) what should diﬀer both behaviors (normal and NTLs). Furthermore, the detection of anomalous observations allows for the inference of
more robust models by discarding those instances resulted from the strongly
irregular samples, which could deviate the models from the representation of
statistically signiﬁcant regular trends in the consumption habits of the user
under analysis. This being said, an outlier detection method can be deﬁned
as the task of classifying elements as normal or diﬀering with respect to the
statistical regularity characterizing a dataset. At this point the concept of
regularity must be determined by the addressed application scenario.
In this context, a baseline taxonomy of outlier detection algorithms comprises 1) parametric methods that rely on prior hypothesis about the statistical model generating the data; and 2) model-free, non-parametric techniques, which avoid any prior assumption about the underlying distribution
of the data or statistical parameter estimates. Among the latter we focus
on distance-based unsupervised outlier detection approaches, which generally hinge on local distance measurements (not for accounting behavioral
diﬀerences) and are capable of eﬃciently handling large datasets . By
properly deﬁning a distance or measure of similarity between samples, subsequent data mining procedures such as cluster analysis can identify group
of samples that do not belong to the set of discovered data clusters. This
identiﬁcation can be done based on diﬀerent distance-based criteria, such as
the density of samples within a given distance threshold. In all such cases
the selection of the distance metric is a key point for this collection of techniques, since the similarity criterion – which roughly depends on the chosen
distance metric – will guide the whole identiﬁcation process. Therefore, it is
clear that the best similarity function must be compliant with the nature of
data and the speciﬁc particularities of the application.
In this regard, several prior contributions have hitherto dealt with the
identiﬁcation of NTLs in energy consumption traces. To begin with, several
contributions have gravitated on the use of machine learning models over supervised datasets, such as Support Vector Machines ,
Neural Networks , Extreme Learning Machines , Path Forests
 , Decision Trees , model ensembles , and statistical
methods . However, all such previous work builds upon the assumption that supervised datasets capture the entire casuistry of symptomatic
anomalies of interest for fraud detection and/or electricity theft, which not
only unrealistic in practice but also yields highly imbalanced datasets that
subsequently jeopardize the model learning process. By contrast, unsupervised anomaly detection in Smart Grids overrides any need for previously
labeled data, yet makes the evaluation and tuning of the model hard to
perform due to the non-utilization of positive examples during the construction of the learner. The literature dealing with electricity fraud using nonsupervised learning models has been relatively scarce, with Self Organizing
Maps and fuzzy clustering schemes mostly used to date.
This manuscript introduces a novel algorithmic approach for acquiring
knowledge of customer’s behaviors (load proﬁling), which allows for the identiﬁcation of consumption behavioral outliers in Smart Grids based on the
hourly measurements provided by the AMIs. The proposed scheme advances
over the state of the art by combining probabilistic data mining and time series analysis; we adopt the so-called Dynamic Time Warping (DTW) metric
as the measure of similarity between consumption traces registered by the
user under analysis, by which such sequences are aligned in a dynamic, nonlinear fashion disregarding any shifts or warps along time . This metric is
then used within two different distance-based learning models, both relying
on density estimations to detect anomalous patterns. A further novel ingredient of this work is a trace encoding strategy that depends on the spanned
hourly statistical ranges of every user, which increases the ﬂexibility of the
models to avoid false alarms. The performance of the derived schemes is assessed and discussed based on simulation results computed over real AMI
data captured by a Spanish utility. Given the obtained scores we conclude
that the proposed method accommodates irregularities of the analyzed consumption traces along time by focusing exclusively on their shape.
The rest of the manuscript is structured as follows: Section 2 poses the
notation used throughout the manuscript, and formulates the problem of outlier detection contextualized for the application tackled in this manuscript.
Section 3 provides an overview of the proposed approach, emphasizing on its
constituent elements in subsections therein. Next, Section 4 describes the
dataset utilized for performance assessment, justiﬁes the diﬀerent emulated
cases over such data and discusses the obtained results. Finally conclusions
are given in Section 5 along with an outline of future research lines.
2. Notation and Problem Statement
As depicted in Figure 1, we assume that an energy distribution company
has deployed a set of N smart meters to monitor the consumption of part of
its customer portfolio. Let data samples registered by the n-th smart meter
be denoted as xn .= {xn
t=1, where t stands for the time dimension discretized
as per the granularity tn
s [minutes] by which the smart meter records data (e.g.
hourly, tn
s = 60 minutes). Here Tn denotes the total number of samples read
for the customer at hand, which may vary among diﬀerent customers due to
e.g. the date on which the smart meter was installed in the user premises. We
further consider that the minimum decisional unit for the outlier detection
model is an entire day (24 hours), for which xn .= {xn
t=1 can be reshaped
as a matrix Xn, with each column containing the (24 · 60)/tn
s values that the
meter for customer n ∈{1, . . . , N} samples during each day. For the sake
of simplicity, in foregoing derivations we will force tn
s = 60 minutes ∀n, such
that Xn will have 24 readings per every day out of a total of Dn .= ⌊T n/24⌋
days monitored for customer n. Samples for day d ∈{1, . . . , Dn} will be
expressed as Xn
d, i.e. by the d-th row in Xn.
The aim of an outlier detection model M n
d′; Xn) is to infer, for user n,
whether a new daily consumption trace Xn
d′ captured by the smart meter of
user n follows the same distribution as that characterizing Xn (declaring it to
be an inlier) or, instead, diﬀers signiﬁcantly (correspondingly, is an outlier).
The latter case serves as a trigger for a further inspection process to conﬁrm
whether the behavioral change is due to e.g. fraud. The model is controlled
by a set of parameters collected in θ, which permit to balance between the
True Positive Rate (TPR, also referred to as sensitivity or recall) and the
True Negative Rate of the model (namely, TNR or speciﬁcity) .
At this point it is important to note that for measuring the TNR and
TPR metrics of any outlier detection model we need supervised labels of
the test traces over which such metrics are computed. In other words, for
assessing the performance of an outlier detection algorithm it is mandatory
to know a priori whether the distribution utilized for producing each of the
test traces corresponds to that utilized for modeling the outlier prototype
that the model should detect.
We refer as ℓn
d′; Xn) ∈{0, 1} to the predicted label by the
model for test trace Xn
Bearing this deﬁnition in mind, the TPR and
TNR scores achieved by model M n
θ ( · ) over a test dataset {Xn
given by TNR (M n
θ ) and TPR (M n
θ ), respectively. Noteworthy is to highlight that these metrics implicitly measure the extent to which the model is
adapted to discriminate among the distribution f 1
X(x) followed by outliers
within {Xn
d′=1 from that followed by regular traces in Xn (correspondingly,
X(x)). While learning f 0
X(x) is a matter of ﬁtting the model to Xn on the
assumption that all consumption traces therein are legitimate, the casuistry
of outliers dictated by f 1
X(x) is driven by the speciﬁcities of the application
scenario itself. To this end, in this work we focus on four diﬀerent hypotheses
for the test trace Xn
d′ which M n
θ ( · ) should declare as an inlier or an outlier:
1. The test trace Xn
d′ belongs to the normal behavioral distribution of customer n, i.e. Xn
X(x) with high likelihood. In this case the model
should declare that Xn
d′ is an inlier, namely, ℓn
d′; Xn) = 0.
2. The test trace Xn
d′ falls again within the trace space spanned by the normal
behavior of customer n. However, in this case a shape-preserving shift (of
δ ∈[−∆max, ∆max] hours) in the time domain is present in the test trace
to account for exogenous factors aﬀecting the consumption patterns of the
user along the time domain. For instance, a domestic user does not necessarily use his/her home appliances at the same time during the week, but
it is often the case that such home duties follow a regular pattern in their
execution. In this case the model should be elastic enough to accommodate this time variability, focus on purely shape-related characterization
of the consumption patterns and predict that ℓn
3. The test trace Xn
d′ reﬂects a subtle energy loss over its time span with respect to a particular legitimate example in Xn. This eﬀect is symptomatic
of sophisticated manipulations by which the meter is slowed down regularly in short time intervals (e.g. by installing a circuit inside the device)
to halt the recording process and under-register the energy consumed by
the customer. Clearly, in this case the model should output ℓn
d′ = 1 depending on the ratio σ ∈(0, 1] between the overall energy of the test trace
and that of the legitimate consumption trace from where it was produced.
4. Meter tampering, by which the meter is deliberately bypassed so that the
device does not record any consumption at all. As a result, abrupt energy
losses are obtained in the data traces of the customer, which emerge in the
data trace of the day in which the tampering was performed as a series
of Zmax zero-valued samples. The model should predict ℓn
d′ = 1 for this
event, and trigger a subsequent manual inspection over the user at hand.
A good outlier detection model should take into account that the goal
of the application is to correctly predict test traces falling within any of
the above 4 categories. Therefore, the design goal can be formulated as a
multi-objective optimization problem where the optimality of the sought set
of model parameters is driven by the trade-oﬀbetween two conﬂicting objectives: the ratio of conﬁrmed outliers (TPR) and the proportion of correctly
identiﬁed inliers (TNR) when the model predicts a test set composed by D′
new consumption traces. Mathematically:
subject to Xn
d′ ∼{f 0,✓
X (x), f 0,δ
X (x), f 1,σ
X (x), f 1,z
X (x)} ∀d′ ∈{1, . . . , D′}, wherein
by a slight abuse in notation we discriminate the particular hypotheses that
each distribution models: normal behavior (f 0,✓
X (x)), shape-preserving time
variability (f 0,δ
X (x)), subtle loss (f 1,σ
X (x)) or tampering (f 1,z
X (x)). In essence:
we pursue the best model conﬁguration to detect all classes of inlier and
outlier traces in the test set, based on the trace set Xn for user n.
The above optimization problem models the conceptual, standard model
adjustment process in data mining, which can be tackled by using diﬀerent
well-known methodologies such as cross-validation . However, the design
challenge goes beyond the numerical reﬁnement of the parameters controlling
the learning process of the model itself. Since a design target is to accommodate time shifts in the load curve that are not symptomatic of NTL, we
opt for distance-based outlier detectors that leverage a similarity metric between time distances that is not aﬀected by such non-linear variations. Two
diﬀerent outlier detection schemes will be designed based on this similarity
measurement, computed not over the original data traces, but rather on their
quantized values based on the hourly statistics of Xn. The next section delves
into the details of these models, along with the utilized similarity distance
and the statistical quantization.
3. Proposed Approach
Figure 2 shows the overall processing ﬂow of the outlier detection methods
proposed in this manuscript. Four are the ingredients that lie at the core of
the developed techniques, which are described as follows:
3.1. Similarity Measure
As argued in the previous section, a elastic measure of similarity between
load proﬁles will be used to accommodate behavioral changes that do not
imply a decrease in the energy consumed by the monitored user (e.g. time
warps). To this end we will embrace the so-called Dynamic Time Warping
(DTW) measure, by which the similarity between two any given consumption
d′ (i.e. traces recorded for user n at days d and d′) can be
computed by searching for a minimum-weight optimal path P between the
(1, 1) and (N, N) vertices of a rectangular N × N grid.
The weight wi,j
associated to vertex (i, j) in this grid correspond to the Euclidean distance
between Xn
d,i (i.e. the consumption measured for user n, day d and hour i)
d′,j, namely, wi,j =
. The DTW metric between traces of
user n corresponding to day d and d′ is given by 
with P = {p1, p2, . . . , pKP} denoting a KP-long warping path composed by
steps pk = (ik, jk) (k ∈{1, . . . , KP}), and P denoting the set of all paths
through the grid fulﬁlling p1 = (1, 1), pk −pk−1 ∈{(1, 1), (0, 1), (1, 0)} and
pKP = (N, N).
When contextualized on the energy application tackled in this manuscript,
the DTW metric allows measuring the degree of dissimilarity between two
consumption traces by dismissing small behavioral shifts over the time domain and hence focusing strictly on diﬀerences in the amplitude of the energy consumed by the customer at hand. The DTW algorithm provides an
adapted metric to assess the similarity between two temporal sequences which
may vary in speed. A pattern in terms of the daily electric consumption must
be ﬂexible enough to cope with time deformations resulting from irregular
house habits or diﬀerent working schedules. Therefore, a concrete consumption pattern does not necessarily correspond to a unique feature vector in
terms of both sequence modulation and periodicity – thus considering a constant window spacing and a point-to-point deﬁnition – but rather to a shape
or a silhouette in a higher-level of abstraction that allows stretching or compressing sections of the series for comparison. In this work we postulate that
the DTW properly deals with such an assumption on the similarity between
two consumption traces under a more elastic consideration of alignment.
3.2. Statistical Trace Encoding
An optional trace encoding strategy is proposed based on the statistical
ranges spanned by the hourly measurements registered for the user at hand.
When computing the DTW metric two distinct strategies can be adopted:
the ﬁrst hinges on computing the similarity between data instances Xn
d′ by using directly the numerical values of the hourly energy consumed
by the user at hand. However, the straightforward use of unprocessed values
might yield an excessive rate of false positives as a result of the inﬂexibility
of the subsequently developed models to tolerate small amplitude deviations
of the hourly consumed energy.
In order to provide the model with a distance distribution capable of
ﬂexibly handling statistically negligible deviation in the energy values, a statistical encoding method has been also designed. This mechanism has been
devised to deliberately group together those samples likely to produce over-
ﬁtted models with minor contributions to the generalization accuracy. To
this end, for each user n, hour h and day d in the training dataset Xn, raw
d,h are ﬁrst transformed into boxplot symbols Xn,♭
d,h ∈Bn according
to statistical ranges previously computed over the training set. These ranges
are representative of the variability, dispersion and skewness of the energy
consumed by user n based on several statistical values: ﬁrst quartile Qn
2, third quartile Qn
3, lower limit Qn
3 + 1.5(Qn
1), absolute minimum Qn
.= mind,h Xn
absolute maximum Qn
.= maxd,h Xn
d,h. Based on these values computed for
user n, a mapping λn : R 7→Bn is constructed, with Bn denoting a discrete
alphabet composed by the median values of the ranges bounded by each pair
max}. For instance, if the value of Xn
between Qn
low and Qn
1, the value of Xn,♭
d,h is set to the median value of all samples in Xn that fall within this range. Once this mapping is constructed, it is
applied to the raw energy consumption traces prior to their similarity computation, eﬀectively smoothing the set of raw data traces with statistically
coherent boundaries.
3.3. Distance-based Behavioral Pattern Search
After distance computation on either the raw energy traces Xn or their
boxplot-encoded variants Xn,♭, a distance-based processing ﬂow follows with
a two-fold aim: to discern behavioral patterns within the consumption traces
of the user, and to decide whether a new trace is an outlier or an inlier
based on the learned knowledge. To this end a ﬁrst distance-based clustering
approach is included in the proposed scheme to identify those instances falling
outside the boundaries of the regions populated by regular patterns. Such
isolated observations or small-sized groups are deemed atypical behaviors
when their distance generally to any cluster center substantially exceeds the
borders conﬁned by the learned pattern . Unfortunately, it is wellknown that serious shortcomings spring up with clustering algorithms such
as K-means in the presence of outliers due to their extreme sensitiveness to
anomalies. This impacts on the ﬁnal cluster arrangement and directly aﬀects
the rate of false negatives, i.e. outliers declared as false positives.
Density-based methods, however, seek extreme observations or local instabilities with respect to neighboring values, although these observations are
not signiﬁcantly diﬀerent from the rest of the population. Under these circumstances, it is imperative to discern the notion of local and global in terms
of the adopted analytical approach. The former relies on a reference set containing all the data points to assess the outlierness of samples, whereas the
resolution of the reference set for the latter is a subset of data objects predominantly referred as their neighborhood, thus preventing the model from
a misleading dominant inﬂuence of anomalous points.
In the proposed scheme we opt for a hierarchical agglomerative distancebased clustering approach to discern behavioral patterns from the similarity
measures DTW(Xn
d′) for user n and ∀(d, d′) ∈{1, . . . , Dn}×{1, . . . , Dn}.
Here cluster or pattern stands for a group of days within the traces recorded
for user n with a consistent, warping-insensitive regularity in their consumption habits as measured by the distance space spanned by the DTW metric.
The linkage method used to compute the dissimilarity between two given
clusters Dn
c ⊆{1, . . . , Dn} and Dc′ ⊆{1, . . . , Dn} such that Dn
is given by the maximum value of DTW(Xn
d′) ∀d ∈Dn
c and ∀d′ ∈Dn
The hierarchical tree is grown automatically to yield a number of clusters
beyond which the average distance between clusters does not increase significantly. The resulting cluster space {Dn
c=1 comprises diﬀerent behavioral
patterns featured by the user over its historical consumption log. Therefore
the proposed scheme should deal with traces belonging to every pattern Dn
independently. To this end a new test trace Xn
d′ is ﬁrst mapped to a cluster
c based on their linkage distance to every cluster in {Dn
c=1, and thereafter
processed through an outlier detection model M n,c
θ ( · ) whose parameters θ
are ﬁtted to the characteristics of the cluster at hand.
3.4. Model Construction and Reﬁnement
After a cluster space {Dn
c=1 has been inferred from the similarity measures computed for all daily traces Xn – or Xn,♭– for user n, a outlier detection model M n,c
θ ( · ) should be conﬁgured and optimized for every discovered
cluster Dn
c . For this purpose two diﬀerent distance-based outlier detection
models will be considered:
• Local Outlier Factor , hereafter referred to as LOF, which is a distancebased algorithm for the detection of anomalies under the locality paradigm.
Similarly to what is assumed in density-based clustering procedures such as
DBSCAN, points belonging to a regular behavior tend to populate delimited regions characterized by a strong measure of intra-similarity (neighborhoods), whereas atypical patterns usually lie on disconnected areas far
from the aforementioned neighborhood convention. LOF avoids taking any
global assumption on the shape of the regions populated by inliers, or the
maximum distance imposed as a threshold to classify a point as an outlier. Instead, this method permits judging the outlierness level of a sample
d′ – its local outlier factor LOF(Xn
d′) – according to the local reachability
of their neighbors. This means that if adjacent counterparts of the test
sample are reachable by their own adjoined instances (under a parametric notion of closeness), it is reasonable to expect that such a object p
is correspondingly accessible at an equivalent radius distance. This permits to infer a numerical degree – the local outlier factor – that reﬂects
the level of isolation of the sample with respect to a restricted neighborhood surrounding it. The parameter set θn,LOF
of this method for cluster
c and user n consists of two main parameters: 1) the number of nearest neighbors used to deﬁne the local neighborhood of the sample; and
2) a threshold γn,LOF
∈ under which the test sample Xn
d′ is declared
to be an inlier (LOF(Xn
d′) ≤γn,LOF
, yielding ℓn
d′ = 0) or an outlier (corr.
d′) > γn,LOF
, hence ℓn
• Least Squares Approach , hereafter referred to as LSA, which is a probabilistic, nonparametric method for anomaly detection.
Kernel models
have paved the way towards more ﬂexible and robust procedures proving useful in ﬁelds such as pattern recognition, denoising or dimensionality reduction. The main goal is to infer intrinsic relations in datasets
by generating new representations capable of turning raw instances into
a collection of more discriminative data samples. LSA hinges on such an
approach to deduce the class-conditional probability of the test dataset
under an analogous assumption of density to the one considered in LOF:
regular data samples should occur in high-probability regions, whereas
anomalous data points occur in low-probability regions.
LSA leverages
this assumption and approximates the conditional probability pL|X(ℓ|x) as
pL|X(ℓ|x) ∝ξℓΘ(x), where ξℓis a vector of real-valued, class-dependent
coeﬃcients, and Θ(x) .= (Φ(x, x1), . . . , Φ(x, xY ))⊤is a vector of Y ≤D
kernel functions, with xy denoting a training sample. As was ﬁrst proven
in and reviewed in , the optimum value ξopt
of ξℓfor ℓ∈{0, 1}
is given by the minimization of a regularized squared loss with a penalty
coeﬃcient ρ that can be adjusted via cross-validation.
Once this optimum vector has been computed, an estimate of the probability of a point
belonging to class ℓ∈{0, 1} can be computed as
pL|X(ℓ|x) ≈
0 Θ(x) + ξopt
The work in extended the above concept to the scenario where not
all classes are represented in the training data but can be present in the
test data, hence being outliers with respect to the training samples. In
essence the approach proposed in this work declares that a given test sample belongs to an anomaly class whenever it occupies a region in the with
low density in the training data; to this end, the high-density regions of
the feature space is characterized by means of a optimally weighted kernel
model (following a regularized loss function similar to the one adopted in
 ), resulting in an estimated outlier conditional probability:
pL|X(1|x) ≈max
0, 1 −ξopt
which takes a value close to 0 when the instance is within a high-density
region (inlier) and close to 1 otherwise (outlier). The parameter set controlling LSA is θn,LSA
c , γn,LSA
}, where ρn
c is the regularization
penalty of the loss function, τ n
c stands for the free parameter in the selected kernel function (e.g.
the standard deviation in a Gaussian kernel Φ(x, x′) = exp(||x −x′||2/2 (τ n
c )2)), and γn,LSA
acts as a threshold on
pL|X(1|x) similarly to γn,LOF
3.5. Algorithm Description
The training, validation and test procedures of the proposed scheme when
applied to the smart meter data of user n are described in Algorithm 1, whose
lines have been split into blocks depending on the processing stage to which
they belong: similarity computation, statistical trace encoding, hierarchical
pattern search, model construction and reﬁnement, and prediction of the test
sample. Once the DTW similarity metric of every pair of daily consumption
traces (either in a raw or a precomputed fashion) is calculated in Lines 1 to 3
(following Subsections 3.1 and 3.2), behavioral patterns in the form of clusters are inferred over the distance space spanned by such similarities (Line 4,
as per Subsection 3.3). The membership to any cluster is thus delegated to
the DTW metric describing the similarity as a wider concept in terms of ﬂexibility and resilience to slight warps os local phase deviations. Following the
conventional F-fold cross-validation strategy to ensure generalizable models,
the selected algorithm (either LOF or LSA as per Subsection 3.4) is fed with
data belonging to each cluster, which eventually yields that all per-cluster
models are individually ﬁtted by means of the parameters set by the ﬁtness
metric evaluation. The prediction is accomplished by ﬁrst mapping the test
sample to its closest cluster using a complete linkage with the DTW as its
inner distance (Line 19), followed by the application of the model associated
to the cluster at hand (Line 20).
4. Experiments and Discussion
In order to assess the performance of the proposed models, several computer experiments have been carried out over real smart meter data provided
by a Spanish utility. The dataset is composed of N = 84 consumers with
their individually recorded consumption traces, containing active energy load
curves obtained by AMIs for one year and a half discretized in an hourly basis. Unfortunately, the provided dataset lacks any supervision in regards to
conﬁrmed NTL cases. Furthermore, to the best of the authors’ knowledge
there is no collection with samples labeled as inlier/outlier publicly available
for research purposes. Consequently, we have attempted at both keeping the
model from being biased and at quantitatively assessing our contribution, for
which a validation (targeted to the grid search for the optimization of the
parameters) and a test set have been emulated. These partitions have been
synthetically generated by following the same procedure: the ﬁrst part of the
datasets has been populated with real data traces, the second one with real
but warped samples (with time shifts of up to ∆max = 4 hours to evaluate
the insensitiveness of the models to warps by virtue of the DTW metric. The
third segment consists of an equivalent number of real data traces instances
modiﬁed with subtle drops in their load proﬁles, drawn uniformly at random
from the range σ ∈[0.8, 0.9]. In other words, hourly measures recorded by
the smart meter become aﬀected by random, slight decreases in their amplitude. Finally, the last part of the validation and test dasets comprises real
data traces with Zmax = 3 zeros randomly inserted through the meter as a
quantiﬁcation drift or a sharp NTL event.
Algorithm 1: Proposed outlier detection approach over AMI traces.
Input: Data traces Xn, test sample Xn
d′, number of folds F, maximum expected
shift ∆max, maximum number Zmax of zero-valued samples expected in a
conﬁrmed outlier, ratio σ of the expected energy decrease of a NTL event,
inner outlier detection model Mθ( · ).
Output: Label ℓn
d′ ∈{0, 1} of test sample.
Statistical Trace Encoding (Section 3.2, optional)
1 Compute bounding statistics {Qn
max} over Xn.
2 Encode Xn to Xn,♭via mapping λn deﬁned by the previously computed
boundaries through Bn.
Distance Matrix Computation (Section 3.1)
3 Compute DTW similarities DTW(Xn
d′) (corr. DTW(Xn,♭
∀d, d′ ∈{1, . . . , Dn} × {1, . . . , Dn}.
Hierarchical Pattern Search (Section 3.3)
4 Extract Cn clusters {Dn
c=1 via hierarchical clustering using the computed
pairwise similarities as a core distance metric for the linkage.
Cluster-wise Model Construction and Refinement (Section 3.4)
5 foreach c = 1 to Cn (clusters) do
Build a grid of ϑ possible values {θ1, . . . , θϑ} for the parameter set θ of model
( · ). Let TNR(θc
sel) = TPR(θc
sel) = 0 (selected).
foreach v = 1 to ϑ (parameter set values) do
foreach f = 1 to F (folds) do
Train a model M n,c
θv ( · ) over a random subset {Xn
c and |Dn,f
| = 0.7 · |Dn
c | (70% training partition).
Build a validation set composed by the remaining samples Dn
plus three replica of this subset with randomly emulated time shifts,
energy decreases and zeroed values driven by {∆max, σ, Zmax}.
Predict labels of the validation set via the trained model, and
compute TNRf(θv), TPRf(θv) based on their true labels.
Compute mean scores TNR(θv), TPR(θv) averaged over f ∈{1, . . . , F}.
Parameter set θc
sel for cluster c is θc
sel = arg max
v∈{1,...,ϑ}
min{TNR(θv), TPR(θv)}.
Test Sample: Cluster Mapping and Prediction
14 Assign a cluster c∗∈{1, . . . , Cn} to Xn
d′ based on its DTW-based linkage distance
to every cluster in {Dn
15 Predict the label ℓn
d′ of the test example as ℓn
d′ = M n,c∗
The primary goal of the experimental benchmark is to provide an empirically validated response to the following questions:
Q1: Do all encoding-model combinations (i.e. LOF, LSA, LOF-box, LSA-box)
perform reasonably well with respect to the targeted casuistry for NTL
events? Which dominates? In terms of which metric? (TNR/TPR)
Q2: When opting for encoding traces based on their statistical boundaries
(LOF-box, LSA-box), does it yield an enhanced robustness against false
positives? (i.e. a higher value of TNR). What is the downside in return?
Q3: How are misclassiﬁed traces distributed over the diﬀerent parts comprising the test dataset? Is there any link to the regularity of the user?
Q4: Is there any chance for increasing the performance scores in a practical
implementation of this scheme?
To this end macroscopic performance score statistics have been computed
based on the results obtained over after a previous data cleansing stage comprising corrupted data discarding.
The parameter grid {θ1, . . . , θϑ} ,over
which models for every discovered cluster were reﬁned via cross-validation,
are, for LOF, {1, 2, . . . , 20} × {0, 0.1, . . . , 1.9, 2}, where the ﬁrst term corresponds to the number of neighbors and the second one stands for the decision threshold γn,LOF
. As for models based on LSA, the parameter grid is
{0, 0.1, . . . , 0.9, 1} × {0, 0.1, . . . , 0.9, 1} × {0.5}, corresponding to ρn
for alleviating the computational complexity of the cross-validation
process. To this end the kernel estimation within LSA-based approaches was
further restrained to a maximum of 50 points instead of resorting to the
whole training set of the cluster at hand. Those representative points can be
emulated by the min(|Dn
c |, 50) medoids computed by a hierarchical clustering model, where we recall that |Dn
c | is taken as the number of samples the
training set of the cluster c. The number of folds is F = 10 in all cases.
As previously stated in Algorithm 1, the ﬁtness function quantifying the
optimality of a parameter set during the cluster-wise cross-validation process
is max{min{TNR, TPR}} for both LOF and LSA approaches. This combined
metric prevents any of the involved metrics from becoming dominated by
the other, hence forcing the model to achieve a high score in one of the two
pursued criteria to the detriment of the other.
4.1. Results and Discussion
In response to Q1, we begin our discussion by analyzing Figure 4, which
depicts a scatter plot comprising the test TNR/TPR scores attained by the
proposed methods for every user in the dataset. Also are included in the
plot ﬁtted Gaussian distributions for every score and technique via Kernel
density estimation with a bandwidth parameter equal to 1 in all cases. A
ﬁrst look on the results plotted in this ﬁgure reveals that indeed both LSA
and LOF beneﬁt from the optional statistical encoding approach (Subsection
3.2) when the focus is placed on maximizing the number of true negative
scores. This is specially notable in the case of LOF, where the average TNR
increases from 0.62 (LOF) to 0.77 (LOF-box). This, as expected, comes along
with a severe penalty in the number of detected positives, with a decrease
in average TPR from 0.70 (LOF) to 0.36 (LOF-box). This particular result
evinces the trade-oﬀbetween both scores, for which the inclusion of algorithmic design options as the statistical encoding scheme is crucial to achieve
performance scores aligned with the operational requirements. For instance,
the operator might conservatively prioritize a low number of false positives
due to internal budgetary/resource constraints for inspection tasks, hence
opting for the aforementioned encoding scheme.
Comparisons between techniques can be better analyzed by redrawing the
results in Figure 4 as a series of violin plots, i.e. an enhanced version of the
conventional boxplot with extended information about the shape of a kernel
distribution ﬁtted to the data samples. Such plots are provided in Figure 5
along with conventional boxplots overlaid over each case. In light of these
results and linking to question Q2, it can be inferred that the naive LOF and
LSA schemes in general outperform their statistically encoded counterparts in
terms of outlier detection (TPR), since they essentially yield a ﬁne-grained
adjusted model capable of discriminating slight deviations from the regular
consumption patterns of the user. However, for users with more chaotic or
unsteady patterns the parameter search procedure of the overall model fails to
ﬁnd a proper balance between sensitivity (TPR) and speciﬁcity (TNR). Due
to the fact that a portion of the validation set (and accordingly another part
of the test set) is produced by emulating minor ﬂuctuations in legitimate
consumption traces, the new data traces are likely to fall in high-density
regions already populated by legitimate user traces, hence being eventually
infeasible to draw boundaries for binary classiﬁcation. At this point it is
interesting to remark that the LSA-box scheme seems to be more resilient
to the TPR degradation expected when including the statistical encoding
within the outlier detection ﬂow, with 70% of the overall set of analyzed
users with TPR scores kept above 0.6 for this scheme.
The discussion follows by addressing question Q3; in this regard, Figure
6 depicts the distribution of the accuracy metric (i.e. the proportion of true
estimations – both positive and negative – with respect to the total number
of samples processed for each user) over the diﬀerent parts in which the
test set is divided: Region 1 (original legitimate test traces of the user),
Region 2 (original traces with random shifts in the time domain), Region 3
(subtle random perturbations in the hourly consumption value of the user)
and Region 4 (sharp zeroing of the consumption trace).
For the sake of
space and clarity results are only shown for the LSA and LSA-box schemes.
Expectedly the use of an elastic measure of similarity at the core of the
classiﬁer design implies that the score statistics between Regions 1 and 2
are similar to each other, thus evincing that the overall model is capable of
accommodating occasional behavioral changes in the consumption habits of
the user that other conventional similarity metrics (e.g. pairwise Euclidean
distance) would declare as a false positive. When focusing on Regions 3 and
4 the obtained results conﬁrm the intuition that subtle variations in Region 3
are signiﬁcantly more challenging to detect as outliers than the zeroed data
traces composing Region 4. Interestingly, accuracy scores of LSA-box for
Region 4 are lower than those of the naive LSA scheme, due to the fact that
small deviations may fall within the computed statistical boundaries driving
the trace encoding strategy of LSA-box. By contrast, zeroed samples playing
the role of malfunctions in the power quantiﬁcation or tampering (namely,
Region 4) are better detected by the LSA-box scheme, with accuracy scores
above 0.8 for 80% of the total set of users in the experimental setup.
The rationale for the diﬀerent performance patterns found between techniques over the regions of the test data traces can be also understood in
connection to the regularity of the user in his/her energy consumption patterns. When translating raw values of the consumed energy to a reduced yet
statistically meaningful alphabet, the overall dataset of the user at hand can
be explained more likely by a reduced set of patterns. A byproduct of this
simpliﬁcation is a better discrimination of outliers when they are characterized by severe amplitude drops, as distances become enlarged by virtue of the
range discretization to their median values. We exemplify this observation in
Figure 7, which shows a boxplot of the hourly energy measurements for two
diﬀerent users in the dataset considering the DTW alignment between the
data traces and the average consumption habit of every customer. As opposed to the consumption irregularity characterizing User A, User B features
relatively more stable consumption patterns, yielding signiﬁcantly better predictive scores than those obtained for user A (i.e. average TNR/TPR scores
equal to 0.95/0.93 versus 0.83/0.58 for LSA-box).
We end the discussion by elaborating on the implementation of the proposed detectors in practice (question Q4). In this context it is important
to remark that scores so far have reported for isolated daily predictions, i.e.
TNR/TPR values correspond to decisions made over one single day. This,
however, lays at an unrealistic extreme with respect to the practical implementation of the proposed detectors, in which the operator would enforce
the inspection department to investigate the equipment installed at certain
user’s premises only after a number consecutive positives have been detected
on his/her data traces.
A naive albeit insightful scheme modeling a more realistic implementation
hinges on voting by majority a number of consecutive predictions for every
user. Results shown in Figure 8 for 3 consecutively voted outcomes of the
model buttress this hypothesis: predictive scores are improved notably by
adopting this practical approach over those obtained by the model predicting
on an individual sample basis (included also in the plot for comparison).
Remarkably, LSA-box achieves TNR/TPR scores above 0.9 for at least 75%
of all users, promisingly paving the way for the deployment and operation of
this model in real smart grid scenarios.
5. Concluding Remarks and Future Research Lines
This manuscript has elaborated on the detection of NTL events in energy
consumption proﬁles captured by AMIs in Smart Grids. In particular we have
proposed a portfolio of techniques incorporating several novel ingredients over
the related literature. First, a elastic measure of similarity between consumption traces has been adopted so as to accommodate the eventual temporal
variability of the consumption patterns featured by the user under analysis,
thus enforcing the overall detector to rather focus on shape patterns within
the consumption traces disregarding the time support over which they occur.
Second, we have deﬁned an optional encoding strategy relying on boundaries
driven by the statistics of the load curves of the user, conceived as a means
to provide ﬂexibility to the overall detector against minor amplitude ﬂuctuations and consequently, to detect true negatives more reliably.
A data mining ﬂow has been built upon two different distance-based learning mechanisms (LOF and LSA) that can be adopted as its inner classiﬁcation
model, incorporating further elements (e.g. distance-based clustering and
cross-validation) aimed at a proper characterization of the user in regards
to the casuistry of NTL events targeted in the paper.
The combination
of distance-based learning algorithms and the optionality of the encoding
strategy has given rise to 4 diﬀerent schemes – namely, LOF, LSA, LOF-box
and LSA-box –, which have been described in detail throughout the article
and compared to each other over a dataset comprising real data traces of
a Spanish utility company. Results obtained therefrom have been analyzed
macroscopically by assessing how each scheme balances the trade-oﬀbetween
sensitivity and speciﬁcity when detecting emulated events reﬂecting diﬀerent
eﬀects of NTL events in the load curves. The observed performance scores
for each technique in the benchmark conﬁrms the postulated hypotheses:
the use of an elastic measure of similarity between time series reduces the
rate of false alarms due to the eventual variability of legitimate consumption
traces along time, whereas the inclusion of an statistical encoding approach
prior to distance computation enhances the reliability of the detector when
predicting legitimate traces (higher true negative rate), at the cost of a degraded discriminability of conﬁrmed NTL events (lower true positive rate).
Nevertheless, the ultimate decision concerning the selection of one model or
another (accepting possibly optimal models and discarding suboptimal ones)
is essentially a business-related matter depending on both the availability of
inspection resources and the interest of the utility company to trigger manual
inspection campaigns. Frequently, in real environments a misclassiﬁcation involves considerable inspection costs derived from checking in situ the reasons
for the predicted NTL event, hence turning the rate of false alarms into the
most critical objective. Among the methods compared in our experiments,
LSA-box stands out as the one achieving the best balance between the rate
of true positives and the rate of true negatives.
Finally, we have presented a more practical detection scheme based on
majority voting consecutive predictions of the proposed NTL detection algorithms, which has been shown to enhance the performance scores signiﬁcantly for all techniques in the benchmark, with values above 0.9 for 75% of
the users for LSA-box with just three votes in the decision. This last result is
specially encouraging for the practical deployment and operation of the proposed scheme, to which research eﬀorts will be invested in the near future.
Other aspect in the research agenda related to this work will gravitate on the
alleviation of the computational complexity characterizing the cluster-wise
parameter setting by selecting the cluster samples over which models are subsequently trained and optimized. Practical policies to periodically reschedule
the overall detector based on the prediction accuracy statistics and the feedback from inspection campaigns will be investigated. The applicability of the
proposed method to other energy-related scenarios (e.g. sub-metering, user
proﬁling, demand-side management) will be also examined.
Acknowledgments
This work has been partially supported by the Basque Government under
the ELKARTEK program ,
as well as by the Spanish Ministerio de Energ´ıa y Competitividad under the
RETOS program .