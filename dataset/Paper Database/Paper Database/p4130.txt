Recurrent Neural Networks for Time Series Forecasting: Current Status
and Future Directions
Hansika Hewamalage∗, Christoph Bergmeir, Kasun Bandara
Faculty of Information Technology, Monash University, Melbourne, Australia.
Recurrent Neural Networks (RNN) have become competitive forecasting methods, as most notably
shown in the winning method of the recent M4 competition. However, established statistical models
such as ETS and ARIMA gain their popularity not only from their high accuracy, but they are also
suitable for non-expert users as they are robust, eﬃcient, and automatic. In these areas, RNNs have
still a long way to go. We present an extensive empirical study and an open-source software framework
of existing RNN architectures for forecasting, that allow us to develop guidelines and best practices for
their use. For example, we conclude that RNNs are capable of modelling seasonality directly if the series
in the dataset possess homogeneous seasonal patterns, otherwise we recommend a deseasonalization
step. Comparisons against ETS and ARIMA demonstrate that the implemented (semi-)automatic
RNN models are no silver bullets, but they are competitive alternatives in many situations.
Time Series Forecasting, Recurrent Neural Networks
© 2020. This manuscript version is made available under the CC-BY-NC-ND 4.0 license 
org/licenses/by-nc-nd/4.0/
1. Introduction
The forecasting ﬁeld in the past has been characterised by practitioners on the one hand discarding
Neural Networks (NN) as not being competitive, and on the other hand NN enthusiasts presenting
many complex novel NN architectures, mostly without convincing empirical evaluations against simpler
univariate statistical methods. In particular, this notion was supported by many time series forecasting
competitions such as the M3, NN3 and NN5 competitions . Consequently, NNs were labelled as not suitable for forecasting .
There is a number of possible reasons for the underperformance of NNs in the past, one being
that individual time series themselves usually are too short to be modeled using complex approaches.
Another possibility may be that the time series’ characteristics have changed over time so that even
long time series may not contain enough relevant data to ﬁt a complex model.
Thus, to model
sequences by complex approaches, it is essential that they have adequate length as well as that they
are generated from a comparatively stable system. Also, NNs are further criticized for their black-box
nature . Thus, forecasting practitioners traditionally have often opted for
more straightforward statistical techniques.
However, we are now living in the Big Data era. Companies have gathered plethora of data over
the years, which contain important information about their business patterns. Big Data in the context
of time series does not necessarily mean that the individual time series contain lots of data. Rather,
it typically means that there are many related time series from the same domain. In such a context,
univariate forecasting techniques that consider individual time series in isolation, may fail to produce
∗Corresponding author. Postal Address: Faculty of Information Technology, P.O. Box 63 Monash University, Victoria
3800, Australia. E-mail address: 
 
December 24, 2020
 
reliable forecasts. They become inappropriate for the Big Data context where a single model could
learn simultaneously from many similar time series. On the other hand, more complex models such as
NNs beneﬁt most from the availability of massive amounts of data.
Therefore, researchers are now looking successfully into the possibility of applying NNs as substitutes to many other machine learning and statistical techniques. Most notably, in the recent M4
competition, a Recurrent Neural Network (RNN) was able to achieve impressive performance and win
the competition . Other examples for successful new developments in the ﬁeld are novel
architectures such as DeepAR, Multi-Quantile Recurrent Neural Network (MQRNN), Spline Quantile
Function RNNs and Deep State Space Models for probabilistic forecasting . Though there is an abundance of Machine Learning and Neural Network forecasting methods in the literature, as Makridakis et al. 
point out, the methods are typically not evaluated rigorously against statistical benchmarks and would
usually perform worse than those. This ﬁnding of Makridakis et al. was arguably one of the
main drivers for organising the M4 competition . Furthermore, often the
datasets used or the code implementations for the NN related forecasting research are not made publicly available, which poses an issue for reproducibility of the claimed performance . A lack of released code implementations also makes it diﬃcult for the forecasting community
to adapt such research work to their practical forecasting objectives.
In contrast, popular statistical models such as ETS and ARIMA that have traditionally supported
forecasting in a univariate context gain their popularity not only from their high accuracy.
also have the advantages of being relatively simple, robust, eﬃcient, and automatic, so that they
can be used by non-expert users.
For example, the forecast 
package in the R programming language implements a number of statistical
techniques related to forecasting such as ARIMA, ETS, Seasonal and Trend Decomposition using
Loess (STL Decomposition) in a single cohesive software package. This package still outshines many
other forecasting packages later developed, mainly due to its simplicity, accuracy, robustness and ease
Consequently, many users of traditional univariate techniques will not have the expertise to develop
and adapt complex RNN models. They will want to apply competitive, yet easy to use models that
can replace the univariate models they currently use in production. Therefore, regardless of the recent
successes of RNNs in forecasting, they may still be reluctant to try RNNs as an alternative since they
may not have the expert knowledge to use the RNNs adequately and achieve satisfactory accuracy. This
is also directly related to the recently emerged dispute in the forecasting community around whether
‘oﬀ-the-shelf’ deep learning techniques are able to outperform classical benchmarks.
Furthermore,
besides the abovementioned intuitions around short isolated series versus large time series databases,
no established guidelines exist as to when traditional statistical methods will outperform RNNs, and
which particular RNN architecture should be used over another or how their parameters should be
tuned to ﬁt a practical forecasting context. Although the results from the M4 forecasting competition
have clearly shown the potential of RNNs, still it remains unclear how competitive RNNs can be
in practice in an automated standard approach, without extensive expert input as in a competition
context. Therefore, it is evident that the forecasting community would beneﬁt from standard software
implementations, as well as guidelines and extensive experimental comparisons of the performance of
traditional forecasting methods and the diﬀerent RNN architectures available.
Recently, few works have addressed the development of standard software in the area. A package
has been developed by Tensorﬂow in Python, for structural time series modelling using the Tensorﬂow
Probability library . This package provides support for generating probabilistic
forecasts by modelling a time series as a sum of several structural components such as seasonality,
local linear trends, and external variables. GluonTS, a Python based open-source forecasting library
recently introduced by Alexandrov et al. is speciﬁcally designed for the purpose of supporting
forecasting researchers with easy experimentation using deep neural networks.
Our work also presents a standard software framework, but focused on RNNs and supported by
a review of the literature and implemented techniques, as well as an extensive empirical study. In
particular, our study has four main contributions. First, we oﬀer a systematic overview and categorization of the relevant literature. Secondly, we perform a rigorous empirical evaluation of a number
of the most popular RNN architectures for forecasting on several publicly available datasets for pure
univariate forecasting problems (i.e., without exogenous variables). The implemented models are standard RNN architectures with adequate preprocessing without considering special and sophisticated
network types such as in Smyl ’s work involving RNNs. This is mainly because our motivation
in this study is to evaluate how competitive are the oﬀ-the-shelf RNN techniques for forecasting against
traditional univariate techniques and thus provide insights to non-expert forecasting practitioners to
start using RNNs. We compare the performance of the involved models against two state-of-the-art
statistical forecasting benchmarks, namely the implementations of ETS and ARIMA models from the
forecast package. We stick to single seasonality forecasting problems to be able to compare with those two benchmarks. Although the state of the art in forecasting can
now be seen as the methods of the M4 competition winners, most notably Smyl and Montero-
Manso et al. , we do not present comparisons against those methods as they are not automated
in a way that they would be straightforwardly applicable to other datasets, and the idea of our research
is to tune RNNs to replace the fully automatic univariate benchmarks that are being used heavily by
practitioners for everyday forecasting activities outside of competition environments. Thirdly, based
on the experiments we oﬀer conclusions as a best practices guideline to tune the networks in general.
We introduce guidelines at every step of the RNN modelling from the initial preprocessing of the data
to tuning the hyperparameters of the models. The methodologies are generic in that they are not
tied to any speciﬁc domain. Thus, the main objective of our study is to make this work reproducible
by other practitioners for their practical forecasting tasks. Finally, all implementations are publicly
available as a cohesive open-source software framework1.
The rest of the paper is structured as follows. We ﬁrst give a comprehensive background study
of all related concepts in Section 2, including the traditional univariate forecasting techniques and
diﬀerent NN architectures for forecasting mentioned in the literature. Section 3 presents the details of
the methodology employed, including the RNN architectures implemented and the corresponding data
preprocessing techniques. In Section 4, we explain the experimental framework used for the study with
a description of the used datasets, the training, validation and testing speciﬁcs and the comparison
benchmarks used. In Section 5, we present a critical analysis of the results followed by the conclusions
in Section 6, and future directions in Section 7.
2. Background Study
This section details the literature related to Univariate Forecasting, Traditional Univariate Forecasting Techniques, Artiﬁcial Neural Networks (ANN) and leveraging cross-series information when
using ANNs.
2.1. Univariate Forecasting
A purely univariate forecasting problem refers to predicting future values of a time series based
on its own past values. That is, there is only one time dependent variable. Given the target series
X = {x1, x2, x3, ..., xt, ..., xT } the problem of univariate forecasting can be formulated as follows:
{xT +1, ..., xT +H} = F(x1, x2, ..., xT ) + ϵ
Here, F is the function approximated by the model developed for the problem with the variable
X. The function predicts the values of the series for the future time steps from T + 1 to T + H, where
H is the intended forecasting horizon. ϵ denotes the error associated with the function approximation
1Available at: 
2.2. Traditional Univariate Forecasting Techniques
Time series forecasting has been traditionally a research topic in Statistics and Econometrics, from
simple methods such as Seasonal Na¨ıve and Simple Exponential Smoothing, to more complex ones
such as ETS and ARIMA . In general, traditional univariate
methods were on top in comparison to other computational intelligence methods at many forecasting
competitions including NN3, NN5 and M3 . The
beneﬁt of the traditional univariate methods is that they work well when the volume of the data is
minimal . The number of parameters to be determined in these techniques is quite
low compared to other complex machine learning techniques. However, such traditional univariate
techniques introduced thus far lack few key requirements involved with complex forecasting tasks.
Since one model is built per each series, a frequent retraining is required which is compute intensive
especially in the case of massive time series databases.
Also, these univariate techniques are not
meant for exploiting cross-series information using global models since they take into account only the
features and patterns inherent in a single time series at a time. This is not an issue if the individual
time series are long enough having many data points so that the models become capable of capturing
the sequential patterns. However in practice this is usually not the case. On the other hand, learning
from many time series can be eﬀectively used to address such problems of limited data availability in
the single series.
2.3. Artiﬁcial Neural Networks
With the ever increasing availability of data, ANNs have become a dominant and popular technique
for machine learning tasks in the recent past. A Feed Forward Neural Network (FFNN) is the most
basic type of ANN. It has only forward connections in between the neurons contrary to the RNNs which
have feedback loops. There are a number of works where ANNs are used for forecasting. Zhang et al.
 provide a comprehensive summary of such work in their paper. According to these authors,
ANNs possess various appealing attributes which make them good candidates for forecasting against
the aforementioned statistical techniques. First, ANNs can model any form of unknown relationship in
the data with minimum a-priori assumptions. Second, ANNs can generalize and transfer the learned
relationships to unseen data. Third, ANNs are universal approximators, meaning that they are capable
of modelling any form of relationship in the data, especially non-linear relationships . The range of functions modelled by an ANN is much higher than the range covered by the
statistical techniques.
The studies of Tang et al. to compare ANNs against the Box-Jenkins methodology for
forecasting establish that ANNs are comparatively better for forecasting problems with long forecasting
horizons. Claveria and Torra derive the same observations with respect to a tourism demand
forecasting problem. However, determining the best network structure and the training procedure for
a given problem are crucial to tune ANNs for maximal accuracy. An equally critical decision is the
selection of the input variables for the modelling . ANNs have been applied
for forecasting in a multitude of domains over the years. Mandal et al. use an ANN for electric
load forecasting along with Euclidean Norm applied on weather information of both the training data
as well as the forecasting duration to derive similarity between data points in the training data and the
forecasts. This technique is called the similar days approach. Eﬀorts have also been made to minimize
the manual intervention in the NN modelling process to make it automated. To this end, Yan 
propose a new form of ANN named as the Generalized Regression Neural Network (GRNN) which
is a special form of a Radial Basis Function (RBF) Network. It requires the estimation of just one
design parameter, the Spread Factor which decides the width of the RBF and consequently how much
of the training samples contribute to the output. A hybrid approach to forecasting is proposed by
Zhang combining an ARIMA model with an ANN and thus leveraging the strengths of both
models. The ARIMA model is used to model the linear component of the time series while the ANN
can then model the residual which corresponds to the non-linear part. This approach is closely related
to boosting, commonly used as an ensembling technique to reduce bias in predictions. The idea has
been inspired from the common concept that a combination of several models can often outperform the
individual models in isolation. Zhang and Berardi and Rahman et al. have also worked
using ANNs for forecasting, speciﬁcally with ensembles.
The most common way to feed time series data into an ANN, speciﬁcally an FFNN is to break
the whole sequence into consecutive input windows and then get the FFNN to predict the window or
the single data point immediately following the input window. Yet, FFNNs ignore the temporal order
within the input windows and every new input is considered in isolation . No
state is carried forward from the previous inputs to the future time steps. This is where RNNs come
into play; a specialized NN developed for modelling data with a time dimension.
2.3.1. Recurrent Neural Networks for Forecasting
RNNs are the most commonly used NN architecture for sequence prediction problems. They have
particularly gained popularity in the domain of natural language processing. Similar to ANNs, RNNs
are universal approximators as well. However, unlike ANNs, the
feedback loops of the recurrent cells inherently address the temporal order as well as the temporal
dependencies of the sequences .
Every RNN is a combination of a number of RNN units. The most popular RNN units commonly
used for sequence modelling tasks are the Elman RNN cell, Long Short-Term Memory (LSTM) cell
and the Gated Recurrent Unit (GRU) . Apart from them, other variants have been introduced such as Depth Gated LSTM, Clockwork
RNN, Stochastic Recurrent Networks and Bidirectional RNN . Nevertheless, these latter RNN units have
hardly been used in the forecasting literature. They were mostly designed with language modelling
tasks in mind.
Jozefowicz et al. perform an empirical evaluation of diﬀerent RNN cells. In particular, those
authors aim to ﬁnd an RNN unit that performs better than the LSTM and GRU in three deﬁned tasks
namely, arithmetic computations, XML modelling where the network has to predict the next character
in a sequence of XML data and a language modelling task using the Penn TreeBank dataset. They do
not cover time series forecasting in particular. The work by Bianchi et al. speciﬁcally targets
the problem of short-term load forecasting. Their experiments systematically test the performance of
all three of the popular recurrent cells, Elman RNN (ERNN) cell, LSTM cell and the GRU cell, and
compare those with Echo State Networks (ESN) and the Non-linear Autoregressive with eXogenous
(NARX) inputs Network. The experiments are performed on both synthetic time series as well as
real-world time series. From the results, the authors conclude that both LSTM and GRU demonstrate
similar performance in the chosen datasets. In essence, it is hard to diﬀerentiate which one is better
in which scenario. Additionally, the ERNN shows a comparable performance to the gated RNN units,
and it is faster to train. The authors argue that gated RNN units can potentially outperform ERNNs
in language modelling tasks where the temporal dependencies can be highly non-linear and abrupt.
Furthermore, the authors state that gradient-based RNN units (ERNN, LSTM, GRU) are relatively
slow in terms of training time due to the time-consuming backpropagation through time procedure.
A recurrent unit can constitute an RNN in various types of architectures. A number of diﬀerent
RNN architectures for forecasting can be found in the literature.
Although most commonly used
for natural language processing tasks, these architectures are used in diﬀerent time series forecasting
tasks as well. The stacked architecture can be claimed as the most commonly used architecture for
forecasting with RNNs. Bandara et al. employ the Stacked model in their work on using a
clustering approach for grouping related time series in forecasting.
Due to the vanishing gradient
problem existent in the vanilla RNN cells, LSTM cells (with peephole connections) are used instead.
The method includes several essential data preprocessing steps and a clustering phase of the related
time series to exploit cross-series information, and those authors demonstrate signiﬁcant results on both
the CIF 2016 as well as the NN5 forecasting competition datasets. In fact, it is the same architecture
used by Smyl to win the CIF 2016 forecasting competition.
In the competition, Smyl has
developed two global models, one for the time series with forecasting horizon 12 and the other one for
the time series with horizon 6. The work by Smyl and Kuber closely follows the aforementioned
stacked architecture with a slight modiﬁcation known as the skip connections. Skip connections allow
layers far below in the stack to directly pass information to layers well above and thus minimize the
vanishing gradient eﬀect. With the skip connections added, the architecture is denoted as the ResNet
architecture which is adapted from the work of He et al. originally for image recognition.
Another RNN architecture popular in neural machine translation tasks is the Sequence to Sequence
(S2S) architecture introduced by Sutskever et al. . The overall model has two components, the
encoder and the decoder which both act as two RNN networks on their own.
Peng et al. 
apply a S2S architecture for host load prediction using GRU cells as the RNN unit in the network.
Those authors test their approach using two datasets, the Google clusters dataset and the Dinda
dataset from traditional Unix systems.
The approach is compared against two other state-of-theart RNN models, an LSTM-based network and the ESN. According to the results, the GRU based
S2S model manages to outperform the other models in both the datasets. The DeepAR model for
probabilistic forecasting, developed by Salinas et al. for Amazon also uses a S2S architecture
for prediction. The authors of that work use the same architecture for both the encoder and the decoder
components, although in practice the two components usually diﬀer. Therefore, the weights of both
the encoder and the decoder are the same. Wen et al. develop a probabilistic forecasting model
using a slightly modiﬁed version of a S2S network. As teacher-signal enforcing of the decoder using
autoregressive connections generally leads to error accumulation throughout the prediction horizon,
those authors use a combination of two Multi Layer Perceptrons (MLP) for the decoder; a global MLP
which encapsulates the encoder outputs and the inputs of the future time steps and a local MLP which
acts upon each speciﬁc time step of the prediction horizon to generate the quantiles for that point.
Since the decoder does not use any autoregressive connections, the technique is called the Direct Multi
Horizon Strategy.
More recently, S2S models aka autoencoders are used in forecasting to extract time series features
followed by another step to generate the actual predictions. Zhu and Laptev use an autoencoder
to address the issue of uncertainty, speciﬁcally in the form of model misspeciﬁcation. The models that
are ﬁtted using the training data may not be the optimal models for the test data, when the training
data patterns are diﬀerent from the test data. The autoencoder can alleviate this by extracting the
time series features during training and calculating the diﬀerence between these features and the
features of the series encountered during testing. This gives the model the intuition of how diﬀerent
the training and test data partitions are. Laptev et al. incorporate an autoencoder to train a
global model across many heterogeneous time series. The autoencoder in this context is expected to
extract the features of each series which are later concatenated to the input window for forecasting by
an LSTM model. Likewise, apart from direct prediction, the S2S model is also used in intermediate
feature extraction steps prior to the actual forecasting.
Bahdanau et al. and Luong et al. present two diﬀerent variants of the S2S model,
with attention mechanisms. The fundamental idea behind these attention mechanisms is to overcome
the problem in S2S models that they encode all the information in the whole time series to just a single
vector and then decode this vector to generate outputs. Embedding all the information in a ﬁxed-size
vector can result in information loss . Hence, the attention model tries to overcome
this by identifying important points in the time series to pay attention to. All the points in the time
series are assigned weights which are computed for each output time step and the more important
points are assigned higher weights than the less important ones. This method has been empirically
proven to be heavily successful in various neural machine translation tasks where the translation of
each word from one language to another requires speciﬁc attention on particular words in the source
language sentence.
Since time series possess seasonality components, attention weights as described above can be used
in a forecasting context. For instance, if a particular monthly time series has a yearly seasonality,
predicting the value for the next immediate month beneﬁts more from the value of the exact same
month of the previous year. This is analogous to assigning more weight to the value of the sequence
exactly 12 months ago. The work of Suilin done for the Kaggle challenge of Wikipedia Web
Traﬃc Forecasting encompasses this idea . However, that author’s work does not use the
traditional Bahdanau et al. or Luong et al. attention since those techniques require recomputation of the attention weights for the whole sequence at each time step, which is computationally
expensive. There are two variants in the attention mechanism proposed by Suilin . In the ﬁrst
method, the encoder outputs that correspond to important points identiﬁed before are directly fed as
inputs to the respective time steps in the decoder. In the second method, the important points concept
is relaxed to take a weighted average of those important points along with their two neighbouring
points. This scheme helps to account for the noise in the time series as well as to cater for the diﬀerent
lengths of the months and the leap years. This latter attention has been further extended to form a
type of hierarchical attention using an interconnected hierarchy of 1D convolution layers corresponding
to weighted averaging the neighbouring points and max pooling layers in between. Despite using the
same weights for all the forecasting steps at the decoder, Suilin claims that this reduces the
error signiﬁcantly.
Cinar et al. use a more complex attention scheme to treat the periods in the time series. The
idea of those authors is that the classical attention mechanism proposed by Bahdanau et al. 
is intended for machine translation and therefore it does not attend to the seasonal periods in time
series in a forecasting condition. Hence, they propose an extension to the Bahdanau-style attention
and name it as Position-based Content Attention Mechanism. An extra term (π(1)) is used in the
attention equations to embed the importance of each time step in the time series to calculate the
outputs for the forecast horizon. Based on whether a particular time step in the history corresponds
to a pseudo-period of the output step or not, the modiﬁed equation can be used to either augment
or diminish the eﬀect of the hidden state. The vector π(1) is expected to be trained along with the
other parameters of the network. Those authors perform experiments using their proposed technique
over six datasets, both univariate and multivariate. They further compare the resulting error with the
traditional attention mechanism as well as ARIMA and Random Forest. The results imply that the
proposed variant of the attention mechanism is promising since it is able to surpass the other models
in terms of accuracy for ﬁve of the tested six datasets. Furthermore, the plots of the attention weights
of the classical attention scheme and the proposed variant denote that by introducing this variant, the
pseudo-periods get assigned more weight whereas in the na¨ıve attention, the weights are increasing
towards the time steps closer to the forecast origin.
Another work by Qin et al. proposes a dual-stage attention mechanism (DA-RNN) for
multivariate forecasting problems. In the ﬁrst stage of the attention which is the input attention,
diﬀerent weights are assigned to diﬀerent driving series based on their signiﬁcance to contribute towards
forecasting at each time step. This is done prior to feeding input to the encoder component of the S2S
network. The input received by each time step of the encoder is a set of values from diﬀerent exogenous
driving series whose inﬂuence is suﬃciently increased or decreased. The weights pertaining to each
driving series’ values are derived using another MLP which is trained along with the S2S network.
In the second stage of attention which is the temporal attention, a usual Bahdanau-style attention
scheme is employed to assign weights to the encoder outputs at each prediction step. In terms of the
experiments, the model is tested using two datasets. An extensive comparison is performed among
many models including ARIMA, NARX RNN, Encoder Decoder, Attention RNN, Input Attention
RNN and the Dual Stage Attention RNN. The Dual Stage Attention model is able to outperform all
the other models in both the datasets. Further experiments using noisy time series reveal that the
Dual Stage Attention Model is comparatively robust to noise.
More recently, Liang et al. have developed a multi-level attention network named GeoMAN
for time series forecasting of Geo-sensory data. According to those authors, the signiﬁcance of that
model in comparison to the DA-RNN model is that this model explicitly handles the special characteristics inherent in Geo-sensory data such as the spatio-temporal correlation. While the DA-RNN has
a single input attention mechanism to diﬀerentiate between diﬀerent driving series, the GeoMAN has
two levels of spatial attention; the local spatial attention which highlights diﬀerent local time series
produced by the same sensor based on their importance to the target series and the global spatial
attention which does the same for the time series produced by diﬀerent surrounding sensors globally.
The concatenation of the context vectors produced by these two attention mechanisms is then fed to
a third temporal attention mechanism used on the decoder. Those authors emphasize the importance
of the two-fold spatial attention mechanism as opposed to one input attention as in the work by Qin
et al. since the latter treats both local and global time series as equal in its attention technique.
The underlying attention scheme used for all three attention steps follows a Bahdanau-style additive
scheme. Empirical evidence on two Geo-sensory datasets have proven the proposed GeoMAN model
to outperform other state-of-the-art techniques such as S2S models, ARIMA, LSTM, and DA-RNN.
Apart from using individual RNN models, several researchers have also experimented using ensembles of RNN models for forecasting. Ensembles are meant for combining multiple weak learners
together to generate a more robust prediction and thus reduce the individual training time of each
base RNN. Smyl segments the problem into two parts; developing a group of specialized RNN
models and ensembling them to derive a combined prediction. That author’s methodology follows the
idea that general clustering based on a standard metric for developing ensembles does not oﬀer the best
performance in the context of forecasting. Therefore this approach randomly assigns time series of the
dataset to one of the RNNs in the pool for training at the ﬁrst epoch. Once all networks are trained,
every time series in the dataset is assigned to a speciﬁc N number of best networks which give the
minimum error after the training in that epoch. RNNs are trained in this manner iteratively using the
newly allocated series until the validation error grows or the number of epochs ﬁnishes. The ensembling
includes using another network to determine which RNN in the pool should make the forecasts for a
given series or feeding the forecasts of each network to another network to make the ﬁnal predictions
or using another network to determine the weights to be assigned to the forecasts of each RNN in the
pool. Despite the complexity associated with these techniques, that author claims that none of them
work well. Simple techniques such as average, weighted average (weight based on training loss of each
network or the rate of becoming a best network) of all the networks or the weighted average of the N
best networks work well on the other hand. This methodology demonstrates encouraging results on
the monthly series of the M3 competition dataset. The RNNs used for the pool comprise of a stacked
LSTM with skip connections. This technique is also used in the winning solution of Smyl at
the M4 competition which proves that this approach works well.
Krstanovic and Paulheim suggest that the higher the diversity of the base learners, the
better the accuracy of the ﬁnal ensemble.
Those authors develop an ensemble using the method
known as Stacking where a number of base LSTM learners are combined using a meta-learner whose
inputs are the outputs of the base learners. The outputs of the meta-learner correspond to the ﬁnal
forecasts expected. The dissimilarity of the base learners is enforced by using a range of values for
the hyperparameters such as dropout rate, number of hidden layers, number of nodes for the layers
and the learning rate. For the meta-learner those authors use Ridge Regression, eXtreme Gradient
Boosting (XGBoost) and Random Forest. The method is compared with other popular techniques
such as LSTM, Ensemble via mean forecast, Moving Average, ARIMA and XGBoost on four diﬀerent
datasets. The suggested ensemble with Stacking in general performs best.
A modiﬁed boosting algorithm for RNNs in the time series forecasting context is proposed by
Assaad et al. based on the AdaBoost algorithm. The novelty aspect of this approach is that the
whole training set is considered for model training at each iteration by using a specialized parameter
k which controls the weight exerted on each training series based on its error in the previous iteration.
Therefore, in every iteration, the series that got a high error in the previous iteration are assigned
more weight.
A value of 0 for k, assigns equal weights for all the time series in the training set.
Furthermore, the base models in this approach are merged together using a weighted median which
is more robust when encountered with outliers as opposed to weighted mean. Experiments using two
time series datasets show promising performance of this ensemble model on both single-step ahead
and multi-step ahead forecasting problems, compared to other models in the literature such as MLP
or Threshold Autoregressive (TAR) Model.
2.4. Leveraging Cross-Series Information
Exploiting cross-series information in forecasting is an idea that gets increased attention lately,
especially in the aftermath of the M4 competition. The idea is that instead of developing one model
per each time series in the dataset, a model is developed by exploiting information from many time
series simultaneously. In the literature such models are often referred to as global models whereas
univariate models which build a model per every series are known as local models . However, the application of global models to a set of time series does not indicate any
interdependence between them with respect to the forecasts. Rather, it means that the parameters
are estimated globally for all the time series available .
The former is
basically the scenario of multivariate forecasting where the value of one time series is driven by other
external time varying variables. Such relationships are directly modelled in the forecast equations of
the method. Yet, a global model trained across series usually works independently on the individual
series in a univariate manner when producing the forecasts.
In modern forecasting problems, often the requirement is to produce forecasts for many time series
which may have similar patterns, as opposed to forecasting just one time series. One common example
in the domain of retail is to produce forecasts for many similar products. In such scenarios, global
models can demonstrate their true potential by learning across series to incorporate more information,
in comparison to the local methods. Trapero et al. use a similar idea in their work for demand
forecasting of stock-keeping units (SKUs). In particular, for SKUs with limited or no promotional
history associated with it, the coeﬃcients of the regression model are calculated by pooling across
many SKUs. However, the regression model used can capture only linear relationships and it does not
maintain any internal state per each time series.
In recent literature, researchers have used the idea of developing global models in the context of
deep neural networks. For RNNs, this means that the weights are calculated globally, yet the state is
maintained per each time series. The winning solution by Smyl at the M4 forecasting competition uses the global model concept with local parameters as well, to cater for individual requirements
of diﬀerent series. Bandara et al. do this by clustering groups of related time series. A global
model is developed per each cluster. Salinas et al. also apply the idea of cross-series information
in their DeepAR model for probabilistic forecasting. More recently, Wen et al. ; Rangapuram
et al. ; Wang et al. and Oreshkin et al. also employ the cross series learning concept in their work using deep neural networks for forecasting. Bandara et al. develop global
models for forecasting in an E-commerce environment by considering sales demand patterns of similar
3. Methodology
In this section, we describe the details of the methodology employed in our comprehensive experimental study. We present the diﬀerent recurrent unit types, the RNN architectures as well as the
learning algorithms that we implement and compare in our work.
3.1. Recurrent Neural Networks
Our work implements a number of RNN architectures along with diﬀerent RNN units. These are
explained in detail next.
3.1.1. Recurrent Units
Out of the diﬀerent RNN units mentioned in the literature, we select the following three types of
recurrent units to constitute the layers of the RNNs in our experiments.
• Elman Recurrent Unit
• Gated Recurrent Unit
• Long Short-Term Memory with Peephole Connections
The base recurrent unit is introduced by Elman . The structure of the basic ERNN cell is as
shown in Figure 1.
Figure 1: Elman Recurrent Unit
ht = σ(Wi · ht−1 + Vi · xt + bi)
zt = tanh(Wo · ht + bo)
In Equations 2a and 2b, ht ∈Rd denotes the hidden state of the RNN cell (d being the cell
dimension). This is the only form of memory in the ERNN cell. xt ∈Rm (m being the size of the
input) and zt ∈Rd denote the input and output of the cell at time step t. Wi ∈Rd×d and Vi ∈Rd×d
denote the weight matrices whereas bi ∈Rd denotes the bias vector for the hidden state. Likewise,
Wo ∈Rd×d and bo ∈Rd signify the weight matrix and the bias vector of the cell output. The current
hidden state depends on the hidden state of the previous time step as well as the current input. This
is supported with the feedback loops in the RNN cell connecting its current state to the next state.
These connections are of extreme importance to consider past information in updating the current cell
state. In the experiments, we use the sigmoid function (indicated by σ) as the activation of the hidden
state and the hyperbolic tangent function (indicated by tanh) as the activation of the output.
The ERNN Cell suﬀers from the well known vanishing gradient and exploding gradient problems
over very long sequences. This implies that the simple RNN cells are not capable of carrying long
term dependencies to the future. When the sequences are quite long, the backpropagated gradients
tend to diminish (vanish) and consequently the weights do not get updated adequately. On the other
hand, when the gradients are huge, they may burst (explode) over long sequences resulting in unstable
weight matrices. Both these issues are ensued from the gradients being intractable and hinder the
ability of the RNN cells to capture long term dependencies.
Over the years, several other variations have been introduced to this base recurrent unit addressing
its shortcomings. The LSTM cell introduced by Hochreiter and Schmidhuber is perhaps the
most popular cell for natural language processing tasks due to its capability to capture long-term
dependencies in the sequence while alleviating gradient vanishing issues. The structure of the LSTM
Cell is illustrated in Figure 2
Figure 2: Basic Long Short-Term Memory Unit
it = σ(Wi · ht−1 + Vi · xt + bi)
ot = σ(Wo · ht−1 + Vo · xt + bo)
ft = σ(Wf · ht−1 + Vf · xt + bf)
˜Ct = tanh(Wc · ht−1 + Vc · xt + bc)
Ct = it ⊙˜Ct + ft ⊙Ct−1
ht = ot ⊙tanh(Ct)
Compared to the basic RNN cell, the LSTM cell has two components to its state, the hidden state
and the internal cell state where the hidden state corresponds to the short-term memory component
and the cell state corresponds to the long-term memory. With its Constant Error Carrousel (CEC)
capability supported by the internal state of the cells, LSTM avoids the vanishing and exploding
gradient issues. Moreover a gating mechanism is introduced which comprises of the input, forget and
the output gates. In the Equations 3a - 3g, ht ∈Rd is a vector which denotes the hidden state of the
cell, where d is the cell dimension. Similarly Ct ∈Rd is the cell state and ˜Ct ∈Rd is the candidate cell
state at time step t which captures the important information to be persisted through to the future.
xt ∈Rd and zt ∈Rd are the same as explained for the basic RNN cell. Wi, Wo, Wf, Wc ∈Rd×d
denote the weight matrices of the input gate, output gate, forget gate and the cell state respectively.
Likewise,Vi, Vo, Vf, Vc ∈Rd×d and bi, bo, bf, bc ∈Rd denote the weight matrices corresponding to the
current input and the bias vectors respectively. it, ot, ft ∈Rd are the input, output and forget gate
The activation function σ of the gates denotes the sigmoid function which outputs values in the
range . In Equation 3e, the input and the forget gates together determine how much of the
past information to retain in the current cell state and how much of the current context to propagate
forward to the future time steps. ⊙denotes the element wise multiplication which is known as the
Hudmard Product. A value of 0 in forget gate ft denotes that nothing should be carried forward
from the previous cell state. In other words, the previous cell state should be completely forgotten
in the current cell state. Following this argument, a value of 1 implies that the previous cell state
should be completely retained. The same notion holds for the other two gates it and ot. A value in
between the two extremes of 0 and 1 for both the input and forget gates can carefully control the value
of the current cell state using only the important information from both the previous cell state and
the current candidate cell state. For the candidate cell state, the activation function is a hyperbolic
tangent function which outputs values in the range [-1, 1]. An important diﬀerence of the LSTM cell
compared to the simple RNN cell is that its output zt is equal to the hidden state ht.
In this review, we use the variant of the vanilla LSTM cell known as the LSTM cell with peephole
connections. In this LSTM unit, the states are updated as per the following equations.
it = σ(Wi · ht−1 + Vi · xt + Pi · Ct−1 + bi)
ot = σ(Wo · ht−1 + Vo · xt + Po · Ct + bo)
ft = σ(Wf · ht−1 + Vf · xt + Pf · Ct−1 + bf)
˜Ct = tanh(Wc · ht−1 + Vc · xt + bc)
Ct = it ⊙˜Ct + ft ⊙Ct−1
ht = ot ⊙tanh(Ct)
The diﬀerence is that the peephole connections let the forget and the input gates of the cell look at
the previous cell state Ct−1 before updating it. In the Equations 4a to 4g, Pi, Po, Pf ∈Rd×d represent
the weight matrices of the input, output, and forget gates respectively. As for the output gate, it can
now inspect the current cell state for generating the output. The implementation used in this research
is the default implementation of peephole connections in the Tensorﬂow framework.
The GRU is another variant introduced by Cho et al. which is comparatively simpler than
the LSTM unit as well as faster in computations. This is due to the LSTM unit having three gates
within the internal gating mechanism, whereas the GRU has only two, the update gate and the reset
gate. The update gate in this unit plays the role of the forget gate and the input gate combined.
Furthermore, similar to the vanilla RNN cell, the GRU cell also has only one component to the state,
i.e the hidden state. Figure 3 and the equations display the functionality of the GRU cell.
Figure 3: Gated Recurrent Unit
ut = σ(Wu · ht−1 + Vu · xt + bu)
rt = σ(Wr · ht−1 + Vr · xt + br)
˜ht = tanh(Wh · rt · ht−1 + Vh · xt + bh)
ht = ut ⊙˜ht + (1 −ut) ⊙ht−1
ut, rt ∈Rd denote the update and reset gates respectively. ˜ht ∈Rd indicates the candidate hidden
state and ht ∈Rd indicates the current hidden state at time step t. The weights and biases follow
the same notation as mentioned before. The reset gate decides how much of the previous hidden state
contributes to the candidate state of the current step. Since the update gate functions alone without
a forget gate, (1 −ut) is used as an alternative. The GRU has gained a lot of popularity owing to its
simplicity (lesser no. of parameters) compared to the LSTM cell and also its eﬃciency in training.
3.1.2. Recurrent Neural Network Architectures
We use the following RNN architectures for our study.
Stacked Architecture
The stacked architecture used in this study closely relates to the architecture mentioned in the work
of Bandara et al. . It is as illustrated in Figure 5.
Figure 4 shows the folded version of the RNN while Figure 5 demonstrates the unfolded version
through time. The idea is that the same RNN unit repeats for every time step, sharing the same
weights and biases between each of them. The feedback loop of the cell helps the network to propagate
the state ht to the future time steps. For the purpose of generalization only the state ht is shown in
the diagram; However, for an LSTM cell, ht should be accompanied by the cell state Ct. The notion
Figure 4: Folded Version of RNN
Figure 5: Stacked Architecture
of stacking means that multiple LSTM layers can be stacked on top of one another. In the most basic
setup, the model has only one LSTM layer. Figure 5 is for such a basic case, whereas Figure 6 is for a
multi-layered scenario. As seen in Figure 6, for many hidden layers the same structure as in Figure 5
is repeated multiple times stacked on top of each other where the output from every layer is directly
fed as input to the next immediate layer above and the ﬁnal forecasts retrieved from the last layer.
As seen in Figure 5, Xt denotes the input to the cell at time step t and ˆYt corresponds to the
output. Xt and ˆYt used for the stacking architecture are vectors instead of single data points. This
is done according to the moving window scheme explained later in Section 4.2.5. At every time step,
the cell functions using its existing weights and produces the output Zt which corresponds to the next
immediate output window of the time series. Likewise, the output of the RNN cell instance of the ﬁnal
time step ˆYT corresponds to the expected forecasts for the particular time series. However, the output
of the cell does not conform to the expected dimension which is the forecasting horizon (H). Since
the cell dimension (d) is an externally tuned hyperparameter it may take on any appropriate value.
Therefore, to project the output of the cell to the expected forecasting horizon, an aﬃne neural layer
is connected on top of every recurrent cell, whose weights are trained altogether with the recurrent
network itself. In Figure 5, this fully connected layer is not shown explicitly and the output ˆYt ∈RH
corresponds to the output of the combined RNN cell and the dense layer.
Figure 6: Multi-layer Stacked Architecture
During the model training process, the error is calculated per each time step and accumulated until
the end of the time series. Let the error per each time step t be et. Then,
et = Yt −ˆYt
where Yt is the actual output vector at time step t, also preprocessed according to the moving
window strategy. For all the time steps, the accumulated error E can be deﬁned as follows.
At the end of every time series, the accumulated error E is used for the Backpropagation Through
Time (BPTT) once for the whole sequence. This BPTT then updates the weights and biases of the
RNN cells according to the optimizer algorithm used.
Sequence to Sequence Architecture
The S2S architecture used in this study is as illustrated in Figure 7. There are few major diﬀerences of
the S2S network compared to the Stacked architecture. The ﬁrst is the input format. The input xt fed
to each cell instance of this network is a single data point instead of a vector. Stated diﬀerently, this
network does not use the moving window scheme of data preprocessing. The RNN cells keep getting
input at each time step and consequently build the state of the network. This component is known as
the Encoder. However, in contrast to the Stacked architecture, the output is not considered per each
time step; rather only the forecasts produced after the last input point of the Encoder are considered.
Figure 7: Sequence to Sequence with Decoder Architecture
Here, every yt corresponds to a single forecasted data point in the forecast horizon. The component
that produces the outputs in this manner is called the Decoder.
The Decoder comprises of a set of RNN cell instances as well, one per each step of the forecast
horizon. The initial state of the Decoder is the ﬁnal state built from the Encoder which is also known as
the context vector. A distinct feature of the Decoder is that it contains autoregressive connections from
the output of the previous time step into the input of the cell instance of the next time step. During
training, these connections are disregarded and the externally fed actual output of each previous time
step is used as means of teacher forcing. This teaches the Decoder component how inaccurate it is
in predicting the previous output and how much it should be corrected. During testing, since the
actual targets are not available, the autoregressive connections are used instead to substitute them
with the generated forecasts. This is known as scheduled sampling where a decision is taken either to
sample from the outputs or the external inputs at the Decoder. The decoder can also accept external
inputs to support exogenous variables whose values are known for the future time steps. Since the
hidden state size may not be equal to 1, which is the expected output size of each cell of the Decoder,
an aﬃne neural layer is applied on top of every cell instance of the Decoder similar to the Stacked
architecture. The error computed for backpropagation in the S2S architecture diﬀers from that of the
Stacked architecture since no error accumulation happens over the time steps of the Encoder. Only
the error at the Decoder is considered for the loss function of the optimizer.
There are basically two types of components for output in a S2S network. The most common is the
Decoder as mentioned above. Inspired by the work of Wen et al. , a dense layer can also be used
in place of the Decoder. We implement both these possibilities in our experiments. However, since
we do not feed any future information into the decoder, we do not use the concept of local MLP as in
Wen et al. and use only one global MLP for the forecast horizon. This technique is expected
to obviate the error propagation issue in the Decoder with autoregressive connections. This model is
illustrated in Figure 8.
As seen in Figure 8, the network no longer contains a Decoder. Only the Encoder exists to take
input per each time step. However, the format of the input in this model can be two fold; either with
a moving window scheme or without it. Both these are tested in our study. In the scheme with the
moving window, each Encoder cell receives the vector of inputs Xt whereas without the moving window
scheme, the input xt corresponds to a single scalar input as explained before for the network with the
Figure 8: Sequence to Sequence with Dense Layer Architecture
Architecture
Output Component
Input Format
Error Computation
Dense Layer
Moving Window
Accumulated Error
Sequence to Sequence
Without Moving Window
Last Step Error
Sequence to Sequence
Dense Layer
Without Moving Window
Last Step Error
Sequence to Sequence
Dense Layer
Moving Window
Last Step Error
Table 1: RNN Architecture Information
Decoder. The forecast for both these is simply the output of the last Encoder time step projected
to the desired forecast horizon using a dense layer without bias (Fully connected layer). Therefore,
the output of the last Encoder step is a vector both with a moving window or without. The error
considered for the backpropagation is the error produced by this forecast.
E = YT −ˆYT
In comparison to the Stacked architecture which is also fed with a moving window input, the only
diﬀerence in this S2S architecture with the dense layer and the moving window input format is that
in the former, the error is calculated per each time step and the latter calculates the error only for the
last time step.
The set of models selected for implementation by considering all the aforementioned architectures
and input formats is shown in Table 1. All the models are implemented using the three RNN units
Elman RNN cell, LSTM cell and the GRU cell and tested across the ﬁve datasets detailed further in
Section 4.1.
3.2. Learning Algorithms
Three learning algorithms are tested in our framework; the Adam optimizer, the Adagrad optimizer
and the COntinuous COin Betting (COCOB) optimizer . Both Adam and Adagrad have built-in Tensorﬂow implementations
while the COCOB optimizer has an open-source implementation which uses Tensorﬂow . The Adam optimizer and the Adagrad optimizer both require the hyperparameter learning
rate, which if poorly tuned may perturb the whole learning process. The Adagrad optimizer introduces
the Adaptive Learning Rate concept where a separate learning rate is kept for each variable of the
function to be optimized. Consequently, the diﬀerent weights are updated using separate equations.
However, the Adagrad optimizer is prone to shrinking learning rates over time as its learning rate
update equations have accumulating gradients in the denominator.
This slows down the learning
process of the optimizer over time. The Adam optimizer, similar to Adagrad, keeps one learning rate
per each parameter. However, to address the issue of vanishing learning rates, the Adam optimizer
uses both exponentially decaying average of gradient moments and the gradients squared in its update
equations. The Adam optimizer is generally expected to perform better than the other optimizers and
Kingma and Ba empirically demonstrate this.
The COCOB optimizer attempts to minimize the loss function by self-tuning its learning rate.
Therefore, this is one step closer to fully automating the NN modelling process since the user is
relieved from the burden of deﬁning the initial learning rate ranges for the hyperparameter tuning
algorithm. Learning algorithms are extremely sensitive to their learning rate. Therefore, ﬁnding the
optimal learning rate is crucial for the model performance. The COCOB algorithm is based on a coin
betting scheme where during each iteration, an amount of money is bet on the outcome of a coin
toss such that the total wealth in possession is maximized. Orabona and Tommasi apply the
same idea to a function optimization where the bet corresponds to the size of the step taken along
the axis of the independent variable. The total wealth and the outcome of the coin ﬂip correspond
to the optimum point of the function and the negative subgradient of the function at the bet point,
respectively. During each round, a fraction of the current total wealth (optimum point) is bet. The
betting strategy is designed such that the total wealth does not become negative at any point and
the fraction of the money bet in each round increases until the outcome of the coin toss remains
constant. For our context this means that as long as the signs of the negative subgradient evaluations
remain the same, the algorithm keeps on making bigger steps along the same direction. This makes
the convergence faster in contrast to other gradient descent algorithms which have a constant learning
rate or a decaying learning rate where the convergence becomes slower close to the optimum. Similar
to the Adagrad optimizer, COCOB too maintains separate coins (learning rates) for each parameter.
4. Experimental Framework
To implement the models we use version 1.12.0 of the Tensorﬂow open-source deep learning framework introduced by Abadi et al. . This section details the diﬀerent datasets used for the experiments along with their associated preprocessing steps, the information of the model training and
testing procedures as well as the benchmarks used for comparison.
4.1. Datasets
The datasets used for the experiments are taken from the following forecasting competitions, held
during the past few years.
• CIF 2016 Forecasting Competition Dataset
• NN5 Forecasting Competition Dataset
• M3 Forecasting Competition Dataset
• M4 Forecasting Competition Dataset
• Wikipedia Web Traﬃc Time Series Forecasting Competition Dataset
• Tourism Forecasting Competition Dataset
As mentioned earlier, our study is limited to using RNN architectures on univariate, multi step
ahead forecasting considering only single seasonality, to be able to straightforwardly compare against
automatic standard benchmark methods. For the NN5 dataset and the Wikipedia web traﬃc dataset,
since they contain daily data with less than two years of data, we consider only the weekly seasonality.
From the M3 and M4 datasets, we only use the monthly time series which contain a single yearly
Dataset Name
No. of Time Series
Forecasting Horizon
Max. Length
Min. Length
Wikipedia Web Traﬃc
Table 2: Dataset Information
seasonality. The NN5, Wikipedia web traﬃc and the Tourism datasets contain non-negative series
meaning that they also have 0 values. Table 2 gives further insight into these datasets.
The CIF 2016 competition dataset has 72 monthly time series with 57 of them having a prediction
horizon of 12 and the remaining 15 having a prediction horizon of 6 in the original competition. Some
of the series having prediction horizon 6 are shorter than two full periods and are thus considered
as having no seasonality. Out of all the time series, 48 series are artiﬁcially generated while the rest
are real time series originating from the banking domain .
competition was held in 2008. This dataset has in total 111 daily time series which represent close
to two years of daily cash withdrawal data from ATM machines in the UK .
The forecasting horizon for all time series is 56. The NN5 dataset also contains missing values. The
methods used to compensate for these issues are as detailed in Section 4.2.2.
Two of the datasets are selected from the M competition series held over the years. From both M3
and M4 competitions we select only the monthly category which contains a single yearly seasonality.
The yearly data contain no seasonality and the series are relatively shorter. In the quarterly data too,
although they contain yearly seasonality, the series are shorter and contain fewer series compared to
the monthly category. The M3 competition, held in 2000, has monthly time series with a prediction
horizon of 18. In particular, this dataset consists of time series from a number of diﬀerent categories
namely, micro, macro, industry, ﬁnance, demography and other. The total number of time series in
the monthly category is 1428 . The M4 competition held in 2018 has
a dataset with similar format to that in the M3 competition. It has the same 6 categories as stated
before and the participants were required to make 18 months ahead predictions for the monthly series.
Compared to the previous competitions, one of the key objectives of the M4 is to increase the number
of time series available for forecasting . Consequently, the whole dataset
consists of 100,000 series and the monthly subcategory alone has a total of 48,000 series.
The other two datasets that we select for the experiments are both taken from Kaggle challenges,
the Web Traﬃc Time Series Forecasting Competition for Wikipedia articles and the Tourism Dataset
Competition . The task of the ﬁrst competition is to predict
the future web traﬃc (number of hits) of a given set of Wikipedia pages (145,000 articles), given their
history of traﬃc for about two years. The dataset involves some metadata as well to denote whether
the traﬃc comes from desktop, mobile, spider or all these sources. For this study, this problem is
reduced to the history of the ﬁrst 997 articles for the period of 1st July 2015 to 31st December 2016.
The expected forecast horizon is from 1st January 2017 to 28th February 2017, covering 59 days. One
important distinction in this dataset compared to the others is that all the values are integers. Since
the NN outputs continuous values both positive and negative, to obtain the ﬁnal forecasts, the values
need to be rounded to the closest non-negative integer. The tourism dataset contains monthly, yearly
and quarterly time series. Again, for our experiments we select the monthly category which has 366
series in total. The requirement in this competition for the monthly category is to predict the next 24
months of the given series. However, the nature of the data is generically termed as ’tourism related’
and no speciﬁc details are given about what values the individual time series hold. The idea of the
competition is to encourage the public to come up with new models that can beat the results which
are initially published by Athanasopoulos et al. using the same tourism dataset.
Table 2 also gives details of the lengths of diﬀerent time series of the datasets. The CIF 2016, M3
and the Tourism datasets have relatively short time series (maximum length being 108, 126 and 309
respectively). The NN5 and Wikipedia Web Traﬃc dataset time series are longer. In the M4 monthly
dataset, lengths of the series vary considerably from 42 to 2794.
Figure 9 shows violin plots of the seasonality strengths of the diﬀerent datasets. To extract the
seasonality strengths, we use the tsfeatures R package .
Figure 9: Violin Plots of Seasonality Strengths. The two datasets NN5 and Tourism have higher seasonality strength
compared to the others. The inter-quantile ranges of the two respective violin plots are located comparatively higher
along the y axis. Among these two, the seasonality strengths of the individual NN5 series vary less from each other
since the inter-quantile range is quite narrow. For the Wikipedia Web Traﬃc dataset, the inter-quantile range is even
more narrow and located much lower along the y axis. This indicates that the time series of the Wikipedia Web Traﬃc
dataset carry quite minimal seasonality compared to the other datasets. For the CIF, M3 monthly and M4 monthly
datasets, the seasonality strengths of the diﬀerent time series are spread across a wide spectrum.
4.2. Data Preprocessing
We apply a number of preprocessing steps in our work that we detail in this section. Most of them
are closely related to the ideas presented in Bandara et al. .
4.2.1. Dataset Split
The training and validation datasets are separated similar to the ideas presented by Suilin 
and Bandara et al. . From each time series, we reserve a part from the end for validation with a
length equal to the forecast horizon. This is for ﬁnding the optimal values of the hyperparameters using
the automated techniques explained in Section 4.3.1. The rest of the time series constitutes the training
data. This approach is illustrated in Figure 10. We use such ﬁxed origin mechanism for validation
instead of the rolling origin scheme since we want to replicate a usual competition setup. Also, given
the fact that we have 36 RNN models implemented and tested across 6 datasets involving thousands
of time series for both training and evaluation, we deem our results representative even with ﬁxed
origin evaluation and it would be computationally extremely challenging to perform a rolling origin
evaluation.
Figure 10: Train Validation Set Split
However, as stated by Suilin , this kind of split is problematic since the last part of the
sequence is not considered for training the model. The further away the test predictions are from the
training set the worse, since the underlying patterns may change during this last part of the sequence.
Therefore, in this work, the aforementioned split is used only for the validation phase. For testing,
the model is re-trained using the whole sequence without any data split. For each dataset, the models
are trained using all the time series available, for the purpose of developing a global model. Diﬀerent
RNN architectures need to be fed data in diﬀerent formats.
4.2.2. Addressing Missing Values
Many machine learning methods cannot handle missing values, so that these need to be properly
replaced by other appropriate substitutes. There are diﬀerent techniques available to ﬁll in missing
values, Mean substitution and Median substitution being two of them.
Linear interpolation and
replacing by “0” are other possible methods. Out of the datasets selected for our experiments, the
NN5 dataset and the Kaggle web traﬃc dataset contain missing values. For the NN5 dataset, we
use a median substitution method. Since the NN5 dataset includes daily data, a missing value on a
particular day is replaced by the median across all the same days of the week along the whole series.
For example, if a missing value is encountered on a Tuesday, the median of all values on Tuesdays of
that time series is taken as the substitute. This approach seems superior to taking the median across
all the available data points, since the NN5 series have a strong weekly seasonality. Compared to the
NN5 dataset, the Kaggle web traﬃc dataset has many more missing values. In addition to that, this
dataset does not diﬀerentiate between missing values and “0” values. Therefore, for the Kaggle web
traﬃc dataset, a simple substitution by “0”s is carried out for all the missing values.
4.2.3. Modelling Seasonality
With regard to modelling seasonality using NNs, there have been mixed notions among the researchers over the years. Some of the early works in this space infer that NNs are capable of modelling
seasonality accurately . However, more recent experiments
suggest that deseasonalization prior to feeding data to the NNs is essential since NNs are weak in
modelling seasonality. Particularly, Claveria et al. empirically show for a tourism demand forecasting problem that seasonally adjusted data can boost the performance of NNs especially in the case
of long forecasting horizons. Zhang and Qi conclude that using both detrending and deseasonalization can improve the forecasting accuracy of NNs. Similar observations are recorded in the work
of Zhang and Kline as well as Nelson et al. . More recently, in the winning solution by
Smyl at the M4 forecasting competition, the same argument that NNs are weak at modelling
seasonality, is put forward. A core part of our research is to investigate whether NNs actually struggle
to model seasonality on their own. Therefore, we run experiments with and without removing the
seasonality. In the following, we describe the deseasonalization procedure we use in case we remove
the seasonality.
Since in theory deterministic seasonality does not change and is therewith known ahead of time,
relieving the NN from the burden of modelling it can ease the task of the NN and let it predict
only the non-deterministic parts of the time series. Following this general consensus, we run a set
of experiments with prior deseasonalization of the time series data. For that we use the Seasonal
and Trend Decomposition using Loess (STL Decomposition) introduced by Cleveland et al. as
a way of decomposing a time series into its seasonal, trend and remainder components. Loess is the
underlying method for estimating non-linear relationships in the data. By the application of a sequence
of Loess smoothers, the STL Decomposition method can eﬃciently separate the seasonality, trend and
remainder components of the time series. However, this technique can only be used with additive trend
and seasonality components. Therefore, in order to convert all multiplicative time series components to
additive format, the STL Decomposition is immediately preceded by a variance stabilization technique
as discussed in Section 4.2.4. The STL Decomposition is potentially able to allow the seasonality to
change over time. However, we use STL in a deterministic way, where we assume the seasonality of
all the time series to be ﬁxed along the whole time span.
In particular, we use the implementation of the STL Decomposition available in the forecast R
package introduced by Hyndman and Khandakar .
By speciﬁcally setting the s.window parameter in the stl method to “periodic”, we make the seasonality deterministic. Hence, we remove
only the deterministic seasonality component from the time series while other stochastic seasonality
components may still remain. The NN is expected to model such stochastic seasonality by itself. The
STL Decomposition is applied in this manner to all the time series, regardless of whether they actually
show seasonal behaviour or not. Nevertheless, this technique requires at least two full periods of the
time series data to determine its seasonality component. In extreme cases where the full length of the
series is less than two periods, the technique considers such sequences as having no seasonality and
returns 0 for the seasonality component.
4.2.4. Stabilizing the Variance in the Data
Variance stabilization is necessary if an additive seasonality decomposition technique such as the
STL decomposition in Section 4.2.3 is used. For time series forecasting, variance stabilization can be
done in many ways, and applying a logarithmic transformation is arguably the most straightforward
way to do so. However, a shortcoming of the logarithm is that it is undeﬁned for negative and zerovalued inputs. All values need to be on the positive scale for the logarithm. For non-negative data,
this can be easily resolved by deﬁning a Log transformation as in Equation 10, as a slightly altered
version of a pure logarithm transformation.
if min(y) > ϵ,
log(yt + 1)
if min(y) ≤ϵ
y in Equation 10 denotes the whole time series. For count data, ϵ can be deﬁned as equal to 0,
whereas for real-valued data, ϵ can be selected as a small positive value close to 0. The logarithm
is a very strong transformation and may not always be adequate.
There are some other similar
transformations that attempt to overcome this shortcoming. One such transformation is the Box-Cox
transformation. It is deﬁned as follows.
As indicated by Equation 11, the Box-Cox transformation is a combination of a log transformation
(when λ = 0) and a power transformation (when λ ̸= 1) denoted by yλ
t . The parameter λ needs to be
carefully chosen. In the case when λ equals 1, the Box-Cox transformation results in yt −1. Thus, the
shape of the data is not changed but the series is simply shifted down by 1 .
Another similar form of power transformation is the Yeo-Johnson transformation shown in Equation 12.
((yt + 1)µ −1)/µ
if µ ̸= 0, yt ≥0,
log(yt + 1)
if µ = 0, yt ≥0,
−[(−yt + 1)(2−µ) −1]/(2 −µ)
if µ ̸= 2, yt < 0,
−log(−yt + 1)
if µ = 2, yt < 0
The parameter µ in the Yeo-Johnson transformation is conﬁned in the range while µ = 1
gives the identity transformation. Values of yt are allowed to be either positive, negative or zero. Even
though this is an advantage over the logarithm, the choice of the optimal value of the parameter µ is
again not trivial.
Due to the complexities associated with selecting the optimal values of the parameters λ and µ in
the Box-Cox and Yeo-Johnson Transformations, and given that all data used in our experiments are
non-negative, we use a Log transformation in our experiments.
4.2.5. Multiple Output Strategy
Forecasting problems are typically multi-step-ahead forecasting problems, which is also what we
focus on in our experiments. Ben Taieb et al. perform an extensive research involving ﬁve
diﬀerent techniques for multi-step-ahead forecasting. The Recursive Strategy which is a sequence of
one-step-ahead forecasts, involves feeding the prediction from the last time step as input for the next
prediction. The Direct Strategy involves diﬀerent models, one per each time step of the forecasting
horizons. The DiRec Strategy combines the concepts from the two above methods and develops multiple models one per each forecasting step where the prediction from each model is fed as input to the
next consecutive model. In all these techniques, the model outputs basically a scalar value corresponding to one forecasting step. The other two techniques tested by Ben Taieb et al. use a direct
multi-step-ahead forecast where a vector of outputs corresponding to the whole forecasting horizon is
directly produced by the model. The ﬁrst technique under this category is known as the Multi-Input
Multi-Output (MIMO) strategy. The advantage of the MIMO strategy comes from producing the
forecasts for the whole output window at once, and thus incorporating the inter-dependencies between
each time step, rather than forecasting each time step in isolation. The DIRMO technique synthesizes
the ideas of the Direct technique and the MIMO technique where each model produces forecasts for
windows of size s (s ∈{1, ..., H}, H being the forecast horizon). In the extreme cases where s = 1
and s = H the technique narrows down to the Direct strategy and the MIMO strategy, respectively.
The experimental analysis in that paper using the NN5 competition dataset demonstrates that the
multiple output strategies are the best overall. The DIRMO strategy requires careful selection of the
output window size s . Wen et al. also state in their work that the
Direct Multi-Horizon strategy also known as the MIMO strategy performs reliably since it avoids error
accumulation over the prediction time steps.
Following these early ﬁndings, we use in our study the multiple output strategy in all the RNN
architectures.
However, the inputs and outputs of each recurrent cell diﬀer between the diﬀerent
architectures. For the S2S model, each recurrent cell of the encoder is fed a single scalar input. For the
stacking model and the S2S model with the dense layer output, a moving window scheme is used to
feed the inputs and create the outputs as in the work of Bandara et al. . In this approach, every
recurrent cell accepts a window of inputs and produces a window of outputs corresponding to the time
steps which immediately follow the fed inputs. The recurrent cell instances of the next consecutive
time step accepts an input window of the same size as the previous cell, shifted forward by one. This
method can also be regarded as an eﬀective data augmentation mechanism .
Figure 11 illustrates this scheme.
Let the length of the whole sequence be l, the size of the input window m and the size of the output
window n. As mentioned in Section 4.2.1, during the training phase, the last n sized piece from this
sequence is left out for validation. The rest of the sequence (of length l−n) is broken down into blocks
of size m + n forming the input output combination for each recurrent cell instance. Likewise in total,
there are l −n∗2−m and l −n−m such blocks for the training and the validation stages respectively.
Figure 11: Moving Window Scheme
Even though the validation stage does not involve explicit training of the model, the created blocks
should still be fed in sequence to build up the state. The output window size is set to be equal to the
size of the forecasting horizon H (n = H). The selection of the input window size is a careful task.
The objective of using an input window as opposed to single input is to relax the duty of the recurrent
unit to remember the whole history of the sequence. Though theoretically the RNN unit is expected
to memorize all the information from the whole sequence it has seen, it is not practically competent
in doing so . The importance of the information from distant time steps tend
to fade as the model continues to see new inputs. However, in terms of the trend, it is intelligible that
only the last few time steps contribute the most in forecasting.
Putting these ideas together, we select the input window size m with two options. One is to make
the input window size slightly bigger than the output window size (m = 1.25 ∗output window size).
The other is to make the input window size slightly bigger than the seasonality period (m = 1.25 ∗
seasonality period). For instance if the time series has weekly seasonality (seasonality period = 7)
with expected forecasting horizon 56, with the ﬁrst option we set the input window size to be 70
(1.25 ∗56), whereas with the second option it is set to be 9 (1.25 ∗7). The constant 1.25 is selected
purely as a heuristic. The idea is to ascertain that every recurrent cell instance at each time step gets
to see at least its last periodic cycle so that it can model any remaining stochastic seasonality. In case,
the total length of the time series is too short, m is selected to be either of the two feasible, or some
other possible value if none of them work. For instance, the forecasting horizon 6 subgroup of the CIF
2016 dataset comprises of such short series and m is selected to be 7 (slightly bigger than the output
window size) even though the seasonality period is equal to 12.
4.2.6. Trend Normalization
The activation functions used in RNN cells, such as the sigmoid or the hyperbolic tangent function
have a saturation area after which the outputs are constant. Hence, when using RNN cells it should be
assured that the inputs fed are normalized adequately such that the outputs do not lie in the saturated
range . To this end, we have performed a per window local normalization step
for the RNN architectures which use the moving window scheme. From each deseasonalized input and
corresponding output window pair, the trend value of the last time step of the input window (found
by applying the STL Decomposition) is deducted. This is carried out for all the input and output
window pairs of the moving window strategy. The red coloured time steps in Figure 11 correspond to
those last time steps of all the input windows. This technique is inspired by the batch normalization
scheme and it also helps address the trend in time series . As for the models
that do not use the moving window scheme, a per sequence normalization is carried out where the
trend value of the last point of the whole training sequence is used for the normalization instead.
4.2.7. Mean Normalization
For the set of experiments that do not use STL Decomposition, it is not possible to perform
the trend normalization as described in Section 4.2.6. For those experiments, we perform a mean
normalization of the time series before applying the Log transformation mentioned in Section 4.2.4. In
Speciﬁcation
Resource 1
Resource 2
Resource 3
CPU Model Name
Intel(R) Core(TM) i7-8700
Intel Xeon Gold 6150
Intel Xeon CPU E5-2680 v3
CPU Architecture
Memory(GB)
GPU Model Name
GP102 [GeForce GTX 1080 Ti]
nVidia Tesla V100
No. of GPUs
Table 3: Hardware Speciﬁcations
this mean normalization, every time series is divided by the mean of that particular time series. The
division instead of subtraction further helps to scale all the time series to a similar range which helps
the RNN learning process. The Log transformation is then carried out on the resulting data.
4.3. Training & Validation Scheme
The methodology used for training the models is detailed in this section. Our work uses the idea
of global models in Section 2.4 and develops one model for all the available time series. However,
it is important to understand that the implemented techniques work well only in the case of related
(similar/homogeneous) time series.
If the individual time series are from heterogeneous domains,
modeling such time series using a single model may not render the best results.
Due to the complexity and the total number of models associated with the study, we run the
experiments on three computing resources in parallel. The details of these machines are as mentioned in
Table 3. Due to the scale of the M4 monthly dataset, the GPU machines are used to run the experiments
on it. Resource 2 and Resource 3 indicate allocated resources from the Massive cluster . Resource 2 is used for GPU only operations whereas Resource 3 is used for CPU only
operations. Resource 1 is used for both CPU and GPU operations.
4.3.1. Hyper-parameter Tuning
The experiments require the proper tuning of a number of hyperparameters associated with the
model training. They are as mentioned below.
1. Minibatch Size
2. Number of Epochs
3. Epoch Size
4. Learning Rate
5. Standard Deviation of the Gaussian Noise
6. L2 Regularization Parameter
7. RNN Cell Dimension
8. Number of Hidden Layers
9. Standard Deviation of the Random Normal Initializer
Minibatch size denotes the number of time series considered for each full backpropagation in the
RNN. This is a more limited version than using all the available time series at once to perform one
backpropagation, which poses a signiﬁcant memory requirement. On the other hand, this could also be
regarded as a more generalized version of the extreme case which uses only a single time series per each
full backpropagation, also known as stochastic gradient descent. Apart from the explicit regularization
schemes used in the RNN models further discussed in Section 4.3.2, minibatch gradient descent also
introduces some implicit regularization to the deep learning models in speciﬁc problem scenarios such
as classiﬁcation . An epoch denotes one full forward and backward pass through
the whole dataset. Therefore, the number of epochs denote how many such passes across the dataset
are required for the optimal training of the RNN. Even within each epoch, the dataset is traversed a
number of times denoted by the epoch size. This is especially useful for those datasets with limited
number of time series to increase the number of datapoints available for training. This is because NNs
are supposed to be trained best when the amount of data available is higher.
Out of the three optimizers used in the experiments, the Adam optimizer and the Adagrad optimizer
both require the appropriate tuning of the learning rate to converge to the optimal state of the network
parameters fast. To reduce the eﬀect of model overﬁtting, two steps are taken; adding Gaussian noise
to the input and using L2 regularization for the loss function (explained further under Section 4.3.2).
Both these techniques require tuning hyperparameters; the standard deviation of the Gaussian noise
distribution and the L2 regularization parameter. Furthermore, the weights of the RNN units are
initialized using random samples drawn from a normal distribution whose standard deviation is tuned
as another hyperparameter.
There are two other hyperparameters that directly relate to the RNN network architecture; the
number of hidden layers and the cell dimension of each RNN cell. For simplicity, both the Encoder
and the Decoder components of the Sequence to Sequence network are composed of the same number
of hidden layers. Also, the same cell dimension is used for the RNN cells in both the Encoder and
the Decoder. However, as explained before in Section 3.1.1, the diﬀerent Recurrent unit types that
we employ in this study, namely the LSTM cell, ERNN cell, and the GRU, have diﬀerent numbers of
trainable parameters for the same cell dimension, with the LSTM having the highest and the ERNN
having the lowest number of parameters respectively. Therefore, to enable a fair comparison, in other
research communities such as natural language processing, it is common practice to consider the total
number of trainable parameters in the diﬀerent models compared . This is to ascertain that the capacities of all the compared models are the
same to clearly distinguish the performance gains achieved purely through the novelty introduced to
the models. As our main aim is to compare software frameworks, we use the cell dimension which is
the natural hyperparameter to be tuned by practitioners, and additionally perform experiments using
the number of trainable hyperparameters instead. For this, we choose the best model combination
identiﬁed from the preceding experiments involving the cell dimension as a hyperparameter and run
it again along with all three recurrent unit types by letting the hyperparameter tuning algorithm
choose the optimal number of trainable parameters as a hyperparameter. Hence, for this experiment,
instead of setting the cell dimension directly, we set the same initial range for the number of trainable
parameters across all the compared models with the three RNN units. As the number of trainable
parameters is directly proportional to the cell dimension, and the deep learning framework we use
allows us to only set the cell dimension, we then calculate the corresponding cell dimension from the
number of parameters, and set this parameter accordingly.
For tuning hyperparameters, there are many diﬀerent techniques available. The most na¨ıve and
the fundamental approach is hand tuning which requires intensive manual experimentation to ﬁnd
the best possible hyperparamters. However, our approach is more targeted towards a fully automated
framework. To this end, Grid Search and Random Search are two of the most basic methods for automated hyperparameter tuning . Grid Search, as its name suggests explores
in the space of a grid of diﬀerent hyperparameter value combinations. For example, if there are two
hyperparameters Γ and Θ, a set of values are speciﬁed for each hyperparameter as in (γ1, γ2, γ3, ...) for
Γ and (θ1, θ2, θ3, ...) for Θ. Then the Grid Search algorithm goes through each pair of hyperparameter
values (γ, θ) that lie in the grid of Γ and Θ and uses them in the model to calculate the error on a
held out validation set. The hyperparameter combination which gives the minimum error is chosen
as the optimal hyperparameter values. This is an exhaustive search through the grid of possible hyperparameter values. In the Random Search algorithm, instead of the exact values, distributions for
the hyperparameters are provided, from which the algorithm randomly samples values for each model
evaluation. A maximum number of iterations are provided for the algorithm until when the model
evaluations are performed on the validation set and the optimal combination thus far is selected.There
are other more sophisticated hyperparameter tuning techniques as well.
Bayesian Optimization
The methods discussed above evaluate an objective function (validation error) point wise. This means
retraining machine learning models from scratch which is quite compute intensive. The idea of the
Bayesian Optimization is to limit the number of such expensive objective function evaluations. The
technique models the objective function using a Gaussian prior over all possible functions . This probabilistic model embeds all prior assumptions regarding the objective function
to be optimized. During every iteration, another instance of the hyperparameter space is selected
to evaluate the objective function value. The decision of which point to select next, depends on the
minimization/maximization of a separate acquisition function which is much cheaper to evaluate than
the objective function itself. The acquisition function can take many forms out of which the implementation of Snoek et al. uses Expected Improvement. The optimization of the acquisition function
considers all the previous objective function evaluations to decide the next evaluation point. Therefore,
the Bayesian Optimization process takes on smarter decisions of function evaluation compared to the
aforementioned Grid Search and Random Search. Given an initial range of values for every hyperparameter and a deﬁned number of iterations, the Bayesian Optimization method initializes using a set
of prior known parameter conﬁgurations and thus attempts to ﬁnd the optimal values for those hyperparameters for the given problem. There are many practical implementations and variants of the basic
Bayesian Optimization technique such as hyperopt, spearmint and bayesian-optimization .
Sequential Model based Algorithm Conﬁguration (SMAC)
SMAC for hyperparameter tuning, is a variant of Bayesian Optimization proposed by Hutter et al.
 . This implementation is based on the Sequential Model Based Optimization (SMBO) concept.
SMBO is a technique founded on the ideas of the Bayesian Optimization but uses a Tree-structured
Parzen Estimator (TPE) for modelling the objective function instead of the Gaussian prior. The TPE
algorithm produces a set of candidate optimal values for the hyperparameters in each iteration, as
opposed to just one candidate in the usual Bayesian Optimization. The tree structure lets the user
deﬁne conditional hyperparameters which depend on one another. The process is carried out until a
given time bound or a number of iterations is reached. The work by Hutter et al. further enhances the basic SMBO algorithm by adding the capability to work with categorical hyperparameters.
The Python implementation of SMAC used for this study is available as a Python package . We use the version 0.8.0 for our experiments. The initial hyperparameter ranges used for
the NN models across the diﬀerent datasets are as listed in Table 4. Additionally, for the experiment
which involves the number of trainable parameters instead of the cell dimension as a hyperparameter,
we set an initial range of 2000 −25000 across all the compared models in all the selected datasets.
This range is selected based on the initial experiments involving the cell dimension.
4.3.2. Dealing with Model Overﬁtting
Overﬁtting in a machine learning model is when the performance on the validation dataset is much
worse than the performance on the training dataset. This means that the model has ﬁtted quite well
onto the training data to the extent that it has lost its generalization capability to model other unseen
data. In NN models, this mostly happens due to the complexity ensued from the large number of
parameters. In order to make the models more versatile and capture unseen patterns, we are using
two techniques in our framework. One method is to add noise to the input to distort it partially. The
noise distribution applied here is a Gaussain distribution with zero mean and the standard deviation
treated as another hyperparameter. The other technique is L2 weight regularization which uses a Ridge
penalty in the loss function. Weight regularization avoids the weights of the network from growing
excessively by incorporating them in the loss function and thus penalizing the network for increasing
its complexity. Given the L2 regularization parameter ψ, the Loss function for the NN can be deﬁned
Batch Size
Epoch Size
Std. Noise
Std. Initializer
Learning Rate
0.01 - 0.08
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
1000 - 1500
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
1000 - 1500
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
1000 - 1500
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
850 - 1000
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
1000 - 1500
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
0.0001 - 0.0008
0.0001 - 0.0008
0.0001 - 0.0008
0.001 - 0.1
0.01 - 0.9
Table 4: Initial Hyperparameter Ranges
L2 regularization
Applied to our context, E in Equation 13 denotes the Mean Absolute Error from the network
outputs. wi represents the trainable parameters of the network where p is the number of all such
parameters. Hence, as seen in Equation 13, L2 regularization adds the squared magnitudes of the
weights multiplied by the regularization parameter ψ. The selection of ψ is paramount since too large
a value results in underﬁtting and on the other hand very small values let the model become overﬁtted.
In our study, ψ is also optimized as a hyperparameter.
4.4. Model Testing
Once an optimal hyperparameter combination is found, those values are used to train the model
and get the ﬁnal forecasts. During testing, the model is trained using all the data available and the
ﬁnal forecasts are written to ﬁles. We address parameter uncertainty by training all the models on 10
diﬀerent Tensorﬂow graph seeds but using the same initially tuned hyperparameters. We do not re-run
the hyperparameter tuning as this would be computationally very expensive. The diﬀerent seeds give
10 diﬀerent initializations to the networks. Once the RNN forecasts are obtained from every model
for the diﬀerent seeds, they are ensembled by taking the median across the seeds.
4.4.1. Data Post-Processing
To calculate the ﬁnal error metrics, the eﬀects of the prior preprocessing is properly reversed on
the generated forecasts. For the experiments which use STL Decomposition, this post-processing is
carried out as follows.
1. Reverse the local normalization by adding the trend value of the last input point.
2. Reverse deseasonalization by adding back the seasonality components.
3. Reverse the log transformation by taking the exponential.
4. Subtract 1, if the data contain 0s
5. For integer data, round the forecasts to the closest integer.
6. Clip all negative values at 0 (To allow for only positive values in the forecasts).
For those experiments which do not use STL Decomposition, the post-processing of the forecasts
is done as follows.
1. Reverse the log transformation by taking the exponential.
2. Subtract 1, if the data contain 0s
3. Multiply the forecasts of every series by its corresponding mean to reverse the eﬀect of mean
4. For integer data, round the forecasts to the closest integer.
5. Clip all negative values at 0 (To allow for only positive values in the forecasts).
4.4.2. Performance Measures
We measure the performance of the models in terms of a number of metrics. Symmetric Mean Absolute Percentage Error (SMAPE) is the most common performance measure used in many forecasting
competitions.
SMAPE = 100%
(|Yk| + |Fk|)/2
H, Fk and Yk indicate the size of the horizon, the forecast of the NN, and the actual forecast,
respectively. As seen in Equation 14, the SMAPE is a metric based on percentage errors. However,
according to Hyndman and Koehler , SMAPE measure is susceptible to instability with values
close to 0. The forecasts of the Wikipedia Web Traﬃc, NN5 and Tourism datasets are heavily aﬀected
by this since they all have 0 values. Therefore, to overcome this we use another variant of the SMAPE
proposed by Suilin . In this metric, the denominator of the above SMAPE is changed as follows.
max(|Yk| + |Fk| + ϵ, 0.5 + ϵ)
where the parameter ϵ is set to 0.1 following Suilin . The idea is that the above metric can
avoid division by values close to zero by switching to an alternate positive constant for the denominator
in SMAPE when the forecasts are too small values. Yet, the SMAPE error metric has several other
pitfalls such as its lack of interpretability and high skewness . Based on
these issues, another metric is proposed by Hyndman and Koehler known as the Mean Absolute
Scaled Error (MASE) to address them. MASE is deﬁned as follows.
k=1 |Fk −Yk|
k=M+1 |Yk −Yk−M|
MASE is a scale-independent measure, where the numerator is the same as in SMAPE, but normalized by the average in-sample one step na¨ıve forecast error or the seasonal na¨ıve forecast error in
case of seasonal data. A value greater than 1 for MASE, indicates that the performance of the tested
model is worse on average than the na¨ıve benchmark and a value less than 1 denotes the opposite.
Therefore, this error metric provides a direct indication of the performance of the model relative to
the na¨ıve benchmark.
The model evaluation of this study is presented using six metrics; Mean SMAPE, Median SMAPE,
Mean MASE, Median MASE, Rank SMAPE and Rank MASE of the time series of each dataset. The
rank measures compare the performance of every model against each other with respect to every time
series in the dataset. Apart from them, when considering all the datasets together, ranks of models
within each dataset with respect to both mean SMAPE and mean MASE metrics are plotted, since
the original SMAPE and MASE values lie in very diﬀerent ranges for the diﬀerent datasets. These
metrics are referred to as Mean SMAPE Ranks and Mean MASE Ranks respectively.
The literature introduces several other performance measures for forecasting such as the Geometric
Mean Relative Absolute Error (GMRAE) . The GMRAE too is a relative
error measure similar to MASE which measures the performance with respect to some benchmark
method, but uses the geometric mean instead of the mean. However, as Chen et al. state in
their work, due to the usage of the geometric mean, values from the GMRAE can be quite small close
to zero, especially in situations like ours where the expected forecasting horizon is very long such as
in the NN5 and the Wikipedia Web Traﬃc datasets and the number of time series is large.
4.5. Benchmarks
As for the benchmarks for comparison, we select two strong, well established traditional univariate
techniques, ets and auto.arima with their default parameters from the forecast package in R. We perform experiments with the two techniques on all the aforementioned datasets and measure the performance in terms of the same metrics mentioned in Section
4.4.2. Furthermore, we perform experiments with a pooled regression similar to the work of Trapero
et al. . As mentioned in Section 2.4, the pooled regression model diﬀers from the RNN in that
it does not maintain a state per every series and it models a linear relationship between the lags and
the target variable. However, similar to the RNNs, the pooled regression models also calculate their
weights globally by considering cross-series information, i.e., a pooled regression model works as a
global AR model. Therefore, we use them in this study as a benchmark against RNNs to diﬀerentiate
between the performance gains in RNNs purely due to building a global model versus due to the choice
of the RNN architectures. We also train unpooled versions of regression models on all the datasets
meaning that the models are built per every series similar to ets and arima. The unpooled regression
models are used in this study to observe the accuracy gains from training regression models as global
models rather than usual univariate models.
For implementing the regression models, we use the linear regression implementation from the
glmnet package in the R programming language . Before feeding the series into
the pooled regression models, we normalize them by dividing every series by its mean value. For the
number of lags, we experiment with two options. The auto.arima model from the forecast package
selects by default up to a maximum of 5 lags for both the AR and MA components combined. An
ARMA model can be approximated by a pure AR model with a higher number of lags . Since the pooled regression model is a pure AR model, to be approximately compatible with
the complexity of the auto.arima model, we use 10 lags (as opposed to 5 lags) as the ﬁrst option for
the number of lags in the pooled regression models. On the other hand, the moving window scheme
as mentioned in Section 4.2.5, already imposes a number of lags on the input window of the relevant
RNN architectures. Therefore, we also build versions of the regression models with this number of
lagged values. Furthermore, as mentioned in Section 4.3.2, the RNN models are regularized using a
Ridge penalty. Hence, to enable a fair comparison, we develop the regression models both with and
without the L2 regularization. To ﬁnd the L2 regularization parameter we use two methods. The
ﬁrst is the built-in grid search technique in the glmnet package using a 10-fold cross-validation with a
mean squared error as the validation error metric. However, to be more comparable with the RNNs,
we also use the Bayesian optimization hyperparameter tuning along with the SMAPE as the validation
error metric to ﬁnd the L2 regularization parameter in the regression models. For this we use a simple
70%-30% split of training and validation sets respectively. For the pooled regression models, the range
for the L2 regularization parameter is set to the 0-1 interval with 50 iterations in the tuning process.
For the unpooled versions of the regression models this initial range is set to the 0-200 interval, since
per one series there is a smaller amount of data available and the regularization parameter may need
a larger value to avoid overﬁtting. For the Bayesian optimization we use the rBayesianOptimization
package in the R programming language . During testing, a recursive strategy is carried
out to produce forecasts up to the forecasting horizon where one point is forecasted at every step,
taking the last forecast as the most recent lag.
4.6. Statistical Tests of the Results
To evaluate the statistical signiﬁcance of the diﬀerences of the performance of multiple techniques,
we perform a non-parametric Friedman rank-sum test. This test determines whether the performance
diﬀerences are statistically signiﬁcant. To further explore these diﬀerences with respect to a control
technique, speciﬁcally the best performing technique, we then perform Hochberg’s post hoc procedure . The signiﬁcance level used is α = 0.05. For those tests that have only
two techniques to compare, we use a non-parametric paired Wilcoxon signed-rank test with Bonferroni
correction, to measure the statistical signiﬁcance of the diﬀerences. For this we apply the wilcox.test
function from the stats package of the R core libraries . The Bonferroni procedure is used to adjust the signiﬁcance level used for comparison at each step, of a number of successive
comparisons . In particular, the procedure divides the signiﬁcance level by the
total number of comparisons for use in every individual comparison. The mean and the median of the
SMAPE measure are used where relevant for all the statistical testing.
5. Analysis of Results
This section provides a comprehensive analysis of the results obtained from the experiments. The
datasets that we use for our experiments are quite diﬀerent from each other in terms of the seasonality
strength, number of time series, and length of individual time series. Thus, it seems reasonable to
assume that the selected datasets cover a suﬃcient number of diﬀerent time series characteristics to
arrive at generalized conclusions. The results of all the implemented models in terms of the mean
SMAPE metric are as shown in Table 5. Results in terms of all the other error metrics are available
in an Online Appendix2. In the tables presenting the results, we use the abbreviations ‘NSTL’, ‘MW’
and ‘NMW’ to denote ‘without using STL Decomposition’, ‘moving window’ and ‘non moving window’
respectively. Furthermore, ‘IW+’ indicates an increased input window size.
Model Name
auto.arima
Pooled Regression Lags 10
Pooled Regression Lags 10 Bayesian Regularized
Pooled Regression Lags 10 Regularized
Pooled Regression Lags Window
Pooled Regression Lags Window Bayesian Regularized
Pooled Regression Lags Window Regularized
Unpooled Regression Lags 10
Unpooled Regression Lags 10 Bayesian Regularized
Unpooled Regression Lags 10 Regularized
Unpooled Regression Lags Window
Unpooled Regression Lags Window Bayesian Regularized
Unpooled Regression Lags Window Regularized
S2S GRU adagrad
S2S GRU adam
S2S GRU cocob
S2S LSTM adagrad
S2S LSTM adam
S2S LSTM cocob
S2S ERNN adagrad
S2S ERNN adam
S2S ERNN cocob
S2SD GRU MW adagrad
S2SD GRU MW adam
S2SD GRU MW cocob
S2SD GRU NMW adagrad
S2SD GRU NMW adam
S2SD GRU NMW cocob
S2SD LSTM MW adagrad
S2SD LSTM MW adam
S2SD LSTM MW cocob
S2SD LSTM NMW adagrad
S2SD LSTM NMW adam
S2SD LSTM NMW cocob
S2SD ERNN MW adagrad
2 
S2SD ERNN MW adam
S2SD ERNN MW cocob
S2SD ERNN NMW adagrad
S2SD ERNN NMW adam
S2SD ERNN NMW cocob
Stacked GRU adagrad
Stacked GRU adam
Stacked GRU cocob
Stacked LSTM adagrad
Stacked LSTM adam
Stacked LSTM cocob
Stacked ERNN adagrad
Stacked ERNN adam
Stacked ERNN cocob
Stacked GRU adagrad(IW+)
Stacked GRU adam(IW+)
Stacked GRU cocob(IW+)
Stacked LSTM adagrad(IW+)
Stacked LSTM adam(IW+)
Stacked LSTM cocob(IW+)
Stacked ERNN adagrad(IW+)
Stacked ERNN adam(IW+)
Stacked ERNN cocob(IW+)
NSTL S2S GRU adagrad
NSTL S2S GRU adam
NSTL S2S GRU cocob
NSTL S2S LSTM adagrad
NSTL S2S LSTM adam
NSTL S2S LSTM cocob
NSTL S2S ERNN adagrad
NSTL S2S ERNN adam
NSTL S2S ERNN cocob
NSTL S2SD GRU MW adagrad
NSTL S2SD GRU MW adam
NSTL S2SD GRU MW cocob
NSTL S2SD GRU NMW adagrad
NSTL S2SD GRU NMW adam
NSTL S2SD GRU NMW cocob
NSTL S2SD LSTM MW adagrad
NSTL S2SD LSTM MW adam
NSTL S2SD LSTM MW cocob
NSTL S2SD LSTM NMW adagrad
NSTL S2SD LSTM NMW adam
NSTL S2SD LSTM NMW cocob
NSTL S2SD ERNN MW adagrad
NSTL S2SD ERNN MW adam
NSTL S2SD ERNN MW cocob
NSTL S2SD ERNN NMW adagrad
NSTL S2SD ERNN NMW adam
NSTL S2SD ERNN NMW cocob
NSTL Stacked GRU adagrad
NSTL Stacked GRU adam
NSTL Stacked GRU cocob
NSTL Stacked LSTM adagrad
NSTL Stacked LSTM adam
NSTL Stacked LSTM cocob
NSTL Stacked ERNN adagrad
NSTL Stacked ERNN adam
NSTL Stacked ERNN cocob
NSTL Stacked GRU adagrad(IW+)
NSTL Stacked GRU adam(IW+)
NSTL Stacked GRU cocob(IW+)
NSTL Stacked LSTM adagrad(IW+)
NSTL Stacked LSTM adam(IW+)
NSTL Stacked LSTM cocob(IW+)
NSTL Stacked ERNN adagrad(IW+)
NSTL Stacked ERNN adam(IW+)
NSTL Stacked ERNN cocob(IW+)
Table 5: Mean SMAPE Results
In Table 5, the best model in every dataset is indicated in boldface. Furthermore, as mentioned before in Section 4.6, since auto.arima and ets are the two established benchmarks chosen, we perform
paired Wilcoxon signed-rank tests with Bonferroni correction for every model against the two benchmarks. In Table 5, † and * denote those models which are signiﬁcantly worse than auto.arima and
ets respectively. Similarly, ‡ and ** denote the models which are signiﬁcantly better than auto.arima
and ets respectively. We see that on the CIF2016, M3, Tourism and M4 datasets no model is significantly better than the two benchmarks. On the NN5 dataset, the RNN models have only managed
to perform signiﬁcantly better than auto.arima but not ets. Moreover, across all the datasets it can
be seen that the RNN models which perform signiﬁcantly better than the benchmarks are mostly the
variants of the Stacked architecture. Also, it is only on the Wikipedia Web Traﬃc dataset, that the
pooled versions of the regression models perform signiﬁcantly better than the benchmark auto.arima.
We observe that in general, increasing the number of lags from 10 to the size of the input window
of the RNNs, improves accuracy of the regression models across all the datasets. On the datasets
Wikipedia Web Traﬃc, M3 and NN5, the inclusion of the cross-series information alone has an eﬀect
since the pooled regression models with increased number of lags have performed better than the
unpooled versions of regression. Furthermore, on the two datasets Wikipedia Web Traﬃc and NN5,
the pooled regression models with increased number of lags have outperformed the auto.arima model
which acts upon every series independently. Although the pooled models have performed worse than
the unpooled models on the CIF, Tourism and M4 datasets, the RNN models have mostly outperformed
both the unpooled and pooled regression models on these datasets. This indicates that even though the
series in those datasets are not quite homogeneous to build global models, the use of RNN architectures
has a clear eﬀect on the ﬁnal accuracy. In fact, on the CIF dataset the eﬀect from the choice of the
RNN architectures has managed to outperform the two statistical benchmarks ets and auto.arima.
On the Wikipedia Web Traﬃc, M3 and NN5 datasets too, the RNNs have outperformed the statistical
benchmarks as well as both the versions of the regression models implying that on these datasets the
cross-series inclusion together with the choice of the RNNs have lead to better accuracy.
Another signiﬁcant observation is that on many datasets, both the Bayesian optimization and
the hyperparameter tuning with 10-fold cross validation for the L2 regularization parameter of the
pooled regression models select values close to 0 so that the models eﬀectively have nearly no L2
regularization. This is why in many datasets the mean SMAPE values are the same for both with and
without regularization. Therefore, for the pooled regression models we cannot conclude that L2 weight
regularization improves the model performance. The model without any L2 regularization is already an
appropriate ﬁt for the data. For the unpooled regression models, we obtain mixed results. As discussed
by Bergmeir et al. , while being applicable to detect overﬁtting, a generic k-fold cross-validation
may lead to an underestimation of the cross-validation errors for models that underﬁt.
our pooled regression models do not show this behaviour, as they have very small L2 regularization
parameters. As for the unpooled models, although for some series the L2 regularization parameters
are high, it is the same case for both 10-fold cross-validation and the normal cross-validation.
5.1. Relative Performance of RNN Architectures
We compare the relative performance of the diﬀerent RNN architectures against each other across
all the datasets, in terms of all the error metrics. The violin plots in Figure 12 illustrate these results.
We see that the best architecture diﬀers based on the error metric. On mean SMAPE, the S2S with
the Dense Layer (S2SD) architecture without the moving window, performs the best. However, on all
the other error metrics, the Stacked architecture performs the best. With respect to the rank error
plots, clearly the Stacked architecture produces the best results for most of the time series. Thus,
the results on the mean SMAPE indicate that the Stacked architecture results in higher errors for
certain outlying time series. The two versions of the S2SD architectures (with and without the moving
window) perform comparably well, although not as good as the Stacked architecture on most cases.
Therefore, in between the two versions of the S2SD architectures, it is hard to derive a clear conclusion
about the best model. The S2S architecture (with the decoder) performs the worst overall.
Figure 12: Relative Performance of Diﬀerent RNN Architectures
With respect to the mean SMAPE values, the Friedman test of statistical signiﬁcance gives an overall p-value of 9.3568 × 10−3 implying that the diﬀerences are statistically signiﬁcant. The Hochberg’s
post-hoc procedure is performed by using as the control method the S2SD MW architecture which
performs the best. The Stacked and the S2SD Non MW architectures do not perform signiﬁcantly
worse, with an adjusted p-value of 0.602. However, the S2S architecture performs signiﬁcanlty worse
with an adjusted p-value of 5.24 × 10−3. In terms of the median SMAPE, the Friedman test of statistical signiﬁcance gives an overall p-value of 0.349, implying that the diﬀerences of the models are not
statistically signiﬁcant.
5.2. Performance of Recurrent Units
The violin plots in Figure 13 demonstrate the performance comparison of the diﬀerent RNN units,
in terms of mean SMAPE ranks and mean MASE ranks. The Friedman test of statistical signiﬁcance
with respect to the mean SMAPE values produces an overall p-value of 0.101. Though the p-value
does not indicate statistical signiﬁcance, from the plots, we can derive that the LSTM with peephole
connections cell performs the best.
The ERNN cell performs the worst and the GRU exhibits a
performance in-between these two.
Figure 13: Relative Performance of Diﬀerent Recurrrent Cell Types
5.3. Performance of Optimizers
The violin plots in Figure 14 illustrate the performance comparison of the diﬀerent optimizers, in
terms of both the mean SMAPE ranks and the mean MASE ranks. From the plots we see that the
Adagrad optimizer performs the worst complying with the ﬁndings in the literature stated under the
Section 3.2. Eventhough the Adam optimizer has been the best optimizer thus far, we can conclude
from this study that the COCOB optimizer performs the best out of the three. However, the Adam
optimizer also shows quite competitive performance in-between these two.
The Friedman test of statistical signiﬁcance with respect to the mean SMAPE values gives an
overall p-value of 0.115. Although there is no strong statistical evidence in terms of signiﬁcance, we
conclude that the Cocob optimizer is further preferred over the other two, since it does not require to
set an initial learning rate. This supports fully automating the forecasting approach, since it eliminates
the tuning of one more external hyperparameter.
5.4. Performance of the Output Components for the Sequence to Sequence Architecture
Figure 15 shows the relative performance of the two output components for the S2S architecture.
The dense layer performs better than the decoder owing to the error accumulation issue associated
with the teacher forcing used in the decoder as mentioned in Section 2.3.1. With teacher forcing,
the autoregressive connections in the decoder tend to carry forward the errors generating from each
Figure 14: Relative Performance of Diﬀerent Optimizers
forecasting step, resulting in even more uncertainty of the forecasts along the prediction horizon. The
output from the paired Wilcoxon signed-rank test with respect to the mean SMAPE values gives an
overall p-value of 2.064 × 10−4 which indicates that the decoder performs signiﬁcantly worse than the
dense layer.
Figure 15: Comparison of the Ouput Component for the Sequence to Sequence with the Dense Layer Architecture
5.5. Comparison of Input Window Sizes for the Stacked Architecture
Figure 16 shows the comparison of the two input window size options for the Stacked architecture on
the two daily datasets, NN5 and Wikipedia Web Traﬃc. Both with and without STL Decomposition
results are plotted.
Here, ‘Large’ denotes an input window size slightly larger than the expected
prediction horizon while ‘Small’ denotes an input window size slightly larger than the seasonality
period which is 7 (for the daily data). From the plots, we can state that large input window sizes help
the Stacked architecture both when STL Decomposition is used and not used. However, with mean
MASE ranks, small input window sizes also perform comparably. For the case when the seasonality is
not removed, making the input window size large improves the accuracy by a huge margin. Thus, large
input windows make it easier for the Stacked architecture to learn the underlying seasonal patterns
in the time series. The output from the paired Wilcoxon signed-rank test with respect to the mean
SMAPE values gives an overall p-value of 2.206 × 10−3 which indicates that the small input window
size performs signiﬁcantly worse than the large input window size when both with STL Decomposition
and without STL Decomposition cases are considered together.
Figure 16: Comparison of Input Window Sizes for the Stacked Architecture
5.6. Analysis of Seasonality Modelling
The results from comparing the models with STL Decomposition and without STL Decomposition
are as illustrated in Figures 17 and 18, for the mean SMAPE metric and the mean MASE metric
respectively.
Due to the scale of the M4 monthly dataset, we do not run all the models without removing
seasonality on that dataset. Rather, only the best few models by looking at the results from the ﬁrst
stage with removed seasonality, are selected to run without removing seasonality. As a consequence,
we do not plot those results here. The plots indicate that on the CIF, M3 and the Tourism datasets,
removing seasonality works better than modelling seasonality with the NN itself. On the M4 monthly
dataset too, the same observation holds looking at the results in Table 5. However, on the Wikipedia
Figure 17: Comparison of the Performance with and without STL Decomposition - Mean SMAPE
Figure 18: Comparison of the Performance with and without STL Decomposition - Mean MASE
Web Traﬃc dataset, except for few outliers, modelling seasonality using the NN itself works almost
as good as removing seasonality beforehand. This can be attributed to the fact that in the Wikipedia
Web Traﬃc dataset, all the series have quite minimal seasonality according to the violin plots in Figure
9, so that there is no big diﬀerence in the data if the seasonality is removed or not. On the other hand,
on the NN5 dataset too, NNs seem to be able to reasonably model seasonality on their own, except
for few outliers.
To further explain this result, we also plot the seasonal patterns of the diﬀerent datasets as shown
in the Figure 19. For the convenience of illustration, in every category of the M4 monthly dataset and
the Wikipedia Web Traﬃc dataset, we plot only the ﬁrst 400 series. Furthermore, from every series
of every dataset, only the ﬁrst 50 time steps are plotted. More seasonal pattern plots for the diﬀerent
categories of the CIF, M3 monthly and M4 monthly datasets are available in the Online Appendix3.
In the NN5 dataset, almost all the series have the same seasonal pattern as in the 4th plot of
Figure 19, with all of them having the same length as indicated in Table 2. All the series in the NN5
dataset start and end on the same dates . Also, according to Figure 9, the NN5 dataset
has higher seasonality, with the seasonality strengths and patterns having very little variation among
the individual series. In contrast, for the Tourism dataset, although it contains series with higher
seasonality, it has a high variation of the individual seasonality strengths and patterns as well as the
lengths of the series. The starting and ending dates are diﬀerent among the series. Looking at the 5th
plot of Figure 19, it is also evident that the series have very diﬀerent seasonal patterns. For the rest of
the datasets too, the lengths as well as the seasonal patterns of the individual series vary considerably.
Therefore, from these observations we can conclude that NNs are capable of modelling seasonality on
their own, when all the series in the dataset have similar seasonal patterns and the lengths of the time
series are equal, with the start and the end dates coinciding.
Table 6 shows the rankings as well as the overall p-values obtained from the paired Wilcoxon
signed-rank tests for comparing the cases with and without STL Decomposition in every dataset. The
overall p-values are calculated for each dataset separately by comparing the with STL Decomposition
and without STL Decomposition cases in all the available models.
Overall p-value
With STL Decomp.
Without STL Decomp.
2.91 × 10−11
2.91 × 10−11
1.455 × 10−10
Wikipedia Web Traﬃc
Table 6: Average rankings and results of the statistical testing for seasonality modelling with respect to mean SMAPE
across all the datasets. On the CIF, M3 and the Tourism datasets, using STL Decomposition has the best ranking. On
the Wikipedia Web Traﬃc and the NN5 datasets, not using STL Decomposition has the best ranking. The calculated
overall p-values obtained from the paired Wilcoxon signed-rank test are shown for the diﬀerent datasets on the Overall
p-value column.
On the CIF, M3 and the Tourism datasets, eliminating STL Decomposition performs signiﬁcantly
worse than applying STL Decomposition. However, on the Wikipedia Web Traﬃc dataset, applying STL Decomposition
performs signiﬁcantly worse than not applying it. On the NN5 dataset, there is no signiﬁcant diﬀerence between using
and not using STL Decomposition.
5.7. Performance of RNN Models Vs. Traditional Univariate Benchmarks
The relative performance of the model types in terms of the mean SMAPE metric and median
SMAPE metric are shown in Figure 20 and Figure 21 respectively. More comparison plots in terms
of the mean MASE and median MASE metrics are available in the Online Appendix4. We identify as
3 
4 
Figure 19: Seasonal Patterns of All the Datasets
Model Name
Stacked GRU cocob
Stacked LSTM cocob
Stacked ERNN cocob
Table 7: Mean SMAPE Results with the Number of Trainable Parameters as a Hyperparameter
a representative suggested model combination from the above sections the Stacked architecture with
LSTM cells with peephole connections and the COCOB optimizer, and therefore indicated it in every
plot as ’Stacked LSTM COCOB’ (With small and large input window sizes). If the best RNN on each
dataset diﬀers from Stacked LSTM COCOB, these best models are also shown in the plots. All the
other RNNs are indicated as ’RNN’.
The performance of the RNN architectures compared to traditional univariate benchmarks depend
on the performance metric used. An RNN architecture is able to outperform the benchmark techniques
on all the datasets except the M4 monthly dataset, in terms of both the SMAPE and MASE error
metrics. On the M4 monthly dataset, some RNNs outperform ETS, but ARIMA performs better than
all of the RNNs. However, since we build the models per each category of the M4 monthly dataset,
we further plot the performance in terms of the diﬀerent categories in Figure 22. From these plots, we
can see that RNNs outperform traditional univariate benchmarks only in the Micro category. Further
plots in terms of the other error metrics are shown in the Online Appendix5.
We also observe that mean error metrics are dominated by some outlier errors for certain time series.
This is due to the observation that on the Tourism dataset, RNNs outperform ETS and ARIMA with
respect to the median SMAPE but not the mean SMAPE. Furthermore, the ’Stacked LSTM COCOB’
model combination in general performs competitively on most datasets. It outperforms the two univariate benchmarks on the CIF 2016, NN5 and the Wikipedia Web Traﬃc datasets. Therefore, we can
state that Stacked model combined with the LSTM with peephole cells and the COCOB optimizer is
a competitive model combination to perform forecasting.
5.8. Experiments Involving the Total Number of Trainable Parameters
The comparison of the relative performance of the recurrent units presented in Section 5.2, is by
considering the cell dimension as a tunable hyperparameter for the RNNs. However, as mentioned
before in Section 4.3.1, in other research communities it is also common to tune the total number of
trainable parameters as a hyperparameter instead of the cell dimension. Therefore, we also perform
experiments by tuning the total number of trainable parameters as a hyperparameter to compare
the relative performance of the diﬀerent recurrent unit types.
For this experiment we select the
best conﬁgurations identiﬁed via the results analyzed thus far. Consequently, we choose the Stacked
architecture along with the COCOB optimizer to run on all the three recurrent unit types. We perform
this experiment on the four datasets CIF, Wikipedia Web Traﬃc, M3 and NN5 where the RNNs
have outperformed the statistical benchmarks. Particularly on the NN5 and Wikipedia Web Traﬃc
datasets, we run the version of the Stacked architecture without applying the STL decomposition with
the increased input window size. Table 7 shows these results in terms of the mean SMAPE values.
The violin plots in Figure 23 indicate the relative performance of the three RNN units in terms of
both the mean SMAPE and mean MASE ranks. In accordance with the results obtained in Section
5.2, Figure 23 also indicates that the LSTM cell with peephole connections performs the best, the
ERNN cell performs the worst and the performance of the GRU is between the other two. Again, the
Friedman test of statistical signiﬁcance performed in terms of the mean SMAPE values produces an
overall p-value of 0.174 which means that this diﬀerence is not statistically signiﬁcant.
5 
Figure 20: Performance of RNNs Compared to Traditional Univariate Techniques - Mean SMAPE
Figure 21: Performance of RNNs Compared to Traditional Univariate Techniques - Median SMAPE
Figure 22: Performance of RNNs Compared to Traditional Univariate Techniques in Diﬀerent M4 Categories - Mean
Figure 23: Relative Performance of Diﬀerent Recurrrent Cell Types under the Same Number of Total Trainable
Parameters
Overall, we see that the results obtained after tuning for the total number of trainable parameters
are coherent with the results obtained by tuning for the cell dimension instead. As mentioned in Section
4.3.1, there is a direct connection between the amount of trainable parameters and the cell dimension.
Following Smyl , the cell dimension is not a very sensitive hyperparameter, which means that
changes in the cell dimension, and consequently changes in the number of trainable parameters, do not
have signiﬁcant eﬀects on the RNN performance. Therefore, we assume that the conclusions derived by
tuning the cell dimension as a hyperparameter hold valid even after correcting for the total number of
trainable parameters in the diﬀerent recurrent units. Nevertheless, since tuning for the cell dimension
is the viable approach due to practical limitations, we conclude that it is suﬃcient to tune the cell
dimension across a similar range to produce a suitable number of trainable parameters in the diﬀerent
recurrent unit types.
5.9. Comparison of the Computational Costs of the RNN Models Vs. the Benchmarks
In addition to the comparisons of the prediction performance presented thus far, we perform a
comparison between the computational times of the RNN models with respect to the chosen standard
benchmarks, on the four datasets CIF2016, Wikipedia Web Traﬃc, M3 and NN5 where an RNN model
outperforms the benchmarks.
The computational times presented in Table 8 are for the best performing RNN models in each
one of those datasets. For the purpose of comparison, both the benchmarks and the RNN models
are allocated 25 CPU cores for the execution. The overall process of the RNN models has diﬀerent
stages in its pipeline such as preprocessing of the data, tuning the hyperparameters and the ﬁnal
model training with the optimal conﬁguration for testing. Therefore, Table 8 shows a breakdown of
the computational costs for these individual stages as well as the total time.
From Table 8 we see that, compared to the standard benchmarks, the RNN models have taken
a considerable amount of computational time for the overall process. For instance, on the horizon
12 category of the CIF dataset, the total time for the RNN is 3749.5 s (1.0 hr) whereas auto.arima
and ets have taken only 85.1 s (1.4 min) and 41.6 s respectively. Similarly on the NN5 dataset, the
RNN model has taken a total of 51490.7 s (14.3 hrs) whereas auto.arima and ets models have taken
1067.7s (17.8min) and 53.3s only. However, it is also evident in Table 8, that most of the computational
time in RNNs is devoted to the hyperparameter tuning using SMAC with 50 iterations. For example,
in the Micro category of the M3 dataset, although the whole pipeline takes 2523.0 s (42.1 min),
Preprocessing
Hyperparameter Tuning
Model Training & Testing
S2SD LSTM NMW cocob
auto.arima
S2SD LSTM NMW cocob
auto.arima
NSTL Stacked GRU adam
auto.arima
S2SD GRU MW adagrad
auto.arima
S2SD GRU MW adagrad
auto.arima
S2SD GRU MW adagrad
auto.arima
S2SD GRU MW adagrad
auto.arima
S2SD GRU MW adagrad
auto.arima
S2SD GRU MW adagrad
auto.arima
Stacked LSTM adam
auto.arima
Table 8: Computational Times Comparison of the RNNs with the Benchmarks (in seconds)
the training of the ﬁnal model with the optimal conﬁguration and testing has accounted for only
565.6 s (9.4 min). On the other hand, the total running time for auto.arima and ets on the same
dataset is 849.8 s (14.2 min) and 73.0 s (1.2 min) respectively. Hence, even though the overall process
of RNNs is computationally costly, the training and testing of one model can be comparable to the
computational costs of the statistical benchmarks. Moreover, compared to the benchmarks which build
one model per every series, the ﬁnal trained models from RNNs are less complex with fewer parameters
than the univariate techniques, on a global scale.
From this comparison we see that RNNs are typically computationally more expensive models
compared to traditional univariate techniques. However, regardless of the computational cost they are
capable of performing better than the traditional univariate benchmarks in all the cases mentioned
in Table 8. This ﬁnding is another aspect of the recent changes in the forecasting community now
acknowledging that complex methods can have merits over simpler statistical benchmarks . Yet, our study shows how RNNs can be trained in a way to achieve such improvements.
With the availability of massive amounts of computational resources nowadays, the improved accuracy
brought forward by the RNNs for forecasting is certainly beneﬁcial for forecasting practitioners.
5.10. Hyperparameter Conﬁgurations
Usually, the larger the initial hyperparameter space that needs to be searched, the higher the
number of iterations of the automated hyperparameter tuning technique should be. This depends on
the number of hyperparameters as well as the initial hyperparameter ranges. We use 50 iterations
of the SMAC algorithm for hyperparameter tuning to be suitable across all the datasets. For our
experiments we choose roughly the same range across all the datasets as shown in Table 4, except for
the minibatch size. The minibatch size needs to be chosen proportional to the size of the dataset.
Usually, for the lower bound of the initial hyperparameter range of the minibatch size, around 1/10th
of the size of the dataset is appropriate. However, we vary the upper bound in diﬀerent orders for the
diﬀerent datasets. For small datasets such as CIF, NN5, Tourism and the diﬀerent categories of M3,
the upper bound diﬀers from the lower bound in the orders of 10s. For bigger datasets such as the
Wikipedia Web Traﬃc, and the diﬀerent categories of M4, we set the upper bound to be larger than the
lower bound in the orders of 100s. For the number of hidden layers in the RNN, many recent studies
suggest that a low value usually performs better . Following this convention, we choose the number of layers between
1-2 and we observe that the RNNs perform well with such low values. On the one hand this is due to
the overﬁtting eﬀects resulting from the increased number of parameters with the added layers. On
the other hand, the number of layers also directly relates to the computational complexity. As for the
learning rates, we see that the convergence of the Adagrad optimizer usually requires higher learning
rates in the range 0.01 - 0.9. For the Adam optimizer the identiﬁed range is smaller in between 0.001
- 0.1. On the other hand, variations in the cell dimension and the standard deviation of the random
normal initializer barely impact the performance of the models. It is also important not to set large
values for the standard deviation of the Gaussian noise and the L2 weight regularization parameters,
since too high a value for them makes the model almost completely underﬁt the data and eliminate
the NN’s eﬀect entirely in producing the ﬁnal forecasts.
6. Conclusions
The motivation of this study is to address some of the key issues in using RNNs for forecasting
and evaluate if and how they can be used by forecasting practitioners with limited knowledge of
these techniques for their forecasting tasks eﬀectively. Through a number of systematic experiments
across datasets with diverse characteristics, we derive conclusions from general data preprocessing best
practices to hyperparameter conﬁgurations, best RNN architectures, recurrent units and optimizers.
All our models exploit cross-series information in the form of global models and thus leverage the
existence of massive time series databases with many related time series.
From our experiments we conclude that the Stacked architecture combined with the LSTM cells
with peephole connections and the COCOB optimizer, fed with deseasonalized data in a moving
window format can be a competitive model generally across many datasets. In particular, though
not statistically signiﬁcant, the LSTM is the best unit type even when correcting for the amount of
trainable parameters. We further conclude that when all the series in the dataset follow homogeneous
seasonal patterns with all of them covering the same duration in time with suﬃcient lengths, RNNs
are capable of capturing the seasonality without prior deseasonalization. Otherwise, RNNs are weak
in modelling seasonality on their own, and a deseasonalization step should be employed. From the
experiments involving the pooled and unpooled versions of the regression models, we can conclude
that the concept of cross-series information helps in certain types of datasets. However, even on those
datasets which involve many heterogeneous series, the strong modelling capabilities of RNNs can drive
them to perform competitively in terms of the forecasting accuracy. Therefore, we can conclude that
leveraging cross-series information has its own beneﬁts on sets of time series while the predictive
capability provided by the RNNs can further improve the forecasting accuracy. With respect to the
computational costs, we observe that RNNs take relatively higher computational times compared to
the statistical benchmarks. However, with the cloud infrastructure nowadays commonly in place at
companies such processing times are feasible. Moreover, our study has empirically proven that RNNs
are good candidates for forecasting which in many cases outperform the statistical benchmarks that are
currently the state-of-the-art in the community. Thus, with the extensive experiments involved with
this study, we can conﬁrm that complex methods now have beneﬁts over simpler statistical benchmarks
in many forecasting situations.
Our procedure is (semi-)automatic as for the initial hyperparameter ranges of the SMAC algorithm,
we select approximately the same range across all the datasets except for the minibatch size which we
select depending on the size of each dataset. Thus, though ﬁtting of RNNs is still not as straightforward
and automatic as for the two state-of-the-art univariate forecasting benchmarks, ets and auto.arima,
our paper and our released code framework are important steps in this direction. We ﬁnally conclude
that RNNs are now a good option for forecasting practitioners to obtain reliable forecasts which can
outperform the benchmarks.
7. Future Directions
The results of our study are limited to point forecasts in a univariate context.
Nevertheless,
modelling uncertainty of the NN predictions through probabilistic forecasting has received growing
attention recently in the community. Also, when considering a complex forecasting scenario such as in
retail industry, the sales of diﬀerent products may be interdependent. Therefore, a sales forecasting task
in such a context requires multivariate forecasting as opposed to univariate forecasting. Furthermore,
although this study considers only single seasonality forecasting, in terms of higher frequency data
with suﬃcient length, it becomes beneﬁcial to model multiple seasonalities in a big data context.
As seen in our study, global NN models often suﬀer from outlier errors for certain time series. This
is probably due to the fact that the average weights of NNs found by ﬁtting global models may not
be suitable for the individual requirements of certain time series. Consequently, it becomes necessary
to develop novel models which incorporate both global parameters as well as local parameters for
individual time series, in the form of hierarchical models, potentially combined with ensembling where
the single models are trained in diﬀerent ways with the existing dataset (e.g., on diﬀerent subsets).
The work by Bandara et al. , Smyl , and Sen et al. are steps in this direction, but
still this topic remains largely an open research question.
In general, deep learning is a fast-paced research ﬁeld, where many new architectures are introduced and discussed rapidly. However, for practitioners it often remains unclear in which situations
the techniques are most useful and how diﬃcult it is to adapt them to a given application case. Recently, CNNs, although initially intended for image processing, have become increasingly popular for
time series forecasting. The work carried out by Shih et al. and Lai et al. follow the
argument that typical RNN-based attention schemes are not good at modelling seasonality. Therefore, they use a combination of CNN ﬁlters to capture local dependencies and a custom attention
score function to model the long-term dependencies. Lai et al. have also experimented with
recurrent skip connections to capture seasonality patterns. On the other hand, Dilated Causal Convolutions are speciﬁcally designed to capture long-range dependencies eﬀectively along the temporal
dimension . Such layers stacked on top of each other can build massive hierarchical attention networks, attending points even way back in the history. They have been recently
used along with CNNs for time series forecasting problems. More advanced CNNs have also been
introduced such as Temporal Convolution Networks (TCN) which combine both dilated convolutions
and residual skip connections. TCNs have also been used for forecasting in recent literature . Recent studies suggest that TCNs are promising NN architectures for sequence modelling
tasks on top of being eﬃcient in training . Therefore, a competitive advantage may
begin to unfold for forecasting practitioners by using CNNs instead of RNNs.
Acknowledgment
This research was supported by the Australian Research Council under grant DE190100045, Facebook Statistics for Improving Insights and Decisions research award, Monash University Graduate
Research funding and MASSIVE - High performance computing facility, Australia.