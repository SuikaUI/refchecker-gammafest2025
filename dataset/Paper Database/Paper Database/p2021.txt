Audio-Visual Scene Analysis with
Self-Supervised Multisensory Features
Andrew Owens
Alexei A. Efros
UC Berkeley
Abstract. The thud of a bouncing ball, the onset of speech as lips open — when
visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that
the visual and audio components of a video signal should be modeled jointly
using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether
video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the
source of sound in a video; (b) audio-visual action recognition; and (c) on/offscreen audio source separation, e.g. removing the off-screen translator’s voice
from a foreign ofﬁcial’s speech. Code, models, and video results are available on
our webpage: 
Introduction
As humans, we experience our world through a number of simultaneous sensory streams.
When we bite into an apple, not only do we taste it, but — as Smith and Gasser point
out — we also hear it crunch, see its red skin, and feel the coolness of its core. The coincidence of sensations gives us strong evidence that they were generated by a common,
underlying event , since it is unlikely that they co-occurred across multiple modalities merely by chance. These cross-modal, temporal co-occurrences therefore provide
a useful learning signal: a model that is trained to detect them ought to discover multimodal structures that are useful for other tasks. In much of traditional computer vision
research, however, we have been avoiding the use of other, non-visual modalities, arguably making the perception problem harder, not easier.
In this paper, we learn a temporal, multisensory representation that fuses the visual
and audio components of a video signal. We propose to train this model without using
any manually labeled data. That is, rather than explicitly telling the model that, e.g., it
should associate moving lips with speech or a thud with a bouncing ball, we have it discover these audio-visual associations through self-supervised training . Speciﬁcally,
we train a neural network on a “pretext” task of detecting misalignment between audio
and visual streams in synthetically-shifted videos. The network observes raw audio and
video streams — some of which are aligned, and some that have been randomly shifted
by a few seconds — and we task it with distinguishing between the two. This turns out
to be a challenging training task that forces the network to fuse visual motion with audio
information and, in the process, learn a useful audio-visual feature representation.
We demonstrate the usefulness of our multisensory representation in three audiovisual applications: (a) sound source localization, (b) audio-visual action recognition;
 
Owens and Efros
“Cutting in kitchen”
Predicted on-screen sound
(a) Sound localization
(b) Action recognition
(c) On/off-screen audio separation
Fig. 1: Applications. We use self-supervision to learn an audio-visual representation that: (a) can
be used to visualize the locations of sound sources in video; (b) is useful for visual and audiovisual action recognition; (c) can be applied to the task of separating on- and off-screen sounds. In
(c), we demonstrate our source-separation model by visually masking each speaker and asking it
to predict the on-screen audio. The predicted sound contains only the voice of the visible speaker.
Please see our webpage for video results: 
and (c) on/off-screen sound source separation. Figure 1 shows examples of these applications. In Fig. 1(a), we visualize the sources of sound in a video using our network’s
learned attention map, i.e. the impact of an axe, the opening of a mouth, and moving
hands of a musician. In Fig. 1(b), we show an application of our learned features to
audio-visual action recognition, i.e. classifying a video of a chef chopping an onion.
In Fig. 1(c), we demonstrate our novel on/off-screen sound source separation model’s
ability to separate the speakers’ voices by visually masking them from the video.
The main contributions of this paper are: 1) learning a general video representation
that fuses audio and visual information; 2) evaluating the usefulness of this representation qualitatively (by sound source visualization) and quantitatively (on an action recognition task); and 3) proposing a novel video-conditional source separation method that
uses our representation to separate on- and off-screen sounds, and is the ﬁrst method to
work successfully on real-world video footage, e.g. television broadcasts. Our feature
representation, as well as code and models for all applications are available online.
Related work
Evidence from psychophysics
While we often think of vision and hearing as being
distinct systems, in humans they are closely intertwined through a process known
as multisensory integration. Perhaps the most compelling demonstration of this phenomenon is the McGurk effect , an illusion in which visual motion of a mouth
changes one’s interpretation of a spoken sound1. Hearing can also inﬂuence vision:
the timing of a sound, for instance, affects whether we perceive two moving objects to
be colliding or overlapping . Moreover, psychologists have suggested that humans
1 For a particularly vivid demonstration, please see: 
watch?v=G-lN8vWm3m0 .
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
fuse audio and visual signals at a fairly early stage of processing , and that the
two modalities are used jointly in perceptual grouping. For example, the McGurk effect is less effective when the viewer ﬁrst watches a video where audio and visuals in
a video are unrelated, as this causes the signals to become “unbound” (i.e. not grouped
together) . This multi-modal perceptual grouping process is often referred to as
audio-visual scene analysis . In this paper, we take inspiration from psychology and propose a self-supervised multisensory feature representation as a computational model of audio-visual scene analysis.
Self-supervised learning
Self-supervised methods learn features by training a model
to solve a task derived from the input data itself, without human labeling. Starting with
the early work of de Sa , there have been many self-supervised methods that learn to
ﬁnd correlations between sight and sound . These methods, however, have
either learned the correspondence between static images and ambient sound , or
have analyzed motion in very limited domains (e.g. only modeled drumstick impacts). Our learning task resembles Arandjelovi´c and Zisserman , which
predicts whether an image and an audio track are sampled from the same (or different)
videos. Their task, however, is solvable from a single frame by recognizing semantics
(e.g. indoor vs. outdoor scenes). Our inputs, by contrast, always come from the same
video, and we predict whether they are aligned; hence our task requires motion analysis
to solve. Time has also been used as supervisory signal, e.g. predicting the temporal
ordering in a video . In contrast, our network learns to analyze audio-visual
actions, which are likely to correspond to salient physical processes.
Audio-visual alignment
While we study alignment for self-supervised learning, it
has also been studied as an end in itself e.g. in lip-reading applications .
Chung and Zisserman , the most closely related approach, train a two-stream network with an embedding loss. Since aligning speech videos is their end goal, they use
a face detector (trained with labels) and a tracking system to crop the speaker’s face.
This allows them to address the problem with a 2D CNN that takes 5 channel-wise concatenated frames cropped around a mouth as input (they also propose using their image
features for self-supervision; while promising, these results are very preliminary).
Sound localization
The goal of visually locating the source of sounds in a video
has a long history. The seminal work of Hershey et al. localized sound sources
by measuring mutual information between visual motion and audio using a Gaussian
process model. Subsequent work also considered subspace methods , canonical correlations , and keypoints . Our model learns to associate motions with sounds
via self-supervision, without us having to explicitly model them.
Audio-Visual Source Separation
Blind source separation (BSS), i.e. separating
the individual sound sources in an audio stream — also known as the cocktail party
problem — is a classic audio-understanding task . Researchers have proposed
many successful probabilistic approaches to this problem . More recent
deep learning approaches involve predicting an embedding that encodes the audio clustering , or optimizing a permutation invariant loss . It is natural to also want
to include the visual signal to solve this problem, often referred to as Audio-Visual
Source Separation. For example, masked frequencies based on their correlation
with optical ﬂow; used graphical models; used priors on harmonics; used
Owens and Efros
a sparsity-based factorization method; and used a clustering method. Other methods use face detection and multi-microphone beamforming . These methods make
strong assumptions about the relationship between sound and motion, and have mostly
been applied to lab-recorded video. Researchers have proposed learning-based methods that address these limitations, e.g. use mixture models to predict separation
masks. Recently, proposed a convolutional network that isolates on-screen speech,
although this model is relatively small-scale (tested on videos from one speaker). We
do on/off-screen source separation on more challenging internet and broadcast videos
by combining our representation with a u-net regression model.
Concurrent work
Concurrently and independently from us, a number of groups have
proposed closely related methods for source separation and sound localization. Gabbay
et al. use a vision-to-sound method to separate speech, and propose a convolutional separation model. Unlike our work, they assume speaker identities are known.
Ephrat et al. and Afouras et al. separate the speech of a user-chosen speaker
from videos containing multiple speakers, using face detection and tracking systems
to group the different speakers. Work by Zhao et al. and Gao et al. separate
sound for multiple visible objects (e.g. musical instruments). This task involves associating objects with the sounds they typically make based on their appearance, while
ours involves the “ﬁne-grained” motion-analysis task of separating multiple speakers.
There has also been recent work on localizing sound sources using a network’s attention
map . These methods are similar to ours, but they largely localize objects and
ambient sound in static images, while ours responds to actions in videos.
Learning a self-supervised multisensory representation
We propose to learn a representation using self-supervision, by training a model to
predict whether a video’s audio and visual streams are temporally synchronized.
Aligning sight with sound
During training, we feed a neural network video clips.
In half of them, the vision and sound streams are synchronized; in the others, we shift
the audio by a few seconds. We train a network to distinguish between these examples.
More speciﬁcally, we learn a model pθ(y | I, A) that predicts whether the image stream
I and audio stream A are synchronized, by maximizing the log-likelihood:
2EI,A,t[log(pθ(y = 1 | I, A0)) + log(pθ(y = 0 | I, At))],
where As is the audio track shifted by s secs., t is a random temporal shift, θ are the
model parameters, and y is the event that the streams are synchronized. This learning
problem is similar to noise-contrastive estimation , which trains a model to distinguish between real examples and noise; here, the noisy examples are misaligned videos.
Fused audio-visual network design
Solving this task requires the integration of lowlevel information across modalities. In order to detect misalignment in a video of human
speech, for instance, the model must associate the subtle motion of lips with the timing
of utterances in the sound. We hypothesize that early fusion of audio and visual streams
is important for modeling actions that produce a signal in both modalities. We therefore
propose to solve our task using a 3D multisensory convolutional network (CNN) with
an early-fusion design (Figure 2).
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
5×7×7 conv, 64 / 
[3×3×3 conv] × 4, 64 / 
3×1×1 conv, 128
1×1×1 conv, 512
tile & concatenate
global average pool
fc & sigmoid
Video frames
65×1×1 conv, 64 / 4
1×1×1 conv, 128
pool / 
Misaligned sound
[3×3×3 conv] × 4, 512 / 
[3×3×3 conv] × 4, 256 / 
[3×3×3 conv] × 4, 128 / 
[15×1×1 conv] × 2, 256 / 4
[15×1×1 conv] × 2, 128 / 4
[15×1×1 conv] × 2, 128 / 4
Fig. 2: Fused audio-visual network. We train an early-fusion, multisensory network to predict
whether video frames and audio are temporally aligned. We include residual connections between
pairs of convolutions . We represent the input as a T × H × W volume, and denote a stride
by “/2”. To generate misaligned samples, we synthetically shift the audio by a few seconds.
Before fusion, we apply a small number of 3D convolution and pooling operations
to the video stream, reducing its temporal sampling rate by a factor of 4. We also apply a series of strided 1D convolutions to the input waveform, until its sampling rate
matches that of the video network. We fuse the two subnetworks by concatenating their
activations channel-wise, after spatially tiling the audio activations. The fused network
then undergoes a series of 3D convolutions, followed by global average pooling .
We add residual connections between pairs of convolutions. We note that the network
architecture resembles ResNet-18 but with the extra audio subnetwork, and 3D
convolutions instead of 2D ones (following work on inﬂated convolutions ).
We train our model with 4.2-sec. videos, randomly shifting the audio by
2.0 to 5.8 seconds. We train our model on a dataset of approximately 750,000 videos
randomly sampled from AudioSet . We use full frame-rate videos (29.97 Hz), resulting in 125 frames per example. We select random 224 × 224 crops from resized
256 × 256 video frames, apply random left-right ﬂipping, and use 21 kHz stereo sound.
We sample these video clips from longer (10 sec.) videos. Optimization details can be
found in Section A1.
Task performance
We found that the model obtained 59.9% accuracy on held-out
videos for its alignment task (chance = 50%). While at ﬁrst glance this may seem low,
we note that in many videos the sounds occur off-screen . Moreover, we found that
this task is also challenging for humans. To get a better understanding of human ability,
we showed 30 participants from Amazon Mechanical Turk 60 aligned/shifted video
pairs, and asked them to identify the one with out-of-sync sound. We gave them 15
Owens and Efros
Fig. 3: Visualizing sound sources. We show the video frames in held-out AudioSet videos with
the strongest class activation map (CAM) response (we scale its range per image to compensate
for the wide range of values).
secs. of video (so they have signiﬁcant temporal context) and used large, 5-sec. shifts.
They solved the task with 66.6% ± 2.4% accuracy.
To help understand what actions the model can predict synchronization for, we also
evaluated its accuracy on categories from the Kinetics dataset (Figure A1). It was
most successful for classes involving human speech: e.g., news anchoring, answering
questions, and testifying. Of course, the most important question is whether the learned
audio-visual representation is useful for downstream tasks. We therefore turn out attention to applications.
Fig. 4: Examples with the weakest class activation map response (c.f. Figure 3).
Visualizing the locations of sound sources
One way of evaluating our representation is to visualize the audio-visual structures
that it detects. A good audio-visual representation, we hypothesize, will pay special
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
Chopping wood
Shufﬂing cards
Dribbling basketball
Playing guitar
Playing organ
Playing clarinet
Tap dancing
Playing xylophone
Fig. 5: Strongest CAM responses for classes in the Kinetics-Sounds dataset , after manually
removing frames in which the activation was only to a face (which appear in almost all categories). We note that no labeled data was used for training. We do not rescale the heat maps per
image (i.e. the range used in this visualization is consistent across examples).
attention to visual sound sources — on-screen actions that make a sound, or whose
motion is highly correlated with the onset of sound. We note that there is ambiguity
in the notion of a sound source for in-the-wild videos. For example, a musician’s lips,
their larynx, and their tuba could all potentially be called the source of a sound. Hence
we use this term to refer to motions that are correlated with production of a sound, and
study it through network visualizations.
To do this, we apply the class activation map (CAM) method of Zhou et al. ,
which has been used for localizing ambient sounds . Given a space-time video patch
Ix, its corresponding audio Ax, and the features assigned to them by the last convolutional layer of our model, f(Ix, Ax), we can estimate the probability of alignment with:
p(y | Ix, Ax) = σ(w⊤f(Ix, Ax)),
where y is the binary alignment label, σ the sigmoid function, and w is the model’s
ﬁnal afﬁne layer. We can therefore measure the information content of a patch — and,
by our hypothesis, the likelihood that it is a sound source — by the magnitude of the
prediction |w⊤f(Ix, Ax)|.
One might ask how this self-supervised approach to localization relates to generative approaches, such as classic mutual information methods . To help understand this, we can view our audio-visual observations as having been produced by a
generative process (using an analysis similar to ): we sample the label y, which de-
Owens and Efros
Multisensory (full)
Multisensory (spectrogram)
Multisensory (random pairing )
Multisensory (vision only)
Multisensory (scratch)
I3D-RGB (scratch) 
Purushwalkam et al. *
C3D *
Shufﬂe *
Wang et al. *
I3D-RGB + ImageNet 
I3D-RGB + ImageNet + Kinetics 
Table 1: Action recognition on UCF-101
(split 1). We compared methods pretrained
without labels (top), and with semantic
labels (bottom). Our model, trained both
with and without sound, signiﬁcantly outperforms other self-supervised methods. Numbers annotated with “*” were obtained from
their corresponding publications; we retrained/evaluated the other models.
termines the alignment, and then conditionally sample Ix and Ax. Rather than computing mutual information between the two modalities (which requires a generative model
that self-supervised approaches do not have), we ﬁnd the patch/sound that provides the
most information about the latent variable y, based on our learned model p(y | Ix, Ax).
Visualizations
What actions does our network respond to? First, we asked which
space-time patches in our test set were most informative, according to Equation 2.
We show the top-ranked patches in Figure 3, with the class activation map displayed
as a heatmap and overlaid on its corresponding video frame. From this visualization,
we can see that the network is selective to faces and moving mouths. The strongest
responses that are not faces tend to be unusual but salient audio-visual stimuli (e.g.
two top-ranking videos contain strobe lights and music). For comparison, we show the
videos with the weakest response in Figure 4; these contain relatively few faces.
Next, we asked how the model responds to videos that do not contain speech, and
applied our method to the Kinetics-Sounds dataset — a subset of Kinetics 
classes that tend to contain a distinctive sound. We show the examples with the highest
response for a variety of categories, after removing examples in which the response was
solely to a face (which appear in almost every category). We show results in Figure 5.
Finally, we asked how the model’s attention varies with motion. To study this, we
computed our CAM-based visualizations for videos, which we have included in the
supplementary video (we also show some hand-chosen examples in Figure 1(a)). These
results qualitatively suggest that the model’s attention varies with on-screen motion.
This is in contrast to single-frame methods models , which largely attend to
sound-making objects rather than actions.
Action recognition
We have seen through visualizations that our representation conveys information about
sound sources. We now ask whether it is useful for recognition tasks. To study this, we
ﬁne-tuned our model for action recognition using the UCF-101 dataset , initializing the weights with those learned from our alignment task. We provide the results in
Table 1, and compare our model to other unsupervised learning and 3D CNN methods.
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
We train with 2.56-second subsequences, following , which we augment with
random ﬂipping and cropping, and small (up to one frame) audio shifts. At test time,
we follow and average the model’s outputs over 25 clips from each video, and use
a center 224 × 224 crop. Please see Section A1 for optimization details.
We see, ﬁrst, that our model signiﬁcantly outperforms self-supervised approaches that have previously been applied to this task, including Shufﬂe-and-Learn
 (82.1% vs. 50.9% accuracy) and O3N (60.3%). We suspect this is in part due
to the fact that these methods either process a single frame or a short sequence, and
they solve tasks that do not require extensive motion analysis. We then compared our
model to methods that use supervised pretraining, focusing on the state-of-the-art I3D
 model. While there is a large gap between our self-supervised model and a version
of I3D that has been pretrained on the closely-related Kinetics dataset (94.5%), the performance of our model (with both sound and vision) is close to the (visual-only) I3D
pretrained with ImageNet (84.2%).
Next, we trained our multisensory network with the self-supervision task of 
rather than our own, i.e. creating negative examples by randomly pairing the audio
and visual streams from different videos, rather than by introducing misalignment. We
found that this model performed signiﬁcantly worse than ours (78.7%), perhaps due to
the fact that its task can largely be solved without analyzing motion.
Finally, we asked how components of our model contribute to its performance. To
test whether the model is obtaining its predictive power from audio, we trained a variation of the model in which the audio subnetwork was ablated (activations set to zero),
ﬁnding that this results in a 5% drop in performance. This suggests both that sound is
important for our results, and that our visual features are useful in isolation. We also
tried training a variation of the model that operated on spectrograms, rather than raw
waveforms, ﬁnding that this yielded similar performance (Section A2). To measure the
importance of our self-supervised pretraining, we compared our model to a randomly
initialized network (i.e. trained from scratch), ﬁnding that there was a signiﬁcant (14%)
drop in performance — similar in magnitude to removing ImageNet pretraining from
I3D. These results suggest that the model has learned a representation that is useful both
for vision-only and audio-visual action recognition.
On/off-screen audio-visual source separation
We now apply our representation to a classic audio-visual understanding task: separating on- and off-screen sound. To do this, we propose a source separation model that uses
our learned features. Our formulation of the problem resembles recent audio-visual and
audio-only separation work . We create synthetic sound mixtures by summing an input video’s (“on-screen”) audio track with a randomly chosen (“off-screen”)
track from a random video. Our model is then tasked with separating these sounds.
We consider models that take a spectrogram for the mixed audio as input
and recover spectrogram for the two mixture components. Our simplest on/off-screen
separation model learns to minimize:
LO(xM, I) = ∥xF −fF (xM, I)∥1 + ∥xB −fB(xM, I)∥1,
where xM is the mixture sound, xF and xB are the spectrograms of the on- and off-
Owens and Efros
Off-screen
Video + mixed audio
Mixed spectrogram
Multisensory net
Fig. 6: Adapting our audio-visual network to a source separation task. Our model separates an input spectrogram into on- and off-screen audio streams. After each temporal downsampling layer,
our multisensory features are concatenated with those of a u-net computed over spectrograms.
We invert the spectrograms to obtain waveforms. The model operates on raw video, without any
preprocessing (e.g. no face detection).
screen sounds that comprise it (i.e. foreground and background), and fF and fB are our
model’s predictions of them conditional on the (audio-visual) video I.
We also consider models that segment the two sounds without regard for their onor off-screen provenance, using the permutation invariant loss (PIT) of Yu et al. .
This loss is similar to Equation 3, but it allows for the on- and off-screen sounds to be
swapped without penalty:
LP(xF , xB, ˆx1, ˆx2) = min(L(ˆx1, ˆx2), L(ˆx2, ˆx1)),
where L(xi, xj) = ∥xi −xF ∥1 + ∥xj −xB∥1 and ˆx1 and ˆx2 are the predictions.
Source separation model
We augment our audio-visual network with a u-net encoder-decoder that
maps the mixture sound to its on- and off-screen components (Figure 6). To provide the
u-net with video information, we include our multisensory network’s features at three
temporal scales: we concatenate the last layer of each temporal scale with the layer of
the encoder that has the closest temporal sampling rate. Prior to concatenation, we use
linear interpolation to make the video features match the audio sampling rate; we then
mean-pool them spatially, and tile them over the frequency domain, thereby reshaping our 3D CNN’s time/height/width shape to match the 2D encoder’s time/frequency
shape. We use parameters for u-net similar to , adding one pair of convolution layers to compensate for the large number of frequency channels in our spectrograms. We
predict both the magnitude of the log-spectrogram and its phase (we scale the phase
loss by 0.01 since it is less perceptually important). To obtain waveforms, we invert the
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
GRID transfer
On/off SDR
SAR On/off SDR On/off SDR On/off SDR
On/off + PIT
Full on/off
Single frame
No early fusion
I3D + Kinetics
u-net PIT 
Deep Sep. 
Table 2: Source separation results on speech mixtures from the VoxCeleb (broken down by gender
of speakers in mixture) and transfer to the simple GRID dataset. We evaluate the on/off-screen
sound prediction error (On/off) using ℓ1 distance to the true log-spectrograms (lower is better).
We also use blind source separation metrics (higher is better) .
VoxCeleb short videos (200ms)
On-SDR SDR SIR SAR
Ours (on/off)
Hou et al. 
Gabbay et al. 
PIT-CNN 
u-net PIT 
Deep Sep. 
Comparison
audiovisual and audio-only separation
methods on short (200ms) videos.
We compare SDR of the on-screen
audio prediction (On-SDR) with
audio resampled to 2 kHz.
predicted spectrogram. We emphasize that our model uses raw video, with no preprocessing or labels (e.g. no face detection or pretrained supervised features).
We evaluated our model on the task of separating speech sounds using the
VoxCeleb dataset . We split the training/test to have disjoint speaker identities (72%,
8%, and 20% for training, validation, and test). During training, we sampled 2.1-sec.
clips from longer 5-sec. clips, and normalized each waveform’s mean squared amplitude
to a constant value. We used spectrograms with a 64 ms frame length and a 16 ms step
size, producing 128 × 1025 spectrograms. In each mini-batch of the optimization, we
randomly paired video clips, making one the off-screen sound for the other. We jointly
optimized our multisensory network and the u-net model, initializing the weights using
our self-supervised representation (see supplementary material for details).
Evaluation
We compared our model to a variety of separation methods: 1) we replaced our selfsupervised video representation with other features, 2) compared to audio-only methods
using blind separation methods, 3) and compared to other audio-visual models.
Since one of our main goals is to evaluate the quality of the learned features, we compared several variations of our model (Table 2). First, we replaced the
multisensory features with the I3D network pretrained on the Kinetics dataset — a
3D CNN-based representation that was very effective for action recognition (Section 5).
This model performed signiﬁcantly worse (11.4 vs. 12.3 spectrogram ℓ1 loss for Equation 3). One possible explanation is that our pretraining task requires extensive motion
analysis, whereas even single-frame action recognition can still perform well .
Owens and Efros
Ground truth
Ground truth
Ground truth
Ground truth
Fig. 7: Qualitative results from our on/off-screen separation model. We show an input frame and
spectrogram for two synthetic mixtures from our test set, and two in-the-wild internet videos
containing multiple speakers. The ﬁrst (a male/male mixture) contains more artifacts than the
second (a female/male mixture). The third video is a real-world mixture in which a female speaker
(simultaneously) translates a male Spanish speaker into English. Finally, we separate the speech
of two (male) speakers on a television news show. Although there is no ground truth for these
real-world examples, the source separation method qualitatively separates the two voices. Please
refer to our webpage ( for video source separation results.
We then asked how much of our representation’s performance comes from motion
features, rather than from recognizing properties of the speaker (e.g. gender). To test
this, we trained the model with only a single frame (replicated temporally to make a
video). We found a signiﬁcant drop in performance (11.4 vs. 14.8 loss). The drop was
particularly large for mixtures in which two speakers had the same gender — a case
where lip motion is an important cue.
One might also ask whether early audio-visual fusion is helpful — the network,
after all, fuses the modalities in the spectrogram encoder-decoder as well. To test this,
we ablated the audio stream of our multisensory network and retrained the separation
model. This model obtained worse performance, suggesting the fused audio is helpful even when it is available elsewhere. Finally, while the encoder-decoder uses only
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
monaural audio, our representation uses stereo. To test whether it uses binaural cues,
we converted all the audio to mono and re-evaluated it. We found that this did not signiﬁcantly affect performance, which is perhaps due to the difﬁculty of using stereo cues
in in-the-wild internet videos (e.g. 39% of the audio tracks were mono). Finally, we
also transferred (without retraining) our learned models to the GRID dataset , a labrecorded dataset in which people speak simple phrases in front of a plain background,
ﬁnding a similar relative ordering of the methods.
Audio-only separation
To get a better understanding of our model’s effectiveness, we
compared it to audio-only separation methods. While these methods are not applicable
to on/off-screen separation, we modiﬁed our model to have it separate audio using an
extra permutation invariant loss (Equation 4) and then compared the methods using
blind separation metrics : signal-to-distortion ratio (SDR), signal-to-interference
ratio (SIR), and signal-to-artifacts ratio (SAR). For consistency across methods, we
resampled predicted waveforms to 16 kHz (the minimum used by all methods), and
used the mixture phase to invert our model’s spectrogram, rather than the predicted
phase (which none of the others predict).
We compared our model to PIT-CNN . This model uses a VGG-style CNN
to predict two soft separation masks via a fully connected layer. These maps are multiplied by the input mixture to obtain the segmented streams. While this method worked
well on short clips, we found it failed on longer inputs (e.g. obtaining 1.8 SDR in the
experiment shown in Table 2). To create a stronger PIT baseline, we therefore created
an audio-only version of our u-net model, optimizing the PIT loss instead of our on/offscreen loss, i.e. replacing the VGG-style network and masks with u-net. We conﬁrmed
that this model obtains similar performance on short sequences (Table 3), and found it
successfully trained on longer videos. Finally, we compared with a pretrained separation
model , which is based on recurrent networks and trained on the TSP dataset .
We found that our audio-visual model, when trained with a PIT loss, outperformed
all of these methods, except for on the SAR metric, where the u-net PIT model was
slightly better (which largely measures the presence of artifacts in the generated waveform). In particular, our model did signiﬁcantly better than the audio-only methods
when the genders of the two speakers in the mixture were the same (Table 2). Interestingly, we found that the audio-only methods still performed better on blind separation
metrics when transferring to the lab-recorded GRID dataset, which we hypothesize is
due to the signiﬁcant domain shift.
Audio-visual separation
We compared to the audio-visual separation model of Hou
et al. . This model was designed for enhancing the speech of a previously known
speaker, but we apply it to our task since it is the most closely related prior method. We
also evaluated the network of Gabbay et al. (a concurrent approach to ours). We
trained these models using the same procedure as ours ( used speaker identities to
create hard mixtures; we instead assumed speaker identities are unknown and mix randomly). Both models take very short (5-frame) video inputs. Therefore, following 
we evaluated 200ms videos (Table 3). For these baselines, we cropped the video around
the speaker’s mouth using the Viola-Jones lip detector of (we do not use face
detection for our own model). These methods use a small number of frequency bands
in their (Mel-) STFT representations, which limits their quantitative performance. To
Owens and Efros
address these limitations, we evaluated only the on-screen audio, and downsampled the
audio to a low, common rate (2 kHz) before computing SDR. Our model signiﬁcantly
outperforms these methods. Qualitatively, we observed that often smooths the input spectrogram, and we suspect its performance on source separation metrics may be
affected by the relatively small number of frequency bands in its audio representation.
Qualitative results
Our quantitative results suggest that our model can successfully separate on- and offscreen sounds. However, these metrics are limited in their ability to convey the quality of
the predicted sound (and are sensitive to factors that may not be perceptually important,
such as the frequency representation). Therefore, we also provide qualitative examples.
Real mixtures
In Figure 7, we show results for two synthetic mixtures from our
test set, and two real-world mixtures: a simultaneous Spanish-to-English translation
and a television interview with concurrent speech. We exploit the fact that our model is
fully convolutional to apply it to these 8.3-sec. videos (4× longer than training videos).
We include additional source separation examples in the videos on our webpage. This
includes a random sample of (synthetically mixed) test videos, as well as results on
in-the-wild videos that contain both on- and off-screen sound.
Multiple on-screen sound sources
To demonstrate our model’s ability to vary its
prediction based on the speaker, we took a video in which two people are speaking on
a TV debate show, visually masked one side of the screen (similar to ), and ran our
source separation model. As shown in Figure 1, when the speaker on the left is hidden,
we hear the speaker on the right, and vice versa. Please see our video for results.
Large-scale training
We trained a larger variation of our model on signiﬁcantly
more data. For this, we combined the VoxCeleb and VoxCeleb2 datasets (approx.
8× as manys videos), as in , and modeled ambient sounds by sampling background
audio tracks from AudioSet approximately 8% of the time. To provide more temporal
context, we trained with 4.1-sec. videos (approx. 256 STFT time samples). We also simpliﬁed the model by decreasing the spectrogram frame length to 40 ms (513 frequency
samples) and increased the weight of the phase loss to 0.2. Please see our webpage for
Discussion
In this paper, we presented a method for learning a temporal multisensory representation, and we showed through experiments that it was useful for three downstream
tasks: (a) pretraining action recognition systems, (b) visualizing the locations of sound
sources, and (c) on/off-screen source separation. We see this work as opening two potential directions for future research. The ﬁrst is developing new methods for learning
fused multisensory representations. We presented one method — detecting temporal
misalignment — but one could also incorporate other learning signals, such as the information provided by ambient sound . The other direction is to use our representation
for additional audio-visual tasks. We presented several applications here, but there are
other audio-understanding tasks could potentially beneﬁt from visual information and,
likewise, visual applications that could beneﬁt from fused audio information.
Audio-Visual Scene Analysis with Self-Supervised Multisensory Features
Acknowledgements
This work was supported, in part, by DARPA grant FA8750-16-
C-0166, U.C. Berkeley Center for Long-Term Cybersecurity, and Berkeley DeepDrive.
We thank Allan Jabri, David Fouhey, Andrew Liu, Morten Kolbæk, Xiaolong Wang,
and Jitendra Malik for the helpful discussions.