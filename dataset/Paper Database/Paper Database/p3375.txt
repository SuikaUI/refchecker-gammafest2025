Neurocomputing 69 714–720
Kernel methods and the exponential family
Ste´ phane Canua,!, Alex Smolab,c
a1-PSI-FRE CNRS 2645, INSA de Rouen, France, St Etienne du Rouvray, France
bStatistical Machine Learning Program, National ICT, Australia
cRSISE, Australian National University, Canberra, 0200 ACT, Australia
Available online 25 January 2006
The success of support vector machine (SVM) has given rise to the development of a new class of theoretically elegant learning
machines which use a central concept of kernels and the associated reproducing kernel Hilbert space (RKHS). Exponential families, a
standard tool in statistics, can be used to unify many existing machine learning algorithms based on kernels (such as SVM) and to invent
novel ones quite effortlessly. A new derivation of the novelty detection algorithm based on the one class SVM is proposed to illustrate the
power of the exponential family model in an RKHS.
r 2005 Published by Elsevier B.V.
Keywords: Kernel methods; Exponential families; Novelty detection
1. Introduction
Machine learning is providing increasingly important
tools in many ﬁelds such as text processing, machine vision,
speech, to name just a few. Among these new tools, kernel
based algorithms have demonstrated their efﬁciency on
many practical problems. These algorithms performed
function estimation, and the functional framework behind
these algorithms is now well known . But still too little is
known about the relation between these learning algorithms and more classical statistical tools such as likelihood, likelihood ratio, estimation and test theory. A key
model to understand this relation is the generalized or non
parametric exponential family. This exponential family is a
generic way to represent any probability distribution since
any distribution can be well approximated by an exponential distribution. The idea here is to retrieve learning
algorithm by using the exponential family model with
classical statistical principle such as the maximum penalized likelihood estimator or the generalized likelihood
ratio test.
Outline. To do so the paper is organized as follows. The
ﬁrst section presents the functional framework and Reproducing Kernel Hilbert Space (RKHS). Then the exponential
family on an RKHS is introduced and classiﬁcation as well as
density estimation and regression kernel-based algorithms
such as SVM are derived. In the ﬁnal section new material is
presented establishing the link between the kernel-based oneclass SVM novelty detection algorithm and classical test
theory. It is shown how this novelty detection can be seen as
an approximation of a generalized likelihood ratio and,
therefore, as an optimal test.
2. Functional framework
Learning can be seen as retrieving a relevant function
among a large class of possible hypotheses. One strength of
kernel-based learning algorithm is the ability to express the
hypotheses
(functions)
representing the way to evaluate a function at a given
point. This is possible because the relationship between the
kernel and the underlying functional space is constituent.
There exists a bijection between a large class of useful
kernels (positive ones) and interesting functional spaces:
the so-called reproducing kernel Hilbert spaces. Details
regarding this bijection are now precised. Let X be the
learning domain (typically X ! Rd).
Deﬁnition 1 (Reproducing Kernel Hilbert Space—RKHS). A
Hilbert space ðH; h:; :iHÞ of functions on a domain X
ARTICLE IN PRESS
www.elsevier.com/locate/neucom
0925-2312/$ - see front matter r 2005 Published by Elsevier B.V.
doi:10.1016/j.neucom.2005.12.009
!Corresponding author. Fax: +33 2 32 95 97 08.
E-mail addresses: , 
(S. Canu), (A. Smola).
(deﬁned pointwise) is an RKHS if the evaluation functional
is continuous on H.
For instance Rn, the set Pk of polynomials of order k, as
any ﬁnite dimensional set of genuine functions form an
RKHS. The space of sequences ‘2 is also an RKHS, the
evaluation function in this case being the value of the series
at location x 2 X ¼ N. Note that the usual L2 spaces, such
as L2ðRnÞ (using the Lebesgue measure) are not RKHS
since their evaluation functionals are not continuous (in
fact, L2 elements are not even deﬁned in a pointwise way).
For more details see and references therein.
Deﬁnition 2 (Positive Kernel). A mapping k from X % X
to R is a positive kernel if it is symmetric and if for any
ﬁnite integer n, any ﬁnite subset fxig; i ¼ 1; n of X and any
sequence of scalar coefﬁcients faig; i ¼ 1; n the following
inequality holds:
aiajKðxi; xjÞX0.
This deﬁnition is equivalent to the one of Aronszajn .
The following corollary arises from [11, Proposition 23]
and [13, Theorem 1.1.1].
Proposition 3 (Bijection between RKHS and Kernel). There
is a bijection between the set of all possible RKHS and the set
of all positive kernels.
Thus Mercer kernels (as deﬁned in ) are a particular
case of a more general situation since every Mercer kernel
is positive in the Aronszajn sense (Deﬁnition 2) while the
converse need not be true.
It is always possible to associate with any positive kernel
a set of function and a dot product (an RKHS) on which
the evaluation functional is continuous. Conversely, the
continuity of the evaluation functional guarantee the
existence of a positive kernel. This continuity is the key
property of the hypothesis set. It is associated with a useful
property to be used hereafter: the reproducing property of
the kernel k in an RKHS. It is closely related to the fact
that, in RKHS, functions are pointwise-deﬁned and the
evaluation functional is continuous. Thus, because of this
continuity, Riesz theorem can be stated as follows
for allf 2 Hand for allx 2 X
we havef ðxÞ ¼ hf ð:Þ; kðx; :ÞiH.
This continuity implies that two closed functions also have
their pointwise values closed, for any x. In the framework
of learning machines this means that if two hypotheses are
closed, then you do not want their prediction at any point
to differ too much.
In the remainder of the paper we assume that we are
given the RKHS, its dot product and its kernel k. When
appropriate we will refer to kðx; :Þ as a map of x into the socalled ‘‘feature space’’. The dot product considered is the
one of the RKHS.
3. Kernel approaches for the exponential family
3.1. Parametric exponential families
We begin by reviewing some basic facts of exponential
families. Denote by X a domain, mðXÞ a (not necessarily
ﬁnite) measure on X and let fðxÞ be a map from X into Rp
(it is called the sufﬁcient statistics). Then a probability
measure on X with respect to f and y 2 Y ! Rp is given by
Pðx; yÞ ¼ expðy>fðxÞ & gðyÞÞ,
gðyÞ ¼ log
expðy>fðxÞÞ dmðxÞ.
Here it is understood that Pðx; yÞ is taken with respect to
the underlying measure mðXÞ. A large class of distributions
can be described this way, as can be seen in Table 1. This
table also shows examples of carrier measures m and
parameter domains Y.
The function gðyÞ is typically referred to as the logpartition function or, since its derivatives generate the
cumulants of Pðx; yÞ, as the cumulant generating function.
It is analytic and convex on the domain on which (3b)
exists. Exponential representation can be seen as a change
of parameterization of the distribution. The main interest
of this transformation is, through the convexity of the logpartition function, the convexity of the associated likelihood with respect to the exponential parameters.
3.2. Nonparametric exponential families
Unfortunately, not all integrals in (3b) can be computed
explicitly in closed form. Moreover, the set of distributions
which can be modeled by mapping x into a ﬁnitedimensional fðxÞ is rather limited. We now consider an
extension of the deﬁnition to nonparametric distributions.
Assume there exists a reproducing kernel Hilbert space
H embedded with the dot product h:; :iH and with a
reproducing kernel k such that kernel kðx; :Þ is a sufﬁcient
statistics of x. Then in exponential families the density
Pðx; yÞ is given by
Pðx; yÞ ¼ mðxÞ expðhyð:Þ; kðx; :ÞiH & gðyÞÞ,
gðyÞ ¼ log
expðhyð:Þ; kðx; :ÞiHÞ dmðxÞ,
where m is the carrier density, which can be absorbed into
the underlying measure as above (and set to 1). Here y is
the natural parameter and gðyÞ is the log-partition function,
also called the cumulant generating function. All we
changed from before is that now y is an element of an
RKHS and fðxÞ is also a map into such a space, given by
fðxÞ ¼ kðx; 'Þ thus
hyð:Þ; kðx; :ÞiH ¼ yðxÞ
Pðx; yÞ ¼ mðxÞ expyðxÞ&gðyÞ.
ARTICLE IN PRESS
S. Canu, A. Smola / Neurocomputing 69 714–720
When we are concerned with estimating conditional
probabilities, the exponential families framework can be
extended to conditional densities
Pðyjx; yÞ ¼ mðyjxÞ expðhyð:Þ; kðx; y; :ÞiH & gðyjxÞÞ
gðyjxÞ ¼ log
expðhyð:Þ; kðx; y; :ÞiHÞ dmðyjxÞ.
gðyjxÞ is commonly referred to as the conditional logpartition function. Both gðyÞ and gðyjxÞ are convex C1
functions in y and they can be used to compute moments of
a distribution
qygðyð:ÞÞ ¼ Epðx;yÞ½kðx; :Þ)
qygðyjxÞ ¼ Epðx;y;yÞ½kðx; yÞjx)
ygðyð:ÞÞ ¼ Varpðx;yÞ½kðx; :Þ)
ygðyjxÞ ¼ Varpðx;y;yÞ½kðx; yÞjx)
We will also assume there exists some prior distribution on
parameter y
Z exp & kyk2
where Z is a normalizing constant.1 In this case, the
posterior density can be written as PðyjxÞ / PðxjyÞPðyÞ. In
this paper only the estimation of parameter y using the
maximum a posteriori principle (MAP) together with the
normal prior will be taken into account.
Note that other priors could have been used such as
conjugate prior.
regularisation terms
could have been considered by optimizing a penalized
likelihood. Relation between the choice of the regularization term and the nature of the estimates is still an open
3.3. Natural parameter space
The exponential family is not deﬁned for any parameter
y but only in a natural parameter space deﬁned as follows:
Deﬁnition 4 (natural parameter space). The natural parameter space of an exponential family is the set of y where it
is deﬁned, i.e. such that
expðhyð:Þ; kðx; :ÞiHÞ dmðxÞo1.
The question is now to ﬁnd out the structure of the
natural parameter space. To do so we can deﬁne admissible
kernels for a given exponential family.
Deﬁnition 5 (admissible kernel). A kernel k is said to be
admissible for a measure m if
expkðx;xÞ1=2 dmðxÞo1.
ARTICLE IN PRESS
Common parametric exponential families used for estimating univariate and discrete distributions
logð1 þ eyÞ
Multinomial
Exponential
& logð1 & eyÞ
2 log 2p & 1
2 log y2 þ 1
R % ð0; 1Þ
2 log 2p & 1
2 log jy2j þ 1
Rn % Cone Rn2
Inv. Normal
2 log p & 2
ðlog x; logð1 & xÞÞ
log Gðy1ÞGðy2Þ
Gðy1 þ y2Þ
ðlog x; &xÞ
log Gðy1Þ & y1 log y2
ðlog jxj; &1
&y1 log jy2j þ y1n log 2 þ Pn
i¼1 log G y1 þ 1 & i
R % Cone Rn2
ðlog x1; . . . ; log xnÞ
i¼1 log GðyiÞ & log GðPn
The notation is geared towards an inner product setting, that is, using fðxÞ and gðyÞ in an explicit form.
1Note that Z need not exist on the entire function space but rather only
on the linear subspace of points where Pðx; yÞ is evaluated. The extension
to entire function spaces requires tools from functional analysis, namely
Radon derivatives with respect to the Gaussian measure imposed by the
prior itself.
S. Canu, A. Smola / Neurocomputing 69 714–720
This allow us to state the following proposition:
Proposition 6. For any admissible kernel k, the associated
natural parameter space includes the unit ball of the
associated RKHS.
Proof. By Cauchy-Schwartz:
expðhyð:Þ; kðx; :ÞiHÞ dmðxÞ
expðkykHkkðx; :ÞkHÞ dmðxÞ
thus, kkðx; :Þk2
H ¼ kðx; xÞ and in the unit ball of the RKHS
Some simple examples of nice behavior can be given. If
the domain X is bounded, then any kernel is admissible.
For unbounded domain and the Gaussian kernel, there
exists some measure m that makes the kernel admissible. As
we will see below, the knowledge of the reference measure
is not needed. Only its existence is required.
4. Nonparametric exponential families for learning
4.1. Kernel logistic regression and Gaussian process
Assume we observe some training data ðxi; yiÞ; i ¼ 1; n.
classiﬁcation
yi 2 f&1; þ1g. In this case we can use the conditional
exponential family to model PðY ¼ yjxÞ. The estimation of
its parameter y using the maximum a posteriori (MAP)
principle aims at minimizing the following cost function:
& log PðyjdataÞ ¼ &
hyð:Þ; kðxi; yi; :ÞiH
þ gðy; xiÞ þ hyð:Þ; yð:ÞiH=2s2 þ C,
where C is some constant term. Note that this can also be
seen as a penalized likelihood cost function and thus
connected to minimum description length principle and
regularized risk minimization.
Proposition 7 (Feature map for binary classiﬁcation). Without loss of generality the kernel for binary classiﬁcation can
be written as
hkðx; y; 'Þ; kðx0; y0; 'ÞiH ¼ yy0kðx; x0Þ.
Proof. Assume that instead of kðx; y; 'Þ we have kðx; y; 'Þ þ
f 0 where f 0 is a function of x only. In this case gðyÞ is
transformed
gðyjxÞ þ f 0ðxÞ
hkðx; y; 'Þ; yð'ÞiH into yðx; yÞ þ f 0ðxÞ). Hence the conditional
density remains unchanged. This implies that we can ﬁnd
an offset f 0 such that P
y kðx; y; 'Þ ¼ 0. The consequence
for binary classiﬁcation is that
kðx; 1; 'Þ þ kðx; &1; 'Þ ¼ 0
and therefore
kðx; y; 'Þ ¼ yk0ðx; 'Þ
for some k0, such as k0ðx; 'Þ ¼ kðx; 1; 'Þ. Taking inner
products proves the claim.
reproducing
hyð:Þ; kðxi; :ÞiHÞ we have
gðy; xiÞ ¼ logðexp yðxiÞ þ exp &yðxiÞÞ.
Then after some algebra the MAP estimator can be found
by minimizing
log ð1 þ expð&2yðxiÞyiÞÞ þ 1
On this minimization problem, the representer theorem
(see for more details) gives us
yiaikðxi; :Þ.
The associated optimization problem can be rewritten in
terms of a
log 1 þ exp &2
yjajkðxi; xjÞ
yjyiaiajkðxi; xjÞ.
It is non-linear and can be solved using Newton method.
The connection is made with the kernel logistic regression
since in our framework we have
log PðY ¼ 1jxÞ
PðY ¼ &1jxÞ ¼ 2
yiaikðxi; xÞ
and thus the decision of classifying a new data x only
depends on the sign of the kernel term through the
following decision rule:
decide class 1
yiaikðxi; xÞ4b,
where b is a decision threshold to be set according to a
given risk. This is closely related with kernel-based receiver
as treated in .
Note that the multiclass problem can be solved by using
derivations
kðxi; yi; x; yÞ ¼ kðxi; xÞdyiy.
4.2. Two-class support vector machines
We now deﬁne the margin of a classiﬁer (binary or not)
as the most pessimistic log-likelihood ratio for classiﬁcation. That is, we deﬁne
rðx; y; yÞ:¼ log Pðyjx; yÞ & max
~yay log Pð~yjx; yÞ.
Clearly, whenever rðx; y; yÞ40, we classify correctly. Moreover, its magnitude gives the logarithm of the ratio between
the correct class and the most dominant incorrect class. In
other words, large values of r imply that we are very
conﬁdent about classifying x as y. In terms of classiﬁcation
ARTICLE IN PRESS
S. Canu, A. Smola / Neurocomputing 69 714–720
accuracy this is a more useful proposition than the loglikelihood, as the latter does not provide necessary and
sufﬁcient conditions for correct classiﬁcation.
It is easy to see that, for binary classiﬁcation, this yields
rðx; y; yÞ ¼ hy; ykðx; 'Þi & hy; &ykðx; 'Þi ¼ 2yyðxÞ
which is the standard deﬁnition of the margin. Instead of
the MAP estimate we optimize with respect to a reference
margin r, i.e. we minimize in the soft case margin
1=2kyk2 þ CPn
i¼1xi with riXr & xi and xiX0 leading to
maxð0; r & rðxi; yi; yÞÞ þ 1
Together with the exponential family model, the minimization of this criterion leads to the maximum margin
classiﬁer. Here again this can be easily generalized to the
multiclass problem.
4.3. One-class support vector machines
The one-class SVM algorithm has been designed to
estimate some quantile from sample data. This is closely
related but simpler than estimating the whole density.
Recently it has been proven that it is a consistent estimate
of the density level set and of the minimum measure set
 (i.e. roughly, for a given p0 2 ½0; 1), the subset C ! X
minimizing some volume and such that PðCÞ ¼ p0).
One-class SVMs are also more relevant when the target
application is novelty detection. As a matter of fact, any
point not belonging to the support of a density can be seen
as a novel one.
Back with our exponential family model for PðxÞ, a
robust approximation of maximum a posteriori (MAP)
estimator for y is the one maximizing
min P0ðxijyÞ
with p0 ¼ expðr & gðyÞÞ. After some tedious algebra, this
problem can be rewritten as
maxðr & hyð:Þ; kðxi; :ÞiH; 0Þ þ 1
On this problem again the representer theorem gives us the
existence of some coefﬁcient ai such that
aikðxi; :Þ
and thus the estimator has the following form:
bPðxÞ ¼ mðxÞ exp
aikðxi; :Þ & b
where coefﬁcients a are determined by solving the one-class
SVM problem (14). Parameter b represents the value of the
log partition function and thus the normalization factor. It
can be hard to compute it but it is possible to do without it
in our applications. The nature and the choice of the
reference measure m depends on the application (see for
a non parametric estimation of the reference measure
together with the exponential family).
Here again the one-class SVM algorithm can be derived
using the exponential family on an RKHS and a relevant
cost function to be minimized.
4.4. Regression
It is possible to see the problem as a generalization of the
classiﬁcation case to continuous y. But in this case, a
generalized version of the representer theorem shows that
parameters a are no longer scalar but functions, leading to
intractable optimization problems. Some additional hypotheses have to be made about the nature of the unknown
distribution. One way to do it is to use the conditional
gaussian representation with its natural parameters
PðyjxÞ ¼ expðyy1ðxÞ þ y2y2ðxÞ & gðy1ðxÞ; y2ðxÞÞÞ
with y1ðxÞ ¼ mðxÞ=s2ðxÞ and y2ðxÞ ¼ &1=2s2ðxÞ where
mðxÞ is the conditional expectation of y given x and s2ðxÞ
its conditional variance. The associated kernel can be
written as follows:
kðxi; yi; x; yÞ ¼ k1ðxi; xÞyyi þ k2ðxi; xÞy2y2
where k1 and k2 are two positive kernels. In this case the
application of the represented theorem gives a heteroscedastic gaussian process (with non-constant variance) as
the model of the data, associated with a convex optimization problem (see for details). Convexity is the main
interest of the framework. Such a model directly using
functions mðxÞ and sðxÞ as unknown have been proposed
before. But the associated optimization problem is non
convex while the use of the exponential family representation leads to convex optimization problem.
5. Application to novelty detection
Let X i; i ¼ 1; 2; . . . ; 2t be a sequence of random variables
distributed according to some distribution Pi. We are
interested in ﬁnding whether or not a change has occurred
at time t. To begin with a simple framework we will assume
the sequence to be stationary from 1 to t and from t þ 1 to
2t, i.e. there exist some distributions P0 and P1 such that
Pi ¼ P0 for i 2 ½1; t) and Pi ¼ P1 for i 2 ½t þ 1; 2t). The
question we are addressing can be seen as determining if
P0 ¼ P1 (no change has occurred) or else P0aP1 (some
change have occurred). This can be restated as the
following statistical test:
H0: P0 ¼ P1;
H1: P0aP1:
ARTICLE IN PRESS
S. Canu, A. Smola / Neurocomputing 69 714–720
In this case the likelihood ratio is the following:
Llðx1; . . . ; x2tÞ ¼
i¼1 P0ðxiÞQ2t
i¼tþ1 P1ðxiÞ
i¼1 P0ðxiÞ
since both densities are unknown the generalized likelihood
ratio (GLR) has to be used
Lðx1; . . . ; x2tÞ ¼
where bP0 and bP1 are the maximum likelihood estimates of
the densities.
Because we want our detection method to be universal,
we want it to work for any possible density. Thus some
approximations have to be done to clarify our framework.
First, assuming both densities P0 and P1 belong to the
generalized exponential family, there exists a reproducing
kernel Hilbert space H embedded with the dot product
h:; :iH and with a reproducing kernel k such that
P0ðxÞ ¼ mðxÞ exphy0ð:Þ; kðx; :ÞiH & gðy0Þ
P1ðxÞ ¼ mðxÞ exphy1ð:Þ; kðx; :ÞiH & gðy1Þ,
where gðyÞ is the so-called log-partition function and m
some reference measure. Note that this factorisation of the
unknown density in two terms (m and the exponential)
makes it possible to focus on a relevant quantity for the
task to be performed. This quantity (the exponential part)
exponential
measure set and it is sufﬁcient to make relevant decisions
regarding the detection of abrupt changes. The analysis of
this factorisation has to be investigated in more details.
Second hypothesis, the functional parameter y0 and y1 of
these densities will be estimated on the data of respectively
the ﬁrst and the second half of the sample by using the oneclass SVM algorithm. By doing so we are following our
initial assumption that before time t we know the
distribution is constant and equal to some P0. The oneclass SVM algorithm provides us with a good estimator of
this density. The situation of bP1ðxÞ is more simple. It is
clearly a robust approximation of the maximum likelihood
estimator.
exponential family model, both estimates can be written as
bP0ðxÞ ¼ mðxÞ exp
i kðx; xiÞ & gðy0Þ
bP1ðxÞ ¼ mðxÞ exp
i kðx; xiÞ & gðy1Þ
where að0Þ
is determined by solving the one-class SVM
problem on the ﬁrst half of the data (x1 to xt). while að1Þ
given by solving the one-class SVM problem on the second
half of the data (xtþ1 to x2t). Using these hypotheses, the
generalized likelihood ratio test is approximated as follows:
Lðx1; . . . ; x2tÞ4s
i¼tþ1 að1Þ
i kðxj; xiÞ & gðy1Þ
i kðxj; xiÞ & gðy0Þ
i kðxj; xiÞ &
i kðxj; xiÞ
where s0 is a threshold to be ﬁxed to have a given risk of the
ﬁrst kind a such that
i kðxj; xiÞ &
i kðxj; xiÞ
It turns out that the variation of P2t
i¼tþ1 að1Þ
i kðxj; xiÞ is very
small in comparison to that of Pt
i kðxj; xiÞ. Thus
bP1ðxÞ can be assumed to be constant, simplifying computations. In this case the test can be performed by only
considering
i kðxj; xiÞ
This is exactly the novelty detection algorithm as proposed
by Scho¨ lkopf et al. and adapted for change detection in
 and continued in . To sum up, we showed how to
derive the heuristic described Eq. (17) as a statistical test
approximating a generalized likelihood ratio test, optimal
under some condition in the Neyman Pearson framework.
This framework can be easily extended to do sequential
hypothesis testing through a Wald sequential probability
ratio test using almost the same likelihood ratio. The
associated decision function is the following:
i kðxj; xiÞ
i kðxj; xiÞ
pick one more sample;
where A and B are two thresholds to be set according to
some preﬁxed error rates. The goal here is to minimize
detection delay.
Note that for practical use the distribution of the test
statistic under the null-hypothesis is required to deﬁne the
threshold levels for a given level of signiﬁcance. To do so,
resampling techniques can be used.
6. Conclusion
In this paper we have illustrated how powerful the link
algorithms,
Reproducing
Hilbert Space and the exponential family is. A lot of learning
algorithms can be revisited using this framework. Here we
have discussed the logistic kernel regression, the SVM, the
gaussian process for regression and the novelty detection
ARTICLE IN PRESS
S. Canu, A. Smola / Neurocomputing 69 714–720
using the one-class SVM. This framework is applicable to
many different cases and other derivations are possible:
exponential family in a RKHS can be used to recover
sequence annotation (via Conditional Random Fields) or
boosting, to name just a few. The exponential family
framework is powerful because it makes it possible to connect
learning algorithm with usual statistical tools such as
posterior densities and likelihood ratio, and to do so with
almost no loss of generality. These links between statistics and
learning have been detailed in the case of novelty detection
restated as a quasi optimal statistical test based on a robust
approximation of the generalized likelihood. Further works
on this ﬁeld regard the application of sequential analysis tools
such as the CUSUM algorithm for real-time novelty
detection minimizing the expectation of the detection delay.
Acknowledgements
National ICT Australia is funded through the Australian
Government’s Backing Australia’s Ability initiative, in part
through the Australian Research Council. This work was
supported by grants of the ARC and by the IST
Programme of the European Community, under the Pascal
Network of Excellence, IST-2002-506778.