AI2: Safety and Robustness CertiÔ¨Åcation of Neural
Networks with Abstract Interpretation
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri‚àó, Martin Vechev
Department of Computer Science
ETH Zurich, Switzerland
Abstract‚ÄîWe present AI2, the Ô¨Årst sound and scalable analyzer for deep neural networks. Based on overapproximation,
AI2 can automatically prove safety properties (e.g., robustness)
of realistic neural networks (e.g., convolutional neural networks).
The key insight behind AI2 is to phrase reasoning about safety
and robustness of neural networks in terms of classic abstract
interpretation, enabling us to leverage decades of advances in
that area. Concretely, we introduce abstract transformers that
capture the behavior of fully connected and convolutional neural
network layers with rectiÔ¨Åed linear unit activations (ReLU), as
well as max pooling layers. This allows us to handle real-world
neural networks, which are often built out of those types of layers.
We present a complete implementation of AI2 together with
an extensive evaluation on 20 neural networks. Our results
demonstrate that: (i) AI2 is precise enough to prove useful
speciÔ¨Åcations (e.g., robustness), (ii) AI2 can be used to certify
the effectiveness of state-of-the-art defenses for neural networks,
(iii) AI2 is signiÔ¨Åcantly faster than existing analyzers based on
symbolic analysis, which often take hours to verify simple fully
connected networks, and (iv) AI2 can handle deep convolutional
networks, which are beyond the reach of existing methods.
Index Terms‚ÄîReliable Machine Learning, Robustness, Neural
Networks, Abstract Interpretation
I. INTRODUCTION
Recent years have shown a wide adoption of deep neural
networks in safety-critical applications, including self-driving
cars , malware detection , and aircraft collision avoidance detection . This adoption can be attributed to the
near-human accuracy obtained by these models , .
Despite their success, a fundamental challenge remains:
to ensure that machine learning systems, and deep neural
networks in particular, behave as intended. This challenge
has become critical in light of recent research showing
that even highly accurate neural networks are vulnerable
to adversarial examples. Adversarial examples are typically
obtained by slightly perturbing an input that is correctly
classiÔ¨Åed by the network, such that the network misclassiÔ¨Åes
the perturbed input. Various kinds of perturbations have been
shown to successfully generate adversarial examples (e.g., ,
 , , , , , , , , , ). Fig. 1
illustrates two attacks, FGSM and brightening, against a digit
classiÔ¨Åer. For each attack, Fig. 1 shows an input in the Original
column, the perturbed input in the Perturbed column, and the
pixels that were changed in the Diff column. Brightened pixels
‚àóRice University, work done while at ETH Zurich.
FGSM , œµ = 0.3
Brightening, Œ¥ = 0.085
Fig. 1: Attacks applied to MNIST images .
are marked in yellow and darkened pixels are marked in purple. The FGSM attack perturbs an image by adding to it
a particular noise vector multiplied by a small number œµ (in
Fig. 1, œµ = 0.3). The brightening attack (e.g., ) perturbs
an image by changing all pixels above the threshold 1 ‚àíŒ¥ to
the brightest possible value (in Fig. 1, Œ¥ = 0.085).
Adversarial examples can be especially problematic when
safety-critical systems rely on neural networks. For instance,
it has been shown that attacks can be executed physically
(e.g., , ) and against neural networks accessible only as
a black box (e.g., , , ). To mitigate these issues,
recent research has focused on reasoning about neural network
robustness, and in particular on local robustness. Local robustness (or robustness, for short) requires that all samples in the
neighborhood of a given input are classiÔ¨Åed with the same
label . Many works have focused on designing defenses
that increase robustness by using modiÔ¨Åed procedures for
training the network (e.g., , , , , ). Others
have developed approaches that can show non-robustness by
underapproximating neural network behaviors or methods
that decide robustness of small fully connected feedforward
networks . However, no existing sound analyzer handles
convolutional networks, one of the most popular architectures.
Key Challenge: Scalability and Precision. The main challenge facing sound analysis of neural networks is scaling to
large classiÔ¨Åers while maintaining a precision that sufÔ¨Åces
to prove useful properties. The analyzer must consider all
possible outputs of the network over a prohibitively large set
of inputs, processed by a vast number of intermediate neurons.
For instance, consider the image of the digit 8 in Fig. 1 and
suppose we would like to prove that no matter how we brighten
the value of pixels with intensity above 1‚àí0.085, the network
will still classify the image as 8 (in this example we have
84 such pixels, shown in yellow). Assuming 64-bit Ô¨Çoating
2018 IEEE Symposium on Security and Privacy
¬© 2018, Timon Gehr. Under license to IEEE.
DOI 10.1109/SP.2018.00058
Brighten(0.085,
Convolutional#
Max Pooling#
Fully Connected#
Fig. 2: A high-level illustration of how AI2 checks that all
perturbed inputs are classiÔ¨Åed the same way. AI2 Ô¨Årst creates
an abstract element A1 capturing all perturbed images. (Here,
we use a 2-bounded set of zonotopes.) It then propagates A1
through the abstract transformer of each layer, obtaining new
shapes. Finally, it veriÔ¨Åes that all points in A4 correspond to
outputs with the same classiÔ¨Åcation.
point numbers are used to express pixel intensity, we obtain
more than 101154 possible perturbed images. Thus, proving
the property by running a network exhaustively on all possible
input images and checking if all of them are classiÔ¨Åed as 8 is
infeasible. To avoid this state space explosion, current methods
(e.g., , , ) symbolically encode the network as
a logical formula and then check robustness properties with
a constraint solver. However, such solutions do not scale to
larger (e.g., convolutional) networks, which usually involve
many intermediate computations.
Key Concept: Abstract Interpretation for AI.
insight of our work is to address the above challenge by leveraging the classic framework of abstract interpretation (e.g., ,
 ), a theory which dictates how to obtain sound, computable,
and precise Ô¨Ånite approximations of potentially inÔ¨Ånite sets of
behaviors. Concretely, we leverage numerical abstract domains
‚Äì a particularly good match, as AI systems tend to heavily
manipulate numerical quantities. By showing how to apply
abstract interpretation to reason about AI safety, we enable one
to leverage decades of research and any future advancements
in that area (e.g., in numerical domains ). With abstract
interpretation, a neural network computation is overapproximated using an abstract domain. An abstract domain consists
of logical formulas that capture certain shapes (e.g., zonotopes,
a restricted form of polyhedra). For example, in Fig. 2, the
green zonotope A1 overapproximates the set of blue points
(each point represents an image). Of course, sometimes, due
to abstraction, a shape may also contain points that will not
occur in any concrete execution (e.g., the red points in A2).
The AI2 Analyzer.
Based on this insight, we developed
a system called AI2 (Abstract Interpretation for ArtiÔ¨Åcial
Intelligence)1. AI2 is the Ô¨Årst scalable analyzer that handles common network layer types, including fully connected
and convolutional layers with rectiÔ¨Åed linear unit activations
(ReLU) and max pooling layers.
To illustrate the operation of AI2, consider the example in
1AI2 is available at: 
Fig. 2, where we have a neural network, an image of the
digit 8 and a set of perturbations: brightening with parameter
0.085. Our goal is to prove that the neural network classiÔ¨Åes
all perturbed images as 8. AI2 takes the image of the digit
8 and the perturbation type and creates an abstract element
A1 that captures all perturbed images. In particular, we can
capture the entire set of brightening perturbations exactly with
a single zonotope. However, in general, this step may result in
an abstract element that contains additional inputs (that is, red
points). In the second step, A1 is automatically propagated
through the layers of the network. Since layers work on
concrete values and not abstract elements, this propagation
requires us to deÔ¨Åne abstract layers (marked with #) that
compute the effects of the layers on abstract elements. The
abstract layers are commonly called the abstract transformers
of the layers. DeÔ¨Åning sound and precise, yet scalable abstract
transformers is key to the success of an analysis based on
abstract interpretation. We deÔ¨Åne abstract transformers for all
three layer types shown in Fig. 2.
At the end of the analysis, the abstract output A4 is
an overapproximation of all possible concrete outputs. This
enables AI2 to verify safety properties such as robustness
(e.g., are all images classiÔ¨Åed as 8?) directly on A4. In fact,
with a single abstract run, AI2 was able to prove that a
convolutional neural network classiÔ¨Åes all of the considered
perturbed images as 8.
We evaluated AI2 on important tasks such as verifying
robustness and comparing neural networks defenses. For example, for the perturbed image of the digit 0 in Fig. 1, we
showed that while a non-defended neural network classiÔ¨Åed
the FGSM perturbation with œµ = 0.3 as 9, this attack is
provably eliminated when using a neural network trained with
the defense of . In fact, AI2 proved that the FGSM attack
is unable to generate adversarial examples from this image for
any œµ between 0 and 0.3.
Main Contributions. Our main contributions are:
‚Ä¢ A sound and scalable method for analysis of deep neural
networks based on abstract interpretation (Section IV).
‚Ä¢ AI2, an end-to-end analyzer, extensively evaluated on
feed-forward and convolutional networks (computing
with 53 000 neurons), far exceeding capabilities of current
systems (Section VI).
‚Ä¢ An application of AI2 to evaluate provable robustness of
neural network defenses (Section VII).
II. REPRESENTING NEURAL NETWORKS AS
CONDITIONAL AFFINE TRANSFORMATIONS
In this section, we provide background on feedforward and
convolutional neural networks and show how to transform
them into a representation amenable to abstract interpretation.
This representation helps us simplify the construction and
description of our analyzer, which we discuss in later sections.
We use the following notation: for a vector x ‚ààRn, xi denotes
its ith entry, and for a matrix W ‚ààRn√óm, Wi denotes its ith
row and Wi,j denotes the entry in its ith row and jth column.
case E1 : f1(x), . . . , case Ek : fk(x)
E ‚àßE | xi ‚â•xj | xi ‚â•0 | xi < 0
Fig. 3: DeÔ¨Ånition of CAT functions.
CAT Functions.
We express the neural network as a composition of conditional afÔ¨Åne transformations (CAT), which
are afÔ¨Åne transformations guarded by logical constraints. The
class of CAT functions, shown in Fig. 3, consists of functions
f : Rm ‚ÜíRn for m, n ‚ààN and is deÔ¨Åned recursively. Any
afÔ¨Åne transformation f(x) = W ¬∑ x + b is a CAT function,
for a matrix W and a vector b. Given sequences of conditions
E1, . . . , Ek and CAT functions f1, . . . , fk, we write:
f(x) = case E1 : f1(x), . . . , case Ek : fk(x).
This is also a CAT function, which returnsx fi(x) for the
Ô¨Årst Ei satisÔ¨Åed by x. The conditions are conjunctions of
constraints of the form xi ‚â•xj, xi ‚â•0 and xi < 0. Finally,
any composition of CAT functions is a CAT function. We often
write f ‚Ä≤‚Ä≤ ‚ó¶f ‚Ä≤ to denote the CAT function f(x) = f ‚Ä≤‚Ä≤(f ‚Ä≤(x)).
Layers. Neural networks are often organized as a sequence
of layers, such that the output of one layer is the input of the
next layer. Layers consist of neurons, performing the same
function but with different parameters. The output of a layer
is formed by stacking the outputs of the neurons into a vector
or three-dimensional array. We will deÔ¨Åne the functionality in
terms of entire layers instead of in terms of individual neurons.
Reshaping of Inputs.
Layers often take three-dimensional
inputs (e.g., colored images). Such inputs are transformed into
vectors by reshaping. A three-dimensional array x ‚ààRm√ón√ór
can be reshaped to xv ‚ààRm¬∑n¬∑r in a canonical way, Ô¨Årst by
depth, then by column, Ô¨Ånally by row. That is, given x:
xv = (x1,1,1 . . . x1,1,r x1,2,1 . . . x1,2,r . . . xm,n,1 . . . xm,n,r)T .
Activation Function.
Typically, layers in a neural network
perform a linear transformation followed by a non-linear
activation function. We focus on the commonly used rectiÔ¨Åed
linear unit (ReLU) activation function, which for x ‚ààR is
deÔ¨Åned as ReLU(x) = max(0, x), and for a vector x ‚ààRm
as ReLU(x)=(ReLU(x1), . . . , ReLU(xm)).
ReLU to CAT. We can express the ReLU activation function
as ReLU = ReLUn ‚ó¶. . . ‚ó¶ReLU1 where ReLUi processes the
ith entry of the input x and is given by:
ReLUi(x) = case (xi ‚â•0): x,
case (xi < 0): Ii‚Üê0 ¬∑ x.
Ii‚Üê0 is the identity matrix with the ith row replaced by zeros.
Fully Connected (FC) Layer.
An FC layer takes a vector
of size m (the m outputs of the previous layer), and passes
it to n neurons, each computing a function based on the
neuron‚Äôs weights and bias, one weight for each component
of the input. Formally, an FC layer with n neurons is a
function FCW,b : Rm ‚ÜíRn parameterized by a weight matrix
W ‚ààRn√óm and a bias b ‚ààRn. For x ‚ààRm, we have:
FCW,b(x) = ReLU(W ¬∑ x + b).
Fig. 4a shows an FC layer computation for x = (2, 3, 1).
Convolutional Layer.
A convolutional layer is deÔ¨Åned by
a series of t Ô¨Ålters F p,q = (F p,q
, .., F p,q
), parameterized by
the same p and q, where p ‚â§m and q ‚â§n. A Ô¨Ålter F p,q
is a function parameterized by a three-dimensional array of
weights W ‚ààRp√óq√ór and a bias b ‚ààR. A Ô¨Ålter takes a
three-dimensional array and returns a two-dimensional array:
: Rm√ón√ór ‚ÜíR(m‚àíp+1)√ó(n‚àíq+1).
The entries of the output y for a given input x are given by:
yi,j = ReLU(
Wi‚Ä≤,j‚Ä≤,k‚Ä≤ ¬∑x(i+i‚Ä≤‚àí1),(j+j‚Ä≤‚àí1),k‚Ä≤ +b).
Intuitively, this matrix is computed by sliding the Ô¨Ålter along
the height and width of the input three-dimensional array, each
time reading a slice of size p√óq√ór, computing its dot product
with W (resulting in a real number), adding b, and applying
ReLU. The function ConvF , corresponding to a convolutional
layer with t Ô¨Ålters, has the following type:
ConvF : Rm√ón√ór ‚ÜíR(m‚àíp+1)√ó(n‚àíq+1)√ót.
As expected, the function ConvF returns a three-dimensional
array of depth t, which stacks the outputs produced by each
Ô¨Ålter. Fig. 4b illustrates a computation of a convolutional layer
with a single Ô¨Ålter. For example:
y1,1,1 = ReLU((1 ¬∑ 0 + 0 ¬∑ 4 + (‚àí1) ¬∑ (‚àí1) + 2 ¬∑ 0) + 1) = 2.
Here, the input is a three-dimensional array in R4√ó4√ó1. As
the input depth is 1, the depth of the Ô¨Ålter‚Äôs weights is also 1.
The output depth is 1 because the layer has one Ô¨Ålter.
Convolutional Layer to CAT.
For a convolutional layer
ConvF , we deÔ¨Åne a matrix W F whose entries are those of the
weight matrices for each Ô¨Ålter (replicated to simulate sliding),
and a bias b
F consisting of copies of the Ô¨Ålters‚Äô biases. We
then treat the convolutional layer ConvF like the equivalent
F . We provide formal deÔ¨Ånitions of W F and b
Appendix A. Here, we provide an intuitive illustration of the
translation on the example in Fig. 4b. Consider the Ô¨Årst entry
y1,1 = 2 of y in Fig. 4b:
y1,1=ReLU(W1,1¬∑x1,1+W1,2¬∑x1,2+W2,1¬∑x2,1+W2,2¬∑x2,2+b).
When x is reshaped to a vector xv, the four entries
x1,1, x1,2, x2,1 and x2,2 will be found in xv
respectively. Similarly, when y is reshaped to yv, the entry
y1,1 will be found in yv
1. Thus, to obtain yv
1 = y1,1, we deÔ¨Åne
the Ô¨Årst row in W F such that its 1st, 2nd, 5th, and 6th entries
are W1,1, W1,2, W2,1 and W2,2. The other entries are zeros.
+ -1 ) = 3
ReLU( 1 -1 0
(a) Fully connected layer FCW,b
(b) Convolutional layer Conv(W,b) (one Ô¨Ålter)
2 -4 ) = 2
max( 3 -2 0
max( 2 -3 -1 5 ) = 5
(c) Max pooling layer MaxPool2,2
Fig. 4: One example computation for each of the three layer types supported by AI2.
We also deÔ¨Åne the Ô¨Årst entry of the bias to be b. For similar
reasons, to obtain yv
2 = y1,2, we deÔ¨Åne the second row in W F
such that its 2nd, 3rd, 6th, and 7th entries are W1,1, W1,2, W2,1
and W2,2 (also b2 = b). By following this transformation, we
obtain the matrix W F ‚ààR9 √ó R16 and the bias b
1 0 0 0 ‚àí1
To aid understanding, we show the entries from W that appear
in the resulting matrix W F in bold.
Max Pooling (MP) Layer.
An MP layer takes a threedimensional array x ‚ààRm√ón√ór and reduces the height m of
x by a factor of p and the width n of x by a factor of q (for p
and q dividing m and n). Depth is kept as-is. Neurons take as
input disjoint subrectangles of x of size p √ó q and return the
maximal value in their subrectangle. Formally, the MP layer
is a function MaxPoolp,q : Rm√ón√ór ‚ÜíR
q √ór that for an
input x returns the three-dimensional array y given by:
yi,j,k = max({xi‚Ä≤,j‚Ä≤,k |
p ¬∑ (i ‚àí1) < i‚Ä≤ ‚â§p ¬∑ i
q ¬∑ (j ‚àí1) < j‚Ä≤ ‚â§q ¬∑ j}).
Fig. 4c illustrates the max pooling computation for p = 2,
q = 2 and r = 1. For example, here we have:
y1,1,1 = max({x1,1,1, x1,2,1, x2,1,1, x2,2,1}) = 2.
Max Pooling to CAT. Let MaxPool‚Ä≤
p,q : Rm¬∑n¬∑r ‚ÜíR
be the function that is obtained from MaxPoolp,q by reshaping
its input and output: MaxPool‚Ä≤
p,q(xv) = MaxPoolp,q(x)v. To
represent max pooling as a CAT function, we deÔ¨Åne a series
of CAT functions whose composition is MaxPool‚Ä≤
q ¬∑r ‚ó¶. . . ‚ó¶f1 ‚ó¶f MP.
Fig. 5: The operation of the transformed max pooling layer.
The Ô¨Årst function is f MP(xv) = W MP ¬∑ xv, which reorders
its input vector xv to a vector xMP in which the values
of each max pooling subrectangle of x are adjacent. The
remaining functions execute standard max pooling. Concretely,
the function fi ‚àà{f1, . . . , f m
q ¬∑r} executes max pooling
on the ith subrectangle by selecting the maximal value and
removing the other values. We provide formal deÔ¨Ånitions of
the CAT functions f MP and fi in Appendix A. Here, we
illustrate them on the example from Fig. 4c, where r = 1.
The CAT computation for this example is shown in Fig. 5.
The computation begins from the input vector xv, which is
the reshaping of x from Fig. 4c. The values of the Ô¨Årst 2 √ó 2
subrectangle in x (namely, 0, 1, 2 and ‚àí4) are separated in
xv by values from another subrectangle (3 and ‚àí2). To make
them contiguous, we reorder xv using a permutation matrix
W MP, yielding xMP. In our example, W MP is:
1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
One entry in each row of W MP is 1, all other entries are zeros.
If row i has entry j set to 1, then the jth value of xv is moved
to the ith entry of xMP. For example, we placed a one in the
Ô¨Åfth column of the third row of W MP to move the value xv
to entry 3 of the output vector.
Next, for each i ‚àà{1, . . . , m
q }, the function fi takes
as input a vector whose values at the indices between i and
i + p ¬∑ q ‚àí1 are those of the ith subrectangle of ¬Øx in Fig. 4c.
It then replaces those p ¬∑ q values by their maximum:
fi(x) = (x1, . . ., xi‚àí1, xk, xi+p¬∑q, . . . , xm¬∑n‚àí(p¬∑q‚àí1)¬∑(i‚àí1)),
where the index k ‚àà{i, . . . , i + p ¬∑ q ‚àí1} is such that
xk is maximal. For k given, fi can be written as a CAT
function: fi(x) = W (i,k) ¬∑ x, where the rows of the matrix
W (i,k) ‚ààR(m¬∑n‚àí(p¬∑q‚àí1)¬∑i)√ó(m¬∑n‚àí(p¬∑q‚àí1)¬∑(i‚àí1)) are given by
the following sequence of standard basis vectors:
e1, . . . , ei‚àí1, ek, ei+p¬∑q, . . . , em¬∑n‚àí(p¬∑q‚àí1)¬∑(i‚àí1).
For example, in Fig. 5, f1(xMP) = W (1,3) ¬∑ xMP deletes 0, 1
and ‚àí4. Then it moves the value 2 to the Ô¨Årst component,
and the values at indices 5, . . . , 16 to components 2, . . . , 13.
Overall, W (1,3) is given by:
0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
As, in general, k is not known in advance, we need to write
fi as a CAT function with a different case for each possible
index k of the maximal value in x. For example, in Fig. 5:
case (x1 ‚â•x2) ‚àß(x1 ‚â•x3) ‚àß(x1 ‚â•x4): W (1,1) ¬∑ x,
case (x2 ‚â•x1) ‚àß(x2 ‚â•x3) ‚àß(x2 ‚â•x4): W (1,2) ¬∑ x,
case (x3 ‚â•x1) ‚àß(x3 ‚â•x2) ‚àß(x3 ‚â•x4): W (1,3) ¬∑ x,
case (x4 ‚â•x1) ‚àß(x4 ‚â•x2) ‚àß(x4 ‚â•x3): W (1,4) ¬∑ x.
In our example, the vector xMP in Fig. 5 satisÔ¨Åes the third
condition, and therefore f1(xMP) = W (1,3) ¬∑ xMP. Taking into
account all four subrectangles, we obtain:
2,2 = f4 ‚ó¶f3 ‚ó¶f2 ‚ó¶f1 ‚ó¶f MP.
In summary, each function fi replaces p¬∑q components of their
input by the maximum value among them, suitably moving
other values. For xv in Fig. 5:
2,2(xv) = W (4,7) ¬∑W (3,6) ¬∑W (2,2) ¬∑W (1,3) ¬∑W MP ¬∑xv.
Network Architectures. Two popular architectures of neural
networks are fully connected feedforward (FNN) and convolutional (CNN). An FNN is a sequence of fully connected
layers, while a CNN consists of all previously described
layer types: convolutional, max pooling, and fully connected.
Fig. 6: (a) Abstracting four points with a polyhedron (gray),
zonotope (green), and box (blue). (b) The points and abstractions resulting from the afÔ¨Åne transformer.
III. BACKGROUND: ABSTRACT INTERPRETATION
We now provide a short introduction to Abstract Interpretation (AI). AI enables one to prove program properties on a
set of inputs without actually running the program. Formally,
given a function f : Rm ‚ÜíRn, a set of inputs X ‚äÜRm,
and a property C ‚äÜRn, the goal is to determine whether the
property holds, that is, whether ‚àÄx ‚ààX. f(x) ‚ààC.
Fig. 6 shows a CAT function f : R2 ‚ÜíR2 that is deÔ¨Åned
¬∑ x and four input points for the function f,
given as X = {(0, 1), (1, 1), (1, 3), (2, 2)}. Let the property be
C = {(y1, y2) ‚ààR2 | y1 ‚â•‚àí2}, which holds in this example.
To reason about all inputs simultaneously, we lift the deÔ¨Ånition
of f to be over a set of inputs X rather than a single input:
Tf : P(Rm) ‚ÜíP(Rn),
Tf(X) = {f(x) | x ‚ààX}.
The function Tf is called the concrete transformer of f.
With Tf, our goal is to determine whether Tf(X) ‚äÜC for
a given input set X. Because the set X can be very large
(or inÔ¨Ånite), we cannot enumerate all points in X to compute Tf(X). Instead, AI overapproximates sets with abstract
elements (drawn from some abstract domain A) and then
deÔ¨Ånes a function, called an abstract transformer of f, which
works with these abstract elements and overapproximates the
effect of Tf. Then, the property C can be checked on the
resulting abstract element returned by the abstract transformer.
Naturally, because AI employs overapproximation, it is sound,
but may be imprecise (i.e., may fail to prove the property when
it holds). Next, we explain the ingredients of AI in more detail.
Abstract Domains.
Abstract domains consist of shapes
expressible as a set of logical constraints. A few popular
numerical abstract domains are: Box (i.e., Interval), Zonotope,
and Polyhedra. These domains provide different precision
versus scalability trade-offs (e.g., Box‚Äôs abstract transformers
are signiÔ¨Åcantly faster than Polyhedra‚Äôs abstract transformers,
but polyhedra are signiÔ¨Åcantly more precise than boxes). The
Box domain consists of boxes, captured by a set of constraints
of the form a ‚â§xi ‚â§b, for a, b ‚ààR‚à™{‚àí‚àû, +‚àû} and a ‚â§b.
A box B contains all points which satisfy all constraints in B.
In our example, X can be abstracted by the following box:
B = {0 ‚â§x1 ‚â§2, 1 ‚â§x2 ‚â§3}.
Note that B is not very precise since it includes 9 integer
points (along with other points), whereas X has only 4 points.
The Zonotope domain consists of zonotopes. A zonotope is a center-symmetric convex closed polyhedron Z ‚äÜRn
that can be represented as an afÔ¨Åne function:
z : [a1, b1] √ó [a2, b2] √ó ¬∑ ¬∑ ¬∑ √ó [am, bm] ‚ÜíRn.
In other words, z has the form z(œµ) = M ¬∑ œµ + b where œµ is a
vector of error terms satisfying interval constraints œµi ‚àà[ai, bi]
for 1 ‚â§i ‚â§m. The bias vector b captures the center of the
zonotope, while the matrix M captures the boundaries of the
zonotope around b. A zonotope z represents all vectors in the
image of z (i.e., z[[a1,1 ] √ó ¬∑ ¬∑ ¬∑ √ó [am, bm]]). In our example,
X can be abstracted by the zonotope z : [‚àí1, 1]3 ‚ÜíR2:
z(œµ1, œµ2, œµ3) = (1 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ2, 2 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ3).
Zonotope is a more precise domain than Box: for our example,
z includes only 7 integer points.
The Polyhedra domain consists of convex closed polyhedra, where a polyhedron is captured by a set of linear
constraints of the form A ¬∑ x ‚â§b, for some matrix A and a
vector b. A polyhedron C contains all points which satisfy the
constraints in C. In our example, X can be abstracted by the
following polyhedron:
C = {x2 ‚â§2 ¬∑ x1 + 1, x2 ‚â§4 ‚àíx1, x2 ‚â•1, x2 ‚â•x1}.
Polyhedra is a more precise domain than Zonotope: for our
example, C includes only 5 integer points.
To conclude, abstract elements (from an abstract domain)
represent large, potentially inÔ¨Ånite sets. There are various
abstract domains, providing different levels of precision and
scalability.
Abstract Transformers. To compute the effect of a function
on an abstract element, AI uses the concept of an abstract transformer. Given the (lifted) concrete transformer
Tf : P(Rm) ‚ÜíP(Rn) of a function f : Rm ‚ÜíRn, an abstract
transformer of Tf is a function over abstract domains, denoted
f : Am ‚ÜíAn. The superscripts denote the number of
components of the represented vectors. For example, elements
in Am represent sets of vectors of dimension m. This also
determines which variables can appear in the constraints
associated with an abstract element. For example, elements
in Am constrain the values of the variables x1, . . . , xm.
Abstract transformers have to be sound. To deÔ¨Åne soundness, we introduce two functions: the abstraction function Œ±
and the concretization function Œ≥. An abstraction function
Œ±m : P(Rm) ‚ÜíAm maps a set of vectors to an abstract
element in Am that overapproximates it. For example, in the
Box domain:
Œ±2({(0, 1), (1, 1), (1, 3), (2, 2)}) = {0 ‚â§x1 ‚â§2, 1 ‚â§x2 ‚â§3}.
A concretization function Œ≥m : Am
‚ÜíP(Rm) does the
opposite: it maps an abstract element to the set of concrete
vectors that it represents. For example, for Box:
Œ≥2({0 ‚â§x1 ‚â§2, 1 ‚â§x2 ‚â§3}) = {(0, 1), (0, 2), (0, 3),
(1, 1), (1, 2), (1, 3),
(2, 1), (2, 2), (2, 3), . . .}.
This only shows the 9 vectors with integer components. We
can now deÔ¨Åne soundness. An abstract transformer T #
sound if for all a ‚ààAm, we have Tf(Œ≥m(a)) ‚äÜŒ≥n(T #
where Tf is the concrete transformer. That is, an abstract
transformer has to overapproximate the effect of a concrete
transformer. For example, the transformers illustrated in Fig. 6
are sound. For instance, if we apply the Box transformer on the
box in Fig. 6a, it will produce the box in Fig. 6b. The box in
Fig. 6b includes all points that f could compute in principle
when given any point included in the concretization of the
box in Fig. 6a. Analogous properties hold for the Zonotope
and Polyhedra transformers. It is also important that abstract
transformers are precise. That is, the abstract output should
include as few points as possible. For example, as we can
see in Fig. 6b, the output produced by the Box transformer
is less precise (it is larger) than the output produced by the
Zonotope transformer, which in turn is less precise than the
output produced by the Polyhedra transformer.
Property VeriÔ¨Åcation. After obtaining the (abstract) output,
we can check various properties of interest on the result. In
general, an abstract output a = T #
f (X) proves a property
Tf(X) ‚äÜC if Œ≥n(a) ‚äÜC. If the abstract output proves a
property, we know that the property holds for all possible
concrete values. However, the property may hold even if it
cannot be proven with a given abstract domain. For example, in
Fig. 6b, for all concrete points, the property C = {(y1, y2) ‚àà
R2 | y1 ‚â•‚àí2} holds. However, with the Box domain, the
abstract output violates C, which means that the Box domain
is not precise enough to prove the property. In contrast, the
Zonotope and Polyhedra domains are precise enough to prove
the property.
In summary, to apply AI successfully, we need to: (a) Ô¨Ånd a
suitable abstract domain, and (b) deÔ¨Åne abstract transformers
that are sound and as precise as possible. In the next section,
we introduce abstract transformers for neural networks that
are parameterized by the numerical abstract domain. This
means that we can explore the precision-scalability trade-off
by plugging in different abstract domains.
IV. AI2: AI FOR NEURAL NETWORKS
In this section we present AI2, an abstract interpretation
framework for sound analysis of neural networks. We begin by
deÔ¨Åning abstract transformers for the different kinds of neural
network layers. Using these transformers, we then show how
to prove robustness properties of neural networks.
A. Abstract Interpretation for CAT Functions
In this section, we show how to overapproximate CAT
functions with AI. We illustrate the method on the example in
z1‚äì(x1 ‚â•0)
z1‚äì(x1 <0)
Fig. 7: Illustration of how AI2 overapproximates neural network states. Blue circles show the concrete values, while green
zonotopes show the abstract elements. The gray box shows the steps in one application of the ReLU transformer (ReLU1).
Fig. 7, which shows a simple network that manipulates twodimensional vectors using a single fully connected layer of the
form f(x) = ReLU2
. Recall that
case (xi ‚â•0): x,
case (xi < 0): Ii‚Üê0 ¬∑ x,
where Ii‚Üê0 is the identity matrix with the ith row replaced
by the zero vector. We overapproximate the network behavior
on an abstract input. The input can be obtained directly (see
Sec. IV-B) or by abstracting a set of concrete inputs to an
abstract element (using the abstraction function Œ±). For our
example, we use the concrete inputs (the blue points) from
Fig 6. Those concrete inputs are abstracted to the green
zonotope z0 : [‚àí1, 1]3 ‚ÜíR2, given as:
z0(œµ1, œµ2, œµ3) = (1 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ2, 2 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ3).
Due to abstraction, more (spurious) points may be added. In
this example, except the blue points, the entire area of the
zonotope is spurious. We then apply abstract transformers
to the abstract input. Note that, if a function f can be
written as f = f ‚Ä≤‚Ä≤ ‚ó¶f ‚Ä≤, the concrete transformer for f is
Tf = Tf ‚Ä≤‚Ä≤ ‚ó¶Tf ‚Ä≤. Similarly, given abstract transformers T #
f ‚Ä≤‚Ä≤, an abstract transformer for f is T #
f ‚Ä≤ . When
a neural network N = f ‚Ä≤
‚Ñì‚ó¶¬∑ ¬∑ ¬∑ ‚ó¶f ‚Ä≤
1 is a composition of
multiple CAT functions f ‚Ä≤
i of the shape f ‚Ä≤
i(x) = W ¬∑ x + b or
fi(x) = case E1 : f1(x), . . . , case Ek : fk(x), we only have
to deÔ¨Åne abstract transformers for these two kinds of functions.
We then obtain the abstract transformer T #
‚Ñì‚ó¶¬∑ ¬∑ ¬∑ ‚ó¶T #
Abstracting AfÔ¨Åne Functions. To abstract functions of the
form f(x) = W ¬∑ x + b, we assume that the underlying abstract domain supports the operator Aff that overapproximates
such functions. We note that for Zonotope and Polyhedra,
this operation is supported and exact. Fig. 7 demonstrates
Aff as the Ô¨Årst step taken for overapproximating the effect
of the fully connected layer. Here, the resulting zonotope
z1 : [‚àí1, 1]3 ‚ÜíR2 is:
z1(œµ1, œµ2, œµ3) =
(2 ¬∑ (1 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ2) ‚àí(2 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ3),
2 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ3) =
(0.5 ¬∑ œµ1 + œµ2 ‚àí0.5 ¬∑ œµ3, 2 + 0.5 ¬∑ œµ1 + 0.5 ¬∑ œµ3).
Abstracting Case Functions.
To abstract functions of the
form f(x) = case E1 : f1(x), . . . , case Ek : fk(x), we Ô¨Årst
split the abstract element a into the different cases (each
deÔ¨Åned by one of the expressions Ei), resulting in k abstract
elements a1, . . . , ak. We then compute the result of T #
for each ai. Finally, we unify the results to a single abstract
element. To split and unify, we assume two standard operators
for abstract domains: (1) meet with a conjunction of linear
constraints and (2) join. The meet (‚äì) operator is an abstract
transformer for set intersection: for an inequality expression
E from Fig. 3, Œ≥n(a) ‚à©{x ‚ààRn | x |= E} ‚äÜŒ≥n(a ‚äìE).
The join (‚äî) operator is an abstract transformer for set union:
Œ≥n(a1) ‚à™Œ≥n(a2) ‚äÜŒ≥n(a1 ‚äîa2). We further assume that
the abstract domain contains an element ‚ä•, which satisÔ¨Åes
Œ≥n(‚ä•) = {}, ‚ä•‚äìE = ‚ä•and a ‚äî‚ä•= a for a ‚ààA.
For our example in Fig. 7, abstract interpretation continues
on z1 using the meet and join operators. To compute the effect
of ReLU1, z1 is split into two zonotopes z2 = z1 ‚äì(x1 ‚â•0)
and z3 = z1 ‚äì(x1 < 0). One way to compute a meet between
a zonotope and a linear constraint is to modify the intervals
of the error terms (see ). In our example, the resulting
zonotopes are z2 : [‚àí1, 1] √ó √ó [‚àí1, 1] ‚ÜíR2 such that
z2(œµ) = z1(œµ) and z3 : [‚àí1, 1] √ó [‚àí1, 0] √ó [‚àí1, 1] ‚ÜíR2 such
that z3(œµ) = z1(œµ) for ¬Øœµ common to their respective domains.
Note that both z2 and z3 contain small spurious areas, because
the intersections of the respective linear constraints with z1 are
not zonotopes. Therefore, they cannot be captured exactly by
the domain. Here, the meet operator ‚äìoverapproximates set
intersection ‚à©to get a sound, but not perfectly precise, result.
Then, the two cases of ReLU1 are processed separately. We
apply the abstract transformer of f1(x) = x to z2 and we
apply the abstract transformer of f2(x) = I0‚Üê0 ¬∑ x to z3. The
resulting zonotopes are z4 = z2 and z5 : [‚àí1, 1]2 ‚ÜíR2 such
that z5(œµ1, œµ3) = (0, 2+0.5¬∑œµ1+0.5¬∑œµ3). These are then joined
to obtain a single zonotope z6. Since z5 is contained in z4,
we get z6 = z4 (of course, this need not always be the case).
Then, z6 is passed to ReLU2. Because z6‚äì(x1 < 0) = ‚ä•, this
results in z7 = z6. Finally, Œ≥2(z7) is our overapproximation of
the network outputs for our initial set of points. The abstract
element z7 is a Ô¨Ånite representation of this inÔ¨Ånite set.
For f(x) = W ¬∑ x + b, T #
f (a) = Aff(a, W, b).
For f(x) = case E1 : f1(x), . . . , case Ek : fk(y),
For f(x) = f2(f1(x)), T #
f (a) = T #
Fig. 8: Abstract transformers for CAT functions.
In summary, we deÔ¨Åne abstract transformers for every kind
of CAT function (summarized in Fig. 8). These deÔ¨Ånitions
are general and are compatible with any abstract domain A
which has a minimum element ‚ä•and supports (1) a meet
operator between an abstract element and a conjunction of
linear constraints, (2) a join operator between two abstract
elements, and (3) an afÔ¨Åne transformer. We assume that
the operations are sound. We note that these operations are
standard or deÔ¨Ånable with standard operations. By deÔ¨Ånition
of the abstract transformers, we get soundness:
Theorem 1. For any CAT function f with transformer
Tf : P(Rm) ‚ÜíP(Rn) and any abstract input a ‚ààAm,
Tf(Œ≥m(a)) ‚äÜŒ≥n(T #
Theorem 1 is the key to sound neural network analysis with
our abstract transformers, as we explain in the next section.
B. Neural Network Analysis with AI
In this section, we explain how to leverage AI with our abstract transformers to prove properties of neural networks. We
focus on robustness properties below, however, the framework
can be applied to reason about any safety property.
For robustness, we aim to determine if for a (possibly unbounded) set of inputs, the outputs of a neural network satisfy
a given condition. A robustness property for a neural network
N : Rm ‚ÜíRn is a pair (X, C) ‚ààP(Rm)√óP(Rn) consisting
of a robustness region X and a robustness condition C. We
say that the neural network N satisÔ¨Åes a robustness property
(X, C) if N(x) ‚ààC for all x ‚ààX.
Local Robustness.
This is a property (X, CL) where X is
a robustness region and CL contains the outputs that describe
the same label L:
 arg max
i‚àà{1,...,n}
For example, Fig. 7 shows a neural network and a robustness
property (X, C2) for X = {(0, 1), (1, 1), (1, 3), (2, 2)} and
C2 = {y | arg max(y1, y2) = 2}. In this example, (X, C2)
holds. Typically, we will want to check that there is some
label L for which (X, CL) holds.
We now explain how our abstract transformers can be used
to prove a given robustness property (X, C).
Robustness Proofs using AI. Assume we are given a neural
network N : Rm ‚ÜíRn, a robustness property (X, C) and
an abstract domain A (supporting ‚äî, ‚äìwith a conjunction of
linear constraints, Aff, and ‚ä•) with an abstraction function Œ±
and a concretization function Œ≥. Further assume that N can
be written as a CAT function. Denote by T #
N the abstract
transformer of N, as deÔ¨Åned in Fig. 8. Then, the following
condition is sufÔ¨Åcient to prove that N satisÔ¨Åes (X, C):
N (Œ±m(X))) ‚äÜC.
This follows from Theorem 1 and the properties of Œ± and Œ≥.
Note that there may be abstract domains A that are not precise
enough to prove that N satisÔ¨Åes (X, C), even if N in fact
satisÔ¨Åes (X, C). On the other hand, if we are able to show
that some abstract domain A proves that N satisÔ¨Åes (X, C),
we know that it holds.
Proving Containment. To prove the property (X, C) given
the result a = T #
N (Œ±m(X)) of abstract interpretation, we
need to be able to show Œ≥n(a) ‚äÜC. There is a general
method if C is given by a CNF formula 
j li,j where
all literals li,j are linear constraints: we show that the negated
j ¬¨li,j is inconsistent with the abstract element
a by checking that a ‚äì
= ‚ä•for all i.
For our example in Fig. 7, the goal is to check that all inputs
are classiÔ¨Åed as 2. We can represent C using the formula
y2 ‚â•y1. Its negation is y2 < y1, and it sufÔ¨Åces to show that
no point in the concretization of the abstract output satisÔ¨Åes
this negated constraint. As indeed z7 ‚äì(y2 < y1) = ‚ä•, the
property is successfully veriÔ¨Åed. However, note that we would
not be able to prove some other true properties, such as y1 ‚â•0.
This property holds for all concrete outputs, but some points
in the concretization of the output z7 do not satisfy it.
V. IMPLEMENTATION OF AI2
The AI2 framework is implemented in the D programming
language and supports any neural network composed of fully
connected, convolutional, and max pooling layers.
Properties. AI2 supports properties (X, C) where X is speci-
Ô¨Åed by a zonotope and C by a conjunction of linear constraints
over the output vector‚Äôs components. In our experiments, we
illustrate the speciÔ¨Åcation of local robustness properties where
the region X is deÔ¨Åned by a box or a line, both of which are
precisely captured by a zonotope.
Abstract Domains. The AI2 system is fully integrated with
all abstract domains supported by Apron , a popular
library for numerical abstract domains, including: Box ,
Zonotope , and Polyhedra .
Bounded Powerset. We also implemented bounded powerset
domains (disjunctive abstractions , ), which can be
instantiated with any of the above abstract domains. An abstract element in the powerset domain P(A) of an underlying
abstract domain A is a set of abstract elements from A, concretizing to the union of the concretizations of the individual
elements (i.e., Œ≥(A) = 
a‚ààA Œ≥(a) for A ‚ààP(A)).
The powerset domain can implement a precise join operator
using standard set union (potentially pruning redundant elements). However, since the increased precision can become
prohibitively costly if many join operations are performed,
the bounded powerset domain limits the number of abstract
elements in a set to N (for some constant N).
We implemented bounded powerset domains on top of standard powerset domains using a greedy heuristic that repeatedly
replaces two abstract elements in a set by their join, until the
number of abstract elements in the set is below the bound N.
For joining, AI2 heuristically selects two abstract elements that
minimize the distance between the centers of their bounding
boxes. In our experiments, we denote by ZonotopeN or
ZN the bounded powerset domain with bound N ‚â•2 and
underlying abstract domain Zonotope.
VI. EVALUATION OF AI2
In this section, we present our empirical evaluation of AI2.
Before discussing the results in detail, we summarize our three
most important Ô¨Åndings:
‚Ä¢ AI2 can prove useful robustness properties for convolutional networks with 53 000 neurons and large fully
connected feedforward networks with 1 800 neurons.
‚Ä¢ AI2 beneÔ¨Åts from more precise abstract domains: Zonotope enables AI2 to prove substantially more properties
over Box. Further, ZonotopeN, with N ‚â•2, can prove
stronger robustness properties than Zonotope alone.
‚Ä¢ AI2 scales better than the SMT-based Reluplex : AI2
is able to verify robustness properties on large networks
with ‚â•1200 neurons within few minutes, while Reluplex
takes hours to verify the same properties.
In the following, we Ô¨Årst describe our experimental setup.
Then, we present and discuss our results.
A. Experimental Setup
We now describe the datasets, neural networks, and robustness properties used in our experiments.
We used two popular datasets: MNIST and
CIFAR-10 (referred to as CIFAR from now on). MNIST
consists of 60 000 grayscale images of handwritten digits,
whose resolution is 28 √ó 28 pixels. The images show white
digits on a black background.
CIFAR consists of 60 000 colored photographs with 3 color
channels, whose resolution is 32 √ó 32 pixels. The images are
partitioned into 10 different classes (e.g., airplane or bird).
Each photograph has a different background (unlike MNIST).
Neural Networks.
We trained convolutional and fully connected feedforward networks on both datasets. All networks
were trained using accelerated gradient descent with at least
50 epochs of batch size 128. The training completed when
each network had a test set accuracy of at least 0.9.
For the convolutional networks, we used the LeNet architecture , which consists of the following sequence
of layers: 2 convolutional, 1 max pooling, 2 convolutional, 1 max pooling, and 3 fully connected layers. We
write np√óq to denote a convolutional layer with n Ô¨Ålters
of size p √ó q, and m to denote a fully connected layer
with m neurons. The hidden layers of the MNIST network are 83√ó3, 83√ó3, 143√ó3, 143√ó3, 50, 50, 10, and those of the
CIFAR network are 243√ó3, 243√ó3, 323√ó3, 323√ó3, 100, 100, 10.
The max pooling layers of both networks have a size of 2√ó2.
We trained our networks using an open-source implementation .
We used 7 different architectures of fully connected feedforward networks (FNNs). We write l √ó n to denote the FNN
architecture with l layers, each consisting of n neurons. Note
that this determines the network‚Äôs size; e.g., a 4 √ó 50 network
has 200 neurons. For each dataset, MNIST and CIFAR, we
trained FNNs with the following architectures: 3√ó20, 6√ó20,
3 √ó 50, 3 √ó 100, 6 √ó 100, 6 √ó 200, and 9 √ó 200.
Robustness Properties.
In our experiments, we consider
local robustness properties (X, CL) where the region X captures changes to lighting conditions. This type of property is
inspired by the work of , where adversarial examples were
found by brightening the pixels of an image.
Formally, we consider robustness regions Sx,Œ¥ that are
parameterized by an input x ‚ààRm and a robustness bound
Œ¥ ‚àà . The robustness region is deÔ¨Åned as:
Sx,Œ¥ = {x‚Ä≤ ‚ààRm | ‚àÄi ‚àà[1, m]. 1‚àíŒ¥ ‚â§xi ‚â§x‚Ä≤
For example, the robustness region for x = (0.6, 0.85, 0.9)
and bound Œ¥ = 0.2 is given by the set:
{(0.6, x, x‚Ä≤) ‚ààR3 | x ‚àà[0.85, 1], x‚Ä≤ ‚àà[0.9, 1]}.
Note that increasing the bound Œ¥ increases the region‚Äôs size.
In our experiments, we used AI2 to check whether all inputs
in a given region Sx,Œ¥ are classiÔ¨Åed to the label assigned to x.
We consider 6 different robustness bounds Œ¥, which are drawn
from the set Œî = {0.001, 0.005, 0.025, 0.045, 0.065, 0.085}.
We remark that our robustness properties are stronger than
those considered in . This is because, in a given robustness
region Sx,Œ¥, each pixel of the image x is brightened independently of the other pixels. We remark that this is useful to
capture scenarios where only part of the image is brightened
(e.g., due to shadowing).
Other perturbations.
Note that AI2 is not limited to certifying robustness against such brightening perturbations. In
general, AI2 can be used whenever the set of perturbed
inputs can be overapproximated with a set of zonotopes in
a precise way (i.e., without adding too many inputs that do
not capture actual perturbations to the robustness region). For
example, the inputs perturbed by an ‚Ñì‚àûattack are captured
exactly by a single zonotope. Further, rotations and translations
have low-dimensional parameter spaces, and therefore can be
overapproximated by a set of zonotopes in a precise way.
Benchmarks.
We selected 10 images from each dataset.
Then, we speciÔ¨Åed a robustness property for each image and
each robustness bound in Œî, resulting in 60 properties per
dataset. We ran AI2 to check whether each neural network
satisÔ¨Åes the robustness properties for the respective dataset.
We compared the results using different abstract domains,
VeriÔ¨Åed robustness
VeriÔ¨Åed robustness
Fig. 9: VeriÔ¨Åed properties by AI2 on the MNIST and CIFAR convolutional networks for each bound Œ¥ ‚ààŒî (x-axis).
including Box, Zonotope, and ZonotopeN with N ranging
between 2 and 128.
We ran all experiments on an Ubuntu 16.04.3 LTS server
with two Intel Xeon E5-2690 processors and 512GB of
memory. To compare AI2 to existing solutions, we also ran
the FNN benchmarks with Reluplex . We did not run
convolutional benchmarks with Reluplex as it currently does
not support convolutional networks.
B. Discussion of Results
In the following, we Ô¨Årst present our results for convolutional networks. Then, we present experiments with different
abstract domains and discuss how the domain‚Äôs precision
affects AI2‚Äôs ability to verify robustness. We also plot AI2‚Äôs
running times for different abstract domains to investigate
the trade-off between precision and scalability. Finally, we
compare AI2 to Reluplex.
Proving Robustness of Convolutional Networks. We start
with our results for convolutional networks. AI2 terminated
within 1.5 minutes when verifying properties on the MNIST
network and within 1 hour when verifying the CIFAR network.
In Fig. 9, we show the fraction of robustness properties
veriÔ¨Åed by AI2 for each robustness bound. We plot separate
bars for Box and Zonotope to illustrate the effect of the
domain‚Äôs precision on AI2‚Äôs ability to verify robustness.
For both networks, AI2 veriÔ¨Åed all robustness properties for
the smallest bound 0.001 and it veriÔ¨Åed at least one property
for the largest bound 0.085. This demonstrates that AI2 can
verify properties of convolutional networks with rather wide
robustness regions. Further, the number of veriÔ¨Åed properties
converges to zero as the robustness bound increases. This
is expected, as larger robustness regions are more likely to
contain adversarial examples.
In Fig. 9a, we observe that Zonotope proves signiÔ¨Åcantly
more properties than Box. For example, Box fails to prove
any robustness properties with bounds at least 0.025, while
Zonotope proves 80% of the properties with bounds 0.025
and 0.045. This indicates that Box is often imprecise and fails
to prove properties that the network satisÔ¨Åes.
Similarly, Fig. 9b shows that Zonotope proves more robustness properties than Box also for the CIFAR convolutional network. The difference between these two domains is, however,
less signiÔ¨Åcant than that observed for the MNIST network. For
example, both Box and Zonotope prove the same properties
for bounds 0.065 and 0.085.
Precision of Different Abstract Domains. Next, we demonstrate that more precise abstract domains enable AI2 to prove
stronger robustness properties. In this experiment, we consider
our 9 √ó 200 MNIST and CIFAR networks, which are our
largest fully connected feedforward networks. We evaluate the
Box, Zonotope, and the ZonotopeN domains for exponentially
increasing bounds of N between 2 and 64. We do not report
results for the Polyhedra domain, which takes several days to
terminate for our smallest networks.
In Fig. 10, we show the fraction of veriÔ¨Åed robustness
properties as a function of the abstract domain used by AI2.
We plot a separate line for each robustness bound. All runs of
AI2 in this experiment completed within 1 hour.
The graphs show that Zonotope proves more robustness
properties than Box. For the MNIST network, Box proves 11
out of all 60 robustness properties (across all 6 bounds), failing
to prove any robustness properties with bounds above 0.005.
In contrast, Zonotope proves 43 out of the 60 properties and
proves at least 50% of the properties across the 6 robustness
bounds. For the CIFAR network, Box proves 25 out of the 60
properties while Zonotope proves 35.
VeriÔ¨Åed robustness
VeriÔ¨Åed robustness
Fig. 10: VeriÔ¨Åed properties as a function of the abstract domain used by AI2 for the 9 √ó 200 network. Each point represents
the fraction of robustness properties for a given bound (as speciÔ¨Åed in the legend) veriÔ¨Åed by a given abstract domain (x-axis).
Time (seconds)
Fig. 11: Average running time of AI2 when proving robustness
properties on MNIST networks as a function of the abstract
domain used by AI2 (x-axis). Axes are scaled logarithmically.
The data also demonstrates that bounded sets of zonotopes
further improve AI2‚Äôs ability to prove robustness properties.
For the MNIST network, Zonotope64 proves more robustness
properties than Zonotope for all 4 bounds for which Zonotope
fails to prove at least one property (i.e., for bounds Œ¥ ‚â•0.025).
For the CIFAR network, Zonotope64 proves more properties
than Zonotope for 4 out of the 5 the bounds. The only
exception is the bound 0.085, where Zonotope64 and Zonotope
prove the same set of properties.
Trade-off between Precision and Scalability.
In Fig. 11,
we plot the running time of AI2 as a function of the abstract
domain. Each point in the graph represents the average running
time of AI2 when proving a robustness property for a given
MNIST network (as indicated in the legend). We use a log-log
plot to better visualize the trade-off in time.
The data shows that AI2 can efÔ¨Åciently verify robustness
of large networks. AI2 terminates within a few minutes for
all MNIST FNNs and all considered domains. Further, we
observe that AI2 takes less than 10 seconds on average to
verify a property with the Zonotope domain.
As expected, the graph demonstrates that more precise
domains increase AI2‚Äôs running time. More importantly, AI2‚Äôs
running time is polynomial in the bound N of ZonotopeN,
which allows one to adjust AI2‚Äôs precision by increasing N.
Comparison to Reluplex. The current state-of-the-art system
for verifying properties of neural networks is Reluplex .
Reluplex supports FNNs with ReLU activation functions, and
its analysis is sound and complete. Reluplex would eventually
either verify a given property or return a counterexample.
To compare the performance of Reluplex and AI2, we ran
both systems on all MNIST FNN benchmarks. We ran AI2
using Zonotope and Zonotope64. For both Reluplex and AI2,
we set a 1 hour timeout for verifying a single property.
Fig. 12 presents our results: Fig. 12a plots the average
running time of Reluplex and AI2 and Fig. 12b shows the
fraction of robustness properties veriÔ¨Åed by the systems. The
data shows that Reluplex can analyze FNNs with at most 600
neurons efÔ¨Åciently, typically within a few minutes. Overall,
both system veriÔ¨Åed roughly the same set of properties.
However, Reluplex crashed during veriÔ¨Åcation of some of the
properties. This explains why AI2 was able to prove slightly
3x100 6x100 6x200 9x200
Time (seconds)
Zonotope64
3x100 6x100 6x200 9x200
VeriÔ¨Åed robustness
Fig. 12: Comparing the performance of AI2 to Reluplex. Each point is an average of the results for all 60 robustness properties
for the MNIST networks. Each point in (a) represents the average time to completion, regardless of the result of the computation.
While not shown, the result of the computation could be a failure to verify, timeout, crash, or discovery of a counterexample.
Each point in (b) represents the fraction of the 60 robustness properties that were veriÔ¨Åed.
more properties than Reluplex on the smaller FNNs.
For large networks with more than 600 neurons, the running
time of Reluplex increases signiÔ¨Åcantly and its analysis often
times out. In contrast, AI2 analyzes the large networks within
a few minutes and veriÔ¨Åes substantially more robustness
properties than Reluplex. For example, Zonotope64 proves
57 out of the 60 properties on the 6 √ó 200 network, while
Reluplex proves 3. Further, Zonotope64 proves 45 out of the
60 properties on the largest 9 √ó 200 network, while Reluplex
proves none. We remark that while Reluplex did not verify
any property on the largest 9 √ó 200 network, it did disprove
some of the properties and returned counterexamples.
We also ran Reluplex without a predeÔ¨Åned timeout to
investigate how long it would take to verify properties on the
large networks. To this end, we ran Reluplex on properties that
AI2 successfully veriÔ¨Åed. We observed that Reluplex often
took more than 24 hours to terminate. Overall, our results
indicate that Reluplex does not scale to larger FNNs whereas
AI2 succeeds on these networks.
VII. COMPARING DEFENSES WITH AI2
In this section, we illustrate a practical application of AI2:
evaluating and comparing neural network defenses. A defense
is an algorithm whose goal is to reduce the effectiveness of
a certain attack against a speciÔ¨Åc network, for example, by
retraining the network with an altered loss function. Since the
discovery of adversarial examples, many works have suggested different kinds of defenses to mitigate this phenomenon
(e.g., , , ). A natural metric to compare defenses
is the average ‚Äúsize‚Äù of the robustness region on some test set.
Intuitively, the greater this size is, the more robust the defense.
We compared three state-of-the-art defenses:
‚Ä¢ GSS extends the loss with a regularization term
encoding the fast gradient sign method (FGSM) attack.
‚Ä¢ Ensemble is similar to GSS, but includes regularization terms from attacks on other models.
‚Ä¢ MMSTV adds, during training, a perturbation layer
before the input layer which applies the FGSMk attack.
FGSMk is a multi-step variant of FGSM, also known as
projected gradient descent.
All these defenses attempt to reduce the effectiveness of the
FGSM attack . This attack consists of taking a network N
and an input x and computing a vector œÅN,x in the input space
along which an adversarial example is likely to be found. An
adversarial input a is then generated by taking a step œµ along
this vector: a = x + œµ ¬∑ œÅN,x.
We deÔ¨Åne a new kind of robustness region, called line, that
captures resilience with respect to the FGSM attack. The line
robustness region captures all points from x to x + Œ¥ ¬∑ œÅN,x
for some robustness bound Œ¥:
LN,x,Œ¥ = {x + œµ ¬∑ œÅN,x | œµ ‚àà[0, Œ¥]}.
This robustness region is a zonotope and can thus be precisely
captured by AI2.
We compared the three state-of-the-art defenses on the
MNIST convolutional network described in Section VI; we
call this the Original network. We trained the Original network
with each of the defenses, which resulted in 3 additional
networks: GSS, Ensemble, and MMSTV. We used 40 epochs
for GSS, 12 epochs for Ensemble, and 10 000 training steps
for MMSTV using their published frameworks.
Robustness bound Œ¥
Fig. 13: Box-and-whisker plot of the veriÔ¨Åed bounds for the
Original, GSS, Ensemble, and MMSTV networks. The boxes
represent the Œ¥ for the middle 50% of the images, whereas
the whiskers represent the minimum and maximum Œ¥. The
inner-lines are the averages.
We conducted 20 experiments. In each experiment, we
randomly selected an image x and computed œÅN,x. Then, for
each network, our goal was to Ô¨Ånd the largest bound Œ¥ for
which AI2 proves the region LN,x,Œ¥ robust. To approximate
the largest robustness bound, we ran binary search to depth 6
and ran AI2 with the Zonotope domain for each candidate
bound Œ¥. We refer to the largest robustness bound veriÔ¨Åed by
AI2 as the veriÔ¨Åed bound.
The average veriÔ¨Åed bounds for the Original, GSS, Ensemble, and MMSTV networks are 0.026, 0.031, 0.042, and 0.209,
respectively. Fig. 13 shows a box-and-whisker plot which
demonstrates the distribution of the veriÔ¨Åed bounds for the
four networks. The bottom and top of each whisker show the
minimum and maximum veriÔ¨Åed bounds discovered during
the 20 experiments. The bottom and top of each whisker‚Äôs
box show the Ô¨Årst and third quartiles of the veriÔ¨Åed bounds.
Our results indicate that MMSTV provides a signiÔ¨Åcant
increase in provable robustness against the FGSM attack.
In all 20 experiments, the veriÔ¨Åed bound for the MMSTV
network was larger than those found for the Original, GSS,
and Ensemble networks. We observe that GSS and Ensemble
defend the network in a way that makes it only slightly more
provably robust, consistent with observations that these styles
of defense are insufÔ¨Åcient , .
VIII. RELATED WORK
In this section, we survey the works closely related to ours.
Adversarial Examples. showed that neural networks are
vulnerable to small perturbations on inputs. Since then, many
works have focused on constructing adversarial examples.
For example, showed how to Ô¨Ånd adversarial examples
without starting from a test point, generated adversarial
examples using random perturbations, demonstrated that
even intermediate layers are not robust, and generated
adversarial examples for malware classiÔ¨Åcation. Other works
presented ways to construct adversarial examples during the
training phase, thereby increasing the network robustness (see
 , , , , , ). formalized the notion of
robustness in neural networks and deÔ¨Åned metrics to evaluate
the robustness of a neural network. illustrated how to
systematically generate adversarial examples that cover all
neurons in the network.
Neural Network Analysis. Many works have studied the robustness of networks. presented an abstraction-reÔ¨Ånement
approach for FNNs. However, this was shown successful for a
network with only 6 neurons. introduced a bounded model
checking technique to verify safety of a neural network for
the Cart Pole system. showed a veriÔ¨Åcation framework,
based on an SMT solver, which veriÔ¨Åed the robustness with
respect to a certain set of functions that can manipulate the
input and are minimal (a notion which they deÔ¨Åne). However,
it is unclear how one can obtain such a set. extended the
simplex algorithm to verify properties of FNNs with ReLU.
Robustness Analysis of Programs. Many works deal with
robustness analysis of programs (e.g., , , , ).
 considered a deÔ¨Ånition of robustness that is similar to
the one in our work, and used a combination of abstract
interpretation and SMT-based methods to prove robustness of
programs. The programs considered in this literature tend to
be small but have complex constructs such as loops and array
operations. In contrast, neural networks (which are our focus)
are closer to circuits, in that they lack high-level language
features but are potentially massive in size.
IX. CONCLUSION AND FUTURE WORK
We presented AI2, the Ô¨Årst system able to certify convolutional and large fully connected networks. The key insight
behind AI2 is to phrase the problem of analyzing neural
networks in the classic framework of abstract interpretation.
To this end, we deÔ¨Åned abstract transformers that capture the
behavior of common neural network layers and presented a
bounded powerset domain that enables a trade-off between
precision and scalability. Our experimental results showed that
AI2 can effectively handle neural networks that are beyond the
reach of existing methods.
We believe AI2 and the approach behind it is a promising
step towards ensuring the safety and robustness of AI systems.
Currently, we are extending AI2 with additional abstract transformers to support more neural network features. We are also
building a library for modeling common perturbations, such as
rotations, smoothing, and erosion. We believe these extensions
would further improve AI2‚Äôs applicability and foster future
research in AI safety.