Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 45–51,
Berlin, Germany, August 7-12, 2016. c⃝2016 Association for Computational Linguistics
Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax
Trees and Strings
Ondˇrej Duˇsek and Filip Jurˇc´ıˇcek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk´e n´amˇest´ı 25, CZ-11800 Prague, Czech Republic
{odusek,jurcicek}@ufal.mff.cuni.cz
We present a natural language generator based on the sequence-to-sequence approach that can be trained to produce natural language strings as well as deep syntax
dependency trees from input dialogue acts,
and we use it to directly compare two-step
generation with separate sentence planning and surface realization stages to a
joint, one-step approach.
We were able to train both setups successfully using very little training data. The
joint setup offers better performance, surpassing state-of-the-art with regards to ngram-based scores while providing more
relevant outputs.
Introduction
In spoken dialogue systems (SDS), the task of natural language generation (NLG) is to convert a
meaning representation (MR) produced by the dialogue manager into one or more sentences in a
natural language. It is traditionally divided into
two subtasks: sentence planning, which decides
on the overall sentence structure, and surface realization, determining the exact word forms and
linearizing the structure into a string . While some generators keep this division and use a two-step pipeline ,
others apply a joint model for both tasks .
We present a new, conceptually simple NLG
system for SDS that is able to operate in both
modes: it either produces natural language strings
or generates deep syntax dependency trees, which
are subsequently processed by an external surface
realizer . This allows us to
show a direct comparison of two-step generation,
where sentence planning and surface realization
are separated, with a joint, one-step approach.
Our generator is based on the sequence-tosequence (seq2seq) generation technique , combined with
beam search and an n-best list reranker to suppress
irrelevant information in the outputs. Unlike most
previous NLG systems for SDS ), it
is trainable from unaligned pairs of MR and sentences alone. We experiment with using much less
training data than recent systems based on recurrent neural networks (RNN) , and we ﬁnd that our generator learns successfully to produce both strings and
deep syntax trees on the BAGEL restaurant information dataset . It is able to
surpass n-gram-based scores achieved previously
by Duˇsek and Jurˇc´ıˇcek , offering a simpler
setup and more relevant outputs.
We introduce the generation setting in Section 2
and describe our generator architecture in Section 3. Section 4 details our experiments, Section 5
analyzes the results. We summarize related work
in Section 6 and offer conclusions in Section 7.
Generator Setting
The input to our generator are dialogue acts (DA)
 representing an action, such
as inform or request, along with one or more attributes (slots) and their values. Our generator operates in two modes, producing either deep syntax trees or natural language
strings (see Fig. 1). The ﬁrst mode corresponds to
the sentence planning NLG stage as it decides the
syntactic shape of the output sentence; the resulting deep syntax tree involves content words (lemmas) and their syntactic form (formemes, purple in
Fig. 1). The trees are linearized to strings using a
restaurant
inform(name=X-name,type=placetoeat,eattype=restaurant,
area=riverside,food=Italian)
X is an Italian restaurant near the river.
Figure 1: Example DA (top) with the corresponding deep syntax tree (middle) and natural language
string (bottom)
surface realizer from the TectoMT translation system . The second generator
mode joins sentence planning and surface realization into one step, producing natural language sentences directly.
Both modes offer their advantages: The twostep mode simpliﬁes generation by abstracting
away from complex surface syntax and morphology, which can be handled by a handcrafted,
domain-independent module to ensure grammatical correctness at all times , and the joint mode does not need to model
structure explicitly and avoids accumulating errors
along the pipeline .
The Seq2seq Generation Model
Our generator is based on the seq2seq approach
 , a type
of an encoder-decoder RNN architecture operating on variable-length sequences of tokens. We
address the necessary conversion of input DA
and output trees/sentences into sequences in Section 3.1 and then describe the main seq2seq component in Section 3.2.
It is supplemented by a
reranker, as explained in Section 3.3.
Sequence Representation of DA, Trees,
and Sentences
We represent DA, deep syntax trees, and sentences
as sequences of tokens to enable their usage in the
sequence-based RNN components of our generator (see Sections 3.2 and 3.3). Each token is represented by its embedding – a vector of ﬂoatingpoint numbers .
To form a sequence representation of a DA,
we create a triple of the structure “DA type, slot,
value” for each slot in the DA and concatenate
the triples (see Fig. 3). The deep syntax tree output from the seq2seq generator is represented in
a bracketed notation similar to the one used by
Vinyals et al. . The inputs to the
reranker are always a sequence of tokens; structure is disregarded in trees, resulting in a list of
lemma-formeme pairs (see Fig. 2).
Seq2seq Generator
Our seq2seq generator with attention 1 starts with the encoder stage,
which uses an RNN to encode an input sequence
x = {x1, . . . , xn} into a sequence of encoder outputs and hidden states h = {h1, . . . , hn}, where
ht = lstm(xt, ht−1), a non-linear function represented by the long-short-term memory (LSTM)
cell .
The decoder stage then uses the hidden states to
generate a sequence y = {y1, . . . , ym} with a second LSTM-based RNN. The probability of each
output token is deﬁned as:
p(yt|y1, . . . , yt−1, x) = softmax((st ◦ct)WY )
Here, st is the decoder state where s0 = hn
and st = lstm((yt−1 ◦ct)WS, st−1), i.e., the decoder is initialized by the last hidden state and
uses the previous output token at each step. WY
and WS are learned linear projection matrices and
“◦” denotes concatenation. ct is the context vector – a weighted sum of the encoder hidden states
i=1 αtihi, where αti corresponds to an
alignment model, represented by a feed-forward
network with a single tanh hidden layer.
On top of this basic seq2seq model, we implemented a simple beam search for decoding
 . It
proceeds left-to-right and keeps track of log probabilities of top n possible output sequences, expanding them one token at a time.
To ensure that the output trees/strings correspond
semantically to the input DA, we implemented a
classiﬁer to rerank the n-best beam search outputs
and penalize those missing required information
and/or adding irrelevant one. Similarly to Wen et
al. , the classiﬁer provides a binary decision for an output tree/string on the presence of
all dialogue act types and slot-value combinations
seen in the training data, producing a 1-hot vector.
1We use the implementation in the TensorFlow framework .
( <root> <root> ( ( X-name n:subj ) be v:ﬁn ( ( Italian adj:attr ) restaurant n:obj ( river n:near+X ) ) ) )
X-name n:subj be v:ﬁn Italian adj:attr restaurant n:obj river n:near+X
Figure 2: Trees encoded as sequences for the seq2seq generator (top) and the reranker (bottom)
inform name X-name inform eattype restaurant
<GO> X is a restaurant .
X is a restaurant . <STOP>
Figure 3: Seq2seq generator with attention
X is a restaurant .
name=X-name
eattype=bar
eattype=restaurant
area=citycentre
inform(name=X-name,eattype=bar,
area=citycentre)
area=riverside
Figure 4: The reranker
The input DA is converted to a similar 1-hot vector and the reranking penalty of the sentence is the
Hamming distance between the two vectors (see
Fig. 4). Weighted penalties for all sentences are
subtracted from their n-best list log probabilities.
We employ a similar architecture for the classi-
ﬁer as in our seq2seq generator encoder (see Section 3.2), with an RNN encoder operating on the
output trees/strings and a single logistic layer for
classiﬁcation over the last encoder hidden state.
Given an output sequence representing a string or
a tree y = {y1, . . . , yn} (cf. Section 3.1), the encoder again produces a sequence of hidden states
h = {h1, . . . , hn} where ht = lstm(yt, ht−1).
The output binary vector o is computed as:
oi = sigmoid((hn · WR + b)i)
Here, WR is a learned projection matrix and b is a
corresponding bias term.
Experiments
We perform our experiments on the BAGEL data
set of Mairesse et al. , which contains
202 DA from the restaurant information domain
with two natural language paraphrases each, describing restaurant locations, price ranges, food
types etc.
Some properties such as restaurant
names or phone numbers are delexicalized (replaced with “X” symbols) to avoid data sparsity.2Unlike Mairesse et al. , we do not use
2We adopt the delexicalization scenario used by Mairesse
et al. and Duˇsek and Jurˇc´ıˇcek .
manually annotated alignment of slots and values
in the input DA to target words and phrases and
let the generator learn it from data, which simpli-
ﬁes training data preparation but makes our task
We lowercase the data and treat plural
-s as separate tokens for generating into strings,
and we apply automatic analysis from the Treex
NLP toolkit to obtain deep syntax trees for training tree-based generator setups.3 Same as Mairesse et al. , we
apply 10-fold cross-validation, with 181 training
DA and 21 testing DA. In addition, we reserve 10
DA from the training set for validation.4
To train our seq2seq generator, we use the
Adam optimizer to minimize unweighted sequence cross-entropy.5
perform 10 runs with different random initialization of the network and up to 1,000 passes over the
training data,6 validating after each pass and selecting the parameters that yield the highest BLEU
score on the validation set. Neither beam search
nor the reranker are used for validation.
We use the Adam optimizer minimizing crossentropy to train the reranker as well.7 We perform
a single run of up to 100 passes over the data,
and we also validate after each pass and select the
parameters giving minimal Hamming distance on
both validation and training set.8
3The input vocabulary size is around 45 (DA types, slots,
and values added up) and output vocabulary sizes are around
170 for string generation and 180 for tree generation (45
formemes and 135 lemmas).
4We treat the two paraphrases for the same DA as separate instances in the training set but use them together as two
references to measure BLEU and NIST scores on the validation and test sets.
5Based on a few preliminary experiments, the learning
rate is set to 0.001, embedding size 50, LSTM cell size 128,
and batch size 20. Reranking penalty for decoding is 100.
6Training is terminated early if the top 10 so far achieved
validation BLEU scores do not change for 100 passes.
7We use the same settings as with the seq2seq generator.
8The validation set is given 10 times more importance.
BLEU NIST ERR
Mairesse et al. ∗
Duˇsek and Jurˇc´ıˇcek 
Greedy with trees
+ Beam search (b. size 100)
+ Reranker (beam size 5)
(beam size 10)
(beam size 100)
Greedy into strings
+ Beam search (b. size 100)
+ Reranker (beam size 5)
(beam size 10)
(beam size 100)
Table 1: Results on the BAGEL data set
NIST, BLEU, and semantic errors in a sample of the output.
∗Mairesse et al. use manual alignments in their work,
so their result is not directly comparable to ours. The zero
semantic error is implied by the manual alignments and the
architecture of their system.
The results of our experiments and a comparison
to previous works on this dataset are shown in Table 1. We include BLEU and NIST scores and the
number of semantic errors (incorrect, missing, and
repeated information), which we assessed manually on a sample of 42 output sentences (outputs
of two randomly selected cross-validation runs).
The outputs of direct string generation show
that the models learn to produce ﬂuent sentences
in the domain style;9 incoherent sentences are rare,
but semantic errors are very frequent in the greedy
search. Most errors involve confusion of semantically close items, e.g., Italian instead of French
or riverside area instead of city centre (see Table 2); items occurring more frequently are preferred regardless of their relevance.
search brings a BLEU improvement but keeps
most semantic errors in place. The reranker is able
to reduce the number of semantic errors while increasing automatic scores considerably. Using a
larger beam increases the effect of the reranker as
expected, resulting in slightly improved outputs.
Models generating deep syntax trees are also
able to learn the domain style, and they have virtually no problems producing valid trees.10 The surface realizer works almost ﬂawlessly on this lim-
9The average sentence length is around 13 tokens.
10The generated sequences are longer, but have a very rigid
structure, i.e., less uncertainty per generation step. The average output length is around 36 tokens in the generated sequence or 9 tree nodes; surface realizer outputs have a similar
length as the sentences produced in direct string generation.
ited domain , leaving
the seq2seq generator as the major error source.
The syntax-generating models tend to make different kinds of errors than the string-based models: Some outputs are valid trees but not entirely
syntactically ﬂuent; missing, incorrect, or repeated
information is more frequent than a confusion of
semantically similar items (see Table 2). Semantic error rates of greedy and beam-search decoding are lower than for string-based models, partly
because confusion of two similar items counts as
two errors. The beam search brings an increase in
BLEU but also in the number of semantic errors.
The reranker is able to reduce the number of errors
and improve automatic scores slightly. A larger
beam leads to a small BLEU decrease even though
the sentences contain less errors; here, NIST re-
ﬂects the situation more accurately.
A comparison of the two approaches goes in favor of the joint setup: Without the reranker, models generating trees produce less semantic errors
and gain higher BLEU/NIST scores.
with the reranker, the string-based model is able
to reduce the number of semantic errors while
producing outputs signiﬁcantly better in terms of
BLEU/NIST.11 In addition, the joint setup does
not need an external surface realizer. The best results of both setups surpass the best results on this
dataset using training data without manual alignments in both automatic metrics12 and the number of semantic errors.
Related Work
While most recent NLG systems attempt to learn
generation from data, the choice of a particular
approach – pipeline or joint – is often arbitrary
and depends on system architecture or particular
generation domain. Works using the pipeline approach in SDS tend to focus on sentence planning,
improving a handcrafted generator 
or using perceptron-guided A* search . Generators taking the joint approach employ various methods, e.g., factored language models , inverted
parsing , or a pipeline of discriminative classiﬁers . Unlike most previous
11The difference is statistically signiﬁcant at 99% level according to pairwise bootstrap resampling test .
12The BLEU/NIST differences are statistically signiﬁcant.
inform(name=X-name, type=placetoeat, eattype=restaurant, area=citycentre, near=X-near,
food=”Chinese takeaway”, food=Japanese)
X is a Chinese takeaway and Japanese restaurant in the city centre near X.
Greedy with trees
X is a restaurant offering chinese takeaway in the centre of town near X. [Japanese]
+ Beam search
X is a restaurant and japanese food and chinese takeaway.
+ Reranker
X is a restaurant serving japanese food in the centre of the city that offers chinese takeaway.
Greedy into strings
X is a restaurant offering italian and indian takeaway in the city centre area near X. [Japanese, Chinese]
+ Beam search
X is a restaurant that serves fusion chinese takeaway in the riverside area near X. [Japanese, citycentre]
+ Reranker
X is a japanese restaurant in the city centre near X providing chinese food. [takeaway]
inform(name=X-name, type=placetoeat, eattype=restaurant, area=riverside, food=French)
X is a French restaurant on the riverside.
Greedy with trees
X is a restaurant providing french and continental and by the river.
+ Beam search
X is a restaurant that serves french takeaway. [riverside]
+ Reranker
X is a french restaurant in the riverside area.
Greedy into strings
X is a restaurant in the riverside that serves italian food. [French]
+ Beam search
X is a restaurant in the riverside that serves italian food. [French]
+ Reranker
X is a restaurant in the riverside area that serves french food.
inform(name=X-name, type=placetoeat, eattype=restaurant, near=X-near, food=Continental, food=French)
X is a French and Continental restaurant near X.
Greedy with trees
X is a french restaurant that serves french food and near X. [Continental]
+ Beam search
X is a french restaurant that serves french food and near X. [Continental]
+ Reranker
X is a restaurant serving french and continental food near X.
Greedy into strings
X is a french and continental style restaurant near X.
+ Beam search
X is a french and continental style restaurant near X.
+ Reranker
X is a restaurant providing french and continental food, near X.
Table 2: Example outputs of different generator setups (beam size 100 is used). Errors are marked in
color (missing, superﬂuous, repeated information, disﬂuency).
NLG systems, our generator is trainable from unaligned pairs of MR and sentences alone.
Recent RNN-based generators are most similar to our work.
Wen et al. combined
two RNN with a convolutional network reranker;
Wen et al. later replaced basic sigmoid
cells with an LSTM. Mei et al. present
the only seq2seq-based NLG system known to
us. We extend the previous works by generating
deep syntax trees as well as strings and directly
comparing pipeline and joint generation. In addition, we experiment with an order-of-magnitude
smaller dataset than other RNN-based systems.
Conclusions and Future Work
We have presented a direct comparison of two-step
generation via deep syntax trees with a direct generation into strings, both using the same NLG system based on the seq2seq approach. While both
approaches offer decent performance, their outputs are quite different. The results show the direct approach as more favorable, with signiﬁcantly
higher n-gram based scores and a similar number
of semantic errors in the output.
We also showed that our generator can learn
to produce meaningful utterances using a much
smaller amount of training data than what is typically used for RNN-based approaches. The resulting models had virtually no problems with producing ﬂuent, coherent sentences or with generating
valid structure of bracketed deep syntax trees. Our
generator was able to surpass the best BLEU/NIST
scores on the same dataset previously achieved
by a perceptron-based generator of Duˇsek and
Jurˇc´ıˇcek while reducing the amount of irrelevant information on the output.
Our generator is released on GitHub at the following URL:
 
We intend to apply it to other datasets for a broader
comparison, and we plan further improvements,
such as enhancing the reranker or including a bidirectional encoder and sequence level
training .
Acknowledgments
This work was funded by the Ministry of Education, Youth and Sports of the Czech Republic
under the grant agreement LK11221 and core research funding, SVV project 260 333, and GAUK
grant 2058214 of Charles University in Prague.
It used language resources stored and distributed
by the LINDAT/CLARIN project of the Ministry
of Education, Youth and Sports of the Czech Republic (project LM2015071). We thank our colleagues and the anonymous reviewers for helpful