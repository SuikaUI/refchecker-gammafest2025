Pragmatic Neural Language Modelling in Machine Translation
Paul Baltescu
University of Oxford
 
Phil Blunsom
University of Oxford
Google DeepMind
 
This paper presents an in-depth investigation on integrating neural language models
in translation systems.
Scaling neural language models is a difﬁcult task, but crucial
for real-world applications. This paper evaluates the impact on end-to-end MT quality
of both new and existing scaling techniques.
We show when explicitly normalising neural models is necessary and what optimisation tricks one should use in such scenarios.
We also focus on scalable training algorithms
and investigate noise contrastive estimation
and diagonal contexts as sources for further
speed improvements. We explore the tradeoffs between neural models and back-off ngram models and ﬁnd that neural models make
strong candidates for natural language applications in memory constrained environments,
yet still lag behind traditional models in raw
translation quality. We conclude with a set of
recommendations one should follow to build a
scalable neural language model for MT.
Introduction
Language models are used in translation systems to
improve the ﬂuency of the output translations. The
most popular language model implementation is a
back-off n-gram model with Kneser-Ney smoothing . Back-off n-gram
models are conceptually simple, very efﬁcient to
construct and query, and are regarded as being extremely effective in translation systems.
Neural language models are a more recent class of
language models that have been
shown to outperform back-off n-gram models using
intrinsic evaluations of heldout perplexity , or when used in
addition to traditional models in natural language
systems such as speech recognizers . Neural language models
combat the problem of data sparsity inherent to traditional n-gram models by learning distributed representations for words in a continuous vector space.
It has been shown that neural language models
can improve translation quality when used as additional features in a decoder or if used for n-best list rescoring . These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating
neural language models in a decoder. We also focus on how effective these models are when used
as the sole language model in a translation system.
This is important because our hypothesis is that most
of the language modelling is done by the n-gram
model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability. Furthermore, neural language models are considerably more compact and
represent strong candidates for modelling language
in memory constrained environments (e.g. mobile
devices, commodity machines, etc.), where back-off
n-gram models trained on large amounts of data do
not ﬁt into memory.
Our results show that a novel combination of
noise contrastive estimation 
 
Figure 1: A 3-gram neural language model is used to predict the word following the context the cat.
and factoring the softmax layer using Brown clusters
 provides the most pragmatic solution for fast training and decoding. Further, we
conﬁrm that when evaluated purely on BLEU score,
neural models are unable to match the benchmark
Kneser-Ney models, even if trained with large hidden layers.
However, when the evaluation is restricted to models that match a certain memory footprint, neural models clearly outperform the n-gram
benchmarks, conﬁrming that they represent a practical solution for memory constrained environments.
Model Description
As a basis for our investigation, we implement a
probabilistic neural language model as deﬁned in
Bengio et al. .1
For every word w in the
vocabulary V , we learn two distributed representations qw and rw in RD.
The vector qw captures
the syntactic and semantic role of the word w when
w is part of a conditioning context, while rw captures its role as a prediction. For some word wi in a
given corpus, let hi denote the conditioning context
wi−1, . . . , wi−n+1. To ﬁnd the conditional probability P(wi|hi), our model ﬁrst computes a context
projection vector:
where Cj ∈RD×D are context speciﬁc transformation matrices and f is a component-wise rectiﬁed
1Our goal is to release a scalable neural language modelling
toolkit at the following URL: 
Exact Decoding
O(|V | × D)
O(|V | × D)
Class Factored
Tree Factored
O(log |V | × D)
O(log |V | × D)
O(|V | × D)
Table 1: Training and decoding complexities for the optimization tricks discussed in section 2.
linear activation. The model computes a set of similarity scores measuring how well each word w ∈V
matches the context projection of hi. The similarity
score is deﬁned as φ(w, hi) = rT
wp + bw, where bw
is a bias term incorporating the prior probability of
the word w. The similarity scores are transformed
into probabilities using the softmax function:
P(wi|hi) =
exp(φ(wi, hi))
w∈V exp(φ(w, hi)),
The model architecture is illustrated in Figure 1. The
parameters are learned with gradient descent to maximize log-likelihood with L2 regularization.
Scaling neural language models is hard because
any forward pass through the underlying neural network computes an expensive softmax activation in
the output layer. This operation is performed during training and testing for all contexts presented as
input to the network. Several methods have been
proposed to alleviate this problem: some applicable
only during training , while others may also speed up
arbitrary queries to the language model .
In the following subsections, we present several
extensions to this model, all sharing the goal of reducing the computational cost of the softmax step.
Table 1 summarizes the complexities of these methods during training and decoding.
Class Based Factorisation
The time complexity of the softmax step is O(|V | ×
D). One option for reducing this excessive amount
of computation is to rely on a class based factorisation trick .
We partition the
vocabulary into K classes {C1, . . . , CK} such that
i=1 Ci and Ci ∩Cj = ∅, ∀1 ≤i < j ≤K.
We deﬁne the conditional probabilities as:
P(wi|hi) = P(ci|hi)P(wi|ci, hi),
where ci is the class the word wi belongs to, i.e.
wi ∈Cci. We adjust the model deﬁnition to also
account for the class probabilities P(ci|hi). We associate a distributed representation sc and a bias term
tc to every class c. The class conditional probabilities are computed reusing the projection vector p
with a new scoring function ψ(c, hi) = sT
The probabilities are normalised separately:
P(ci|hi) =
exp(ψ(ci, hi))
j=1 exp(ψ(cj, hi))
P(wi|ci, hi) =
exp(φ(wi, hi))
w∈Cci exp(φ(w, hi))
|V | and the word classes have roughly
equal sizes, the softmax step has a more manageable
time complexity of O(
|V | × D) for both training
and testing.
Tree Factored Models
One can take the idea presented in the previous section one step further and construct a tree over the
vocabulary V . The words in the vocabulary are used
to label the leaves of the tree. Let n1, . . . , nk be the
nodes on the path descending from the root (n1) to
the leaf labelled with wi (nk). The probability of the
word wi to follow the context hi is deﬁned as:
P(wi|hi) =
P(nj|n1, . . . , nj−1, hi).
We associate a distributed representation sn and bias
term tn to each node in the tree. The conditional
probabilities are obtained reusing the scoring function ψ(nj, hi):
P(nj|n1, . . . , nj−1, hi) =
exp(ψ(nj, hi))
n∈S(nj) exp(ψ(n, hi)),
where S(nj) is the set containing the siblings of nj
and the node itself. Note that the class decomposition trick described earlier can be understood as a
tree factored model with two layers, where the ﬁrst
layer contains the word classes and the second layer
contains the words in the vocabulary.
The optimal time complexity is obtained by using
balanced binary trees. The overall complexity of the
normalisation step becomes O(log |V |×D) because
the length of any path is bounded by O(log |V |) and
because exactly two terms are present in the denominator of every normalisation operation.
Inducing high quality binary trees is a difﬁcult
problem which has received some attention in the
research literature .
Results have been somewhat
unsatisfactory, with the exception of Mnih and Hinton , who did not release the code they used
to construct their trees. In our experiments, we use
Huffman trees which do not have
any linguistic motivation, but guarantee that a minimum number of nodes are accessed during training.
Huffman trees have depths that are close to log |V |.
Noise Contrastive Estimation
Training neural language models to maximise data
likelihood involves several iterations over the entire
training corpus and applying the backpropagation
algorithm for every training sample. Even with the
previous factorisation tricks, training neural models is slow. We investigate an alternative approach
for training language models based on noise contrastive estimation, a technique which does not require normalised probabilities when computing gradients . This method has already been used for training neural language models
for machine translation by Vaswani et al. .
The idea behind noise contrastive training is to
transform a density estimation problem into a classi-
ﬁcation problem, by learning a classiﬁer to discriminate between samples drawn from the data distribution and samples drawn for a known noise distribution. Following Mnih and Teh , we set the
unigram distribution Pn(w) as the noise distribution
and use k times more noise samples than data samples to train our models. The new objective is:
log P(C = 1|θ, wi, hi)
log P(C = 0|θ, nij, hi),
where nij are the noise samples drawn from Pn(w).
The posterior probability that a word is generated
Language pairs
# sentences
Table 2: Statistics for the parallel corpora.
from the data distribution given its context is:
P(C = 1|θ, wi, hi) =
P(wi|θ, hi)
P(wi|θ, hi) + kPn(wi).
Mnih and Teh show that the gradient of J(θ)
converges to the gradient of the log-likelihood objective when k →∞.
When using noise contrastive estimation, additional parameters can be used to capture the normalisation terms. Mnih and Teh ﬁx these parameters to 1 and obtain the same perplexities, thereby
circumventing the need for explicit normalisation.
However, this method does not provide any guarantees that the models are normalised at test time.
In fact, the outputs may sum up to arbitrary values,
unless the model is explicitly normalised.
Noise contrastive estimation is more efﬁcient than
the factorisation tricks at training time, but at test
time one still has to normalise the model to obtain
valid probabilities. We propose combining this approach with the class decomposition trick resulting
in a fast algorithm for both training and testing. In
the new training algorithm, when we account for
the class conditional probabilities P(ci|hi), we draw
noise samples from the class unigram distribution,
and when we account for P(wi|ci, hi), we sample
from the unigram distribution of only the words in
the class Cci.
Experimental Setup
In our experiments, we use data from the 2014 ACL
Workshop in Machine Translation.2 We train standard phrase-based translation systems for French →
English, English →Czech and English →German
using the Moses toolkit .
commentary corpora as parallel data for training
2The data is available here:
 
org/wmt14/translation-task.html.
Vocabulary
English (en)
Czech (cs)
German (de)
Table 3: Statistics for the monolingual corpora.
the translation systems. The parallel corpora were
tokenized, lowercased and sentences longer than 80
words were removed using standard text processing
tools.3 Table 2 contains statistics about the training
corpora after the preprocessing step. We tuned the
translation systems on the newstest2013 data
using minimum error rate training and
we used the newstest2014 corpora to report
uncased BLEU scores averaged over 3 runs.
The monolingual training data used for training
news commentary
news crawl
2007-2013 corpora. The corpora were tokenized
and lowercased using the same text processing
scripts and the words not occuring the in the target
side of the parallel data were replaced with a special
<unk> token. Statistics for the monolingual data
after the preprocessing step are reported in Table 3.
Throughout this paper we report results for 5gram language models, regardless of whether they
are back-off n-gram models or neural models. To
construct the back-off n-gram models, we used
a compact trie-based implementation available in
KenLM , because otherwise we
would have had difﬁculties with ﬁtting these models
in the main memory of our machines. When training neural language models, we set the size of the
distributed representations to 500, we used diagonal
context matrices and we used 10 negative samples
for noise contrastive estimation, unless otherwise indicated. In cases where we perform experiments on
only one language pair, the reader should assume we
used French→English data.
Normalisation
The key challenge with neural language models is
scaling the softmax step in the output layer of the
3We followed the ﬁrst two steps from 
cdec-decoder.org/guide/tutorial.html.
33.01 (120.446)
31.55 (115.119)
Table 4: A comparison between standard back-off n-gram
models and neural language models. The perplexities for
the English language models are shown in parentheses.
network. This operation is especially problematic
when the neural language model is incorporated as
a feature in the decoder, as the language model is
queried several hundred thousand times for any sentence of average length.
Previous publications on neural language models
in machine translation have approached this problem in two different ways. Vaswani et al. 
and Devlin et al. simply ignore normalisation
when decoding, albeit Devlin et al. alter their
training objective to learn self-normalised models,
i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al.
 use noise contrastive estimation to speed up
training, while Devlin et al. train their models
with standard gradient descent on a GPU.
The second approach is to explicitly normalise the
models, but to limit the set of words over which the
normalisation is performed, either via class-based
factorisation or using a shortlist containing only the
most frequent words in the vocabulary and scoring
the remaining words with a back-off n-gram model
 . Tree factored models follow the
same general approach, but to our knowledge, they
have never been investigated in a translation system
before. These normalisation techniques can be successfully applied both when training the models and
when using them in a decoder.
Table 4 shows a side by side comparison of out of
the box neural language models and back-off n-gram
models. We note a signiﬁcant drop in quality when
neural language models are used (roughly 1.5 BLEU
for fr→en and en→de and 0.5 BLEU for en→cs).
This result is in line with Zhao et al. and
shows that by default back-off n-gram models are
much more effective in MT. An interesting observation is that the neural models have lower perplexities
than the n-gram models, implying that BLEU scores
Normalisation
Unnormalised
Class Factored
Tree Factored
Table 5: Qualitative analysis of the proposed normalisation schemes with an additional back-off n-gram model.
Normalisation
Unnormalised
Class Factored
Tree Factored
Table 6: Qualitative analysis of the proposed normalisation schemes without an additional back-off n-gram
and perplexities are only loosely correlated.
Table 5 and Table 6 show the impact on translation quality for the proposed normalisation schemes
with and without an additional n-gram model. We
note that when KenLM is used, no signiﬁcant differences are observed between normalised and unnormalised models, which is again in accordance with
the results of Zhao et al. . However, when
the n-gram model is removed, class factored models
perform better (at least for fr→en and en→de), despite being only an approximation of the fully normalised models. We believe this difference in not
observed in the ﬁrst case because most of the language modelling is done by the n-gram model (as
indicated by the results in Table 4) and that the neural models only act as a differentiating feature when
the n-gram models do not provide accurate probabilities. We conclude that some form of normalisation
is likely to be necessary whenever neural models are
used alone. This result may also explain why Zhao
et al. show, perhaps surprisingly, that normalisation is important when reranking n-best lists with
recurrent neural language models, but not in other
cases. (This is the only scenario where they use neural models without supporting n-gram models.)
Table 5 and Table 6 also show that tree factored
models perform poorly compared to the other candidates. We believe this is likely to be a result of
the artiﬁcial hierarchy imposed by the tree over the
vocabulary.
Normalisation
Clustering
Class Factored
Brown clustering
Class Factored
Frequency binning
Tree Factored
Huffman encoding
Table 7: Qualitative analysis of clustering strategies on
fr→en data.
Average decoding time
Unnormalised NLM
Class Factored NLM
Tree Factored NLM
Table 8: Average decoding time per sentence for the proposed normalisation schemes.
Table 7 compares two popular techniques for
obtaining word classes: Brown clustering and frequency binning
 . From these results, we learn
that the clustering technique employed to partition
the vocabulary into classes can have a huge impact
on translation quality and that Brown clustering is
clearly superior to frequency binning.
Another thing to note is that frequency binning
partitions the vocabulary in a similar way to Huffman encoding.
This observation implies that the
BLEU scores we report for tree factored models are
not optimal, but we can get an insight on how much
we expect to lose in general by imposing a tree structure over the vocabulary (on the fr→en setup, we
lose roughly 0.7 BLEU points). Unfortunately, we
are not able to report BLEU scores for factored models using Brown trees because the time complexity
for constructing such trees is O(|V |3).
We report the average time needed to decode a
sentence for each of the models described in this paper in Table 8. We note that factored models are slow
compared to unnormalised models. One option for
speeding up factored models is using a GPU to perform the vector-matrix operations. However, GPU
integration is architecture speciﬁc and thus against
our goal of making our language modelling toolkit
usable by everyone.
Perplexity
Table 9: A comparison between stochastic gradient descent (SGD) and noise contrastive estimation (NCE) for
class factored models on the fr→en data.
Training time
Unnormalised NCE
Class Factored NCE
Tree Factored SGD
Table 10: Training times for neural models on fr→en
In this section, we are concerned with ﬁnding scalable training algorithms for neural language models. We investigate noise contrastive estimation as
a much more efﬁcient alternative to standard maximum likelihood training via stochastic gradient descent. Class factored models enable us to conduct
this investigation at a much larger scale than previous results has slightly over 1M tokens), thereby
gaining useful insights on how this method truly
performs at scale. (In our experiments, we use a
2B words corpus and a 100k vocabulary.) Table 9
summarizes our ﬁndings. We obtain a slightly better BLEU score with stochastic gradient descent, but
this is likely to be just noise from tuning the translation system with MERT. On the other hand, noise
contrastive training reduces training time by a factor
Table 10 reviews the neural models described in
this paper and shows the time needed to train each
We note that noise contrastive training requires roughly the same amount of time regardless
of the structure of the model. Also, we note that
this method is at least as fast as maximum likelihood training even when the latter is applied to tree
factored models. Since tree factored models have
lower quality, take longer to query and do not yield
any substantial beneﬁts at training time when compared to unnormalised models, we conclude they
represent a suboptimal language modelling choice
Perplexity
Training time
Table 11: A side by side comparison of class factored
models with and without diagonal contexts trained with
noise contrastive estimation on the fr→en data.
for machine translation.
Diagonal Context Matrices
In this section, we investigate diagonal context matrices as a source for reducing the computational
cost of calculating the projection vector. In the standard deﬁnition of a neural language model, this cost
is dominated by the softmax step, but as soon as
tricks like noise contrastive estimation or tree or
class factorisations are used, this operation becomes
the main bottleneck for training and querying the
model. Using diagonal context matrices when computing the projection layer reduces the time complexity from O(D2) to O(D). A similar optimization is achieved in the backpropagation algorithm, as
only O(D) context parameters need to be updated
for every training instance.
Devlin et al. also identiﬁed the need for
ﬁnding a scalable solution for computing the projection vector. Their approach is to cache the product between every word embedding and every context matrix and to look up these terms in a table as
needed. Devlin et al. ’s approach works well
when decoding, but it requires additional memory
and is not applicable during training.
Table 11 compares diagonal and full context matrices for class factored models. Both models have
similar BLEU scores, but the training time is reduced by a factor of 3 when diagonal context matrices are used. We obtain similar improvements when
decoding with class factored models, but the speed
up for unnormalised models is over 100x!
Quality vs. Memory Trade-off
Neural language models are a very appealing option
for natural language applications that are expected
to run on mobile phones and commodity computers, where the typical amount of memory available is limited to 1-2 GB. Nowadays, it is becom-
Figure 2: A graph highlighting the quality vs. memory
trade-off between traditional n-gram models and neural
language models.
ing more and more common for these devices to
include reasonably powerful GPUs, supporting the
idea that further scaling is possible if necessary. On
the other hand, ﬁtting back-off n-gram models on
such devices is difﬁcult because these models store
the probability of every n-gram in the training data.
In this section, we seek to gain further understanding
on how these models perform under such conditions.
In this analysis, we used Heaﬁeld ’s triebased implementation with quantization for constructing memory efﬁcient back-off n-gram models.
A 5-gram model trained on the English monolingual data introduced in section 3 requires 12 GB of
memory. We randomly sampled sentences with an
acceptance ratio ranging between 0.01 and 1 to construct smaller models and observe their performance
on a larger spectrum. The BLEU scores obtained using these models are reported in Figure 2. We note
that the translation quality improves as the amount
of training data increases, but the improvements are
less signiﬁcant when most of the data is used.
The neural language models we used to report
results throughout this paper are roughly 400 MB
in size. Note that we do not use any compression
techniques to obtain smaller models, although this
is technically possible (e.g. quantization). We are
interested to see how these models perform for various memory thresholds and we experiment with setting the size of the word embeddings between 100
and 5000. More importantly, these experiments are
meant to give us an insight on whether very large
neural language models have any chance of achieving the same performance as back-off n-gram models in translation tasks. A positive result would imply that signiﬁcant gains can be obtained by scaling
these models further, while a negative result signals
a possible inherent inefﬁciency of neural language
models in MT. The results are shown in Figure 2.
From Figure 2, we learn that neural models perform signiﬁcantly better (over 1 BLEU point) when
there is under 1 GB of memory available. This is exactly the amount of memory generally available on
mobile phones and ordinary computers, conﬁrming
the potential of neural language models for applications designed to run on such devices. However, at
the other end of the scale, we can see that back-off
models outperform even the largest neural language
models by a decent margin and we can expect only
modest gains if we scale these models further.
Conclusion
This paper presents an empirical analysis of neural
language models in machine translation. The experiments presented in this paper help us draw several useful conclusions about the ideal usage of these
language models in MT systems.
The ﬁrst problem we investigate is whether normalisation has any impact on translation quality and
we survey the effects of some of the most frequently
used techniques for scaling neural language models. We conclude that normalisation is not necessary
when neural models are used in addition to back-off
n-gram models. This result is due to the fact that
most of the language modelling is done by the ngram model. (Experiments show that out of the box
n-gram models clearly outperform their neural counterparts.) The MT system learns a smaller weight
for neural models and we believe their main use is
to correct the inaccuracies of the n-gram models.
On the other hand, when neural language models
are used in isolation, we observe that normalisation
does matter. We believe this result generalizes to
other neural architectures such as neural translation
models . We
observe that the most effective normalisation strategy in terms of translation quality is the class-based
decomposition trick.
We learn that the algorithm
used for partitioning the vocabulary into classes has
a strong impact on the overall quality and that Brown
clustering is a good choice. Decoding with class factored models can be slow, but
this issue can be corrected using GPUs, or if a comprise in quality is acceptable, unnormalised models
represent a much faster alternative. We also conclude that tree factored models are not a strong candidate for translation since they are outperformed by
unnormalised models in every aspect.
We introduce noise contrastive estimation for
class factored models and show that it performs almost as well as maximum likelihood training with
stochastic gradient descent. To our knowledge, this
is the ﬁrst side by side comparison of these two techniques on a dataset consisting of a few billions of
training examples and a vocabulary with over 100k
tokens. On this setup, noise contrastive estimation
can be used to train standard or class factored models in a little over 1 day.
We explore diagonal context matrices as an optimization for computing the projection layer in the
neural network.
The trick effectively reduces the
time complexity of this operation from O(D2) to
O(D). Compared to Devlin et al. ’s approach
of caching vector-matrix products, diagonal context
matrices are also useful for speeding up training and
do not require additional memory. Our experiments
show that diagonal context matrices perform just as
well as full matrices in terms of translation quality.
We also explore the trade-off between neural language models and back-off n-gram models. We observe that in the memory range that is typically available on a mobile phone or a commodity computer,
neural models outperform n-gram models with more
than 1 BLEU point. On the other hand, when memory is not a limitation, traditional n-gram models
outperform even the largest neural models by a sizable margin (over 0.5 BLEU in our experiments).
Our work is important because it reviews the most
important scaling techniques used in neural language modelling for MT. We show how these methods compare to each other and we combine them to
obtain neural models that are fast to both train and
test. We conclude by exploring the strengths and
weaknesses of these models into greater detail.
Acknowledgments
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.