Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 484–495
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 484–495
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
 
Towards End-to-End Reinforcement Learning of
Dialogue Agents for Information Access
Bhuwan Dhingra⋆
Lihong Li†
Xiujun Li†
Jianfeng Gao†
Yun-Nung Chen‡
Faisal Ahmed†
⋆Carnegie Mellon University, Pittsburgh, PA, USA
†Microsoft Research, Redmond, WA, USA
‡National Taiwan University, Taipei, Taiwan
⋆ 
†{lihongli,xiul,jfgao}@microsoft.com
‡ 
This paper proposes KB-InfoBot1 — a
multi-turn dialogue agent which helps
users search Knowledge Bases (KBs)
without composing complicated queries.
Such goal-oriented dialogue agents typically need to interact with an external
database to access real-world knowledge.
Previous systems achieved this by issuing
a symbolic query to the KB to retrieve entries based on their attributes. However,
such symbolic operations break the differentiability of the system and prevent endto-end training of neural dialogue agents.
In this paper, we address this limitation
by replacing symbolic queries with an induced “soft” posterior distribution over the
KB that indicates which entities the user is
interested in. Integrating the soft retrieval
process with a reinforcement learner leads
to higher task success rate and reward in
both simulations and against real users.
We also present a fully neural end-to-end
agent, trained entirely from user feedback,
and discuss its application towards personalized dialogue agents.
Introduction
The design of intelligent assistants which interact
with users in natural language ranks high on the
agenda of current NLP research. With an increasing focus on the use of statistical and machine
learning based approaches ,
the last few years have seen some truly remarkable conversational agents appear on the market
(e.g. Apple Siri, Microsoft Cortana, Google Allo).
These agents can perform simple tasks, answer
1The source code is available at: 
com/MiuLab/KB-InfoBot
factual questions, and sometimes also aimlessly
chit-chat with the user, but they still lag far behind a human assistant in terms of both the variety and complexity of tasks they can perform.
In particular, they lack the ability to learn from
interactions with a user in order to improve and
adapt with time. Recently, Reinforcement Learning (RL) has been explored to leverage user interactions to adapt various dialogue agents designed,
respectively, for task completion , information access , and
chitchat .
We focus on KB-InfoBots, a particular type of
dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in Figure 1. Such agents
must necessarily query databases in order to retrieve the requested information. This is usually
done by performing semantic parsing on the input
to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen
et al. , Williams and Zweig , and Li
et al. ’s work. We call such an operation
a Hard-KB lookup. While natural, this approach
has two drawbacks: (1) the retrieved results do not
carry any information about uncertainty in semantic parsing, and (2) the retrieval operation is non
differentiable, and hence the parser and dialog policy are trained separately. This makes online endto-end learning from user feedback difﬁcult once
the system is deployed.
In this work, we propose a probabilistic framework for computing the posterior distribution of
the user target over a knowledge base, which we
term a Soft-KB lookup. This distribution is constructed from the agent’s belief about the attributes
of the entity being searched for. The dialogue policy network, which decides the next system action,
receives as input this full distribution instead of a
handful of retrieved results. We show in our ex-
Actor=Bill Murray
Release Year=1993
Find me the Bill Murray’s movie.
I think it came out in 1993.
When was it released?
Groundhog Day is a Bill Murray
movie which came out in 1993.
KB-InfoBot
Entity-Centric Knowledge Base
Groundhog Day
Bill Murray
Nicole Kidman
Mad Max: Fury Road
Figure 1: An interaction between a user looking
for a movie and the KB-InfoBot. An entity-centric
knowledge base is shown above the KB-InfoBot
(missing values denoted by X).
periments that this framework allows the agent to
achieve a higher task success rate in fewer dialogue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end
trainable KB-InfoBot, all of whose components
are updated online using RL.
Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training.
Running experiments on human subjects, on the
other hand, is unfortunately too expensive.
common workaround in the dialogue community
 is to instead use user
simulators which mimic the behavior of real users
in a consistent manner. For training KB-InfoBot,
we adapt the publicly available2 simulator described in Li et al. .
Evaluation of dialogue agents has been the subject of much research . While the metrics for evaluating an
InfoBot are relatively clear — the agent should return the correct entity in a minimum number of
turns — the environment for testing it not so much.
Unlike previous KB-based QA systems, our focus
is on multi-turn interactions, and as such there are
no publicly available benchmarks for this problem. We evaluate several versions of KB-InfoBot
with the simulator and on real users, and show
that the proposed Soft-KB lookup helps the reinforcement learner discover better dialogue policies. Initial experiments on the end-to-end agent
also demonstrate its strong learning capability.
2 
Related Work
Our work is motivated by the neural GenQA and neural enquirer models for querying KBs via natural language in a fully “neuralized” way. However, the
key difference is that these systems assume that
users can compose a complicated, compositional
natural language query that can uniquely identify
the element/answer in the KB. The research task
is to parse the query, i.e., turning the natural language query into a sequence of SQL-like operations. Instead we focus on how to query a KB
interactively without composing such complicated
queries in the ﬁrst place. Our work is motivated
by the observations that (1) users are more used to
issuing simple queries of length less than 5 words
 ; (2) in many cases, it is unreasonable to assume that users can construct compositional queries without prior knowledge of the
structure of the KB to be queried.
Also related is the growing body of literature
focused on building end-to-end dialogue systems,
which combine feature extraction and policy optimization using deep neural networks. Wen et al.
 introduced a modular neural dialogue
agent, which uses a Hard-KB lookup, thus breaking the differentiability of the whole system. As a
result, training of various components of the dialogue system is performed separately. The intent network and belief trackers are trained using
supervised labels speciﬁcally collected for them;
while the policy network and generation network
are trained separately on the system utterances.
We retain modularity of the network by keeping
the belief trackers separate, but replace the hard
lookup with a differentiable one.
Dialogue agents can also interface with the
database by augmenting their output action space
with predeﬁned API calls . The API calls modify
a query hypothesis maintained outside the end-toend system which is used to retrieve results from
this KB. This framework does not deal with uncertainty in language understanding since the query
hypothesis can only hold one slot-value at a time.
Our approach, on the other hand, directly models
the uncertainty to construct the posterior over the
Wu et al. presented an entropy minimization dialogue management strategy for In-
foBots. The agent always asks for the value of
the slot with maximum entropy over the remaining entries in the database, which is optimal in
the absence of language understanding errors, and
serves as a baseline against our approach. Reinforcement learning neural turing machines (RL-
NTM) also allow
neural controllers to interact with discrete external
interfaces. The interface considered in that work
is a one-dimensional memory tape, while in our
work it is an entity-centric KB.
Probabilistic KB Lookup
This section describes a probabilistic framework
for querying a KB given the agent’s beliefs over
the ﬁelds in the KB.
Entity-Centric Knowledge Base (EC-KB)
A Knowledge Base consists of triples of the form
(h, r, t), which denotes that relation r holds between the head h and tail t.
We assume that
the KB-InfoBot has access to a domain-speciﬁc
entity-centric knowledge base (EC-KB) where all head entities are of
a particular type (such as movies or persons), and
the relations correspond to attributes of these head
entities. Such a KB can be converted to a table
format whose rows correspond to the unique head
entities, columns correspond to the unique relation
types (slots henceforth), and some entries may be
missing. An example is shown in Figure 1.
Notations and Assumptions
Let T denote the KB table described above and
Ti,j denote the jth slot-value of the ith entity.
1 ≤i ≤N and 1 ≤j ≤M. We let V j denote the
vocabulary of each slot, i.e. the set of all distinct
values in the j-th column. We denote missing values from the table with a special token and write
Ti,j = Ψ. Mj = {i : Ti,j = Ψ} denotes the set
of entities for which the value of slot j is missing.
Note that the user may still know the actual value
of Ti,j, and we assume this lies in V j. We do not
deal with new entities or relations at test time.
We assume a uniform prior G ∼U[{1, ...N}]
over the rows in the table T , and let binary random variables Φj ∈{0, 1} indicate whether the
user knows the value of slot j or not. The agent
maintains M multinomial distributions pt
v ∈V j denoting the probability at turn t that the
user constraint for slot j is v, given their utterances
1 till that turn. The agent also maintains M binomials qt
j = Pr(Φj = 1) which denote the probability that the user knows the value of slot j.
We assume that column values are independently distributed to each other. This is a strong
assumption but it allows us to model the user goal
for each slot independently, as opposed to modeling the user goal over KB entities directly. Typically maxj |V j| < N and hence this assumption
reduces the number of parameters in the model.
Soft-KB Lookup
T (i) = Pr(G = i|Ut
1) be the posterior probability that the user is interested in row i of the
table, given the utterances up to turn t. We assume all probabilities are conditioned on user inputs Ut
1 and drop it from the notation below. From
our assumption of independence of slot values,
we can write pt
j=1 Pr(Gj = i), where
Pr(Gj = i) denotes the posterior probability of
user goal for slot j pointing to Ti,j. Marginalizing
this over Φj gives:
Pr(Gj = i) =
Pr(Gj = i, Φj = φ)
j Pr(Gj = i|Φj = 1)+
j) Pr(Gj = i|Φj = 0).
For Φj = 0, the user does not know the value of
the slot, and from the prior:
Pr(Gj = i|Φj = 0) = 1
For Φj = 1, the user knows the value of slot j, but
this may be missing from T , and we again have
two cases:
Pr(Gj = i|Φj = 1) =
Here, Nj(v) is the count of value v in slot j. Detailed derivation for (3) is provided in Appendix A.
Combining (1), (2), and (3) gives us the procedure
for computing the posterior over KB entities.
Towards an End-to-End-KB-InfoBot
We claim that the Soft-KB lookup method has two
beneﬁts over the Hard-KB method – (1) it helps
the agent discover better dialogue policies by providing it more information from the language understanding unit, (2) it allows end-to-end training
of both dialogue policy and language understanding in an online setting. In this section we describe
several agents to test these claims.
Belief Trackers
Policy Network
Beliefs Summary
KB-InfoBot
Figure 2: High-level overview of the end-to-end
KB-InfoBot. Components with trainable parameters are highlighted in gray.
Figure 2 shows an overview of the components of
the KB-InfoBot. At each turn, the agent receives a
natural language utterance ut as input, and selects
an action at as output. The action space, denoted
by A, consists of M +1 actions — request(slot=i)
for 1 ≤i ≤M will ask the user for the value of
slot i, and inform(I) will inform the user with an
ordered list of results I from the KB. The dialogue
ends once the agent chooses inform.
We adopt a modular approach, typical to goaloriented dialogue systems ,
consisting of: a belief tracker module for identifying user intents, extracting associated slots,
and tracking the dialogue state ; an interface with the database to query for relevant results
(Soft-KB lookup); a summary module to summarize the state into a vector; a dialogue policy which
selects the next system action based on current
state . We assume the agent
only responds with dialogue acts.
A templatebased Natural Language Generator (NLG) can
be easily constructed for converting dialogue acts
into natural language.
Belief Trackers
The InfoBot consists of M belief trackers, one for
each slot, which get the user input xt and produce
two outputs, pt
j, which we shall collectively
call the belief state: pt
j is a multinomial distribution over the slot values v, and qt
j is a scalar probability of the user knowing the value of slot j. We
describe two versions of the belief tracker.
Hand-Crafted Tracker:
We ﬁrst identify mentions of slot-names (such as “actor”) or slot-values
(such as “Bill Murray”) from the user input ut, using token-level keyword search. Let {w ∈x} denote the set of tokens in a string x3, then for each
slot in 1 ≤j ≤M and each value v ∈V j, we
compute its matching score as follows:
j[v] = |{w ∈ut} ∩{w ∈v}|
A similar score bt
j is computed for the slot-names.
A one-hot vector reqt ∈{0, 1}M denotes the previously requested slot from the agent, if any. qt
set to 0 if reqt[j] is 1 but st
j[v] = 0 ∀v ∈V j, i.e.
the agent requested for a slot but did not receive a
valid value in return, else it is set to 1.
Starting from an prior distribution p0
j (based on
the counts of the values in the KB), pt
j[v] is updated as:
j[v] ∝pt−1
j + 1(reqt[j] = 1)
Here C is a tuning parameter, and the normalization is given by setting the sum over v to 1.
Neural Belief Tracker:
For the neural tracker
the user input ut is converted to a vector representation xt, using a bag of n-grams (with n = 2)
representation. Each element of xt is an integer
indicating the count of a particular n-gram in ut.
We let V n denote the number of unique n-grams,
hence xt ∈NV n
Recurrent neural networks have been used for
belief tracking since the output distribution at turn t depends on all user inputs till that turn. We use a
Gated Recurrent Unit (GRU) for
each tracker, which, starting from h0
j = 0 computes ht
j = GRU(x1, . . . , xt) (see Appendix B for
details). ht
j ∈Rd can be interpreted as a summary
of what the user has said about slot j till turn t.
The belief states are computed from this vector as
j = softmax .
Each slot is summarized into an entropy statistic
over a distribution wt
j computed from elements of
the KB posterior pt
T as follows:
T (i) + p0
j is a prior distribution over the values of
slot j, estimated using counts of each value in the
KB. The probability mass of v in this distribution is the agent’s conﬁdence that the user goal has
value v in slot j. This two terms in (8) correspond
to rows in KB which have value v, and rows whose
value is unknown (weighted by the prior probability that an unknown might be v). Then the summary statistic for slot j is the entropy H(wt
KB posterior pt
T is also summarized into an entropy statistic H(pt
The scalar probabilities qt
j are passed as is to
the dialogue policy, and the ﬁnal summary vector
is ˜st = [H(˜pt
1), ..., H . It asks for the
slot ˆj = arg min H(˜pt
j) with the minimum entropy, except if – (i) the KB posterior entropy
T ) < αR, (ii) H(˜pt
j) < min(αT , βH(˜p0
(iii) slot j has already been requested Q times. αR,
αT , β, Q are tuned to maximize reward against the
simulator.
Neural Policy Network:
For the neural approach, similar to , we use an RNN to allow the network to maintain an internal state of
dialogue history. Speciﬁcally, we use a GRU unit
followed by a fully-connected layer and softmax
nonlinearity to model the policy π over actions in
A (W π ∈R|A|×d, bπ ∈R|A|):
π = GRU(˜s1, ..., ˜st)
π = softmax(W πht
During training, the agent samples its actions
from the policy to encourage exploration. If this
action is inform(), it must also provide an ordered
set of entities indexed by I = (i1, i2, . . . , iR) in
the KB to the user. This is done by sampling R
items from the KB-posterior pt
T . This mimics a
search engine type setting, where R may be the
number of results on the ﬁrst page.
Parameters of the neural components (denoted by
θ) are trained using the REINFORCE algorithm
 . We assume that the learner has
access to a reward signal rt throughout the course
of the dialogue, details of which are in the next
We can write the expected discounted
return of the agent under policy π as J(θ) =
(γ is the discounting factor). We
also use a baseline reward signal b, which is the
average of all rewards in a batch, to reduce the
variance in the updates .
When only training the dialogue policy π using
this signal, updates are given by (details in Appendix C):
∇θJ(θ) = Eπ
∇θ log πθ(ak)
For end-to-end training we need to update both
the dialogue policy and the belief trackers using
the reinforcement signal, and we can view the retrieval as another policy µθ (see Appendix C). The
updates are given by:
∇θJ(θ) =Ea∼π,I∼µ
h ∇θ log µθ(I)+
∇θ log πθ(ah)
In the case of end-to-end learning, we found that
for a moderately sized KB, the agent almost always fails if starting from random initialization.
In this case, credit assignment is difﬁcult for the
agent, since it does not know whether the failure
is due to an incorrect sequence of actions or incorrect set of results from the KB. Hence, at the
beginning of training we have an Imitation Learning (IL) phase where the belief trackers and policy network are trained to mimic the hand-crafted
agents. Assume that ˆpt
j are the belief states
from a rule-based agent, and ˆat its action at turn t.
Then the loss function for imitation learning is:
j(θ))+H(ˆqt
j(θ))−log πθ(ˆat)
D(p||q) and H(p, q) denote the KL divergence
and cross-entropy between p and q respectively.
The expectations are estimated using a minibatch of dialogues of size B.
For RL we use
RMSProp and for IL we use
vanilla SGD updates to train the parameters θ.
Experiments and Results
Previous work in KB-based QA has focused on
single-turn interactions and is not directly comparable to the present study. Instead we compare different versions of the KB-InfoBot described above
to test our claims.
KB-InfoBot versions
We have described two belief trackers – (A) Hand-
Crafted and (B) Neural, and two dialogue policies
– (C) Hand-Crafted and (D) Neural.
Rule agents use the hand-crafted belief trackers and hand-crafted policy (A+C). RL agents use
the hand-crafted belief trackers and the neural policy (A+D). We compare three variants of both sets
of agents, which differ only in the inputs to the
dialogue policy. The No-KB version only takes
entropy H(ˆpt
j) of each of the slot distributions.
The Hard-KB version performs a hard-KB lookup
and selects the next action based on the entropy of
the slots over retrieved results. This is the same
approach as in Wen et al. , except that
we take entropy instead of summing probabilities.
The Soft-KB version takes summary statistics of
the slots and KB posterior described in Section 4.
At the end of the dialogue, all versions inform the
user with the top results from the KB posterior pt
hence the difference only lies in the policy for action selection. Lastly, the E2E agent uses the neural belief tracker and the neural policy (B+D), with
a Soft-KB lookup. For the RL agents, we also append ˆqt
j and a one-hot encoding of the previous
maxj |V j|
Table 1: Movies-KB statistics for four splits. Refer to Section 3.2 for description of columns.
agent action to the policy network input. Hyperparameter details for the agents are provided in Appendix D.
User Simulator
Training reinforcement learners is challenging because they need an environment to operate in. In
the dialogue community it is common to use simulated users for this purpose .
In this work we adapt the publicly-available user
simulator presented in Li et al. to follow a simple agenda while interacting with the
KB-InfoBot, as well as produce natural language
utterances .
Details about the simulator are included in Appendix E. During training, the simulated user also provides a reward signal at the
end of each dialogue. The dialogue is a success
if the user target is in top R = 5 results returned by the agent; and the reward is computed
as max(0, 2(1 −(r −1)/R)), where r is the actual rank of the target. For a failed dialogue the
agent receives a reward of −1, and at each turn it
receives a reward of −0.1 to encourage short sessions4. The maximum length of a dialogue is 10
turns beyond which it is deemed a failure.
We use a movie-centric KB constructed using the
IMDBPy5 package. We constructed four different splits of the dataset, with increasing number of
entities, whose statistics are given in Table 1. The
original KB was modiﬁed to reduce the number
of actors and directors in order to make the task
more challenging6. We randomly remove 20% of
the values from the agent’s copy of the KB to simulate a scenario where the KB may be incomplete.
The user, however, may still know these values.
4A turn consists of one user action and one agent action.
5 
6We restricted the vocabulary to the ﬁrst few unique values of these slots and replaced all other values with a random
value from this set.
X-Large KB
Table 2: Performance comparison. Average (±std error) for 5000 runs after choosing the best model
during training. T: Average number of turns. S: Success rate. R: Average reward.
Simulated User Evaluation
We compare each of the discussed versions along
three metrics: the average rewards obtained (R),
success rate (S) (where success is deﬁned as providing the user target among top R results), and
the average number of turns per dialogue (T). For
the RL and E2E agents, during training we ﬁx the
model every 100 updates and run 2000 simulations
with greedy action selection to evaluate its performance. Then after training we select the model
with the highest average reward and run a further
5000 simulations and report the performance in
Table 2. For reference we also show the performance of an agent which receives perfect information about the user target without any errors, and
selects actions based on the entropy of the slots
(Max). This can be considered as an upper bound
on the performance of any agent .
In each case the Soft-KB versions achieve the
highest average reward, which is the metric all
agents optimize. In general, the trade-off between
minimizing average turns and maximizing success
rate can be controlled by changing the reward signal. Note that, except the E2E version, all versions
share the same belief trackers, but by re-asking
values of some slots they can have different posteriors pt
T to inform the results. This shows that
having full information about the current state of
beliefs over the KB helps the Soft-KB agent discover better policies. Further, reinforcement learning helps discover better policies than the handcrafted rule-based agents, and we see a higher reward for RL agents compared to Rule ones. This is
due to the noisy natural language inputs; with perfect information the rule-based strategy is optimal.
Interestingly, the RL-Hard agent has the minimum
number of turns in 2 out of the 4 settings, at the
cost of a lower success rate and average reward.
This agent does not receive any information about
the uncertainty in semantic parsing, and it tends to
Success Rate
Figure 3: Performance of KB-InfoBot versions
when tested against real users. Left: Success rate,
with the number of test dialogues indicated on
each bar, and the p-values from a two-sided permutation test. Right: Distribution of the number
of turns in each dialogue (differences in mean are
signiﬁcant with p < 0.01).
inform as soon as the number of retrieved results
becomes small, even if they are incorrect.
Among the Soft-KB agents,
we see that
E2E>RL>Rule, except for the X-Large KB. For
E2E, the action space grows exponentially with
the size of the KB, and hence credit assignment
gets more difﬁcult. Future work should focus on
improving the E2E agent in this setting. The dif-
ﬁculty of a KB-split depends on number of entities it has, as well as the number of unique values
for each slot (more unique values make the problem easier). Hence we see that both the “Small”
and “X-Large” settings lead to lower reward for
the agents, since maxj |V j|
is small for them.
Human Evaluation
We further evaluate the KB-InfoBot versions
trained using the simulator against real subjects,
recruited from the author’s afﬁliations.
session, in a typed interaction, the subject was ﬁrst
presented with a target movie from the “Medium”
KB-split along with a subset of its associated slot-
Rank Dialogue
Rank Dialogue
can i get a movie directed by maiellaro
find a movie directed by hemecker
peter greene acted in a family comedy - what was it?
request actor
request actor
request actor
i dont know
request mpaa_rating
request mpaa_rating
request mpaa_rating
not sure about that
i dont know
i don't know that
request critic_rating
request critic_rating
request critic_rating
i don't remember
the critics rated it as 6.5
request genre
request critic_rating
i think it's a crime movie
request critic_rating
Figure 4: Sample dialogues between users and the KB-InfoBot (RL-Soft version). Each turn begins
with a user utterance followed by the agent response. Rank denotes the rank of the target movie in the
KB-posterior after each turn.
values from the KB. To simulate the scenario
where end-users may not know slot values correctly, the subjects in our evaluation were presented multiple values for the slots from which
they could choose any one while interacting with
the agent. Subjects were asked to initiate the conversation by specifying some of these values, and
respond to the agent’s subsequent requests, all in
natural language. We test RL-Hard and the three
Soft-KB agents in this study, and in each session
one of the agents was picked at random for testing. In total, we collected 433 dialogues, around
20 per subject. Figure 3 shows a comparison of
these agents in terms of success rate and number of
turns, and Figure 4 shows some sample dialogues
from the user interactions with RL-Soft.
In comparing Hard-KB versus Soft-KB lookup
methods we see that both Rule-Soft and RL-Soft
agents achieve a higher success rate than RL-Hard,
while E2E-Soft does comparably. They do so in an
increased number of average turns, but achieve a
higher average reward as well. Between RL-Soft
and Rule-Soft agents, the success rate is similar,
however the RL agent achieves that rate in a lower
number of turns on average. RL-Soft achieves a
success rate of 74% on the human evaluation and
80% against the simulated user, indicating minimal overﬁtting. However, all agents take a higher
number of turns against real users as compared to
the simulator, due to the noisier inputs.
The E2E gets the highest success rate against
the simulator, however, when tested against real
users it performs poorly with a lower success
rate and a higher number of turns. Since it has
more trainable components, this agent is also most
prone to overﬁtting. In particular, the vocabulary
of the simulator it is trained against is quite limited (V n = 3078), and hence when real users
NLG Temperature
Average Reward
Figure 5: Average rewards against simulator as
temperature of softmax in NLG output is increased. Higher temperature leads to more noise
in output. Average over 5000 simulations after selecting the best model during training.
provided inputs outside this vocabulary, it performed poorly. In the future we plan to ﬁx this
issue by employing a better architecture for the
language understanding and belief tracker components Hakkani-T¨ur et al. ; Liu and Lane
 ; Chen et al. , as well as by pretraining on separate data.
While its generalization performance is poor,
the E2E system also exhibits the strongest learning capability. In Figure 5, we compare how different agents perform against the simulator as the
temperature of the output softmax in its NLG is increased. A higher temperature means a more uniform output distribution, which leads to generic
simulator responses irrelevant to the agent questions. This is a simple way of introducing noise
in the utterances. The performance of all agents
drops as the temperature is increased, but less
so for the E2E agent, which can adapt its belief
tracker to the inputs it receives. Such adaptation
is key to the personalization of dialogue agents,
which motivates us to introduce the E2E agent.
Conclusions and Discussion
This work is aimed at facilitating the move towards end-to-end trainable dialogue agents for information access.
We propose a differentiable
probabilistic framework for querying a database
given the agent’s beliefs over its ﬁelds (or slots).
We show that such a framework allows the downstream reinforcement learner to discover better dialogue policies by providing it more information.
We also present an E2E agent for the task, which
demonstrates a strong learning capacity in simulations but suffers from overﬁtting when tested on
real users.
Given these results, we propose the
following deployment strategy that allows a dialogue system to be tailored to speciﬁc users via
learning from agent-user interactions. The system
could start off with an RL-Soft agent (which gives
good performance out-of-the-box). As the user interacts with this agent, the collected data can be
used to train the E2E agent, which has a strong
learning capability. Gradually, as more experience
is collected, the system can switch from RL-Soft
to the personalized E2E agent. Effective implementation of this, however, requires the E2E agent
to learn quickly and this is the research direction
we plan to focus on in the future.
Acknowledgements
We would like to thank Dilek Hakkani-T¨ur and reviewers for their insightful comments on the paper. We would also like to acknowledge the volunteers from Carnegie Mellon University and Microsoft Research for helping us with the human
evaluation. Yun-Nung Chen is supported by the
Ministry of Science and Technology of Taiwan under the contract number 105-2218-E-002-033, Institute for Information Industry, and MediaTek.