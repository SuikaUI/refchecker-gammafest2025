Measuring Information Transfer
Thomas Schreiber
Max Planck Institute for Physics of Complex Systems, N¨othnitzer Str. 38, 01187 Dresden, Germany
 
An information theoretic measure is derived that quantiﬁes
the statistical coherence between systems evolving in time.
The standard time delayed mutual information fails to distinguish information that is actually exchanged from shared
information due to common history and input signals. In our
new approach, these inﬂuences are excluded by appropriate
conditioning of transition probabilities. The resulting transfer
entropy is able to distinguish driving and responding elements
and to detect asymmetry in the coupling of subsystems.
The time evolution of a system may be called irregular if it generates information at a non-zero rate.
stochastic or deterministically chaotic systems, this is
quantiﬁed by the entropy.
For a system consisting of
more than one component, important information on its
structure can be obtained by measuring to which extent the individual components contribute to information
production and at what rate they exchange information
among each other.
This paper proposes a method to
answer the latter question on the basis of time series observations.
Many authors have used mutual information to
quantify the overlap of the information content of two
(sub-) systems. Unfortunately, mutual information neither contains dynamical nor directional information. Introducing a time delay in one of the observations is an
important, if somewhat arbitrary, improvement in this
respect, but still does not explicitly distinguish information that is actually exchanged from that due to the response to a common input signal or history.
The purpose of this paper is to motivate and derive an
alternative information theoretic measure, to be called
transfer entropy, that shares some of the desired properties of mutual information but takes the dynamics of
information transport into account. With minimal assumptions about the dynamics of the system and the
nature of their coupling one will be able to quantify the
exchange of information between two systems, separately
for both directions, and, if desired, conditional to common input signals.
This work may be seen in the context of a considerable number of recently proposed measures for the
nonlinear coherence of signals, used to study generalized
synchronization phenomena in many contects, most notably in physiological systems. While these measures are
often very powerful for a speciﬁc set of applications, it
is also important to aim at an understanding of the underlying theoretical concepts. In the generic case that
neither of the systems, nor their coupling may be assumed to be deterministic, information theory seems to
be an appropriate starting point.
Let us brieﬂy recall the most basic concepts of information theory. The average number of bits needed to
optimally encode independent draws of the discrete variable I following a probability distribution p(i) is given
by the Shannon entropy 
p(i) log2 p(i)
where the sum extends over all states i the process can
assume. The base of the logarithm only determines the
units used for measuring information and will be dropped
henceforth.
In order to construct an optimal encoding that uses
just as many bits as given by the entropy, it is necessary to know the probability distribution p(i).
excess number of bits that will be coded if a diﬀerent distribution q(i) is used instead of p(i) is given by
the Kullback entropy KI = P
i p(i) log p(i)/q(i).
We will later also need the Kullback entropy for conditional probabilities p(i|j). For a single state j we have
i p(i|j) log p(i|j)/q(i|j). Summation over j with
respect to p(j) yields
p(i, j) log p(i|j)
The mutual information of two processes I and J
with joint probability pIJ(i, j) can be seen as the excess amount of code produced by erroneously assuming
that the two systems are independent, i.e.
qIJ(i, j) = pI(i) pJ(j) instead of pIJ(i, j).
The corresponding Kullback entropy is
p(i, j) log
which is the well known formula for the mutual information.
Here and in the following, we omitted the
summation index and the subscript of the probabilities
specifying the process. This derivation shows that mutual information is a natural way to quantify the deviation from independence of two processes.
MIJ = HI +HJ −HIJ ≥0. Note that MIJ is symmetric
under the exchange of I and J and therefore does not
contain any directional sense.
A related, non-symmetric quantity is the conditional
entropy HI|J = −P p(i, j) log p(i|j) = HIJ −HJ. However, since HI|J −HJ|I = HI −HJ, it is non-symmetric
only due to the diﬀerent individual entropies and not due
to information ﬂow. Mutual information can be given a
directional sense in a somwhat ad-hoc way by introducing a time lag in either one of the variables and compute
p(in, jn−τ) log p(in, jn−τ)
p(i) p(j) .
As we will see below, considering the two systems at different times occurs naturally as soon as transition probabilities are introduced. This will yield a more justiﬁed
approach to measuring information transfer that explicitly incorporates directional, dynamical structure.
One can incorporate dynamical structure by studying
transition probabilities rather than static probabilities.
Consider a system that may be approximated by a stationary Markov process of order k, that is, the conditional probability to ﬁnd I in state in+1 at time n + 1
observes p(in+1|in, . . . , in−k+1) = p(in+1|in, . . . , in−k).
Henceforth we will use the shorthand notation i(k)
(in, . . . , in−k+1) for words of length k, or k dimensional
delay embedding vectors.
The average number of bits needed to encode one additional state of the system if all previous states are known
is given by the entropy rate
p(in+1, i(k)
n ) log p(in+1|i(k)
Since p(in+1|i(k)
n ) = p(i(k+1)
n+1 )/p(i(k)
n ), this is just the difference between the Shannon entropies of the processes
given by k + 1 and k dimensional delay vectors constructed from I: hI = HI(k+1) −HI(k).
If I is obtained by coarse graining a continuous system X at resolution ǫ, the entropy HX(ǫ) and entropy
rate hX(ǫ) will depend on the partitioning and in general
diverge like log ǫ when ǫ →0. However, for the special
case of a deterministic dynamical system, limǫ→0 hX(ǫ) =
hKS may exist and is then called the Kolmogorov–Sinai
entropy. (For non-Markov systems, also the limit k →∞
needs to be taken.)
Confusingly, the opposite is true
for the mutual information. For generic noisy interdependence, limǫ→0 MXY (ǫ) is ﬁnite and independent of
the partition, but for deterministically coupled processes,
MXY (ǫ) will diverge as ǫ →0.
The Shannon entropy and its generalization, the mutual information, are properties of the static probability
distributions while the dynamics of the processes is contained in the transition probabilities. For the study of
the dynamics of shared information between processes
it is therefore desirable to generalize the entropy rate,
rather than Shannon entropy, to more than one system.
In the next section I will propose such generalizations, in
particular one that is non-symmetric under the exchange
of the two processes.
The most straightforward way to construct a mutual
information rate by generalizing hI to two processes
(I, J) is again by measuring the deviation from independence. The corresponding Kullback entropy is sometimes
called transinformation and is still symmetric under the
exchange of I and J. It is therefore preferable to measuring the deviation from the generalized Markov property
p(in+1|i(k)
n ) = p(in+1|i(k)
In the absence of information ﬂow from J to I, the state
of J has no inﬂuence on the transition probabilities on
system I. The incorrectness of this assumption can again
be quantiﬁed by a Kullback entropy (1) by which we
deﬁne the transfer entropy:
p(in+1, i(k)
n ) log p(in+1|i(k)
p(in+1|i(k)
This is the central concept of this paper. The most natural choices for l are l = k or l = 1. Usually, the latter
is preferable for computational reasons. TJ→I is now explicitly non-symmetric since it measures the degree of
dependence of I on J and not vice versa.
For coarse grained states (I, J) of continuous systems
(X, Y ), the limit limǫ→0 TY →X(ǫ) is ﬁnite and independent of the partition, except for the case of deterministic
coupling, when TY →X(ǫ) diverges as ǫ →0. In this respect, transfer entropy behaves like mutual information.
If computationally feasible, the inﬂuence of a known common driving force Z may be excluded by conditioning the
probabilities under the logarithm to zn as well.
For numerical and practical applications, the limit ǫ →
0 is not obtainable and has to be replaced appropriately.
Either one can study transfer entropy as a function of the
resolution, or one can ﬁx a resolution for the scope of a
study. Furthermore, there are several methods of coarse
graining and a partition consisting of a ﬁxed mesh of
boxes is not always the best choice. Fixed boxes are only
suitable in cases where data can be produced with little
eﬀort and small statistical errors at reasonable speed of
computation are desired.
For time series applications, an alternative implementation using generalized correlation integrals is preferable. Mutual information and redundancies have been
generalized for their estimation by order q correlation integrals . It is possible to follow the same arguments
in generalizing transfer entropy. However, for the computationally most attractive case q = 2, we would have
to give up positivity of TI→J. Instead, we propose an
implementation of the deﬁnition (4) where the probability measure p(in+1, i(k)
n ) is realized by a sum over all
available realizations of (xn+1, x(k)
n ) in a time series.
The transition probabilities are expressed by joint probabilities and then obtained by kernel estimation, e.g.
transfer entropy [bits]
Transfer entropy TIm−1→Im for the coupling direction as a function of the coupling strength ǫ in a tent map
lattice (binary partition). Errorbars: error of the mean of 10
runs of 100000 iterates. Line: theoretical curve α2ǫ2/ ln(2)
with ﬁtted α = 0.77.
ˆpr(xn+1, xn, yn) = 1
xn+1 −xn′+1
We use the step kernel Θ(x > 0) = 1; Θ(x ≤0) = 0.
The norm | · | can be simply the maximum distance but
other norms and kernels can be considered. In particular,
diﬀerent overall scales of X and Y can be accounted for
by using appropriate weights. Similarly to standard dimension and entropy calculations, fast neighbour search
strategies are advisable for all but the smallest data
Dynamically correlated pairs should be excluded
Since these technical issues are the same as
in many nonlinear time series methods, the reader is referred to the discussion in the literature .
In order to demonstrate the use of transfer entropy, let
us study three examples, two spatio-temporal systems
and a bi-variate physiological time series. In a one dimensional lattice of unidirectionally coupled maps
n+1 = f(ǫxm−1
+ (1 −ǫ)xm
information can be transported only in the direction of
increasing m. One of the simplest cases is given by the
tent map, f(x < 0.5) = 2x; f(x ≥0.5) = 2 −2x. Let
us study coarse grained states Im with im
n deﬁned by a
partition at x0 = 0.5. At zero coupling, all static and
transfer probabilities are equal to 1/2, M(τ) = 0 for
all values of τ, and also TIm−1→Im = TIm→Im−1 = 0.
For nonzero coupling, we still have TIm→Im−1 = 0, but
TIm−1→Im becomes positive. For small coupling, it can
be assumed that the invariant density at a single site
is essentially unchanged whence the transition probabilities p(Im
) are changed by an amount proportional to ǫ. In particular, p(0|0, 0), p(0|1, 1), p(1|0, 1), and
p(1|1, 0) are increased by a factor 1 + αǫ with α = O(1).
All others are decreased by that amount.
Evaluating
(4) in lowest order of ǫ with k = l = 1, we obtain
TIm−1→Im = α2ǫ2/ ln(2)+O(ǫ4). For this particular case,
the changes in p(im
) exactly cancel out and the
information [bits]
Transfer entropies TXm−1→Xm and TXm→Xm−1
information MX1,X2(τ = 1) and MX2,X1(τ = 1) (dashed lines)
as functions of the coupling strength ǫ for a unidirectionally
coupled Ulam lattice. For both quantities, the upper line denotes the direction Xm−1 →Xm while the lower line shows
Xm+1 →Xm. Although the lattice undergoes a sequence of
bifurcations, the transfer entropy T clearly reﬂects the unidirectional character of the coupling. It also consistently outperforms the time delayed mutual information in this respect.
See text for further details.
mutual information is zero. Figure 1 shows a numerical
veriﬁcation of these results for a spatially periodic lattice
of 100 maps. Averages of 10 runs of 105 iterates after 105
transients are shown.
The transfer entropy TIm→Im−1
and both directions of M(τ = 1) were found consistent
with zero and are therefore not shown.
The situation is more complicated for the Ulam map
f(x) = 2−x2 and non-small coupling. For each coupling,
a bi-variate time series was generated using a lattice
of 100 points (random initial conditions) and recording
10000 iterates of x1
n after 105 steps of transients.
Correlation sums at r = 0.2 were used to compute mutual information in both directions, MX1,X2(τ = 1) and
MX2,X1(τ = 1), as well as transfer entropies TX1→X2 and
TX2→X1 with k = l = 1. Neighbors closer in time than
100 iterates were excluded from the kernel estimation.
Figure 2 shows M and T as functions of the coupling
strength. Both M and T are able to detect the anisotropy
since the information is consistently larger in the positive
direction. The lattice undergoes a number of bifurcations
when the coupling is changed.
Around ǫ = 0.18, the
asymptotic state is of temporal and spatial period two.
For this case, the mutual information is found to be 1 bit.
This is correct although information is neither produced
nor exchanged and reﬂects the static correlation between
the sites. The transfer entropy ﬁnds a zero rate of information transport, as desired.
Around this pariodic
window, the mutual information is non-zero in both directions and the signature of the unidirectional coupling
is less pronounced. Around ǫ = 0.82, the lattice settles
to a (spatially inhomogenious) ﬁxed point state. Here
both measures correctly show zero information transfer.
The most important ﬁnding, however, is that the trans-
heart rate
breath rate
Bi-variate time series of the breath rate (upper)
and instantaneous heart rate (lower) of a sleeping human. The
data is sampled at 2 Hz. Both traces have been normalized
to zero mean and unit variance.
Transfer entropies T(heart →breath) (solid line),
T(breath →heart) (dotted line), and time delayed mutual information M(τ = 0.5 s) (directions indistinguishable, dashed
lines) for the physiological time series shown in Fig. 3.
fer entropy for the negative direction remains consistent
with zero for all couplings, reﬂecting the causality in the
As a last example, take a bi-variate time series (see
Fig. 3) of the breath rate and instantaneous heart rate of
a sleeping human suﬀering from sleep apnea . Figure 4 shows that while time delayed mutual
information is almost symmetric between both series, the
transfer entropy indicates a stronger ﬂow of information
from the heart rate to the breath rate than vice versa
over a signiﬁcant range of length scales r. Note that for
small r, the curves deﬂect down to zero due to the ﬁnite
sample size. This result is consistent with the observation that the patient breathes in bursts which seem to
occur whenever the heart rate crosses some threshold.
Certainly, both signals could instead be responding to a
common external trigger.
In conclusion, the new transfer entropy is able to detect the directed exchange of information between two
Unlike mutual information, it is designed to
ignore static correlations due to the common history or
common input signals. Most prominent applications include multivariate analysis of time series and the study
of spatially extended systems.
Several authors have proposed to use time delayed
mutual information M(∆l, τ) as a function of spatial distance ∆l and temporal delay τ to deﬁne a velocity of
information transport in spatio-temporal systems.
Often, one ﬁnds that M(∆l, τ) for ﬁxed ∆i reaches a local
maximum at some lag τ ∗. Hence a velocity can be de-
ﬁned by the ratio ∆i/τ∗, in particular if that ratio is
fairly constant over the resolvable range of values for ∆i.
This reasoning has been challenged by giving an example where the above interpretation implies super-luminar
communication. In fact, much of the common information is due to the common history that allows the lattice to partially synchronize. Preliminary results indicate
that appropriate conditioning for the common history by
replacing time delayed mutual information by a variant
of Eq.(4) resolves this aparent paradox. However, conditioning with respect to a large number of variables poses
immense numerical problems whence this study will be
concluded at a later time.
Part of this work has been supported by the SFB 237
of the Deutsche Forschungsgemeinschaft.
 C. E. Shannon and W. Weaver, “The Mathematical Theory of Information”, University of Illinois Press, Urbana,
IL .
 J. Arnhold, P. Grassberger, K. Lehnertz, and C. E. Elger,
Physica (Amsterdam) 134D, 419 ; M. G. Rosenblum, A. S. Pikovsky, and J. Kurths, Phys. Rev. Lett.
76, 1804 ; M. Le Van Quyen, C. Adam, M. Baulac,
J. Martinerie, and F. J. Varela, Brain Research 792, 24
 S. Kullback, “Information theory and statistics”, Wiley,
New York .
 P. Grassberger, T. Schreiber, and C. Schaﬀrath, Int. J.
Bifurcat. Chaos 1, 521 ; B. Pompe, J. Stat. Phys.
73, 587 ; D. Prichard and J. Theiler, Physica D 84,
476 .
 H. Kantz and T. Schreiber, “Nonlinear time series analysis”, Cambridge University Press, Cambridge MA .
 D. R. Rigney, A. L. Goldberger, W. Ocasio, Y. Ichimaru,
G. B. Moody, and R. Mark, Multi-channel physiological
data: Description and analysis, in A. S. Weigend and N. A.
Gershenfeld, eds., “Time series prediction:
Forecasting
the future and understanding the past”, Santa Fe Institute Studies in the Science of Complexity, Proc. Vol. XV,
Addison-Wesley, Reading MA .
 K. Kaneko, Physica D 23, 436 ; J. A. Vastano and
H. L. Swinney, Phys. Rev. Lett. 60, 1773 .
 T. Schreiber, J. Phys. A 23, L393 .