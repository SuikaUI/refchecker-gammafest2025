IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
Trainable Nonlinear Reaction Diffusion: A
Flexible Framework for Fast and Effective
Image Restoration
Yunjin Chen and Thomas Pock
Abstract—Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a
ﬂexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By
embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with
time-dependent parameters (i.e., linear ﬁlters and inﬂuence functions). In contrast to previous nonlinear diffusion models, all the
parameters, including the ﬁlters and the inﬂuence functions, are simultaneously learned from training data through a loss based
approach. We call this approach TNRD – Trainable Nonlinear Reaction Diffusion. The TNRD approach is applicable for a variety of
image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative
applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained
nonlinear diffusion models largely beneﬁt from the training of the parameters and ﬁnally lead to the best reported performance on
common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take
only a small number of diffusion steps, thus are highly efﬁcient. Moreover, they are also well-suited for parallel computation on GPUs,
which makes the inference procedure extremely fast.
Index Terms—nonlinear reaction diffusion, loss speciﬁc training, image denoising, image super resolution, JPEG deblocking
INTRODUCTION
MAGE restoration is the process of estimating uncorrupted images from noisy or blurred ones. It is one of the most fundamental operations in image processing, video processing, and low-level
computer vision. For several decades, image restoration remains
an active research topic and hence new approaches are constantly
emerging. There exists a huge amount of literature addressing the
topic of image restoration problems, see for example for a
In recent years, the predominant approaches for image restoration are non-local methods based on patch modeling, for example,
image denoising with (i) Gaussian noise , , , ,
(ii) multiplicative noise , or (iii) Poisson noise , image
interpolation , image deconvolution , etc. Most state-ofthe-art techniques mainly concentrate on achieving utmost image
restoration quality, with little consideration on the computational
efﬁciency, e.g., , , , despite the fact that it is a critical
factor for real applications. However, there are a few exceptions.
For example, there are two notable exceptions for the task of Gaussian denoising, BM3D and the recently proposed Cascade of
Shrinkage Fields (CSF) model, which simultaneously offer
high efﬁciency and high image restoration quality.
It is well-known that BM3D is a highly engineered Gaussian
Y.J. Chen is with the Institute for Computer Graphics and Vision, Graz
University of Technology, 8010 Graz, Austria.
E-mail: chenyunjin 
T. Pock is with the Institute for Computer Graphics and Vision, Graz
University of Technology, 8010 Graz, Austria, as well as Digital Safety
& Security Department, AIT Austrian Institute of Technology GmbH, 1220
Vienna, Austria. E-mail: 
This work was supported by the Austrian Science Fund (FWF) under the
China Scholarship Council (CSC) Scholarship Program and the START
project BIVISION, No. Y729.
image denoising algorithm. It involves a block matching process,
which is challenging for parallel computation on GPUs, alluding
to the fact that it is not straightforward to accelerate BM3D
algorithm on parallel architectures. In contrast, the recently proposed CSF model offers high levels of parallelism, making it well
suited for GPU implementation, thus owning high computational
efﬁciency.
In this paper, we propose a ﬂexible learning framework to generate fast and effective models for a variety of image restoration
problems. Our approach is based on learning optimal nonlinear
reaction diffusion models. The learned models preserve the structural simplicity of these models and hence it is straightforward
to implement the corresponding algorithms on massive parallel
hardware such as GPUs.
Nonlinear diffusion for image restoration
Partial differential equation (PDEs) have become a standard approach for various problems in image processing. On the one hand
they come along with a sound mathematical framework that allow
to make clear statements about the existence and regularity of the
solutions. On the other hand, efﬁcient numerical algorithms have
been developed, that allow to compute the solution of PDEs in
very short time , . Although recent PDE approaches have
shown good performance for a number of image processing task,
they still fail to produce state-of-the-art quality for classical image
restoration tasks.
In the seminal work , Perona and Malik (PM) proposed a
nonlinear diffusion model, which is given as the following PDE
∂t = div(g(|∇u|)∇u)
 
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
where ∇is the gradient operator, t denotes the time, f is a initial
image. The function g is known as edge-stopping function 
or diffusivity function , and a typical g-function is given
by g(z) = 1/(1 + z2). The proposed PM diffusion model (1)
leads to a nonlinear anisotropic1 diffusion model which is able to
preserve and enhance image edges. Hence, it is well suited for a
number of image processing tasks such as image denoising and
segmentation.
Improvements from the side of PDEs
A ﬁrst variant of the PM model is the so-called biased anisotropic
diffusion (also known as reaction diffusion) proposed by Nordstr¨om , which introduces a bias term (forcing term) to free the
user from the difﬁculty of specifying an appropriate stopping time
for the PM diffusion process. This additional term reacts against
the strict smoothing effect of the pure PM diffusion, therefore
resulting in a nontrivial steady-state.
Subsequent works consider modiﬁcations of the diffusion or
the reaction term for the reaction diffusion model , ,
 , e.g., Acton et al. exploited a more complicated reaction
term to enhance oriented textures; proposed to replace the
ordinary diffusion term with a ﬂow equation based on mean
curvature. A notable work is the forward-backward diffusion
model proposed by Gilboa et al. , which incorporates explicit
inverse diffusion with negative diffusivity coefﬁcient by carefully
choosing the diffusivity function. The resulting diffusion processes
can adaptively switch between forward and backward diffusion
processes. In subsequent work , the theoretical framework for
discrete forward-and-backward diffusion ﬁltering has been investigated. Researchers also propose to exploit higher-order nonlinear
diffusion ﬁltering, which involves larger linear ﬁlters, e.g., fourthorder diffusion models , , . Meanwhile, theoretical
properties about the stability and local feature enhancement of
higher-order nonlinear diffusion ﬁltering are established in .
It should be noted that the above mentioned diffusion models
are handcrafted, including elaborate selections of the diffusivity
functions, optimal stopping times and proper reaction forces. It
is a generally difﬁcult task to design a good-performing PDE for
a speciﬁc image processing problem, as good insights into this
problem and a deep understanding of the behavior of the PDEs
are usually required. Therefore, an attempt to learn PDEs from
training data via an optimal control approach was made in ,
where the PDEs to be trained have the form of
∂t = κ(u) + a(t)⊤O(u)
Coefﬁcients a(t) are free parameters (i.e., combination weights)
to train. κ(u) is related to the TV regularization and O(u)
denotes a set of operators (invariants) over u, e.g., ∥∇u∥2
Improvements
statistics/regularization
As shown in , , , , there exist a strong connection between anisotropic diffusion models and variational models
1. Anisotropic diffusion in this paper is understood in the sense that the
smoothing induced by PDEs can be favored in some directions and prevented
in others. The diffusivity is not necessary to be a tensor. It should be noted that
this deﬁnition is different from Weickert’s terminology , where anisotropic
diffusion always involves a diffusion tensor, and the PM model is regarded as
an isotropic model.
adopting image priors derived from the statistics of natural images.
Let us consider the discrete version of the PM model (1), where
images are represented as column vectors, i.e., u ∈RN. The
discrete PM model is formulated as the following discrete PDE
with an explicit ﬁnite difference scheme
i Λ(ut)∇iut .= −
i φ(∇iut) ,
where matrices ∇x and ∇y ∈RN×N are ﬁnite difference approximation of the gradient operators in x-direction and y-direction,
respectively and ∆t denotes the time step. Λ(ut) ∈RN×N is
deﬁned as a diagonal matrix
Λ(ut) = diag
(∇xut)2p + (∇yut)2p
p=1,··· ,N ,
where function g is the edge-stopping function mentioned before. If ignoring the coupled relation between ∇xu and ∇yu,
the PM model can also be written in the form φ(∇iu) =
(φ(∇iu)1, · · · , φ(∇iu)N)⊤∈RN with function φ(z) = zg(z),
known as inﬂuence function or ﬂux function . In this paper,
we stick to this discrete and decoupled formulation, as it is the
starting point of our approach.
As shown in previous works, e.g., , , the diffusion step
(3) corresponds to a gradient descent step to minimize the energy
functional given as
ρ((ki ∗u)p) ,
the functions ρ (e.g., ρ(z) = log(1 + z2)) is the so-called
penalty function.2 It is worthwhile to mention that the matrixvector product ∇xu can be interpreted as a 2D convolution of
u with the linear ﬁlter kx = [−1, 1] (∇y corresponds to the
linear ﬁlter ky = [−1, 1]⊤). The energy functional (4) can be
also understood from the aspects of image statistics, image prior
and image regularization. As a consequence, a lot of efforts listed
below have been made to improve the capability of model (4).
a) More ﬁlters of larger kernel size were considered in , ,
 , , instead of relatively small kernel size, such as usually
used pair-wise kernels. The resulting regularization model leads
to the so-called ﬁelds of experts (FoE) image prior, which
works well for many image restoration problems.
b) Instead of hand-crafted ones with ﬁxed shape, more ﬂexible
penalty functions were exploited, and they were learned from
data , , , . Especially, as shown in that
those unusual penalties such as inverted penalties (i.e., ρ(z)
decreasing as a function of |z|) were found to be necessary.
c) In order accelerate the inference phase related to the model (4),
in , , it was proposed to truncate the gradient descent
procedure to ﬁxed iterations/stages, and then train this truncated
optimization algorithm based on the FoE prior model.
d) In order to further increase the ﬂexibility of multi-stage models,
Schmidt and Roth considered varying parameters per stage.
Motivations and Contributions
In this paper we concentrate on nonlinear diffusion process due
to its high efﬁciency. Taking into consideration the improvements
mentioned in Sec. 1.1.2, we propose a trainable nonlinear diffusion
2. Note that ρ′(z) = φ(z).
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
model with (1) ﬁxed iterations (also referred to as stages), (2)
more ﬁlters of larger kernel size, (3) ﬂexible penalties in arbitrary
shapes, (4) varying parameters for each iteration, i.e., time varying
linear ﬁlters and penalties. Then all the parameters (i.e., linear
ﬁlters and penalties) in the proposed model are simultaneously
trained from training data in a supervised way. The proposed
approach results in a novel learning framework to train effective
image diffusion models. It turns out that the trained diffusion
processes leads to state-of-the-art performance, while preserve
the property of high efﬁciency of diffusion based approaches.
In summary, our proposed nonlinear diffusion process offers the
following advantages:
1) It is conceptually simple as it is merely a standard nonlinear
diffusion model with trained ﬁlters and inﬂuence functions;
2) It has broad applicability to a variety of image restoration
problems. In principle, all the diffusion based models can be
revisited with appropriate training;
3) It yields excellent results for several tasks in image restoration, including Gaussian image denoising,single image super
resolution and JPEG deblocking;
4) It is highly computationally efﬁcient, and well suited for
parallel computation on GPUs.
A shorter paper has been presented as a conference version
 . In this paper, we incorporate additional contents listed as
1) We investigate more details of the training phase, such as the
inﬂuence of (a) initialization, (b) the model capacity and (c)
the number of training samples;
2) We consider more detailed analysis of trained models, such
as how the trained models generate the patterns;
3) We exploit an additional application of single image super
resolution to further illustrate the potential breadth of our
proposed learning framework.
PROPOSED REACTION DIFFUSION PROCESS
In this section, we ﬁrst describe our learning based reaction
diffusion model for image restoration, and then we show the relations between the proposed model and existing image restoration
Proposed nonlinear diffusion model
The fundamental idea of our proposed learning based reaction
diffusion model is described in Sec. 1.2. In addition, we incorporate a reaction term in order to apply our model for different
image processing problems, as shown later. As a consequence,
our proposed nonlinear reaction diffusion model is formulated as
diffusion term
−ψt(ut−1, f)
reaction term
where Ki ∈RN×N is a highly sparse matrix, implemented as 2D
convolution of the image u with the ﬁlter kernel ki, i.e., Kiu ⇔
ki ∗u, Ki is a set of linear ﬁlters and Nk is the number of ﬁlters.
In practice, we set ∆t = 1, as we can freely scale the functions
i and ψt on the right hand side.
Note that our proposed diffusion process is truncated after
a few stages, usually less than 10. Moreover, the linear ﬁlters
and inﬂuence functions are adjustable and vary across stages. As
our proposed approach is inspired by nonlinear reaction diffusion
model but with trainable ﬁlters and inﬂuence functions, we coin
our method Trainable Nonlinear Reaction Diffusion (TNRD).
In practice, we train the proposed nonlinear diffusion model
(5) for speciﬁc image restoration problem by exploiting application speciﬁc reaction terms ψ(u). For classical image restoration
problems, such as Gaussian denoising, image deblurring, image
super resolution and image inpainting, we can set the reaction
term to be the gradient of a data term, i.e. ψ(u) = ∇uD(u).
For example, if we choose Dt(u, f) =
2 ∥Au −f∥2
have ψt(u) = λtA⊤(Au −f), where f is the degraded input
image, A is the associated linear operator, and λt is related to the
strength of the reaction term. In the case of Gaussian denoising,
A is the identity matrix; for image super resolution, A is related
to the down sampling operation and for image deconvolution, A
corresponds to the linear blur kernel.
A more general formulation of the proposed diffusion model
Note that the data term D(u) related to (5) should be differentiable. In order to handle the problems involving a nondifferentiable data term, e.g., the JPEG deblocking problem investigated in Section 6, we consider a more general form of our
proposed diffusion model as follows
ut = ProxGt
iut−1) + ψt(ut−1, f)
where ProxGt(ˆu) is the proximal mapping operation related
to the non-differentiable function Gt, given as
ProxGt(ˆu) = min
Related image restoration models
As mentioned in Sec. 1.1.2, there exist natural connection between
anisotropic diffusion and image regularization based energy functional. Therefore, Eq. (6) can be interpreted as a forward-backward
step3 at ut−1 for the energy functional given by
Et(u, f) =
i(u) + Dt(u, f) + Gt(u, f) ,
iu)p) are the regularizers. Since
the parameters {Kt
i} vary across the stages, (7) is a dynamic
energy functional, which changes at each iteration.
i} keep the same across stages, the functional (7)
with G = 0 corresponds to the FoE prior regularized variational
model for image restoration , , . In our work, we do
not exactly solve this minimization problem anymore. In contrast,
we run the gradient descent step for several iterations, and each
gradient descent step is optimized by training. More importantly,
we are allowed to investigate more generalized penalties.
To the best of our knowledge, Zhu and Mumford were
the ﬁrst to consider learning reaction diffusion models from data.
The linear ﬁlters appearing in the prior were chosen (not fully
trained) from a general ﬁlter bank by minimizing the entropy of
probability distributions of natural images. The associated penalty
functions were learned based on the maximum entropy principle.
3. The forward step is a gradient descent step w.r.t the function D and the
backward step is the proximal operation w.r.t the function G.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
λ1 AT (Au0 – f)
Nonlinearity
Convolution
Convolution
Reaction force
λ2 AT (Au1 – f)
Nonlinearity
λT AT (AuT-1 – f)
Nonlinearity
Gradient Descent Step/Diffusion Step
Fig. 1. The architecture of our proposed diffusion model with a reaction force ψt = λtA⊤(Au −f) and G = 0. It is represented as a feed-forward
network. Note that the additional convolution step with the rotated kernels ¯ki (cf. Equ. 15) does not appear in conventional feed-forward CNs.
However, our proposed diffusion model is a multi-stage diffusion
process with multiple image priors, where the ﬁlters and penalties
are fully trained from clean/degraded pairs.
Some more works have also been dedicated to train the
penalties in a diffusion model. In , the authors trained the
penalty functions in the way that they ﬁrst computed the image
statistics appropriate to certain chosen ﬁlters (e.g., horizontal and
vertical image derivatives), and then considered mixture models
of ﬁxed shape having a peak near zero and two heavy tails in
both sides to ﬁt to image statistics. Analogously, in , potential
functions of ﬁxed shape are chosen to resemble zero mean ﬁlter
responses.
In very recent work , Schmidt and Roth exploited an
additive form of half-quadratic optimization to solve problem
(7). The resulting model shares similarities with classical wavelet
shrinkage and hence it is termed “cascade of shrinkage ﬁelds”
(CSF). The CSF model relies on the assumption that the data
term in (7) is quadratic and the operator A can be interpreted as
a convolution operation, such that the corresponding subproblem
can be solved in closed-form using the discrete Fourier transform
(DFT). However, our proposed diffusion model does not have this
restriction on the data term. In principle, any smooth data term is
appropriate. Moreover, as shown in the following sections, we can
even handle the case of non-smooth data terms.
As already mentioned, , also proposed to train an
optimized gradient descent algorithm for the energy functional
similar to (7). However, their model is much more constrained,
since they exploited the same ﬁlters for each gradient descent
step and more importantly, they make use of a hand-selected
inﬂuence function. This clearly restricts the model capability, as
demonstrated in Sec. 4.
Comparing our model to the model of (cf. (2)), one can
see that this approach learns only a linear model based on predeﬁned image invariants. Therefore, this model can be interpreted
as a simpliﬁed version of our model (5) with ﬁxed linear ﬁlters and
inﬂuence functions, and only the weight of each term is optimized.
The proposed diffusion model also bears an interesting link
to convolutional networks (CNs) applied to image restoration
problems in . One can see that each iteration (stage) of our
proposed diffusion process involves convolution operations with a
set of linear ﬁlters, and thus it can be treated as a convolutional
network. The architecture of our proposed diffusion model is
shown in Figure 1, where it is represented as a common feedforward network. We refer to this network in the following as
diffusion network.
λ AT (Au – f )
Nonlinearity
Convolution
Convolution
Reaction force
Fig. 2. Our diffusion network can also be interpreted as a CN with a
feedback step, which makes it different from conventional feed-forward
networks. Due to the feedback step, it can be categorized into recurrent
networks .
However, we can introduce a feedback step to explicitly
illustrate the special architecture of our diffusion network that
we subtract “something” from the input image. Therefore, our
diffusion model can be represented in a more compact way in
Figure 2, where one can see that the structure of our CN model
is different from conventional feed-forward networks. Due to this
feedback step, it can be categorized into recurrent networks .
It should be noted that the nonlinearity (i.e., inﬂuence functions
in the context of nonlinear diffusion) in our proposed network are
trainable. However, conventional CNs make use of ﬁxed activation
function, e.g., the ReLU function or sigmoid functions .
LEARNING FRAMEWORK
We train our diffusion networks in a supervised manner, namely
we ﬁrstly prepare the input/output pairs for a certain image processing task, and then exploit a loss minimization scheme to learn
the model parameters Θt for each stage t of the diffusion process.
The training dataset consists of S training samples {f s, us
where f s is a noisy observation and us
gt is the corresponding
ground truth clean image. The model parameters Θt of each stage
include the parameters of (1) the reaction force weight λ, (2) linear
ﬁlters and (3) inﬂuence functions, i.e., Θt = {λt, φt
Overall training model
In the supervised manner, a training cost function is required to
measure the difference between the output of the diffusion network
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
and the ground-truth image. As our goal is to train a diffusion
network with T stages, the cost function is formulated as
L(Θ1,··· ,T ) =
where uT is the output of the ﬁnal stage T. In our work we exploit
the usual quadratic loss function4, deﬁned as
As a consequence, the training task is generally formulated as
the following optimization problem
t = ProxGt
t−1) + ψt(us
t = 1 · · · T ,
where Θ = {Θt}t=T
t=1 and I0 is the initial status of the diffusion
process. Note that the above loss function only depends on the
output of the ﬁnal stage T, i.e., the parameters in all stages
are simultaneously trained such that the output of the diffusion
process - uT is optimized.
We call this training scheme joint
training similar to .
The joint training strategy is a minimization problem with respect to the parameters in all stages
{Θ1, Θ2, · · · , ΘT }.
One can see that our training model is also a deep model with
many stages (layers). It is well-known that deep models are usually
sensitive to initialization, and therefore training from scratch is
prone to getting stuck at bad local minima. As a consequence,
people usually consider a greedy layer-wise pre-training to
provide a good initialization for the joint training (ﬁne tune).
In our work, we also consider a greedy training scheme
similar to , to pre-train our diffusion network stage-by-stage,
where each stage is greedily trained such that the output of each
stage is optimized, i.e., for stage t, we minimize the cost function
t is the output of stage t of the diffusion process. Note that
this is a minimization problem only with respect to the parameters
Θt in stage t.
Parameterizing the inﬂuence functions φt
i and weights λt
In this paper, we aim to investigate arbitrary inﬂuence functions. In
order to conduct a fast and accurate training, an effective function
parameterization method is required. Following the work of ,
we parameterize the inﬂuence function via standard radial basis
functions (RBFs), i.e., each function φ is represented as a weighted
linear combination of a family of RBFs as follows
4. This loss function is related to the PSNR quality measure. Note that as
shown in , other quality measures, such as structural similarity (SSIM)
and mean absolute error (MAE) can be chosen to deﬁne the loss function. At
present we only consider the quadratic loss function due to its simplicity.
where ϕ represents different RBFs. In this paper, we exploit RBFs
with equidistant centers µj and uniﬁed scaling γj. We investigate
two typical RBFs : (1) Gaussian radial basis ϕg and (2)
triangular-shaped radial basis ϕt, given as
|z −µ| > γ
respectively. The basis functions are shown in Figure 3, together
with an example of the function approximation by using two
different RBF methods. In our work, we have investigated both
function approximation methods, and we ﬁnd that they lead
to similar results. We only present the results obtained by the
Gaussian RBF in this paper.
In our work, the linear kernels kt
i related to the linear operators Kt
i are deﬁned as a linear combination of Discrete Cosine
Transform (DCT) basis kernels br, i.e.,
where the kernels kt
i are normalized to get rid of an ambiguity
appearing in the proposed diffusion model. More details can be
found in the supplemental material. The kernels are formed in this
way in order to keep the expected property of zero-mean.
The weights λt in our model are constrained to be positive.
To this end, we set λt ←eλt in the training phase for our
implementation.
Computing gradients
For both greedy training and joint training, we make use of
gradient-based algorithms (e.g., the L-BFGS algorithm ) for
optimization. The key point is to compute the gradients of the
loss function with respect to the training parameters. In greedy
training, the gradient of the loss function at stage t with respect to
the model parameters Θt is computed using standard chain rule,
∂ℓ(ut, ugt)
· ∂ℓ(ut, ugt)
where ∂ℓ(ut,ugt)
= ut −ugt is directly derived from (9),
is computed from the diffusion process for speciﬁc task. For
the applications exploited in this paper, such as image denoising
with Gaussian noise, single image super resolution and JPEG
deblocking, we present the detailed derivations of
∂Θt in the
supplemental material.
In the joint training, we compute the gradients of the loss function with respect to Θt by using the standard back-propagation
technique widely used in the neural networks learning ,
namely, ∂ut
∂Θt is computed by using
∂ℓ(uT , ugt)
· · · ∂ℓ(uT , ugt)
Compared to the greedy training, we additionally need to
calculate ∂ut+1
∂ut . For the investigated image processing problems
in this paper, we provide all necessary derivations in the supplemental material.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
Fig. 3. Function approximation via Gaussian ϕg(z) or triangular-shaped ϕt(z) radial basis function, respectively for the function φ(z) =
20. Both approximation methods use 63 basis functions equidistantly centered at [−310 : 10 : 310].
Experimental setup and implementation details
Boundary condition of the convolution operations
In our convolution based diffusion network, the image size stays
the same when an image goes through the network, and we use
the symmetric boundary condition for convolution calculation. In
our original diffusion model (6), there is matrix transpose K⊤v,
which exactly corresponds to the convolution operation ¯k ∗v (¯k
is obtained by rotating the kernel k 180 degrees) in the cases of
periodic and zero-padding boundary conditions. It should be noted
that in the case of symmetric boundary condition used in this
paper, this result holds only in the central image region. However,
we still want to explicitly use the formulation ¯k ∗v to replace
K⊤v, because the former can signiﬁcantly simplify the derivation
of the gradients required for training.
We ﬁnd that the direct replacement introduces some artifacts
at the image boundary. In order to avoid these artifacts, we
symmetrically pad the input image before it is sent to the diffusion
network, and then we discard those padding pixels in the ﬁnal
output. More details are found in the supplemental material.
RBF kernels
Images exploited in this paper have the dynamic range in ,
and the ﬁlters have unit norm. In order to cover most of the
ﬁlter response, we consider inﬂuence functions in the range
[−310, 310]. We use 63 Gaussian RBFs with equidistant centers
at [−310 : 10 : 310], and set the scaling parameter γ = 10.
Experimental setup
Model capacity: In our work, we train the proposed diffusion
network with at most 8 stages to observe its saturation behavior
after certain stages. We ﬁrst greedily train T stages of our model
with speciﬁc model capacity, then conduct a joint training to reﬁne
the parameters of the whole T stages.
In this paper, we mainly consider four different diffusion
networks with increasing capacity:
3×3, Fully trained model with 8 ﬁlters of size 3 × 3 ,
5×5, Fully trained model with 24 ﬁlters of size 5 × 5 ,
7×7, Fully trained model with 48 ﬁlters of size 7 × 7 ,
9×9, Fully trained model with 80 ﬁlters of size 9 × 9 ,
where TNRDT
m×m denotes a nonlinear diffusion process of stage
T with ﬁlters of size m × m. The ﬁlters number is m2 −1,
if not speciﬁed. For example, TNRDT
7×7 model contains (48 ×
48 (ﬁlters) + 48 × 63 (penalties) + 1 (λ)) · T = 5329 · T free
parameters.
Training and test dataset: In order to make a fair comparison to
previous works, we make use of the same training datasets used
in previous works for our training, and then evaluate the trained
models on commonly used test datasets. For image processing
problems investigated in this paper, i.e., Gaussian denoising, single
image super resolution and JPEG deblocking, we consider the
following training and test datasets, respectively.
a) Gaussian denoising. Following , we use the same 400
training images, and crop a 180 × 180 region from each image,
resulting in a total of 400 training samples of size 180 × 180,
i.e., roughly 13 million pixels. We then evaluate the denoising
performance of a trained model on a standard test dataset of
68 natural images, which is suggested by , and later widely
used for Gaussian denoising testing. Note that the test images
are strictly separate from the training datasets.
b) Single image super resolution. The publicly available framework of Timofte et al. provides a perfect base to compare single image super resolution algorithms. It includes 91
training images and two test datasets Set5 and Set14. Many
recent state-of-the-art learning based image super resolution
approaches , accomplish their comparison based on
this framework. Therefore, we also use the same 91 training
images. We crop 4-5 sub-images of size 150 × 150 from each
training image, according to its size, and this ﬁnally gives us
421 training samples. We then evaluate the performance of the
trained models on the Set5 and Set14 dataset.
c) JPEG deblocking. We train the diffusion models using the same
training images as in the case of Gaussian denoising. In the
test phase, we follow the test procedure in for performance
evaluation. The test images are converted to gray-value, and
scaled by a factor of 0.5, resulting 200 images of size 240×160.
Approximate training time
Note that the calculation of the gradients of the loss function in
(13) is very efﬁcient even using a simple Matlab implementation,
since it mainly involves 2D convolutions. The training time varies
greatly for different conﬁgurations. Important factors include (1)
model capacity, (2) number of training samples, (3) number of
iterations taken by the L-BFGS algorithm, and (4) number of
Gaussian RBF kernels used for function approximation. We report
the most time consuming cases as follows.
In training, computing the gradients ∂L
∂Θ with respect to the
parameters of one stage for 400 images of size 180 × 180 takes
about 35s (TNRD5×5), 75s (TNRD7×7) or 165s (TNRD9×9)
using Matlab implementation on a server with CPUs: Intel(R)
Xeon E5-2680 @ 2.80GHz Noisy u0 (20.17)
(b) Stage 1: u1 (27.26)
(c) Stage 2: u2 (28.40)
(d) Stage 3: u3 (28.18)
(e) Stage 4: u4 (28.63)
(f) Stage 5: u5 (29.63)
Fig. 4. An image denoising example for noise level σ = 25 to illustrate how our learned TNRD5
5×5 works. (b) - (e) are intermediate results at stage
1 - 4, and (f) is the output of stage 5, i.e., the ﬁnal denoising result.
in Matlab, 63 Gaussian RBF kernels for the inﬂuence function
parameterization). We typically run 200 L-BFGS iterations for
optimization. Therefore, the total training time, e.g., for the
7×7 model is about 5 × (200 × 75)/3600 = 20.8h. Code
for learning and inference is available on the authors’ homepage
www.GPU4Vision.org5. For the training of the Gaussian denoising
task, we have also accomplished a GPU implementation, which is
about 4-5 times faster than our CPU implementation.
TRAINING FOR GAUSSIAN DENOISING
For the task of Gaussian denoising, we consider the following
energy functional
ρi(ki ∗u) + λ
By setting D(u) = λ
2 and G(u) = 0, we arrive at the
following diffusion process with u0 = f
ut = ut−1 −
i ∗ut−1) + λt(ut−1 −f)
where we explicitly use a convolution kernel ¯ki (obtained by
rotating the kernel ki 180 degrees) to replace the K⊤
sake of model simplicity, but we have to pad the input image.
The gradients ∂ut
∂ut−1 required in training are computed
from this equation. Detailed derivations are presented in the
supplemental material.
We started with the training for TNRDT
5×5. We ﬁrst considered
the greedy training phase to train a diffusion process up to 8 stages
(i.e., T ≤8), in order to observe the asymptotic behavior of the
diffusion network. In the greedy training, only parameters in one
stage were trained at a time. We exploited a plain initialization
to start the training, namely linear ﬁlters and inﬂuence functions
were initialized from the modiﬁed DCT ﬁlters and the function
φ(z) = 2z/(1 + z2), respectively.
After the greedy training was completed, we conducted joint
training for a diffusion model of certain stages (e.g., T = 2, 5, 8),
to simultaneously tune the parameters in all stages. We initialized
the joint training using parameters learned in greedy training, as
this is guaranteed not to degrade the training performance.
We ﬁrst trained our diffusion models for the Gaussian denoising problem with standard deviation σ = 25. The noisy training
5. 
images were generated by adding synthetic Gaussian noise with
σ = 25 to the clean images. Once we obtained a trained model,
we evaluated its denoising performance on 68 natural images
following the same test protocol as in , . Figure 4 shows
a denoising example for noise level σ = 25 to illustrate the
denoising process of our learned TNRD5
5×5 diffusion network.
We present the ﬁnal results of the joint training in Table
1, together with a selection of recent state-of-the-art denoising
algorithms, namely BM3D , LSSC , EPLL-GMM ,
opt-MRF , RTF model , the CSF model and WNNM
 , as well as two similar approaches ARF and opt-GD
 , which also train an optimized gradient descent inference.
We downloaded these algorithms from the corresponding author’s
homepage, and used them as is. Unfortunately, we are not able
to present comparisons with , , as their codes are not
available.
From Table 1, one can see that (1) the performance of the
5×5 model saturates after stage 5, i.e., in practice, 5 stages
are typically enough; (2) our TNRD5
5×5 model has achieved
signiﬁcant improvement (28.78 vs.28.60), compared to a similar
model CSF5
5×5, which has the same model capacity and (3)
moreover, our TNRD8
5×5 model is on par with so far the bestreported algorithm - WNNM. It turns out that our trained models
perform surprisingly well for image denoising. Then, a natural
question arises: what is the critical factor for the effectiveness of
the trained diffusion models?
Understanding the proposed diffusion models
There are actually two main aspects in our training model: (1)
the linear ﬁlters and (2) the inﬂuence functions. In order to have
a better understanding of the trained models, we went through
a series of experiments to investigate the impact of these two
Concentrating on the model capacity of 24 ﬁlters of size 5×5,
we considered the training of a diffusion process with 10 steps, i.e.,
T = 10 for the Gaussian denoising of noise level σ = 25. We
exploited two main classes of conﬁgurations: (A) the parameters
of every stage are the same and (B) every diffusion stage is
different from each other. In both conﬁgurations, we consider two
cases: (I) only train the linear ﬁlters with ﬁxed inﬂuence function
φ(z) = 2z/(1 + z2) and (II) simultaneously train the ﬁlters and
inﬂuence functions.
Based on the same training dataset and test dataset, we obtained the following results: (A.I) every diffusion step is the same,
and only the ﬁlters are optimized with ﬁxed inﬂuence function.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
(a) Truncated convex
(b) Negative Mexican hat
(c) Truncated concave
(d) Double-well penalty
Fig. 5. The ﬁgure shows four characteristic inﬂuence functions (left plot
in each subﬁgure) together with their corresponding penalty functions
(right plot in each subﬁgure), learned by our proposed method in the
5×5 model. A major ﬁnding in this paper is that our learned penalty
functions signiﬁcantly differ from the usual penalty functions adopted
in partial differential equations and energy minimization methods. In
contrast to their usual robust smoothing properties which is caused by a
single minimum around zero, most of our learned functions have multiple
minima different from zero and hence are able to enhance certain image
structures. See Sec. 4.3 for more information.
This is a similar conﬁguration to previous works , . The
trained model achieves a test performance of 28.47dB. (A.II)
with additional tuning of the inﬂuence functions, the resulting
performance is boosted to 28.60dB. (B.I) every diffusion step
can be different, but only the linear ﬁlters are trained with ﬁxed
inﬂuence functions. The corresponding model obtains a result of
28.56dB, which is equivalent to the variational model with the
same model capacity. Finally (B.II) with additional optimization
of the inﬂuence functions, the trained model leads to a signiﬁcant
improvement with the result of 28.86dB.
The analytical experiments demonstrate that without the training of the inﬂuence functions, there is no chance to achieve
signiﬁcant improvements over previous works, no matter how
hard we tune the linear ﬁlters. Therefore, we believe that the most
critical factor of our proposed training model lies in the adjustable
inﬂuence functions. A closer look at the learned inﬂuence functions of the TNRD5
5×5 model in Sec.4.2 strengthens our argument.
Comparing our proposed TNRD model to the CSF model ,
one can see that the degree of freedom is in principle the same,
since in both models the ﬁlters and the non-linear functions can
be learned. Therefore, one would expect a similar performance of
both models in practice. However, it turns out the performance
of the CSF model is inferior to our TNRD model in the case of
Gaussian denoising task. The reason for the performance gap is
still unclear and we plane to investigate it in future work.
Learned inﬂuence functions
A close inspection of the learned 120 penalty functions6 ρ in the
5×5 model demonstrated that most of the penalties resemble
6. The penalty function ρ(z) is integrated from the inﬂuence function φ(z)
according to the relation φ(z) = ρ′(z)
four representative shapes shown in Figure 5.
(a) Truncated convex penalty functions with low values around
zero to promote smoothness.
(b) Negative Mexican hat functions, which have a local minimum
at zero and two symmetric local maxima.
(c) Truncated concave functions with smaller values at the two
(d) Double-well functions, which have a local maximum (not a
minimum any more) at zero and two symmetric local minima.
At ﬁrst glance, the learned penalty functions (except (a)) differ
signiﬁcantly from the usually adopted penalty functions used in
PDE and energy minimization methods. However, it turns out that
they have a clear meaning for image regularization.
Regarding the penalty function (b), there are two critical points
(indicated by red triangles). When the magnitude of the ﬁlter
response is relatively small (i.e., less than the critical points), probably it is stimulated by the noise and therefore the penalty function
encourages smoothing operation as it has a local minimum at zero.
However, once the magnitude of the ﬁlter response is large enough
(i.e., across the critical points), the corresponding local patch
probably contains a real image edge or certain structure. In this
case, the penalty function encourages to increase the magnitude
of the ﬁlter response, alluding to an image sharpening operation.
Therefore, the diffusion process controlled by the inﬂuence function (b), can adaptively switch between image smoothing (forward
diffusion) and sharpening (backward diffusion). We ﬁnd that the
learned inﬂuence function (b) is closely similar to an elaborately
designed function in a previous work , which leads to an
adaptive forward-and-backward diffusion process.
Similar forms of the learned penalty function in (c) with a
concave shape are also observed in previous work on image prior
learning . This penalty function also encourages to sharpen
the image edges. Concerning the learned penalty function (d), as
it has local minima at two speciﬁc points, it prefers speciﬁc image
structure, implying that it helps to form certain image structure.
We also ﬁnd that this penalty function is exactly the type of
bimodal expert functions for texture synthesis employed in .
Therefore, our learned penalty functions conﬁrmed existing
robust penalties based prior models and many priors exploiting
some unusual penalties, which can produce patterns and enhance
preferred features. As a consequence, the diffusion process involving the learned inﬂuence functions does not perform pure
image smoothing any more for image processing. In contrast, it
leads to a diffusion process for adaptive image smoothing and
sharpening, distinguishing itself from previous commonly used
image regularization techniques.
Pattern formation using the learned inﬂuence functions
In the previous work on Gibbs reaction diffusion7 , it is
shown that those unconventional penalty functions such as Figure
5(c) have signiﬁcant meaning in visual computation, as they can
produce patterns. We also ﬁnd that those unconventional penalty
functions learned in our models can produce some interesting
image patterns.
7. The terminology of “reaction diffusion” in is a bit different from
ours. In our formulation, “reaction term” is related to the data term, while
in , it means the diffusion term controlled by those downright penalty
functions.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
Parameters Θ
Influence functions
Parameter λ
Fig. 6. Well distributed gradients ∂L
∂Θ over stages for the TNRD5
5×5 model at the initialization point Θ0 with a plain setting. One can see that the
“vanishing gradient” phenomenon in the back-propagation phase of a conventional deep model does not appear in our training model.
Fig. 7. Patterns synthesized from uniform noise using our learned diffusion models. (a) is generated by (16) using the parameters (linear ﬁlters
and inﬂuence functions) in a stage of our learned TNRD5
5×5 for image
denoising, (b) is generated by (16) using the parameters in a stage of
our learned TNRD5
7×7 for image super resolution and (c) is also from a
stage of our learned TNRD5
7×7 for image super resolution.
We consider the following diffusion process involving our
learned linear ﬁlters and the corresponding inﬂuence functions
¯ki ∗φi(ki ∗ut−1) ,
where the ﬁlters ki and inﬂuence functions φi are chosen from
a certain stage of the learned models. Note that we do not incorporate a reaction term in this diffusion model. We run (16) from
starting points u0 (uniform noise images in the range ), and
it converges to a local minimum8. Some synthesized patterns are
shown in Figure 7. One can see that the diffusion model with our
learned inﬂuence functions and ﬁlters can produce edge-like image
structure and repeated patterns from fully random images. This
kind of diffusion model is known as Gibbs reaction diffusion in
 . We provide another example in Figure 13 to demonstrate how
our learned diffusion models can generate meaningful patterns for
image super resolution.
Important aspects of the training framework
Inﬂuence of initialization
Our training model is also a deep model with many stages (layers),
but we ﬁnd that it is not very sensitive to initialization. Based on
the training for Gaussian denoising, we conducted experiments
with fully random initializations and some plain settings.
8. The corresponding diffusion processes are unstable, and therefore we have
to restrict the image dynamic range to .
We ﬁrstly investigated the case of greedy training where the
model was trained stage by stage and the parameters of one stage
were trained at a time. We initialized the parameters using fully
random numbers in the range [−0.5, 0.5]. It turns out that the
resulting models with different initializations lead to a deviation
within 0.01dB in the test phase. That is to say, the greedy training
strategy is not sensitive to initialization.
Then, we considered the case of joint training, where all the
parameters in all stages were trained at a time. We also initialized
the training with fully random numbers in the range [−0.5, 0.5].
In this case, it turns out that the resulting models lead to inferior
results, e.g., in the case of TNRD5
5×5 (28.61 vs.28.78). However,
plain initializations can generate equivalent results. For example,
we considered a plain initialization (all stages were initialized
from the modiﬁed DCT ﬁlters and an uniﬁed inﬂuence function
φ(z) = 2z/(1 + z2)), the resulting models performed almost
the same as those models trained from some good initializations
such as parameters obtained from greedy training, e.g.,TNRD5
(28.75 vs.28.78) and TNRD5
7×7, (28.91 vs.28.92).
We believe that this appealing property of our training framework is attributed to the well-distributed gradients across stages.
We show in Figure 6 an example to illustrate the gradients of the
training loss function with respect to the parameter of all stages.
One can see that the well-known phenomenon of “vanishing
gradient” in the back-propagation phase of a usual deep model
does not appear in our training model. We believe that the reason
for the well-distributed gradients is that our training model is more
constrained. For example, in a more general sense, the rotated
kernel ¯ki in our formulation is not necessary to be the rotated
version of the kernel ki, and it can be an arbitrary kernel. However,
we stick to this form, as it has a clear meaning derived from energy
minimization.
Inﬂuence of the number of training samples
In our training, we do not consider any regularization for the
training parameters, and we ﬁnally reach good-performing models.
A probable reason is that we have exploited sufﬁcient training
samples (400 samples of size 180 × 180). Thus an interesting
question arises: how many samples are sufﬁcient for our training?
In order to answer this question, we re-train the TNRD5
model using different size of training dataset, and then evaluate
the test performance of trained models. We summarize the results
in Figure 8. One can see that (1) too few training samples 
Fig. 8. Inﬂuence of the number of training examples for the training
model TNRD5
Filter size (pixel)
Test PSNR (dB)
Fig. 9. Inﬂuence of the ﬁlter size (based on a relatively small training
data set of 400 images of size 180 × 180)
(a) 48 ﬁlters of size 7 × 7 in stage 1
(b) 48 ﬁlters of size 7 × 7 in stage 5
Fig. 10. Trained ﬁlters (in the ﬁrst and last stage) of the TNRD5
model for the noise level σ = 25. We can ﬁnd ﬁrst, second and higherorder derivative ﬁlters, as well as rotated derivative ﬁlters along different
directions. These ﬁlters are effective for image structure detection, such
as image edge and texture.
40 images) will clearly lead to over-ﬁtting, thus inferior test
performance, and (2) 200 images are typically enough to prevent
over-ﬁtting.
Inﬂuence of the ﬁlter size
In our model, the size of involved ﬁlters is a free parameter. In
principle, we can exploit ﬁlters of any size, but in practice, we
need to consider the trade-off between run time and accuracy.
In order to investigate the inﬂuence of the ﬁlter size, we
increase the ﬁlter size to 7×7 and 9×9. We ﬁnd that increasing the
ﬁlter size from 5 × 5 to 7 × 7 brings a signiﬁcant improvement of
0.14dB ( TNRD5
7×7 vs.TNRD5
5×5) as show in Table 1. However,
when we further increase the ﬁlter size to 9 × 9, the resulting
9×9 only leads to a performance of 28.96dB (a slight
improvement of 0.05dB relative to the TNRD5
7×7 model). We can
conjecture that further increasing the ﬁlter size to 11 × 11 might
bring negligible improvements.
Note that the above conclusion is drawn from a relatively
small training data set of 400 images of size 180 × 180. It
should be mentioned that when the size of the model increase,
the size of training data set should also increase to avoid over-
ﬁtting. However, our current CPU implementation for training
prevents us from training with larger model and large-scale data
sets (millions). A faster implementation on GPUs together with the
stochastic gradient descent optimization strategy is left to future
We also consider a model with smaller ﬁlters, 3 × 3. We
summarize the results of different model capacities in Figure 9.
In practice, we prefer the TNRD5
7×7 model as it provides the best
trade-off between performance and computation time. Therefore,
in later applications, we only consider TNRDT
7×7 models.
Fig. 10 shows the trained ﬁlters of the TNRD5
7×7 model in
the ﬁrst and last stage for the task of Gaussian denoising. One
can ﬁnd many edge and image structure detection ﬁlters along
different directions and in different scales.
Training for different noise levels and comparison
to recent state-of-the-arts
The above training experiments are based on Gaussian noise of
level σ = 25. We also trained diffusion models for the noise
levels σ = 15 and σ = 50. The test performance is summarized
in Table 1, together with comparison to very recent state-of-theart denoising algorithms. In experiments, we observed that joint
training can always gain an improvement of about 0.1dB over the
greedy training for the cases of T ≥5.
From Table 1, one can see that for all noise levels, the resulting TNRD7×7 model achieves the highest average PSNR. The
7×7 model outperforms the benchmark - BM3D method by
0.35dB in average. This is a notable improvement as few methods
can surpass BM3D more than 0.3dB in average . Moreover,
7×7 model also surpasses the best-reported algorithm -
WNNM method, which is quite slow as shown in Table 2.
The algorithm structure of our TNRD model is similar to the
CSF model, which is well-suited for parallel computation on
GPUs. We implemented our trained models on GPU using CUDA
programming to speed up the inference procedure, and ﬁnally
it leads to signiﬁcantly improved performance, see Table 2. We
make a run time comparison to other denoising algorithms based
on strictly enforced single-threaded CPU computation ( e.g., start
Matlab with -singleCompThread) for a fair comparison, see Table
2. We only present the results of some selective algorithms, which
either have the best denoising result or run time performance. We
refer to for a comprehensive run time comparison of various
algorithms9.
9. LSSC, EPLL, opt-MRF and RTF5 methods are much slower than BM3D
on the CPU, cf. .
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
opt-MRF7×7
Average PSNR (dB) on 68 images from for image denoising with
σ = 15, 25 and 50. All the TNRD models are jointly trained. Note that
among those algorithms similar to our model, opt-MRF7×7, ARF4
and opt-GD10
5×5 only train the ﬁlters with ﬁxed penalty function
log(1 + z2). In the opt-MRF7×7 model, 48 ﬁlters of size 7 × 7 (2304
free parameters), for ARF4
5×5, 13 ﬁlters of size 5 × 5 (325 free
parameters) and for the opt-GD10
5×5 algorithm, 24 ﬁlters of size 5 × 5
(600 free parameters) are trained. The CSF model and our approach
train both the ﬁlters and nonlinearities, thus involving more parameters,
e.g., the TNRD5
7×7 model involves 26,645 free parameters and the
corresponding the CSF5
7×7 model involves 24,245 free parameters.
Run time comparison for image denoising (in seconds) with different
implementations. (1) The run time results with gray background are
evaluated with the single-threaded implementation on Intel(R) Xeon(R)
CPU E5-2680 v2 @ 2.80GHz; (2) the blue colored run times are
obtained with multi-threaded computation using Matlab parfor on the
above CPUs; (3) the run time results colored in red are executed on a
NVIDIA GeForce GTX 780Ti GPU. We do not count the memory
transfer time between CPU/GPU for the GPU implementation (if
counted, the run time will nearly double)
We see that our TNRD model is generally faster than the CSF
model with the same model capacity. It is reasonable, because
in each stage the CSF model involves additional DFT and inverse
DTF operations, i.e., our model only requires a portion of the computation of the CSF model. Even though the BM3D is a non-local
model, it still possesses high computational efﬁciency. In contrast,
another non-local model - WNNM achieves compelling denoising
results at the expense of huge computation time. Moreover, the
WNNM algorithm is hardly applicable for high resolution images
(e.g., 10 mega-pixels) due to its huge memory requirements. Note
that our model can be also easily implemented with multi-threaded
CPU computation.
In summary, our TNRD5
7×7 model outperforms these recent
state-of-the-arts, meanwhile it is the fastest method even with a
CPU implementation. We present an illustrative denoising example in Figure 11 on an image from the test dataset. More denoising
examples can be found in the supplemental material based on
images from the test dataset and a megapixel-size natural image
of size 1050 × 1680.
SINGLE IMAGE SUPER RESOLUTION (SISR)
As demonstrated in the last section that our trained diffusion model
can lead to explicit backward diffusion process, which sharpens
image structures like edges. This is the very property demanded
for the task of image super resolution. Therefore, we are motivated
to investigate the SISR problem with our proposed approach.
We start with the following energy functional
ρi(ki ∗u) + λ
2 ∥Au −f∥2
where the linear operator A is a bicubic interpolation which links
the high resolution (HR) image h to the low resolution (LR) image
f via f = Ah. Casting D(u) = λ
2 ∥Au −f∥2
2 and G(u) = 0, the
energy functional (17) suggests the following diffusion process
ut = ut−1 −
i ∗ut−1) + λtA⊤(Aut−1 −f)
where the starting point u0 is given by the direct bicubic interpolation of the LR image f. Computing the gradients
∂ut−1 with respect to (18) can be done with little modiﬁcations
to the derivations for image denoising. Detailed derivations are
presented in the supplemental material.
We considered the model capacity of TNRD5
7×7, and trained
diffusion models for three upscaling factors ×2, ×3 and ×4, using
exactly the same 91 training images as in previous works ,
 . The trained models are evaluated on two different test data
sets: Set5 and Set14. Following previous works , , ,
the trained models are only applied to the luminance component
of an image, and a regular bicubic upscaling method is applied to
the color components.
The test results are summarized in Table 3 and Table 4. One
can see that in terms of average PSNR, our trained diffusion model
7×7 leads to signiﬁcant improvements over very recent
state-of-the-arts in all cases, meanwhile it is still among the fast
algorithms10. A SISR example is shown in Figure 12 to illustrate
its effectiveness. One can see that our approach can obtain more
accurate image edges, as shown in the zoom-in parts. More SISR
examples can be found in the supplemental material.
We apply the learned diffusion parameters to the diffusion
equation (16). It turns out that the diffusion process can also
generate some interesting patterns from random images, as shown
in Figure 7. We believe that this ability to generate image patterns
from weak evidence is the main reason for the superiority of
our trained model for the SISR task. In order to further validate
our argument, we carry out a toy SISR experiment based on a
synthesized image with repeated hexagons. The results are shown
in Figure 13, where one can see that our trained model can better
reconstruct those repeated image structures.
JPEG DEBLOCKING EXPERIMENTS
In order to further demonstrate the applicability of our proposed
framework for those problems with a non-smooth data term, we
investigate the JPEG deblocking problem - suppressing the block
artifacts in the JPEG compressed images, which is formulated as a
10. Note that our approach is a Matlab implementation, while some of other
algorithms are based on C++ implementations, such as SR-CNN.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
(a) Noisy, 20.17dB
(b) BM3D, 27.53dB/CPU: 2.5s
7×7, 28.00dB/GPU: 0.55s
(d) WNNM, 27.94dB/CPU: 393.2s
5×5, 28.16dB/GPU: 9.1ms
7×7, 28.23dB/GPU: 20.3ms
Fig. 11. Denoising results on a test image of size 481 × 321 (σ = 25) by different methods (compared with BM3D , WNNM and CSF model
 ), together with the corresponding computation time either on CPU or GPU. Note the differences in the highlighted region.
K-SVD 
SR-CNN 
PSNR (dB) and run time (s) performance for upscaling factors ×2, ×3 and ×4 on the Set5 dataset. All the methods use the same 91 training
images as in .
non-smooth optimization problem. Motivated by , we consider
the following variational model based on the FoE image prior
ρi(ki ∗u) + IQ(Du) ,
where IQ is a indicator function over the set Q (quantization
constraint set). In JPEG compression, information loss happens in
the quantization step, where all possible values in the range [d −
0.5, d + 0.5] (d is an integer) are quantized to a single number d.
Given a compressed data, we only know d. Therefore, all possible
values in the interval [d −0.5, d + 0.5] deﬁne a convex set Q
which is a box constraint. The sparse matrix D ∈RN×N denotes
the block DCT transform. We refer to for more details.
By setting D(u) = 0 and G(u) = IQ(Du), we obtain the
following diffusion process
ut = D⊤projQ
where projQ(·) denotes the orthogonal projection onto Q. More
details can be found in the supplemental material.
We also trained diffusion models for the JPEG deblocking
problem. We followed the test procedure in for performance evaluation. We distorted the images by JPEG block-
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
K-SVD 
SR-CNN 
coastguard
average performance
Upscaling factor ×3 performance in terms of PSNR(dB) and runtime (s) per image on the Set14 dataset.
(a) Bicubic / 29.43dB
(b) K-SVD / 31.10dB
(c) ANR / 31.09dB
(d) SR-CNN / 32.39dB
(e) RFL / 32.10dB
7×7 / 33.61dB
Fig. 12. A super resolution example for the “Monarch” image from Set14 with an upscaling factor ×3. Note the differences in the highlighted region
that our model achieves more clean and sharp image edges. Best viewed on screen and zoom in.
(a) Original
(b) Bicubic / 14.18dB
(c) ANR / 15.07dB
(d) SR-CNN / 15.65dB
(e) RFL / 15.03dB
7×7 / 17.22dB
Fig. 13. A toy experiment on a synthesized image with repeated hexagons for the upscaling factor ×3.
ing artifacts. We considered three compression quality settings
q = 10, 20 and 30 for the JPEG encoder.
We trained three nonlinear diffusion TNRD7×7 models for different compression parameter q. We found that for JPEG deblocking, 4 stages are already enough. Results of the trained models are
shown in Table 5, compared with several representative deblocking
approaches. We see that our trained TNRD4
7×7 outperforms all
the competing approaches in terms of PSNR.
Concerning the
run time, our model takes about 11.2s to handle an image of
size 1024 × 1024 with CPU computation, while the strongest
competitor (in terms of run time) - SADCT consumes about
56.5s11. Furthermore, our model is extremely fast on GPUs. For
the same image size the GPU implementation takes about 0.095s.
See the supplemental material for JPEG deblocking examples.
11. RTF is slower than SADCT, as it depends on the output of SADCT.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , VOL. XX, NO. XX, 2016
RTF TNRD4
JPEG deblocking results for natural images, reported with average
PSNR values.
DISCUSSION, SUMMARY AND FUTURE WORK
We have proposed a trainable nonlinear reaction diffusion framework for effective image restoration. Its critical point lies in the
additional training of the inﬂuence functions. We have trained our
models for the problem of Gaussian denoising, single image super
resolution and JPEG deblocking. Based on standard test datasets,
the trained models result in the best-reported results. We believe
that the effectiveness of the trained diffusion models is attributed
to the following desired properties of the models
• Anisotropy. In the trained ﬁlters, we can ﬁnd rotated derivative ﬁlters in different directions, cf. Fig 10, which will make
the diffusion happen in some special directions.
• Higher order. The learned ﬁlters contain ﬁrst, second and
higher-order derivative ﬁlters, cf. Fig 10.
• Adaptive forward/backward diffusion through the learned
nonlinear functions. Nonlinear functions corresponding to
explicit backward diffusion appear in the learned nonlinearity, cf. Fig 5.
Meanwhile, the structure of trained models is very simple and
well-suited for parallel computation on GPUs. As a consequence,
the resulting algorithms are signiﬁcantly faster than all competing
algorithms and hence are also applicable to the restoration of high
resolution images.
Discussion
One possible limitation of the proposed TNRD approach is that
one has to deﬁne the ground truth - the expected output of the diffusion network during training. For image restoration applications
in this paper, this is not a problem as we have a clear choice for
the ground truth. However, for those applications with ambiguous
ground truth, e.g., image structure extraction , we will have to
make efforts to deﬁne the ground truth.
Furthermore, the trained diffusion networks will only perform
well in the way they are trained. For example, the trained model
based on noise level σ = 25 will break for an input image with
noise σ = 50, and the trained model for upscaling factor ×3
will also lead to inferior performance when it is applied to the
SISR problem of upscaling factor ×2. It is generally hard to train
a universal diffusion model to handle all the noise levels or all
upscaling factors.
Our approach is to optimize a time-discrete PDE, which is
inspired by FoE based model, but we do not aim to minimize a
series of FoE based energies. Our model directly learns an optimal
trajectory for a certain possibly unknown energy functional, the
minimizer of which provides a good estimate of the demanded
solution. Probably, such a functional can not be modeled by a
single FoE energy, while our learned gradient descent/forwardbackward steps provide good approximation to the local gradients
of this unknown functional.
Future work
From an application point of view, we think that it will be interesting to consider learned nonlinear reaction diffusion based models
also for other image processing tasks such as image inpainting,
blind image deconvolution, optical ﬂow. Moreover, since learning
the inﬂuence functions turned out to be crucial, we believe that
learning optimal nonlinearities (i.e., activation functions) in standard CNs could lead to a similar performance increase. There are
actually two recent works , to investigate a parameterized
ReLU function in standard deep convolutional networks, which
indeed brings improvements even with little freedom to tune
the activation functions. Finally, it will also be interesting to
investigate the unconventional penalty functions learned by our
approach in usual energy minimization approaches.