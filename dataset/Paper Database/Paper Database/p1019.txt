Mach Learn 72: 173–188
DOI 10.1007/s10994-008-5071-9
Large margin vs. large volume in transductive learning
Ran El-Yaniv · Dmitry Pechyony · Vladimir Vapnik
Received: 23 June 2008 / Revised: 23 June 2008 / Accepted: 23 June 2008 / Published online: 8 July 2008
Springer Science+Business Media, LLC 2008
Abstract We consider a large volume principle for transductive learning that prioritizes the
transductive equivalence classes according to the volume they occupy in hypothesis space.
We approximate volume maximization using a geometric interpretation of the hypothesis
space. The resulting algorithm is deﬁned via a non-convex optimization problem that can
still be solved exactly and efﬁciently. We provide a bound on the test error of the algorithm
and compare it to transductive SVM (TSVM) using 31 datasets.
Keywords Transductive learning · Large margin · Large volume · TSVM · Learning
principles
1 Introduction
Alternative learning models that utilize unlabeled data have received considerable attention
in the past few years. Two prominent models are semi-supervised and transductive learning.
The main attraction of these models is empirical evidence indicating that they can often
allow for more efﬁcient and signiﬁcantly faster learning in terms of sample complexity.
In this paper we focus on distribution-free transductive learning. In this setting the learning algorithm is given a ﬁxed individual set of unlabeled data points whose labels are hidden.
Then, a training sample is selected randomly from the set and the labels of the training points
are revealed. The goal is to predict the labels of the remaining unlabeled points as accurately
as possible.
Editors: Walter Daelemans, Bart Goethals, Katharina Morik.
R. El-Yaniv · D. Pechyony ()
Computer Science Department, Technion-Israel Institute of Technology, Haifa 32000, Israel
e-mail: 
R. El-Yaniv
e-mail: 
NEC Laboratories America, Princeton, NJ 08540, USA
e-mail: 
Mach Learn 72: 173–188
Fig. 1 Large-margin vs.
large-volume prior
In both transduction and semi-supervised induction we would like to beneﬁt from additional unlabeled data to improve learning rates and accuracy. In semi-supervised induction
the additional unlabeled data may provide useful information on the marginal unknown distribution p(x) of unlabeled samples x. In distribution-free transduction, p(x) is already fully
known. In this regard, the transductive learning problem is easier. Moreover, in transduction
the given test examples provide an additional advantage that can be appreciated from a
learning theoretic perspective, which views learning as a selection process of a hypothesis
from an hypothesis class. In this perspective, the conceptual advantage of transduction over
(semi-supervised) induction is the possibility to perform data dependent selection and ranking of candidate hypotheses after observing the unlabeled data. This, in turn, allows us to
ﬁx our attention to smaller hypothesis spaces that are more relevant to the data at hand.
In transductive binary classiﬁcation, any hypothesis space (say, hyperplanes) is reduced
to a ﬁnite collection of equivalence classes, given the unlabeled data. All hypotheses in
the same class are identical in their binary classiﬁcation of the data. For example, consider
Fig. 1, in which C1,...,C4 represent four “clouds” of unlabeled data. In this case all the
hyperplanes passing in between C3 and C4 (and, in general, between Ci and Ci+1) are in the
same equivalence class (as well as inﬁnitely many other hyperplanes that are not shown).
The extra advantage in transduction is the possibility to prioritize these equivalence classes
in accordance with our prior beliefs about the goodness of hypotheses given the current data.
A classic principle for prioritizing equivalence classes is the large margin principle introduced in . According to this principle, the priority (or prior) of an equivalence
class is proportional to the largest margin obtained by any of its members. In our example
in Fig. 1 we should prefer the equivalence classes of h1 and h3 over the class of h2 because
they achieve a larger margin. This large margin consideration motivated the transductive
support vector machine (TSVM) approach for transduction.
In this paper we consider a different, large volume principle, whereby the prior of an
equivalence class is proportional to its “volume” in the hypothesis space. For example, in
the case of hyperplanes, in Fig. 1 we should prefer the equivalence class of h2 because it has
a much larger volume in the hyperplane space. This is depicted in Fig. 1 by the number of
dashed lines that pass between the clouds.
The large volume transductive principle was brieﬂy treated in for the case
of hyperplanes. Here we further study this principle and instead of hyperplane spaces we
consider general soft classiﬁcation vectors whose set of equivalence classes with respect to
all data points (ignoring labels) contains all possible dichotomies. Symmetry is broken by
generating equivalence classes of non-uniform volume, deﬁned via a non axis aligned datadependent ellipsoid. Since exact or quantiﬁable volume approximation is computationally
hard, we resort to a cruder approach whereby we measure the angles between hypotheses to
the principal axes of the ellipsoid. This approach makes sense because long principal axes
lie in regions of large volume. This construction leads to a general family of transductive algorithms and here we focus on one instantiation. Although the resulting algorithm is deﬁned
Mach Learn 72: 173–188
in terms of a non-convex optimization problem, we develop an efﬁcient global optimum solution using a known technique. We also derive a transductive data-dependent error bound
for this algorithm.
Our empirical evaluation of the new algorithm over a large number of datasets shows its
overwhelming advantage over TSVM (and SVM) in text categorization and image classi-
ﬁcation problems. However, on a different set of UCI datasets, TSVM and SVM are signiﬁcantly superior to the new algorithm. In our analysis of this ﬁnding, we identify some
factors that inﬂuence the success and failure of our algorithm. In particular we show that our
algorithm has signiﬁcant advantage over TSVM when TSVM outperforms SVM.
2 The transductive setting
We consider the following distribution-free transductive model . A ﬁxed set Xm+u = {x1,...,xm+u} of m + u points from some space X is given.
The binary labels yi ∈{±1} of these points are also ﬁxed but unknown to us. We uniformly
at random pick a subset Xm ⊆Xm+u of size m (among all subsets of size m), and the labels
of these points are provided. Rearranging indices, we denote by Sm = {(xi,yi)}m
i=1 the given
labeled points, and by Xu = {xj}m+u
j=m+1, the remaining u unlabeled points. Using Sm and Xu,
our goal is to guess the labels of points in Xu as accurately as possible.
Fixing m and u, we consider soft “hypotheses” that are vectors
h = (h1,...hm+u) ∈Rm+u,
where hi is the soft, or conﬁdence-rated label of example xi given by “hypothesis” h. The
vector h can be also interpreted as a functional response vector w.r.t. some underlying function f such that for any 1 ≤i ≤m + u, hi = f (xi). Based on knowledge of the full-sample
Xm+u, the learning algorithms we consider select an hypothesis space H = H(Xm+u) of such
soft classiﬁcation vectors. Then, given the labels of training points the algorithm selects one
hypothesis h ∈H. For actual (binary) classiﬁcation of xi the algorithm outputs sgn(hi).
Two quantities of interest, for an hypothesis h, are its transductive risk, or test error,
i=m+1 ℓ(hi,yi), deﬁned w.r.t. some loss function ℓ, and the training or empirical error (w.r.t. ℓ), Rℓ
i=1 ℓ(hi,yi). In this paper ℓwill be instantiated to the
zero-one loss, the hinge loss, and linear loss functions. Whenever we omit ℓfrom Rℓ
m, we assume that the zero-one loss function is used.
3 Transductive maximum power inference
Let H be any (soft) hypothesis space. A crucial observation, made by Vapnik for a
classiﬁcation setting, is that after the introduction of the unlabeled data Xm+u, the set H is
partitioned into a ﬁnite number of equivalence classes H1,...,HN, such that all hypotheses
in Hk,k = 1,...,N, generate the same dichotomy of Xm+u. Suppose that there exists some
underlying distribution P (h) over H such that one hypothesis is selected randomly according to P and the selected hypothesis determines the labels of points in Xm+u. Vapnik deﬁned the power of an equivalence class Hk as the probability mass (in terms of P )
of all the soft hypotheses in it,
k = 1,...,N.
Mach Learn 72: 173–188
The power function provides a preference relation over all the dichotomies of Xm+u that can
be generated by H. So, for example, if we utilize an empirical error minimization framework, then, among all equivalence classes that classify the training set correctly, we should
prefer one that has the largest power. We term this principle ‘transductive maximum power inference’. The principle can
also be extended to structural risk minimization.
In practice, of course, we do not know the underlying hypothesis distribution (and moreover, such a distribution may not exist) so in order to implement maximum power inference
we must make a guess about some prior distribution P over H, or directly infer Power(Hk)
for k = 1,...,N. Obviously, a good prior distribution P should reﬂect auxiliary knowledge
on the effectiveness of soft hypotheses.
If the power function is only dependent on the unlabeled data (and not on the train/test
partition and the labels), the following error bound, which is an immediate consequence of
Theorem 22 in , provides a compelling motivation for maximum power
inference: for any 0 < δ < 1, with probability of at least 1 −δ over choices of Sm, for all
k = 1,...,N,
Ru(Hk) ≤Rm(Hk) +
Power(Hk) + ln 1
where Rm(Hk) (respectively Ru(Hk)) is the training (respectively test) error obtained by any
instance of Hk. The error bound (2) implies that if an equivalence class Hk with a large
power is empirically successful (over the training set), its test error over Xu is guaranteed to
be small, with high probability.
4 On priors and powers
The bound (2), which essentially provides a sufﬁcient condition for transductive learning,
tells us that the power of the equivalence classes is a crucial quantity that can directly affect
the learning speed and accuracy. Power assignment can be based on ‘low-level’ considerations, via prior assignment for hypotheses, as in (1). However, this assignment can also be
done directly on complete equivalence classes, without deﬁning a prior distribution P over
soft hypotheses. In the latter case, the power is simply a prior over equivalence classes.
Various approaches of deﬁning prior directly over equivalence classes have been considered in the past. The most well known approach is the maximum margin principle given
by . The margin of a hyperplane is its minimal distance to any example in
the full sample. By the maximum margin principle, the prior of the equivalence class Hk
is proportional to the maximal margin obtained by any hyperplane h ∈Hk. The maximalmargin principle motivated the well known transductive SVM (TSVM) approach. Other
prior assignment approaches using compression, clustering and graph cuts are discussed
in and .
Effective power assignments must rely on some specialized knowledge that requires insight into the learning problem at hand. For some problems, priors on soft hypotheses (or
power of equivalence classes) can be difﬁcult to identify or quantify. Vapnik proposed an
alternative prior encoding scheme through the universum . The universum is a set of unlabeled examples belonging to the same domain X, but are known
not to belong to any one of the two classes. The power of an equivalence class should be
Mach Learn 72: 173–188
taken as the number of dichotomies it obtains over the universum examples.1 Since counting
the number of dichotomies is computationally hard, it was proposed to approximate it with
the number of contradictions. A universum example x contradicts an equivalence class Hk
if there exists a pair of soft hypotheses h,h′ ∈Hk, such that h(x) ̸= h′(x). We term this
approximation as the (universum) maximum contradiction principle.
Although the universum approach as presented above is transductive, it can also be motivated for induction. In fact, the ﬁrst empirical study and validation of the universum idea is
within an inductive setting , where an hypothesis class of hyperplanes
is considered and it is suggested to approximate the number of contradictions of an equivalence class Hk by the minimum, over all h ∈Hk, of the sum of ℓ1-distances of h from all
universum examples. The intuition behind this approximation is that a very close proximity
of h to a universum example x implies that a slight perturbation in the direction of h will
generate a new h′ that classiﬁes x differently. The success of this approximated maximum
contradictions principle depends on the choice of universum examples and it was shown
in that a combination of both the maximum margin and the maximum
contradictions principles can outperform the maximum margin principle alone, if the universum is effectively selected.
In some domains universum examples arise naturally. For example, in a binary recognition problem where we want to separate the digits ‘1’ and ‘2’, examples of other digits can
form an effective universum . But in general, universum examples may
be hard to generate, especially in problems where we cannot easily perceive the membership
of the universum examples to the domain.
5 A large volume principle
Consider a transductive classiﬁcation setting and assume for now that H (which may depend
on Xm+u) is ﬁnite. We consider the assignment of a prior measure P over H. In the absence
of any other knowledge, by the principle of insufﬁcient reason, the prior of any two soft
hypotheses (not necessarily from the same equivalence class) should be the same. This, of
course, does not imply that the powers of two equivalence classes are identical.2 According
to (1), if P is uniform and H is ﬁnite then the power of any equivalence class is proportional
to its size. A straightforward extension of this argument to a continuously inﬁnite (soft) H
results in a power function that assigns to each equivalence class the geometric volume of
soft hypotheses contained in it. We term this application of the maximal power inference
principle with a uniform prior (over the soft hypotheses) the large volume principle.
There are a few previous works that explicitly or implicitly utilized a large volume principle for an hypothesis space of (kernelized) hyperplanes. Vapnik proposed
to approximate the volume of an equivalence class (of hyperplanes) by the distance between
convex hulls of positive and negative examples. As shown by , this distance is precisely the margin.
Tong and Koller exploited a duality between hyperplanes and instance points,
where hyperplanes are viewed as points on a sphere and examples are viewed as hyperplanes passing through the sphere. They approximated the volume of an equivalence class
1In philosophical terms, the universum is used to measure falsiﬁability (or speciﬁcity)—the more powerful
equivalence classes are those that are more falsiﬁable by the universum points .
2Note that whenever the number of equivalence classes is Ω(2m+u), if the power is uniform over classes, we
cannot bound the transductive test error.
Mach Learn 72: 173–188
(corresponding to the version space) by the radius of the maximally inscribed ball within a
conic section. This radius is precisely the margin and the approximation can be arbitrarily
poor whenever the equivalence class is an elongated section.
Again for hyperplanes, Graepel et al. approximated the volume of hypothesis
equivalence classes using a kernel billiard algorithm. Their algorithm operates in a transductive setting, but considers equivalence classes deﬁned by training points and a single
test point. In contrast, we consider here equivalence classes deﬁned by all training and test
Finally, we observe that one can approximate the volume using uniformly drawn universum examples. In this case one can show that, asymptotically, the equivalence classes with
larger volume will have a larger number of contradictions.3
The main difference between our contribution and the previous work described above is
that instead of hyperplane spaces we consider a much richer space of general soft classiﬁcation vectors. This space, unlike hyperplanes, generates all possible 2m+u dichotomies.
6 Transductive learning using the large volume principle
We describe a transductive learning scheme that approximates the large volume principle.
This scheme motivates a family of new transductive algorithms. In this section we develop
and analyze one instantiation of this scheme.
6.1 Volume approximation
Our approach for approximating the volume of the equivalence classes relies on hypothesis
spaces that can be represented as ellipsoids in Rm+u. Each soft hypothesis in the hypothesis
space is a point in the ellipsoid. We approximate the volume of an equivalence class Hk by
the angles between an (arbitrary) hypothesis in Hk and the principal axes of the ellipsoid.
Let the full sample Xm+u be given and ﬁxed. Let h ∈Rm+u be a soft transductive hypothesis, and Q, a positive deﬁnite (m + u) × (m + u) matrix that may depend on Xm+u.
The matrix Q is considered as a hyperparameter and in Sect. 9 we give an example for its
instantiation. Consider a hypothesis space HQ = {h | hT Qh ≤1}. Geometrically, HQ is an
ellipsoid in Rm+u, centered at zero. We denote it by E(HQ). Since Q is positive deﬁnite, the
set {sign(h) : h ∈HQ} contains all 2m+u possible dichotomies of Xm+u.
The Cartesian coordinate system divides the space Rm+u into 2m+u quadrants. Each
quadrant corresponds to one equivalence class in terms of hard classiﬁcation. For any
1 ≤k ≤2m+u, the volume of the equivalence class Hk is the volume of the intersection
of E(HQ) with the kth quadrant. For example, Fig. 2(a) shows four equivalence classes,
Hk,k = 1,...,4, for the case m + u = 2. Ultimately, we would like to compute the exact volume of these quadrant intersections. However, currently known algorithms for approximate volume computation of general convex bodies seem to be too slow for practical
purposes .
We resort to the following heuristic approximation. Let {(λi,vi)}m+u
i=1 be the eigenvalues
of Q along with their eigenvectors, such that for all 2 ≤i ≤m + u, λi−1 ≤λi. We assume
3Proof outline: consider a dual space of hyperplanes with the offset b = 0 (w.l.o.g.). This space is a sphere and
full sample points are hyperplanes passing through the origin and cutting the sphere. Each equivalence class
is a conical section of this sphere. In the dual space, uniformly drawn universum examples are equivalent to
uniformly drawn hyperplanes. Thus, a universum example generates a contradiction in a conical section iff
its hyperplane cuts the section. If the conical section is large then it will be cut by many hyperplanes.
Mach Learn 72: 173–188
Fig. 2 Visualization of
hypothesis space: (a) equivalence
classes have different volumes;
(b) equivalence classes have the
same volume
w.l.o.g. that for any 1 ≤i ≤m + u, ∥vi∥2 = 1. The direction and length of the ith principal
axis of E(HQ) are, respectively, vi and √1/λi. As shown in Fig. 2(a), the volume of Hk is
proportional to the length of the principal axes of the ellipsoid, which falls in its quadrant.
In the extreme case of a perfect sphere (Fig. 2(b)), all equivalence classes are of the same
volume and cannot be differentiated. Therefore, we should prefer skewed ellipsoids that
ultimately reﬂect useful priors on hypothesis effectiveness. In Sect. 9.1 we give example of
such skewed ellipsoids that yield preference for “smoother” hypotheses.
The number of principal axes is always m + u whereas the number of quadrants (and
equivalent classes) is 2m+u. Hence, the vast majority of quadrants do not contain any principal axis and, unlike the 2-dimensional case, we cannot estimate the volume of an equivalence class using a corresponding principal axis. We propose to estimate the volume using a
weighted sum of axes’ lengths such that the weights are determined by the polar proximity
of an hypothesis to the principal axes.
Fix i and j such that 1 ≤i < j ≤m+u and λi < λj. Then, the length of the ith principal
axis is larger than the length of the jth one. Hence, the local neighborhood of vi has a larger
volume than that of vj. A small angle between an hypothesis h and some long principal axis
is taken as an evidence that its equivalence class has large volume. Conversely, a small angle
to a short principal axis is taken as an evidence of a small volume. Note that these two opposing conditions cannot be satisﬁed simultaneously since the principal axes are orthogonal.
In Sect. 9 we brieﬂy discuss the meaning of the eigenvectors for a particular Q of interest.
Let 0 ≤a1 ≤a2 ≤··· ≤am+u be an increasing sequence of weights. For any soft hypothesis h let
The expression (hT vi)2/∥h∥2
2 is the square of the cosine of the angle between h and the
unit-length vector vi. The monotone increasing sequence of ai’s corresponds to a monotone
decreasing sequence of the lengths of vi’s. Thus, the weighted sum (3) gives larger weight to
the angular closeness to short principal axes than to the long ones. Consequently, we expect
(3) to be large when h lies in the equivalence class of low volume and be small when h lies
in the equivalence class of high volume.
6.2 Approximate volume regularization (AVR) algorithm
We propose the following natural instantiation of the {ai}m+u
i=1 such that they are inversely
proportional to the lengths of their corresponding principal axes. Let di = √1/λi be the
length of the ith largest principal axis of the ellipsoid and for any h ∈Rm+u, set ai = 1/d2
Mach Learn 72: 173–188
This volume approximation motivates the following family of transductive algorithms,
which implements the large volume principle:
h∈HQ Rm(h) + γ · hT Qh
where γ > 0 is a regularization parameter.4
Instead of the 0/1 loss empirical error in (4), due to computational considerations (see
Remark 3 in Sect. 9), we instantiate the loss function to the linear loss, ℓ(hi,yi)
Fixing t > 0 and constraining h to be of length t we eliminate the denominator in (4). Also,
we replace the constraint h ∈HQ with h ∈Rm+u (see below). The resulting optimization
problem is
hiyi + γ · hT Qh
We refer to the optimization problem (5)–(6) as the Approximate Volume Regularization
(AVR) algorithm. Due to constraint (6) the loss −hiyi of each training example is lower
bounded by −t. Notice that while the optimization in (5) is done in Rm+u, the regularization
is done relative to HQ. The reason is that under the constraint (6) the complexity term
hT Qh is a weighted sum of the squares of cosines between h and the principal axes of
E(HQ). Thus, the optimization problem (5)–(6) is directly implied by (4) under the above
instantiations of the free parameters.
While the optimization problem (5)–(6) is not convex, it can be solved efﬁciently and
exactly (to obtain a global optimum) using the method of . This solution
is developed in Sect. 7.
7 Global optimum AVR optimization
Following , we solve (5)–(6) using Lagrange multipliers. Set
Φ(h,ρ) = −1
hiyi + γ · hT Qh −ρ(∥h∥2
where ρ is a Lagrange multiplier. Then, h∗= minh∈Rm+u,ρ∈R Φ(h,ρ) is a solution of (5)–(6).
The minimum of Φ(h,ρ) is achieved when its partial derivatives are zero. Let y ∈Rm+u be a
vector of labels, with the ﬁrst m entries being the training labels and the last u entries being
4Note that one deﬁciency of the above approximation is that for two hypotheses h and h′ from the same
equivalence class, V (h) is not in general identical to V (h′).
Mach Learn 72: 173–188
zeros. Differentiating Φ(h,ρ) w.r.t. h and ρ, and equating both these derivatives to zero we
−y/m + 2γ Qh −2ρh = 0;
2 −t2 = 0.
It follows from (7) that5
2m(γ Q −ρI)−1y.
is a solution of (5)–(6).
The expression (9) contains the unknown ρ, which we now determine. Let VDVT be the
spectral decomposition of Q, such that VDVT = Q, VT V = VVT = I and D is a diagonal matrix with its diagonal elements λi being the eigenvalues of Q. Then (7)–(8) can be
rewritten as
−y/m + 2γ VDVT h −2ρVVT h = 0,
hT VVT h −t2 = 0.
Letting u = VT h and d = VT y, (10)–(11) becomes
−d/m + 2γ Du −2ρu = 0,
uT u −t2 = 0.
Isolating u at (12) and substituting it in (13) we get
(2m)2 dT (γ D −ρI)−2d −t2 = 0.
Let di be the ith component of d. Since the matrix D is diagonal, (14) is equivalent to
(γ λi −ρ)2 −t2 = 0.
Equation (15) has multiple ρ solutions. As shown by , the smallest possible ρ that satisﬁes (15) also minimizes Φ(h,ρ). Thus, our goal
is to ﬁnd the smallest ρ satisfying (15). Since the matrix Q is positive deﬁnite, all λi’s are
strictly positive. Therefore, the function
(γ λi −ρ)2 −t2
has the structure depicted in Fig. 3. Considering this structure, our algorithm for ﬁnding the
smallest ρ such that f (ρ) = 0 is as follows: Let λ be the smallest λi such that di ̸= 0. We
consider the interval [λ −t1,λ −t2] such that t1 > 0, t2 > 0, f (λ −t1) < 0, f (λ −t2) > 0
and ﬁnd the root of f in this domain using a binary search.
5Here we assume that the value of ρ satisfying (7)–(8) is not an eigenvalue of γ Q and the inverse (γ Q −
ρI)−1 exists. If this assumption does not hold (this can be easily veriﬁed by checking for each eigenvalue of
γ Q if it satisﬁes (7)–(8)), then we can slightly perturb the hyperparameter γ to satisfy the assumption.
Mach Learn 72: 173–188
Fig. 3 Structure of the function
f (ρ). k is the index of the
smallest eigenvalue λi such that
8 A risk bound
In this section we derive a transductive risk bound for the AVR algorithm (5)–(6). Our
derivation relies on a known general transductive risk bound for ‘unlabeled/labeled (UL)
decompositions’ of transductive algorithms as discussed in .
The soft classiﬁcation output h∗of any transductive algorithm can always be represented
as h∗= K ·α, where K is an (m+u)×(m+u) matrix depending only on the unlabeled full
sample Xm+u, and α is an (m + u) × 1 vector that can depend on both Sm and Xu. Such a
decomposition is termed a UL (unlabeled-labeled) decomposition . Let Kij be the (i,j)th entry of K and ∥K∥2
Fro = m+u
ij, be the squared Frobenius
norm of K. For UL decompositions, the following holds.
Theorem 1 Let A be a transductive algorithm and h∗=
K · α be its UL decomposition. Suppose that ∥α∥2 ≤μ1. Let c0 = √(32ln(4e))/3 < 5.05
△= 1/m + 1/u. Let 
H be the set of soft hypotheses that can be generated by the
algorithm when operated on any training/test partition. Then,6 for any ν > 0 and δ > 0,
with probability of at least 1 −δ over the choice of the training set of size m from Xm+u, for
Ru(h) ≤Rℓν
m (h) + μ1
min(m,u) +
2W ln(1/δ).
Since the (m + u) × (m + u) matrix V (deﬁned in Sect. 7) consists of orthonormal
columns, the solution h∗of (5)–(6) can be represented as h∗= Vα. The matrix V depends
only on the unlabeled examples. Hence the last equation is a UL decomposition of the AVR
algorithm, with K
△= V. By (6) we have that t2 = ∥h∗∥2
2 = αT VT Vα = αT α. Since each
column of V has unit length, ∥K∥2
Fro = m + u. Substituting μ1 = t and ∥K∥2
Fro = m + u in
(17), we obtain7
Ru(h) ≤Rℓν
m (h) + (t/ν)
min(m,u) +
2W ln(1/δ).
Notice that the matrix Q inﬂuences the bound (18) indirectly, through the empirical error
term. If t/ν is a constant then the bound (18) converges at rate 1/√min(m,u). In general,
there is a trade-off between the values of t and ν. If t is very small then, due to the constraint
6The loss function ℓν used in the empirical error term is the hinge loss. For a positive real ν, ℓν(y1,y2) = 0
if y1y2 ≥ν and ℓν(y1,y2) = min{1, 1 −y1y2/ν}.
7Using the standard technique of (see Theorem 18 there) it is possible to
extend (18) to be uniform in ν.
Mach Learn 72: 173–188
(6), all entries of the hypothesis h generated by AVR are very close to zero. In this case, to
achieve a small empirical hinge-loss, the value of ν should also be small.
9 Experimental results
We tested the AVR algorithm over 31 binary problems including all 7 datasets from
 and all 8 datasets from . We also generated 6 datasets of image classiﬁcation problems from the COIL-100 dataset , and took all 10 possible binary problems from the comp.* “super-category” in the
20-newsgroups dataset. We randomly subsampled large datasets to contain exactly 1500 examples and in all experiments we used a training set of size 100. We represented text datasets
using word-based TF-IDF scores and normalized other datasets using a linear transformation
such that the dynamic range of their attributes is .
We compared AVR with SVM and TSVM .8 In all problems, with
the exception of the text datasets, SVM and TSVM were applied with an RBF kernel. For
the text problems, slightly better performance was obtained with a linear kernel. All relevant hyperparameters of the SVM, TSVM and the AVR algorithm were selected using 5fold cross validation (over the training set), from “reasonable” grids of possible values. For
SVM/TSVM, the grid contained 80 possible hyperparameter assignments9 and for AVR, 72
assignments (as described below).
9.1 On the AVR hyperparameters
The AVR algorithm has a number of parameters. The main parameter, which is left unspeciﬁed, is the matrix Q. One natural choice for Q is graph-based using the unnormalized
Laplacian.10 We constructed Q using the unlabeled data Xm+u as follows. We computed an
(m+u)×(m+u) similarity matrix S, whose (i,j)th entry is the cosine of the angle between
xi and xj. Then we built an undirected k-nearest neighbors weighted graph, G = G(Xm+u),
where there is an edge between xi and xj iff xi is among the k most similar (according to
S) neighbors of xj, or vice versa. Edge weights were taken to be the corresponding entries
from S.11 The value of k was selected using 5-fold cross validation from the set {5,10,100}.
Let M be the adjacency matrix of G, and let D be the diagonal matrix whose (i,i)th entry
is the sum of the ith row of M. Let L = D −M be the unnormalized Laplacian of G.
8We applied both SVM and TSVM using the UniverSVM package of F. Sinz, R. Collobert, J. Weston and L.
Bottou, available at 
9The SVM hyperparameters are C (weight of errors of labeled examples) and σ
(the “width”
considered
(C,1/σ 2) ∈{2−5,2−3,2−1,21,23,25,27,29} ×
{2−15,2−13,2−11,2−9,2−7,2−5,2−3,2−1,21,23}. TSVM has additional hyperparameter C∗(weight
of errors of unlabeled examples) and we considered all triples (C,C∗,1/σ 2) = {2−3,21,25,29} ×
{2−3,21,25,29} × {2−13,2−9,2−5,2−1,23}. For text datasets the hyperparameter σ was not used.
10There are, of course, other possibilities for Q, such as a normalized Laplacian, linear models and RBF kernels. We have done preliminary experiments with each one of these possibilities for Q and found that they result in roughly the same performance as the choice of Q based on the
unnormalized Laplacian.
11Our dataset normalization procedure (described above) implies that the entries of S are non-negative and
thus, edge weights in G are non-negative as well.
Mach Learn 72: 173–188
Remark 1 (On the meaning of the eigenvectors of L) Let
G = {g | g ∈Rm+u,∥g∥2 = 1}.
For any g ∈G, let gT Lg = m+u
i,j=1(gi −gj)2mij be its “soft smoothness” w.r.t. the graph
G(Xm+u), where mij is the (i,j)th entry of M. By the Rayleigh-Ritz theorem , the smallest eigenvalue of L is λ1 = ming∈G gT Lg and its eigenvector is
v1 = argming∈G gT Lg. Thus, the ﬁrst eigenvector v1 has the maximal smoothness (of value
λ1). A generalization of this theorem in implies that for any 1 ≤
r ≤m + u, λr = ming∈G,gT v1=0,...,gT vr−1=0 gT Lg, and the minimum is achieved by vr. Thus,
the r-th largest eigenvector is the maximally smooth vector (with smoothness λr) among all
the vectors that are orthogonal to the r −1 maximally smooth vectors. By taking Q
we therefore prioritize highly smooth equivalent classes.
Although we would like to assign Q
△= L, it is well known that L is positive semi-deﬁnite.
We need Q to be positive-deﬁnite to ensure a ﬁnite volume.12 To this end, we truncate the
larger eigenvectors of L using the following simple heuristic. We ﬁx two parameters: a
threshold τ > 0 and 0 < c < 1, and truncate the l = ⌈c · (m + u)⌉largest eigenvectors to
length τ. Let μ be the “stretch factor” of the lth largest vector (new length/old length). To
preserve length proportions among the m+u−l smallest eigenvectors we stretch (or shrink)
them by a factor μ. In our experiments we selected the values of the hyperparameters τ and
c from the sets {1,10} and {0.05,0.1,0.9,0.95}, respectively, and set t = 1. Finally, the
hyperparameter γ was selected from {0.01/m,1/m,100/m}.
Remark 2 (On the computational complexity of AVR) By our construction of Q the computational complexity of AVR is dominated by the eigendecomposition of the graph Laplacian13 L. In general the complexity of this decomposition is O((m + u)3). For small
k ≪m + u, the matrix L is very sparse and the eigendecomposition can be computed faster.
In Sect. 10 we discuss a fast method for constructing Q without performing costly eigendecompositions.
Remark 3 (On the choice of the loss function) The solution (9) of the AVR optimization problem involves the inversion of the (m + u) × (m + u) matrix γ Q −ρI. This operation is computationally expensive and has time complexity of O((m + u)2.376) . Let {(λi,vi)}m+u
i=1 be the eigendecomposition of Q. Recall
that by our method of constructing Q, we know its eigendecomposition before computing
(γ Q −ρI)−1. Since
(γ Q −ρI)−1 =
γ λi −ρ vivT
given the eigendecomposition of Q, the inverse (γ Q −ρI)−1 can be computed fast. Note
that the eigendecomposition of Q is independent of the training/test partition and the choice
of the hyperparameters γ , τ and c. Thus, we can reuse the eigendecomposition of Q for
different values of γ , τ, c and different training/test partitions and speed-up our experiments.
12If Q is semi-deﬁnite then its smallest eigenvalue λ1 is zero. In this case the length (1/√λ1) of the principal
vector v1 (corresponding to λ1) of the ellipse E(HQ) (see Fig. 2) is inﬁnite.
13Recall that this eigendecomposition is required in order to make L positive deﬁnite.
Mach Learn 72: 173–188
This reuse would be impossible if instead of the linear loss −yihi we took the commonly
used squared loss14 (yi −hi)2, resulting in an order of magnitude slow-down.
9.2 Results
Our results for the 31 datasets appear in Tables 1 and 2. Each experiment was performed
12 times with different random train/test partitions. In Tables 1 and 2, each entry is an
average (± standard error of the mean) of these 12 experiments. It is evident that AVR
overwhelmingly outperforms SVM and TSVM on the dataset collection of Table 1. In particular, AVR exhibits excellent performance in text categorization and image classiﬁcation.
However, AVR is signiﬁcantly inferior to SVM/TSVM on the UCI datasets of Table 2.
Table 1 Results for three dataset collections
Datasets from 
23.07 ± 0.44
18.62±1.09
20.21±1.40
25.26 ± 0.40
23.29 ± 0.87
6.03 ± 0.53
9.07 ± 0.45
8.11 ± 0.95
17.41 ± 1.31
17.15 ± 1.39
31.75±1.21
32.92±0.47
48.94 ± 0.99
25.47±0.74
24.05±0.88
24.73±0.47
Image datasets
12.35±0.45
12.21±0.39
11.63±0.37
9.37 ± 0.23
8.25 ± 0.34
20.04 ± 0.56
18.44 ± 0.61
12.17±0.71
12.35 ± 0.67
9.73 ± 0.35
25.75 ± 1.46
24.69 ± 1.89
15.62±0.87
24.46 ± 1.07
23.13 ± 0.90
Text (20 Newsgroups)
graphics/misc
19.79 ± 1.46
17.54 ± 1.09
14.76±0.34
graphics/pc
16.86 ± 1.79
13.96 ± 1.39
graphics/mac
12.85 ± 1.68
10.39 ± 1.28
graphics/X
20.99 ± 2.12
16.42 ± 1.17
14.36±0.76
21.25 ± 1.79
19.40 ± 1.30
16.12±0.68
13.74 ± 1.70
11.83±1.24
10.90±0.35
16.91 ± 2.13
13.63±1.44
12.85±0.49
23.41±2.00
20.40±1.21
20.42±0.78
9.79 ± 2.27
8.76 ± 1.38
10.73 ± 2.55
8.27 ± 1.35
14If we use the squared loss, then instead of (9) we would obtain h =
2m (γ Q −ρI + I∗)−1y, where I∗is
a diagonal matrix whose (i,i)th entry equals 1, if the ith example is in the training set, and zero otherwise.
The inverse in the last expression cannot be computed using the eigendecomposition of Q.
Mach Learn 72: 173–188
Table 2 UCI datasets taken from 
27.96±1.06
27.97±1.07
36.78 ± 0.83
34.52±0.86
32.99±1.09
36.63 ± 1.20
12.44±0.70
11.75±0.59
15.62 ± 0.78
1.93 ± 0.82
20.16 ± 0.26
ionosphere
16.50 ± 0.61
26.96±1.73
29.08±1.67
37.75 ± 2.05
7.06 ± 0.43
Fig. 4 Accuracy of the eigenvectors of Q
9.3 Analysis of results
We investigated further cases where the AVR succeeded and failed and found two empirical
characterizations of its performance.
9.3.1 Accuracy of eigenvectors
Let {λi,vi}m+u
i=1 be the eigenvectors and eigenvalues of Q, such that λ1 ≤λ2 ≤··· ≤λm+u.
Since vi ∈Rm+u, we consider it as a vector of soft classiﬁcations of the full sample examples from Xm+u. In particular, we consider the jth entry of vi as the soft classiﬁcation of
xj. For each 1 ≤i ≤m + u we computed the training accuracy of vi and −vi and took the
best accuracy among these two numbers. The resulting graphs of eigenvector training accuracies, averaged over 12 training/test partitions, are shown in Fig. 4 for 4 datasets. For each
Mach Learn 72: 173–188
Fig. 5 Comparison of AVR
versus TSVM: (a) TSVM looses
to SVM; (b) TSVM wins over
dataset we chose the value of k yielding the best performance in hindsight. We truncated
the graphs to include the 400 smallest eigenvalues, since the accuracies of the eigenvectors
corresponding to larger eigenvalues are almost always as those obtained by the eigenvectors
corresponding to the eigenvalues with indices 200–400.
Figure 4 shows that in two datasets where AVR succeeds (DIGIT and MUSH) there are
a few very accurate eigenvectors, which correspond to small eigenvalues of Q. Moreover,
in these datasets there is a large gap in the accuracy of these eigenvectors and the others and
there are no accurate eigenvectors corresponding to large eigenvalues. In contrast, in datasets
where AVR failed (BCI and MONK) the accuracy of the eigenvectors corresponding to small
eigenvalues is quite low. Qualitatively similar effects were observed in all the other datasets.
The above characterization in terms of the accuracies of the eigenvectors of Q suggests
the following heuristic to quickly assess whether we should use AVR or large margin methods (SVM or TSVM). If the accuracy of the leading eigenvectors of Q is high relative to the
accuracy of the large-margin methods, then run AVR with cross validation to determine its
best parameters. Otherwise, use a large-margin method.
9.3.2 Magniﬁcation of TSVM success and failure
Using the results of Tables 1 and 2 we divide all 31 datasets into two categories. The ﬁrst
category consists of the datasets where SVM outperformed TSVM. The second category
consists of those where TSVM outperformed SVM. There are 4 datasets in the former category and 27 in the latter. Note that in this partition we measure performance by considering
only average errors and ignore standards error of the means.
A comparison of AVR and TSVM over datasets of these two categories is depicted in
Fig. 5 using scatter plots. In these plots each point represents a comparison on a single
dataset. If the point falls below the dashed line then AVR outperformed TSVM, and vice
versa. It is evident that if SVM outperformed TSVM then TSVM also outperformed AVR.
Conversely, if TSVM outperformed SVM, then in the signiﬁcant majority of the datasets,
AVR also outperformed TSVM. Thus, in cases where transductive learning was effective
(in the sense that TSVM outperformed SVM), the AVR algorithm magniﬁed the success of
TSVM, and vice versa,
10 Concluding remarks
We developed a new transductive technique based on a large volume principle. The new
technique is well motivated using the transductive maximal power inference. The resulting
AVR algorithm that approximates this scheme is extremely successful in three 72: 173–188
four) sets of problems we examined (in particular, in text categorization and image classi-
ﬁcation problems) and fails in a set of UCI problems. The main questions are: why does
AVR fail in the last set? How can we make a better data-dependent selection of the ellipsoid
One possible direction could be to explicitly design Q matrices by encoding in eigenvectors and eigenvalues useful prior knowledge and information about the given data. We note
that such constructions can also be beneﬁcial from a computational complexity viewpoint
since they would save the need for the spectral decompositions we currently perform.
It would be very interesting to identify datasets’ characteristics that give an advantage
to either the large margin or the large volume principle. In this regard, we note that in our
comparison here these two principles were applied on different hypothesis spaces, namely,
(kernelized) hyperplanes in the case of large margin and arbitrary soft response vectors in the
case of large volume (which has much larger capacity). It would be of interest to compare
these two principles w.r.t. the same space. We observed that the AVR algorithm magni-
ﬁed the success and failure of TSVM. We plan to further investigate this interesting effect.
further. Finally, a technical interesting question is how to better approximate or provide
approximation guarantees for volume assessments in the context of our elliptic hypothesis