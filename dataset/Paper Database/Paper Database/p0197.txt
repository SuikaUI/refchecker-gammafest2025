DeeperForensics-1.0: A Large-Scale Dataset for
Real-World Face Forgery Detection
Liming Jiang1 Ren Li2 Wayne Wu1,2 Chen Qian2 Chen Change Loy1†
1Nanyang Technological University
2SenseTime Research
 
 
 
 
 
Figure 1: DeeperForensics-1.0 dataset is a new large-scale dataset for real-world face forgery detection.
We present our on-going effort of constructing a largescale benchmark for face forgery detection. The ﬁrst version of this benchmark, DeeperForensics-1.0, represents the
largest face forgery detection dataset by far, with 60, 000
videos constituted by a total of 17.6 million frames, 10
times larger than existing datasets of the same kind. Extensive real-world perturbations are applied to obtain a more
challenging benchmark of larger scale and higher diversity.
All source videos in DeeperForensics-1.0 are carefully collected, and fake videos are generated by a newly proposed
end-to-end face swapping framework. The quality of generated videos outperforms those in existing datasets, validated by user studies. The benchmark features a hidden test
set, which contains manipulated videos achieving high deceptive scores in human evaluations. We further contribute
a comprehensive study that evaluates ﬁve representative detection baselines and make a thorough analysis of different
settings. 1,2
1 GitHub: 
2 Project page: 
† Corresponding author.
1. Introduction
Face swapping has become an emerging topic in computer vision and graphics. Indeed, many works 
on automatic face swapping have been proposed in recent
years. These efforts have circumvented the cumbersome
and tedious manual face editing processes, hence expediting the advancement in face editing. At the same time, such
enabling technology has sparked legitimate concerns, particularly on its potential for being misused and abused. The
popularization of “Deepfakes” on the internet has further
set off alarm bells among the general public and authorities, in view of the conceivable perilous implications. Accordingly, there is a dire need for countermeasures to be in
place promptly, particularly innovations that can effectively
detect videos that have been manipulated.
Working towards forgery detection, various groups have
contributed datasets (e.g., FaceForensics++ , Deep Fake
Detection and DFDC ) comprising manipulated
video footages.
The availability of these datasets has
undoubtedly provided essential avenues for research into
forgery detection. Nonetheless, the aforementioned datasets
suffer several drawbacks. Videos in these datasets are either of a small number, of low quality, or overly artiﬁ-
 
cial. Understandably, these datasets are inadequate to train
a good model for effective forgery detection in real-world
scenarios. This is particularly true when current advances
in human face editing are able to produce extremely realistic videos, rendering forgery detection a highly challenging
task. On another note, we observe high similarity between
training and test videos, in terms of their distribution, in certain works . Their actual efﬁcacy in detecting realworld face forgery cases, which are much more variable and
unpredictable, remains to be further elucidated.
We believe that forgery detection models can only be
enhanced when trained with a dataset that is exhaustive
enough to encompass as many potential real-world variations as possible. To this end, we propose a large-scale
dataset named DeeperForensics-1.0 consisting of 60, 000
videos with a total of 17.6 million frames for real-world face
forgery detection. The main steps of our dataset construction are shown in Figure 1. We set forth three yardsticks
when constructing this dataset: 1) Quality. The dataset shall
contain videos more realistic and much closer to the distribution of real-world detection scenarios. (Section 3.1 and
3.2) 2) Scale. The dataset shall be made up of a large-scale
video sets. (Section 3.3) 3) Diversity. There shall be suf-
ﬁcient variations in the video footages (e.g., compression,
blurry, transmission errors) to match those that may be encountered in the real world (Section 3.3).
The primary challenge in the preparation of this dataset
is the lack of good-quality video footages.
Speciﬁcally,
most publicly available videos are shot under an unconstrained environment resulting in large variations, including
but not limited to suboptimal illumination, large occlusion
of the target faces, and extreme head poses. Importantly, the
lack of ofﬁcial informed consents from the video subjects
precludes the use of these videos, even for non-commercial
purposes. On the other hand, while some videos of manipulated faces are deceptively real, a larger number remains
easily distinguishable by human eyes. The latter is often
caused by model negligence towards appearance variations
or temporal differences, leading to preposterous and incongruous results.
We approach the aforementioned challenge from two
perspectives. 1) Collecting fresh face data from 100 individuals with informed consents (Section 3.1). 2) Devising
a novel method, DeepFake Variational Auto-Encoder (DF-
VAE), to enhance existing videos (Section 3.2). In addition,
we introduce diversity into the video footages through deliberate addition of distortions and perturbations, simulating
real-world scenarios. We collate the newly collected data
and the DF-VAE-modiﬁed videos into the DeeperForensics-
1.0 dataset, with the aim of further expanding it gradually
over time. We benchmark ﬁve representative open-source
forgery detection methods using our dataset as well as a hidden test set containing manipulated videos that achieve high
deceptive ranking in user studies.
We summarize our contributions as follows: 1) We propose a new dataset, DeeperForensics-1.0 that is larger in
scale than existing ones, of high quality and rich diversity.
To improve its quality, we introduce a carefully designed
data collection and a novel framework, DF-VAE, that effectively mitigate obvious fabricated effects of existing manipulated videos. DeeperForensics-1.0 dataset shall facilitate
future research in forgery detection of human faces in realworld scenarios. 2) We benchmark results of existing representative forgery detection methods on our dataset, offering
insights into the current status and future strategy in face
forgery detection.
2. Related Work
This paper includes two main aspects of face forgery detection related to other works: dataset and benchmark. We
will cover some important works in this section.
Face forgery detection datasets. Building a dataset for
forgery detection requires a huge amount of effort on
data collection and manipulation. Early forgery detection
datasets comprise images captured under highly restrictive
conditions, e.g., MICC F2000 , Wild Web dataset ,
Realistic Tampering dataset .
Owing to the urgency in video-based face forgery detection, some prominent groups have devoted their efforts to
create face forensics video datasets (see Table 1). UADFV
 contains 98 videos, i.e., 49 real videos from YouTube
and 49 fake ones generated by FakeAPP . DeepFake-
TIMIT manually selects 16 similar looking pairs of
people from VidTIMIT database. For each of the 32
subjects, they generate about 10 videos using low-quality
and high-quality versions of faceswap-GAN , resulting
in a total of 620 fake videos. Celeb-DF includes 408
YouTube videos, mostly of celebrities, from which 795 fake
videos are synthesized. FaceForensics++ is the ﬁrst
large-scale face forensic dataset that consists of 4, 000 fake
videos manipulated by four methods (i.e., DeepFakes ,
Face2Face , FaceSwap , NeuralTextures )), and
1, 000 real videos from YouTube. Afterwards, Google joins
FaceForensics++ and contributes Deep Fake Detection 
dataset with 3, 431 real and fake videos from 28 actors.
Recently, Facebook invites 66 individuals and builds the
DFDC preview dataset , which includes 5, 214 original
and tampered videos with three types of augmentations.
In comparison, we invite 100 paid actors and collect
high-resolution source data with various
poses, expressions, and illuminations. 3DMM blendshapes
 are taken as reference to supplement some extremely
exaggerated expressions. We get consents from all the actors for using and manipulating their faces. In contrast to
prior works, we also propose a new end-to-end face swapping method (i.e., DF-VAE) and systematically apply seven
types of perturbations to the fake videos at ﬁve intensity levels. The mixture of distortions to a single video makes our
Table 1: The most relevant datasets compared to our dataset. DeeperForensics-1.0 is an order of magnitude larger in scale than existing
datasets w.r.t. both real and fake parts. We build a professional indoor environment to better control the important attributes of the collected
data. 100 paid actors give consents to the use and manipulation of their faces by signing a formal agreement. We employ seven types
of perturbations at ﬁve intensity levels, leading to 35 perturbations in total. The video may be subjected to a mixture of more than one
perturbation. In contrast to prior works, we also introduce a new end-to-end high-ﬁdelity face swapping method.
Total videos
(real : fake)
Controlled
Perturbations
(total number)
Perturbations
UADFV 
DeepFake-TIMIT 
Celeb-DF 
FaceForensics++ 
Deep Fake Detection 
(joins FaceForensics++)
DFDC Preview Dataset 
DeeperForensics-1.0 (Ours)
dataset better imitate real-world scenarios. Ultimately, we
construct DeeperForensics-1.0 dataset, which contains up
to 60, 000 high-quality videos with a total of 17.6 million
Face forgery detection benchmarks.
A new prominent benchmark, FaceForensics Benchmark , for facial
manipulation detection has been proposed recently.
benchmark includes six image-level face forgery detection
baselines . Although FaceForensics
Benchmark adds distortions to the videos by converting
them into different compression rates, a deeper exploration
of more perturbation types and their mixture is missing.
Celeb-DF also provides a face forgery detection benchmark including seven methods 
trained and tested on different datasets. In aforementioned
benchmarks, the test set usually shares a similar distribution
with the training set. Such an assumption inherently introduces biases and renders these methods impractical for face
forgery detection in real-world settings with much more diverse and unknown fake videos.
In our benchmark, we introduce a challenging hidden
test set with manipulated videos that achieve high deceptive
scores in user studies, to better simulate real-world distribution. Various perturbations are analyzed to make our benchmark more comprehensive. In addition, we mainly exploit
video-level forgery detection baselines .
Temporal information – a signiﬁcant cue for video forgery
detection besides single-frame quality – has been considered. We will elaborate our benchmark in Section 4.
3. A New Large-Scale Face Forensics Dataset
The main contribution of this paper is a new largescale
real-world
detection,
DeeperForensics-1.0, which provides an alternative to existing databases. DeeperForensics-1.0 consists of 60, 000
videos with 17.6 million frames in total, including 50, 000
original collected videos and 10, 000 manipulated videos.
To construct a dataset more suitable for real-world face
YouTube Source
Collected Source
Figure 2: Comparison of using only YouTube video and the collected video as source data, with the same method and setting.
forgery detection, we design this dataset with careful consideration of quality, scale, and diversity. In Section 3.1
and 3.2, we will discuss the details of data collection and
methodology (i.e., DF-VAE) to improve quality. In Section 3.3, we will show how to ensure large scale and high
diversity of DeeperForensics-1.0.
3.1. Data Collection
Source data is the ﬁrst factor that highly affects quality. Taking results in Figure 2 as an example, the source
data collection increases the robustness of our face swapping method to extreme poses, since videos on the internet
usually have limited head pose variations.
We refer to the identity in the driving video as the “target” face and the identity of the face that is swapped onto
the driving video as the “source” face. Different from previous works, we ﬁnd that the source faces play a much more
critical role than the target faces in building a high-quality
dataset. Speciﬁcally, the expressions, poses, and lighting
conditions of source faces should be much richer in order to
perform robust face swapping. Hence, our data collection
mainly focuses on source face videos. Figure 3 shows the
Figure 3: Diversity in identities, poses, expressions, and illuminations in our collected source data.
Figure 4: Examples of 3DMM blendshapes in our data collection.
diversity in different attributes of our data collection.
We invite 100 paid actors to record the source videos.
Similar to , we obtain consents from all the actors
for using and manipulating their faces to avoid the portrait
right issues. The participants are carefully selected to ensure variability in genders, ages, skin colors, and nationalities. We maintain a roughly equal proportion w.r.t. each of
the attributes above. In particular, we invite 55 males and
45 females from 26 countries. Their ages range from 20 to
45 years old to match the most common age group appearing on real-world videos. The actors have four typical skin
tones: white, black, yellow, brown, with ratio 1:1:1:1. All
faces are clean without glasses or decorations.
Different from previous data collection in the wild (see
Table 1), we build a professional indoor environment for a
more controllable data collection. We only use the facial
regions (detected and cropped by LAB ) of the source
data, so we can neglect the background. We set seven HD
cameras from different angles: front, left, left-front, right,
right-front, oblique-above, oblique-below. The resolution
of our recorded videos is high . We train
the actors in advance to keep the collection process smooth.
We request the actors to turn their heads and speak naturally
with eight expressions: neutral, angry, happy, sad, surprise,
contempt, disgust, fear. The head poses range from −90◦
Figure 5: Examples of style mismatch problems in prominent face
forensics datasets.
to +90◦. Furthermore, the actors are asked to perform 53
expressions deﬁned in 3DMM blendshapes (see Figure 4) to supplement some extremely exaggerated expressions. When performing 3DMM blendshapes, the actors
also speak naturally to avoid excessive frames that show a
closed mouth.
In addition to expressions and poses, we systematically
set nine lighting conditions from various directions: uniform, left, top-left, bottom-left, right, top-right, bottomright, top, bottom. The actors are only asked to turn their
heads under uniform illumination, so the lighting remains
unchanged on speciﬁc facial regions to avoid many duplicated data samples recorded by the cameras set at different
angles. In the end, our collected data contain over 50, 000
videos with a total of 12.6 million frames – an order of magnitude more than existing datasets.
3.2. DeepFake Variational Auto-Encoder
To tackle low visual quality problems of previous works,
we consider three key requirements in formulating a high-
ﬁdelity face swapping method: 1) It should be general and
scalable for us to generate large number of videos with high
quality. 2) The problem of face style mismatch caused by
appearance variations need to be addressed. Some failure
cases of existing methods are shown in Figure 5. 3) Temporal continuity of generated videos should be taken into
consideration.
Based on the aforementioned requirements, we propose
DeepFake Variational Auto-Encoder (DF-VAE), a novel
learning-based face swapping framework. DF-VAE consists of three main parts, namely a structure extraction mod-
Figure 6: The main framework of DeepFake Variational Auto-Encoder. In training, we reconstruct the source and target faces in blue and
orange arrows, respectively, by extracting landmarks and constructing an unpaired sample as the condition. Optical ﬂow differences are
minimized after reconstruction to improve temporal continuity. In inference, we swap the latent codes and get the reenacted face in green
arrows. Subsequent MAdaIN module fuses the reenacted face and the original background resulting in the swapped face.
ule, a disentangled module, and a fusion module. We will
give a brief and intuitive understanding of the DF-VAE
framework below. Please refer to the Appendix for detailed
derivations and results.
Disentanglement of structure and appearance. The ﬁrst
step of our method is face reenactment – animating the
source face with similar expression as the target face, without any paired data. Face swapping is considered as a subsequent step of face reenactment that performs fusion between
the reenacted face and the target background. For robust
and scalable face reenactment, we should cleanly disentangle structure (i.e., expression and pose) and appearance representation (i.e., texture, skin color, etc.) of a face. This
disentanglement is rather difﬁcult because structure and appearance representation are far from independent. We describe our solution as follows.
Let x1:T ≡{x1, x2, ..., xT } ∈X be a sequence of
source face video frames, and y1:T ≡{y1, y2, ..., yT } ∈Y
be the sequence of corresponding target face video frames.
We ﬁrst simplify our problem and only consider two speciﬁc snapshots at time t, xt and yt. Let ˜xt, ˜yt, dt represent
the reconstructed source face, the reconstructed target face,
and the reenacted face, respectively.
Consider the reconstruction procedure of the source face
xt. Let sx denotes the structure representation and ax denotes the appearance information. The face generator can
be depicted as the posteriori estimate pθ (xt|sx, ax). The
solution of our reconstruction goal, marginal log-likelihood
˜xt ∼log pθ (xt), by a common Variational Auto-Encoder
(VAE) can be written as:
log pθ (xt) = DKL (qφ (sx, ax|xt) ∥pθ (sx, ax|xt))
+L (θ, φ; xt) ,
where qφ is an approximate posterior to achieve the evidence lower bound (ELBO) in the intractable case, and the
second RHS term L (θ, φ; xt) is the variational lower bound
w.r.t. both the variational parameters φ and generative parameters θ.
In Eq. (1), we assume that both sx and ax are latent priors computed by the same posterior xt. However, the separation of these two variables in the latent space is rather
difﬁcult without additional conditions. Therefore, we employ a simple yet effective approach to disentangle these
two variables.
The blue arrows in Figure 6 demonstrate the reconstruction procedure of the source face xt. Instead of feeding a
single source face xt, we sample another source face x′ to
construct unpaired data in the source domain. To make the
structure representation more evident, we use the stacked
hourglass networks to extract landmarks of xt in the
structure extraction module and get the heatmap ˆxt. Then
we feed the heatmap ˆxt to the Structure Encoder Eα, and x′
to the Appearance Encoder Eβ. We concatenate the latent
representations (small cubes in red and green) and feed it to
the Decoder Dγ. Finally, we get the reconstructed face ˜xt,
i.e., marginal log-likelihood of xt.
Therefore, the latent structure representation sx in
Eq. (1) becomes a more evident heatmap representation ˆxt,
Figure 7: Many-to-many (three-to-three) face swapping by a single model with obvious reduction of style mismatch problems. This ﬁgure
shows the results between three source identities and three target identities. The whole process is end-to-end.
which is introduced as a new condition. The unpaired sample x′ with the same identity w.r.t. xt is another condition,
being a substitute for ax. Eq. (1) can be rewritten as a conditional log-likelihood:
log pθ (xt|ˆxt, x′) = DKL (qφ (zx|xt, ˆxt, x′) ∥pθ (zx|xt, ˆxt, x′))
+L (θ, φ; xt, ˆxt, x′) ,
The ﬁrst RHS term KL-divergence is non-negative, we get:
 xt|ˆxt, x′
≥L(θ, φ; xt, ˆxt, x′)
= Eqφ(zx|xt,ˆxt,x′)
 zx|xt, ˆxt, x′
 xt, zx|ˆxt, x′
and L(θ, φ; xt, ˆxt, x′) can also be written as:
L (θ, φ; xt, ˆxt, x′) = −DKL (qφ (zx|xt, ˆxt, x′) ∥pθ (zx|ˆxt, x′))
+ Eqφ(zx|xt,ˆxt,x′) [log pθ (xt|zx, ˆxt, x′)] .
We let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:
log qφ (zx|xt, ˆxt, x′) ≡log N
 zx; µ, σ2I
where I is an identity matrix.
Exploiting the reparameterization trick , the non-differentiable operation of
sampling can become differentiable by an auxiliary variable with independent marginal.
In this case, zx
qφ (zx|xt, ˆxt, x′) is implemented by zx = µ + σϵ where
ϵ is an auxiliary noise variable ϵ ∼N(0, 1). Finally, the
approximate posterior qφ(zx|xt, ˆxt, x′) is estimated by the
separated encoders, Structure Encoder Eα and Appearance
Encoder Eβ, in an end-to-end training process by standard
gradient descent.
We discuss the whole workﬂow of reconstructing the
source face. In the target face domain, the reconstruction
procedure is the same, as shown by orange arrows in Figure 6.
During training, the network learns structure and appearance information in both the source and the target domains.
It is noteworthy that even if both yt and x′ belong to arbitrary identities, our effective disentangled module is capable
of learning meaningful structure and appearance information of each identity. During inference, we concatenate the
appearance prior of x′ and the structure prior of yt (small
cubes in red and orange) in the latent space, and the reconstructed face dt shares the same structure with yt and
keeps the appearance of x′. Our framework allows concatenations of structure and appearance latent codes extracted
from arbitrary identities in inference and permits many-tomany face reenactment.
In summary, DF-VAE is a new conditional variational
auto-encoder with robustness and scalability. It condi-
w/o MAdaIN
Figure 8: Comparison of the swapped face styles without or with
MAdaIN module.
tions on two posteriors in different domains. In the disentangled module, the separated design of two encoders Eα
and Eβ, the explicit structure heatmap, and the unpaired
data construction jointly force Eα to learn structure information and Eβ to learn appearance information.
Style matching and fusion. To ﬁx the obvious style mismatch problems as shown in Figure 5, we introduce a
masked adaptive instance normalization (MAdaIN) module.
We place a typical AdaIN network after the reenacted
face dt. In the face swapping scenario, we only need to
adjust the style of the face area and use the original background. Therefore, we use a mask mt to guide AdaIN 
network to focus on style matching of the face area. To
avoid boundary artifacts, we apply Gaussian Blur to mt and
get the blurred mask mb
In our face swapping context, dt is the content input of
MAdaIN, yt is the style input. MAdaIN adaptively computes the afﬁne parameters from the face area of the style
MAdaIN (c, s) = σ (s)
where c = mb
t · dt, s = mb
With the very lowcost MAdaIN module, we reconstruct dt again by Decoder
Dδ. The blurred mask mb
t is used again to fuse the reconstructed image with the background of yt. At last, we get
the swapped face dt. Figure 8 shows the effectiveness of
MAdaIN module for style matching and fusion.
The MAdaIN module is jointly trained with the disentangled module in an end-to-end manner. Thus, by a single
model, DF-VAE can perform many-to-many face swapping
with obvious reduction of style mismatch and facial boundary artifacts (see Figure 7 for the face swapping between
three source identities and three target identities). Even if
there are multiple identities in both the source domain and
the target domain, the quality of face swapping does not degrade.
Temporal consistency constraint. Temporal discontinuity
of fake videos leads to obvious ﬂickering of the face area,
making them very easy to be spotted by forgery detection
methods and human eyes. To improve temporal continuity,
we let the disentangled module to learn temporal information of both the source face and the target face.
Table 2: Seven types of distortions in DeeperForensics-1.0.
Distortion Type
Color saturation change
Local block-wise distortion
Color contrast change
Gaussian blur
White Gaussian noise in color components
JPEG compression
Video compression rate change
For simpliﬁcation, we make a Markov assumption that
the generation of the frame at time t sequentially depends
on its previous P frames x(t−p):(t−1). In our experiment,
we set P = 1 to balance quality improvement and training
In order to build the relationship between a current frame
and previous ones, we further make an intuitive assumption
that the optical ﬂows should remain unchanged after reconstruction. We use FlowNet 2.0 to estimate the optical
ﬂow ˜xf w.r.t. ˜xt and xt−1, xf w.r.t. xt and xt−1. Since face
swapping is sensitive to minor facial details which can be
greatly affected by ﬂow estimation, we do not warp xt−1 by
the estimated ﬂow like . Instead, we minimize the difference between ˜xf and xf to improve temporal continuity
while keeping stable facial detail generation. To this end,
we propose a new temporal consistency constraint, which
can be written as:
Ltemporal =
CHW ∥˜xf −xf∥1,
where C = 2 for a common form of optical ﬂow.
We only discuss the temporal continuity w.r.t. the source
face in this section because the case of the target face is the
same. If multiple identities exist in one domain, temporal
information of all these identities can be learned in an endto-end manner.
3.3. Scale and Diversity
Our extensive data collection and the proposed DF-VAE
method are designed to improve the quality of manipulated
videos in DeeperForensics-1.0 dataset. In this section, we
will mainly discuss the scale and diversity aspects.
We provide 10, 000 manipulated videos with 5 million
frames. It is also an order of magnitude more than the previous datasets. We take 1, 000 reﬁned YouTube videos collected by FaceForensics++ as the target videos. Each
face of our collected 100 identities is swapped onto 10 target videos, thus 1, 000 raw manipulated videos are generated directly by DF-VAE in an end-to-end process. Thanks
to the scalability and multimodality of DF-VAE, the time
overhead of model training and data generation is reduced
to 1/5 compared to the common Deepfakes methods, with
no degradation in quality. Thus, a larger-scale dataset construction is possible.
To ensure diversity, we apply various perturbations to
better simulate videos in real scenes. Speciﬁcally, as shown
in Table 2, seven types of distortions deﬁned in Image
Quality Assessment (IQA) are included. Each of
these distortions is divided into ﬁve intensity levels. We
apply random-type distortions to the 1, 000 raw manipulated videos at ﬁve different intensity levels, producing a
total of 5, 000 manipulated videos. Besides, an additional
of 1, 000 robust manipulated videos are generated by adding
random-type, random-level distortions to the 1, 000 raw manipulated videos. Moreover, in contrast to all the previous
datasets, each sample of another 3, 000 manipulated videos
in DeeperForensics-1.0 is subjected to a mixture of more
than one distortion.
The variability of perturbations improves the diversity of DeeperForensics-1.0 to better imitate
the data distribution of real-world scenarios.
DeeperForensics-1.0 is a new large-scale dataset consisting of over 60, 000 videos with 17.6 million frames
for real-world face forgery detection. High-quality source
videos and manipulated videos constitute two main contributions of the dataset. The diversity of perturbations applying to the manipulated videos ensures the robustness of
DeeperForensics-1.0 to simulate real scenes.
dataset is released, free to all research communities, for developing face forgery detection and more general humanface-related research.
3.4. User Study
To examine the quality of DeeperForensics-1.0 dataset,
we engage 100 professional participants, most of whom
specialize in computer vision research. We believe these
participants are qualiﬁed and well-trained in assessing realness of tempered videos. The user study is conducted on
DeeperForensics-1.0 and six former datasets, i.e., UADFV
 , DeepFake-TIMIT , Celeb-DF , FaceForensics++ , Deep Fake Detection , DFDC . We randomly select 30 video clips from each of these datasets and
prepare a platform for the participants to evaluate their realness. Similar to the user study of , the participants are
asked to provide their feedbacks to the statement “The video
clip looks real.” and give scores at ﬁve levels (1-clearly disagree, 2-weakly disagree, 3-borderline, 4-weakly agree, 5clearly agree. We assume that users who give a score of 4
or 5 think the video is “real”). The user study results are
presented in Table 3. The quality of our dataset is appreciated by most of the participants. Compared to the previous
datasets, DeeperForensics-1.0 achieves the highest realism
rating. Although Celeb-DF also gets very high realness
scores, the scale of our dataset is much larger.
4. Video Forgery Detection Benchmark
Dataset split. In our benchmark, we exploit 1, 000 raw manipulated videos in Section 3.3 and 1, 000 YouTube videos
from FaceForensics++ as our standard set. The videos
The percentage of user study ratings for UADFV,
DeepFake-TIMIT, Celeb-DF, FaceForensics++, Deep Fake Detection, DFDC, and DeeperForensics-1.0 dataset.
A higher score
means the users think the videos are more realistic.
UADFV 
DeepFake-TIMIT 
Celeb-DF 
FaceForensics++ 
Deep Fake Detection 
DeeperForensics-1.0 (Ours)
are split into training, validation, and test set with a ratio
of 7 : 1 : 2. The identities of the swapped faces may be
duplicated because faces of 100 invited actors are swapped
onto 1, 000 driving videos. To avoid data leak, we randomly
choose unrepeated 70, 10, and 20 identities, and group all
the videos according to the identities. Similar to , the
test and training sets share a close distribution in our standard set.
Other experiments in our benchmark are conducted on
different variants of the standard set. These variants share
the same 1, 000 driving videos with the standard set. We
will detail them in Section 4.2. For a fair comparison, all
the experiments are conducted in the same split setting.
Hidden test set. For real-world scenarios, some experiments conducted in previous works may not perform a convincing evaluation due to the huge biases caused
by a close distribution between the training and the test sets.
The aforementioned standard set has the same setting with
these works. As a result, strong detection baselines obtain
very high accuracy on the standard test set as demonstrated
in Section 4.2. However, the ultimate goal of the face forensics dataset is to help detect forgery in real scenes. Even if
the accuracy on the standard test set is high, the models may
easily fail in real-world scenarios.
We argue that the test set of real-world face forgery detection should not share a close distribution with the training
set. What we need is a test set that better simulates the realworld setting. We call it “hidden” test set. To better imitate
fake videos in the real scene, the hidden test set should satisfy three factors: 1) Multiple sources. Fake videos in-thewild should be manipulated by different unknown methods.
2) High quality. Threatening fake videos should have high
quality to fool human eyes. 3) Diverse distortions. Different perturbations should be taken into consideration.
Thus, in our initial benchmark, we introduce a challenging hidden test set with 400 carefully selected videos. First,
we collect fake videos generated by several unknown face
swapping methods to ensure multiple sources. Then, we obscure all selected videos multiple times with diverse hidden
distortions that are commonly seen in real scenes. Finally,
we only select videos that can fool at least 50 out of 100
human observers in a user study. The ground truth labels
are hidden and are used on our host server to evaluate the
Table 4: The binary detection accuracy of the baselines on the
hidden test set when trained on four manipulated methods in
FaceForensics++ (FF++):
DeepFakes (DF), Face2Face (F2F),
FaceSwap (FS), NeuralTextures (NT), and on DeeperForensics-
1.0 standard training set without distortions.
DeeperForensics-1.0
Test (acc)
ResNet+LSTM 
XceptionNet 
accuracy of detection models. Besides, the hidden test set
will be enlarged constantly to get future versions along with
development of Deepfakes technology. Fake videos manipulated by future face swapping methods will be included as
long as they can pass the human test supported by us.
4.1. Baselines
Existing studies primarily provide image-level
face forgery detection benchmark. However, fake videos
in-the-wild are much more menacing than manipulated images. We propose to conduct evaluation mainly based on
video classiﬁcation methods for two reasons. First, imagelevel face forgery detection methods do not consider any
temporal information – an important cue for video-based
tasks. Second, image-level methods have been widely studied. We only choose one image-level method, Xception-
Net , which achieves the best performance in , as
one part of our benchmark for reference. The other four
video-based baselines are C3D , TSN , I3D ,
and ResNet+LSTM , all of which have achieved
promising results in video classiﬁcation tasks. Details of
all the baselines will be introduced in our Appendix.
4.2. Results and Analysis
Owing to the goal of detecting fakes in real-world scenarios, we mainly explore how common distortions appearing in real scenes affect the model performance. Accuracies of face forgery detection on the standard test set and
the introduced hidden test set are evaluated under various
Evaluation of effectiveness of DeeperForensics-1.0. For
a fair comparison, we evaluate DeeperForensics-1.0 and the
state-of-the-art FaceForensics++ dataset because they
use the same driving videos. In this setting, we use 1, 000
raw manipulated videos without distortions in the standard
set of DeeperForensics-1.0. For FaceForensics++, the same
split is applied to its four subsets. All the models are tested
on the hidden test set (see Table 4).
The baselines trained on the standard training set of
DeeperForensics-1.0 achieve much better performance on
the hidden test set than all the four subsets of FaceForensics++. This proves the higher quality of DeeperForensics-
1.0 over prior works, making it more useful for real-world
Table 5: The binary detection accuracy of the baselines when
trained and tested on DeeperForensics-1.0 dataset with different
distortion perturbations. We analyze different training and testing settings on the standard set without distortions (std), the standard set with single-level distortions (std/sing), and the standard
set with random-level distortions (std/rand).
Test (acc)
ResNet+LSTM 
XceptionNet 
face forgery detection. In Table 4, I3D obtains the best
performance on the hidden test set when trained on the standard training set. We conjecture that the temporal discontinuity of fake videos leads to higher accuracy by this videolevel forgery detection method.
Evaluation of dataset perturbations. We study the effect
of perturbations towards the forgery detection model performance. In contrast to prior work , we try to evaluate
the baseline accuracies when applying different distortions
to the training and the test sets, in order to explore the function of perturbations in face forensics dataset.
In this setting, we conduct all the experiments on
DeeperForensics-1.0 dataset with high diversity of perturbations.
We use 1, 000 manipulated videos in the standard set (std), 1, 000 manipulated videos with single-level
(level-5), random-type distortions (std/sing), 1, 000 manipulated videos with random-level, random-type distortions
(std/rand). The data split is the same as that of the standard
set with a ratio of 7 : 1 : 2.
In Column 2 of Table 5, we ﬁnd the accuracy is nearly
100% when the models are trained and tested on the standard set. This is reasonable because the strong baselines
perform very well in a clean dataset with the same distribution. In Columns 3 and 4, the accuracy decrease compared to Column 2, when we choose std/sing and std/rand
as the test set. Most of the video-level methods except C3D
 are more robust to perturbations on test set than XceptionNet . This setting is very common because different
distributions of the training and the test sets lead to decrease
in model accuracies. Hence, the lack of perturbations in the
face forensics dataset cutbacks the model performance for
real-world face forgery detection with even more complex
data distribution.
When we apply corresponding distortions to the training
and test sets, the accuracy will increase (Column 5 and 6
in Table 5) compared to Column 3 and 4. However, this
setting is impractical because the distributions of the training and test sets are still the same. We should augment the
test set to better simulate the real-world distribution. Thus,
some evaluation settings in previous works are unreasonable. If we swap the training set and the test set of
std/sing and std/rand to further randomize the condition, re-
Table 6: The binary detection accuracy of the baselines on the
hidden test set when trained on DeeperForensics-1.0 dataset with
the standard set without distortions (std), combination of std and
the standard set with single-level distortions (std+std/sing), combination of std and the standard set with random-level distortions
(std+std/rand), combination of std and the standard set with the
mixed distortions(std+std/mix).
std+std/sing
std+std/rand
std+std/mix
Test (acc)
ResNet+LSTM 
XceptionNet 
sults shown in Column 7 and 8 indicate that the accuracy
remains high. This evaluation setting shows the possibility
that with the same generation method, exerting appropriate
distortions to the training set can make face forgery detection models more robust to real-world perturbations.
Evaluation of variants of training set for real-world face
forgery detection. We have conducted several experiments
for evaluations of possible perturbations. Nevertheless, the
case is more complex in real scenes because no information about the fake videos is available. The video may be
subjected to more than one type and diverse levels of distortions. In addition to distortions, the method manipulating
the faces is unknown.
From the evaluation of perturbations, we ﬁnd the possibility of augmenting the training set to improve detection model performance. Thus, we further evaluate baseline performance on the hidden test set by devising some
variants of the training set.
We perform experiments
on DeeperForensics-1.0.
In this setting, other than std,
std/sing, and std/rand, we use additional 1, 000 manipulated
videos, each of which is subjected to a mixture of three
random-level, random-type distortions (std/mix). We combine std with std/sing, std/rand, and std/mix, respectively,
yielding three new training sets (with the same data split as
the former settings).
Column 2 in Table 6 shows the low accuracy when the
models trained on std and tested on the hidden test set (same
as Column 6 in Table 4). Columns 3 and 4 indicate that the
accuracy of all the baseline models increase when trained
on std+std/sing and std+std/rand. The accuracy of I3D 
and ResNet+LSTM , are over 80% in some cases.
In a more complex setting, when the models are trained on
std+std/mix, Column 5 shows the accuracy of all the detection baselines further increase.
The results suggest that designing suitable training set
variants has the potential to help increase face forgery detection accuracy, and applying various distortions to ensure
the diversity of DeeperForensics-1.0 is necessary. In addition, compared to image-level method, video-level face
forgery detection methods have more potential capabilities
to crack real-world fake videos as shown in Table 6.
Although the accuracy on the challenging hidden test set
is still not very high, we provide two initial directions for
future real-world face forgery detection research: 1) Improving the source data collection and generation method
to ensure the quality of the training set; 2) Augmenting the
training set by various distortions to ensure its diversity. We
welcome researchers to make our benchmark more comprehensive.
5. Discussion
In this work, we propose a new large-scale dataset
named DeeperForensics-1.0 to facilitate the research of face
forgery detection towards real-world scenarios. We make
several efforts to ensure good quality, large scale, and high
diversity of this dataset. Based on the dataset, we further
benchmark existing representative forgery detection methods, offering insights into the current status and future strategy in face forgery detection. Several topics can be considered as future works. 1) We will continue to collect more
source and target videos to further expand DeeperForensics.
2) We plan to invite interested researchers for contributing
their video falsiﬁcation methods to enlarge our hidden test
set, as long as the fakes can pass the human test supported
by us. 3) A better evaluation metric for face forgery detection methods is also an interesting research topic.
Acknowledgments.
This work is supported by the
SenseTime-NTU Collaboration Project, Singapore MOE
AcRF Tier 1 , NTU SUG, and NTU
NAP. We gratefully acknowledge the exceptional help from
Hao Zhu and Keqiang Sun for their contribution on source
data collection and coordination.