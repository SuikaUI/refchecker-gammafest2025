Network Representation Learning: A Survey
Daokun Zhang, Jie Yin, Xingquan Zhu Senior Member, IEEE, Chengqi Zhang Senior Member, IEEE
Abstract—With the widespread use of information technologies, information networks are becoming increasingly popular to capture
complex relationships across various disciplines, such as social networks, citation networks, telecommunication networks, and
biological networks. Analyzing these networks sheds light on different aspects of social life such as the structure of societies,
information diffusion, and communication patterns. In reality, however, the large scale of information networks often makes network
analytic tasks computationally expensive or intractable. Network representation learning has been recently proposed as a new learning
paradigm to embed network vertices into a low-dimensional vector space, by preserving network topology structure, vertex content,
and other side information. This facilitates the original network to be easily handled in the new vector space for further analysis. In this
survey, we perform a comprehensive review of the current literature on network representation learning in the data mining and machine
learning ﬁeld. We propose new taxonomies to categorize and summarize the state-of-the-art network representation learning
techniques according to the underlying learning mechanisms, the network information intended to preserve, as well as the algorithmic
designs and methodologies. We summarize evaluation protocols used for validating network representation learning including
published benchmark datasets, evaluation methods, and open source algorithms. We also perform empirical studies to compare the
performance of representative algorithms on common datasets, and analyze their computational complexity. Finally, we suggest
promising research directions to facilitate future study.
Index Terms—Information networks, graph mining, network representation learning, network embedding.
INTRODUCTION
Nformation networks are becoming ubiquitous across a
large spectrum of real-world applications in forms of
social networks, citation networks, telecommunication networks and biological networks, etc. The scale of these networks ranges from hundreds to millions or even billions of
vertices . Analyzing information networks plays a crucial
role in a variety of emerging applications across many disciplines. For example, in social networks, classifying users
into meaningful social groups is useful for many important tasks, such as user search, targeted advertising and
recommendations; in communication networks, detecting
community structures can help better understand the rumor
spreading process; in biological networks, inferring interactions between proteins can facilitate new treatments for
diseases. Nevertheless, efﬁcient analysis of these networks
heavily relies on the ways how networks are represented.
Often, a discrete adjacency matrix is used to represent
a network, which only captures neighboring relationships
between vertices. Indeed, this simple representation cannot
embody more complex, higher-order structure relationships,
such as paths, frequent substructure etc. As a result, such
a traditional routine often makes many network analytic
tasks computationally expensive and intractable over largescale networks. Taking community detection as an example,
Daokun Zhang and Chengqi Zhang are with the Centre for Artiﬁcial
Intelligence, FEIT, University of Technology Sydney, Australia
Email: , .
Jie Yin is with the Discipline of Business Analytics, The University of
Sydney, Australia.
Email: .
Xingquan Zhu is with the Dept. of CEECS, Florida Atlantic University,
Email: .
Manuscript received 3 Dec., 2017; revised 26 Apr. 2018; accepted 12 June
2018. (Corresponding author: Jie Yin.)
most existing algorithms involve calculating the spectral
decomposition of a matrix with at least quadratic time
complexity with respect to the number of vertices. This
computational overhead makes algorithms hard to scale to
large-scale networks with millions of vertices.
Recently, network representation learning (NRL) has
aroused a lot of research interest. NRL aims to learn latent,
low-dimensional representations of network vertices, while
preserving network topology structure, vertex content, and
other side information. After new vertex representations are
learned, network analytic tasks can be easily and efﬁciently
carried out by applying conventional vector-based machine
learning algorithms to the new representation space. This
obviates the necessity for deriving complex algorithms that
are applied directly on the original network.
Earlier work related to network representation learning
dates back to the early 2000s, when researchers proposed
graph embedding algorithms as part of dimensionality reduction techniques. Given a set of i.i.d. (independent and
identically distributed) data points as input, graph embedding algorithms ﬁrst calculate the similarity between
pairwise data points to construct an afﬁnity graph, e.g., the
k-nearest neighbor graph, and then embed the afﬁnity graph
into a new space having much lower dimensionality. The
idea is to ﬁnd a low-dimensional manifold structure hidden
in the high-dimensional data geometry reﬂected by the constructed graph, so that connected vertices are kept closer to
each other in the new embedding space. Isomap , Locally
Linear Embedding (LLE) and Laplacian Eigenmap are
examples of algorithms based on this rationale. However,
graph embedding algorithms are designed on i.i.d. data
mainly for dimensionality reduction purpose. Most of these
algorithms usually have at least quadratic time complexity
with respect to the number of vertices, so the scalability is a
major issue when they are applied to large-scale networks.
Since 2008, signiﬁcant research efforts have shifted to
the development of effective and scalable representation
learning techniques that are directly designed for complex
information networks. Many NRL algorithms, e.g., , ,
 , , have been proposed to embed existing networks,
showing promising performance for various applications.
These algorithms embed a network into a latent, lowdimensional space that preserves structure proximity and
attribute afﬁnity, such that the original vertices of the network can be represented as low-dimensional vectors. The
resulting compact, low-dimensional vector representations
can be then taken as features to any vector-based machine
learning algorithms. This paves the way for a wide range
of network analytic tasks to be easily and efﬁciently tackled
in the new vector space, such as node classiﬁcation ,
 , link prediction , , clustering , recommendation , , similarity search , and visualization .
Using vector representation to represent complex networks
has now been gradually advanced to many other domains,
such as point-of-interest recommendation in urban computing , and knowledge graph search in knowledge
engineering and database systems.
Challenges
Despite its great potential, network representation learning
is inherently difﬁcult and is confronted with several key
challenges that we summarize as follows.
Structure-preserving: To learn informative vertex representations, network representation learning should preserve
network structure, such that vertices similar/close to each
other in the original structure space should also be represented similarly in the learned vector space. However, as
stated in , , the structure-level similarity between
vertices is reﬂected not only at the local neighborhood
structure but also at the more global community structure.
Therefore, the local and global structure should be simultaneously preserved in network representation learning.
Content-preserving: Besides structure information, vertices
of many networks are attached with rich content on attributes. Vertex attributes not only exert huge impacts on
the forming of networks, but also provide direct evidence to
measure attribute-level similarity between vertices. Therefore, if properly imported, attribute content can compensate
network structure to render more informative vertex representations. However, due to heterogeneity of the two information sources, how to effectively leverage vertex attributes
and make them compensate rather than deteriorate network
structure is an open research problem.
Data sparsity: For many real-world information networks,
due to the privacy or legal restrictions, the problem of
data sparsity exists in both network structure and vertex
content. At the structure level, only very limited links are
sometimes observed, making it difﬁcult to discover the
structure-level relatedness between vertices that are not explicitly connected. At the vertex content level, many values
of vertex attributes are usually missing, which increases
the difﬁculty of measuring content-level vertex similarity.
Thus, it is challenging for network representation learning
to overcome the data sparsity problem.
Scalability: Real-world networks, social networks in particular, consist of millions or billions of vertices. The large
scale of the networks challenges not only the traditional
network analytic tasks but also the newborn network representation learning task. Without special concern, learning
vertex representations for large-scale networks with limited
computing resources may cost months of time, which is
practically infeasible, especially for the case involving a
large number of trails for tuning parameters. Therefore,
it is necessary to design NRL algorithms that can learn
vertex representations efﬁciently and meanwhile guarantee
the effectiveness for large-scale networks.
Our Contribution
This survey provides a comprehensive up-to-date review
of the state-of-the-art network representation learning techniques, with a focus on the learning of vertex representations. It covers not only early work on preserving network
structure, but also a new surge of recent studies that incorporate vertex content and/or vertex labels as auxiliary
information into the learning process of network embedding. By doing so, we hope to provide a useful guideline
for the research community to better understand (1) new
taxonomies of network representation learning methods, (2)
the characteristics, uniqueness, and the niche of different
types of network embedding methods, and (3) the resources
and future challenges to stimulate research in the area. In
particular, this survey has four major contributions:
We propose new taxonomies to categorize existing
network representation learning techniques according to the underlying learning mechanisms, the network information intended to preserve, as well as the
algorithmic designs and methodologies. As a result,
this survey provides new angles to better understand
the existing work.
We provide a detailed and thorough study of the
state-of-the-art network representation learning algorithms. Compared to the existing graph embedding
surveys, we not only review a more comprehensive set of research work on network representation
learning, but also provide multifaceted algorithmic
perspectives to understand the advantages and disadvantages of different algorithms.
We summarize evaluation protocols used for validating network representation learning techniques,
including published benchmark datasets, evaluation
methods, and open source algorithms. We also perform empirical studies to compare the performance
of representative algorithms, along with a detailed
analysis of computational complexity.
To foster future research, we suggest six promising
future research directions for network representation
learning, and summarize the limitations of current
research work and propose new research ideas for
each direction.
Related Surveys and Differences
A few graph embedding and representation learning related
surveys exist in the recent literature. The ﬁrst is , which
reviews a few representative methods for network representation learning and visits some key concepts around the
idea of representation learning and its connections to other
related ﬁeld such as dimensionality reduction, deep learning, and network science. categorizes representative
network embedding algorithms from a methodology perspective. reviews a few representation learning methods
for embedding individual vertices as well as subgraphs, especially those inspired by deep learning, within an encoderdecoder framework. Yet, the majority of embedding algorithms reviewed by these surveys primarily preserve network structure. Recently, , extend to cover work
leveraging other side information, such as vertex attributes
and/or vertex labels, to harness representation learning.
In summary, existing surveys have the following limitations. First, they typically focus on one single taxonomy
to categorize the existing work. None of them provides a
multifaceted view to analyze the state-of-the-art network
representation learning techniques and to compare their
advantages and disadvantages. Second, existing surveys do
not have in-depth analysis of algorithm complexity and optimization methods, or they do not provide empirical results
to compare the performance of different algorithms. Third,
there is a lack of summary on available resources, such as
publicly available datasets and open source algorithms, to
facilitate future research. In this work, we provide the most
comprehensive survey to bridge the gap. We believe that
this survey will beneﬁt both researchers and practitioners
to gain a deep understanding of different approaches, and
provide rich resources to foster future research in the ﬁeld.
Organization of the Survey
The rest of this survey is organized as follows. In Section 2,
we provide preliminaries and deﬁnitions required to understand the problem and the models discussed next. Section 3
proposes new taxonomies to categorize the existing network
representation learning techniques. Section 4 and Section 5
review representative algorithms in two categories, respectively. A list of successful applications of network representation learning are discussed in Section 6. In Section 7,
we summarize the evaluation protocols used to validate
network representation learning, along with a comparison
of algorithm performance and complexity. We discuss potential research directions in Section 8, and conclude the
survey in Section 9.
NOTATIONS AND DEFINITIONS
In this section, as preliminaries, we ﬁrst deﬁne important
terminologies that are used to discuss the models next, followed by a formal deﬁnition of the network representation
learning problem. For ease of presentation, we ﬁrst deﬁne a
list of common notations that will be used throughout the
survey, as shown in Table 1.
Deﬁnition 1 (Information Network). An information network is deﬁned as G = (V, E, X, Y ), where V denotes
a set of vertices, and |V | denotes the number of vertices
in network G. E ⊆(V × V ) denotes a set of edges connecting the vertices. X ∈R|V |×m is the vertex attribute
matrix, where m is the number of attributes, and the
element Xij is the value of the i-th vertex on the j-th
attribute. Y ∈R|V |×|Y| is the vertex label matrix with
A summary of common notations
The given information network
Set of vertices in the given information network
Set of edges in the given information network
Number of vertices
Number of edges
Number of vertex attributes
Dimension of learned vertex representations
X ∈R|V |×m
The vertex attribute matrix
Set of vertex labels
Number of vertex labels
Y ∈R|V |×|Y|
The vertex label matrix
Y being a set of labels. If the i-th vertex has the k-th
label, the element Yik = 1; otherwise, Yik = −1. Due to
privacy concern or information access difﬁculty, vertex
attribute matrix X is often sparse and vertex label matrix
Y is usually unobserved or partially observed. For each
(vi, vj) ∈E, if information network G is undirected, we
have (vj, vi) ∈E; if G is directed, (vj, vi) unnecessarily
belongs to E.1 Each edge (vi, vj) ∈E is also associated
to a weight wij, which is equal to 1, if the information
network is binary (unweighted).
Intuitively, the generation of information networks is
not groundless, but guided or dominated by certain latent
mechanisms. Although the latent mechanisms are hardly
known, they can be reﬂected by some network properties that widely exist in information networks. Hence, the
common network properties are essential for the learning
of vertex representations that are informative to accurately
interpret information networks. Below, we introduce several
common network properties.
Deﬁnition 2 (First-order Proximity). The ﬁrst-order proximity is the local pairwise proximity between two connected vertices . For each vertex pair (vi, vj), if
(vi, vj) ∈E, the ﬁrst-order proximity between vi and
vj is wij; otherwise, the ﬁrst-order proximity between vi
and vj is 0. The ﬁrst-order proximity captures the direct
neighbor relationships between vertices.
Deﬁnition 3 (Second-order Proximity and High-order Proximity). The second-order proximity captures the 2-step
relations between each pair of vertices . For each
vertex pair (vi, vj), the second order proximity is determined by the number of common neighbors shared
by the two vertices, which can also be measured by the
2-step transition probability from vi to vj equivalently.
Compared with the second-order proximity, the highorder proximity captures more global structure,
which explores k-step (k ≥3) relations between each
pair of vertices. For each vertex pair (vi, vj), the higherorder proximity is measured by the k-step (k ≥3) transition probability from vertex vi to vertex vj, which can
also be reﬂected by the number of k-step (k ≥3) paths
from vi to vj. The second-order and high-order proximity capture the similarity between a pair of, indirectly
connected, vertices with similar structural contexts.
1Without any speciﬁc declaration, the networks discussed in this
survey are assumed to be undirected.
Fig. 1. An illustrative example of structural role proximity. Vertex 4 and
vertex 12 have similar structural roles, but are located far away from
each other.
Deﬁnition 4 (Structural Role Proximity). The structural role
proximity depicts similarity between vertices serving as
the similar roles in their neighborhood, such as edge of
a chain, center of a star, and a bridge between two communities. In communication and trafﬁc networks, vertices’ structural roles are important to characterize their
properties. Different from the ﬁrst-order, second-order
and high-order proximity, which capture the similarity
between vertices close to each other in the network, the
structural role proximity tries to discover the similarity
between distant vertices while sharing the equivalent
structural roles. As is shown in Fig. 1, vertex 4 and vertex
12 are located far away from each other, while they serve
as the same structural role, center of a star. Thus, they
have high structural role proximity.
(Intra-community
Proximity). The intracommunity proximity is the pairwise proximity between
vertices in a same community. Many networks have
community structure, where vertex-vertex connections
within the same community are dense, but connections
to vertices outside the community are sparse . As
cluster structure, a community preserves certain kinds
of common properties of vertices within it. For example,
in social networks, communities might represent social
groups by interest or background; in citation networks,
communities might represent related papers on a same
topic. The intra-community proximity captures such
cluster structure by preserving the common property
shared by vertices within a same community .
Vertex attribute: In addition to network structure, vertex
attributes can provide direct evidence to measure contentlevel similarity between vertices. As shown in , , ,
vertex attributes and network structure can help each other
ﬁlter out noisy information and compensate each other to
jointly learn informative vertex representations.
Vertex label: Vertex labels provide direct information
about the semantic categorization of each network vertex
to certain classes or groups. Vertex labels are strongly
inﬂuenced by and inherently correlated to both network
structure and vertex attributes . Though vertex labels
are usually partially observed, when coupled with network
structure and vertex attributes, they encourage a network
structure and vertex attribute consistent labeling, and help
learn informative and discriminative vertex representations.
Deﬁnition 6 (Network Representation Learning (NRL)).
Given an information network G = (V, E, X, Y ), by
integrating network structure in E, vertex attributes in X
and vertex labels in Y (if available), the task of network
representation learning is to learn a mapping function
f : v 7−→rv ∈Rd, where rv is the learned vector
representation of vertex v, and d is the dimension of
the learned representation. The transformation f preserves the original network information, such that two
vertices similar in the original network should also be
represented similarly in the learned vector space.
The learned vertex representations should satisfy the
following conditions: (1) low-dimensional,i.e., d ≪|V |, in
other words, the dimension of learned vertex representations should be much smaller than the dimension of the
original adjacency matrix representation for memory efﬁciency and the scalability of subsequent network analytic
tasks; (2) informative, i.e., the learned vertex representations should preserve vertex proximity reﬂected by network
structure, vertex attributes and vertex labels (if available);
(3) continuous, i.e., the learned vertex representations should
have continuous real values to support subsequent network
analytic tasks, like vertex classiﬁcation, vertex clustering, or
anomaly detection, and have smooth decision boundaries to
ensure the robustness of these tasks.
(a) Input: Information Network
(b) Output: Vertex Representations
Fig. 2. A conceptual view of network representation learning. Vertices in
(a) are indexed using their ID and color coded based on their community
information. The network representation learning in (b) transforms all
vertices into a two-dimensional vector space, such that vertices with
structural proximity are close to each other in the new embedding space.
Fig. 2 demonstrates a conceptual view of network representation learning, using a toy network. In this case, only
network structure is considered to learn vertex representations. Given an information network shown in Fig. 2(a),
the objective of NRL is to embed all network vertices into
a low-dimensional space, as depicted in Fig. 2(b). In the
embedding space, vertices with structural proximity are
represented closely to each other. For example, as vertex 7
and vertex 8 are directly connected, the ﬁrst-order proximity
enforces them close to each other in the embedding space.
Though vertex 2 and vertex 5 are not directly connected,
they are also embedded closely to each other because they
have high second-order proximity, which is reﬂected by 4
common neighbors shared by these two vertices. Vertex 20
and vertex 25 are not directly connected, nor do they share
common direct neighbors. However, they are connected by
many k-step paths (k ≥3), which proves that they have
high-order proximity. Thus, vertex 20 and vertex 25 also
have close embeddings. Different from other vertices, vertex
10–16 clearly belong to the same community in the original
network. This intra-community proximity guarantees the
images of these vertices also exhibit a clear cluster structure
in the embedding space.
Network Representation Learning
Unsupervised NRL
(Section 4)
Semi-supervised NRL
(Section 5)
Unsupervised Structure
Preserving NRL
(Section 4.1)
Unsupervised Content
Augmented NRL
(Section 4.2)
Semi-supervised Structure
Preserving NRL
(Section 5.1)
Semi-supervised Content
Augmented NRL
(Section 5.2)
Structure Information
microscopic structure
macroscopic structure
Information Sources
network structure
vertex attributes
Information Sources
network structure
vertex labels
Information Sources
network structure
vertex attributes
vertex labels
mesoscopic structure
Fig. 3. The proposed taxonomy to summarize network representation learning techniques. We categorize network representation learning into two
groups, unsupervised network representation learning and semi-supervised network representation learning, depending on whether vertex labels
are available for learning. For each group, we further categorize methods into two subgroups, depending on whether the representation learning is
based on network topology structure only, or augmented with information from node content.
CATEGORIZATION
In this section, we propose a new taxonomy to categorize
existing network representation learning techniques in the
literature, as shown in Fig. 3. The ﬁrst layer of the taxonomy
is based on whether vertex labels are provided for learning.
According to this, we categorize network representation
learning into two groups: unsupervised network representation
learning and semi-supervised network representation learning.
Unsupervised network representation learning. In this
setting, there are no labeled vertices provided for learning
vertex representations. Network representation learning is
therefore considered as a generic task independent of subsequent learning, and vertex representations are learned in
an unsupervised manner.
Most of the existing NRL algorithms fall into this category. After vertex representations are learned in a new
embedding space, they are taken as features to any vectorbased algorithms for various learning tasks. Unsupervised
NRL algorithms can be further divided into two subgroups
based on the type of network information available for
learning: unsupervised structure preserving methods that
preserve only network structure, and unsupervised content
augmented methods that incorporate vertex attributes and
network structure to learn joint vertex embeddings.
Semi-supervised network representation learning. In this
case, there exist some labeled vertices for representation
learning. Because vertex labels play an essential role in determining the categorization of each vertex with strong correlations to network structure and vertex attributes, semisupervised network representation learning is proposed to
take advantage of vertex labels available in the network for
seeking more effective joint vector representations.
In this setting, network representation learning is coupled with supervised learning tasks such as vertex classiﬁcation. A uniﬁed objective function is often formulated to
simultaneously optimize the learning of vertex representations and the classiﬁcation of network vertices. Therefore,
the learned vertex representations can be both informative and discriminative with respect to different categories.
Semi-supervised NRL algorithms can also be categorized
into two subgroups, semi-supervised structure preserving
methods and semi-supervised content augmented methods.
Table 2 summarizes all NRL algorithms, according to
the information sources that they use for representation
learning. In general, there are three main types of information sources: network structure, vertex attributes, and
vertex labels. Most of the unsupervised NRL algorithms
focus on preserving network structure for learning vertex
representations, and only a few algorithms (e.g., TADW ,
HSCA ) attempt to leverage vertex attributes. By contrast,
under the semi-supervised learning setting, half of the algorithms intend to couple vertex attributes with network
structure and vertex labels to learn vertex representations.
On both settings, most of the algorithms focus on preserving
microscopic structure, while very few algorithms (e.g., M-
NMF , DP , HARP ) attempt to take advantage
of the mesoscopic and macroscopic structure.
Approaches to network representation learning in the
above two different settings can be summarized into ﬁve
categories from algorithmic perspectives.
Matrix factorization based methods. Matrix factorization based methods represent the connections
between network vertices in the form of a matrix
and use matrix factorization to obtain the embeddings. Different types of matrices are constructed
to preserve network structure, such as the k-step
transition probability matrix, the modularity matrix,
or the vertex-context matrix . By assuming that
such high-dimensional vertex representations are
only affected by a small quantity of latent factors,
matrix factorization is used to embed the highdimensional vertex representations into a latent,
low-dimensional structure preserving space.
Factorization strategies vary across different algo-
A summary of NRL algorithms according to the information sources they use for learning
Algorithms
Network Structure
Vertex Attributes
Vertex Labels
Microscopic
Mesoscopic
Macroscopic
Structural Role
Intra-community
Unsupervised
Social Dim. , , 
DeepWalk 
GraRep 
node2vec 
M-NMF 
GraphGAN 
struct2vec 
GraphWave 
UPP-SNE 
Semi-supervised
TLINE 
SemiNE 
TriDNR 
Planetoid 
rithms according to their objectives. For example,
in the Modularity Maximization method , eigen
decomposition is performed on the modularity matrix to learn community indicative vertex representations ; in the TADW algorithm , inductive
matrix factorization is carried out on the vertexcontext matrix to simultaneously preserve vertex
textual features and network structure in the learning of vertex representations. Although matrix factorization based methods have been proved effective in learning informative vertex representations,
the scalability is a major bottleneck because carrying
out factorization on a matrix with millions of rows
and columns is memory intensive and computationally expensive or, sometime, even infeasible.
Random walk based methods. For scalable vertex
representation learning, random walk is exploited to
capture structural relationships between vertices. By
performing truncated random walks, an information network is transformed into a collection of vertex sequences, in which, the occurrence frequency
of a vertex-context pair measures the structural distance between them. Borrowing the idea of word
representation learning , , vertex representations are then learned by using each vertex to predict its contexts. DeepWalk is the pioneer work in
using random walks to learn vertex representations.
node2vec further exploits a biased random walk
strategy to capture more ﬂexible contextual structure.
As the extensions of the structure only preserving
version, algorithms like DDRW , GENE 
and SemiNE incorporate vertex labels with network structure to harness representation learning,
PPNE imports vertex attributes, and Tri-DNR
 enforces the model with both vertex labels and
attributes. As these models can be trained in an online manner, they have great potential to scale up.
Edge modeling based methods. Different from approaches that use matrix or random walk to capture network structure, the edge modeling based
methods directly learn vertex representations from
vertex-vertex connections. For capturing the ﬁrstorder and second-order proximity, LINE models
A categorization of NRL algorithms from methodology perspectives
Methodology
Algorithms
Disadvantage
Matrix Factorization
Social Dim. , , GraRep , HOPE ,
GraphWave , M-NMF , TADW , HSCA ,
MMDW , DMF , LANE 
capture global structure
high time and memory cost
Random Walk
DeepWalk , node2vec , APP , DDRW ,
GENE , TriDNR , UPP-SNE , struct2vec ,
SNS , PPNE , SemiNE 
relatively efﬁcient
only capture local structure
Edge Modeling
LINE , TLINE , LDE , pRBM ,
GraphGAN 
only capture local structure
Deep Learning
DNGR , SDNE 
capture non-linearity
high time cost
DP , HARP , Planetoid 
capture global structure
a joint probability distribution and a conditional
probability distribution, respectively, on connected
vertices. To learn the representations of linked documents, LDE models the document-document
relationships by maximizing the conditional probability between connected documents. pRBM 
adapts the RBM model to linked data by making the hidden RBM representations of connected
vertices similar to each other. GraphGAN 
adopts Generative Adversarial Nets (GAN) to
accurately model the vertex connectivity probability. Edge modeling based methods are more efﬁcient
compared to matrix factorization and random walk
based methods. However, these methods cannot
capture global network structure as they only consider observable vertex connectivity information.
Deep learning based methods. To extract complex structure features and learn deep, highly nonlinear vertex representations, deep learning techniques , are also applied to network representation learning. For example, DNGR applies the stacked denoising autoencoders (SDAE) 
on the high-dimensional matrix representations to
learn deep low-dimensional vertex representations.
SDNE uses a semi-supervised deep autoencoder model to model non-linearity in network
structure. Deep learning based methods have the
ability to capture non-linearity in networks, but
their computational time cost is usually high. Traditional deep learning architectures are designed
for 1D, 2D, or 3D Euclidean structured data, but
efﬁcient solutions need to be developed on non-
Euclidean structured data like graphs.
Hybrid methods. Some other methods make use of
a mixture of above methods to learn vertex representations. For example, DP enhances spectral embedding and DeepWalk with the degree penalty principle to preserve the macroscopic
scale-free property. HARP takes advantage of
random walk based methods (DeepWalk and
node2vec ) and edge modeling based method
(LINE ) to learn vertex representations from small
sampled networks to the original network.
We summarize all ﬁve categories of network representation learning techniques and compare their advantages and
disadvantages in Table 3.
UNSUPERVISED
REPRESENTATION
In this section, we review unsupervised network representation learning methods by separating them into two subsections, as outlined in Fig. 3. After that, we summarize key
characteristics of the methods and compare their differences
across the two categories.
Unsupervised Structure Preserving Network Representation Learning
Structure preserving network representation learning refers
to methods that intend to preserve network structure, in
the sense that vertices close to each other in the original
network space should be represented similarly in the new
embedding space. In this category, research efforts have
been focused on designing various models to capture structure information conveyed by the original network as much
as possible.
Network Structure
Microscopic Structure
Macroscopic Structure
Mesoscopic Structure
Structural Role Proximity
Intra-community Proximity
(Sec. 4.1.1)
(Sec. 4.1.2)
(Sec. 4.1.3)
(Sec. 4.1.4)
Fig. 4. Categorization of network structure.
We summarize network structure considered for learning vertex representations into three types: (i) microscopic
structure, which includes local closeness proximity, i.e.,
the ﬁrst-order, second-order, and high-order proximity, (ii)
mesoscopic structure, which captures structural role proximity and the intra-community proximity, and (iii) macroscopic structure, which captures global network properties,
such as the scale-free property or small world property.
The following subsections are organized according to our
categorization of network structure, as depicted in Fig. 4.
Microscopic Structure Preserving NRL
This category of NRL algorithms aim to preserve local structure information among directly or indirectly connected
Random Walks
Representation Learning
Fig. 5. The workﬂow of DeepWalk. It ﬁrst generates random walk sequences from a given network, and then applies the Skip-Gram model
to learn vertex representations.
vertices in their neighborhood, including ﬁrst-order, secondorder, and high-order proximity. The ﬁrst-order proximity
captures the homophily, i.e., directly connected vertices tend
to be similar to each other, while the second-order and highorder proximity captures the similarity between vertices
sharing common neighbors. Most of structure preserving
NRL algorithms fall into this category.
DeepWalk. DeepWalk generalizes the idea of the
Skip-Gram model , that utilizes word context in
sentences to learn latent representations of words, to the
learning of latent vertex representations in networks, by
making an analogy between natural language sentence and
short random walk sequence. The workﬂow of DeepWalk
is given in Fig. 5. Given a random walk sequence with
length L, {v1, v2, · · · , vL}, following Skip-Gram, DeepWalk
learns the representation of vertex vi by using it to predict
its context vertices, which is achieved by the optimization
−log Pr({vi−t, · · · , vi+t} \ vi|f(vi)),
where {vi−t, · · · , vi+t} \ vi are the context vertices of vertex
vi within t window size. Making conditional independence
assumption, the probability Pr({vi−t, · · · , vi+t} \ vi|f(vi))
is approximated as
Pr ({vi−t, · · · , vi+t} \ vi|f(vi)) =
j=i−t,j̸=i
Pr(vj|f(vi)).
Following the DeepWalk’s learning architecture, vertices
that share similar context vertices in random walk sequences
should be represented closely in the new embedding space.
Considering the fact that context vertices in random walk
sequences describe neighborhood structure, DeepWalk actually represents vertices sharing similar neighbors (direct
or indirect) closely in the embedding space, so the secondorder and high-order proximity is preserved.
Large-scale Information Network Embedding (LINE).
Instead of exploiting random walks to capture network
structure, LINE learns vertex representations by explicitly modeling the ﬁrst-order and second-order proximity.
To preserve the ﬁrst-order proximity, LINE minimizes the
following objective:
O1 = d(ˆp1(·, ·), p1(·, ·)).
For each vertex pair vi and vj with (vi, vj) ∈E, p1(·, ·) is
the joint distribution modeled by their latent embeddings
rvi and rvj. ˆp1(vi, vj) is the empirical distribution between
them. d(·, ·) is the distance between two distributions.
To preserve the second-order proximity, LINE minimizes
the following objective:
λid(ˆp2(·|vi), p2(·|vi)),
where p2(·|vi) is the context conditional distribution for
each vi ∈V modeled by vertex embeddings, ˆp2(·|vi) is
the empirical conditional distribution and λi is the prestige
of vertex vi. Here, vertex context is determined by its
neighbors, i.e., for each vj, vj is vi’s context, if and only
if (vi, vj) ∈E.
By minimizing these two objectives, LINE learns two
kinds of vertex representations that preserve the ﬁrst-order
and second-order proximity, and takes their concatenation
as the ﬁnal vertex representation.
GraRep extends the skip-gram model to capture the
high-order proximity, i.e., vertices sharing common k-step
neighbors (k ≥1) should have similar latent representations. Speciﬁcally, for each vertex, GraRep deﬁnes its kstep neighbors (k ≥1) as context vertices, and for each
1 ≤k ≤K, to learn k-step vertex representations, GraRep
employs the matrix factorization version of skip-gram:
U k, Σk, V ki
= SV D(Xk).
where Xk is the log k-step transition probability matrix. The
k-step representation for vertex vi is constructed as the ith
row of matrix U k
2 , where U k
d is the ﬁrst-d columns
of U k and Σk
d is the diagonal matrix composed of the top
d singular values. After k-step vertex representations are
learned, GraRep concatenates them together as the ﬁnal
vertex representations.
Deep Neural Networks for Graph Representations
(DNGR). To overcome the weakness of truncated random
walks in exploiting vertex contextual information, i.e., the
difﬁculty in capturing correct contextual information for
vertices at the boundary of sequences and the difﬁculty
in determining the walk length and the number of walks,
DNGR utilizes the random surﬁng model to capture
contextual relatedness between each pair of vertices and
preserves them into |V |-dimensional vertex representations
X. To extract complex features and model non-linearities,
DNGR applies the stacked denoising autoencoders (SDAE) 
to the high-dimensional vertex representations X to learn
deep low-dimensional vertex representations.
Structural
SDNE is a deep learning based approach that uses
a semi-supervised deep autoencoder model to capture
non-linearity in network structure. In the unsupervised
component, SDNE learns the second-order proximity preserving vertex representations via reconstructing the |V |dimensional vertex adjacent matrix representations, which
tries to minimize
vi ) ⊙bi∥2
where r(0)
vi = Si: is the input representation and ˆr(0)
reconstructed representation. bi is a weight vector used to
penalize construction error more on non-zero elements of S.
Fig. 6. Two different neighborhood sampling strategies considered by
node2vec: BFS and DFS.
In the supervised component, SDNE imports the ﬁrstorder proximity by penalizing the distance between connected vertices in the embedding space. The loss function
for this objective is deﬁned as:
where r(K)
is the K-th layer representation of vertex vi, with
K being the number of hidden layers.
In all, SDNE minimizes the joint objective function:
L = L2nd + αL1st + νLreg,
where Lreg is a regularization term to prevent overﬁtting.
After solving the minimization of (8), for vertex vi, the K-th
layer representation r(K)
is taken as its representation rvi.
node2vec. In contrast to the rigid strategy of deﬁning
neighborhood (context) for each vertex, node2vec designs a ﬂexible neighborhood sampling strategy, i.e., biased
random walk, which smoothly interpolates between two extreme sampling strategies, i.e., Breadth-ﬁrst Sampling (BFS)
and Depth-ﬁrst Sampling (DFS), as illustrated in Fig. 6.
The biased random walk exploited in node2vec can better
preserve both the second-order and high-order proximity.
Following the skip-gram architecture, given the set of
neighbor vertices N(vi) generated by biased random walk,
node2vec learns the vertex representation f(vi) by optimizing the occurrence probability of neighbor vertices N(vi)
conditioned on the representation of vertex vi, f(vi):
log Pr(N(vi)|f(vi)).
High-order Proximity Preserved Embedding (HOPE).
HOPE learns vertex representations that capture the
asymmetric high-order proximity in directed networks. In
undirected networks, the transitivity is symmetric, but it
is asymmetric in directed networks. For example, in an
directed network, if there is a directed link from vertex vi
to vertex vj and from vertex vj to vertex vk, it is more likely
to have a directed link from vi to vk, but not from vk to vi.
To preserve the asymmetric transitivity, HOPE learns
two vertex embedding vectors U s, U t ∈R|V |×d, which
is called source and target embedding vectors, respectively. After constructing the high-order proximity matrix
S from four proximity measures, i.e., Katz Index ,
Rooted PageRank , Common Neighbors and Adamic-
Adar. HOPE learns vertex embeddings by solving the following matrix factorization problem:
Us,Ut ∥S −U s · U tT∥2
A summary of microscopic structure preserving NRL algorithms
Algorithms
First-order
Second-order
High-order
DeepWalk 
GraRep 
node2vec 
GraphGAN 
Asymmetric Proximity Preserving graph embedding
(APP). APP is another NRL algorithm designed to
capture asymmetric proximity, by using a Monte Carlo
approach to approximate the asymmetric Rooted PageRank
proximity . Similar to HOPE, APP has two representations for each vertex vi, the one as a source role rs
vi and the
other as a target role rt
vi. For each sampled path starting
from vi and ending with vj, the representations are learned
by maximizing the target vertex vj’s occurrence probability
conditioned on the source vertex vi:
Pr(vj|vi) =
v∈V exp(rsvi · rtv).
GraphGAN. GraphGAN learns vertex representations by modeling the connectivity behavior through an adversarial learning framework. Inspired by GAN (Generative
Adversarial Nets) , GraphGAN works through two components: (i) Generator G(v|vc), which ﬁts the distribution of
the vertices connected to vc across V and generates the likely
connected vertices, and (ii) Discriminator D(v, vc), which
outputs a connecting probability for the vertex pair (v, vc),
to differentiate the vertex pairs generated by G(v|vc) from
the ground truth. G(v|vc) and D(v, vc) compete in a way
that G(v|vc) tries to ﬁt the true connecting distribution as
much as possible and generates fake connected vertex pairs
to fool D(v, vc), while D(v, vc) tries to increase its discriminative power to distinguish the vertex pairs generated by
G(v|vc) from the ground truth. The competition is achieved
by the following minimax game:
 Ev∼Prtrue(·|vc) [log D(v, vc; θD)]
+ Ev∼G(·|vc;θG) [log(1 −D(v, vc; θD))]
Here, G(v|vc; θG) and D(v, vc; θD) are deﬁned as following:
G(v|vc; θG) =
exp(gv · gvc)
v̸=vc exp(gv · gvc),
D(v, vc; θD) =
1 + exp(dv · dvc),
where gv ∈Rk and dv ∈Rk is the representation vector for
generator and discriminator, respectively, and θD = {dv},
θG = {gv}. After the minimax game in Eq. (12) is solved, gv
serves as the ﬁnal vertex representations.
Summary: The proximity preserved by microscopic
structure preserving NRL algorithms is summarized in Table 4. Most algorithms in this category preserve the secondorder and high-order proximity, whereas only LINE ,
SDNE and GraphGAN consider the ﬁrst-order
proximity. From the methodology perspective, DeepWalk
 , node2vec and APP employ random walks to
capture vertex neighborhood structure. GraRep and
HOPE are realized by performing factorization on a
|V | × |V | scale matrix, making them hard to scale up.
LINE and GraphGAN directly model the connectivity behavior, while deep learning based methods (DNGR 
and SDNE ) learn non-linear vertex representations.
Structural Role Proximity Preserving NRL
Besides local connectivity patterns, vertices often share similar structural roles at a mesoscopic level, such as centers
of stars or members of cliques. Structural role proximity
preserving NRL aims to embed vertices that are far away
from each other but share similar structural roles close to
each other. This not only facilitates the downstream structural role dependent tasks but also enhances microscopic
structure preserving NRL.
struct2vec. struct2vec ﬁrst encodes the vertex structural role similarity into a multilayer graph, where the
weights of edges at each layer are determined by the
structural role difference at the corresponding scale. Deep-
Walk is then performed on the multilayer graph to learn
vertex representations, such that vertices close to each other
in the multilayer graph (with high structural role similarity)
are embedded closely in the new representation space.
For each vertex pair (vi, vj), considering their k-hop
neighborhood formed by their neighbors within k steps,
their structural distance at scale k, Dk(vi, vj), is deﬁned as
Dk(vi, vj) = Dk−1(vi, vj) + g(s(Rk(vi)), s(Rk(vj))),
where Rk(vi) is the set of vertices in vi’s k-hop neighborhood, s(Rk(vi)) is the ordered degree sequence of the
vertices in Rk(vi), and g(s(Rk(vi)), s(Rk(vj))) is the distance between the ordered degree sequences s(Rk(vi)) and
s(Rk(vj)). When k = 0, D0(vi, vj) is the degree difference
between vertex vi and vj.
GraphWave. By making use of the spectral graph
wavelet diffusion patterns, GraphWave embeds vertex
neighborhood structure into a low-dimensional space and
preserves the structural role proximity. The assumption is
that, if two vertices residing distantly in the network share
similar structural roles, the graph wavelets starting at them
will diffuse similarly across their neighbors.
For vertex vk, its spectral graph wavelet coefﬁcients Ψk
is deﬁned as
Ψk = UDiag(gs(λ1), · · · , gs(λ|V |))U Tδk,
where U is the eigenvector matrix of the graph Laplacian
L and λ1, · · · , λ|V | are the eigenvalues, gs(λ) = exp(−λs)
is the heat kernel, and δk is the one-hot vector for k. By
taking Ψk as a probability distribution, the spectral wavelet
distribution pattern in Ψk is then encoded into its empirical
characteristic function:
Then vk’s low-dimensional representation is then obtained
by sampling the 2-dimensional parametric function of φk(t)
at d evenly separated points t1, t2, · · · , td as:
f(vk) = [Re(φk(t1)), · · · , Re(φk(td)),
Im(φk(t1)), · · · , Im(φk(td))].
Structural and Neighborhood Similarity preserving
network embedding (SNS). SNS enhances a random
walk based method with structural role proximity. To preserve vertex structural roles, SNS represents each vertex as a
Graphlet Degree Vector with each element being the number
of times the given vertex is touched by the corresponding
orbit of graphlets. The Graphlet Degree Vector is used to
measure the vertex structural role similarity.
Given a vertex vi, SNS uses its context vertices C(vi) and
structurally similar vertices S(vi) to predict its existence,
which is achieved by maximizing the following probability:
Pr(vi|C(vi), S(vi)) =
u∈V exp(r′u · hvi),
vi is the output representation of vi and hvi is
the hidden layer representation for predicting vi, which is
aggregated from the input representations ru, for each u in
C(vi) and S(vi).
Summary: struct2vec and GraphWave take advantage of structural role proximity to learn vertex representations that facilitate speciﬁc structural role dependent tasks,
e.g., vertex classiﬁcation in trafﬁc networks, while SNS 
enhances a random walk based microscopic structure preserving NRL algorithm with structural role proximity. Technically, random walk is employed by struct2vec and SNS,
while matrix factorization is adopted by GraphWave.
Intra-community Proximity Preserving NRL
Another interesting feature that real-world networks exhibit
is the community structure, where vertices are densely
connected to each other within the same community, but
sparsely connected to vertices from other communities. For
example, in social networks, people from the same interest
group or afﬁliation often form a community. In citation networks, papers on similar research topics tend to frequently
cite each other. Intra-community preserving NRL aims to
leverage the community structure that characterizes key vertex properties to learn informative vertex representations.
Learning Latent Social Dimensions. The social dimension based NRL algorithms try to construct social actors’
embeddings through their membership or afﬁliation to a
number of social dimensions. To infer these latent social
dimensions, the phenomenon of “community” in social networks is considered, stating that social actors sharing similar
properties often form groups with denser within-group
connections. Thus, the problem boils down to one classical
network analytic task—community detection—that aims to
discover a set of communities with denser within-group
connections than between-group connections. Three clustering techniques, including modularity maximization ,
spectral clustering and edge clustering are employed to discover latent social dimensions. Each social
dimension describes the likelihood of a vertex belonging to
a plausible afﬁliation. These methods preserve the global
community structure, but neglect local structure properties,
e.g., the ﬁrst-order and second-order proximity.
Modularized Nonnegative Matrix Factorization (M-
NMF). M-NMF augments the second-order and highorder proximity with broader community structure to learn
more informative vertex embeddings U ∈R|V |×d using the
following objective:
M,U,H,C ∥S −MU T∥2
F + α∥H −UCT∥2
F −βtr(HTBH)
s.t., M ≥0, U ≥0, H ≥0, C ≥0, tr(HTH) = |V |,
where vertex embedding U
is learned by minimizing
∥S −MU T∥2
F , with S ∈R|V |×|V | being the vertex pairwise
proximity matrix, which captures the second-order and the
high-order proximity when taken as representations. The
community indicative vertex embedding H is learned by
maximizing tr(HTBH), which is essentially the objective
of modularity maximization with B being the modularity
matrix. The minimization on ∥H −UCT∥2
F makes these
two embeddings consistent with each other by importing
a community representation matrix C.
Summary: The algorithms of learning latent social dimensions
 , , only consider the community
structure to learn vertex representation, while M-NMF 
integrates microscopic structure (the second-order and highorder proximity) with the intra-community proximity. These
methods primarily rely on matrix factorization to detect
community structure, which makes them hard to scale up.
Macroscopic Structure Preserving NRL
Macroscopic structure preserving methods aim to preserve
certain global network properties in a macroscopic view.
Only very few recent studies are developed for this purpose.
Degree penalty principle (DP). Many real-world networks present the macroscopic scale-free property, which
depicts the phenomenon that vertex degree follows a longtailed distribution, i.e., most vertices are sparsely connected
and only few vertices have dense edges. To capture the
scale-free property, proposes the degree penalty principle (DP): penalizing the proximity between high-degree
vertices. This principle is then coupled with two NRL algorithms (i.e., spectral embedding and DeepWalk ) to
learn scale-free property preserving vertex representations.
Hierarchical Representation Learning for Networks
(HARP). To capture the global patterns in networks,
HARP samples small networks to approximate the
global structure. The vertex representations learned from
sampled networks are taken as the initialization for inferring
the vertex representations of the original network. In this
way, global structure is preserved in the ﬁnal representations. To obtain smooth solutions, a series of smaller networks are successively sampled from the original network
by coalescing edges and vertices, and the vertex representations are hierarchically inferred back from the smallest
network to the original network. In HARP, DeepWalk 
and LINE are used to learn vertex representations.
Summary: DP and HARP are both realized
by adapting the existing NRL algorithms to capture the
macroscopic structure. The former tries to preserve the
scale-free property, while the latter makes the learned vertex
representations respect the global network structure.
Unsupervised Content Augmented Network Representation Learning
Besides network structure, real-world networks are often
attached with rich content as vertex attributes, such as webpages in webpage networks, papers in citation networks,
and user metadata in social networks. Vertex attributes
provide direct evidence to measure content-level similarity
between vertices. Therefore, network representation learning can be signiﬁcantly improved if vertex attribute information is properly incorporated into the learning process.
Recently, several content augmented NRL algorithms have
been proposed to incorporate network structure and vertex
attributes to reinforce the network representation learning.
Text-Associated DeepWalk (TADW)
TADW ﬁrstly proves the equivalence between Deep-
Walk and the following matrix factorization:
W,H ∥M −W TH∥2
where W and H are learned latent embeddings and M
is the vertex-context matrix carrying transition probability
between each vertex pair within k steps. Then, textual
features are imported through inductive matrix factorization
W,H ∥M −W THT∥2
where T is vertex textual feature matrix. After (21) is solved,
the ﬁnal vertex representations are formed by taking the
concatenation of W and HT.
Homophily, Structure, and Content Augmented Network Representation Learning (HSCA)
Despite its ability to incorporate textural features, TADW 
only considers structural context of network vertices, i.e.,
the second-order and high-order proximity, but ignores the
important homophily property (the ﬁrst-order proximity) in
its learning framework. HSCA is proposed to simultaneously integrates homophily, structural context, and vertex
content to learn effective network representations.
For TADW, the learned representation for the i-th vertex vi is
:i , (HT:i)TT, where W:i and T:i is the i-th
column of W and T, respectively. To enforce the ﬁrstorder proximity, HSCA introduces a regularization term to
enforce homophily between directly connected nodes in the
embedding space, which is formulated as
R(W, H) = 1
where S is the adjacent matrix. The objective of HSCA is
W,H ∥M −W THT∥2
F ) + µR(W, H), (23)
where λ and µ are the trade-off parameters. After solving
the above optimization problem, the concatenation of W
and HT is taken as the ﬁnal vertex representations.
Paired Restricted Boltzmann Machine (pRBM)
By leveraging the strength of Restricted Boltzmann Machine
(RBM) , designs a novel model called Paired RBM
(pRBM) to learn vertex representations by combining vertex
attributes and link information. The pRBM considers the
networks with vertices associated with binary attributes.
For each edge (vi, vj) ∈E, the attributes for vi and vj are
v(i) and v(j) ∈{0, 1}m, and their hidden representations
are h(i) and h(j) ∈{0, 1}d. Vertex hidden representations
are learned by maximizing the joint probability of pRBM
deﬁned over v(i), v(j), h(i) and h(j):
Pr(v(i), v(j), h(i), h(j), wij; θ)
= exp(−E(v(i), v(j), h(i), h(j), wij))/Z,
where θ = {W ∈Rd×m, b ∈Rd×1, c ∈Rm×1, M ∈Rd×d}
is the parameter set and Z is the normalization term. To
model the joint probability, the energy function is deﬁned
E(v(i), v(j), h(i), h(j), wij) =
−wij(h(i))TMh(j) −(h(i))TWv(i) −cTv(i) −bTh(i)
−(h(j))TWv(j) −cTv(i) −bTh(j),
where wij(h(i))TMh(j) forces the latent representations of
vi and vj to be close and wij is the weight of edge (vi, vj).
User Proﬁle Preserving Social Network Embedding
UPP-SNE leverages user proﬁle features to enhance the
embedding learning of users in social networks. Compared
with textural content features, user proﬁles have two unique
properties: (1) user proﬁles are noisy, sparse and incomplete
and (2) different dimensions of user proﬁle features are
topic-inconsistent. To ﬁlter out noise and extract useful
information from user proﬁles, UPP-SNE constructs user
representations by performing a non-linear mapping on
user proﬁle features, which is guided by network structure.
The approximated kernel mapping is used in UPP-
SNE to construct user embedding from user proﬁle features:
f(vi) = ϕ(xi) =
1 xi), · · · , cos(µT
1 xi), · · · , sin(µT
where xi is the user proﬁle feature vector of vertex vi and
µi is the corresponding coefﬁcient vector.
To supervise the learning of the non-linear mapping and
make user proﬁles and network structure complement each
other, the objective of DeepWalk is used:
−log Pr({vi−t, · · · , vi+t} \ vi|f(vi)),
where {vi−t, · · · , vi+t}\vi is the context vertices of vertex vi
within t window size in the given random walk sequence.
Property Preserving Network Embedding (PPNE)
representations,
PPNE jointly optimizes two objectives: (i) the structuredriven objective and (ii) the attribute-driven objective.
Following DeepWalk, the structure-driven objective aims
to make vertices sharing similar context vertices represented
closely. For a given random walk sequence S, the structuredriven objective is formulated as
u∈context(v)
The attribute-driven objective aims to make the vertex representations learned by Eq. (28) respect the vertex attribute
similarity. A realization of the attribute-driven objective is
u∈pos(v)∪neg(v)
P(v, u)d(v, u),
where P(u, v) is the attribute similarity between u and v,
d(u, v) is the distance between u and v in the embedding
space, and pos(v) and neg(v) is the set of top-k similar and
dissimilar vertices according to P(u, v), respectively.
Summary: The above unsupervised content augmented
NRL algorithms incorporate vertex content features in three
ways. The ﬁrst, used by TADW and HSCA , is to
couple the network structure with vertex content features
via inductive matrix factorization . This process can be
considered as a linear transformation on vertex attributes
constrained by network structure. The second is to perform
a non-linear mapping to construct new vertex embeddings
that respect network structure. For example, RBM and
the approximated kernel mapping is used by pRBM 
and UPP-SNE , respectively, to achieve this goal. The
third used by PPNE is to add an attribute preserving
constraint to the structure preserving optimization objective.
SEMI-SUPERVISED
REPRESENTA-
TION LEARNING
Label information attached with vertices directly indicates
vertices’ group or class afﬁliation. Such labels have strong
correlations, although not always consistent, to network
structure and vertex attributes, and are always helpful in
learning informative and discriminative network representations. Semi-supervised NRL algorithms are developed
along this line to make use of vertex labels available in the
network for seeking more effective vertex representations.
Semi-supervised Structure Preserving NRL
The ﬁrst group of semi-supervised NRL algorithms aim
to simultaneously optimize the representation learning that
preserves network structure and discriminative learning. As
a result, the information derived from vertex labels can help
improve the representative and discriminative power of the
learned vertex representations.
Discriminative Deep Random Walk (DDRW)
Inspired by the discriminative representation learning ,
 , DDRW proposes to learn discriminative network
representations through jointly optimizing the objective of
DeepWalk together with the following L2-loss Support
Vector Classiﬁcation objective:
(σ(1 −YikβTrvi))2 + 1
where σ(x) = x, if x > 0 and otherwise σ(x) = 0.
The joint objective of DDRW is thus deﬁned as
L = ηLDW + Lc.
where LDW is the objective function of Deekwalk. The
objective (31) aims to learn discriminative vertex representations for binary classiﬁcation for the k-th class. DDRW is
generalized to handle multi-class classiﬁcation by using the
one-against-rest strategy .
Max-Margin DeepWalk (MMDW)
Similarly, MMDW couples the objective of the matrix factorization version DeepWalk with the following multi-class Support Vector Machine objective with
{(rv1, Y1:), · · · , (rvT , YT :)} training set:
W,ξ LSV M = min
i −ξi, ∀i, j,
where li = k with Yik = 1, ej
i = 1 for Yij = −1, and ej
for Yij = 1.
The joint objective of MMDW is
U,H,W,ξ L =
U,H,W,ξ LDW + 1
i −ξi, ∀i, j.
where LDW is the objective of the matrix factorization
version of DeepWalk.
Transductive LINE (TLINE)
Along similar lines, TLINE is proposed as a semisupervised extension of LINE that simultaneously learns
LINE’s vertex representations and an SVM classiﬁer. Given
a set of labeled and unlabeled vertices {v1, v2, · · · , vL} and
{vL+1, · · · , v|V |}, TLINE trains a multi-class SVM classiﬁer
on {v1, v2, · · · , vL} by optimizing the objective:
max(0, 1 −Yikwk
Trvi) + λ∥wk∥2
Based on LINE’s formulations that preserve the ﬁrstorder and second-order proximity, TLINE optimizes two
objective functions:
OT LINE(1st) = Oline1 + βOsvm,
OT LINE(2nd) = Oline2 + βOsvm.
Inheriting LINE’s ability to deal with large-scale networks,
TLINE is claimed to be able to learn discriminative vertex
representations for large-scale networks with low time and
memory cost.
Group Enhanced Network Embedding (GENE)
GENE integrates group (label) information with network
probabilistic
assumes that vertices should be embedded closely in lowdimensional space, if they share similar neighbors or join
similar groups. Inspired by DeepWalk and document
modeling , , the mechanism of GENE for learning
group label informed vertex representations is achieved by
maximizing the following log probability:
log Pr(vj|vj−t, · · · , vj+t, gi)+
log Pr(ˆvj|gi)
where Y is the set of different groups, Wgi is the set of
random walk sequences labeled with gi, ˆWgi is the set of
vertices randomly sampled from group gi.
Semi-supervised Network Embedding (SemiNE)
SemiNE learns semi-supervised vertex representations
in two stages. In the ﬁrst stage, SemiNE exploits the Deep-
Walk framework to learn vertex representations in an
unsupervised manner. It points out that DeepWalk does not
consider the order information of context vertex, i.e., the
distance between the context vertex and the central vertex,
when using the context vertex vi+j to predict the central
vertex vi. Thus, SemiNE encodes the order information into
DeepWalk by modeling the probability Pr(vi+j|vi) with jdependent parameters:
Pr(vi+j|vi) =
exp(Φ(vi) · Ψj(vi+j))
u∈V exp(Φ(vi) · Ψj(u)),
where Φ(·) is the vertex representation and Ψj(·) is the
parameter for calculating Pr(vi+j|vi).
In the second stage, SemiNE learns a neural network that
tunes the learned unsupervised vertex representations to ﬁt
vertex labels.
Semi-supervised Content Augmented NRL
Recently, more research efforts have shifted to the development of label and content augmented NRL algorithms
that investigate the use of vertex content and labels to assist
with network representation learning. With content information incorporated, the learned vertex representations are
expected to be more informative, and with label information
considered, the learned vertex representations can be highly
customized for the underlying classiﬁcation task.
Tri-Party Deep Network Representation (TriDNR)
Using a coupled neural network framework, TriDNR 
representations
information
sources: network structure, vertex content and vertex labels.
To capture the vertex content and label information, TriDNR
adapts the Paragraph Vector model
 to describe the
vertex-word correlation and the label-word correspondence
by maximizing the following objective:
log Pr(w−b : wb|ci) +
log Pr(w−b : wb|vi), (39)
where {w−b : wb} is a sequence of words inside a contextual
window of length 2b, ci is the class label of vertex vi, and L
is the set of indices of labeled vertices.
TriDNR is then realized by coupling the Paragraph Vector objective with DeepWalk objective:
max (1 −α)LDW + αLP V ,
where LDW is the DeepWalk maximization objective function and α is the trade-off parameter.
Linked Document Embedding (LDE)
LDE is proposed to learn representations for linked
documents, which are actually the vertices of citation or
webpage networks. Similar to TriDNR , LDE learns
vertex representations by modeling three kinds of relations,
i.e., word-word-document relations, document-document
relations, and document-label relations. LDE is realized by
solving the following optimization problem:
(wi,wj,dk)∈P
log Pr(wj|wi, dk)
j:(vi,vj)∈E
log Pr(dj|di)
log Pr(yi|di)
Here, the probability Pr(wj|wi, dk) is used to model wordword-document relations, which means the probability that
in document dk, word wj is a neighboring word of wi. To
capture word-word-document relations, triplets (wi, wj, dk)
are extracted, with the word-neighbor pair (wi, wj) occurring in document dk. The set of triplets (wi, wj, dk) is denoted by P. The document-document relations are captured
by the conditional probability between linked document
pairs (di, dj), Pr(dj|di). The document-label relations are
also considered by modeling Pr(yi|di), the probability for
the occurrence of class label yi conditioned on document di.
In (41), W , D and Y is the embedding matrix for words,
documents and labels, respectively.
Discriminative Matrix Factorization (DMF)
To empower vertex representations with discriminative ability, DMF enforces the objective of TADW (21) with an
empirical loss minimization for a linear classiﬁer trained on
labeled vertices:
i Htj)2 + µ
(Yn1 −ηTxn)2
where wi is the i-th column of vertex representation matrix
W and tj is j-th column of vertex textual feature matrix T,
and L is the set of indices of labeled vertices. DMF considers
binary-class classiﬁcation, i.e. Y = {+1, −1}. Hence, Yn1 is
used to denote the class label of vertex vn.
DMF constructs vertex representations from W rather
that W and HT. This is based on empirical ﬁndings that W
contains sufﬁcient information for vertex representations. In
the objective of (42), xn is set to [wT
n , 1]T, which incorporates the intercept term b of the linear classiﬁer into η. The
optimization problem (42) is solved by optimizing W, H
and η alternately. Once the optimization problem is solved,
the discriminative and informative vertex representations
together with the linear classiﬁer are learned, and work
together to classify unlabeled vertices in networks.
Predictive Labels And Neighbors with Embeddings
Transductively Or Inductively from Data (Planetoid)
Planetoid leverages network embedding together with
vertex attributes to carry out semi-supervised learning.
Planetoid learns vertex embeddings by minimizing the loss
for predicting structural context, which is formulated as
Lu = −E(i,c,γ) log σ(γwT
where (i, c) is the index for vertex context pair (vi, vc), ei
is the embedding of vertex vi, wc is the parameter vector
for context vertex vc, and γ ∈{+1, −1} indicates whether
the sampled vertex context pair (i, c) is positive or negative.
The triple (i, c, γ) is sampled according to both the network
structure and vertex labels.
Planetoid then maps the learned vertex representations e
and vertex attributes x to hidden layer space via deep neural
network, and concatenates these two hidden layer representations together to predict vertex labels, by minimizing the
following classiﬁcation loss:
log p(yi|xi, ei),
To integrate network structure, vertex attributes and
vertex labels together, Planetoid jointly minimizes the two
objectives (43) and (44) to learn vertex embedding e with
deep neural networks.
Label informed Attribute Network Embedding (LANE)
LANE learns vertex representations by embedding the
network structure proximity, attribute afﬁnity, and label
proximity into a uniﬁed latent representation. The learned
representations are expected to capture both network structure and vertex attribute information, and label information
if provided. The embedding learning in LANE is carried
out in two stages. During the ﬁrst stage, vertex proximity
in network structure and attribute information are mapped
into latent representations U(G) and U(A), then U(A) is
incorporated into U(G) by maximizing their correlations.
In the second stage, LANE employs the joint proximity
(determined by U(G)) to smooth label information and
uniformly embeds them into another latent representation
U(Y ), and then embeds U(A), U(G) and U(Y ) into a uniﬁed
embedding representation H.
We now summarize and compare the discriminative learning strategies used by semi-supervised NRL algorithms in
Table 5 in terms of their advantages and disadvantages.
Three strategies are used to achieve discriminative learning. The ﬁrst strategy (i.e., DDRW , MMDW ,
TLINE , DMF , SemiNE ) is to enforce classi-
ﬁcation loss minimization on vertex representations, i.e.,
ﬁtting the vertex representations to a classiﬁer. This provides
a direct way to separate vertices of different categories
from each other in the new embedding space. The second
strategy (used by GENE , TriDNR , LDE and
Planetoid ) is achieved by modeling vertex label relation,
such that vertices with same labels have similar vector
A summary of semi-supervised NRL algorithms
Discriminative Learning Strategy
Loss function
Disadvantage
ﬁtting a classiﬁer
hinge loss
a) directly optimize classiﬁcation loss;
b) perform better in sparsely labeled scenarios
prone to overﬁtting
hinge loss
TLINE 
hinge loss
square loss
SemiNE 
logistic loss
modeling vertex label relation
likelihood loss
a) better capture intra-class proximity;
b) generalization to other tasks
require more labeled data
TriDNR 
likelihood loss
likelihood loss
Planetoid 
likelihood loss
joint vertex label embedding
correlation loss
representations. The third strategy used by LANE is
to jointly embed vertices and labels into a common space.
Fitting vertex representations to a classiﬁer can take
advantage of the discriminative power in vertex labels.
Algorithms using this strategy only require a small number
of labeled vertices (e.g., 10%) to achieve signiﬁcant performance gain over their unsupervised counterparts. They are
thus more effective for discriminative learning in sparsely
labeled scenarios. However, ﬁtting vertex representations
to a classiﬁer is more prone to overﬁtting. Regularization
and DropOut are often introduced to overcome this
problem. By contrast, modeling vertex label relation and
joint vertex embedding requires more vertex labels to make
vertex representations more discriminative, but they can
better capture intra-class proximity, i.e., vertices belonging
to the same class are kept closer to each other in the new
embedding space. This allows them to have generalized
beneﬁts on tasks like vertex clustering or visualization.
APPLICATIONS
Once new vertex representations are learned via network
representation learning techniques, traditional vector-based
algorithms can be used to solve important analytic tasks,
such as vertex classiﬁcation, link prediction, clustering,
visualization, and recommendation. The effectiveness of
the learned representations can also be validated through
assessing their performance on these tasks.
Vertex Classiﬁcation
Vertex classiﬁcation is one of the most important tasks
in network analytic research. Often in networks, vertices
are associated with semantic labels characterizing certain
aspects of entities, such as beliefs, interests, or afﬁliations. In
citation networks, a publication may be labeled with topics
or research areas, while the labels of entities in social network may indicate individuals’ interests or political beliefs.
Often, because network vertices are partially or sparsely
labeled due to high labeling costs, a large portion of vertices
in networks have unknown labels. The problem of vertex
classiﬁcation aims to predict the labels of unlabeled vertices
given a partially labeled network , . Since vertices
are not independent but connected to each other in the
form of a network via links, vertex classiﬁcation should
exploit these dependencies for jointly classifying the labels
of vertices. Among others, collective classiﬁcation proposes
to construct a new set of vertex features that summarize
label dependencies in the neighborhood, which has been
shown to be most effective in classifying many real-world
networks , .
Network representation learning follows the same principle that automatically learns vertex features based on network structure. Existing studies have evaluated the discriminative power of the learned vertex representations under
two settings: unsupervised settings (e.g., , , , ,
 ), where vertex representations are learned separately,
followed by applying discriminative classiﬁers like SVM
or logistic regression on the new embeddings, and semisupervised settings (e.g.,
 , , , , ), where
representation learning and discriminative learning are simultaneously tackled, so that discriminative power inferred
from labeled vertices can directly beneﬁt the learning of informative vertex representations. These studies have proved
that better vertex representations can contribute to high
classiﬁcation accuracy.
Link Prediction
Another important application of network representation
learning is link prediction , , which aims to infer
the existence of new relationships or emerging interactions
between pairs of entities based on the currently observed
links and their properties. The approaches developed to
solve this problem can enable the discovery of implicit or
missing interactions in the network, the identiﬁcation of
spurious links, as well as understanding the network evolution mechanism. Link prediction techniques are widely
applied in social networks to predict unknown connections
among people, which can be used to recommend friendship
or identify suspicious relationships. Most of the current
social networking systems are using link prediction to automatically suggest friends with a high degree of accuracy. In biological networks, link prediction methods have
been developed to predict previously unknown interactions
between proteins, thus signiﬁcantly reducing the costs of
empirical approaches. Readers can refer to the survey papers , for the recent progress in this ﬁeld.
Good network representations should be able to capture explicit and implicit connections between network
vertices thus enabling application to link prediction. 
and predict missing links based on the learned vertex representations on social networks. also applies
network representation learning to collaboration networks
and protein-protein interaction networks. They demonstrate
that on these networks links predicted using the learned
representations achieve better performance than traditional
similarity-based link prediction approaches.
Clustering
Network clustering refers to the task of partitioning network vertices into a set of clusters, such that vertices are
densely connected to each other within the same cluster,
but connected to few vertices from other clusters .
Such cluster structures, or communities widely occur in a
wide spectrum of networked systems from bioinformatics,
computer science, physics, sociology, etc., and have strong
implications. For example, in biology networks, clusters
may correspond to a group of proteins having the same
function; in the network of webpages, clusters are likely
pages having similar topics or related content; in social
networks, clusters may indicate groups of people having
similar interests or afﬁliations.
Researchers have proposed a large body of network
clustering algorithms based on various metrics of similarity
or strength of connection between vertices. Min-max cut and
normalized cut methods , seek to recursively partition a graph into two clusters that maximize the number of
intra-cluster connections and minimize the number of intercluster connections. Modularity-based methods (e.g., ,
 ) aim to maximize the modularity of a clustering, which
is the fraction of intra-cluster edges minus the expected
fraction assuming the edges were randomly distributed.
A network partitioning with high modularity would have
dense intra-cluster connections but sparse inter-cluster connections. Some other methods (e.g., ) try to identify
nodes with similar structural roles like bridges and outliers.
Recent NRL methods (e.g., GraRep , DNGR , M-
NMF , and pRBM ) used the clustering performance
to evaluate the quality of the learned network representations on different networks. Intuitively, better representations would lead to better clustering performance. These
works followed the common approach that ﬁrst applies
an unsupervised NRL algorithm to learn vertex representations, and then performs k-means clustering on the
learned representations to cluster the vertices. In particular, pRBM showed that NRL methods outperform the
baseline that uses original features for clustering without
learning representations. This suggests that effective representation learning can improve the clustering performance.
Visualization
Visualization techniques play critical roles in managing,
exploring, and analyzing complex networked data. 
surveys a range of methods used to visualize graphs from an
information visualization perspective. This work compares
various traditional layouts used to visualize graphs, such as
tree-, 3D-, and hyperbolic-based methods, and shows that
classical visualization techniques are proved effective for
small or intermediate sized networks; they however confront a big challenge when applied to large-scale networks.
Few systems can claim to deal effectively with thousands
of vertices, although networks with this order of magnitude
often occur in a wide variety of applications. Consequently,
a ﬁrst step in the visualization process is often to reduce
the size of the network to display. One common approach
is essentially to ﬁnd an extremely low-dimensional representation of a network that preserves the intrinsic structure,
i.e., keeping similar vertices close and dissimilar vertices far
apart, in the low-dimensional space .
Network representation learning has the same objective
that embeds a large network into a new latent space of
low dimensionality. After new embeddings are obtained
in the vector space, popular methods such as t-distributed
stochastic neighbor embedding (t-SNE) can be applied
to visualize the network in a 2-D or 3-D space. By taking the
learned vertex representations as input, LINE used the t-
SNE package to visualize the DBLP co-author network after
the authors are mapped into a 2-D space, and showed that
LINE is able to cluster authors in the same ﬁeld to the same
community. HSCA illustrated the advantages of the
content-augmented NRL algorithm by visualizing citations
networks. Semi-supervised algorithms (e.g., TLINE ,
TriDNR , and DMF ) demonstrated that the visualization results have better clustering structures with vertex
labels properly imported.
Recommendation
In addition to structure, content, and vertex label information, many social networks also include geographical
and spatial-temporal information, and users can share their
experiences online with their friends for point of interest
(POI) recommendation, e.g., transportation, restaurant, and
sightseeing landmark, etc. Examples of such location-based
social networks (LBSN) include Foursquare, Yelp, Facebook
Places, and many others. For these types of social networks,
POI recommendation intends to recommend user interested
objects, depending on their own context, such as the geographic location of the users and their interests. Traditionally, this is solved by using approaches, such as collaborative
ﬁltering, to leverage spatial and temporal correlation between user activities and geographical distance . However, because each user’s check-in records are very sparse,
ﬁnding similar users or calculating transition probability
between users and locations is a signiﬁcant challenge.
Recently, spatial-temporal embedding , , has
emerged to learn low-dimensional dense vectors to represent users, locations, and point-of-interests etc. As a result,
each user, location, and POI can be represented as a lowdimensional vector, respectively, for similarity search and
many other analysis. An inherent advantage of such spatialtemporal aware embedding is that it alleviates the data sparsity problem, because the learned low dimensional vector is
typically much more dense than the original representation.
As a result, it makes query tasks, such as top-k POI search,
much more accurate than traditional approaches.
Knowledge Graph
Knowledge graphs represent a new type of data structure
in database systems which encode structured information
of billions of entities and their rich relations. A knowledge
graph typically contains a rich set of heterogeneous objects and different types of entity relationships. Such networked entities form a gigantic graph and is now powering
many commercial search engines to ﬁnd similar objects
online. Traditionally, knowledge graph search is carried
out through database driven approaches to explore schema
mapping between entities, including entity relationships.
Recent advancement in network representation learning has
inspired structured embeddings of knowledge bases .
Such embedding methods learn a low-dimensional vector representation for knowledge graph entities, such that
generic database queries, such as top-k search, can be carried out by comparing vector representation of the query
object and objects in the database.
In addition to using vector representation to represent
knowledge graph entities, researchers have also proposed
to use such representation to further enhance and complete
the knowledge graph itself. For example, knowledge graph
completion intends to discover complete relationships between entities, and a recent work has proposed to use
graph context to ﬁnd missing links between entities. This is
similar to link prediction in social networks, but the entities
are typically heterogeneous and a pair of entities may also
have different types of relationships.
EVALUATION PROTOCOLS
In this section, we discuss evaluation protocols for validating the effectiveness of network representation learning.
This includes a summary of commonly used benchmark
datasets and evaluation methods, followed by a comparison
of algorithm performance and complexity.
Benchmark Datasets
Benchmark datasets play an important role for the research
community to evaluate the performance of newly developed NRL algorithms as compared to the existing baseline methods. A handful of network datasets have been
made publicly available to facilitate the evaluation of NRL
algorithms across different tasks. We summarize a list of
network datasets used by most of the published network
representation learning papers in Table 6.
Table 6 summarizes the main characteristics of the publicly available benchmark datasets, including the type of
network (directed or undirected, binary or weighted), number of vertices |V |, number of edges |E|, number of labels
|Y|, whether the network is multi-labeled or not, as well
as whether network vertices are attached with attributes. In
Table 6, according to the property of information networks,
we classify benchmark datasets into eight different types:
Social Network. The BlogCatalog, Flickr and YouTube
datasets are formed by users of the corresponding online
social network platforms. For the three datasets, vertex
labels are deﬁned by user interest groups but user attributes
are unavailable. The Facebook network is a combination
of 10 Facebook ego-networks, where each vertex contains
user proﬁle attributes. The Amherst, Hamilton, Mich and
Rochester datasets are the Facebook networks formed
by users from the corresponding US universities, where
each user has six user proﬁle features. Often, user proﬁle
features are noisy, incomplete, and long-tail distributed.
Language Network. The language network Wikipedia is
a word co-occurrence network constructed from the entire
set of English Wikipedia pages. There is no class label on this
network. The word embeddings learned from this network
are evaluated by word analogy and document classiﬁcation.
Citation Network. The citation networks are directed information networks formed by author-author citation relationships or paper-paper citation relationships. They are
collected from different databases of academic papers, such
as DBLP and Citeseer. Among the commonly used citation
networks, DBLP (AuthorCitation) is a weighted citation
network between authors with the edge weight deﬁned by
the number of papers written by one author and cited by the
other author, while DBLP (PaperCitation) , Cora, Citeseer,
PubMed and Citeseer-M10 are the binary paper citation
networks, which are also attached with vertex text attributes
as the content of papers. Compared with user proﬁle features in social networks, the vertex text features here are
more topic-centric, informative and can better complement
network structure to learn effective vertex representations.
Collaboration Network. The collaboration network Arxiv
GR-QC describes the co-author relationships for papers
in the research ﬁeld of General Relativity and Quantum
Cosmology. In this network, vertices represent authors and
edges indicate co-author relationships between authors. Because there is no category information for vertices, this
network is used for the link prediction task to evaluate the
quality of learned vertex representations.
Webpage Network. Webpage networks (Wikipedia, WebKB and Political Blog ) are composed of real-world
webpages and hyperlinks between them, where the vertex
represents a webpage and the edge indicates that there is
a hyperlink from one webpage to another. Webpage text
content is often collected as vertex features.
Biological Network. As a typical biological network, the
Protein-Protein Interaction network is a subgraph of the
PPI network for Homo Sapiens. The vertex here represents
a protein and the edge indicates that there is an interaction
between proteins. The labels of vertices are obtained from
the hallmark gene sets and represent biological states.
Communication Network. The Enron Email Network is
formed by the Email communication between Enron employees, with vertices being employees and edges representing the email communicated between employees. Employees are labeled as 7 roles (e.g., CEO, president and manager),
according to their functions.
Trafﬁc Network. European Airline Networks used in 
are constructed from 6 airlines operating ﬂights between European airports: 4 commercial airlines (Air France, Easyjet,
Lufthansa, and RyanAir) and 2 cargo airlines (TAP Portugal,
and European Airline Transport). For each airline network,
vertices are airports and edges represent the direct ﬂights
between airports. In all, 45 airports are labeled as hub
airports, regional hubs, commercial hubs, and focus cities,
according to their structural roles.
Evaluation Methods
It is difﬁcult to directly compare the quality of the vertex
representations learned by different NRL algorithms, due
to the unavailability of ground truth. Alternatively, in order
to evaluate the effectiveness of NRL algorithms on learned
vertex representations, several network analytic tasks are
commonly used for comparison studies.
A summary of benchmark datasets for evaluating network representation learning.
Multi-label
Vertex attr.
Social Network
BlogCataloga
undirected, binary
undirected, binary
undirected, binary
undirected, binary
Amherstd 
undirected, binary
Hamiltond 
undirected, binary
Michd 
undirected, binary
Rochesterd 
undirected, binary
Language Network
Wikipedia 
undirected, weighted
1,000,924,086
Citation Network
DBLP (PaperCitation) , 
directed, binary
DBLP (AuthorCitation) , 
directed, weighted
20,580,238
directed, binary
directed, binary
directed, binary
Citeseer-M10f
directed, binary
Collaboration network
Arxiv GR-QC 
undirected, binary
Webpage Network
Wikipediae
directed, binary
directed, binary
Political Blog 
directed, binary
Biological Network
Protein-Protein Interaction 
undirected, binary
Communication Network
Enron Email Networkg
undirected, binary
Trafﬁc Network
European Airline Networksh
undirected, binary
a 
b 
c 
d 
e 
f 
g 
h 
Network Reconstruction. The aim of network reconstruction is to reconstruct the original network from the learned
vertex representations by predicting the links between vertices based on the inner product or similarity between vertex
representations. The known links in the original network
serve as the ground truth for evaluating reconstruction
performance. precision@k and MAP are often used
as evaluation metrics. This evaluation method can check
whether the learned vertex representations well preserve
network structure and support network formation.
Vertex Classiﬁcation. As an evaluation method for NRL,
vertex classiﬁcation is conducted by taking learned vertex
representations as features to train a classiﬁer on labeled
vertices. The classiﬁcation performance on unlabeled vertices is used to evaluate the quality of the learned vertex representations. Different vertex classiﬁcation settings, including binary-class classiﬁcation, multi-class classiﬁcation, and
multi-label classiﬁcation, are often carried out, depending
on the underlying network characteristics. For binary-class
classiﬁcation, F1 score is used as the evaluation criterion.
For multi-class and multi-label classiﬁcation, Micro-F1 and
Macro-F1 are adopted as evaluation criteria.
Vertex Clustering. To validate the effectiveness of NRL
algorithms, vertex clustering is also carried out by applying
k-means clustering algorithm to the learned vertex representations. Communities in networks are served as the ground
truth to assess the quality of clustering results, which is
measured by Accuracy and NMI (normalized mutual information) . The hypothesis is that, if the learned vertex
representations are indeed informative, vertex clustering on
learned vertex representations should be able to discover
community structures. That is, good vertex representations
are expected to generate good clustering results.
Link Prediction. Link prediction can be used to evaluate
whether the learned vertex representations are informative
to support the network evolution mechanism. To perform
link prediction on a network, a portion of edges are ﬁrst
removed, and vertex representations are learned from the remaining network. Finally, the removed edges are predicted
with the learned vertex representations. The performance of
link prediction is measured by AUC and precision@k.
Visualization. Visualization provides a straightforward way
to visually evaluate the quality of the learned vertex representations. Often, t-distributed stochastic neighbor embedding (t-SNE) is applied to project the learned vertex representation vectors into a 2-D space, where the distribution
of vertex 2-D mappings can be easily visualized. If vertex
representations are of good quality, in the 2-D space, vertices
within a same class or community should be embedded
closely, and the 2-D mappings of vertices in different classes
or communities should be far apart from each other.
In Table 7, we summarize the type of information networks and network analytic tasks used to evaluate the
quality of vertex representations learned by existing NRL
algorithms. We also provide hyperlinks for the codes of
respective NRL algorithms if available to help interested
readers to further study these algorithms or run experiments
for comparison. Overall, social networks and citation net-
A summary of NRL algorithms with respect to the evaluation methodology
Network Type
Evaluation Method
Social Dim. , , 
Social Network
Vertex Classiﬁcation
DeepWalk 
Social Network
Vertex Classiﬁcation
 
Citation Network
Language Network
Social Network
Vertex Classiﬁcation
Visualization
 
GraRep 
Citation Network
Language Network
Social Network
Vertex Classiﬁcation
Vertex Clustering
Visualization
 
Language Network
Vertex Clustering
Visualization
 
Collaboration Network
Language Network
Social Network
Network Reconstruction
Vertex Classiﬁcation
Link Prediction
Visualization
 
node2vec 
Biological Network
Language Network
Social Network
Vertex Classiﬁcation
Link Prediction
 
Social Network
Citation Network
Network Reconstruction
Link Prediction
Social Network
Citation Network
Collaboration Network
Link Prediction
GraphGAN 
Citation Network
Language Network
Social Network
Vertex Classiﬁcation
Link Prediction
Unsupervised
M-NMF 
Social Network
Webpage Network
Vertex Classiﬁcation
Vertex Clustering
 
struct2vec 
Trafﬁc Network
Vertex Classiﬁcation
GraphWave 
Trafﬁc Network
Communication Network
Vertex Clustering
Visualization
 
Social Network
Language Network
Biological Network
Vertex Classiﬁcation
Social Network
Citation Network
Collaboration Network
Network Reconstruction
Link Prediction
Vertex Classiﬁcation
Social Network
Collaboration Network
Citation Network
Vertex Classiﬁcation
Visualization
Citation Network
Webpage Network
Vertex Classiﬁcation
 
Citation Network
Webpage Network
Vertex Classiﬁcation
Visualization
 
Social Network
Vertex Clustering
UPP-SNE 
Social Network
Vertex Classiﬁcation
Vertex Clustering
Social Network
Citation Network
Webpage network
Vertex Classiﬁcation
Link Prediction
Social Network
Vertex Classiﬁcation
Citation Network
Webpage Network
Vertex Classiﬁcation
Visualization
 
TLINE 
Citation Network
Collaboration Network
Vertex Classiﬁcation
Visualization
Semi-supervised
Social Network
Vertex Classiﬁcation
SemiNE 
Social Network
Network Reconstruction
Vertex Classiﬁcation
Link prediction
TriDNR 
Citation Network
Vertex Classiﬁcation
Visualization
 
Social Network
Citation Network
Vertex Classiﬁcation
Citation Network
Vertex Classiﬁcation
Visualization
 CC
Planetoid 
Citation Network
Vertex Classiﬁcation
Visualization
 
Social Network
Vertex Classiﬁcation
works are frequently used as benchmark datasets, and vertex classiﬁcation is most commonly used as the evaluation
method in both unsupervised and semi-supervised settings.
Empirical Results
We observe from the literature that empirical evaluation
is often carried out on different datasets under different
settings. There is a lack of consistency on empirical results
to determine the best performing algorithms and their circumstances. Therefore, we perform benchmark experiments
to fairly compare the performance of several representative
NRL algorithms on the same set of datasets. Note that, because semi-supervised NRL algorithms are task-dependent:
the target task may be binary or multi-class, or multi-label
classiﬁcation, or because they use different classiﬁcation
strategies, it would be difﬁcult to assess the effectiveness of
network embedding under the same settings. Therefore, our
empirical study focuses on comparing seven unsupervised
NRL algorithms (DeepWalk , LINE , node2vec , M-
NMF , TADW , HSCA , UPP-SNE ) on vertex
classiﬁcation and vertex clustering, which are the two most
commonly used evaluation methods in the literature.
Our empirical studies are based on seven benchmark
datasets: Amherst, Hamilton, Mich, Rochester, Citeseer,
Cora and Facebook. Following , for Amherst, Hamilton,
Mich and Rochester, only the network structure is used and
Vertex Classiﬁcation Results on Seven Datasets
Training ratio = 5%
Training ratio = 50%
Vertex Clustering Results on on Seven Datasets
the attribute “year” is used as class label, which is a good
indicator of community structure. For Citeseer and Cora,
the research area is used as the class label. The class label of
Facebook dataset is given by the attribute “education type”.
Experimental Setup
For random walk based methods, DeepWalk, node2vec and
UPP-SNE, we uniformly set the number of walks, walk
length and window size as 10, 80, 10, respectively. For
UPP-SNE, we use the implementation that is optimized
by stochastic gradient descent. The parameter p and q of
node2vec are set to 1, as the default setting. For M-NMF,
we set α and β as 1. For all algorithms, the dimension of
learned vertex representations is set to 256. For LINE, we
learn 128-dimensional vertex representations with the ﬁrstorder proximity preserving version and the second-order
proximity preserving version respectively and concatenate
them together to obtain 256-dimensional vertex representations. The other parameters of the above algorithms are all
set to their default values.
Taking the learned vertex representations as input, we
carry out vertex classiﬁcation and vertex clustering experiments to evaluate the quality of learned vertex representations. For vertex classiﬁcation, we randomly select 5%
and 50% samples to train an SVM classiﬁer (with the LI-
BLINEAR implementation ) and test it on the remaining
samples. We repeat this process 10 times and report the
averaged Micro-F1 and Macro-F1 values. We adopt Kmeans to perform vertex clustering. To reduce the variance
caused by random initialization, we repeat the clustering
process for 20 times and report the averaged Accuracy and
NMI values.
Performance Comparison
Table 8 and 9 compare the performance of different algorithms on vertex classiﬁcation and vertex clustering. For
each dataset, the best performing method across all baselines is bold-faced. For the attributed networks (Citeseer,
Cora and Facebook), the underlined results indicate the
best performer among the structure only preserving NRL
algorithms (DeepWalk, LINE, node2vec and M-NMF).
Table 8 shows that among structure only preserving
NRL algorithms, when the training ratio is 5%, node2vec
achieves the best classiﬁcation performance overall, and
when the training ratio is 50%, M-NMF performs best in
terms of Micro-F1 while DeepWalk is the winner of Macro-
F1. Here, M-NMF does not exhibit signiﬁcant advantage
over DeepWalk, LINE and node2vec. This is probably due
to that the parameter α and β of N-NMF are not optimally tuned; their values must be carefully chosen so as
to achieve a good trade-off between different components.
On attributed networks (Citeseer, Cora and Facebook), the
content augmented NRL performs much better than the
structure only preserving NRL algorithms. This proves that
vertex attributes can largely contribute to learning more
informative vertex representations. When training ratio is
5%, UPP-SNE is the best performer. This indicates that the
UPP-SNE’s non-linear mapping provides a better way to
construct vertex representations from vertex attributes than
the linear mapping, as is done in TADW and HSCA. When
training ratio is 50%, TADW achieves the best overall classiﬁcation performance, although in some cases, it is slightly
outperformed by HSCA. On citation networks (Citeseer and
Cora), HSCA performs better than TADW, while it yields
worse performance than TADW on Facebook. This might be
caused by the fact that the homophily property of Facebook
social network is weaker than that of citation networks. The
homophily preserving objective should be weighted less to
make HSCA achieve satisfactory performance on Facebook.
Table 9 shows that LINE achieves the best clustering
performance on Amherst, Hamilton, Mich and Rochester.
As LINE’s vertex representations capture both the ﬁrstorder and second-order proximity, it can better preserve
the community structure, leading to good clustering per-
Complexity analysis
Complexity
Optimization Method
Unsupervised
Social Dim. , , 
Eigen Decomposition
GraRep 
O(|V ||E| + d|V |2)
GraphWave 
DeepWalk 
O(d|V | log |V |)
Stochastic Gradient Descent
O(dI|V |2)
node2vec 
GraphGAN 
O(|V | log |V |)
struct2vec 
O(dmI|V |)
M-NMF 
O(dI|V |2)
Alternative Optimization
O(|V ||E| + dI|E| + dmI|V | + d2I|V |)
O(|V ||E| + dI|E| + dmI|V | + d2I|V |)
UPP-SNE 
O(I|E| · nnz(X))
Gradient Descent
Semi-supervised
O(d|V | log |V |)
Stochastic Gradient Descent
TLINE 
SemiNE 
TriDNR 
O(d · nnz(X) log m + d|V | log |V |)
O(dI · nnz(X) + dI|E| + dI|Y||V |)
O(|V ||E| + dI|E| + dmI|V | + d2I|V |)
Alternative Optimization
O(m|V |2 + dI|V |2)
formance. On Citeseer, Cora and Facebook, the content
augmented NRL algorithm UPP-SNE performs best. As
UPP-SNE constructs vertex representations from vertex attributes via a non-linear mapping, the well preserved content information favors the best clustering performance. On
Citeseer and Cora, node2vec performs much better than
other structure only preserving NRL algorithms, including
its equivalent version DeepWalk. For each vertex context
pair (vi, vj), DeepWalk and node2vec use two different
strategies to approximate the probability Pr(vj|vi): hierarchical softmax , and negative sampling . The
better clustering performance of node2vec over DeepWalk
proves the advantage of negative sampling over hierarchical
softmax, which is consistent with the word embedding
results as reported in .
Complexity Analysis
To better understand the existing NRL algorithms, we provide a detailed analysis of their time complexity and underlying optimization methods in Table 10. A new notation I is
introduced to represent the number of iterations and we use
nnz(·) to denote the number of non-zero entries of a matrix.
In a nutshell, four kinds of solutions are used to optimize the
objectives of the existing NRL algorithms: (1) eigen decomposition that involves ﬁnding top-d eigenvectors of a matrix, (2) alternative optimization that optimizes one variable
with the remaining variables ﬁxed alternately, (3) gradient
descent that updates all parameters at each iteration for
optimizing the overall objective, and (4) stochastic gradient
descent that optimizes the partial objective stochastically in
an on-line mode.
Both unsupervised and semi-supervised NRL algorithms mainly adopt stochastic gradient descent to solve
their optimization problems. The time complexity of these
algorithms is often linear with respect to the number of
vertices/edges, which makes them scalable to large-scale
networks. By contrast, other optimization strategies usually
involve higher time complexity, which is quadratic with
regards to the number of vertices, or even higher with
the scale of the number of vertices times the number of
edges. The corresponding NRL algorithms usually perform
factorization on a |V | × |V | structure preserving matrix,
which is quite time-consuming. Efforts have been made to
reduce the complexity of matrix factorization. For example,
TADW , DMF and HSCA take advantage of the
sparsity of the original vertex-context matrix. HOPE 
and GraphWave adopt advanced techniques 
to perform matrix eigen decomposition.
FUTURE RESEARCH DIRECTIONS
In this section, we summarize six potential research directions and future challenges to stimulate research on network
representation learning.
Task-dependence: To date, most existing NRL algorithms
are task-independent, and task-speciﬁc NRL algorithms
have primarily focused on vertex classiﬁcation under the
semi-supervised setting. Only very recently, a few studies
have started to design task-speciﬁc NRL algorithms for link
prediction , community detection , , , ,
class imbalance learning , active learning , and
information retrieval . The advantage of using network
representation learning as an intermediate layer to solve the
target task is that the best possible information preserved in
the new representation can further beneﬁt the subsequent
task. Thus, a desirable task-speciﬁc NRL algorithm must
preserve information critical to the speciﬁc task in order to
optimize its performance.
Theory: Although the effectiveness of the existing NRL
algorithms has been empirically proved through experiments, the underlying working mechanism has not been
well understood. There is a lack of theoretical analysis with
regard to properties of algorithms and what contributes to
good empirical results. To better understand DeepWalk ,
LINE , and node2vec ,
 discovers their theoretical connections to graph Laplacians. However, in-depth
theoretical analysis about network representation learning is
necessary, as it provides a deep understanding of algorithms
and helps interpret empirical results.
Dynamics: Current research on network representation
learning has mainly concerned static networks. However,
in real-life scenarios, networks are not always static. The
underlying network structure may evolve over time, i.e.,
new vertices/edges appear while some old vertices/edges
disappear. The vertices/edges may also be described by
some time-varying information. Dynamic networks have
unique characteristics that make static network embedding
fail to work: (i) vertex content features may drift over time;
(ii) the addition of new vertices/edges requires learning
or updating vertex representations to be efﬁcient; and (iii)
network size is not ﬁxed. The work on dynamic network
embedding is rather limited; the majority of existing approaches (e.g., , , ) assume that the node set
is ﬁxed and deal with the dynamics caused by the deletion/addition of edges only. However, a more challenging
problem is to predict the representations of new added
vertices, which is referred to as “out-of-sample” problem. A
few attempts such as , , are made to exploit
inductive learning to address this issue. They learn an
explicit mapping function from a network at a snapshot,
and use this function to infer the representations of out-ofsample vertices, based on their available information such as
attributes or neighborhood structure. However, they have
not considered how to incrementally update the existing
mapping function. How to design effective and efﬁcient
representation learning algorithms in complex dynamic domains still requires further exploration.
Scalability: The scalability is another driving factor to
advance the research on network representation learning.
Several NRL algorithms have made attempts to scale up
to large-scale networks with linear time complexity with
respect to the number of vertices/edges. Nevertheless, the
scalability still remains a major challenge. Our ﬁndings
on complexity analysis show that random walk and edge
modeling based methods that adopt stochastic gradient
descent optimization are much more efﬁcient than matrix
factorization based methods that are solved by eigen decomposition and alternative optimization. Matrix factorization
based methods have shown great promise in incorporating
vertex attributes and discovering community structures, but
their scalability needs to be improved to handle networks
with millions or billions of vertices. Deep learning based
methods can capture non-linearity in networks, but their
computational cost is usually high. Traditional deep learning architectures take advantage of GPU to speed up training on Euclidean structured data . However, networks
do not have such a structure, and therefore require new
solutions to improve the scalability .
Heterogeneity and semantics: Representation learning for
heterogeneous information networks (HIN) is one promising research direction. The vast amounts of existing work
has focused on homogeneous network embedding, where
all vertices are of the same type and edges represent a single
relation. However, there is an increasing need to study
heterogeneous information networks with different types
of vertices and edges, such as DBLP, DBpedia, and Flickr.
An HIN is composed of different types of entities, such as
text, images, or videos, and the interdependencies between
entities are very complex. This makes it very difﬁcult to
measure rich semantics and proximity between vertices and
seek a common and coherent embedding space. Recent
studies by , , , , , , , ,
 , have investigated the use of various descriptors (e.g., metapath or meta structure) to capture semantic
proximity between distant HIN vertices for representation
learning. However, the research along this line is still at early
stage. Further research requires to investigate better ways
for capturing the proximity between cross-modal data, and
their interplay with network structure.
Another interesting direction is to investigate edge semantics in signed networks, where vertices have both positive and negative relationships. Signed networks are ubiquitous in social networks, such as Epinions and Slashdot, that
allow users to form positive or negative friendship/trust
connection to other users. The existence of negative links
makes the traditional homophily based network representation learning algorithms unable to be directly applied. Some
studies , , tackle signed network representation learning through directly modeling the polar of links.
How to fully encode network structure and vertex attributes
for signed network embedding remains an open question.
Robustness: Real-world networks are often noisy and uncertain, which makes traditional NRL algorithms unable to
produce stable and robust representations. ANE (Adversarial Network Embedding) and ARGA (Adversarially
Regularized Graph Autoencoder) learn robust vertex
representations via enforcing an adversarial learning regularizer . To deal with the uncertainty in the existence of
edges, URGE (Uncertain Graph Embedding) encodes
the edge existence probability into the vertex representation
learning process. It is of great importance to have more
research efforts on enhancing the robustness of network
representation learning.
CONCLUSION
This survey provides a comprehensive review of the stateof-the-art network representation learning algorithms in the
data mining and machine learning ﬁeld. We propose a taxonomy to summarize existing techniques into two settings:
unsupervised setting and semi-supervised settings. According to the information sources they use and the methodologies they employ, we further categorize different methods
at each setting into subgroups, review representative algorithms in each subgroup, and compare their advantages and
disadvantages. We summarize evaluation protocols used for
validating existing NRL algorithms, compare their empirical
performance and complexity, as well as point out a few
emerging research directions and the promising extensions.
Our categorization and analysis not only help researchers
to gain a comprehensive understanding of existing methods
in the ﬁeld, but also provide rich resources to advance the
research on network representation learning.
ACKNOWLEDGMENTS
The work was supported by the US National Science Foundation (NSF) through grant IIS-1763452, and the Australian
Research Council (ARC) through grant LP160100630 and
DP180100966. Daokun Zhang was supported by China
Scholarship Council (CSC) with No. 201506300082 and a
post-graduate scholarship from Data61, CSIRO in Australia.