Mach Learn 65:247–271
DOI 10.1007/s10994-006-9449-2
An analysis of diversity measures
E. K. Tang · P. N. Suganthan · X. Yao
Received: 5 January 2005 / Revised: 6 April 2006 / Accepted: 30 May 2006 / Published online: 14 July 2006
Springer Science + Business Media, LLC 2006
Abstract Diversity among the base classiﬁers is deemed to be important when constructing
a classiﬁer ensemble. Numerous algorithms have been proposed to construct a good classiﬁer
ensemble by seeking both the accuracy of the base classiﬁers and the diversity among them.
However, there is no generally accepted deﬁnition of diversity, and measuring the diversity
explicitly is very difﬁcult. Although researchers have designed several experimental studies to
compare different diversity measures, usually confusing results were observed. In this paper,
we present a theoretical analysis on six existing diversity measures (namely disagreement
measure, double fault measure, KW variance, inter-rater agreement, generalized diversity
and measure of difﬁculty), show underlying relationships between them, and relate them to
the concept of margin, which is more explicitly related to the success of ensemble learning
algorithms. We illustrate why confusing experimental results were observed and show that
the discussed diversity measures are naturally ineffective. Our analysis provides a deeper
understanding of the concept of diversity, and hence can help design better ensemble learning
algorithms.
Keywords Classiﬁer ensemble . Diversity measures . Margin distribution . Majority vote .
Disagreement measure . Double fault measure . KW variance . Interrater agreement .
Generalized diversity . Measure of difﬁculty . Entropy measure . Coincident failure diversity
Editor: Tom Fawcett
E. K. Tang . P. N. Suganthan
School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798
e-mail: 
P. N. Suganthan ()
e-mail: 
School of Computer Science, University of Birmingham, Birmingham, B15 2TT, UK
e-mail: 
Mach Learn 65:247–271
1. Introduction
Also known as committees of learners, mixtures of experts, multiple classiﬁer systems, and
classiﬁer ensembles, ensemble learning has been well established as a research area in the
past decades. Some well-known algorithms in this ﬁeld, such as bagging and
boosting ,havebeenproventobeeffectiveinsolving
pattern classiﬁcation tasks. Each classiﬁer in the ensemble is referred as a base classiﬁer.
Intuitively speaking, the key to the success of a classiﬁer ensemble is that the base classiﬁers
perform diversely. Empirical results have illustrated that there exists positive correlation
between accuracy of the ensemble and diversity among the base classiﬁers . Further, most of the existing ensemble learning algorithms
 can be interpreted as building diverse base classiﬁers implicitly.
In an ensemble learning algorithm, if the term “diversity” is deﬁned explicitly and optimized, we say the algorithm seeks diversity explicitly. Otherwise the algorithm seeks diversity
implicitly. Since many ensemble algorithms have been successfully proposed by seeking diversity implicitly, it is natural to consider whether we can perform better by seeking the
diversity explicitly. However, despite the popularity of the term diversity, there is no single
deﬁnition and measure of it. Although several measures have been proposed to represent
the diversity and are optimized explicitly in different ensemble learning algorithms, none of
these measures is proven superior to the others and why these diversity measures are useful
is still unclear. Solid empirical as well as theoretical validation of the explicitly diversitydriven algorithms is absent from this ﬁeld. In particular, three main questions in the analysis
of diversity have arisen from several previous empirical studies in relation to designing ensemble learning
algorithms:
1. When one seeks a set of diverse and accurate base classiﬁers, what cost function is optimized? Does optimizing the cost function guarantee good generalization performance?
2. Is there a trade-off between diversity and accuracy of the base classiﬁers? In other words,
do we have to sacriﬁce the accuracy of some base classiﬁers in order to increase the
diversity?
3. How to make use of the existing diversity measures for designing good classiﬁer ensembles? Besides the existing diversity measures, is a new precise diversity measure necessary
for designing ensemble learning algorithms?
Although all of the three questions are important, none of them has been thoroughly answered,
while several contradictory conclusions have been drawn from experimental studies. On
the other hand, in spite of the incomplete understanding of the concept of diversity, more
complete theories have been proposed to explain the success of classiﬁer ensembles. For
example, Schapire et al. introduced the concept of margin to analyze the behavior of
boosting type algorithms. This concept was then easily generalized to analyze other classes
of ensemble learning algorithms . Further, bias-variance
decomposition is also employed to explain the success of classiﬁer ensembles 65:247–271
this framework, the three questions presented above are answered by both theoretical and
experimental analysis.
The remainder of this paper is organized as follows. We ﬁrst introduce some relevant
concepts of classiﬁer ensembles, diversity measures and the margin. In Section 3, we analyze theoretically and experimentally the relationships among diversity measures, average
classiﬁcation accuracy of base classiﬁers and margin of the ensemble. We further discuss the
applications of diversity measures in an ensemble learning algorithm in Section 4. Conclusions are presented in Section 5.
2. Related background
Let a labeled training set be Tr = {(x1, y1), (x2, y2),. . . , (xN, yN)}, where yi is the class label
of xi. The base classiﬁers H = {h1,h2,. . . ,hL} of an ensemble are trained on the training
set, and the output of a base classiﬁer h j on sample xi is h j(xi). Given this set of base
classiﬁers, together with a corresponding set of weights w = [w1, . . . ,wL]T , where w j ≥0
and  w j = 1, the ensemble classiﬁes the samples by taking a weighted vote among the
base classiﬁers and choosing the class label that receives the largest weighted vote. For every
1 ≤i ≤N and 1 ≤j ≤L, the oracle output matrix O of an ensemble is deﬁned as:
Oi j = 1 if training sample xi is classiﬁed correctly by base classiﬁer h j
Oi j = −1 otherwise
Hence, the oracle output matrix is an N-by-L matrix whose elements take either 1 or –1. We
focus our investigation on the oracle output of the classiﬁers because:
1. Many other authors discussed the diversity in terms of oracle outputs, and most of the
diversity measures are deﬁned based on oracle outputs. Actually, all of the ten diversity
measures summarized by Kuncheva and Whitaker are based on oracle outputs
and the majority vote rule.
2. The oracle output incorporates no a priori knowledge of the data and makes no assumption
onwhatthebaseclassiﬁeris.Itonlyconcernswhetherasampleisclassiﬁedcorrectlyornot.
Hence, the oracle output provides a general model for analyzing a classiﬁer ensemble, and
conclusions drawn on this model can be easily generalized to various ensemble learning
The main notations used in this paper are summarized as follows:
L: total number of base classiﬁers
N: total number of training samples
mi: margin of an ensemble on the training sample xi
P: average classiﬁcation accuracy of the base classiﬁers on the training data
p j: classiﬁcation accuracy of the base classiﬁer h j
li: product of L and sum of the weights of the base classiﬁers that classify the training sample
xi incorrectly, li = L 
Oi j=−1 w j
Oi j: oracle output of the classiﬁer h j on the training sample xi
div: diversity among the base classiﬁers in the ensemble
Mach Learn 65:247–271
The deﬁnitions above directly yield the following equations:
When all base classiﬁers of the ensemble are uniformly weighted (i.e. w j = 1/L for j =
1 to L), we have the majority vote rule. Since majority vote is simple and is employed in
most works concerned with diversity measures, we will ﬁrst present our analysis based on it
and then generalize our conclusions to the non-uniformly weighted case. For the uniformly
weighted case, we simply replace the weights 1/L with 1 for the deﬁnition of li. Hence li
becomes the number of base classiﬁers that classify the training sample xi incorrectly. This
small modiﬁcation will not inﬂuence our analysis. In the subsequent sections, unless we refer
explicitly to the non-uniform weights, all deﬁnitions and derivations are based on majority
vote rule.
2.1. The diversity measures
Since a unique deﬁnition of diversity does not exist, we consider six diversity measures in
this work. These measures were proposed by different researchers independently. When we
mention diversity, we refer to the diversity that is deﬁned by these measures.
2.1.1. The disagreement measure
In 1996, Skalak proposed the disagreement measure to evaluate the diversity between
two base classiﬁers. Ho also employed the disagreement to measure diversity in a
decision forest. This measure is deﬁned based on the intuition that two diverse classiﬁers
perform differently on the same training data. Given two base classiﬁers h j and hk, let
n(a, b) be the number of training samples on which the oracle output of h j and hk is a and b
respectively. The diversity between the two base classiﬁers is measured by:
n(1, −1) + n(−1, 1)
n(1, 1) + n(−1, 1) + n(1, −1) + n(−1, −1).
Diversity within the whole set of base classiﬁers is then calculated by averaging over all pairs
of base classiﬁers:
Mach Learn 65:247–271
Since for any pair of base classiﬁers: n(1, 1) + n(1, −1) + n(−1, 1) + n( −1, −1) = N, we
(n j,k(1, −1) + n j,k(−1, 1)).
The diversity increases with the value of the disagreement measure.
2.1.2. The double-fault measure
Giacinto and Roli proposed the double-fault measure to select classiﬁers that are least
related from a pool of classiﬁers. The double-fault measure between a pair of base classiﬁers
is calculated by:
n(1, 1) + n(−1, 1) + n(1, −1) + n(−1, −1).
This measure also arose from the intuition that two classiﬁers should perform differently to
be diverse. Giacinto and Roli claimed that the more different two classiﬁers are, the fewer
the coincident errors between them. Same as the disagreement measure, the diversity within
the whole set of base classiﬁers is calculated as follows:
n j,k(−1, −1).
The diversity decreases when the value of the double-fault measure increases.
2.1.3. Kohavi-Wolpert variance
The Kohavi-Wolpert variance was proposed by Kohavi and Wolpert in their decomposition formula of the classiﬁcation error of a classiﬁer. This measure originated from
the bias-variance decomposition of the error of a classiﬁer. The original expression of the
variability of the predicted class label y for a sample x is
variancex = 1
P(y = ωi|x)2
where C is the number of classes. Since C = 2 in the case of oracle output, and P(y = 1 | x) +
P(y = −1 | x) = 1, we can get
variancex = 1
2(1 −P(y = 1 | x)2 −P(y = −1 | x)2) = P(y = 1 | x)P(y = −1 | x)
= P(O = 1 | x)P(O = −1 | x)
Mach Learn 65:247–271
As the term P(O = −1 | x) can be estimated by P(O = −1 | x) = li
L , Kuncheva and Whitaker
 presented a modiﬁed version of Eq. (9) to measure the diversity of an ensemble:
li(L −li).
The diversity increases with values increasing of the KW variance.
2.1.4. Measurement of inter-rater agreement
Thismeasureisdevelopedasameasureofinter-rater(inter-classiﬁer)reliability ,
called k. It can be used to measure the level of agreement within a set of classiﬁers, hence it
is also based on the assumption that a set of classiﬁers should disagree with one another to
be diverse. The diversity decreases when the value of k increases. The k is calculated by:
i=1 (L −li)li
NL(L −1)P(1 −P).
2.1.5. Generalized diversity
This measure is proposed by Partidge and Krzanowski . The heuristic behind this
measure is similar to that of the Double-Fault measure. Given two classiﬁers, Partidge and
Krzanowski argued that maximum diversity is achieved when failure of one classiﬁer is
accompanied by correct classiﬁcation by the other classiﬁer and minimum diversity occurs
when two classiﬁers fail together. Therefore, for a sample xi that is randomly drawn from the
training set, let T j denote the probability that li = j, the generalized diversity is deﬁned as:
The diversity increases with increasing values of the generalized diversity.
2.1.6. The measure of “difﬁculty”
This measure comes from the study of Hansen and Salamon . Deﬁning a discrete
random variable V, Vi = (L −li)/L for a sample xi that is randomly drawn from the training
set, the measure of difﬁculty was deﬁned as the variance of V over the whole training set.
diff = var(Vi).
The diversity increases with decreasing values of the measure of difﬁculty. The intuition of
this measure can be explained as: A diverse classiﬁer ensemble has a smaller value for this
measure since every training sample can at least be classiﬁed correctly by a portion of all the
base classiﬁers, which is likely to result in lower variance of V.
Mach Learn 65:247–271
Table 1 Summary of diversity
Table 1 shows a summary of the six measures. The “+” means that diversity is greater
when the measure is larger, and the “−” means that diversity is greater when the measure is
2.2. The margin of ensembles
To explain the remarkable success of the boosting algorithm, Schapire et al. introduced
the concept of margin into the area of ensemble learning. Let vi,C be the total vote that the
weighted ensemble casts for label C on sample xi. The margin of the ensemble on this sample
is deﬁned as mi = vi,y j −
c̸=y j vi,c. Given a set of base classiﬁers and the weights w = [w1,
. . . , wL]T , where w j ≥0 and  w j = 1, margin of the ensemble can be calculated by
Since boosting type algorithms construct an ensemble with respect to the vote rule, the
margin concept was easily generalized to all the ensembles that employ the vote rule as the
combination method . Several extensive studies have shown that the
generalization performance of an ensemble is related to the distribution of its margins on
the training samples. Schapire et al. proved that achieving a larger margin on the training set
results in an improved bound on the generalization error of the ensemble. They also proposed
the upper bound explicitly as the sum of a function of distribution of the margins and a
complexity penalty term . R¨atsch et al. analyzed the margins by
focusing on the “minimum margin” of the ensemble on training samples. Employing boosting
as an example, they explained the good generalization performance of an ensemble in terms
of the minimum margin that can be achieved by it: the ensemble with the largest minimum
margin will have the best generalization error bound1 .
Since the generalization error itself is actually immeasurable, this relationship provides us
an approach to analyze the relationship between diversity and generalization performance.
3. Analysis of diversity measures
3.1. Relationship between diversity and margins of an ensemble (majority vote case)
If we regard the generalization performance of an ensemble as a function F that is parameterized by the average classiﬁcation accuracy P of the base classiﬁers and the diversity among the
base classiﬁers (div), our analysis can be formulated as follows: We analyze how F changes
with respect to div if P is ﬁxed, and how P and div interact with each other to inﬂuence F.
1When the data is noisy, the ensemble that is constructed by maximizing the margin may over-ﬁt, which means
the generalization performance of the ensemble may decrease when the minimum margin is maximized. Some
discussion is presented in Section 3.
Mach Learn 65:247–271
Aiming to maximize the diversity between base classiﬁers, we begin with answering the ﬁrst
question that is mentioned in Section 1, the cost function for the six diversity measures can
be reformulated as:
The disagreement measure:
dis = 2L(1 −P)
The double-fault measure:
The Kohavi-Wolpert variance:
KW = 1 −P −
The measurement of inter-rater agreement:
k = LP −P −L
NL(L −1)P(1 −P)
The generalized diversity:
NL(L −1)(1 −P)
The measure of “Difﬁculty”:
i −L (1 −P)2
It can be observed from expressions (15)–(20) that all these cost functions contain the terms
i . Based on this observation, we propose the statement:
Lemma 1. If we regard the average classiﬁcation accuracy P of the base classiﬁers as a
constant, the diversity (div) is maximized only when all the training samples are classiﬁed
correctly by the same number of base classiﬁers, which means:
li = L (1 −P)
We call Eq. (21) the uniformity condition for maximizing the diversity. Appendix A provides
detailed derivations of Eqs. (15)–(20) and proof of Lemma 1.
In the uniformly weighted case the margin of an ensemble on sample xi is calculated by
equations:
mi = L −2li
Mach Learn 65:247–271
= N(2P −1) .
Since min (mi) ≤1
min (mi) ≤2P −1
As mentioned in Section 2.2, the best generalization error bound of an ensemble can be
achievedbymaximizingtheminimummarginoftheensembleonthetrainingsamples.Hence,
the best generalization error bound of the ensemble is achieved when min (mi) = 2P −1. It
is obvious that the equality in Eq. (25) holds only when:
mi = 2P −1
which means:
li = L (1 −P)
Therefore, for the six diversity measures discussed in this work, the answer to the ﬁrst question
posed in the Section 1 can be summarized by Eqs. (15)–(20) and the Lemma below:
Lemma 2. If P is regarded as a constant and the maximum diversity is achievable, maximizing the diversity among the base classiﬁers is equivalent to maximizing the minimum margin
of the ensemble on the training samples.
As a result, seeking diversity in an ensemble can be viewed as an implicit way to maximize the
minimum margin of the ensemble. The key observations presented in previous experimental
studies can be explained by this relationship.
3.2. Ineffectiveness of the diversity measures
In addition to theoretical derivations, we conducted several experiments to illustrate relationships among the average accuracy P, the diversity and the minimum margin of an ensemble.
In the ﬁrst experiment, setting the number of samples N = 100, the number of base classiﬁers
L ∈{15, 150, 1500} and the average accuracy P = 0.7, we randomly generate 10000 pseudo
oracle output matrices. These matrices are used to represent a set of classiﬁer ensembles.
The corresponding diversity measures and the minimum margins are calculated and relationships among them are plotted. The maximum minimum margin in this case is 0.4. For
each diversity measure, the optimized value is also presented in the ﬁgures. Similar plots are
observed for all the three values of L, but we only plot those ﬁgures corresponding to L =
150 in Figs. 1.1–1.6 to save space.
In the past, experimental study has revealed that large diversity does not always correspond
to good generalization performance even when P is ﬁxed .
That is why usefulness of diversity measures was questioned. From the previous subsection
and Figs. 1.1–1.6, two reasons of this discrepancy can be summarized:
Mach Learn 65:247–271
Fig. 1 The ﬁgures show the relationships between diversity measures and the minimum margin. L = 150,
horizontal axis represents the diversity measures and vertical axis represents the minimum margin
1. For a given P, the maximum diversity is usually not achievable.
According to the deﬁnition, li’s take discontinuous values, while L(1−P) is a continuous
variablesincePiscontinuous.Hence,Eq.(21)cannotbesatisﬁedingeneral.Whenweattempt
to seek diversity, we usually achieve only large diversity but not the maximum diversity.
2. The minimum margin of an ensemble is not monotonically increasing with respect to
diversity.
From Figs. 1.1–1.6, we observed that the minimum margin of a classiﬁer ensemble is not
monotonically increasing with the diversity, albeit positive correlation is as obvious as in the
literature. Therefore, enlarging diversity is different from enlarging the minimum margin.
Mach Learn 65:247–271
When P is a constant, we can ﬁnd from Eqs. (15)–(20) that the smaller the termN
larger the diversity. From Eq. (23), we can also ﬁnd that the minimum margin is determined
by the maximum li. The smaller the max(li), the larger the minimum margin. Since a smaller
i does not always mean a smaller max(li), a larger value for the diversity does not
guarantee a larger value for the minimum margin.
The two above-mentioned reasons result in the fact that large diversity may not consistently
correspond to a better generalization performance. This is a main drawback of using diversity
measures in the implementation of an ensemble learning algorithm.
The ﬁrst experiment only analyzed the diversity measures under the assumption that P is
a constant. However, P and div actually interact with each other to inﬂuence the performance
of an ensemble. By taking a closer look at Eqs. (21) and (25), one ﬁnds that P determines
the upper bound of the minimum margin, while distribution of li over the training set determines difference between the achieved minimum margin and this bound. To illustrate
this in the second experiment, we set L at 150 and tune P to 3/5, 2/3, 3/4, 4/5 and 9/10.
In Figs. 2.1–2.6, we plot the relationship between each measure and the minimum margin
with different values of P in one ﬁgure (Bigger versions of these ﬁgures can be found in our
online supplementary material2). The ﬁgures show that increasing average accuracy does
increase the upper-bound of the minimum margin, but the realized minimum margin may or
may not increase (although the positive correlation between diversity and minimum margin
is obvious in Fig. 2). In order to maximize the minimum margin of an ensemble, we need to
maximize P and simultaneously satisfy the uniformity condition in Eq. (21). However, we
have presented that the diversity measures cannot represent the uniformity condition well.
Hence, we cannot expect a monotonic relationship between the generalization performance
and the interactions between accuracy and diversity. This explains why experimental results
in the past were inconclusive and contradictory. It was usually deemed that there exists a
trade-off between the average accuracy and diversity. In other words, smaller P may correspond to larger div. This hypothesis is not reasonable according to our decomposition of
the diversity measures. From Eqs. (15)–(20), all the diversity measures described in Section
2.1 can be formulated as div = a −(bP + c N
i ), where a, b and c are constants. Hence
relationship between div and P is inﬂuenced by the term N
i . To study the relationship
between P and N
i , we carried out the third set of experiments. We set L at 15, randomly
choose values for P between [0.5, 1] and generate ensembles with the chosen P. 10000 ensembles are generated again and relationship between P and the termN
i is plotted in
Fig. 3. An obvious negative correlation between P and the termN
i can be observed with
a correlation coefﬁcient value of –0.9689. The strong negative correlation between P and
i implies that a smaller P is not likely to result in a smaller value of 65:247–271
Fig. 2 The ﬁgures show the relationships between diversity measures and the minimum margin. L = 15,
horizontal axis represents the diversity measures and vertical axis represents the minimum margin. The upper
bound of the minimum margin is also shown in the ﬁgures corresponding to P = 3/5, 2/3, 3/4, 4/5, and 9/10
based on the concept of margin maximization, may also be over-trained on noisy training data.
Hence, regularization terms are incorporated in both boosting and SVM algorithms . In contrast, we cannot ﬁnd a regularization term
in the discussed six diversity measures. This observation implies that even when maximum
values for the diversity measures are achieved, we may only obtain undesirably over-ﬁtted
solutions.
Mach Learn 65:247–271
Fig. 3 Relationship between P
i . Horizontal axis
represents P and vertical axis is
All the experiments above do not really construct classiﬁer ensembles and evaluate their
performance on real-world problems. Hence we also carried out a simple empirical experiment to demonstrate our analysis. In this experiment, we employ least squares support vector
machine (LSSVM) as the base classiﬁer, and the ensembles are evaluated on 11 different 2-class datasets described in the work of R¨atsch et al. . For each
of the datasets, the experiment was conducted for 100 times on randomly partitioned training
and test sets with 60% for training and 40% for testing. For each pair of training and test sets,
300 different LSSVMs are generated. We ﬁrst select the classiﬁer with the best performance.
Then we employ the diversity measures to sequentially select other base classiﬁers. Pseudo
code of the experiment is presented below:
1. Generate 300 base classiﬁers using the training data.
2. Select the base classiﬁer h1 which has the smallest training error, the oracle output of this
classiﬁer is denoted by O1,O = O1
3. Greedily select other base classiﬁers
For i = 1:149
for j = all the unselected classiﬁer
calculate div([O,O j]);
select classiﬁer h j such that div([O,O j]) is maximized;
O = [O,O j],
4. Apply the ensemble to test data using majority vote rule.
Results of the experiment are presented in Table 2. Among all the 11 datasets, exploiting
diversity to construct ensemble only achieved better performance on breast cancer and thyroid
datasets. For the other nine datasets, sometimes results of the ensembles are comparable to
that of single LSSVM and SVM (e.g. GD for heart dataset, DF, k and diff for twonorm
dataset). But we can observe that the ensembles perform signiﬁcantly worse in many cases,
such as Dis for heart dataset and k for diabetis dataset. According to previous analysis, these
empirical results show that seeking diversity explicitly is not likely to generate promising
classiﬁcation performance consistently.
Mach Learn 65:247–271
Table 2 Classiﬁcation results of several well-known classiﬁcation methods and ensembles achieved by
seeking diversity explicitly. SVM is support vector machine and LSSVM is least-squares support vector
15.3 ± 1.4 12.7 ± 0.9 15.3 ± 1.4 13.4 ± 0.7 12.6 ± 0.7 13.1 ± 1.2 10.8 ± 0.6 11.6 ± 0.7
25.8 ± 4.6 25.4 ± 4.0 25.8 ± 4.6 25.4 ± 4.8 25.4 ± 4.0 25.3 ± 4.5 26.8 ± 4.5 26.0 ± 4.7
28.7 ± 2.0 24.7 ± 1.7 28.7 ± 2.0 28.9 ± 2.1 27.2 ± 1.7 26.9 ± 1.6 23.5 ± 1.8 23.5 ± 1.7
35.2 ± 1.6 35.1 ± 1.8 35.2 ± 1.9 35.2 ± 2.7 35.3 ± 1.7 35.2 ± 2.0 34.2 ± 1.9 32.4 ± 1.8
25.1 ± 2.2 25.3 ± 2.2 25.1 ± 2.2 26.1 ± 2.1 25.9 ± 2.0 25.8 ± 2.0 23.6 ± 2.1 23.6 ± 2.1
26.2 ± 4.7 16.5 ± 3.6 26.2 ± 4.5 20.2 ± 3.5 17.5 ± 2.9 18.2 ± 3.2 16.4 ± 3.1 16.0 ± 3.3
24.8 ± 4.7 22.7 ± 1.2 24.8 ± 0.7 23.2 ± 0.8 22.7 ± 1.2 24.7 ± 1.7 22.6 ± 0.8 22.4 ± 1.0
Waveform 18.5 ± 1.3 11.8 ± 0.7 18.5 ± 1.5 13.0 ± 0.7 12.3 ± 0.8 12.2 ± 0.6
∗The results of regularized ADABOOSTING and SVM are originally presented by R¨atsch et al. .
Based on all the results presented above, we are able to make the following conclusions:
Compared to those algorithms that seek diversity implicitly, exploiting diversity measures to
seek diversity explicitly is ineffective in consistently achieving ensembles with good generalization performance. Firstly, the change of measured diversity cannot provide consistent
guidance on whether a set of base classiﬁers possesses good generalization performance.
Secondly, the measures are naturally correlated to the average accuracy of the base classi-
ﬁers. This property is not desirable since we do not require the diversity measures to become
another estimate for the classiﬁcation accuracy. Finally, all the discussed diversity measures
contain no regularization term. Therefore, even if these existing diversity measures can be
maximized, the achieved ensemble may be an over-ﬁt. It should be noted that the diversity
measures are still useful. A more detailed discussion on exploiting the diversity measures is
presented in Section 4.
3.3. Relationship between diversity and margins of an ensemble (General case)
So far we have presented our analysis based on the assumption that all the individual base
classiﬁers are assigned the same weights. Now we will show that this assumption is not
strictly necessary. Lemmas 1 and 2 can be generalized to the non-uniform weights of the
base classiﬁers, which also validates our experimental analysis in Section 3.2 for the nonuniformly weighted case. For brevity we present the disagreement measure as an example
here, detailed derivation for the other ﬁve measures can be found in Appendix B.
Since the weights of base classiﬁers are no longer same now, some equations we introduced
in previous sections need to be modiﬁed. Given L base classiﬁers, the N-by-L oracle output
matrix O and a weighting vector w = [w1, w2, . . . , wL]T (where w j ≥0 and  w j = 1) for
the base classiﬁers, Eq. (5) is modiﬁed as:
wiw jdis j,k
Mach Learn 65:247–271
and deﬁnition of li remains as:
With the deﬁnition presented by Eqs. (1) and (14), Eqs. (2) and (23) still hold in this case.
Using some simple derivations, we have:
wiw j(n j,k(1, −1) + n j,k(−1, 1)) = NwT w −wT OT Ow
Let {Ow}i be the ith element of vector Ow. For each training sample xi, we have
w j = L(1 −{Ow}i)
L −li = L −L
w j = L(1 + {Ow}i)
Which gives
li(L −li) = L2(N −wT OT Ow)
So Eq. (27) can be rewritten as:
li(L −li) −1 −wT w
Since Eq. (2) still holds in this case, we have
li = NL(1 −P)
dis = 2(1 −P)
i −1 −wT w
Mach Learn 65:247–271
As w is the given weighting vector, the term
2L(L−1) can be regarded as a constant when
optimizing dis with respect to li. Therefore, except for an additional L2 term in the denominator, the equation above is almost the same as Eq. (15). When optimizing Eq. (31), since
both the cost function and the constraint are in the same form as we have shown for majority
vote case in Appendix A, the solution of this problem will also in the same form as in the
majority case. Therefore, the previous analysis is also applicable to non-uniformly weighted
case, and Lemma 2 can be generalized as:
Lemma 3 (generalized form of Lemma 2). Given L base classiﬁers weighted by weights w1,
w2, . . . , wL, if P is regarded as a constant and the maximum diversity is achievable, maximizing diversity among the base classiﬁers is equivalent to maximizing the minimum margin
of the ensemble on the training samples.
4. Discussions
Up to this point, we have analyzed six diversity measures. All of them require satisfying the
uniformity condition. When the uniformity condition is maximized, the minimum margin
of the classiﬁer ensemble is maximized. In order to design a classiﬁer ensemble, simply
analyzing relationship between diversity, training accuracy and margin of the ensemble is
not enough. All the conclusions of the theoretical and experimental results must be considered
in a much larger context. Hence, in this section we try to propose some answers for the third
question presented in Section 1 by examining possible applications of diversity measures in
an ensemble learning algorithm.
A typical ensemble learning algorithm can be summarized in three stages:
1. Given the training samples, generate a set of base classiﬁers
2. Select a subset of the generated base classiﬁers
3. Construct the ensemble with a combination scheme
These procedures can be illustrated using two examples, a boosting algorithm and the neural
network ensemble proposed by Giacinto and Roli . In a boosting algorithm, we ﬁrst
generate the base classiﬁers sequentially. After that, the base classiﬁers are pruned either to
avoid over-ﬁtting or to reduce computational complexity for testing. Here pruning functions
as a classiﬁer selector and several methods have been proposed in the literature . Finally, the selected base classiﬁers are combined
according to the predeﬁned weighted vote rule. In the second example, after generating the
base classiﬁers, a clustering method is employed to cluster them, then only one classiﬁer is
chosen from each cluster to construct the ensemble, and majority vote is employed as the
combination rule.
In these procedures, a classiﬁer ensemble is actually constructed in stages 2 and 3. What is
the optimal base classiﬁer is unclear in stage 1 and any base classiﬁer generated in this stage
maybeusefulfortheﬁnalensemble.Therefore,thegoalinthisstageisonlytogeneratediverse
classiﬁers rather than achieving a good classiﬁer ensemble. Bagging, boosting and random
subspace methods can all be employed to seek diversity in this stage. Bagging and random
subspace methods generate different classiﬁers by varying the training data randomly, while
boosting achieves diversity by using a deterministic procedure. In this situation, one can also
employ diversity measures by including them in the objective function. Since varying training
data (implemented as bagging or random subspace method) may not inﬂuence performance of
the classiﬁer substantially, exploiting a diversity measure is likely to generate more diverse
Mach Learn 65:247–271
classiﬁers because difference between classiﬁers is required more explicitly. Whether the
deﬁnition of diversity is precise or not is not very important in this stage.
The problem to be solved in the second stage is much more difﬁcult than in the ﬁrst stage.
If one exploits diversity measures as criteria to select the base classiﬁers, then the diversity
measure is required to be precise, since the choice of diversity measure will directly inﬂuence
the ﬁnal ensemble and subsequently the classiﬁcation result. Unfortunately, as shown in the
previous section, none of the six diversity measures is suitable for this task. In addition to
all the theoretical analyses already presented, another problem that is important for practical
implementation needs to be noted.
Matrix Cover Problem
Input: A positive constant θ and a real-valued matrix O of size N-by-T such that, for any
integer 1 ≤i ≤N, T
j=1 Oi j > θ
Output: A minimal subset L of the columns of O such that, for all 1 ≤i ≤N, L
j=1 Oi j > θ
By replacing θ with the minimum margin calculated from L and P, the base classiﬁer selection
problem can be formulated as a matrix cover problem. Tamon and Xiang proved that
the Matrix Cover Problem below is NP-complete. Hence, we can hardly achieve the optimal
subset of base classiﬁers corresponding to the optimal margin. As the diversity measures are
not single-valued functions except for the optimized case (i.e. Eq. (21) is satisﬁed), different
ensembles may have the same P and diversity div but different performance on the data. One
cannot differentiate between these kinds of ensembles by employing diversity measures,
which will cause problems in the implementation of the algorithm. On the other hand, the
base classiﬁer selection problem has been well studied in the pattern recognition literature
as a feature selection problem. Hence, instead of using diversity measures, we can employ a
well-developed feature selection method to select base classiﬁers, such as sequential forward
(as we employed in the experiment on real-world datasets) and sequential ﬂoating forward
scheme with different selection criteria.
After selecting L base classiﬁers, a combination scheme should be decided in the third
stage. This problem is naturally a classiﬁcation problem in an L-dimensional space. Hence
effective classiﬁcation methods can be easily exploited. Diversity measures are generally not
applicable in this stage. Another application of diversity measures is to visualize relationships of the base classiﬁers in an ensemble or different ensembles. This application is not
directly related to ensemble design. Hence, the precise deﬁnition of diversity is also not quite
important.
A frequently asked question is: What is a “good” diversity measure for designing an ensemble learning algorithm? We have discussed three possible applications of the diversity
measures in an ensemble learning algorithm: generating individual classiﬁers, visualizing
relevant properties of the ensemble and selecting base classiﬁers. In our opinion, the existing
diversity measures are sufﬁcient for the ﬁrst two purposes, but are not for the last one. Our
analysis partially suggests some evaluation criteria for proposing new diversity measures.
As maximizing the minimum margin of the ensemble (or satisfying the uniformity condition) is the objective of maximizing diversity, one will expect the minimum margin to be a
single-valued function of diversity and expect it to asymptotically converge to the maximum
value. Different objectives other than the margin can also be used, but no matter what it is,
it should be monotonic and single-valued with respect to the measure.
Mach Learn 65:247–271
5. Conclusions
In the pattern recognition ﬁeld, many good algorithms have heuristically been proposed. Subsequently, theoretical analyses are developed to explain the good performance and further
improvethem.Reviewingtheliteratureofensemblelearning,onecanﬁndseveraltheoriesthat
tend to explain the success of ensemble learning algorithms, such as the concept of diversity,
the concept of margin, the ambiguity decomposition , the biasvariance decomposition and the bias-variance-covariance decomposition .
According to the numerous previous works, all of these theories do present some of the underlying mechanism of ensemble learning, and thus should be related to one another. However,
to our knowledge, such studies mainly exist in the regression context .
We have reviewed six existing measures that quantify the diversity among base classiﬁers
of a classiﬁer ensemble, and demonstrated explicitly the relation between these measures
and the margin maximization concept, which accounts for the success of several pattern
classiﬁcation algorithms. Since the diversity measures are motivated from different areas of
pattern classiﬁcation, identifying the link between these measures is required to study why
these measures are useful or not for classiﬁcation. In this process, we presented the uniformity condition for maximizing both the diversity and the minimum margin of an ensemble
and demonstrated theoretically and experimentally the ineffectiveness of the diversity measures for constructing ensembles with good generalization performance. We believe that our
analysis has highlighted some relationships between different theories in the classiﬁcation
context, and could hence help design better ensemble learning algorithms.
In addition to the presented six diversity measures, many existing diversity measures have
originated from different research areas that remotely relate to pattern recognition ﬁeld rather
than from pattern recognition ﬁeld itself. It is natural that not all of them can be put into one
framework,3 and a thorough analysis of all the diversity measures is out of the scope of
this work. However, previous experimental studies have shown that most diversity measures
perform similarly. Thus we can expect other diversity measures to have similar properties as
the measures analyzed in this work.
Appendix A
Proofs of Eqs. (15)–(20) and Lemma 1
Proof for the disagreement measure:
As deﬁned in Section 2, for a sample xi, li base classiﬁers classify it incorrectly
and the other L−li classiﬁers classify it correctly. Then for this sample, there are
li(L−li) pairs of base classiﬁers whose oracle outputs are different. Hence, the term
k= j+1 (n j,k (1, −1) + n j,k (−1, 1)) is equivalent to the termN
i=1 li (L −li), and
Eq. (6) can be re-written as:
li (L −li)
3 In the online supplementary materials, we also discuss two more diversity measures that are relevant to our
Mach Learn 65:247–271
We derive from Eq. (2) that:
li = NL (1 −P) .
To maximize the diversity, dis need to be maximized. By substituting Eq. (A2) into Eq. (A1),
dis = 2L (1 −P)
If P is regarded as a constant, the diversity maximization problem becomes a Lagrangian
formulation: We are given the constrained maximization problem:
dis = 2L (1 −P)
With respect to the constraint
li = NL (1 −P) .
By introducing the Lagrangian multiplier, we obtain
Ldis = 2L (1 −P)
li −λ · NL (1 −P) .
For all i, differentiating Eq. (A3) with respect to li, we obtain
NL (L −1) + λ = 0.
Hence, for all i:
li = λNL (L −1)
which means
li = L (1 −P) .
Mach Learn 65:247–271
Therefore, Eq. (15) is maximized when Eq. (21) is satisﬁed.
max (dis) = 2L P (1 −P)
if and only if:
li = L (1 −P) ∀i.
Proof for the double-fault measure:
According to the deﬁnition, for each sample xi, there are li(li −1)/2 pairs of base classiﬁers
whose oracle outputs on xi are −1. By observing that the term L
k= j+1 n j,k (−1, −1)
is equivalent to the term N
i=1 li (li −1)/2 , we re-write Eq. (8) as:
li (li −1)
In this case, we need to minimize the DF to maximize the diversity. Similar to the proof
for dis, after solving the minimization problem by introducing a Lagrangian multiplier, we
min (DF) = (1 −P) (L −LP −1)
if and only if Eq. (21) is satisﬁed.
Proof for the Kohavi-Wolpert variance
From the proof for the ﬁrst two measures, it is easy to derive that:
li (L −li)
We need to maximize Eq. (17) to maximize the diversity. Then the proof is exactly the same
as the proof for the disagreement measure. If and only if Eq. (21) is satisﬁed, the maximum
diversity can be achieved as:
max (KW) = P (1 −P) .
Proof for the measurement of inter-rater agreement:
It can be derived directly from the deﬁnition of this measure that:
i=1 (L −li)li
NL (L −1) P (1 −P)
Mach Learn 65:247–271
= 1 −NL2 (1 −P) −N
NL (L −1) P (1 −P)
= LP −P −L
NL (L −1) P (1 −P).
The maximum diversity can be achieved when Eq. (18) is minimized, which means
i needs to be minimized. Similar to the former three proofs, we only need to satisfy
Eq. (21) and:
min (k) = −
Proof for the generalized diversity:
We ﬁrst prove that the terms L
L TJ and L
L(L−1)Tj are equivalent to the terms
i=1 li(li−1)
respectively:
Let n(j) be the number of samples classiﬁed incorrectly by j base classiﬁers, then:
n ( j) = N
Tj = n ( j)
From the deﬁnition of li, we can get
We can obtain:
Mach Learn 65:247–271
Similarly,
= j ( j −1)
L (L −1)Tj =
L (L −1)Tj
j ( j −1) n ( j)
li (li −1)
i=1 li (li −1)
From Eqs. (A15) and (A16), Eq. (12) can be re-written as:
i=1 li (li −1)
NL (L −1) (1 −P)
NL (L −1) (1 −P).
If and only if Eq. (21) is satisﬁed, the generalized diversity is maximized:
max (GD) =
Proof for the measure of “Difﬁculty”:
From the deﬁnition of this measure, we need to minimize the term:
i −L (1 −P)2 .
It is obvious that when Eq. (21) is satisﬁed
min (V ) = 0.
Mach Learn 65:247–271
Appendix B
In this section we show that our analysis can also be generalized to non-uniformly weighted
case for the other diversity measures besides the disagreement measure.
Proof for Double-Fault measure:
Similar to Eq. (27), Eq. (8) is redeﬁned as:
wkw jn j,k (−1, −1)
Let 1 be an N-by-L matrix with all the elements as one. It is easy to get:
wkw jn j,k (−1, −1) = 1
wT (O −1)T (O −1) w
Since there is:
i = L2wT (O −1)T (O −1) w
Eq. (A19) can be re-written as:
NL3 (L −1)
Again, by optimizing DF with respect to li, we can validate our analysis in the non-uniformly
weighted case. If w j is one for all base classiﬁers, Eq. (A21) is almost the same as Eq. (16) exceptforanadditionalL2 terminthedenominator.Therefore,Eq.(16)isaspecialcaseof(A21).
Proof for the Kohavi-Wolpert variance
Following the same rationale described in Section 2.1.3, we have
P (O = −1 | x) =
P (O = 1 | x) =
w j = 1 −li
Then we get
P (y = ωk |x)2
li (L −li)
(A22) is exactly same as Eq. (10), hence our analysis is equivalent for both uniformly and
non-uniformly weighted case.
Mach Learn 65:247–271
Proof for the measurement of inter-rater agreement and “Difﬁculty”:
According to Eqs. (11) and (20), the deﬁnitions of these two measures directly include the
term li and do not contain any additional constraints on w. Therefore, Eqs. (11) and (20)
are valid for both uniformly and non-uniformly weighted cases. Since Eq. (2) also remains
unchanged in non-uniformly weighted case, it is obvious that any analysis based on li’s in
uniformly weighted case can be generalized to non-uniformly weighted case for these two
measures, and the Lemma 3 holds.
Proof for the generalized diversity:
Because Eqs. (A15) and (A16) directly result in Eq. (19), to generalize Eq. (19) to nonuniformly weighted case, we only need to check whether Eqs. (A15) and (A16) still hold if
the weights are non-uniform. If the weights are non-uniform, li and j in the original deﬁnition
will no longer only take integers from 0 to L, but they are still discontinuous. We deﬁne 
as the set that contains all possible values of li. Then Eq. (A11) can be re-written as (A23)
and Eq. (A12) still holds.
n ( j) = N
Further, Eqs. (A13) and (A14) can be modiﬁed as:
Combining Eqs. (A24) and (A25) with Eq. (2), we can prove that (A15) still holds when the
weights are non-uniform. Eq. (A16) can be shown to be true in a similar manner. Therefore,
Eq. (19) also holds in non-uniformly weighted case and Lemma 3 is true for the generalized
diversity measure.
Acknowledgments We thank the referees for their valuable comments.