Traﬃc Light Status Detection Using Movement Patterns of Vehicles
Joseph Campbell
A Thesis Presented in Partial Fulﬁllment
of the Requirements for the Degree
Master of Science
Approved June 2016 by the
Graduate Supervisory Committee:
Georgios Fainekos, Chair
Heni Ben Amor
Panagiotis Artemiadis
ARIZONA STATE UNIVERSITY
August 2016
Traditional methods for detecting the status of traﬃc lights used in autonomous
vehicles may be susceptible to errors, which is troublesome in a safety-critical environment. In the case of vision-based recognition methods, failures may arise due to
disturbances in the environment such as occluded views or poor lighting conditions.
Some methods also depend on high-precision meta-data which is not always available. This thesis proposes a complementary detection approach based on an entirely
new source of information: the movement patterns of other nearby vehicles. This
approach is robust to traditional sources of error, and may serve as a viable supplemental detection method. Several diﬀerent classiﬁcation models are presented for
inferring traﬃc light status based on these patterns. Their performance is evaluated
over real-world and simulation data sets, resulting in up to 97% accuracy in each set.
ACKNOWLEDGEMENTS
I would like to thank my advisor, Dr. Georgios Fainekos, for the incredible research
opportunities and mentorship he has provided over the past several years. I would
like to thank Dr. Heni Ben Amor, Dr. Panagiotis Artemiadis, Dr. Theodore Pavlic,
Dr. Umit Ogras, and Dr. Yu Zhang for the invaluable advice and guidance they
have given me. I would like to thank Dr. Marcelo Ang for the amazing research
experience at the National University of Singapore that made this work possible, as
well as everyone at SMART who graciously provided assistance and made me feel
so welcome, in particular: Dr. James Fu, Dr. Baoxing Qin, and Scott Pendleton.
I would to thank my co-authors and labmates at the Cyber Physical Systems Lab
for the wonderful discussions and collaborations: Kangjin Kim, Bardh Hoxha, Erkan
Tuncali, Ujjwal Gupta, Adel Dokhanchi, Rahul Srinivasa, Shakiba Yaghoubi, Wei
Wei, and Dylan Lusi.
This material is based upon work supported by the National Science Foundation
under EAPSI Fellowship Grant No. 1515589 and CPS 1446730. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the
author(s) and do not necessarily reﬂect the views of the National Science Foundation.
TABLE OF CONTENTS
LIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
LIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Summary of Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
RELATED WORK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
PROBLEM FORMULATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
METHODOLOGY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Nearest Neighbor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
Artiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Single-layer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Multilayer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Bidirectional Long Short Term Memory Networks . . . . . . . . . . . . . . . . . 21
EXPERIMENTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Real Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Simulated Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Classiﬁer Architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
LIST OF TABLES
SUMO Parameters for Vehicle Behaviors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Mean Test Accuracy for 1-Nearest Neighbor, Feedforward Neural Network, and Bidirectional Long Short Term Memory Classiﬁers. . . . . . . . . . 28
Mean Test Accuracy for Noisy Data Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Bidirectional Long Short Term Memory Test Accuracy With At Least
α Observed Vehicles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
LIST OF FIGURES
Example of Traﬃc Light Occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Example of Intersection Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Nearest Neighbors of a Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
Single-layer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
Linearly Separable and Inseparable Data Sets . . . . . . . . . . . . . . . . . . . . . . . . 16
Multilayer Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Autonomous Vehicle Used in Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Example of a SUMO Road Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Distribution of Test Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Bidirectional Long Short Term Memory Test Accuracy Per Number of
Observed Vehicles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Bidirectional Long Short Term Memory Test Accuracy Per Mean Observation Length . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
Misclassiﬁcation Due To Pedestrians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
INTRODUCTION
Autonomous vehicles are a promising technology, potentially yielding many societal beneﬁts such as fewer traﬃc-related fatalities, reduced pollution and energy
consumption, and greater mobility to those incapable of operating a standard automobile. However, these are complex systems that must be capable of interacting with
human-operated automobiles, pedestrians, and infrastructure, not to mention other
intelligent systems. Unsurprisingly, there are signiﬁcant challenges that must be overcome before autonomous vehicles can be considered safe for widespread introduction
into present-day road networks.
One such challenge is the ability to safely navigate complex environments such
as intersections while maintaining compliance with local traﬃc regulations. Vehicles and pedestrians with varying directions of travel cross paths while being guided
by traﬃc lights that are optimized for identiﬁcation by human drivers. This issue
has been partially addressed with the introduction of intelligent traﬃc light systems
which actively communicate their signal to nearby vehicles through Vehicular Ad Hoc
Networks . Nonetheless, as observed by , this remains an open problem as
such systems have thus far been limited to small-scale academic experiments and a
timely integration into current road networks seems unlikely.
It is for this reason that recent work has focused on the real-time identiﬁcation of
traﬃc light signals via vision-based systems . This approach can work well but is
subject to errors that can lead to the misidentiﬁcation of traﬃc light signals. These
errors can arise due to poor lighting conditions which interfere with the camera sensor
or an obstructed view resulting from a dirty lens or another vehicle, as demonstrated
Figure 1.1: Example of traﬃc light occlusion by other vehicles. The light is obscured
in the top image but becomes visible in the bottom image as the traﬃc starts moving.
in Fig. 1.1. This can potentially lead to disastrous results – an autonomous vehicle
erroneously passing through an intersection could ﬁnd itself in a situation in which it
is unable to avoid a collision.
Contributions
This thesis proposes a complementary traﬃc light identiﬁcation system that uses
an alternative source of information: the behavior of other nearby vehicles based on
positional data. Conceptually, the system infers the status of a traﬃc light from
the movements of other vehicles around or in the corresponding intersection. The
advantage of such a system is not that it has fewer failure-inducing cases than a
vision-based system, but rather that they are diﬀerent failure cases. Ideally, this
system will be paired with a vision-based one such that they complement each other
and reduce the total points of failure.
Consider the following scenario: a traﬃc light is non-functional due to an extenuating circumstance. As is typical on US roads, a law enforcement oﬃcer is directing
traﬃc through the intersection. A typical vision-based recognition system is of no
help in this scenario, however, by observing when other vehicles start to pass through
the intersection and from which direction, the proposed system can infer which traﬃc
the oﬃcer is allowing to pass through the intersection.
Similarly, consider a situation in which a vehicle stops at a red light behind a
larger vehicle that is occluding the traﬃc light. Suppose the preceding vehicle suﬀers
a mechanical failure and is blocking traﬃc; the traﬃc light cycles through its phases
and surrounding traﬃc bypasses the oﬀending vehicle in other lanes. Once again, a
vision-based system would not be of assistance in this scenario, however, the proposed
system may indicate that the traﬃc light is green and therefore alternative action
should be taken.
The contributions of this thesis are as follows: we formally deﬁne the problem,
present a system for predicting the state of a traﬃc light based on the spatial movement of nearby vehicles, and evaluate its eﬀectiveness in simulated and real-world
conditions.
Summary of Publications
A subset of the most salient results from this thesis has been submitted in the
following publication and is currently under review.
• J. Campbell, H.B. Amor, M.H. Ang Jr.
and G. Fainekos, Traﬃc Light
Status Detection Using Movement Patterns of Vehicles under review in
2016 IEEE 19th International Conference on Intelligent Transportation Systems
(ITSC), 2016 
The following publications are not necessarily directly related to this thesis topic,
however they represent a signiﬁcant amount of time and eﬀort. Therefore, this section
will brieﬂy describe the publications that the author has contributed to during the
course of this degree.
• K. Kim, J. Campbell, W. Duong, Y. Zhang and G. Fainekos, DisCoF+:
Asynchronous DisCoF with Flexible Decoupling for Cooperative Pathﬁnding in 2015 IEEE International Conference on Automation Science and Engineering (CASE), 2015 
This work expands on the previously introduced distributed cooperative multirobot path planning algorithm DisCoF . In DisCoF, robots initially plan
independently and dynamically couple together into groups when there is risk
of a path conﬂict, which occurs when more than one robot attempts to occupy
the same position at the same time. When coupled, robots plan together so as
to avoid these conﬂicts, yet by coupling only when necessary the search space
remains small. The extension, DisCoF+, adds the notion of decoupling in which
robots are no longer grouped together when the risk of a path conﬂict passes.
Additionally, the prior requirement that robots must operate in synchronized
time steps has been removed, allowing asynchronous planning. My contribution
to this work was an experimental simulation in which multiple iRobot Creates
employed the DisCoF+ algorithm to cooperatively plan in an obstacle-ﬁlled
environment in the Webots simulator.
• U. Gupta, J. Campbell, U.Y. Ogras, R. Ayoub, M. Kishinevsky, F. Paterna and S. Gumussoy, Adaptive Performance Prediction for Integrated
GPUs to appear in 2016 IEEE/ACM International Conference on Computer
Aided Design (ICCAD), 2016 
In this paper we introduce an adaptive prediction model based on Recursive
Least Squares which is capable of accurately predicting GPU performance in
an eﬃcient manner at run-time. This is particularly relevant for power management in embedded devices such as smart phones, as an accurate performance
model enables Dynamic Voltage and Frequency Scaling (DVFS) algorithms. My
contribution was to assist in the development of the prediction model, perform
experiments on a physical platform, and analyze the results.
• J. Campbell, C.E. Tuncali, P. Liu, T.P. Pavlic, U. Ozguner and G. Fainekos,
Modeling Concurrency and Reconﬁguration in Vehicular Systems: A
π-Calculus Approach to appear in 2016 IEEE International Conference on
Automation Science and Engineering (CASE), 2016 
J. Campbell, C.E. Tuncali, T.P. Pavlic and G. Fainekos, Toward Modeling Concurrency and Reconﬁguration in Vehicular Systems in 9th Interaction and Concurrency Experience, Satellite Workshop of DisCoTec 2016,
This work introduces a hierarchical modeling framework for cooperation among
autonomous vehicles. Supervisory communication and control is handled by a
high-level layer via π-calculus expressions, and low-level dynamics and continuous control are deﬁned by hybrid automata. My contribution was the initial
development of the framework, deﬁnition of the π-calculus expressions and hybrid automata for a vehicle platoon case study, and experimental analysis of
the framework.
RELATED WORK
Vision-based traﬃc light detection systems have been widely analyzed in previous
works. The majority of these works have focused purely on image recognition . Of particular interest, however, are those that seek to minimize the risk posed by
errors inherent to vision-based detection systems. In , the authors propose using a
detailed map of traﬃc lights to act as prior knowledge so that the detection system
knows when it should be able to see traﬃc lights. If traﬃc lights are not detected
at an expected position, the autonomous vehicle can take preventative action such
as slowing down under the assumption that the light is red or yellow. If the light
is actually red or yellow, then this course of action is the correct one. If the light
is actually green then the vehicle slowing is an annoyance at best, and results in a
collision with human-operated vehicles due to unpredictability at worst. If the system
has a poor detection rate this could start to degrade the ﬂow of traﬃc along a road
as vehicles slow down for a green light. In addition, this approach requires detailed
prior map knowledge of the traﬃc lights.
Similarly, in the authors acknowledge the diﬃculties in building a purely
vision-based traﬃc light detection system and so augment theirs with prior map
knowledge as well as temporal information. While yielding good results, the system
still fails to identify traﬃc light signals in certain cases. Indeed, the authors indicate
that a possible approach for improvement would be to introduce 3-dimensional LIDAR
data into the mix in order to improve recognition of the traﬃc lights themselves.
In , the authors use the movement patterns of pedestrians to apply semantic
labels to the environment. They infer the location of pedestrian crossings, sidewalks,
and building entrances and exits based on the activity patterns of pedestrians. This
is similar in spirit, if not in execution, to the labeling of traﬃc lights based on vehicle
movement patterns introduced in this thesis.
PROBLEM FORMULATION
We can formalize the problem from a probabilistic perspective as follows: let Z
be a discrete random variable which represents the state of a traﬃc light with respect
to a target vehicle. The speciﬁc value of Z is denoted by z, and in this work can take
the value of either green or red. The goal is to then determine the probability that
a traﬃc light is either green or red with respect to our target vehicle at a speciﬁc
point in time t: p(Zt = zt). To simplify the notation, from this point on we will refer
to this probability as simply p(zt).
Clearly we cannot determine an accurate probability for p(zt) without additional
Figure 3.1: A scenario in which vehicle A’s current position is ambiguous, as there
are multiple paths it could have taken which could be used to infer diﬀerent traﬃc
light states.
information. Therefore, we would like to consider observations of nearby vehicles
when determining this probability. The simplest approach is to consider the spatial
position of every nearby vehicle independently at each point in time. We deﬁne the
state g of vehicle n at time t as a vector:
xn,t, yn,t
If we place the target vehicle at the origin of a Cartesian coordinate plane with
the positive x-axis extending towards the front of the vehicle and the positive y-axis
extending towards the left-hand side of the vehicle, then x is the distance along the
x-axis from the target vehicle to the observed vehicle n. Similarly, y is the distance
along the y-axis to vehicle n. This yields a conditional probability of the following
form, where N is the total number of observable vehicles at time t.
p(zt|g1,t, g2,t, ..., gN,t) = p(zt|g1:N,t)
However, this approach has a signiﬁcant drawback which is visualized in Fig. 3.1.
If vehicle A is an observable vehicle at time t, it may have taken several diﬀerent
paths to arrive at this position: path 1, 2, or 3. Each of these paths could result in
a diﬀerent traﬃc light state zt. For example, if vehicle B is our target vehicle and
we are observing A, zt could hold the value of green if vehicle A followed path 2, red
for path 1, and either green or red for path 3 (depending on local traﬃc regulations
for right-on-red turns). This leads to an ambiguous situation, in which the state of
vehicle A at this point in time does not necessarily help us determine zt.
We can alleviate this problem if we consider a temporal trace of the position. We
could alter the vehicle state to include information on the position over time in the
form of velocity.
xn,t, yn,t, ˙xn,t, ˙yn,t
This is susceptible to the same ambiguity problem, however. In the example from
Fig. 3.1, if path 2 resulted from A accelerating through a light which recently turned
green, then the velocity at time t could be roughly the same for all paths. The same
holds true when acceleration is considered.
xn,t, yn,t, ˙xn,t, ˙yn,t, ¨xn,t, ¨yn,t
A more eﬀective approach is to consider the state of vehicle n not just for a single
time step t, but rather over a time window, i.e., t −1, t −2, and so on. If we consider
a window size of T time steps in the past, then we can represent the state of vehicle
n as a time series s at time t.
sn,t = gn,t, gn,t−1, ..., gn,t−Tn
p(zt|s1,t, s2,t, ..., sN,t) = p(zt|s1:N,t)
If observations are ideal, then the entire path for vehicle A is now taken into
account and there is no more ambiguity. In practice, this may not be the case and
the eﬀective window size Tn may vary from vehicle to vehicle. For example, A may
only enter the sensor range of our target vehicle once it reaches the position depicted
in Fig. 3.1. We now provide a formal problem statement and deﬁne our assumptions.
Problem: Given a set of observations L of nearby vehicles, determine p(zt|L).
1. L is either a set of independent vehicle states g, or a set of independent series
of states s.
2. Each series s may contain a variable number of states, however, they must
correspond to sequential time points.
3. At least one vehicle must be observed for at least one time step.
In practice, Assumption 2 is not strong as this is implicitly satisﬁed by a Bayesian
tracking algorithm in this work.
METHODOLOGY
The problem we have deﬁned is in general known as a classiﬁcation problem: given
an input, classify it by applying a discrete label. In this case, the input to our problem
is a set of vehicle observations L and we wish to label it with a discrete value zt, either
green or red. This work tackles the classiﬁcation problem via a method known as
supervised learning . In supervised learning, an existing data set of labeled inputs
is used to infer the label of other inputs. For example, we could collect the labeled
data set – known as a training set – by observing nearby vehicles at an intersection
and applying labels by directly observing the traﬃc light. We can then use this data
set to infer the labels for future observations where the traﬃc light is not observable.
The rest of this chapter will provide an introduction to several classiﬁcation methods
and how they are employed in this work. For simplicity, the examples will assume
the input data is the most basic vehicle state as deﬁned in Eq. 3.1.
Nearest Neighbor
So how can the training data be used to infer labels for other observations? The
simplest method for doing so is to directly compare the new observation to the training data and use the most populous label among the closest training inputs , as
measured by a distance metric. Figure 4.1 shows the three closest neighbors to the
new observation A as determined by euclidean distance.
Once the neighbors are
identiﬁed, it is a simple matter of ”majority rule”; the label that is used by the most
neighbors is applied to the new point. In Fig. 4.1, two of the three closest neighbors
are green, therefore we say that the new point’s label is also green. This method is
Figure 4.1: Left: We would like to classify the new observation point, A. Right: the
three nearest neighbors to A are circled. Since the majority of neighbors are labeled
green, then so too is A.
known as k-Nearest Neighbors (k-NN), where k is the number of neighbors that is
used – typically an odd number so that a majority is guaranteed. One way to view
this method is that it implicitly divides the input space up into discrete regions, with
each region corresponding to a speciﬁc label . New points are given the label that
corresponds to the region in which they lie. The borders that separate regions are
known as decision boundaries.
The disadvantages of this method are two-fold: k-NN is not probabilistic, meaning
that it infers zt directly given L and not p(zt|L); and k-NN is a special type of nonparametric model that must store and operate over all training data instances .
The latter point is especially problematic, as we would like a model that can operate
in real-time on an autonomous vehicle. This places an upper limit on how many
training data instances can be stored and may adversely aﬀect the ability of k-NN to
generalize.
Artiﬁcial Neural Networks
Therefore, we turn to parametric models whose computational and storage requirements for classiﬁcation do not scale with the number of training data instances.
Artiﬁcial neural networks are parametric mathematical models capable of accurately
approximating any continuous function . More speciﬁcally, it has been previously
shown that ANNs can accurately approximate a Bayesian posterior , depending
on the network complexity and cost function. This indicates that unlike k-NN, ANNs
can eﬃciently approximate p(zt|L), making them an ideal classiﬁcation method for
this problem.
Single-layer Perceptron
Single-layer perceptrons are the simplest type of artiﬁcial neural network .
The general idea is that we want to construct a function – known as a discriminant
– which accepts an input and produces a classiﬁcation label. If there are only two
labels, the most basic discriminant is simply a linear combination of the inputs .
on,t(gn,t) = w0xn,t + w1yn,t = wTgn,t
If on,t > 0, then the input is given one label; for on,t < 0, the other label. The
coeﬃcients w0 and w1 are referred to as weights and are considered parameters of the
model. These coeﬃcients are determined during a training phase which utilizes the
training data set. This model is easily extended to an arbitrary number of labels
oz,n,t(gn,t) = wT
where the label is determined by the largest value of o
max oz,n,t
ogreen,n,t(gn,t)
ored,n,t(gn,t)
Figure 4.2: Single-layer Perceptron.
This model can be viewed in terms of layered network of nodes as shown in Fig. 4.2.
For a single-layer perceptron, there is only one layer which performs computations:
the output layer. This is also referred to as a feedforward neural network (FFNN)
since the network connections are acyclic and directed from input to output.
This model can be further generalized by passing the linear combination of inputs
to a nonlinear activation function f.
oz,n,t(gn,t) = f(wT
It has been shown that for a logistic sigmoid or softmax activation function
the outputs accurately approximate a posterior probability. Thus, the single-layer
perceptron yields the desired probability
p(zt|gn,t) = f(wT
However, owing to its simplicity, the single-layer perceptron is limited in its classi-
ﬁcation ability. Though a nonlinear activation function is used, it is monotonic .
The result is that the single-layer perceptron is a linear discriminant and can only
Figure 4.3: Left: a linearly separable data set. Right: a linearly inseparable data set.
generate linear decision boundaries. This makes it relatively ineﬀective unless the
training data is linearly separable, i.e., points with diﬀerent labels can be separated
by a straight line as in Fig. 4.3.
Multilayer Perceptron
A multilayer perceptron is simply a single-layer perceptron with one or more
additional layers between the input and output. By taking linear combinations of the
inputs more than once in succession, multilayer perceptrons are capable of producing
nonlinear decision boundaries.
In fact, with just one additional layer (two layers
total) a multilayer perceptron can approximate any continuous function . These
additional layers are referred to as hidden layers, and result in a network such as in
Fig. 4.4. Following the notation in , the linear combination of inputs for any given
node of a multilayer perceptron can be generalized as
p(Zt = green|gn,t)
p(Zt = red|gn,t)
Figure 4.4: Multilayer Perceptron.
which is transformed by a nonlinear activation function f to yield the output value
where ok is the output value of node k with j connecting nodes from the previous
layer. By applying this equation recursively, it is possible to ﬁnd the output values
of the network.
It was previously mentioned that the weights of a network are determined during
a training phase. More speciﬁcally, we use an algorithm that feeds the network inputs
from the training data and compares the network’s output to the actual output. The
network weights are then iteratively adjusted in order to minimize the output error.
This is commonly accomplished with an algorithm known as backpropagation .
Backpropagation consists of two steps: propagate the training inputs forward
through the network starting at the input layer to compute the output of every node,
then propagate the error backwards through the network starting at the output layer
and use it to update the weights of each node. The ﬁrst step simply requires applying
Eq. 4.6 to each node for every input in the training data. For the second step we must
ﬁrst determine a suitable measure of error for the network. With a softmax activation
function used in the output layer of the network, the error can be expressed as the
cross entropy of K network outputs and the expected outputs t from a single training
instance .
We are interested in the partial derivative of this error with respect to the network
weights so that we can evaluate how the weights contribute to the error.
The node error δ is what we will use to modify the weights of the network. For
nodes in the output layer, this is simply the diﬀerence in the network outputs and
the expected output.
δk = ok −tk
In the hidden layers, it is expressed as the diﬀerentiated nonlinear activation function
multiplied by the weighted errors in the subsequent nodes (remember that we are
propagating backward).
δj = f ′(aj)
Once δ has been found for each node in the network, it is used to calculate the change
in weights.
∆wk,j = αδkoj
p(Zt = green|gn,t)
p(Zt = red|gn,t)
Figure 4.5: Recurrent Neural Network
Recurrent Neural Networks
The examples until this point have used the independent vehicle states g deﬁned
in Eq. 3.1 as inputs. So how can we classify with the time series of states s deﬁned in
Eq. 3.5? In a feedforward neural network such as a multilayer perceptron, the nodes
are not allowed to form cycles; inputs propagate through the network layers from the
input layer to the output layer. This is suitable for classiﬁcation when separate input
vectors are treated as independent, however, it is not ideal when we would like to
consider some inputs as dependent and use multiple inputs to derive a single output.
This is the case when approximating the posterior probability in Eq. (3.6), as it is
conditional on a time series of vehicle states sn,t as deﬁned in Eq. (3.5). This can
be accomplished with recurrent neural networks (RNNs) , which are a variant
of feedforward networks that are allowed to form cyclical connections among hidden
layer nodes as shown in Fig. 4.5. Simply speaking, this allows an RNN to produce an
output from a sequence of prior inputs, e.g., the time series in Eq. (3.5), as opposed
to a single input.
The self-connections in the hidden layer nodes are associated with a time delay
and essentially act as a sort of memory to retain the hidden layer output values for
the previous input. Suppose we have the ﬁrst input of a time series at t −Tn. Each
hidden layer node will receive the input values along with its previous output value
from the self-connection. However, since this is the ﬁrst input there is no previous
output value and so it receives the initial state instead. The ﬁnal network output at
this point is typically discarded, since we are only interested in the output once all
inputs in the series have been processed. For the next input at t −Tn + 1, the hidden
layer nodes receive the new inputs as well as the output values they calculated for
t −Tn. This process continues until all inputs have been processed, at which point
the network output is used.
Recurrent neural networks are still trained with backpropagation, however, an
extra step is necessary due to the cyclical connections. A copy of the network
is made for each input in the time series and the cyclical connections are replaced
with a connection to this copy, essentially forming a multilayer perceptron with many
layers. This is known as unfolding and allows backpropagation to function just as it
would without the cyclical connections.
We use RNNs to estimate p(zt|sn,t) by generating a single output zt from a sequence of feature vectors gn,t, which together form the time series sn,t of state vectors
for vehicle n. This is known as sequence classiﬁcation . However, this is not
the same as the posterior probability deﬁned in Eq. (3.6) which is conditional on all
observed vehicles, not just one. The feature vector to our network must be a constant
size. This rules out simply concatenating the feature vectors of all observed vehicles,
since the number of observed vehicles may vary at any given time. Instead, we can
take the mean probability of p(zt|sn,t) for all observed vehicles at time t and use that
as an approximation.
ˆp(zt|s1:N,t) ≈
n=1 p(zt|sn,t)
Bidirectional Long Short Term Memory Networks
A variant of the RNN known as the Bidirectional Long Short Term Memory
(BLSTM) network was shown to be exceptionally well-suited for sequence classiﬁcation . Standard RNNs suﬀer from a problem known as the vanishing gradient ,
in which the hidden layer node weights for previous inputs converge to zero over time,
thus preventing an RNN from eﬀectively learning from inputs that span a long time
period. The Long Short Term Memory (LSTM) network was designed to mitigate
this problem by introducing the concept of LSTM nodes that are more eﬀective at
retaining previous values. Long Short Term Memory nodes are themselves a composite of connected nodes, and they replace (some) standard nodes in a recurrent neural
The bidirectional aspect of a BLSTM network is a concept taken from Bidirectional Recurrent Neural Networks (BRNNs) . It is sometimes practical to take
future inputs into account when making predictions with recurrent neural networks.
This can be achieved by delaying the output when training a network; for example,
using a delay of M with the training input/output pair s1:N,t/zt−M. However, it was
found that if this delay is too large then it adversely aﬀects the network prediction accuracy, thereby limiting how far into the future a network can account. An
alternative approach is to train an RNN with a reversed time series input, e.g., sN:1.
This led to the Bidirectional Recurrent Neural Network in which two RNNs – one
processing the input series forward in time and one backward in time – are connected
to the same output layer. This architecture yields greater prediction accuracy as it
predicts based on past inputs as well as future inputs.
Several existing classiﬁcation methods have been discussed in this chapter. Of
these, the Nearest Neighbor, Feedforward Neural Network (Multilayer Perceptron),
and Bidirectional Long Short Term Memory Networks have been identiﬁed as the
most promising for ﬁnding the posterior probability established in Chapter 3. These
methods will be empirically evaluated and analyzed in Chapter 5.
EXPERIMENTS
Experimental Setup
In order to evaluate how well these methods can approximate the posterior probabilities, we collected two sets of data with which to perform experiments. The ﬁrst
set was generated from real-world sensor data collected by an autonomous vehicle in
Singapore. The second set was generated by the SUMO traﬃc simulator . The
methodology behind this data collection is discussed in this chapter, along with the
architecture of all classiﬁers used in experiments.
Real Data Collection
As this work is targeting autonomous vehicle applications, it is a priority to test
against real-world data collected by an autonomous vehicle. While synthetic data sets
are suitable for a proof of concept, there is an unknown factor in regards to whether a
system will work as designed in a real environment, especially so with a safety-critical
system. Towards that end, we collected data with the Shared Computer Operated
Transport autonomous vehicle platform at the Singapore-MIT Alliance for Research
and Technology’s (SMART) Autonomous Vehicles lab.
Data collection was performed in Singapore, at 56 intersections within the vicinity of the National University of Singapore campus. Spatial point cloud data was
collected with a SICK LMS 151 LIDAR sensor operating at 50Hz. As there is no
ground truth available with which to form the vehicle time series, the data was fed
through a two-stage vehicle tracking algorithm.
Figure 5.1: The autonomous vehicle that performed data collection.
The ﬁrst stage decomposes the point cloud data into a subset of clusters, in
which each cluster consists of a collection of points in close proximity to each other.
The clusters are then tracked over several measurement frames to yield an average
spatial position and a velocity vector for a given point in time.
The second stage treats these independent measurements as observations to a
particle ﬁlter-based multi-target tracking algorithm . In this algorithm, particle
ﬁlters are used to model distinct vehicle tracks. Each new observation is associated
with the particle ﬁlter that has the highest likelihood of producing that observation.
Vehicle time series are then derived from the particle ﬁlters and down-sampled to
10Hz. Supervised labels were manually generated from camera inputs collected simultaneously with the LIDAR data. This process yielded a data set consisting of
1011 unique time series.
Figure 5.2: The SUMO road network for an intersection in New York City.
Simulated Data Collection
Despite the inherent value of real-world data, there is a limit to how much can
be feasibly collected. Additionally, due to practical constraints, we could only collect
data from nearby intersections which limits how well we can generalize. Therefore,
we turned to synthetic data generated with the SUMO traﬃc simulator . Road
networks for 13 intersections were generated from OpenStreetMap data: 3 in Tempe,
Arizona, 2 in New York City, New York, and 8 in Singapore. An example of an
intersection in New York is shown in Fig. 5.2.
Simulations were run in which traﬃc passed through the intersections from each
direction and either traveled straight, turned left, or turned right. The vehicles were
uniformly distributed to one of three behavior models: aggressive, average, and submissive. These behaviors were manually deﬁned and adjusted user-facing parameters.
Table 5.1 lists the parameters for all behaviors.
SUMO is capable of writing ﬂoating car data (FCD) output, which contains the
position, velocity, and heading of every vehicle at each sampling interval for the
duration of the simulation. To correspond with the real data set, the sampling interval
was ﬁxed to 10Hz. This data was then transformed with respect to a chosen target
vehicle, and used to generate state vectors for each other vehicle within a 50m sensor
Aggressive
Submissive
Acceleration
Deceleration
Speed Factor
Speed Deviation
Minimum Gap
Maximum Speed
Impatience
Table 5.1: SUMO parameters for vehicle behaviors. The current speed is obtained by
multiplying the road’s speed limit by a sample from a normal distribution centered
at Speed Factor with a standard deviation of Speed Deviation.
Minimum gap is
the following distance between a vehicle and its leader. Sigma is the variability in a
vehicle’s behavior. Tau is the following time between a vehicle and its leader, e.g. 3s.
Impatience is the vehicle’s disposition to forcibly changing lanes in front of others.
range of the target. Since the FCD data includes a vehicle identiﬁer, these states
can then be assembled into a time series for each vehicle. These time series’ were
segmented in order to coincide with the states of the intersection’s traﬃc light and
labeled as either green or red. This process yielded a data set consisting of 2311
unique time series.
Classiﬁer Architecture
The BLSTM network used in these experiments is composed of an input layer
followed by two parallel LSTM layers with 32 nodes each; one layer processes the
input sequence forward and one layer backwards. The output from the LSTM layers
is concatenated into a dropout layer with a 0.5 drop rate. The FFNN is a standard
multilayer feedforward network with 3 hidden layers and 256 nodes per layer. In
both networks, the size of the input layer is dependent on the vehicle state, while the
output layer always consists of two nodes in order to produce a one-hot encoding of
zt. The networks are trained using RMSProp backpropagation with categorical crossentropy loss and a softmax activation function. The K-Nearest Neighbor algorithm
was evaluated for K = 1.
Results and Discussion
The ﬁrst experiment of interest is to evaluate the relative performance of each
classiﬁer on our data sets.
The classiﬁcation accuracy is evaluated for the posterior probabilities produced by both the FFNN and BLSTM classiﬁers, with a
train/validation/test set split of 60%/20%/20%, as well as the 1-NN classiﬁer with a
80%/20% train/test split. This experiment reveals that despite being the simplest,
the 1-NN classiﬁer performs signiﬁcantly better than all other classiﬁers on the Real
data set with a 97% classiﬁcation rate. This is an unexpected result, and interested
in whether the noise reduction caused by the Bayesian tracking had a signiﬁcant impact on the 1-NN performance, we created a Real (No Track) data set with only the
raw measurements obtained by the clustering algorithm. However, despite slightly
reduced performance, the 1-NN classiﬁer is still the best performer on this data set.
Results for BLSTM and feature sets containing acceleration are not included for this
Feature Set
(No Track)
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
Table 5.2: The mean test accuracy for 1-Nearest Neighbor, Feedforward Neural Network, and Bidirectional Long Short Term Memory classiﬁers. The best classiﬁer for
each data set is highlighted in green.
data set as they require the time series information provided by the tracking algorithm.
Furthermore, 1-NN has the highest classiﬁcation accuracy on the Simulation data
set. This seems to indicate that the Sim data set is a good approximation of the
real data set since it yields similar results, but on further analysis the test sample
distribution between the two data sets is strikingly diﬀerent. This can be observed in
Fig. 5.3. The Real data set is heavily skewed, with a large portion of the test samples
coming from intersections where only a small number of vehicles were observed for a
short period of time. Meanwhile, the Sim data has a much ﬂatter distribution over a
wider domain.
Num observed vehicles
Num samples
Mean observation length (s)
Num samples
Simulation
Figure 5.3: The distribution of test samples in each data set according to the number of observed vehicles per time step, and the mean observation length among all
observed vehicles per time step.
In order to determine whether this distribution has a prominent eﬀect on classiﬁcation accuracy, we ran an optimization algorithm to minimize the Kullback-Leibler
divergence between the distributions of the two data sets. This was accomplished by
truncating a random portion of the time series by a variable factor. The initial KL
divergence between the Real and Sim data sets is 1.35, however, after several rounds
of this optimization routine that was reduced to 0.09. The resulting data set is referred to as Sim-Real, and it can be seen in Fig. 5.3 that the associated test sample
distribution is similar to that of the Real data set. As in the other data sets, the 1-NN
classiﬁer is again the best performer on the Sim-Real data and indicates robustness
to changes in the test sample distribution. Additionally, since the simulation data
yields similar results to the real-world data and is capable of closely approximating
the real-world observation distribution, we consider it an accurate representation of
the real-world data.
The only time we observed the 1-NN classiﬁer perform poorly is on data sets with
a considerable amount of noise. Gaussian noise with a standard deviation of 2.0 was
Feature Set
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
Table 5.3: The mean test accuracy for 1-Nearest Neighbor, Feedforward Neural Network, and Bidirectional Long Short Term Memory classiﬁers. The best classiﬁer for
each noisy data set is highlighted in green, while the classiﬁers that are not signiﬁcantly diﬀerent are highlighted in yellow.
applied to all values in the Sim and Sim-Real data sets, resulting in Noisy variations.
The results in Table 5.3 show that 1-NN yielded a considerably worse classiﬁcation
accuracy in this scenario, while BLSTM was largely unaﬀected by the additional noise
and achieved the best accuracy with 87% and 74% on the Sim and Sim-Real data
sets respectively.
The results in Tables 5.2 and 5.3 also allow us to examine the impact of the diﬀerent vehicle states deﬁned in Eqs. (3.1), (3.3), and (3.4) on the overall accuracy. The
ﬁrst observation we can make is that the addition of velocity information into the
feature set results in a statistically signiﬁcant (p-value < 0.05) increase in accuracy
Num observed vehicles
Test accuracy
Num observed vehicles
Test accuracy
Figure 5.4: The BLSTM (full feature set) test accuracy for real data (solid blue line)
and simulation data (solid green line) at each time step in relation to the number of
observed vehicles.
for every classiﬁer on every data set. This is a strong result, and in line with the
hypothesis that the introduction of velocity information will help alleviate the intersection ambiguity problem. However, it is interesting to note that the addition of
acceleration information does not always lead to a further increase in accuracy. The
noisy data sets, in particular, actually exhibit either a statistically signiﬁcant decrease
in accuracy or no change at all. This suggests that we can reduce the complexity of
our classiﬁers without penalizing accuracy on noisy data sets by leaving acceleration
out of the feature set.
The second experiment is designed to test how the BLSTM classiﬁer would perform in a realistic scenario. In real-world use, we do not have access to the full vehicle
time series in the data sets; we only have the vehicle observations that have occurred
until the current time step. The network is ﬁrst trained with the full vehicle time
series from all but one of the intersections. With the remaining time series, time is
treated as a discrete value and incremented in steps. At every time step, the network is used to estimate the mean temporal probability in Eq. (4.13) from the vehicle
Mean observation length (s)
Test accuracy
Mean observation length (s)
Test accuracy
Figure 5.5: The BLSTM (full feature set) test accuracy for real data (solid blue
line) and simulation data (solid green line) at each time step in relation to the mean
observation length among all observed vehicles.
time series that have had an observation within the past 3 seconds. Only the observations that have occurred before the current time step are considered. The mean
classiﬁcation accuracy for all time steps is shown in Table 5.4.
With further analysis, it is evident that a signiﬁcant number of misclassiﬁed time
steps occur when only one vehicle is observed. As more distinct vehicles are observed,
the classiﬁcation accuracy increases, which is an intuitive result. This is visualized in
Fig. 5.4. If we examine the distribution of test samples over the number of observed
vehicles, it is clear that a large portion of the samples occur when only one vehicle is
observable. Taking this into consideration, if the test accuracy is evaluated only for
time steps in which two or more vehicles are observed, then the accuracy increases
from 71% to 84% on the Real data set as shown in Table 5.4.
In contrast, the
simulation data has a ﬂatter distribution, and as a result the corresponding accuracy
does not see a proportional increase. With the positive correlation between accuracy
and the number of vehicles, we might also expect such a relationship between the
classiﬁcation accuracy and the length of time that vehicles are observed. The plots
Feature Set
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
x, y, ˙x, ˙y
x, y, ˙x, ˙y, ¨x, ¨y
Table 5.4: The BLSTM mean test accuracy for all time steps with at least α observed
in Fig. 5.5 show a positive correlation, indicating that this is true to some extent.
Furthermore, there is also a positive relationship between the accuracy and the
BLSTM classiﬁer’s prediction conﬁdence, which we deﬁne as the maximum probability among all values of zt. In other words, as the classiﬁer observes more vehicles
it grows more conﬁdent in the prediction and this results in a higher classiﬁcation
accuracy. However, there are instances in which this does not hold true. Speciﬁcally,
it can be seen that the accuracy is poor while the prediction conﬁdence is high for the
Real data set when the mean observation length is between 4s and 5s in Fig. 5.5. On
further analysis, this occurred when the target vehicle was stopped in front of pedestrians crossing a red light. A single vehicle was tracked for several seconds moving
directly in front of the target vehicle with an average speed of 2.83m/s. The most
likely scenario is that the pedestrians crossing the street were mistaken for a vehicle
turning left, which resulted in a high conﬁdence prediction of a green light when in
fact, the light was red.
Figure 5.6: Scenario in which pedestrians mistaken for a vehicle result in a misclassiﬁcation with high conﬁdence. The camera image is on the left, and the corresponding time series given by the particle ﬁlter is on the right.
CONCLUSION
This thesis has shown that it is possible to accurately infer the current state of a
traﬃc light by analyzing the spatial movements of nearby vehicles with respect to a
target vehicle. This method was evaluated on real-world data gathered in Singapore
and synthetic data generated from a traﬃc simulator. In both cases, encouraging
results were achieved with three diﬀerent classiﬁers: a feedforward neural network, a
bidirectional long short-term memory network, and a nearest neighbor classiﬁer. It
was found that in most tested scenarios, a nearest neighbor classiﬁer obtained the best
classiﬁcation results (at the cost of higher computational and storage requirements).
However, if the data is particularly noisy, better accuracy may be achieved with a
BLSTM classiﬁer.
Similar to a vision-based approach, the methodology presented here has failure
cases in which inference produces wrong results. The most obvious case is when no
vehicles are in observation range, however, it was also seen that in some scenarios
more than one vehicle may need to be in observation range in order to make an
accurate prediction. Likewise, there are speciﬁc instances in which the inference may
be wrong if other vehicles are only observed for an extremely brief period of time.
However, since the failure cases for our approach and a vision-based approach are
not the same, we envision that the best use of our system is to combine it with a
traditional vision-based method. Diﬀerent failure cases suggests that the systems will
complement each other, and result in a more robust detection system.
This work has also introduced the notion that synthetic data generated from a
traﬃc simulator can be manipulated such that the KL divergence with respect to
another data set is minimized. The result is that the distribution of observations
for the synthetic data closely aligns with that of another data set. We have used
this approach to approximate a real-world data set, and conjecture that this can be
extended to generate arbitrary distributions in order to test our method under varying
traﬃc conditions. This also raises the interesting question of whether it is possible
to train our classiﬁer on a combination of real-world and synthetic data, and then
employ this classiﬁer in other real-world scenarios. We will examine these possibilities
in future work.