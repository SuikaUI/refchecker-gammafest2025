BIROn - Birkbeck Institutional Research Online
Adam, S.P. and Karras, D.A. and Magoulas, George D. and Vrahatis, M.N.
 Solving the linear interval tolerance problem for weight initialization of
neural networks. Neural Networks 54 , pp. 17-37. ISSN 0893-6080.
Downloaded from: 
Usage Guidelines:
Please refer to usage guidelines at or alternatively
contact .
Solving the linear interval tolerance problem for weight
initialization of neural networks
S.P. Adama,b,∗, D.A. Karrasc, G.D. Magoulasd, M.N. Vrahatisa
aComputational Intelligence Laboratory, Department of Mathematics, University of Patras, GR-26110
Patras, Greece
bDepartment of Computer Engineering, Technological Educational Institute of Epirus, 47100 Arta, Greece
cDepartment of Automation, Technological Educational Institute of Sterea Hellas, 34400 Psahna, Evia,
dDepartment of Computer Science and Information Systems, Birkbeck College, University of London, Malet
Street, London WC1E 7HX, UK
Determining good initial conditions for an algorithm used to train a neural network is
considered a parameter estimation problem dealing with uncertainty about the initial
weights. Interval Analysis approaches model uncertainty in parameter estimation problems using intervals and formulating tolerance problems. Solving a tolerance problem
is deﬁning lower and upper bounds of the intervals so that the system functionality is
guaranteed within predeﬁned limits. The aim of this paper is to show how the problem
of determining the initial weight intervals of a neural network can be deﬁned in terms
of solving a linear interval tolerance problem. The proposed Linear Interval Tolerance
Approach copes with uncertainty about the initial weights without any previous knowledge or speciﬁc assumptions on the input data as required by approaches such as fuzzy
sets or rough sets. The proposed method is tested on a number of well known benchmarks for neural networks trained with the back-propagation family of algorithms. Its
eﬃciency is evaluated with regards to standard performance measures and the results
obtained are compared against results of a number of well known and established initialization methods. These results provide credible evidence that the proposed method
outperforms classical weight initialization methods.
∗Corresponding author
Email address: (S.P. Adam)
 
March 14, 2014
Neural networks, Weight initialization, Interval analysis, Linear interval tolerance
1. Introduction
The purpose of Interval Analysis (IA) is to set upper and lower bounds on the eﬀect
produced on some computed quantity by diﬀerent types of mathematical computing
errors (rounding, approximation, uncertainty etc) . Intervals are used to model uncertainty in parameter estimation problems such
as the noise associated with measured data. Such problems arise in engineering design
or mathematical modeling where tolerances in the relevant parameters need to be de-
ﬁned in terms of upper and lower bounds so that the desired functionality is guaranteed
within these bounds. The interval-based algorithms are used to reliably approximate
the set of consistent values of parameters by inner and outer intervals and thus take into
account all possible options in numerical constraint satisfaction problems.
The promising features of IA motivated researchers from diﬀerent disciplines to
invest in the study and implementation of IA methods whenever reliable numerical
computations are required. Currently, this research ﬁeld is rapidly growing due to the
increasing computation power of modern hardware. Examples of applications range
from ﬁnite element analysis and data analysis , to stock market forecasting , reliability of mechanical design
 , and many more. Research in the area of neural networks
has also beneﬁtted from IA and a number of eﬀorts utilizing concepts and methods
from IA are reported in the literature. Examples are those by de Weerdt et al. on
the use of IA for optimizing the neural network output, Ishibuchi & Nii on the
generalization ability of neural networks, Xu et al. on robust stability criteria
for interval neural networks, Li et al. regarding training of neural networks, and
An important problem encountered when training a neural network is to determine
appropriate initial values for the connection weights. Eﬀective weight initialization
is associated to performance characteristics such as the time needed to successfully
train the network and the generalization ability of the trained network. Inappropriate weight initialization is very likely to increase the training time or even to cause
non convergence of the training algorithm, while another unfortunate result may be to
decrease the network’s ability to generalize well, especially when training with backpropagation (BP), a procedure suﬀering from local minima, . These are defaults and limitations for having successful practical application of neural networks in real life processes.
The importance manifested by the research community for this subject has been
demonstrated by the number of research work published in this area. The proposed
approaches can be, roughly, divided into two categories. Methods in the ﬁrst category
perform input data clustering in order to extract signiﬁcant information (feature vectors or reference patterns) pertaining the pattern space and initial connection weights
are chosen to be near the centers of these clusters. The main drawback of these methods is the computational cost needed to preprocess the input data. Often this cost may
be prohibitive for these methods to be used in real world applications. The second
category includes those methods that are based on random selection of initial weights
from a subset of Rn, which is an interval deﬁned considering important properties of
the pattern space and/or the parameters of the training process.
The notion of the interval, underlying random weight selection methods, suggests
the idea to use IA in order to deal with uncertainty about the initial weights. Hence,
the unknown initial weights are considered to be intervals with unknown bounds. Under generally adopted assumptions about the input to any node, the resulting unknown
interval quantity is then limited within speciﬁc upper and lower bounds. Ensuring
that scientiﬁc computations provide results within guaranteed limits is an issue mentioned by researchers in IA as a tolerance problem. In consequence, the approach
proposed herein gives rise to formulating a linear interval tolerance problem which is
solved to determine signiﬁcant intervals for the initial weights. Shary ; Pivkina
& Kreinovich ; Beaumont & Philippe and other researchers propose different methods for solving a tolerance problem. Besides formulating the problem of
determining initial weights as a linear interval tolerance problem, we also present here
a new algorithm for deﬁning the required solution to the speciﬁc tolerance problem.
The proposed linear interval tolerance approach (LIT-Approach) deals with uncertainty about the initial weights based exclusively on numerical information of the
patterns without any assumption on the distribution of the input data. IA provides the
means of handling uncertainty in parameters in much the same way this happens with
other approaches such as the possibilistic approach with Fuzzy sets , Evidence theory , Rough sets or methods combining properties of these approaches. However, methods using fuzzy sets require parameters of
the membership functions to be tuned and eventually some preprocessing of the input
data to be done if pertinent input variables need to be identiﬁed. Moreover, when using
rough sets one needs to process the input data in order to deal with the indiscernibility
relation and establish upper and lower approximations of the concepts pertaining the
problem, see . Finally, application of the Dempster-Shafer
(evidence) theory is a matter of subjective estimation of uncertainty as it assumes that
values of belief (or plausibility) are given by an expert. Unlike all these approaches,
the interval computation used for LIT-Approach needs only elementary statistics of the
input data to be computed such the sample mean, the sample standard deviation or the
median and the quartiles of the sample.
It is worth noting here the approach formulated by Jamett & Acu˜na as an
interval approach for weight initialization. The solution proposed “solves the network
weight initialization problem, performing an exhaustive search for minima by means
of interval arithmetic. Then, the global minimum is obtained once the search has been
limited to the region of convergence”. For the experimental evaluation proposed, interval weights are initially deﬁned as wide as necessary (with amplitudes up to 106). In
addition, the IA solution adopted by these researchers extends to deﬁning an interval
version of the gradient descent procedure. On the contrary, the method presented in this
paper uses IA concepts only for computing eﬀective intervals for the initial weights and
therefore it is not computationally expensive.
The sections of this paper are organized as follows. Section 2 is devoted to a presentation of the IA concepts underpinning the LIT-Approach. Section 3 presents the
analysis of LIT-Approach including both theoretical results and the weight initialization algorithm. Section 4 is dedicated to the experimental evaluation of our approach
and its comparison with well known initialization procedures. Finally, Section 5 summarizes the paper with some concluding remarks.
2. Interval Analysis and the Tolerance Problem
2.1. Interval Arithmetic
The arithmetic deﬁned on sets of intervals, rather than sets of real numbers is called
interval arithmetic. An interval or interval number I is a closed interval [a, b] ⊂R of
all real numbers between (and including) the endpoints a and b, with a ⩽b. The terms
interval number and interval are used interchangeably. Whenever a = b the interval is
said to be degenerate, thin or even point interval. An interval X may be also denoted
, [X] or even [XL, XU] where subscripts L and U stand for lower and upper
bounds respectively. Interval variables may be uppercase or lowercase, . In this paper, identiﬁers for intervals and interval objects (variables or
vectors) will be denoted with boldface lowercase such as x, y, z and boldface uppercase
notation will be used for matrices, e.g. X. Lowercase letters will be used for the square
bracketed notation of intervals [x, x], or the elements of an interval as a set. An interval
[x, x] where x = −x is called a symmetric interval. Finally, if x = [x, x] then the
following notation will be used in this paper.
rad(x) = (x −x)/2, is the radius of the interval x
mid(x) = (x + x)/2, is the midpoint (meanvalue) of the interval x
|x| = max{|x|, |x|}, is the absolute value (magnitude) of the interval x
IR, denotes the set of real intervals
IRn, denotes the set of n−dimensional vectors of real intervals
Let ⋄denote one of the elementary arithmetic operators {+, −, ×, ÷} for the simple
arithmetic of real numbers x, y. If x, y denote real intervals then the four elementary
arithmetic operations are deﬁned by the rule
x ⋄y = { x ⋄y | x ∈x, y ∈y}
This deﬁnition guarantees that x ⋄y ∈x ⋄y for any arithmetic operator and any values
of x and y. In practical calculations each interval arithmetic operation is reduced to
operations between real numbers. If x = [x, x] and y = [y, y] then it can be shown that
the above deﬁnition produces the following intervals for each arithmetic operation:
x + y = [x + y, x + y]
x −y = [x −y, x −y]
xy, xy, xy, xy
xy, xy, xy, xy
x ÷ y = x × 1
, provided that 0 <
The usual algebraic laws of arithmetic operations applied to real numbers need to be
reconsidered regarding ﬁnite arithmetic on intervals. For instance, a non-degenerate
(thick) interval has no inverse with respect to addition and multiplication. So, if x, y
are non-degenerate intervals then,
x + y = z ⇏x = z −y,
x × y = z ⇏x = z × 1
The following sub-distributive law holds for non-degenerate intervals x, y and z,
x × (y + z) ⊆x × y + x × z.
One may easily verify that the usual distributive law holds if x is a point interval or
if both y and z are point intervals. Hereafter, the multiplication operator × will be
omitted as in usual algebraic expressions with real numbers. A very important property
of interval arithmetic operations is that,
a,b,c,d ∈IR
a ⊆b, c ⊆d
a ⋄c ⊆b ⋄d,
⋄∈{+, −, ×, ÷}.
This property is the inclusion isotony of interval arithmetic operations and it is considered to be the fundamental principle of IA. More details on interval arithmetic and
its extensions can be found in Hansen & Walster ; Alefeld & Mayer ;
Neumaier .
2.2. Interval Linear Systems
An interval linear system is a system of the form,
where A ∈IRm×n, also noted
, is an m-by-n matrix of real intervals, b ∈IRm,
also noted
, is an m-dimensional vector of real intervals and x is the n-dimensional
vector of unknown interval variables. Solving a system of linear interval equations
has attracted the interest of several researchers in the ﬁeld of IA for more than forty
years. Initially, research focused on systems with square interval matrices (A ∈IRn×n)
and a number of diﬀerent methods for studying and solving such systems have been
To solve the above system of interval linear equations Ax = b, generally, means to
compute the solution set deﬁned as
(A, b) = {x ∈Rn | ˜Ax = ˜b for real ˜A ∈A, ˜b ∈b}.
That is, P(A, b) is the set of all solutions for all matrices ˜A ∈A with real elements and
all vectors ˜b ∈b having real number components. This set is generally not an interval
vector but a rather complicated set that is usually impractical to deﬁne and use . In practice, deﬁning this solution set resulted in proposing methods such as the interval versions of Gaussian elimination or the Gauss-Seidel method
which compute vectors that bound P(A, b). Note that these interval algorithms diﬀer
signiﬁcantly from corresponding point algorithms as they use preconditioning with a
point matrix for the algorithms to be eﬀective . Other frequently used methods are those based on the Rump/Krawczyk iteration .
An important issue was to deﬁne the narrowest interval vector containing the solution set P(A, b). This interval vector is called the hull of the solution set. Determining
the hull is a problem that is NP-hard as shown by Heindl et al. , and so, in general, methods try to compute only outer bounds for the hull. Other important research
results include: the work by Rohn on the solvability of systems of linear interval equations with rectangular matrices, the algorithm proposed by Hansen , to
solve over-determined systems and the work presented by Kubica on interval
methods for under-determined nonlinear systems.
For such methods one may refer to Alefeld & Herzberger ; Neumaier ;
Kreinovich et al. ; Hansen & Walster ; Hansen ; Kearfott .
The number of diﬀerent methods proposed to solve systems of linear interval equations underlines the importance of the subject, especially regarding the diﬃculty to
generally indentify the hull of the solution set of such a system. Research eﬀort has
been dedicated on the evaluation of diﬀerent methods solving systems of linear interval equations. Important works on this matter include Neumaier ; Goldsztejn
 ; Ning & Kearfott ; Rohn .
2.3. Tolerance Problem and the Tolerance Solution Set
The tolerance problem arises in engineering design and system modeling and refers
to the estimation of the tolerance of certain parameters of a system or a device so that
its behaviour i.e. its output is guaranteed within speciﬁed bounds. In mathematical
terms, if F : Rn →Rm is the mapping relating variables x = (x1, x2, . . . , xn)⊤with output parameters y = (y1, y2, . . . , ym)⊤, then the tolerance problem is associated with the
computation of a domain for the variables of F such that the corresponding y = F (x)
lie within some predeﬁned range, .
In Shary the tolerance problem is described as a particular problem related
with the analysis of a system. Using intervals and quantiﬁer formalism to model uncertainty, about a system’s parameters, Shary deﬁnes three types of solutions to the
general input-state-output equation describing a system. These solutions are sets of
values providing answers to diﬀerent issues of systems analysis. Hence, according
to Shary , for the interval equation F (a, x) = b of a system with n unknown
parameters x ∈Rn, there are three particular cases of the general AE-solution set:
• the United solution set consisting of the solutions of all point equation systems
of the form F (˜a, x) = ˜b with ˜a ∈a and ˜b ∈b,
• the Controllable solution set containing all point vectors x such that for any ˜b ∈b
one can ﬁnd the right ˜a ∈a such that F (˜a, x) = ˜b, and ﬁnally,
• the Tolerable (or Tolerance) solution set formed by all point vectors x such that
for any ˜a ∈a the image F (˜a, x) ∈b.
In the case of a static linear system F has the form of the interval linear system Ax = b
and the solution set deﬁned by (7) is the United solution set. Using the notation introduced in Shary the solution sets deﬁned previousely are:
United solution set:
∃∃(A, b) = {x ∈Rn | (∃˜A ∈A)(∃˜b ∈b)( ˜Ax = ˜b)}
Controllable solution set:
∃∀(A, b) = {x ∈Rn | (∀˜b ∈b)(∃˜A ∈A)( ˜Ax = ˜b)}
Tolerance solution set:
∀∃(A, b) = {x ∈Rn | (∀˜A ∈A)(∃˜b ∈b)( ˜Ax = ˜b)}
⊆(A, b) = {x ∈Rn | Ax ⊆b}
Both the Controllable and the Tolerance solution sets are subsets of the more general
United solution set. The speciﬁc uncertainty problem deﬁnes which of the above solution sets contains the solution of the problem. With respect to the assumption that F
describes the input-output relation of a static linear system, the tolerance solution set
provides answers to the question whether there are input signals ˜x to the system such
that the output Ax remains within speciﬁed limits b. Moreover, it is worth noting here
that the elements of the solution sets, as deﬁned previously, are not just points in Rn
but they may be intervals in IRn as well .
3. Weight Initialization with the LIT-Approach
3.1. Random Selection of Initial Weights
Random initialization of connection weights seems to be the most widely used approach for real world applications. A number of approaches such as those presented in
this section claim the reputation to provide improvement in BP convergence speed and
avoidance of bad local minima, .
Unless diﬀerently deﬁned, hereafter din denotes the number of inputs to a node.
Fahlman studies on random weight initialization techniques resulted in the
use of a uniform distribution over the interval [−1.0, 1.0]. This seems to constitute a
simpliﬁed approach for use in any problem without further hypotheses.
Boers & Kuiper initialize the weights using a uniform distribution over the
−3/ √din, 3/ √din
. This interval is deﬁned so that the stimulus of any node
is located around the origin of the axes where the sigmoid activation function has its
steepest slope. This interval is the same as the one deﬁned by the conventional method
of Wessels & Barnard . However, in order to avoid false local minima detected
when applying this conventional method, Wessels & Barnard also propose a
more reﬁned method adopting a diﬀerent strategy for the input-to-hidden layer connections and for the hidden-to-output layer connections.
Bottou deﬁnes the interval
−a/ √din, a/ √din
, where a is chosen so that the
weight variance corresponds to the points of the maximal curvature of the activation
function. For the logistic sigmoid activation function a is set to be approximately equal
to 2.38 and 0.66 for the hyperbolic tangent. Criticism on this approach concerns the
fact that it was not compared against other methods.
Kim & Ra calculated a lower bound for the initial length of the weight vector of a neuron to be
η/din where η is the learning rate used by the training procedure.
Smieja based on the study of the hyperplanes dynamics, proposes uniformly
distributed weights normalized to the magnitude 2/ √din for each node. The thresholds
for the hidden units are initialized to a random value in the interval
−√din/2, √din/2
and the thresholds of the output nodes are set to zero.
Drago & Ridella proposed a method aiming to avoid ﬂat regions in the error surface in an early stage of training. Their method is called statistically controlled
activation weight initialization (SCAWI). They determine the maximum magnitude of
the weights through statistical analysis. They show that the maximum magnitude of
the weights is a function of the paralyzed neuron percentage (PNP), which is in turn
related to the convergence rate. By determining the optimal range of PNP through
computer simulations, the maximum magnitude of the weights can be obtained. The
weights are uniformly distributed over the interval [−r, r] with r = 1.3/
1 + niv2 for
the hidden layer nodes and r = 1.3/ √1 + 0.3nh for the output layer nodes. Here, ni denotes the number of inputs to the network and nh is the number of nodes in the hidden
layer. In addition v2 is the mean of the expectation of the quadratic values of the inputs,
Nguyen & Widrow proposed a simple modiﬁcation of the widely used random initialization process of Fahlman . The weights connecting the output units
to the hidden units are initialized with small random values over the interval [−0.5, 0.5].
The initial weights at the ﬁrst layer are designed to improve the learning capabilities of
the hidden units. Using the magniﬁcation factor deﬁned by the relation, β = 0.7H1/N
where H is the number of hidden units and N is the number of inputs, the weights are
randomly selected in the interval [−1, 1] and then scaled by v = βv/∥v ∥where v is the
ﬁrst layer weight vector. Results obtained by Pavelka & Proch´azka , provide signiﬁcant experimental evidence on the superiority of Nguyen-Widrow’s method against
typical random initialization techniques.
In addition to the above, a number of interesting methods related to this context
have been formulated by Osowski ; Chen & Nutter ; Yam & Chow ; LeCun ; Schmidhuber & Hochreiter , as well as by others researchers.
Despite the availability of such an armory of weight initialization methods, it seems
that, there does not exist any, widely accepted, assessment, regarding the eﬀectiveness of these methods with some speciﬁc problem or a class of problems. Research
eﬀorts concerning the comparison of diﬀerent weight initialization techniques include
those reported in Thimm & Fiesler ; Fern´andez-Redondo & Hern´andez-Espinosa
 . Thimm and Fiesler compared several random weight initialization schemes
using a very large number of computer experiments. They concluded that the best initial weight variance is determined by the dataset, but diﬀerences for small deviations
are not signiﬁcant and weights in the range [−0.77, 0.77] seem to give the best mean
performance. Fern´andez-Redondo & Hern´andez-Espinosa presented an extensive experimental comparison of seven weight initialization methods; those reported
by Kim & Ra ; Li et al. ; Palubinskas ; Shimodaira ; Yoon
et al. ; Drago & Ridella . Researchers claim that methods described in
Palubinskas ; Shimodaira above proved to give the better results from all
methods tested. However, they argue that the method presented in Shimodaira 
suﬀers from the need of pre-processing.
3.2. Analysis of the LIT-Approach
Let us consider a multi-layer perceptron (MLP) with 3 layers, input, hidden and
output. Let N, H and O denote the number of nodes of the three layers, respectively.
The analysis presented hereafter refers to any node, say j (1 ⩽j ⩽H), in the hidden
layer and so the results apply without any further assumption to every node in the hidden layer. Nodes in the hidden and the output layers are considered to have a sigmoid
activation function which is either the logistic function or the hyperbolic tangent. In
consequence, the output of any node, say the jth, is given by
y j = sig(
wjixi + wjb), 1 ⩽j ⩽H,
while output of a node in the output layer is given by
wk jyj + wkb), 1 ⩽k ⩽O.
Note that wji is the weight of the connection from the ith input node to the jth hidden
one. Moreover, wjb and wkb denote the weights of the bias connections to the jth hidden
and the kth output nodes respectively.
Sigmoid functions (sig) are able to eﬀectively discriminate between inputs when
these inputs lie in the so-called active region of their domain, that is the input range
where the derivative of the activation function has a large value. When training the
network, in order to avoid problems such as premature saturation, a realistic hypothesis
is to start training with such weight values that the node input would be in the active
region of the sigmoid function, . Then,
the training algorithm is responsible to explore the domain of deﬁnition of the sigmoid
function, in order to determine those values of the weights that minimize the error of
the network output. For any node, say the jth, in the hidden layer having its input in
the active region of the sigmoid means that:
wjixi + wjb ⩽a,
where −a and a are the lower and the upper bounds of the active region of the sigmoid
activation function.
Suppose that p patterns are available for training and each pattern is represented by
an N-dimensional vector x = (x1, x2, . . . , xN)⊤. Then expression (14) yields the following linear system of p inequalities with N + 1 unknown variables wj1, wj2, . . . , wjN, wjb.

i + wjb ⩽+a
i + wjb ⩽+a
· · · · · · · · · · · · · · · · · · · · · · · · · · ·
i + w jb ⩽+a
Note that in general, p > N +1 and so this system is over-determined and has a solution
only if p −(N + 1) pattern vectors are linearly dependent. Problems where the number
of features is higher than the number of patterns are known as High Dimension Low
Sample Size (HDLSS) problems and constitute a special research topic, .
Weight initialization methods deﬁne symmetric intervals for selecting values of the
initial weights. Hence, it is legitimate to assume that each unknown weight wji is a real
number taken from a symmetric interval [wji] = [−wji, wji], 1 ⩽i ⩽N and [wjb] =
[−wjb, wjb] is the symmetric interval for the unknown thresholds. If [a] = [−a, a]
denotes the interval for the active range of the activation function of the jth node, then
expression (14) may be written in interval form as,
[w ji]xi + [wjb] ⊆[a].
In accordance to subsection 2.3 this relation deﬁnes wji as a solution to the tolerance
problem associated with the equation
[wji]xi + [wjb] = [a].
From another point of view, if one considers the p input patterns available for training then this equation expands to the following interval system of linear equations,

1 + [wj2]x1
2 + · · · + [wjN]x1
N + [wjb] = [a]
1 + [wj2]x2
2 + · · · + [wjN]x2
N + [wjb] = [a]
· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·
1 + [wj2]xp
2 + · · · + [wjN]xp
N + [wjb] = [a]
Let us denote this system Xwj = a, with X being the p × (N + 1) matrix formed
by the p × N matrix of the pattern values augmented with the p-dimensional vector
(1, 1, . . . , 1)⊤for the threshold, wj = ([wj1], [wj2], . . . , [wjN], [wjb])⊤is the (N + 1)dimensional vector of the unknown weight intervals and a = ([a], [a], . . . , [a])⊤the
p-dimensional vector of the right hand side. Note that the elements of X are real numbers which are here considered as point intervals in order to comply with notation of
subsection 2.3.
From an algebraic point of view a solution to this interval linear system is an interval vector wa such that substituting it to (S2) and executing all interval arithmetic
operations yields the valid equality Xwa = a. All the interval vectors that are algebraic solutions to some interval linear equation or a system of equations form a nonempty set. So, speaking about the solution of an interval equation (system of equations, inequalities, etc.) on its own is irrelevant with respect to the speciﬁc uncertainty
problem modelled by the interval equation (system of equations etc.) at hand . The right thing would be to refer to the solution of some speciﬁc problem statement relating to the interval equation (system of equations, inequalities, etc.). Hence,
what really matters with the solution of the latter interval system is to obtain an interval vector, say w∗
j2], . . . , [w∗
jb])⊤, such that for all given patterns
xk ∈RN, 1 ⩽k ⩽p the following relation is valid,
jb] ⊆[a] .
This statement clearly identiﬁes the system (S2) as the formulation of an interval linear
tolerance problem for the initialization of the connection weights to any node in the
hidden layer of an MLP.
Diﬀerent algorithms have been proposed to construct interval solutions to the linear
tolerance problem in terms of its inner interval approximations, . Prior to discussing the existence of an algorithm for deriving a
solution for this linear interval tolerance problem we need to discuss the non emptiness
of the tolerance solution set of the system (S2).
Lemma 1. Consider the interval linear system Ax = b, where A ∈IRm×n is an mby-n matrix of real intervals, b ∈IRm is an m-dimensional vector of real intervals
b = {b1, b2, · · · , bm} and x is the n-dimensional vector of unknown interval variables.
If 0 ∈bk for all k ∈{1, 2, · · · , m} then the tolerance solution of this system is not empty.
Proof. It is straightforward to see that the trivial m-dimensional vector t0 = (0, 0, · · · , 0)
is such that At0 ⊆b. Thus the tolerance solution set of this system is not empty.
However, the trivial solution may not be adequate for the problem at hand. To further advance with this issue one may prove the algebraic solvability of the given system
(S2) then solve the system and ﬁnally select the solutions that are in the tolerance solution set . Another way to proceed is a constructive approach which
consists in proposing an algorithm for constructing tolerance solutions. A number of
approaches are presented in Shary . The proposed LIT-Approach is also a constructive one.
Here let us present the algorithm of Shaidurov using the same notation as given in
Shary . Let an interval m × n matrix A = (aij) and an interval right-hand side m
dimensional vector b = (bi) and let P
∀∃(X, b) denote the solution set to the associated
linear interval tolerance problem.
For some given t ∈P
∀∃(X, b), t = (t1, t2, · · · , tn)⊤calculate the intervals
rad(bi) −|mid(bi) −Pn
j=1 ai jt j|
j=1 |ai j|
i = 1, 2, · · · , m, and then put ρ = min
1⩽i⩽m ri. The vector (t + ρe) is a solution to the linear
tolerance problem. Note that e is the interval vector ([−1, 1], [−1, 1], . . . , [−1, 1])⊤.
Regarding the tolerance problem for weight initialization the hypothesis of having
an initial solution to start with this algorithm can be satisﬁed by taking the initial vector
t to be the trivial vector (0, 0, · · · , 0). A similar method proposed by Neumaier 
as well as other approaches can be found in Shary . Moreover, a discussion regarding various aspects and optimality criteria of the diﬀerent algorithms can be found
in Shary and Pivkina & Kreinovich . The question concerning the best
approach when solving the weight initialization tolerance problem depends on the performance parameters set for the weight initialization problem itself. We consider that
this question has both theoretical and practical importance and needs to be separately
addressed outside this paper.
3.2.1. Theoretical results
Hereafter, we present our approach to constructing a solution to the tolerance problem for the initialization of weights. We take advantage of the fact that the intervals
are symmetric and build the proposed method based on the following mathematical
results omitting the hypothesis of disposing an initial solution vector t. Without loss
of generality and for the sake of readability the notation used is the same as above for
equations (15)–(17).
Lemma 2. For any symmetric intervals w1 and w2 such that w1 ⊆w2 and any real
numbers x1 and x2 such that x1 ⩽x2 then the relation x1w1 ⊆x2w2 is satisﬁed.
Proof. The relation x1 ⩽x2 implies that [x1, x1] ⊆[x2, x2] holds true for the point
intervals corresponding to x1 and x2. Hence, given that the interval multiplication is
inclusion isotonic the relation x1w1 ⊆x2w2 is satisﬁed.
Lemma 3. Consider the interval equation [x][w] = [a], where [a] is a symmetric
interval, [a] = [−a, a], and [x] = [xL, xU] with 0 < xL ⩽xU. Then the solution of the
equation is [−w, w] = [−a, a]/xU.
Proof. Let us assume that [w] is an interval of the form [wL, wU]. Then the multiplication operation of intervals implies for [−a, a] that −a = min{xLwL, xLwU, xUwL, xUwU}
and a = max{xLwL, xLwU, xUwL, xUwU}. Moreover, the inequality 0 < xL ⩽xU implies that wL < 0 < wU and so −a = xUwL and a = xUwU. Thus, the solution of the
interval equation is [−w, w] = [−a, a]/xU.
When the coeﬃcient of [w] is not an interval [x] but a ﬁnite set of p real numbers
x1, x2, . . . , xp then one may consider this as an interval linear system of p equations of
the variable [w]. Then, the following Lemma 4 gives a solution to this interval linear
Lemma 4. Consider the interval system of p linear equations with one variable [w] of
the form, x[w] = [a], where [a] is a symmetric interval, [a] = [−a, a], and x is a real
number from a set with ﬁnite number of elements, x ∈X = {x1, x2, . . . , xp}. Suppose
that xm = max
xk∈X |xk|. Then the interval [wm] which is a solution of the interval equation
xm[w] = [a], is such that, ∀xk ∈X, xk[wm] ⊆[a], and hence, [wm] is a member of the
tolerance solution set for this interval system.
Proof. One may observe that, [wm] = [−a, a]/xm, according to Lemma 3 and considering xm = [xm, xm] to be a point inteval. Given that, |xk| ⩽xm, for any xk ∈X, it follows
that, xk ⩽xm, and xk/xm ⩽1. In consequence, xk[wm] = [−a, a]xk/xm ⊆[−a, a].
Hence [wm] is a solution in the tolerance solution set.
The following proposition is a generalization of the previous Lemma 4 for an interval
system of p linear equations with n unknown variables and symmetric right-hand side
intervals.
Proposition 1. Consider the interval system of linear equations of the form, x1[w1] +
x2[w2] + · · · + xn[wn] = [a], with [a] being a symmetric interval, [a] = [−a, a], and
each xi a real number from a set with ﬁnite number of elements, that is, xi ∈Xi =
i , . . . , xp
i } ⊂R, 1 ⩽i ⩽n. In addition, for 1 ⩽i ⩽n let xm
xl∈Xi |xl|,
and [w∗] be the interval deﬁned by the relation [w∗] = [−a, a]/P
i . Then the vector
2], . . . , [w∗
n]) with [w∗
i ] = [w∗], 1 ⩽i ⩽n constitutes a solution in the
tolerance solution set for this interval system.
Proof. For every xk
i ∈Xi, 1 ⩽k ⩽p, it stands that xk
i . Then, according to
Lemma 2, the relation xk
i ] is valid. So, for any combination of elements
of the sets X1, X2, . . . , Xn, we have:
Adding the above relations and given that interval addition is inclusion isotonic we
have that,
2] + · · · + xkn
2] + · · · + xm
2 + · · · + xm
n )[−a, a]
= [−a, a].
This proves the proposition.
This proposition applies directly to the interval linear system (S2) above or to
Xwj = a. Notice that each of the sets Xi corresponds to a column vector of X and the
interval vector ([w1], [w2], . . . , [wn])⊤stands for the interval vector wj of the weights to
any node j. So the following relation deﬁnes a solution to the system (S2).
ji] = [−a, a]/(U + 1),
u(i), and u(i) = max
i |), where |xk
i | denotes the absolute value of xk
These intervals stand for any weight interval [wji] as well as for the bias [wjb] and
verify relation (17). So, this solution is a member of the tolerance solution set.
3.2.2. Reﬁning the method
The above approach eﬀectively tackles the problem of neural saturation by decoupling the weights from the patterns. This problem has already been addressed by other
researchers using mathematically questionable hypotheses . The
solution provided by this approach takes into account the outliers for each component
of the input sample. However, in practice the random selection of weights reduces the
impact of the input to the hidden node y j induced by outliers with large values. Recalling the arguments of Wessels & Barnard , the standard deviation of the input
y j to a hidden node is given by σy j = (w √din)/3 where din is the number of inputs to
the node and w deﬁnes the interval [−w, w] where the weights are randomly selected
from. It is easy to verify that if w is computed using our approach then even for small
values of din (e.g. 5) the value of σyj is very small (0.53) and tends to become smaller
(→0.13) as din increases. This means that the intervals computed by the proposed
method can be widened while still satisfying the tolerance conditions. Hence, the idea
is to “modulate” each interval with respect to the eﬀective range of the input sample
and thus diﬀerentiate the weight intervals corresponding to diﬀerent features of the input data. This is achieved by taking into account some statistics of the input data (e.g.
the variance).
Let us denote sxi a statistic providing summary information about the ith input data
component xi such as the third quartile (Q3) or any q-quantile marking the boundary
of approximately 4/5 of the input data. These statistics provide important information
about the location of the majority of the input data regardless the distribution of the
sample. If the input data display normal distribution then some multiple of the sample
standard deviation can be used instead. Given this hypothesis and following deﬁnitions
of Proposition 1 above we may conclude that [w∗
ji]sxi ⊆[w∗
i . Equating the two sides
of this relation and solving permits to derive the interval
which eﬀectively satiﬁes the previous assumptions. Moreover, this relation widens the
weight intervals with respect to the majority of the input data and as argued previously
it complies “statistically” with the tolerance problem solution.
In the above heuristic using some suitably chosen sxi, such as Q3, to divide the
right-hand side of (20) is done in order ensure enlargement of the weight intervals with
respect to the majority of the input data. In descriptive statistics, outliers are expected
to lie outside the interval [Q1 −k(Q3 −Q1), Q3 + k(Q3 −Q1)] for some nonnegative
constant k and Q3 −Q1 being the Inter Quartile Range (IQR) . Note that typically, for statistical packages such as Minitab and SPSS, k = 1.5
 . So, if the value of an outlier, say xl
i, is used instead of sxi, then
this oulier should be carrefully chosen otherwise depending on this value the fraction
i in equation (20) tends to one. In consequence, depending on the input data distribution and the outlier used this heuristic will probably result in unnoticeable (i.e.
insigniﬁcant from practical point of view) enlargement of the weight intervals.
Furthermore, the use of the above heuristic results in deﬁning interval weights
whose ranges are inversely proportional to the variance of the corresponding input
data components. So, for an input data component, say xi, with a high variance value,
deﬁning a shorter weight interval implies that it is likely to select smaller weight values
for this input. In consequence, for some given wjb the intercept −wjb/wji of the hyperplane deﬁned by a hidden node with the xi axis (see Figure 1) is more likely to cover
the range of values of xi being positioned inside the majority of the values of the input
data distribution, rather than an intercept that passes through the axes origin, or one that
lies far away from the values of xi. On the contrary when the values of xi have a small
variance then the initial weight interval should be larger. This implies that the initial
weights are likely to have large values so that the intercept −w jb/wji is more likely
to be in the range of values of xi, see Figure 1. Moreover, the other beneﬁt expected
by deﬁning intervals with variable ranges is to diversify as much as possible the sets
of initial weights selected for the hidden nodes. Hence, diﬀerent nodes tend to deﬁne
initial hyperplanes whose distance from the origin of the axes given by |wjb|/
as diversiﬁed as possible.
Concerning the inital distance of any hyperplane from the origin of the axes we
need to note that 0 ⩽|wjb|/
ji. The eﬀect of widening produced by (20) on the
weight intervals tends to move the hyperplanes towards the beginning of the pattern
axes as it tends to increase the denominator in the distance formula. On the other hand,
theoretically there is no upper bound for this distance. This is a common issue to all
weight initialization techniques that randomly select initial weights from some interval
deﬁned around 0 with very small real values. In our approach this may occur if all
weights are selected from extremeley narrow symmetric intervals which in their turn
are computed if the interval [−a, a] is divided by a big number corresponding to the
quantity U + 1, when the problem at hand has a huge number of features. However, as
we will show later in section 4 even in the case of a real life problem such as the MNIST
dataset with 784 features the algorithm demonstrates a very interesting behaviour outperforming other weight initialization techniques. A thorough
study of the UCI repository of machine learning database 
shows that problems with a very big number of features are treated as dimensionality
reduction or feature extraction ones before being considered as classiﬁcation or regression problems.
The above considerations and the results obtained are valid for continuous valued
input patterns. For some input xi which is binary or a constant value then sxi = 0. This
constitutes a major incovenience as it results in a division by 0 for the fraction xm
equation (20). To avoid this problem we choose to leave the interval [w∗
ji] unchanged
by imposing sxi = 1. For this we require β ⩽sxi where this lower bound is deﬁned as
β = 0.1. Whenever sxi < β we impose sxi = 1. The following formula summarizes the
rule for computing sxi.
sgn sxi −β + 1 sxi −1
sgn sxi −β −1
where sgn denotes the sign function. This choice introduces a kind of “discontinuity”
which can be avoided if one chooses sxi = β. However, even this option is still a heuristic one. In a future correspondence we could investigate the possibility to adaptively
deﬁne β as an interval derived by the data and discuss the impact of such a formulae on
speciﬁc experiments. In the present research the benchmarks and real world problems
tackled provide no hints as to which is the optimum formula for sxi deﬁnition in this
speciﬁc case.
Typically, normalization or scaling is applied so that the input samples are in the interval [−1, 1], mainly in order to facilitate training .
These operations normally do not alter the status of the input data. So, the previous
considerations remain valid and the use of the term sxi for properly modulating the
original weight intervals [w∗
ji] still applies after normalization or scaling of the input
data. For the rest of this paper, we assume that the values of the input patterns are
normalized to be in the interval [−1, 1] or the interval . Under these hypotheses
we may state that the relation [w∗
ji]sxi ⊆[w∗
ji]1 is valid and suggests that solving the
following equations:
permits to deﬁne the intervals,
that obviously satisfy the relation,
j1]sx1 + [W∗
j2]sx2 + · · · + [W∗
jN]sxN + [w∗
j2] + · · · + [w∗
Hence, the interval vector W*
j2], . . . , [W∗
jb])⊤is a solution in the
tolerance solution set of the interval system (S2). Recall that sxi is computed using
formula (21).
3.2.3. Initializing hidden-to-hidden and hidden-to-output layer connection weights
The analysis presented above focuses on eﬀective initialization of weights of the
input-to-hidden layer connections. Earlier implementations of a complete algorithm
were based on minimal assumptions regarding the initial values of weights for hiddento-hidden and hidden-to-output layer connections, that is, random selection of values
in the interval [−1, 1]. This choice gave rather satisfactory results in the case of small
sized networks and datasets, see section 4, suites 1 and 2 of experiments. In order to
deﬁne a full scale algorithm for initializing weights of any MLP two issues are considered here. The ﬁrst deals with saturation of the nodes in any hidden layer, while the
second deﬁnes an order of magnitude for the weights of connections leading to ouput
layer nodes.
In order to avoid saturation of any node in the kth hidden layer we adopt the hypotheses of the previous analysis. This means that weights of connections linking a
node in the hidden layer k with the ouputs of nodes in the layer k −1 are randomly
selected in the interval [−ak/(Hk−1 + 1), ak/(Hk−1 + 1)], where Hk−1 is the number of
nodes of the layer k −1 and ak is the active range of the activation function of the node
in the hidden layer k. The previous formula for nodes in the hidden layer k is derived
considering that the outputs of the layer k −1 have a maximum value equal to 1. In
practice, instead of (Hk−1 + 1) the value of Hk−1 can be used without any diﬀerence
regarding the training performance.
For the weights of the hidden-to-output connections diﬀerent approaches are proposed by diﬀerent researchers (subsection 3.1). In order to optimize the choice of these
weights we used the formula [−3A/ √din, 3A/ √din] introduced in Wessels & Barnard
 where instead of din we set H for the number of hidden layer nodes. The authors in that paper determined the value of the scale factor A = 1 through experiments
with small sized networks. We adopted the same approach but we also experimented
with networks with a higher number of nodes in the hidden layer. For these networks
when A = 1 the fraction 3A/
H becomes too small yielding extremely narrow weight
intervals for the hidden-to-output layer connections which slow the training process.
By gradually increasing the value of A we observed that the network performance improved and so we came up with the following rule of thump.
The value of A = 1 is valid for networks with a relatively small number of nodes
in the hidden layer i.e. H ⪅30. For medium to larger sized networks i.e. H > 30
the best network performance was observed when A > 1. Experimented with H = 36
we found that A ≈1.2 and A ≈3 for H = 300. Finally, for H = 650 we noticed that
A should be set to 4 for nodes with the logistic sigmoid activation function while for
nodes with the hyperbolic tangent this value should be A ≈2. We cannot guarantee that
these results are optimal for every considered dataset. However, the resulting intervals
roughly conﬁrm the ﬁndings for the weight intervals reported in Nguyen & Widrow
 and Thimm & Fiesler . To the best of our knowledge there is no speciﬁc
study on this matter in the literature and in light of these results this should constitute
an interesting point for deeper investigation.
3.3. Algorithm and Discussion
3.3.1. Algorithm Description
The algorithm implementing the above approach computes one speciﬁc interval
ji] for each component i of the input data as well as the interval [w∗
jb] for the thresh-
old. Thus, n+1 intervals are computed once and they are used for selecting the weights
of any node in the hidden layer.
Input Data Coding
1. Continuous input data are scaled to be in the interval [−1, 1] (or ). Binary
variables are set to {−1, 1} (or {0, 1}).
2. For each continuous valued input data variable xi compute the third quartile Q3
and set sxi = Q3. If xi displays normal distribution compute the sample standard
deviation σxi and set sxi = 2σxi. If xi can be approximated by the normal distribution then sxi = kσxi for some suitably chosen k. If xi is not continuous then
Apply rule (21) above.
3. Deﬁne the value of the parameter a for the bounds of the active region interval
[−a, a] depending on the type of the activation function of the jth node, see
subsection 3.3.2 hereafter.
Computing Weights of Input to Hidden Layer Connections
4. For each node j in the hidden layer and any input connection i the weight wji
is randomly selected with uniform distribution from the interval [W∗
ji] deﬁned
using relation (23) above.
5. For each node j in the hidden layer the weight wjb of the bias is randomly selected with uniform distribution from the interval [w∗
jb] deﬁned using relation
(19) above.
Computing Weights of Connections from Hidden Layer (k −1) to Hidden Layer (k)
6. These weights are random numbers selected to be uniformly distributed in the
interval [−ak/Hk−1, ak/Hk−1] as deﬁned in the previous subsection.
Computing Weights of Hidden to Output Layer Connections
7. Weights of the hidden to the output layer connections are random numbers selected to be uniformly distributed in the interval
scale factor A is deﬁned in the previous subsection.
3.3.2. Discussion
Step 3 of the algorithm requires setting the bounds of [−a, a] for the active region
of the sigmoid activation function. This interval is assumed to be the region where
the derivative of the sigmoid activation function is greater than or equal to 0.04Dmax
or 0.05Dmax where Dmax denotes the maximum magnitude of the derivative of the sigmoid, . For example in case a logistic sigmoid activation
function is used then a = 4.59 or a = 4.34. For the experiments shown in this paper the
values adopted are those deﬁned by the Neural Network Toolbox of MATLAB, that is,
a = 4 for the logistic sigmoid and a = 2 for the hyperbolic tangent. These values are
computed for λ = 1 where λ is the slope parameter of the sigmoid activation function.
Most of the issues pertaining the fomulation of the LIT-Approach were analyzed
and resolved in earlier subsections. Here we will brieﬂy refer to the ability of the
proposed method to cope with prematurely saturated units and symmetry breaking.
These matters are reported in the literature as troubles
of neural network training that need to be addressed by weight initialization. Regarding premature saturation of the units the proposed method by default deﬁnes initial
weights which prevent saturation of the hidden nodes at an early stage of training. In
addition, symmetry breaking that is preventing nodes from adopting similar functions
is addressed using random weight selection from intervals with diﬀerent bounds.
Besides these matters, Wessels & Barnard note that another problem is what
they call false local minima for which they name three possible causes. These are the
following: Stray hidden nodes, that is nodes deﬁning initial decision boundaries which
have been moved out of the region of the sample patterns. Hidden nodes having duplicating function are the nodes that deﬁne separating hyperplanes having the same initial
position and orientation. Finally, dead regions in the pattern space are created when
in these regions the hidden nodes are arranged so that they all happen to be inactive,
that is, there are no hyperplanes deﬁned by the hidden nodes inside these regions. The
LIT-Approach tackles these issues based on the way it deﬁnes the weight intervals. The
issues regarding stray hidden nodes and dead regions are suﬃciently addressed based
on the way the LIT-Approach deﬁnes the initial weight intervals and then on the way
the hidden nodes deﬁne the initial hyperplanes to be in the heart of the pattern data, see
subsection 3.2.2. Moreover, hidden nodes are not likely to have duplicating function
due to the random weight selection. The LIT-Approach, while not speciﬁcally designed
to tackle these speciﬁc problems, it, however, addresses them eﬃciently as shown by
the results of the experiments hereafter. Based on the advantages of distributions such
as those proposed in Sonoda & Murata there might be improvements concerning how the LIT-approach tackles random weight selection, now deﬁned by uniform
distribution.
Finally, we need to note that the proposed approach does not intend to deal with
the problem of structural local minima in the weight space. This issue concerns the
training phase of an MLP and it has eﬀectively been tackled in Magoulas et al. .
4. Experimental Evaluation
In order to assess the eﬀectiveness of the proposed method we designed and conducted three diﬀerent suites of experiments. The ﬁrst suite deals with the comparison
of the performance of the proposed method against six diﬀerent weight initialization
methods which are based on random selection of initial weights from predeﬁned intervals. The benchmarks used for this ﬁrst suite mainly concern classiﬁcation problems,
while one of them deals with regression and a second with prediction of a highly nonlinear phenomenon. Moreover, a number of experiments were executed on function
approximation and they are presented in a separate subsection. The second suite constitutes a thorough comparison of the proposed LIT-Approach with the well known
initialization procedure proposed by Nguyen & Widrow .
The performance measures considered for all experiments are: the convergence
success of the training algorithm, the convergence rate and the generalization performance achieved for the test patterns. The convergence success of the training algorithm
is the number of initial weight sets for which the training algorithm reached the prede-
ﬁned convergence criteria. The convergence rate is the number of epochs needed for
the training to converge. For benchmarks with continuous valued output, generalization
performance is computed using the mean absolute error of the output of the network
and the target output, and for classiﬁcation benchmarks, generalization is deﬁned as
the percentage of successfully classiﬁed previously unknown test patterns. The analysis of the experimental results was carried out using the statistical analysis package
SPSS v17.0 , STATService 2.0 and the R
statistical computing environment.
Hereafter, training a network for some speciﬁc benchmark with initial weights selected using some weight initialization method is called a trial. A training experiment
is a set of trials corresponding to training the network for some speciﬁc benchmark
using a set of initial weights selected by the same weight initialization method.
4.1. Suite 1 of Experiments
4.1.1. Experimental Setup
This suite of experiments was set up in order to investigate the eﬃciency of the
proposed approach on a relatively broad spectrum of real world problems. Comparison is done against the following (in alphabetical order of the abbreviations used)
well known weight initialization methods; BoersK, , Bottou,
 , KimRa, , NW, , SCAWI,
 , and Smieja, .
The real world problems adopted for the experiments are benchmarks reported in
various research papers used to compare performance of diﬀerent weight initialization
methods, as for example Fern´andez-Redondo & Hern´andez-Espinosa ; Thimm
& Fiesler ; Yam & Chow . These real world problems are brieﬂy
described in the following paragraph. Detailed description and more information can
de found in the UCI repository of machine learning database 
and references cited therein.
1. Auto MPG prediction (inputs:7, outputs:1). This dataset concerns city-cycle fuel
consumption in miles per gallon, to be predicted in terms of 3 multi-valued discrete and 4 continuous attributes. The number of instances is 398. Six patterns
with missing values have been removed.
2. British language vowels recognition (inputs:10, outputs:11). As stated in the
benchmark summary, this is a speaker independent recognition problem of the
eleven steady state vowels of British English using a speciﬁed training set of
10 linear prediction coeﬃcients derived log area ratios. The original dataset
comprises 991 instances pronounced by diﬀerent speakers. A subset containing
the ﬁrst 330 instances were retained for training and testing.
3. Glass identiﬁcation (inputs:9, outputs:1). Based on 9 attributes, this classiﬁcation of types of glass was motivated by criminological investigation. The dataset
used is the glass2 downloadable from PROBEN1 ftp site. It consists of
214 instances already pre-processed and so there are no missing values.
4. Servo prediction (inputs:12, outputs:1). Originally this benchmark was created
by Karl Ulrich (MIT) in 1986 and refers to a highly non-linear phenomenon that
is predicting the rise time of a servomechanism in terms of two (continuous) gain
settings and two (discrete) choices of mechanical linkages. The dataset consists
of 167 patterns and has no missing values.
5. Solar sunspot prediction (inputs:12, outputs:1). The dataset contains the sunspot
activity for the years 1700 to 1990. The task is to predict the sun spot activity
for one of those years given the activity of the preceding twelve years. A total of
279 diﬀerent patterns are derived from the raw data.
6. Wine classiﬁcation (inputs:13, outputs:3). These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found
in each of the three types of wines. The dataset contains 178 instances and has
no missing values.
The original datasets were preprocessed to eliminate duplicate patterns and values were
scaled to match requirements set by the weight selection procedures. These operations
were performed according to PROBEN1 guidelines, . Unless otherwise
stated, the datasets were partitioned to training sets using approximately 75% of the
patterns and to test sets using the remaining 25%. For the Servo prediction benchmark
the training set was made using 84 patterns and the test set using 83 patterns. The
training and the test sets are deﬁned once and used for all experiments. During network
training the patterns of the training set are presented in the same order using the trains
i.e. the online sequential training procedure of MATLAB.
A total number of 42 experiments were set up for these 6 problems and the 7 weight
initialization methods. Each experiment was carried out using a set of 100 initial weight
vectors, selected by the corresponding method. The same network architecture was
initialized with these vectors and trained using online BP. The network architecture
and the training parameters, used in this arrangement, are reported in Table 1. These
parameters are similar to those found by Thimm & Fiesler .
a) Learning rate is the rate used by the vanilla BP online algorithm.
b) Convergence criterion is either the goal set for the minimization of the error
function or the minimum percentage of the training patterns correctly classiﬁed
by the network.
c) Max cycles denote the maximum number of BP cycles. During a cycle all training patterns are presented to the network in random order and weights are updated after every training pattern. Training stops when Max cycles number is
d) Input data scale indicates the interval used by all weight initialization algorithms
except the Nguyen-Widrow algorithm, which scales input data values in the interval [−1, 1].
Table 1: Architectures of networks and training parameters used for the Suite 1 of the experiments
Activation
Convergence
Input data
architecture
British Vowels
† logsig denotes the logistic sigmoid function and tansig is the hyperbolic tangent
4.1.2. Analysis of the Results
Tables 2, 3 and 4 report the experimental results on the benchmarks considered for
the aforementioned performance measures. A quick look at these results shows that
the proposed approach improves network performance for all parameters.
The comparison of the eﬃciency of the diﬀerent initialization methods is based
Table 2: Convergence success results in 100 trials for the Suite 1 of the experiments
Initialization Algorithms
British Vowels
on the statistical analysis of the results obtained. In order to evaluate the statistical signiﬁcance of the observed performance one-way ANOVA was
used to test equality of means. ANOVA relies on three assumptions: independence,
normality and homogeneity of variances of the samples. This procedure is robust with
respect to violations of these assumptions except in the case of unequal variances with
unequal sample sizes, which is true for the Glass benchmark as the larger group size is
more than 1.5 times the size of the smaller group.
The validity of the normality assumption is omitted and Levene’s test for testing
equality of variances is conducted, . Homogeneity of
variances is rejected for all cases by Levene’s test and so Tamhane’s posthoc procedure,
 , is applied to perform multiple comparisons analysis
of the samples. The signiﬁcance level set for these tests is α = 0.05. The p-value
(Sig.) indicated for each initialization method, concerns comparison with the proposed
method and the mean value is marked with an ∗when equality of means is rejected
p-value < 0.05. The analysis of the Glass benchmark results was performed pairwise
between successful initialization methods using the Mann-Whitney test.
Table 5 reports for each initialization method how many times a method delivers
Table 3: Convergence rate results for the Suite 1 of the experiments
Benchmarks
Initialization Algorithms
British Vowels
∗denotes that the mean value of the initialization method is signiﬁcantly diﬀerent from the mean value of LIT-A
using the indicated p-values (Sig.) computed by the posthoc analysis of the ANOVA results.
– denotes that the initialization method failed to meet the convergence criteria exceeding the maximum number of
cycles in all trials.
superior, equal or inferior performance when compared (pairwise comparisons) with
all other methods regarding convergence rate and generalization. The advantage offered by the proposed method to achieve better convergence rate is manifested by these
results. So, performance of a neural network when weights are initialized with the
proposed method is superior in 42% of the cases. In 50% of the cases performance is
the same with all other methods and only in 6% the proposed method delivers inferior
performance to the training algorithm.
In terms of generalization the proposed method though having a marginally better
score when compared to the method of Kim and Ra proves to be better than all the other
methods in all benchmarks, except in the case of the Glass benchmark, see Table 4. For
the Glass benchmark the generalization performance seems to be better for the methods
of Boers-Kuiper, Bottou and Nguyen-Widrow compared to our LIT-A method. However, one should also take into account the number of successful experiments for each
method. Generalization “achieved” by the proposed method is superior in 47% of the
cases, while in 42% of the cases performance is the same with other methods and only
in 11% of the cases the proposed method delivered inferior performance to the training
algorithm. Only the method of Kim and Ra seems to give similar performance with the
proposed LIT-Approach.
4.1.3. Non-parametric Statistical Analysis and Posthoc Procedures
In order to comply with reported best practice in the evaluation of the performance
of neural networks, , we
evaluated the statistical signiﬁcance of the observed performance results applying the
Friedman test. This test ranks the performance of a set of k algorithms and can detect a
signiﬁcant diﬀerence in the performance of at least two algorithms. More speciﬁcally,
the Friedman test is a non-parametric statistical procedure similar to the parametric
two-way ANOVA used to test if at least two of the k samples represent populations
with diﬀerent medians. The null hypothesis H0 for Friedman‘s test states equality of
medians between the populations while the alternative hypothesis H1 is deﬁned as the
negation of the null hypothesis.
Table 6 uses two subtables to depict the average rankings computed through the
above statistical test for the convergence rate and the generalization performance. At
the bottom of each subtable we give the statistic of each test along with the corresponding p-value. The p-values computed strongly suggest rejection of the null hypothesis
at the α = 0.05 level of signiﬁcance. This means that the initialization algorithms have
some pattern of larger and smaller scores (medians) among them i.e. there exist significant diﬀerences among the considered algorithms.
The signiﬁcant diﬀerences detected by the above test procedure concern the overall
comparison of the algorithms as a set entailing that the performance of at least one
Table 4: Generalization performance results for the Suite 1 of the experiments
Benchmarks
Initialization Algorithms
British Vowels
* denotes that the mean value of the initialization method is signiﬁcantly diﬀerent from the mean value of LIT-A using
the indicated p-values (Sig.) computed by the posthoc analysis of the ANOVA results.
– denotes that the initialization method failed to meet the convergence criteria exceeding the maximum number of cycles
in all trials.
initialization algorithm diﬀers from the others. However, the Friedman test cannot provide information on which algorithms are diﬀerent from the others and so a multiple
comparison analysis needs to be conducted. For the sake of our evaluation we need to
carry out a multiple comparisons analysis between performance of the LIT-Approach
and performance of each one of the other initialization methods. This is a multiple
comparisons (pairwise) analysis with a control algorithm which
results in formulating k −1 hypotheses one for each of the k −1 comparisons, where
in our case k = 7. A better performance for the convergence rate of an algorithm trans-
Table 5: Summary of pairwise comparisons score for each method for the Suite 1 of the experiments
Initialization
Convergence rate
Generalization
lates here to a smaller number of epochs and a better performance for generalization is
taken to be a smaller classiﬁcation or approximation error. So, the objective of the tests
is minimization and in consequence the control procedure is automatically selected to
be the algorithm with the lowest ranking score. This algorithm is LIT-Approach for
both performance measures, see Table 6.
For the non-parametric test (Friedman) used we consider the ranking scores com-
Table 6: Average ranking achieved by the Friedman test (Suite 1 of the experiments)
Initialization
Convergence Rate
Generalization
puted for each algorithm. Then the posthoc analysis aims in determining if the diﬀerence between the ranking score of the proposed LIT-Approach and the ranking score of
each of the other algorithms are signiﬁcantly diﬀerent. The test statistic z and the corresponding p-value for comparing LIT-Approach and each of the other algorithms are
computed using the online STATService environment. The p-value
(2-tailed) corresponding to the z-statistic of each comparison is determined using normal approximation and can be compared with some appropriate level of signiﬁcance
However, these p-values are not suitable for multiple comparisons as they do not
account for the Family-Wise Error Rate (FWER) produced by accumulation of Type
I error in the case of a family of hypotheses associated with the multiple comparisons
tests . To cope with this matter, instead of using posthoc procedures to adjust the level of signiﬁcance α, we choose to compute the adjusted p-values
(APVs) corresponding to the Holm (Bonferroni-Holm) and the Benjamini-Hochberg
adjustment methods. Information on these adjustment methods can be found in R-
Documentation and references cited therein. These APVs can be used to test
the corresponding hypotheses i.e. to compare corresponding algorithms directly with
any signiﬁcance level α and give a “metric” of how diﬀerent these algorithms are .
The unadjusted and the adjusted p-values for the pairwise comparison of the proposed algorithm with each one of the other methods are presented in Table 7 for both
convergence rate and generalization. Note that the precision retained for the p-values
given in this Table and all similar Tables hereafter is up to the fourth decimal digit.
The adjusted p-values for the Friedman test in this Table show signiﬁcant diﬀerence
between the ranking of the LIT-Approach and the other methods for both convergence
rate and generalization. This translates to an improvement of the LIT-Approch over all
the other weight initialization algorithms.
The computations necessary for Table 6 as well as for all similar Tables hereafter in
this paper were carried out using STATService . Computations for
Table 7 as well as for all similar Tables hereafter were executed using the R environment for Statistical Data Analysis. Finally, it is worth noting that the results obtained
using the non-parametric statistical analysis conﬁrm those provided by ANOVA.
4.1.4. Comments and Remarks
Despite the stochastic nature of the training scheme adopted for this suite we may
argue that the results obtained are suggestive of the potential oﬀered by the proposed
Table 7: p-values of multiple comparisons (Suite 1 of the experiments)
Initialization
Unadjusted
Bonferroni-Holm
Benjamini-Hochberg
Convergence rate (Control algorithm is LIT-A)
Generalization (Control algorithm is LIT-A)
method. In terms of convergence success (Table 2) the proposed method seems to contribute to the best score for the training algorithm. Moreover, the advantage oﬀered
by the proposed method to achieve better convergence rate is manifested by the results
given in Table 3 and Table 5. Lastly, one may easily notice that in terms of generalization performance the proposed method though marginally superior when compared,
using ANOVA, to the Kim-Ra it proves to be better than all the other methods in all
benchmarks, except the Glass benchmark, see Tables 4 and 5. These conclusions are
strongly supported by the non-parametric statistical analysis with the Friedman test.
Though these results are indicative and for comparison purposes, they provide signiﬁcant evidence regarding the eﬃciency of the proposed method.
4.2. Function approximation
4.2.1. Setup of the Experiments
In order to cover the whole range of problems for which MLPs are used, one needs
to consider the problem of approximating analytically deﬁned non-linear functions.
This constitutes a necessary prerequisite for a “fair” comparison ﬁrst and foremost
with the method of Nguyen and Widrow, as these researchers initially demonstrated
their method on a function approximation problem. The functions used as benchmarks
are deﬁned in the following paragraphs.
Function 1. The function for this benchmark is the one reported in the original paper
of Nguyen & Widrow ,
y = 0.5 sin
sin (2πx2).
The network used here, is a 3-layer (2-21-1) architecture with the hyperbolic tangent
activation function for the hidden layer nodes as well as for the output node. A total of
625 (= 25 × 25) points are randomly selected, using uniform ditribution, in the interval
[−1, 1] × [−1, 1]. Among these points 450 are used for training and 175 for testing the
Function 2. The function considered here is a variant of the function considered in
Yam & Chow . This function is a mapping of eight input variables, taken
in the interval , into three output ones deﬁned by the following three equations:
y1 = (x1x2 + x3x4 + x5x6 + x7x8) /4
y2 = √(x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8)/8
y3 = (1 −y1)1/3
For this benchmark, a 8-12-3 network architecture was used with logistic activation
functions for nodes of the hidden and the output layer. A set of 75 patterns is formed
by randomly sampling, with uniform distribution, values for the input variables and
calculating output values. Among these input-output patterns, 50 are used for training
the network and 25 for testing, as in Yam & Chow .
Function 3. The function used here is a real-valued non-linear function of two variables, taken in the interval [−1, 1], deﬁned by the formula:
y = sin (2πx1x2)/(2πx1x2).
A 3-layer network architecture with 30 nodes in the hidden layer was adopted for this
benchmark. All nodes in the hidden layer as well as the output node have a hyperbolic
tangent activation function. The training set is formed by taking 320 patterns of the
total 400 (= 20 × 20) that are randomly selected using uniform distribution. The rest
80 patterns constitute the test set.
A total of 21 training experiments were executed for the above 3 functions and the
7 (LIT-A plus other six) weight initialization methods considered in this section. Each
training experiment is made up of a hundred (100) initial weight vectors derived using
one of the weight initialization methods. Networks in all experiments are trained using
the Levenberg-Marquardt method (LM), . The performance goal for the network output error is set to 1.0e−04
for Functions 1 and 2, and 1.0e −03 for Function 3. If the performance goal is not
met when a maximum number of 1000 epochs is reached then the training stops. The
learning rate for all experiments of Function 1 is set to 0.1 and 0.5 for the other two
benchmarks. Results of the training experiments are reported in Tables 8 – 10 hereafter.
For the benchmarks Function1 and Function3 LIT-A was applied using 1.5σxi for
the term sxi. This choice is based on the assumption that the input data are approximately normally distributed, and therefore sxi in the LIT-Approach was “roughly” approximated using 1.5σxi instead of the third quartile Q3.
4.2.2. Analysis of the Results
The results obtained regarding the performance measures set are shown in Tables
8–10. A rough observation of these results shows that the proposed LIT-Approach remains on top of the other methods as in the previous suite 1 concerning convergence
rate (Table 9) while being among the best methods regarding convergence succes (Table 8) and generalization (Table 10).
Comparison between the performance of the diﬀerent initialization methods, for
Table 8: Convergence success results for the Function Approximation benchmarks
Benchmarks
Initialization Methods
Function 1
Function 2
Function 3
Table 9: Convergence rate results for the Function Approximation benchmarks
Benchmarks
Initialization Algorithms
Function 1
Function 2
Function 3
Table 10: Generalization performance results for the Function Approximation benchmarks
Benchmarks
Initialization Algorithms
Function 1
Function 2
Function 3
the function mapping experiments, is carried out using the non-parametric Friedman
test. The average rankings computed are reported in Table 11. The hypotheses of the
Friedman test and the posthoc procedures performed are the same as those for the suite
1 of the experiments. The unadjusted and the adjusted p-values of the posthoc procedures are given in Table 12. The fact that the control procedure for the convergence rate
is LIT-Approach together with the p-values in Table 12 underline the superiority of the
proposed method. On the other hand the algorithm of Bottou is considered to be the
control procedure for the generalization. This does not prove that Bottou’s method performs better than LIT-Approach as the corresponding unadjusted and adjusted p-values
denote that the performance of these algorithms is the same.
Table 11: Average ranking achieved by the Friedman test (Function Approximation benchmarks)
Initialization
Convergence Rate
Generalization
Table 12: p-values of multiple comparisons (Function Approximation benchmarks)
Initialization
Unadjusted
Bonferroni-Holm
Benjamini-Hochberg
Convergence rate (Control algorithm is LIT-A)
Generalization (Control algorithm is Bottou)
4.2.3. Comments and Remarks
In terms of convergence success (Table 8) all initialization methods seem to have
similar performance while the method of Bottou gives the best results as the networks
initialized with this method are trapped in local minima for only 9 trials (2% of the
total number of trials). The results reported in Table 9 support the improvement in
convergence rate oﬀered by the proposed method, when compared with other weight
initialization methods in the context of these function approximation problems. Con-
cerning generalization, results in Table 10 indicate that the performance observed for
the LIT-Approach seems to be among the best of all methods. These remarks are con-
ﬁrmed by the results of the Friedman test in Table 11 and the posthoc procedures in 12,
especially regarding the convergence rate.
On the other hand generalization performance of the LIT-Approach is found to
be marginally weaker than the performance of Bottou’s method. Compared with the
results of suite 1 performance of the LIT-Approach in terms of generalization is considered here suboptimal. Various reasons may account for this. One reason is the fact
that the distribution of the input data for benchmarks Function1 and Function3 was
considered to be the normal and therefore it was “roughly” approximated using the
term 1.5σxi. Actually, it seems that deﬁning weight intervals with diﬀerent ranges for
the input variables seems meaningless for function approximation problems as these
are diﬀerent from classiﬁcation ones. Moreover, speaking about ouliers and extreme
input data in the case of a function approximation benchmark is useless. However, in
the context of the LIT-Approach we have not considered that function approximation
should be treated as a special case. So, it remains as an open issue for further investigation, in the case of function approximation, the estimation of the optimal value k for the
term 1.5σxi, given a speciﬁc function approximation problem. However, these claims,
as well as others, need to be further investigated with more benchmarks together with
taking into account the whole input set of training patterns.
What is noteworthy here is the poor generalization performance achieved by the
network when initialized with the Nguyen-Widrow method; in this case, it is very
likely that the network gets trapped in local minima. One should note that, seemingly,
the Nguyen-Widrow method showed notable performance in convergence rate when
experimenting with the non-linear Function 1, which is reported in the original paper
by Nguyen and Widrow .
4.3. Suite 2 of Experiments
4.3.1. Experimental Setup
Setting up this suite is motivated by the importance the research community has
devoted to the weight initialization method of Nguyen and Widrow. In addition, pop-
ular neural network packages, such as the Neural Network Toolbox of MATLAB and
the Encog Neural Network Framework , use this technique as the default initialization algorithm for neural networks.
The datasets used for these experiments and basic features of the problems are
brieﬂy outlined here. More details on these benchmarks can be found in the UCI
repository of machine learning database and references cited
1. Iris classiﬁcation benchmark (inputs:4, outputs:3). This benchmark is known as
Fisher’s Iris problem. Based on the values of sepal length and width, petal length
and width, the class of iris plant needs to be predicted. The data set contains 3
classes of 50 instances each, where each class refers to a type of iris plant. The
training set used consists of 120 examples and the test set of 30 examples.
2. Pima Indians Diabetes problem (inputs:8, outputs:2). The aim of this real worldclassiﬁcation task is to decide when a Pima Indian individual is diabetes positive
or not. The values of the input attributes represent personal data and result from a
medical examination. The dataset consists of a total of 768 patterns. The training
set used consists of 576 patterns and the test set of the 192 remaining patterns.
3. Thyroid classiﬁcation problem (inputs:21, outputs:3). Based on patient query
data and patient examination data the task is to decide whether the patient’s thyroid has over function, normal function, or under function. Using the original
thyroid1 dataset the training set is made from 5400 patterns while the test set is
made from 1800 patterns.
4. Yeast classiﬁcation problem (inputs:8, outputs:10). Yeast is a relatively complicated organism possessing diﬀerent types of proteins, related to the cytoskeletal structure of the cell, the nucleus organization, membrane transporters and
metabolic related proteins (as mitochondrial proteins). After the necessary preprocessing, Yeast data is found to include 1453 patterns, that is, there are 1453
proteins labeled according to 10 sites. As the data set is radically imbalanced,
the training set was generated by randomly selecting approximately the 70% of
patterns from each of the 10 sites, giving a total of 1019 training patterns. The
rest of the patterns, i.e. 434, were included in the test set.
5. Gene2 classiﬁcation (inputs:120, outputs:3). This is a binary problem with 120
input attributes and 3 output classes. The goal of this classiﬁcation task is to
decide, from a window of 60 DNA sequence elements (nucleotides), whether the
middle is either an intron/exon boundary (a donor), or an exon/intron boundary
(an acceptor), or none of them. The dataset for this problem was created based on
the Splice-junction Gene Sequences dataset from the UCI repository, . It consists of 2990 patterns (duplicates are excluded) and it is
partitioned to form the training set (2243 patterns) and the test set (747 patterns).
The original datasets were preprocessed to eliminate duplicate patterns and values were
scaled to match requirements set by the weight selection procedures. These operations
were performed according to PROBEN1 guidelines, . Unless otherwise
stated, the training sets used are made with 75% of the patterns of the initial dataset
and the test sets with the rest 25% of the patterns.
For each benchmark a neural network architecture was deﬁned. Batch processing
was used for training with ﬁve well known training algorithms, namely, the Adaptive
gradient descent with momentum (AGDM) , Resilient back-propagation (RBP) , the Levenberg-Marquardt
method (LM) , Scaled
conjugate gradient (SCG) and the Broyden-Fletcher-Goldfarb-Shanno
method (BFGS) . For each benchmark a set of a thousand (1000)
initial weight vectors was created by each one of the two initialization methods and
used in all experiments. A total of 50 (= 5 benchmarks × 5 training algorithms × 2
initialization methods) experiments were carried out, giving a total of 25000 weights
for each initialisation method. Architectures of the networks and training parameters
used are those reported in the literature to be the most appropriate for each problem; see Table 13. Note that the value of the momentum coeﬃcient
was set to 0.5 and all networks are fully connected without intra-layer or supra-layer
connections.
Table 13: Architectures of networks and training parameters used for the Suite 2 of experiments
Activation
architecture
* logsig denotes the logistic sigmoid function and tansig is the hyperbolic tangent
† Learning rate is the initial learning rate used for training
‡ Training stops when at least one of the following conditions is satisﬁed, Max epochs is reached, Error of the network
output becomes lower than or equal to Goal for MSE, Min gradient is reached during training.
Table 14: Convergence success results for the Suite 2 of experiments
Training Algorithms
All numbers indicated are in the range 0 . . . 100 to denote the percentage of successful training trials
4.3.2. Analysis of the Results
The results obtained regarding the performance measures set are shown in Tables
14, 15 and 16. A rough observation of these results shows that the proposed LIT-
Approach delivers successful network performance for all parameters.
The statistical analysis, applied on results in Tables 15 and 16, concerns the comparison of the two initialization techniques using t-test for independent unpaired samples data. However, while the samples are independent by default, application of the
t-test assumes that these samples are drawn from normally distributed populations with
equal variances. The Shapiro-Wilk test was used to
test the normality assumption, with α = 0.05, while SPSS automatically uses Levene’s
test for equality of variances, . In all experiments, except one
(training Gene2 benchmark with the BFGS algorithm for generalization performance)
the hypothesis of normality is rejected (p-value < 0.05) for the results of both weight
initialization methods and so the non-parametric Mann-Whitney test for independent
samples is used to compare equality of medians. The statistical signiﬁcance of the
comparison, that is the p-value (Sig.) indicated underneath the results for every pair of
experiments, is the one calculated by the Mann-Whitney test (or the t-test, when the
normality assumption is validated), .
Results in Table 15, are indicative that in all cases, except in two of them, the pro-
Table 15: Convergence rate results for the Suite 2 of experiments
Training Algorithms
posed method produced initial weight vectors which permitted faster convergence of
the training algorithm in use. In the other two cases, the Iris and the Yeast benchmarks,
the two weight initialization methods seem to allow for the same performance of the
BFGS training algorithm.
With regards to generalization, results are presented in Table 16 and denote the percentage of successfully classiﬁed unknown patterns. In the case of Gene2 benchmark
for networks trained with the BFGS algorithm, mean values (marked with *) are used
instead of medians, as the Shapiro-Wilk test approved the normality of populations
which allowed us to use the t-test.
Results, in Table 16, show that the proposed initialization method led to networks
that generalize better in about 50% of the cases (12 out of the 25 comparisons) while
the Nguyen-Widrow initialization technique was superior in only 3 cases. One may
notice inconsistency between the p-values indicating signiﬁcant diﬀerence between the
medians while these medians appear, for the Iris problem and some training algorithms,
to be the same. Indeed, the values of the medians suggest that the two methods display
the same performance in terms of generalization. However, for the NW algorithm the
score of 95.56% is taken over 18.2% of successful trials while for the LIT-Approach
95.56% corresponds to 100% of successful trials.
4.3.3. Comments and Remarks
The proposed method demonstrates better performance than the one proposed by
Nguyen and Widrow in all cases. However, despite the fact that these benchmarks
concern classiﬁcation problems while the method of Nguyen and Widrow was originally demonstrated for a function approximation problem, we believe that, what really
aﬀects performance of this method is the neural networks architecture itself and not
the type of the problem at hand. The magniﬁcation factor multiplying the randomly
selected input-to-hidden layer weights for a (2-21-1) network is given by the formula
0.7H1/N = 211/2 ≃4.5 where H denotes the number of hidden layer nodes. Weights
of this magnitude seem to deﬁne in the weight space a starting point which accelerates
convergence of the training algorithm.
On the other hand, it is easy to notice that when the network has a diﬀerently
Table 16: Generalization performance results (%) for the Suite 2 of experiments
Training Algorithms
(%) all numbers in this table denote percentage.
† denotes that with the BFGS algorithm, mean values are used instead of medians, as the Shapiro-Wilk test approved the normality of populations which allowed us to use the t-test.
shaped architecture, that is, the number of input nodes is higher than the number of
hidden nodes, the factor H1/N tends to reduce to 1. In consequence the initialization
method tends to degenerate to the commonly used random weight selection in the in-
terval [−1, 1]. Hence, performance of the Nguyen-Widrow method strongly depends
on the number of nodes of the input and the hidden layers. This argument may be
experimentally conﬁrmed by progressively training a network while gradually increasing the number of hidden layer nodes. Some training trials we performed on the Iris
problem showed that convergence rate of the Nguyen-Widrow method increases when
increasing the hidden layer units. Nevertheless, the price to pay for this is the decrease
in generalization performance.
4.4. Suite 3 of Experiments
4.4.1. Experimental Setup
The objective of this suite is twofold. Firstly, it was set up in order to test the ability of the LIT-Approach to deal with problems having a big number of features and
thus see if the assumptions underlying the method remain valid when the method addresses large real life problems. The second objective is to test the performance of the
LIT-Approach against more recent competitors that do not belong to the “family” of
methods which randomly select initial weights from some predeﬁned interval. Among
such methods we retained the following:
Linear-Least-Squares initialization of MLPs through backpropagation of the desired
response (LLSQ). The method uses a technique for backpropagating the desired response of an MLP through its nonlinear layers. This permits to train each layer by
solving a linear system of equations per layer which is the solution to a linear least
squares problem. The authors claim that besides initialization the method can be used
for training the network .
Computing Linear-Least-Squares layer by layer in forward direction (FLLS). The outputs of the ﬁrst hidden layer are assigned with random numbers in the active region of
the activation function of the nodes. The inverses of these output values are computed
and a linear least squares problem is solved to deﬁne the weights of the input to the
hidden layer. Using these weights and the input patterns, actual outputs are computed
and used to repeat the process towards the next layer until the output layer. The method
determines the initial weights by successively solving one linear least squares problem
per layer in a feed forward way .
Particle Swarm Optimization based weight initialization (PSOI). The method was initially proposed in van den Bergh . Particle Swarm Optimization is used to deﬁne
the most pertinent initial weights which are then used for subsequent training by BP.
Actually, PSO performs pretraining of the MLP for some iterations before activating
BP. The method is an evolutionary approach to weight initialization which, however,
suﬀers itself from the initialization problem. In our experiments PSO is activated for
20 iterations and the weights computed are used, in the sequel, by the online BP.
Hereafter we will refer to these methods using their acronyms. In addition to the
above in this suite we used the method of Bottou. Hence we form a complete test
of comparisons between ﬁve methods; the previous three and the two methods that
had the best performance in the other test suites. It is important to note that in this
suite the algorithm implementing LIT-Approach uses the third quartile of the input
data Q3 instead of some multiple of the standard deviation. Moreover, the weights of
the hidden-to-output nodes are computed using assumptions introduced in subsection
3.3. The benchmarks used for this test suite are deﬁned hereafter in alphabetical order.
1. Far-infrared Laser (FIL) (inputs:50, outputs:1). This is an extension of the Data
Set A from the Santa Fe Competition Data consisting of sampled values from the emission intensity of far-infrared laser (NH3-
FIR) . It is a time series forecasting problem and the aim
is to predict the intensity of a far infared laser at a particular moment from past
samples. In our tests we choose to predict the value of the quantity x at time k+1
given the past 50 samples xk, xk−1, · · · , xk−49 as in Yam & Chow . The total number of patterns is 10043. The training set was made with 8000 patterns
and the rest 2043 were used for the test set.
2. Landsat Satellite Data (LSAT) (inputs:36, outputs:6). This dataset was generated
taking a small section (82 rows and 100 columns) from the original Landsat data.
The dataset consists of the multi-spectral values of pixels in 3x3 neighborhoods
in a satellite image. The aim is to predict the classiﬁcation associated with the
central pixel in each neighborhood. Each line in the data contains 36 values, that
is, the pixel values in the four spectral bands times the 9 pixels in the 3x3 neighbourhood. The classiﬁcation label of the central pixel is a number corresponding
to one of the seven classes. Note that class 6 has no examples in this dataset.
The total number of 6435 patterns available was partitioned in the training set
composed of 4435 patterns and the test set having 2000 patterns.
3. Multiple Features Data Set (MFEAT) (inputs:649, outputs:10). This dataset was
created by Robert P.W. Duin, Dept. of Applied Physics, Univ. of Delft. It
consists of features of handwritten digits (‘0’–‘9’) extracted from a collection of
Dutch utility maps. A number of 200 patterns per class, that is a total of 2000
patterns have been digitized in binary images. These digits are represented in
terms of the following six feature sets given in separate ﬁles:
- mfeat-fou: 76 Fourier coeﬃcients of the character shapes
- mfeat-fac: 216 proﬁle correlations
- mfeat-kar: 64 Karhunen-Lo`eve coeﬃcients
- mfeat-pix: 240 pixel averages in 2 x 3 windows
- mfeat-zer: 47 Zernike moments
- mfeat-mor: 6 morphological features
The 2000 patterns, contained in each ﬁle, are stored in ASCII on 2000 lines. The
ﬁrst 200 patterns correspond to class ‘0’, the next 200 to class ‘1’, that is, sets of
200 patterns for each of the classes ‘0’–‘9’. The training set for our experiments
consists of 1500 patterns and the test set has 500 patterns.
4. The MNIST database of handwritten digits (inputs:784, outputs:10). The MNIST
database was constructed from NIST’s Special Database 1 (SD-1) and Special
Database 3 (SD-3), which contain binary images of handwritten digits . The MNIST training set has a total of 60000 patterns, that is, 30000
patterns from SD-1 and 30000 patterns from SD-3. The test set is composed of
5000 patterns from SD-1 and 5000 patterns from SD-3 that is a total of 10000
patterns. The sets of writers of the training set and test set were disjoint. For
performance reasons of our experiments we formed a training set consisting of
10% of the patterns of the original training set and a test set with 10% of the
patterns of the original test set. For every class in the database we selected the
ﬁrst 10% of the patterns belonging to this class thus forming a balanced sample
of the original dataset. So the training set for our experiments consists of 6000
patterns and the test set has 1000 patterns.
A total of 18 (16+2) training experiments were executed for the above 4 benchmarks
and the 4 (LIT-A, Bottou, FLLS, PSOI) weight initialization methods considered in
this subsection. The other 2 experiments concern the LLSQ method which was tested
only against the FIL and LSAT benchmarks. Each training experiment is made up of
a hundred (100) initial weight vectors derived using one of the weight initialization
methods. The same network architecture was initialized with these vectors and trained
using online BP. The network architecture and the training parameters, used in this
arrangement, are reported in Table 17. Benchmarks are listed in increasing order of the
number of features using their acronyms.
Table 17: Architectures of networks and training parameters used for the Suite 3 of the experiments
Activation
Convergence
Input data
architecture
649-649-10
784-300-10
† logsig denotes the logistic sigmoid function and tansig is the hyperbolic tangent
‡ Landsat benchmark uses the hyperbolic tangent for the hidden layer nodes and the logistic sigmoid for nodes
in the output layer. All other networks use the same activation function for all nodes.
4.4.2. Analysis of the Results
Tables 18, 19 and 20 report the experimental results on the benchmarks for the performance measures considered. The symbol – is used in these Tables to denote that the
corresponding initialization method failed to meet the convergence criteria exceeding
the maximum number of cycles in all trials. A quick look at these results shows that the
three newly introduced initialization methods have very poor performane especially in
the case of the benchmarks with a big number of features. We need to note that regarding the method LLSQ these tables report the results only for the ﬁrst two benchmarks
that is LSAT and FIL.
The comparison between the performance of the initialization methods was carried
out using ANOVA for the LSAT and FIL benchmarks. The diﬀerence between the
proposed LIT-Aproach and the other methods is indicated with a * and supported by
the corresponding p-value (Sig.). In addition comparison of the initialization methods
in these benchmarks is carried out using the non-parametric test of Friedman and the
posthoc procedures of Bonferroni-Holm and Benjamini-Hochberg in the same context
as for the suite 1 of the experiments. The average rankings computed are reported in
Table 21. These rankings roughly conﬁrm the results observed regarding the mean
values of the performance parameters. Pairwise comparison results reported in Table 22 reward the performance of LIT-Approach in terms of convergence rate while
they reveal that the proposed method has the same performace with Bottou’s method
regarding generalization.
Table 18: Convergence success results in 100 trials for the Suite 3 of the experiments
Initialization Algorithms
4.4.3. Comments and Remarks
The results of these experiments coincide with those already obtained in the previous suites. As seen above the LIT-Approach is dominant in terms of convergence speed
and seems to be equal, or at most slightly weaker, in terms of generalization compared
with Bottou’s method. The method of Bottou is generally powerful while it seems to
Table 19: Convergence rate results for the Suite 3 of the experiments
Benchmarks
Initialization Algorithms
∗denotes that the mean value of the initialization method is signiﬁcantly diﬀerent
from the mean value of LIT-A using the indicated p-values (Sig.) computed by
the posthoc analysis of the ANOVA results.
– denotes that the initialization method failed to meet the convergence criteria exceeding the maximum number of cycles in all trials.
** denotes that the initialization method was not tested for this benchmark.
be weaker when addressing problems with big number of features such as MFEAT and
MNIST. These remarks are also supported by the non-parametric statistical analysis
tests, Tables 21 and 22, for both performance characteristics. What is disarming is the
seemingly bad performance of the other methods. We need to note here that in these
experiments we do take into account the resources needed in terms of time and memory for an initialization procedure to run. The reason is that the weight initialization
methods based on random weight selection need very little memory to run and they
preprocess the input patterns only in order to extract simple statistics of the sample.
On the other hand the memory and time requirements set by methods not based on
random weight selection constitute a serious barrier for their application in real life
problems. Finally, besides the limitations analyzed in subsection 3.2, LIT-Approach
seems to perform very well even in the case of the selected real life problems with big
Table 20: Generalization performance results for the Suite 3 of the experiments
(FIL in mean absolute error)
Benchmarks
Initialization Algorithms
∗denotes that the mean value of the initialization method is signiﬁcantly diﬀerent from
the mean value of LIT-A using the indicated p-values (Sig.) computed by the posthoc
analysis of the ANOVA results.
– denotes that the initialization method failed to meet the convergence criteria exceeding
the maximum number of cycles in all trials.
** denotes that the initialization method was not tested for this benchmark.
Table 21: Average ranking achieved by the Friedman test (Suite 3 of the experiments)
Initialization
Convergence Rate
Generalization
number of patterns. It is worth noting that we have not considered benchmarks with
even higher number of features as these are normally treated with feature selection
and/or dimensionality reduction methods before applying a classiﬁcation method that
Table 22: p-values of multiple comparisons (Suite 3 of the experiments)
Initialization
Unadjusted
Bonferroni-Holm
Benjamini-Hochberg
Convergence rate (Control algorithm is LIT-A)
Generalization (Control algorithm is Bottou)
requires parameter initialization.
5. Conclusion
In this paper we studied an interval analysis approach for neural network weight
initialization with the aim to deal with uncertainty about the initial weights. Instead of
algebraically solving a linear interval system we formulated and solved a linear interval tolerance problem. Hence, a self contained standalone algorithm is proposed that
inherently includes major concepts such as: the number of inputs to a node in the ﬁrst
hidden layer, the statistical information of the input data, eﬀective positioning of the
hyperplanes in the pattern space and full utilization of the dynamic range of the activation function. Both the theoretical analysis and the experimental results suggest that
the proposed LIT-Approach successfully tackles the problem of neural saturation while
avoiding false local minima.
The proposed LIT-Approach has been compared against other well known random
weight initialization techniques on a number of well known real world benchmarks.
The experiments carried out cover a broad range of problems using networks with
architectures of increasing complexity. The results obtained are suggestive of the ef-
ﬁciency of the proposed method while providing an overall classiﬁcation framework
for some of the most well known weight initialization methods. For all performance
characteristics set, the proposed method is either on top of other initialization methods
or at least exhibits similar performance with the other methods. Moreover, it is easy
to notice that the proposed method demonstrates stable inter-problem performance behaviour. We believe that all these features make the proposed method a reliable algorithm for use in real life problems.
Solving the linear interval tolerance problem seems to successfully address the
problem of weight initialization. However, some important questions need to be investigated in future research. These questions concern the possibility to deﬁne among
all solutions in the Tolerance solution set, the optimal one, if any, for the weight initialization problem; the evaluation of diﬀerent algorithms solving the linear interval
tolerance problem for weight initialization, and others, such as the potential oﬀered by
such an approach to other types of neural networks.
Acknowledgment
The authors would like to thank the anonymous reviewers for their valuable suggestions and comments on earlier draft of the manuscript, that helped to signiﬁcantly
improve the paper at hand. Special thanks are due to Prof. Erdogmus for kindly oﬀering
the software implementing the method LLSQ, as well as to Mr. George Dimakopoulos,
statistical analyst at the Technological Educational Institute of Epirus, for his helpful
comments regarding ANOVA.