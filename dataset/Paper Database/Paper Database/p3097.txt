Progressive Neural Networks
Andrei A. Rusu*, Neil C. Rabinowitz*, Guillaume Desjardins*, Hubert Soyer,
James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell
* These authors contributed equally to this work
Google DeepMind
London, UK
{andreirusu, ncr, gdesjardins, soyer, kirkpatrick, korayk, razp, raia}@google.com
Learning to solve complex sequences of tasks—while both leveraging transfer and
avoiding catastrophic forgetting—remains a key obstacle to achieving human-level
intelligence. The progressive networks approach represents a step forward in this
direction: they are immune to forgetting and can leverage prior knowledge via
lateral connections to previously learned features. We evaluate this architecture
extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze
games), and show that it outperforms common baselines based on pretraining and
ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs
at both low-level sensory and high-level control layers of the learned policy.
Introduction
Finetuning remains the method of choice for transfer learning with neural networks: a model is
pretrained on a source domain (where data is often abundant), the output layers of the model are
adapted to the target domain, and the network is ﬁnetuned via backpropagation. This approach was
pioneered in by transferring knowledge from a generative to a discriminative model, and has
since been generalized with great success . Unfortunately, the approach has drawbacks which
make it unsuitable for transferring across multiple tasks: if we wish to leverage knowledge acquired
over a sequence of experiences, which model should we use to initialize subsequent models? This
seems to require not only a learning method that can support transfer learning without catastrophic
forgetting, but also foreknowledge of task similarity. Furthermore, while ﬁnetuning may allow us
to recover expert performance in the target domain, it is a destructive process which discards the
previously learned function. One could copy each model before ﬁnetuning to explicitly remember all
previous tasks, but the issue of selecting a proper initialization remains. While distillation offers
one potential solution to multitask learning , it requires a reservoir of persistent training data for
all tasks, an assumption which may not always hold.
This paper introduces progressive networks, a novel model architecture with explicit support for transfer across sequences of tasks. While ﬁnetuning incorporates prior knowledge only at initialization,
progressive networks retain a pool of pretrained models throughout training, and learn lateral connections from these to extract useful features for the new task. By combining previously learned features
in this manner, progressive networks achieve a richer compositionality, in which prior knowledge is
no longer transient and can be integrated at each layer of the feature hierarchy. Moreover, the addition
of new capacity alongside pretrained networks gives these models the ﬂexibility to both reuse old
computations and learn new ones. As we will show, progressive networks naturally accumulate
experiences and are immune to catastrophic forgetting by design, making them an ideal springboard
for tackling long-standing problems of continual or lifelong learning.
The contributions of this paper are threefold. While many of the individual ingredients used in
progressive nets can be found in the literature, their combination and use in solving complex sequences
 
of tasks is novel. Second, we extensively evaluate the model in complex reinforcement learning
domains. In the process, we also evaluate alternative approaches to transfer (such as ﬁnetuning) within
the RL domain. In particular, we show that progressive networks provide comparable (if not slightly
better) transfer performance to traditional ﬁnetuning, but without the destructive consequences.
Finally, we develop a novel analysis based on Fisher Information and perturbation which allows us to
analyse in detail how and where transfer occurs across tasks.
Progressive Networks
Continual learning is a long-standing goal of machine learning, where agents not only learn (and
remember) a series of tasks experienced in sequence, but also have the ability to transfer knowledge
from previous tasks to improve convergence speed . Progressive networks integrate these
desiderata directly into the model architecture: catastrophic forgetting is prevented by instantiating
a new neural network (a column) for each task being solved, while transfer is enabled via lateral
connections to features of previously learned columns. The scalability of this approach is addressed
at the end of this section.
A progressive network starts with a single column: a deep neural network having L layers with
hidden activations h(1)
∈Rni, with ni the number of units at layer i ≤L, and parameters Θ(1)
trained to convergence. When switching to a second task, the parameters Θ(1) are “frozen” and a new
column with parameters Θ(2) is instantiated (with random initialization), where layer h(2)
input from both h(2)
i−1 and h(1)
i−1 via lateral connections. This generalizes to K tasks as follows: 1:
where W (k)
∈Rni×ni−1 is the weight matrix of layer i of column k, U (k:j)
∈Rni×nj are the lateral
connections from layer i −1 of column j, to layer i of column k and h0 is the network input. f is
an element-wise non-linearity: we use f(x) = max(0, x) for all intermediate layers. A progressive
network with K = 3 is shown in Figure 1.
Figure 1: Depiction of a three column progressive network. The ﬁrst two columns on the left (dashed arrows)
were trained on task 1 and 2 respectively. The grey box labelled a represent the adapter layers (see text). A third
column is added for the ﬁnal task having access to all previously learned features.
These modelling decisions are informed by our desire to: (1) solve K independent tasks at the end of
training; (2) accelerate learning via transfer when possible; and (3) avoid catastrophic forgetting.
In the standard pretrain-and-ﬁnetune paradigm, there is often an implicit assumption of “overlap”
between the tasks. Finetuning is efﬁcient in this setting, as parameters need only be adjusted
slightly to the target domain, and often only the top layer is retrained . In contrast, we make
no assumptions about the relationship between tasks, which may in practice be orthogonal or even
adversarial. While the ﬁnetuning stage could potentially unlearn these features, this may prove
difﬁcult. Progressive networks side-step this issue by allocating a new column for each new task,
whose weights are initialized randomly. Compared to the task-relevant initialization of pretraining,
1Progressive networks can also be generalized in a straightforward manner to have arbitrary network width
per column/layer, to accommodate varying degrees of task difﬁculty, or to compile lateral connections from
multiple, independent networks in an ensemble setting. Biases are omitted for clarity.
columns in progressive networks are free to reuse, modify or ignore previously learned features via
the lateral connections. As the lateral connections U (k:j)
are only from column k to columns j < k,
previous columns are not affected by the newly learned features in the forward pass. Because also the
parameters {Θ(j); j < k} are kept frozen (i.e. are constants for the optimizer) when training Θ(k),
there is no interference between tasks and hence no catastrophic forgetting.
Application to Reinforcement Learning.
Although progressive networks are widely applicable,
this paper focuses on their application to deep reinforcement learning. In this case, each column is
trained to solve a particular Markov Decision Process (MDP): the k-th column thus deﬁnes a policy
π(k)(a | s) taking as input a state s given by the environment, and generating probabilities over
actions π(k)(a | s) := h(k)
L (s). At each time-step, an action is sampled from this distribution and
taken in the environment, yielding the subsequent state. This policy implicitly deﬁnes a stationary
distribution ρπ(k)(s, a) over states and actions.
In practice, we augment the progressive network layer of Equation 2 with non-linear lateral connections which we call adapters. They serve both to improve initial conditioning and perform
dimensionality reduction. Deﬁning the vector of anterior features h(<k)
i−1 = [h(1)
i−1 · · · h(j)
i−1 · · · h(k−1)
of dimensionality n(<k)
i−1 , in the case of dense layers, we replace the linear lateral connection with a
single hidden layer MLP. Before feeding the lateral activations into the MLP, we multiply them by a
learned scalar, initialized by a random small value. Its role is to adjust for the different scales of the
different inputs. The hidden layer of the non-linear adapter is a projection onto an ni dimensional
subspace. As the index k grows, this ensures that the number of parameters stemming from the lateral
connections is in the same order as
. Omitting bias terms, we get:
i−1 + U (k:j)
where V (k:j)
∈Rni−1×n(<k)
i−1 is the projection matrix. For convolutional layers, dimensionality
reduction is performed via 1 × 1 convolutions .
Limitations.
Progressive networks are a stepping stone towards a full continual learning agent:
they contain the necessary ingredients to learn multiple tasks, in sequence, while enabling transfer
and being immune to catastrophic forgetting. A downside of the approach is the growth in number of
parameters with the number of tasks. The analysis of Appendix 2 reveals that only a fraction of the
new capacity is actually utilized, and that this trend increases with more columns. This suggests that
growth can be addressed, e.g. by adding fewer layers or less capacity, by pruning , or by online
compression during learning. Furthermore, while progressive networks retain the ability to solve
all K tasks at test time, choosing which column to use for inference requires knowledge of the task
label. These issues are left as future work.
Transfer Analysis
Unlike ﬁnetuning, progressive nets do not destroy the features learned on prior tasks. This enables
us to study in detail which features and at which depth transfer actually occurs. We explored two
related methods: an intuitive, but slow method based on a perturbation analysis, and a faster analytical
method derived from the Fisher Information .
Average Perturbation Sensitivity (APS).
To evaluate the degree to which source columns contribute to the target task, we can inject Gaussian noise at isolated points in the architecture (e.g. a
given layer of a single column) and measure the impact of this perturbation on performance. A
signiﬁcant drop in performance indicates that the ﬁnal prediction is heavily reliant on the feature map
or layer. We ﬁnd that this method yields similar results to the faster Fisher-based method presented
below. We thus relegate details and results of the perturbation analysis to the appendix.
Average Fisher Sensitivity (AFS).
We can get a local approximation to the perturbation sensitivity
by using the Fisher Information matrix . While the Fisher matrix is typically computed with
respect to the model parameters, we compute a modiﬁed diagonal Fisher ˆF of the network policy π
with respect to the normalized activations 2 at each layer ˆh(k)
. For convolutional layers, we deﬁne
ˆF to implicitly perform a summation over pixel locations. ˆF can be interpreted as the sensitivity of
the policy to small changes in the representation. We deﬁne the diagonal matrix ˆF, having elements
ˆF(m, m), and the derived Average Fisher Sensitivity (AFS) of feature m in layer i of column k as:
AFS(i, k, m)
where the expectation is over the joint state-action distribution ρ(s, a) induced by the progressive
network trained on the target task. In practice, it is often useful to consider the AFS score per-layer
AFS(i, k) = P
m AFS(i, k, m), i.e. summing over all features of layer i. The AFS and APS thus
estimate how much the network relies on each feature or column in a layer to compute its output.
Related Literature
There exist many different paradigms for transfer and multi-task reinforcement learning, as these
have long been recognized as critical challenges in AI research . Many methods for
transfer learning rely on linear and other simple models (e.g. ), which is a limiting factor to their
applicability. Recently, there have been new methods proposed for multi-task or transfer learning
with deep RL: . In this work we present an architecture for deep reinforcement learning
that in sequential task regimes that enables learning without forgetting while supporting individual
feature transfer from previous learned tasks.
Pretraining and ﬁnetuning was proposed in and applied to transfer learning in , generally
in unsupervised-to-supervised or supervised-to-supervised settings. The actor-mimic approach 
applied these principles to reinforcement learning, by ﬁne-tuning a DQN multi-task network on new
Atari games and showing that some responded with faster learning, while others did not. Progressive
networks differ from the ﬁnetuning direction substantially, since capacity is added as new tasks are
Progressive nets are related to the incremental and constructive architectures proposed in neural
network literature. The cascade-correlation architecture was designed to eliminate forgetting while
incrementally adding and reﬁning feature extractors . Auto-encoders such as use incremental
feature augmentation to track concept drift, and deep architectures such as have been designed
that speciﬁcally support feature transfer. More recently, in , columns are separately trained on
individual noise types, then linearly combined, and use columns for image classiﬁcation. The
block-modular architecture of has many similarities to our approach but focuses on a visual
discrimination task. The progressive net approach, in contrast, uses lateral connections to access
previously learned features for deep compositionality. It can be used in any sequential learning setting
but is especially valuable in RL.
Experiments
We evaluate progressive networks across three different RL domains. First, we consider synthetic
versions of Pong, altered to have visual or control-level similarities. Next, we experiment broadly
with random sequences of Atari games and perform a feature-level transfer analysis. Lastly, we
demonstrate performance on a set of 3D maze games. Fig. 2 shows examples from selected tasks.
We rely on the Async Advantage Actor-Critic (A3C) framework introduced in . Compared to
DQN , the model simultaneously learns a policy and a value function for predicting expected
future rewards. A3C is trained on CPU using multiple threads and has been shown to converge faster
than DQN on GPU. This made it a more natural ﬁt for the large amount of sequential experiments
required for this work.
2The Fisher of individual neurons (fully connected) and feature maps (convolutional layers) are computed
over ρπ(k)(s, a). The use of a normalized representation ˆh is non-standard, but makes the scale of ˆF comparable
across layers and columns.
(a) Pong variants
(b) Labyrinth games
(c) Atari games
Figure 2: Samples from different task domains: (a) Pong variants include ﬂipped, noisy, scaled, and recoloured
transforms; (b) Labyrinth is a set of 3D maze games with diverse level maps and diverse positive and negative
reward items; (c) Atari games offer a more challenging setting for transfer.
We report results by averaging the top 3 out of 25 jobs, each having different seeds and random
hyper-parameter sampling. Performance is evaluated by measuring the area under the learning curve
(average score per episode during training), rather than ﬁnal score. The transfer score is then deﬁned
as the relative performance of an architecture compared with a single column baseline, trained only
on the target task (baseline 1). We present transfer score curves for selected source-target games, and
summarize all such pairs in transfer matrices. Models and baselines we consider are illustrated in
Figure 3. Details of the experimental setup are provided in section 3 of the Appendix.
(1) Baseline 1
(2) Baseline 2
(3) Baseline 3
(5) Progressive Net
(6) Progressive Net
(4) Baseline 4
target task
source task
Figure 3: Illustration of different baselines and architectures. Baseline 1 is a single column trained on the target
task; baseline 2 is a single column, pretrained on a source task and ﬁnetuned on the target task (output layer
only); baseline 3 is the same as baseline 2 but the whole model is ﬁnetuned; and baseline 4 is a 2 column
progressive architecture, with previous column(s) initialized randomly and frozen.
The ﬁrst evaluation domain is a set of synthetic variants of the Atari game of Pong ("Pong Soup")
where the visuals and gameplay have been altered, thus providing a setting where we can be conﬁdent
that there are transferable aspects of the tasks. The variants are Noisy (frozen Gaussian noise is added
to the inputs); Black (black background); White (white background); Zoom (input is scaled by 75%
and translated); V-ﬂip, H-ﬂip, and VH-ﬂip (input is horizontally and/or vertically ﬂipped). Example
frames are shown in Fig. 2. The results of training two columns on the Pong variants, including all
relevant baselines are shown in Figure 4. Transfer scores are summarized over all target tasks in
Figure 4: (a) Transfer matrix. Colours indicate transfer scores (clipped at 2). For progressive nets, the ﬁrst
column is trained on Pong, Noisy, or H-ﬂip (table rows); the second column is trained on each of the other pong
variants (table columns). (b) Example learning curves.
We can make several observations from these results. Baseline 2 (single column, only output layer is
ﬁnetuned; see Fig. 3) fails to learn the target task in most experiments and thus has negative transfer.
This approach is quite standard in supervised learning settings, where features from ImageNet-trained
nets are routinely repurposed for new domains. As expected, we observe high positive transfer with
baseline 3 (single column, full ﬁnetuning), a well established paradigm for transfer. Progressive
networks outperform this baseline however in terms of both median and mean score, with the
difference being more pronounced for the latter. As the mean is more sensitive to outliers, this
suggests that progressive networks are better able to exploit transfer when transfer is possible (i.e.
when source and target domains are compatible). Fig. 4 (b) lends weight to this hypothesis, where
progressive networks are shown to signiﬁcantly outperform the baselines for particular game pairs.
Progressive nets also compare favourably to baseline 4, conﬁrming that progressive nets are indeed
taking advantage of the features learned in previous columns.
Detailed analysis
Figure 5: (a) Transfer analysis for 2-column nets on Pong variants. The relative sensitivity of the network’s
outputs on the columns within each layer (the AFS) is indicated by the darkness of shading. (b) AFS values
for the 8 feature maps of conv. 1 of a 1-column Pong net. Only one feature map is effectively used by the net;
the same map is also used by the 2-column versions. Below: spatial ﬁlter components (red = positive, blue =
negative). (c) Activation maps of the ﬁlter in (b) from example states of the four games.
We use the metric derived in Sec. 3 to analyse what features are being transferred between Pong
variants. We see that when switching from Pong to H-Flip, the network reuses the same components
of low and mid-level vision (the outputs of the two convolutional layers; Figure 5a). However, the
fully connected layer must be largely re-learned, as the policy relevant features of the task (the relative
locations/velocities of the paddle and ball) are now in a new location. When switching from Pong
to Zoom, on the other hand, low-level vision is reused for the new task, but new mid-level vision
features are learned. Interestingly, only one low-level feature appears to be reused: (see Fig. 5b): this
is a spatio-temporal ﬁlter with a considerable temporal DC component. This appears sufﬁcient for
detecting both ball motion and paddle position in the original, ﬂipped, and zoomed Pongs.
Finally, when switching from Pong to Noisy, some new low-level vision is relearned. This is likely
because the ﬁrst layer ﬁlter learned on the clean task is not sufﬁciently tolerant to the added noise.
In contrast, this problem does not apply when moving from Noisy to Pong (Figure 5a, rightmost
column), where all of vision transfers to the new task.
Atari Games
We next investigate feature transfer between randomly selected Atari games . This is an interesting
question, because the visuals of Atari games are quite different from each other, as are the controls
and required strategy. Though games like Pong and Breakout are conceptually similar (both involve
hitting a ball with a paddle), Pong is vertically aligned while Breakout is horizontal: a potentially
insurmountable feature-level difference. Other Atari game pairs have no discernible overlap, even at
a conceptual level.
To this end we start by training single columns on three source games (Pong, River Raid, and
Seaquest) 3 and assess if the learned features transfer to a different subset of randomly selected
target games (Alien, Asterix, Boxing, Centipede, Gopher, Hero, James Bond, Krull, Robotank, Road
Runner, Star Gunner, and Wizard of Wor). We evaluate progressive networks with 2, 3 and 4 columns,
3Progressive columns having more than one “source” column are trained sequentially on these source games,
i.e. Seaquest-River Raid-Pong means column 1 is ﬁrst trained on Seaquest, column 2 is added afterwards and
trained on River Raid, and then column 3 added and trained on Pong.
Figure 6: Transfer scores and example learning curves for Atari target games, as per Figure 4.
Median (%)
Median (%)
Median (%)
Baseline 1
Baseline 2
Baseline 3
Baseline 4
Progressive 2 col
Progressive 3 col
Progressive 4 col
Table 1: Transfer percentages in three domains. Baselines are deﬁned in Fig. 3.
comparing to the baselines of Figure 3). The transfer matrix and selected transfer curves are shown
in Figure 6, and the results summarized in Table 1.
Across all games, we observe from Fig. 6, that progressive nets result in positive transfer in 8 out
of 12 target tasks, with only two cases of negative transfer. This compares favourably to baseline
3, which yields positive transfer in only 5 of 12 games. This trend is reﬂected in Table 1, where
progressive networks convincingly outperform baseline 3 when using additional columns. This is
especially promising as we show in the Appendix that progressive network use a diminishing amount
of capacity with each added column, pointing a clear path to online compression or pruning as a
means to mitigate the growth in model size.
Now consider the speciﬁc sequence Seaquest-to-Gopher, an example of two dissimilar games. Here,
the pretrain/ﬁnetune paradigm (baseline 3) exhibits negative transfer, unlike progressive networks
(see Fig.6b, bottom), perhaps because they are more able to ignore the irrelevant features. For the
sequence Seaquest[+River Raid][+Pong]-to-Boxing, using additional columns in the progressive
networks can yield a signiﬁcant increase in transfer (see Fig. 6b, top).
Detailed Analysis
Figure 6 demonstrates that both positive and negative transfer is possible with progressive nets. To
differentiate these cases, we consider the Average Fisher Sensitivity for the 3 column case (e.g., see
Fig. 7a). A clear pattern emerges amongst these and other examples: the most negative transfer
coincides with complete dependence on the convolutional layers of the previous columns, and no
learning of new visual features in the new column. In contrast, the most positive transfer occurs
when the features of the ﬁrst two columns are augmented by new features. The statistics across all
3-column nets (Figure 7b) show that positive transfer in Atari occurs at a "sweet spot" between heavy
reliance on features from the source task, and heavy reliance on all new features for the target task.
At ﬁrst glance, this result appears unintuitive: if a progressive net ﬁnds a valuable feature set from a
source task, shouldn’t we expect a high degree of transfer? We offer two hypotheses. First, this may
simply reﬂect an optimization difﬁculty, where the source features offer fast convergence to a poor
local minimum. This is a known challenge in transfer learning : learned source tasks confer an
inductive bias that can either help or hinder in different cases. Second, this may reﬂect a problem of
Figure 7: (a) AFS scores for 3-column nets with lowest (left) and highest (right) transfer scores on the 12 target
Atari games. (b) Transfer statistics across 72 three-column nets, as a function of the mean AFS across the three
convolutional layers of the new column (i.e. how much new vision is learned).
exploration, where the transfered representation is "good enough" for a functional, but sub-optimal
The ﬁnal experimental setting for progressive networks is Labyrinth, a 3D maze environment where
the inputs are rendered images granting partial observability and the agent outputs discrete actions,
including looking up, down, left, or right and moving forward, backwards, left, or right. The tasks as
well as the level maps are diverse and involve getting positive scores for ‘eating’ good items (apples,
strawberries) and negative scores for eating bad items (mushrooms, lemons). Details can be found
in the appendix. While there is conceptual and visual overlap between the different tasks, the tasks
present a challenging set of diverse game elements (Figure 2).
Figure 8: Transfer scores and example learning curves for Labyrinth tasks. Colours indicate transfer (clipped at
2). The learning curves show two examples of two-column progressive performance vs. baselines 1 and 3.
As in the other domains, the progressive approach yields more positive transfer than any of the
baselines (see Fig. 8a and Table 1). We observe less transfer on the Seek Track levels, which have
dense reward items throughout the maze and are easily learned. Note that even for these easy cases,
baseline 2 shows negative transfer because it cannot learn new low-level visual features, which
are important because the reward items change from task to task. The learning curves in Fig. 8b
exemplify the typical results seen in this domain: on simpler games, such as Track 1 and 2, learning
is rapid and stable by all agents. On more difﬁcult games, with more complex game structure, the
baselines struggle and progressive nets have an advantage.
Conclusion
Continual learning, the ability to accumulate and transfer knowledge to new domains, is a core
characteristic of intelligent beings. Progressive neural networks are a stepping stone towards continual
learning, and this work has demonstrated their potential through experiments and analysis across
three RL domains, including Atari, which contains orthogonal or even adversarial tasks. We believe
that we are the ﬁrst to show positive transfer in deep RL agents within a continual learning framework.
Moreover, we have shown that the progressive approach is able to effectively exploit transfer for
compatible source and task domains; that the approach is robust to harmful features learned in
incompatible tasks; and that positive transfer increases with the number of columns, thus corroborating
the constructive, rather than destructive, nature of the progressive architecture.