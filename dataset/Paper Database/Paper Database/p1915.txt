Convolutional Two-Stream Network Fusion for Video Action Recognition
Christoph Feichtenhofer
Graz University of Technology
 
Graz University of Technology
 
Andrew Zisserman
University of Oxford
 
Recent applications of Convolutional Neural Networks
(ConvNets) for human action recognition in videos have
proposed different solutions for incorporating the appearance and motion information. We study a number of ways
of fusing ConvNet towers both spatially and temporally
in order to best take advantage of this spatio-temporal
information. We make the following ﬁndings: (i) that rather
than fusing at the softmax layer, a spatial and temporal
network can be fused at a convolution layer without loss of
performance, but with a substantial saving in parameters;
(ii) that it is better to fuse such networks spatially at
the last convolutional layer than earlier, and that additionally fusing at the class prediction layer can boost
accuracy; ﬁnally (iii) that pooling of abstract convolutional
spatiotemporal
neighbourhoods
boosts performance. Based on these studies we propose
a new ConvNet architecture for spatiotemporal fusion of
video snippets, and evaluate its performance on standard
benchmarks
architecture
stateof-the-art results. Our code and models are available at
 vgg/software/two stream action
1. Introduction
Action recognition in video is a highly active area of research with state of the art systems still being far from human performance. As with other areas of computer vision,
recent work has concentrated on applying Convolutional
Neural Networks (ConvNets) to this task, with progress
over a number of strands: learning local spatiotemporal ﬁlters ), incorporating optical ﬂow snippets ,
and modelling more extended temporal sequences .
However, action recognition has not yet seen the substantial gains in performance that have been achieved in
other areas by ConvNets, e.g. image classiﬁcation , human face recognition , and human pose estimation .
Indeed the current state of the art performance on standard benchmarks such as UCF-
101 and HMDB51 is achieved by a combination of
ConvNets and a Fisher Vector encoding of hand-crafted
Spatial Stream
Temporal Stream
Figure 1. Example outputs of the ﬁrst three convolutional layers
from a two-stream ConvNet model . The two networks separately capture spatial (appearance) and temporal information at a
ﬁne temporal scale. In this work we investigate several approaches
to fuse the two networks over space and time.
features (such as HOF over dense trajectories ).
Part of the reason for this lack of success is probably
that current datasets used for training are either too small
or too noisy (we return to this point below in related work).
Compared to image classiﬁcation, action classiﬁcation in
video has the additional challenge of variations in motion
and viewpoint, and so might be expected to require more
training examples than that of ImageNet (1000 per class)
– yet UCF-101 has only 100 examples per class. Another
important reason is that current ConvNet architectures are
not able to take full advantage of temporal information and
their performance is consequently often dominated by spatial (appearance) recognition.
As can be seen from Fig. 1, some actions can be identi-
ﬁed from a still image from their appearance alone (archery
in this case). For others, though, individual frames can be
ambiguous, and motion cues are necessary. Consider, for
example, discriminating walking from running, yawning
from laughing, or in swimming, crawl from breast-stroke.
The two-stream architecture incorporates motion information by training separate ConvNets for both appearance
in still images and stacks of optical ﬂow. Indeed, this work
showed that optical ﬂow information alone was sufﬁcient to
discriminate most of the actions in UCF101.
Nevertheless, the two-stream architecture (or any previous method) is not able to exploit two very important cues
for action recognition in video: (i) recognizing what is mov-
 
ing where, i.e. registering appearance recognition (spatial
cue) with optical ﬂow recognition (temporal cue); and (ii)
how these cues evolve over time.
Our objective in this paper is to rectify this by developing an architecture that is able to fuse spatial and temporal
cues at several levels of granularity in feature abstraction,
and with spatial as well as temporal integration. In particular, Sec. 3 investigates three aspects of fusion: (i) in Sec. 3.1
how to fuse the two networks (spatial and temporal) taking
account of spatial registration? (ii) in Sec. 3.2 where to fuse
the two networks? And, ﬁnally in Sec. 3.3 (iii) how to fuse
the networks temporally? In each of these investigations
we select the optimum outcome (Sec. 4) and then, putting
this together, propose a novel architecture (Sec. 3.4) for
spatiotemporal fusion of two stream networks that achieves
state of the art performance in Sec. 4.6.
We implemented our approach using the MatConvNet
toolbox and made our code publicly available at
 
2. Related work
Several recent work on using ConvNets for action recognition in temporal sequences have investigated the question
of how to go beyond simply using the framewise appearance
information, and exploit the temporal information. A natural extension is to stack consecutive video frames and extend 2D ConvNets into time so that the ﬁrst layer learns
spatiotemporal features. study several approaches for
temporal sampling, including early fusion (letting the ﬁrst
layer ﬁlters operate over frames as in ), slow fusion
(consecutively increasing the temporal receptive ﬁeld as the
layers increase) and late fusion (merging fully connected
layers of two separate networks that operate on temporally
distant frames). Their architecture is not particularly sensitive to the temporal modelling, and they achieve similar
levels of performance by a purely spatial network, indicating that their model is not gaining much from the temporal
information.
The recently proposed C3D method learns 3D ConvNets on a limited temporal support of 16 consecutive
frames with all ﬁlter kernels of size 3×3×3. They report
better performance than by letting all ﬁlters operate
over space and time. However, their network is considerably deeper than with a structure similar to the very
deep networks in . Another way of learning spatiotemporal relationships is proposed in , where the authors
factorize 3D convolution into a 2D spatial and a 1D temporal convolution. Speciﬁcally, their temporal convolution is
a 2D convolution over time as well as the feature channels
and is only performed at higher layers of the network.
 compares several temporal feature pooling architectures to combine information across longer time periods.
They conclude that temporal pooling of convolutional layers performs better than slow, local, or late pooling, as well
as temporal convolution. They also investigate ordered sequence modelling by feeding the ConvNet features into a
recurrent network with Long Short-Term Memory (LSTM)
cells. Using LSTMs, however did not give an improvement
over temporal pooling of convolutional features.
The most closely related work to ours, and the one we
extend here, is the two-stream ConvNet architecture proposed in .
The method ﬁrst decomposes video into
spatial and temporal components by using RGB and optical ﬂow frames. These components are fed into separate
deep ConvNet architectures, to learn spatial as well as temporal information about the appearance and movement of
the objects in a scene. Each stream is performing video
recognition on its own and for ﬁnal classiﬁcation, softmax
scores are combined by late fusion. The authors compared
several techniques to align the optical ﬂow frames and concluded that simple stacking of L = 10 horizontal and vertical ﬂow ﬁelds performs best. They also employed multitask learning on UCF101 and HMDB51 to increase the
amount of training data and improve the performance on
both. To date, this method is the most effective approach
of applying deep learning to action recognition, especially
with limited training data. The two-stream approach has recently been employed into several action recognition methods .
Also related to our work is the bilinear method 
which correlates the output of two ConvNet layers by performing an outer product at each location of the image. The
resulting bilinear feature is pooled across all locations into
an orderless descriptor. Note that this is closely related to
second-order pooling of hand-crafted SIFT features.
In terms of datasets, introduced the Sports-1M
dataset which has a large number of videos (≈1M) and
classes (487). However, the videos are gathered automatically and therefore are not free of label noise. Another
large scale dataset is the THUMOS dataset that has over
45M frames. Though, only a small fraction of these actually contain the labelled action and thus are useful for
supervised feature learning. Due to the label noise, learning spatiotemporal ConvNets still largely relies on smaller,
but temporally consistent datasets such as UCF101 or
HMDB51 which contain short videos of actions. This
facilitates learning, but comes with the risk of severe over-
ﬁtting to the training data.
3. Approach
We build upon the the two-stream architecture in .
This architecture has two main drawbacks: (i) it is not able
to learn the pixel-wise correspondences between spatial and
temporal features (since fusion is only on the classiﬁcation
scores), and (ii) it is limited in temporal scale as the spatial
ConvNet operates only on single frames and the temporal
ConvNet only on a stack of L temporally adjacent optical
ﬂow frames (e.g. L = 10). The implementation of addressed the latter problem to an extent by temporal pooling
across regularly spaced samples in the video, but this does
not allow the modelling of temporal evolution of actions.
3.1. Spatial fusion
In this section we consider different architectures for fusing the two stream networks. However, the same issues
arise when spatially fusing any two networks so are not tied
to this particular application.
To be clear, our intention here is to fuse the two networks (at a particular convolutional layer) such that channel
responses at the same pixel position are put in correspondence. To motivate this, consider for example discriminating between the actions of brushing teeth and brushing hair.
If a hand moves periodically at some spatial location then
the temporal network can recognize that motion, and the
spatial network can recognize the location (teeth or hair)
and their combination then discriminates the action.
This spatial correspondence is easily achieved when the
two networks have the same spatial resolution at the layers
to be fused, simply by overlaying (stacking) layers from one
network on the other (we make this precise below). However, there is also the issue of which channel (or channels)
in one network corresponds to the channel (or channels) of
the other network.
Suppose for the moment that different channels in the
spatial network are responsible for different facial areas
(mouth, hair, etc), and one channel in the temporal network
is responsible for periodic motion ﬁelds of this type. Then,
after the channels are stacked, the ﬁlters in the subsequent
layers must learn the correspondence between these appropriate channels (e.g. as weights in a convolution ﬁlter) in
order to best discriminate between these actions.
To make this more concrete, we now discuss a number of
ways of fusing layers between two networks, and for each
describe the consequences in terms of correspondence.
A fusion function f : xa
t, →yt fuses two feature
t ∈RH×W ×D and xb
t ∈RH′×W ′×D′, at time t, to
produce an output map yt ∈RH′′×W ′′×D′′, where W, H
and D are the width, height and number of channels of
the respective feature maps. When applied to feedforward
ConvNet architectures, consisting of convolutional, fullyconnected, pooling and nonlinearity layers, f can be applied at different points in the network to implement e.g.
early-fusion, late-fusion or multiple layer fusion. Various
fusion functions f can be used.
We investigate the following ones in this paper, and, for simplicity, assume that
H = H′ = H′′, W = W ′ = W ′′, D = D′, and also drop
the t subscript.
Sum fusion. ysum = f sum(xa, xb) computes the sum of
the two feature maps at the same spatial locations i, j and
feature channels d:
i,j,d = xa
i,j,d + xb
where 1 ≤i ≤H, 1 ≤j ≤W, 1 ≤d ≤D and xa, xb, y ∈
Since the channel numbering is arbitrary, sum fusion
simply deﬁnes an arbitrary correspondence between the networks. Of course, subsequent learning can employ this arbitrary correspondence to its best effect, optimizing over the
ﬁlters of each network to make this correspondence useful.
Max fusion. ymax = f max(xa, xb) similarly takes the
maximum of the two feature map:
i,j,d = max{xa
where all other variables are deﬁned as above (1).
Similarly to sum fusion, the correspondence between
network channels is again arbitrary.
Concatenation fusion. ycat = f cat(xa, xb) stacks the
two feature maps at the same spatial locations i, j across
the feature channels d:
i,j,2d = xa
i,j,2d−1 = xb
where y ∈RH×W ×2D.
Concatenation does not deﬁne a correspondence, but
leaves this to subsequent layers to deﬁne (by learning suitable ﬁlters that weight the layers), as we illustrate next.
Conv fusion. yconv = f conv(xa, xb) ﬁrst stacks the two
feature maps at the same spatial locations i, j across the feature channels d as above (3) and subsequently convolves the
stacked data with a bank of ﬁlters f ∈R1×1×2D×D and biases b ∈RD
yconv = ycat ∗f + b,
where the number of output channels is D, and the ﬁlter has
dimensions 1 × 1 × 2D. Here, the ﬁlter f is used to reduce
the dimensionality by a factor of two and is able to model
weighted combinations of the two feature maps xa, xb at the
same spatial (pixel) location. When used as a trainable ﬁlter
kernel in the network, f is able to learn correspondences of
the two feature maps that minimize a joint loss function.
For example, if f is learnt to be the concatenation of two
permuted identity matrices 1′ ∈R1×1×D×D, then the ith
channel of the one network is only combined with the ith
channel of the other (via summation).
Note that if there is no dimensionality reducing convlayer injected after concatenation, the number of input
channels of the upcoming layer is 2D.
Bilinear fusion. ybil = f bil(xa, xb) computes a matrix
outer product of the two features at each pixel location, followed by a summation over the locations:
The resulting feature ybil ∈RD2 captures multiplicative
interactions at corresponding spatial locations. The main
drawback of this feature is its high dimensionality. To make
bilinear features usable in practice, it is usually applied at
ReLU5, the fully-connected layers are removed and
power- and L2-normalisation is applied for effective classi-
ﬁcation with linear SVMs.
The advantage of bilinear fusion is that every channel of
one network is combined (as a product) with every channel
of the other network. However, the disadvantage is that all
spatial information is marginalized out at this point.
Discussion: These operations illustrate a range of possible fusion methods. Others could be considered, for example: taking the pixel wise product of channels (instead of
their sum or max), or the (factorized) outer product without
sum pooling across locations .
Injecting fusion layers can have signiﬁcant impact on
the number of parameters and layers in a two-stream network, especially if only the network which is fused into
is kept and the other network tower is truncated, as illustrated in Fig. 2 (left). Table 1 shows how the number of
layers and parameters are affected by different fusion methods for the case of two VGG-M-2048 models (used in )
containing ﬁve convolution layers followed by three fullyconnected layers each.
Max-, Sum and Conv-fusion at
ReLU5 (after the last convolutional layer) removes nearly
half of the parameters in the architecture as only one tower
of fully-connected layers is used after fusion. Conv fusion
has slightly more parameters (97.58M) compared to sum
and max fusion (97.31M) due to the additional ﬁlter that
is used for channel-wise fusion and dimensionality reduction. Many more parameters are involved in concatenation
fusion, which does not involve dimensionality reduction after fusion and therefore doubles the number of parameters
in the ﬁrst fully connected layer. In comparison, sum-fusion
at the softmax layer requires all layers (16) and parameters
(181.4M) of the two towers.
In the experimental section (Sec. 4.2) we evaluate and
compare the performance of each of these possible fusion
methods in terms of their classiﬁcation accuracy.
3.2. Where to fuse the networks
As noted above, fusion can be applied at any point in the
two networks, with the only constraint that the two input
t ∈RH×W ×D and xb
t ∈RH′×W ′×D, at time t,
have the same spatial dimensions; i.e. H = H′, W = W ′.
This can be achieved by using an “upconvolutional” layer
 , or if the dimensions are similar, upsampling can be
achieved by padding the smaller map with zeros.
Table 2 compares the number of parameters for fusion at
different layers in the two networks for the case of a VGG-
M model. Fusing after different conv-layers has roughly the
same impact on the number of parameters, as most of these
Figure 2. Two examples of where a fusion layer can be placed.
The left example shows fusion after the fourth conv-layer. Only a
single network tower is used from the point of fusion. The right
ﬁgure shows fusion at two layers (after conv5 and after fc8) where
both network towers are kept, one as a hybrid spatiotemporal net
and one as a purely spatial network.
are stored in the fully-connected layers. Two networks can
also be fused at two layers, as illustrated in Fig. 2 (right).
This achieves the original objective of pixel-wise registration of the channels from each network (at conv5) but does
not lead to a reduction in the number of parameters (by half
if fused only at conv5, for example). In the experimental
section (Sec. 4.3) we evaluate and compare both the performance of fusing at different levels, and fusing at multiple
layers simultaneously.
2D Pooling
3D Pooling
3D Conv + 3D Pooling
Different ways of fusing temporal information. (a)
2D pooling ignores time and simply pools over spatial neighbourhoods to individually shrink the size of the feature maps for each
temporal sample. (b) 3D pooling pools from local spatiotemporal
neighbourhoods by ﬁrst stacking the feature maps across time and
then shrinking this spatiotemporal cube. (c) 3D conv + 3D pooling
additionally performs a convolution with a fusion kernel that spans
the feature channels, space and time before 3D pooling.
3.3. Temporal fusion
We now consider techniques to combine feature maps xt
over time t, to produce an output map yt. One way of processing temporal inputs is by averaging the network predictions over time (as used in ). In that case the architecture
only pools in 2D (xy); see Fig. 3(a).
Now consider the input of a temporal pooling layer as
feature maps x ∈RH×W ×T ×D which are generated by
stacking spatial maps across time t = 1 . . . T.
3D Pooling: applies max-pooling to the stacked data
within a 3D pooling cube of size W ′ × H′ × T ′. This is
a straightforward extension of 2D pooling to the temporal
domain, as illustrated in Fig. 3(b). For example, if three
Spatiotemporal Loss
Temporal Loss
3D Pooling
3D Conv fusion + 3D Pooling
Figure 4. Our spatiotemporal fusion ConvNet applies two-stream ConvNets, that capture short-term information at a ﬁne temporal scale
2 ), to temporally adjacent inputs at a coarse temporal scale (t + Tτ). The two streams are fused by a 3D ﬁlter that is able to
learn correspondences between highly abstract features of the spatial stream (blue) and temporal stream (green), as well as local weighted
combinations in x, y, t. The resulting features from the fusion stream and the temporal stream are 3D-pooled in space and time to learn
spatiotemporal (top left) and purely temporal (top right) features for recognising the input video.
temporal samples are pooled, then a 3 × 3 × 3 max pooling
could be used across the three stacked corresponding channels. Note, there is no pooling across different channels.
3D Conv + Pooling: ﬁrst convolves the four dimensional
input x with a bank of D′ ﬁlters f ∈RW ′′×H′′×T ′′×D×D′
and biases b ∈RD
y = xt ∗f + b,
as e.g. in , followed by 3D pooling as described above.
This method is illustrated in Fig. 3(c). The ﬁlters f are able
to model weighted combinations of the features in a local
spatio-temporal neighborhood using kernels of size W ′′ ×
H′′ × T ′′ × D. Typically the neighborhood is 3 × 3 × 3
(spatial × temporal).
Discussion. The authors of evaluate several additional methods to combine two-stream ConvNets over time.
They ﬁnd temporal max-pooling of convolutional layers
among the top performers. We generalize max-pooling here
to 3D pooling that provides invariance to small changes of
the features’ position over time. Further, 3D conv allows
spatio-temporal ﬁlters to be learnt . For example,
the ﬁlter could learn to center weight the central temporal
sample, or to differentiate in time or space.
3.4. Proposed architecture
We now bring together the ideas from the previous sections to propose a new spatio-temporal fusion architecture
and motivate our choices based on our empirical evaluation
in Sec. 4. The choice of the spatial fusion method, layer and
temporal fusion is based on the experiments in sections 4.2,
4.3 and 4.5, respectively.
Our proposed architecture (shown in Fig. 4) can be
viewed as an extension of the architecture in Fig. 2 (left)
over time. We fuse the two networks, at the last convolutional layer (after ReLU) into the spatial stream to convert
it into a spatiotemporal stream by using 3D Conv fusion
followed by 3D pooling (see Fig. 4, left). Moreover, we do
not truncate the temporal stream and also perform 3D Pooling in the temporal network (see Fig. 4, right). The losses
of both streams are used for training and during testing we
average the predictions of the two streams. In our empirical evaluation (Sec. 4.6) we show that keeping both streams
performs slightly better than truncating the temporal stream
after fusion.
Having discussed how to fuse networks over time, we
discuss here the issue of how often to sample the temporal
sequence. The temporal fusion layer receives T temporal
chunks that are τ frames apart; i.e. the two stream towers
are applied to the input video at time t, t + τ, . . . t + Tτ. As
shown in Fig. 4 this enables us to capture short scale (t± L
temporal features at the input of the temporal network (e.g.
the drawing of an arrow) and put them into context over a
longer temporal scale (t + Tτ) at a higher layer of the network (e.g. drawing an arrow, bending a bow, and shooting
an arrow).
Since the optical ﬂow stream has a temporal receptive
ﬁeld of L = 10 frames, the architecture operates on a total
temporal receptive ﬁeld of T ×L. Note that τ < L results in
overlapping inputs for the temporal stream, whereas τ ≥L
produces temporally non-overlapping features.
After fusion, we let the 3D pooling operate on T spatial feature maps that are τ frames apart. As features may
change their spatial position over time, combining spatial
and temporal pooling to 3D pooling makes sense. For example, the output of a VGG-M network at conv5 has an
input stride of 16 pixels and captures high level features
from a receptive ﬁeld of 139 × 139 pixels. Spatiotemporal pooling of conv5 maps that are τ frames distant in time
can therefore capture features of the same object, even if
they slightly move.
3.5. Implementation details
Two-Stream architecture. We employ two pre-trained
ImageNet models. First, for sake of comparison to the original two-stream approach , the VGG-M-2048 model 
with 5 convolutional and 3 fully-connected layers. Second,
the very deep VGG-16 model that has 13 convolutional
and 3 fully-connected layers. We ﬁrst separately train the
two streams as described in , but with some subtle differences: We do not use RGB colour jittering; Instead of
decreasing the learning rate according to a ﬁxed schedule,
we lower it after the validation error saturates; For training
the spatial network we use lower dropout ratios of 0.85 for
the ﬁrst two fully-connected layers. Even lower dropout ratios (up to 0.5) did not decrease performance signiﬁcantly.
For the temporal net, we use optical ﬂow stacking
with L = 10 frames . We also initialised the temporal
net with a model pre-trained on ImageNet, since this generally facilitates training speed without a decrease in performance compared to our model trained from scratch. The
network input is rescaled beforehand, so that the smallest
side of the frame equals 256. We also pre-compute the optical ﬂow before training and store the ﬂow ﬁelds as JPEG
images (with clipping of displacement vectors larger than
20 pixels). We do not use batch normalization .
Two-Stream ConvNet fusion.
For fusion, these networks are ﬁnetuned with a batch size of 96 and a learning rate starting from 10−3 which is reduced by a factor of
10 as soon as the validation accuracy saturates. We only
propagate back to the injected fusion layer, since full backpropagation did not result in an improvement. In our experiments we only fuse between layers with the same output
resolution; except for fusing a VGG-16 model at ReLU5 3
with a VGG-M model at ReLU5, where we pad the slightly
smaller output of VGG-M (13 × 13, compared to 14 × 14)
with a row and a column of zeros. For Conv fusion, we
found that careful initialisation of the injected fusion layer
(as in (4)) is very important. We compared several methods
and found that initialisation by identity matrices (to sum the
two networks) performs as well as random initialisation.
Spatiotemporal architecture. For our ﬁnal architecture
described in Sec. 3.4, the 3D Conv fusion kernel f has dimension 3 × 3 × 3 × 1024 × 512 and T = 5, i.e. the spatiotemporal ﬁlter has dimension H′′ × W ′′ × T ′′ = 3 × 3 × 3,
the D = 1024 results from concatenating the ReLU5 from
the spatial and temporal streams, and the D′ = 512 matches
the number of input channels of the following FC6 layer.
The 3D Conv ﬁlters are also initialised by stacking two
identity matrices for mapping the 1024 feature channels to
512. Since the activations of the temporal ConvNet at the
last convolutional layer are roughly 3 times lower than its
appearance counterpart, we initialise the temporal identity
matrix of f by a factor of 3 higher. The spatiotemporal part
of f is initialised using a Gaussian of size 3 × 3 × 3 and
σ = 1. Further, we do not fuse at the prediction layer during training, as this would bias the loss towards the temporal architecture, because the spatiotemporal architecture
requires longer to adapt to the fused features.
Training 3D ConvNets is even more prone to overﬁtting than the two-stream ConvNet fusion, and requires additional augmentation as follows. During ﬁnetuning, at each
training iteration we sample the T = 5 frames from each of
the 96 videos in a batch by randomly sampling the starting
frame, and then randomly sampling the temporal stride (τ)
∈ (so operating over a total of between 15 and 50
frames). Instead of cropping a ﬁxed sized 224 × 224 input
patch, we randomly jitter its width and height by ±25% and
rescale it to 224 × 224. The rescaling is chosen randomly
and may change the aspect-ratio. Patches are only cropped
at a maximum of 25% distance from the image borders (relative to the width and height). Note, the position (and size,
scale, horizontal ﬂipping) of the crop is randomly selected
in the ﬁrst frame (of a multiple-frame-stack) and then the
same spatial crop is applied to all frames in the stack.
Testing. Unless otherwise speciﬁed, only the T = 5
frames (and their horizontal ﬂips) are sampled, compared
to the 25 frames in , to foster fast empirical evaluation.
In addition we employ fully convolutional testing where the
entire frame is used (rather than spatial crops).
4. Evaluation
4.1. Datasets and experimental protocols
We evaluate our approach on two popular action recognition datasets. First, UCF101 , which consists of 13320
action videos in 101 categories.
The second dataset is
HMDB51 , which contains 6766 videos that have been
annotated for 51 actions. For both datasets, we use the provided evaluation protocol and report the mean average accuracy over the three splits into training and test data.
4.2. How to fuse the two streams spatially?
For these experiments we use the same network architecture as in ; i.e. two VGG-M-2048 nets . The fusion layer is injected at the last convolutional layer, after
rectiﬁcation, i.e. its input is the output of ReLU5 from the
two streams. This is chosen because, in preliminary experiments, it provided better results than alternatives such as the
non-rectiﬁed output of conv5. At that point the features are
Fusion Method
Fusion Layer
#parameters
Sum (ours)
Concatenation
Bilinear 
Performance comparison of different spatial fusion
strategies (Sec. 3.1) on UCF101 (split 1). Sum fusion at the softmax layer corresponds to averaging the two networks predictions
and therefore includes the parameters of both 8-layer VGG-M
models. Performing fusion at ReLU5 using Conv or Sum fusion
does not signiﬁcantly lower classiﬁcation accuracy. Moreover, this
requires only half of the parameters in the softmax fusion network.
Concatenation has lower performance and requires twice as many
parameters in the FC6 layer (as Conv or Sum fusion). Only the bilinear combination enjoys much fewer parameters as there are no
FC layers involved; however, it has to employ an SVM to perform
comparably.
already highly informative while still providing coarse location information. After the fusion layer a single processing
stream is used.
We compare different fusion strategies in Table 1 where
we report the average accuracy on the ﬁrst split of UCF101.
We ﬁrst observe that our performance for softmax averaging (85.94%) compares favourably to the one reported in
 . Second we see that Max and Concatenation perform
considerably lower than Sum and Conv fusion. Conv fusion performs best and is slightly better than Bilinear fusion and simple fusion via summation. For the reported
Conv-fusion result, the convolution kernel f is initialised
by identity matrices that perform summation of the two feature maps. Initialisation via random Gaussian noise ends up
at a similar performance 85.59% compared to identity matrices (85.96%), however, at a much longer training time.
This is interesting, since this, as well as the high result of
Sum-fusion, suggest that simply summing the feature maps
is already a good fusion technique and learning a randomly
initialised combination does not lead to signiﬁcantly different/better results.
For all the fusion methods shown in Table 1, fusion at FC
layers results in lower performance compared to ReLU5,
with the ordering of the methods being the same as in Table 1, except for bilinear fusion which is not possible at FC
layers. Among all FC layers, FC8 performs better than FC7
and FC6, with Conv fusion at 85.9%, followed by Sum fusion at 85.1%. We think the reason for ReLU5 performing
slightly better is that at this layer spatial correspondences
between appearance and motion are fused, which would
have already been collapsed at the FC layers .
4.3. Where to fuse the two streams spatially?
Fusion from different layers is compared in Table 2.
Conv fusion is used and the fusion layers are initialised
Fusion Layers
#parameters
ReLU5 + FC8
ReLU3 + ReLU5 + FC6
Table 2. Performance comparison for Conv fusion (4) at different
fusion layers. An earlier fusion (than after conv5) results in weaker
performance. Multiple fusions also lower performance if early layers are incorporated (last row). Best performance is achieved for
fusing at ReLU5 or at ReLU5+FC8 (but with nearly double the
parameters involved).
UCF101 (split 1)
HMDB51 (split 1)
VGG-M-2048
VGG-M-2048
Late Fusion
Table 3. Performance comparison of deep vs. very
deep (VGG-16) Two-Stream ConvNets on the UCF101 (split1)
and HMDB51 (split1). Late fusion is implemented by averaging
the prediction layer outputs. Using deeper networks boosts performance at the cost of computation time.
by an identity matrix that sums the activations from previous layers. Interestingly, fusing and truncating one net at
ReLU5 achieves around the same classiﬁcation accuracy on
the ﬁrst split of UCF101 (85.96% vs 86.04%) as an additional fusion at the prediction layer (FC8), but at a much
lower number of total parameters (97.57M vs 181.68M).
Fig. 2 shows how these two examples are implemented.
4.4. Going from deep to very deep models
For computational complexity reasons, all previous experiments were performed with two VGG-M-2048 networks (as in ). Using deeper models, such as the very
deep networks in can, however, lead to even better performance in image recognition tasks . Following
that, we train a 16 layer network, VGG-16, on UCF101
and HMDB51.
All models are pretrained on ImageNet
and separately trained for the target dataset, except for the
temporal HMDB51 networks which are initialised from the
temporal UCF101 models. For VGG-16, we use TV-L1 optical ﬂow and apply a similar augmentation technique
as for 3D ConvNet training (described in Sec. 3.5) that samples from the image corners and its centre . The learning rate is set to 50−4 and decreased by a factor of 10 as
soon as the validation objective saturates.
The comparison between deep and very deep models is
shown in Table 3. On both datasets, one observes that going
to a deeper spatial model boosts performance signiﬁcantly
(8.11% and 10.29%), whereas a deeper temporal network
yields a lower accuracy gain (3.91% and 3.73%).
Table 4. Spatiotemporal two-stream fusion on UCF101 (split1) and
HMDB51 (split1). The models used are VGG-16 (spatial net) and
VGG-M (temporal net). The “+” after a fusion layer indicates that
both networks and their loss are kept after fusing, as this performs
better than truncating one network. Speciﬁcally, at ReLU5 we fuse
from the temporal net into the spatial network, then perform either
2D or 3D pooling at Pool5 and compute a loss for each tower.
During testing, we average the FC8 predictions for both towers.
4.5. How to fuse the two streams temporally?
Different temporal fusion strategies are shown in Table 4.
In the ﬁrst row of Table 4 we observe that conv fusion performs better than averaging the softmax output (cf. Table
3). Next, we ﬁnd that applying 3D pooling instead of using
2D pooling after the fusion layer increases performance on
both datasets, with larger gains on HMDB51. Finally, the
last row of Table 4 lists results for applying a 3D ﬁlter for
fusion which further boosts recognition rates.
4.6. Comparison with the state-of-the-art
Finally, we compare against the state-of-the-art over all
three splits of UCF101 and HMDB51 in Table 5. We use
the same method as shown above, i.e. fusion by 3D Conv
and 3D Pooling (illustrated in Fig. 4). For testing we average 20 temporal predictions from each network by densely
sampling the input-frame-stacks and their horizontal ﬂips.
One interesting comparison is to the original two-stream approach , we improve by 3% on UCF101 and HMDB51
by using a VGG-16 spatial (S) network and a VGG-M
temporal (T) model, as well as by 4.5% (UCF) and 6%
(HMDB) when using VGG-16 for both streams. Another
interesting comparison is against the two-stream network in
 , which employs temporal conv-pooling after the last
dimensionality reduction layer of a GoogLeNet architecture. They report 88.2% on UCF101 when pooling over
120 frames and 88.6% when using an LSTM for pooling.
Here, our result of 92.5% clearly underlines the importance
of our proposed approach. Note also that using a single
stream after temporal fusion achieves 91.8%, compared to
maintaining two streams and achieving 92.5%, but with far
fewer parameters and a simpler architecture.
As a ﬁnal experiment, we explore what beneﬁt results
from a late fusion of hand-crafted IDT features with
our representation. We simply average the SVM scores of
the FV-encoded IDT descriptors (i.e. HOG, HOF, MBH)
with the predictions (taken before softmax) of our ConvNet
representations. The resulting performance is shown in Table 6. We achieve 93.5% on UCF101 and 69.2% HMDB51.
This state-of-the-art result illustrates that there is still a de-
Spatiotemporal ConvNet 
Composite LSTM Model 
Two-Stream ConvNet (VGG-M) 
Factorized ConvNet 
Two-Stream Conv Pooling 
Two-Stream ConvNet (VGG-16, )
Two-Stream ConvNet (VGG-16, ours)
Ours (S:VGG-16, T:VGG-M)
Ours (S:VGG-16, T:VGG-16,
single tower after fusion)
Ours (S:VGG-16, T:VGG-16)
Table 5. Mean classiﬁcation accuracy of best performing ConvNet
approaches over three train/test splits on HMDB51 and UCF101.
For our method we list the models used for the spatial (S) and
temporal (T) stream.
IDT+higher dimensional FV 
C3D+IDT 
TDD+IDT 
Ours+IDT (S:VGG-16, T:VGG-M)
Ours+IDT (S:VGG-16, T:VGG-16)
Table 6. Mean classiﬁcation accuracy on HMDB51 and UCF101
for approaches that use IDT features .
gree of complementary between hand-crafted representations and our end-to-end learned ConvNet approach.
5. Conclusion
We have proposed a new spatiotemporal architecture for
two stream networks with a novel convolutional fusion layer
between the networks, and a novel temporal fusion layer
(incorporating 3D convolutions and pooling). The new architecture does not increase the number of parameters signiﬁcantly over previous methods, yet exceeds the state of
the art on two standard benchmark datasets. Our results suggest the importance of learning correspondences between
highly abstract ConvNet features both spatially and temporally. One intriguing ﬁnding is that there is still such an
improvement by combining ConvNet predictions with FVencoded IDT features. We suspect that this difference may
vanish in time given far more training data, but otherwise it
certainly indicates where future research should attend.
Finally, we return to the point that current datasets are
either too small or too noisy. For this reason, some of the
conclusions in this paper should be treated with caution.
Acknowledgments.
We are grateful for discussions with
Karen Simonyan.
Christoph Feichtenhofer is a recipient
of a DOC Fellowship of the Austrian Academy of Sciences. This work was supported by the Austrian Science
Fund (FWF) under project P27076, and also by EPSRC Programme Grant Seebibyte EP/M013774/1. The GPUs used
for this research were donated by NVIDIA.