The Annals of Applied Statistics
2014, Vol. 8, No. 3, 1892–1919
DOI: 10.1214/14-AOAS761
⃝Institute of Mathematical Statistics, 2014
JOINT ESTIMATION OF MULTIPLE RELATED
BIOLOGICAL NETWORKS1
By Chris J. Oates∗, Jim Korkola†,
Joe W. Gray† and Sach Mukherjee‡,§,2
University of Warwick∗, OHSU Knight Cancer Institute†, MRC
Biostatistics Unit‡ and University of Cambridge§
Graphical models are widely used to make inferences concerning interplay in multivariate systems. In many applications, data are
collected from multiple related but nonidentical units whose underlying networks may diﬀer but are likely to share features. Here we
present a hierarchical Bayesian formulation for joint estimation of
multiple networks in this nonidentically distributed setting. The approach is general: given a suitable class of graphical models, it uses
an exchangeability assumption on networks to provide a corresponding joint formulation. Motivated by emerging experimental designs
in molecular biology, we focus on time-course data with interventions, using dynamic Bayesian networks as the graphical models. We
introduce a computationally eﬃcient, deterministic algorithm for exact joint inference in this setting. We provide an upper bound on
the gains that joint estimation oﬀers relative to separate estimation
for each network and empirical results that support and extend the
theory, including an extensive simulation study and an application
to proteomic data from human cancer cell lines. Finally, we describe
approximations that are still more computationally eﬃcient than the
exact algorithm and that also demonstrate good empirical performance.
1. Introduction.
Graphical models are widely used to represent multivariate systems. Vertices in a graph (or network; we use both terms interchangeably) G are identiﬁed with random variables and edges between
Received February 2013; revised April 2014.
1Supported in part by NCI U54 CA112970,
UK EPSRC EP/E501311/1
EP/D002060/1, and the Cancer Systems Biology Center grant from the Netherlands Organisation for Scientiﬁc Research.
2A recipient of a Royal Society Wolfson Research Merit Award.
Key words and phrases. Bayesian network, hierarchical model, belief propagation, information sharing.
This is an electronic reprint of the original article published by the
Institute of Mathematical Statistics in The Annals of Applied Statistics,
2014, Vol. 8, No. 3, 1892–1919. This reprint diﬀers from the original in pagination
and typographic detail.
OATES, KORKOLA, GRAY AND MUKHERJEE
the vertices describe conditional independence statements or, with suitable
modeling and semantic extensions, causal inﬂuences between the variables.
In many applications a key statistical challenge is to construct a network
estimator ˆG(y), based on data y, that performs well in a sense appropriate
to the application. Such “network inference” is increasingly a mainstream
approach in many disciplines, including neuroscience, sociology and computational biology.
Network inference methods usually assume that the data are identically
distributed (speciﬁcally, that data sets satisfy an exchangeability assumption). However, in many applications, data are not identically distributed,
but are instead obtained from multiple related but nonidentical units (or
“individuals”; we use both terms interchangeably). This paper concerns network inference in this nonidentically distributed setting.
Our work is motivated by biological networks in cancer. Multiple studies have demonstrated the remarkable genomic heterogeneity of cancer [The
1000 Genomes Project Consortium , The Cancer Genome Atlas Network ]. At the same time, the question of how such heterogeneity
is manifested at the level of biological networks has remained poorly understood. We focus in particular on protein signaling networks in human
cancer cell lines. Signaling networks describe biochemical interplay between
proteins and are central to cancer biology. However, sequence and transcript
data alone are inadequate for the study of signaling and, indeed, these data
types can be discordant with the abundance of signaling proteins and posttransitional modiﬁcations (including phosphorylation) that are key to the
process [Akbani et al. ]. Recent developments in proteomics, including reverse-phase protein arrays [or RPPA, see Hennessy et al. ; this
technology provides the data we analyze below], have improved the ability
to interrogate signaling heterogeneity.
To ﬁx ideas, we begin by describing the speciﬁc application that motivates
this work. We consider time-course phosphoprotein measurements obtained
using RPPA technology (details appear below) for 6 cell lines. The goal of the
study is to infer cell line-speciﬁc protein signaling networks Gj, j = 1,...,6,
and additionally to highlight experimentally testable diﬀerences between
them. Prior network information is available from the literature, but it is
believed that cell line-speciﬁc genetic alterations may induce diﬀerences with
respect to the “literature network” (and between cell lines). At the same
time, the amount of data per cell line is limited (6 time points in each of
4 conditions, making a total of 24 data points per cell line j, constituting
data yj). Since the cell lines j are closely related, yet potentially diﬀerent
with respect to underlying networks, a key inferential question is how to
“borrow strength” between the network estimation problems. That is, we
seek a joint estimator of the cell line-speciﬁc networks {G1 ···G6} based
JOINT NETWORK INFERENCE
on the entire (nonidentically distributed) data set {y1 ···y6} that shares
information between the estimation problems while preserving the ability to
identify cell line-speciﬁc network structure.
This application is an example of a more general class of biological applications, where individuals j could correspond to, for example, diﬀerent
patients or cell lines (or groups thereof; e.g., disease subtypes) and the networks themselves to gene regulatory or protein signaling networks that could
depend on the genetic and epigenetic state of the individuals. Indeed, continuing reduction in the unit cost of biochemical assays has led to an increase
in experimental designs that include panels of potentially heterogeneous individuals [Barretina et al. , Cao et al. , Maher , The
Cancer Genome Atlas Network ]. As in the signaling example above,
in such settings, given individual-speciﬁc data yj, there is scientiﬁc interest
in individual-speciﬁc networks Gj and their similarities and diﬀerences.
Following Werhli and Husmeier , Penfold et al. and others, we focus on the case of directed networks Gj that are exchangeable
in the sense that inference is invariant to permutation of individuals j ∈
J = {1,... ,J}. We model data on all individuals {yj :j ∈J } within a joint
Bayesian framework. Regularization of individual networks is achieved by
introducing a latent network G to couple inference across all individuals.
We report posterior marginal inclusion probabilities for every possible edge
in each individual network Gj as well as the latent network G. The high-level
formulation we propose is general and, in principle, essentially any graphical model of interest could be embedded within the framework described to
enable joint estimation.
In general, the individual j’s could have complex, hierarchical relationships, for example, with j’s belonging to groups and subgroups [e.g., corresponding to cancer types and subtypes; see Curtis et al. ]. The
exchangeable case we consider corresponds in a sense to the simplest possible hierarchy in which each individual is dependent on a single latent graph
(see Figure 1). In settings where groups can be treated as approximately
homogeneous, the approach presented in this paper can be trivially used
to give group-level estimates, by using j to index groups rather than individuals, with all data for group j modeled as dependent on graph Gj.
This corresponds to an assumption of identically distributed data within
(but not between) groups. In the empirical study presented below we consider also robustness of our approach under violation of the exchangeability
assumption.
For the application to time-course data from protein signaling that we
focus on, we present a detailed development using directed graphical models
called dynamic Bayesian networks (DBNs). These are directed acyclic graphs
(DAGs) with explicit time indices [Murphy ]. The main contributions
of this paper are as follows:
OATES, KORKOLA, GRAY AND MUKHERJEE
• Bayesian computation. For the time-course setting, we put forward an ef-
ﬁcient and exact algorithm. This is done by exploiting factorization properties of the DBN likelihood, analytic marginalization over continuous
parameters and belief propagation. In moderate dimensional settings this
allows exact joint estimation to be carried out in seconds to minutes (we
discuss computational complexity below).
• Theory. We provide a result that quantiﬁes the statistical eﬃciency of
joint relative to separate estimation and that gives a suﬃcient condition
for improved performance.
• Empirical investigation. The availability of an eﬃcient Bayesian algorithm
enables, for the ﬁrst time, a comprehensive empirical study of joint estimation, including a wide range of simulation regimes and an application
to experimental data from a panel of human cancer cell lines. For several
classical (nonjoint) DBNs, including a recent causal variant suitable for
interventional data [Spencer, Hill and Mukherjee ], we formulate
corresponding joint estimators. This allows us to investigate the eﬀect of
joint estimation itself; we ﬁnd that it often provides gains relative to the
corresponding individual-level estimators. Some computationally favorable approximations to joint inference are described that we ﬁnd perform
well under a range of conditions.
Joint estimation has previously been discussed in the Gaussian graphical
model (GGM) literature [Danaher, Wang and Witten ]. In contrast to
GGMs, motivated by biological applications, we focus on DAG models with
a causal interpretation. Approaches to context-speciﬁc DAG structure based
on the embellishment of Bayesian networks include Boutilier et al. ,
Geiger and Heckerman . Our approach diﬀers by regularizing based
on network structure alone; we do not place exchangeability assumptions
on the data-generating parameters. Related work that is based on DAGs
includes Niculescu-Mizil and Caruana , Werhli and Husmeier ,
Dondelinger, L`ebre and Husmeier . In a sequel to the present work,
Oates, Costa and Nichols provide an exact algorithm for joint maximum a posteriori (MAP) estimate of multiple (static) DAGs. In contrast,
here we focus on Bayesian model-averaging (as opposed to MAP estimation)
and on time-course data (or, more generally, Bayesian networks with a ﬁxed
ordering of the variables).
In a similar vein to the present paper, Oyen and Lane estimated
multiple DAGs sharing a common ordering of the vertices, but they considered only applications involving J = 2 individuals. Our work is closely
related to Penfold et al. , who also considered Bayesian joint estimation of directed graphs from time-course data. However, as we discuss
in detail below, the methodology they propose is prohibitively computationally expensive for the applications we consider here. In comparison, the
JOINT NETWORK INFERENCE
exact algorithm we propose oﬀers massive computational gains that in turn
allow us to present a much more extensive study of joint estimation than
has hitherto been possible. Furthermore, we allow for prior information regarding the network structure (including individual-speciﬁc characteristics)
and present theoretical results concerning the statistical eﬃciency of joint
network estimation.
The remainder of the paper is organized as follows. In Section 2 we describe a hierarchical Bayesian formulation and in Section 3 we discuss computationally eﬃcient joint inference in the case of DBNs. Empirical results
are presented in Section 4, including an application to protein signaling in
cancer. Finally, we close with a discussion of our ﬁndings in Section 5.
2. Joint network inference: The general case.
We describe a general statistical formulation for joint network inference that can be coupled to essentially any class of graphical models. For computational tractability it may
be necessary to place restrictions on the class of graphical models; in Section 3 we present a detailed development for DBNs that are well-suited to
our motivating application in cancer.
2.1. Hierarchical model.
Consider a space G of graphs on the vertex set
P = {1,...,P}. To keep the presentation general, we do not specify the type
of graph or restrictions on G at this stage (the special case of DBNs for
time-course data is described below). As shown in Figure 1, each individual
network Gj ∈G is modeled with dependence on a latent network G ∈G
Joint network inference (JNI). A hierarchical model for analysis of multivariate data from multiple, nonidentical units or individuals, indexed by j. (Shaded
nodes are unobserved. G0 = prior network, G = latent network, Gj = network speciﬁc
to individual j, θj = parameters for individual j, Yj = observables for individual j,
Zj = ancillary information available on individual j, η,λj = inverse temperature hyperparameters, φj = hyperparameters deﬁning a prior on θj. Panel notation is used to indicate
the presence of multiple individuals j ∈J . Note that in practice we take λj ≡λ for all
OATES, KORKOLA, GRAY AND MUKHERJEE
that in turn depends on a prior network G0 ∈G (Section 2.2). In this way,
estimates of the individual networks Gj are regularized by shrinkage toward
the common latent network G that, in turn, may be constrained by an
informative network prior. As in any graphical model, observations Yj on
individual j are dependent upon a graph Gj and parameters θj. Here Zj
denotes any ancillary information available on individual j. The model is
speciﬁed by
p(G|G0,η) ∝exp(−ηd(G,G0)),
p(Gj|G,λ,Zj) ∝exp(−λjdj(Gj,G;Zj))
and a suitably chosen graphical model likelihood p(Yj|Gj,θj,Zj). Equation
(1) follows the “network prior” approach of Mukherjee and Speed 
that was proposed for biological applications where subjective prior structural information is available. The functionals dj,d:G ×G →R and hyperparameters η,λj must be speciﬁed (Section 2.2). This paper restricts attention
to exchangeable models, in particular, we consider functionals dj that are independent of the index j. We refer to the above formulation as joint network
inference (JNI).
2.2. Network prior.
The network prior [equation (1)] requires a penalty
functional d:G × G →R and a prior network G0 ∈G, with the former capturing how close a candidate network G ∈G is to the latter. We discuss
choice of G0 below. Given G0, a simple choice of penalty function d is the
structural Hamming distance (SHD) given by d(G,G0) = ∥G −G0∥, where
i,j |mi,j| is the ℓ1-norm of an adjacency matrix and the diﬀerential
network G −G0 is deﬁned to have edges that occur in exactly one of the
networks G, G0 [see also Ibrahim and Chen , Imoto et al. ].
The hyperparameter η controls the strength of the prior network G0 [equation (1)]. Motivated by an application in cancer biology where prior structural information G0 is available, we follow Penfold et al. by restricting attention to SHD priors, however, our statistical formulation is general
(see below) and compatible with other penalty functionals. Alternatively,
one could employ a beta-binomial prior as described in, for example, Dondelinger, L`ebre and Husmeier , that allows for the hyperparameters
of the binomial to be integrated out [see also Oyen and Lane ]. Note
that in the latter case it is not possible to integrate speciﬁc prior structural
information, making beta-binomial priors unsuitable for the application that
this paper considers.
Given a latent network G, individual networks Gj are regularized in a
similar way, as dj(Gj,G) = ∥Gj −G∥. In their work on combining multiple
data sources, Werhli and Husmeier allow the λj to vary over individuals j ∈J . Likewise, Penfold et al. learn the λj on a per-individual
JOINT NETWORK INFERENCE
basis. However, in both studies, hyperparameter elicitation is nontrivial (see
Section 3.3). In the present paper, we consider only the special case where
λ1 = λ2 = ··· = λJ := λ.
A graph G ∈G can be characterized by (i) its adjacency matrix or (ii)
its parent sets as G = (π1,...,πP ), where πp ⊆P = {1··· P} are the parents
of vertex p in G. We write Gp for the set of possible parent sets for p, such
that formally G = G1 × ··· × GP . Although we focus on SHD priors, the
inference procedures presented in this paper apply to the more general class
of “modular” priors, that may be factored over p ∈P and written in the
p),dj(Gj,G;Zj) =
for some functionals dp,dj
p :Gp × Gp →R. Here π0
p are parent sets for
variable p, corresponding to G0 and Gj, respectively.
In general, inference for the JNI model [equations (1), (2)] will be computationally intensive, as demonstrated in Werhli and Husmeier , Penfold et al. . In Section 3 below we show that eﬃcient, exact inference
is nevertheless possible within the DBN class of graphical models.
3. Joint network inference: DBNs.
The JNI model and network priors,
as described above, are general. To apply the JNI framework in a particular context requires an appropriate likelihood at the individual level, that
is, speciﬁcation of the joint distribution p(Yj|Gj,θj,Zj) of observables Yj
given network Gj, ancillary information Zj and parameters θj, together
with a prior distribution p(θj|Gj,Zj) over model parameters. We focus on
time-course data, using DBNs and exploiting families of conjugate prior
distributions. We show that factorization properties of the DBN likelihood
permit computationally tractable joint inference and provide an explicit algorithm based on belief propagation.
3.1. DBN formulation.
A DBN is a graphical model based on a DAG
on the vertex set P × T , where T is a set of time indices [Figure 8(a);
see Murphy ]. This DAG with PT vertices is known as the “unrolled”
DAG. Here, following Hill et al. and others, we use DBNs that permit
only edges forward in time and that are stationary in the sense that neither
the network nor parameters change with time. For such DBNs, the network
can be described by a directed graph G with exactly P vertices, with edges
understood to go forward in time in the unrolled DAG [see Appendix B
and Figure 8(b)]. Note that G may have cycles. In what follows, all graphs
(prior, latent and individual) describe the latter P-vertex representation.
Under a modular network prior, structural inference for DBNs can be
carried out eﬃciently as described in Hill et al. . In brief, the posterior
OATES, KORKOLA, GRAY AND MUKHERJEE
Gj|y factorizes into a product of local posteriors πj
p|y, one factor for each
target variable p. Background and assumptions for DBNs are summarized in
Appendix B; for general background on DBNs we refer the interested reader
to Murphy and for relevant details concerning the class of DBNs used
here to Hill et al. .
Write y(t) for the matrix of observed data at time t for all individuals j
and variables p. In order to simplify notation, we deﬁne a data-dependent
functional
P(X) = p(X(1))
p(X(t)|y(t −1))
that implicitly conditions upon observed history. Let yj
p(t) denote the observed value of variable p in individual j at time t. The above notation allows
us to conveniently summarize the product
p(2)|y(1),πj
p(m)|y(m −1),πj
p). Thus, we have that, for DBNs, the full likelihood also satisﬁes
p(y|G1,...,GJ,Z1,...,ZJ) =
where y denotes the complete data (for all individuals, variables and times).
In other words, the parent sets πj
p for p ∈P, j ∈J are mutually orthogonal
in the Fisher sense, so that inference for each may be performed separately.
3.2. Eﬃcient, exact joint estimation.
We carry out exact inference in
this setting using belief propagation [Pearl ]. Belief propagation is
an iterative procedure in which messages are passed between variables in
such a way as to compute exact marginal distributions; in this respect belief
propagation belongs to a more general class of iterative algorithms known
as “sum-product” algorithms [Kschischang, Frey and Loeliger ]. Our
algorithm is summarized as follows (for simplicity we suppress dependence
upon ancillary information Zj):
(1) We begin by marginalizing over parameters θj and caching the local
scores P(yj
p) for all parent sets πj
p ∈Gp, all variables p ∈P and all individuals j ∈J ; these could be obtained using any DBN likelihood. In this
paper we exploited conjugate priors to obtain exact expressions for marginal
likelihoods [equation (33), see Appendix C for details].
(2) Following marginalization, the JNI graphical model collapses to the
discrete Bayesian network shown in Figure 2, whose nodes are themselves
JOINT NETWORK INFERENCE
Marginalization of JNI over continuous (unknown) parameters θj. (Shaded nodes
are unobserved. G0 = prior network, G = latent network, Gj = network speciﬁc to individual j, Yj = observables for individual j, Zj = ancillary information available on individual
j. Hyperparameters η, λj, φj are suppressed for clarity. Panel notation is used to indicate
the presence of multiple individuals j ∈J .)
(3) Posterior marginal distributions p(πp|yp,π0
p) and p(πj
p) are then
computed using belief propagation on this discrete Bayesian network. Pseudocode for this step is provided in Algorithm 1 in Appendix D.
Let AJNI denote the P × P matrix of marginal posterior inclusion probabilities for edges in the latent network G, that is, (AJNI)ip := p(i ∈πp|y,G0).
These quantities are analogous to posterior inclusion probabilities in Bayesian
variable selection and are computed, using Bayesian model averaging, as
(AJNI)ip = p(i ∈πp|y,G0) =
1i∈πpp(πp|y,π0
where 1A is the indicator of the event A and similarly for individual networks
JNI)ip := p , we reduced the space of parent sets Gp using
an in-degree sparsity restriction of the form |πj
p| ≤c for all πj
p ∈Gp, p ∈P,
j ∈J . Thus, the cardinality of the space of parent sets |Gp| = O(P c) is
polynomial in P, where it was previously super-exponential. As in variable
selection, the bound c should be chosen large enough that Gp includes the
true data-generating model with high probability.
Caching of selected probabilities is used to avoid redundant recalculation.
Pseudocode is provided in Algorithm 1 in Appendix D, consisting of three
phases of computation. Storage costs are dominated by phases I and II,
each requiring the caching of O(JP 1+c) terms. Phase II dominates computational eﬀort, with total (serial) algorithmic complexity O(J2P 1+2c). However, within-phase computation is “embarrassingly parallel” in the sense that
all calculations are independent (indicated by square parentheses notation
in the pseudocode). In practice, we have found that problems of size P ≤20,
J ≤20, c ≤3 can be solved within minutes using serial computation on a
standard laptop computer. We provide serial and parallel MATLAB R2014a
implementations in Supplement B [Oates et al. ].
OATES, KORKOLA, GRAY AND MUKHERJEE
3.3. Network prior elicitation.
Elicitation of hyperparameters for network priors is an important and nontrivial issue. Here we specify the hyperparameters λ,η in a subjective manner. We do so due to reported diﬃculties
in estimation of hyperparameters for related models [Werhli and Husmeier
 , Dondelinger, L`ebre and Husmeier , Penfold et al. ]. We
present three criteria below that, for the special case of SHD, are simple to
implement and can be used for expert elicitation. These heuristics seek to
relate the hyperparameters to more directly interpretable measures of the
similarity and diﬀerence that they induce between prior, latent and individual networks: (i) First, we note the following formula for the probability of
maintaining the status (present/absent) of a candidate parent i ∈P between
the latent network G and an individual network Gj:
hλ := p(i /∈πj
This probability provides an interpretable way to consider the inﬂuence of λ.
For example, a prior conﬁdence of hλ ≈0.73 that a given edge status in G is
preserved in a particular individual Gj translates into an odds ratio hλ/(1−
hλ) ≈2.7 and a hyperparameter λ ≈1 ]). An analogous equation relates η and hη :=
p(i /∈πp∆π0
p), allowing prior strength to be set in terms of the probability
that an edge status in the prior network G0 is maintained in the latent
network G. (ii) A second, related approach is to consider the expected total
SHD between an individual network Gj and the latent network G:
E(∥Gj −G∥) = P 2(1 −hλ).
This can be interpreted as the average number of edge changes needed to
obtain Gj from G. An analogous equation holds for η and hη. (iii) Third,
in certain applications, the latent network G may not have a direct scientiﬁc interpretation, in which case the criteria presented above may be
unintuitive. Then, hyperparameters can be elicited by consideration of (a)
similarity between individual networks Gj,Gk and (b) concordance of individual networks Gj with the prior network G0 ] for further discussion).
3.4. An information sharing bound.
Below we consider the extent to
which information can be shared between individuals within JNI, providing an upper bound that is attained as the number of individuals J grows
large. To formalize the contribution to inference from information sharing,
we consider the case in which no data is available on a speciﬁc individual
(without loss of generality, individual j = 1) and analytically quantify the extent to which JNI can estimate the true network G1 by “borrowing strength”
JOINT NETWORK INFERENCE
from the data Y2,...,YJ that represent observations on the remaining individuals. (Over-lines will be used to signify the “true” data-generating networks.) As a baseline, write Aj
0 = p(i ∈πj
p|Yj) for the (naive) estimator that
prohibits the sharing of information between individuals. For simplicity we
restrict attention to the case where no network prior is used (η = 0), the
data-generating hyperparameter λ is known and in-degree restrictions are
not in place (c = P). Then, with neither data nor prior information available
on individual 1, it trivially follows that
EY,G,G1,...,GJ|η,λ
where the expectation is taken over all possible data-generating networks
and corresponding data.
From standard, independent network inference we know that consistent
estimation requires unbiasedness of the likelihood function p(Yj|Gj), in the
sense that EYj|Gjp(Yj|Gj) is maximized by Gj = Gj. We therefore begin by
constructing the analogous regularity condition for joint estimation: Write R
for the matrix that encodes the prior metric on G as (R)G,G′ = exp(−λ∥G−
G′∥)/C(λ), where C(λ) = P
G∈G exp(−λ∥G∥). Write S for the matrix of
expected Bayesian scores (S)Gj,Gj = EYj|Gjp(Yj|Gj).
Assumption (Joint regularity).
For each column of the matrix M =
(RSR)G,G, the nondiagonal entries are strictly smaller than the diagonal
entry, that is, MG,G < MG,G for all G ̸= G.
To gain intuition for the joint regularity assumption, consider the special
case where λ →∞; here R = I and we only require that the expected local
Bayesian score (S)Gj,Gj is maximized by Gj = G, that is, we recover the
unbiasedness condition from standard network inference.
Under the joint regularity assumption, there exists 0 < ε < 1
EY,G,G1,...,GJ|η,λ
where f(J) ≤2P 2εJ−1 →0 as J →∞.
See Appendix A.
Comparing equation (11) to (10), we see that information sharing oﬀers
gains in estimation, agreeing with intuition, with larger gains when the true
networks are almost homogeneous (λ large). Moreover, the statistical power
of JNI to estimate G1 converges to its maximum exponentially quickly as
OATES, KORKOLA, GRAY AND MUKHERJEE
4. Results.
The proposed methodology was compared against several
existing network inference algorithms. We restricted attention to methods
that are compatible with time-course data and, following the majority of the
literature, carry out estimation for each individual separately. The computational demands of Niculescu-Mizil and Caruana , Werhli and Husmeier
 , Penfold et al. precluded application in this setting. Speciﬁcally, in the simulated data examples we report below, over 3000 rounds
of inference were performed in total, on problems larger than DREAM4
(P = 10, J = 5). Using the approach of Penfold et al. , these experiments would have required more than 10 years serial computational time;
in contrast, our approach required less than 24 hours serial computation on
a standard laptop. Thus, we consider the following methods:
(i) DBN. A dynamic Bayesian network, as described in Hill et al. ,
including nonlinear interaction terms. For this choice of model it is possible
to construct a fully conjugate set of priors, delivering a closed-form expression for the local Bayesian score P(yj
p,Zj). The model is summarized in
Appendix B.
(ii) IDBN. Spencer, Hill and Mukherjee recently proposed an extension of Hill et al. that allows analysis of data sets that contain
interventions; this is outlined in Appendix B. Interventional DBNs (IDBNs)
inherit the computational advantages of DBNs, in the sense that there is a
closed-form expression for the local Bayesian score, but extend DBNs in a
causal direction. We considered two alternative implementations of IDBNs:
(i) IDBN. The approach of Spencer, Hill and Mukherjee was applied to each individual separately. (ii) Mono IDBN. Data on all individuals
were pooled together and fed into a single IDBN analysis, an approach that
Werhli and Husmeier described as “monolithic.”
(iii) Rel Nets. A popular approach within the bioinformatics community
is to score edges based on Pearson correlation of participating nodes [“relevance networks”; see, e.g., Butte et al. ]. Here, we used a timecourse analogue in which the correlation is calculated between successive
time points.
(iv) LASSO. An ℓ1-penalized likelihood was used to obtain estimates for
coeﬃcients in a linear autoregressive model. Coeﬃcients were estimated for
each variable independently, taking each variable in turn as the response.
The penalty parameters λp were each selected using leave-one-out crossvalidation. Nonzero coeﬃcients indicated the presence of edges. Further details appear in Supplement A [Oates et al. ].
Note that DBN and IDBN are able to integrate a prior network G0, whereas
Rel Nets and LASSO are not. JNI facilitates joint estimation given a suitable
graphical model likelihood. We applied JNI to the DBN and IDBN models
described above. This resulted in several proposed estimators:
JOINT NETWORK INFERENCE
(v) J-DBN. JNI applied to DBN.
(vi) J-IDBN. JNI applied to IDBN.
(vii) Fixed IDBN. Here we formed the likelihood assuming a single graph
for all individuals and the latent network (i.e., G1 = ··· = GJ = G) but with
parameters allowed to diﬀer. This can be considered a joint analogue of
Mono IDBN that allows individual-speciﬁc parameter values.
(viii) AJ-IDBN. A computationally eﬃcient approximation to J-IDBN,
in which the latent network topology is ﬁrst estimated using Fixed IDBN.
This is in turn used as an informative network prior within J independent
rounds of IDBN. In this way information sharing is allowed to occur, but at
the expense of a coherent joint posterior.
In the empirical study below we compare JNI variants (v)–(viii) against
existing methods (i)–(iv).
4.1. Performance metrics.
The proposed methodology addresses three
questions, some or all of which may be of scientiﬁc interest depending on
the application: (i) estimation of the latent network G, (ii) estimation of
individual networks G1,...,GJ, and (iii) estimation of diﬀerences between
individual networks [“diﬀerential networks”; Ideker and Krogan ]. We
quantify performance for each task using the area under the receiver operating characteristic (ROC) curve (AUR). This metric, equivalent to the probability that a randomly chosen true edge is preferred by the inference scheme
to a randomly chosen false edge, summarizes, across a range of thresholds,
the ability to select edges in the data-generating network. AUR may be
computed relative to the true latent network G or relative to the true individual networks Gj, quantifying performance on tasks (i) and (ii), respectively. Both sets of results are presented below, in the latter case averaging
AUR over all individual networks. For (iii), in order to assess ability to estimate diﬀerential networks, we computed AUR scores based on the statistics
ip = |p(i ∈πj
p|y,G0,Zj) −p(i ∈πp|y,G0,Z1,...,ZJ)| that should be close
to one if i ∈πj
p∆πp, otherwise F j
ip should be close to zero.
It is easy to show that inference for the latent network, under only the
prior (i.e., ˆG = G0), attains mean AUR equal to hη. Similarly, prior inference
for the individual networks (i.e., ˆGj = G0) attains mean AUR equal to 1 −
hη −hλ + 2hηhλ. This provides a baseline for the proposed methodology at
tasks (i) and (ii) and allows performance to be decomposed into AUR due
to prior knowledge and AUR contributed through inference.
Using a systematic variation of data-generating parameters, we deﬁned 15
distinct data-generating regimes described below. For all 15 regimes we considered 50 independent data sets; standard errors accompany average AUR
scores. Results presented below use a computationally favorable in-degree
restriction c = 3. In order to check robustness to c, a subset of experiments
OATES, KORKOLA, GRAY AND MUKHERJEE
were repeated using c = 4, with close agreement observed ]).
4.2. Simulation study.
4.2.1. Data generation.
Data were generated according to DBN models
(Appendix B) as described in detail in Supplement A [Oates et al. ].
This data-generating scheme was extended to mimic interventional experiments that are a feature of our application to breast cancer. In this case,
for each time course, a randomly chosen variable is marked as the target
of an interventional treatment. Data were then generated according to the
augmented likelihood described in Appendix B ]. In order to extend this model, which is for a single
cell type, to simulate a heterogeneous population, we selected three protein species per individual (at random) and deleted their outgoing edges
to obtain the data-generating networks Gj ]).
4.2.3. Estimator performance.
We consider the three estimation tasks:
Latent network.
We investigated ability to recover the latent network G.
The existing approaches (i)–(iv) estimate only individual-speciﬁc networks.
For estimation of the latent, shared network using these methods, we simply took an unweighted average of the J estimated adjacency matrices. The
proposed joint estimators (v)–(viii) were assigned hyperparameter values
η = 1,λ = 2 [λ = 3 for Xu et al. ] based on the heuristic of equation (8);
sensitivity to misspeciﬁed hyperparameter values is investigated later in Section 4.2.4. Results based on simulated data with interventions are displayed
in STable 3 ]). We found
little diﬀerence in the ability of J-IDBN, Fixed IDBN and AJ-IDBN to recover the latent network structure across a wide range of regimes, though
JOINT NETWORK INFERENCE
J-IDBN achieved best performance in 9 out of 15 regimes. Interestingly,
we found that the IDBN estimator, which performs an unweighted average of J independent inferences, performed signiﬁcantly worse than each of
J-IDBN, Fixed-IDBN and AJ-IDBN in, respectively, 15, 13 and 11 out of
15 regimes. Similarly, all the above approaches clearly outperformed Mono
IDBN and Rel Nets, which were in turn outperformed by inference based on
the prior alone, demonstrating the importance of accounting for individualspeciﬁc parameter values. The joint formulation of DBNs (J-DBN) significantly outperformed standard DBNs, with higher AUR in all 15 regimes.
LASSO performed best in the regime with long time series (n = 10) but
failed in other regimes to outperform inference based on the prior alone. We
obtained qualitatively similar results for both alternative data-generating
schemes ]).
Individual networks.
At this task, J-IDBN outperformed all other approaches in 9 out of 15 regimes. AJ-IDBN oﬀered a similar level of performance
and together these estimators demonstrated better performance compared
to alternatives in 13 out of 15 regimes. Since AJ-IDBN avoids intensive computation, this may provide a practical estimator of individual networks in
higher dimensional settings. Again, the joint approaches J-IDBN and J-DBN
both outperformed the standard approaches IDBN and DBN, respectively,
demonstrating an increase in statistical power resulting from the proposed
methodology. Rel Nets and LASSO performed poorly at this task. Similar
results were observed using the alternative data-generating schemes ]).
Diﬀerential networks.
Since JNI regularizes between individuals, we sought
to test whether it could eliminate spurious diﬀerences and thereby improve
estimation of diﬀerential networks. Diﬀerential networks may also be estimated using existing methods (i)–(iv); to do so, in each case we compared
individual network estimates with the estimate of the latent network obtained as described in Section 4.2.3 above. We found that, while estimation of diﬀerential networks appears to be more challenging than the other
tasks, J-IDBN outperformed the other approaches in 7 out of 15 regimes.
Moreover, the J-IDBN and J-DBN methods outperformed IDBN and DBN,
respectively, in all 15 regimes. These results suggest that coherence of joint
analysis aids in suppressing spurious features for estimation of diﬀerential
network topology. Rel Nets performed poorly at this task and LASSO performed slightly better. Intriguingly, AJ-IDBN performed well in estimating
diﬀerential networks, performing best in 7 out of 15 regimes. This suggests
that the approximate joint estimator may be suited to estimation of diﬀerential networks. Results on the noninterventional data sets supported this
conclusion ]). On
the Xu et al. data sets, however, IDBN and Rel Nets were among the
OATES, KORKOLA, GRAY AND MUKHERJEE
best performing estimators ]), despite being misspeciﬁed for the nonlinear data-generating
4.2.4. Robustness.
We assess three aspects of robustness:
Hyperparameter misspeciﬁcation.
For the above investigation we used equation (8) to elicit hyperparameters η,λ. This was possible because the datagenerating parameters were known by design, however, in general this will
not be the case. We therefore sought to empirically investigate the eﬀect
of hyperparameter misspeciﬁcation. SFigure 3 ]) displays how performance of the J-IDBN estimator
for latent networks depends on the choice of hyperparameters λ,η. Performance does not appear to be highly sensitive to the precise hyperparameter
values used and there is a large region in which AUR remains high.
Outliers and batch eﬀects.
The biological data sets that motivate this study
often contain outliers. At the same time, experimental design may lead to
batch eﬀects. In order to probe estimator robustness, we generated data
as described above, with the addition of outliers and certain batch eﬀects.
Speciﬁcally, Gaussian noise from the contamination model 0.95N(0,0.12) +
0.05N(0,102) was added to all data prior to inference. At the same time,
one individual’s data were replaced entirely by Gaussian white noise to simulate a (strong) batch eﬀect that could arise, for example, if preparation
of a speciﬁc biological sample was incorrect. The relative decrease in performance at feature detection is reported in SFigure 5 ]). We found that J-IDBN remained the bestperforming estimator for all three estimation problems. However, for the
diﬀerential network estimation task, in particular, the decrease in performance was pronounced for joint methods.
Nonexchangeability.
SFigure 6 ]) displays the result of inference on data where the exchangeability
assumption is violated. It can be seen that the performance of all (exchangeable) estimators decreases in these circumstances, but the magnitude of the
decrease is small (e.g., for estimation of individual networks, J-IDBN experiences a 0.01 decrease in AUR). We note that the proposed estimators can be
extended to nonexchangeable settings where elements of the structure that
relates individuals are known; see Oates and Mukherjee for further
4.3. Protein signaling networks in breast cancer.
We consider experimental data derived from human breast cancer cell lines, focusing on protein
signaling networks within which many (wild type) causal relationships are
JOINT NETWORK INFERENCE
Signaling downstream of the epidermal growth factor receptor (EGFR). The graph
shown summarizes known causal links characterized by extensive biochemistry. (Note that
edges in the graph represent high-level summaries of often complex molecular interactions
that may involve latent chemical species.)
well understood from extensive biochemistry (Figure 3). The investigation
presented below serves three purposes: First, it allows investigation of the
applicability of the proposed joint approaches to experimental data. Second, it allows investigation of the use of ancillary information, in the form
of mutational status and histological information. Finally, the results and
approach are relevant to the topical question of exploring signaling heterogeneity across cancer cell lines.
Data were obtained using reverse-phase protein arrays [Hennessy et al.
 ] from J = 6 breast cancer cell lines ]). Data comprised observations for the P = 17 proteins shown in Figure 3 ]; we note that these data form
part of a larger study including further cell lines and proteins). Speciﬁcally, y
contains the logarithms of the measured concentrations. Data were acquired
under treatment with an EGFR/HER2 inhibitor Lapatinib (“EGFRi”), an
Akt inhibitor (“Akti”), EGFRi and Akti in combination, and without inhibition (“DMSO”) at 0.5, 1, 2, 4, 8 and 24 hours following Serum stimulation,
giving a total of nj = 24 observations of each variable in each individual cell
4.3.1. Informative priors on causal structure.
For the cancer cell lines
analyzed here, ancillary information is available in the form of genetic aberrations (mutation statuses) and histological proﬁling. These were obtained
OATES, KORKOLA, GRAY AND MUKHERJEE
from published sources [Neve et al. ] and online databases [Forbes et
al. ] and reproduced in STable 2 ]). These sources give causally relevant information on structure speciﬁc to the individual cell lines j ∈J . We used this information to
help specify priors on the graphs Gj, considering in particular two cases:
(i) Loss-of-function mutations in kinase domains; in line with the nature of
the mutation, here we set the prior probability on edges emanating from
the mutant protein to zero. Where the mutation is known to also aﬀect the
ability of a protein to be phosphorylated, then incoming edges were also
assigned zero prior probability. (ii) Cell lines with ectopic expression of the
receptor HER2 are known to depend heavily upon EGFR signaling. In this
case the network prior did not penalize edges emanating from the EGFR
receptor nodes. A full discussion of ancillary data appears in Supplement A
[Oates et al. ].
In addition to the cell-line-speciﬁc mutational information above, decades
of experimental work (including interventional, biochemical and biophysical
studies) have provided a wealth of information about (wild type) causal
relationships between nodes. We used this noncell line-speciﬁc information
to specify a prior graph G0 that was common to all cell lines j ∈J (shown
in Figure 3). Cancer signaling is expected to diﬀer with respect to wild type
signaling, but a priori we expect the diﬀerences to be small in number. In
light of this observation, we used subjective elicitation (Section 3.3) to set
hyperparameters λ = 4,η = 5, corresponding to E(∥Gj −G∥) ≈5, E(∥G −
4.3.2. Validation.
In order to test performance, we ﬁrst considered the
latent network G, comparing estimates to the (causal) literature network
shown in Figure 3. For a fair assessment we used an empty prior network
G0. Inferred networks are displayed in SFigure 7 ]). Results demonstrated good recovery of the literature
network, with J-IDBN attaining the highest AUR (0.67, p < 0.01, permutation test; Figure 4). As in the simulation study, J-IDBN outperformed
IDBN, with AJ-IDBN and Fixed IDBN representing good alternative estimators and the remaining estimators performing poorly. This suggests the
conclusions drawn in Section 4.2 apply also to the analysis of biological time
series data. In particular, modeling of interventions appears to be crucial
in this setting, in line with the conclusions of Spencer, Hill and Mukherjee
4.3.3. Inference for cell line networks.
We investigated inference for cell
line-speciﬁc networks Gj (Figure 5), taking the prior network G0 from the
literature (Figure 3). In order to assess results, we exploited the fact that cell
lines AU565 and SKBR3 derive from the same patient. We would therefore
JOINT NETWORK INFERENCE
Results from breast cancer cell line data, comparison with network based on
literature. The methods shown were used to estimate a latent network; AUR is with respect
to the literature-based network shown in Figure 3; the latter was not used to provide prior
information in these experiments. (Asterisks denote AUR scores which were signiﬁcant at
the 1% level under a permutation test with AUR as the statistic and 10,000 samples used
to obtain an empirical null distribution.)
expect these two cell lines to be most similar at the network level. J-IDBN
networks for AU565 and SKBR3 were indeed the most similar, maximizing
the Pearson correlation coeﬃcient between corresponding posterior marginal
inclusion probabilities over all
= 15 pairs of cell lines. In contrast, standard IDBNs did not do so (Figure 6). Figure 7 compares posterior inclusion
probabilities (or analogous edge weights for the non-Bayesian methods) for
AU565 against SKBR3. We ﬁnd posterior edge probabilities from these two
Breast cancer data; cell line-speciﬁc networks inferred by J-IDBN. (Edge width
and color are proportional to posterior marginal inclusion probabilities. The layout of vertices is congruent to Figure 3, which can be used as a key.)
OATES, KORKOLA, GRAY AND MUKHERJEE
Breast cancer data; pairwise similarity between cell line-speciﬁc networks inferred
by J-IDBN (left) and IDBN (right). J-IDBN identiﬁes AU 565 and SKBR 3 as having
the most similar networks; these cell lines were originally derived from the same patient.
In contrast, IDBN does not do so. [Colors denote Bonferroni −log(p) values based on the
Pearson correlation coeﬃcient of posterior inclusion probabilities for pairs of cell lines, so
that red indicates a high degree of similarity. For presentation the diagonal is set to zero.]
lines are closer under JNI estimators compared with standard, independent
estimators. However, a thorough assessment of the accuracy of the individual cell line-speciﬁc networks requires additional experimental work and is
beyond the scope of this paper.
5. Discussion.
We focused on three related structure learning problems
arising in the context of a set of nonidentical but exchangeable units or
individuals:
(1) Estimation of a shared network from the heterogeneous data.
Comparison of posterior edge probabilities obtained from analysis of data from
two breast cancer cell lines (AU 565 and SKBR 3) that were originally derived from the
same patient. The joint estimators J-IDBN and J-DBN improve the Spearman correlation
coeﬃcient (“rho”) between posterior edge probabilities compared to independent inference
using IDBN and DBN.
JOINT NETWORK INFERENCE
(2) Estimation of networks for speciﬁc individuals.
(3) Learning features speciﬁc to individuals (“diﬀerential networks”).
Each problem may be of independent scientiﬁc interest; the joint approaches
investigated here address all three problems simultaneously within a coherent statistical framework. We considered simulated data, with and without
model misspeciﬁcation, as well as proteomic data obtained from cancer cell
lines. For all three problems we demonstrated that a joint analysis performs
at least as well as independent or simpler aggregate analyses.
We considered modular priors (that factorize over nodes) that facilitated
eﬃcient computation. However, it may be useful to consider richer priors
for joint estimation. One possibility that is pertinent to applications in cancer biology would be hierarchical regularization that allows entire pathways
to be either active or inactive. However, we note that this would require
revisiting hyperparameter elicitation since the heuristics we described are
speciﬁc to SHD priors. We restricted the joint model to have equal inverse
temperatures λ1 = ··· = λJ := λ. Relaxing this assumption may improve
robustness to batch eﬀects that target single individuals, since then weak
informativeness (λj ≈0) may be learned from data. It would also be interesting to distinguish between G \ Gj (“loss of function”) and Gj \ G (“gain
of function”) features. In this work we did not explore information sharing
through parameter values θj, yet this may yield more powerful estimators
of network structure in settings where individuals’ parameters θj,θk are not
independent.
The case of exchangeable networks that we considered here represents the
simplest of a more general class of models for related networks. In a sequel
to the present paper [Oates and Mukherjee ], we discuss the case
where multiple individuals are related according to a known tree structure.
In this more general setting, eﬃcient algorithms based on belief propagation
continue to apply, since the tree constraint ensures that the corresponding
factor graph is acyclic and so the sum-product lemma continues to hold
[Kschischang, Frey and Loeliger ]. Still more general (and challenging)
is the case where both the networks and the hierarchical structure that relate
them to one another are unknown. Oates et al. present a ﬁrst
step in this direction, in the context of MAP estimation for nonexchangeable
APPENDIX A: PROOF OF THEOREM
The following Lemma shows that, under the joint regularity assumption,
JNI is a consistent estimator of the true latent network G in the limit J →∞:
Let η = 0. Then under the joint regularity assumption there
exists 0 < ε < 1 such that EY,G,G1,...,GJ|η,λp(G|Y) > 1 −|G|εJ.
OATES, KORKOLA, GRAY AND MUKHERJEE
Since we are using a ﬂat prior (η = 0) on G, we have, suppressing
dependence upon λ,
G∈G p(Y|G),
so from Jensen’s inequality
EY,G1,...,GJ|G,λp(G|Y) ≥
EY,G1,...,GJ|G,λp(Y|G)
G∈G EY,G1,...,GJ|G,λp(Y|G)
EY,G1,...,GJ|G,λp(Y|G)
EY,G1,...,GJ|G,λp(Y|G)
EY,G1,...,GJ|G,λp(Y|G)
EY,G1,...,GJ|G,λp(Y|G)
EYj,Gj|G,λp(Yj|G)
EYj,Gj|G,λp(Yj|G).
The joint regularity assumption is equivalent to the requirement that
EYj,Gj|G,λp(Yj|G) has a unique maximum at G = G, since
EYj,Gj|G,λp(Yj|G) = EGj|G,λEYj|Gj
p(Yj|Gj)p(Gj|G)
[EYj|Gjp(Yj|Gj)]p(Gj|G)
(RT )G,Gj(S)Gj,Gj(R)Gj,G
= (RT SR)G,G = (RSR)G,G,
where we have used that R is symmetric. It follows that
EYj,Gj|G,λp(Yj|G)
EYj,Gj|G,λp(Yj|G) < 1.
We therefore conclude that
EY,G1,...,GJ|G,λp(G|Y) > 1 −|G|εJ.
JOINT NETWORK INFERENCE
Since equation (22) is independent of G, the result follows.
Proof of Theorem.
Since no observables are available on the ﬁrst
individual (Y1 = ∅), we have
p(G1|G)G1.
We also require the “oracle” estimator (O-JNI); this is simply JNI but with
G ﬁxed and known, that is,
p(G1|G)G1.
Note that EG|η,λ∥A1
O-JNI −G1∥= EG1,G|λ∥G −G1∥= P 2(1 −hλ). We begin
by showing that JNI approximates O-JNI:
JNI = (1 −p(G|Y))
and, by the triangle inequality,
(1 −p(G|Y))
≤(1 −p(G|Y)) sup
∥G1∥+ (1 −p(G|Y)) sup
≤2(1 −p(G|Y))P 2.
Again, by the triangle inequality,
JNI −G1∥≤∥A1
O-JNI∥+ ∥A1
O-JNI −G1∥.
Taking expectations and applying the Lemma produces
EY,G|η,λ∥A1
JNI −G1∥≤2P 2|G|εJ−1 + P 2(1 −hλ),
as required.
OATES, KORKOLA, GRAY AND MUKHERJEE
Dynamic Bayesian networks (DBNs). (a) An “unrolled” dynamic Bayesian
network (DBN) showing each variable at successive time points. (b) The corresponding
“static” representation of DBN (a) with exactly one vertex for each variable.
APPENDIX B: DYNAMIC BAYESIAN NETWORKS
For the DBNs used here, an edge (p,q) from p ∈P to q ∈P in Gj ∈G
implies that Y j
q (t), the observed value of variable q in individual j at time
t, depends directly upon Y j
p (t −1), the observed value of p in individual j
at time t −1 [Figure 8(a); note that t indexes the sample index rather than
actual sampling time]. Let Yj denote a vector containing all observations
for individual j. Then Yj(t) is conditionally independent of {Yj(t −τ) :
τ ≥2} given Yj(t −1), θj, Gj and Zj (ﬁrst-order Markov assumption).
These conditional independence relations are conveniently summarized as
a (static) network Gj with exactly P vertices [Figure 8(b)]; note that this
latter network need not be acyclic.
Hill et al. describe a DBN rooted in the Bayesian linear model.
Speciﬁcally, the response Y j
p (t) is predicted by covariates Yj(t −1), that is,
p = X0α + Xj
where ε ∼N(0n×1,σ2In×n). In many cases multiple time series will be available. In this case the vector Yj
p contains the concatenated time series. The
matrix X0 = [1{t=1} 1{t>1}]n×2 contains a term for the initial time point in
each experiment. The elements of Xj
p corresponding to initial observations
p (1) are simply set to zero. Parameters θj
p = {α,β,σ} are speciﬁc to model
p, variable p and individual j. In the simplest case, given data Y = y, the
model-speciﬁc component Xj
p of the design matrix consists of the raw predictors yj
p(t −1), where yj
Z denotes the elements of the vector yj(t −1)
JOINT NETWORK INFERENCE
belonging to the set A, though more complex basis functions may be used,
including interaction terms. For experiments performed in this paper, interaction terms were taken to be all possible products of parent variables,
following Hill et al. .
Spencer, Hill and Mukherjee modeled interventional data by modiﬁcation to the DAG using ideas from causal inference [Pearl ]. We
mention brieﬂy some of the key ideas and refer the interested reader to
the references for full details. A “perfect intervention” corresponds to 100%
removal of the target’s activity with 100% speciﬁcity. In the context of protein phosphorylation, kinases may be intervened upon using chemical agents.
Spencer, Hill and Mukherjee make the simplifying assumptions that
these interventions are perfect [the “perfect out ﬁxed eﬀects” (POFE) approach]. We refer the reader to Spencer, Hill and Mukherjee for an
extended discussion of POFE. This changes the DAG structure to model
the intervention and also estimates an additional ﬁxed eﬀect parameter to
model the change under intervention in the log-transformed data. When
generating data for the simulation study in Section 4.2 we take ﬁxed eﬀects
to equal zero.
APPENDIX C: EXACT MARGINAL LIKELIHOOD FOR DBN
Hill et al. employed an exact Bayesian approach to capture the suitability of the candidate parent set πj
p. In brief, a Jeﬀreys prior p(α,σ|πj
Zj) ∝1/σ for σ > 0 was placed over the common parameters. Prior to inference, the noninterventional components of the design matrix are orthogonalized using the transformation (Xj
l(In −P0)il(Xj
p)lk, where
P0 = X0 ]. A g-prior was placed on regression coeﬃcients [Zellner ], given by
p,φj,Zj ∼N(0b×1,φjσ2(XT
where b = dim(β). Using these priors alongside either DBNs or IDBNs as
outlined above, the marginal likelihood can be obtained in closed-form:
(φj + 1)b/2
In×n −P0 −
p, a = dim(α) and b = dim(β). Empirical
investigations have previously demonstrated good results for network inference based on the above marginal likelihood [Hill et al. , Spencer, Hill
and Mukherjee ].
OATES, KORKOLA, GRAY AND MUKHERJEE
The hyperparameter φj, that is related to the weight of the parameter
prior p(β|α,σ) relative to the data yj
p, was selected in this paper using the
conditional empirical Bayes procedure outlined in George and Foster ,
corresponding to
p) = arg maxgP(yj
For computational eﬃciency, we evaluated the argument over a set of eight
candidate values corresponding to prior weights of 0, 10, 20, 30, 40, 50% and
(100/n)% (the unit information prior). Alternative strategies for eliciting gpriors are discussed in Bayarri et al. , Liang et al. .
APPENDIX D: BELIEF PROPAGATION FOR JNI
Exact inference for JNI is based on belief propagation [Pearl ].
Algorithm 1 displays pseudocode for exact joint model averaging. We also
indicate computational complexity in terms of the number M = |Gp| of possible parent sets and the number J of individuals. Computational complexity
of calculating marginal likelihoods P(yj
p) will partly depend upon sample size n; scaling exponents shown here assume O(n) = O(1). Algorithm 1
contains pseudocode for computation of posterior marginal inclusion probabilities for edges in both the latent network G and individual-speciﬁc networks Gj. For simplicity, we suppress dependence upon ancillary data Zj
throughout.
Algorithm 1 Belief propagation for JNI
1: for p ∈P do
Compute and cache P(yj
p) [∀j ∈J ] [∀πp ∈Gp]
Compute and cache [∀j ∈J ] [∀πp ∈Gp]
p|πp) [O(M)]
Compute and cache [∀j ∈J ] [∀πp,πj
p(πp|yp,π0
p) ∝p(πp|π0
p|πp) [O(J)]
πp∈Gp p(πp|π0
k∈J \{j} P(yk
p|πp)[O(MJ)]
Phase III:
Compute and cache [∀j ∈J ] [∀i ∈P]
p(i ∈πp|y,G0) = P
πp∈Gp 1i∈πpp(πp|yp,π0
p|y,G0) = P
p∈Gp 1i∈πj
11: end for
JOINT NETWORK INFERENCE
Acknowledgments.
We are grateful to the Editor and anonymous referees
for feedback that has improved the content and presentation of this paper.
We would also like to thank J. D. Aston, F. Dondelinger, C. A. Penfold, S.
E. F. Spencer and S. M. Hill for helpful discussion and comments.
SUPPLEMENTARY MATERIAL
Supplement A: Additional results and protocols
(DOI: 10.1214/14-AOAS761SUPPA; .pdf). Includes: Alternative data generating models; robustness to in-degree restriction, outliers, batch eﬀects and
nonexchangeability; ancillary information for breast cancer; inferred wild
type networks for breast cancer.
Supplement B: Computational implementation
(DOI: 10.1214/14-AOAS761SUPPB; .zip). MATLAB R2014a code (serial
and parallel) implementing joint network inference.