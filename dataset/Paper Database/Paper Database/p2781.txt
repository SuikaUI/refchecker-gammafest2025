Corpora Generation for Grammatical Error Correction
Jared Lichtarge∗, Chris Alberti∗, Shankar Kumar∗, Noam Shazeer∗, Niki Parmar∗, Simon Tong∗
Google Research
{lichtarge,chrisalberti,shankarkumar,noam,nikip,simon}@google.com
Grammatical Error Correction (GEC) has
been recently modeled using the sequenceto-sequence framework. However, unlike sequence transduction problems such as machine translation, GEC suffers from the lack
of plentiful parallel data. We describe two approaches for generating large parallel datasets
for GEC using publicly available Wikipedia
The ﬁrst method extracts sourcetarget pairs from Wikipedia edit histories with
minimal ﬁltration heuristics, while the second method introduces noise into Wikipedia
sentences via round-trip translation through
bridge languages. Both strategies yield similar sized parallel corpora containing around
4B tokens. We employ an iterative decoding
strategy that is tailored to the loosely supervised nature of our constructed corpora. We
demonstrate that neural GEC models trained
using either type of corpora give similar performance.
Fine-tuning these models on the
Lang-8 corpus and ensembling allows us to
surpass the state of the art on both the CoNLL-
2014 benchmark and the JFLEG task. We provide systematic analysis that compares the two
approaches to data generation and highlights
the effectiveness of ensembling.
∗Equal contribution.
Listing order is random.
conducted systematic experiments to determine useful variants of the Wikipedia revisions corpus, pre-training and ﬁnetuning strategies, and iterative decoding. Chris implemented
the ensemble and provided background knowledge and resources related to GEC. Shankar ran training and decoding
experiments using round-trip translated data.
Jared, Chris
and Shankar wrote the paper.
Noam identiﬁed Wikipedia
revisions as a source of training data. Noam developed the
heuristics for using the full Wikipedia revisions at scale and
conducted initial experiments to train Transformer models for
GEC. Noam and Niki provided guidance on training Transformer models using the Tensor2Tensor toolkit. Simon proposed using round-trip translations as a source for training
data, and corrupting them with common errors extracted from
Wikipedia revisions. Simon generated such data for this paper.
Introduction
Much progress in the Grammatical Error Correction (GEC) task can be credited to approaching
the problem as a translation task from an ungrammatical source language to
a grammatical target language. This has enabled
Neural Machine Translation (NMT) sequence-tosequence (S2S) models and techniques to be applied to the GEC task . However, the efﬁcacy of NMT techniques
is degraded for low-resource tasks . This poses difﬁculties for S2S
approaches to GEC, as Lang-8, the largest publicly
available parallel corpus, contains only ∼25M
words .
Motivated by
this data scarcity, we present two contrasting approaches to generating parallel data for GEC that
make use of publicly available English language
Wikipedia revision histories12.
Our ﬁrst strategy is to mine real-world errors. We attempt to accumulate source–target pairs
from grammatical errors and their human-curated
corrections gleaned from the Wikipedia revision
histories.
Unlike previous work , we apply minimal
ﬁltering so as to generate a large and noisy corpus of ∼4B tokens (Table 1). As a consequence
of such permissive ﬁltering, the generated corpus
contains a large number of real grammatical corrections, but also noise from a variety of sources,
including edits with drastic semantic changes, imperfect corrections, ignored errors, and Wikipedia
Our second strategy is to synthesize data by
corrupting clean sentences.
We extract target
sentences from Wikipedia, and generate corre-
1 
2Last accessed: December 15, 2017
 
sponding source sentences by translating the target into another language and back. This roundtrip translation introduces relatively clean errors,
so the generated corpus is much less noisy than
the human-derived Wikipedia corpus. However,
these synthetic corruptions, unlike human errors,
are limited to the domain of errors that the translation models are prone to making.
Both approaches beneﬁt from the broad scope of topics in
Wikipedia.
# sentences
Wikipedia Revisions
Round-Trip Translation
Table 1: Statistics computed over extant training sets for
GEC (top) and corpora generated from Wikipedia in this
work (bottom).
We train the Transformer sequence-to-sequence
model on data generated
from the two schemes.
Fine-tuning the models
on the Lang-8 corpus gives us additional improvements which allow a single model to surpass the
state-of-art on both the CoNLL-2014 and the JF-
LEG tasks. Finally, we explore how to combine
the two data sources by comparing a single model
trained on all the data to an ensemble of models.
Data Generation from Wikipedia
Revision Histories
Wikipedia provides a dump of the revision histories of all Wikipedia pages. For each Wikipedia
page, the dump contains chronological snapshots
of the entire content of the page before and after
every submitted edit; thus two consecutive snapshots characterize a single revision to the page.
Because a small number of popular pages see disproportionate trafﬁc, some pages grow very large.
As we are interested in the edits between snapshots, and not identical content that is typically in
higher proportion in the revision histories for the
largest pages, we discard pages larger than 64Mb.
To prevent remaining large pages from skewing
the dataset towards their topics with their many
revisions, we downsample consecutive revisions
from individual pages, selecting only log1.5(n)
pairs for a page with a total of n revisions. This
reduces the total amount of data 20-fold. Each
remaining pair of consecutive snapshots forms a
source–target pair. The process for extracting examples from a page’s revision history is illustrated
in Figure 1.
From the XML of each page in a source–target
pair, we extract and align the text, removing nontext elements. We then probabilistically cut the
aligned text, skipping over non-aligned sequences.
Two cuts bound an example pair, for which the
source sequence is provided by the older snapshot,
and the target sequence by the newer snapshot.
Following extraction of the examples, we do a
small amount of corruption and ﬁltration in order to train a model proﬁcient at both spelling
and grammar correction. We probabilistically introduce spelling errors in the source sequences at
a rate of 0.003 per character, randomly selecting
deletion, insertion, replacement, or transposition
of adjacent characters for each introduced error.
We throw out examples exceeding a maximum
length of 256 word-pieces. The majority of examples extracted by this process have identical source
and target. Since this is not ideal for a GEC parallel corpus, we downsample identity examples by
99% to achieve 3.8% identical examples in the ﬁnal dataset. The data generation scripts we use
have been opensourced3.
In Figure 2, we show examples of extracted
source–target pairs. While some of the edits are
grammatical error corrections, the vast majority
Data Generation from Round-trip
Translations
As an alternative approach to extracting the edits from Wikipedia revisions, we extract sentences
from the identity examples that were discarded
during edit extraction, and generate a separate parallel corpus by introducing noise into those sentences using round-trip translation via a bridge
language. Therefore, the original sentence from
Wikipedia is the target sentence and output of
the round-trip translation is the corresponding
source sentence. The round trip translations introduce noise according to both the weaknesses
of the translation models and the various inherent ambiguities of translation. We create a corrupted dataset using each bridge language.
use French (Fr), German (De), Japanese (Ja) and
Russian (Ru) as bridge languages because they
are high-resource languages and relatively dissim-
3 
blob/master/tensor2tensor/data_generators/wiki_
revision.py
source : target
Revision downsampling
log1.5(n) sampled revisions
Single page history
n revisions
Example pair extraction
extract <text>
Figure 1: Process for extracting source–target pairs from revision history of a Wikipedia page. See Figure 2 for actual
Special terms have been coined to denote many imfortant
technical concepts in the game of Go. Such technical
Players of the game of Go often use jargon terms to describe
situations on the board and surrounding the game. Such technical
What we no wcall "disco balls" was first used in nightclubs in the 1920s.
What we now call "disco balls" were first used in nightclubs in the 1920 's.
Artillery in 1941 and was medically dis-charged
Artillery in 1941 he was later medically discharged with
Examples drawn from Revisions
Examples drawn from Round-Trip Translations
The County of Fitzroy is a county in Queensland,
The County of Fitzroy is a county (a cadastral division) in Queensland,
At the same time, she became a jounalist for news, such as "NHK News
7" and "Shutoken News 845".
At the same time, she became a newscaster for some news shows , such
as "NHK News 7" and "Shutoken News 845".
Aerolineas held a strong company through rthe 90's and
they even aded Sydney as a goal for a little while.
Aerolineas kept on being a strong company thru the 90's and
they even added Sydney as a destination for a little while.
Figure 2: Example source–target pairs from each corpus.
ilar from each other. Thus, we compute a total
of four corrupted datasets.
The translations are
obtained using a competitive machine translation
system .
These round trip translated sentence-pairs contained only a small fraction of identity translations compared to those that are present in realworld GEC corpora. To address this deﬁciency,
we augment this corpus with 2.5% identity translations. Analogous to Section 2, we want the models to learn both spelling and grammar correction.
Thus, we randomly corrupt single characters via
insertion, deletion, and transposition, each with
a probability of 0.005/3. Round-trip translations
do not contain some types of word and phrase errors (e.g., your/you’re, should of/should have) and
so we additionally corrupt the translated text by
stochastically introducing common errors identi-
ﬁed in Wikipedia. We ﬁrst examine the Wikipedia
revision histories to extract edits of up to three
words whose source and target phrases are close
in edit distance, and which do not contain numbers or capitalization. For each of the remaining
edits (original, revised), we compute the probability that the user typed original when they intended
to type revised:
P(original|revised) = C(original, revised)
C(revised)
where C(x) refers to the counts of x in the corpus. We then probabilistically apply these rules to
corrupt the translated text.
This process produces a parallel corpus identical in size to the Wikipedia Revision corpus,
though with vastly different characteristics. Because the target sentences are Wikipedia sentences
that were left unchanged for at least one Wikipedia
revision, they are less likely to contain poor grammar, misspellings, or spam than the target sequences of the revisions data.
Round−trip: pretrained
Edits: pretrained
Edits: Fine−tuned
Round−trip: Fine−tuned
Figure 3: F0.5 with iterative decoding on the CoNLL dev set.
Triangles indicate performance with single-shot decoding.
Also, the errors introduced by round-trip translation are relatively clean, but they represent only
a subset of the domain of real-world errors. In
contrast, the Wikipedia data likely has good coverage of the domain of real-world grammatical errors, but is polluted by signiﬁcant noise. Examples
from both corpora are shown in Figure 2. Examples of round-trip translations for each bridge language are shown in Table 2.
Iterative Decoding
Many sentences that require grammatical correction contain multiple errors. As a result, it can
be difﬁcult to correct all errors in a single decoding pass. This is speciﬁcally a problem when
using models trained on noisy parallel data such
as Lang-8 where the target sentences still contain grammatical errors.
Following other work
on the GEC task , we employ an iterative decoding algorithm that allows the model to make
multiple incremental corrections.
This allows
the model multiple chances to suggest individually high-conﬁdence changes, accruing incremental improvements until it cannot ﬁnd any more edits to make.
Our iterative decoding algorithm is presented in
Algorithm 1. Given the source sentence S and a
hypothesis H, Cost(H) refers to the negative log
probability −logP(H|S) using the sequence-tosequence model. In each iteration, the algorithm
performs a conventional beam search but is only
allowed to output a rewrite (non-identity translation) if it has high conﬁdence i.e., its cost is less
than the cost of the identity translation times a prespeciﬁed threshold. Using iterative decoding allows a stricter threshold value than what is optimal for single-shot decoding, as a change ignored
for being low conﬁdence in one decoding iteration
may be selected in the next.
Using incremental edits produces a signiﬁcant
improvement in performance over single-shot decoding for models trained on the Wikipedia revision data, a highly noisy corpus, while models
trained on the relatively clean round-trip translation data see no improvment. All models ﬁnetuned
on Lang-8 see improvement with iterative decoding (Figure 3, Table 3).
Algorithm 1: Iterative Decoding
Data: I, beam, threshold, MAXITER
Result: ˆT
for i ∈{1, 2, ..., MAXITER} do
Nbestlist = Decode(I, beam)
CIdentity = +∞
CNon-Identity = +∞
HNon-Identity = NULL
for H ∈Nbestlist do
if H = I then
CIdentity = Cost(H);
else if Cost(H) < CNon-Identity then
CNon-Identity = Cost(H)
HNon-Identity = H
▷Rewrite if non-identity cost < identity cost
if CNon-Identity/CIdentity < threshold then
ˆT = HNon-Identity
▷Output rewrite.
▷Output identity.
▷Input for next iteration.
In Table 4, we show an example of iterative decoding in action. The model continues to reﬁne
the input until it reaches a sentence that does not
require any edits. We generally see fewer edits being applied as the model gets closer to the ﬁnal
In this work, we use the Transformer sequence-tosequence model , using the
Tensor2Tensor opensource implementation.4 We
use 6 layers for both the encoder and the decoder, 8
attention heads, embedding size dmodel = 1024, a
position-wise feed forward network at every layer
of inner size dff = 4096, and Adafactor as optimizer with inverse squared root decay 5. The word tokens are split into
subwords using a variant of the byte-pair encoding technique , described
in Schuster and Nakajima .
We train the Transformer model for 5 epochs
4 
5We used the “transformer clean big tpu” setting.
“The Adventures of Patchhead“ makes its second and ﬁnal appearance.
Bridge Language
“The Adventures of Patchhead “ makes his secnod and ﬁnal appearance.
“The Adventures of Patchhead” makes its second and last appearance.
“The Adventures of Patchhead” makes its second and last apparance.
“Patchhead Adventure” is the ﬁnal appearance of the second time.
He is not so tolerant of the shortcomings of those outside his family.
Bridge Language
He is not so tolerant of the weaknesses of those outside his family.
He is not so tolerant to the defects of the outside of his family.
He is not so tolerant of the shortcomings of those outside his family,.
He is not so tolerant of the shortcomings of those outside his family.
Table 2: Example sentences generated via round-trip translation with introduced spelling errors.
CoNLL-2014
single-shot
single-shot
single-shot
single-shot
Table 3: Comparing iterative decoding to single-shot decoding for two models, trained on all Wikipedia revisions data
and on all round-trip translation (RTT) data.
this is nto the pizzza that i ordering
this is not the pizza that I ordering
This is not the pizza that I ordering
This is not the pizza that I ordered
This is not the pizza that I ordered.
This is not the pizza that I ordered.
Table 4: Iterative decoding on a sample sentence.
with a batch size of approximately 64,000 word
pieces. While training on the Wikipedia corpora,
we set the learning rate to 0.01 for the ﬁrst 10,000
steps, then decrease it proportionally to the inverse
square root of the number of steps after that.
We then ﬁnetune our models on Lang-8 for 50
epochs and use a constant learning rate of 3 ×
10−5. We stop the ﬁne-tuning before the models
start to overﬁt on a development set drawn from
Experiments
Evaluation
We report results on the CoNLL-2014 test set and the JFLEG test set . Our initial experiments with iterative decoding showed that increasing beam sizes beyond 4 did not yield improvements in performance. Thus, we report all results
using a beam size of 4. Our ensemble models are
obtained by decoding with 4 identical Transformers trained and ﬁnetuned separately. Ensembles
of neural translation systems are typically constructed by computing the logits from each individual system and combining them using either an
arithmetic average or a geometric average . Similar
to Cromieres et al. , we ﬁnd that a geometric average outperforms an arithmetic average.
Hence, we report results using only this scheme.
 ,
we preprocess JFLEG development and test sets
with a spell-checking component but do not
apply spelling correction to CoNLL sets.
CoNLL sets, we pick the best iterative decoding
threshold and number of iterations on a subset of
the CoNLL-2014 training set, sampled to have the
same ratio of modiﬁed to unmodiﬁed sentences as
the CoNLL-2014 dev set. For JFLEG, we pick the
best decoding threshold on the JFLEG dev set.We
report performance of our models by measuring
F0.5 with the M2 scorer on the CoNLL-2014 dev and test sets, and
the GLEU+ metric on the
JFLEG dev and test sets. Table 5 reports statistics
computed over the development and test sets.
Test/Dev Set
# sentences
# annotators
CoNLL-2014 dev
CoNLL-2014 test
JFLEG test
Table 5: Statistics for test/dev data.
Data from Wikipedia Revisions
In extracting examples from Wikipedia revision
histories, we set a number of variables, selecting
Revision Dataset
CoNLL-2014
Default setting
Max-edit-28
Max-edit-6
Dwnsample-1.35
+ ﬁnetuning on Lang-8
Default setting
Max-edit-28
Max-edit-6
Dwnsample-1.35
Lang-8 only
Table 6: Performance of the models trained on variants of
data extracted from Wikipedia revision histories (top panel)
and then ﬁne-tuned on Lang-8 (bottom panel), and of a model
trained only on Lang-8 with the same architecture.
rate of revision downsampling, and maximum edit
distance. We generate four data sets using variations of these values: Default setting uses the
default values described in Section 2, Max-edit-
28 and Max-edit-6 correspond to maximum edit
distance of 28 and 6 wordpieces respectively, and
Dwnsample-1.35 corresponds to a revision downsampling rate of log1.35(n) for a page with a total
of n revisions (whereas the default setting uses a
rate of log1.5(n)). We train a ﬁfth model on the
union of the datasets. Table 6 shows that varying
the data generation parameters led to modest variation in performance, but training on the union of
the diverse datasets did not yield any beneﬁt. Finetuning yields large improvements for all models.
As a sanity check, we also trained a model only
on Lang-8 with the same architecture. All pretrained and ﬁne-tuned models substantially outperform this Lang-8 only model, conﬁrming the
usefulness of pre-training.
Round Trip Translations
As for the Revision data, we train a model on each
of the round-trip translation datasets, and a ﬁfth
model on the union of their data, then ﬁne-tune all
models. The results are shown in Table 7. Using
Japanese as the bridge language gives the best performance on CoNLL-2014, even when compared
to the model trained on all round-trip data. This
is likely because the error patterns generated using Japanese round-trip translations are very similar to those in CoNLL-2014 set, created from
non-native speakers of English .
Pooling all round-trip translations dilutes this similarity and lowers performance on CoNLL-2014.
However, the model trained on all data performs
best on the JFLEG set, which has a different distribution of errors relative to CoNLL-2014 . After ﬁne-tuning, all round-trip models perform considerably better than the Lang-8
Bridge Language
CoNLL-2014
Round-Trip Translations
+ ﬁnetuning on Lang-8
Lang-8 only
Table 7: Performance of the models trained on the roundtrip translations (top panel) and ﬁne-tuned on Lang8 (bottom
panel) and of a model trained only on Lang-8 with the same
architecture.
Combining Data Sources
Having generated multiple diverse datasets, we investigate strategies for utilizing combinations of
data from multiple sources. For each corpus, we
train a single model on all data and compare its
performance to an ensemble of the 4 individuallytrained models (Table 8). The ensemble clearly
outperforms the single model for both types of
data. We additionally train a single model on the
union of all Revisions and Round-Trip Translated
datasets reported on in Tables 6 and 7, which we
compare to an ensemble of the 8 models trained
individually on those datasets.
When Wikipedia edits are combined with the
round-trip translations, the single-model performance remains unchanged on CoNLL-2014, while
the ensemble shows an improvement. This suggests that when utilizing disparate sources of data,
an ensemble is preferable to combining the data.
Comparison with Other systems
We compare the performance of our best individual system, trained on all revisions, the best ensemble of 8 models trained from both revisions
and roundtrip translations on the CoNLL-2014
and JFLEG datasets (Table 9).
We only report
performance of models that use publicly available
CoNLL-2014
Ensemble (4)
Round-Trip Translations
Ensemble (4)
Revisions + Round-Trip Translations
Ensemble (8)
Table 8: Combining datasets using either a single model
trained on all data versus an ensemble of models. All models
are ﬁne-tuned on Lang-8.
Lang-8 and CoNLL datasets. Our single system
trained on all revisions outperforms all previous
systems on both datasets, and our ensemble improves upon the single system result6.
Error Analysis
All models trained on Wikipedia-derived data are
demonstrated to beneﬁt signiﬁcantly from ﬁnetuning on Lang-8 (Tables 6 and 7). In Table 10,
we compare example corrections proposed by
two Wikipedia-derived models to the corrections
proposed by their ﬁne-tuned counterparts.
changes proposed by the revisions-trained model
often appear to be improvements to the original
sentence, but fall outside the scope of GEC. Models ﬁnetuned on Lang-8 learn to make more conservative corrections.
The ﬁnetuning on Lang-8 can be viewed as an
adaptation technique that shifts the model from
the Wikipedia-editing task to the GEC task. On
Wikipedia, it is common to see substantial edits that make the text more concise and readable,
replacing “which is RFID for short” with
“(RFID)”, or removing less important clauses like
“Then we can see that”.
But these are not appropriate for GEC as they are editorial style ﬁxes
rather than grammatical ﬁxes. The models trained
on round-trip translation seem to be make fewer
drastic changes.
Table 11 reports F0.5 across broad error categories for models trained from revisions and
round-trip translations on the CoNLL-2014 test
6Using non-public sentences beyond the regular Lang-8
and CoNLL datasets, Tao et al. recently obtained
an F0.5 of 61.3 on CoNLL-2014 and a GLEU of 62.4 on JF-
LEG. Using ﬁnetuning data beyond the standard datasets, we
obtain an F0.5 of 62.8 on CoNLL-2014 and a GLEU of 65.0
set. The error categories were tagged using the
approach in Bryant et al. . Although the
overall F0.5 of the 2 ensembles are similar, there
are notable differences on speciﬁc categories. The
ensemble using round-trip translation performs
considerably better on prepositions and pronouns
while the revision ensemble is better on morphology and orthography. Thus, each system may have
advantages on speciﬁc domains.
Related Work
Progress in GEC has accelerated rapidly since
CoNLL-2014
2014). Rozovskaya and Roth combined a
Phrase Based Machine Translation (PBMT) model
trained on the Lang-8 dataset with error speciﬁc classiﬁers.
Dowmunt and Grundkiewicz combined a
PBMT model with bitext features and a larger language model. The ﬁrst Neural Machine Translation (NMT) model to reach the state of the art on
CoNLL-2014 used
an ensemble of four convolutional sequence-tosequence models followed by rescoring. The current state of the art using publicly available Lang-8 and CoNLL
data was achieved by Grundkiewicz and Junczys-
Dowmunt with a hybrid PBMT-NMT system. A neural-only result with an F0.5 of 56.1 on
CoNLL-2014 was reported by Junczys-Dowmunt
et al. using an ensemble of neural Transformer models , where the
decoder side of each model is pretrained as a language model. From a modeling perspective, our
approach can be viewed as a direct extension of
this last work. Rather than pretraining only the decoder as a language model, we pretrain on a large
amount of parallel data from either Wikipedia revision histories or from round-trip translations.
While pretraining on out-of-domain data has been
employed previously for neural machine translation , it has not been
presented in GEC thus far, perhaps due to the absence of such large datasets.
Tao et al. 
apply iterative decoding, where two neural models, trained in left-to-right and right-to-left directions, are applied in an interleaved manner. Similar to their study, we ﬁnd that iterative decoding
can improve the performance of GEC.
Prior work 
MLConvembed
Ensemble (4) +EO +LM +SpellCheck
Junczys-Dowmunt et al. 
Single Transformer
Ensemble (4)
Ensemble (4) +LM
Grundkiewicz and Junczys-Dowmunt 
Hybrid PBMT +NMT +LM
Best Single Model
Best Ensemble
Table 9: Comparison of recent state-of-the-art models (top) and our best single-system and ensemble models (bottom) on the
CoNLL-2014 and JFLEG datsets. Only systems trained with publicly available Lang-8 and CoNLL datasets are reported.
Recently, a new coming surveillance technology called radio-frequency identiﬁcation which is RFID
for short has caused heated discussions on whether it should be used to track people.
Recently, a surveillance technology called radio frequency identiﬁcation (RFID) has caused heated
discussions on whether it should be used to track people.
+ﬁnetuning
Recently, a new surveillance technology called radio-frequency identiﬁcation, which is RFID for
short, has caused heated discussions on whether it should be used to track people.
Recently, a new coming surveillance technology called radio-frequency identiﬁcation, which is RFID
for short, has caused heated discussions on whether it should be used to track people.
Round-Trip
Recently, a new coming surveillance technology called radio-frequency identiﬁcation which is RFID
for short has caused heated discussions on whether it should be used to track people.
+ﬁnetuning
Recently, a new upcoming surveillance technology called radio-frequency identiﬁcation which is RFID
for short has caused heated discussions on whether it should be used to track people.
Recently, a new surveillance technology called radio-frequency identiﬁcation which is RFID for short
has caused heated discussions on whether it should be used to track people.
Then we can see that the rising life expectancies can also be viewed as a challenge for us to face.
The rising life expectancy can also be viewed as a challenge for people to face.
+ﬁnetuning
Then we can see that the rising life expectancy can also be viewed as a challenge for us to face.
Then we can see that the rising life expectancies can also be viewed as a challenge for us to face.
Round-Trip
Then we can see that the rising life expectancies can also be viewed as a challenge for us to face.
+ﬁnetuning
Then we can see that the rising life expectancy can also be viewed as a challenge for us to face.
Then we can see that the rising life expectancies can also be viewed as a challenge for us to face.
Table 10: Corrections from models trained on (a) Wikipedia revisions and (b) round-trip translations using Japanese as a bridge
language, along with suggestions from their Lang-8 ﬁnetuned counterparts. Also shown are the corrections from the ensembles
of 4 wikipedia models as well as 4 models trained on round trip translations. Example sentences are from the CoNLL-2014 dev
2010), has investigated multiple strategies
for generating artiﬁcial errors in GEC.
et al. show that preposition corrections
extracted from Wikipedia revisions improve the
quality of a GEC model for correcting preposition
errors. Back-translation addresses data sparsity by introducing noise into a clean corpus using a translation model trained in the clean to noisy direction. However, training such a reverse translation
model also requires access to parallel data which
is scarce for GEC. In contrast, round-trip translation attempts to introduce noise via bridge translations. Round-trip translations have been investigated for GEC. Madnani et al.
Madnani et al.
 combine round-trip translations to generate
a lattice from which the best correction is extracted
using a language model. D´esilets et al. use
round-trip translations for correcting preposition
errors. In contrast to these approaches, we employ round-trip translations for generating a large
parallel training corpus for neural GEC models.
Discussion
Motivated by data scarcity for the GEC task, we
present two contrasting approaches for generating large parallel corpora from the same publicly
available data source. We believe both techniques
offer promising research avenues for further development on the task.
We show that models trained exclusively on
minimally ﬁltered English Wikipedia revisions
can already be valuable for the GEC task. This
approach can be easily extended to the many other
languages represented in Wikipedia, presenting an
Error Type
Round-trip Translations
Pre-trained
Fine-tuned
Pre-trained
Fine-tuned
Determiner
Morphology
Orthography
Preposition
Punctuation
Word Order
Table 11: F0.5 across error categories on the CoNLL-2014 test set.
opportunity to extend GEC into languages that
may have no extant GEC corpora. While we expect pre-training on Wikipedia to give us a reasonable model, it may be crucial to ﬁne-tune this
model on small amounts of clean, in-domain corpora to achieve good performance.
When extracting examples from the Wikipedia
revisions, we implemented minimal ﬁltration in
pursuit of simplicity, and to produce a sufﬁciently
large dataset.
Implementing more complex ﬁltration in order to reduce the noise in the generated dataset will likely be a productive avenue to increase the value of this approach. The
performance achieved by the reported Wikipedia
revisions-trained models, both with and without
ﬁnetuning, may be used as a baseline by which
to evaluate smaller, cleaner datasets drawn from
Wikipedia revisions.
Round-trip translation takes advantage of the
advanced state of the task of Machine Translation relative to GEC by leveraging extant translation models as a source of grammatical-style
data corruption. In this work, we only experiment
with producing English-language GEC corpora,
but this technique can be extended to any of the
many languages for which translation models exist. It would be useful to assess how the translation
quality inﬂuences the performance of the resulting
GEC model. In our experiments with round-trip
translation, we used target sentences drawn from
Wikipedia to maintain a reasonable comparability between the two techniques. However, there is
no constraint preventing the application of roundtrip translation to diverse data sources; any source
of clean text can be turned into a parallel GEC
corpus. This can be used to increase diversity in
the generated data, or to generate domain-speciﬁc
GEC corpora (e.g. patents).
We observe that pooling two diverse data
sources used to train competitively performing
models on the same task can degrade performance.
This suggests that within datasets useful for a speciﬁc task, there may be greater value to be discovered in ﬁnding optimal partitions of the data for
training models which can then be combined using
ensembles. Prior work in combining diverse data
sources includes addition of special tokens and meta-learning and text
simpliﬁcation .
Acknowledgements
We thank Jayakumar Hoskere,
Emily Pitler,
Slav Petrov, Daniel Andor, Alla Rozovskaya and
Antonis Anastasopoulos for helpful suggestions.
We also thank Jayakumar Hoskere, Shruti Gupta
and Anmol Gulati for providing various GEC
resources that were used in this paper.