Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 3982–3992,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
Nils Reimers and Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
www.ukp.tu-darmstadt.de
BERT and RoBERTa has set a new state-of-the-art
performance on sentence-pair regression tasks
like semantic textual similarity (STS). However, it requires that both sentences are fed
into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences
requires about 50 million inference computations (~65 hours) with BERT. The construction
of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering.
In this publication, we present Sentence-BERT
(SBERT), a modiﬁcation of the pretrained
BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the
effort for ﬁnding the most similar pair from 65
hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.
We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks,
where it outperforms other state-of-the-art
sentence embeddings methods.1
Introduction
In this publication, we present Sentence-BERT
(SBERT), a modiﬁcation of the BERT network using siamese and triplet networks that is able to
derive semantically meaningful sentence embeddings2. This enables BERT to be used for certain
new tasks, which up-to-now were not applicable
for BERT. These tasks include large-scale seman-
1Code available: 
sentence-transformers
2With semantically meaningful we mean that semantically
similar sentences are close in vector space.
tic similarity comparison, clustering, and information retrieval via semantic search.
BERT set new state-of-the-art performance on
various sentence classiﬁcation and sentence-pair
regression tasks. BERT uses a cross-encoder: Two
sentences are passed to the transformer network
and the target value is predicted. However, this
setup is unsuitable for various pair regression tasks
due to too many possible combinations. Finding
in a collection of n = 10 000 sentences the pair
with the highest similarity requires with BERT
n·(n−1)/2 = 49 995 000 inference computations.
On a modern V100 GPU, this requires about 65
hours. Similar, ﬁnding which of the over 40 million existent questions of Quora is the most similar
for a new question could be modeled as a pair-wise
comparison with BERT, however, answering a single query would require over 50 hours.
A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences
are close. Researchers have started to input individual sentences into BERT and to derive ﬁxedsize sentence embeddings. The most commonly
used approach is to average the BERT output layer
(known as BERT embeddings) or by using the output of the ﬁrst token (the [CLS] token). As we
will show, this common practice yields rather bad
sentence embeddings, often worse than averaging
GloVe embeddings .
To alleviate this issue, we developed SBERT.
The siamese network architecture enables that
ﬁxed-sized vectors for input sentences can be derived.
Using a similarity measure like cosinesimilarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These
similarity measures can be performed extremely
efﬁcient on modern hardware, allowing SBERT
to be used for semantic similarity search as well
as for clustering. The complexity for ﬁnding the
most similar sentence pair in a collection of 10,000
sentences is reduced from 65 hours with BERT to
the computation of 10,000 sentence embeddings
(~5 seconds with SBERT) and computing cosinesimilarity (~0.01 seconds).
By using optimized
index structures, ﬁnding the most similar Quora
question can be reduced from 50 hours to a few
milliseconds .
We ﬁne-tune SBERT on NLI data, which creates sentence embeddings that signiﬁcantly outperform other state-of-the-art sentence embedding
methods like InferSent and
Universal Sentence Encoder . On
seven Semantic Textual Similarity (STS) tasks,
SBERT achieves an improvement of 11.7 points
compared to InferSent and 5.5 points compared to
Universal Sentence Encoder. On SentEval , an evaluation toolkit for
sentence embeddings, we achieve an improvement
of 2.1 and 2.6 points, respectively.
SBERT can be adapted to a speciﬁc task.
sets new state-of-the-art performance on a challenging argument similarity dataset and on a triplet dataset to distinguish sentences from different sections of a Wikipedia article .
The paper is structured in the following way:
Section 3 presents SBERT, section 4 evaluates
SBERT on common STS tasks and on the challenging Argument Facet Similarity (AFS) corpus
 . Section 5 evaluates SBERT
on SentEval. In section 6, we perform an ablation
study to test some design aspect of SBERT. In section 7, we compare the computational efﬁciency of
SBERT sentence embeddings in contrast to other
state-of-the-art sentence embedding methods.
Related Work
We ﬁrst introduce BERT, then, we discuss stateof-the-art sentence embedding methods.
BERT is a pre-trained
transformer network , which
set for various NLP tasks new state-of-the-art results, including question answering, sentence classiﬁcation, and sentence-pair regression. The input
for BERT for sentence-pair regression consists of
the two sentences, separated by a special [SEP]
token. Multi-head attention over 12 (base-model)
or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the ﬁnal label. Using this setup, BERT set a
new state-of-the-art performance on the Semantic
Textual Semilarity (STS) benchmark . RoBERTa showed, that
the performance of BERT can further improved by
small adaptations to the pre-training process. We
also tested XLNet , but it led in
general to worse results than BERT.
A large disadvantage of the BERT network
structure is that no independent sentence embeddings are computed, which makes it difﬁcult to derive sentence embeddings from BERT. To bypass
this limitations, researchers passed single sentences through BERT and then derive a ﬁxed sized
vector by either averaging the outputs (similar to
average word embeddings) or by using the output
of the special CLS token ; Zhang et al. ; Qiao et al. ).
These two options are also provided by the popular bert-as-a-service-repository3. Up to our knowledge, there is so far no evaluation if these methods
lead to useful sentence embeddings.
Sentence embeddings are a well studied area
with dozens of proposed methods. Skip-Thought
 trains an encoder-decoder architecture to predict the surrounding sentences.
InferSent uses labeled
data of the Stanford Natural Language Inference
dataset and the Multi-
Genre NLI dataset to train
a siamese BiLSTM network with max-pooling
over the output.
Conneau et al. showed, that
InferSent consistently outperforms unsupervised
methods like SkipThought.
Universal Sentence
Encoder trains a transformer
network and augments unsupervised learning with
training on SNLI. Hill et al. showed, that
the task on which sentence embeddings are trained
signiﬁcantly impacts their quality. Previous work
 found that
the SNLI datasets are suitable for training sentence embeddings. Yang et al. presented
a method to train on conversations from Reddit
using siamese DAN and siamese transformer networks, which yielded good results on the STS
benchmark dataset.
Humeau et al. addresses the run-time
overhead of the cross-encoder from BERT and
present a method (poly-encoders) to compute
a score between m context vectors and pre-
3 
bert-as-service/
Sentence A
Sentence B
(u, v, |u-v|)
Softmax classifier
Figure 1: SBERT architecture with classiﬁcation objective function, e.g., for ﬁne-tuning on SNLI dataset.
The two BERT networks have tied weights (siamese
network structure).
computed candidate embeddings using attention.
This idea works for ﬁnding the highest scoring
sentence in a larger collection.
However, polyencoders have the drawback that the score function
is not symmetric and the computational overhead
is too large for use-cases like clustering, which
would require O(n2) score computations.
Previous neural sentence embedding methods
started the training from a random initialization.
In this publication, we use the pre-trained BERT
and RoBERTa network and only ﬁne-tune it to
yield useful sentence embeddings. This reduces
signiﬁcantly the needed training time: SBERT can
be tuned in less than 20 minutes, while yielding
better results than comparable sentence embedding methods.
SBERT adds a pooling operation to the output
of BERT / RoBERTa to derive a ﬁxed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token,
computing the mean of all output vectors (MEANstrategy), and computing a max-over-time of the
output vectors (MAX-strategy). The default conﬁguration is MEAN.
In order to ﬁne-tune BERT / RoBERTa, we create siamese and triplet networks to update the weights such that the produced
sentence embeddings are semantically meaningful
and can be compared with cosine-similarity.
The network structure depends on the available
Sentence A
Sentence B
cosine-sim(u, v)
Figure 2: SBERT architecture at inference, for example, to compute similarity scores. This architecture is
also used with the regression objective function.
training data. We experiment with the following
structures and objective functions.
Classiﬁcation Objective Function. We concatenate the sentence embeddings u and v with
the element-wise difference |u−v| and multiply it
with the trainable weight Wt ∈R3n×k:
o = softmax(Wt(u, v, |u −v|))
where n is the dimension of the sentence embeddings and k the number of labels. We optimize
cross-entropy loss. This structure is depicted in
Regression Objective Function. The cosinesimilarity between the two sentence embeddings
u and v is computed (Figure 2). We use meansquared-error loss as the objective function.
Triplet Objective Function. Given an anchor
sentence a, a positive sentence p, and a negative
sentence n, triplet loss tunes the network such that
the distance between a and p is smaller than the
distance between a and n.
Mathematically, we
minimize the following loss function:
max(||sa −sp|| −||sa −sn|| + ϵ, 0)
with sx the sentence embedding for a/n/p, || · ||
a distance metric and margin ϵ. Margin ϵ ensures
that sp is at least ϵ closer to sa than sn. As metric
we use Euclidean distance and we set ϵ = 1 in our
experiments.
Training Details
We train SBERT on the combination of the SNLI
 and the Multi-Genre NLI
Avg. GloVe embeddings
Avg. BERT embeddings
BERT CLS-vector
InferSent - Glove
Universal Sentence Encoder
SBERT-NLI-base
SBERT-NLI-large
SRoBERTa-NLI-base
SRoBERTa-NLI-large
Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels
for various Textual Similarity (STS) tasks. Performance is reported by convention as ρ × 100. STS12-STS16:
SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.
 dataset. The SNLI is a collection of 570,000 sentence pairs annotated with
the labels contradiction, eintailment, and neutral.
MultiNLI contains 430,000 sentence pairs
and covers a range of genres of spoken and written
text. We ﬁne-tune SBERT with a 3-way softmaxclassiﬁer objective function for one epoch.
used a batch-size of 16, Adam optimizer with
learning rate 2e−5, and a linear learning rate
warm-up over 10% of the training data. Our default pooling strategy is MEAN.
Evaluation - Semantic Textual
Similarity
We evaluate the performance of SBERT for common Semantic Textual Similarity (STS) tasks.
State-of-the-art methods often learn a (complex)
regression function that maps sentence embeddings to a similarity score. However, these regression functions work pair-wise and due to the combinatorial explosion those are often not scalable if
the collection of sentences reaches a certain size.
Instead, we always use cosine-similarity to compare the similarity between two sentence embeddings. We ran our experiments also with negative Manhatten and negative Euclidean distances
as similarity measures, but the results for all approaches remained roughly the same.
Unsupervised STS
We evaluate the performance of SBERT for STS
without using any STS speciﬁc training data. We
use the STS tasks 2012 - 2016 , the STS benchmark , and the SICK-Relatedness dataset
 . These datasets provide labels between 0 and 5 on the semantic relatedness
of sentence pairs. We showed in that Pearson correlation is badly suited for
STS. Instead, we compute the Spearman’s rank
correlation between the cosine-similarity of the
sentence embeddings and the gold labels.
setup for the other sentence embedding methods
is equivalent, the similarity is computed by cosinesimilarity. The results are depicted in Table 1.
The results shows that directly using the output
of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLStoken output only achieves an average correlation
of 29.19. Both are worse than computing average
GloVe embeddings.
Using the described siamese network structure
and ﬁne-tuning mechanism substantially improves
the correlation, outperforming both InferSent and
Universal Sentence Encoder substantially.
only dataset where SBERT performs worse than
Universal Sentence Encoder is SICK-R. Universal
Sentence Encoder was trained on various datasets,
including news, question-answer pages and discussion forums, which appears to be more suitable
to the data of SICK-R. In contrast, SBERT was
pre-trained only on Wikipedia (via BERT) and on
While RoBERTa was able to improve the performance for several supervised tasks, we only
observe minor difference between SBERT and
SRoBERTa for generating sentence embeddings.
Supervised STS
The STS benchmark (STSb) provides is a popular dataset to evaluate supervised
STS systems. The data includes 8,628 sentence
pairs from the three categories captions, news, and
forums. It is divided into train (5,749), dev (1,500)
and test (1,379). BERT set a new state-of-the-art
performance on this dataset by passing both sentences to the network and using a simple regres-
sion method for the output.
Not trained for STS
Avg. GloVe embeddings
Avg. BERT embeddings
InferSent - GloVe
Universal Sentence Encoder
SBERT-NLI-base
SBERT-NLI-large
Trained on STS benchmark dataset
BERT-STSb-base
84.30 ± 0.76
SBERT-STSb-base
84.67 ± 0.19
SRoBERTa-STSb-base
84.92 ± 0.34
BERT-STSb-large
85.64 ± 0.81
SBERT-STSb-large
84.45 ± 0.43
SRoBERTa-STSb-large
85.02 ± 0.76
Trained on NLI data + STS benchmark data
BERT-NLI-STSb-base
88.33 ± 0.19
SBERT-NLI-STSb-base
85.35 ± 0.17
SRoBERTa-NLI-STSb-base
84.79 ± 0.38
BERT-NLI-STSb-large
88.77 ± 0.46
SBERT-NLI-STSb-large
86.10 ± 0.13
SRoBERTa-NLI-STSb-large
86.15 ± 0.35
Table 2: Evaluation on the STS benchmark test set.
BERT systems were trained with 10 random seeds and
4 epochs. SBERT was ﬁne-tuned on the STSb dataset,
SBERT-NLI was pretrained on the NLI datasets, then
ﬁne-tuned on the STSb dataset.
We use the training set to ﬁne-tune SBERT using the regression objective function. At prediction time, we compute the cosine-similarity between the sentence embeddings. All systems are
trained with 10 random seeds to counter variances
 .
The results are depicted in Table 2.
We experimented with two setups:
Only training on
STSb, and ﬁrst training on NLI, then training on
STSb. We observe that the later strategy leads to a
slight improvement of 1-2 points. This two-step
approach had an especially large impact for the
BERT cross-encoder, which improved the performance by 3-4 points. We do not observe a signiﬁcant difference between BERT and RoBERTa.
Argument Facet Similarity
We evaluate SBERT on the Argument Facet Similarity (AFS) corpus by Misra et al. . The
AFS corpus annotated 6,000 sentential argument
pairs from social media dialogs on three controversial topics: gun control, gay marriage, and
death penalty. The data was annotated on a scale
from 0 (“different topic”) to 5 (“completely equivalent”). The similarity notion in the AFS corpus
is fairly different to the similarity notion in the
STS datasets from SemEval. STS data is usually
descriptive, while AFS data are argumentative excerpts from dialogs. To be considered similar, arguments must not only make similar claims, but
also provide a similar reasoning. Further, the lexical gap between the sentences in AFS is much
Hence, simple unsupervised methods as
well as state-of-the-art STS systems perform badly
on this dataset .
We evaluate SBERT on this dataset in two scenarios: 1) As proposed by Misra et al., we evaluate
SBERT using 10-fold cross-validation. A drawback of this evaluation setup is that it is not clear
how well approaches generalize to different topics. Hence, 2) we evaluate SBERT in a cross-topic
setup. Two topics serve for training and the approach is evaluated on the left-out topic. We repeat
this for all three topics and average the results.
SBERT is ﬁne-tuned using the Regression Objective Function. The similarity score is computed
using cosine-similarity based on the sentence embeddings. We also provide the Pearson correlation r to make the results comparable to Misra et
al. However, we showed 
that Pearson correlation has some serious drawbacks and should be avoided for comparing STS
systems. The results are depicted in Table 3.
Unsupervised methods like tf-idf,
GloVe embeddings or InferSent perform rather
badly on this dataset with low scores. Training
SBERT in the 10-fold cross-validation setup gives
a performance that is nearly on-par with BERT.
However, in the cross-topic evaluation, we observe a performance drop of SBERT by about 7
points Spearman correlation.
To be considered
similar, arguments should address the same claims
and provide the same reasoning. BERT is able to
use attention to compare directly both sentences
(e.g. word-by-word comparison), while SBERT
must map individual sentences from an unseen
topic to a vector space such that arguments with
similar claims and reasons are close.
much more challenging task, which appears to require more than just two topics for training to work
on-par with BERT.
Wikipedia Sections Distinction
Dor et al. use Wikipedia to create a thematically ﬁne-grained train, dev and test set for
sentence embeddings methods.
Wikipedia articles are separated into distinct sections focusing
on certain aspects.
Dor et al. assume that sen-
Unsupervised methods
Avg. GloVe embeddings
InferSent - GloVe
10-fold Cross-Validation
SVR 
BERT-AFS-base
SBERT-AFS-base
BERT-AFS-large
SBERT-AFS-large
Cross-Topic Evaluation
BERT-AFS-base
SBERT-AFS-base
BERT-AFS-large
SBERT-AFS-large
Table 3: Average Pearson correlation r and average
Spearman’s rank correlation ρ on the Argument Facet
Similarity (AFS) corpus . Misra et
al. proposes 10-fold cross-validation. We additionally
evaluate in a cross-topic scenario: Methods are trained
on two topics, and are evaluated on the third topic.
tences in the same section are thematically closer
than sentences in different sections. They use this
to create a large dataset of weakly labeled sentence triplets: The anchor and the positive example come from the same section, while the negative example comes from a different section of
the same article.
For example, from the Alice
Arnold article: Anchor: Arnold joined the BBC
Radio Drama Company in 1988., positive: Arnold
gained media attention in May 2012., negative:
Balding and Arnold are keen amateur golfers.
We use the dataset from Dor et al. We use the
Triplet Objective, train SBERT for one epoch on
the about 1.8 Million training triplets and evaluate
it on the 222,957 test triplets. Test triplets are from
a distinct set of Wikipedia articles. As evaluation
metric, we use accuracy: Is the positive example
closer to the anchor than the negative example?
Results are presented in Table 4. Dor et al. ﬁnetuned a BiLSTM architecture with triplet loss to
derive sentence embeddings for this dataset. As
the table shows, SBERT clearly outperforms the
BiLSTM approach by Dor et al.
Evaluation - SentEval
SentEval is a popular
toolkit to evaluate the quality of sentence embeddings. Sentence embeddings are used as features
for a logistic regression classiﬁer. The logistic regression classiﬁer is trained on various tasks in a
10-fold cross-validation setup and the prediction
accuracy is computed for the test-fold.
mean-vectors
skip-thoughts-CS
Dor et al.
SBERT-WikiSec-base
SBERT-WikiSec-large
SRoBERTa-WikiSec-base
SRoBERTa-WikiSec-large
Table 4: Evaluation on the Wikipedia section triplets
dataset . SBERT trained with triplet
loss for one epoch.
The purpose of SBERT sentence embeddings
are not to be used for transfer learning for other
Here, we think ﬁne-tuning BERT as described by Devlin et al. for new tasks is
the more suitable method, as it updates all layers
of the BERT network. However, SentEval can still
give an impression on the quality of our sentence
embeddings for various tasks.
We compare the SBERT sentence embeddings
to other sentence embeddings methods on the following seven SentEval transfer tasks:
• MR: Sentiment prediction for movie reviews
snippets on a ﬁve start scale .
• SUBJ: Subjectivity prediction of sentences
from movie reviews and plot summaries
 .
• MPQA: Phrase level opinion polarity classi-
ﬁcation from newswire .
• SST: Stanford Sentiment Treebank with binary labels .
• TREC: Fine grained question-type classiﬁcation from TREC .
• MRPC: Microsoft Research Paraphrase Corpus from parallel news sources (Dolan et al.,
The results can be found in Table 5. SBERT
is able to achieve the best performance in 5 out
of 7 tasks.
The average performance increases
by about 2 percentage points compared to InferSent as well as the Universal Sentence Encoder.
Even though transfer learning is not the purpose of
SBERT, it outperforms other state-of-the-art sentence embeddings methods on this task.
Avg. GloVe embeddings
Avg. fast-text embeddings
Avg. BERT embeddings
BERT CLS-vector
InferSent - GloVe
Universal Sentence Encoder
SBERT-NLI-base
SBERT-NLI-large
Table 5: Evaluation of SBERT sentence embeddings using the SentEval toolkit. SentEval evaluates sentence
embeddings on different sentence classiﬁcation tasks by training a logistic regression classiﬁer using the sentence
embeddings as features. Scores are based on a 10-fold cross-validation.
It appears that the sentence embeddings from
SBERT capture well sentiment information: We
observe large improvements for all sentiment tasks
(MR, CR, and SST) from SentEval in comparison
to InferSent and Universal Sentence Encoder.
The only dataset where SBERT is signiﬁcantly
worse than Universal Sentence Encoder is the
TREC dataset. Universal Sentence Encoder was
pre-trained on question-answering data, which appears to be beneﬁcial for the question-type classi-
ﬁcation task of the TREC dataset.
Average BERT embeddings or using the CLStoken output from a BERT network achieved bad
results for various STS tasks (Table 1), worse than
average GloVe embeddings. However, for Sent-
Eval, average BERT embeddings and the BERT
CLS-token output achieves decent results (Table 5), outperforming average GloVe embeddings.
The reason for this are the different setups. For
the STS tasks, we used cosine-similarity to estimate the similarities between sentence embeddings.
Cosine-similarity treats all dimensions
equally. In contrast, SentEval ﬁts a logistic regression classiﬁer to the sentence embeddings. This
allows that certain dimensions can have higher or
lower impact on the classiﬁcation result.
We conclude that average BERT embeddings /
CLS-token output from BERT return sentence embeddings that are infeasible to be used with cosinesimilarity or with Manhatten / Euclidean distance.
For transfer learning, they yield slightly worse
results than InferSent or Universal Sentence Encoder. However, using the described ﬁne-tuning
setup with a siamese network structure on NLI
datasets yields sentence embeddings that achieve
a new state-of-the-art for the SentEval toolkit.
Ablation Study
We have demonstrated strong empirical results for
the quality of SBERT sentence embeddings. In
this section, we perform an ablation study of different aspects of SBERT in order to get a better
understanding of their relative importance.
strategies
(MEAN, MAX, and CLS). For the classiﬁcation
objective function, we evaluate different concatenation methods. For each possible conﬁguration,
we train SBERT with 10 different random seeds
and average the performances.
The objective function (classiﬁcation vs. regression) depends on the annotated dataset. For the
classiﬁcation objective function, we train SBERTbase on the SNLI and the Multi-NLI dataset. For
the regression objective function, we train on the
training set of the STS benchmark dataset. Performances are measured on the development split of
the STS benchmark dataset. Results are shown in
Pooling Strategy
Concatenation
(|u −v|, u ∗v)
(u, v, u ∗v)
(u, v, |u −v|)
(u, v, |u −v|, u ∗v)
Table 6: SBERT trained on NLI data with the classiﬁcation objective function, on the STS benchmark
(STSb) with the regression objective function. Con-
ﬁgurations are evaluated on the development set of the
STSb using cosine-similarity and Spearman’s rank correlation. For the concatenation methods, we only report
scores with MEAN pooling strategy.
When trained with the classiﬁcation objective
function on NLI data, the pooling strategy has a
rather minor impact. The impact of the concatenation mode is much larger. InferSent and Universal Sentence Encoder both use (u, v, |u −v|, u ∗v) as input
for a softmax classiﬁer. However, in our architecture, adding the element-wise u ∗v decreased the
performance.
The most important component is the elementwise difference |u −v|. Note, that the concatenation mode is only relevant for training the softmax classiﬁer. At inference, when predicting similarities for the STS benchmark dataset, only the
sentence embeddings u and v are used in combination with cosine-similarity. The element-wise
difference measures the distance between the dimensions of the two sentence embeddings, ensuring that similar pairs are closer and dissimilar pairs
are further apart.
When trained with the regression objective
function, we observe that the pooling strategy has
a large impact. There, the MAX strategy perform
signiﬁcantly worse than MEAN or CLS-token strategy. This is in contrast to ,
who found it beneﬁcial for the BiLSTM-layer of
InferSent to use MAX instead of MEAN pooling.
Computational Efﬁciency
Sentence embeddings need potentially be computed for Millions of sentences, hence, a high
computation speed is desired. In this section, we
compare SBERT to average GloVe embeddings,
InferSent , and Universal
Sentence Encoder .
For our comparison we use the sentences from
the STS benchmark . We compute average GloVe embeddings using a simple for-loop with python dictionary lookups and
InferSent4 is based on PyTorch.
Universal Sentence Encoder, we use the Tensor-
Flow Hub version5, which is based on Tensor-
Flow. SBERT is based on PyTorch. For improved
computation of sentence embeddings, we implemented a smart batching strategy: Sentences with
similar lengths are grouped together and are only
padded to the longest element in a mini-batch.
This drastically reduces computational overhead
from padding tokens.
Performances were measured on a server with
Intel i7-5820K CPU @ 3.30GHz, Nvidia Tesla
4 
5 
universal-sentence-encoder-large/3
V100 GPU, CUDA 9.2 and cuDNN. The results
are depicted in Table 7.
Avg. GloVe embeddings
Universal Sentence Encoder
SBERT-base
SBERT-base - smart batching
Table 7: Computation speed (sentences per second) of
sentence embedding methods. Higher is better.
On CPU, InferSent is about 65% faster than
SBERT. This is due to the much simpler network architecture.
InferSent uses a single Bi-
LSTM layer, while BERT uses 12 stacked transformer layers. However, an advantage of transformer networks is the computational efﬁciency
There, SBERT with smart batching
is about 9% faster than InferSent and about 55%
faster than Universal Sentence Encoder.
batching achieves a speed-up of 89% on CPU and
48% on GPU. Average GloVe embeddings is obviously by a large margin the fastest method to compute sentence embeddings.
Conclusion
We showed that BERT out-of-the-box maps sentences to a vector space that is rather unsuitable to be used with common similarity measures
like cosine-similarity. The performance for seven
STS tasks was below the performance of average
GloVe embeddings.
To overcome this shortcoming, we presented
Sentence-BERT
BERT in a siamese / triplet network architecture. We evaluated the quality on various common benchmarks, where it could achieve a signiﬁcant improvement over state-of-the-art sentence embeddings methods. Replacing BERT with
RoBERTa did not yield a signiﬁcant improvement
in our experiments.
SBERT is computationally efﬁcient. On a GPU,
it is about 9% faster than InferSent and about 55%
faster than Universal Sentence Encoder. SBERT
can be used for tasks which are computationally
not feasible to be modeled with BERT. For example, clustering of 10,000 sentences with hierarchical clustering requires with BERT about 65 hours,
as around 50 Million sentence combinations must
be computed. With SBERT, we were able to reduce the effort to about 5 seconds.
Acknowledgments
This work has been supported by the German
Research Foundation through the German-Israeli
Project Cooperation (DIP, grant DA 1600/1-1 and
grant GU 798/17-1). It has been co-funded by the
German Federal Ministry of Education and Research (BMBF) under the promotional references
03VP02540 (ArgumenText).