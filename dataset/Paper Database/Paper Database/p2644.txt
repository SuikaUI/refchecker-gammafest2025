Contextualizing object detection and classification
Song, Z., Chen, Q., Huang, Z., Hua, Y., & Yan, S. . Contextualizing object detection and classification. In
Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on (pp. 1585-1592). Institute of
Electrical and Electronics Engineers Inc..
 
Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on
Document Version:
Peer reviewed version
Queen's University Belfast - Research Portal:
Link to publication record in Queen's University Belfast Research Portal
General rights
Copyright for the publications made accessible via the Queen's University Belfast Research Portal is retained by the author(s) and / or other
copyright owners and it is a condition of accessing these publications that users recognise and abide by the legal requirements associated
with these rights.
Take down policy
The Research Portal is Queen's institutional repository that provides access to Queen's research output. Every effort has been made to
ensure that content in the Research Portal does not infringe any person's rights, or applicable UK laws. If you discover content in the
Research Portal that you believe breaches copyright or violates any law, please contact .
Open Access
This research has been made openly available by Queen's academics and its Open Research team. We would love to hear how access to
this research benefits you. – Share your feedback with us: 
Download date:26. Mar. 2025
Contextualizing Object Detection and Classiﬁcation
Zheng Song1∗, Qiang Chen1∗, Zhongyang Huang2, Yang Hua2, Shuicheng Yan1
1 Department of Electrical and Computer Engineering, National University of Singapore, Singapore
2 Panasonic Singapore Laboratories, Singapore
{zheng.s, chenqiang, eleyans}@nus.edu.sg,{zhongyang.huang, yang.hua}@sg.panasonic.com
In this paper, we investigate how to iteratively and mutually boost object classiﬁcation and detection by taking
the outputs from one task as the context of the other one.
First, instead of intuitive feature and context concatenation
or postprocessing with context, the so-called Contextualized
Support Vector Machine (Context-SVM) is proposed, where
the context takes the responsibility of dynamically adjusting
the classiﬁcation hyperplane, and thus the context-adaptive
classiﬁer is achieved. Then, an iterative training procedure
is presented. In each step, Context-SVM, associated with
the output context from one task (object classiﬁcation or
detection), is instantiated to boost the performance for the
other task, whose augmented outputs are then further used
to improve the former task by Context-SVM. The proposed
solution is evaluated on the object classiﬁcation and detection tasks of PASCAL Visual Object Challenge (VOC) 2007
and 2010, and achieves the state-of-the-art performance.
1. Introduction
Object detection and classiﬁcation are two key tasks for
image understanding, and have attracted much attention
in the past decades.
The object classiﬁcation task aims
to predict the existence of objects within images, whereas
the object detection targets localizing the objects.
Several image databases tailored for these two tasks have been
constructed, such as Caltech-101 /256 and PAS-
CAL Visual Object Challenge (VOC) and many efforts
 have been devoted for these two tasks.
Beyond various image descriptors and modeling methods, the usage of context has become more and more popular for enhancing the algorithmic performance. Many recent
studies demonstrated considerable improvement for object
detection and classiﬁcation by using external information,
which is independently retrieved and complementary with
traditional image descriptors. Speciﬁcally, the external context includes user-provided tags , surrounding texts
* indicates equal contributions
Learn to Detect
Learn to Classify
Iterative Contextualization
Itterative
Contextualizatioon
Contextualizatioon
Context from
Classification
Context from
Figure 1: Illustration of the iterative contextualizing procedure.
The object detection and classiﬁcation tasks utilize context from
each other and mutually boost performance iteratively. For better
viewing, please see original color PDF ﬁle.
from Internet , geo-tags and time stamps , etc.
The context may also be the information lying within
individual images. Intuitively, the spatial locations of objects and background scene from global view can be used as
inside-image context . Further, when we consider
object detection and classiﬁcation tasks together, these two
tasks can provide natural comprehensive context for each
other without any external assistance, and thus can be mutually contextualized for performance boosting .
In this paper, we develop a novel mutual contextualization scheme for object detection and classiﬁcation based
on the so-called Contextualized Support Vector Machine
(Context-SVM) method. Extensive experiments show that
Context-SVM can efﬁciently learn the context models under various conditions and effectively utilize context information for performance boosting. We implement and evaluate the proposed scheme on object detection and classiﬁ-
cation tasks of the VOC 2007 and VOC 2010 datasets ,
and the results are superior over the state-of-the-art on most
object categories.
First, we present a contextualized learning scheme via
Context-SVM with the following characteristics:
• Adaptive contextualization:
As many studies have
shown , context should be activated to be supportive mostly for those ambiguous samples and thus
the context effectiveness should be conditional on the
ambiguity of sample classiﬁcation. The Context-SVM
is superior over traditional learning schemes by complying this principle in its mathematical formulation.
• Conﬁgurable model complexity: The contextualization
process should be efﬁcient for both detection and classiﬁcation tasks, and thus the solution should not involve many parameters.
In this work, the Context-
SVM with tractable control on the complexity of the
context model is well formulated, and thus the generalization capability is guaranteed.
Then we propose an iterative contextualization procedure based on the Context-SVM, such that the performance
of object classiﬁcation and detection can be iteratively and
mutually boosted as shown in Figure 1.
2. Related Work
Harzallah et al. introduced the pioneering work for
object detection and classiﬁcation contextualization through
probability combination in postprocessing. In this work, we
instead develop the learning scheme which seamlessly integrates the context information for collaborative learning.
Traditionally, the context is considered as special features. Most of the existing strategies utilize the
context via feature concatenation, model fusion or conﬁdence combination, and take the context as another independent component. However, context may have instable
distribution, and its reliability and noise level are not controllable. Therefore adaptive integration of context is required to avoid the inappropriate usage of context information. In this work, we follow this line to design the learning
scheme for utilizing context information.
Also, some methods have been proposed to model the
context in a comprehensive manner, e.g. , but they are
served for a more speciﬁc purpose and not easily generalized to our requirement.
3. Contextualized SVM
In this work, the context is generally deﬁned as certain
extra supportive information for one task, which is retrieved
independently from the subject task 1. In the section, we
1We refer the main/principal task concerned as the subject task.
ﬁrst introduce the probabilistic motivation of the contextualized SVM (Context-SVM) and then derive its linear formulation based on the probabilistic motivation. Finally, we extend the linear Context-SVM to the kernel version for more
general usage.
3.1. Probabilistic Motivation
i ∈Rn denote the features of a sample for the subject task, xc
i ∈Rm denote the features of the corresponding
context, and yi denotes the ground-truth class label. Then
the entire training data can be expressed as
i}, yi; i = 1, 2, ..., N}.
Generally, the objective of a discriminative learning model
can be deﬁned as to maximize:
P(y = yi|Xi),
namely the Maximum a Posteriori (MAP).
There are two components within Xi, and often the independent assumption of the subject features xf
i and the context xc
i is made and then the probability of label y for a given
sample Xi can be approximated as:
p(y|Xi) ≈p(y|xf
The inference based on (2) is right for the traditional solution of conﬁdence combination or multiple feature/model fusion .
The independence assumption, however, is often invalid
for real data, and hence we propose to infer the label probability by (3) which explicitly models the conditional usage
of context with respect to the given subject features:
p(y|Xi) = p(y|xf
i) ∝p(y|xf
i ) · p(y, xc
More speciﬁcally, we aim to infer the label probability
via two components simultaneously. The ﬁrst one is based
on the subject features, i.e. p(y|xf
i ), and the second one is
based on the context features, which contribute to the inference while only ambiguous decision from the ﬁrst component is expected, i.e. p(y, xc
The second component is critical for a contextualized
learning model. For object detection, the context of scene
information from object classiﬁcation is nearly the same for
all detected windows within one image and might not be
necessary for many windows. Instead, only the most ambiguous detections need the assistance from context.
For object classiﬁcation, the context from object detection generally shows low reliability due to the possible false
alarms and the selective usage of context can effectively
avoid the disturbance caused by the false context to those
already high-conﬁdent object patterns.
3.2. Context-SVM: Formulation and Solution
General Formulation
For ease of formulation, we only concern the binary classi-
ﬁcation problem for object detection or classiﬁcation task,
i.e. yi ∈{+1, −1} and the Nc-class problem can be decomposed into Nc binary classiﬁcation problems through
one-vs-all strategy. SVM provides a general supervised
learning framework by maximum margin optimization, and
in this work, we extend SVM by introducing a novel parameterized model to describe the dependence between the
context features and the subject features.
The general SVM learns a classiﬁer over the subject feature space and obtains a ﬁxed hyperplane:
0 · xf + b = 0.
As the corresponding context features xc
i can provide extra supportive information for the classiﬁcation of xf
propose to utilize xc
i to adapt w0 for sample Xi. Then a
sample-speciﬁc wi can be obtained to substitute w0, which
essentially optimizes the margin of sample i and can consequently improve the discriminative power of the classi-
ﬁer. More speciﬁcally, we introduce a transformation matrix P ∈Rn×m to utilize xc
i for the subject classiﬁcation,
w0 −→wi = Pxc
The number of parameters brought by P is very large,
which may easily make the derived model overﬁtting, and
thus we introduce a complexity constraint over P. That is,
the matrix P is constrained as a low-rank matrix, expressed
as the sum of R rank-1 matrices in (6) in which ur ∈Rn
and qr ∈Rm,
and then the complexity of the context model could be
well controlled with R × (m + n) parameters, where R is
the rank of P. As latter introduced, the P in constrained
form will better interpret how the proposed contextualized
learning model adaptively utilizes the context for inference.
By substituting P into (5), we obtain (7), and the socalled margin for sample Xi could be derived as in (8):
γi = yi(wT
These two equations well show the more insightful meaning
of the contextualized SVM formulation:
Adaptive Contextualization
dbyconte
Positives
StrongNegatives
AmbiguousSamples
StrongPositives
Figure 2: Illustration of the relationship between original sample
conﬁdence and conﬁdence variation amount from context. The
blue and red dots represent positive and negative samples respectively. The x-axis denotes the sample conﬁdence in subject feature space and y-axis denotes the absolute amount of conﬁdence
changed by the contextualization procedure.
The conﬁdences
are converted into probabilistic values within 0 and 1 indicating
strongest negative and positive decisions respectively. For better
viewing, please see original color PDF ﬁle.
• The adaptive hyperplane wi is the combination of
the subject hyperplane w0 and R rectiﬁcations via
{qr, ur}’s with the corresponding contributions determined by the context feature xc
i. Intuitively, we can
i as a switch to determine whether the context should be activated while the value qT
i determines how to rectify w0.
• The reﬁned margin expression corresponds exactly to
our probabilistic motivation. The {ur} and {qr} collaboratively model the component p(y, xc
i ) in (3).
The decomposition of P helps us better understand
that {ur} serve to judge the discrimination ambiguity of xf
i , and {qr} are utilized to integrate the context
feature xc
i for the classiﬁcation of the samples with different ambiguities.
Instantiate {ur}
As aforementioned, we design {ur} to highlight samples
which are classiﬁed ambiguously with their subject features
i }. Practically, we instantiate {ur} as a set of hyperplanes parallel to a learned hyperplane w0 in subject feature
space by traditional SVM:
ur = αrw0 + βr, r = 1, 2, · · · , R.
Intuitively, for αr > 0, if we set αr and βr properly
such that all {uT
i } are within , those samples classi-
ﬁed as negative by w0 with high conﬁdences shall be suppressed, namely their corresponding values of {uT
be small. At the same time, for αr < 0, if we set αr and
βr properly such that all {uT
i } are within , those
samples classiﬁed as positive by w0 with high conﬁdences
shall be suppressed, namely their corresponding values of
i } shall be small. Therefore we can sample multiple
combinations of αr and βr, and both strong negative and
positive samples shall be suppressed by {ur} such that the
samples with ambiguous decisions by w0 are highlighted.
Our empirical experiments show that using larger R may
derive better ambiguity modeling but may also lead to over-
ﬁtting, and it is a good trade-off by setting R = 2, i.e. using
two auxiliary hyperplanes u1 and u2 and set α1 > 0 and
α2 < 0. Then the combination of u1 and u2 can provide a
rough yet efﬁcient judgment for the decision ambiguity of
a sample and force the context model to concentrate on the
samples with large ambiguities.
We illustrate one exemplar contextualization result by
Context-SVM on object classiﬁcation task of the “aeroplane” category in Figure 2. This ﬁgure shows the adaptive contextualization with respect to the sample ambiguity:
the samples with higher ambiguities (i.e. samples lying in
the middle of the ﬁgure) are changed largely by the contextualization procedure while the well-classiﬁed samples (i.e.
samples lying on the two sides of the ﬁgure) are nearly not
Optimization for Context-SVM
Based on the instantiated {ur}, we can formulate the
Context-SVM as a max-margin optimization problem with
the margin described as the average of the rectiﬁed individual margins related to ∥wi∥’s, namely,
s.t. yi(wT
i + b) −1 + ξi ≥0, ξi ≥0, ∀i,
where C is a tunable parameter for balancing two items and
ξi’s are relaxation parameters.
This formulation can be further compiled with respect to
{qr} and w0 as:
s.t. yi[(Uiv)T xf
i + b] −1 + ξi ≥0,
ξi ≥0, ∀i,
where the matrices Ui = [In, u1xcT
i , · · · , uRxcT
v = [w0; q1; q2; · · · ; qR] and In is an n × n identity matrix.
Note that in this optimization problem, there are only
(R × m + n) parameters to optimize, and generally R is
small. Therefore the overﬁtting issue can be well alleviated.
It is easy to prove 2 that (11) can be converted to a standard
SVM problem and its solution can be derived with standard
SVM solvers.
3.3. Kernel Extension
For many visual understanding problems, image descriptors are further encoded as similarity measurements or kernel matrices, and there is no explicit vector representation
2Details are omitted here due to the space limitation.
for each image.
Therefore, it is necessary to generalize
the Context-SVM formulation to the case with only kernel matrices available. We consider the problem in a feature space F induced by certain nonlinear mapping function φ : Rn →F.
For a properly chosen φ, an inner
product ⟨·, ·⟩can be deﬁned on F which induces a Reproducing Kernel Hilbert Space (RKHS). More speciﬁcally,
j )⟩= K(xf
j ) where K(·, ·) is a positive
semi-deﬁnite kernel function.
The context-adaptive hyperplane for each sample can be
deﬁned as:
i))T · φ(xf
i ) + b = 0,
which is similar to (7).
By Representer Theorem , ur and w0 can be expressed as linear combinations of {φ(xf
Thus, there
exist sets of coefﬁcients such that ur = N
i=1 βriφ(xf
and w0 = N
i=1 αiφ(xf
Let βr = [βr1, · · · , βrN]T ,
α = [α1, · · · , αN]T and Φ(Xf) = [φ(xf
1), · · · , φ(xf
The context-aware hyperplane can then be expressed as:
i + Φ(Xf)α)T · φ(xf
i ) + b = 0,
namely, (R
i + α)T · K(:, i) + b = 0, where K
is the kernel matrix with Kij = ⟨φ(xf
j )⟩and K(:, i)
is the i-th column vector of the matrix K.
Then the overall formulation for kernel Context-SVM is:
i KBiz + C
s.t. yi[(Biz)T K(:, i) + b] −1 + ξi ≥0,
ξi ≥0, ∀i,
[IN, β1xcT
i , · · · , βRxcT
[α; q1; q2; · · · ; qR], and IN is an N × N identity matrix.
The main differences between the kernel version and the
linear version include: 1) the original subject feature vector
i is replaced by the column vector of the kernel matrix K,
and 2) l2 regularizer in the objective contains a kernel matrix. Thus, the same optimization approach can be used for
solving the kernel extension of Context-SVM.
4. Application: Contextualizing Object Detection and Classiﬁcation
In this section, we apply the Context-SVM to contextualize two prevalent tasks of image understanding, namely
object detection and classiﬁcation.
4.1. Initializations
The initial object detection and classiﬁcation models
Mdet(0) and Mcls(0) for the ﬁrst iteration are learned based
Algorithm 1 Contextualizing Classiﬁcation and Detection
Mdet(0): Initial object detection model,
Mcls(0): Initial object classiﬁcation model,
{Ii}: Training images,
R: Rank of the matrix P.
For t = 1, 2, . . . , Tmax
1. Extract detection features and context for each image,
i (t) ←extract(Ii), ∀i,
i(t) ←eval(Mcls(t −1), Ii), ∀i.
2. Instantiate {ur} with {{xf
i (t)}, R} and Mdet(t −1).
3. Learn Mdet(t) via Context-SVM on {xf
4. Similarly, learn Mcls(t) via Context-SVM by using the outputs
from Mdet(t) as context.
Output Mdet(Tmax), Mcls(Tmax).
on the state-of-the-art algorithms. We follow the part-based
model proposed by Felzenswalb et al. for the initial detection model training. The Histogram of Gradient
(HOG) and Local Binary Pattern (LBP) features are
used for object description and the number of part models
for each object category is set to be 6.
For object classiﬁcation task, the traditional Bag-of-
Words (BoW) model is employed.
We ﬁrst extract
the low-level features including SIFT and its color variants , LBP and HOG by dense sampling strategy in
three scales. Each image is represented by BoW model with
spatial pyramid matching . The kernel function is based
on χ2 distance for each type of feature, and then all kernels
are combined to an average kernel for kernelized Context-
4.2. Iterative Mutual Contextualization
The detailed algorithm for contextualizing object detection and classiﬁcation by iterative Context-SVM is listed in
Algorithm 1. More speciﬁcally, the context features for detection and classiﬁcation refer to the probabilities of object
existence in each image. And each object category is represented in one probabilistic value. Thus the context feature
values are within and the dimension of context feature
vector is the number of object categories. The context from
the object classiﬁcation task is obtained by converting classiﬁcation scores on each image to probabilities via sigmoid
scaling. And the context features from the object detection
task are obtained by converting the detected highest score
for each object category to the probability in the same manner as for object classiﬁcation. If there is no object detected
for certain category, the corresponding entry in context feature vector is set as 0.
At the t-th step, the context features of one task (e.g. detection) are obtained by evaluating the (t −1)-th model of
the other task (e.g. classiﬁcation) on the training data {Ii}.
We use cross validation method to obtain context from object classiﬁcation in (15) as kernel model is easy to overﬁt
on its training data. 10-fold of training data are used and we
evaluate each fold via the model trained on all other folds.
Then we instantiate {ur} based on the extracted subject features and the learnt model from the previous step, and ﬁnally proceed to conduct Context-SVM based on {ur}, subject features and the corresponding context features for all
training images.
For training stage of iterative contextualization, the additional computation cost of optimization for the Context-
SVM is trivial comparing to the cost of the subject task, i.e.
the feature extraction and kernel vector calculation for object classiﬁcation and the mining of training samples from
sub-windows of each image for object detection.
5. Experiments
5.1. Datasets and Metrics
datasets are widely used as testbeds for evaluating
algorithms for image understanding tasks and provide a
common evaluation platform for both object classiﬁcation
and detection.
These datasets are extremely challenging
since the objects vary signiﬁcantly in size, view angle,
illumination, appearance and pose. We use PASCAL VOC
2007 and 2010 datasets for experiments in this paper. The
twenty object categories of VOC datasets are as illustrated
in Table 1.
VOC 2007 and VOC 2010 datasets contain 9,963 nd
21,738 images respectively. The two datasets are divided
into “train”, “val” and “test” subsets, i.e. 25% for training,
25% for validation and 50% for testing. The annotations
for the whole dataset of VOC 2007 and “train”, “val” set of
VOC 2010 are provided while the annotations for “test” set
of VOC 2010 are still conﬁdential and can only be evaluated
on the web server with limited trials. The employed evaluation metric is Average Precision (AP) complying with the
PASCAL challenge rules.
In the following experiments, we ﬁrst evaluate the performance boosting capability from iterative mutual contextualization on VOC 2010 “train/val” dataset (i.e. “train” set
for training and “val” set for test) since frequent evaluations
of the performance are required. Then several traditional
methods for contextualizing object detection and classiﬁcation are compared with our iterative Context-SVM on the
VOC 2010 trainval/test dataset. Finally, we evaluate the optimal conﬁguration on PASCAL VOC 2007 and 2010 trainval/test datasets and compare it with the state-of-the-art performance ever reported.
5.2. Iterative Performance Boosting via Mutual
Contextualization
To evaluate the effectiveness of our proposed iterative
mutual contextualization process, we conduct three experi-
diningtable
pottedplant
Figure 3: Illustration of performance improvement with comparison Precision-recall curves of object detection (upper row) and classiﬁcation (lower row). The performance of baseline (without contextualization) and those of Context-SVM at iteration 1-3 are plotted.
Figure 4: Representative examples of the baseline (without contextualization) and Context-SVM at iteration 3. The detections are shown
via the detected bounding boxes on images (with proper threshold): the green boxes with dashed lines denote the false alarms from baseline,
which are further removed by contextualization and red boxes denote the true detections of both methods. The classiﬁcation results are
compared by the conﬁdences for each object category before (green) and after (red) contextualization. For better viewing, please see
original color PDF ﬁle.
ments on VOC 2010 “train/val” dataset. Firstly, we demonstrate the performance improvement measured by mean AP
for all the 20 classes in Figure 5. In this experiment, the
mutual contextualization is conducted for 3 iterations, and
obvious performance improvement is observed for the ﬁrst
and second iteration. As the improvement from the third iteration becomes trivial, we set the maximum iteration number, namely Tmax to 3 for all the experiments in this work.
In the second experiment, we show exactly how the
mutual contextualization process beneﬁts each class by
Precision-Recall curves of several representative classes in
Figure 3, and also we show the representative object detection and classiﬁcation results in Figure 4 for the third
experiment. As can be observed from Figure 3, great performance improvement can be achieved for the ﬁrst two iterations and in the 3rd iteration, certain amount of improvement can still be achieved for several classes such as “bus”
and “dog”.
From Figure 4, it may be observed that the
Figure 5: Mean AP values of 20 classes on VOC 2010 train/val
dataset along iterative contextualization.
Context-SVM shows good stability in reﬁning the classes
even without accurate context such as “pottedplant”. The
example detection results show that the improvement of object detection is mainly achieved by effective removal of the
ambiguous negatives while the object classiﬁcation beneﬁts
from detection context by calling back those missing objects, e.g. the “person” and “chair” missed in the baseline
results as shown in Figure 4.
Table 1: Contextualization method comparison on the PASCAL VOC 2010 (trainval/test) dataset. “Det” and “Cls” respectively denote
object detection and classiﬁcation tasks.
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
Det Fuse 
50.5 49.8 16.0 10.4 30.4 54.3 43.3 38.3 15.9 30.0 24.1 23.1 47.8
11.8 33.5 27.5 47.3 38.8 34.5
Det Our Method
53.1 52.7 18.1 13.5 30.7 53.9 43.5 40.3 17.7 31.9 28.0 29.5 52.9
12.6 36.2 28.7 50.5 40.7 36.8
Cls MKL 
91.4 76.6 66.7 72.3 53.1 83.7 77.1 75.3 62.9 59.8 57.1 63.6 76.5
44.1 64.1 48.4 84.0 75.5 70.3
Cls Fuse 
90.7 74.0 67.2 73.9 53.8 81.7 74.1 73.6 60.9 59.8 60.5 62.3 75.1
45.8 61.7 56.0 85.9 76.0 70.2
Cls Our Method
92.2 77.7 69.2 75.7 53.5 84.7 80.9 76.1 62.8 65.5 63.1 65.6 79.6
47.5 71.9 55.2 86.3 76.7 73.0
5.3. Contextualization Methods Comparison
In this subsection, our proposed iterative mutual contextualization method is compared with the method proposed
by Harzallah et al.
in , which combines the conﬁdences from several probabilistic models and is the most
representative one among those conﬁdence combination approaches . For object classiﬁcation, Multiple Kernel Learning (MKL) method used in is also implemented for comparison, which is a general model fusion method and widely used to combine features in kernel form for object classiﬁcation. An extra linear kernel is
constructed for the context features from the object detection task, and then two kernels are combined with MKL.
MKL performs very bad for object detection task, and thus
we do not report the result of MKL for object detection
here. The main reason is that the context is ﬁxed for all
candidate windows within an image and the inaccurate context may severely affect the results for quite many candidate
The experiment results are evaluated using the latest
VOC 2010 “trainval/test” dataset and shown in Table 1. The
comparison results show that the proposed iterative and mutual contextualization method outperforms these two traditional contextualization methods for most object categories.
5.4. Comparison with State-of-the-art Performance
We also compare the proposed contextualization method
with the reported state-of-the-art object detection and classiﬁcation approaches on VOC 2007 and VOC 2010 datasets.
The detailed performance comparison is shown in Table 2
and Table 3.
We compare with the best known VOC 2007 performance from several recent papers in Table 2. For object
detection, the methods compared include [MIT 2010] by
Zhu et al. using latent hierarchical structural learning,
[UCI 2009] by Desai et al. using context of object layout, [INRIA 2009] by Harzallah et al. fusing classiﬁcation scores, and [UoC 2010] by Felzenswalb et al. using part-based model with context of object co-occurrence.
For the detection challenge of 2007, our method outperforms 13 classes out of 20 classes and the MAP outperforms
the second best [UoC 2010] by 3.6%.
The well-known methods of VOC 2007 object classiﬁcation task compared are: [INRIA Genetic] the winner of VOC 2007, [NEC 2010] performing nonlinear
feature transformation on descriptors, [INRIA 2009] fusing
detection scores, and [TagModal] using extra tag information of VOC 2007 dataset. Our method signiﬁcantly
outperforms the competing methods for 12 classes out of
20 classes. Note that our MAP achieves leading by 3.8%
to the result of [TagModal]. It well validates the effectiveness of the proposed strategy in utilizing detection context
for object classiﬁcation.
For VOC 2010 dataset, we compare with the recently
released results from the VOC 2010 challenge , which
are all obtained through the combinations of multiple methods including mutual combination of detection and classiﬁcation. Necessary postprocessing is also implemented
in these methods. Therefore for a fair comparison, we re-
ﬁne the framework used by Chen et al. in their submission
[NUSPSL KERNELREGFUSING] (NUSPSL) with
the following differences: 1) the combination of detection
and classiﬁcation is further reﬁned by the proposed iterative
Context-SVM and 2) we exclude the fusion of other learning schemes used in to verify the effectiveness of the
Context-SVM.
The comparison results are shown in Table 3, from
which we may observe that the classiﬁcation results from
our proposed method outperform in 16 classes out of 20
classes, and 3.3% in mean AP over the second best VOC
2010 submission [NLPR Context]. Note that the submission [NLPR Contex] combines the best-performed detection results in this challenge for classiﬁcation. Our proposed
method also outperforms the winner submission [NUSPSL]
in 12 classes out of 20 classes and achieves the highest
mean AP even without the fusion with other learning methods. The object detection results from our proposed method
based on Context-SVM also outperform 7 classes out of 20
classes, and our method achieves the highest mean AP together with the winner submission [NLPR Contex], which
outperforms 6 classes out of 20 classes in this competition.
6. Conclusions
In this paper, we proposed an iterative contextualization
scheme to mutually boost performance of both object detection and classiﬁcation tasks. We ﬁrst proposed the socalled Contextualized SVM to seamlessly integrate external context features and subject features for general classi-
ﬁcation, and then Context-SVM was further utilized to iteratively and mutually boost performance of object detec-
Table 2: Comparison with the state-of-the-art performance of object classiﬁcation and detection on PASCAL VOC 2007 (trainval/test).
Detection on VOC 2007
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
MIT 2010 
29.4 55.8 9.4 14.3 28.6 44.0 51.3 21.3 20.0 19.3 25.2 12.5 50.4
15.1 19.7 25.1 36.8 39.3 29.6
UCI 2009 
28.8 56.2 3.2 14.2 29.4 38.7 48.7 12.4 16.0 17.7 24.0 11.7 45.0
15.2 16.1 20.1 34.2 35.4 27.1
INRIA 2009 
35.1 45.6 10.9 12.0 23.2 42.1 50.9 19.0 18.0 31.5 17.2 17.6 49.6
18.9 27.3 24.7 29.9 39.7 28.9
UoC 2010 
31.2 61.5 11.9 17.4 27.0 49.1 59.6 23.1 23.0 26.3 24.9 12.9 60.1
13.4 18.8 36.2 49.1 43.0 34.1
Our method
38.6 58.7 18.0 18.7 31.8 53.6 56.0 30.6 23.5 31.1 36.6 20.9 62.6
18.8 23.5 41.8 53.6 45.3 37.7
Classiﬁcation on VOC 2007
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
INRIA Genetic 
77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5
36.3 44.7 50.6 79.2 53.2 59.4
SuperVec 
79.4 72.5 55.6 73.8 34.0 72.4 83.4 63.6 56.6 52.8 63.2 49.5 80.9
36.4 46.5 59.8 83.3 58.9 64.0
INRIA 2009 
77.2 69.3 56.2 66.6 45.5 68.1 83.4 53.6 58.3 51.1 62.2 45.2 78.4
52.4 54.4 54.3 75.8 62.1 63.5
TagModal 
87.9 65.5 76.3 75.6 31.5 71.3 77.5 79.2 46.2 62.7 41.4 74.6 84.6
48.0 67.7 44.3 86.1 52.7 66.7
Our Method
82.5 79.6 64.8 73.4 54.2 75.0 87.5 65.6 62.9 56.4 66.0 53.5 85.0
53.9 61.0 67.5 83.6 70.6 70.5
Table 3: Comparison with the state-of-the-art performance of object classiﬁcation and detection on PASCAL VOC 2010 (trainval/test).
Detection on VOC 2010
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
NLPR Context 
53.3 55.3 19.2 21.0 30.0 54.4 46.7 41.2 20.0 31.5 20.7 30.3 48.6
10.2 34.4 26.5 50.3 40.3 36.8
MITUCLA 
54.2 48.5 15.7 19.2 29.2 55.5 43.5 41.7 16.9 28.5 26.7 30.9 48.3
35.8 30.8 47.2 40.8 36.0
NUS Context 
49.1 52.4 17.8 12.0 30.6 53.5 32.8 37.3 17.7 30.6 27.7 29.5 51.9
14.8 27.9 49.5 38.4 34.2
56.7 39.8 16.8 12.2 13.8 44.9 36.9 47.7 12.1 26.9 26.5 37.2 42.1
12.1 37.8 33.0 41.5 41.7 32.9
Our Method
53.1 52.7 18.1 13.5 30.7 53.9 43.5 40.3 17.7 31.9 28.0 29.5 52.9
12.6 36.2 28.7 50.5 40.7 36.8
Classiﬁcation on VOC 2010
plane bike bird boat bottle bus
cat chair cow table dog horse motor person plant sheep sofa train
NLPR Context 
90.3 77.0 65.3 75.0 53.7 85.9 80.4 74.6 62.9 66.2 54.1 66.8 76.1
41.6 66.3 57.0 85.0 74.3 71.2
NEC Nonlin 
93.3 72.9 69.9 77.2 47.9 85.6 79.7 79.4 61.7 56.6 61.1 71.1 76.7
38.1 63.9 55.8 87.5 72.9 70.9
NUSPSL 
93.0 79.0 71.6 77.8 54.3 85.2 78.6 78.8 64.5 64.0 62.7 69.6 82.0
48.6 64.9 59.6 89.4 76.4 73.8
Our Method
93.1 78.9 73.2 77.1 54.3 85.3 80.7 78.9 64.5 68.4 64.1 70.3 81.3
48.9 72.6 58.2 87.8 76.6 74.5
tion and classiﬁcation tasks. The proposed solution was extensively evaluated on both PASCAL VOC 2007 and VOC
2010 datasets and achieved the state-of-the-art performance
for both tasks.
Acknowledgement
This work is supported by NRF/IDM Program, under research Grant NRF2008IDMIDM004-029.