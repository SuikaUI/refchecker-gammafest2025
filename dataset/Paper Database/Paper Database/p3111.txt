yperspectral sensors are a new class of optical sensor that collect a spectrum from each
point in a scene. They differ from
multispectral sensors in that the number of
bands is much higher (20 or more) and the spectral
bands are contiguous. For remote sensing applications,
they are typically deployed on either aircraft or satellites. The data product from a hyperspectral sensor is a
three-dimensional array or “cube” of data with the
width and length of the array corresponding to spatial
dimensions and the spectrum of each point as the third
dimension. While this data cube is a convenient way to
view the product, at most two of the dimensions can be
acquired simultaneously. In the most common configuration, the spectral and one spatial dimension are acquired simultaneously to give a high quality spectral
signature for each point in the scene. AVIRIS and
many other sensors directed toward terrain classification
are flying spot sensors that acquire an image by raster
scanning the scene in width while the platform moves to
build up the second spatial dimension. More recently, the
spatial resolution of hyperspectral sensors has been improved by using a two-dimensional focal plane array that
simultaneously acquires a spectrum along a line of points
in the scene. The second dimension is then built up by
motion of the platform. The sensors TRWIS-III ,
DARKHORSE , SEBASS , and AHI use this
approach. Hyperspectral sensors have a wide range of remote sensing applications including: terrain classification,
environmental monitoring, agricultural monitoring, geological exploration, and surveillance.
With the introduction of sensors capable of high spatial
and spectral resolution, there has been an increasing interest in using spectral imagery to detect small objects or features of interest. However, detection algorithms that
presume a target signature are subject to signal mismatch
losses because of the complications of converting sensor
data into material spectra. The sensor collects spectral radiance, i.e., radiant flux per unit area per unit solid angle
per unit wavelength . Spectral signatures in the visible
IEEE SIGNAL PROCESSING MAGAZINE
JANUARY 2002
U.S. Government work not protected by U.S. copyright.
David W. J. Stein, Scott G. Beaven,
Lawrence E. Hoff, Edwin M. Winter,
Alan P. Schaum, and Alan D. Stocker
©DIGITAL VISION LTD.
to near infrared (VNIR), defined by wavelengths of approximately 0.4 to 1 µm, and short wave infrared
(SWIR), defined by wavelengths of approximately 1 to
2.5 µm, are given in terms of reflectance, i.e., the ratio of
reflected radiance to incident irradiance . In the long
wave infrared (LWIR), covering wavelengths ranging
from approximately 7 to 15 µm, the material spectrum is
measured in terms of emissivity, the ratio of the emission
from the material to that of a blackbody at the same temperature . To apply known-signal detection algorithms, the reflectance or emissivity spectra of the objects
of interest must be converted into radiance at the sensor,
or the radiance data collected by the sensor must be converted into reflectance or emissivity in a process known as
atmospheric calibration . If calibration panels are not
available in the scene, then atmospheric compensation of
hyperspectral imagery is based on models of illumination
and atmospheric scattering and absorption as a function
of wavelength, and for LWIR, temperature estimation.
Atmospheric transmittance and illumination depend on
many factors such as vertical temperature profile, water
vapor concentration, concentration of mixed gasses, the
concentration and types of aerosols, solar angle, cloud
cover, shadowing, and viewing geometry - . Errors
in estimates of environmental and sensor parameters may
lead to significant errors in the estimate of reflectance
spectra , which in turn leads to signal processing mismatch losses. Local variations in illumination and temperature further complicate the conversion of radiance to
reflectivity or emissivity, respectively .
As an alternative to converting sensor data into reflectivity or emissivity, matched target detection algorithms
may be implemented by converting spectral signatures
into radiance if the prevailing atmospheric, illumination,
and sensor parameters are known. Furthermore, if the parameters are known within a range of values, then subspace target models may be developed such that for a
large set of conditions the target in radiance at the output of the sensor is well represented by the sub- space,
i.e., the angle between the target and the sub- space is
small . Algorithms have been developed to detect
targets contained within a subspace - . If the
target subspaces are designed to be invariant to likely
variation in significant parameters, then atmospheric
compensation is not required for target detection purposes. However, detection performance generally diminishes as the dimension of the signature subspace increases if SNR is constant.
Target matching approaches are further complicated
by the large number of possible objects of interest and uncertainty as to the reflectance/emission spectra of these
objects. For example, the surface of an object of interest
may consist of several materials, and the spectra may be
affected by weathering or other unknown factors. One
may be interested in a large number of possible objects
each with several signatures. Thus, the multiplicity of
possible spectra associated with the objects of interest and
the complications of atmospheric compensation have
lead to the development and application of anomaly detectors that seek to distinguish observations of unusual
materials from typical background materials without reference to target signatures or target subspaces. In addition, change detectors are used to identify changes within
a scene that occur over time without presupposing target
signatures.
Anomalies are defined with reference to a model of the
background. Background models are developed adaptively using reference data from either a local neighborhood of the test pixel or a large section of the image. Local
and global spectral anomalies are defined as observations
that deviate in some way from the neighboring clutter
background or the image-wide clutter background, respectively. Both approaches have their merits. The local
spectral anomaly detector is susceptible to false alarms
that are isolated spectral anomalies. For example, consider a scene containing isolated trees on a grass plain.
Each separate tree may be detected as a local spectral
anomaly even if the image contains a separate region with
many pixels of trees. The global spectral anomaly detection algorithms are not susceptible to this type of clutter
generated false alarm. However, the global anomaly detector will not find an isolated target in the open if the signature is similar to that of previously classified background material.
In this article we develop anomaly detectors, i.e., detectors that do not presuppose a signature model of one
or more dimensions, for three clutter models: the local
normal model, the global normal mixture model, and the
global linear mixture model. The local normal model
treats the neighborhood of a pixel as having a normal
probability distribution. The normal mixture model considers the observation from each pixel as arising from one
of several possible classes such that each class has a normal
probability distribution. The linear mixture model considers each observation to be a linear combination of fixed
spectra, known as endmembers, that are, or may be, associated with materials in the scene, and the coefficients, interpreted as fractional abundance, are constrained to be
nonnegative and sum to one. We show how the generalized likelihood ratio test (GLRT) may be used to derive
anomaly detectors for the local normal and global normal
mixture models. The anomaly detector applied with the
linear mixture approach proceeds by identifying target
like endmembers based on properties of the histogram of
the abundance estimates and employing a matched filter
JANUARY 2002
IEEE SIGNAL PROCESSING MAGAZINE
Hyperspectral sensors differ from
multispectral sensors in that the
number of bands is much higher
and the spectral bands are
contiguous.
in the space of abundance estimates. To overcome the
limitations of the individual models, we develop a joint
decision logic, based on a maximum entropy probability
model and the GLRT, that utilizes multiple decision statistics, and we apply this approach using the detection
statistics derived from the three clutter models. Examples demonstrate that the joint decision logic can improve detection performance in comparison with the
individual anomaly detectors. We also describe the application of linear prediction filters to repeated images of
the same area to detect changes that occur within the
scene over time.
Anomaly Detection Algorithms
and the Generalized Likelihood Ratio Test
The GLRT is the foundation for the anomaly detection algorithms applied to the local normal, normal mixture, and
joint exploitation models. We describe the GLRT for unknown signals and, more generally, for signals that belong
to a known subspace. We give asymptotic distributions, as
the number of reference samples approaches infinity, of the
statistics assuming Gaussian noise, and we use these distributions to demonstrate that the performance of the anomaly detectors increases with SNR and diminishes with
increasing dimension of the signal subspace. These results
are used to compare the performance of two methods of
implementing anomaly detectors: a bank of low-dimensional subspace detectors or a single high-dimensional
subspace detector. The GLRT and the probability density
functions (pdfs) that account for limited sample sizes are
developed in more detail in .
Thresholding the likelihood ratio provides the hypothesis test that satisfies various optimality criteria including:
maximum probability of detection for a given probability
of false alarm, minimum expected cost, and minimization
of maximal expected cost . The pdf of the data is often
assumed to have a parametric form, and parameters,θ, of
the distribution are estimated from reference data. Alternatively, the GLRT approach estimates parameters from
test and reference data. One postulates a reference data set
consisting of N (we allow N =0,
which implies that the reference data set is empty) independent identically distributed (iid) samples of dimension K having pdf p0
and a set of test vectors,
, that is to be classified as arising
from either pdf p1
(H1) or p0
(H 0). The
Kelly and Reed and Yu developed GLRTs
for multidimensional image data assuming that the spectrum of the received signal and the covariance of the background are unknown. We denote the normal distribution
of mean µ and covariance Γ by N( , )
symbolizes
that the random vector x has pdf p, and x
that the pdf of xN converges to p in measure as N →∞.
The data are modeled as
such that s andΓ x are unknown. We assume a single pixel
target; the multipixel target version is a weighted sum of
the single-pixel test described here . Let x be the observation vector under test, and let{
a set of reference data. The GLRT is then
is the sample
covariance matrix of the reference data and (.)T denotes
the conjugate transpose for complex valued data that,
when applied to real data, reduces to the ordinary
transpose. As N →∞, RX converges to
RX is a single pixel form of the Reed Xiaoli algorithm
that is often approximated by (4). It is monotonically related to minus the log of the probability of the observation x under the null hypothesis. It arises in many other
contexts including - , and it may be interpreted as
a multivariate energy detector.
Asymptotic forms, as N →∞, of the probability distributions of RX, under the H 0 and H1 hypotheses are
given in terms of χ 2 and noncentral χ 2 densities, respectively . Let χ n
2 ()⋅denote the χ 2 density on n degrees
of freedom (dof), and let χ
denote the noncentral
χ 2 density on n dof having noncentrality parameter λ.
Let δ =1 2, if the data are real or complex valued, respectively. As N →∞,
Note that the distribution of the test statistic under H 0
is independent of the unknown parameters, and thus the
test statistic has the constant false alarm rate (CFAR)
IEEE SIGNAL PROCESSING MAGAZINE
JANUARY 2002
The GLRT has been extended to situations in which
the target signature is unknown and assumed to be in a
subspace . Scharf and Friedlander develop the
GLRT for the following model:
where W is a matrix such that the columns of W span an
interference subspace, V is a matrix such that the columns
of V span a signal subspace, and n is additive noise such
σ Γ . W, V, and Γ are assumed known,
= dim( ), θ and φ are assumed unknown, and σ 2 may
be known or unknown. General procedures have not
been developed for simultaneously estimating W and Γ.
If either 1) φ is locally constant or 2) the data may be segmented into regions such that φ is essentially constant on
each region, the termWφ may be absorbed into the noise
which is then modeled by n
µ Γ), where the parameters µ and Γ are estimated locally or for each segment.
With W =0, Γ may be estimated from background reference data, and if Γ =
K , a basis for W may be estimated as the eigenvectors of a background data
correlation matrix having eigenvalues greater than σ 2 .
The GLRT has also been derived for subspace signal
models such that the background covariance matrix is unknown. Yu et al. derive the GLRT for multiband data in
which the target signature is confined to a subset of the
bands . Kraut et al. consider the more general case of
a subspace signature model . They derive the GLRT
where V is as above, n
σ Γ , Γ is unknown, andσ 2 may be either known or unknown. Ifσ 2 is
known, then without loss of generality, one may assume
= . The case of unknown σ 2 results in a GLRT
such that the probability distribution under the null hypothesis is invariant to scalar variation of the test data.
The detection statistic for the case rank ( )
V >1and σ 2
is described. As above, let $Γ be the sample covariance of
the reference data. $Γ is positive definite with probability 1
and Hermitian, and therefore, it has Cholesky
factorization
−1 2 V , and P
, i.e., P$Φ is orthogonal projection (in the whitened space) onto ~Φ, the
subspace spanned by the columns of $Φ. The GLRT for
this case is 
The distributions of these GLRT for finite N are derived
in . KSM is a generalization of RX, and
computationally the difference lies in the projection operator in (9).
We compare the predicted performance of two methods of implementing anomaly detection algorithms. For
each possible target signature of interest, one might develop a subspace that encapsulates the uncertainty as to illumination and atmospheric transmission and
utilize a bank of subspace detectors. Alternatively, one
might develop a single subspace that encapsulates all uncertainty as to the target model. Figs. 1 and 2 illustrate the
performance of these approaches by comparing probability of detection (Pd) at a fixed probability of false alarm
(Pfa). The Pd of a bank of filters at a given Pfa is easily calculated from the Pd at each Pfa of the individual filters
and the assumptions: 1) the filters have independent output under the null hypothesis and 2) if a target corresponding to the target model of one of the filters is
present, then that filter will have maximal output. Further
analysis is required if the filters do not have independent
output under the null hypothesis. Fig. 1 shows the asymptotic performance of a bank of subspace detectors for
which the dimension of the signal subspace, D, is 6 as a
function of SNR and the number of subspaces, B. Fig. 2
shows the asymptotic performance of RX as a function of
SNR and K, the dimension of the observation space. Fig.
2 also illustrates the performance of KSM as a function of
SNR and D, the dimension of the signal subspace, by letting K
in the figure. Fig. 2 illustrates the degradation,
at a fixed SNR, in predicted performance as the dimension of the observation space or the signal subspace increases. Comparing Figs. 1 and 2 shows that a bank of
1000 subspace detectors, such that each subspace has dimension 6, has better predicted performance than a 100
dimensional RX or KSM detector, assuming, as noted
above, independent filters. In general, the filters will not
be independent under H 0 unless the subspaces satisfy appropriate orthogonality conditions. For example, the
bank of 1000 filters, each using a subspace of dimension
6, requires an SNR of 17.5 to achieve a predicted Pd of
0.5, while the RX detector applied to an observation
space of dimension 100 or the KSM detector applied to a
signal subspace of dimension 100 requires an SNR of 19
dB to have a predicted Pd of 0.5. However, the bank of
subspace detectors has a much greater computational
load as the set of target subspaces must be accurately determined, and the quadratic form is computed at each
pixel for each subspace detector while the RX and KSM
algorithms are implemented with a single quadratic form.
JANUARY 2002
IEEE SIGNAL PROCESSING MAGAZINE
Furthermore, the bank of subspace detectors may suffer
from mismatch losses that will degrade performance.
In section four anomaly detectors with non-Gaussian
pdfs are utilized. Assume the following model:
where θ0 is a set of parameters of the pdf p0. If θ0 is
known or estimated from a large sample of reference data
and s is unknown, then the GLRT is well approximated
Note that (13) generalizes the RX algorithm (4).
Models of Hyperspectral Imagery and
Anomaly Detection Algorithms
Anomaly detection algorithms are used to distinguish observations from the background when target models are
not available or are unreliable. In this section we outline
three models: the local Gaussian model, the global
Gaussian mixture model, and the global linear mixture
model. These models have been widely used in the community and provide complementary interpretations of
the data. For each model we describe an associated anomaly detection algorithm. The anomaly detection algorithms for the local and global normal mixture models are
applications of the GLRT.
Local Normal Model
The Gaussian model leads to the anomaly detectors (3),
(4). If the mean and covariance matrix are estimated
within a neighborhood of every test pixel, one obtains
the RX algorithm . The local Gaussian model may
not be valid for hyperspectral data if relatively small regions contain multiple materials. The BHEP
(Barringhaus, Henze, Epps, and Pulley) test was
used to evaluate the local normal model. The BHEP test
statistic compares the empirical characteristic function
of the data, transformed to zero mean and identity
covariance, with the characteristic function of the normal distribution having mean zero and identity
covariance matrix. The test is consistent, invariant under
affine transformations, and applicable to any number of
samples and data dimensions. The first three moments
of the limiting distribution, as the number of samples
approaches infinity, are known and can be used to approximate thresholds of the test statistic corresponding
to prescribed probabilities of rejecting the null hypothesis when it is true. The test was applied to a hyperspectral
scene to evaluate the goodness-of-fit of a local normal
model. For these data, at a probability of false rejection
of 10-5, the local normal hypothesis was rejected for
more than 90% of the pixels. Thus, the local normal
model may not capture the complexity of hyperspectral
imagery. The presence of multiple materials in close
proximity suggests that global mixture distributions will
provide more accurate descriptions.
Gaussian Mixture Model
The Gaussian mixture model is one method of characterizing spectral data observed from nonhomogenous,
multicomponent scenes. This approach models each datum as an observation of a random vector having one of
IEEE SIGNAL PROCESSING MAGAZINE
JANUARY 2002
L 1. Limiting performance of KSM for D=6 and B=1,10,100,1000
where D is the dimension of the subspace, and B is the number of filters in the filter bank.
L 2. Limiting performance of RX for K=2,5,10,25,50,100, where K
is the dimension of the observation space, and also the limiting performance of the KSM algorithm if K is replaced by D,
the dimension of the signal subspace. Note that the greater
the uncertainty in the signal, as manifested in a signal
subspace of larger dimension, the greater the SNR required for
equivalent performance.
several possible multivariate Gaussian distributions. The
pdf of the scene is a Gaussian mixture distribution:
where π c is the probability of class c.
It is applied as a global model since the parameters are
estimated over relatively large regions. In our work, the
parameters of the Gaussian mixture model are estimated
using the stochastic expectation maximization (SEM) approach , which is a variation on the expectation maximization (EM) algorithm . For each iteration SEM
randomly assigns observations to current classes, using
the conditional distribution of the classes given the observations, and updates the class statistics as the class sample
statistics. The iterations cease when a convergence criterion based on mean likelihood or the change in the parameter values is satisfied.
The Gaussian mixture model may also be used for
scene classification. Pixel classification, i.e., segmentation, is achieved by assigning individual pixels to a class
using a maximum a posteriori (MAP) classifier . The
MAP classifier identifies the most likely class for a given
observation according to Bayes Law.
Detection algorithms may be derived from the Gaussian mixture clutter model. Anomaly detection may be
achieved by applying the GLRT (13) to the model (14).
Alternatively, data may be classified according to the
MAP classifier, and a class conditional RX algorithm, (4),
may be used to identify anomalies. This approach is denoted GMRX.
Linear Mixture Model
Hyperspectral imagery may also contain pixels that are
composed of several materials, and the linear mixture
model has been developed to analyze such data .
This approach models the observation xi as
is the spectrum (reflective or emissive) of
a pure pixel of material m, a
is the fractional abundance of material m at pixel i, and ε is additive noise. The
observations are modeled as convex combinations of constituent spectra, s m, known as endmembers. The constraints imposed on the abundance values represent the
facts that a pixel cannot contain more than a full pixel nor
a negative quantity of a substance, that materials replace
each other rather than combine via superposition, and
that the set of endmembers should include all material
types present in the scene. The linear mixture model is a
global model in that the set of endmembers is assumed to
be valid over relatively large regions. Fig. 3 illustrates observations expressed as a convex combination of three
endmembers. The endmember identified as a shade point
consists of sensor noise and path radiance.
Several different procedures have been developed to
automatically find the endmembers, s m , from
hyperspectral data. If M endmembers are to be found, the
data are typically transformed to a subspace of dimension
M-1. The endmembers have been obtained as the vertices
of the minimal volume simplex, or an approximation
thereof, that encloses a subset of the transformed data.
The ORASIS algorithm, the first demonstrated real-time
autonomous endmember selection method, uses this approach . Constructing the minimal volume enclosing
simplex is computationally intensive, however the algorithm does not require pure pixels in order to uniquely
identify endmember spectra . Alternatively, one may
define the endmembers to be the vertices of the maximal
volume simplex such that the vertices consist of observation vectors. This approach underlies the NFINDR algorithm . For this method to produce endmembers,
there must be at least one pure pixel of each material .
Alternatively, the iterative error analysis (IEA) approach
finds the endmembers by performing a series of constrained unmixings and choosing the endmembers to be
those observations that minimize the error in the unmixed image .
Given the endmembers, the abundance estimates are
obtained as the solution of a constrained least squares
problem. If the inequality constraints are utilized, the
abundance estimates are the solution of a quadratic programming problem. To reduce the computational requirements, certain authors have neglected the inequality
constraints, and the abundance coefficients are then
found as the solution to a system of linear equations. The
computational burden is then dominated by the computation of a matrix vector product at each pixel.
NFINDR has been coupled with a two-step procedure, the stochastic target detector (STD), for the autonomous detection of global spectral anomalies. This
process first identifies certain of the endmembers as target-like and then applies a matched filter for each of the
target-like endmembers in the abundance space. Targets
are assumed to be rare, and furthermore, the abundance
of a target is assumed to be near one on a pixel that contains the target and near zero on a pixel that does not.
Therefore the histogram of the abundance values of a target endmember will have a high peak to RMS value, and
only a small percentage of abundance values, approximately the percent of pixels containing the target, will be
above a noise threshold. Using these criteria, the indices
of the abundance vector a
are segregated
into sets I
consisting
of the indices of the clutter components and the target-like components, respectively. Let x
be the clutter and target like components of the abundance vector a i, respectively. For
each target-like component,1≤
t,the following detection problem presents itself, dropping the index i:
JANUARY 2002
IEEE SIGNAL PROCESSING MAGAZINE
where: m x
τ are the mean of x and y, respectively,
when a target corresponding to component τis present in
a pixel; m x and m yare the mean of x and y under the null
hypothesis; the additive components, c x and c y , are the
fluctuations of the clutter-like and target-like components. Both the null and alternative hypotheses have a
mean value added to a zero mean random fluctuation.
Note that the mean in the alternative hypotheses replaces
the clutter mean in the null hypotheses. These hypotheses
are modeling the fact that the target obscures the background and does not add to it. The target and clutter are
assumed to experience similar fluctuations in the scene.
We assume that
in component
Under these assumptions the matched filter for each target type may be written as , :
The matched filter is the likelihood ratio assuming
Gaussian distributed noise, and it is the linear filter having
maximal SNR for distributions having second-order moments. The linear transform H x
is the linear prediction filter of y given x having minimal
mean square error, and the covariance matrix ofy
. A bank of matched filters of the form
(17) is used to detect the presence of any of the targets
Multiple Algorithm Fusion
Three complementary statistical models of hyperspectral
data and associated detection algorithms that do not presume a target signature or subspace of signatures have
been presented. Evaluation of the resulting detection algorithms on hyperspectral imagery provides examples of
each algorithm outperforming the other two. Since many
of the conditions and parameters that govern performance are unknown or poorly characterized in an operational setting, it can be very difficult to dynamically select
the optimal detection algorithm. To obtain more consistent anomaly detection performance from a single algorithm, one might investigate more general statistical
models , . Alternatively, we construct a multiple-algorithm-decision or fusion criterion that combines
multiple discrimination features. In a conventional fusion
approach, simple logical operations (e.g., AND, OR)
would be applied to the individual detector outputs at the
post-threshold or object-declaration stage. These ad-hoc
rules tend to produce highly variable performance results
from one case to another (including occasional losses in
sensitivity), and it is notoriously difficult to specify the
“best” fusion logic in advance. In the approach described
here, a modeled joint distribution of the detection statistics is constructed, and the multiple algorithm decision
criterion is obtained by applying a threshold test to the resulting joint pdf (13). As described below, the joint distribution is obtained by modeling the marginal distribution
of the detection statistics and their correlation.
Marginal Distributions
Gamma mixture distributions were used to model the output of the RX, GMRX, and STD algorithms. The gamma
density with shape parameter V and scale parameter a is
given by p
AnMcomponentgammamixturedistributionisdefinedby
As described above, if the background has a normal
pdf, then the output of a quadratic detector has a central
or noncentral χ 2 distribution (5), (6). The noncentral χ 2
function may be expressed as a mixture of gamma distributions . Furthermore, parameter estimation error
distorts the output statistics of quadratic detectors from
χ 2 to noncentral χ 2 and thus to gamma-mixture. The
output of quadratic detectors acting on normal mixture
IEEE SIGNAL PROCESSING MAGAZINE
JANUARY 2002
Endmember b
Endmember a
Shade point
L 3. Data are depicted as a convex combination of a shade point
and two other endmembers.
data may be modeled by a mixture of noncentral χ 2 distributions, which in turn may be modeled as a mixture of
gamma distributions. Thus, the output of the GMRX and
RX algorithms should be well modeled using gamma
mixture pdfs, and this has been verified empirically. We
have also found empirically that the positive valued side
of the output of the NFINDR-STD algorithm is well
modeled using gamma mixture distributions. Fig. 4 illustrates the fit of a three-term gamma mixture model complementary cumulative distribution function (CCDF) to
the empirical CCDF of the output of the GMRX detector. For these data, the RX and STD output were similarly well described by a three-term gamma mixture
Modeling the Joint Distribution
Given a set of marginal distributions, we construct the
maximum entropy joint distribution such that the
marginals of the joint distribution are equal to the given
marginals and such that the normal-score correlation of
the joint distribution is equal to the empirical normal-score correlation . Let x
random vector such that xk has marginal distribution
= Ψ with CDF Ψk. Let Φ be the zero mean unit variance normal CDF. Then
is the normal score correlation of
where M denotes the determinant of the matrix M and
∇T x is the Jacobian of T at x; f is the pull back of the
normal pdf p via the transform T. Then f is the distribution having maximum entropy among distributions with
marginal distributions ψ k, and normal score correlation
C . We use this construction to model the joint distribution of the (RX,GMRX,STD) output using the
gamma mixture marginals described above. Joint decision curves are defined as contours of the joint density
function (13). This defines the multiple algorithm fusion
(MAF) approach. Alternatively, one may transform the
data so that the marginals have exponential densities, refit
the marginals with gamma densities , and apply the
above construction to obtain the joint density.
Fig. 5 illustrates the joint decision boundaries, i.e., the
contours of the joint pdf obtained from the above construction applied to the RX and GMRX algorithms. The
asterisks and diamonds are GMRX-RX values of background pixels and test objects, respectively. Evidently, the
number of false alarms incurred in order to place all targets in the correct class is significantly diminished if the
joint decision contours, rather than thresholds applied to
the RX or GMRX values alone, are used to segregate the
background region from the target region.
Detection Performance
The algorithms were applied to a hyperspectral image in
order to compare performance. The relative performance
of RX, based on the local normal model, GMRX, based
on the Gaussian mixture model, NFINDR-STD, based
on the linear mixture model, and MAF is illustrated in
Figs. 6 and 7. Fig. 6 shows, for a fixed Pd, the ratio of the
number of false alarms incurred using each of the algorithms to the number incurred using the RX algorithm.
For this example, GMRX produces more false alarms and
NFINDR-STD produces fewer false alarms than RX,
while MAF yields fewer false alarms then either of the individual algorithms. In Fig. 8, the algorithms are compared on the basis of relative probability of detection at a
fixed number of false alarms using MAF as the standard.
For this example, the algorithms are ranked in order of increasing number of targets detected as follows: RX,
GMRX, NFINDR-STD, and MAF. These results demonstrate improved detection performance from jointly
exploiting anomaly detection algorithms based on the
three background clutter models.
JANUARY 2002
IEEE SIGNAL PROCESSING MAGAZINE
Tail Probability
df1=42.0046, 1am1=0.0232864, wt1=0.906323
df2=18.8218, 1am2=0.0589712, wt2=0.0815714
df3=0.903831, 1am3=0.904172, wt3=0.0121052
Detector Threshold
L 4. Comparison of the empirical CCDF of the GMRX output and
the CCDF of the three-term gamma mixture fit to hyperspectral
Hyperspectral Change Detection
If an object of interest is inserted into a scene, moves
within a scene, or undergoes a change of its reflectivity or
emissivity during a time interval, then change detection
techniques may be used to ascertain this event. Geographically registered images from the beginning and end
of this time interval are assumed to be available. Due to
variations in atmospheric conditions, background illumination, temperature, and possibly sensor response, simple subtraction of observed radiances from two perfectly
registered images typically produces a cluttered residual,
in which it is impossible to distinguish natural changes
from the removal or appearance of small objects. However, at each wavelength, the radiance measurements of
the same material viewed under two sets of conditions are
linearly related . Jensen applied this relationship to
multispectral Landsat imagery to identify changes in urban land use . He separately estimates the linear regression coefficients for each wavelength. This approach
is generalized in the Chronochrome algorithm . It
employs a multivariate linear regression filter to predict
the radiance of an image of a scene from an earlier radiance image of the scene. Changes occurring at any pixel in
the image are indicated by thresholding an RX detection
statistic applied to the residual consisting of the difference
between the image and the predicted image.
Spectrally dependent changes in path radiance, atmospheric attenuation, scene illumination, and sensor gain
and offset that accumulate over time all modify sensed
radiances according to an affine transformation . Let
xi, be a hyperspectral vector collected at time i. One expects that x
, where the offset O models the
change in path radiance and the gain G models the change
in illumination, temperature, sensor gain, and atmospheric transmission from time t1 to time t 2 . All of these
physical effects are consistent with the matrixG being diagonal. However, in practice nondiagonal matrices provide noticeably better models, which may be due to
imperfect pixel-to-pixel registration or spatial variations
in temperature (LWIR) or illumination (VNIR-SWIR).
A multivariate regression analysis of hyperspectral
data collected at several times can often be used to estimate the model parameters Oand G. This first step in the
Chronochrome method produces a least mean squared
estimate $x2 of any pixel value at time 2 given by
where the parameters E x
2 , Γ11, and Γ21 are estimated from the data. Equation (23) describes a regression hyperplane.
The second step in the Chronochrome algorithm consists of finding those pixels for which the estimated signal
IEEE SIGNAL PROCESSING MAGAZINE
JANUARY 2002
Relative Pd
Algorithm/Model
L 7. Detection performance of anomaly detectors at a fixed probability of false alarm relative to fusion of multiple anomaly detectors .
Relative No. False Alarms
Algorithm/Model
L 6. False alarm performance of anomaly detectors based on different stochastic background models at a fixed probability of
detection relative to the RX algorithm.
L 5. Contours of the maximum entropy joint distribution of the RX
and GMRX statistics.
is improbable. This is fundamentally the problem addressed by the “RX” algorithm for detecting multivariate
anomalies, which prescribes that the magnitude of
be compared to a threshold after it is whitened with its own covariance structure. That is, one looks
for large values of
The transformation (23) fails to account for local variation in temperature or illumination affecting longwave
and reflectance data, respectively, as the regression coefficients are computed globally. For example, in the 8-12
µm region, radiance is driven by temperature, which is
unpredictable over periods of only a few hours. The
emissivity of most background constituents are high, typically above 0.9, implying that there is a rather well defined single spectral direction corresponding to
temperature. This direction is approximated as the
eigenvector corresponding to the largest eigenvalue of
the matrix Γε, and it is suppressed by the RX statistic.
Fig. 8 illustrates the application of Chronochrome to
data from the hyperspectral thermal sensor SEBASS .
The first plot shows projections of spectral signals collected on two consecutive days. Each scatter plot manifests the well-defined edges and triangular structures
consistent with a linear mixture model. The second plot
overlays predictions of the signals using (24) atop the actual data, and it shows large apparent errors. The final
plot reveals that the error ε in fact tends to be concentrated in one direction (temperature). The off-diagonal
outliers shown in the third plot all correspond to pixels on
vehicles that had been present in only one scene. These
anomalous signals are sufficiently distinct from those associated with temperature that RX applied to ε enables
target detection at high SNR.
Summary and Conclusions
Anomaly detectors for hyperspectral data, i.e., detectors
that do not require target signature models, have been developed based on fundamental detection theoretic principles, including the GLRT and approximations thereof. In
this article, we have outlined the underlying theory for
the application of anomaly detection to systems with inherently high dimensionality, and we have demonstrated
that the performance improves with SNR and diminishes
with increasing dimension. SNR depends on the characteristics of the targets, clutter, environment, and sensor.
The differences between the anomaly detection approaches are tied to the underlying clutter models that are
imposed. Here we have given an overview of three principle types of background clutter models: locally Gaussian,
Gaussian-mixture, and linear mixture models. Additionally, a method for jointly exploiting multiple anomaly
detectors through fusion of their outputs is described.
Sample relative performance of the detectors and their
joint performance support the use of multiple-model
anomaly detection in the analysis of hyperspectral data.
The Chronochrome algorithm extends the idea of anomaly detection to the detection of changes within a scene
occurring over time without employing signature models. Future work on anomaly detection should include the
development of dimensionality reduction transforms that
preserve SNR for the targets of interest so that the improved performance provided by reduced dimensionality
may be realized. Figs. 1 and 2 suggest that improved
anomaly detection could result from using a bank of
subspace detectors rather than a global anomaly detector,
and methods of defining this collection of subspaces
should be explored. Further improvements in anomaly
detection may result from using improved clutter models
as suggested by the work on stochastic mixture models
that unifies the linear mixture and Gaussian mixture ap-
JANUARY 2002
IEEE SIGNAL PROCESSING MAGAZINE
L 8. (a) Day 1 scatter (red), day 2 scatter (green). (b)
Chronochrome prediction (green) of day 1 data (red). (c) Error
scatter concentrated in temperature direction, with real
changes displaced from diagonal.
proaches , . Furthermore, the requirements for
image registration in the application of Chronochrome
need to be established.
Acknowledgments
The authors would like to thank the Defense Advanced
Research Projects Agency and the Office of Naval Research for supporting this work and the reviewers for
their helpful suggestions.
David W.J. Stein received the B.A. in mathematics-economics from Reed College in 1979 and the Ph.D. in
mathematics from Brandeis University in 1986. He is
presently a Scientist at SPAWAR Systems Center, San
Diego, where he has worked on modeling and signal processing issues for SONAR, RADAR, and spectral sensors. He has published over 30 journal and conference
articles. He holds five patents in the area of signal processing.
Scott G. Beaven is a Corporate Senior Staff member at
Space Computer Corporation, where he works on
hyperspectral and multisensor signal processing. He has
worked in the area of remote sensing since 1990. From
1990-1995 he was an ONR Doctoral Fellow at the University of Kansas Radar Systems and Remote Sensing
Laboratory, where he worked on microwave sensing of
the polar oceans for ONR and NASA. From 1995-2000
he was with the Space and Naval Warfare Systems Center
(SPAWAR) in San Diego, where he was involved in the
analysis and processing of hyperspectral sensor imagery
for DARPA and ONR-sponsored programs.
member of IEEE and has published over 30 conference
and journal articles.
Lawrence E. Hoff received the B.S.E.E. degree from Iowa
State University, Ames, the M.S.E.E. degree from the
University of Michigan, Ann Arbor, and the Ph.D. degree in information and computer science from the University of California, San Diego. He has specialized in
target detection and classification, signal processing, and
information theory. Since 1989 his work has been focused on target detection and recognition of targets in
hyperspectral infrared and in UHF synthetic aperture radar data. He currently works as an independent contractor for Hoff Engineering.
Edwin M. Winter is Research Director for sensor systems
at Technical Research Associates, Inc. He received his
Ph.D. in geophysics from the University of California,
Los Angeles, in 1972 and has been active in remote sensing for 29 years. His recent technical emphasis has been
the analysis of data from hyperspectral sensors. He has
presented papers at SPIE, IEEE, ERIM, EUROPTO,
IRIS and other technical meetings and has published papers in the Physical Review, Remote Sensing of the Environment, Applied Optics as well as numerous conference
proceedings.
Alan P. Schaum received a Ph.D. in theoretical physics in
1978 from the Johns Hopkins University, where he had
applied relativistic quantum field theory to the study of elementary particle bound states. Until 1983 he worked at
Bell Laboratories, applying information and queuing theories to network planning problems. For the past 18 years
at the Naval Research Laboratory, he has developed signal and image processing methods in support of intelligence operations. He has published over 70 technical
papers and is first author on more than 50 of these. His
current research concentrates on hyperspectral detection
theory and the modeling of terrestrial background signatures and their dynamics.
Alan D. Stocker has been Vice President of Space Computer Corporation, Los Angeles, since 1987. He received
his M.S.E.E. from Stanford University in 1981 and previously worked on the technical staff of Hughes Aircraft
Corporation and MARK Resources, Inc. in the area of radar signal processing. For the past nine years his primary
technical focus has been the development and implementation of data processing algorithms for multispectral and
hyperspectral imaging sensors. He has authored over 30
papers on various aspects of hyperspectral data analysis
and processing, including sensor correction and calibration, spectral signature phenomenology, target detection
and recognition, and real-time systems.