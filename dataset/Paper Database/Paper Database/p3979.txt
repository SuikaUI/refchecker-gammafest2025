IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 10, OCTOBER 2005
Consistent Independent Component
Analysis and Prewhitening
Aiyou Chen and Peter J. Bickel
Abstract—We study the statistical merits of two techniques
used in the literature of independent component analysis (ICA).
First, we analyze the characteristic-function based ICA method
(CHFICA) and study its statistical properties such as consistency,
-consistency, and robustness against small additive
noise. Second, we study the validity of prewhitening: a preprocessing technique used by many ICA algorithms, as applied to
the CHFICA method. In particular, we establish the surprising
effectiveness of this technique even when some components have
heavy tails and others do not. A fast new algorithm implementing
the prewhitened CHFICA method is also provided.
Index Terms—Asymptotic normality, characteristic function,
consistency, incomplete Cholesky decomposition, independent
component analysis, prewhitening.
I. INTRODUCTION
VER the past decade, independent component analysis
(ICA) has received much attention in many different
ﬁelds, such as signal processing and machine learning ,
 , , . It has been used as a standard statistical tool for
blind source separation, e.g., in brain imaging analysis .
Formally, the classical ICA model is of the form
is a random vector of observations,
is a random vector of hidden sources
with mutually independent components, and
is a nonsingular
mixing matrix. Deﬁne
, which is usually called the
unmixing matrix. It is well known that
) is identiﬁable
up to ambiguity of order and scaling if and only if at most one
’s components is Gaussian , , . We call these
assumptions identiﬁability conditions. To specify
we need to put some scale and permutation constraints either on
independently and identically distributed
(i.i.d.) samples of
, ICA aims
to estimate the unmixing matrix
and thus to recover each
hidden source using
. This type of problem is also called blind source separation
(BSS) in the engineering literature. Many statistical approaches
Manuscript received March 6, 2004; revised December 23, 2004. This work
was supported by the National Science Foundation under Grant DMS-01-04075.
Part of the results of this paper appeared in the Fifth International Conference on
Independent Component Analysis, August 2004, Granada, Spain. The associate
editor coordinating the review of this manuscript and approving it for publication was Dr. Behrouz Farhang-Boroujeny.
A. Chen is with Bell Laboratories, Lucent Technologies, Murray Hill, NJ
07974 USA (e-mail: ).
P. J. Bickel is with the Department of Statistics, University of California,
Berkeley, CA 94720 USA (e-mail: ).
Digital Object Identiﬁer 10.1109/TSP.2005.855098
have been proposed for such BSS. Some are based on contrast
functions or estimating equations derived from maximal likelihood (ML), mutual information, as well as other criteria under
speciﬁc parametric models for the sources. These methods use
features that do not determine the distribution uniquely, and
they are therefore not consistent without further assumptions.
An example is the Joint Approximate Diagonalization of Eigenmatrices (JADE) method, which relies on the source distribution’s fourth multilinear cumulant not vanishing . Such inconsistency can be readily explained by marginal mismatch of
the hidden sources’ distributions . Other methods (e.g., ,
 , , and ) are based on nonparametric approximation
of some feature functions of hidden sources, such as probability
density function or density score, which do determine the distribution. Theoretical analysis of these methods is not available
(but see Chen and Bickel ). See and references therein
for an extensive review of ICA methodologies and algorithms.
The characteristic-function based ICA method (CHFICA)
 has the virture of not requiring an estimation of delicate
parameters such as densities and, yet, should be consistent
under general conditions. Eriksson and Koivunen showed
that the CHFICA method performed very well under simulations and gave a formal argument for consistency.
Prewhitening is a popularly used preprocessing technique in
the ICA literature, which speeds algorithms up substantially. Its
validity is expected when all hidden sources have ﬁnite second
moments, and, under second moment constraints, Cardoso 
obtained a lower bound on estimation errors of the prewhitened
ICA algorithms. We found in simulations that even with heavytailed data on some sources, prewhitened CHFICA still works
In this paper, we address both the question of
-consistency
and asymptotic normality for CHFICA and its surprisingly good
performance after prewhitening. The paper is organized as follows. In Section II, after reviewing CHFICA, we study its
consistency,
-consistency, and asymptotic normality, as
deﬁned in Theorem 1, and robustness properties. In Section III,
after reviewing prewhitening as a preprocessing technique for
ICA, we propose a new algorithm to implement prewhitened
CHFICA by using incomplete Cholesky decomposition. In
Section IV, we study the consistency of prewhitening in terms
of the acting parameter space of the matrix
and show that
the prewhitened CHFICA method can be consistent, even when
some of the hidden sources do not have ﬁnite second moments.
In our conclusion in Section V, we review the performances of
the procedures we have considered both from the theoretical
and numerical point of view, which, as we have noted, are in
agreement. Some technical details are given in the Appendix,
1053-587X/$20.00 © 2005 IEEE
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 10, OCTOBER 2005
whereas others are in the technical report available on the
WEB ( 
In this paper, for any matrix, say
denote its th
row and th column separately.
denotes the Euclidean norm
for a vector and denotes the Frobenius norm for a matrix. We
use boldface uppercase to denote random vectors and random
II. STATISTICAL PROPERTIES OF CHFICA
A. Review of the Characteristic-Function Based ICA Method
For a random vector , its characteristic function (cf.) is de-
and its empirical characteristic
function (e.cf.), given
i.i.d. observations
, is deﬁned
well known that
can be factorized into the product of its
marginal characteristic function if and only if
has mutually
independent components. For
, then the difference between
and the product of its marginal cf., for example
-dim probability density function
, can be considered to be a measurement of the dependence level among
components. To make sure that
if and only if
components are mutually independent, it is sufﬁcient to choose
such that it is continuous and
is a one-dimensional
(1-D) positive density function.
To avoid unidentiﬁability, we consider a true matrix
whose rows are scaled and permuted such that I) each of its
rows has norm 1; II) the element with maximal modulus in each
row is positive; III) the rows are ordered by
iff there exists
). We denote the set of matrices
that satisfy conditions I-III by
It is obvious that if
nonsingular matrix, then by
rescaling and permuting its rows appropriately the transformed
matrix will belong to
. We denote such a row-rescaling-permuting transformation as
. Note that the matrix
CHFICA makes use of this criterion given by (2) as follows.
Using the observations of
, the CHFICA estimator of the matrix
is deﬁned by
B. Convergence Rates and Some Simulations
is a function of
can be obtained by
directly solving the above optimization problem. Like many
other ICA methods, we approximate a feature function of hidden
sources: the characteristic function. The advantage here is that
the characteristic function can be estimated easily and consistently by the corresponding e.cf. Eriksson and Koivunen 
argued for the consistency of this estimator using extensive simulations and some heuristics. Here, we give a rigorous proof of
this claim and also study the
-consistency and asymptotic
normality of the CHFICA estimator.
, and deﬁne
. Obviously,
a cf. corresponding to the density function
We will often omit the subscripts of
the dimension is obvious from their arguments.
For a sequence of random vectors with ﬁnite dimension
. For two 1-D random sequences,
Theorem 1: Suppose that
is a symmetric density function
be the true underlying
unmixing matrix. Then
Under the identiﬁability conditions, the estimator
deﬁned by (3) is consistent, that is,
ii) If the ﬁrst and second derivative functions of
are denoted by
separately, are both bounded,
-consistent estimate of
, that is,
iii) Under the same conditions as for ii),
is asymptotically
normal, i.e.,
random variables, and its elements are decided by the
following equations: For
th entry of
. The explicit formulae for
are given in the
supplement .
can be consistently estimated by estimating
and the variance-covariance
. Having done this, one
can put conﬁdence bands on
. Alternatively, the normality
result shows that one can use the nonparametric bootstrap distribution of
for this purpose.
CHEN AND BICKEL: CONSISTENT INDEPENDENT COMPONENT ANALYSIS AND PREWHITENING
Theorem 1 is a special case of Theorem 2 below in the case of
no additive noise. The complete proof of Theorem 1 is in .
Here is the outline of our proof.
Outline of the Proof of i): The main idea is that the contrast function
can be expressed as a
-process1 indexed
. Then, we can apply the Uniform Law of Large
Numbers (ULLN) for this
-process such that
Then, since
can be identiﬁed uniquely in
, the consistency of
follows from the same arguments of compactness and continuity as for classical likelihood inference.
Outline of the Proof of ii) and iii): The key is to parametrize the 1-D curves from
indexed by corresponding
tangent vectors (a compact set) using the manifold structure of
(each row is on a
-D unit ball). Then, by making use of a
second-order Taylor expansion,
can be expressed as
a ratio of two terms indexed by
-processes separately. Then,
the convergence rate can be obtained by applying ULLN and
the central limit theorem for the
-processes in the denominator
and numerator. Asymptotic normality follows similarly by the
delta method. Note that
is not an M estimate so that the analysis using the U processes is needed.
Although different choices of
can lead to different
performance, taking
Gaussian or Laplace seems to be a
good choice. Fig. 1 shows some further simulation results
by using MATLAB, where the above characteristic function-based ICA method was implemented by an algorithm,
called PCFICA, described in Section III-B. Its performance is
compared with that of several other ICA algorithms such as
FastICA , JADE , KGV , and EFFICA , where
EFFICA has been proven to be asymptotically efﬁcient under
moderate conditions. Eight hidden sources were used, which
were generated from
, exponential (1), t-distribution
(3), lognormal (0,1),
, logistic (0,1), Weibull (1,1), and
exponential
, independently. An 8
matrix was generated randomly, the sample size used was 1000,
and the experiment was replicated 100 times with the same
mixing matrix to obtain the boxplots, where the estimation
were measured by the so-called “Amari
error” , i.e., for two
(it is necessary to normalize each row of
). The simulation also suggests that the CHFICA estimator
may not be efﬁcient.
Note that the computational complexity for calculating
directly from (4) is at least
 . For PCFICA,
1The U-process based on P and indexed by F is
(f):= (n m)!
f(X(j ); . . . ; X(j )
f 2 F, where I
= f(j ; . . . ; j
2 f1; . . . ; ng and j
6= j ; for k 6=
Performances of different ICA algorithms: m = 8 hidden sources
and n = 1000 sample size were used, and the experiment was replicated 100
times; the boxplots of Amari errors were based on quartiles and the numbers
above the boxplots were the average running time (seconds per experiment) of
the corresponding algorithms.
we used prewhitening as a preprocessing tool and the incomplete Cholesky decomposition to speed up the computation (see
Section III-B).
C. Robustness Against Small Additive Noise
In practice, the model (1) rarely holds exactly. A more realistic situation allows some additive noise. That is, the observation
can be modeled as
are the same as in the previous sections,
1 random vector, independent of
, standing for an additive
noise vector (for example, sensor noise), and
is the magnitude
of additive noise. This is usually called the noisy ICA model. In
 , there is a good review of studies of thess types of models.
Our objective here is to study how the cf.-based ICA method behaves in the presence of noise. We borrow Bickel and Doksum
 ’s approach in their study of the robustness of Box–Cox transformations. To be more precise, we assume a large sample size
, and further,
. We study how the estimation error behaves in relation to its natural scale
Theorem 2: Suppose that
satisﬁes the conditions of
Theorem 1. Then, as
, we have the following.
ii) If further
The idea of our proof is similar to that outlined for Theorem 1
in Section II-B. We refer to for the complete proof.
Part i) of Theorem 2 is trivial. Part ii) says that as long as
is of smaller order than
, the effect of the noise on
minimal. Put another way, if the ratio of noise scale to the estimation error is small, the ratio of Bias
Var for estimation of
is also small. Fig. 2 shows a simulation study to illustrate
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 10, OCTOBER 2005
Performance of PCFICA w.r.t. different noise scales, where i.i.d.
N(0; 1) additive noise and the previous m = 8 hidden sources were used to
generate noisy mixtures by (8); n = 1000, and each experiment is replicated
100 times to obtain the boxplots; the ﬁrst boxplot corresponds to no noise, i.e.,
r(n) = 0, the second boxplot r(n) = 0:1, and so on.
Theorem 2(ii), where the eight hidden sources above were used
, and without loss of generality, the identity matrix was used for the mixing matrix. Each boxplot, which was
obtained by 100 replications, corresponds to a different noise
. By comparing the medians, the slope before
the right-hand side of (9) is about 0.4. The variance term (about
0.5 by the ﬁrst boxplot, which corresponds to
be roughtly interpreted as the signal standard deviation (about
. In this example, the error due to additive
noise will dominate the estimation error only if
Both the simulation and the theorem say that CHFICA can
provide fairly good estimates of
, even in the presence of
small additive noise in model (1). It turns out that the bias term
due to additive noise is not sensitive to the sample size
the exact distribution of the additive noise for small
leave out further simulations due to space limitations. Because
of its global consistency and robustness properties, CHFICA
can serve as a good starting point for the more efﬁcient techniques of Chen and Bickel .
III. NEW PREWHITENED ICA ALGORITHM
Prewhitening is a popularly used preprocessing technique in
the ICA literature. For example, many famous ICA algorithms
such as FastICA , JADE , KCCA, KGV have
used this preprocessing technique. In general, prewhitening
is expected to be valid when all hidden sources have ﬁnite
second moments. In this section, we ﬁrst brieﬂy review this
concept and then provide a new algorithm for implementing
the characteristic function-based ICA method. Delicate studies
of prewhitening and prewhitened CHFICA (PCFICA) are left
to Section IV.
A. Introduction to Prewhitening
Since the matrix
for model (1) can be arbitrary, naively,
we have to optimize some constrast function, for example,
(4), over all
nonsingular matrices to obtain an estimate. However, prewhitening can project the optimization
onto the Stiefel manifold of orthogonal matrices . Optimization on a Stiefel manifold can be solved efﬁciently
be the square root
obtained by Singular Value Decomposition
(SVD), i.e., let
be the SVD decomposition
, which is an identity matrix, and
is a diagonal matrix comprised of
’s eigenvalues;
, and (1) is equivalent to
Without loss of generality, we may assume that each hidden
has unitary variance (we assume currently
a ﬁnite variance for each source and lose this assumption later). By considering the covariance matrix, we have
, and thus
must be an orthogonal matrix. Notice that
is still an ICA model, but restricting to orthogonal matrices is
computationally very advantageous. Since
can be estimated
directly by the sample covariance matrix of
, a prewhitened ICA algorithm
ﬁrst estimates an orthogonal unmixing matrix
by ﬁtting the ICA model with input
in some way and then estimates the unmixing matrix by
because of (10). This is the so-called
prewhitening (or whitening) technique. Note that
efﬁcient estimate of the variance-covariance matrix of
and only if the distribution of
) is Gaussian so that
the resulting estimate is not efﬁcient. However,if both
are estimated
-consistently, this procedure will lead to
-consistent estimate of
B. New Algorithm for CHFICA Using Prewhitening
Here is our prewhitened CHFICA estimator (PCFICA): First,
deﬁned by (10) using
is the same as
deﬁned in (3), except that
is now replaced by
is the set of
orthogonal matrices; second, let
CHEN AND BICKEL: CONSISTENT INDEPENDENT COMPONENT ANALYSIS AND PREWHITENING
and obtain an estimate of
. The key problem is the
optimization of (11). Algebraic expansion leads to
This formula was used by Kankainen in the context of
testing total independence. Evaluation of this contrast function requires
operations, which is computationally
impractical with large sample sizes. We provide an algorithm
to approximate this function by using the Gaussian kernel
and incomplete Cholesky decomposition, which makes the
computation feasible for reasonable sizes of
. Notice that
so that the ﬁrst term on the
right-hand side of (13) does not depend on
. We only need to
consider the second term and last term.
) to be an
matrix such that
This is a Gram matrix generated by a 1-D Gaussian kernel,
which is known to be non-negative deﬁnite. Let
sum of the
th column of
of all entries of
). Then, the contrast function consisting of
the second term and the third term on the right-hand side of (13)
To reduce computational complexity, we propose to approximate
using incomplete Cholesky (IC) decomposition , where
lower triangular matrix. Let
be the threshold value that controls the summation
of the remaining diagonals by IC. Permutation is necessary to
optimal such that tr
, for some
permutation matrix
, which is chosen automatically by the IC
algorithm. We used the IC algorithm described in and set
for simulations. Now, the complexity of evaluting the
approximate
. The partial derivative
can also be approximated in
operations by using the approximate Gram
is not less than the number of eigenvalues of
such that the remaining eigensum is controlled by , Wright 
showed that a non-negative deﬁnite matrix can be approximated
well by IC with the same order as that by spectral decomposition if its eigenvalues decrease quickly enough. See Bach and
Jordan for empirical studies, and, for instance, to Widom
 for theoretical results on the rate of decay of the eigenvalues of such Gram matrices. To further understand the magnitude of
needed by IC to achieve a given error , we generated
1000 random samples
from the mixture
Gaussian distribution
and obtained
Report the magnitude of h
(x-axis) chosen by incomplete Cholesky
decomposition with given approximation errors  (y-axis), where the Gram
matrix was generated by the 1-D Gaussian kernel and 1000 samples from
0:4N ( 1; 1) + 0:6N (1; 1).
1000 Gram matrix
is the sample standard
deviation of . Fig. 3 visualizes the IC simulation results.
Since applying the permutation matrix
is equivalent to permuting the order of
, which does not
change the value of
, for simplicity, we write
. It is noticeable that
is still non-negative deﬁnite.
the approximation error of
by replacing
bounded by
and, thus, is ignorable with
when at most one source has inﬁnite variance.
When all sources have heavy tails, we suggest
considering the convergence rate of the contrast function (13)
(see for such convergence rates).
Once we can evaluate the contrast function
and its partial derivative
, the minimization of
in the domain
of orthogonal matrices can be done efﬁciently by using the gradient algorithm described in . Here, the gradient of
and the geodesic starting from
in the gradient
is determined as
where the matrix exponential can be calculated efﬁciently by
diagonalization. It is noticeable that the contrast function is not
convex, mainly due to the identiﬁability ambiguity. Restarting
initial points is necessary to obtain the global minimizer of
, which is closest to the truth.
IV. STATISTICAL PROPERTIES OF PREWHITENED CHFICA
Since we are, in this paper, interested in
to estimate
is an estimator obtained by
any prewhitened ICA algorithm described in Section III-A. It is
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 10, OCTOBER 2005
clear that
is the set of all
possible values of
the acting parameter space
for the estimation of
using prewhitening.
, by classical theories,
surely. Then, we can approximate
. It can be shown
that for the prewhitened CHFICA algorithm, if all sources have
ﬁnite second moments, the properties claimed in Theorem 1
continue to hold. However, when
, i.e., some
sources have heavy tails,
diverges, and how
is approximated in
is unclear. One would think that if one or more
sources do not have ﬁnite second moments, then prewhitening
would cause a breakdown of these prewhitened ICA algorithms. To our surprise, simulations of Kernel ICA (KGV) 
and prewhitened CHFICA gave excellent results, even when
there exist heavy-tailed sources (for example, one is uniformly
distributed on , and the other is Cauchy distributed). This
is different from the super-efﬁciency phenomena for i.i.d.
heavy-tailed sources studied in . The following subsections
develop some statistical theory for this phenomenon.
A. Consistent Acting Space
In this section, we prove that with prewhitening, the acting
parameter space is consistent in the sense of Theorem 3, regardless of whether all hidden sources have ﬁnite second moments
Theorem 3: Under the identiﬁability conditions
Proof: Suppose that
has sample covariance matrix
is orthogonal, and
Hence, it is enough to prove that
This is completed by Theorem 5, which we have developed and
proven in the Appendix.
This theorem says that for all kinds of hidden sources, there
exists at least a sequence of points in the acting parameter space
with prewhitening, which converges to
in probability. This
result is independent of the particular ICA algorithm.
B. Consistent Prewhitened CHFICA
The previous subsection provides the possibility that some
prewhitened ICA algorithm may be able to obtain consistent
estimates of the unmixing matrix, even with the existence of
heavy-tailed sources. This begs the question of whether an implemented algorithm is consistent. Our goal in this subsection is
to study the prewhitened CHFICA method. Fig. 4 shows some
simulation results in the case of two sources: One has a uniform distribution on , and the other is Cauchy distributed.
To detect whether ICA algorithms can obtain consistent estimates in such a situation, the sample size was increased from I
). We compare PCFICA with three
other ICA algorithms: FastICA, JADE, and KGV. From Fig. 4,
we can see that as the sample size increases, the estimation error
measured by the Amari error for PCFICA and KGV decreases
toward zero more signiﬁcantly than that for FastICA and JADE.
However, the simulation also suggests that the convergence rate
of the PCFICA estimator is slower than
Consistency of different ICA algorithms with prewhitening when m =
2: One is uniform on , and the other is Cauchy: One hundred replications
were used to obtain the boxplots based on quartiles, where the sample sizes were
1000 for case I and 8000 for case II.
The main result of this study is given in the following
Theorem 4: Suppose that the identiﬁability conditions
hold in model (1). Let
be as usual. The estimator
deﬁned by [(11) and (12)] is consistent, i.e.,
, in either of the three cases.
All components of
have ﬁnite variances.
ii) All but one component of
have ﬁnite variances.
, and both components of
have heavy tails and
stable distributions.
We provide some heuristics for the above results. The crucial step of a prewhitened ICA algorithm is to separate, by Theorem 3, the whitened mixture
is a data-dependent orthogonal matrix deﬁned in the previous
subsection and, thus, is equivalent to separating
for which the identity matrix is a true unmixing matrix. By
Theorem 5 below,
is almost diagonal, and thus, by
prewhitening, we essentially separate mixtures of individually
rescaled hidden sources, which are still mutually independent,
despite their weak time dependence. The result for Case i) becomes obvious. In Case ii), the heavy-tailed rescaled source is
asymptotically zero; fortunately, the orthogonal unmixing structure makes sure that the one (almost zero) can be separated
well if the other
sources can be separated consistently.
Thus, in Cases i) and ii), the consistency results can also apply
to other prewhitened ICA algorithms, which can estimate
consistently without using prewhitening. The simulation shown
in Fig. 4 is for Case ii). Zibulevsky and Pearlmutter had
some different heuristics for Case ii) by considering the sparseness property of heavy-tailed hidden sources. Case iii) is more
sophisticated. See for the complete proof. As a special case
of iii), when both components have the same symmetric and
stable distribution, Shereshevski et al. showed that without
prewhitening, the unmixing matrix can be estimated super efﬁciently, for example, by a quasi maximum likelihood estimation
(MLE). Under such a situation, we conjecture that the corresponding estimates using prewhitening would also be super-ef-
ﬁcient, but we leave this for further analysis.
CHEN AND BICKEL: CONSISTENT INDEPENDENT COMPONENT ANALYSIS AND PREWHITENING
V. CONCLUSION
In this paper, we have analyzed the CHFICA method both
theoretically and numerically under the setup of classical ICA
models. First, the CHFICA estimate is consistent under minimal
identiﬁability conditions, whereas many well-known ICA algorithms such as FastICA and JADE, which are based on parametric methods and thus can be uniﬁed under the framework
of quasi MLE or equivalently by parametrizing hidden sources
with particular density families (see Lee et al. ), are shown
by Cardoso to be inconsistent when the hidden sources do
not belong to such families. Second, CHFICA is
-consistent,
asymptotically normal, and robust against small additive noise
under mild conditions. Third, the acting parameter space for
prewhitened ICA algorithms is shown to always be able to capture the true value of the unmixing matrix asymptotically, and in
particular, prewhitened CHFICA is consistent even in the presence of heavy-tailed sources. Numerically, although the computational complexity of CHFICA is
, we have proposed a
fast algorithm (PCFICA) to implement it by using prewhitening
and incomplete Cholesky decomposition. Simulation results of
PCFICA are in agreement with the above theoretical analysis.
be the sample covariance matrix of an
, which has mutually independent components. Denote
its diagonal elements by
, and deﬁne
Theorem 5: If none of
’s component is degenerate, then
Proof: Suppose the sample correlation matrix of
be the matrix
. It is clear that
From the following Proposition, we have
. By Mathias’s theorem , we have
be independent.
are i.i.d. copies of
them. Deﬁne the sample correlation coefﬁcient
, it is well known that
Proposition: Let
be independent. Suppose that
are i.i.d. realizations of them. Then,
Proof: When both
are ﬁnite second moment,
the result is well-known (see, for example, Bickel and Doksum
 ). If either is inﬁnite, the result follows from the following
Lemma 1 (Klass): If
by monotone
transformation.
Proof: It is enough to consider the case
Notice that
a.s.; then, there exists
a.s. By truncation
Lemma 2: If
are independent, and
This, together with Lemma 1, implies that if further
degenerated, then
Proof: It is enough to prove the result for non-negative
. As in the proof of Lemma 1, we have
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 10, OCTOBER 2005
where the second
in the last inequality is due to
ACKNOWLEDGMENT
The authors are very grateful to M. Klass for technical discussions and the proof of Lemma 1 and would like to thank four
referees and the associate editor for very helpful comments.