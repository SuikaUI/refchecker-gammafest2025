Multiple Bernoulli Relevance Models for Image and Video Annotation
S. L. Feng, R. Manmatha and V. Lavrenko ∗
Multimedia Indexing and Retrieval Group
Center for Intelligent Information Retrieval
University of Massachusetts
Amherst, MA, 01003
Retrieving images in response to textual queries requires
some knowledge of the semantics of the picture. Here, we
show how we can do both automatic image annotation and
retrieval (using one word queries) from images and videos
using a multiple Bernoulli relevance model. The model assumes that a training set of images or videos along with
keyword annotations is provided.
Multiple keywords are
provided for an image and the speciﬁc correspondence between a keyword and an image is not provided. Each image is partitioned into a set of rectangular regions and a
real-valued feature vector is computed over these regions.
The relevance model is a joint probability distribution of
the word annotations and the image feature vectors and is
computed using the training set. The word probabilities are
estimated using a multiple Bernoulli model and the image
feature probabilities using a non-parametric kernel density
The model is then used to annotate images in
a test set. We show experiments on both images from a
standard Corel data set and a set of video key frames from
NIST’s Video Trec. Comparative experiments show that the
model performs better than a model based on estimating
word probabilities using the popular multinomial distribution. The results also show that our model signiﬁcantly outperforms previously reported results on the task of image
and video annotation.
1. Introduction
Searching and ﬁnding large numbers of images and videos
from a database is a challenging problem. The conventional
approach to this problem is to search on image attributes
like color and texture. Such approaches suffer from a number of problems. They do not really capture the semantics
∗This work was supported in part by the Center for Intelligent Information Retrieval and in part by the National Science Foundation under grant
number IIS-9909073 and in part by SPAWARSYSCEN-SD grant number
N66001-02-1-8903. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily
reﬂect those of the sponsor.
of the problem well and they often require people to pose
image queries using color or texture which is difﬁcult for
most people to do. The traditional “low-tech” solution to
this problem practiced by librarians is to annotate each image manually with keywords or captions and then search on
those captions or keywords using a conventional text search
engine. The rationale here is that the keywords capture the
semantic content of the image and help in retrieving the images. This technique is also used by television news organizations to retrieve ﬁle footage from their videos. While
“low-tech”, such techniques allow text queries and are successful in ﬁnding the relevant pictures. The main disadvantage with manual annotations is the cost and difﬁculty of
scaling it to large numbers of images.
Automatically annotating images/videos would solve
this problem while still retaining the advantages of a semantic search. Here, we propose approaches to automatically annotating and retrieving images/videos by learning a
statistical generative model called a relevance model using a
set of annotated training images. The images are partitioned
into rectangles and features are computed over these rectangles. We then learn a joint probability model for (continuous) image features and words called a relevance model and
use this model to annotate test images which we have not
seen. Words are modeled using a multiple Bernoulli process and images modeled using a kernel density estimate.
We test this model using a Corel dataset provided by 
and show that it outperforms previously reported results on
other models. It performs 4 times better than a model based
on machine translation and better than one which models
word probabilities using a multinomial to represent words.
Existing annotation models by analogy with the
text retrieval world have used the multinomial distribution
to model annotation words. We believe that annotation text
has very different characteristics than full text in documents
and hence a Bernoulli distribution is more appropriate.
In image/video annotation, a multinomial would split the
probability mass between multiple words. For example, if
an image was annotated with “person, grass”, with perfect
annotation the probability for each word would be equal to
0.5. On the other hand another image which has just one
annotation “person” would have a probability of 1.0 with
perfect annotation. If we want to ﬁnd images of people,
when rank ordering these images by probability the second
image would be preferred to the ﬁrst although there is no
reason for preferring one image over another. The problem
can be made much worse when the annotation lengths for
different images differ substantially. A similar effect occurs
when annotations are hierarchical. For example, let one image be annotated “face, male face, Bill Clinton” and a second image be annotated with just “face”. The probability
mass would be split three ways (0.33 each) in the ﬁrst case
while in the second image “face” would have a probability
of 1. Again the second image would be preferred for the
query “face”, although there is no reason for preferring one
over the other. The Bernoulli model avoids this problem
by making decisions about each annotation independent of
the other words. Thus, in all the above examples, each of
the words would have a probability of 1 (assuming perfect
annotation).
It has been argued that the Corel dataset is much
easier to annotate and retrieve and does not really capture
the difﬁculties inherent in more challenging (real) datasets
like the news videos in Trec Video We therefore, experimented with a subset of news videos (ABC, CNN) from the
Trec Video dataset. We show that in fact we obtain comparable or even better performance (depending on the task) on
this dataset and that again the Bernoulli model outperforms
a multinomial model.
The speciﬁc contributions of this work include:
1. A probabilistic generative model which uses a
Bernoulli process to generate words and kernel density estimate to generate image features. This model
simultaneously learns the joint probabilities of associating words with image features using a training set
of images with keywords and then generates multiple
probabilistic annotations for each image.
2. Signiﬁcant improvements in annotation performance
over a number of other models on both a standard
Corel dataset and a real word news video dataset.
3. Large improvements in annotation performance by using a rectangular grid instead of regions obtained using
a segmentation algorithm (see for a related result).
4. Substantial improvements in retrieval performance on
one word queries over a multinomial model.
The focus of this paper is on models and not on features.
We use features similar to those used in 
The rest of this paper is organized as follows. We ﬁrst
discuss the multiple Bernoulli relevance model and its relation to the multinomial relevance model. This is followed
by a discussion of related work in this area. The next section describes the datasets and the results obtained. Finally,
we conclude the paper.
Multiple-Bernoulli Relevance Model
In this section we describe a statistical model for automatic annotation of images and video frames. Our model is
called Multiple-Bernoulli Relevance Model (MBRM) and
is based on the Continuous-space Relevance Model (CRM)
proposed by . CRM has proved to be very successful
on the tasks of automatic image annotation and retrieval.
In the rest of this section we discuss two shortcomings of
the CRM in the video domain and propose a possible way
of addressing these shortcomings. We then provide a formal description of our model as a generative process and
complete the section with a brief discussion of estimation
Relation of MBRM and CRM
CRM is a probabilistic model for image annotation and
retrieval. The basic idea behind CRM is to reduce an image
to a set of real-valued feature vectors, and then model the
joint probability of observing feature vectors with possible
annotation words. The feature vectors in are based on
automatic segmentation of the target image into regions
and are modeled using a kernel-based probability density
function. The annotation words are modeled with a multinomial distribution. The joint distribution in of words
and feature vectors relies on a doubly non-parametric approach, where expectations are computed over each annotated image in the training set.
We believe the CRM model makes two assumptions that
make it ill-suited for annotations in the image/video domain.
1. Segmentation: The CRM relies on automatic segmentation of the image into semantically-coherent regions. While the CRM does not make any assumptions
about correspondence of annotation words to image regions, the overall annotation performance is strongly
affected by the quality of segmentation. In addition,
automatic segmentation is a rather expensive process
that is poorly suited for large-scale video datasets.
2. Multinomial: CRM assumes that annotation words
for any given image follow a multinomial distribution. This is a reasonable assumption in the Corel 
dataset, where all annotations are approximately equal
in length and words reﬂect prominence of objects in the
image. However, in our video dataset individual
frames have hierarchical annotations which do not follow the multinomial distribution. The length of the annotations also varies widely for different video frames.
Furthermore, video annotations focus on presence of
an object in a frame, rather than its prominence.
In the next two subsections we show how we can improve
results by modifying these assumptions.
Rectangular image regions
In the current model, rather than attempting segmentation,
we impose a ﬁxed-size rectangular grid on each image. The
image is then represented as a set of tiles. Using a grid
provides a number of advantages. First, there is a very signiﬁcant reduction in the computational time required for the
model. Second, each image now contains a ﬁxed number of
regions, which simpliﬁes parameter estimation. Finally, using a grid makes it somewhat easier to incorporate context
into the model. For example, relative position could greatly
aid in distinguishing adjacent tiles of water and sky. To evaluate the effect of using rectangular regions versus segmentation, we ran experiments with the CRM model but with
rectangular regions as input - we call this CRM-Rectangles.
The experiments in Section 4 show that this alone improves
the mean per-word precision by about 38% - a substantial
improvement in performance. We believe this is because
segmentation is done on a per image basis. The CRM model
cannot undo any problems that occur with segmentation.
However, using a rectangular grid (with more regions than
produced by the segmentation) allows the model to learn
using a much larger set of training images what the correct
association of words and image regions should be.
Multiple-Bernoulli word model
Another major contribution of the current model over the
CRM is in our use of the multiple-Bernoulli distribution
for modeling image annotations. In this section we highlight the differences between the multiple-Bernoulli and
the multinomial model, and articulate why we believe that
multiple-Bernoulli is a better alternative.
The multinomial model is meant to reﬂect the prominence of words in a given annotation. The event space of
the model is the set of all strings over a given vocabulary,
and consequently words can appear multiple times in the annotation. In addition, the probability mass is shared by all
words in the vocabulary, and during the estimation process
the words compete for this probability mass. As a result,
an image I1 annotated with a single word “face” will assign all probability mass to that word, so P(face|I1) = 1.
At the same time, an image I2 annotated with two words
“face” and “person” will split the probability mass, so
texture,shape,
color, ...
Figure 1: MBRM viewed as a generative process. The annotation w is a binary vector sampled from the underlying
multiple-Bernoulli model. The image is produced by ﬁrst
sampling a set of feature vectors {g1. . .gn}, and then generating image regions {r1. . .rn} from the feature vectors.
Resulting regions are tiled to form the image.
P(face|I2) =
2. Thus the multinomial distribution models prominence of a word in the annotation, favoring single
words, or words that occur multiple times in an annotation.
Arguably, both images I1 and I2 contain a face, so the
probability of “face” should be equal. This can be modeled by a multiple-Bernoulli model, which explicitly focuses on presence or absence of words in the annotation,
rather than on their prominence. The event space of the
multiple-Bernoulli model is the set of all subsets of a given
vocabulary.
Each subset can be represented as a binary
occurrence vector in {0, 1}V .
Individual components of
the vector are assumed to be independent and identically
(Bernoulli-) distributed given the particular image.
In our dataset, image annotations are hierarchical and
have greatly varying length. No word is ever used more than
once in any given annotation, so modeling word frequency
is pointless. Finally, words are assigned to the annotation
based on merely the presence of an object in a frame, not
on its prominence. We believe that a Bernoulli model provides a much closer match for this environment. Our hypothesis is supported by experimental results which will be
discussed in section 4.
MBRM as a generative model.
Let V denote the annotation vocabulary, T denote the training set of annotated images, and let J be an element of T .
According to the previous section J is represented as a set of
image regions rJ = {r1. . .rn} along with the corresponding annotation wJ ∈{0, 1}V. We assume that the process
that generated J is based on two distinct probability distributions. First, we assume that the set of annotation words
wJ is a result of |V| independent samples from every component of some underlying multiple-Bernoulli distribution
PV(·|J). Second, for each image region r we sample a realvalued feature vector g of dimension k. The feature vector is
sampled from some underlying multi-variate density func-
tion PG(·|J). Finally, the rectangular region r is produced
according to some unknown distribution conditioned on g.
We make no attempt to model the process of generating r
from g. The resulting regions r1. . .rn are tiled to form the
Now let rA = {g1. . .gnA} denote the feature vectors of
some image A, which is not in the training set T . Similarly, let wB be some arbitrary subset of V. We would like
to model P(rA, wB), the joint probability of observing an
image deﬁned by rA together with annotation words wB.
We hypothesize that the observation {rA, wB} came from
the same process that generated one of the images J ∗in
the training set T . However, we don’t know which process
that was, and so we compute an expectation over all images
J∈T . The overall process for jointly generating wB and
rA is as follows:
1. Pick a training image J∈T with probability PT (J)
2. Sample wB from a multiple-Bernoulli model PV(·|J).
3. For a = 1 . . . nA:
(a) Sample a generator vector ga from the probability density PG(·|J).
Figure 1 shows a graphical dependency diagram for the
generative process outlined above. We show the process of
generating a simple image consisting of three regions and a
corresponding 3-word annotation. Note that the number of
words in the annotation nB does not have to be the same as
the number of image regions nA. Formally, the probability
of a joint observation {rA, wB} is given by:
P(rA, wB) =
(1 −PV(v|J))
Equation (1) makes it evident how we can use MBRM
for annotating new images or video frames. Given a new
(un-annotated) image we can split it into regions rA, compute feature vectors g1. . .gn for each region and then use
equation 1 to determine what subset of vocabulary w∗is
most likely to co-occur with the set of feature vectors:
In practice we only consider subsets of a ﬁxed size (5
words). One can show that the maximization in equation (2)
can be done very efﬁciently because of the factored nature
of the Bernoulli component. Essentially it can be shown that
the equations may be simpliﬁed so that P(wi|J) may be
computed independently for each word. This simpliﬁcation
arises because each word occurs at most once as the caption
of an image. Space constraints preclude us from providing
the proof.
Estimating Parameters of the Model
In this section we will discuss simple but effective estimation techniques for the three components of the model: PT ,
PV and PG. PT (J) is the probability of selecting the underlying model of image J to generate some new observation
r, w. In the absence of any task knowledge we use a uniform prior PT (J) = 1/NT , where NT is the size of the
training set.
PG(·|J) is a density function responsible for generating
the feature vectors g1. . .gn, which are later mapped to image regions rJ according to PR. We use a non-parametric
kernel-based density estimate for the distribution PG. Assuming gJ = {g1. . .gn} to be the set of regions of image J
we estimate:
PG(g|J) = 1
−(g −gi))⊤Σ−1(g −gi))
Equation (3) arises out of placing a Gaussian kernel over
the feature vector gi of every region of image J. Each kernel
is parametrized by the feature covariance matrix Σ. As a
matter of convenience we assumed Σ = β·I, where I is
the identity matrix. β plays the role of kernel bandwidth: it
determines the smoothness of PG around the support point
gi. The value of β is selected empirically on a held-out
portion of the training set T .
PV(v|J) is the v’th component of the multiple-Bernoulli
distribution that is assumed to have generated the annotation
wJ of image J∈T . The Bayes estimate using a beta prior
(conjugate to a Bernoulli) for each word is given by:
PV(v|J) = µ δv,J + Nv
here µ is a smoothing parameter estimated using the
training and validation set, δv,J = 1 if the word v occurs
in the annotation of image J and zero otherwise. Nv is the
number of training images that contain v in the annotation
and N is the total number of training images.
Related Work
Our model differs from traditional object recognition approaches in a number of ways (for example . Such approaches require a separate model to be trained
for each object to be recognized That is, even though the
form of the statistical model may be the same, learning two
different objects like a car and a person requires two separate training runs (one for each object). Each training run
requires positive and negative examples for that particular
object. On the other hand, in the relevance model approach
described here all the annotation words are learned at the
same time - each training image usually has many annotations. While some of the newer object recognition techniques do not require training examples of the objects to
be cut out of the background, they still seem to require one
object in each image. Our model on the other hand can handle multiple objects in the same training image and can also
ascribe annotations to the backgrounds like sky and grass.
Unlike the more traditional object recognition techniques
we label the entire picture and not speciﬁc image regions in
a picture. This is as a librarian’s manual annotation shows
more than sufﬁcient for tasks like retrieving images from
a large database. The joint probability model that we propose takes context into account i.e. from training images it
learns that an elephant is more likely to be associated with
grass and sky and less likely to be associated with buildings
and hence if there are image regions associated with grass,
this increases the probability of recognizing the object as an
elephant. Traditional object recognition models do not do
The model described here is closest in spirit to the annotation models proposed by . Duygulu et
al proposed to describe images using a vocabulary of
blobs. First, regions are created using a segmentation algorithm like normalized cuts. For each region, features are
computed and then blobs are generated by clustering the
image features for these regions across images. Each image is generated by using a certain number of these blobs.
Their Translation Model applies one of the classical statistical machine translation models to translate from the set of
keywords of an image to the set of blobs forming the image.
On the surface, MBRM appears to be similar to one of
the intermediate models considered by Blei and Jordan .
Speciﬁcally, their GM-mixture model employs a similar dependence structure among the random variables involved.
However, the topological structure of MBRM is quite different from the one employed by . GM-mixture assumes
a low-dimensional topology, leading to a fully-parametric
model where 200 or so “latent aspects” are estimated using the EM algorithm. To contrast that, MBRM makes no
assumptions about the topological structure, and leads to
a doubly non-parametric approach, where expectations are
computed over every individual point in the training set.
In addition they model words using a multinomial process.
Blei and Jordan used a different subset of the Corel dataset
and hence it is difﬁcult to make a direct quantitative comparison with their models.
MBRM is also related to the cross-media relevance
model (CMRM) , which is also doubly non-parametric.
There are three signiﬁcant differences between MBRM and
CMRM. First, CMRM is a discrete model and cannot take
advantage of continuous features. In order to use CMRM
for image annotation we have to quantize continuous feature
vectors into a discrete vocabulary (similarly to the translation models).
MBRM, on the other hand, directly
models continuous features. The second difference is that
CMRM relies on clustering of the feature vectors into blobs.
Annotation quality of the CMRM is very sensitive to clustering errors, and depends on being able to a-priori select
the right cluster granularity: too many clusters will result
in extreme sparseness of the space, while too few will lead
us to confuse different objects in the images. MBRM does
not rely on clustering and consequently does not suffer from
the granularity issues. Finally, CMRM also models words
using a multinomial process.
We would like to stress that the difference between
MBRM and previously discussed models is not merely conceptual. In section 4 we will show that MBRM performs
signiﬁcantly better than all previously proposed models on
the tasks of image annotation and retrieval. To ensure a fair
comparison, we show results on exactly the same data set
and similar feature representations as used in .
4. Experimental Results
We tested the algorithms using two different datasets, the
Corel data set from Duygulu et al and a set of video key
frames from NIST’s Video Trec . To provide a meaningful comparison between MBRM and CRM-Rectangles,
we do comparative experiments using the same set of features extracted from the same set of rectangular grids. For
the Corel dataset we also compare the results with those of
Duygulu et al and the CRM model.
4.1. Datasets and Feature sets
The Corel data set consists of 5000 images from 50 Corel
Stock Photo cds.
1 Each cd includes 100 images on the
same topic, and each image is also associated with 1-5 keywords. Overall there are 371 keywords in the dataset. In
experiments, we divided this dataset into 3 parts: a training
set of 4000 images, a validation set of 500 images and a test
set of 500 images. The validation set is used to ﬁnd model
parameters. After ﬁnding the parameters, we merged the
4000 training set and 500 validation set to form a new training set. This corresponds to the training set of 4500 images
and the test set of 500 images used by Duygulu et al .
We used a subset of NIST’s Video Trec dataset (for computational reasons we did not use the entire data set). The
1We thank Kobus Barnard for making the Corel dataset available at
 2002
Translation
CRM-Rectangles
#words with recall ≥0
Results on 49 best words, as in 
Mean Per-word Recall
Mean Per-word Precision
Results on all 260 words
Mean Per-word Recall
Mean Per-word Precision
Table 1: Performance comparison on the task of automatic image annotation on the Corel dataset. CRM and CRM-Rectangles
are essentially the same model but the former uses regions produced by a segmentation algorithm while the latter uses a grid.
Note that using a grid improves performance. MBRM performs best beating even CRM-Rectangles by a small amount.
data set consists of 12 mpeg ﬁles, each of which is a 30minutes video section of CNN or ABC news and advertisements. 5200 key frames were extracted and provided by
NIST for this dataset. The participants in TREC annotated
a portion of the videos. The word vocabulary for human annotation is represented as a hierarchical tree with each annotation word as a node, which means many key frames are
annotated hierarchically, e.g. a key frame can be assigned
a set of words like ”face, male face, male news subject”.
This means that the annotation length for key frames can
vary widely. There are 137 keywords in the whole dataset
after we ignore all the audio annotations.
We randomly
divide the dataset into a training set (1735 key frames), a
validation set (1735 key frames) and a test set (1730 key
frames). As for the Corel set, the validation set is used to
ﬁnd system parameters, and then merged into the training
set after we ﬁnd the parameters.
Every image in these two sets is partitioned into rectangular grids, and a feature vector is then calculated for every
grid region. The number of rectangles is empirically selected (using the training and validation sets) and is 24 for
the Corel set, and 35 for the video dataset set. There are 30
features: 18 color features (including region color average,
standard deviation and skewness) and 12 texture features
(Gabor energy computed over 3 scales and 4 orientations).
4.2. Results of Automatic Image Annotation
In this section we evaluate the performance of our MBRM
on automatic image annotation. Given an un-annotated image or key frame, we can calculate the generative probability of every candidate word in the vocabulary conditioned
on it. For the Corel set, we take the top 5 words (according to probability) as automatic annotation of that image.
For the video set, we take the top 6 (the average length of
human annotations over all key frames) words. Figure 2
shows examples of the automatic annotations obtained using the CRM-Rectangles and MBRM models on the TREC
Video. These results are obtained on the same dataset with
identical preprocessing, features and training sets.
The ﬁrst evaluation on annotation is done as in 
using recall and precision calculated for every word in the
test set. For this part of the process we do not use the actual rankings. Let A be the number of images automatically annotated with a given word, B the number of images correctly annotated with that word. C is the number of
images having that word in ground-truth annotation. Then
recall = B
C , and precision = B
A. To evaluate the system
performance, recall and precision values are averaged over
the testing words. The ﬁrst set of results are shown for the
Corel dataset in Table 1. Results are reported for all (260)
words in the test set. They are also reported for the top 49
annotations to make a direct comparison with . The three
relevance model approaches are clearly much better than
the translation model approach in with MBRM outperforming all other models (4 times better than the translation
model). CRM-Rectangles and CRM are identical except
for the fact that CRM-Rectangles uses regions partitioned
into rectangles while the regions in the CRM model are obtained using normalized cuts segmentation. As the results
show this improves the performance signiﬁcantly (almost
38% improvement in precision). Segmentation is a difﬁcult
error prone process in computer vision. The segmentation is
done on a per image basis and hence there is some chance of
combining semantically distinct regions together. Since the
probabilistic model deals with regions as entities, it cannot
undo segmentation errors (if for example two distinct image
regions are combined together in the segmentation). However, if we start from a rectangular partition, the probabilistic model which learns from multiple training images has a
better chance of associating the rectangular regions with the
correct words. We believe that this accounts for the better
performance using a rectangular partition.
Table 2 compares the annotation performance of CRM-
Rectangles and MBRM and we see that the Bernoulli model
is slightly better than the other model at annotation.
Ranked Retrieval with Single Word
The annotation results reported above ignore rank order.
That is, imagine that one wanted to ﬁnd all car images. One
CRM-Rectangles
food outdoors
face male face
outdoors sky
nonstudio setting
graphics and text
transportation
text overlay
graphics and text
news subject monologue
water body
sport event
text overlay
male news person
basketball
physical violence
nonstudio setting
graphics and text
face indoors
outdoors sky
nonstudio setting
graphics and text
text overlay
studio setting
transportation
text overlay
nonstudio setting
news subject monologue
water body
sport event
people event
female face
basketball
physical violence
face,male face
female news person
Figure 2: Top automatic annotations produced by the CRM-Rectangles and MBRM models. MBRM performs better than
CRM-Rectangles for the ﬁrst two images. For the last three images, the annotations are identical. Note that for many video
frames are annotated with the words graphics and text and text overlay because of the station logos - difﬁcult to see in these
CRM-Rectangles
#words with recall ≥0
Results on all 110 words.
Mean Per-word Recal
Mean Per-word Precision
Results on all words with recall ≥0
Mean Per-word Recall
Mean Per-word Precision
Table 2: Performance on the automatic annotation task on
the Trec Video dataset. MBRM performs better than CRM-
Rectangles.
would ideally like to rank these according to the probability of annotation and hope that the top ranked ones are all
cars. In fact, in large databases most users are not likely to
even want to see more than 10 or 20 images in response
to a query.
Rank order is, therefore, very important for
such applications. Figures 3-6 show the performance of
CRM-Rectangles and MBRM in response to one word text
queries. Although the annotation performance of the two
models does not seem to be that different, the results show
that the retrieval performance can be very different. To evaluate rank order, one can look at the performance on ranked
retrieval in response to one word queries. Given a query
word, the system will return all the images which are automatically annotated with that word, ranked according to the
probabilities of that word generated by these images. We
use a metric called mean average precision to evaluate the
retrieval performances. Average precision is the average of
precision values at the ranks where relevant (here ’relevant’
means that the ground-truth annotation of this image contains the query word) items occurs, which is further averaged over all queries to give mean average precision. Table
Figure 3: First 4 ranked results for the query “car” in the
Corel collection using a) CRM-Rectangles and b) MBRM.
3 shows that for ranked retrieval the Bernoulli model substantially outperforms (by 15% for the Corel dataset and by
16% for the Trec Video dataset) the multinomial model.
Figure 4: First 4 ranked results for the query “tiger” in the
Corel collection using a) CRM-Rectangles and b) MBRM.
Figure 5: First 4 ranked results for the query “Bill Clinton”
in the Trec Video collection using a) CRM-Rectangles and
b) MBRM. Note the ﬁrst picture shows Bill Clinton with
Toni Blair.
Figure 6: First 4 ranked results for the query “car” in
the Trec Video collection using a) CRM-Rectangles and b)
5. Summary and Conclusions
We have proposed a multiple-Bernoulli relevance model for
image annotation, to formulate the process of a human annotating images. The results show that it outperforms, especially on the ranked retrieval task, the (multinomial) continuous relevance model and other models on both the Corel
dataset and a more realistic Trec Video dataset. Future work
will include a more extensive retrieval task with this model,
which allows for longer text strings. Other extensions may
include larger datasets, better features and more sophisticated models.