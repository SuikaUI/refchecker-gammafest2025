Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3132–3142
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Few-Shot and Zero-Shot Multi-Label Learning
for Structured Label Spaces
Anthony Rios
Department of Computer Science
University of Kentucky
Lexington, KY
 
Ramakanth Kavuluru
Division of Biomedical Informatics
University of Kentucky
Lexington, KY
 
Large multi-label datasets contain labels that
occur thousands of times (frequent group),
those that occur only a few times (few-shot
group), and labels that never appear in the
training dataset (zero-shot group). Multi-label
few- and zero-shot label prediction is mostly
unexplored on datasets with large label spaces,
especially for text classiﬁcation. In this paper, we perform a ﬁne-grained evaluation to
understand how state-of-the-art methods perform on infrequent labels. Furthermore, we
develop few- and zero-shot methods for multilabel text classiﬁcation when there is a known
structure over the label space, and evaluate
them on two publicly available medical text
datasets: MIMIC II and MIMIC III. For fewshot labels we achieve improvements of 6.2%
and 4.8% in R@10 for MIMIC II and MIMIC
III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot
labels are 17.3% and 19%.
Introduction
Unlike in binary or multi-class problems, for
multi-label classiﬁcation a model assigns a set of
labels to each input instance . Large-scale multi-label text classiﬁcation
problems can be found in several domains. For
example, Wikipedia articles are annotated with
labels used to organize documents and facilitate
search .
Biomedical articles indexed by the PubMed search engine are
manually annotated with medical subject headings . In healthcare facilities, medical records are assigned a set of
standardized codes for billing purposes .
Sorted Label ID
Label Frequency
More than 50%
of all ICD-9 labels
never appear
in the MIMIC III
~5000 labels
Figure 1: This plot shows the label frequency distribution of ICD-9 codes in MIMIC III.
There are two major difﬁculties when developing machine learning methods for large-scale
multi-label text classiﬁcation problems. First, the
documents may be long, sometimes containing
more than a thousand words . Finding the relevant information in a large
document for a speciﬁc label results in needle in a
haystack situation. Second, data sparsity is a common problem; as the total number of labels grows,
a few labels may occur frequently, but most labels
will occur infrequently. Rubin et al. refer to
datasets that have long-tail frequency distributions
as “power-law datasets”. Methods that predict infrequent labels fall under the paradigm of few-shot
classiﬁcation which refers to supervised methods
in which only a few examples, typically between 1
and 5, are available in the training dataset for each
label. With predeﬁned label spaces, some labels
may never appear in the training dataset. Zeroshot problems extend the idea of few-shot classi-
ﬁcation by assuming no training data is available
for the labels we wish to predict at test time. In
this paper, we explore both of these issues, long
documents and power-law datasets, with an emphasis on analyzing the few- and zero-shot aspects
of large-scale multi-label problems.
In Figure 1, we plot the label frequency distribution of diagnosis and procedure labels for the
entire MIMIC III dataset. A
few labels occur more than 10,000 times, around
5,000 labels occur between 1 and 10 times, and of
the 17,000 diagnosis and procedure labels, more
than 50% never occur. There are a few reasons
a label may never occur in the training dataset.
In healthcare, sevearl disorders are rare; therefore corresponding labels may not have been observed yet in a particular clinic. Sometimes new
labels may be introduced as the ﬁeld evolves leading to an emerging label problem. This is intuitive for applications such as hashtag prediction on
Twitter. For example, last year it would not have
made sense to annotate tweets with the hashtag
#EMNLP2018. Yet, as this year’s conference approaches, labeling tweets with the #EMNLP2018
will help users ﬁnd relevant information.
Infrequent labels may not contribute heavily to
the overall accuracy of a multi-label model, but
in some cases, correct prediction of such labels
is crucial but not straightforward. For example,
in assigning diagnosis labels to EMRs, it is important that trained human coders are both accurate and thorough. Errors may cause unfair ﬁnancial burden on the patient. Coders may have an
easier time assigning frequent labels to EMRs because they are encountered more often. Also, frequent labels are generally easier to predict using
machine-learning based methods. However, infrequent or obscure labels will be easily confused or
missed causing billing mistakes and/or causing the
coders to spend more time annotating each record.
Thus, we believe methods that handle infrequent
and unseen labels in the multi-label setting are important.
Current evaluation methods for large-scale
multi-label classiﬁcation mostly ignore infrequent
and unseen labels. Popular evaluation measures
focus on metrics such as micro-F1, recall at
k (R@k), precision at k (P@k), and macro-F1. As
it is well-known that micro-F1 gives more weight
to frequent labels, papers on this topic also report
macro-F1, the average of label-wise F1 scores,
which equally weights all labels. Unfortunately,
macro-F1 scores are generally low and the corresponding performance differences between methods are small.
Moreover, it is possible to improve macro-F1 by only improving a model’s performance on frequent labels, further confounding
its interpretation. Hence we posit that macro-F1
is not enough to compare large-scale multi-label
learning methods on infrequent labels and it does
not directly evaluate zero-shot labels. Here, we
take a step back and ask: can the model predict
the correct few-shot (zero-shot) labels from the
set of all few-shot (zero-shot) labels? To address
this, we test our approach by adapting the generalized zero-shot classiﬁcation evaluation methodology by Xian et al. to the multi-label setting.
In this paper, we propose and evaluate a neural
architecture suitable for handling few- and zeroshot labels in the multi-label setting where the output label space satisﬁes two constraints: (1). the
labels are connected forming a DAG and (2). each
label has a brief natural language descriptor. These
assumptions hold in several multi-label scenarios including assigning diagnoses/procedures to
EMRs, indexing biomedical articles with medical
subject headings, and patent classiﬁcation. Taking advantage of this prior knowledge on labels is
vital for zero-shot prediction. Speciﬁcally, using
the EMR coding use-case, we make the following
contributions:
1. We overcome issues arising from processing
long documents by introducing a new neural
architecture that expands on recent attentionbased CNNs ). Our model learns to predict few- and
zero-shot labels by matching discharge summaries in EMRs to feature vectors for each
label obtained by exploiting structured label
spaces with graph CNNs ).
2. We provide a ﬁne-grained evaluation of stateof-the-art EMR coding methods for frequent,
few-shot, and zero-shot labels. By evaluating
power-law datasets using an extended generalized zero-shot methodology that also includes few-shot labels, we present a nuanced
analysis of model performance on infrequent
Related Work
Large-Scale Text Classiﬁcation.
Linear methods have been successfully applied to large-scale
problems . For traditional
micro- and macro-F1 measures, Tang et al. 
show that linear methods suffer using naive thresh-
olding strategies because infrequent labels generally need a smaller threshold. Generative models
have also been promising for datasets with many
labels .
Intuitively, by using a prior distribution over the label space, infrequent labels can be modeled better.
large-scale classiﬁcation is also pursued as “extreme classiﬁcation” where the focus is on ranking measures that
ignore infrequent labels. Neural networks (NNs)
perform well for many small-scale classiﬁcation
tasks . Recently, researchers have been exploring NN methods for large-scale problems. Yang et al. 
develop a hierarchical attentive NN for datasets
with over a million documents, but their datasets
contain few labels. Nam et al. show that
feed-forward NNs can be successfully applied to
large-scale problems through the use of a multilabel binary cross-entropy loss function. Vani et al.
 introduce a grounded recurrent neural network (RNN) that iteratively updates its predictions
as it processes a document word-by-word. Baumel
et al. experiment with both CNNs and
RNNs for medical coding. Finally, Mullenbach
et al. expand on prior ACNNs to develop a labelwise attention framework where the most informative ngrams are extracted for each label in the
Our attention mechanism extends their
work to the zero-shot setting.
neural networks are generally considered to need
large datasets, they have been shown to work well
on few-shot classiﬁcation tasks.
To handle infrequent labels, most NN methods use a k-NNlike approach. Siamese NNs 
learn a nonlinear distance metric using a pairwise loss function. Matching networks introduce an instance-level attention
method to ﬁnd relevant neighbors. Prototypical
Networks average all instances
in each class to form “prototype label vectors”
and train using a traditional cross-entropy loss.
In our prior work , we
combine matching networks with a sophisticated
thresholding strategy. However, in Rios and Kavuluru we did not explore the few- and zeroshot settings.
Zero-shot learning has not been widely explored in the large-scale multi-label classiﬁcation
scenario. Like neural few-shot methods, neural
zero-shot methods use a matching framework. Instead of matching input instances with other instances, they are matched to predeﬁned label vectors.
For example, the Attributes and Animals
Dataset contains images of animals and the label vectors consist of features describing the types of animals (e.g., stripes: yes).
When feature vectors for labels are not available,
the average of the pretrained word embeddings of
the class names have been used. The attribute label embedding method uses a
pairwise ranking loss to match zero-shot label vectors to instances. Romera-Paredes and Torr 
introduced the “embarrassingly simple zero-shot
learning” (ESZSL) method which is trained using a mean squared error loss. A few zero-shot
methods do not translate well to multi-label problems.
CONSE averages
the embeddings for the top predicted supervised
label vectors to match to zero-shot label vectors.
CONSE assumes that both supervised and zeroshot labels cannot be assigned to the same instance.
In this paper, we expand on the generalized zero-shot evaluation methodology introduced by Xian et al. to large-scale multilabel classiﬁcation. Finally, it is important to note
that zero-shot classiﬁcation has been previously
studied in the multi-label setting . However, they focus on image classiﬁcation and use datasets with around 300 labels.
Graph Convolutional Neural Networks.
generalize
spaces. Defferrard et al. developed spectral methods to perform efﬁcient graph convolutions. Kipf and Welling assume a graph
structure is known over input instances and apply GCNNs for semi-supervised learning. GCNNs
are applied to relational data (e.g., link prediction)
by Schlichtkrull et al. . GCNNs have also
had success in other NLP tasks such as semantic
role labeling , dependency parsing ,
and machine translation .
There are three GCNN papers that share similarities with our work. (i) Peng et al. use a
GCNN on a word co-occurrence graph for large-scale text classiﬁcation where the GCNN operates
on documents/words, while our GCNN operates
on the labels.
(ii) Chen et al. use GC-
NNs on structured label spaces. However, their
overweight
• 690-698.99 - OTHER INFLAMMATORY CONDIT...
• 690 - Erythematosquamous dermatosis
• 690.1 - Seborrheic dermatitis
• 690.10 - Seborrheic dermatitis, unspecified
• 690.11 - Seborrhea capitis
All ICD-9 Descriptors
Predictions
Label-wise
Convolution
2-Layer GCNN
Figure 2: This ﬁgure provides a visual overview of our method. Intuitively, our method has two main components.
The ﬁrst component is a CNN that operates operates on the EMRs. The other component is a 2-layer GCNN which
creates the label-speciﬁc attention vectors and label-vectors used for ranking using ICD-9 descriptions as input.
experiments focus on smaller label spaces and do
not handle/assess zero-shot and few-shot labels.
Also, their experiments for text classiﬁcation do
not incorporate attention and simply use an average of word vectors to represent each document.
(iii) Wang et al. propose a zero-shot GCNN
image classiﬁcation method for structured multi-
-class problems.
We believe their method may
transfer to the multi-label text classiﬁcation setting
but exact modiﬁcations to affect that are not clear
(i.e., their semi-supervised approach may not be
directly applicable). Likewise, porting to text is
nontrivial for long documents.
Figure 2 shows the overall schematic of our architecture. Intuitively, we incorporate four main components. First, we assume we have the full English
descriptor/gloss for each label we want to predict.
We form a vector representation for each label by
averaging the word embeddings for each word in
its descriptor. Second, the label vectors formed
from the descriptor are used as attention vectors
(label-wise attention) to ﬁnd the most informative
ngrams in the document for each label. For each
label, this will produce a separate vector representation of the input document. Third, the label
vectors are passed through a two layer GCNN to
incorporate hierarchical information about the label space. Finally, the vectors returned from the
GCNN are matched to the document vectors to
generate predictions.
Convolutional Neural Network.
Contrary to
prior CNN methods for text , instead
of using a max-over-time pooling layer, we learn
to ﬁnd relevant ngrams in a document for each
label via label-wise attention . The CNN will return a document feature
matrix D ∈R(n−s+1)×u where each column of D
is a feature map, u is the total number of convolution ﬁlters, n is the number of words in the document, and s is the width of convolution ﬁlters.
Label Vectors.
To be able to predict labels that
were not in the training dataset, we avoid learning label speciﬁc parameters.
We use the label
descriptors to generate a feature vector for each label. First, to preprocess each descriptor, we lowercase all words and remove stop-words. Next, each
label vector is formed by averaging the remaining
words in the descriptor
i = 1, . . . , L,
where vi ∈Rd, L is the number of labels, and N
is the index set of the words in the descriptor. Prior
zero-shot work has focused on projecting input instances into the same semantic space as the label
vectors . For zero-shot
image classiﬁcation, this is a non-trivial task. Because we work with textual data, we simply share
the word embeddings between the convolutional
layer and the label vector creation step to form vi.
Label-Wise Attention.
Similar to the work
by Mullenbach et al. , we employ label-wise
attention to avoid the needle in the haystack situation encountered with long documents. The issue
with simply using a single attention vector or using
max-pooling is that we assume a single vector can
capture everything required to predict every label.
For example, with a single attention, we would
only look at one spot in the document and assume
that spot contains the relevant information needed
to predict all labels. In the multi-class setting, this
assumption is plausible. However, for large multilabel problems, the relevant information for each
label may be scattered throughout the document
– the problem is worse when the documents are
very long. Using label-wise attention, our model
can focus on different sections. We also need to
ﬁnd relevant information for zero-shot classes. So
we use the label vectors vi rather than learning label speciﬁc attention parameters. First, we pass
the document feature matrix D through a simple
feed-forward neural network
D2 = tanh(D Wb + bb)
where Wb ∈Ru×d and bb ∈Rd. This mapping is
important because the dimensionality of the ngram
vectors (rows) in D depends on u, the number of
scores we generate for each ngram. Given D2, we
generate the label-wise attention vector
ai = softmax(D2 vi),
i = 1, . . . , L,
where ai ∈Rn−s+1 measures how informative
each ngram is for the i-th label. Finally, we use
D, and generate L label-speciﬁc document vector
representations
i = 1, . . . , L,
such that ci ∈Ru. Intuitively, ci is the weighted
average of the rows in D forming a vector representation of the document for the i-th label.
GCNN Output Layer.
Traditionally, the output
layer of a CNN would learn label speciﬁc parameters optimized via a cross-entropy loss. Instead,
our method attempts to match documents to their
corresponding label vectors. In essence, this becomes a retrieval problem. Before using each document representation ci to score its corresponding
label, we take advantage of the structured knowledge we have over our label space using a 2-layer
GCNN. For both the MIMIC II and MIMIC III
datasets, this information is hierarchical. A snippet of the hierarchy can be found in Figure 2.
Starting with the label vectors vi, we combine
the label vectors of the children and parents for the
i-th label to form
i = f function, and Nc (Np) is the index
set of the i-th label’s children (parents). We use
different parameters to distinguish each edge type.
In this paper, given we only deal with hierarchies,
the edge types include edges from parents, from
children, and self edges. This can be adapted to
arbitrary DAGs, where parent edges represent all
incoming edges and the child edges represent all
outgoing edges for each node.
The second layer follows the same formulation
as the ﬁrst layer with
i = f(W2v1
where W2 ∈Rq×q, W2
p ∈Rq×q, W2
g ∈Rq. Next, we concatenate both the averaged description vector (from equation (1)) with
the GCNN label vector to form
i = vi || v2
i ∈Rd+q. Now, to compare the ﬁnal label
i with its document vector ci, we transform the document vector into
ei = ReLU(Woci + bo),
i = 1, . . . , L,
where Wo ∈R(q+d)×u and bo ∈Rq+d. This
transformation is required to match the dimension
to that of v3
i . Finally, the prediction for each label
i is generated via
ˆyi = sigmoid(eT
i = 1, . . . , L.
During experiments, we found that using either the
output layer GCNN or a separate GCNN for the
attention vectors (equation (2)) did not result in an
improvement and severely slowed convergence.
We train our model using a multilabel binary cross-entropy loss 
−yi log(ˆyi) −(1 −yi) log(1 −ˆyi)
where yi ∈{0, 1} is the ground truth for the i-th
label and ˆyi is our sigmoid score for the i-th label.
Experiments
In this paper, we use two medical datasets for
evaluation purposes:
MIMIC II and MIMIC III .
Both datasets contain discharge summaries annotated with a set of ICD-9 diagnosis and procedure labels. Discharge summaries are textual documents consisting of, but not limited to, physician descriptions of procedures performed, diagnoses made, the patient’s medical history, and discharge instructions. Following a generalized zeroshot learning evaluation methodology , we split the ICD-9 labels into three groups
based on frequencies in the training dataset: The
frequent group S that contains all labels that occur > 5 times, the few-shot group F that contains
labels that occur between 1 and 5 times, and the
zero-shot group Z of labels that never occur in the
training dataset, but occur in the test/dev sets. The
groups are only used for evaluation. That is, during training, systems are optimized over all labels
simultaneously. Instances that do not contain fewor zero-shot classes are removed from their respective groups during evaluation. This grouping
is important to assess how each model performs
across labels grouped by label frequency.
evaluation methodology differs from that of Xian
et al. in two ways. First, because each instance is labeled with multiple labels, the same
instance can appear in all groups — S, F, and Z.
Second, instead of top-1 accuracy or HIT@k evaluation measures, we focus on R@k to handle multiple labels. At a high level, we want to examine
whether a model can distinguish the correct fewshot (zero-shot) labels from the set of all few-shot
(zero-shot) labels. Therefore, the R@k measures
in Tables 2 and 3, and Figure 3 are computed relative to each group.
Evaluation Measures.
The overall statistics for
these two datasets are reported in Table 1. For
reproducibility purposes, we use the same training/test splits of the MIMIC II as Perotte et al.
Dataset statistics for MIMIC II and
MIMIC III.
 . Following the procedures in Perotte et al.
 and Vani et al. , for each diagnosis
and procedure label assigned to each medical report, we add its parents using the ICD-9 hierarchy.
Each report in MIMIC II is annotated with nearly
37 labels on average using hierarchical label expansion.
MIMIC III does not contain a standardized
training/test split. Therefore, we create our own
split that ensures the same patient does not appear
in both the training and test datasets. Unlike the
MIMIC II dataset, we do not augment the labels
using the ICD-9 hierarchy. The ICD-9 hierarchy
has three main levels. For MIMIC III, level 0 labels make up about 5% of all occurrences, level 1
labels make up about 62%, and level 2 (leaf level)
labels make up about 33%. Also, each MIMIC III
instance contains16 ICD-9 labels on average.
ICD-9 Structure and Descriptors.
The International Classiﬁcation of Diseases (ICD) contains
alphanumeric diagnosis and procedure codes that
are used by hospitals to standardize their billing
practices. In the following experiments, we use
the 9th edition of the ICD1. Each ICD-9 identiﬁer
contains between 3 to 5 alphanumeric characters
of the form abc.xy. The alphanumeric structure
deﬁnes a simple hierarchy over all ICD-9 codes.
For example, “systolic heart failure” (428.2) and
“diastolic heart failure” (428.3) are both children
of the “heart failure” code 428. Furthermore, sequential codes are grouped together. For instance,
numeric codes in the range 390-459 contain “Diseases of the Circulatory System”. Furthermore,
each code, including groups of codes (390-459),
contain short descriptors, where the average descriptor length contains seven words2.
work, we use both the group descriptors and in-
1The US transitioned from ICD-9 to ICD-10 in 2015. Unfortunately, at the time of publication, large publicly available
ICD-10 EMR datasets are unavailable.
2The descriptors and hierarchy used in this paper can be
found at 
ontologies/ICD9CM
Harmonic Average
Logistic *
CNN *
ACNN *
Match-CNN 
ESZSL + W2V
ESZSL + W2V 2
ESZSL + GRALS
Table 2: MIMIC II results across frequent (S), few-shot (F), and zero-shot (Z) groups. We mark prior methods for
MIMIC datasets that we implemented with a *.
Harmonic Average
Logistic *
CNN *
ACNN *
Match-CNN 
ESZSL + W2V
ESZSL + W2V 2
ESZSL + GRALS
Table 3: MIMIC III results across frequent (S), few-shot (F), and zero-shot (Z) groups. We mark prior methods for
MIMIC datasets that we implemented with a *.
dividual descriptors as input to the GCNN. At test
time, we ignore the group codes.
Implementation Details.
For the CNN component of our model, we use 300 convolution
ﬁlters with a ﬁlter size of 10. We use 300 dimensional word embeddings pretrained on PubMed
biomedical article titles and abstracts. To avoid
overﬁtting, we use dropout directly after the embedding layer with a rate of 0.2. For training we
use the ADAM optimizer
with a minibatch size of 8 and a learning rate
of 0.001. q, the GCNN hidden layer size, is set
The code for our method is available
 
multi-label-zero-shot.
Thresholding has a large inﬂuence on traditional
multi-label evaluation measures such as micro-F1
and macro-F1 . Hence, we report both recall at k (R@k) and precision at k
(P@k) which do not require a speciﬁc threshold.
R@k is preferred for few- and zero-shot labels,
because P@k quickly goes to zero as k increases
and gets bigger than the number of group speciﬁc
labels assigned to each instance. Furthermore, for
medical coding, these models are typically used as
a recommendation engine to help coders. Unless
a label appears at the top of the ranking, the annotator will not see it. Thus, ranking metrics better
measure the usefulness of our systems.
Baseline Methods.
For the frequent and fewshot labels we compare to state-of-the-art methods on the MIMIC II and MIMIC III datasets including ACNN and a
CNN method introduced in Baumel et al. .
We also compare with the L1 regularized logistic
regression model used in Vani et al. . Finally, we compare against our prior EMR coding
method, Match-CNN .
Table 4: P@k, R@k, and macro-F1 results over all labels (the union of S, F, and Z).
For zero-shot learning, we compare our results
with ESZSL .
To use ESZSL, we must specify feature vectors
for each label. For zero-shot methods, the label
vectors used are crucial regardless of the learning
method used. Therefore, we evaluate ESZSL with
three different sets of label vectors. We average
200 dimensional ICD-9 descriptor word embeddings generated by Pyysalo et al. which are
pretrained on PubMed, Wikipedia, and PubMed
Central (ESZSL + W2V). We lowercased descriptors and removed stop-words. We also compare
with label vectors derived from our own 300 dimensional embeddings (ESZSL + W2V 2) pretrained on PubMed indexed titles and abstracts.
Finally, we generate label vectors using the ICD-9
hierarchy. Speciﬁcally, let Y ∈RN×L be the document label matrix where N is the total number
of documents. We factorize Y into two matrices
U ∈RN×300 and V ∈R300×L using graph regularized alternating least squares (GRALS) . Finally, we also report a baseline
using a random ordering on labels, which is important for zero-shot labels — because the total
number of such labels is small, the chance that the
correct label is in the top k is higher compared to
few-shot and frequent labels.
We compare two variants of our method: zeroshot attentive GCNN (ZAGCNN), which is the full
method described in Section 3 and a simpler variant without the GCNN layers, zero-shot attentive
CNN (ZACNN)3.
Table 2 shows the results for MIMIC II.
Because the label set for each medical record is
augmented using the ICD-9 hierarchy, we expect
methods that use the hierarchy to have an advan-
3We name our methods with the “zero-shot” preﬁx because they are primarily designed for such scenarios, although as we show later that these methods are effective for
both few-shot and frequent labels
Recall at k
Figure 3: This graph plots the MIMIC III R@k for fewshot (F) labels at different k values.
tage. Table 2 results do not rely on thresholding
because we evaluate using the relative ranking of
groups with similar frequencies. ACNN performs
best on frequent labels. For few-shot labels, ZA-
GCNN outperforms ACNN by over 10% in R@10
and by 8% in R@5; compared to these R@k gains
for few-shot labels, our loss on frequent labels is
minimal (< 1%). We ﬁnd that the word embedding derived label vectors work best for ESZSL
on zero-shot labels. However, this setup is outperformed by GRALS derived label vectors on the
frequent and few-shot labels. On zero-shot labels,
ZAGCNN outperforms the best ESZSL variant by
over 16% for both R@5 and R@10. Also, we ﬁnd
that the GCNN layers help both few- and zeroshot labels. Finally, similar to the setup in Xian
et al. , we also compute the harmonic average across all R@5 and all R@10 scores. The
metric is only computed for methods that can predict zero-shot classes. We ﬁnd that ZAGCNN outperforms ZACNN by 4% for R@10.
We report the MIMIC III results in Table 3.
Unlike for MIMIC II, the label sets were not expanded using the ICD-9 hierarchy. Yet, we ﬁnd
substantial improvements on both few- and zeroshot labels using a GCNN. ZAGCNN outperforms
ACNN by almost 5% and ZACNN by 1% in
R@10 on few-shot classes. However, ACNN still
outperforms all other methods on frequent labels,
but by only 0.3% when compared with ZAGCNN.
For zero-shot labels, ZAGCNN outperforms ZA-
CNN by over 5% and outperforms the best ES-
ZSL method by nearly 20% in R@10. We ﬁnd
that ZACNN slightly underperforms ZAGCNN on
frequent labels with more prominent differences
showing up for infrequent labels.
In Table 4 we compare the P@10, R@10, and
macro-F1 measures across all three groups (the
union of S, F, and Z) on the MIMIC III dataset.
We emphasize that the evaluation metrics are calculated over all labels and are not averages of the
metrics computed independently for each group.
We ﬁnd that R@10 is nearly equivalent to the
R@10 on the frequent group in Table 3. Furthermore, we ﬁnd that ACNN outperforms ZAGCNN
in P@10 by almost 4%.
To compare all methods with respect to macro-F1, we simply threshold
each label at 0.5. Both R@k and P@k give more
weight to frequent labels, thus it is expected that
ACNN outperforms ZAGCNN for frequent labels.
However, we also ﬁnd that ACNN outperforms our
methods with respect to Macro-F1.
Given macro-F1 equally weights all labels, does
the higher macro score mean ACNN performs better across infrequent labels? In Figure 3, we plot
the MIMIC III R@k for the neural methods with
k ranging from 1 to 100. We ﬁnd as k increases,
the differences between ZAGCNN and ACNN become more evident. Given Figure 3 and the scores
in Table 3, it is clear that ACNN does not perform better than ZAGCNN with respect to fewand zero-shot labels. The improvement in macro-
F1 for ACNN is because it performs better on frequent labels.
In general, infrequent labels will
have scores much less than 0.5. If we rank all
labels (S ∪F ∪Z), we ﬁnd that few-shot labels
only occur among the top 16 ranked labels (average number of labels for MIMIC III) for 6% of the
test documents that contain them. This suggests
that many frequent irrelevant labels have higher
scores than the correct few-shot label.
Why do the rankings among few- and zero-shot
labels matter if they are rarely ranked above irrelevant frequent labels? If we can predict which instances contain infrequent labels (novelty detection), then we can help human coders by providing them with multiple recommendation lists — a
list of frequent labels and a list of infrequent/zeroshot labels. Also, while we would ideally want a
single method that performs best for both frequent
and infrequent labels, currently we ﬁnd that there
is a trade-off between them. Hence it may be reasonable to use different methods in combination
depending on label frequency.
Conclusion and Future Work
In this paper, we performed a ﬁne-grained evaluation of few- and zero-shot label learning in the
large-scale multi-label setting. We also introduced
a neural architecture that incorporates label descriptors and the hierarchical structure of the label
spaces for few- and zero-shot prediction. For these
infrequent labels, previous evaluation methodologies do not provide a clear picture about what
By evaluating power-law datasets using
a generalized zero-shot learning methodology, we
provide a staring point toward a better understanding. Our proposed architecture also provides large
improvements on infrequent labels over state-ofthe-art automatic medical coding methods.
We believe there are two important avenues for
future work.
1. For medical coding, a wealth of unstructured
domain expertise is available in biomedical
research articles indexed by PubMed. These
articles are annotated with medical subject
headings (MeSH terms), which are organized
in a hierarchy. Relationships between MeSH
terms and ICD-9 codes are available in Uni-
ﬁed Medical Language System ). If we can take advantage
of all this structured and unstructured information via methods such as transfer learning
or multi-task learning, then we may be able
to predict infrequent labels better.
2. For our method to be useful for human
coders, it is important to develop an accurate
novelty detector. We plan to study methods
for determining if an instance contains an infrequent label and if it does, how many infrequent labels it should be annotated with.
In essence, this is an extension of the Meta-
Labeler methodology and
open classiﬁcation . If we
can predict if an instance contains infrequent
labels, then we can recommend few- and
zero-shot labels only when necessary.
Acknowledgements
Thanks to the outstanding reviewers who provided
invaluable insights to improve our manuscript.
This research is supported by the U.S. National Library of Medicine through grant R21LM012274.
We also gratefully acknowledge the support of the
NVIDIA Corporation for its donation of the Titan
X Pascal GPU used for this research.