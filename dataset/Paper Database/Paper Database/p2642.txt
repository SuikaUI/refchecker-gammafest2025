A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues
Iulian Vlad Serban
University of Montreal
2920 chemin de la Tour,
Montr´eal, QC, Canada
Alessandro Sordoni∗
Maluuba Inc
2000 Rue Peel,
Montr´eal, QC, Canada
McGill University
3480 Rue University,
Montr´eal, QC, Canada
Laurent Charlin
HEC Montr´eal
3000 chemin de la Cˆote-Sainte-
Catherine, Montr´eal, QC, Canada
Joelle Pineau
McGill University
3480 Rue University,
Montr´eal, QC, Canada
Aaron Courville and Yoshua Bengio†
University of Montreal
2920 chemin de la Tour,
Montr´eal, QC, Canada
Sequential data often possesses hierarchical structures with
complex dependencies between sub-sequences, such as found
between the utterances in a dialogue. To model these dependencies in a generative framework, we propose a neural networkbased generative architecture, with stochastic latent variables
that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and
compare it with other recent neural-network architectures. We
evaluate the model performance through a human evaluation
study. The experiments demonstrate that our model improves
upon recently proposed models and that the latent variables
facilitate both the generation of meaningful, long and diverse
responses and maintaining dialogue state.
Introduction
Recurrent neural networks (RNNs) have recently demonstrated excellent results on a number of machine learning
problems involving the generation of sequential structured
outputs , including dialogue , language modelling machine translation and speech recognition . However, the underlying
RNNs often follow a shallow (ﬂat) generation process, where
the model variability or stochasticity only occurs when an
output (e.g. word) is sampled. Injecting all the variability at
the output level is often limiting, because the model is forced
to generate all high-level structure locally on a step-by-step
basis . In particular, this is a problem for sequential data
such as natural language data, which naturally possess a hierarchical generation process with complex intra-sequence
dependencies. For instance, natural language dialogue has at
least two levels of structure; within an utterance the structure
∗A. S. was at University of Montreal when this work was carried
†Y. B. is a CIFAR Senior Fellow
Copyright c⃝2017, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
is dominated by local statistics of the language (e.g. word cooccurrences), while across utterances there is a distinct source
of variance characterized by aspects such as conversation
topic and speaker goals. If a model only injects variability at
the word level, it will have to decide on the conversation topic
and speaker goals incrementally as it generates the words inside each utterance. This may lead to incoherent topics and
inconsistent user goals .
We attack this problem in the natural language generation setting, speciﬁcally for (unstructured) dialogue response generation. Given a dialogue context in natural language, the model is tasked with generating an appropriate response word by word. This task has been investigated recently by many researchers using the sequenceto-sequence framework . Such models are not speciﬁcally designed
for the goal-oriented setting, in which dialogue systems were
originally developed . Nevertheless, major software companies are now developing nongoal-oriented models, which daily interact with millions of
people. Two examples are Microsoft’s Xiaolice and Google’s Smart Reply system , which at its core uses a sequence-to-sequence
model. Currently, these models do not incorporate a hierarchical generation structure. Consequently, they cannot represent
higher level variability and often fail to generate meaningful,
diverse on-topic responses .
Motivated by these shortcomings, we develop a hierarchical latent variable RNN architecture to explicitly model
generative processes with multiple levels of variability. The
model is a hierarchical sequence-to-sequence model with
a continuous high-dimensional latent variable attached to
each dialogue utterance, trained by maximizing a variational
lower bound on the log-likelihood. In order to generate a
response, the model ﬁrst generates a sample of the continuous latent variable – representing the high-level semantic
content of the response – and then it generates the response
word by word conditioned on the latent variable. We apply the model to generate responses for Twitter conversations 
Li et al. 2016). We evaluate the model and compare it to competing models through manual inspection and quantitatively
using a human evaluation study on Amazon Mechanical Turk.
The results demonstrate that the model substantially improves
upon earlier models, and further highlight how the latent variables facilitate the generation of long utterances, with higher
information content, and maintain dialogue context.
Technical Background
Recurrent Neural Network Language Model
A recurrent neural network (RNN), with parameters θ, models a variable-length sequence of tokens (w1, . . . , wM) by
decomposing the probability distribution over outputs:
Pθ(w1, . . . , wM) =
Pθ(wm | w1, . . . , wm−1)Pθ(w1).
The model processes each observation recursively. At each
time step, the model observes an element and updates its
internal hidden state, hm = fθ(hm−1, wm), where f is
a parametrized non-linear function, called the activation
or gating function, such as the hyperbolic tangent, the
LSTM gating unit or
the GRU gating unit . The hidden
state summarizes the past sequence and parametrizes the output distribution of the model: Pθ(wm+1 | w1, . . . , wm) =
Pθ(wm+1 | hm). We assume the outputs lie within a discrete
vocabulary V . Under this assumption the RNN Language
Model (RNNLM) —one of the
simplest generative RNN models for discrete sequences—
parametrizes the output distribution using the softmax function applied to an afﬁne transformation of the hidden state
hm. The model parameters are learned by maximizing the
training log-likelihood using gradient descent.
Hierarchical Recurrent Encoder-Decoder (HRED)
The hierarchical recurrent encoder-decoder model (HRED)
 is an extension of
the RNNLM. It generalizes the encoder-decoder architecture
 to the dialogue setting. HRED models
each output sequence with a two-level hierarchy: a sequence
of sub-sequences, and sub-sequences of tokens. In particular,
a dialogue is modelled as a sequence of utterances (subsequences), with each utterance being a sequence of words:
Pθ(w1, . . . , wN) =
Pθ(wn|w<n),
Pθ(wn,m|wn,<m, w<n),
where, wn is the n’th utterance in a dialogue, wn,m is the
m’th word in the n’th utterance, and Mn is the number of
words in the n’th utterance. HRED consists of three RNN
modules: an encoder RNN, a context RNN and a decoder
RNN. Each utterance is deterministically encoded into a realvalued vector by the encoder RNN:
n,0 = 0, henc
n,m = f enc
n,m−1, wn,m) ∀m = 1, . . . , Mn,
where f enc
is either a GRU or a bidirectional GRU function.
The last hidden state of the encoder RNN is given as input to
the context RNN, which updates its internal hidden state to
reﬂect all the information up until that utterance:
where f con
is a GRU function taking as input two vectors.
This hidden state is given to the decoder RNN:
n,0 = 0, hdec
n,m = f dec
n,m−1, wn,m, hcon
∀m = 1, . . . , Mn,
where f dec
is the LSTM gating function taking as input three
vectors. The output distribution is given by transforming
n,m through a one-layer neural network (MLP) f mlp
followed by an afﬁne transformation and the softmax function:
Pθ(wn,m+1|wn,≤m, w<n) = e
wn,m+1f mlp
where O ∈R|V |×d is the word embedding matrix for the
output distribution with embedding dimensionality d ∈N.
The Restricted Shallow Generation Process
It has been
observed that RNNLM and HRED, and similar models based
on RNN architectures, have critical problems generating
meaningful and diverse dialogue responses . We believe these problems are caused by
the ﬂat sequential generation process followed by RNNLM
and HRED, where each word is sampled conditioned only
on previous words. We call this a shallow generation process,
because the only source of variation is modelled through the
conditional output distribution. This process is problematic
from a probabilistic perspective, because the model is forced
to generate all high-level structure locally on a step-by-step
basis . For example, for generating dialogue responses such
a model has to decide the conversation topic in the middle
of the generation process – when it is generating the ﬁrst
topic-related word – and, afterwards, for each future word the
model will have to decide whether to change or to remain on
the same topic. This makes it difﬁcult for the model to generate long-term structure. The shallow generation process is
also problematic from a computational learning perspective:
the state hm in the RNNLM—or correspondingly the state of
the decoder RNN in HRED—has to summarize all the past
information up to time step m in order to (a) generate a probable next token (short-term objective) and (b) occupy a position in embedding space which sustains an output trajectory,
for generating probable future tokens (long-term objective).
Due to the vanishing gradient effect, the short-term goals
will dominate the output distribution . In particular, for sequences with high variability, the models are likely to favour short-term predictions
as opposed to long-term predictions, because it is easier to
only learn hm for predicting the next token compared to sustaining a long-term trajectory hm, hm+1, hm+2, . . . , which
at every time step is perturbed by noisy inputs (e.g. words
given as input).
Latent Variable Hierarchical Recurrent
Encoder-Decoder (VHRED)
Figure 1: VHRED computational graph. Diamond boxes
represent deterministic variables and rounded boxes represent
stochastic variables. Full lines represent the generative model
and dashed lines represent the approximate posterior model.
Motivated by the restricted shallow generation process, we
propose the latent variable hierarchical recurrent encoderdecoder (VHRED) model. This model augments the HRED
model with a stochastic latent variable at the utterance level,
which is trained by maximizing a variational lower-bound
on the log-likelihood. This allows it to model hierarchicallystructured sequences in a two-step generation process—ﬁrst
sampling the latent variable, and then generating the output
sequence—while maintaining long-term context.
VHRED contains a continuous high-dimensional stochastic latent variable zn ∈Rdz for each utterance n = 1, . . . , N,
which is conditioned on all the previous observed tokens.
The model generates the n’th utterance tokens wn through a
two-level hierarchical generation process:
Pθ(zn | w<n) = N(μprior(w<n), Σprior(w<n)),
Pθ(wn | zn, w<n) =
Pθ(wn,m | zn, w<n, wn,<m),
where N(μ, Σ) is the multivariate normal distribution with
mean μ ∈Rdz and covariance matrix Σ ∈Rdz×dz, which is
constrained to be a diagonal matrix.
VHRED (Figure 1) contains the same three components
as the HRED model. The encoder RNN deterministically
encodes a single utterance into a ﬁxed-size real-valued vector,
which the context RNN takes as input in order to compute
its hidden state hcon
for the n’th utterance. The vector hcon
transformed through a two-layer feed-forward neural network
with hyperbolic tangent gating function. A matrix multiplication is applied to the output of the feed-forward network,
which deﬁnes the multivariate normal mean μprior. Similarly,
for the diagonal covariance matrix Σprior a different matrix
multiplication is applied to the net’s output followed by softplus function, to ensure positiveness .
The model’s latent variables are inferred by maximizing
the variational lower-bound, which factorizes into independent terms for each sub-sequence (utterance):
log Pθ(w1, . . . , wN)
−KL [Qψ(zn | w1, . . . , wn)||Pθ(zn | w<n)]
+ EQψ(zn|w1,...,wn) [log Pθ(wn | zn, w<n)] ,
where KL[Q||P] is the Kullback-Leibler (KL) divergence
between distributions Q and P. The distribution Qψ is the
approximate posterior distribution – also known as the encoder model or recognition model – which approximates the
intractable true posterior distribution:
Qψ(zn | w1, . . . , wN)
= N(μposterior(w1, . . . , wn), Σposterior(w1, . . . , wn))
≈Pψ(zn | w1, . . . , wN),
where μposterior and Σposterior respectively deﬁne the approximate posterior mean and posterior covariance matrix (assumed diagonal) as a function of the previous utterances
w1, . . . , wn−1 and the current utterance wn. The posterior
mean μposterior and covariance Σposterior are determined similar to the prior. At the n’th utterance, a feed-forward network
takes as input the concatenation of both hcon
(summary of
past utterances) and henc
n+1,Mn+1 (current utterance summary).
The network’s output is transformed through a matrix multiplication to give the mean, and by matrix multiplication and
a softplus function to give the diagonal covariance matrix.
At generation time, the model conditions on the previous observed utterances and draws zn from the prior
N(μprior(w<n), Σprior(w<n)). The sample and the output
of the context RNN are given as input to the decoder RNN:
n,0 = 0, hdec
n,m = f dec
n,m−1, wn,m, hcon
∀m = 1, . . . , Mn,
and the output tokens are sampled according to eq. (3). When
training the model, for each utterance in a training example
n = 1, . . . , N, a sample zn is drawn from the approximate
posterior N(μposterior(w1, . . . , wn), Σposterior(w1, . . . , wn)).
This sample is used to estimate the gradient w.r.t. the variational lower-bound given by eq. (4).
As will be shown in the next section, VHRED alleviates
the problems arising from the insufﬁcient shallow generation
process followed by the RNNLM and HRED models. The
variation of the output sequence is now modelled in two ways:
at the utterance-level (sequence-level) with the conditional
prior distribution over z, and at the word-level (sub-sequencelevel) with the conditional distribution over word tokens. The
effect of the variable z corresponds to higher-level decisions
about what to generate, like the conversation topic, speaker
goals or sentiment of the utterance. By representing highlevel information about the sequence, z helps model longterm output trajectories. This allows the decoder RNN hidden
state to focus only on summarizing the current utterance.
Alternative Architectures
In the course of developing the
VHRED architecture we considered different variants. We experimented with a model where the context RNN hidden state
was not given as input to the decoder RNN. However,
this architecture performed worse because all the context
information had to be passed through the latent variable zn,
which effectively made zn an information bottleneck. We
also experimented with a variant where the mean of the latent
variable zn would depend on the mean of the previous latent
variable zn−1. However, this destabilized the training process.
Lastly, we experimented with a variant, where the posterior
distribution for zn would also be conditioned on the future
context RNN states hcon
n+1. This additional information did
not improve performance w.r.t. the variational lower bound.
Experimental Evaluation
We apply VHRED to dialogue response generation. Given
a dialogue context, the model must generate an appropriate
response. This task has been studied extensively in the recent
literature .
We experiment on the Twitter Dialogue Corpus . The task is to generate utterances
to append to existing Twitter conversations. The dataset is
extracted using a procedure similar to Ritter et al. ,
and is split into training, validation and test sets, containing
respectively 749,060, 93,633 and 10,000 dialogues each.1
Each dialogue contains on average 6.27 utterances and 94.16
words. The dialogues are substantially longer than recent
large-scale language modelling corpora, such as the 1 Billion Word Language Model Benchmark ,
which focus on modelling single sentences.
Training and Evaluation Procedures
We implement all models using Theano . We optimize all models using Adam
 . We early stop and select hyperparameters using the variational lower-bound or log-likelihood
on the validation set. At test time, we use beam search
with 5 beams for outputting responses with the RNN decoders . For the VHRED models, we sample
the latent variable zn, and condition on it when executing
beam search with the RNN decoder. We use word embedding
dimensionality of size 400. All models were trained with
a learning rate of 0.0001 or 0.0002 and with mini-batches
containing 40 or 80 training examples. We use truncated
back-propagation and gradient clipping.
Baselines We compare to an LSTM model with 2000 hidden units. The architecture was chosen w.r.t. validation set
log-likelihood. We also compare to the HRED model. The
HRED model encoder RNN is a bidirectional GRU RNN
encoder, where the forward and backward RNNs each have
1000 hidden units. The context RNN and decoder RNN have
each 1000 hidden units. This architecture performed best in
preliminary experiments w.r.t. validation set log-likelihood.
Both the LSTM and HRED models have previously been proposed for dialogue response generation . For reference with earlier work not
based on neural networks, we also compare to the TF-IDF
retrieval model .
1The Twitter tweet IDs will be made available upon publication.
VHRED The encoder and context RNNs for VHRED are
parametrized in the same way as the corresponding HRED
model. The only difference is in the parametrization of the
decoder RNN, which takes as input the context RNN output
vector concatenated with the generated stochastic latent variable. Furthermore, we initialize the feed-forward networks of
the prior and posterior distributions with values drawn from
a zero-mean normal distribution with variance 0.01 and with
biases equal to zero. We also multiply the diagonal covariance matrices of the prior and posterior distributions with 0.1
to make training more stable, because a high variance makes
the gradients w.r.t. the reconstruction cost unreliable, which
is fatal at the beginning of training.
Further, VHRED’s encoder and context RNNs are initialized to the parameters of the converged HRED model. We
use the two heuristics proposed by Bowman et al. : we
drop words in the decoder with a ﬁxed drop rate of 25% and
multiply the KL terms in eq. (4) by a scalar, which starts at
zero and linearly increases to 1 over the ﬁrst 60,000 training batches. Applying these heuristics helped substantially
to stabilize the training process and improve the learned
representations of the stochastic latent variables. We also
experimented with the batch normalization training procedure for the feed-forward neural networks. but found that this
made training very unstable without any substantial gains in
performance w.r.t. the variational bound.
Human Evaluation Evaluation of dialogue system responses is a difﬁcult and open problem . Inspired by metrics used
for evaluating machine translation and information retrieval
systems, researchers have begun adopting word-overlap metrics such as BLEU. However, Liu et al. show that
such metrics have little correlation with human evaluations
of response quality. Similarly, metrics such as word perplexity have also been criticized as inappropriate for evaluation
 . We therefore conduct a human
evaluation to compare the responses from different models.
We carry out the human study on Amazon Mechanical
Turk (AMT). Our setup follows that of Sordoni et al. .
We show human evaluators a dialogue context along with
two potential responses: one response generated from each
model (generated conditioned on dialogue context). We ask
evaluators to choose the response most appropriate to the
dialogue context. If the evaluators are indifferent to either of
the two responses, or if they cannot understand the dialogue
context, they can choose neither response. For each pair of
models we conduct two experiments: one where the example
contexts contain at least 80 unique tokens (long context), and
one where they contain at least 20 (not necessarily unique)
tokens (short context). This helps compare how well each
model can integrate the dialogue context into its response,
since it has previously been argued that for long contexts
hierarchical RNNs models fare better .
The results (Table 1) show that VHRED is clearly preferred
in the majority of the experiments. In particular, VHRED
is strongly preferred over the HRED and TF-IDF baseline
models for both short and long context settings. VHRED is
also strongly preferred over the LSTM baseline model for
long contexts, although the LSTM model is preferred over
Table 1: Wins, losses and ties (in %) of VHRED against baselines based on the human study (mean preferences ± 90%
conﬁdence intervals, where ∗indicates signiﬁcant differences
at 90% conﬁdence)
Short Contexts
VHRED vs LSTM
42.5 ±2.6∗
VHRED vs HRED
42.0 ±2.8∗
VHRED vs TF-IDF
51.6 ±3.3∗
Long Contexts
VHRED vs LSTM
41.9 ±2.2∗
VHRED vs HRED
41.5 ±2.8∗
VHRED vs TF-IDF
47.9 ±3.4∗
Figure 2: Human evaluator preferences for VHRED vs LSTM
by context length excluding ties. For short contexts humans
prefer the generic responses generated by LSTM, while
for long contexts humans prefer the semantically richer responses generated by VHRED.
VHRED for short contexts. In conclusion, VHRED performs
substantially better overall than competing models.
For short contexts, the LSTM model is often preferred
over VHRED because the LSTM model tends to generate very generic responses (see Table 3). This behaviour
was also reported in previous work . Such generic or safe responses are reasonable
for a wide range of contexts. Thus, human evaluators are more
likely to rate them as appropriate compared to semantically
richer responses (e.g. responses related to a speciﬁc topic)
when the context is short. However, a model that only outputs
generic responses is generally undesirable for dialogue as it
leads to less engaging and meaningless conversations. On the
other hand, VHRED is explicitly designed for incorporating
long contexts and for outputting a diverse set of responses
by sampling of the latent variable. Thus, VHRED generates
longer sentences with more semantic content than the LSTM
model (see Table 3). This can be riskier as longer utterances
are more likely to contain small mistakes, which can lead
to lower human preference for a single utterance. However,
response diversity is crucial for maintaining interesting conversations . This conclusion is supported
by examination of the human preferences w.r.t. context length
(see Figure 2), which shows human preferences for VHRED
increase as the dialogue contexts become longer.
Metric-based Evaluation To evaluate how semantically
relevant and on-topic the responses are, we further report
results for three word embedding-based topic similarity metrics proposed by Liu et al. : Embedding Average (Average), Embedding Extrema (Extrema) and Embedding Greedy
(Greedy) .2 To analyze the information content
of the responses, we also report average entropy (in bits) –
w.r.t. the maximum likelihood unigram model over the generated responses – per word and per response.3
The results are given in Table 3. According to the topic
similarity metrics, VHRED responses are substantially more
on-topic compared to LSTM and HRED. According to
the entropy metrics, VHRED responses also contain substantially more information content. In comparison to the
generic responses of the baseline models , this suggests the hierarchical generation process facilitates the generation of more on-topic responses, as
well as semantically diverse and meaningful responses. This
indicates the VHRED hidden states traverse a larger area of
the semantic space compared to the HRED and LSTM.
Qualitative Evaluation The conclusions above are also
supported by a qualitative assessment of the generated responses. In the examples shown in Table 2. we see that
VHRED has learned to better model smilies and slang (ﬁrst
example in Table 2). Furthermore, VHRED appears to be better at generating stories and imaginative actions compared to
competing models (second example in Table 2). The third example in Table 2 is a case where VHRED generated response
is more interesting, yet may be less preferred by humans as
it is slightly incompatible with the context, compared to the
generic LSTM response – although topic switches do occur
frequently. Finally, VHRED is able to continue conversations in different languages (ﬁfth example in Table 2). This
came as a surprise to us, because we had preprocessed the
dataset by ﬁltering out non-English tweets. VHRED, however, learned to distinguish between English, Spanish and
Dutch conversations from the remaining non-English tweets
in the preprocessed dataset. Such aspects are not measured by
the human study, but are evident in our qualitative inspection.
In support of these ﬁndings, we also carried out an analysis
of the latent representations learned by VHRED. Our analysis
showed VHRED has learned to separate different types of
responses, such as how are you and thank you responses, and
responses with Spanish or Dutch language.
Related Work
Previous research on dialogue models has investigated latent
variable models. Zhai and Williams propose three
2We use the embeddings trained on Google News Corpus: https:
//code.google.com/archive/p/word2vec/.
3The unigram model is trained on the training set, and the entropy is computed on the preprocessed tokenized dataset.
Table 2: Twitter examples for the neural network models. The →token indicates a change of turn.
jamaica blood!! <url> →lol. yes [name]!. [name]! →what’s poppin in toronto homie
VHRED: nothing much. just chillin. what’s good with you?
LSTM: something!!!!!!!
HRED: something!!!!!!
reaches for [name]! →- approaches him warily - hey there buddy. swings him into the air - how you doin? →tilts his head and your
wariness and reaches for your face [name].
VHRED: - looks up at you and smiles - you’re so cute. - kisses your cheek -
LSTM: - smiles - i’m ﬁne thank you. how are you?
HRED: - grins, looking at you as i look at you my math teacher over here talkn bout nickels nd i’m tryna make big
money i need to be in calculus →lol
VHRED: i need to go to the gym and get my hair done lol
HRED: how u been
i think i want an ipad2. but i already have an ibook. want vs. need. →
if you have an ibook. then you deserve an ipad!
VHRED: that’s what i’m thinking. i’m going to buy a new one.
LSTM: i want one!!!!
HRED: i don’t know what to do with it. i don’t know what to do.
ah echt? dan anrtwoord ik volgende keer via twitter als je me smst. →
haha doe maar p. goed bezig hoor myrthe d →goed bezig? die snap
ik even niet p
VHRED: haha, ik heb er geen zin in de something, maar ik heb er geen something meer something
LSTM: haha something something something
HRED: something something something something something something something something
Table 3: Response evaluation using topic similarity metrics
(Average, Greedy and Extrema) and entropy metrics (entropy
per word Hw, and entropy per utterance HU).
models combining hidden Markov models and topic models. Unlike VHRED, these models were developed solely
for learning representations – not for generating responses.
Learning latent representations for dialogue was also pursued
by Bangalore et al. , by Crook et al. and others.
The use of a stochastic latent variable learned by maximizing a variational lower bound is inspired by the variational autoencoder (VAE) . Such models have
been used predominantly for generating images in the continuous domain . However, there has also
been recent work applying these architectures for generating
sequences, such as the Variational Recurrent Neural Networks (VRNN) , which was applied for
speech and handwriting synthesis, and Stochastic Recurrent
Networks (STORN) , which
was applied for music generation and motion capture modelling. Unlike VHRED, these models sample a separate latent
variable at each time step of the decoder; they do not exploit
hierarchical structure for modelling higher-level variability.
Most similar to our work is the Variational Recurrent Autoencoder (VRAE)
 and the Variational Autoencoder Language
Model(VAELM) , which apply encoderdecoder architectures to generative music modelling and language modelling respectively. Unlike VRAE and VAELM,
the VHRED latent variable is conditioned on all previous
utterances. This makes the latent variables co-dependent
through the observed tokens, but also enables VHRED to generate multiple utterances on the same topic. Further, VHRED
uses a hierarchical architecture similar to the HRED model,
which enables it to model long-term context. It also has a
direct deterministic connection between the context and decoder RNN, which allows the model to transfer deterministic pieces of information between its components. Finally,
VHRED achieves improved results beyond the autoencoder
framework, where the objective is conditional generation.
Conclusion
Current sequence-to-sequence models for dialogue response
generation follow a shallow generation process, which limits
their ability to model high-level variability. Consequently,
these models fail to generate meaningful and diverse on-topic
responses. To overcome these problems, we have introduced
a hierarchical latent variable neural network architecture,
called VHRED. VHRED uses a hierarchical generation process in order to exploit the with-in sequence structure in
utterances and is trained using a variational lower bound on
the log-likelihood. We have applied VHRED to the task of
dialogue response generation, where it yields a substantial
improvement over competing models in several ways, including quality of responses as measured in a human evaluation
study. The empirical results highlight the advantages of the
hierarchical generation process for generating meaningful
and diverse on-topic responses.
The proposed model can easily be extended to several
other sequential generation tasks that exhibit a hierarchical
structure, such as document-level machine translation, web
query prediction, music composition, multi-sentence document summarization and image caption generation.
Acknowledgments
The authors thank Michael Noseworthy and Sungjin Ahn for constructive feedback. The authors
acknowledge NSERC, Canada Research Chairs and CIFAR
for funding. Ryan Lowe and Joelle Pineau were funded
by the Samsung Advanced Institute of Technology (SAIT).
This research was enabled in part by support provided by
Calcul Qubec (www.calculquebec.ca) and Compute Canada
(www.computecanada.ca).