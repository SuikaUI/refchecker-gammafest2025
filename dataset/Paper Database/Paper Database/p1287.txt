Pyramid Scene Parsing Network
Hengshuang Zhao1
Jianping Shi2
Xiaojuan Qi1
Xiaogang Wang1
Jiaya Jia1
1The Chinese University of Hong Kong
2SenseTime Group Limited
{hszhao, xjqi, leojia}@cse.cuhk.edu.hk, , 
Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the
capability of global context information by different-regionbased context aggregation through our pyramid pooling
module together with the proposed pyramid scene parsing
network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing
task, while PSPNet provides a superior framework for pixellevel prediction. The proposed approach achieves state-ofthe-art performance on various datasets. It came ﬁrst in ImageNet scene parsing challenge 2016, PASCAL VOC 2012
benchmark and Cityscapes benchmark. A single PSPNet
yields the new record of mIoU accuracy 85.4% on PASCAL
VOC 2012 and accuracy 80.2% on Cityscapes.
1. Introduction
Scene parsing, based on semantic segmentation, is a fundamental topic in computer vision. The goal is to assign
each pixel in the image a category label. Scene parsing provides complete understanding of the scene. It predicts the
label, location, as well as shape for each element. This topic
is of broad interest for potential applications of automatic
driving, robot sensing, to name a few.
Difﬁculty of scene parsing is closely related to scene and
label variety. The pioneer scene parsing task is to classify 33 scenes for 2,688 images on LMO dataset . More
recent PASCAL VOC semantic segmentation and PASCAL
context datasets include more labels with similar
context, such as chair and sofa, horse and cow, etc. The
new ADE20K dataset is the most challenging one with
a large and unrestricted open vocabulary and more scene
classes. A few representative images are shown in Fig. 1.
To develop an effective algorithm for these datasets needs
to conquer a few difﬁculties.
State-of-the-art scene parsing frameworks are mostly
based on the fully convolutional network (FCN) . The
deep convolutional neural network (CNN) based methods
boost dynamic object understanding, and yet still face chal-
Figure 1. Illustration of complex scenes in ADE20K dataset.
lenges considering diverse scenes and unrestricted vocabulary. One example is shown in the ﬁrst row of Fig. 2, where
a boat is mistaken as a car. These errors are due to similar
appearance of objects. But when viewing the image regarding the context prior that the scene is described as boathouse
near a river, correct prediction should be yielded.
Towards accurate scene perception, the knowledge graph
relies on prior information of scene context.
that the major issue for current FCN based models is lack
of suitable strategy to utilize global scene category clues.
For typical complex scene understanding, previously to get
a global image-level feature, spatial pyramid pooling 
was widely employed where spatial statistics provide a good
descriptor for overall scene interpretation. Spatial pyramid
pooling network further enhances the ability.
Different from these methods, to incorporate suitable
global features, we propose pyramid scene parsing network
(PSPNet). In addition to traditional dilated FCN for
pixel prediction, we extend the pixel-level feature to the
specially designed global pyramid pooling one. The local
and global clues together make the ﬁnal prediction more
We also propose an optimization strategy with
 
deeply supervised loss. We give all implementation details,
which are key to our decent performance in this paper, and
make the code and trained models publicly available 1.
Our approach achieves state-of-the-art performance on
all available datasets. It is the champion of ImageNet scene
parsing challenge 2016 , and arrived the 1st place on
PASCAL VOC 2012 semantic segmentation benchmark ,
and the 1st place on urban scene Cityscapes data . They
manifest that PSPNet gives a promising direction for pixellevel prediction tasks, which may even beneﬁt CNN-based
stereo matching, optical ﬂow, depth estimation, etc.
follow-up work. Our main contributions are threefold.
• We propose a pyramid scene parsing network to embed difﬁcult scenery context features in an FCN based
pixel prediction framework.
• We develop an effective optimization strategy for deep
ResNet based on deeply supervised loss.
• We build a practical system for state-of-the-art scene
parsing and semantic segmentation where all crucial
implementation details are included.
2. Related Work
In the following, we review recent advances in scene
parsing and semantic segmentation tasks. Driven by powerful deep neural networks , pixel-level
prediction tasks like scene parsing and semantic segmentation achieve great progress inspired by replacing the
fully-connected layer in classiﬁcation with the convolution
layer . To enlarge the receptive ﬁeld of neural networks,
methods of used dilated convolution. Noh et al. 
proposed a coarse-to-ﬁne structure with deconvolution network to learn the segmentation mask. Our baseline network
is FCN and dilated network .
Other work mainly proceeds in two directions.
line is with multi-scale feature ensembling.
Since in deep networks, higher-layer feature contains more
semantic meaning and less location information. Combining multi-scale features can improve the performance.
The other direction is based on structure prediction. The
pioneer work used conditional random ﬁeld (CRF) as
post processing to reﬁne the segmentation result. Following
methods reﬁned networks via end-to-end modeling. Both of the two directions ameliorate the localization
ability of scene parsing where predicted semantic boundary
ﬁts objects. Yet there is still much room to exploit necessary
information in complex scenes.
To make good use of global image-level priors for diverse scene understanding, methods of extracted
global context information with traditional features not
from deep neural networks. Similar improvement was made
1 
under object detection frameworks .
Liu et al. 
proved that global average pooling with FCN can improve
semantic segmentation results. However, our experiments
show that these global descriptors are not representative
enough for the challenging ADE20K data. Therefore, different from global pooling in , we exploit the capability of global context information by different-region-based
context aggregation via our pyramid scene parsing network.
3. Pyramid Scene Parsing Network
We start with our observation and analysis of representative failure cases when applying FCN methods to scene
parsing. They motivate proposal of our pyramid pooling
module as the effective global context prior. Our pyramid
scene parsing network (PSPNet) illustrated in Fig. 3 is then
described to improve performance for open-vocabulary object and stuff identiﬁcation in complex scene parsing.
3.1. Important Observations
The new ADE20K dataset contains 150 stuff/object
category labels (e.g., wall, sky, and tree) and 1,038 imagelevel scene descriptors (e.g., airport terminal, bedroom, and
street). So a large amount of labels and vast distributions
of scenes come into existence. Inspecting the prediction
results of the FCN baseline provided in , we summarize
several common issues for complex-scene parsing.
Mismatched Relationship
Context relationship is universal and important especially for complex scene understanding. There exist co-occurrent visual patterns. For example, an airplane is likely to be in runway or ﬂy in sky
while not over a road. For the ﬁrst-row example in Fig. 2,
FCN predicts the boat in the yellow box as a “car” based on
its appearance. But the common knowledge is that a car is
seldom over a river. Lack of the ability to collect contextual
information increases the chance of misclassiﬁcation.
Confusion Categories
There are many class label pairs
in the ADE20K dataset that are confusing in classiﬁcation. Examples are ﬁeld and earth; mountain and hill;
wall, house, building and skyscraper. They are with similar appearance. The expert annotator who labeled the entire
dataset, still makes 17.60% pixel error as described in .
In the second row of Fig. 2, FCN predicts the object in the
box as part of skyscraper and part of building. These results should be excluded so that the whole object is either
skyscraper or building, but not both. This problem can be
remedied by utilizing the relationship between categories.
Inconspicuous Classes
Scene contains objects/stuff of
arbitrary size. Several small-size things, like streetlight and
signboard, are hard to ﬁnd while they may be of great importance. Contrarily, big objects or stuff may exceed the
Figure 2. Scene parsing issues we observe on ADE20K dataset. The ﬁrst row shows the issue of mismatched relationship – cars are
seldom over water than boats. The second row shows confusion categories where class “building” is easily confused as “skyscraper”. The
third row illustrates inconspicuous classes. In this example, the pillow is very similar to the bed sheet in terms of color and texture. These
inconspicuous objects are easily misclassiﬁed by FCN.
receptive ﬁeld of FCN and thus cause discontinuous prediction. As shown in the third row of Fig. 2, the pillow
has similar appearance with the sheet.
Overlooking the
global scene category may fail to parse the pillow. To improve performance for remarkably small or large objects,
one should pay much attention to different sub-regions that
contain inconspicuous-category stuff.
To summarize these observations, many errors are partially or completely related to contextual relationship and
global information for different receptive ﬁelds.
deep network with a suitable global-scene-level prior can
much improve the performance of scene parsing.
3.2. Pyramid Pooling Module
With above analysis, in what follows, we introduce the
pyramid pooling module, which empirically proves to be an
effective global contextual prior.
In a deep neural network, the size of receptive ﬁeld can
roughly indicates how much we use context information.
Although theoretically the receptive ﬁeld of ResNet is
already larger than the input image, it is shown by Zhou et
al. that the empirical receptive ﬁeld of CNN is much
smaller than the theoretical one especially on high-level layers. This makes many networks not sufﬁciently incorporate
the momentous global scenery prior. We address this issue
by proposing an effective global prior representation.
Global average pooling is a good baseline model as the
global contextual prior, which is commonly used in image
classiﬁcation tasks . In , it was successfully applied to semantic segmentation. But regarding the complexscene images in ADE20K , this strategy is not enough to
cover necessary information. Pixels in these scene images
are annotated regarding many stuff and objects. Directly
fusing them to form a single vector may lose the spatial relation and cause ambiguity. Global context information along
with sub-region context is helpful in this regard to distinguish among various categories. A more powerful representation could be fused information from different sub-regions
with these receptive ﬁelds. Similar conclusion was drawn in
classical work of scene/image classiﬁcation.
In , feature maps in different levels generated by
pyramid pooling were ﬁnally ﬂattened and concatenated to
be fed into a fully connected layer for classiﬁcation. This
global prior is designed to remove the ﬁxed-size constraint
of CNN for image classiﬁcation. To further reduce context
information loss between different sub-regions, we propose
a hierarchical global prior, containing information with different scales and varying among different sub-regions. We
Figure 3. Overview of our proposed PSPNet. Given an input image (a), we ﬁrst use CNN to get the feature map of the last convolutional
layer (b), then a pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatenation layers to form the ﬁnal feature representation, which carries both local and global context information in (c). Finally, the representation
is fed into a convolution layer to get the ﬁnal per-pixel prediction (d).
call it pyramid pooling module for global scene prior construction upon the ﬁnal-layer-feature-map of the deep neural network, as illustrated in part (c) of Fig. 3.
The pyramid pooling module fuses features under four
different pyramid scales. The coarsest level highlighted in
red is global pooling to generate a single bin output. The
following pyramid level separates the feature map into different sub-regions and forms pooled representation for different locations. The output of different levels in the pyramid pooling module contains the feature map with varied
sizes. To maintain the weight of global feature, we use 1×1
convolution layer after each pyramid level to reduce the dimension of context representation to 1/N of the original
one if the level size of pyramid is N. Then we directly upsample the low-dimension feature maps to get the same size
feature as the original feature map via bilinear interpolation.
Finally, different levels of features are concatenated as the
ﬁnal pyramid pooling global feature.
Noted that the number of pyramid levels and size of each
level can be modiﬁed. They are related to the size of feature
map that is fed into the pyramid pooling layer. The structure abstracts different sub-regions by adopting varying-size
pooling kernels in a few strides. Thus the multi-stage kernels should maintain a reasonable gap in representation.
Our pyramid pooling module is a four-level one with bin
sizes of 1×1, 2×2, 3×3 and 6×6 respectively. For the type
of pooling operation between max and average, we perform
extensive experiments to show the difference in Section 5.2.
3.3. Network Architecture
With the pyramid pooling module, we propose our pyramid scene parsing network (PSPNet) as illustrated in Fig. 3.
Given an input image in Fig. 3(a), we use a pretrained
ResNet model with the dilated network strategy 
to extract the feature map. The ﬁnal feature map size is 1/8
of the input image, as shown in Fig. 3(b). On top of the
Figure 4. Illustration of auxiliary loss in ResNet101. Each blue
box denotes a residue block. The auxiliary loss is added after the
res4b22 residue block.
map, we use the pyramid pooling module shown in (c) to
gather context information. Using our 4-level pyramid, the
pooling kernels cover the whole, half of, and small portions
of the image. They are fused as the global prior. Then we
concatenate the prior with the original feature map in the
ﬁnal part of (c). It is followed by a convolution layer to
generate the ﬁnal prediction map in (d).
To explain our structure, PSPNet provides an effective
global contextual prior for pixel-level scene parsing. The
pyramid pooling module can collect levels of information,
more representative than global pooling . In terms of
computational cost, our PSPNet does not much increase it
compared to the original dilated FCN network. In end-toend learning, the global pyramid pooling module and the
local FCN feature can be optimized simultaneously.
4. Deep Supervision for ResNet-Based FCN
Deep pretrained networks lead to good performance
 .
However, increasing depth of the network
may introduce additional optimization difﬁculty as shown
in for image classiﬁcation. ResNet solves this problem with skip connection in each block. Latter layers of
deep ResNet mainly learn residues based on previous ones.
We contrarily propose generating initial results by supervision with an additional loss, and learning the residue afterwards with the ﬁnal loss. Thus, optimization of the deep
network is decomposed into two, each is simpler to solve.
An example of our deeply supervised ResNet101 
model is illustrated in Fig. 4. Apart from the main branch
using softmax loss to train the ﬁnal classiﬁer, another classiﬁer is applied after the fourth stage, i.e., the res4b22
residue block. Different from relay backpropagation 
that blocks the backward auxiliary loss to several shallow
layers, we let the two loss functions pass through all previous layers. The auxiliary loss helps optimize the learning
process, while the master branch loss takes the most responsibility. We add weight to balance the auxiliary loss.
In the testing phase, we abandon this auxiliary branch
and only use the well optimized master branch for ﬁnal prediction. This kind of deeply supervised training strategy
for ResNet-based FCN is broadly useful under different experimental settings and works with the pre-trained ResNet
This manifests the generality of such a learning
strategy. More details are provided in Section 5.2.
5. Experiments
Our proposed method is successful on scene parsing
and semantic segmentation challenges. We evaluate it in
this section on three different datasets, including ImageNet
scene parsing challenge 2016 , PASCAL VOC 2012
semantic segmentation and urban scene understanding
dataset Cityscapes .
5.1. Implementation Details
For a practical deep learning system, devil is always in
the details. Our implementation is based on the public platform Caffe . Inspired by , we use the “poly” learning
rate policy where current learning rate equals to the base one
multiplying (1 −
maxiter)power. We set base learning rate
to 0.01 and power to 0.9. The performance can be improved
by increasing the iteration number, which is set to 150K for
ImageNet experiment, 30K for PASCAL VOC and 90K for
Cityscapes. Momentum and weight decay are set to 0.9 and
0.0001 respectively. For data augmentation, we adopt random mirror and random resize between 0.5 and 2 for all
datasets, and additionally add random rotation between -
10 and 10 degrees, and random Gaussian blur for ImageNet
and PASCAL VOC. This comprehensive data augmentation
scheme makes the network resist overﬁtting. Our network
contains dilated convolution following .
During the course of experiments, we notice that an appropriately large “cropsize” can yield good performance
and “batchsize” in the batch normalization layer is
of great importance. Due to limited physical memory on
GPU cards, we set the “batchsize” to 16 during training.
To achieve this, we modify Caffe from together with
Mean IoU(%)
Pixel Acc.(%)
ResNet50-Baseline
ResNet50+B1+MAX
ResNet50+B1+AVE
ResNet50+B1236+MAX
ResNet50+B1236+AVE
ResNet50+B1236+MAX+DR
ResNet50+B1236+AVE+DR
Table 1. Investigation of PSPNet with different settings. Baseline
is ResNet50-based FCN with dilated network. ‘B1’ and ‘B1236’
denote pooled feature maps of bin sizes {1 × 1} and {1 × 1, 2 ×
2, 3 × 3, 6 × 6} respectively. ‘MAX’ and ‘AVE’ represent max
pooling and average pooling operations individually. ‘DR’ means
that dimension reduction is taken after pooling. The results are
tested on the validation set with the single-scale input.
branch and make it support batch normalization on data
gathered from multiple GPUs based on OpenMPI. For the
auxiliary loss, we set the weight to 0.4 in experiments.
5.2. ImageNet Scene Parsing Challenge 2016
Dataset and Evaluation Metrics
The ADE20K dataset
 is used in ImageNet scene parsing challenge 2016. Different from other datasets, ADE20K is more challenging
for the up to 150 classes and diverse scenes with a total
of 1,038 image-level labels. The challenge data is divided
into 20K/2K/3K images for training, validation and testing.
Also, it needs to parse both objects and stuff in the scene,
which makes it more difﬁcult than other datasets. For evaluation, both pixel-wise accuracy (Pixel Acc.) and mean of
class-wise intersection over union (Mean IoU) are used.
Ablation Study for PSPNet
To evaluate PSPNet, we conduct experiments with several settings, including pooling
types of max and average, pooling with just one global feature or four-level features, with and without dimension reduction after the pooling operation and before concatenation. As listed in Table 1, average pooling works better than
max pooling in all settings. Pooling with pyramid parsing
outperforms that using global pooling. With dimension reduction, the performance is further enhanced. With our proposed PSPNet, the best setting yields results 41.68/80.04 in
terms of Mean IoU and Pixel Acc. (%), exceeding global
average pooling of 40.07/79.52 as idea in Liu et al. by
1.61/0.52. And compared to the baseline, PSPNet outperforming it by 4.45/2.03 in terms of absolute improvement
and 11.95/2.60 in terms of relative difference.
Ablation Study for Auxiliary Loss
The introduced auxiliary loss helps optimize the learning process while not in-
ﬂuencing learning in the master branch.
We experiment
with setting the auxiliary loss weight α between 0 and 1 and
show the results in Table 2. The baseline uses ResNet50based FCN with dilated network, with the master branch’s
softmax loss for optimization. Adding the auxiliary loss
Loss Weight α
Mean IoU(%)
Pixel Acc.(%)
ResNet50 (without AL)
ResNet50 (with α = 0.3)
ResNet50 (with α = 0.4)
ResNet50 (with α = 0.6)
ResNet50 (with α = 0.9)
Table 2. Setting an appropriate loss weight α in the auxiliary
branch is important. ‘AL’ denotes the auxiliary loss. Baseline is
ResNet50-based FCN with dilated network. Empirically, α = 0.4
yields the best performance. The results are tested on the validation set with the single-scale input.
Figure 5. Performance grows with deeper networks. The results
are obtained on the validation set with the single-scale input.
Mean IoU(%)
Pixel Acc.(%)
PSPNet(50)
PSPNet(101)
PSPNet(152)
PSPNet(269)
PSPNet(50)+MS
PSPNet(101)+MS
PSPNet(152)+MS
PSPNet(269)+MS
Table 3. Deeper pre-trained model get higher performance. Number in the brackets refers to the depth of ResNet and ‘MS’ denotes
multi-scale testing.
branch, α = 0.4 yields the best performance. It outperforms
the baseline with an improvement of 1.41/0.94 in terms of
Mean IoU and Pixel Acc. (%). We believe deeper networks
will beneﬁt more given the new augmented auxiliary loss.
Ablation Study for Pre-trained Model
Deeper neural
networks have been shown in previous work to be beneﬁcial
to large scale data classiﬁcation. To further analyze PSPNet,
we conduct experiments for different depths of pre-trained
ResNet. We test four depths of {50, 101, 152, 269}. As
shown in Fig. 5, with the same setting, increasing the depth
of ResNet from 50 to 269 can improve the score of (Mean
IoU + Pixel Acc.) / 2 (%) from 60.86 to 62.35, with 1.49 absolute improvement. Detailed scores of PSPNet pre-trained
from different depth ResNet models are listed in Table 3.
Mean IoU(%)
Pixel Acc.(%)
SegNet 
DilatedNet 
CascadeNet 
ResNet50-Baseline
ResNet50+DA
ResNet50+DA+AL
ResNet50+DA+AL+PSP
ResNet269+DA+AL+PSP
ResNet269+DA+AL+PSP+MS
Table 4. Detailed analysis of our proposed PSPNet with comparison with others. Our results are obtained on the validation set
with the single-scale input except for the last row. Results of FCN,
SegNet and DilatedNet are reported in . ‘DA’ refers to data
augmentation we performed, ‘AL’ denotes the auxiliary loss we
added and ‘PSP’ represents the proposed PSPNet. ‘MS’ means
that multi-scale testing is used.
Final Score (%)
360+MCG-ICT-CAS SP
(our single model)
DilatedNet 
SegNet 
Table 5. Results of ImageNet scene parsing challenge 2016. The
best entry of each team is listed. The ﬁnal score is the mean of
Mean IoU and Pixel Acc. Results are evaluated on the testing set.
More Detailed Performance Analysis
We show our
more detailed analysis on the validation set of ADE20K in
Table 4. All our results except the last-row one use singlescale test.
“ResNet269+DA+AL+PSP+MS” uses multiscale testing. Our baseline is adapted from ResNet50 with
dilated network, which yields MeanIoU 34.28 and Pixel
Acc. 76.35. It already outperforms other prior systems possibly due to the powerful ResNet .
Our proposed architecture makes further improvement
compared to the baseline.
Using data augmentation,
our result exceeds the baseline by 1.54/0.72 and reaches
35.82/77.07. Using the auxiliary loss can further improve
it by 1.41/0.94 and reaches 37.23/78.01. With PSPNet, we
notice relatively more signiﬁcant progress for improvement
of 4.45/2.03. The result reaches 41.68/80.04. The difference from the baseline result is 7.40/3.69 in terms of absolute improvement and 21.59/4.83 (%) in terms of relativity.
A deeper network of ResNet269 yields even higher performance up to 43.81/80.88. Finally, the multi-scale testing
scheme moves the scores to 44.94/81.69.
Results in Challenge
Using the proposed architecture,
our team came in the 1st place in ImageNet scene parsing
Figure 6. Visual improvements on ADE20K, PSPNet produces
more accurate and detailed results.
challenge 2016. Table 5 shows a few results in this competition. Our ensemble submission achieves score 57.21%
on the testing set. Our single-model yields score 55.38%,
which is even higher than a few other multi-model ensemble submissions. This score is lower than that on the validation set possibly due to the difference of data distributions
between validation and testing sets. As shown in column
(d) of Fig. 2, PSPNet solves the common problems in FCN.
Fig. 6 shows another few parsing results on validation set of
ADE20K. Our results contain more accurate and detailed
structures compared to the baseline.
5.3. PASCAL VOC 2012
Our PSPNet also works satisfyingly on semantic segmentation. We carry out experiments on the PASCAL VOC
2012 segmentation dataset , which contains 20 object
categories and one background class. Following the procedure of , we use augmented data with the annotation of resulting 10,582, 1,449 and 1,456 images for
training, validation and testing. Results are shown in Table 6, we compare PSPNet with previous best-performing
methods on the testing set based on two settings, i.e., with
or without pre-training on MS-COCO dataset . Methods pre-trained with MS-COCO are marked by ‘†’. For fair
comparison with current ResNet based frameworks in scene parsing/semantic segmentation task, we build
our architecture based on ResNet101 while without postprocessing like CRF. We evaluate PSPNet with severalscale input and use the average results following .
Figure 7. Visual improvements on PASCAL VOC 2012 data. PSP-
Net produces more accurate and detailed results.
As shown in Table 6, PSPNet outperforms prior methods on both settings. Trained with only VOC 2012 data, we
achieve 82.6% accuracy2 – we get the highest accuracy on
all 20 classes. When PSPNet is pre-trained with MS-COCO
dataset, it reaches 85.4% accuracy3 where 19 out of the 20
classes receive the highest accuracy. Intriguingly, our PSP-
Net trained with only VOC 2012 data outperforms existing
methods trained with the MS-COCO pre-trained model.
One may argue that our based classiﬁcation model is
more powerful than several prior methods since ResNet
was recently proposed.
To exhibit our unique contribution, we show that our method also outperforms stateof-the-art frameworks that use the same model, including
FCRNs , LRR , and DeepLab . In this process,
we even do not employ time-consuming but effective postprocessing, such as CRF, as that in .
Several examples are shown in Fig. 7. For “cows” in row
one, our baseline model treats it as “horse” and “dog” while
PSPNet corrects these errors. For “aeroplane” and “table”
in the second and third rows, PSPNet ﬁnds missing parts.
For “person”, “bottle” and “plant” in following rows, PSP-
Net performs well on these small-size-object classes in the
images compared to the baseline model. More visual comparisons between PSPNet and other methods are included
in Fig. 9.
5.4. Cityscapes
Cityscapes is a recently released dataset for semantic
urban scene understanding. It contains 5,000 high quality
pixel-level ﬁnely annotated images collected from 50 cities
2 
3 
boat bottle
horse mbike person plant sheep sofa train
76.8 34.2 68.9 49.4
75.3 74.7 77.6
37.4 70.9 55.1
Zoom-out 
85.6 37.3 83.2 62.5
85.1 80.7 84.9
49.2 71.7 63.3
DeepLab 
84.4 54.5 81.5 63.6
85.1 79.1 83.4
50.4 73.1 63.7
CRF-RNN 
87.5 39.0 79.7 64.2
87.6 80.8 84.4
47.8 78.3 67.1
DeconvNet 
89.9 39.3 79.7 63.9
87.4 81.2 86.1
54.3 80.7 65.0
85.2 43.9 83.3 65.2
89.0 82.7 85.3
52.0 77.3 65.1
87.7 59.4 78.4 64.9
89.3 83.5 86.1
53.4 77.9 65.0
Piecewise 
90.6 37.6 80.0 67.8
92.0 85.2 86.2
58.2 80.8 72.3
91.8 71.9 94.7 71.2
95.2 89.9 95.9
64.0 85.1 76.3
CRF-RNN† 90.4 55.3 88.7 68.4
88.3 82.4 85.1
53.5 77.4 70.1
BoxSup† 
89.8 38.0 89.2 68.9
89.6 83.0 87.7
55.8 81.2 70.7
Dilation8† 
91.7 39.6 87.8 63.1
89.7 82.9 89.8
56.0 80.2 64.7
89.0 61.6 87.7 66.8
91.2 84.3 87.6
61.3 79.4 66.4
Piecewise† 
94.1 40.7 84.1 67.8
93.4 84.3 88.4
63.8 80.9 73.0
FCRNs† 
91.9 48.1 93.4 69.3
94.2 87.5 92.8
59.7 85.5 72.7
92.4 45.1 94.6 65.2
95.1 89.1 92.3
57.4 85.7 77.3
DeepLab† 
92.6 60.4 91.6 63.4
95.0 88.4 92.6
60.0 86.8 74.5
95.8 72.7 95.0 78.9
94.7 92.0 95.7
66.9 88.8 82.0
Table 6. Per-class results on PASCAL VOC 2012 testing set. Methods pre-trained on MS-COCO are marked with ‘†’.
CRF-RNN 
SiCNN 
Dilation10 
DeepLab 
Piecewise 
Table 7. Results on Cityscapes testing set. Methods trained using
both ﬁne and coarse data are marked with ‘‡’.
in different seasons. The images are divided into sets with
numbers 2,975, 500, and 1,525 for training, validation and
testing. It deﬁnes 19 categories containing both stuff and
objects. Also, 20,000 coarsely annotated images are provided for two settings in comparison, i.e., training with only
ﬁne data or with both the ﬁne and coarse data. Methods
trained using both ﬁne and coarse data are marked with ‘‡’.
Detailed results are listed in Table 7. Our base model is
ResNet101 as in DeepLab for fair comparison and the
testing procedure follows Section 5.3.
Statistics in Table 7 show that PSPNet outperforms other
methods with notable advantage. Using both ﬁne and coarse
data for training makes our method yield 80.2 accuracy.
Several examples are shown in Fig. 8. Detailed per-class
results on testing set are shown in Table 8.
6. Concluding Remarks
We have proposed an effective pyramid scene parsing
network for complex scene understanding. The global pyra-
Figure 8. Examples of PSPNet results on Cityscapes dataset.
mid pooling feature provides additional contextual information. We have also provided a deeply supervised optimization strategy for ResNet-based FCN network. We hope the
implementation details publicly available can help the community adopt these useful strategies for scene parsing and
semantic segmentation and advance related techniques.
Acknowledgements
We would like to thank Gang Sun and Tong Xiao for
their help in training the basic classiﬁcation models, Qun
Luo for technical support. This work is supported by a grant
from the Research Grants Council of the Hong Kong SAR
(project No. 2150760).
Figure 9. Visual comparison on PASCAL VOC 2012 data. (a) Image. (b) Ground Truth. (c) FCN . (d) DPN . (e) DeepLab . (f)
road swalk build. wall fence pole tlight
person rider
train mbike bike mIoU
CRF-RNN 
SiCNN+CRF 96.3
Dilation10 
DeepLab 
Piecewise 
Table 8. Per-class results on Cityscapes testing set. Methods trained using both ﬁne and coarse set are marked with ‘‡’.