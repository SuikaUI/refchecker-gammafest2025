Extracting Training Data from Large Language Models
Nicholas Carlini1
Florian Tramèr2
Eric Wallace3
Matthew Jagielski4
Ariel Herbert-Voss5,6
Katherine Lee1
Adam Roberts1
Tom Brown5
Dawn Song3
Úlfar Erlingsson7
Alina Oprea4
Colin Raffel1
3UC Berkeley
4Northeastern University
It has become common to publish large (billion parameter)
language models that have been trained on private datasets.
This paper demonstrates that in such settings, an adversary can
perform a training data extraction attack to recover individual
training examples by querying the language model.
We demonstrate our attack on GPT-2, a language model
trained on scrapes of the public Internet, and are able to extract
hundreds of verbatim text sequences from the model’s training
data. These extracted examples include (public) personally
identiﬁable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our
attack is possible even though each of the above sequences
are included in just one document in the training data.
We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly,
we ﬁnd that larger models are more vulnerable than smaller
models. We conclude by drawing lessons and discussing possible safeguards for training large language models.
Introduction
Language models (LMs)—statistical models which assign a
probability to a sequence of words—are fundamental to many
natural language processing tasks. Modern neural-networkbased LMs use very large model architectures (e.g., 175 billion parameters ) and train on massive datasets (e.g., nearly
a terabyte of English text ). This scaling increases the
ability of LMs to generate ﬂuent natural language ,
and also allows them to be applied to a plethora of other
tasks , even without updating their parameters .
At the same time, machine learning models are notorious
for exposing information about their (potentially private) training data—both in general and in the speciﬁc case of
language models . For instance, for certain models it
is known that adversaries can apply membership inference
attacks to predict whether or not any particular example
was in the training data.
East Stroudsburg Stroudsburg...
--- Corporation Seabank Centre
------ Marine Parade Southport
Peter W---------
 
+-- 7 5--- 40--
Fax: +-- 7 5--- 0--0
Memorized text
Figure 1: Our extraction attack. Given query access to a
neural network language model, we extract an individual person’s name, email address, phone number, fax number, and
physical address. The example in this ﬁgure shows information that is all accurate so we redact it to protect privacy.
Such privacy leakage is typically associated with overﬁtting
 —when a model’s training error is signiﬁcantly lower
than its test error—because overﬁtting often indicates that a
model has memorized examples from its training set. Indeed,
overﬁtting is a sufﬁcient condition for privacy leakage 
and many attacks work by exploiting overﬁtting .
The association between overﬁtting and memorization has—
erroneously—led many to assume that state-of-the-art LMs
will not leak information about their training data. Because
these models are often trained on massive de-duplicated
datasets only for a single epoch , they exhibit little
to no overﬁtting . Accordingly, the prevailing wisdom has
been that “the degree of copying with respect to any given
work is likely to be, at most, de minimis” and that models
do not signiﬁcantly memorize any particular training example.
 
Contributions.
In this work, we demonstrate that large language models memorize and leak individual training examples. In particular, we propose a simple and efﬁcient method
for extracting verbatim sequences from a language model’s
training set using only black-box query access. Our key insight is that, although training examples do not have noticeably lower losses than test examples on average, certain worstcase training examples are indeed memorized.
In our attack, we ﬁrst generate a large, diverse set of highlikelihood samples from the model, using one of three generalpurpose sampling strategies. We then sort each sample using
one of six different metrics that estimate the likelihood of
each sample using a separate reference model (e.g., another
LM), and rank highest the samples with an abnormally high
likelihood ratio between the two models.
Our attacks directly apply to any language model, including
those trained on sensitive and non-public data . We use
the GPT-2 model released by OpenAI as a representative
language model in our experiments. We choose to attack
GPT-2 to minimize real-world harm—the GPT-2 model and
original training data source are already public.
To make our results quantitative, we deﬁne a testable definition of memorization. We then generate 1,800 candidate
memorized samples, 100 under each of the 3×6 attack conﬁgurations, and ﬁnd that over 600 of them are verbatim samples
from the GPT-2 training data (conﬁrmed in collaboration with
the creators of GPT-2). In the best attack conﬁguration, 67%
of candidate samples are verbatim training examples. Our
most obviously-sensitive attack extracts the full name, physical address, email address, phone number, and fax number
of an individual (see Figure 1). We comprehensively analyze
our attack, including studying how model size and string frequency affects memorization, as well as how different attack
conﬁgurations change the types of extracted data.
We conclude by discussing numerous practical strategies to
mitigate privacy leakage. For example, differentially-private
training is theoretically well-founded and guaranteed to
produce private models if applied at an appropriate record
level, but it can result in longer training times and typically
degrades utility. We also make recommendations, such as
carefully de-duplicating documents, that empirically will help
to mitigate memorization but cannot prevent all attacks.
Background & Related Work
To begin, we introduce the relevant background on large
(billion-parameter) neural network-based language models
(LMs) as well as data privacy attacks.
Language Modeling
Language models are a fundamental building block of current state-of-the-art natural language processing pipelines
 . While the unsupervised objectives used
to train these models vary, one popular choice is a “next-step
prediction” objective . This approach constructs
a generative model of the distribution
Pr(x1,x2,...,xn),
where x1,x2,...,xn is a sequence of tokens from a vocabulary
V by applying the chain rule of probability
Pr(x1,x2,...,xn) = Πn
i=1Pr(xi | x1,...,xi−1).
State-of-the-art LMs use neural networks to estimate this
probability distribution. We let fθ(xi | x1,...,xi−1) denote
the likelihood of token xi when evaluating the neural network f with parameters θ. While recurrent neural networks
(RNNs) used to be a common choice for the neural network architecture of LMs, attention-based models 
have recently replaced RNNs in state-of-the-art models. In
particular, Transformer LMs consist of a sequence of attention layers and are the current model architecture of choice.
Because we believe our results are independent of the exact
architecture used, we will not describe the Transformer architecture in detail here and instead refer to existing work .
Training Objective.
A language model is trained to maximize the probability of the data in a training set X . In this
paper, each training example is a text document—for example,
a speciﬁc news article or webpage from the internet. Formally,
training involves minimizing the loss function
L(θ) = −logΠn
i=1 fθ(xi | x1,...,xi−1)
over each training example in the training dataset X . Because
of this training setup, the “optimal” solution to the task of
language modeling is to memorize the answer to the question “what token follows the sequence x1,...,xi−1?” for every preﬁx in the training set. However, state-of-the-art LMs
are trained with massive datasets, which causes them to not
exhibit signiﬁcant forms of memorization: empirically, the
training loss and the test loss are nearly identical .
Generating Text.
A language model can generate new
text (potentially conditioned on some preﬁx x1,...,xi)
by iteratively sampling ˆxi+1 ∼fθ(xi+1|x1,...,xi) and then
ˆxi+1 back into the model to sample
fθ(xi+2|x1,..., ˆxi+1). This process is repeated until a desired
stopping criterion is reached. Variations of this text generation
method include deterministically choosing the most-probable
token rather than sampling (i.e., “greedy” sampling) or setting
all but the top-n probabilities to zero and renormalizing the
probabilities before sampling (i.e., top-n sampling1 ).
Our paper focuses on the GPT variant of Transformer LMs . Speciﬁcally, we demonstrate our training data extraction attacks on GPT-2, a family of LMs that
1For notational clarity, we write top-n instead of the more common top-k
because we will use the constant k for a separate purpose.
were all trained using the same dataset and training algorithm,
but with varying model sizes. GPT-2 uses a word-pieces 
vocabulary with a byte pair encoder .
GPT-2 XL is the largest model with 1.5 billion parameters.
For the remainder of this paper, the “GPT-2” model refers
to this 1.5 billion parameter model or, when we speciﬁcally
indicate this, its Small and Medium variants with 124 million
and 334 million parameters, respectively.
The GPT-2 model family was trained on data scraped from
the public Internet. The authors collected a dataset by following outbound links from the social media website Reddit. The
webpages were cleaned of HTML, with only the document
text retained, and then de-duplicated at the document level.
This resulted in a ﬁnal dataset of 40GB of text data, over
which the model was trained for approximately 12 epochs.2
As a result, GPT-2 does not overﬁt: the training loss is only
roughly 10% smaller than the test loss across all model sizes.
Training Data Privacy
It is undesirable for models to remember any details that are
speciﬁc to their (potentially private) training data. The ﬁeld
of training data privacy develops attacks (to leak training data
details) and defenses (to prevent leaks).
Privacy Attacks.
When models are not trained with
privacy-preserving algorithms, they are vulnerable to numerous privacy attacks. The least revealing form of attack is the
membership inference attack : given a trained
model, an adversary can predict whether or not a particular
example was used to train the model. Separately, model inversion attacks reconstruct representative views of a subset
of examples (e.g., a model inversion attack on a face recognition classiﬁer might recover a fuzzy image of a particular
person that the classiﬁer can recognize).
Training data extraction attacks, like model inversion attacks, reconstruct training datapoints. However, training data
extraction attacks aim to reconstruct verbatim training examples and not just representative “fuzzy” examples. This makes
them more dangerous, e.g., they can extract secrets such as
verbatim social security numbers or passwords. Training data
extraction attacks have until now been limited to small LMs
trained on academic datasets under artiﬁcial training setups
(e.g., for more epochs than typical) , or settings
where the adversary has a priori knowledge of the secret they
want to extract (e.g., a social security number) .
Protecting Privacy.
An approach to minimizing memorization of training data is to apply differentially-private training
techniques . Unfortunately, training models
with differentially-private mechanisms often reduces accuracy because it causes models to fail to capture the long
2Personal communication with the GPT-2 authors.
tails of the data distribution . Moreover, it increases
training time, which can further reduce accuracy because current LMs are limited by the cost of training . As
a result, state-of-the-art LMs such as GPT-2 , GPT-3 ,
and T5 do not apply these privacy-preserving techniques.
Threat Model & Ethics
Training data extraction attacks are often seen as theoretical
or academic and are thus unlikely to be exploitable in practice
 . This is justiﬁed by the prevailing intuition that privacy
leakage is correlated with overﬁtting , and because stateof-the-art LMs are trained on large (near terabyte-sized )
datasets for a few epochs, they tend to not overﬁt .
Our paper demonstrates that training data extraction attacks
are practical. To accomplish this, we ﬁrst precisely deﬁne
what we mean by “memorization”. We then state our threat
model and our attack objectives. Finally, we discuss the ethical
considerations behind these attacks and explain why they are
likely to be a serious threat in the future.
Deﬁning Language Model Memorization
There are many ways to deﬁne memorization in language
modeling. As mentioned earlier, memorization is in many
ways an essential component of language models because
the training objective is to assign high overall likelihood to
the training dataset. LMs must, for example, “memorize” the
correct spelling of individual words.
Indeed, there is a research direction that analyzes neural
networks as repositories of (memorized) knowledge .
For example, when GPT-2 is prompted to complete the sentence “My address is 1 Main Street, San Francisco CA”, it
generates “94107”: a correct zip code for San Francisco, CA.
While this is clearly memorization in some abstract form,we
aim to formalize our deﬁnition of memorization in order to
restrict it to cases that we might consider “unintended” .
Eidetic Memorization of Text
We deﬁne eidetic memorization as a particular type of memorization.3 Informally, eidetic memorization is data that has
been memorized by a model despite only appearing in a small
set of training instances. The fewer training samples that contain the data, the stronger the eidetic memorization is.
To formalize this notion, we ﬁrst deﬁne what it means for
a model to have knowledge of a string s. Our deﬁnition is
loosely inspired by knowledge deﬁnitions in interactive proof
systems : a model fθ knows a string s if s can be extracted
by interacting with the model. More precisely, we focus on
black-box interactions where the model generates s as the
most likely continuation when prompted with some preﬁx c:
3Eidetic memory (more commonly called photographic memory) is the
ability to recall information after seeing it only once.
Deﬁnition 1 (Model Knowledge Extraction) A string s is
extractable4 from an LM fθ if there exists a preﬁx c such that:
s′: |s′|=N
fθ(s′ | c)
We abuse notation slightly here to denote by fθ(s′ | c) the
likelihood of an entire sequence s′. Since computing the most
likely sequence s is intractable for large N, the argmax in
Deﬁnition 1 can be replaced by an appropriate sampling strategy (e.g., greedy sampling) that reﬂects the way in which the
model fθ generates text in practical applications. We then
deﬁne eidetic memorization as follows:
Deﬁnition 2 (k-Eidetic Memorization) A string s is keidetic memorized (for k ≥1) by an LM fθ if s is extractable
from fθ and s appears in at most k examples in the training
data X: |{x ∈X : s ⊆x}| ≤k.
Key to this deﬁnition is what “examples” means. For GPT-
2, each webpage is used (in its entirety) as one training example. Since this deﬁnition counts the number of distinct training
examples containing a given string, and not the total number
of times the string occurs, a string may appear multiple times
on one page while still counting as k = 1 memorization.
This deﬁnition allows us to deﬁne memorization as a spectrum. While there is no deﬁnitive value of k at which we might
say that memorization is unintentional and potentially harmful, smaller values are more likely to be so. For any given k,
memorizing longer strings is also “worse” than shorter strings,
although our deﬁnition omits this distinction for simplicity.
For example, under this deﬁnition, memorizing the correct
spellings of one particular word is not severe if the word occurs in many training examples (i.e., k is large). Memorizing
the zip code of a particular city might be eidetic memorization,
depending on whether the city was mentioned in many training examples (e.g., webpages) or just a few. Referring back to
Figure 1, memorizing an individual person’s name and phone
number clearly (informally) violates privacy expectations, and
also satisﬁes our formal deﬁnition: it is contained in just a
few documents on the Internet—and hence the training data.
Threat Model
Adversary’s Capabilities.
We consider an adversary who
has black-box input-output access to a language model. This
allows the adversary to compute the probability of arbitrary
sequences fθ(x1,...,xn), and as a result allows the adversary
to obtain next-word predictions, but it does not allow the
adversary to inspect individual weights or hidden states (e.g.,
attention vectors) of the language model.
4This deﬁnition admits pathological corner cases. For example, many
LMs when when prompted with “Repeat the following sentence: _____.” will
do so correctly. This allows any string to be “known” under our deﬁnition.
Simple reﬁnements of this deﬁnition do not solve the issue, as LMs can also
be asked to, for example, down-case a particular sentence. We avoid these
pathological cases by prompting LMs only with short preﬁxes.
This threat model is highly realistic as many LMs are
available through black-box APIs. For example, the GPT-
3 model created by OpenAI is available through black-box
API access. Auto-complete models trained on actual user data
have also been made public, although they reportedly use
privacy-protection measures during training .
Adversary’s Objective.
The adversary’s objective is to extract memorized training data from the model. The strength
of an attack is measured by how private (formalized as being
k-eidetic memorized) a particular example is. Stronger attacks
extract more examples in total (both more total sequences,
and longer sequences) and examples with lower values of k.
We do not aim to extract targeted pieces of training data, but
rather indiscriminately extract training data. While targeted
attacks have the potential to be more adversarially harmful,
our goal is to study the ability of LMs to memorize data
generally, not to create an attack that can be operationalized
by real adversaries to target speciﬁc users.
Attack Target.
We select GPT-2 as a representative
LM to study for our attacks. GPT-2 is nearly a perfect target.
First, from an ethical standpoint, the model and data are public,
and so any memorized data that we extract is already public.5
Second, from a research standpoint, the dataset (despite being
collected from public sources) was never actually released
by OpenAI. Thus, it is not possible for us to unintentionally
“cheat” and develop attacks that make use of knowledge of
the GPT-2 training dataset.
Risks of Training Data Extraction
Training data extraction attacks present numerous privacy
risks. From an ethical standpoint, most of these risks are mitigated in our paper because we attack GPT-2, whose training
data is public. However, since our attacks would apply to any
LM, we also discuss potential consequences of future attacks
on models that may be trained on private data.
Data Secrecy.
The most direct form of privacy leakage occurs when data is extracted from a model that was trained
on conﬁdential or private data. For example, GMail’s autocomplete model is trained on private text communications between users, so the extraction of unique snippets of
training data would break data secrecy.
Contextual Integrity of Data.
The above privacy threat
corresponds to a narrow view of data privacy as data secrecy.
5Since the training data is sourced from the public Web, all the outputs
of our extraction attacks can also be found via Internet searches. Indeed,
to evaluate whether we have found memorized content, we search for the
content on the Internet and are able to ﬁnd these examples relatively easily.
A broader view of the privacy risks posed by data extraction stems from the framework of data privacy as contextual
integrity . That is, data memorization is a privacy infringement if it causes data to be used outside of its intended
context. An example violation of contextual integrity is shown
in Figure 1. This individual’s name, address, email, and phone
number are not secret—they were shared online in a speciﬁc
context of intended use (as contact information for a software
project)—but are reproduced by the LM in a separate context.
Due to failures such as these, user-facing applications that use
LMs may inadvertently emit data in inappropriate contexts,
e.g., a dialogue system may emit a user’s phone number in
response to another user’s query.
Small-k Eidetic Risks.
We nevertheless focus on k-eidetic
memorization with a small k value because it makes extraction
attacks more impactful.While there are cases where large-k
memorization may still matter (for example, a company may
refer to the name of an upcoming product multiple times in
private—and even though it is discussed often the name itself
may still be sensitive) we study the small-k case.
Moreover, note that although we frame our paper as an “attack”, LMs will output memorized data even in the absence of
an explicit adversary. We treat LMs as black-box generative
functions, and the memorized content that we extract can be
generated through honest interaction with the LM. Indeed, we
have even discovered at least one memorized training example among the 1,000 GPT-3 samples that OpenAI originally
released in its ofﬁcial repository .
Ethical Considerations
In this paper, we will discuss and carefully examine speciﬁc
memorized content that we ﬁnd in our extraction attacks. This
raises ethical considerations as some of the data that we extract contains information about individual users.
As previously mentioned, we minimize ethical concerns by
using data that is already public. We attack the GPT-2 model,
which is available online. Moreover, the GPT-2 training data
was collected from the public Internet , and is in principle
available to anyone who performs the same (documented)
collection process as OpenAI, e.g., see .
However, there are still ethical concerns even though the
model and data are public. It is possible—and indeed we
ﬁnd it is the case—that we might extract personal information for individuals from the training data. For example, as
shown in Figure 1, we recovered a person’s full name, address, and phone number. In this paper, whenever we succeed
in extracting personally-identifying information (usernames,
phone numbers, etc.) we partially mask out this content with
. We are aware of the fact that this does not
provide complete mediation: disclosing that the vulnerability
exists allows a malicious actor to perform these attacks on
their own to recover this personal information.
Just as responsible disclosure still causes some (limited)
harm, we believe that the beneﬁts of publicizing these attacks
outweigh the potential harms. Further, to make our attacks
public, we must necessarily reveal some sensitive information.
We contacted the individual whose information is partially
shown in Figure 1 to disclose this fact to them in advance
and received permission to use this example. Our research
ﬁndings have also been disclosed to OpenAI.
Unfortunately, we cannot hope to contact all researchers
who train large LMs in advance of our publication. We thus
hope that this publication will spark further discussions on the
ethics of memorization and extraction among other companies
and research teams that train large LMs .
Initial Training Data Extraction Attack
We begin with a simple strawman baseline for extracting
training data from a language model in a two-step procedure.
• Generate text. We generate a large quantity of data by
unconditionally sampling from the model (Section 4.1).
• Predict which outputs contain memorized text. We
next remove the generated samples that are unlikely to
contain memorized text using a membership inference
attack (Section 4.2).
These two steps correspond directly to extracting model
knowledge (Deﬁnition 1), and then predicting which strings
might be k-eidetic memorization (Deﬁnition 2).
Initial Text Generation Scheme
To generate text, we initialize the language model with a onetoken prompt containing a special start-of-sentence token and
then repeatedly sample tokens in an autoregressive fashion
from the model (see Section 2.1 for background). We hope
that by sampling according to the model’s assigned likelihood,
we will sample sequences that the model considers “highly
likely”, and that likely sequences correspond to memorized
text. Concretely, we sample exactly 256 tokens for each trial
using the top-n strategy from Section 2.1 with n = 40.
Initial Membership Inference
Given a set of samples from the model, the problem of training
data extraction reduces to one of membership inference: predict whether each sample was present in the training data .
In their most basic form, past membership inference attacks
rely on the observation that models tend to assign higher con-
ﬁdence to examples that are present in the training data .
Therefore, a potentially high-precision membership inference
classiﬁer is to simply choose examples that are assigned the
highest likelihood by the model.
Since LMs are probabilistic generative models, we follow
prior work and use a natural likelihood measure: the per-
plexity of a sequence measures how well the LM “predicts”
the tokens in that sequence. Concretely, given a sequence of
tokens x1,...,xn, the perplexity is deﬁned as
log fθ(xi|x1,...,xi−1)
That is, if the perplexity is low, then the model is not very
“surprised” by the sequence and has assigned on average a
high probability to each subsequent token in the sequence.
Initial Extraction Results
We generate 200,000 samples using the largest version of
the GPT-2 model (XL, 1558M parameters) following the text
generation scheme described in Section 4.1. We then sort
these samples according to the model’s perplexity measure
and investigate those with the lowest perplexity.
This simple baseline extraction attack can ﬁnd a wide variety of memorized content. For example, GPT-2 memorizes
the entire text of the MIT public license, as well as the user
guidelines of Vaughn Live, an online streaming site. While
this is “memorization”, it is only k-eidetic memorization for
a large value of k—these licenses occur thousands of times.
The most interesting (but still not eidetic memorization for
low values of k) examples include the memorization of popular individuals’ Twitter handles or email addresses (omitted
to preserve user privacy). In fact, all memorized content we
identify in this baseline setting is likely to have appeared in
the training dataset many times.
This initial approach has two key weaknesses that we can
identify. First, our sampling scheme tends to produce a low
diversity of outputs. For example, out of the 200,000 samples
we generated, several hundred are duplicates of the memorized user guidelines of Vaughn Live.
Second, our baseline membership inference strategy suffers
from a large number of false positives, i.e., content that is
assigned high likelihood but is not memorized. The majority
of these false positive samples contain “repeated” strings (e.g.,
the same phrase repeated multiple times). Despite such text
being highly unlikely, large LMs often incorrectly assign high
likelihood to such repetitive sequences .
Improved Training Data Extraction Attack
The proof-of-concept attack presented in the previous section
has low precision (high-likelihood samples are not always in
the training data) and low recall (it identiﬁes no k-memorized
content for low k). Here, we improve the attack by incorporating better methods for sampling from the model (Section 5.1)
and membership inference (Section 5.2).
Improved Text Generation Schemes
The ﬁrst step in our attack is to randomly sample from the language model. Above, we used top-n sampling and conditioned
the LM on the start-of-sequence token as input. This strategy
has clear limitations : it will only generate sequences that
are likely from beginning to end. As a result, top-n sampling
from the model will cause it to generate the same (or similar)
examples several times. Below we describe two alternative
techniques for generating more diverse samples from the LM.
Sampling With A Decaying Temperature
As described in Section 2.1, an LM outputs the probability of
the next token given the prior tokens Pr(xi | x1,...,xi−1). In
practice, this is achieved by evaluating the neural network z =
fθ(x1,...,xi−1) to obtain the “logit” vector z, and then computing the output probability distribution as y = softmax(z)
deﬁned by softmax(z)i = exp(zi)/∑n
j=1 exp(zj).
One can artiﬁcially “ﬂatten” this probability distribution
to make the model less conﬁdent by replacing the output
softmax(z) with softmax(z/t), for t > 1. Here, t is called the
temperature. A higher temperature causes the model to be
less conﬁdent and more diverse in its output.
However, maintaining a high temperature throughout the
generation process would mean that even if the sampling
process began to emit a memorized example, it would likely
randomly step off the path of the memorized output. Thus,
we use a softmax temperature that decays over time, starting
at t = 10 and decaying down to t = 1 over a period of the
ﬁrst 20 tokens (≈10% of the length of the sequence). This
gives a sufﬁcient amount of time for the model to “explore”
a diverse set of preﬁxes while also allowing it to follow a
high-conﬁdence paths that it ﬁnds.
Conditioning on Internet Text
Even when applying temperature sampling, there are still
some preﬁxes that are unlikely to be sampled but nevertheless
occur in actual data. As a ﬁnal strategy, our third sampling
strategy seeds the model with preﬁxes from our own scrapes
of the Internet. This sampling strategy ensures that we will
generate samples with a diverse set of preﬁxes that are similar
in nature to the type of data GPT-2 was trained on.
We follow a different data collection process as used in
GPT-2 (which follows Reddit links) in order to reduce the likelihood that our dataset has any intersection with the model’s
training data. In particular, we select samples from a subset
of Common Crawl6 to feed as context to the model.7
6 
7It is possible there is some intersection between these two datasets, effectively allowing this strategy to “cheat”. We believe this does not considerably
affect results. First, any overlap between the two datasets is rare on average.
Second, because we only use between the ﬁrst 5 to 10 tokens of each sample,
any possible overlap will be small in absolute terms.
200,000 LM
Generations
LM (GPT-2)
Generations
(using one of 6 metrics)
Deduplicate
Training Data Extraction Attack
Evaluation
Memorization
Figure 2: Workﬂow of our extraction attack and evaluation. 1) Attack. We begin by generating many samples from GPT-2
when the model is conditioned on (potentially empty) preﬁxes. We then sort each generation according to one of six metrics and
remove the duplicates. This gives us a set of potentially memorized training examples. 2) Evaluation. We manually inspect
100 of the top-1000 generations for each metric. We mark each generation as either memorized or not-memorized by manually
searching online, and we conﬁrm these ﬁndings by working with OpenAI to query the original training data. An open-source
implementation of our attack process is available at 
As in prior work , we perform basic data-sanitization
by removing HTML and JavaScript from webpages, and we
de-duplicate data on a line-by-line basis. This gives us a
dataset of 50MB of text. We randomly sample between 5 and
10 tokens of context from this scraped data and then continue
LM generation with top-n sampling as in Section 4.1.
Improved Membership Inference
Performing membership inference by ﬁltering out samples
with low likelihood has poor precision due to failures in the
underlying language model: there are many samples that are
assigned spuriously high likelihood. There are predominantly
two categories of such samples:
• Trivial memorization. We identify many cases where
GPT-2 outputs content that is uninteresting because of
how common the text is. For example, it repeats the numbers from 1 to 100 with high probability.
• Repeated substrings. One common failure mode of LMs
is their propensity to repeatedly emit the same string over
and over . We found many of the high-likelihood
samples that are not memorized are indeed repeated texts
(e.g., “I love you. I love you...”).
Our insight is that we can ﬁlter out these uninteresting (yet
still high-likelihood samples) by comparing to a second LM.
Given a second model that accurately captures text likelihood,
we should expect it will also assign high likelihood to these
forms of memorized content. Therefore, a natural strategy
for ﬁnding more diverse and rare forms of memorization
is to ﬁlter samples where the original model’s likelihood is
“unexpectedly high” compared to a second model. Below we
discuss four methods for achieving this.
Comparing to Other Neural Language Models.
that we have access to a second LM that memorizes a different
set of examples than GPT-2. One way to achieve this would be
to train a model on a disjoint set of training data, in which case
it is unlikely that the two models will memorize the same data
for small k. An alternate strategy is to take a much smaller
model trained on the same underlying dataset: because smaller
models have less capacity for memorization, we conjecture
that there are samples that are k-eidetic memorized (for small
k) by the largest GPT-2 model, but which are not memorized
by smaller GPT-2 models. Speciﬁcally, we use the Small
(117M parameters) and Medium (345M parameters) models.
Comparing to zlib Compression.
It is not necessary that
we compare to another neural LM; any technique that quantiﬁes some notion of “surprise” for a given sequence can be
useful. As a simple baseline method, we compute the zlib 
entropy of the text: the number of bits of entropy when the
sequence is compressed with zlib compression. We then use
the ratio of the GPT-2 perplexity and the zlib entropy as our
membership inference metric. Although text compressors are
simple, they can identify many of the examples of trivial memorization and repeated patterns described above (e.g., they are
excellent at modeling repeated substrings).
Comparing to Lowercased Text.
Instead of detecting
memorization by comparing one model to another model,
another option detects memorization by comparing the perplexity of the model to the perplexity of the same model on a
“canonicalized” version of that sequence. Speciﬁcally, we measure the ratio of the perplexity on the sample before and after
lowercasing it, which can dramatically alter the perplexity of
memorized content that expects a particular casing.
Perplexity on a Sliding Window.
Sometimes a model is
not conﬁdent when the sample contains one memorized substring surrounded by a block of non-memorized (and high
perplexity) text. To handle this, we use the minimum perplexity when averaged over a sliding window of 50 tokens.8
Evaluating Memorization
We now evaluate the various data extraction methods and
study common themes in the resulting memorized content.
Methodology
An overview of our experimental setup is shown in Figure 2.
We ﬁrst build three datasets of 200,000 generated samples
(each of which is 256 tokens long) using one of our strategies:
• Top-n (§4.1) samples naively from the empty sequence.
• Temperature (§5.1.1) increases diversity during sampling.
• Internet (§5.1.2) conditions the LM on Internet text.
We order each of these three datasets according to each of
our six membership inference metrics:
• Perplexity: the perplexity of the largest GPT-2 model.
• Small: the ratio of log-perplexities of the largest GPT-2
model and the Small GPT-2 model.
• Medium: the ratio as above, but for the Medium GPT-2.
• zlib: the ratio of the (log) of the GPT-2 perplexity and the
zlib entropy (as computed by compressing the text).
• Lowercase: the ratio of perplexities of the GPT-2 model
on the original sample and on the lowercased sample.
• Window: the minimum perplexity of the largest GPT-2
model across any sliding window of 50 tokens.
For each of these 3×6 = 18 conﬁgurations, we select 100
samples from among the top-1000 samples according to the
chosen metric.9 This gives us 1,800 total samples of potentially memorized content. In real-world attacks, adversaries
will look to uncover large amounts of memorized content and
thus may generate many more samples. We focus on a smaller
set as a proof-of-concept attack.
Data De-Duplication.
To avoid “double-counting” memorized content, we apply an automated fuzzy de-duplication
step when we select the 100 samples for each conﬁguration.
Given a sample s, we deﬁne the trigram-multiset of s, denoted tri(s) as a multiset of all word-level trigrams in s (with
words split on whitespace and punctuation characters). For
example, the sentence “my name my name my name” has
two trigrams (“my name my” and ”name my name”) each of
8Chosen after a cursory hyper-parameter sweep and manual analysis.
9To favor low-ranked samples, while also exploring some of the higherranked samples, we select the 100 samples so that the fraction of selected
samples with rank below k is
multiplicity 2. We mark a sample s1 as a duplicate of another
sample s2, if their trigram multisets are similar, speciﬁcally if
|tri(s1)∩tri(s2)| ≥|tri(s1)|/2.
Evaluating Memorization Using Manual Inspection.
For each of the 1,800 selected samples, one of four authors
manually determined whether the sample contains memorized text. Since the training data for GPT-2 was sourced
from the public Web, our main tool is Internet searches. We
mark a sample as memorized if we can identify a non-trivial
substring that returns an exact match on a page found by a
Google search.
Validating Results on the Original Training Data.
Finally, given the samples that we believe to be memorized,
we work with the original authors of GPT-2 to obtain limited query access to their training dataset. To do this we sent
them all 1,800 sequences we selected for analysis. For efﬁciency, they then performed a fuzzy 3-gram match to account
for memorization with different possible tokenizations. We
marked samples as memorized if all 3-grams in the memorized sequence occurred in close proximity in the training
dataset. This approach eliminates false negatives, but has false
positives. It can conﬁrm that our samples are memorized but
cannot detect cases where we missed memorized samples.
In some experiments below, we report exact counts for how
often a particular sequence occurs in the training data. We
obtained these counts by asking the GPT-2 authors to perform
a separate grep over the entire dataset to get an exact count.
In total across all strategies, we identify 604 unique memorized training examples from among the 1,800 possible candidates, for an aggregate true positive rate of 33.5% (our best
variant has a true positive rate of 67%). Below, we categorize
what types of content is memorized by the model, and also
study which attack methods are most effective.
Categories of Memorized Content.
We manually grouped
the memorized samples into different categories (a description of these categories is in Appendix A). The results are
shown in Table 1. Most memorized content is fairly canonical
text from news headlines, log ﬁles, entries from forums or
wikis, or religious text. However, we also identify a signiﬁcant
amount of unique data, containing 128-bit UUIDs, (correctlyresolving) URLs containing random substrings, and contact
information of individual people and corporations. In Section 6.3, we study these cases in more detail.
Efﬁcacy of Different Attack Strategies.
Table 2 shows
the number of memorized samples broken down by the different text generation and membership inference strategies.
US and international news
Log ﬁles and error reports
License, terms of use, copyright notices
Lists of named items (games, countries, etc.)
Forum or Wiki entry
Valid URLs
Named individuals (non-news samples only)
Promotional content (products, subscriptions, etc.)
High entropy (UUIDs, base64 data)
Contact info (address, email, phone, twitter, etc.)
Conﬁguration ﬁles
Religious texts
Pseudonyms
Donald Trump tweets and quotes
Web forms (menu items, instructions, etc.)
Lists of numbers (dates, sequences, etc.)
Table 1: Manual categorization of the 604 memorized training
examples that we extract from GPT-2, along with a description of each category. Some samples correspond to multiple
categories (e.g., a URL may contain base-64 data). Categories
in bold correspond to personally identiﬁable information.
Sampling conditioned on Internet text is the most effective
way to identify memorized content, however, all generation
schemes reveal a signiﬁcant amount of memorized content.
For example, the baseline strategy of generating with top-n
sampling yields 191 unique memorized samples, whereas
conditioning on Internet text increases this to 273.
As discussed earlier, looking directly at the LM perplexity
is a poor membership inference metric when classifying data
generated with top-n or temperature sampling: just 9% and
3% of inspected samples are memorized, respectively. The
comparison-based metrics are signiﬁcantly more effective at
predicting if content was memorized. For example, 67% of
Internet samples marked by zlib are memorized.
Figure 3 compares the zlib entropy and the GPT-2 XL
perplexity for each sample, with memorized examples highlighted. Plots for the other strategies are shown in Figure 4 in
Appendix B. Observe that most samples fall along a diagonal,
i.e., samples with higher likelihood under one model also have
higher likelihood under another model. However, there are
numerous outliers in the top left: these samples correspond to
those that GPT-2 assigns a low perplexity (a high likelihood)
but zlib is surprised by. These points, especially those which
are extreme outliers, are more likely to be memorized than
those close to the diagonal.
The different extraction methods differ in the type of memorized content they ﬁnd. A complete breakdown of the data is
given in Appendix A; however, to brieﬂy summarize:
4 5 6 7 89
GPT-2 Perplexity
zlib Entropy
All Samples
Figure 3: The zlib entropy and the perplexity of GPT-2 XL for
200,000 samples generated with top-n sampling. In red, we
show the 100 samples that were selected for manual inspection. In blue, we show the 59 samples that were conﬁrmed
as memorized text. Additional plots for other text generation
and detection strategies are in Figure 4.
1. The zlib strategy often ﬁnds non-rare text (i.e., has a high
k-memorization). It often ﬁnds news headlines, license
ﬁles, or repeated strings from forums or wikis, and there
is only one “high entropy” sequence this strategy ﬁnds.
2. Lower-casing ﬁnds content that is likely to have irregular
capitalization, such as news headlines (where words are
capitalized) or error logs (with many uppercase words).
3. The Small and Medium strategies often ﬁnd rare content.
There are 13 and 10 high entropy examples found by using the Small and Medium GPT-2 variants, respectively
(compared to just one with zlib).
Examples of Memorized Content
We next manually analyze categories of memorized content
that we ﬁnd particularly compelling. (Additional examples
are presented in Appendix C.) Recall that since GPT-2 is
trained on public data, our attacks are not particularly severe.
Nevertheless, we ﬁnd it useful to analyze what we are able to
extract to understand the categories of memorized content—
with the understanding that attacking a model trained on a
sensitive dataset would give stronger results.
Personally Identiﬁable Information.
We identify numerous examples of individual peoples’ names, phone numbers,
addresses, and social media accounts.
Text Generation Strategy
Temperature
Perplexity
Total Unique
Table 2: The number of memorized examples (out of 100
candidates) that we identify using each of the three text generation strategies and six membership inference techniques.
Some samples are found by multiple strategies; we identify
604 unique memorized examples in total.
We ﬁnd 46 examples that contain individual peoples’
names. When counting occurrences of named individuals,
we omit memorized samples that relate to national and international news (e.g., if GPT-2 emits the name of a famous
politician, we do not count this as a named individual here).
We further ﬁnd 32 examples that contain some form of contact
information (e.g., a phone number or social media handle).
Of these, 16 contain contact information for businesses, and
16 contain private individuals’ contact details.
Some of this memorized content is exclusive to just a few
documents. For example, we extract the usernames of six
users participating in an IRC conversation that appeared in
exactly one training document.
We identify 50 examples of memorized URLs that
correctly resolve to live webpages. Many of these URLs contain uncommon pieces of text, such as random numbers or
base-64 encoded strings. We also identify several URLs that
resolve correctly but we cannot identify their source (and we
thus do not count them as “memorized” in our evaluation).
We identify 31 generated samples that contain snippets of memorized source code. Despite our ability to recover
the source code verbatim, we are almost always unable to
recover the original authorship notices or terms of use. Often,
this information is given either before the code itself or in a
LICENSE ﬁle that appears separately. For many of these samples, we can also extend their length and recover thousands
of lines of (near verbatim) source code (see Section 6.4).
Unnatural Text.
Memorization is not limited to naturallooking text. We ﬁnd 21 instances of random number sequences with at least 50 bits of entropy.10 For example, we
10We estimate the entropy through manual analysis by guessing the entropy
space given the format of the string.
Occurrences in Data
Table 3: Examples of k = 1 eidetic memorized, highentropy content that we extract from the training data. Each
is contained in just one document. In the best case, we extract
a 87-characters-long sequence that is contained in the training
dataset just 10 times in total, all in the same document.
extract the following UUID:
1e4bd2a8-e8c8-4a62-adcd-40a936480059
from the model; a Google search for this string identiﬁes just
3 documents containing this UUID, and it is contained in just
one GPT-2 training document (i.e., it is 1-eidetic memorized).
Other memorized random number sequences include UUIDs
contained in only a few documents (not listed to preserve
privacy), git commit hashes, random IDs used for ad tracking,
and product model numbers.
Table 3 gives nine examples of k = 1 eidetic memorized
content, each of which is a random sequences between 10
and 87 characters long. In each of these cases, the memorized
example is contained in exactly one training document, and
the total number of occurrences within that single document
varies between just 10 and 311.
Data From Two Sources.
We ﬁnd samples that contain
two or more snippets of memorized text that are unrelated to
one another. In one example, GPT-2 generates a news article
about the (real) murder of a woman in 2013, but then attributes
the murder to one of the victims of a nightclub shooting in
Orlando in 2016. Another sample starts with the memorized
Instagram biography of a pornography producer, but then goes
on to incorrectly describe an American fashion model as a
pornography actress. This type of generation is not k-eidetic
memorization (these independent pieces of information never
appear in the same training documents), but it is an example
of a contextual integrity violation.
Removed Content.
Finally, GPT-2 memorizes content that
has since been removed from the Internet, and is thus now
primarily accessible through GPT-2. We are aware of this
content as it is still cached by Google search, but is no longer
present on the linked webpage. Some of this data is not particularly interesting in its own right, e.g., error logs due to a
misconﬁgured webserver that has since been ﬁxed. However,
the fact that this type of memorization occurs highlights that
LMs that are trained entirely on (at-the-time) public data may
end up serving as an unintentional archive for removed data.
Extracting Longer Verbatim Sequences
In our previous experiments, we extract strings of 256 tokens
in length. Here, we brieﬂy investigate if we can extract longer
sequences. In particular, we extend the length of some of the
memorized sequences by seeding the model with each sample
and continuing to generate. To do this, we apply a beamsearch-like decoding method introduced in prior work 
instead of greedy decoding which often fails to generate long
verbatim sequences.
We can extend many of the memorized samples. For example, we identify a piece of source code taken from a repository
on GitHub. We can extend this snippet to extract an entire
ﬁle, namely 1450 lines of verbatim source code. We can
also extract the entirety of the MIT, Creative Commons, and
Project Gutenberg licenses. This indicates that while we have
extracted 604 memorized examples, we could likely extend
many of these to much longer snippets of memorized content.
Memorization is Context-Dependent
Consistent with recent work on constructing effective
“prompts” for generative LMs , we ﬁnd that the memorized content is highly dependent on the model’s context.
For example, GPT-2 will complete the prompt “3.14159”
with the ﬁrst 25 digits of π correctly using greedy sampling.
However, we ﬁnd that GPT-2 “knows” (under Deﬁnition 2)
more digits of π because using the beam-search-like strategy
introduced above extracts 500 digits correctly.
Interestingly, by providing the more descriptive prompt
“pi is 3.14159”, straight greedy decoding gives the ﬁrst 799
digits of π—more than with the sophisticated beam search.
Further providing the context “e begins 2.7182818, pi begins
3.14159”, GPT-2 greedily completes the ﬁrst 824 digits of π.
This example demonstrates the importance of the context:
in the right setting, orders of magnitude more extraction is
feasible than when the context is just slightly suboptimal.
We ﬁnd that this holds true for our memorized examples as
well. None of the 273 extracted samples found using Internet
conditioning can be reliably reproduced when using the same
preﬁx initially provided to GPT-2 that produced this sample.
However, nearly all can be reproduced with high probability
if we provided the entire sequence of data up to (but not
including) the beginning of the memorized content.
The important lesson here is that our work vastly underestimates the true amount of content that GPT-2 memorized.
There are likely prompts that would identify much more memorized content, but because we stick to simple prompts we do
not ﬁnd this memorized content.
Correlating Memorization with
Model Size & Insertion Frequency
Thus far, we have shown that language models can memorize
verbatim training strings, even when they are trained for few
epochs and achieve small train-test accuracy gaps. A natural
question is how many times a string must appear for it to be
memorized (i.e., k in Deﬁnition 2). Prior work has investigated
LM memorization by varying the number of times particular
“canary” tokens were inserted into a training dataset . The
main limitation of this approach is that it is synthetic: canaries
are inserted artiﬁcially after the dataset has been collected
and may not be representative of natural data.
Here, we study how well GPT-2 memorizes naturally occurring canaries in the training data. In particular, we consider
a piece of memorized content with the following preﬁx:
{"color":"fuchsia","link":" 
reddit.com/r/The_Donald/comments/
The reddit.com URL above is completed by a speciﬁc
6-character article ID and a title. We located URLs in this
speciﬁc format in a single document on pastebin.com. Each
URL appears a varying number of times in this document,
and hence in the GPT-2 training dataset.11 Table 4 shows
a subset of the URLs that appear more than once, and their
respective counts in the document.12 This allows us to ask
the question: how many times must an example appear in the
training dataset for us to extract it?
We attempt two approaches to extract URLs of
this format, and run three variants of GPT-2 (XL, Medium, and
Small). The two approaches vary the “difﬁculty” of the attack,
so even if the more difﬁcult fails the easier may succeed.
First, we directly prompt each variant of GPT-2 with the
preﬁx above, and use top-n sampling to generate 10,000 possible extensions. Then, we test whether any of the URLs in
the training document were among those that were emitted
by GPT-2. We count a URL as emitted if it matches verbatim
with one of the 10,000 generations.
Some URLs are not extractable with this technique, and
so we make the problem easier for GPT-2 by additionally
providing GPT-2 the 6-character random token that begins
each URL. Given this additional preﬁx, we then sample from
11The purpose of this text dump was to tag users of Reddit who posted
frequently on speciﬁc topics. In doing so, this page repeats some of the same
links many times because many users comment on the same links.
12We conﬁrmed with OpenAI that the counts here are within 5% of the
true counts of these URLs in the training data.
Occurrences
Memorized?
URL (trimmed)
51y/milo_evacua...
zin/hi_my_name...
7ne/for_all_yo...
5mj/fake_news_...
5wn/reddit_admi...
lp8/26_evening...
jla/so_pizzagat...
ubf/late_night...
eta/make_christ...
6ev/its_ofﬁcia...
3c7/scott_adams...
k2o/because_his...
tu3/armynavy_ga...
Table 4: We show snippets of Reddit URLs that appear a
varying number of times in a single training document. We
condition GPT-2 XL, Medium, or Small on a prompt that
contains the beginning of a Reddit URL and report a ✓if
the corresponding URL was generated verbatim in the ﬁrst
10,000 generations. We report a 1/2 if the URL is generated by
providing GPT-2 with the ﬁrst 6 characters of the URL and
then running beam search.
the model using the beam search procedure. This task is easier in two ways: we have ﬁrst provided more context and
additionally use a higher recall sampling strategy.
Table 4 summarizes the key results. Under the
more difﬁcult of the two approaches, the full-sized 1.5 billion
parameter GPT-2 model emits all examples that are inserted
33 times or more, the medium-sized 345 million parameter
memorizes half of the URLs, and the smallest 117 million
parameter model memorizes none of these URLs.
When given the additional context and using beam search,
the medium model can emit four more URLs, and the small
model only emits the one URL that was inserted 359 times.
These results illustrate two fundamental lessons in LM
memorization. First, larger models memorize signiﬁcantly
more training data: even hundreds of millions of parameters
are not enough to memorize some of the training points. The
ability of LMs to improve with model size has been extensively studied ; we show a negative trend where these
improvements come at the cost of decreased privacy. Second,
for the largest LM, complete memorization occurs after just
33 insertions. This implies that any potentially sensitive information that is repeated a non-trivial amount of times is at risk
for memorization, even if it was only repeated multiple times
in a single training document.
Mitigating Privacy Leakage in LMs
Now that we have shown that memorized training data can
be extracted from LMs, a natural question is how to mitigate
these threats. Here we describe several possible strategies.
Training With Differential Privacy.
Differential privacy
(DP) is a well-established notion of privacy that offers strong guarantees on the privacy of individual records in
the training dataset. Private machine learning models can be
trained with variants of the differentially private stochastic gradient descent (DP-SGD) algorithm which is widely implemented . Large companies have even used DP in production machine learning models to protect users’ sensitive
information . The tradeoffs between privacy and utility
of models have been studied extensively: differentially-private
training typically prevents models from capturing the long
tails of the data distribution and thus hurts utility .
In the content of language modeling, recent work demonstrates the privacy beneﬁts of user-level DP models . Unfortunately, this work requires labels for which users contributed each document; such labels are unavailable for data
scraped from the open Web. It may instead seem natural to
aim for DP guarantees at the granularity of individual webpages, but rare snippets of text (e.g., an individual’s name
and contact information as in Figure 1) might appear in more
than one webpage. It is thus unclear how to apply DP in a
principled and effective way on Web data.
Curating the Training Data.
One cannot manually vet the
extremely large training datasets used for training LMs. However, there are methods to limit the amount of sensitive content that is present, e.g., by identifying and ﬁltering personal
information or content with restrictive terms of use .
Aside from attempting to remove sensitive content, it is
also important to carefully de-duplicate the data. Many language modeling datasets are de-duplicated at the documentor paragraph-level, which means that a single document can
still contain many repeated occurrences of a sensitive piece
of content. We envision more sophisticated strategies to deduplicate the training data, or limit the contribution of any
single source of training data.
It is also vital to carefully source the training data. Many of
the potentially-sensitive training examples that we extracted
(e.g., individuals’ personal information) came from websites
that are known to host sensitive content, e.g., pastebin is the
12th most popular domain in GPT-2’s training set.
Overall, sanitizing data is imperfect—some private data
will always slip through—and thus it serves as a ﬁrst line of
defense and not an outright prevention against privacy leaks.
Limiting Impact of Memorization on Downstream Applications.
In many downstream applications, e.g., dialogue
systems and summarization models , LMs are ﬁnetuned on task-speciﬁc data. On the positive side, this ﬁnetuning process may cause the LM to “forget” some of
the data that is memorized during the pre-training stage. On
the negative side, ﬁne-tuning may introduce its own privacy
leakages if the task-speciﬁc data also contains private information. An interesting direction for future work is to explore
how memorization is inherited by ﬁne-tuned models.
Downstream applications built on top of language models
could also attempt to ﬁlter out generated text that contains
memorized content, if such content can be reliably detected
(e.g., using various membership inference strategies).
Auditing ML Models for Memorization.
Finally, after
mitigating privacy leaks, it is vital to audit models to empirically determine the privacy level they offer in practice .
Auditing is important even when using differential privacy,
as it can complement theoretical upper bounds on privacy
leakage . We envision using our proposed methods, as well
as existing attacks , to audit LMs.
Lessons and Future Work
Extraction Attacks Are a Practical Threat.
Prior work
shows that (100× to 1000× smaller) language models potentially memorize training data in semi-realistic settings .
Our results show that state-of-the-art LMs do memorize their
training data in practice, and that adversaries can extract this
data with simple techniques. Our attacks are practical even
when the data contains a given sequence only a few times.
As our attacks interact with a language model as a blackbox, our results approximate the worst-case behavior of language models when interacting with benign users. In particular, among 600,000 (honestly) generated samples, our attacks
ﬁnd that at least 604 (or 0.1%) contain memorized text.
Note that this is likely an extremely loose lower bound. We
only manually inspected 1,800 potential candidate memorized
samples; if we had started with more candidates we would
likely have identiﬁed signiﬁcantly more memorized content.
Developing improved techniques for extracting memorized
data, including attacks that are targeted towards speciﬁc content, is an interesting area for future work.
Memorization Does Not Require Overﬁtting.
It is often
believed that preventing overﬁtting (i.e., reducing the traintest generalization gap) will prevent models from memorizing
training data. However, large LMs have no signiﬁcant traintest gap, and yet we still extract numerous examples verbatim
from the training set. The key reason is that even though
on average the training loss is only slightly lower than the
validation loss, there are still some training examples that have
anomalously low losses. Understanding why this happens is
an important problem for future work .
Larger Models Memorize More Data.
Throughout our
experiments, larger language models consistently memorized
more training data than smaller LMs. For example, in one
setting the 1.5 billion parameter GPT-2 model memorizes
over 18× as much content as the 124 million parameter model
(Section 7). Worryingly, it is likely that as LMs become bigger
(in fact they already are 100× larger than the GPT-2 model we
study ), privacy leakage will become even more prevalent.
Memorization Can Be Hard to Discover.
Much of the
training data that we extract is only discovered when prompting the LM with a particular preﬁx. Currently, we simply
attempt to use high-quality preﬁxes and hope that they might
elicit memorization. Better preﬁx selection strategies 
might identify more memorized data.
Adopt and Develop Mitigation Strategies.
We discuss
several directions for mitigating memorization in LMs, including training with differential privacy, vetting the training
data for sensitive content, limiting the impact on downstream
applications, and auditing LMs to test for memorization. All
of these are interesting and promising avenues of future work,
but each has weaknesses and are incomplete solutions to
the full problem. Memorization in modern LMs must be addressed as new generations of LMs are emerging and becoming building blocks for a range of real-world applications.
Conclusion
For large language models to be widely adopted, they must
address the training data memorization problems that we have
identiﬁed. Our extraction attacks are practical and efﬁcient,
and can recover hundreds of training examples from a model,
even when they are contained in just one training document.
Our analysis is best viewed as a cautionary tale of what
could happen when training large LMs on sensitive data. Even
though our attacks target GPT-2 (which allows us to ensure
that our work is not harmful), the same techniques apply
to any LM. Moreover, because memorization gets worse as
LMs become larger, we expect that these vulnerabilities will
become signiﬁcantly more important in the future.
There will therefore need to be techniques developed to
speciﬁcally address our attacks. Training with differentiallyprivate techniques is one method for mitigating privacy leakage, however, we believe that it will be necessary to develop
new methods that can train models at this extreme scale (e.g.,
billions of parameters) without sacriﬁcing model accuracy
or training time. More generally, there are many open questions that we hope will be investigated further, including why
models memorize, the dangers of memorization, and how to
prevent memorization.
Acknowledgements
We are grateful for comments on early versions of this paper
by Dan Boneh, Andreas Terzis, Carey Radebaugh, Daphne Ippolito, Christine Robson, Kelly Cooke, Janel Thamkul, Austin
Tarango, Jack Clark, Ilya Mironov, and Om Thakkar. Florian
Tramèr is supported by NSF award CNS-1804222.
Summary of Contributions
• Nicholas, Dawn, Ariel, Tom, Colin and Úlfar proposed the
research question of extracting training data from GPT-2
and framed the threat model.
• Colin, Florian, Matthew, and Nicholas stated the memorization deﬁnitions.
• Florian, Ariel, and Nicholas wrote code to generate candidate memorized samples from GPT-2 and verify the ground
truth memorization.
• Florian, Nicholas, Matthew, and Eric manually reviewed
and categorized the candidate memorized content.
• Katherine, Florian, Eric, and Colin generated the ﬁgures.
• Adam, Matthew, and Eric ran preliminary investigations in
language model memorization.
• Nicholas, Florian, Eric, Colin, Katherine, Matthew, Ariel,
Alina, Úlfar, Dawn, and Adam wrote and edited the paper.
• Tom, Adam, and Colin gave advice on language models
and machine learning background.
• Alina, Úlfar, and Dawn gave advice on the security goals.