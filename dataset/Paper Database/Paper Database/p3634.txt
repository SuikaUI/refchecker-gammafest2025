Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6174–6181,
November 16–20, 2020. c⃝2020 Association for Computational Linguistics
BAE: BERT-based Adversarial Examples for Text Classiﬁcation
Siddhant Garg∗†
Amazon Alexa AI Search
Manhattan Beach, CA, USA
 
Goutham Ramakrishnan∗†
Health at Scale Corporation
San Jose, CA, USA
 
Modern text classiﬁcation models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassiﬁed by the model.
Recent works in NLP use rule-based synonym
replacement strategies to generate adversarial
These strategies can lead to outof-context and unnaturally complex token replacements, which are easily identiﬁable by
humans. We present BAE, a black box attack
for generating adversarial examples using contextual perturbations from a BERT masked language model.
BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM
to generate alternatives for the masked tokens.
Through automatic and human evaluations, we
show that BAE performs a stronger attack, in
addition to generating adversarial examples
with improved grammaticality and semantic
coherence as compared to prior work.
Introduction
Recent studies have exposed the vulnerability
of ML models to adversarial attacks, small input perturbations which lead to misclassiﬁcation
by the model. Adversarial example generation
in NLP is more challenging than in commonly studied computer vision
tasks because of (i) the discrete
nature of the input space and (ii) the need to ensure
semantic coherence with the original text. A major
bottleneck in applying gradient based or generator model 
based approaches to generate adversarial examples
in NLP is the backward propagation of the perturbations from the continuous embedding space to
the discrete token space.
∗Equal contribution by authors
† Work completed as a graduate student at UW-Madison
Figure 1: We use BERT-MLM to predict masked tokens in the text for generating adversarial examples.
The MASK token replaces a word (BAE-R attack) or
is inserted to the left/right of the word (BAE-I).
Initial works for attacking text models relied on
introducing errors at the character level or adding and deleting
words for creating adversarial examples. These
techniques often result in unnatural looking adversarial examples which lack grammatical correctness, thereby being easily identiﬁable by humans.
Rule-based synonym replacement strategies
 have recently lead to more natural looking adversarial examples. Jin et al. combine both these works
by proposing TextFooler, a strong black-box attack
baseline for text classiﬁcation models. However,
the adversarial examples generated by TextFooler
solely account for the token level similarity via
word embeddings, and not the overall sentence semantics. This can lead to out-of-context and unnaturally complex replacements (see Table 3), which
are easily human-identiﬁable. Consider a simple
example: “The restaurant service was poor”. Token level synonym replacement of ‘poor’ may lead
to an inappropriate choice such as ‘broke’, while
a context-aware choice such as ‘terrible’ leads to
better retention of semantics and grammaticality.
Therefore, a token replacement strategy contingent on retaining sentence semantics using a pow-
erful language model can alleviate the errors made by existing techniques for homonyms (tokens having
multiple meanings). In this paper, we present BAE
(BERT-based Adversarial Examples), a novel technique using the BERT masked language model
(MLM) for word replacements to better ﬁt the overall context of the English language. In addition to
replacing words, we also propose inserting new tokens in the sentence to improve the attack strength
of BAE. These perturbations in the input sentence
are achieved by masking a part of the input and
using a LM to ﬁll in the mask (See Figure 1).
Our BAE attack beats the previous baselines by a
large margin on empirical evaluation over multiple
datasets and models. We show that, surprisingly,
just a few replace/insert operations can reduce the
accuracy of even a powerful BERT classiﬁer by
over 80% on some datasets. Moreover, our human
evaluation reveals the improved grammaticality of
the adversarial examples generated by BAE over
the baseline TextFooler, which can be attributed to
the BERT-MLM. To the best of our knowledge, we
are the ﬁrst to use a LM for generating adversarial
examples. We summarize our contributions as:
• We propose BAE, an adversarial example generation technique using the BERT-MLM.
• We introduce 4 BAE attack modes by replacing and inserting tokens, all of which are almost always stronger than previous baselines
on 7 text classiﬁcation datasets.
• Through human evaluation, we show that BAE
yields adversarial examples with improved
grammaticality and semantic coherence.
Methodology
Problem Deﬁnition.
We are given a dataset
(S, Y ) = {(S1, y1), . . . (Sm, ym)} and a trained
classiﬁcation model C : S →Y . We assume the
soft-label black-box setting where the attacker can
only query the classiﬁer for output probabilities on
a given input, and does not have access to the model
parameters, gradients or training data. For an input pair (S=[t1, . . . , tn], y), we want to generate
an adversarial example Sadv such that C(Sadv)̸=y.
Additionally we would like Sadv to be grammatically correct and semantically similar to S.
BAE. For generating an adversarial example Sadv,
we introduce 2 types of token-level perturbations:
(i) Replace a token t ∈S with another and (ii) Insert a new token t′ in S. Some tokens in the input
contribute more towards the ﬁnal prediction by C
Algorithm 1: BAE-R Pseudocode
Input: Sentence S = [t1, . . . , tn], ground truth label
y, classiﬁer model C
Output: Adversarial Example Sadv
Initialization: Sadv ←S
Compute token importance Ii ∀ti ∈S
for i in descending order of Ii do
SM ←Sadv[1:i−1][M]Sadv[i+1:n]
Predict top-K tokens T for mask M ∈SM
T ←FILTER(T)
L = {} // python-style dict
for t ∈T do
L[t] = Sadv[1:i−1][t]Sadv[i+1:n]
if ∃t ∈T s.t C(L[t]) ̸= y then
Return: Sadv ←L[t′] where C(L[t′]) ̸= y,
L[t′] has maximum similarity with S
Sadv ←L[t′] where L[t′] causes maximum
reduction in probability of y in C(L[t′])
Return: Sadv ←None
than others. Replacing these tokens or inserting a
new token adjacent to them can thus have a stronger
effect on altering the classiﬁer prediction. This intuition stems from the fact that the replaced/inserted
tokens changes the local context around the original token. We estimate token importance Ii of each
ti ∈S, by deleting ti from S and computing the decrease in probability of predicting the correct label
y, similar to Jin et al. ; Ren et al. .
The Replace (R) and Insert (I) operations are
performed on a token t by masking it and inserting
a mask token adjacent to it respectively. The pretrained BERT-MLM is used to predict the mask
tokens (See Figure 1). BERT-MLM is a powerful
LM trained on a large training corpus (∼2 billion
words), and hence the predicted mask tokens ﬁt
well into the grammar and context of the text.
The BERT-MLM, however, does not guarantee
semantic coherence to the original text as demonstrated by the following simple example. Consider
the sentence: ‘the food was good’. For replacing
the token ‘good’, BERT-MLM may predict the token ‘bad’, which ﬁts well into the grammar and context of the sentence, but changes the original sentiment of the sentence. To achieve a high semantic
similarity with the original text on introducing perturbations, we ﬁlter the set of top K tokens (K is a
pre-deﬁned constant) predicted by BERT-MLM for
the masked token, using a Universal Sentence Encoder (USE) based sentence similarity scorer . For the R operation, we additionally
ﬁlter out predicted tokens that do not form the same
part of speech (POS) as the original token.
Adversarial
TextFooler
31.0 (0.747)
28.0 (0.829)
20.0 (0.828)
25.49 (0.906)
21.0 (0.827)
20.0 (0.885)
22.0 (0.852)
24.17 (0.914)
17.0 (0.924)
22.0 (0.928)
23.0 (0.933)
19.11 (0.966)
16.0 (0.902)
19.0 (0.924)
8.0 (0.896)
15.08 (0.949)
4.0 (0.848)
9.0 (0.902)
5.0 (0.871)
7.50 (0.935)
TextFooler
42.0 (0.776)
36.0 (0.827)
31.0 (0.854)
21.18 (0.910)
16.0 (0.821)
23.0 (0.846)
23.0 (0.856)
20.81 (0.920)
18.0 (0.934)
26.0 (0.941)
29.0 (0.924)
19.49 (0.971)
13.0 (0.904)
17.0 (0.916)
20.0 (0.892)
15.56 (0.956)
2.0 (0.859)
9.0 (0.891)
14.0 (0.861)
7.87 (0.938)
TextFooler
30.0 (0.787)
27.0 (0.833)
32.0 (0.877)
30.74 (0.902)
36.0 (0.772)
31.0 (0.856)
46.0 (0.835)
44.05 (0.871)
20.0 (0.922)
25.0 (0.936)
31.0 (0.929)
32.05 (0.958)
11.0 (0.899)
16.0 (0.916)
22.0 (0.909)
20.34 (0.941)
14.0 (0.830)
12.0 (0.871)
16.0 (0.856)
19.21 (0.917)
Table 1: Automatic evaluation of adversarial attacks on 4 Sentiment Classiﬁcation tasks. We report the test set
accuracy. The average semantic similarity, between the original and adversarial examples, obtained from USE are
reported in parentheses. Best performance, in terms of maximum drop in test accuracy, is highlighted in boldface.
If multiple tokens can cause C to misclassify S
when they replace the mask, we choose the token
which makes Sadv most similar to the original S
based on the USE score. If no token causes misclassiﬁcation, then we choose the one that decreases
the prediction probability P(C(Sadv)=y) the most.
We apply these token perturbations iteratively in
decreasing order of token importance, until either
C(Sadv)̸=y (successful attack) or all the tokens of
S have been perturbed (failed attack).
We present 4 attack modes for BAE based on the
R and I operations, where for each token t in S:
• BAE-R: Replace token t (See Algorithm 1)
• BAE-I: Insert a token to the left or right of t
• BAE-R/I: Either replace token t or insert a
token to the left or right of t
• BAE-R+I: First replace token t, then insert a
token to the left or right of t
Experiments
Datasets and Models.
We evaluate BAE on
different text classiﬁcation tasks. Amazon, Yelp,
IMDB are sentiment classiﬁcation datasets used in
recent works and MR contains movie reviews based on
sentiment polarity. MPQA is a dataset for opinion polarity detection,
Subj for classifying a sentence as subjective or objective and TREC for question type classiﬁcation.
We use 3 popular text classiﬁcation models: word-LSTM , word-CNN and a ﬁne-tuned
BERT base-uncased classiﬁer.
We train models on the training data and perform
the adversarial attack on the test data. For complete
model details, refer to Appendix A.
As a baseline, we consider TextFooler which performs synonym replacement using
a ﬁxed word embedding space .
We only consider the top K=50 synonyms from
the BERT-MLM predictions and set a threshold of
0.8 for the cosine similarity between USE based
embeddings of the adversarial and input text.
Automatic Evaluation Results.
We perform
the 4 BAE attacks and summarize the results in
Tables 1 and 2. Across datasets and models, our
BAE attacks are almost always more effective than
the baseline attack, achieving signiﬁcant drops of
40-80% in test accuracies, with higher average semantic similarities as shown in parentheses.
With just one exception, BAE-R+I is the
strongest attack since it allows both replacement
and insertion at the same token position.
observe a general trend that the BAE-R and
BAE-I attacks often perform comparably, while
the BAE-R/I and BAE-R+I attacks are much
stronger. We observe that the BERT classiﬁer is
more robust to BAE and TextFooler attacks than
Adversarial
TextFooler
48.49 (0.745)
58.5 (0.882)
42.4 (0.834)
45.66 (0.748)
50.2 (0.899)
32.4 (0.870)
40.94 (0.871)
49.8 (0.958)
18.0 (0.964)
31.60 (0.820)
43.1 (0.946)
20.4 (0.954)
25.57 (0.766)
29.0 (0.929)
11.8 (0.874)
TextFooler
48.77 (0.733)
58.9 (0.889)
47.6 (0.812)
44.43 (0.735)
51.0 (0.899)
29.6 (0.843)
44.43 (0.876)
49.8 (0.958)
15.4 (0.953)
32.17 (0.818)
41.5 (0.940)
13.0 (0.936)
27.83 (0.764)
31.1 (0.922)
8.4 (0.858)
TextFooler
36.23 (0.761)
69.5 (0.858)
42.8 (0.866)
43.87 (0.764)
77.2 (0.828)
37.2 (0.824)
33.49 (0.862)
74.6 (0.918)
32.2 (0.931)
24.53 (0.826)
64.0 (0.903)
23.6 (0.908)
24.34 (0.766)
58.5 (0.875)
20.2 (0.825)
Table 2: Automatic evaluation of adversarial attacks on MPQA,
Subj and TREC datasets. Other details follow those from Table 1.
All 4 modes of BAE attacks almost always outperform TextFooler.
(a) Word-LSTM
Figure 2: Graphs comparing attack effectiveness on the TREC dataset, as a function
of maximum % perturbation to the input.
the word-LSTM and word-CNN possibly due to its
large size and pre-training on a large corpus.
The TextFooler attack is sometimes stronger than
the BAE-R attack for the BERT classiﬁer. We attribute this to the shared parameter space between
the BERT-MLM and the BERT classiﬁer before
ﬁne-tuning.
The predicted tokens from BERT-
MLM may not be able to drastically change the
internal representations learned by the BERT classiﬁer, hindering their ability to adversarially affect
the classiﬁer prediction.
Additionally, we make some interesting observations pertaining to the average semantic similarity
of the adversarial examples with the original sentences (computed using USE). From Tables 1, 2 we
observe that across different models and datasets,
all BAE attacks have higher average semantic similarity than TextFooler. Notably, the BAE-I attack
achieves the highest semantic similarity among all
the 4 modes. This can be explained by the fact that
all tokens of the original sentence are retained, in
the original order, in the adversarial example generated by BAE-I. Interestingly, we observe that the
average semantic similarity of the BAE-R+I attack is always higher than the BAE-R attack. This
lends support to the importance of the ‘Insert’ operation in ameliorating the effect of the ‘Replace’
operation. We further investigate this through an
ablation study discussed later.
Effectiveness.
We study the effectiveness of BAE
on limiting the number of R/I operations permitted
on the original text. We plot the attack performance
as a function of maximum % perturbation (ratio of
number of word replacements and insertions to the
length of the original text) for the TREC dataset.
From Figure 2, we clearly observe that the BAE
attacks are consistently stronger than TextFooler.
The classiﬁer models are relatively robust to perturbations up to 20%, while the effectiveness saturates
at 40-50%. Surprisingly, a 50% perturbation for the
TREC dataset translates to replacing or inserting
just 3-4 words, due to the short text lengths.
Qualitative Examples.
We present adversarial
examples generated by the attacks on sentences
from the IMDB and Yelp datasets in Table 3. All
attack strategies successfully changed the classiﬁcation to negative, however the BAE attacks produce
more natural looking examples than TextFooler.
The tokens predicted by the BERT-MLM ﬁt well in
the sentence context, while TextFooler tends to replace words with complex synonyms, which can be
easily detected. Moreover, BAE’s additional degree
of freedom to insert tokens allows for a successful
attack with fewer perturbations.
Human Evaluation.
We perform human evaluation of our BAE attacks on the BERT classiﬁer.
For 3 datasets, we consider 100 samples from each
test set shufﬂed randomly with their successful ad-
Original [Positive Sentiment]: This ﬁlm offers many delights and surprises.
TextFooler: This ﬂick citations disparate revel and surprises.
BAE-R: This movie offers enough delights and surprises
BAE-I: This lovely ﬁlm platform offers many pleasant delights and surprises
BAE-R/I: This lovely ﬁlm serves several pleasure and surprises .
BAE-R+I: This beautiful movie offers many pleasant delights and surprises .
Original [Positive Sentiment]: Our server was great and we had perfect service.
TextFooler: Our server was tremendous and we assumed faultless services.
BAE-R: Our server was decent and we had outstanding service.
BAE-I: Our server was great enough and we had perfect service but.
BAE-R/I: Our server was great enough and we needed perfect service but.
BAE-R+I: Our server was decent company and we had adequate service.
Table 3: Qualitative examples of each attack on the BERT classiﬁer
(Replacements: Red, Inserts: Blue)
Sentiment Accuracy (%)
Naturalness (1-5)
Table 4: Human evaluation results (TF:
TextFooler and R(R+I): BAE-R(R+I)).
versarial examples from BAE-R, BAE-R+I and
TextFooler. We calculate the sentiment accuracy
by asking 3 annotators to predict the sentiment for
each sentence in this shufﬂed set. To evaluate the
naturalness of the adversarial examples, we ﬁrst
present the annotators with 50 other original data
samples to get a sense of the data distribution. We
then ask them to score each sentence (on a Likert
scale of 1-5) in the shufﬂed set on its grammar
and likelihood of being from the original data. We
average the 3 scores and present them in Table 4.
Both BAE-R and BAE-R+I attacks almost
always outperform TextFooler in both metrics.
BAE-R outperforms BAE-R+I since the latter inserts tokens to strengthen the attack, at the expense
of naturalness and sentiment accuracy. Interestingly, the BAE-R+I attacks achieve higher average semantic similarity scores than BAE-R, as discussed in Section 3. This exposes the shortcomings
of using USE for evaluating the retention of semantics of adversarial examples, and reiterates the
importance of human-centered evaluation. The gap
between the scores on the original data and the adversarial examples speaks for the limitations of the
attacks, however BAE represents an important step
forward towards improved adversarial examples.
Replace vs. Insert.
Our BAE attacks allow insertion operations in addition to replace. We analyze
the beneﬁts of this ﬂexibility of R/I operations in
Table 5: Analyzing relative importance of ‘Replace’
and ‘Insert’ perturbations for BAE. A denotes %
of test instances which are successfully attacked by
BAE-R/I, but not BAE-R, i.e. A : (R/I) ∩R. Similarly, B : (R/I) ∩I and C : (R/I) ∩R ∩I.
Table 5. From Table 5, the splits A and B are the
% of test points which compulsorily need I and R
operations respectively for a successful attack. We
can observe that the split A is larger than B thereby
indicating the importance of the I operation over R.
Test points in split C require both R and I operations for a successful attack. Interestingly, split C
is largest for Subj, which is the most robust to attack (Table 2) and hence needs both R/I operations.
Thus, this study gives positive insights towards the
importance of having the ﬂexibility to both replace
and insert words.
We present complete effectiveness graphs and
details of human evaluation in Appendix B and C.
BAE is implemented1 in TextAttack , a popular suite of NLP adversarial attacks.
Conclusion
In this paper, we have presented a new technique for generating adversarial examples (BAE)
through contextual perturbations based on the
BERT Masked Language Model. We propose inserting and/or replacing tokens from a sentence,
in their order of importance for the text classiﬁcation task, using a BERT-MLM. Automatic and
human evaluation on several datasets demonstrates
the strength and effectiveness of our attack.
Acknowledgments
The authors thank Arka Sadhu, Kalpesh Krishna,
Aws Albarghouthi, Yingyu Liang and Justin Hsu
for providing in-depth feedback for this research.
The authors thank Jack Morris and Jin Yong Yoo
for integrating BAE in the TextAttack framework.
This work is supported, in part, by the National
Science Foundation CCF under award 1652140.
1 
blob/master/textattack/attack_recipes/
bae_garg_2019.py