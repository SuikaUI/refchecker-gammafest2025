Gaussian approximation potentials: The accuracy of quantum mechanics, without the
Albert P. Bart´ok and Mike C. Payne
Cavendish Laboratory, University of Cambridge, J J Thomson Avenue, Cambridge, CB3 0HE, UK
Risi Kondor
Center for the Mathematics of Information, California Institute of Technology, MC 305-16, Pasadena, CA 91125, USA
G´abor Cs´anyi
Engineering Laboratory, University of Cambridge, Trumpington Street, Cambridge, CB2 1PZ, UK
 
We introduce a class of interatomic potential models that can be automatically generated from
data consisting of the energies and forces experienced by atoms, as derived from quantum mechanical
calculations. The models do not have a ﬁxed functional form and hence are capable of modeling
complex potential energy landscapes.
They are systematically improvable with more data.
apply the method to bulk crystals, and test it by calculating properties at high temperatures. Using
the interatomic potential to generate the long molecular dynamics trajectories required for such
calculations saves orders of magnitude in computational cost.
PACS numbers: 65.40.De,71.15.Nc,31.50.-x,34.20.Cf
Atomic scale modeling of materials is now routinely
and widely applied, and encompasses a range of techniques from exact quantum chemical methods through
density functional theory (DFT) and semi-empirical
quantum mechanics to analytic interatomic potentials
The associated trade-oﬀs in accuracy and computational cost are well known. Arguably, there is a gap
between models that treat electrons explicitly, and those
that do not.
Models in the former class are in practice limited to handling a few thousand atoms, while the
simple analytic interatomic potentials are limited in accuracy, regardless of how they are parametrized.
panels in the top row of Fig. 1 illustrates the typical
performance of analytic potentials in bulk semiconductors. Perhaps surprisingly, potentials that are generally
regarded as adequate for describing these bulk phases
show signiﬁcant deviation from the quantum mechanical potential energy surface. This in turn gives rise to
signiﬁcant errors in predicting properties such as elastic
constants and phonon spectra.
In this letter we are concerned with the problem of
modeling the Born-Oppenheimer potential energy surface (PES) of a set of atoms, but without recourse to
simulating the electrons explicitly.
We mostly restrict
our attention to modeling the bulk phases of carbon, silicon, germanium, iron and gallium nitride, using a uniﬁed
framework. Even such single-phase potentials could be
useful for calculating physical properties, e.g. the thermal expansion coeﬃcient, the phonon contribution to the
thermal conductivity, the temperature dependence of the
phonon modes, or as part of QM/MM hybrid schemes .
The ﬁrst key insight is that this is actually practicable:
the reason that interatomic potentials are at all useful is
that the PES is a relatively smooth function of the nu-
Error in model force / eV/Å
C GAP rcut = 2.0 Å
C GAP rcut = 3.7 Å
Si GAP rcut = 4.8 Å
Ge GAP rcut = 5.0 Å
Ge Tersoff
Si Tersoff
DFT force / eV/Å
FIG. 1: Deviation of atomic forces between DFT and various
models: the Brenner and Tersoﬀ potentials, and diﬀerent GAP models for diﬀerent semiconductors. In the bottom
row the horizontal lines corresponds to the smallest standard
deviation of the error theoretically attainable given the range
of the potential (see text). The conﬁgurations are taken from
molecular dynamics runs at 1000 K.
clear coordinates. Improving potential modeling is diﬃcult not because the PES is rough, but because it does
not easily decompose into simple closed functional forms.
Secondly, away from isolated quantum critical points, the
behavior of atoms is localized in the sense that if the total
energy of a system is written as a sum of atomic energies,
where rij = rj −ri is the relative position between atoms
 
i and j, then good approximations of E can be obtained
by restricting the set of atoms over which the index j runs
to some ﬁxed neighborhood of atom i, i.e., rij < rcut. In
fact, we take Eq. (1) with this restriction as the deﬁning
feature of an interatomic potential. Note that in general
it is desirable to separate out Coulomb and dispersion
terms from the atomic energy function, ε({rij}), because
the covalent part that remains can then be localized much
better for the same overall accuracy. The strict localization of ε enables the independent computation of atomic
energies. However, it also puts a limit on the accuracy
with which the PES can be approximated. Consider an
atom whose environment inside rcut is ﬁxed. The true
quantum mechanical force on this atom will show a variation, depending on its environment outside the cutoﬀ.
An estimate of this variance is shown on Fig. 1 by the
horizontal lines: no interatomic potential with the given
cutoﬀcan have a lower typical force error.
To date, two works have attempted to model the PES
in its full generality.
In the ﬁrst , small molecules
were modeled by expanding the total energy in polynomials of all the atomic coordinates, without restricting the range of the atomic energy function. While this
gave extremely accurate results, it cannot scale to more
than a few atoms. More recently, a neural network was
used to model the atomic energy . Our philosophy and
aims are similar to the latter work: we compute ε({rij})
by interpolating a set of stored reference quantum mechanical results using a carefully constructed measure of
similarity between atomic neighborhoods. We strive for
computational eﬃciency in our use of expensive ab initio data by using both the total energy and the atomic
forces to obtain the best possible estimate for ε given
our assumptions about its smoothness. Furthermore, our
scheme makes the generation of potential models automatic, with almost no need for human intervention in
going from quantum mechanical data to the ﬁnal interatomic potential model. In the following, we present an
overview of our formalism. Detailed derivations are given
in the Supplementary Information (SI).
The atomic energy function is invariant under translation, rotation and the permutation of atoms. One of
the key ideas in the present work is to represent atomic
neighborhoods in a transformed system of coordinates
that accounts for these symmetries. Ideally, this mapping should be one-to-one: mapping diﬀerent neighborhood conﬁgurations to the same coordinates would introduce systematic errors into the model that cannot be
improved by adding more quantum mechanical data. We
begin by forming a local atomic density from the neighbors of atom i, as
ρi(r) = δ(r) +
δ(r −rij)fcut(|rij|),
where fcut(r) = 1/2+cos(πr/rcut)/2 is a cutoﬀfunction,
in which the cutoﬀradius rcut reﬂects the spatial scale of
the interactions. The choice of cutoﬀfunction is somewhat ad-hoc: any smooth function with compact support
could be used.
The local atomic density is invariant to permuting the
atoms in the neighborhood. One way to achieve rotational invariance as well would be to expand it in spherical harmonics and a set of radial basis functions and appropriately combine the resulting coeﬃcients, similarly
to how the structure factor is computed from Fourier
components.
However, just as the structure factor (a
two-point correlation) is missing all “phase” information
(the relative phases of the diﬀerent plane waves), such
a set of spherical invariants would lose a lot of information about the conﬁguration of the neighborhood. In
contrast, the bispectrum , which is a three-point correlation function, is a much richer system of invariants,
and can provide an almost one-to-one representation of
the atomic neighborhood.
In our method we ﬁrst project the atomic density onto
the surface of the four-dimensional unit sphere, similarly to how the Riemann sphere is constructed, with
the transformation
= arctan(y/x)
= arccos(z/|r|)
θ0 = |r|/r0
where r0 > rcut/π. The advantage of this is that the 4D
surface contains all the information from the 3D spherical
region inside the cutoﬀ, including the radial dimension,
and thus 4D spherical harmonics (also called Wigner matrices, U j
m′m ) constitute a natural complete basis for
the interior of the 3D sphere, without the need for radial
basis functions. The projection of the atomic density on
the surface of the 4D sphere can therefore be expanded in
4D spherical harmonics using coeﬃcients (dropping the
atomic index i for clarity)
m′m = ⟨U j
The bispectrum built from these coeﬃcients is given by
Bj1,j2,j =
j1m1j2m2Cjm′
j1m1j2m2 are the ordinary Clebsch-Gordan coeﬃcients. The elements of this three-index array, which we
will denote by bi for atom i, are invariant with respect
to permutation of atoms and rotations of 4D space, and
hence also 3D space. In practice, we use only a truncated
version, with j, j1, j2 ≤Jmax, corresponding to a limit in
the spatial resolution with which we describe the atomic
neighborhood.
Determining the PES is now reduced to interpolating the atomic energy in the truncated bispectrum-space,
and for this we use a non-parametric method called Gaussian Process (GP) regression . In the GP framework, assuming Gaussian basis functions, the best estimate for the atomic energy function is given by
l[(bl−bn,l)/θl]2 ≡
αnG(b, bn),
where n and l range over the reference conﬁgurations
and bispectrum components, respectively and {θl} are
(hyper)parameters. The GP is called a non-parametric
method because the kernels G are not ﬁxed but centered
on the data, and hence, loosely, any continuous function
can in principle be obtained from Eq. (6) .
The GP diﬀers from a simple radial basis function
least-squares ﬁt in the way the coeﬃcients αn are computed. The covariance, i.e. the measure of similarity, of
the reference conﬁgurations is deﬁned as
Cnn′ = δ2G(b, b′) + σ2I
where δ and σ are two further hyperparameters and I
is the identity matrix. The interpolation coeﬃcients are
then given by
{αn} ≡α = C−1y,
where y = {yn} is the set of reference values (quantum
mechanical energies). This simple expression for the coeﬃcients is derived in detail in . Thus Eq. (6) gives
the atomic energy function in closed form as a function
of the quantum mechanical data.
In addition to preserving exact symmetries, another
hurdle is that although we wish to infer the atomic energy
function, the data we can collect directly are not values
of atomic energies, but total energies of sets of atoms,
and forces on atoms, the latter being sums of partial
derivatives of neighboring atomic energies . Furthermore, our data will be heavily correlated: e.g. the neighborhoods of atoms in a slightly perturbed ideal crystal
are very similar to each other. Both of these problems
are solved by applying a sparsiﬁcation procedure , in
which a predetermined number (much smaller than the
total data size) of “sparse” conﬁgurations are chosen randomly from the set of all conﬁgurations and the data values y in Eq. (8) are replaced by linear combinations of
all data values. The models in this work used 300 such
sparse conﬁgurations. The ﬁnal expression for the model,
which we call GAP, is derived in the SI.
All the DFT data in this work was generated with the
Castep package .
The reference conﬁgurations were
obtained by randomly displacing the atoms and the lattice vectors from their equilibrium values in 2, 8, 16 and
64-atom cubic unit cells by up to 0.2 ˚A.
The lower panels of Fig. 1 show the performance of
the GAP model for semiconductors in terms of the accuracy of forces for near-bulk conﬁgurations. For diamond,
DFT GAP Brenner Tersoﬀ(T)
1x1 unreconstructed
2x1 Pandey
1118 1081 1072
Table of relaxed diamond surface energies in
J/m2(top) and elastic constants, in units of GPa (bottom).
Frequency / THz
Frequency / THz
Temperature / K
Frequency / THz
a) Phonon dispersion curves for diamond using
the GAP model (lines), DFT (triangles) and experiment
(squares) ; b) temperature dependence of the Γ25′ mode
from MD using GAP (circles, 250 atoms, 20 ps) and experiment (squares) . In accordance with common practice 
the calculated points have been shifted by a constant to agree
with experiment at zero temperature to account for the anharmonic eﬀects of zero-point motion and a quantum correction
to the kinetic temperature is also applied ; c) phonon dispersion of iron using GAP (solid), Finnis-Sinclair potential
 (dotted) and DFT (triangles).
the GAP model is shown to improve signiﬁcantly as the
cutoﬀis increased (there is also systematic improvement
as Jmax is increased, this is shown in the SI). For all
three materials the RMS errors in the energy are less
than 1 meV/at. Table I shows the elastic constants. It
is remarkable that the existing potentials are not able
to reproduce all elastic constants to better than 25% for
any setting of their parameters. Fig. 2 shows the phonon
spectrum for diamond and iron. For diamond, the GAP
model shows excellent accuracy at zero temperature over
most of the Brillouin zone, with a slight deviation for
optical modes in the Λ direction. The agreement with
experiment is also good for the frequency of the Raman
mode as a function of temperature. For iron, the agreement with DFT is even better. We also computed the
linear thermal expansion coeﬃcient of diamond, shown
in Fig. 3, using two diﬀerent methods, applicable at low
and high temperatures. Our low temperature curve is derived from the phonon spectrum via the quasi-harmonic
approximation and agrees well with the DFT and experimental results.
At higher temperatures higher order anharmonic terms come into play, so we use molecular dynamics (MD) and obtain good agreement with experiment, showing that the GAP model is accurate signiﬁcantly beyond the small displacements that control
Finally, we extended the GAP model by including
reference conﬁgurations generated by random displacements around a diamond vacancy and graphite. Fig. 4
shows the energetics of the transition path for a migrating
vacancy in diamond and the transition from rhombohedral graphite to diamond. The agreement with DFT is
excellent, demonstrating that we can construct a truly
reactive model that describes the sp2-sp3 transition correctly, in contrast to currently used interatomic potentials.
Even for the small systems considered above, the GAP
model is orders of magnitude faster than standard planewave DFT codes, but signiﬁcantly more expensive than
simple analytical potentials. The computational cost is
roughly comparable to the cost of numerical bond order
potential models . The current implementation of the
GAP model takes 0.01 s/atom/timestep on a single CPU
core. For comparison, a timestep of the 216-atom unit
cell of Fig. 3 takes 191 s/atom using Castep, about 20,000
times longer, while the same for iron would take more
than a million times longer.
Temperature / K
Linear thermal expansion coeﬃcient of diamond in
the GAP model (dashed) and DFT (dash-dotted) using the
quasi-harmonic approximation , and derived from MD (216
atoms, 40 ps) with GAP (solid) and the Brenner potential
(dotted). Experimental results are shown with squares .
In summary, we have outlined a framework for automatically generating ﬁnite range interatomic potential
models from quantum-mechanically calculated atomic
forces and energies.
The models were tested on bulk
semiconductors and iron and were found to have remarkable accuracy in matching the ab initio potential
energy surface at a fraction of the cost, thus demonstrating the fundamental capabilities of the method.
Preliminary data for GaN, presented in the SI, shows that
the extension to multi-component and charged systems
is straightforward by augmenting the local energy with
a simple Coulomb term using ﬁxed charges. Our long
The energetics of the linear transition path for a
migrating vacancy (top) and for the rhombohedral graphite
to diamond transformation.
term goal is to expand the range of interpolated conﬁgurations and thus create“general” interatomic potentials
for one- and two-component materials whose accuracy
approaches that of quantum mechanics.
The authors thank Sebastian Ahnert, Noam Bernstein, Zoubin Ghahramani, Edward Snelson and Carl
Rasmussen for discussions.
APB is supported by the
EPSRC. GC acknowledges support from the EPSRC under grant number EP/C52392X/1. Part of the computational work was carried out on the Darwin Supercomputer of the University of Cambridge High Performance
Computing Service.
 A. Szabo and N. S. Ostlund, Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory .
 M. C. Payne et al., Rev. Mod. Phys. 64, 1045 .
 M. Finnis, Interatomic Forces in Condensed Matter .
 D. W. Brenner, phys. stat. sol. (b) 217, 23 .
 D. W. Brenner et al., J. Phys. Cond. Mat. 14, 783 .
 J. Tersoﬀ, Phys. Rev. B 39, 5566 .
 N. Bernstein, J. R. Kermode, and G. Cs´anyi, Rep. Prog.
Phys. 72, 026501 .
 A. Brown et al., J. Chem. Phys. 119, 8790 .
 J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401
 S. A. Dianat and R. M. Rao, Opt. Eng. 29, 504 .
 D. A. Varshalovich, A. N. Moskalev, and V. K. Khersonskii, Quantum theory of angular momentum .
 C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning .
Information
Inference,
and Learning Algorithms .
 S. Ahnert, Ph.D. thesis, University of Cambridge .
 E. Snelson and Z. Ghahramani, in Advances in Neural
Information Processing Systems 18, edited by Y. Weiss,
B. Sch¨olkopf, and J. Platt , pp. 1257–
 S. J. Clark et al., Zeit. Krist. 220, 567 .
 J. L. Warren, J. L. Yarnell, G. Dolling, and R. A. Cowley,
Phys. Rev. 158, 805 .
 M. S. Liu, L. A. Bursill, S. Prawer, and R. Beserman,
Phys. Rev. B 61, 3391 .
 G. Lang et al., Phys. Rev. B 59, 6182 .
 C. Z. Wang, C. T. Chan, and K. M. Ho, Phys. Rev. B
42, 11276 .
 M. W. Finnis and J. E. Sinclair, Phil. Mag. A 50, 45
 M. Mrovec,
D. Nguyen-Manh,
D. G. Pettifor,
V. Vitek, Phys. Rev. B 69, 094115 .
 P. Pavone et al., Phys. Rev. B 48, 3156 .
 G. A. Slack and S. F. Bartram, J. App. Phys. 46, 89
Supplementary Information
Bispectrum
An arbitrary function ρ deﬁned on the surface of a 4D sphere can be numerically represented using the
hyperspherical harmonic functions U j
m′m(φ, θ, θ0). The hyperspherical harmonics form an orthonormal basis
set thus ρ can be represented as
The expansion coeﬃcients cj
m′m can be calculated via
m′m = ⟨U j
where ⟨.|.⟩denotes the inner product. For clarity, the vectors cj are constructed from the expansion coef-
ﬁcients cj
m′m. A unitary operation ˆR, such as a rotation, acting on ρ transforms the coeﬃcient vectors cj
according to
c′j = Rjcj,
where Rj are unitary matrices, i.e.
 Rj† Rj = I.
The direct product of two rotational matrices, Rj1 and Rj2 can be decomposed into a direct product of
Rj matrices by a unitary transformation
Rj1 ⊗Rj2 =
where the matrix Hj1j2 is the four-dimensional analouge of the Clebsch-Gordan coeﬃcients. In fact, the elements of the matrix are obtained as product of Clebsch-Gordan coeﬃcients: Hlmm′
l1m1l2m2Clm′
The direct product of the coeﬃcient vectors cj1 and cj2 transforms according to the direct product of the
rotational matrices
cj1 ⊗cj2 →
cj1 ⊗cj2 =
cj1 ⊗cj2.
We deﬁne gj1,j2,j—using the fact that Hj1,j2 is unitary—as follows:
gj1,j2,j ≡Hj1,j2cj1 ⊗cj2,
which transforms under rotation as gj,j1,j2 →Rjgj1,j2,j. The cubic rotational invariants, also known as the
bispectrum, can be constructed as
Bj1,j2,j =
 cj† gj1,j2,j.
Finally, we arrive to the expression for the bispectrum elements, computed as
Bj1,j2,j =
j1m1j2m2Cjm′
The truncated version of the bispectrum results in a ﬁnite array, with 4, 23 and 69 elements for Jmax = 1,
3 and 5, respectively.
 
Gaussian Process Regression
Notation and formulae:
Number of raw atomic neighbourhood conﬁgurations
Number of sparse atomic neighbourhood conﬁgurations
bispectrum of nth reference conﬁguration
bispectrum of mth sparse conﬁguration
bispectrum of conﬁguration for which prediction is sought (“test conﬁguration”)
vector of data values at the raw conﬁgurations
Covariance function, the measure of similarity of two conﬁgurations
C(xn, xn′), covariance matrix of raw conﬁgurations
C(xm, xm′), covariance matrix of sparse conﬁgurations
[kn]m = C(xn, xm), covariance matrix of sparse and raw conﬁgurations, CMN = (CNM)T
C(xm, x∗), covariance vector of test and sparse conﬁgurations
Diag(diag(CN −CNMC−1
intrinsic noise of data values (hyperparameter)
CM + CMN(Λ + σ2I)−1CNM, pseudo-covariance matrix of the sparse conﬁgurations
M CMN(Λ + σ2I)−1y, prediction of atomic energy for test conﬁguration
C(x∗, x∗) −kT
M )k∗+ σ2, variance of prediction for test conﬁguration
where diag(A) is the vector of diagonal elements of the matrix A, and Diag(v) is the matrix whose diagonal
elements are the components of vector v and the oﬀ-diagonal elements are zero.
Covariance function (kernel)
We use a Gaussian kernel. This kernel enables us to assign a separate spatial scale hyperparameter (θi) to
each element of the feature vector, therefore the kernel provides a more ﬂexible description than a Gaussian
kernel with a single hyperparameter. The next three equations show how the covariance function is evaluated
if the function values are available for both conﬁgurations, if we have the derivative of the function available
at one conﬁguration, and if the derivatives are given for both conﬁgurations.
C′(xn, xm)
C′′(xn, xm)
Linear combinations
In our case, only the linear combination of atomic energies can be directly observed. We cannot determine
the atomic contributions to the total energy of a system uniquely from an electronic structure calculation.
Similarly, the atomic forces—although they are available from ﬁrst-principles calculations—are not derivatives
of atomic energies, but are sums of derivatives of diﬀerent atomic contributions. It is possible to use a Gaussian
Process to infer the underlying function even if only linear combinations of function values are available. Now
let y is the vector of K observed values (total energies and atomic force components). Let y′ be the vector
of N unobserved values of atomic energies and its derivatives corresponding to the N atomic neighborhood
conﬁgurations. Let the N × K matrix L describe the relationship of the K observations to the N unknown
values. The elements of L are 0s and 1s, and
The covariance of the K observations is then given by
CKK = LTCNNL
Putting it all together
The sparsiﬁcation and the linear combinations are used together to give the ﬁnal expression for the atomic
energy, in such a way that the unobserved values y′ are not needed,
 diag(LT CNNL −LT CNMC−1
(now a K × K diagonal matrix)
CM + CMNL(Λ + σ2I)−1LT CNM
−1 CMNL(Λ + σ2I)−1y
The data: density functional theory
The DFT data was generated using the local density approximation in case of carbon and the PBE generalized
gradient approximation for silicon, germanium, GaN and iron. The electronic Brillouin zone was sampled by
using a Monkhorst-Pack k-point grid, with a k-point spacing of at most 0.3˚A
−1 for insulators and 0.14˚A
for iron. The plane wave cutoﬀwas 350, 300, 300, 350, 500 eV for C, Si, Ge, GaN and Fe respectively and
the energies were extrapolated to correct for the ﬁnite basis set. Ultrasoft pseudopotentials were used with
4 valence electrons for all group IV ions, 3 electrons for Ga, 5 electrons for N and 8 for Fe ions.
Testing the GAP parameters
Figure 1 shows the improvement in the distribution of force errors of the GAP model for diamond as the
cutoﬀradius is increased. Figure 2 shows the same as Jmax is increased, which corresponds to increasing the
spatial resolution of the bispectrum.
Potential for gallium nitride
Figures 3 and 4 show the force errors and the phonon spectrum of a simple GAP model for gallium nitride.
The long range Coulomb interactions of the ions is signiﬁcant in this system, so we augmented the local
energy of the original GAP model by an Ewald sum of ﬁxed charges (+1 for Ga and -1 for N). The Gaussian
Process regression was carried out on forces and energies which were obtained from the DFT calculations by
subtracting this Coulomb contribution. The LO/TO splitting in the phonon spectrum shows that the model
captures the long range character of the ionic interactions correctly.
Figure 1: Force correlation of GAP models for diamond with diﬀerent spatial cutoﬀs.
Force correlation of GAP models for diamond with diﬀerent resolution of representation. The
number of invariants were 4, 23 and 69 for Jmax =1, 3 and 5, respectively.
DFT force / eV/˚A
Force error / eV/˚A
Figure 3: Force errors in GaN of the GAP model augmented with a simple Ewald sum of ﬁxed charges with
reference to DFT forces.
0.25 0.5 0.75
Figure 4: Phonon spectrum of GaN calculated with GAP (solid lines) and PBE-DFT (open squares).