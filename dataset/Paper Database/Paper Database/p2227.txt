Measuring the Class-imbalance Extent of Multi-class
Jonathan Ortigosa-Hern´andez, I˜naki Inza, and Jose A. Lozano
Since many important real-world classiﬁcation problems involve learning from
unbalanced data, the challenging class-imbalance problem has lately received considerable attention in the community.
Most of the methodological contributions
proposed in the literature carry out a set of experiments over a battery of speciﬁc
datasets. In these cases, in order to be able to draw meaningful conclusions from
the experiments, authors often measure the class-imbalance extent of each tested
dataset using imbalance-ratio, i.e. dividing the frequencies of the majority class by
the minority class.
In this paper, we argue that, although imbalance-ratio is an informative measure
for binary problems, it is not adequate for the multi-class scenario due to the fact
that, in that scenario, it groups problems with disparate class-imbalance extents
under the same numerical value. Thus, in order to overcome this drawback, in this
paper, we propose imbalance-degree as a novel and normalised measure which is
capable of properly measuring the class-imbalance extent of a multi-class problem.
Experimental results show that imbalance-degree is more adequate than imbalanceratio since it is more sensitive in reﬂecting the hindrance produced by skewed multiclass distributions to the learning processes.
Introduction
Most of the well-known traditional machine learning techniques are designed to solve
classiﬁcation problems showing reasonably balanced class distributions . However,
this assumption does not always hold in reality. Occasionally, real-world problems have
skewed class distributions and, due to this, they present training datasets where several
classes are represented by an extremely large number of examples, while some others
are represented by only a few. This particular situation is known as the class-imbalance
problem, a.k.a. learning from unbalanced data , and it is considered in the literature
as a major obstacle to building precise classiﬁers: the solutions obtained for problems
showing class-imbalance through the traditional learning techniques are usually biased
towards the most probable classes showing a poor prediction power for the least probable
classes . Thus, in an attempt to overcome this obstacle, hundreds of methodological
solutions have been proposed recently in order to balance the prediction powers for both
the most and the least probable classes.
According to , the proposed solutions can be mainly categorised into the following
three major groups: (i) the development of inbuilt mechanisms , which change the
classiﬁcation strategies to impose a bias toward the minority classes, (ii) the usage of data
sampling methods , which modify the class distribution to change the balance between
the classes, and (iii) the adoption of cost-sensitive learning techniques which assume
higher misclassiﬁcation costs for examples of the minority classes.
Usually, every paper proposed within those categories shares the same experimental
setup: the proposed method is compared against one or several competing methods over a
dozen or so datasets. However, although this experimental setup is reasonable enough to
support an argument that the new method is “as good as” or “better than” the state-ofthe-art, it still leaves many unanswered questions . Besides, it is costly in computing
time . Thus, in order to be able to perform more meaningful analyses, some authors
complement this experimental schema with a study of the inherent properties of the
checked datasets by extracting from them a set of informative measures . By
means of this data characterisation, more solid empirical conclusions may be eﬃciently
extracted: on the one hand, a better understanding of the problem faced may be achieved
since it is a structured manner of investigating and explaining which intrinsic features
of the data are a↵ecting the classiﬁers . On the other hand, the measured data can
be related to the classiﬁer performance so that the applicability and performance of a
classiﬁer based upon the data can be predicted, avoiding a great amount of computing
time .
In the literature, authors often measure the class-imbalance extent. In those works,
imbalance-ratio is the most frequently used summary of the class-imbalance extent due
to its simplicity . It reﬂects the (expected) number of instances of the most probable
class for each instance of the least probable class.
However, in this paper, we state
that whilst it is a very informative summary of the class-imbalance extent for binary
problems, it is not capable of completely and honestly describing the disparity among
the frequencies of more than two classes. In the multi-class scenario, there exists other
classes rather than the most and least probable classes and they are not taken into
account for the calculation of this summary. This may lead to the undesired situation
of characterising multi-class problems with disparate class-imbalance extents using the
same imbalance-ratio.
In order to clarify this drawback, let’s consider the toy example presented in Figure 1;
Imagine that a 3-class problem with an imbalance-ratio of 20 (100 : 5) is provided. This
means that there are 20 examples of the most probable class (c1) for each example of the
least probable class (c3). However, by means of just imbalance-ratio, little knowledge
can be extracted regarding the remaining class c2, i.e. the number of examples of c2 can
vary from 5 to 100, and all these 95 di↵erent possible scenarios share an imbalance-ratio
equal to 20.
As can be easily noticed, the scenario with 100 examples for the second class – Figure
1a –, is far less problematic than having only 5 examples of the second class – Figure 1b
–. While there is only one minority class in the former scenario, we ﬁnd two minority
classes in the latter. So, it can be straightforwardly concluded that imbalance-ratio is not
'()* +,-)./+0
(a) Best unbalanced scenario.
'()* +,-)./+0
(b) Worst unbalanced scenario.
Figure 1: Extreme cases of an unbalanced ternary toy example showing an imbalanceratio of 20.
a proper summary of the class-imbalance extent in the multi-class scenario as it groups
diverse problems with di↵erent class-imbalance extents under the same numerical value.
Thus, in order to bridge this gap, in this paper, we propose a new summary which
is capable of properly shortening the class distributions of both binary and multi-class
classiﬁcation problems into a single value. This measure, which we name imbalancedegree, represents the existing di↵erence between a purely balanced distribution and the
studied unbalanced problem, and it has the following three interesting properties:
1. By means of a single real value in the range [0, K), where K is the number of
classes, it not only summarises the class distribution of a given problem but also
inherently expresses the number of majority and minority classes.
2. Depending on the requirements of the experimental setup and the degree of sensitivity sought, this measure can be instantiated with any common distance between
vectors or divergence between probability distributions.
3. A unique mapping between the class distributions and the numerical value of
imbalance-degree is ensured for problems showing di↵erent numbers of majority
and minority classes. Therefore, diverse problems cannot share a common numerical value as happens with imbalance-ratio.
Experimental results show that imbalance-degree is a more appropriate summary
than imbalance-ratio. In the multi-class framework, the former is not only able of differentiating class distributions than the latter groups with the same value but it also
achieves a greater correlation with the hindrance that skewed class distributions cause
in the learning processes.
The rest of the paper is organised as follows: Section 2 introduces the framework,
notation, and a review of the most-commonly used measures and summaries of the
class distribution. In Section 3, we introduce imbalance-degree as a more informative
measure for the multi-class scenario. After that, Section 4 presents an empirical study
of the adequateness of the proposed measure. Finally, Section 5 sums up the paper.
Problem Formulation and State-of-the-art Measures for the Classimbalance Extent
Let γK be a K-class classiﬁcation problem with a generative model given by the generalised joint probability density function
⇢(x, c) = p(c)⇢(x|c),
where p(c) is a multinomial distribution representing the class probabilities and ⇢(x|c) is
the conditional distribution of the feature space. For convenience, henceforth, we rewrite
the former as ⌘= (⌘1, ⌘2, . . . , ⌘K), where each ⌘i = p(ci) stands for the probability of
each categorical class ci. Also, we denote the special case of equiprobability as e =
(e1, e2, . . . , eK), where 8i, ⌘i = 1/K = ei. Then, depending on the outline of its class
distribution ⌘, every classiﬁcation problem γK can be catalogued into one of the following
groups: (i) γK may be a balanced problem, (ii) an unbalanced problem showing multimajority, or (iii) a multi-minority unbalanced problem. The formal deﬁnitions for these
groups, as expressed in and , are the following:
Deﬁnition 1. A K-class classiﬁcation problem, γK, is balanced if it exhibits a uniform
distribution between its classes. Otherwise, it is considered to be unbalanced. Formally,
γK is balanced () ⌘= e.
Deﬁnition 2. A multi-class classiﬁcation problem (K > 2), γK, shows a multi-majority
class-imbalance if most of the classes have a higher or equal probability than equiprobability, i.e.
γK is multi-majority ()
Deﬁnition 3. An unbalanced classiﬁcation problem, γK with K > 2, shows a multiminority class-imbalance when most of the class probabilities are below the equiprobability. Formally,
γK is multi-minority ()
(E) is the indicator function, 1 if the event E is true, 0 otherwise. Note that Figure
1a and Figure 1b correspond to multi-majority and multi-minority problems respectively,
and that only when facing multi-class problems do Deﬁnition 2 and 3 make sense.
Unfortunately, in most of the real-world cases, the generative model, along with the
real class distribution, is unknown.
Thus, authors must estimate ⌘from a training
dataset D in order to not only classify γK into one of the groups proposed in the deﬁnitions, but also to be capable of using a close approximation of the real class distribution
to properly validate the conclusions exposed in their experimental schemas.
Then, let D = {(x(1), c(1)), . . . , (x(l), c(l))} = {(x(n), c(n))}l
n=1 be deﬁned as a supervised training dataset of size l drawn from the generative function1. There, let the
class labels {c(n)}l
n=1 be i.i.d. random values drawn from ⌘and let each observation
n=1 2 D be also an i.i.d. random value but drawn from ⇢(x|ci). In order to estimate the class distribution ⌘, we deﬁne the empirical distribution ⇣= (⇣1, ⇣2, . . . , ⇣K). ⇣
is a multinomial distribution with K categories, which exhibits the information available
in the dataset about the class distribution of the problem γK. There, each statistic ⇣i
estimates each class probability ⌘i by just determining the frequency of the class ci in
the dataset. Formally, the statistic is deﬁned as follows:
(c(n) = ci).
Unless otherwise stated, henceforth, we only use the estimator ⇣of the class distribution
since having an unknown generative model is the most common scenario.
in the event of knowing the generative model, all the methodologies presented can be
directly used with ⌘by just substituting the empirical class distribution by the real class
distribution in the formulae.
A few measures for the class-imbalance extent of the class distribution using the
empirical class distribution ⇣have been already utilised in the experimental setups of
the state-of-the-art literature: the most simple manner to measure the class-imbalance
extent of a given problem is just to write down the empirical class distribution ,
⇣, or to directly transcribe the occurrences of all the classes in the dataset,
i.e. l = (l1, l2, . . . , lK) s.t. 8i, li = l⇣i.
These descriptions seem to be a good choice due to the fact that they contain all
the information available in the dataset with regards to the class-imbalance extent of
the generative class distribution ⌘. However, analysing them can be quite tedious in
problems with a large number of class values, especially in highly multi-class problems
(K ≥1, 000, ). In those cases, it is very common to ﬁnd unbalanced distributions
among the classes. Additionally, these solutions are also more diﬃcult to read and/or
compare than single value summaries. Therefore, functions d(·) which assign di↵erent
single real numbers to disparate values of ⇣, i.e.
and which are somehow correlated with the hindrance that skewed class distributions cause on learning
algorithms mainly dominate the class-imbalance literature . Regarding the summaries, imbalance-ratio (IR) between the majority and minority classes is, to the best
of our knowledge, the only summary for ⇣used for multi-class problems. It is calculated
by dividing the maximum statistic ⇣i by the minimum. Formally,
IR(⇣) = maxi ⇣i
1Note that we assume that D is i.i.d. from eq. (1). Therefore, in this work, we only focus on the
case that the nature of the class-imbalance is in the probability distribution, not on the case of having
a biased training dataset.
Table 1: Summary of measures for the class-imbalance extent of the class distribution ⌘
used in the literature and our proposal.
distribution
⇣= (⇣1, ⇣2, . . . , ⇣K)
It is the most informative measure.
multi-class
Frequency of
the classes
l = (l1, l2, . . . , lK) s.t. 8i, li =
informative
(equivalent
empirical class distribution).
multi-class
Imbalanceratio
IR(⇣) = max
It is a single value and
Inappropriate
multi-class
injection is lost.
Imbalancedegree
d∆(⇣, e)/d∆(◆m, e) + (m −1)
easily readable summary
appropriate for binary
and multi-class problems.
A total injection can
only be achieved by the
metric/divergence ∆.
It is trivial to prove that IR : ⇣7!
is an injective function for binary problems.
This property makes this summary appropriate for such scenarios due to the fact that
all possible unbalanced scenarios yield to di↵erent IR values and that any ⇣can be
easily recovered from the IR(⇣). However, when the number of classes outnumbers 2,
the injection is lost (as previously shown in the toy example of Figure 1, where multimajority and multi-minority problems share the same numerical value).
This is an
inappropriate characteristic for a summary of the class-imbalance extent since previous
papers have shown that multi-minority problems are harder than multi-majority .
This may imply that IR is not correlated with the hindrance produced by skewed multiclass distributions.
Therefore, it can be concluded that neither of the presented measures (summarised
in Table 1) for the class-imbalance extent are appropriate for multi-class unbalanced
Imbalance-degree
In this section, our aim is to propose a new and more suitable summary for any empirical
class distribution ⇣with K ≥2 which, at least, fulﬁls the following properties: (i) it
must be an easily readable ﬁnite single valued summary of a multinomial distribution
and (ii) it needs to be correlated with the hindrance that highly unbalanced datasets
cause in the learning processes.
Thus, since the class distribution does harm the learning processes as it extremely
diverges from the balanced one , it is immediate to use a distance/similarity function,
d∆(⇣, e), between both the empirical and balanced distributions, ⇣and e, to summarise
the degree of skewness of a classiﬁcation problem γK. Here, ∆stands for any chosen
distance between vectors or divergence between probability distributions which can be
found in the literature.
However, just relying on the direct usage of a distance/similarity function has, for
our purpose, two undesirable properties which may clash with our aim of having an
informative easily readable or comparable summary function:
1. Similar to IR, di↵erent values for di↵erent number of majority/minority classes
cannot be assured. For instance, imagine we use the Kullback-Leibler divergence
 as a summary of two diverse class distributions ⇣(1) = (0.027009, 0.486495, 0.486495)
and ⇣2 = (0.712853, 0.143573, 0.143573). There, both calculi reach the same value:
dKL(⇣(1), e) = dKL(⇣(2), e) = 0.273.
2. Although a measure is always a ﬁnite positive value, it is not necessarily upper
bounded. For example, Kullback-Leibler divergence may be unbounded, and Manhattan and Euclidean distances , in this context, are upper bounded by the
values 2 and 1, respectively.
In order to overcome these drawbacks, we purposely divide the space of class distributions so that we can operate on the distance/similarity function and obtain an adequate
summary: let ZK be deﬁned as the set containing all the possible empirical distributions
⇣of a K-class problem and let ZK
m ⇢ZK, m 2 {0, 1, . . . , K −1} be a subset containing
all the empirical class distributions containing exactly m minority classes. Formally,
⇣2 ZK : m =
Straightaway, this severance of ZK into K di↵erent subsets ZK
m allows us to tackle
both problems:
1. On the one hand, di↵erent values for di↵erent numbers of minority/majority classes
can be directly provided in the summary function by just forcing di↵erent ranges of
values to di↵erent subsets. Here, the range (m −1, m] is assigned to each subset
m in the summary (0 for ZK
2. On the other hand, a common upper bound for each subset, and consequently to
the summary, can also be assured by applying a 0-1 normalisation to the distance
of the empirical class distribution (a range of size 1 has been assigned to each
subset). This is achieved through the division of d∆(⇣, e) by d∆(◆m, e), being ◆m
the distribution in ZK
m most distant to e.
Then, through the application of these amendments on the distance/similarity function, we deﬁne our main proposal as:
Deﬁnition 4. The imbalance-degree (ID) of a multi-class dataset showing an empirical class distribution ⇣is given by
ID(⇣) = d∆(⇣, e)
d∆(◆m, e) + (m −1),
where m is the number of minority classes, d∆is the chosen distance/similarity function
to instantiate ID, and ◆m is the distribution showing exactly m minority classes with the
highest distance to e (arg max⇣2ZK
m d∆(⇣, e)).
In eq. (8), the term m −1 is intentionally added to the normalisation term to ensure
di↵erent values for di↵erent values of m, i.e. ID(⇣) 2 (m−1, m] when ⇣2 ZK
m. Moreover,
in the purely balanced scenario ⇣= e, our proposal ID(e) = 0 due to the fact that,
conventionally, d∆(e, e)/d∆(e, e) = 1.
Empirical Study
In order to determine the appropriateness of ID (over IR) as a summary of the classimbalance extent in the multi-class framework, we deﬁne two di↵erent sets of experiments
to empirically corroborate the following hypotheses:
• H1: While IR has a deﬁcient resolution to summarise the class-imbalance extent
in the multi-class scenario, ID o↵ers a wide variety of high resolution summaries.
• H2: When used on real-world multi-class classiﬁcation problems, ID is more sensitive to the class-imbalance extent than IR. I.e. ID is more accurate than IR in
informing about a poor performance of traditional learning systems.
Since ID can be instantiated with any chosen distance/similarity function, we ﬁrst
introduce the measures used in the experiments: from the metrics in the vector space
 , Manhattan2, Euclidean and Chebyshev distances are chosen.
Together, the fdivergences , the most utilised measures for probability distributions, are also included.
Within the latter group, we introduce Kullback-Leibler divergence , Hellinger 
(closely related to, although di↵erent from, the Bhattacharyya distance ) and total
variation distances, and χ2-divergence . These measures are mathematically deﬁned
in Table 2.
Additionally, in order to use eq. (8), the furthest distribution ◆m = (◆1, ◆2, . . . , ◆K)
to e must be calculated for every instantiation and every subset ZK
m. Opportunely, this
class distribution coincides for all the considered measures and for all values of m. It
satisﬁes that
(◆i = 0) = m ^
= K −m −1,
i.e. the furthest distribution is composed of (i) m minority classes with zero probability,
(ii) K −m −1 (all but one) majority classes with probability 1/K, and (iii) a majority
class with the remaining probability 1−(K −m−1)/K. This distribution always shows
the lowest entropy in the subset ZK
m, whilst the balanced setting e corresponds to
the distribution with the highest entropy in Z. Note that, by symmetry, there may be
2Manhattan distance has been left out of the experimentation due to the fact that, for our purposes,
it is equivalent to total variation distance for any K ≥2.
!"#$%$&'(-)(*+(( ,-+ $./-0 (1(%%2&*(+ )20/$&'()
Figure 2: Calculating ID using the Hellinger distance for the dataset autos (K = 6,
IR = 16, IDHE = 2.44).
up to K! di↵erent furthest distributions ◆m. Fortunately, ID(⇣) is not a↵ected by an
arbitrary choice of ◆since the entropy, H(◆m), and distance values, d∆(◆m, e), remain
equal for all furthest distributions.
In order illustrate calculation of ID using the distance/similarity functions considered, Figure 2 shows, in a bar chart, an example to instantiate ID using the Hellinger
distance, dHE(⇣, e), on the UCI dataset called autos . The numbers above the black
bars represent the value of each normaliser d∆(◆m, e) for all possible scenarios of m of
minority classes in a 6-class problem. Since autos has 3 (out of 6) minority classes and
dHE(⇣, e) = 0.25, the problem has a (normalised) ID of 2.44 (0.25/0.58 = 0.44 plus
Study 1: Resolution and Diverseness of Imbalance-degree
The resolution of a measure is the smallest change which can be quantiﬁed. As previously
put forward, IR cannot be considered as a measure which has a satisfactory resolution
for multi-class problems; it only changes based on either the most or the least probable
classes. In the toy example, for instance, it groups 95 di↵erent class distributions using
the value IR = 20.
Thus, in order to corroborate the ﬁrst hypothesis, those 95 scenarios are used to not
only show that ID is capable of assigning diverse and reasonable values to them, but also
to study the behaviours of the di↵erent instantiations of ID. Consequently, Figure 3 plots
the values of ID for the indicated 95 di↵erent frequency scenarios, i.e. l = (100, l2, 5),
where l2 = {5, . . . , 100}, from the toy problem.
The abscissa shows the number l2
of instances of the second class and the ordinate shows the value of ID. From Table
2, Euclidean distance (IDEU), Kullback-Leibler divergence (IDKL), Hellinger distance
(IDHE), total variation distance (IDTV ) and chi-square divergence (IDCS) are plotted.
Table 2: Mathematical formulae for the distance/similarity functions used to instantiate
ID in the empirical studies.
Distance/Similarity Function
Metrics in the vector space
Euclidean distance
Chebyshev distance
f-divergences
Kullback-Leibler divergence
Hellinger distance
Total variation distance
Chi-square divergence
Note that Chevbysev distance is not included3.
Results show that ID is capable of di↵erentiating each and every di↵erent scenario
that IR groups with the value 20.
Moreover, it can be seen that ID instantiations
behave di↵erently as result of the diversity of their distance/similarity functions: whilst
all the instantiations share a similar monotonically decreasing shape up to the limiting
point where the number the m changes from 2 to 1 (l2 = 53), above that limit, two
di↵erent groups of instantiations can be perceived. On the one hand, IDEU, IDKL and
IDCS show a convex shape since they descent down to a minimum and then slightly
On the other hand, IDHE and IDTV show a quasi-linear behaviour which
starts increasing soon after reaching the limiting point. Thus, it can be straightforwardly
concluded that there might be instantiations of ID which are more adequate to summarise
the class-imbalance extent than others. Seemingly, the latter group of instantiations
(IDHE and IDTV ) are more appropriate as they reﬂect the increased intricacy of the
classiﬁcation problem above the limiting point. When l2 > 53, the probability of the
minority class c3 distance itself from the equiprobability causing an increase in the
intricacy of the classiﬁcation problem. In Section 4.2, we also deal with this issue by
empirically determining which ID instantiations are more adequate summaries in realworld multi-class datasets. Finally, we believe that, in practise, the above mentioned
diversity may also be potentially exploited to adapt ID to di↵erent requirements and
constraints resulting from real-world unbalanced problems.
3It holds that instantiations of ID using Chevbysev and total variation distances are equivalent for
the case K = 3.
!"#$"%$&' &( $)*"+"',--.-/#-- $' +! (%&0 1#&*+-))
Figure 3: The variation of ID∆in all the 95 possible scenarios (l2 = {5, . . . , 100}) of the
toy problem of Figure 1. For these scenarios, IR = 20.
Study 2: Sensitivity and Validity of Imbalance-degree
A measure is sensitive to recognise a given set of events if it is capable of valuing them
di↵erently. Speciﬁcally, we can consider a summary of the class-imbalance extent to
be sensitive to recognise the hindrance that highly unbalanced data produce in the
traditional learning systems if it is correlated with the performance of those learning
systems. Thus, to determine which instantiation of ID is more sensitive to the exposed
hindrance than IR, in this section, the following experiment is carried out:
A database containing the 15 unbalanced multi-class datasets recommended in the
key work of is assembled and the value of each summary presented in this paper (see
Table 2) is calculated for each dataset. Their values, along with some main characteristics of the datasets, are presented in Table 3. There, each row corresponds to a dataset
and each column stands for a characteristic (name, features and number of classes) or a
summary (empirical class distribution, number of occurrences, IR and IDs). Afterwards,
each dataset is used to feed a representative learning algorithm from the traditional major learning paradigms . Speciﬁcally, for each problem, a di↵erent classiﬁer is learnt
using 5 di↵erent popular supervised algorithms4: C4.5 (Decision trees), RIPPER (Decision rules), Neural Networks (Connectionism), Na¨ıve Bayes (Probabilistic), and SVM
(Statistical learning). In order to assess the performance of each learnt classiﬁer, three
di↵erent performance scores which are highly recommended for multi-class unbalanced
problems are used : the arithmetic mean among the recall of the classes (A), the
geometric mean among the recalls (G), and the minimum recall obtained (min). In order
to obtain the values of these performance scores for each dataset, we estimate them using
10 ⇥10 fold cross-validation5.
4In this experimentation, all learning and error estimation tasks have been performed using the
software Weka 3 .
5These results can be downloaded, along with the source code.
Table 3: Characteristics of the studied unbalanced datasets and the value of the summaries introduced in this paper for
each dataset.
Empirical class distribution
Occurrences
0.02/0.13/0.30/0.29/0.18/0.08
3/20/48/46/29/13
0.46/0.08/0.46
288/49/288
contraceptive
0.43/0.23/0.35
629/333/511
dermatology
0.31/0.17/0.20/0.13/0.14/0.05
112/61/72/49/52/20
0.43/0.23/0.15/0.10/
143/77/52/35/
0.06/0.01/0.01/0.01
0.33/0.36/0.08/0.00/0.06/0.04/0.14
70/76/17/0/13/9/29
hayes-roth
0.39/0.39/0.23
lymphography
0.01/0.55/0.41/0.03
new-thyroid
0.70/0.16/0.14
pageblocks
0.90/0.06/0.01/0.02/0.01
492/33/8/12/3
0.10/0.10/0.10/0.10/0.10/
115/114/114/106/114/
0.10/0.10/0.10/0.10/0.10
106/105/115/105/106
0.78/0.00/0.00/0.16/0.06/0.00/0.00
1706/2/6/338/123/0/0
0.02/0.05/0.93
0.33/0.40/0.27
0.16/0.29/0.31/0.03/0.03/
244/429/463/44/51/
0.11/0.02/0.02/0.01/0.00
163/35/30/20/5
Table 4: Pearson correlation coeﬃcient (⇥100) among the performance of the major learning paradigms on the datasets of
Table 3 and the studied summaries.
Decision trees
Decision rules
Connectionism
Probabilistic
Statistical learn.
(Neural Net.)
(Na¨ıve Bayes)
IDEU −(m + 1)
IDCH −(m + 1)
IDKL −(m + 1)
IDHE −(m + 1)
IDT V −(m + 1)
IDCS −(m + 1)
(a) CDD for the arithmetic mean among the
recalls, A.
(b) CDD for the geometric mean among the
recalls, G.
(c) CDD for minimum recall obtained, min.
Figure 4: Pearson correlation ranking between the performance of the supervised learning paradigms on the studied datasets and the summaries, ↵= 0.05.
Then, the correlation between the estimated values for the performance scores and
the summaries, IR and ID, are determined using the Pearson product-moment correlation
coeﬃcient so that H2 may be checked. Since a licit calculation of the correlation
requires an ideal scenario with a ﬁxed number of minority/majority classes, we emulate
this requirement by subtracting (m −1) from the ID value before the calculation so
that all considered classiﬁcation problems are normalised in the same range . The
results are presented in Table 4; rows represent the summaries and columns represent the
estimated values for each score in each learning paradigm. Since the utilised scores assign
higher values to better performance, an adequate summary is expected to have a negative
correlation; the lowest the correlation, the better the sensitivity. We conclude from the
results that summaries are, in general, negatively correlated with the performance of the
classiﬁers, and that instantiations of ID are more sensitive than IR as the former obtain
a lower negative correlation. The best results (highlighted in Table 4) are obtained by
IDTV and IDHE.
Finally, to determine if there are summaries signiﬁcantly more sensitive to the hindrance produced by skewed class distributions, a statistical hypothesis testing procedure
is performed: Friedman test with Sha↵er’s static post-hoc with ↵= 0.05 . The
test results are represented by means of critical di↵erence diagrams (CDD) , which
show, in a numbered line, the arithmetic mean of the ranks of the correlation between
each summary and the estimation of each score in the database. If there is no statistically signiﬁcant di↵erence between two summaries, they are connected in the diagram
by a straight grey line. Figures 4a, 4b, and 4 show the CDD for the Pearson correlation
between the summaries and A, G and min, respectively. Results conﬁrm the second
hypothesis, in all rankings, IR shows the worst behaviour and signiﬁcant di↵erences are
found between IR and other instantiations of ID for the performance scores. Moreover,
they also show that instantiating ID using either Hellinger or total variation (Manhattan)
distances produces signiﬁcant robust summaries of the class-imbalance extent.
Authors often measure the class-imbalance extent in their experimental schemas when
there is a reasonable suspicion of having unbalanced problems in the checked database.
Up to now, the most utilised summary of the class-imbalance extent of a dataset was the
imbalance-ratio, i.e. the (expected) number of instances of the most probable class for
each instance of the least probable class. Although it is a powerful measure for binary
problems, in this paper, we prove that it is a suboptimal summary for the multi-class
scenario. For that reason, we propose a new more adequate and robust summary of
the class-imbalance extent to deal with multiple classes, named imbalance-degree. It has
three interesting properties: (i) it is a single easy-readable real value in the range [0, K),
where K is the number of classes. (ii) Depending on the requirements of the sensitivity
sought in the tackled problem, it can be instantiated by any chosen metric or divergence.
(iii) It is an injective function for di↵erent class distributions showing di↵erent numbers
of majority/minority classes. Empirical results show that imbalance-degree has a higher
resolution and is more sensitive to express the hindrance that skewed class distributions
cause in the traditional supervised algorithms than imbalance-ratio.
Additionally, it
can also be concluded that either Hellinger, total variation or Manhattan distances
are recommended distance/similarity functions to instantiate our proposal, imbalancedegree.
This work can be extended in several ways.
For example, only 8 di↵erent distance/similarity functions over 15 datasets are used in this paper.
A more exhaustive analysis can be carried out using a larger number of distance/similarity functions
 over a larger set of unbalanced problems in order to statistically determine which
functions behave di↵erently and are recommended for highly di↵erent class-imbalanced
scenarios.
Another straightforward future path to this research can be a study on the variation
of the correlation between ID and the performance of the classiﬁers when class-imbalance
techniques, such as SMOTE , are used. This could be a step forward in determining
which intrinsic features of the data are a↵ecting the classiﬁers , and whether the
performance of a classiﬁer can be predicted based upon the available data . However,
note that, although the negative correlation between ID and the performance is expected
to decrease as long as the class-imbalance techniques alleviate the hindering e↵ect of the
class distribution, there might exist other hindering aspects which may harm the
performance of the classiﬁers.
Acknowledgments
This work is partially supported by the Basque Government (IT609-13 and Elkartek
BID3A) and the Spanish Ministry of Economy and Competitiveness (TIN2013-41272P).
Moreover, Jonathan Ortigosa-Hern´andez is partly ﬁnanced by the Ministry of Science
and Innovation (MEC-FPU AP2008-00766) and Jose A. Lozano by both the Basque
Government and the Spanish Ministry of Economy and
Competitiveness .