Few-Shot Image Recognition by Predicting Parameters from Activations
Siyuan Qiao1
Chenxi Liu1
Wei Shen1,2
Alan Yuille1
Johns Hopkins University1
Shanghai University2
{siyuan.qiao, cxliu, alan.yuille}@jhu.edu
 
In this paper, we are interested in the few-shot learning
problem. In particular, we focus on a challenging scenario
where the number of categories is large and the number of
examples per novel category is very limited, e.g. 1, 2, or 3.
Motivated by the close relationship between the parameters
and the activations in a neural network associated with the
same category, we propose a novel method that can adapt
a pre-trained neural network to novel categories by directly
predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast
inference is realized by a single forward pass. We evaluate
our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classiﬁcation accuracy on novel categories by a signiﬁcant margin
while keeping comparable performance on the large-scale
categories. We also test our method on the MiniImageNet
dataset and it strongly outperforms the previous state-ofthe-art methods.
1. Introduction
Recent years have witnessed rapid advances in deep
learning , with a particular example being visual recognition on large-scale image datasets, e.g., ImageNet . Despite their great performances on benchmark datasets, the machines exhibit clear difference with
people in the way they learn concepts. Deep learning methods typically require huge amounts of supervised training
data per concept, and the learning process could take days
using specialized hardware, i.e. GPUs. In contrast, children
are known to be able to learn novel visual concepts almost
effortlessly with a few examples after they have accumulated enough past knowledge . This phenomenon motivates computer vision research on the problem of few-shot
learning, i.e., the task to learn novel concepts from only a
few examples for each category .
Formally, in the few-shot learning problem ,
we are provided with a large-scale set Dlarge with categories
Clarge and a few-shot set Dfew with categories Cfew that do not
overlap with Clarge. Dlarge has sufﬁcient training samples for
each category whereas Dfew has only a few examples (< 6
in this paper). The goal is to achieve good classiﬁcation
performances, either on Dfew or on both Dfew and Dlarge.
We argue that a good classiﬁer should have the following
properties: (1) It achieves reasonable performance on Cfew.
(2) Adapting to Cfew does not degrade the performance on
Clarge signiﬁcantly (if any). (3) It is fast in inference and
adapts to few-shot categories with little or zero training, i.e.,
an efﬁcient lifelong learning system .
Both parametric and non-parametric methods have been
proposed for the few-shot learning problem. However, due
to the limited number of samples in Dfew and the imbalance between Dlarge and Dfew, parametric models usually
fail to learn well from the training samples . On the
other hand, many non-parametric approaches such as nearest neighbors can adapt to the novel concepts easily without
severely forgetting the original classes. But this requires
careful designs of the distance metrics , which can be
difﬁcult and sometimes empirical. To remedy this, some
previous work instead adapts feature representation to the
metrics by using siamese networks . As we will
show later through experiments, these methods do not fully
satisfy the properties mentioned above.
Activations
Figure 1: Illustration of pre-training on Dlarge (black) and
few-shot novel category adaptation to Dfew (green). The
green circles are the novel categories, and the green lines
represent the unknown parameters for categories in Cfew.
In this paper, we present an approach that meets the desired properties well. Our method starts with a pre-trained
deep neural network on Dlarge. The ﬁnal classiﬁcation layers
 
Figure 2: Our motivation: t-SNE results on the average activations ¯ay of each category before the fully connected layer
of a 50-layer ResNet pre-trained on Dlarge from ImageNet (left) and the parameters wy of each category in the last
fully connected layer (right). Each point represents a category. Highlighted points with the same color and shape correspond
to the same category. Circles are mammals, triangles are birds, diamonds are buses, and squares are home appliances.
(the fully connected layer and the softmax layer) are shown
in Figure 1. We use wy ∈Rn to denote the parameters for
category y in the fully connected layer, and use a(x) ∈Rn
to denote the activations before the fully connected layer
of an image x.
Training on Dlarge is standard; the real
challenge is how to re-parameterize the last fully connected
layer to include the novel categories under the few-shot constraints, i.e., for each category in Cfew we have only a few
examples. Our proposed method addresses this challenge
by directly predicting the parameters wy (in the fully connected layer) using the activations belonging to that category, i.e. Ay = {a(x)|x ∈Dlarge ∪Dfew, Y (x) = y}, where
Y (·) denotes the category of the image.
This parameter predictor stems from the tight relationship between the parameters and activations. Intuitively in
the last fully connected layer, we want wy · ay to be large,
for all ay ∈Ay. Let ¯ay ∈Rn be the mean of the activations in Ay. Since it is known that the activations of images
in the same category are spatially clustered together , a
reasonable choice of wy is to align with ¯ay in order to maximize the inner product, and this argument holds true for all
y. To verify this intuition, we use t-SNE to visualize
the neighbor embeddings of the activation statistic ¯ay and
the parameters wy for each category of a pre-trained deep
neural network, as shown in Figure 2. Comparing them and
we observe a high similarity in both the local and the global
structures. More importantly, the semantic structures 
are also preserved in both activations and parameters, indicating a promising generalizability to unseen categories.
These results suggest the existence of a categoryagnostic mapping from the activations to the parameters
given a good feature extractor a(·). In our work, we parameterize this mapping with a feedforward network that is
learned by back-propagation. This mapping, once learned,
is used to predict parameters for both Cfew and Clarge.
We evaluate our method on two datasets. The ﬁrst one
is MiniImageNet , a simpliﬁed subset of ImageNet
ILSVRC 2015 , in which Clarge has 80 categories and
Cfew has 20 categories. Each category has 600 images of
size 84 × 84. This small dataset is the benchmark for natural images that the previous few-shot learning methods are
evaluated on. However, this benchmark only reports the
performances on Dfew, and the accuracy is evaluated under 5-way test, i.e., to predict the correct category from
only 5 category candidates. In this paper, we will take a
step forward by evaluating our method on the full ILSVRC
2015 , which has 1000 categories. We split the categories into two sets where Clarge has 900 and Cfew has the rest
100. The methods will be evaluated under 1000-way test
on both Dlarge and Dfew. This is a setting that is considerably larger than what has been experimented in the few-shot
learning before. We compare our method with the previous
work and show state-of-the-art performances.
The rest of the paper is organized as follows: §2 deﬁnes
and explains our model, §3 presents the related work, §4
shows the experimental results, and §5 concludes the paper.
The key component of our approach is the categoryagnostic parameter predictor φ : ¯ay →wy (Figure 3). More
generally, we could allow the input to φ to be a statistic representing the activations of category y. Note that we use
the same mapping function for all categories y ∈Clarge, because we believe the activations and the parameters have
similar local and global structure in their respective space.
Once this mapping has been learned on Dlarge, because of
this structure-preserving property, we expect it to generalize to categories in Cfew.
Activation Statistics of a Pug
Parameter Predictor ϕ
Parameter Predictor ϕ
Parameter Predictor ϕ
Parameters for Pug
Parameters for Jay
Parameters for Hen
Activations
Activation Statistics of a Jay
Activation Statistics of a Hen
Figure 3: Building the fully connected layer by parameter prediction from activation statistics.
2.1. Learning Parameter Predictor
Since our ﬁnal goal is to do classiﬁcation, we learn φ
from the classiﬁcation supervision.
Speciﬁcally, we can
learn φ from Dlarge by minimizing the classiﬁcation loss
(with a regularizer ||φ||) deﬁned by
(y,x)∈Dlarge
−φ (¯ay) a(x) + log
+ λ||φ||
Eq. 1 models the parameter prediction for categories
y ∈Clarge. However, for the few-shot set Cfew, each category only has a few activations, whose mean value is the
activation itself when each category has only one sample.
To model this few-shot setting in the large-scale training
on Dlarge, we allow both the individual activations and the
mean activation to represent a category.
Concretely, let
sy ∈Ay ∪¯ay be a statistic for category y. Let Slarge denote
a statistic set {s1, ..., s|Clarge|} with one for each category in
Clarge. We sample activations sy for each category y from
Ay ∪¯ay with a probability pmean to use ¯ay and 1 −pmean to
sample uniformly from Ay. Now, we learn φ to minimize
the loss deﬁned by
(y,x)∈Dlarge
−φ (sy) a(x) + log
+ λ||φ||
2.2. Inference
During inference we include Cfew, which calls for a
statistic set for all categories S = {s1, ..., s|C|}, where
C = Clarge ∪Cfew. Each statistic set S can generate a set of
parameters {φ(s1), ..., φ(s|C|)} that can be used for building a classiﬁer on C. Since we have more than one possible
set S from the dataset D = Dlarge ∪Dfew, we can do classi-
ﬁcation based on all the possible S. Formally, we compute
the probability of x being in category y by
P(y|x) = eES[φ(sy)a(x)]/
φ(sy′ )a(x)
However, classifying images with the above equation is
time-consuming since it computes the expectations over the
entire space of S which is exponentially large. We show in
the following that if we assume φ to be a linear mapping,
then this expectation can be computed efﬁciently.
In the linear case φ is a matrix Φ. The predicted parameter for category y is
ˆwy = Φ · sy
The inner product of x before the softmax function for category y is
h(sy, a(x)) = ˆwy · a(x) = Φ · sy · a(x)
If a(x) and sy are normalized, then by setting Φ as the identity matrix, h(sy, a(x)) is equivalent to the cosine similarity between sy and a(x). Essentially, by learning Φ, we
are learning a more general similarity metric on the activations a(x) by capturing correlations between different dimensions of the activations. We will show more comparisons between the learned Φ and identity matrix in §4.1.
Because of the linearity of φ, the probability of x being
in category y simpliﬁes to
P(y|x) = ea(x)·φ(ES[sy])/
ea(x)·φ(ES
= ea(x)·Φ·ES[sy]/
ea(x)·Φ·ES
Now ES[sy] can be pre-computed which is efﬁcient. Adapting to novel categories only requires updating the corresponding ES[sy]. Although it is ideal to keep the linearity
of φ to reduce the amount of computation, introducing nonlinearity could potentially improve the performance.
keep the efﬁciency, we still push in the expectation and approximate Eq. 3 as in Eq. 6.
When adding categories y ∈Cfew, the estimate of ES[sy]
may not be reliable since the number of samples is small.
Besides, Eq. 2 models the sampling from one-shot and
mean activations. Therefore, we take a mixed strategy for
Activations
Statistic Set
Training Activation Set
Parameter Set
Classification Loss
Fully Connected and SoftMax
Figure 4: Illustration of the novel category adaption (a) and the training strategies for parameter predictor φ (b). (b): red and
solid arrows show the feedforward data ﬂow, while blue and dashed arrow shows the backward gradient ﬂow.
parameter prediction, i.e., we use ES[sy] to predict parameters for category y ∈Clarge, but for Cfew we treat each sample as a newly added category, as shown in Figure 4a. For
each novel category in Cfew, we compute the maximal response of the activation of the test image to the parameter
set predicted from each activation in the statistic set of the
corresponding novel category in Cfew. We use them as the
inputs to the SoftMax layer to compute the probabilities.
2.3. Training Strategy
The objective of training is to ﬁnd φ that minimizes
Eq. 2. There are many methods to do this. We approach
this by using stochastic gradient decent with weight decay
and momentum. Figure 4b demonstrates the training strategy of the parameter predictor φ. We train φ on Dlarge with
categories Clarge. For each batch of the training data, we
sample |Clarge| statistics sy from Ay ∪¯ay to build a statistic
set S with one for each category y in Clarge. Next, we sample a training activation set T from Dlarge with one for each
category in Clarge. In total, we sample 2|Clarge| activations.
The activations in the statistic sets are fed to φ to generate
parameters for the fully connected layer. With the predicted
parameters for each category in Clarge, the training activation
set then is used to evaluate their effectiveness by classifying
the training activations. At last, we compute the classiﬁcation loss with respect to the ground truth, based on which
we calculate the gradients and back-propagate them in the
path shown in Figure 4b. After the gradient ﬂow passes
through φ, we update φ according to the gradients.
2.4. Implementation Details
Full ImageNet Dataset
Our major experiments are conducted on ILSVRC 2015 . ILSVRC 2015 is a largescale image dataset with 1000 categories, each of which
has about 1300 images for training, and 50 images for validation. For the purpose of studying both the large-scale
and the few-shot settings at the same time, ILSVRC 2015
is split to two sets by the categories. The training data from
900 categories are collected into Dlarge, while the rest 100
categories are gathered as set Dfew.
We ﬁrst train a 50-layer ResNet on Dlarge. We use
the outputs of the global average pooling layer as the activation a(x) of an image x. For efﬁciency, we compute
the activation a(x) for each image x before the experiments
as well as the mean activations ¯ay. Following the training
strategy shown in §2.3, for each batch, we sample 900 activations as the statistic set and 900 activations as the training
activation set. We compute the parameters using the statistic
set, and copy the parameters into the fully connected layer.
Then, we feed the training activations into the fully connected layer, calculate the loss and back-propagate the gradients. Next, we redirect the gradient ﬂow into φ. Finally,
we update φ using stochastic gradient descent. The learning
rate is set to 0.001. The weight decay is set to 0.0005 and
the momentum is set to 0.9. We train φ on Dlarge for 300
epochs, each of which has 250 batches. pmean is set to 0.9.
For the parameter predictor, we implement three different φ: φ1, φ2 and φ2∗. φ1 is a one-layer fully connected
model. φ2 is deﬁned as a sequential network with two fully
connected layers in which each maps from 2048 dimensional features to 2048 dimensional features and the ﬁrst
one is followed by a ReLU non-linearity layer . The
ﬁnal outputs are normalized to unity in order to speed up
training and ensure generalizability. By introducing nonlinearity, we observe slight improvements on the accuracies
for both Clarge and Cfew. To demonstrate the effect of minimizing Eq. 2 instead of Eq. 1, we train another φ2∗which
has the same architecture with φ2 but minimizes Eq. 1. As
we will show later through experiments, φ2∗has strong bias
towards Clarge.
MiniImageNet Dataset
For comparison purposes, we
also test our method on MiniImageNet dataset , a simpliﬁed subset of ILSVRC 2015. This dataset has 80 categories for Dlarge and 20 categories for Dfew. Each category
has 600 images. Each image is of size 84 × 84. For the
fairness of comparisons, we train two convolutional neural
networks to get the activation functions a(·). The ﬁrst one
is the same as that of Matching Network , and the second one is a wide residual network . We train the wide
residual network WRN-28-10 on Dlarge, following its
conﬁguration for CIFAR-100 dataset . There are some
minor modiﬁcations to the network architecture as the input
size is different. To follow the architecture, the input size is
set to 80×80. The images will be rescaled to this size before
training and evaluation. There will be 3 times of downsampling rather than 2 times as for CIFAR dataset. The training
process follows WRN-28-10 . We also use the output
of the global average pooling layer as the activation a(x) of
an image x. For the parameter predictor φ, we train it by
following the settings of φ2 for the full ImageNet dataset
except that now the dimension corresponds to the output of
the activations of the convolutional neural networks. The
two architectures will be detailed in the appendix.
3. Related Work
3.1. Large-Scale Image Recognition
We have witnessed an evolution of image datasets over
the last few decades. The sizes of the early datasets are
relatively small. Each dataset usually collects images on
the order of tens of thousands. Representative datasets include Caltech-101 , Caltech-256 , Pascal VOC ,
and CIFAR-10/100 . Nowadays, large-scale datasets
are available with millions of detailed image annotations,
e.g. ImageNet and MS COCO . With datasets of
this scale, machine learning methods that have large capacity start to prosper, and the most successful ones are convolutional neural network based .
3.2. Few-Shot Image Recognition
Unlike large-scale image recognition, the research on
few-shot learning has received limited attention from the
community due to its inherent difﬁculty, thus is still at an
early stage of development. As an early attempt, Fei-Fei
et al. proposed a variational Bayesian framework for oneshot image classiﬁcation . A method called Hierarchical Bayesian Program Learning was later proposed
to speciﬁcally approach the one-shot problem on character
recognition by a generative model. On the same character
recognition task, Koch et al. developed a siamese convolutional network to learn the representation from the
dataset and modeled the few-shot learning as a veriﬁcation
task. Later, Matching Network was proposed to approach the few-shot learning task by modeling the problem as a k-way m-shot image retrieval problem using attention and memory models.
Following this work, Ravi
and Larochelle proposed a LSTM-based meta-learner optimizer , and Chelsea et al. proposed a model-agnostic
meta learning method . Although they show state-of-theart performances on their few-shot learning tasks, they are
not ﬂexible for both large-scale and few-shot learning since
k and m are ﬁxed in their architectures. We will compare
ours with these methods on their tasks for fair comparisons.
3.3. Uniﬁed Approach
Learning a metric then using nearest neighbor is applicable but not necessarily optimal to the uniﬁed
problem of large-scale and few-shot learning since it is possible to train a better model on the large-scale part of the
dataset using the methods in §3.1. Mao et al. proposed
a method called Learning like a Child speciﬁcally for
fast novel visual concept learning using hundreds of examples per category while keeping the original performance.
However, this method is less effective when the training examples are extremely insufﬁcient, e.g. < 6 in this paper.
4. Results
4.1. Full ImageNet Classiﬁcation
In this section we describe our experiments and compare our approach with other strong baseline methods. As
stated in §1, there are three aspects to consider in evaluating
a method: (1) its performance on the few-shot set Dfew, (2)
its performance on the large-scale set Dlarge, and (3) its computation overhead of adding novel categories and the complexity of image inference. In the following paragraphs, we
will cover the settings of the baseline methods, compare the
performances on the large-scale and the few-shot sets, and
discuss their efﬁciencies.
Baseline Methods
The baseline methods must be applicable to both large-scale and few-shot learning settings. We compare our method with a ﬁne-tuned 50-layer
ResNet , Learning like a Child with a pre-trained
50-layer ResNet as the starting network, Siamese-Triplet
Network using three 50-layer ResNets with shared
parameters, and the nearest neighbor using the pre-trained
50-layer ResNet convolutional features. We will elaborate
individually on how to train and use them.
As mentioned in §2.4, we ﬁrst train a 900-category classiﬁer on Dlarge. We will build other baseline methods using this classiﬁer as the staring point. For convenience, we
denote this classiﬁer as Rpt
large, where pt stands for “pretrained”. Next, we add the novel categories Cfew to each
method. For the 50-layer ResNet, we ﬁne tune Rpt
large with
the newly added images by extending the fully connected
layer to generate 1000 classiﬁcation outputs. Note that we
will limit the number of training samples of Cfew for the
few-shot setting. For Learning like a Child, however, we
ﬁx the layers before the global average pooling layer, extend the fully connected layer to include 1000 classes, and
Top-1 Clarge
Top-5 Clarge
Top-1 Cfew
Top-5 Cfew
NN + Cosine
NN + Cosine
NN + Cosine
Triplet Network 
Triplet Network 
Triplet Network 
Fine-Tuned ResNet 
Learning like a Child 
NN + Cosine
NN + Cosine
NN + Cosine
Triplet Network 
Triplet Network 
Triplet Network 
Fine-Tuned ResNet 
Learning like a Child 
NN + Cosine
NN + Cosine
NN + Cosine
Triplet Network 
Triplet Network 
Triplet Network 
Fine-Tuned ResNet 
Learning like a Child 
Table 1: Comparing 1000-way accuracies with feature extractor a(·) pre-trained on Dlarge. For different Dfew settings, red:
the best few-shot accuracy, and blue: the second best.
only update the parameters for Cfew in the last classiﬁcation
layer. Since we have the full access to Dlarge, we do not
need Baseline Probability Fixation . The nearest neighbor with cosine distance can be directly used for both tasks
given the pre-trained deep features.
The other method we compare is Siamese-Triplet Network . Siamese network is proposed to approach
the few-shot learning problem on Omniglot dataset .
In our experiments, we ﬁnd that its variant Triplet Network is more effective since it learns feature representation from relative distances between positive and negative pairs instead of directly doing binary classiﬁcation from
the feature distance. Therefore, we use the Triplet Network
from on the few-shot learning problem, and upgrade its
body net to the pre-trained Rpt
large. We use cosine distance as
the distance metric and ﬁne-tune the Triplet Network. For
inference, we use nearest neighbor with cosine distance. We
use some techniques to improve the speed, which will be
discussed later in the efﬁciency analysis.
Few-Shot Accuracies
We ﬁrst investigate the few-shot
learning setting where we only have several training examples for Cfew. Speciﬁcally, we study the performances of
different methods when Dfew has for each category 1, 2, and
3 samples. It is worth noting that our task is much harder
than the previously studied few-shot learning: we are evaluating the top predictions out of 1000 candidate categories,
i.e., 1000-way accuracies while previous work is mostly interested in 5-way or 20-way accuracies .
With the pre-trained Rpt
large, the training samples in Dfew
are like invaders to the activation space for Clarge. Intuitively,
there will be a trade-off between the performances on Clarge
and Cfew. This is true especially for non-parametric methods. Table 1 shows the performances on the validation set of
ILSVRC 2015 . The second column is the percentage
of data of Dlarge in use, and the third column is the number
of samples used for each category in Dfew. Note that ﬁnetuned ResNet and Learning like a Child require
ﬁne-tuning while others do not.
Triplet Network is designed to do few-shot image inference by learning feature representations that adapt to the
Top-1 Clarge
Top-5 Clarge
Top-1 Cfew
Top-5 Cfew
Table 2: Oracle 1000-way accuracies of the feature extractor a(·) pre-trained on Dlarge.
chosen distance metric. It has better performance on Cfew
compared with the ﬁne-tuned ResNet and Learning like a
Child when the percentage of Dlarge in use is low. However, its accuracies on Clarge are sacriﬁced a lot in order to
favor few-shot accuracies. We also note that if full category supervision is provided, the activations of training a
classiﬁer do better than that of training a Triplet Network.
We speculate that this is due to the less supervision of training a Triplet Network which uses losses based on ﬁxed distance preferences. Fine-tuning and Learning like a Child are
training based, thus are able to keep the high accuracies on
Dlarge, but perform badly on Dfew which does not have suf-
ﬁcient data for training. Compared with them, our method
shows state-of-the-art accuracies on Cfew without compromising too much the performances on Clarge.
Table 1 also compares φ2 and φ2∗, which are trained to
minimize Eq. 2 and Eq. 1, respectively. Since during training φ2∗only mean activations are sampled, it shows a bias
towards Clarge. However, it still outperforms other baseline
methods on Cfew. In short, modeling using Eq. 2 and Eq. 1
shows a tradeoff between Clarge and Cfew.
Here we explore the upper bound performance
on Cfew. In this setting we have all the training data for Clarge
and Cfew in ImageNet. For the ﬁxed feature extractor a(·)
pre-trained on Dlarge, we can train a linear classiﬁer on Clarge
and Cfew, or use nearest neighbor, to see what are the upper
bounds of the pre-trained a(·). Table 2 shows the results.
The performances are evaluated on the validation set of
ILSVRC 2015 which has 50 images for each category.
The feature extractor pre-trained on Dlarge demonstrates reasonable accuracies on Cfew which it has never seen during
training for both parametric and non-parametric methods.
Efﬁciency Analysis
We brieﬂy discuss the efﬁciencies of
each method including ours on the adaptation to novel categories and the image inference. The methods are tested on
NVIDIA Tesla K40M GPUs. For adapting to novel categories, ﬁne-tuned ResNet and Learning like a Child require
re-training the neural networks. For re-training one epoch
of the data, ﬁne-tuned ResNet and Learning like a Child
both take about 1.8 hours on 4 GPUs. Our method only
needs to predict the parameters for the novel categories using φ and add them to the original neural network. This process takes 0.683s using one GPU for adapting the network
Top-k Similarity
Figure 5: Visualization of the upper-left 256 × 256 submatrix of φ1 in log scale (left) and top-k similarity between
φ1, 1 and wpt
large (right). In the right plotting, red and solid
lines are similarities between φ1 and wpt
large, and green and
dashed lines are between 1 and wpt
to 100 novel categories with one example each. Siamese-
Triplet Network and nearest neighbor with cosine distance
require no operations for adapting to novel categories as
they are ready for feature extraction.
For image inference, Siamese-Triplet Network and nearest neighbor are very slow since they will look over the entire dataset. Without any optimization, this can take 2.3
hours per image when we use the entire Dlarge. To speed
up this process in order to do comparison with ours, we ﬁrst
pre-compute all the features. Then, we use a deep learning
framework to accelerate the cosine similarity computation.
At the cost of 45GB memory usage and the time for feature
pre-computation, we manage to lower the inference time of
them to 37.867ms per image. Fine-tuned ResNet, Learning
like a Child and our method are very fast since at the inference stage, these three methods are just normal deep neural
networks. The inference speed of these methods is about
6.83ms per image on one GPU when the batch size is set to
32. In a word, compared with other methods, our method is
fast and efﬁcient in both the novel category adaptation and
the image inference.
Comparing Activation Impacts
In this subsection we investigate what φ1 has learned that helps it perform better
than the cosine distance, which is a special solution for onelayer φ by setting φ to the identity matrix 1. We ﬁrst visualize the matrix φ1
ij in log scale as shown in the left image of
Figure 5. Due to the space limit, we only show the upperleft 256×256 submatrix. Not surprisingly, the values on the
diagonal dominates the matrix. We observe that along the
diagonal, the maximum is 0.976 and the minimum is 0.744,
suggesting that different from 1, φ1 does not use each activation channel equally. We speculate that this is because
the pre-trained activation channels have different distributions of magnitudes and different correlations with the classiﬁcation task. These factors can be learned by the last fully
connected layer of Rpt
large with large amounts of data but are
Fine-Tuned Baseline
28.86 ± 0.54%
49.79 ± 0.79%
Nearest Neighbor
41.08 ± 0.70%
51.04 ± 0.65%
Matching Network 
43.56 ± 0.84%
55.31 ± 0.73%
Meta-Learner LSTM 
43.44 ± 0.77%
60.60 ± 0.71%
48.70 ± 1.84%
63.11 ± 0.92%
Ours-Simple
54.53 ± 0.40%
67.87 ± 0.20%
59.60 ± 0.41%
73.74 ± 0.19%
Table 3: 5-way accuracies on MiniImageNet with 95% con-
ﬁdence interval. Red: the best, and blue: the second best.
assumed equal for every channel in cosine distance. This
motivates us to investigate the impact of each channel of
the activation space.
For a ﬁxed activation space, we deﬁne the impact of its
j-th channel on mapping φ by Ij(φ) = P
i |φij|. Similarly, we deﬁne the activation impact Ij(·) on wpt
large which
is the parameter matrix of the last fully connected layer of
large. For cosine distance, Ij(1) = 1, ∀j. Intuitively, we
are evaluating the impact of each channel of a on the output
by adding all the weights connected to it. For wpt
large which
is trained for the classiﬁcation task using large-amounts
of data, if we normalize I(wpt
large) to unity, the mean of
large) over all channel j is 2.13e-2 and the standard deviation is 5.83e-3. wpt
large does not use channels equally, either.
In fact, φ1 has a high similarity with wpt
large. We show
this by comparing the orders of the channels sorted by their
impacts. Let top-k(S) ﬁnd the indexes of the top-k elements
of S. We deﬁne the top-k similarity of I(φ) and I(wpt
large, k) = card
top-k(I(φ)) ∩top-k(I(wpt
where card is the cardinality of the set. The right image of
Figure 5 plots the two similarities, from which we observe
high similarity between φ and wpt
large compared to the random order of 1. From this point of view, φ1 outperforms
the cosine distance due to its better usage of the activations.
4.2. MiniImageNet Classiﬁcation
In this subsection we compare our method with the previous state-of-the-arts on the MiniImageNet dataset. Unlike
ImageNet classiﬁcation, the task of MiniImageNet is to ﬁnd
the correct category from 5 candidates, each of which has 1
example or 5 examples for reference. The methods are only
evaluated on Dfew, which has 20 categories. For each task,
we uniformly sample 5 categories from Dfew. For each of
the category, we randomly select one or ﬁve images as the
references, depending on the settings, then regard the rest
images of the 5 categories as the test images. For each task,
we will have an average accuracy over this 5 categories. We
repeat the task with different categories and report the mean
of the accuracies with the 95% conﬁdence interval.
Table 3 summarizes the few-shot accuracies of our
method and the previous state-of-the-arts. For fair comparisons, we implement two convolutional neural networks.
The convolutional network of Ours-Simple is the same
as that of Matching Network while Ours-WRN uses
WRN-28-10 as stated in §2.4. The experimental results demonstrate that our average accuracies are better than
the previous state-of-the-arts by a large margin for both the
Simple and WRN implementations.
It is worth noting that the methods are not
evaluated in the full ImageNet classiﬁcation task. This is
because the architectures of these methods, following the
problem formulation of Matching Network , can only
deal with the test tasks that are of the same number of reference categories and images as that of the training tasks,
limiting their ﬂexibilities for classiﬁcation tasks of arbitrary
number of categories and reference images. In contrast, our
proposed method has no assumptions regarding the number
of the reference categories and the images, while achieving
good results on both tasks. From this perspective, our methods are better than the previous state-of-the-arts in terms of
both the performance and the ﬂexibility.
5. Conclusion
In this paper, we study a novel problem: can we develop
a uniﬁed approach that works for both large-scale and fewshot learning. Our motivation is based on the observation
that in the ﬁnal classiﬁcation layer of a pre-trained neural
network, the parameter vector and the activation vector have
highly similar structures in space. This motivates us to learn
a category-agnostic mapping from activations to parameters. Once this mapping is learned, the parameters for any
novel category can be predicted by a simple forward pass,
which is signiﬁcantly more convenient than re-training used
in parametric methods or enumeration of training set used
in non-parametric approaches.
We experiment our novel approach on the MiniImageNet
dataset and the challenging full ImageNet dataset. The challenges of the few-shot learning on the full ImageNet dataset
are from the large number of categories (1000) and the very
limited number (< 4) of training samples for Cfew. On the
full ImageNet dataset, we show promising results, achieving state-of-the-art classiﬁcation accuracy on novel categories by a signiﬁcant margin while maintaining comparable performance on the large-scale classes. We further visualize and analyze the learned parameter predictor, as well
as demonstrate the similarity between the predicted parameters and those of the classiﬁcation layer in the pre-trained
deep neural network in terms of the activation impact. On
the small MiniImageNet dataset, we also outperform the
previous state-of-the-art methods by a large margin. The experimental results demonstrate the effectiveness of the proposed method for learning a category-agnostic mapping.