IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005
Exact Bayesian Curve Fitting
and Signal Segmentation
Paul Fearnhead
Abstract—We consider regression models where the underlying
functional relationship between the response and the explanatory
variable is modeled as independent linear regressions on disjoint
segments. We present an algorithm for perfect simulation from the
posterior distribution of such a model, even allowing for an unknown number of segments and an unknown model order for the
linear regressions within each segment. The algorithm is simple,
can scale well to large data sets, and avoids the problem of diagnosing convergence that is present with Monte Carlo Markov
Chain (MCMC) approaches to this problem. We demonstrate our
algorithm on standard denoising problems, on a piecewise constant
AR model, and on a speech segmentation problem.
Index Terms—Changepoints, denoising, forward-backward algorithm, linear regression, model uncertainty, perfect simulation.
I. INTRODUCTION
A. Overview
EGRESSION problems are common in signal processing.
The aim is to estimate, from noisy measurements, a functional relationship between a response and a set of explanatory
variables. We consider the approach of , who model this functional relationship as a sequence of linear regression models on
disjoint segments. Both the number and position of the segments
and the order and parameters of the linear regression models are
to be estimated. A Bayesian approach to this inference is taken.
In , Bayesian inference is performed via the reversible
jump MCMC methodology of . We consider applying recently developed perfect simulation ideas to this problem.
These ideas are closely related to the Forward–Backward algorithm and methods for product partition models , . To
deﬁne segments, we need to assume that the response can be ordered linearly through “time.” (Whereas time may be artiﬁcal,
in estimating a polynomial relationship between a response and
an explanatory variable, time can be deﬁned so that the order
that responses are observed is in increasing value of the explanatory variable.) The perfect simulation algorithm requires independence between segments and utilizes the Markov property
of changepoint models in such cases. It involves a recursion for
the probability of the data from time
onwards, conditional on
a changepoint immediately before time , given similar quantities for all times after . Once these probabilities have been
calculated for all , simulating from the posterior distribution of
the number and position of the changepoints is straight forward.
Manuscript received August 13, 2003; revised July 27, 2004. The associate
editor coordinating the review of this manuscript and approving it for publication was Zixiang Xiong.
The author is with the Department of Mathematics and Statistics, Lancaster
University, Lancaster, LA1 4YF U.K. (e-mail: ).
Digital Object Identiﬁer 10.1109/TSP.2005.847844
This approach to perfect simulation is different from the more
common approach based on coupling from the past , which
has been used, for example, on the related problem of reconstructing signals via wavelets .
We develop the existing methodology by allowing for model
uncertainty within each segment. We also implement a Viterbi
version of the algorithm to perform maximum a posteriori
(MAP) estimation. The advantages of our approach over
MCMC are that we have the following.
The perfect simulation algorithm draws independent
samples from the true posterior distribution and avoids
the problems of diagnosing convergence that occur
with MCMC.
The recursions of the algorithm are generic and simpler
than designing efﬁcient MCMC moves.
The computational cost of the algorithm can scale linearly with the length of data analyzed, thus making it
applicable for large data sets.
The ﬁrst advantage is particularly important. There are a
number of examples of published results from MCMC analyses
that have later proven to be inaccurate because the MCMC
algorithm had not been run long enough (for example, compare
the results of and or those of with those of ).
Our approach avoids this problem by enabling iid draws from
the true posterior (the goal of Bayesian inference). Thus, it can
be viewed as enabling “exact Bayesian” inference for these
In Bayesian inference, the posterior distribution depends on
the choice of prior distribution. When inference is made conditional on a speciﬁc model, uninformative priors can then be
chosen so that the posterior reﬂects the information in the data.
The regression problem we address involves model choice, and
for such problems, uninformative priors do not exist, as the
choice of prior affects the Bayes factor between different competing models. The use of uninformative priors for the parameters can severely penalize models with larger numbers of parameters (see for more details).
One approach to choosing priors for inference problems that
include model uncertainty is to let the data inform the choice of
prior , , for example, by using a hierarchical model with
hyperpriors on the prior parameters . However, the inclusion
of hyperpriors on the regression parameters violates the independence assumption required for perfect simulation. We suggest two possible approaches for choosing the prior parameters.
The simpler is based on a recursive use of the perfect simulation
algorithm, with the output of a preliminary run of the algorithm
being used to choose the prior parameters. Alternatively, if hyperpriors are used, the perfect simulation algorithm can then be
1053-587X/$20.00 © 2005 IEEE
Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore. Restrictions apply.
FEARNHEAD: EXACT BAYESIAN CURVE FITTING AND SIGNAL SEGMENTATION
incorporated within a simple MCMC algorithm, which mixes
over the hyperparameters.
The outline of the paper is as follows. In Section II we describe our modeling assumptions together with the methodology
for exact simulation and MAP estimation; we also describe how
to use the exact simulation algorithm within MCMC for the
case of Bayesian inference with hyperpriors. In Section III, we
demonstrate our method on standard denoising problems and on
speech segmentation.
II. MODEL AND METHOD
Our model is based on that of . We assume we have
observations
. Throughout, we use the notation
, which is the th to th entries of
the vector
segments, deﬁned by the ordered changepoints
, we model the
observations
, which are associated with the th segment by a linear regression of order
. Denote the
parameters by the vector
, and the matrix of basis functions by
Then, we have
is a vector of independent and identically distributed (iid) Gaussian random variables with mean 0 and variance
. For examples, see the polynomial regression model of
Section III-A or the auto regression model of Section III-B.
The number and positions of the changepoints and the order,
parameters, and variance of the regression model for each segment are all assumed to be unknown. We introduce conjugate
priors for each of these.
The prior on the changepoints is given by
, for some probability
. For the th regression parameter of
the th segment
, we have a normal prior with mean 0 and
, independent of all other regression parameters.
We assume an Inverse-Gamma prior for the noise variances
with parameters
. The priors are independent for
different segments. Finally, we constrain
and introduce
an arbitrary discrete prior
, again assuming independence
between segments.
We now describe how perfect simulation can be performed
for this model. We then discuss how the data can be used to
choose the prior parameters of the model
B. Perfect Simulation
1) Recursions: Deﬁne for
changepoint at
. The model of Section II-A has a Markov
property that enables
to be calculated in terms of
, by averaging over the position of the next changepoint
Consider a segment with observations
linear regression model order . Let
matrix of basis vectors for the th-order linear regression model
on this segment. Let
be the prior variance on the regression parameters for this segment, and let
identity matrix. Deﬁne
Finally, deﬁne
is a segment, model order
where (1) is obtained by integrating out the regression parameters and variance.
The intuition behind this recursion is that (suppressing the conditioning on a changepoint at
for notational convenience)
next changepoint at
no further changepoints
The respective joint probabilities are given by the two sets of
sums over the model order that appears on the right-hand side
of (2). See for a formal proof of this recursion.
2) Simulation: Once the
s have been calculated for
, it is straightforward to recursively simulate the
changepoints and linear regression orders. To simulate the
changepoints, we set
, and then recursively simulate
for some value
. The conditional posterior distribution of
Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005
For the th segment, the posterior distribution of the model order
is given by
3) Viterbi Algorithm: A Viterbi algorithm can be used
to calculate the MAP estimate of the changepoint positions and
model orders. Deﬁne
changepoint at
MAP estimate for
and 0 otherwise, and
where the maximum is taken over
to be the values of
and , which achieve the maximum. Then, the MAP estimate of
the changepoints
and the model orders
can be obtained recursively by the following:
; iii) set
, and go to (b).
The MAP estimate produced by this algorithm may have a
different number of segments than the MAP estimate of the
number of segments (see Section III-B). An alternative approach
to MAP estimation is to ﬁx the number of segments to the MAP
, say, and calculate the MAP estimate of the position
of the changepoints and the model orders conditional on
A simple adaptation of the above algorithm can perform such
a conditional MAP estimation of the changepoints and model
orders. Deﬁne
changepoint at
segments after
MAP estimate for
where the maximum is taken over
as the values of
that achieve the maximum (with
). Then, the
MAP estimate of the changepoints and model orders can be
obtained by the following.
, iii) set
, and go to (b).
4) Implementation: As written, (2) suffers from numerical
instabilities. This can be overcome by calculating
recursively, using
Evaluating
can be achieved in computational
time, which is
, as the matrix multiplications involved in
calculating the
s can be done recursively. For example,
term required for
can be calculated from the
equivalent term required for
. This is also true for
terms required in
When calculating the
values, we store the values for
that we have calculated. These stored values can then
be used in the simulation stage of the algorithm. An efﬁcient
algorithm for simulating large samples from the posterior distribution once the
values have been calculated is given in
 . The main computational cost of the perfect simulation is that
of evaluating the recursions to calculate the
s; once calculated, simulating samples are computationally very cheap.
The computational complexity of the recursion for the
. However, computational savings can be made in general as the terms in (2) tend to become negligible for sufﬁciently
large . We suggest truncating the sum in (2) at term
becomes smaller than some predetermined value, for example,
In a limiting regime, where data is observed over longer
time periods (as opposed to observations being made more frequently) such that as
increases, the number of changepoints
increases linearly with
, this simpliﬁcation is likely to make
the algorithm’s complexity
. See Section III for empirical
evidence of this.
C. Hyperpriors and MCMC
We have described an algorithm for perfect simulation from
the model of Section II-A. While this algorithm produces iid
draws from the true posterior distribution of the model, the usefulness of the approach and the model will depend on the choice
of prior parameters
The choice of these parameters deﬁnes the Bayes factors for the
competing models and, hence, the posterior distribution from
which it is sampled.
The approach of , which is also used by , lets the data
choose the prior parameters. In these two papers, this is achieved
by introducing uninformative hyperpriors on the prior parameters. Unfortunately, using hyperpriors introduces dependence
Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore. Restrictions apply.
FEARNHEAD: EXACT BAYESIAN CURVE FITTING AND SIGNAL SEGMENTATION
between the segments, such that the approach of Section II-B is
no longer directly applicable.
One solution is to use the results from a preliminary analysis
of the data to choose the prior parameters. Thus, we implement
the perfect simulation algorithm using a default choice of the
prior parameters . New values of
can be chosen based on the
perfect samples of the number of changepoints as well as the
regression variance and parameters. For example, the
could be chosen so that the prior means of the regression variance, parameters, and number of changepoints are close to the
posterior means from the preliminary analysis. We denote such
estimated values of the prior parameters by . If necessary, this
approach could be iterated a number of times until there is little
change in the posterior means. We call this a recursive approach.
A less ad hoc approach, which mimics that of , is to use a
hyperprior for
. A simple MCMC algorithm in this case is as
Update the number of segments
, the changepoints,
and model orders conditional on .
Update the
conditional
, the changepoints, the model orders, and .
conditional on
, the changepoints, the
model orders, and, for
If conjugate hyperpriors are used (for example, those of or
of Section III), then Gibbs updates can be used in steps b) and
c). The perfect simulation algorithm can be used in step a) to
simulate the changepoints and model orders from the full conditional, given . However, we advocate a more efﬁcient (in terms
of computing time) approach, which is to use an independence
proposal from the posterior distribution conditional on the prior
parameters being
We test and compare the accuracy and efﬁciency of both the
recursive and MCMC approaches on a number of examples in
Section III.
III. EXAMPLES
A. Polynomial Regression
For our ﬁrst class of examples, we assume that for each segment, there is a polynomial relationship between the response
and the explanatory variable. Here, we assume that the response
is either constant, linear, or quadratic. For a segment consisting
of observations
and explanatory variables
matrix of basis vectors for the quadratic relationship is
taken to be
(Top) Blocks function and observations and (bottom) estimates
(posterior means) based on the recursive and MCMC approaches. The two
estimates are almost exact and indistinguishable on the plot. The average
square error of the two estimates are 0.0045 and 0.0043, respectively.
(Top) Heavisine function and observations and estimates (posterior
means) based on the (bottom) recursive and MCMC approaches. The two
estimates are almost exact and indistinguishable on the plot. The average
square error of the two estimates are 0.0266 and 0.0264, respectively.
which is the mean value of the
th power of the explanatory
The reason for this choice of model is that the basis vectors
are orthogonal, and for a given segment, the regression parameters are independent under the posterior distribution. This helps
with the interpretability of the parameter estimates and slightly
reduces the computational cost of perfect simulation. The ﬁrstand second-order models are obtained by taking the ﬁrst and
ﬁrst two columns of
, respectively.
We tested our algorithm on the four test data sets of .
These have been previously analyzed by and , among
others. Each data set consists of 2048 equally spaced observations of an underlying functional relationship (for example, see
Figs. 1 and 2). The noise variance was 1.0 throughout, which
gives a signal-to-noise ratio of 7 in each case. In our simulation
study, we focused primarily on the computational aspects of our
approach. The accuracy of inference from a related model to the
one we use for these test data sets is given in .
as in . Initial parameter values were
. We ﬁrst tried the recursive approach, with two preliminary runs being used to obtain an estimate of the prior parameter values . Second, we implemented the MCMC approach, assuming an Inverse-Gamma
prior on the
s, a uniform prior on
, and an improper Jeffreys’
. In implementing step a) of the MCMC algorithm of
Authorized licensed use limited to: IEEE Xplore. Downloaded on February 19, 2009 at 08:06 from IEEE Xplore. Restrictions apply.
IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 53, NO. 6, JUNE 2005
RESULTS OF ANALYSIS OF THE FOUR TEST DATA SETS
Section II-C, we used proposals from the posterior distribution
conditional on
In calculating the
s for each data set, we truncated the
sum in (2) when (3) was less than
. By varying this cutoff,
we could tell that any inaccuracies introduced were negligible.
For each data set, evaluating the recursions to calculate the
s took less than 10 sec on a 900-MHz Pentium PC.
Summaries of the computational aspects of the perfect simulation and the MCMC algorithms are given in Table I. These include the average number of terms calculated in the sum of (2)
; the autocorrelation time of the MCMC algorithm
the acceptance probability of the independence sampler. The
autocorrelation time was calculated as the maximum estimated
time for all the prior parameters.
The average number of terms calculated for the sums in (2)
was much less in all cases than the roughly 1000 that would
occur if no truncation of the sum was used. The average number
of terms depends primarily on the average length of the segments for realizations that have non-negligible posterior probability. For example, the Heavisine function had fewest segments
(as few as 5) and, hence, the most terms, whereas, for example,
Bumps had many more segments (at least 35) and, thus, fewer
The MCMC algorithm mixed extremely well in all cases. The
acceptance probabilities of the independence sampler in part a)
of the algorithm were high (close to 1 for Blocks and Heavisine).
The autocorrelation times were also low because they were close
to 1 for Blocks and Heavisine, which suggests near iid samples
from the target posterior distribution.
For each dataset, the estimates based on the recursive approach and those based on the MCMC approach were almost
identical. For example, the two estimates for the Blocks and the
Heavisine data sets are shown in Figs. 1 and 2. The average mean
square errors of the estimates were also almost identical in all
cases. As can be seen from these ﬁgures, the reconstruction of
the Blocks and Heavisine functions are very good.
B. Auto Regressive Processes
Our second example is based on an analyzing data from
a piecewise constant AR process. Such models are used for
speech data . We considered models of order up to 3. For
a segment consisting of observation
, the matrix of basis
vectors for the third-order model is
(Top) Simulated AR process and (bottom) the posterior distribution of
the position of the changepoints. The true changepoint positions are denoted by
dashed lines.
MAP ESTIMATES OF CHANGEPOINTS AND MODEL ORDER
(Top) Speech data with conditional MAP estimates of the changepoints
given by vertical dashed lines and (bottom) posterior distribution of the
changepoint positions.
The matrices for the ﬁrst- and second-order models consist of
the ﬁrst and ﬁrst two columns of
, respectively.
We simulated 2000 data points from a model with nine breakpoints. The data is shown in Fig. 3. We only used the recursive
approach to analyze this data, as the MCMC approach had produced a negligible difference for the polynomial regression examples.
As above, we implemented the recursions for the
truncating the sums when (3) was less than
. On average,
250 terms were required in the summation. For simplicity, we
summarize our results in terms of the MAP estimate of the
changepoint positions and the model orders (see Table II) and
the posterior distribution of the changepoint positions 
and those estimated by Punskaya et al. (dashed lines).
The MAP estimate of the changepoint positions and model
orders consists of only eight changepoints, whereas the MAP
estimate of the number of changepoints is 9 (posterior probability 0.48). The MAP estimates given in Table II are conditional
on there being nine changepoints. For the unconditional MAP
estimate, the seventh changepoint is missed, but otherwise, the
estimates of the changepoint positions and model orders are unchanged. The MAP estimate incorrectly infers the model order
for segments 7 and 8. In each case, the MAP estimate is one
less than the true model order, and the AR coefﬁcients that are
incorrectly estimated as 0 are both small (0.1 and 0.2).
C. Speech Segmentation
We also used our method to analyze a real speech signal ,
which has also previously been analyzed in the literature ,
 , . We analyzed the data under a piecewise AR model
that allowed the AR orders of between 1 and 6 for each segment.
We implemented the recursive approach, where an initial run of
the exact simulation algorithm is used to construct suitable prior
parameters.
The signal, MAP estimates of the changepoints, and posterior
distribution of the changepoints are given in Fig. 4. A comparison of our estimates of the changepoint positions to previous
esimates are shown in Table III, where we give our MAP estimates, both conditional and unconditional, on the MAP estimate
for the number of changepoints.
Our MAP estimates are similar to those of Punskaya et al.
 , except for the inclusion by the conditional MAP estimate
of an extra changepoint near the beginning of the signal and the
inclusion by both MAP estimates of an extra changepoint near
the end of the signal. Fig. 5 shows these two regions and the
estimated changepoints from the different methods.
IV. CONCLUSION
We have presented a novel algorithm for performing exact
Bayesian inference for regression models, where the underlying
function relationship consists of independent linear regressions
on disjoint segments. The algorithm is both scalable and easy to
implement. It avoids the problems of diagnosing convergence
that are common with MCMC methods.
We have focused on models suggested by , but the algorithm can be applied more generally. The main requirement is
that of independence between the parameters associated with
each segment.
The regression problem we have addressed involves model
uncertainty. In practice, the accuracy of Bayesian inference for
such model-choice problems depends on the choice of prior. We
considered two approaches to choosing these priors, both based
on letting the data inform the choice of prior parameters. In our
examples, we found that the simpler of the two (the recursive approach) performs as well as the approach based on introducing
hyperparameters, and we would suggest such an approach in
We have also demonstrated how MAP estimates of the
changepoints can be obtained. There are two ways of deﬁning
the MAP estimate, depending on whether or not the MAP
estimate of the number of changepoints is ﬁrst calculated, and
then, the changepoints are inferred, conditional on this number
of changepoints. In some cases, these different approaches
can give different estimates for the number and position of the
changepoints: For example, when there is a likely changepoint
in some period of time but there is a lot of uncertainty over when
this changepoint occured, conditioning on the MAP number
of changepoints will pick up a changepoint during this period
of time, but it may be omitted otherwise (see Section III-B).
Note that for the related problem of inferring changepoints in
continuous time, it would clearly be correct to conditon on
the number of changepoints, as it is inappropriate to compare
joint densities of positions of changepoints that are of different
dimension.
ACKNOWLEDGMENT
The author would like to thank E. Punskaya for providing the
speech data.