Memory Matching Networks for One-Shot Image Recognition∗
Qi Cai †, Yingwei Pan †, Ting Yao ‡, Chenggang Yan §, and Tao Mei ‡
† University of Science and Technology of China, Hefei, China
‡ Microsoft Research, Beijing, China
§ Hangzhou Dianzi University, Hangzhou, China
{cqcaiqi, panyw.ustc}@gmail.com, {tiyao, tmei}@microsoft.com, 
In this paper, we introduce the new ideas of augmenting
Convolutional Neural Networks (CNNs) with Memory and
learning to learn the network parameters for the unlabelled
images on the ﬂy in one-shot learning.
Speciﬁcally, we
present Memory Matching Networks (MM-Net) — a novel
deep architecture that explores the training procedure, following the philosophy that training and test conditions must
match. Technically, MM-Net writes the features of a set of
labelled images (support set) into memory and reads from
memory when performing inference to holistically leverage
the knowledge in the set. Meanwhile, a Contextual Learner
employs the memory slots in a sequential manner to predict the parameters of CNNs for unlabelled images. The
whole architecture is trained by once showing only a few
examples per class and switching the learning from minibatch to minibatch, which is tailored for one-shot learning
when presented with a few examples of new categories at
test time. Unlike the conventional one-shot learning approaches, our MM-Net could output one uniﬁed model irrespective of the number of shots and categories. Extensive
experiments are conducted on two public datasets, i.e., Omniglot and miniImageNet, and superior results are reported
when compared to state-of-the-art approaches. More remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95% to 99.28% and from 49.21% to 53.37%
on miniImageNet.
1. Introduction
The recent advances in deep Convolutional Neural Networks (CNNs) have demonstrated high capability in visual
recognition. For instance, an ensemble of residual nets 
achieves 3.57% top-5 error on the ImageNet test set, which
is even lower than 5.1% of the reported human-level performance. The achievements have relied on the fact that learn-
∗This work was performed at Microsoft Research Asia.
ing deep CNNs requires large quantities of annotated data.
As a result, the standard optimization of deep CNNs does
not offer a satisfactory solution for learning new categories
from very little data, which is generally referred to as “One-
Shot or Few-Shot Learning” problem. One possible way to
alleviate this problem is to capitalize on the idea of transfer
learning by ﬁne-tuning a pre-trained network from
another task with more labelled data. However, as pointed
out in , the beneﬁt of a pre-trained network will greatly
decrease especially when the network was trained on the
task or data which is very different from the target one, not
to mention that the very little data may even break down
the whole network due to overﬁtting. More importantly, the
general training procedure which contains a number of examples per category in each batch does not match inference
at test time when only a single or very few examples of a
new category is given. This discrepancy affects the generalization of the learnt deep CNNs from prior knowledge.
We propose to mitigate the aforementioned two issues in
our one-shot learning framework. First, we induce from a
single or few examples per category to form a small set of
labelled images (support set) in each batch of training. The
optimization of our framework is then performed by recognizing other instances (unlabelled images) from the categories in the support set correctly. As such, the training
strategy is amended particularly for one-shot learning so as
to match inference in the test stage. Moreover, a memory
module is leveraged to compress and generalize the input
set into slots in the memory and produce the outputs holistically on the whole support set, which further enhances the
recognition. Second, we feed the memory slots into one Recurrent Neural Networks (RNNs), as a contextual learner, to
predict the parameters of CNNs for the unlabelled images.
As a result, the contextual learner captures both long-term
memory across all the categories in the training and shortterm knowledge speciﬁed on the categories at test time.
Note that our solution does not require a ﬁne-tuning process and computes the parameters on the ﬂy. In addition, the
memory is an uniform medium which could convert differarXiv:1804.08281v1 [cs.CV] 23 Apr 2018
ent size of support sets into common memory slots, making
it very ﬂexible to train an uniﬁed model irrespective of the
number of shots and categories.
By consolidating the idea of learning a learner to predict
parameters in networks and matching training and inference
strategy, we present a novel Memory Matching Networks
(MM-Net) for one-shot image recognition, as shown in Figure 1. Speciﬁcally, a single or few examples per category
are fed into a batch every time as a support set of labelled
images in training. A deep CNNs is exploited to learn image representations, which update the memory through a
write controller. A read controller enhances the image representations with the memory across all the categories to
produce feature embeddings of images in the support set.
Meanwhile, we take the memory slots as a sequence of inputs to a contextual learner, i.e., bidirectional Long Short-
Term Memory (bi-LSTM) networks, to predict the parameters of the convolutional layers in the CNNs. The outputs of
CNNs are regarded as embeddings of unlabelled images. As
such, the contextual relations between categories are also
explored in learning network parameters. The dot product
between the embeddings of a given unlabelled image and
each image in the support set is computed as the similarity
and the label of the nearest one is assigned to this unlabelled
image. The whole deep network is end-to-end optimized by
minimizing the error of predicting the labels in the batch
conditioned on the support set. It is also worth noting that
we could form each batch with different number of shots
and categories in training stage to learn an uniﬁed architecture for performing inference on any one-shot learning scenarios. At inference time, the support set is then replaced
by the examples from new categories and there is no any
change in the procedure.
The main contribution of this work is the proposal of
Memory Matching Networks for addressing the issue of
one-shot learning in image recognition. The solution also
leads to the elegant views of how the discrepancy between training and inference in one-shot learning should be
amended and how to make the parameters of CNNs computable on the ﬂy in the context of very little data, which
are problems not yet fully understood in the literature.
2. Related Work
One-Shot Learning. The research of one-shot learning has proceeded mainly along following directions: data
augmentation, transfer learning, deep embedding learning,
and meta-learning. Data augmentation method is
the most natural solution for one-shot learning by enlarging training data via data manufacturing. Transfer learning
approaches aim to recycle the knowledge learned
from previous tasks for one-shot learning.
Wang et al.
exploit the generic category agnostic transformation from
small-sample models to the underlying large-sample models for one-shot learning in . Deep embedding learning attempts to create a low-dimensional embedding space, where the transformed representations are
more discriminative. learns the deep embedding space
with a siamese network and classiﬁes images by a nearestneighbor rule. Later in , Matching Network is developed to transform the support set and testing samples into a
shared embedding space with matching mechanism. Metalearning models mainly frame the learning problem at two levels: the rapid learning to acquire the knowledge within each task and the gradual learning to extract
knowledge learned across all tasks. For instance, proposes an LSTM-based meta-learner model to learn the exact optimization algorithm, which is utilized to train another
neural network classiﬁer in the few-shot regime.
Parameter Prediction in CNNs. Parameter prediction
in CNNs refers to evolve one network to generate the structure of weights for another network. is one of the early
works that suggests the concept of fast weights in which
one network can produce the changes of context-dependent
weights for a second network. Later in , Denil et al.
demonstrate the signiﬁcant redundancy in the parameterization of several deep learning models and it is possible to accurately predict most parameters given only a few weights.
Next, a few subsequent works study practical applications
with the fast weights concept, e.g., image question answering and zero-shot image recognition .
Memory Networks. Memory Networks is ﬁrst proposed
in by augmenting neural networks with an external
memory component which can be easily read and written
through read and write controllers. Later in , Memory
Networks is further extended to End-to-end Memory Networks, which is trained in an end-to-end manner and requires signiﬁcantly less supervision compared with original
Memory Networks. Moreover, Chandar et al. explore a
form of Hierarchical Memory Networks , allowing the
read controller to efﬁciently access extremely large memories. Recently, Key-Value Memory Networks stores
prior knowledge in a key-value structured memory before
reading them for prediction, making the knowledge to be
stored more ﬂexibly. In this work, we adopt the Key-Value
Memory Networks as the memory module to store the encoded contextual information speciﬁed on the categories
into the key-value structured memory.
In summary, our work belongs to deep embedding learning method for one-shot learning. However, most of the
above methods in this direction mainly focus on forming
the deep embedding space with the simple objective of
matching-based classiﬁcation (i.e., to maximize the matching score between unlabelled image and the support images
with the same label). Our work is different that we enhance
the one-shot learning by leveraging memory module to additionally integrate the contextual information across sup-
𝑎𝑖,𝑛= 𝑆𝑜𝑓𝑡𝑚𝑎𝑥 𝑒𝑖,𝑛
Read Controller
Memory Module
Write Controller
Contextual Learner
Similarity
cn = ∑𝑎𝑖,𝑛𝑚𝑖
Support Set
Unlabelled Image
Kernel for conv.
Figure 1. The overview of Memory Matching Networks (MM-Net) for one-shot image recognition (better viewed in color). Given a support
set consisting of a single or few labelled examples per category, a deep CNNs is exploited to learn rich image representations, followed
by a memory module to compress and generalize the input support set into slots in the memory via a write controller. A read controller
in memory module further enhances the representation (embedding) learning of images in the support set by holistically exploiting the
memory across all the categories. Meanwhile, a contextual learner, i.e., bi-LSTM, is adopted to explore the contextual relations between
categories by encoding the memory slots in a sequential manner for predicting the parameters of CNNs, whose outputs are regarded as
embeddings of unlabelled images. The dot product between the embeddings of a given unlabelled image and each image in the support set is
computed as the similarity and the label of the nearest one is assigned to this unlabelled image. The training of our MM-Net exactly matches
the inference. In addition, the memory is an uniform medium which could convert different size of support sets into common memory
slots, making it ﬂexible to train an uniﬁed model with a mixed strategy for performing inference on any one-shot learning scenarios.
port samples into the deep embedding architectures. It is
worth noting that also involves contextual information
for one-shot learning. Ours is fundamentally different in the
way that all the CNNs in need to be learnt at training
stage, as opposed to directly predicting the parameters of
CNNs for unlabelled image based on the contextual information encoded in the memory slots of this work, which is
better-suited for one-shot learning during inference on unseen categories.
3. One-Shot Image Recognition
The basic idea of Memory Matching Networks (MM-
Net) for one-shot learning is to construct an embedding
space where the unseen objects can be rapidly recognized
from a few labelled images (support set). MM-Net ﬁrstly
utilizes a memory module to encode and generalize the
whole support set into memory slots, which are endowed
with the contextual information speciﬁed on the categories.
The training of MM-Net is then performed by contextually
embedding the whole support set with the memory across
all the categories via read controller. Meanwhile, a contextual learner is devised to predict the parameters of CNNs
for embedding unlabelled image conditioned on the contextual relations between categories. Both of the embeddings
of support set and unlabelled image are further leveraged
to retrieve the label of unlabelled image through matching
mechanism in the embedding space. Our MM-Net is trained
in a learning to learn manner and can be adapted ﬂexibly for
recognizing any new objects by only feed-forwarding the
support set. An overview of MM-Net is shown in Figure 1.
3.1. Problem Formulation
Suppose we have a small support set with N image-label
pairs S = {(xn, yn)}N
n=1 from C object classes, where each
class contains few or even one single image. In the standard
setting of one-shot learning, our ultimate target is to recognize a class from a single labelled image. Hence, given
an unlabelled image ˆx, we aim to predict its class ˆy with
the prior knowledge mined from the support set S, which is
ˆy = arg max
P (yn|ˆx, S) ,
where P (yn|ˆx, S) is the probability of classifying ˆx with
the class yn conditioned on S and C is the set of class labels. Inspired by the recent success of Matching Networks
in one-shot learning , we formulate our one-shot object recognition model in a non-parametric manner based
on matching mechanism which retrieves the class label of
unlabelled image by comparing the matching scores with
all the labelled images (support set) in the learnt embedding
space. Accordingly, the probability of classifying ˆx with
the class label yn we exploit here can be interpreted as the
matching score between ˆx and the support sample xn with
label yn, which is measured as the dot product between their
embedded representations
P (yn|ˆx, S) = f(ˆx|S)⊤· g (xn|S) ,
where f (·) and g (·) are two deep embedding functions for
unlabelled image ˆx and support image xn given the whole
support set S, respectively. Please note that derived from
the idea of Memory Networks , we leverage a memory
module to explicitly generalize the whole support set into
memory slots, which are endowed with the contextual information among support set S and can be further integrated
into the learning of both f (·) and g (·).
3.2. Encoding Support Set with Memory Module
Inspired from the recent success of Recurrent Neural
Networks (RNNs) for sentence modeling in machine translation and image/video captioning , one natural
way to model the contextual relationship across the support
samples in support set S is to adopt the RNNs based models
as in , whose latent state is treated as the memory.
However, such kind of memory is typically too small and
not compartmentalized enough to accurately remember the
previous knowledge, let alone the contextual information
across diverse object classes with few or even one single
image per class. Taking the inspiration from Memory Networks which manipulates a large external memory that
can be ﬂexibly read and written to, we design a memory
module to encode the contextual information within support
set into the memory through write controller.
Memory. The memory in our memory module is denoted as M = {(mk
i=1 consisting of M key-value
pairs, where each memory slot is composed of a memory
i and the corresponding memory value mv
the memory key mk
i ∈RDm denotes the Dm-dimensional
memory representation of the i-th memory slot and the
memory value mv
i is an integer representing the class label
of the i-th memory slot.
Write controller. Given the support set S, the memory
module is utilized to encode the sequence of N support images into M memory slots with write controller, aiming to
distill the intrinsic characteristics of classes. Thus, we devise the memory updating strategy in our write controller
as a dynamic feature aggregation problem to exploit both
the intrinsic universal characteristic of each class beyond individual samples and the remarkable diversity within each
class. The core issue for this design is about whether the
write controller should jointly aggregate visually similar
support samples into one memory slot by sequentially updating the corresponding memory key or individually seek
one new memory slot to store the distinctive samples. The
former one is triggered when the input support sample
shares the same class label/memory value with the visually
similar memory key, otherwise the later one is adopted.
The vector formulas for the memory updating strategy
in write controller are given below. At n-th time step, the
current input support image xn and its class label yn are
written into memory slots to update the previous memory
Mn−1 via write controller, producing memory Mn. In particular, let zn ∈RDz denote the Dz-dimensional visual feature of the support image xn. One transformation matrices
Tz ∈RDm×Dz is ﬁrstly employed to project the support
image zn into the mapping zk
n in memory key space:
Next, for the input support image, we mine its nearest
neighbor (i.e., the most visually similar memory key) from
previous memory Mn−1 with respect to dot product similarity between its representation in memory key space zk
and each memory key mk
i . Here we denote in as the index
of xn’s nearest neighbor in memory Mn−1. The memory
updating is then conducted in a different way depending on
whether the memory value of xn’s nearest neighbor mv
is exactly matched with the xn’s class label yn or not. If
in = yn, we only update the memory key mk
in by integrating it with zk
n and then normalizing it:
Otherwise, when mv
in ̸= yn, we store the key-value pair
n, yn) in the next new memory slot. Note that if there is
no available memory slot left, the memory key mk
in is updated as in Eq.(4). After encoding the whole support set S
into memory via write controller, the ﬁnal memory MN is
endowed with the contextual information within support set.
Please note that we denote the two deep embedding functions f(ˆx|S) and g(xn|S) as f(ˆx|MN) and g(xn|MN) in
the following sections, respectively.
3.3. Contextual Embedding for Support Set
The most typical way to transform images from the support set into the embedding space is to embed each sample
independently through a shared deep embedding architecture g(xn) in discriminative learning, while the holistical
contextual information within support set is not fully exploited. Here we develop a contextual embedding function
for support set g (xn|MN) to embed xn conditioned on the
memory MN via read controller of memory module, with
the intuition that the holistical contextual information endowed in the memory across all the categories can guide g
to produce more discriminative representation of xn.
Read controller. Technically, for each support image xn
and its embedded representation zk
n in memory key space,
we ﬁrstly measure the dot product similarity ai,n between
n and each memory key mk
i followed by a softmax function, and then retrieve the aggregated memory vector cn by
calculating the sum of each memory key weighted by ai,n:
ai,n = Softmax(zk
i=1 ai,nmk
where Softmax(vi) = evi/P
j evj. The above memory
retrieval process is conducted by read controller. Besides, a
shortcut connection is additionally constructed between the
input and output of read controller, making the optimization
more easier. Thus, the ﬁnal output representation of xn via
contextual embedding is measured as:
g(xn|MN) = Tccn + zn ∈RDz,
where Tc ∈RDz×Dm is the transformation matrix for mapping the aggregated memory cn into the embedding space
and Dz is the embedding space dimension.
3.4. Contextual Embedding for Unlabelled Images
The standard deep embedding function f(ˆx; W) in discriminative learning consists of stacks of convolutional layers that are parameterized by matrix W in general. The optimization of parameters W often requires enormous training data and a lengthy iterative process to generalize well
on unseen samples.
However, in the extreme case with
only a single labelled example of each class, it is insufﬁcient to train the deep embedding architecture and directly
ﬁne-tuning this architecture often results in poor performance on the recognition of new category. To address the
aforementioned challenges for one-shot learning, we devise
a novel contextual embedding architecture f(ˆx; W|MN)
for unlabelled image by incorporating the contextual relations between categories mined from memory MN into
the deep embedding function. In particular, the parameters
W of this contextual embedding architecture are learnt in a
feed-forward manner conditioned on memory MN without
backpropagation, obviating the need of ﬁne-tuning to adapt
to the new category.
Contextual Learner. A novel deep architecture, named
as contextual learner, is especially designed to synthesis
the parameters W of contextual embedding architecture
f(ˆx; W|MN) depending on the memory MN of support
set. Speciﬁcally, we denote the output parameters W ∈
RDw for contextual learner as
W = ω(MN; W ′),
where ω (·) is the encoding function of contextual learner
that transforms the memory MN into the target parameters
W and W ′ are the parameters of contextual learner ω(·).
Inspired by the success of bidirectional LSTM (bi-LSTM)
 in several inherently sequential tasks (e.g., machine
translation , speech recognition and video generation ), we leverage bi-LSTM to contextually encode
the memory MN in a sequential manner. In particular ,
bi-LSTM consisting of forward and backward LSTMs ,
which read the memory slots of MN in its natural order
M) and the reverse order (from mk
respectively. The encoded representation ←→
memory MN is achieved by directly summating the ﬁnal
hidden states of two LSTMs, where Dr denotes the dimension of LSTM hidden state. The output parameters W are
calculated as
where Tp ∈RDw×Dr is the transformation matrix. Accordingly, by synthesizing the parameters of contextual embedding with our contextual learner, the contextual relations
between categories are elegantly integrated into this deep
embedding architecture f(ˆx; W|MN) for the unlabelled
image, which encourages the transformed representation to
be more discriminative for image recognition.
Factorized Architectures. When designing the speciﬁc
architecture of the contextual embedding for unlabelled images, the traditional convolutional layer is modiﬁed with
factorized design for signiﬁcantly reducing the number
of parameters within convolutional ﬁlters, making parameter prediction with contextual learner more feasible.
3.5. Training Procedure
After obtaining the embedded representations of both
unlabelled image and the whole support set, we follow
the prior works to train our model for the widely
adopted task of one-shot learning: the C-way k-shot image
recognition task, i.e., classifying a disjoint set of unlabelled
images given a set of C unseen classes with only k labelled
images per class. Speciﬁcally, for each batch in the training stage, we ﬁrstly sample C categories uniformly from all
training categories with k examples per category, forming
the labelled support set S. The corresponding unlabelled
images set B are randomly sampled from the rest data belonging to the C categories in training set. Hence, given the
support set S and input unlabelled images set B, the softmax loss is then formulated as:
L(S, B) = −
I(ˆy=yn) log
eP (yn|ˆx,S)
eP (yt|ˆx,S) ,
where ˆy ∈C represents the class label of ˆx and P (yn|ˆx, S)
denotes the probability of classifying ˆx with the class label of xn as in Eq.(2). The indicator function Icondition = 1
if condition is true; otherwise Icondition = 0. By minimizing the softmax loss over a training batch, our MM-Net is
trained to recognize the correct class labels of all the images
in B conditioned on the support set S. Accordingly, in the
test stage, given the support set S′ containing C categories
never seen during training, our model can rapidly predict
the class label for an unlabelled image through matching
mechanism, without any ﬁne-tuning on the novel categories
due to its non-parametric property.
Mixed training strategy. In the above mentioned training procedure, each training batch is constructed with the
uniform setting which exactly matches the test setting (Cway k-shot), targeting for mimicking the test situation for
one-shot learning. However, such matching mechanism indicates that the learnt model is only suitable for the pre-
ﬁxed C-way k-shot test scenario, making it difﬁcult to generalize to other C′-way k′-shot task (where C′ ̸= C or
k′ ̸= k). Accordingly, to enhance the generalization of
the one-shot learning model, we devise a mixed training
strategy by constructing each training batch with different
number of shots and categories to learn an uniﬁed architecture for performing inference on any one-shot learning
scenarios. Please note that the memory could be regarded
as an uniform medium which converts different size of support sets into common memory slots. As a result, the mixed
training strategy can be applied to learn an uniﬁed model
irrespective of the number of shots and categories.
4. Experiment
We evaluate and compare our MM-Net with state-ofthe-art approaches on two datasets, i.e., Omniglot and
miniImageNet . The former is the most popular oneshot image recognition benchmark of handwritten characters and the latter is a recently released subset of ImageNet
4.1. Datasets
Omniglot. Omniglot contains 32,460 images of handwritten characters. It consists of 1,623 different characters
within 50 alphabets ranging from well-established international languages like Latin and Korean to lesser-known local dialects. Each character was hand drawn by 20 different
people via Amazon’s Mechanical Turk, leading to 20 images per character. We follow the most common split in
 , taking 1,200 characters for training and the rest 423
for testing. Moreover, the same data preprocessing in 
is adopted, i.e., each image is resized to 28 × 28 pixels and
rotated by multiples of 90 degrees as data augmentation.
miniImageNet. The miniImageNet dataset is a recent
collection of ImageNet for one-shot image recognition. It is
composed of 100 classes randomly selected from ImageNet
 and each class contains 600 images with the size of
84 × 84 pixels. Following the widely used setting in prior
work , we take 64 classes for training, 16 for validation
and 20 for testing, respectively.
4.2. Experimental Settings
Evaluation Metrics.
All of our experiments revolve
around the same basic task: the C-way k-shot image recognition task. In the test stage, we randomly select a support
set consisting of C novel classes with k labelled images per
class from the test categories and then measure the classi-
ﬁcation accuracy of the disjoint unlabelled images (15 images per class) for evaluation. To make the evaluation more
convincing, we repeat such evaluation procedures 500 times
for each setting and report the ﬁnal mean accuracy for each
setting. Moreover, the 95% Conﬁdence Intervals (CIs) of
the mean accuracy is also present, which statistically describes the uncertainty inherent in performance estimation
like standard deviation. The smaller the conﬁdence interval,
the more precise the mean accuracy performance.
Network Architectures and Parameter Settings. For
fair comparison with other baselines, we adopt a widely
adopted CNNs in as the embedding function for
support set g (·), consisting of four convolutional layers.
Each convolutional layer is devised with a 3×3 convolution
with 64 ﬁlters followed by batch normalization, a ReLU
non-linearity and a 2 × 2 max-pooling. Accordingly, the
ﬁnal output embedding space dimension Dz is 64 on Omniglot and 1,600 on miniImageNet, respectively. The contextual embedding for unlabelled image f (·) is similar to
g (·) except that the last convolution layer is developed with
factorized design and its parameters are predicted based on
the contextual memory of support set.
For the memory
module, the dimension of each memory key Dm is set as
512. For contextual learner, we set the size of hidden layer
in bi-LSTM as 512. Our MM-Net is trained by Adam 
optimizer. The initial learning rate is set as 0.001 and we
decrease it to 50% every 20,000 iterations. The batch size
is set as 16 and 4 for Omniglot and miniImageNet.
4.3. Compared Approaches
To empirically verify the merit of our MM-Net model,
we compare with the following state-of-the-art methods: (1)
Siamese Networks (SN) optimizes siamese networks
with weighted L1 loss of distinct input pairs for one-shot
learning. (2) Matching Networks (MN) performs oneshot learning with matching mechanism in the embedding
space, which is further developed into fully-contextual embedding version (MN-FCE) by utilizing bi-LSTM to contextually embed samples.
(3) Memory-Augmented Neural Networks (MANN) devises a memory-augmented
neural network to rapidly assimilate new data for one-shot
learning. (4) Model-Agnostic Meta-Learning (MAML) 
learns easily adaptable model parameters through gradient descent in a meta-learning fashion. (5) Meta-Learner
LSTM (ML-LSTM) designs a LSTM-based metalearner to learn an update rule for optimizing the network.
(6) Siamese with Memory (SM) presents a life-long
memory module to remember past training samples and
makes predictions based on stored previous samples. (7)
Meta-Networks (Meta-N) takes the loss gradient as
meta information to rapidly generate the parameters of classiﬁcation networks. (8) Memory Matching Networks (MM-
Net) is the proposal in this paper. Moreover, a slightly different version of this run is named as MM-Net−, which is
trained without the mixed training strategy.
4.4. Results on Omniglot
Table 1 shows the performances of different models on
Omniglot dataset.
Overall, the results across 1-shot and
5-shot learning on 5 and 20 categories consistently indi-
Table 1. Mean accuracy (%) ± CIs (%) of our MM-Net and other
state-of-the-art methods on Omniglot dataset.
5-way Accuracy
20-way Accuracy
Meta-N 
98.7 ± 0.4
99.9 ± 0.1
95.8 ± 0.3
98.9 ± 0.2
99.28 ± 0.08
99.77 ± 0.04
97.16 ± 0.10
98.93 ± 0.05
cate that our proposed MM-Net achieves superior performances against other state-of-the-art techniques including
deep embedding models (SN, MN, SM) and meta-learning
approaches (MANN, Meta-N, MAML). In particular, the
5-way and 20-way accuracy of our MM-Net can achieve
99.28% and 97.16% on 1-shot learning, making the absolute improvement over the best competitor Meta-N by
0.33% and 0.16%, respectively, which is generally considered as a signiﬁcant progress on this dataset.
As expected, the 5-way and 20-way accuracies are boosted up
to 99.77% and 98.93% respectively when provided 5 labelled images (5 shot) from each category. SN, which simply achieves the deep embedding space through pairwise
learning, is still effective in 5-way task. However, the accuracy is decreased sharply when searching nearest neighbor
in the embedding space in 20-way 1-shot scenario. Furthermore, MN, MANN, SM, Meta-N, MAML, and MM-Net
lead to a large performance boost over SN, whose training strategy does not match the inference. The results basically indicate the advantage of bridging the discrepancy
between how the model is trained and exploited at test time.
SM by augmenting CNNs with a life-long memory module
to exploit the contextual memory among previous labelled
samples for one-shot learning, improves MN, but the performances are still lower than our MM-Net. This conﬁrms
the effectiveness of the contextual learner for directly synthesizing the parameters of CNNs, obviating adapting the
embedding to novel classes with ﬁne-tuning.
4.5. Results on miniImageNet
The performance comparisons on miniImageNet are
summarized in Table 2. Our MM-Net performs consistently
better than other baselines. In particular, the 5-way accuracies of 1-shot and 5-shot learning can reach 53.37% and
66.97%, respectively, which is to-date the highest performance reported on miniImageNet, making the absolute improvement over MAML by 4.67% and 3.86%. MN-FCE
exhibits better performance than MN, by further taking contextual information within support set into account for embedding learning of images. ML-LSTM and MAML which
learns an update rule to ﬁne-tune the CNNs or the easily
adaptable parameters of CNNs could be generally consid-
Table 2. Mean accuracy (%) ± CIs (%) of our MM-Net and other
state-of-the-art methods on miniImageNet dataset.
5-way Accuracy
43.40 ± 0.78
51.09 ± 0.71
MN-FCE 
43.56 ± 0.84
55.31 ± 0.73
ML-LSTM 
43.44 ± 0.77
60.60 ± 0.71
48.70 ± 1.84
63.11 ± 0.92
Meta-N 
49.21 ± 0.96
52.74 ± 0.45
65.82 ± 0.37
53.37 ± 0.48
66.97 ± 0.35
Mean accuracy (%) of MM-Net by varying training strategies for 5-way k-shot image recognition task (k
{1, 2, 3, 4, 5}) on miniImageNet.
Mixed k-shot
Mixed C-way k-shot
ered as extensions of MN in a meta-learning fashion, resulting in better performance. There is a performance gap
between Meta-N and our MM-Net−. Though both runs involve the parameters prediction of CNNs, they are fundamental different in the way of parameters prediction. Meta-
N predicts the parameters of the classiﬁcation networks for
unlabelled images based on the loss gradient of support set,
while our MM-Net−leverages contextual information in
memory to jointly predict the parameters of CNNs for unlabelled images and contextually encode support images. As
indicated by our results, MM-Net−is beneﬁted from the
memory-augmented CNNs for both support set and unlabelled images, and leads to apparent improvements. In addition, MM-Net by additionally leveraging the mixed training strategy outperforms MM-Net−.
4.6. Experimental Analysis
We further analyze the effect of training strategy, the hidden state size of bi-LSTM in contextual learner, the image
representation embedding visualization, and the similarity
matrix over test images for 5-way k-shot image recognition
task on miniImageNet Dataset.
Training strategy.
We ﬁrst present the analysis to
demonstrate the generalization of our MM-Net by employing mixed training strategy for various test scenarios. Table 3 details the performance comparisons between several
training strategies (i.e., uniform and mixed training strategies) with respect to different test tasks (i.e., 1, 2, 3, 4 and
5-shot). Overall, for each test scenario, there is a clear performance gap between all the ﬁve uniform training strate-
5-way Accuracy (%)
Figure 2. The effect of the hidden state size in our contextual
learner’s bi-LSTM on miniImagenet.
(a) MN 
(b) MM-Net
Figure 3. Image representation embedding visualizations of MN
and our MM-Net on miniImagenet using t-SNE . Each image
is visualized as one point and colors denote different classes.
gies (i.e., 1, 2, 3, 4 and 5-shot) and our proposed mixed
training strategies (i.e., Mixed k-shot and Mixed C-way kshot). In particular, the peak performance of MM-Net is
achieved when we adopt the Mixed C-way k-shot setting by
changing both C and k (C ∈{2, 3, 4, 5}, k ∈{1, 2, 3, 4, 5})
for constructing training batches. This empirically demonstrates the effectiveness of mixed training strategy for generalizing our MM-Net model to various test scenarios, obviating re-training the model on the new testing task. Note
that Mixed k-shot, a simpliﬁed version of Mixed C-way
k-shot which constructs each training batch with different
number of shots but always in 5-way manner, still outperforms all the ﬁve uniform training strategies.
Hidden state size of bi-LSTM in contextual learner.
In order to show the relationship between the performance
and hidden state size of bi-LSTM in contextual learner, we
compare the results of the hidden state size in the range of
128, 256, 512 and 1,024 on both 1-shot and 5-shot tasks.
The 5-way accuracy with the different hidden state size is
shown in Figure 2. As illustrated in the ﬁgure, the performance difference by using different hidden state size is
within 0.013 on both 1-shot and 5-shot tasks, which practically eases the selection for the optimal hidden state size.
Image representation embedding visualization. Figure 3 shows the t-SNE visualizations of embedding of
image representation learnt by MN and our MM-Net under
5-way 5-shot scenario. Speciﬁcally, we randomly select 5
classes from miniImageNet testing set and the embedded
(a) MN 
(b) MM-Net
Figure 4. Similarity matrix of MN and our MM-Net on
miniImagenet (vertical axis: 5 labelled images per class in support set; horizontal axis: 5 unlabelled test images per class). The
warmer colors indicate higher similarities.
representations of all the 2,975 images (excluding the 25
images in support set) are then projected into 2-dimensional
space using t-SNE. It is clear that the embedded image representations by MM-Net are better semantically separated
than those of MN.
Similarity matrix visualization. Figure 4 further shows
the visualizations of similarity matrix learnt by MN and
our MM-Net under 5-way 5-shot scenario. In particular,
the similarity matrix is constructed by measuring the dot
product similarities between the randomly selected support
set (25 images in 5 classes) and the corresponding 25 unlabelled test images. Note that every ﬁve images belong to
the same class. Thus we can clearly see that most intraclass similarities of MM-Net are higher than those of MN
and the inter-class similarities of MM-Net are mostly lower
than MN, demonstrating that the representation learnt by
our MM-Net are more discriminative for image recognition.
5. Conclusions
We have presented Memory Matching Networks (MM-
Net), which explores a principled way of training the network to do one-shot learning as at inference. Particularly,
we formulate the training by only utilizing one single or
very few examples per category to form a support set of labelled images in each batch and switching the training from
batch to batch, which is much like how it will be tested
when presented with a few examples of new categories.
Furthermore, through a new design of Memory module, the
feature embeddings of images in the support set are contextually augmented with the holistic knowledge across categories in the set. Meanwhile, to better generalize the networks to the new categories with very little data, we construct a contextual learner which sequentially exploits the
memory slots to predict the parameters of CNNs on the ﬂy
for unlabeled images. Experiments conducted on both Omniglot and miniImageNet datasets validate our proposal and
analysis. Performance improvements are clearly observed
when comparing to other one-shot learning techniques.