COP: Customized Deep Model Compression via Regularized Correlation-Based
Filter-Level Pruning
Wenxiao Wang1,2 , Cong Fu1,3 , Jishun Guo4 , Deng Cai1,2∗and Xiaofei He1,2
1State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China
2Fabu Inc., Hangzhou, China
3Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China
4GAC R&D Center, Guangzhou, China
 , , , {dengcai,
xiaofeihe}@cad.zju.edu.cn
Neural network compression empowers the effective yet unwieldy deep convolutional neural
networks (CNN) to be deployed in resourceconstrained scenarios.
Most state-of-the-art approaches prune the model in ﬁlter-level according
to the “importance” of ﬁlters. Despite their success, we notice they suffer from at least two of
the following problems: 1) The redundancy among
ﬁlters is not considered because the importance is
evaluated independently. 2) Cross-layer ﬁlter comparison is unachievable since the importance is de-
ﬁned locally within each layer. Consequently, we
must manually specify layer-wise pruning ratios.
3) They are prone to generate sub-optimal solutions because they neglect the inequality between
reducing parameters and reducing computational
cost. Reducing the same number of parameters in
different positions in the network may reduce different computational cost. To address the above
problems, we develop a novel algorithm named as
COP (correlation-based pruning), which can detect the redundant ﬁlters efﬁciently.
the cross-layer ﬁlter comparison through global
normalization.
We add parameter-quantity and
computational-cost regularization terms to the importance, which enables the users to customize the
compression according to their preference (smaller
or faster). Extensive experiments have shown COP
outperforms the others signiﬁcantly. The code is
released at 
Introduction
The growing demands of deploying deep models to resourceconstrained devices such as mobile phones and FPGA have
posed great challenges for us. Network pruning has become
one of the most popular methods to compress the model without much loss in performance, and methods of network pruning could be divided into two categories: weight-level pruning [Han et al., 2015; Guo et al., 2016; Dong et al., 2017]
∗Corresponding author
Figure 1: The ﬁgure is a sketch of VGG16. Consider the following two pruning plans: a) prune 1 ﬁlter from conv4 2; b) prune 2
ﬁlters from conv3 2. With plan a), we can reduce 9216 parameters
and 14.5 million FLOPs. With plan b), we can reduce 9216 parameters and 57.8 million FLOPs. a) and b) reduce the same amount of
parameters but different computational cost.
and ﬁlter-level pruning [Li et al., 2016; Liu et al., 2017;
He et al., 2018b].
The weight-level pruning methods try to ﬁnd out unimportant weights and set them to zeros. In other words, weightlevel pruning can compress deep models because it induces
sparsity in ﬁlters. However, it contributes little to accelerating them unless specialized libraries (such as cuSPARSE) are
used. Unfortunately, the support for these libraries on mobile
devices, especially on FPGA is very limited. Thus, ﬁlter-level
pruning methods are proposed to address this problem. They
locate and remove unimportant ﬁlters in convolutional layers
or unimportant nodes in fully connected layers. In this way,
both space and inference time cost can be saved a lot.
Undoubtedly, the key factor inﬂuencing a ﬁlter-level pruning method’s performance is how it deﬁnes the importance of
ﬁlters. We ﬁnd the current state-of-the-art methods have at
least two of the following problems.
High Redundancy.
Deep models with rich parameters may
suffer from high redundancy[Denil et al., 2013] among ﬁlters,
i.e., different ﬁlters may have similar expressibility. Some
methods 
Lack of Global Vision.
Under the approaches deﬁnition
of some previous methods , the ﬁlters can be
only compared within the same layer. Thus, it is difﬁcult to
specify how many ﬁlters should be pruned for each layer.
Sub-optimal Pruning Strategy.
All the existing methods
are prone to generate sub-optimal pruning strategies because
they neglect that pruning the same number of parameters may
not means reducing the same amount of computation cost.
Take Figure 1 as an example. Pruning the same number of parameters in different layers of VGG16, we may reduce more
computation cost by pruning the lower layers. Previous methods are insensitive to such differences.
To address the above limitations, we develop a correlationbased pruning algorithm (COP) to detect the redundancy
among ﬁlters efﬁciently.
We normalize the ﬁlter importance of different layers to the same scale to enable global
comparison.
In addition, we add parameter-quantity and
computation-cost regularization terms to enable ﬁne-grained
ﬁlter pruning. The users can customize the compression simply through weight allocation according to their preference,
i.e., whether they want to reduce more parameters or inference complexity.
It is worthwhile to highlight the advantages of COP:
• We propose a novel ﬁlter-level pruning algorithm for
deep model compression, which can a) reduce the redundancy among ﬁlters signiﬁcantly, b) learn proper pruning ratios for different layers automatically, and c) enable ﬁne-grained pruning given users’ preference.
• Extensive experiments on public datasets have demonstrated COP’s advantages over the current state-of-theart methods.
• We also evaluate COP on specially designed compact
neural network, MobileNets. COP still produces reasonably good compression ratio with little loss on the
inference performance.
Related Works
Network pruning has become one of the most popular methods to compress and accelerate deep CNNs. Network pruning includes two categories: weight-level pruning and ﬁlterlevel pruning. Weight-level pruning sets unimportant weights
in ﬁlters to zero, which induces sparsity into ﬁlters. However, according to [Luo et al., 2017; Liu et al., 2017;
Molchanov et al., 2016], it is difﬁcult for weight-level pruning to accelerate deep CNNs without specialized libraries or
hardware. Filter-level pruning solves this problem, it prunes
unimportant ﬁlters in convolutional layers, which compresses
and accelerates the deep CNNs simultaneously.
In recent years, many ﬁlter-level pruning methods have
been proposed. [Li et al., 2016] evaluates the importance of
ﬁlters through the sum of its absolute weights, i.e., l1 norm,
and decide pruned ratio for each layer manually.
al., 2017] evaluates the importance of ﬁlters through reconstruction loss and use the same pruned ratio for all layers, i.e.,
large loss induced by pruning means high importance of the
pruned ﬁlter. [He et al., 2017] evaluates the importance of ﬁlters based on LASSO regression. All the methods mentioned
above only evaluate the local importance for ﬁlters, i.e., the
importance could only be compared within the same layer,
so they have to specify the pruned ratio for each layer manually. [Molchanov et al., 2016] evaluates the importance of
ﬁlters through Taylor expansion, whose value could be compared globally.
[Liu et al., 2017] also evaluates the global
importance of ﬁlters based on the scale of batch normalization (BN) layer. However, as we say in Section 1, all these
methods neglect the inequality between reducing parameters
and reducing computational cost, the users cannot customize
the pruned model for different purposes(smaller or faster).
PFA is a ﬁlter-level pruning method, which decides the
pruned ratio for each layer by performing PCA on feature
maps and evaluates the importance of ﬁlters by doing correlation analysis on feature maps. PFA is similar to our method
because both PFA and COP use correlation to evaluating the
importance of ﬁlters, however, there are three main differences between PFA and COP: 1) PFA is a data-driven method.
It performs the correlation analysis on the activated feature
maps, while COP performs correlation analysis on the ﬁlter
weights. PFA needs all the training data when evaluating the
importance of ﬁlters, which consumes more computing resources; however, COP only uses the trained model parameters when evaluating ﬁlters’ importance. 2) PFA is a twostage pruning method, i.e., it needs to perform PCA ﬁrst and
then correlation analysis when pruning ﬁlters; COP only performs correlation analysis. 3) As we mentioned in Section 1,
PFA is a sub-optimal pruning method; COP uses two regularization terms to generate ﬁne-grained pruning plans, so users
could customize their pruned model for different purposes.
Symbols and Annotations
Assuming there are L layers in a deep model, let P l be the
lth layer in the deep model, whose input is Xl, output is Y l
and weight is W l. We will omit superscript l for simplicity in
the cases of no confusion.
When P l is a fully connected layer, W l is of shape M l ×
N l, where M l is the number of input nodes and N l is the
number of output nodes; Xl is of shape M l × 1; Y l is of
shape N l × 1. Let Xl
m be the mth node of Xl, Y l
n be the nth
node of Y l. Let ⃗ωm be the mth row of W l, namely ⃗ωm =
[Wm,1, · · · , Wm,n].
When P l is a convolutional layer, W l is of shape Kl ×
Kl × M l × N l, where Kl is the kernel width (assumed to be
symmetric), M l is the number of input channel, N l is the
number of output channel; Xl is of shape Il × Il × M l;
Y l is of shape Ol × Ol × N l; Il, Ol are the size of input and output feature maps respectively. Xl
m is the mth
input feature map in the lth layer. Let ⃗ωi,j,m be the vector
[Wi,j,m,1, Wi,j,m,2, · · · , Wi,j,m,n].
µ⃗ωm is the mean of ⃗ωm. σ⃗ωm is the standard deviation of
⃗ωm. Let E[·] be the expectation function.
Note that, in a convolutional layer, pruning feature maps
is equivalent to pruning ﬁlters, and the importance of feature
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Figure 2: The ﬁgure is an illustration of the pruning process on a
fully connected layer. Xm1 and Xm2 are input nodes, ⃗ωm1 and
⃗ωm2 are the weights connected with them. If ⃗ωm2 ≈α⃗ωm1, then
Xm2 could be pruned. In the ﬁne-tuned model, Xm1 would change
to (Xm1 + αXm2) because ⃗ωm1Xm1 + ⃗ωm2Xm2 ≈⃗ωm1(Xm1 +
maps could also be seen as the importance of ﬁlters. Therefore, for simplicity, we will not differentiate pruning ﬁlters
from pruning feature maps.
We propose three techniques to avoid the three limitations
mentioned in Section 1. 1) Evaluate the correlation-based
importance of ﬁlters to remove redundant ﬁlters.
2) Normalize the correlations from all layers to the same scale
for global comparison.
3) Adding parameter-quantity and
computational-cost regularization terms to the importance
evaluation. The users could choose freely whether they want
to reduce more parameters or more computational cost by adjusting the weights of two regularization terms.
Normalized Correlation-Based Importance
We will ﬁrst take fully connected layers as examples and introduce the calculation of correlation-based importance in detail, then generalize it to convolutional layers.
Figure 2 is an illustration of our idea. In a fully connected
layer, Yn is calculated as Yn = P
m Wm,nXm, we omit the
bias term for simplicity. If ∃m1, m2 ∈[1, M], α ∈R such
that ⃗ωm2 ≈α⃗ωm1, then Yn could also be computed as Equation 1 for all n ∈[1, N]. ϵ is the loss induced because ⃗ωm2
and ⃗ωm1 are not strictly linearly related. So, we could merge
Xm1 and Xm2 through pruning and ﬁne-tuning.
m/∈{m1,m2}
Wm,nXm + Wm1,nXm1 + Wm2,nXm2
m/∈{m1,m2}
Wm,nXm + Wm1,nXm1 + αWm1,nXm2 + ϵ
m/∈{m1,m2}
Wm,nXm + Wm1,n(Xm1 + αXm2) + ϵ
⃗ωm2 ≈α⃗ωm1 implies that ⃗ωm1 and ⃗ωm2 are activated in
similar patterns. On the other hand, it also implies Xm1 and
compute the mean importance
Figure 3: The ﬁgure is a residual block without linear projection.
The input feature maps are on the left of the dashed boxes, and the
output feature maps are on the right. The numbers on the feature
maps are the importance of feature maps. We compute the mean
importance of input feature maps and corresponding output feature
maps as overall importance and prune the input and output feature
maps simultaneously.
Xm2 express similar information. We propose to measure the
redundancy between ⃗ωm1 and ⃗ωm2 by using Pearson correlation, which also indicates the similarity between Xm1 and
Xm2 (Equation 2).
sim(Xm1, Xm2) = corr(⃗ωm1, ⃗ωm2)
= E[(⃗ωm1 −µ⃗ωm1)(⃗ωm2 −µ⃗ωm2)]
σ⃗ωm1σ⃗ωm2
The redundancy evaluation mentioned above could be generalized to convolutional layer with slight modiﬁcations because a fully connected layer could be seen as a convolutional
layer whose weights are of shape 1 × 1 × M × N. For a convolutional layer whose ﬁlters are of shape K × K × M × N,
one ﬁlter can be seen as a group of K × K independent
We regroup the ﬁlter tensors into K × K sets of
nodes and calculate the pair-wise node-correlations accordingly. Finally, we calculate the ﬁlter-correlation by averaging
the node-correlations on the respective K × K nodes of the
given ﬁlter (as shown in Equation 3).
sim(Xm1, Xm2) =
corr(⃗ωi,j,m1, ⃗ωi,j,m2) (3)
For two correlated ﬁlters, we have to remove one of them.
It is hard to tell which is more important if we only look at
their correlation. Instead, if one ﬁlter is highly correlated with
many other ﬁlters, we believe it can be removed because other
ﬁlters can take over its job. Thus, among the correlation coef-
ﬁcients between the given ﬁlter and the others, we select the
k highest ones and average them to get the importance of the
However, the importance deﬁned is still a local one because the correlations are calculated among ﬁlters within the
same layer. Generally, the correlation distributions of different layers are quite different due to their different functions
and scopes. Thus, we need to normalize the importance to
enable cross-layer comparison. There are various methods
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Table 1: The table shows the results of all the algorithms on CIFAR
and ImageNet. Alg means the algorithm names. Acc means the
classiﬁcation accuracy, Prr means the parameter-reduction ratio, and
Frr means the FLOPs-reduction ratio. Higher is better. VGG and
ResNet are the baseline models for pruning. Results with “*” are
got with their released code, and others are from original papers.
to normalize the importance. [Molchanov et al., 2016] proposed a simple layer-wise l2-normalization which re-scales
the importance distributions of different layers. Similarly, one
can also normalize different importance distributions with l1normalization. However, we argue that they are not suitable
for correlation-based importance because they will cause improper scaling of the importance. For instance, for layers with
many ﬁlters, the importance of each ﬁlter tends to be very
small because the denominator is large. Consequently, the
procedure will always try to prune the layers with more ﬁlters. We propose to use max-normalization. Speciﬁcally, we
normalize the correlation distribution of each layer by dividing them by the maximal importance to align the correlation
distributions to . Formally, we deﬁne the normalized importance as Equation 4.
maxp̸=q(sim(Xlp, Xlq))),
p, q, n ∈[1, M] and n ̸= m
Regularizers
As we discussed above, reducing the same number of parameters from different positions in a neural network may reduce
quite a different amount of computation cost. To make our approach aware of such differences, we add two regularization
terms to enable ﬁne-grained pruning plan generation. As a result, the users can customize the pruning plan through simple
weight-allocation. Speciﬁcally, we add parameter-quantity
and computational-cost regularization terms when evaluating
the importance of ﬁlters. Pruning a ﬁlter of the lth layer in-
ﬂuences the parameter quantity and computational cost of the
lth layer and the (l+1)th layer, so the parameter quantity (Sl)
and computational cost (Cl) related with the lth layer are de-
ﬁned in Equation 5. Further, the regularizers are deﬁned in
Equation 6. Note that ﬁlters in the same layer share the same
regularizers, and regularizers for fully connected layers could
be computed in a similar way. Adding these two terms to the
importance deﬁned above will empower the procedure with
the sensitivity of parameter-quantity and computation-cost.
Thus, it can generate ﬁne-grained pruning plans.
Sl = (KlKlM lN l) + (Kl+1Kl+1M l+1N l+1)
Cl = 2IlIlKlKlM lN l
+ 2Il+1Il+1Kl+1Kl+1M l+1N l+1
Regl =β(1 −
log(max(Cu)))
log(max(Su))), u ∈[1, L]
Finally, we deﬁne the regularized importance of Xl
Equation 7.
m) = Imp(Xl
The COP algorithm is a three-step pipeline : 1) Calculate
the regularized importance for all ﬁlters and nodes; 2) Specify
a global pruning ratio and remove the least important ones
according to the ratio; 3) Fine-tune the pruned model with
original data. Note that we only prune the network and ﬁnetune it once. In contrast, many existing methods follow an
alternative-prune-and-ﬁne-tune manner.
Pruning for Depth-Separable Convolutional Layer.
depth-separable convolutional layer contains a depth-wise
convolutional layer and a point-wise convolutional layer. A
point-wise convolutional layer is actually a convolutional
layer whose ﬁlters are of shape 1 × 1 × M × N, so we prune
the point-wise layers as normal convolutional layers. As for
depth-wise convolutional layer, the number of input and output feature maps are always the same; the input feature maps
of a point-wise layer are also the output feature maps of a
depth-wise layer [Howard et al., 2017]. Therefore, pruning
the point-wise layer will immediately prune the depth-wise
layer; we do not need to prune the depth-wise layer again.
Pruning for Residual Block.
A residual block contains
more than one convolutional layer, and the number of input
and output feature maps must be equal for a residual block
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
Table 2: The table shows the results of pruning MobileNets on CI-
FAR10 and ImageNet.
Mob means the baseline model.
means thinner MobileNet with width multiplier x. COP-x means
the pruned MobileNets through our approach with ﬁlter-preserving
Table 3: The table shows the results of using different redundancydetectors and global normalization methods in our model. “cos”,
“dp”, “cor” mean cosine, dot-product and correlation redundancy
detectors respectively, and “l1”, “l2” and “max” are global normalization methods. The experiments are performed on CIFAR10.
unless there is an extra linear projection in the block [He et
al., 2016]. Thus for all residual blocks which do not contain
a linear projection, we compute the mean importance of input and output feature maps as overall importance and prune
them simultaneously. Please refer to Figure 3 for details.
Experimental Settings
Datasets and Architecture
We perform the experiments on two well-known public
datasets, CIFAR [Krizhevsky and Hinton, 2009] and ImageNet [Russakovsky et al., 2015]. Speciﬁcally, we use both
CIFAR10 and CIFAR100 in the CIFAR collection.
We test the compression performance of different algorithms on several famous large CNN models, including VG-
GNet [Simonyan and Zisserman, 2014], ResNet [He et al.,
2016], and MobileNet [Howard et al., 2017].
Evaluation Protocol
Following the previous works [Jiang et al., 2018; Liu et al.,
2017; Yu et al., 2018], we record the parameter-reduction ra-
COP(γ = 0, β = 0)
COP(γ = 0, β = 3)
COP(γ = 1, β = 1)
COP(γ = 3, β = 0)
COP(γ = 0, β = 0)
COP(γ = 0, β = 3)
COP(γ = 1, β = 1)
COP(γ = 3, β = 0)
Table 4: The table shows the results of using different β and γ on CI-
FAR100. β is the weight of computational-cost regularization terms,
and γ is the weight of parameter-quantity regularization terms.
tio (Prr) and FLOPs-reduction ratio (Frr) of each algorithm
compared with the original model.
A higher parameterreduction ratio means a smaller model size, and a higher
FLOPs-reduction ratio means a faster inference.
Compared Algorithms
We select several recent state-of-the-art methods which are
all ﬁlter-level pruning algorithms:
• NS [Liu et al., 2017] evaluates the importance of ﬁlters
according to the BN’s scaling factors 1.
• NRE [Jiang et al., 2018] proposes to prune the model by
minimizing the reconstruction error of nonlinear units.
• PFA [Suau et al., 2018] decides the pruned ratio for each
layer by performing PCA on feature maps. They evaluate the importance of ﬁlters by doing correlation analysis
on feature maps.
• SFP [He et al., 2018a] evaluates the importance of ﬁlters
with l2-norm and prunes ﬁlters in a soft manner, i.e., the
pruned ﬁlters may be retrieved after ﬁne-tuning 2.
• PFGM [He et al., 2018b] evaluates the importance of
ﬁlters by analyzing the geometric correlation among the
ﬁlters within the same layer.
We also compare our pruned MobileNets with thinner
MobileNets[Howard et al., 2017]. MobileNets are compact
deep neural networks which reduce the models’ size and computational cost by replacing convolutional layers with depthseparable convolutional layers. [Howard et al., 2017] also
proposes thinner MobileNets to balance accuracy and resources consumption.
Conﬁguration
We set the batch-size of the stochastic gradient descent algorithm (SGD) to be 128, the momentum coefﬁcient to be 0.9
for all models on CIFAR. For the VGG16 model on CIFAR,
we use a weight decay rate of 0.0015, and we use a weight
decay rate of 0.0006 and 0.0002 for the ResNet32 and MobileNet respectively on CIFAR.
1 
2 
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)
COP-Pruned
COP-Preserved
SFP-Preserved
NS-Preserved
Figure 4: The ﬁgure shows the output feature maps of “conv1 1” in VGG16; the input is a random image from CIFAR10. COP-pruned means
feature maps which are pruned by COP, COP-preserved means feature maps which are not pruned. SFP-preserved and NS-preserved mean
feature maps which are preserved by these two algorithms. As the ﬁgure shows, COP-pruned contains many feature maps which look similar,
but feature maps in COP-preserved are less similar to each other. However, SFP-preserved and NS-preserved still contain many feature maps
which are similar, so the model pruned by these two algorithms are still of high redundancy. The ﬁgure implies that COP eliminates more
redundancy of CNNs than SFP and NS.
When training on ImageNet dataset, we use the same architecture and training policy with the original paper [Simonyan
and Zisserman, 2014] for VGG11. We use the same architecture with the original paper [Howard et al., 2017] for MobileNets but the same training policy with VGG11 because
it takes too much time to train MobileNets with the ofﬁcial
training policy.
We use k = 3 for TopK when pruning the model.
Results and Analysis
Compression Efﬁciency
For the methods which do not report the results on given
datasets and do not release their code either, we pass them
in the experiments. The performance of different algorithms
on the three datasets is given in Table 1, we experiment with
several pruned ratio on COP and choose the maximal pruned
ratio under the constraint of acceptable accuracy loss. On
CIFAR, all the algorithms prune the VGG16 and ResNet32
models to get their compressed versions. On ImageNet, all
the algorithms prune the VGG11 and ResNet18 models to
get their compressed versions. As is shown in Table 1, we
can draw some conclusions as follows:
1. COP achieves a higher compression ratio and speedup
ratio with similar accuracy to other algorithms. Especially, COP gets the highest numbers on all three metrics
on CIFAR100.
2. COP can prune much more parameters than most of the
compared algorithms (e.g., NS, SFP, and PFGM) on different datasets and architecture because COP can detect
the redundancy among ﬁlters better.
3. For the algorithms which can prune the similar number of parameters to COP (e.g., NRE and PFGM), COP
gets more speed-up, which is the contribution of the
computation-cost and parameter-size regularizers.
Though the MobileNet is a specially designed compact network, we can still prune it without much performance loss.
The results are given in Table 2. MobileNet has its own way
to compress itself [Howard et al., 2017]. We can see that COP
can get better numbers in all the metrics.
Ablation Study
Redundancy Detector.
We have tried dot-product similarity, cosine similarity, and Pearson correlation as the
redundancy detectors, and also tried l1-, l2-, and maxnormalization methods as discussed above. The results are
shown in Table 3.
We can see that Pearson correlation
and max-normalization are the best conﬁgurations because
the Pearson correlation detect the redundancy better and the
max-normalization is insensitive to the layer-width.
Regularizer Efﬁciency.
We try different weights for two
regularization terms, and the results are shown in Table 4. We
can see that the regularizer do have signiﬁcant impacts on the
compression. Larger γ reduces more parameters while larger
β reduces more computation cost, which is exactly consistent
with our expectation.
Case Study
COP focuses on observing redundant ﬁlters, so our pruned
models contain less redundancy than others. With less redundancy, feature maps in the model mainly express diversi-
ﬁed information. Figure 4 is an example of visualized feature
maps of “conv1 1” in VGG16. Every feature map pruned by
COP expresses similar information to those preserved. We
compare COP with SFP and NS. SFP and NS can prune many
unimportant feature maps, but there are still some similar feature maps being preserved.
Conclusion
We propose a novel ﬁlter-level pruning method, COP, to address the limitations of previous works in the following aspects: removing redundancy among ﬁlters; enabling crosslayer importance comparison; generating ﬁne-grained pruning strategies (sensitive to desired computation cost and
model size). Extensive experiments have shown our significant advantages over other state-of-the-art methods. Moreover, COP can also prune specially designed compact networks such as MobileNet and get larger compression ratio
and speedup ratio than its own compression method.
Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)