Collaborative Filtering via Group-Structured
Dictionary Learning
Zolt´an Szab´o∗, Barnab´as P´oczos†, and Andr´as L˝orincz∗
∗Faculty of Informatics, E¨otv¨os Lor´and University, P´azm´any P´eter s´et´any 1/C, H-1117 Budapest, Hungary
Email: , , Web: 
†Carnegie Mellon University, Robotics Institute, 5000 Forbes Ave, Pittsburgh, PA 15213
Email: , Web: 
Abstract—Structured sparse coding and the related structured
dictionary learning problems are novel research areas in machine
learning. In this paper we present a new application of structured
dictionary learning for collaborative ﬁltering based recommender
systems. Our extensive numerical experiments demonstrate that
the presented technique outperforms its state-of-the-art competitors and has several advantages over approaches that do not put
structured constraints on the dictionary elements.
Keywords-collaborative ﬁltering, structured dictionary learning
I. INTRODUCTION
The proliferation of online services and the thriving electronic commerce overwhelms us with alternatives in our daily
lives. To handle this information overload and to help users
in efﬁcient decision making, recommender systems (RS) have
been designed. The goal of RSs is to recommend personalized
items for online users when they need to choose among several
items. Typical problems include recommendations for which
movie to watch, which jokes/books/news to read, which hotel
to stay at, or which songs to listen to.
One of the most popular approaches in the ﬁeld of recommender systems is collaborative ﬁltering (CF). The underlying
idea of CF is very simple: Users generally express their tastes
in an explicit way by rating the items. CF tries to estimate
the users’ preferences based on the ratings they have already
made on items and based on the ratings of other, similar users.
For a recent review on recommender systems and collaborative
ﬁltering, see e.g., .
Novel advances on CF show that dictionary learning based
approaches can be efﬁcient for making predictions about
users’ preferences . The dictionary learning based approach
assumes that (i) there is a latent, unstructured feature space
(hidden representation) behind the users’ ratings, and (ii) a
rating of an item is equal to the product of the item and the
user’s feature. To increase the generalization capability, usually
ℓ2 regularization is introduced both for the dictionary and for
the users’ representation.
There are several problems that belong to the task of dictionary learning , a.k.a. matrix factorization . This set of
problems includes, for example, (sparse) principal component
A compressed version of the paper has been accepted for publication at
the 10th International Conference on Latent Variable Analysis and Source
Separation .
analysis , independent component analysis , independent
subspace analysis , non-negative matrix factorization ,
and structured dictionary learning, which will be the target of
our paper.
One predecessor of the structured dictionary learning problem is the sparse coding task , which is a considerably
simpler problem. Here the dictionary is already given, and we
assume that the observations can be approximated well enough
using only a few dictionary elements. Although ﬁnding the
solution that uses the minimal number of dictionary elements
is NP hard in general , there exist efﬁcient approximations.
One prominent example is the Lasso approach , which
applies convex ℓ1 relaxation to the code words. Lasso does
not enforce any group structure on the components of the
representation (covariates).
However, using structured sparsity, that is, forcing different
kind of structures (e.g., disjunct groups, trees) on the sparse
codes can lead to increased performances in several applications. Indeed, as it has been theoretically proved recently
structured sparsity can ease feature selection , , and
makes possible robust compressed sensing with substantially
decreased observation number . Many other real life
applications also conﬁrm the beneﬁts of structured sparsity,
for example (i) automatic image annotation , (ii) groupstructured feature selection for micro array data processing
 – , (iii) multi-task learning problems (a.k.a. transfer
learning) – , (iv) multiple kernel learning , , (v)
face recognition , and (vi) structure learning in graphical
models , . For an excellent review on structured
sparsity, see .
All the above mentioned examples only consider the structured sparse coding problem, where we assume that the
dictionary is already given and available to us. A more
interesting (and challenging) problem is the combination of
these two tasks, i.e., learning the best structured dictionary
and structured representation. This is the structured dictionary
learning (SDL) problem. SDL is more difﬁcult; one can ﬁnd
only few solutions in the literature – . This novel ﬁeld
is appealing for (i) transformation invariant feature extraction
 , (ii) image denoising/inpainting , , , (iii)
background subtraction , (iv) analysis of text corpora ,
and (v) face recognition .
Our goal is to extend the application domain of SDL in
the direction of collaborative ﬁltering. With respect to CF,
further constraints appear for SDL since (i) online learning
is desired and (ii) missing information is typical. There are
good reasons for them: novel items/users may appear and user
preferences may change over time. Adaptation to users also
motivate online methods. Online methods have the additional
advantage with respect to ofﬂine ones that they can process
more instances in the same amount of time, and in many cases
this can lead to increased performance. For a theoretical proof
of this claim, see . Furthermore, users can evaluate only a
small portion of the available items, which leads to incomplete
observations, missing rating values. In order to cope with these
constraints of the collaborative ﬁltering problem, we will use
a novel extension of the structured dictionary learning problem, the so-called online group-structured dictionary learning
(OSDL) . OSDL allows (i) overlapping group structures
with (ii) non-convex sparsity inducing regularization, (iii)
partial observation (iv) in an online framework.
Our paper is structured as follows: We brieﬂy review the
OSDL problem, its cost function, and optimization method
in Section II. We cast the CF problem as an OSDL task in
Section III. Numerical results are presented in Section IV.
Conclusions are drawn in Section V.
Notations. Vectors (a) and matrices (A) are denoted by
bold letters. diag(a) represents the diagonal matrix with coordinates of vector a in its diagonal. The ith coordinate of vector
a is ai. Notation |·| means the number of elements of a set and
the absolute value for a real number. For set O ⊆{1, . . . , d},
aO ∈R|O| denotes the coordinates of vector a ∈Rd in O. For
matrix A ∈Rd×D, AO ∈R|O|×D stands for the restriction
of matrix A to the rows O. I and 0 denote the identity and
the null matrices, respectively. AT is the transposed form of
A. For a vector, the max operator acts coordinate-wise. The
ℓp (quasi-)norm of vector a ∈Rd is ∥a∥p = (Pd
i=1 |ai|p)
(p > 0). Sd
p = {a ∈Rd : ∥a∥p ≤1} denotes the ℓp unit sphere
in Rd. The point-wise and scalar products of a, b ∈Rd are
denoted by a ◦b = [a1b1; . . . ; adbd] and by ⟨a, b⟩= aT b,
respectively. For a set system G, the coordinates of vector
a ∈R|G| are denoted by aG (G ∈G), that is, a = (aG)G∈G.
ΠC(x) = argminc∈C∥x −c∥2 is the projection of point
x ∈Rd to the convex closed set C ⊆Rd. Partial derivative of
function h w.r.t. variable x in x0 is ∂h
∂x(x0). The non-negative
ortant of Rd is Rd
+ = {x ∈Rd : xi ≥0 (∀i)}. For sets, × and
\ denote direct product and difference, respectively.
II. THE OSDL PROBLEM
In this section we brieﬂy review the OSDL approach, which
will be our major tool to solve the CF problem. The OSDL
cost function is treated in Section II-A, its optimization idea
is detailed in Section II-B.
A. Cost Function
The online group-structured dictionary learning (OSDL)
task is deﬁned with the following quantities. Let the dimension
of the observations be denoted by dx. Assume that in each
time instant (i = 1, 2, . . .) a set Oi ⊆{1, . . . , dx} is given,
that is, we know which coordinates are observable at time i,
and the observation is xOi. Our goal is to ﬁnd a dictionary
D ∈Rdx×dα that can approximate the observations xOi well
from the linear combination of its columns. The columns of
D are assumed to belong to a closed, convex, and bounded
set D = ×dα
i=1Di. To formulate the cost of dictionary D,
ﬁrst a ﬁxed time instant i, observation xOi, dictionary D is
considered, and the hidden representation αi associated to this
(xOi, D, Oi) triple is deﬁned. Representation αi is allowed to
belong to a closed, convex set A ⊆Rdα (αi ∈A) with
certain structural constraints. The structural constraint on αi
are expressed by making use of a given G group structure,
which is a set system (also called hypergraph) on {1, . . ., dα}.
It is also assumed that weight vectors dG ∈Rdα (G ∈G)
are available for us and that they are positive on G and 0
otherwise. Representation α belonging to a triple (xO, D, O)
is deﬁned as the solution of the structured sparse coding task
l(xO, DO) = lA,κ,G,{dG}G∈G,η(xO, DO)
2 ∥xO −DOα∥2
where l(xO, DO) denotes the loss, κ > 0, and
Ω(y) = ΩG,{dG}G∈G,η(y) = ∥(∥dG ◦y∥2)G∈G∥η
is the structured regularizer associated to G and {dG}G∈G,
η ∈(0, 2). Here, the ﬁrst term of (2) is responsible for the
quality of approximation on the observed coordinates, whereas
for η ≤1 the other term [(3)] constrains the solution according
to the group structure G similarly to the sparsity inducing
regularizer Ωin : it eliminates the terms ∥dG ◦y∥2
(G ∈G) by means of ∥·∥η. The OSDL problem is deﬁned
as the minimization of the cost function:
D∈D ft(D) :=
l(xOi, DOi),
that is, the goal is to minimize the average loss belonging to
the dictionary, where ρ is a non-negative forgetting factor. If
ρ = 0, the classical average ft(D) = 1
i=1 l(xOi, DOi) is
recovered.
As an example, let Di = Sdx
(∀i), A = Rdα. In this
case, columns of D are restricted to the Euclidean unit sphere
and we have no constraints for α. Now, let |G| = dα and
G = {desc1, . . . , descdα}, where desci represents the ith node
and its children in a ﬁxed tree. Then the coordinates αi are
searched in a hierarchical tree structure and the hierarchical
dictionary D is optimized accordingly.
B. Optimization
Optimization of cost function (4) is equivalent to the joint
optimization of dictionary D and representation {αi}t
D∈D,{αi∈A}t
ft(D, {αi}t
2 ∥xOi −DOiαi∥2
2 + κΩ(αi)
D is optimized by using the sequential observations xOi
online in an alternating manner:
1) The actual dictionary estimation Dt−1 and sample xOt
is used to optimize (2) for representation αt.
2) For the estimated representations {αi}t
i=1, the dictionary estimation Dt is derived from the quadratic optimization problem
ˆft(Dt) = min
D∈D ft(D, {αi}t
1) Representation optimization (α): Note that (2) is a
non-convex optimization problem with respect to α. The
variational properties of norm η can be used to overcome this
problem. One can show, alike to , that by introducing an
auxiliary variable z ∈R|G|
+ , the solution α of the optimization
task (9) is equal to the solution of (2):
α∈A,z∈R|G|
J(α, z), where
2 ∥xOt −(Dt−1)Otα∥2
αT diag(ζ)α + ∥z∥β
ζ = ζ(z) ∈Rdα and ζj = P
2 /zG. The
optimization of (9) can be carried out by iterative alternating
steps. One can minimize the quadratic cost function on the
convex set A for a given z with standard solvers .
Then, one can use the variation principle and ﬁnd solution
z = (zG)G∈G for a ﬁxed α by means of the explicit expression
zG = ∥dG ◦α∥2−η
∥(∥dG ◦α∥2)G∈G∥η−1
Note that for numerical stability, smoothing z = max(z, ε)
(0 < ε ≪1) is suggested in practice.
2) Dictionary optimization (D): The block-coordinate descent (BCD) method is used for the optimization of D:
columns dj in D are optimized one-by-one by keeping the
other columns (di, i ̸= j) ﬁxed. For a given j, ˆft is quadratic
in dj. The minimum is found by solving
∂dj (uj) = 0,
and then this solution is projected to the constraint set Dj
(dj ←ΠDj(uj)). One can show by executing the differentiation that uj satisﬁes the linear equation system
Cj,tuj = bj,t −ej,t + Cj,tdj,
i,j ∈Rdx×dx,
∆iDαiαi,j ∈Rdx,
i = [b1,t, . . . , bdα,t],
matrices Cj,t are diagonal, Bt ∈Rdx×dα, and ∆i ∈Rdx×dx
is the diagonal matrix representation of the Oi set (for j ∈
Oi the jth diagonal is 1 and is 0 otherwise). It is sufﬁcient
to update statistics {{Cj,t}dα
j=1, Bt, {ej,t}dα
j=1} online for the
optimization of ˆft, which can be done exactly for Cj,t and
Cj,t = γtCj,t−1 + ∆tα2
Bt = γtBt−1 + ∆txtαT
where γt =
ρ and the recursions are initialized by (i)
Cj,0 = 0, B0 = 0 for ρ = 0 and (ii) in an arbitrary way for
ρ > 0. According to numerical experiences,
ej,t = γtej,t−1 + ∆tDtαtαt,j,
is a good approximation for ej,t with the actual estimation
Dt and with initialization ej,0 = 0. It may be worth noting
that the convergence speed is often improved if statistics are
updated in mini-batches {xOt,1, . . . , xOt,R}.1
III. OSDL BASED COLLABORATIVE FILTERING
We formulate the CF task as an OSDL optimization problem
in Section III-A. According to the CF literature, oftentimes
neighbor-based corrections improve the precision of the estimation. We also use this technique (Section III-B) to improve
the OSDL estimations.
A. CF Casted as an OSDL Problem
Below, we transform the CF task into an OSDL problem.
Consider the tth user’s known ratings as OSDL observations
xOt. Let the optimized group-structured dictionary on these
observations be D. Now, assume that we have a test user
and his/her ratings, i.e., xO ∈R|O|. The task is to estimate
x{1,...,dx}\O, that is, the missing coordinates of x (the missing
ratings of the user) that can be accomplished as follows:
1) Remove the rows of the non-observed {1, . . ., dx}\O
coordinates from D. The obtained |O|×dα sized matrix
DO and xO can be used to estimate α by solving the
structured sparse coding problem (2).
2) Using the estimated representation α, estimate x as
B. Neighbor Based Correction
According to the CF literature, neighbor based correction
schemes may further improve the precision of the estimations
 . This neighbor correction approach
assumption
jokes/movies) are rated similarly and
• can be adapted to OSDL-based CF estimation in a natural
Here, we detail the idea. Let us assume that the similarities
sij ∈R (i, j ∈{1, . . . , dx}) between individual items are
given. We shall provide similarity forms in Section IV-B. Let
dkαt ∈R be the OSDL estimation for the rating of the kth
non-observed item of the tth user (k ̸∈Ot), where dk ∈
R1×dα is the kth row of matrix D ∈Rdx×dα, and αt ∈Rdα
is computed according to Section III-A.
Let the prediction error on the observable item neighbors (j)
of the kth item of the tth user (j ∈Ot\{k}) be djαt −xjt ∈
R. These prediction errors can be used for the correction of
1The Matlab code of the OSDL method is available at 
hu/szzoli.
the OSDL estimation (dkαt) by taking into account the sij
similarities:
ˆxkt = dkαt + γ1
j∈Ot\{k} skj(djαt −xjt)
j∈Ot\{k} skj
ˆxkt = γ0(dkαt) + γ1
j∈Ot\{k} skj(djαt −xjt)
j∈Ot\{k} skj
where k ̸∈Ot. Here, (19) is analogous to the form of ,
(20) is a simple modiﬁcation: it modulates the ﬁrst term with
a separate γ0 weight.
IV. NUMERICAL RESULTS
We have chosen the Jester dataset (Section IV-A) for the
illustration of the OSDL based CF approach. It is a standard
benchmark for CF. We detail our preferred item similarities in
Section IV-B. To evaluate the CF based estimation, we use the
performance measures given in Section IV-C. Section IV-D is
about our numerical experiences.
A. The Jester Dataset
The dataset contains 4, 136, 360 ratings from 73, 421
users to 100 jokes on a continuous [−10, 10] range. The worst
and best possible gradings are −10 and +10, respectively. A
ﬁxed 10 element subset of the jokes is called gauge set and it
was evaluated by all users. Two third of the users have rated
at least 36 jokes, and the remaining ones have rated between
15 and 35 jokes. The average number of user ratings per joke
B. Item Similarities
In the neighbor correction step (19) or (20) we need the sij
values representing the similarities of the ith and jth items.
We deﬁne this value as the similarity of the ith and jth rows
(di and dj) of the optimized OSDL dictionary D :
sij = sij(di, dj) =
max(0, ⟨di, dj⟩)
∥di∥2 ∥dj∥2
sij = sij(di, dj) =
∥di∥2 ∥dj∥2
where β > 0 is the parameter of the similarity measure.
Quantities sij are non-negative; if the value of sij is close
to zero (large) then the ith and jth items are very different
(very similar).
C. Performance Measure
In our numerical experiments we used the RMSE (root mean
square error) and the MAE (mean absolute error) measure for
the evaluation of the quality of the estimation, since these are
the most popular measures in the CF literature. The RMSE
and MAE measure is the average squared/absolute difference
of the true and the estimated rating values, respectively:
(xit −ˆxit)2,
|xit −ˆxit|,
where S denotes either the validation or the test set.
D. Evaluation
Here we illustrate the efﬁciency of the OSDL-based CF estimation on the Jester dataset (Section IV-A) using the RMSE
and MAE performance measures (Section IV-C). We start our
discussion with the RMSE results. The MAE performance
measure led to similar results; for the sake of completeness we
report these results at the end of this section. To the best of our
knowledge, the top results on this database are RMSE = 4.1123
 and RMSE = 4.1229 . Both works are from the same
authors. The method in the ﬁrst paper is called item neighbor
and it makes use of only neighbor information. In , the
authors used a bridge regression based unstructured dictionary
learning model—with a neighbor correction scheme—, they
optimized the dictionary by gradient descent and set dα to
100. These are our performance baselines.
To study the capability of the OSDL approach in CF, we
focused on the following issues:
• Is structured dictionary D beneﬁcial for prediction purposes, and how does it compare to the dictionary of
classical (unstructured) sparse dictionary?
• How does the OSDL parameters and the similarity/neighbor correction applied affect the efﬁciency of the
prediction?
• How do different group structures G ﬁt to the CF task?
In our numerical studies we chose the Euclidean unit sphere
for Di = Sdx
(∀i), and A = Rdα, and no additional weighting
was applied (dG = χG, ∀G ∈G, where χ is the indicator
function). We set η of the group-structured regularizer Ωto
0.5. Group structure G of vector α was realized on
• a d × d toroid (dα = d2) with |G| = dα applying r ≥0
neighbors to deﬁne G. For r = 0 (G = {{1}, . . ., {dα}})
the classical sparse representation based dictionary is
recovered.
• a hierarchy with a complete binary tree structure. In this
– |G| = dα, and group G of αi contains the ith node
and its descendants on the tree, and
– the size of the tree is determined by the number of
levels l. The dimension of the hidden representation
is then dα = 2l −1.
The size R
of mini-batches was
forgetting
structure inducing
regularizer Ω
26 , . . . ,
214 }. We studied similarities S1,
S2 [see (21)-(22)] with both neighbor correction schemes
[(19)-(20)]. In what follows, corrections based on (19) and (20)
will be called S1, S2 and S0
2, respectively. Similarity parameter β was chosen from the set {0.2, 1, 1.8, 2.6, . . ., 14.6}.
In the BCD step of the optimization of D, 5 iterations were
applied. In the α optimization step, we used 5 iterations,
whereas smoothing parameter ǫ was 10−5.
We used a 90% −10% random split for the observable
ratings in our experiments, similarly to :
• training set (90%) was further divided into 2 parts:
– we chose the 80% observation set {Ot} randomly,
and optimized D according to the corresponding xOt
observations,
– we used the remaining 10% for validation, that is
for choosing the optimal OSDL parameters (r or
l, κ, ρ), BCD optimization parameter (R), neighbor
correction (S1, S2, S0
2), similarity parameter (β),
and correction weights (γis in (19) or (20)).
• we used the remaining 10% of the data for testing.
The optimal parameters were estimated on the validation set,
and then used on the test set. The resulting RMSE/MAE score
was the performance of the estimation.
1) Toroid Group Structure.: In this section we provide
results using toroid group structure. We set d = 10. The size
of the toroid was 10 × 10, and thus the dimension of the
representation was dα = 100.
In the ﬁrst experiment we study how the size of neighborhood (r) affects the results. This parameter corresponds to the
“smoothness” imposed on the group structure: when r = 0,
then there is no relation between the dj ∈Rdα columns in D
(no structure). As we increase r, the dj feature vectors will be
more and more aligned in a smooth way. To this end, we set the
neighborhood size to r = 0 (no structure), and then increased
it to 1, 2, 3, 4, and 5. For each (κ, ρ, β), we calculated the
RMSE of our estimation, and then for each ﬁxed (κ, ρ) pair, we
minimized these RMSE values in β. The resulting validation
and test surfaces are shown in Fig. 1. For the best (κ, ρ) pair,
we also present the RMSE values as a function of β (Fig. 2).
In this illustration we used S0
1 neighbor correction and R = 8
mini-batch size. We note that we got similar results using
R = 16 too. Our results can be summarized as follows.
• For a ﬁxed neighborhood parameter r, we have that:
– The validation and test surfaces are very similar
(see Fig. 1(e)-(f)). It implies that the validation
surfaces are good indicators for the test errors. For
the best r, κ and ρ parameters, we can observe
that the validation and test curves (as functions of
β) are very similar. This is demonstrated in Fig. 2,
where we used r = 4 neighborhood size and S0
neighbor correction. We can also notice that (i) both
curves have only one local minimum, and (ii) these
minimum points are close to each other.
– The quality of the estimation depends mostly on the
κ regularization parameter. As we increase r, the best
κ value is decreasing.
– The estimation is robust to the different choices of
forgetting factors (see Fig. 1(a)-(e)). In other words,
this parameter ρ can help in ﬁne-tuning the results.
• Structured dictionaries (r > 0) are advantageous over
those methods that do not impose structure on the dictionary elements (r = 0). For S0
2 neighbor
corrections, we summarize the RMSE results in Table I.
Based on this table we can conclude that in the studied
parameter domain
– the estimation is robust to the selection of the minibatch size (R). We got the best results using R = 8.
Similarly to the role of parameter ρ, adjusting R can
be used for ﬁne-tuning.
1 neighbor correction lead to the smallest
RMSE value.
– When we increase r up to r = 4, the results improve.
However, for r = 5, the RMSE values do not
improve anymore; they are about the same that we
have using r = 4.
– The smallest RMSE we could achieve was 4.0774,
and the best known result so far was RMSE = 4.1123
 . This proves the efﬁciency of our OSDL based
collaborative ﬁltering algorithm.
– We note that our RMSE result seems to be significantly better than the that of the competitors: we
repeated this experiment 5 more times with different
randomly selected training, test, and validation sets,
and our RMSE results have never been worse than
Similarity parameter (β)
Validation curve
Test curve
Fig. 2: RMSE validation and test curves for toroid group
structure using the optimal neighborhood size r = 4, regularization weight κ =
210 , forgetting factor ρ =
25 , mini-batch
size R = 8, and similarity parameter β = 3.4. The applied
neighbor correction was S0
In the second experiment we studied how the different
neighbor corrections (S1, S2, S0
2) affect the performance
of the proposed algorithm. To this end, we set the neighborhood parameter to r = 4 because it proved to be optimal in the
previous experiment. Our results are summarized in Table II.
From these results we can observe that
• our method is robust to the selection of correction methods. Similarly to the ρ and R parameters, the neighbor
correction scheme can help in ﬁne-tuning the results.
• The introduction of γ0 in (20) with the application of S0
2 instead of S1 and S2 proved to be advantageous
in the neighbor correction phase.
• For the studied CF problem, the S0
1 neighbor correction
method (with R = 8) lead to the smallest RMSE value,
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16
Fig. 1: RMSE validation surfaces [(a)-(e)] and test surfaces (f) as a function of forgetting factor (ρ) and regularization (κ).
For a ﬁxed (κ, ρ) parameter pair, the surfaces show the best RMSE values optimized in the β similarity parameter. The group
structure (G) is toroid. The applied neighbor correction was S0
1. (a): r = 0 (no structure). (b): r = 1. (c): r = 2. (d): r = 3.
(e)-(f): r = 4, on the same scale.
TABLE I: Performance (RMSE) of the OSDL prediction using
toroid group structure (G) with different neighbor sizes r
(r = 0: unstructured case). First-second row: mini-batch size
R = 8, third-fourth row: R = 16. Odd rows: S0
1, even rows:
2 neighbor correction. For ﬁxed R, the best performance is
highlighted with boldface typesetting.
• The R ∈{8, 16} setting yielded us similarly good results.
Even with R = 16, the RMSE value was 4.0777.
2) Hierarchical Group Structure.: In this section we provide results using hierarchical α representation. The group
structure G was chosen to represent a complete binary tree.
In our third experiment we study how the number of levels
(l) of the tree affects the results. To this end, we set the number
of levels to l = 3, 4, 5, and 6. Since dα, the dimension of the
hidden representation α, equals to 2l −1, these l values give
rise to dimensions dα = 7, 15, 31, and 63. Validation and test
TABLE II: Performance (RMSE) of the OSDL prediction for
different neighbor corrections using toroid group structure (G).
Columns: applied neighbor corrections. Rows: mini-batch size
R = 8 and 16. The neighbor size was set to r = 4. For ﬁxed R,
the best performance is highlighted with boldface typesetting.
surfaces are provided in Fig. 3(a)-(c) and (e)-(f), respectively.
The surfaces show for each (κ, ρ) pair, the minimum RMSE
values taken in the similarity parameter β. For the best (κ, ρ)
parameter pair, the dependence of RMSE on β is presented in
Fig. 3(d). In this illustration we used S0
1 neighbor correction,
and the mini-batch size was set to R = 8. Our results are
summarized below. We note that we obtained similar results
with mini-batch size R = 16.
• For ﬁxed number of levels l, similarly to the toroid group
structure (where the size r of the neighborhood was
– validation and test surfaces are very similar, see
Fig. 3(b)-(c). Validation and test curves as a function
of β behave alike, see Fig. 3(d).
TABLE III: Performance (RMSE) of the OSDL prediction for
different number of levels (l) using binary tree structure (G).
First-second row: mini-batch size R = 8, third-fourth row:
R = 16. Odd rows: S0
1, even rows: S0
2 neighbor correction.
For ﬁxed R, the best performance is highlighted with boldface
typesetting.
TABLE IV: Performance (RMSE) of the OSDL prediction for
different neighbor corrections using binary tree structure (G).
Rows: mini-batch size R = 8 and 16. Columns: neighbor
corrections. Neighbor size: r = 4. For ﬁxed R, the best
performance is highlighted with boldface typesetting.
– the precision of the estimation depends mostly on
the regularization parameter κ; forgetting factor ρ
enables ﬁne-tuning.
• The obtained RMSE values are summarized in Table III
2 neighbor corrections. According to the
table, the quality of estimation is about the same for
mini-batch size R = 8 and R = 16; the R = 8 based
estimation seems somewhat more precise. Considering
the neighbor correction schemes S0
1 provided
better predictions.
• As a function of the number of levels, we got the best
result for l = 4, RMSE = 4.1220; RMSE values decrease
until l = 4 and then increase for l > 4.
• Our best obtained RMSE value is 4.1220; it was achieved
for dimension only dα = 15. We note that this small
dimensional, hierarchical group structure based result is
also better than that of with RMSE = 4.1229, which
makes use of unstructured dictionaries with dα = 100.
The result is also competitive with the RMSE = 4.1123
value of .
In our fourth experiment we investigate how the different
neighbor corrections (S1, S2, S0
2) affect the precision of
the estimations. We ﬁxed the number of levels to l = 4, since
it proved to be the optimal choice in our previous experiment.
Our results are summarized in Table IV. We found that
• the estimation is robust to the choice of neighbor corrections,
• it is worth including weight γ0 [see (20)] to improve the
precision of prediction, that is, to apply correction S0
2 instead of S1 and S2, respectively.
• the studied R ∈{8, 16} mini-batch sizes provided similarly good results.
• for the studied CF problem the best RMSE value was
achieved using S0
1 neighbor correction and mini-batch
size R = 8.
When we used the MAE performance measure, our results
Similarity parameter (β)
Validation curve
Test curve
Fig. 5: MAE validation and test curves for toroid group structure using the optimal neighborhood size r = 4, regularization
weight κ =
210 , forgetting factor ρ =
25 , mini-batch size
R = 8, and similarity parameter β = 3.4. The applied
neighbor correction was S0
TABLE V: Performance (MAE) of the OSDL prediction using
toroid group structure (G) with different neighbor sizes r
(r = 0: unstructured case). First-second row: mini-batch size
R = 8, third-fourth row: R = 16. Odd rows: S0
1, even rows:
2 neighbor correction. For ﬁxed R, the best performance is
highlighted with boldface typesetting.
were similar to those of the RMSE. We got the best results
using toroid group structure, thus we present more details for
this case.
• With the usage of structured dictionaries we can get better
results: the estimation errors were decreasing when we
increased the neighbor size r up to 4. (Table V). The
validation and test surfaces/curves are very similar, see
Fig. 4(e)-(f), Fig. 5.
• The quality of the estimation depends mostly on the κ
regularization parameter (Fig. 4(a)-(e)). The applied ρ
forgetting factor, R mini-batch size and neighbor correction method can help in ﬁne-tuning the results, see
Fig. 4(a)-(e), Table V and Table VI, respectively.
• The smallest MAE we could achieve was 3.1544, using
r = 4 neighbor size, S0
1 neighbor correction and R = 8
mini-batch size. The baseline methods led to MAE
= 3.1616, MAE = 3.1606 results. Our approach
outperformed both of the state-of-the-art competitors. We
also repeated this experiment 5 more times with different
randomly selected training, test, and validation sets, and
our MAE results have never been worse than 3.155. This
demonstrates the efﬁciency of our approach.
V. CONCLUSIONS
We have dealt with collaborative ﬁltering (CF) based recommender systems and extended the application domain of
structured dictionaries to CF. We used online group-structured
dictionary learning (OSDL) to solve the CF problem; we
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16 1/8
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16 1/8
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16 1/8
Similarity parameter (β)
Validation curve
Test curve
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16 1/8
Forgetting factor (ρ)
Regularization (−log2(κ))
1/64 1/32 1/16 1/8
Fig. 3: RMSE validation surfaces [(a)-(b), (e)-(f)] and test surfaces (c) as a function of forgetting factor (ρ) and regularization
(κ). (d): validation and test curve using the optimal number of levels l = 4, regularization weight κ =
22 , forgetting factor
ρ = 0, mini-bach size R = 8, similarity parameter β = 1.8. Group structure (G): complete binary tree. Neighbor correction: S0
(a)-(c),(e)-(f): for ﬁxed (κ, ρ) parameter pair, the surfaces show the best RMSE values optimized in the β similarity parameter.
(a): l = 3. (b)-(c): l = 4, on the same scale. (e): l = 5. (f): l = 6.
TABLE VI: Performance (MAE) of the OSDL prediction for
different neighbor corrections using toroid group structure (G).
Columns: applied neighbor corrections. Rows: mini-batch size
R = 8 and 16. The neighbor size was set to r = 4. For ﬁxed R,
the best performance is highlighted with boldface typesetting.
casted the CF estimation task as an OSDL problem. We
demonstrated the applicability of our novel approach on joke
recommendations. Our extensive numerical experiments show
that structured dictionaries have several advantages over the
state-of-the-art CF methods: more precise estimation can be
obtained, and smaller dimensional feature representation can
be sufﬁcient by applying group structured dictionaries. Moreover, the estimation behaves robustly as a function of the
OSDL parameters and the applied group structure.
ACKNOWLEDGMENTS
The Project is supported by the European Union and co-
ﬁnanced by the European Social Fund . The research was partly supported by the
Department of Energy (grant number DESC0002607).