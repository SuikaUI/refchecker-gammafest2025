Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 4129–4142,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
Certiﬁed Robustness to Adversarial Word Substitutions
Aditi Raghunathan
Kerem G¨oksel
Percy Liang
Computer Science Department, Stanford University
{robinjia,aditir,kerem,pliang}@cs.stanford.edu
State-of-the-art NLP models can often be
fooled by adversaries that apply seemingly
label-preserving
transformations
(e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially
large family of label-preserving transformations, in which every word in the input can
be replaced with a similar word.
the ﬁrst models that are provably robust to all
word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the
worst-case loss that any combination of word
substitutions can induce. To evaluate models’
robustness to these transformations, we measure accuracy on adversarially chosen word
substitutions applied to test examples.
IBP-trained models attain 75% adversarial accuracy on both sentiment analysis on IMDB
and natural language inference on SNLI. In
comparison, on IMDB, models trained normally and ones trained with data augmentation
achieve adversarial accuracy of only 8% and
35%, respectively.
Introduction
Machine learning models have achieved impressive accuracy on many NLP tasks, but they are
surprisingly brittle. Adding distracting text to the
input , paraphrasing the text
 , replacing
words with similar words ,
or inserting character-level “typos” can signiﬁcantly degrade a model’s performance. Such perturbed inputs are called adversarial examples, and
have shown to break models in other domains as
well, most notably in vision S(x, 3) S(x, 4)
Input reviewaaa
Substitution words
…delivered one
Perturbed reviewaaa
Figure 1: Word substitution-based perturbations in sentiment analysis. For an input x, we consider perturbations ˜x, in which every word xi can be replaced with
any similar word from the set S(x, i), without changing the original sentiment. Models can be easily fooled
by adversarially chosen perturbations (e.g., changing
“best” to “better”, “made” to “delivered”, “ﬁlms” to
“movies”), but the ideal model would be robust to all
combinations of word substitutions.
Goodfellow et al., 2015). Since humans are not
fooled by the same perturbations, the widespread
existence of adversarial examples exposes troubling gaps in models’ understanding.
In this paper, we focus on the word substitution
perturbations of Alzantot et al. . In this setting, an attacker may replace every word in the input with a similar word (that ought not to change
the label), leading to an exponentially large number of possible perturbations. Figure 1 shows an
example of these word substitutions. As demonstrated by a long line of work in computer vision,
it is challenging to make models that are robust to
very large perturbation spaces, even when the set
of perturbations is known at training time .
Our paper addresses two key questions. First,
is it possible to guarantee that a model is robust
against all adversarial perturbations of a given in-
put? Existing methods that use heuristic search
to attack models are slow and cannot provide guarantees of robustness, since the space of possible perturbations is too large to search exhaustively. We
obtain guarantees by leveraging Interval Bound
Propagation (IBP), a technique that was previously applied to feedforward networks and CNNs
in computer vision . IBP
efﬁciently computes a tractable upper bound on
the loss of the worst-case perturbation. When this
upper bound on the worst-case loss is small, the
model is guaranteed to be robust to all perturbations, providing a certiﬁcate of robustness.
apply IBP to NLP settings, we derive new interval bound formulas for multiplication and softmax
layers, which enable us to compute IBP bounds for
LSTMs and
attention layers . We also
extend IBP to handle discrete perturbation sets,
rather than the continuous ones used in vision.
Second, can we train models that are robust in
this way? Data augmentation can sometimes mitigate the effect of adversarial examples , but it is insuf-
ﬁcient when considering very large perturbation
spaces . Adversarial training strategies from computer vision rely on gradient information, and therefore
do not extend to the discrete perturbations seen in
NLP. We instead use certiﬁably robust training, in
which we train models to optimize the IBP upper
bound .
We evaluate certiﬁably robust training on two
tasks—sentiment analysis on the IMDB dataset
 and natural language inference on the SNLI dataset . Across various model architectures (bagof-words, CNN, LSTM, and attention-based), certiﬁably robust training consistently yields models
which are provably robust to all perturbations on a
large fraction of test examples. A normally-trained
model has only 8% and 41% accuracy on IMDB
and SNLI, respectively, when evaluated on adversarially perturbed test examples. With certiﬁably
robust training, we achieve 75% adversarial accuracy for both IMDB and SNLI. Data augmentation fares much worse than certiﬁably robust training, with adversarial accuracies falling to 35% and
71%, respectively.
We consider tasks where a model must predict a
label y ∈Y given textual input x ∈X.
example, for sentiment analysis, the input x is a
sequence of words x1, x2, . . . , xL, and the goal
is to assign a label y ∈{−1, 1} denoting negative or positive sentiment, respectively. We use
z = (x, y) to denote an example with input x and
label y, and use θ to denote parameters of a model.
Let f(z, θ) ∈R denote some loss of a model with
parameters θ on example z. We evaluate models
on f0-1(z, θ), the zero-one loss under model θ.
Perturbations by word substitutions
Our goal is to build models that are robust to labelpreserving perturbations. In this work, we focus
on perturbations where words of the input are substituted with similar words. Formally, for every
word xi, we consider a set of allowed substitution
words S(x, i), including xi itself. We use ˜x to denote a perturbed version of x, where each word
˜xi is in S(x, i). For an example z = (x, y), let
Bperturb(z) denote the set of all allowed perturbations of z:
Bperturb(z) = {(˜x, y) : ˜xi ∈S(x, i) ∀i}.
Figure 1 provides an illustration of word substitution perturbations. We choose S(x, i) so that ˜x is
likely to be grammatical and have the same label
as x (see Section 5.1).
Robustness to all perturbations
Let F(z, θ) denote the set of losses of the network
on the set of perturbed examples deﬁned in (1):
F(z, θ) = {f(˜z, θ) : ˜z ∈Bperturb(z)}.
We deﬁne the robust loss as max F(z, θ), the loss
due to worst-case perturbation.
A model is robust at z if it classiﬁes all inputs in the perturbation set correctly, i.e., the robust zero-one loss
max F0-1(z, θ) = 0.
Unfortunately, the robust
loss is often intractable to compute, as each word
can be perturbed independently. For example, reviews in the IMDB dataset 
have a median of 1031 possible perturbations and
max of 10271, far too many to enumerate.
instead propose a tractable upper bound by constructing a set O(z, θ) ⊇F(z, θ). Note that
max O0-1(z, θ) = 0 ⇒max F0-1(z, θ) = 0
⇔robust at z.
Therefore, whenever max O0-1(z, θ) = 0, this
fact is sufﬁcient to certify robustness to all perturbed examples Bperturb(z).
However, since
O0-1(z, θ) ⊇F0-1(z, θ), the model could be robust even if max O0-1(z, θ) ̸= 0.
Certiﬁcation via Interval Bound
Propagation
We now show how to use Interval Bound Propagation (IBP) to obtain
a superset O(z, θ) of the losses of perturbed inputs F(z, θ), given z, θ, and Bperturb(z). For notational convenience, we drop z and θ. The key
idea is to compute upper and lower bounds on the
activations in each layer of the network, in terms
of bounds computed for previous layers. These
bounds propagate through the network, as in a
standard forward pass, until we obtain bounds on
the ﬁnal output, i.e., the loss f. While IBP bounds
may be loose in general, Section 5.2 shows that
training networks to minimize the upper bound on
f makes these bounds much tighter .
Formally, let gi denote a scalar-valued function
of z and θ (e.g., a single activation in one layer of
the network) computed at node i of the computation graph for a given network. Let dep(i) be the
set of nodes used to compute gi in the computation
graph (e.g., activations of the previous layer). Let
Gi denote the set of possible values of gi across all
examples in Bperturb(z). We construct an interval
Oi = [ℓi, ui] that contains all these possible values of gi, i.e., Oi ⊇Gi. Oi is computed from the
intervals Odep(i) = {Oj : j ∈dep(i)} of the dependencies of gi. Once computed, Oi can then be
used to compute intervals on nodes that depend on
i. In this way, bounds propagate through the entire
computation graph in an efﬁcient forward pass.
We now discuss how to compute interval
bounds for NLP models and word substitution perturbations. We obtain interval bounds for model
inputs given Bperturb(z) (Section 3.1), then show
how to compute Oi from Odep(i) for elementary
operations used in standard NLP models (Section 3.2). Finally, we use these bounds to certify
robustness and train robust models.
Bounds for the input layer
Previous work applied IBP
to continuous image perturbations, which are
naturally represented with interval bounds (Dvi-
Figure 2: Bounds on the word vector inputs to the neural network. Consider a word (sentence of length one)
x = a with the set of substitution words S(x, 1) =
{a, b, c, d, e}. (a) IBP constructs axis-aligned bounds
around a set of word vectors. These bounds may be
loose, especially if the word vectors are pre-trained and
ﬁxed. (b) A different word vector space can give tighter
IBP bounds, if the convex hull of the word vectors is
better approximated by an axis-aligned box.
jotham et al., 2018). We instead work with discrete word substitutions, which we must convert
into interval bounds Oinput in order to use IBP.
Given input words x = x1, . . . , xL, we assume
that the model embeds each word as ginput =
[φ(x1), . . . , φ(xL)] ∈RL×d, where φ(xi) ∈Rd is
the word vector for word xi. To compute Oinput ⊇
Ginput, recall that each input word xi can be replaced with any ˜xi ∈S(x, i). So, for each coordinate j ∈{1, . . . , d}, we can obtain an interval
bound Oinput
] for ginput
by computing the smallest axis-aligned box that contains all
the word vectors:
w∈S(x,i) φ(w)j, uinput
w∈S(x,i) φ(w)j.
Figure 2 illustrates these bounds. We can view
this as relaxing a set of discrete points to a convex
set that contains all of the points. Section 4.2 discusses modeling choices to make this box tighter.
Interval bounds for elementary functions
Next, we describe how to compute the interval of
a node i from intervals of its dependencies. Gowal
et al. show how to efﬁciently compute interval bounds for afﬁne transformations (i.e., linear layers) and monotonic elementwise nonlinearities (see Appendix 3). This sufﬁces to compute interval bounds for feedforward networks and
However, common NLP model components like LSTMs and attention also rely on softmax (for attention), element-wise multiplication
(for LSTM gates), and dot product (for computing
attention scores). We show how to compute interval bounds for these new operations. These building blocks can be used to compute interval bounds
not only for LSTMs and attention, but also for any
model that uses these elementary functions.
For ease of notation, we drop the superscript
i on gi and write that a node computes a result
zres = g(zdep) where zres ∈R and zdep ∈Rm for
m = |dep(i)|. We are given intervals Odep such
j ] for each coordinate
j and want to compute Ores = [ℓres, ures].
Softmax layer.
The softmax function is often
used to convert activations into a probability distribution, e.g., for attention. Gowal et al. 
uses unnormalized logits and does not handle softmax operations. Formally, let zres represent the
normalized score of the word at position c. We
have zres =
j=1 exp(zdep
). The value of zres is
largest when zdep
takes its largest value and all
other words take the smallest value:
We obtain a similar expression for ℓres. Note that
ℓres and ures can each be computed in a forward
pass, with some care taken to avoid numerical instability (see Appendix A.2).
Element-wise multiplication and dot product.
Models like LSTMs incorporate gates which perform element-wise multiplication of two activations. Let zres = zdep
where zres, zdep
R. The extreme values of the product occur at one
of the four points corresponding to the products of
the extreme values of the inputs. In other words,
ℓres = min
ures = max
Propagating
multiplication
nodes therefore requires four multiplications.
Dot products between activations are often used
to compute attention scores.1
The dot product
is just the sum of the element-wise
product zdep
2 . Therefore, we can bound the
dot product by summing the bounds on each element of zdep
2 , using the formula for elementwise multiplication.
1This is distinct from an afﬁne transformation, because
both vectors have associated bounds; in an afﬁne layer, the
input has bounds, but the weight matrix is ﬁxed.
Final layer
Classiﬁcation models typically output a single
logit for binary classiﬁcation, or k logits for k-way
classiﬁcation. The ﬁnal loss f(z, θ) is a function
of the logits s(x). For standard loss functions, we
can represent this function in terms of elementwise monotonic functions (Appendix 3) and the
elementary functions described in Section 3.2.
1. Zero-one loss: f(z, θ) = I[max(s(x)) = y]
involves a max operation followed by a step
function, which is monotonic.
2. Cross entropy: For multi-class, f(z, θ) =
softmax(s(x)). In the binary case, f(z, θ) =
σ(s(x)), where the sigmoid function σ is
monotonic.
we can compute bounds on the loss
O(z, θ) = [ℓﬁnal, uﬁnal] from bounds on the logits.
Certiﬁably Robust Training with IBP
Finally, we describe certiﬁably robust training, in
which we encourage robustness by minimizing the
upper bound on the worst-case loss . Recall that for an
example z and parameters θ, uﬁnal(z, θ) is the upper bound on the loss f(z, θ). Given a dataset D,
we optimize a weighted combination of the normal
loss and the upper bound uﬁnal,
(1 −κ)f(z, θ) + κ uﬁnal(z, θ),
where 0 ≤κ ≤1 is a scalar hyperparameter.
As described above, we compute uﬁnal in a modular fashion:
each layer has an accompanying
function that computes bounds on its outputs given
bounds on its inputs. Therefore, we can easily apply IBP to new architectures. Bounds propagate
through layers via forward passes, so the entire objective (7) can be optimized via backpropagation.
Gowal et al. found that this objective was
easier to optimize by starting with a smaller space
of allowed perturbations, and make it larger during
training. We accomplish this by artiﬁcially shrinking the input layer intervals Oinput
towards the original value φ(xi)j by a factor of ϵ:
←φ(xi)j −ϵ(φ(xi)j −ℓinput
←φ(xi)j + ϵ , we evaluate on
two standard NLP datasets: the IMDB sentiment
analysis dataset and the Stanford Natural Language Inference (SNLI) dataset
 . For IMDB, the model is
given a movie review and must classify it as positive or negative. For SNLI, the model is given
two sentences, a premise and a hypothesis, and
is asked whether the premise entails, contradicts,
or is neutral with respect to the hypothesis. For
SNLI, the adversary is only allowed to change the
hypothesis, as in Alzantot et al. , though it
is possible to also allow changing the premise.
implemented
IMDB. The bag-of-words model (BOW) averages
the word vectors for each word in the input, then
passes this through a two-layer feedforward network with 100-dimensional hidden state to obtain
a ﬁnal logit. The other models are similar, except
they run either a CNN or bidirectional LSTM on
the word vectors, then average their hidden states.
All models are trained on cross entropy loss.
We implemented two models for SNLI.
The bag-of-words model (BOW) encodes the
premise and hypothesis separately by summing
their word vectors, then feeds the concatenation of
these encodings to a 3-layer feedforward network.
We also reimplement the Decomposable Attention
model , which uses attention
between the premise and hypothesis to compute
richer representations of each word in both sentences. These context-aware vectors are used in
the same way BOW uses the original word vectors to generate the ﬁnal prediction. Both models
are trained on cross entropy loss. Implementation
details are provided in Appendix A.4.
Word vector layer.
The choice of word vectors
affects the tightness of our interval bounds. We
choose to deﬁne the word vector φ(w) for word w
as the output of a feedforward layer applied to a
ﬁxed pre-trained word vector φpre(w):
φ(w) = ReLU(gword(φpre(w))),
where gword is a learned linear transformation.
Learning gword with certiﬁably robust training encourages it to orient the word vectors so that the
convex hull of the word vectors is close to an
axis-aligned box. Note that gword is applied before bounds are computed via (4).2 Applying gword
after the bound calculation would result in looser
interval bounds, since the original word vectors
φpre(w) might be poorly approximated by interval
bounds (e.g., Figure 2a), compared to φ(w) (e.g.,
Figure 2b). Section 5.7 conﬁrms the importance
of adding gword. We use 300-dimensional GloVe
vectors as our φpre(w).
Experiments
Word substitution perturbations.
We base our
sets of allowed word substitutions S(x, i) on the
substitutions allowed by Alzantot et al. .
They demonstrated that their substitutions lead to
adversarial examples that are qualitatively similar
to the original input and retain the original label,
as judged by humans. Alzantot et al. de-
ﬁne the neighbors N(w) of a word w as the n = 8
nearest neighbors of w in a “counter-ﬁtted” word
vector space where antonyms are far apart .3 The neighbors must also lie within
some Euclidean distance threshold. They also use
a language model constraint to avoid nonsensical perturbations: they allow substituting xi with
˜xi ∈N(xi) if and only if it does not decrease the
log-likelihood of the text under a pre-trained language model by more than some threshold.
We make three modiﬁcations to this approach.
First, in Alzantot et al. , the adversary applies substitutions one at a time, and the neighborhoods and language model scores are computed
2 Equation (4) must be applied before the model can combine information from multiple words, but it can be delayed
until after processing each word independently.
3 Note that the model itself classiﬁes using a different
set of pre-trained word vectors; the counter-ﬁtted vectors are
only used to deﬁne the set of allowed substitution words.
relative to the current altered version of the input.
This results in a hard-to-deﬁne attack surface, as
changing one word can allow or disallow changes
to other words. It also requires recomputing language model scores at each iteration of the genetic
attack, which is inefﬁcient. Moreover, the same
word can be substituted multiple times, leading
to semantic drift. We deﬁne allowed substitutions
relative to the original sentence x, and disallow repeated substitutions. Second, we use a faster language model that allows us to query longer contexts; Alzantot et al. use a slower language
model and could only query it with short contexts.
Finally, we use the language model constraint only
at test time; the model is trained against all perturbations in N(w). This encourages the model to be
robust to a larger space of perturbations, instead of
specializing for the particular choice of language
model. See Appendix A.3 for further details.
Analysis of word neighbors.
One natural question is whether we could guarantee robustness by
having the model treat all neighboring words the
same. We could construct equivalence classes of
words from the transitive closure of N(w), and
represent each equivalence class with one embedding. We found that this would lose a signiﬁcant
amount of information. Out of the 50,000 word
vocabulary, 19,122 words would be in the same
equivalence class, including the words “good”,
“bad”, “excellent”, and “terrible.” Of the remaining words, 24,389 (79%) have no neighbors.
Baseline training methods.
We compare certi-
ﬁably robust training (Section 3) with both standard training and data augmentation, which has
been used in NLP to encourage robustness to
various types of perturbations . In data augmentation, for
each training example z, we augment the dataset
with K new examples ˜z by sampling ˜z uniformly
from Bperturb(z), then train on the normal cross
entropy loss. For our main experiments, we use
K = 4. We do not use adversarial training because it would require running an adversarial search procedure at each training step, which would be prohibitively slow.
Evaluation of robustness.
We wish to evaluate
robustness of models to all word substitution perturbations. Ideally, we would directly measure robust accuracy, the fraction of test examples z for
which the model is correct on all ˜z ∈Bperturb(z).
However, evaluating this exactly involves enumerating the exponentially large set of perturbations, which is intractable. Instead, we compute
tractable upper and lower bounds:
1. Genetic attack accuracy: Alzantot et al. 
demonstrate the effectiveness of a genetic algorithm that searches for perturbations ˜z that
cause model misclassiﬁcation. The algorithm
maintains a “population” of candidate ˜z’s and
repeatedly perturbs and combines them.
used a population size of 60 and ran 40 search
iterations on each example. Since the algorithm
does not exhaustively search over Bperturb(z),
accuracy on the perturbations it ﬁnds is an upper bound on the true robust accuracy.
2. Certiﬁed accuracy: To complement this upper
bound, we use IBP to obtain a tractable lower
bound on the robust accuracy. Recall from Section 3.3 that we can use IBP to get an upper
bound on the zero-one loss.
From this, we
obtain a lower bound on the robust accuracy
by measuring the fraction of test examples for
which the zero-one loss is guaranteed to be 0.
Experimental details.
For IMDB, we split
development subsets, putting reviews for different
movies into different splits (matching the original train/test split). For SNLI, we use the ofﬁcial
train/development/test split. We tune hyperparameters on the development set for each dataset. Hyperparameters are reported in Appendix A.4.
Main results
Table 1 and Table 2 show our main results for
IMDB and SNLI, respectively. We measure accuracy on perturbations found by the genetic attack (upper bound on robust accuracy) and IBPcertiﬁed accuracy (lower bound on robust accuracy) on 1000 random test examples from IMDB,4
and all 9824 test examples from SNLI. Across
many architectures, our models are more robust
to perturbations than ones trained with data augmentation. This effect is especially pronounced
on IMDB, where inputs can be hundreds of words
long, so many words can be perturbed.
IMDB, the best IBP-trained model gets 75.0% accuracy on perturbations found by the genetic at-
4We downsample the test set because the genetic attack is
slow on IMDB, as inputs can be hundreds of words long.
Genetic attack
(Upper bound)
IBP-certiﬁed
(Lower bound)
Standard training
Robust training
Data augmentation
Table 1: Robustness of models on IMDB. We report accuracy on perturbations obtained via the genetic attack
(upper bound on robust accuracy), and certiﬁed accuracy obtained using IBP (lower bound on robust accuracy) on 1000 random IMDB test set examples. For
all models, robust training vastly outperforms data augmentation (p < 10−63, Wilcoxon signed-rank test).
Genetic attack
(Upper bound)
IBP-certiﬁed
(Lower bound)
Normal training
DECOMPATTN
Robust training
DECOMPATTN
Data augmentation
DECOMPATTN
Table 2: Robustness of models on the SNLI test set.
For both models, robust training outperforms data augmentation (p < 10−10, Wilcoxon signed-rank test).
tack, whereas the best data augmentation model
gets 35.2%.
Normally trained models are even
worse, with adversarial accuracies below 10%.
Certiﬁed accuracy.
Certiﬁably robust training yields models with tight guarantees on
robustness—the upper and lower bounds on robust
accuracy are close. On IMDB, the best model is
guaranteed to be correct on all perturbations of
74.2% of test examples, very close to the 75.0%
accuracy against the genetic attack. In contrast, for
data augmentation models, the IBP bound cannot
guarantee robustness on almost all examples. It
is possible that a stronger attack (e.g., exhaustive
search) could further lower the accuracy of these
models, or that the IBP bounds are loose.
LSTM models can be certiﬁed with IBP, though
they fare worse than other models. IBP bounds
may be loose for RNNs because of their long computation paths, along which looseness of bounds
can get ampliﬁed. Nonetheless, in Appendix A.7,
Clean accuracy
Genetic search accuracy
Robust training
Data augmentation
Normal training
Figure 3: Trade-off between clean accuracy and genetic attack accuracy for CNN models on IMDB. Data
augmentation cannot achieve high robustness. Certiﬁably robust training yields much more robust models,
though at the cost of some clean accuracy. Lines connect Pareto optimal points for each training strategy.
we show on synthetic data that robustly trained
LSTMs can learn long-range dependencies.
Clean versus robust accuracy
Robust training does cause a moderate drop in
clean accuracy (accuracy on unperturbed test examples) compared with normal training.
IMDB, our normally trained CNN model gets 89%
clean accuracy, compared to 81% for the robustly
trained model. We also see a drop on SNLI: the
normally trained BOW model gets 83% clean accuracy, compared to 79% for the robustly trained
model. Similar drops in clean accuracy are also
seen for robust models in vision only has
85% clean accuracy, but comparable normallytrained models get > 96% accuracy.
We found that the robustly trained models
tend to underﬁt the training data—on IMDB, the
CNN model gets only 86% clean training accuracy, lower than the test accuracy of the normally
trained model. The model continued to underﬁt
when we increased either the depth or width of
the network. One possible explanation is that the
attack surface adds a lot of noise, though a large
enough model should still be able to overﬁt the
training set. Better optimization or a tighter way to
compute bounds could also improve training accuracy. We leave further exploration to future work.
Next, we analyzed the trade-off between clean
and robust accuracy by varying the importance
Number of words perturbed
Robust training
Data augmentation
Normal training
Figure 4: Number of words perturbed by the genetic
attack to cause errors by CNN models on 1000 IMDB
development set examples. Certiﬁably robust training
reduces the effect of many simultaneous perturbations.
placed on perturbed examples during training.
We use accuracy against the genetic attack as
our proxy for robust accuracy, rather than IBPcertiﬁed accuracy, as IBP bounds may be loose
for models that were not trained with IBP. For
data augmentation, we vary K, the number of augmented examples per real example, from 1 to 64.
For certiﬁably robust training, we vary κ⋆, the
weight of the certiﬁed robustness training objective, between 0.01 and 1.0. Figure 3 shows tradeoff curves for the CNN model on 1000 random
IMDB development set examples. Data augmentation can increase robustness somewhat, but cannot reach very high adversarial accuracy.
certiﬁably robust training, we can trade off some
clean accuracy for much higher robust accuracy.
Runtime considerations
IBP enables efﬁcient computation of uﬁnal(z, θ),
but it still incurs some overhead. Across model
architectures, we found that one epoch of certi-
ﬁably robust training takes between 2× and 4×
longer than one epoch of standard training. On
the other hand, IBP certiﬁcates are much faster to
compute at test time than genetic attack accuracy.
For the robustly trained CNN IMDB model, computing certiﬁcates on 1000 test examples took 5
seconds, while running the genetic attack on those
same examples took over 3 hours.
Error analysis
We examined development set examples on which
models were correct on the original input but incorrect on the perturbation found by the genetic
attack. We refer to such cases as robustness errors.
We focused on the CNN IMDB models trained
normally, robustly, and with data augmentation.
We found that robustness errors of the robustly
trained model mostly occurred when it was not
conﬁdent in its original prediction. The model had
> 70% conﬁdence in the correct class for the original input in only 14% of robustness errors. In contrast, the normally trained and data augmentation
models were more conﬁdent on their robustness
errors; they had > 70% conﬁdence on the original
example in 92% and 87% of cases, respectively.
We next investigated how many words the genetic attack needed to change to cause misclassiﬁcation, as shown in Figure 4.
For the normally trained model, some robustness errors involved only a couple changed words (e.g., “I’ve
ﬁnally found a movie worse than ...” was classiﬁed negative, but the same review with “I’ve ﬁnally discovered a movie worse than...” was classiﬁed positive), but more changes were also common (e.g., part of a review was changed from “The
creature looked very cheesy” to “The creature
seemed supremely dorky”, with 15 words changed
in total). Surprisingly, certiﬁably robust training
nearly eliminated robustness errors in which the
genetic attack had to change many words: the genetic attack either caused an error by changing a
couple words, or was unable to trigger an error
In contrast, data augmentation is unable
to cover the exponentially large space of perturbations that involve many words, so it does not prevent errors caused by changing many words.
Training schedule
We investigated the importance of slowly increasing ϵ during training, as suggested by Gowal et al.
 . Fixing ϵ = 1 during training led to a 5
point reduction in certiﬁed accuracy for the CNN.
On the other hand, we found that holding κ ﬁxed
did not hurt accuracy, and in fact may be preferable. More details are shown in Appendix A.5.
Word vector analysis
We determined the importance of the extra feedforward layer gword that we apply to pre-trained
word vectors, as described in Section 4.2.
compared with directly using pre-trained word
vectors, i.e. φ(w) = φpre(w). We also tried using gword but applying interval bounds on φpre(w),
then computing bounds on φ(w) with the IBP for-
mula for afﬁne layers. In both cases, we could
not train a CNN to achieve more than 52.2% certi-
ﬁed accuracy on the development set. Thus, transforming pre-trained word vectors and applying interval bounds after is crucial for robust training.
In Appendix A.6, we show that robust training
makes the intervals around transformed word vectors smaller, compared to the pre-trained vectors.
Related Work and Discussion
Recent work on adversarial examples in NLP has
proposed various classes of perturbations, such
as insertion of extraneous text , word substitutions ,
paraphrasing , and character-level noise . These works
focus mainly on demonstrating models’ lack of robustness, and mostly do not explore ways to increase robustness beyond data augmentation. Data
augmentation is effective for narrow perturbation
spaces ,
but only confers partial robustness in other cases
 . Ebrahimi
et al. tried adversarial training for character-level perturbations, but
could only use a fast heuristic attack at training
time, due to runtime considerations. As a result,
their models were still be fooled by running a more
expensive search procedure at test time.
Provable defenses have been studied for simpler NLP models and attacks, particularly for tasks
like spam detection where real-life adversaries try
to evade detection. Globerson and Roweis 
train linear classiﬁers that are robust to adversarial feature deletion. Dalvi et al. analyzed
optimal strategies for a Naive Bayes classiﬁer and
attacker, but their classiﬁer only defends against a
ﬁxed attacker that does not adapt to the model.
Recent work in computer vision has sparked renewed interest in adversarial examples.
work in this area focuses on L∞-bounded perturbations, in which each input pixel can be changed
by a small amount. The word substitution attack
model we consider is similar to L∞perturbations,
as the adversary can change each input word by
a small amount. Our work is inspired by work
based on convex optimization and builds directly
on interval bound propagation , which has certiﬁed robustness of computer vision models to L∞attacks.
Adversarial training via projected gradient descent
 has also been shown to improve robustness, but assumes that inputs are continuous. It could be applied in NLP by relaxing
sets of word vectors to continuous regions.
This work provides certiﬁcates against word
substitution perturbations for particular models.
Since IBP is modular, it can be extended to
other model architectures on other tasks.
an open question whether IBP can give nontrivial bounds for sequence-to-sequence tasks like
machine translation . In principle, IBP can handle
character-level typos , though typos yield more perturbations per word than we consider in this work. We
are also interested in handling word insertions and
deletions, rather than just substitutions. Finally,
we would like to train models that get state-ofthe-art clean accuracy while also being provably
robust; achieving this remains an open problem.
In conclusion, state-of-the-art NLP models are
accurate on average, but they still have signiﬁcant
blind spots. Certiﬁably robust training provides
a general, principled mechanism to avoid such
blind spots by encouraging models to make correct predictions on all inputs within some known
perturbation neighborhood. This type of robustness is a necessary (but not sufﬁcient) property of
models that truly understand language. We hope
that our work is a stepping stone towards models
that are robust against an even wider, harder-tocharacterize space of possible attacks.
Acknowledgments
This work was supported by NSF Award Grant no.
1805310 and the DARPA ASED program under
FA8650-18-2-7882. R.J. is supported by an NSF
Graduate Research Fellowship under Grant No.
DGE-114747. A.R. is supported by a Google PhD
Fellowship and the Open Philanthropy Project AI
Fellowship.
We thank Allen Nie for providing
the pre-trained language model, and thank Peng
Qi, Urvashi Khandelwal, Shiori Sagawa, and the
anonymous reviewers for their helpful comments.
Reproducibility
All code, data, and experiments are available on
Codalab at