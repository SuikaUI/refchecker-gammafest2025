Published as a workshop paper at ICLR 2019
FAST GRAPH REPRESENTATION LEARNING WITH
PYTORCH GEOMETRIC
Matthias Fey & Jan E. Lenssen
Department of Computer Graphics
TU Dortmund University
44227 Dortmund, Germany
{matthias.fey,janeric.lenssen}@udo.edu
We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In
addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D
data processing. PyTorch Geometric achieves high data throughput by leveraging
sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing eﬃcient mini-batch handling for input examples of diﬀerent size. In this work,
we present the library in detail and perform a comprehensive comparative study of
the implemented methods in homogeneous evaluation scenarios.
INTRODUCTION
Graph Neural Networks (GNNs) recently emerged as a powerful approach for representation learning
on graphs, point clouds and manifolds . Similar to the
concepts of convolutional and pooling layers on regular domains, GNNs are able to (hierarchically)
extract localized embeddings by passing, transforming, and aggregating information .
However, implementing GNNs is challenging, as high GPU throughput needs to be achieved on
highly sparse and irregular data of varying size. Here, we introduce PyTorch Geometric (PyG), a
geometric deep learning extension library for PyTorch which achieves high
performance by leveraging dedicated CUDA kernels. Following a simple message passing API, it
bundles most of the recently proposed convolutional and pooling layers into a single and uniﬁed
framework. All implemented methods support both CPU and GPU computations and follow an
immutable data ﬂow paradigm that enables dynamic changes in graph structures through time. PyG
is released under the MIT license and is available on GitHub.1 It is thoroughly documented and
provides accompanying tutorials and examples as a ﬁrst starting point.2
In PyG, we represent a graph = (X, (I, E)) by a node feature matrix X ∈ℝ푁×퐹of 푁nodes and
a sparse adjacency tuple (I, E) of 퐸edges, where I ∈ℕ2×퐸encodes edge indices in COOrdinate
(COO) format and E ∈ℝ퐸×퐷(optionally) holds 퐷-dimensional edge features. All user facing APIs,
e.g., data loading routines, multi-GPU support, data augmentation or model instantiations are heavily
inspired by PyTorch to keep them as familiar as possible.
Neighborhood Aggregation.
Generalizing the convolutional operator to irregular domains is typically expressed as a neighborhood aggregation or message passing scheme 
휙 
scatter_ ⬚(푰)
Figure 1: Computation scheme of a GNN layer by leveraging gather and scatter methods based on
edge indices I, hence alternating between node parallel space and edge parallel space.
where ⬚denotes a diﬀerentiable, permutation invariant function, e.g., sum, mean or max, and 훾
and 휙denote diﬀerentiable functions, e.g., MLPs. In practice, this can be achieved by gathering
and scattering of node features and vectorized element-wise computation of 훾and 휙, as visualized in
Figure 1. Although working on irregularly structured input, this scheme can be heavily accelerated by
the GPU. In contrast to implementations via sparse matrix multiplications, the usage of gather/scatter
proves to be advantageous for low-degree graphs and non-coalesced input (cf. Appendix A), and
allows for the integration of central node and multi-dimensional edge information while aggregating.
We provide the user with a general MessagePassing interface to allow for rapid and clean prototyping of new research ideas. In order to use, users only need to deﬁne the methods 휙, i.e., message,
and 훾, i.e., update, as well as chosing an aggregation scheme ⬚. For implementing 휙, node features
are automatically mapped to the respective source and target nodes.
Almost all recently proposed neighborhood aggregation functions can be lifted to this interface, including (but not limited to) the methods already integrated into PyG: For learning on arbitrary graphs
we have implemented GCN and its simpliﬁed version (SGC) from Wu et al.
 , the spectral chebyshev and ARMA ﬁlter convolutions , GraphSAGE , the attention-based operators GAT and AGNN , the Graph Isomorphism Network (GIN) from Xu
et al. , the Approximate Personalized Propagation of Neural Predictions (APPNP) operator
 , the Dynamic Neighborhood Aggregation (DNA) operator and
the signed operator for learning in signed networks .
For learning on point clouds, manifolds and graphs with multi-dimensional edge features, we provide the relational GCN operator from Schlichtkrull et al. , PointNet++ ,
PointCNN , and the continuous kernel-based methods MPNN , MoNet , SplineCNN and
the edge convolution operator (EdgeCNN) from Wang et al. .
In addition to these operators, we provide high-level implementations of, e.g., maximizing mutual information , autoencoding graphs , aggregating jumping knowledge , and predicting temporal events in knowledge graphs
 .
Global Pooling.
PyG also supports graph-level outputs as opposed to node-level outputs by providing a variety of readout functions such as global add, mean or max pooling. We additionaly oﬀer
more sophisticated methods such as set-to-set , sort pooling 
or the global soft attention layer from Li et al. .
Hierarchical Pooling.
To further extract hierarchical information and to allow deeper GNN models, various pooling approaches can be applied in a spatial or data-dependent manner. We currently
provide implementation examples for Graclus and voxel grid pooling , the iterative farthest point sampling algorithm followed by 푘-NN or query ball graph generation , and diﬀerentiable pooling mechanisms such as DiﬀPool and
top푘pooling .
Published as a workshop paper at ICLR 2019
Table 1: Semi-supervised node classiﬁcation with both ﬁxed and random splits.
81.4 ± 0.7
77.8 ± 2.2
70.2 ± 1.0
67.7 ± 1.7
78.4 ± 0.4
75.8 ± 2.2
81.5 ± 0.6
79.4 ± 1.9
71.1 ± 0.7
68.1 ± 1.7
79.0 ± 0.6
77.4 ± 2.4
83.1 ± 0.4
81.0 ± 1.4
70.8 ± 0.5
69.2 ± 1.9
78.5 ± 0.3
78.3 ± 2.3
81.7 ± 0.1
80.2 ± 1.6
71.3 ± 0.2
68.7 ± 1.6
78.9 ± 0.1
76.5 ± 2.4
82.8 ± 0.6
80.7 ± 1.4
72.3 ± 1.1
68.9 ± 1.6
78.8 ± 0.3
77.7 ± 2.6
83.3 ± 0.5
82.2 ± 1.5
71.8 ± 0.5
70.0 ± 1.4
80.1 ± 0.2
79.4 ± 2.2
Mini-batch Handling.
Our framework supports batches of multiple graph instances (of potentially
diﬀerent size) by automatically creating a single (sparse) block-diagonal adjacency matrix and concatenating feature matrices in the node dimension. Therefore, neighborhood aggregation methods
can be applied without modiﬁcation, since no messages are exchanged between disconnected graphs.
In addition, an automatically generated assignment vector ensures that node-level information is not
aggregated across graphs, e.g., when executing global aggregation operators.
Processing of Datasets.
We provide a consistent data format and an easy-to-use interface for the
creation and processing of datasets, both for large datasets and for datasets that can be kept in memory
during training. In order to create new datasets, users just need to read/download their data and
convert it to the PyG data format in the respective process method. In addition, datasets can be
modiﬁed by the use of transforms, which take in separate graphs and transform them, e.g., for
data augmentation, for enhancing node features with synthetic structural graph properties , to automatically generate graphs from point clouds or to sample point clouds from
PyG already supports a lot of common benchmark datasets often found in literature which are automatically downloaded and processed on ﬁrst instantiation. In detail, we provide over 60 graph
kernel benchmark datasets3 , e.g., PROTEINS or IMDB-BINARY, the citation graphs Cora, CiteSeer, PubMed and Cora-Full , the Coauthor CS/Physics and Amazon Computers/Photo datasets from Shchur et al. ,
the molecule datasets QM7b and QM9 , the
protein-protein interaction graphs from Hamilton et al. , and the temporal datasets Bitcoin-
OTC , ICEWS and GDELT . In
addition, we provide embedded datasets like MNIST superpixels , FAUST , ModelNet10/40 , ShapeNet , COMA and the PCPNet dataset from Guerrero et al. .
EMPIRICAL EVALUATION
We evaluate the correctness of the implemented methods by performing a comprehensive comparative study in homogeneous evaluation scenarios. Descriptions and statistics of all used datasets can
be found in Appendix B. For all experiments, we tried to follow the hyperparameter setup of the
respective papers as closely as possible. The individual experimental setups can be derived and all
experiments can be replicated from the code provided at our GitHub repository.4
Semi-supervised Node Classiﬁcation.
We perform semi-supervised node classiﬁcation (cf. Table 1) by reporting average accuracies of (a) 100 runs for the ﬁxed train/val/test split from Kipf &
Welling , and (b) 100 runs of randomly initialized train/val/test splits as suggested by Shchur
et al. , where we additionally ensure uniform class distribution on the train split.
Nearly all experiments show a high reproducibility of the results reported in the respective papers.
However, test performance is worse for all models when using random data splits. Among the experiments, the APPNP operator generally performs best, while the ARMA
3Kernel datasets: 
4 
Published as a workshop paper at ICLR 2019
Table 2: Graph classiﬁcation.
74.6 ± 7.7
73.1 ± 3.8
80.6 ± 2.1
72.6 ± 4.5
89.3 ± 3.3
74.9 ± 8.7
73.8 ± 3.6
79.7 ± 1.7
72.4 ± 3.6
89.1 ± 1.9
85.7 ± 7.7
72.1 ± 5.1
79.3 ± 2.7
72.8 ± 4.5
89.6 ± 2.6
83.4 ± 7.5
72.6 ± 4.9
79.8 ± 2.4
72.1 ± 5.1
90.3 ± 3.0
77.1 ± 7.2
73.0 ± 4.1
79.6 ± 2.0
72.2 ± 4.2
88.8 ± 3.2
76.3 ± 7.5
72.7 ± 4.1
79.7 ± 2.2
72.5 ± 4.6
87.6 ± 2.4
85.0 ± 10.3
75.1 ± 3.5
78.9 ± 2.3
72.6 ± 3.9
92.1 ± 2.6
SAGE w/o JK
73.7 ± 7.8
72.7 ± 3.6
79.6 ± 2.4
72.1 ± 4.4
87.9 ± 1.9
GlobalAttention
74.6 ± 8.0
72.5 ± 4.5
79.6 ± 2.2
72.3 ± 3.8
87.4 ± 2.5
73.7 ± 6.9
73.6 ± 3.7
79.6 ± 2.3
72.2 ± 4.2
89.6 ± 2.4
77.3 ± 8.9
72.4 ± 4.1
77.7 ± 3.1
72.4 ± 3.8
74.9 ± 6.7
Table 3: Point cloud classiﬁcation.
ModelNet10
PointNet++
Table 4: Training runtime comparison.
 , SGC , GCN and GAT operators follow closely behind.
Graph Classiﬁcation.
We report the average accuracy of 10-fold cross validation on a number of
common benchmark datasets (cf. Table 2) where we randomly sample a training fold to serve as a
validation set. We only make use of discrete node features. In case they are not given, we use one-hot
encodings of node degrees as feature input. For all experiments, we use the global mean operator
to obtain graph-level outputs. Inspired by the Jumping Knowledge framework , we
compute graph-level outputs after each convolutional layer and combine them via concatenation.
For evaluating the (global) pooling operators, we use the GraphSAGE operator as our baseline. We
omit Jumping Knowledge when comparing global pooling operators, and hence report an additional
baseline based on global mean pooling. For each dataset, we tune (1) the number of hidden units
∈{16, 32, 64, 128} and (2) the number of layers ∈{2, 3, 4, 5} with respect to the validation set.
Due to standardized evaluations and network architectures, not all results are aligned with their oﬃcial reported values. For example, except for DiﬀPool , (global) pooling operators
do not perform as beneﬁcally as expected to their respective (ﬂat) counterparts, especially when
baselines are enhanced by Jumping Knowledge . However, the potential of more
sophisticated approaches may not be well-reﬂected on these simple benchmark tasks . Among the ﬂat GNN approaches, the GIN layer generally achieves the best
Point Cloud Classiﬁcation.
We evaluate various point cloud methods on ModelNet10 where we uniformly sample 1,024 points from mesh surfaces based on face area (cf. Table 3).
As hierarchical pooling layers, we use the iterative farthest point sampling algorithm followed by a
new graph generation based on a larger query ball , MPNN and SplineCNN ) or based on a ﬁxed
Published as a workshop paper at ICLR 2019
number of nearest neighbors and PointCNN ). We
have taken care to use approximately the same number of parameters for each model.
All approaches perform nearly identically with PointCNN taking a slight lead. We
attribute this to the fact that all operators are based on similar principles and might have the same
expressive power for the given task.
Runtime Experiments.
We conduct several experiments on a number of dataset-model pairs to
report the runtime of a whole training procedure for 200 epochs obtained on a single NVIDIA GTX
1080 Ti (cf. Table 4). As it shows, PyG is very fast despite working on sparse data. Compared to the
Degree Bucketing (DB) approach of the Deep Graph Library (DGL) v0.2 , PyG
trains models up to 40 times faster. Although runtimes are comparable when using gather and scatter
optimizations (GS) inside DGL, we could further improve runtimes of GAT 
by up to 7 times by providing our own optimized sparse softmax kernels.
ROADMAP AND CONCLUSION
We presented the PyTorch Geometric framework for fast representation learning on graphs, point
clouds and manifolds. We are actively working to further integrate existing methods and plan to
quickly integrate future methods into our framework. All researchers and software engineers are
invited to collaborate with us in extending its scope.
ACKNOWLEDGMENTS
This work has been supported by the German Research Association (DFG) within the Collaborative
Research Center SFB 876, Providing Information by Resource-Constrained Analysis, projects A6
and B2. We thank Moritz Ludolph and all other contributors for their amazing involvement in this
project. Last but not least, we thank Christopher Morris for fruitful discussions, proofreading and
helpful advice.