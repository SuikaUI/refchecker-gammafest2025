Semi-Supervised and Task-Driven Data
Augmentation
Krishna Chaitanya1, Neerav Karani1, Christian Baumgartner1, Olivio Donati2,
Anton Becker2, Ender Konukoglu1
1 Computer Vision Lab, ETH Zurich
2 University Hospital of Zurich, Zurich, Switzerland
Abstract. Supervised deep learning methods for segmentation require
large amounts of labelled training data, without which they are prone to
overﬁtting, not generalizing well to unseen images. In practice, obtaining a large number of annotations from clinical experts is expensive and
time-consuming. One way to address scarcity of annotated examples is
data augmentation using random spatial and intensity transformations.
Recently, it has been proposed to use generative models to synthesize realistic training examples, complementing the random augmentation. So
far, these methods have yielded limited gains over the random augmentation. However, there is potential to improve the approach by (i) explicitly
modeling deformation ﬁelds (non-aﬃne spatial transformation) and intensity transformations and (ii) leveraging unlabelled data during the
generative process. With this motivation, we propose a novel task-driven
data augmentation method where to synthesize new training examples,
a generative network explicitly models and applies deformation ﬁelds
and additive intensity masks on existing labelled data, modeling shape
and intensity variations, respectively. Crucially, the generative model is
optimized to be conducive to the task, in this case segmentation, and
constrained to match the distribution of images observed from labelled
and unlabelled samples. Furthermore, explicit modeling of deformation
ﬁelds allow synthesizing segmentation masks and images in exact correspondence by simply applying the generated transformation to an input
image and the corresponding annotation. Our experiments on cardiac
magnetic resonance images (MRI) showed that, for the task of segmentation in small training data scenarios, the proposed method substantially
outperforms conventional augmentation techniques.
Introduction
Precise segmentation of anatomical structures is crucial for several clinical applications. Recent advances in deep neural networks yielded automatic segmentation algorithms with unprecedented accuracy. However, such methods heavily
rely on large annotated training datasets. In this work, we consider the problem
of medical image segmentation in the setting of small training datasets.
This article has been accepted at the 26th international conference on Information
Processing in Medical Imaging (IPMI) 2019.
 
Authors Suppressed Due to Excessive Length
Let us ﬁrst consider the question: why is a large training dataset necessary for the
success of deep learning methods? One hypothesis is that a large training dataset
exposes a neural network to suﬃcient variations in factors, such as shape, intensity and texture, thereby allowing it to learn a robust image to segmentation
mask mapping. In medical images, such variations may arise from subject speciﬁc
shape diﬀerences in anatomy or lesions. Image intensity and contrast characteristics may diﬀer substantially according to the image acquisition protocol or even
between scanners for the same acquisition protocol. When the training dataset
is small, deep learning methods are susceptible to faring poorly on unseen test
images either due to not identifying such variations or because the test images
appear to have been drawn from a distribution diﬀerent to the training images.
We conjecture that one way to train a segmentation network on a small training
dataset more robustly could be to incorporate into the training dataset, intensity
and anatomical shape variations observed from a large pool of unlabelled images. Speciﬁcally, we propose to generate synthetic image-label pairs by learning
generative models of deformation ﬁelds and intensity transformations that map
the available labelled training images to the distribution of the entire pool of
available images, including labelled as well as unlabelled. Additionally, we explicitly encourage the synthesized image-label pairs to be conducive to the task
at hand. We carried out extensive evaluation of the proposed method, in which
the method showed substantial improvements over existing data augmentation
as well as semi-supervised learning techniques for segmentation of cardiac MRIs.
Related work: Due to the high cost of obtaining large amount of expert annotations, robust training of machine learning methods in the small training dataset
setting has been widely studied in the literature. Focusing on the methods that
are most relevant to the proposed method, we broadly classify the related works
into two categories:
Data augmentation is a technique wherein the training dataset is enlarged
with artiﬁcially synthesized image-label pairs. The main idea is to transform
training images in such a way that the corresponding labels are either unchanged
or get transformed in the same way. Some commonly used data augmentation
methods are aﬃne transformations (such as translation, rotation, scaling, ﬂipping, cropping, etc.) and random elastic deformations . Leveraging recent
advances in generative image modelling , several works proposed to map randomly sampled vectors from a simple distribution to realistic image-label pairs as
augmented data for medical image segmentation problems . Such methods
are typically trained on already labelled data, with the objective of interpolating
within the training dataset. In an alternative direction, proposed to synthesize data for augmentation by simply linearly interpolating the available images
and the corresponding labels. Surprisingly, despite employing clearly unrealistic
images, this method led to substantial improvements in medical image segmentation when the available training dataset is very small. None of these data
augmentation methods use unlabelled images that may be more readily available
and all of them, except for those based on generative models, are hand-crafted
rather than optimized based on data.
Semi-Supervised and Task-Driven Data Augmentation
Semi-supervised learning (SSL) methods are another class of techniques that
are suitable in the setting of learning with small labelled training datasets. The
main idea of these methods is to regularize the learning process by employing unlabelled images. Approaches based on self-training alternately train a network
with labeled images, estimate labels for the unlabelled images using the network
and update the network with both the available true image-label pairs and the
estimated labels for the unlabelled images. propose a SSL method based on
adversarial learning, where the joint distribution of unlabelled image-estimated
labels pairs is matched to that of the true labelled images-label pairs. Interestingly, show that many SSL methods fail to provide substantial gains over the
supervised baseline that is trained with data augmentation and regularization.
Weakly-supervised learning tackles the issue of expensive pixel-wise annotations by training on weaker labels, such as scribbles and image-wide labels .
Finally, other regularization methods that do not necessarily leverage unlabelled
images may also aid in preventing over-ﬁtting to small training datasets.
In a supervised learning setup, an objective function LS({XL, YL}) that measures discrepancy between ground truth labels, YL, and predictions of a network
S on training images, XL, is minimized with respect to a set of learnable parameters wS of the network, i.e.
LS({XL, YL})
When data augmentation is employed in the supervised learning setup, Eq. 2 is
minimized with respect to wS.
LS({XL, YL} ∪{XG, YG})
Here, XG and YG refer to generated images and labels obtained by aﬃne or
elastic transformations of XL, YL or by using methods such as Mixup .
The set {{XL, YL} ∪{XG, YG}} is referred to as the augmented training set.
In augmentation methods based on generative models , the parameters of S
are still optimized according to Eq. 2, but the generative process for XG and
YG involves two other networks: a generator G and a discriminator D. The
corresponding parameters wG and wD are estimated according to the generative
adversarial learning (GAN) framework by optimizing:
wD Ex,y∼p(xL,yL)[log D(x, y)] + Ez∼pz(z)[log(1 −D(G(z))]
G takes as input a vector z sampled from a known distribution pz(z) and maps
that to a {XG, YG} pair. D is optimized to distinguish between outputs of G and
real {XL, YL} pairs, while G is optimized to generate {XG, YG} pairs such that
D responds to them similarly as to {XL, YL}. Thus, {XG, YG} are encouraged
to be “realistic” in the sense that they cannot be distinguished by D.
Authors Suppressed Due to Excessive Length
(a) Deformation ﬁeld cGAN
(b) Additive Intensity ﬁeld cGAN
Fig. 1: Modules for task-driven and semi-supervised data augmentation.
Semi-Supervised and Task-Driven Data Augmentation
Instead of solving the optimization given in Eq. 3 for generating the augmentation image-label pairs {XG, YG}, we propose solving Eq. 4:
wS LS({XL, YL} ∪{XG, YG}) + Lreg,wG({XL} ∪{XUL})
This incorporates two ideas. The ﬁrst term dictates that {XG, YG} be such that
they are beneﬁcial for minimizing the segmentation loss LS. Secondly, note that
Lreg,wG depends not only on the labelled images {XL} (as in Eq. 3), but also on
the unlabelled images {XUL}. It is a regularization term based on an adversarial
loss, which incorporates information about the image distribution that can be
extracted from both {XL} and {XUL}. This is achieved by synthesizing {XG} as
GC({XL}), where GC denotes a conditional generative model. GC is modelled in
two diﬀerent ways: one for deformation ﬁeld (non-aﬃne spatial transformations)
and one for intensity transformations. In both cases, the formulation is such that
as a certain labelled image is mapped to an augmentation image, the mapping
to obtain the corresponding augmentation label readily follows.
Deformation Field Generator : The deformation ﬁeld generator, GC = GV ,
is trained to create samples from the distribution of deformation ﬁelds that can
potentially map elements from {XL} to those in the combined set {XL}∪{XUL}.
GV takes as input an image from {XL} and a vector z, sampled from a unit
Gaussian distribution, and outputs a dense per-pixel deformation ﬁeld, v. The
input image and its corresponding label (in 1-hot encoding) are warped using
bilinear interpolation according to v to produce XG,V and YG,V respectively.
Additive Intensity Field Generator : The intensity ﬁeld generator, GC =
GI, is trained to draw random samples from the distribution of additive intensity
ﬁelds that can potentially map elements from {XL} to those in {XL} ∪{XUL}.
GI, takes as input an element of {XL} and a noise vector and outputs an intensity
mask, ∆I. ∆I is added to the input image to give the transformed image XG,I,
while its segmentation mask YG,I remains the same as that of the input image.
Regularization Loss : For both the conditional generators, the regularization
term, Lreg,wG, in Eq. 4 is formulated as in Eq. 5. The corresponding discriminator
Semi-Supervised and Task-Driven Data Augmentation
networks DC are trained to minimize the usual adversarial objective (Eq. 6).
Lreg,wGC = λadvEz∼pz(z)[log(1 −DC(GC(z, XL)))] + λbigLGC,big
LDC = −Ex∼p(x)[log DC(x)] −Ez∼pz(z)[log(1 −DC(GC(z, XL)))]
The generated images are obtained as GV (z, XL) = v ◦XL and GI(z, XL) =
∆I + XL, where ◦denotes a bilinear warping operation. In our experiments,
we observe that with only the adversarial loss term in Lreg,wGC , the generators
tend to create only the identity mapping. So, we introduce the LGC,big term
to incentivize non-trivial transformations. We formulate LGV ,big and LGI,big as
−||v||1 and −||∆I||1 respectively.
Optimization Sequence : The method starts by learning the optimal data
augmentation for the segmentation task. Thus, all networks S, GC and DC are
optimized according to Eq. 4. The generative models for the deformation ﬁelds
and the intensity ﬁelds are trained separately. Once this is complete, both GV
and GI are ﬁxed and the parameters of S are re-initialized. Now, S is trained
again according to Eq. 2, using the original labelled training data {XL, YL} and
augmentation data {XG, YG} generated using the trained GV or GI or both.
Dataset and Network details
Dataset Details
We used a publicly available dataset hosted as part of MICCAI’17 ACDC
challenge 3. It comprises of short-axis cardiac cine-MRIs of 100 subjects
from 5 groups - 20 normal controls and 20 each with 4 diﬀerent cardiac abnormalities. The in-plane and through-plane resolutions of the images range from
0.70x0.70mm to 1.92x1.92mm and 5mm to 10mm respectively. Expert annotations are provided for left ventricle (LV), myocardiam (Myo) and right ventricle
(RV) for both end-systole (ES) and end-diastole (ED) phases of each subject.
For our experiments, we only used the ES images.
Pre-processing
We apply the following pre-processing steps to all images of the dataset: (i)
bias correction using N4 algorithm, (ii) normalization of each 3d image by
linearly re-scaling the intensities as: (x −x2)/(x98 −x2), where x2 and x98 are
the 2nd and 98th percentile in the bias corrected 3d image, (iii) re-sample each
slice of each 3d image and the corresponding labels to an in-plane resolution of
1.367x1.367mm using bi-linear and nearest neighbour interpolation respectively
and crop or pad them to a ﬁxed size of 224x224.
3 
Authors Suppressed Due to Excessive Length
Network Architectures
There are three types of networks in the proposed method (see Fig. 1): a segmentation network S, a generator network G and a discriminator network D. In
this sub-section, we describe their architectures. Expect for the last layer of G,
the same architecture is used for the GV and GI networks used for modelling
both the deformation ﬁelds and the intensity transformations.
Generator: G takes as input an image from {XL} and a noise vector z of dimension 100, which are both ﬁrst passed through separate sub-networks, Gsubnet,X
and Gsubnet,z. Gsubnet,X, consists of 2 convolutional layers, while Gsubnet,z, consists of a fully-connected layer, followed by reshaping of the output, followed by
5 convolutional layers, interleaved with bilinear upsampling layers. The outputs
of the two sub-networks are of the same dimensions. They are concatenated and
passed through a common sub-network, Gsubnet,common, consisting of 4 convolutional layers, the last of which is diﬀerent for GV and GI. The ﬁnal convolutional
layer for GV outputs two feature maps corresponding to the 2-dimensional deformation ﬁeld v, while that for GI outputs a single feature map corresponding
to the intensity mask ∆I. The ﬁnal layer of GI employs the tanh activation to
cap the range of the intensity mask. All other layers use the ReLU activation.
All convolutional layers have 3x3 kernels except for the ﬁnal ones in both GV
and GI and are followed by batch-normalization layers before the activation.
Discriminator: D consists of 5 convolutional layers with kernel size of 5x5
and stride 2. The convolutions are followed by batch normalization layers and
leaky ReLU activations with the negative slope of the leak set to 0.2. After the
convolutional layers, the output is reshaped and passed through 3 fully-connected
layers, with the ﬁnal layer having an output size of 2.
Segmentation Network: We use a U-net like architecture for S. It has
an encoding and a decoding path. In the encoder, there are 4 convolutional
blocks, each consisting of 2 3x3 convolutions, followed by a max-pooling layer.
The decoder consists of 4 convolutional blocks, each made of a concatenation
with the corresponding features of the encoder, followed by 2 3x3 convolutions,
followed by bi-linear upsampling with factor 2. Batch normalization and ReLU
activation are employed in all layers, except the last one.
Training Details
Weighted cross-entropy is used as the segmentation loss, LS. We empirically set
the weights of the 4 output labels to 0.1 (background) and 0.3 (each of the 3
foreground labels). The background loss is considered while learning the augmentations, but not while learning the segmentation task alone. We empirically
set λadv and λbig to 1 and 10−3 respectively. The batch size is set to 20 and each
training is run for 10000 iterations. The model parameters that provide the best
dice score on the validation set are chosen for evaluation. Adam optimizer is used
for all networks with an initial learning rate of 10−3, β1 = 0.9 and β2 = 0.999.
Semi-Supervised and Task-Driven Data Augmentation
Experiments
We divide the dataset into test (Xts), validation (Xvl), labelled training (XL)
and unlabelled training (XUL) sets which consist of 20, 2, NL and 25 3d images
respectively. As we are interested in the few labelled training images scenario,
we run all our experiments in two settings: with NL set to 1 and 3. Xts, Xvl
and XUL are selected randomly a-priori and ﬁxed for all experiments. Xts and
XUL are chosen such that they consist of equal number of images from each
group (see Sec. 3.1) of the dataset. A separate set of 10 images (2 from each
group), XL,total, is selected randomly. Each experiment is run 5 times with XL
as NL images randomly selected from XL,total. When NL is 3, it is ensured that
the images in XL come from diﬀerent groups. Further, each of the 5 runs with
diﬀerent XL is run thrice in order to account for variations in convergence of the
networks. Thus, overall, we have 15 runs for each experiment.
The following experiments were done thrice for each choice of Xtr,L:
– No data augmentation (Augnone): S is trained without data augmentation.
– Aﬃne data augmentation(AugA): S is trained with data augmentation
comprising of aﬃne transformations. These consist of rotation (randomly chosen between -15deg and +15deg), scaling (with a factor randomly chosen uniformly between 0.9 and 1.1), another possible rotation that is multiple of
45deg (angle=45deg*N where N is randomly chosen between 0 to 8), and ﬂipping along x-axis. For each slice in a batch, a random number between 0 and
5 is uniformly sampled and accordingly, either the slice is left as it is or is
transformed by one of the 4 stated transformations.
All the following data augmentation methods, each training batch (batch
size=bs) is ﬁrst applied aﬃne transformations as explained above. The batch
used for training consists of half of these images along with bs/2 augmentation
images obtained according to the particular augmentation method.
– Random elastic deformations (AugA,RD): Elastic augmentations are
modelled as in , where a deformation ﬁeld is created by sampling each
element of a 3x3x2 matrix from a Gaussian distribution with mean 0 and
standard deviation 10 and upscaling it to the image dimensions using bi-cubic
interpolation.
– Random contrast and brightness ﬂuctuations (AugA,RI): This
comprises of an image contrast adjustment step: x = (x −¯x) ∗c + ¯x, followed
by a brightness adjustment step: x = x + b. We sample c and b uniformly in
[0.8,1.2] and [-0.1,0.1] respectively.
– Deformation ﬁeld transformations (AugA,GD): Augmentation data is
generated from the trained deformation ﬁeld generator GV .
– Intensity ﬁeld transformations (AugA,GI): Augmentation data is generated from the trained intensity ﬁeld generator GI.
– Both deformation and intensity ﬁeld transformations (AugA,GD,GI):
In this experiment, we sample data from GV and GI to obtain transformed
images XV and XI respectively. We also get an additional set of images which
Authors Suppressed Due to Excessive Length
contain both deformation and intensity transformations XV I. These are obtained by conditioning GI on spatially transformed images XV . The augmentation data comprises of all these images {XV , XI, XV I}.
– MixUp (AugA,Mixup): Augmentation data ({XG, YG}) is generated using the original annotated images XL and their linear combinations using the
Mixup formulation as stated in Eq. 7.
XG = λXLi + (1 −λ)XLj, YG = λYLi + (1 −λ)YLj
where λ is sampled from beta distribution Beta(α, α) with α ∈(0, ∞) and
λ ∈[0, 1) which controls the ratio to mix the image-label pairs (XLi, YLi),
(XLj, YLj) selected randomly from the set of labelled training images.
– Mixup over deformation and intensity ﬁeld transformations
(AugA,GD,GI,Mixup): Mixup is applied over diﬀerent pairs of available images: original data (XL), their aﬃne transformations and the images generated
using deformation and intensity ﬁeld generators {XV , XI, XV I}.
– Adversarial Training (Adv Tr): Here, we investigate the beneﬁt of the
method proposed in on our dataset (explained in Sec. 1), in both supervised (SL) and semi-supervised (SSL) settings.
Evaluation : The segmentation performance of each method is evaluated using
the Dice similarity coeﬃcient (DSC) over 20 test subjects for three foreground
structures: left ventricle (LV), myocardiam (Myo) and right ventricle (RV).
Results and Discussion
Table 1 presents quantitative results of our experiments. The reported numbers
are the mean dice scores over the 15 runs for each experiments as described in
Sec. 4. It can be observed that the proposed method provides substantial improvements over other data augmentation methods as well as the semi-supervised
adversarial learning method, especially in the case where only 1 3D volume is
used for training. The improvements can also be visually observed in Fig. 2. In
the rest of this section, we discuss the results of speciﬁc experiments.
Perhaps unsurprisingly, the lowest performance occurs when neither data augmentation nor semi-supervised training is used. Data augmentation with aﬃne
transformations already provides remarkable gains in performance. Both random
elastic deformations and random intensity ﬂuctuations further improve accuracy.
The proposed augmentations based on learned deformation ﬁelds improve performance as compared to random elastic augmentations. These results show the
beneﬁt of encouraging the deformations to span the geometric variations present
in entire population (labelled as well as unlabelled images), while still generating
images that are conducive to the training of the segmentation network. Some
examples of the generated deformed images are shown in Fig. 3. Interestingly,
the anatomical shapes in these images are not always realistic. While this may
appear to be counter-intuitive, perhaps preserving realistic shapes of anatomical
structures in not essential to obtaining the best segmentation neural network.
Similar observations can be made about the proposed augmentations based on
learned additive intensity masks as compared to random intensity ﬂuctuations.
Semi-Supervised and Task-Driven Data Augmentation
Again, the improvements may be attributed to encouraging the intensity transformations to span the intensity statistics present in the population, while being
beneﬁcial for the segmentation task. Qualitatively, also as before, the generated
intensity masks (Fig. 3) do not necessarily lead to realistic images.
As both GV and GI are designed to capture diﬀerent characteristics of the entire
dataset, using both the augmentations together may be expected to provide
a higher beneﬁt than employing either one in isolation. Indeed, we observe a
substantial improvement in dice scores with our experiments.
As an additional experiment, we investigated the eﬀect of excluding the regularization term from the training of the generators, GV and GI (λadv = λbig = 0).
While the resulting augmentations still resulted in better performance than random deformations or intensity ﬂuctuations, their beneﬁts were lesser than that
from the ones that were trained with the regularization. This shows that although
the adversarial loss does not ensure the generation of realistic images, it is still
advantageous to include unlabelled images in the learning of the augmentations.
Augmentations obtained from the Mixup method also lead to a substantial
improvement in performance as compared to using aﬃne transformations, random elastic transformations or random intensity ﬂuctuations. Interestingly, this
beneﬁt also occurs despite the augmented images being not realistic looking at
all. One reason for this behaviour might be that the Mixup augmentation method
provides soft probability labels for the augmented images - such soft targets have
been hypothesized to aid optimization by providing more task information per
training sample . Even so, Mixup can only generate augmented images that
are linear combinations of the available labelled images and it is not immediately
clear how to extend this method to use unlabelled images. Finally, we see that
Mixup provides a marginal improvement when applied over the original images
together with the augmentations obtained from the trained generators GV and
GI. This demonstrates the complementary beneﬁts of the two approaches.
While semi-supervised adversarial learning provides improvement in performance
as compared to training with no data augmentation, these beneﬁts are only as
much as those obtained with simple aﬃne augmentation. This observation seems
to be in line with works such as .
Conclusion
One of the challenging requirements for the success of deep learning methods
in medical image analysis problems is that of assembling large-scale annotated
datasets. In this work, we propose a semi-supervised and task-driven data augmentation approach to tackle the problem of robust image segmentation in the
setting of training datasets consisting of as few as 1 to 3 labelled 3d images. This
is achieved via two novel contributions: (i) learning conditional generative models
of mappings between labelled and unlabelled images, in a formulation that also
readily enables the corresponding segmentation annotations to be appropriately
mapped and (ii) guiding these generative models with task-dependent losses. In
the small labelled data setting, for the task of segmenting cardiac MRIs, we show
Authors Suppressed Due to Excessive Length
Number of 3D training volumes used
AugA,GD(λadv = 1, λbig = 10−3)
AugA,GD(λadv = 0, λbig = 0)
AugA,GI(λadv = 1, λbig = 10−3)
AugA,GI(λadv = 0, λbig = 0)
AugA,GD,GI(λadv = 1, λbig = 10−3) 0.651⋆
AugA,Mixup 
AugA,GD,GI,Mixup
Adv Tr SL 
Adv Tr SSL 
Table 1: Average Dice score (DSC) results over 15 runs of 20 test subjects for
the proposed method and relevant works. ∗, †, ⋆denotes statistical signiﬁcance
over AugA,RD, AugA,RI abd AugA,Mixup respectively. (Wilcoxon signed rank test
with threshold p value of 0.05).
that the proposed augmentation method substantially outperforms the conventional data augmentation techniques. Interestingly, we observe that in order to
obtain improved segmentation performance, the generated augmentation images
do not necessarily need to be visually hyper-realistic.