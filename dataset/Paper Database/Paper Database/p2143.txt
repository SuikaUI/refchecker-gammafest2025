A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
A Survey on Visual Transformer
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,
Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao Fellow, IEEE
Abstract—Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the
self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to
computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of
networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive
bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision
transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we
explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient
transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the
self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the
challenges and provide several further research directions for vision transformers.
Index Terms—Transformer, Self-attention, Computer Vision, High-level vision, Low-level vision, Video.
INTRODUCTION
EEP neural networks (DNNs) have become the fundamental
infrastructure in today’s artificial intelligence (AI) systems.
Different types of tasks have typically involved different types
of networks. For example, multi-layer perceptron (MLP) or the
fully connected (FC) network is the classical type of neural
network, which is composed of multiple linear layers and nonlinear activations stacked together , . Convolutional neural
networks (CNNs) introduce convolutional layers and pooling
layers for processing shift-invariant data such as images , .
And recurrent neural networks (RNNs) utilize recurrent cells to
process sequential data or time series data , . Transformer is
a new type of neural network. It mainly utilizes the self-attention
mechanism , to extract intrinsic features and shows great
potential for extensive use in AI applications.
Transformer was first applied to natural language processing
(NLP) tasks where it achieved significant improvements , ,
 . For example, Vaswani et al. first proposed transformer
based on attention mechanism for machine translation and English
constituency parsing tasks. Devlin et al. introduced a new language representation model called BERT (short for Bidirectional
Encoder Representations from Transformers), which pre-trains a
transformer on unlabeled text taking into account the context of
each word as it is bidirectional. When BERT was published, it
obtained state-of-the-art performance on 11 NLP tasks. Brown et
al. pre-trained a massive transformer-based model called
GPT-3 (short for Generative Pre-trained Transformer 3) on 45
Kai Han, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui
Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang,
and Yunhe Wang are with Huawei Noah’s Ark Lab. E-mail: {kai.han,
yunhe.wang}@huawei.com.
Hanting Chen, Zhenhua Liu, Yehui Tang, and Zhaohui Yang are also with
School of EECS, Peking University.
Dacheng Tao is with the School of Computer Science, in the Faculty of
Engineering, at The University of Sydney, 6 Cleveland St, Darlington,
NSW 2008, Australia. E-mail: .
Corresponding to Yunhe Wang and Dacheng Tao.
All authors are listed in alphabetical order of last name (except the
primary and corresponding authors).
TB of compressed plaintext data using 175 billion parameters.
It achieved strong performance on different types of downstream
natural language tasks without requiring any fine-tuning. These
transformer-based models, with their strong representation capacity, have achieved significant breakthroughs in NLP.
Inspired by the major success of transformer architectures in
the field of NLP, researchers have recently applied transformer
to computer vision (CV) tasks. In vision applications, CNNs are
considered the fundamental component , , but nowadays
transformer is showing it is a potential alternative to CNN. Chen et
al. trained a sequence transformer to auto-regressively predict
pixels, achieving results comparable to CNNs on image classification tasks. Another vision transformer model is ViT, which
applies a pure transformer directly to sequences of image patches
to classify the full image. Recently proposed by Dosovitskiy et
al. , it has achieved state-of-the-art performance on multiple
image recognition benchmarks. In addition to image classification,
transformer has been utilized to address a variety of other vision
problems, including object detection , , semantic segmentation , image processing , and video understanding .
Thanks to its exceptional performance, more and more researchers
are proposing transformer-based models for improving a wide
range of visual tasks.
Due to the rapid increase in the number of transformer-based
vision models, keeping pace with the rate of new progress is
becoming increasingly difficult. As such, a survey of the existing
works is urgent and would be beneficial for the community. In
this paper, we focus on providing a comprehensive overview
of the recent advances in vision transformers and discuss the
potential directions for further improvement. To facilitate future
research on different topics, we categorize the transformer models
by their application scenarios, as listed in Table 1. The main
categories include backbone network, high/mid-level vision, lowlevel vision, and video processing. High-level vision deals with the
interpretation and use of what is seen in the image , whereas
mid-level vision deals with how this information is organized into
what we experience as objects and surfaces . Given the gap
 
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
2017.6 | Transformer
Solely based on attention
mechanism, the Transformer is
proposed and shows great
performance on NLP tasks.
2018.10 | BERT
Pre-training transformer models
begin to be dominated in the
field of NLP.
2020.5 | GPT-3
A huge transformer with
170B parameters, takes a
big step towards general
NLP model.
2020.10 | ViT
Pure transformer
architectures work well for
visual recognition.
2021 | ViT Variants
Variants of ViT models,
e.g., DeiT, PVT, TNT,
2020.5 | DETR
A simple yet effective
framework for high-level vision
by viewing object detection as
a direct set prediction problem.
End of 2020 | IPT/SETR/CLIP
Applications of transformer
model on low-level vision,
segment and multimodal
tasks, respectively.
2022 | DALLE2/StableDiffsusion
Generating high-quality
images from natural
language descriptions with
diffusion models.
2023 | GPT4
A generalized multimodal model for both
language and vision
Fig. 1: Key milestones in the development of transformer. The vision transformer models are marked in red.
between high- and mid-level vision is becoming more obscure
in DNN-based vision systems , , we treat them as a
single category here. A few examples of transformer models that
address these high/mid-level vision tasks include DETR , deformable DETR for object detection, and Max-DeepLab 
for segmentation. Low-level image processing mainly deals with
extracting descriptions from images (such descriptions are usually
represented as images themselves) . Typical applications of
low-level image processing include super-resolution, image denoising, and style transfer. At present, only a few works , 
in low-level vision use transformers, creating the need for further
investigation. Another category is video processing, which is an
important part in both computer vision and image-based tasks. Due
to the sequential property of video, transformer is inherently well
suited for use on video tasks , , in which it is beginning
to perform on par with conventional CNNs and RNNs. Here, we
survey the works associated with transformer-based visual models
in order to track the progress in this field. Figure 1 shows the
development timeline of vision transformer — undoubtedly, there
will be many more milestones in the future.
The rest of the paper is organized as follows. Section 2
discusses the formulation of the standard transformer and the selfattention mechanism. Section 4 is the main part of the paper, in
which we summarize the vision transformer models on backbone,
high/mid-level vision, low-level vision, and video tasks. We also
briefly describe efficient transformer methods, as they are closely
related to our main topic. In the final section, we give our
conclusion and discuss several research directions and challenges.
Due to the page limit, we describe the methods of transformer in
NLP in the supplemental material, as the research experience may
be beneficial for vision tasks. In the supplemental material, we also
review the self-attention mechanism for CV as the supplementary
of vision transformer models. In this survey, we mainly include the
representative works (early, pioneering, novel, or inspiring works)
since there are many preprinted works on arXiv and we cannot
include them all in limited pages.
FORMULATION OF TRANSFORMER
Transformer was first used in the field of natural language
processing (NLP) on machine translation tasks. As shown in
Figure 2, it consists of an encoder and a decoder with several
transformer blocks of the same architecture. The encoder generates encodings of inputs, while the decoder takes all the encodings
and using their incorporated contextual information to generate
the output sequence. Each transformer block is composed of a
multi-head attention layer, a feed-forward neural network, shortcut
connection and layer normalization. In the following, we describe
each component of the transformer in detail.
Fig. 2: Structure of the original transformer (image from ).
Self-Attention
In the self-attention layer, the input vector is first transformed into
three different vectors: the query vector q, the key vector k and the
value vector v with dimension dq = dk = dv = dmodel = 512.
Vectors derived from different inputs are then packed together into
three different matrices, namely, Q, K and V. Subsequently, the
attention function between different input vectors is calculated as
follows (and shown in Figure 3 left):
Step 1: Compute scores between different input vectors
with S = Q · K⊤;
Step 2: Normalize the scores for the stability of gradient
with Sn = S/√dk;
Step 3: Translate the scores into probabilities with softmax
function P = softmax(Sn);
Step 4: Obtain the weighted value matrix with Z = V·P.
The process can be unified into a single function:
Attention(Q, K, V) = softmax = sin(
PE(pos, 2i + 1) = cos(
in which pos denotes the position of the word in a sentence, and
i represents the current dimension of the positional encoding. In
this way, each element of the positional encoding corresponds to
a sinusoid, and it allows the transformer model to learn to attend
by relative positions and extrapolate to longer sequence lengths
during inference. In apart from the fixed positional encoding in the
vanilla transformer, learned positional encoding and relative
positional encoding are also utilized in various models ,
Multi-Head Attention. Multi-head attention is a mechanism
that can be used to boost the performance of the vanilla selfattention layer. Note that for a given reference word, we often
want to focus on several other words when going through the
sentence. A single-head self-attention layer limits our ability to
focus on one or more specific positions without influencing the
Fig. 3: (Left) Self-attention process. (Right) Multi-head attention.
The image is from .
attention on other equally important positions at the same time.
This is achieved by giving attention layers different representation
subspace. Specifically, different query, key and value matrices are
used for different heads, and these matrices can project the input
vectors into different representation subspace after training due to
random initialization.
To elaborate on this in greater detail, given an input vector
and the number of heads h, the input vector is first transformed
into three different groups of vectors: the query group, the key
group and the value group. In each group, there are h vectors with
dimension dq′ = dk′ = dv′ = dmodel/h = 64. The vectors
derived from different inputs are then packed together into three
different groups of matrices: {Qi}h
i=1, {Ki}h
i=1 and {Vi}h
The multi-head attention process is shown as follows:
MultiHead(Q′, K′, V′) = Concat(head1, · · · , headh)Wo,
where headi = Attention(Qi, Ki, Vi).
Here, Q′ (and similarly K′ and V′) is the concatenation of
i=1, and Wo ∈Rdmodel×dmodel is the projection weight.
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Other Key Concepts in Transformer
Feed-Forward Network. A feed-forward network (FFN) is applied after the self-attention layers in each encoder and decoder. It
consists of two linear transformation layers and a nonlinear activation function within them, and can be denoted as the following
FFN(X) = W2σ(W1X),
where W1 and W2 are the two parameter matrices of the
two linear transformation layers, and σ represents the nonlinear
activation function, such as GELU . The dimensionality of the
hidden layer is dh = 2048.
Residual Connection in the Encoder and Decoder. As shown in
Figure 2, a residual connection is added to each sub-layer in the
encoder and decoder. This strengthens the flow of information in
order to achieve higher performance. A layer-normalization 
is followed after the residual connection. The output of these
operations can be described as:
LayerNorm(X + Attention(X)).
Here, X is used as the input of self-attention layer, and the query,
key and value matrices Q, K and V are all derived from the same
input matrix X. A variant pre-layer normalization (Pre-LN) is also
widely-used , , . Pre-LN inserts the layer normalization inside the residual connection and before multi-head attention
or FFN. For the normalization layer, there are several alternatives
such as batch normalization . Batch normalization usually
perform worse when applied on transformer as the feature values
change acutely . Some other normalization algorithms ,
 , have been proposed to improve training of transformer.
Final Layer in the Decoder. The final layer in the decoder is used
to turn the stack of vectors back into a word. This is achieved by a
linear layer followed by a softmax layer. The linear layer projects
the vector into a logits vector with dword dimensions, in which
dword is the number of words in the vocabulary. The softmax
layer is then used to transform the logits vector into probabilities.
When used for CV tasks, most transformers adopt the original
transformer’s encoder module. Such transformers can be treated
as a new type of feature extractor. Compared with CNNs which
focus only on local characteristics, transformer can capture longdistance characteristics, meaning that it can easily derive global
information. And in contrast to RNNs, whose hidden state must
be computed sequentially, transformer is more efficient because
the output of the self-attention layer and the fully connected layers
can be computed in parallel and easily accelerated. From this, we
can conclude that further study into using transformer in computer
vision as well as NLP would yield beneficial results.
VISION TRANSFORMER
In this section, we review the applications of transformerbased models in computer vision, including image classification,
high/mid-level vision, low-level vision and video processing. We
also briefly summarize the applications of the self-attention mechanism and model compression methods for efficient transformer.
Backbone for Representation Learning
Inspired by the success that transformer has achieved in the field of
NLP, some researchers have explored whether similar models can
learn useful representations for images. Given that images involve
Backbone for Representation Learning
Convolution
CNN + Transformer
Transformer
AlexNet/ResNet/DenseNet
BoTNet/CeiT
ViT/PVT/TNT/Swin
Fig. 4: A taxonomy of backbone using convolution and attention.
more dimensions, noise and redundant modality compared to text,
they are believed to be more difficult for generative modeling.
Other than CNNs, the transformer can be used as backbone
networks for image classification. Wu et al. adopted ResNet
as a convenient baseline and used vision transformers to replace
the last stage of convolutions. Specifically, they apply convolutional layers to extract low-level features that are then fed into
the vision transformer. For the vision transformer, they use a
tokenizer to group pixels into a small number of visual tokens,
each representing a semantic concept in the image. These visual
tokens are used directly for image classification, with the transformers being used to model the relationships between tokens. As
shown in Figure 4, the works can be divided into purely using
transformer for vision and combining CNN and transformer. We
summarize the results of these models in Table 2 and Figure 6
to demonstrate the development of the backbones. In addition to
supervised learning, self-supervised learning is also explored in
vision transformer.
Pure Transformer
ViT. Vision Transformer (ViT) is a pure transformer directly
applies to the sequences of image patches for image classification
task. It follows transformer’s original design as much as possible.
Figure 5 shows the framework of ViT.
To handle 2D images, the image X ∈Rh×w×c is reshaped
into a sequence of flattened 2D patches Xp ∈Rn×(p2·c) such
that c is the number of channels. (h, w) is the resolution of the
original image, while (p, p) is the resolution of each image patch.
The effective sequence length for the transformer is therefore n =
hw/p2. Because the transformer uses constant widths in all of its
layers, a trainable linear projection maps each vectorized path to
the model dimension d, the output of which is referred to as patch
embeddings.
Similar to BERT’s [class] token, a learnable embedding is
applied to the sequence of embedding patches. The state of this
embedding serves as the image representation. During both pretraining and fine-tuning stage, the classification heads are attached
to the same size. In addition, 1D position embeddings are added
to the patch embeddings in order to retain positional information.
It is worth noting that ViT utilizes only the standard transformer’s
encoder (except for the place for the layer normalization), whose
output precedes an MLP head. In most cases, ViT is pre-trained
on large datasets, and then fine-tuned for downstream tasks with
smaller data.
ViT yields modest results when trained on mid-sized datasets
such as ImageNet, achieving accuracies of a few percentage
points below ResNets of comparable size. Because transformers
lack some inductive biases inherent to CNNs–such as translation
equivariance and locality–they do not generalize well when trained
on insufficient amounts of data. However, the authors found
that training the models on large datasets (14 million to 300
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
million images) surpassed inductive bias. When pre-trained at
sufficient scale, transformers achieve excellent results on tasks
with fewer datapoints. For example, when pre-trained on the
JFT-300M dataset, ViT approached or even exceeded state of
the art performance on multiple image recognition benchmarks.
Specifically, it reached an accuracy of 88.36% on ImageNet, and
77.16% on the VTAB suite of 19 tasks.
Touvron et al. proposed a competitive convolution-free
transformer, called Data-efficient image transformer (DeiT), by
training on only the ImageNet database. DeiT-B, the reference vision transformer, has the same architecture as ViT-B and employs
86 million parameters. With a strong data augmentation, DeiT-
B achieves top-1 accuracy of 83.1% (single-crop evaluation) on
ImageNet with no external data. In addition, the authors observe
that using a CNN teacher gives better performance than using
a transformer. Specifically, DeiT-B can achieve top-1 accuracy
84.40% with the help of a token-based distillation.
Fig. 5: The framework of ViT (image from ).
Variants of ViT. Following the paradigm of ViT, a series of
variants of ViT have been proposed to improve the performance
on vision tasks. The main approaches include enhancing locality,
self-attention improvement and architecture design.
The original vision transformer is good at capturing long-range
dependencies between patches, but disregard the local feature
extraction as the 2D patch is projected to a vector with simple
linear layer. Recently, the researchers begin to pay attention to
improve the modeling capacity for local information , ,
 . TNT further divides the patch into a number of subpatches and introduces a novel transformer-in-transformer architecture which utilizes an inner transformer block to model the
relationship between sub-patches and an outer transformer block
for patch-level information exchange. Twins and CAT 
alternately perform local and global attention layer-by-layer. Swin
Transformers , performs local attention within a window and introduces a shifted window partitioning approach for
cross-window connections. Shuffle Transformer , further
utilizes the spatial shuffle operation instead of shifted window
partitioning to allow cross-window connections. RegionViT 
generates regional tokens and local tokens from an image, and
local tokens receive global information via attention with regional
tokens. In addition to the local attention, some other works propose
to boost local information through local feature aggregation, e.g.,
T2T . These works demonstrate the benefit of the local
information exchange and global information exchange in vision
transformer.
As a key component of transformer, self-attention layer provides the ability for global interaction between image patches.
Improving the calculation of self-attention layer has attracted
many researchers. DeepViT proposes to establish crosshead communication to re-generate the attention maps to increase
the diversity at different layers. KVT introduces the k-NN
attention to utilize locality of images patches and ignore noisy
tokens by only computing attentions with top-k similar tokens. Refiner explores attention expansion in higher-dimension space
and applied convolution to augment local patterns of the attention
maps. XCiT performs self-attention calculation across feature
channels rather than tokens, which allows efficient processing of
high-resolution images. The computation complexity and attention
precision of the self-attention mechanism are two key-points for
future optimization.
The network architecture is an important factor as demonstrated in the field of CNNs. The original architecture of ViT
is a simple stack of the same-shape transformer block. New
architecture design for vision transformer has been an interesting
topic. The pyramid-like architecture is utilized by many vision
transformer models , , , , , including
PVT , HVT , Swin Transformer and PiT . There
are also other types of architectures, such as two-stream architecture and U-net architecture , . Neural architecture
search (NAS) has also been investigated to search for better
transformer architectures, e.g., Scaling-ViT , ViTAS ,
AutoFormer and GLiT . Currently, both network design
and NAS for vision transformer mainly draw on the experience of
CNN. In the future, we expect the specific and novel architectures
appear in the filed of vision transformer.
In addition to the aforementioned approaches, there are some
other directions to further improve vision transformer, e.g., positional encoding , , normalization strategy , shortcut
connection and removing attention , , , .
Transformer with Convolution
Although vision transformers have been successfully applied to
various visual tasks due to their ability to capture long-range
dependencies within the input, there are still gaps in performance
between transformers and existing CNNs. One main reason can be
the lack of ability to extract local information. Except the above
mentioned variants of ViT that enhance the locality, combining the
transformer with convolution can be a more straightforward way
to introduce the locality into the conventional transformer.
There are plenty of works trying to augment a conventional
transformer block or self-attention layer with convolution. For
example, CPVT proposed a conditional positional encoding
(CPE) scheme, which is conditioned on the local neighborhood
of input tokens and adaptable to arbitrary input sizes, to leverage
convolutions for fine-level feature encoding. CvT , CeiT ,
LocalViT and CMT analyzed the potential drawbacks
when directly borrowing Transformer architectures from NLP and
combined the convolutions with transformers together. Specifically, the feed-forward network (FFN) in each transformer block is
combined with a convolutional layer that promotes the correlation
among neighboring tokens. LeViT revisited principles from
extensive literature on CNNs and applied them to transformers,
proposing a hybrid neural network for fast inference image classification. BoTNet replaced the spatial convolutions with
global self-attention in the final three bottleneck blocks of a
ResNet, and improved upon the baselines significantly on both
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
TABLE 2: ImageNet result comparison of representative CNN and
vision transformer models. Pure transformer means only using a few
convolutions in the stem stage. CNN + Transformer means using
convolutions in the intermediate layers. Following , , the
throughput is measured on NVIDIA V100 GPU and Pytorch, with
224×224 input size.
Throughput
ResNet-50 , 
ResNet-101 , 
ResNet-152 , 
EfficientNet-B0 
EfficientNet-B1 
EfficientNet-B2 
EfficientNet-B3 
EfficientNet-B4 
Pure Transformer
DeiT-Ti , 
DeiT-S , 
DeiT-B , 
T2T-ViT-14 
T2T-ViT-19 
T2T-ViT-24 
PVT-Small 
PVT-Medium 
PVT-Large 
TNT-S 
TNT-B 
CPVT-S 
CPVT-B 
Swin-T 
Swin-S 
Swin-B 
CNN + Transformer
Twins-SVT-S 
Twins-SVT-B 
Twins-SVT-L 
Shuffle-T 
Shuffle-S 
Shuffle-B 
CMT-S 
CMT-B 
VOLO-D1 
VOLO-D2 
VOLO-D3 
VOLO-D4 
VOLO-D5 
instance segmentation and object detection tasks with minimal
overhead in latency.
Besides, some researchers have demonstrated that transformer
based models can be more difficult to enjoy a favorable ability of
fitting data , , , in other words, they are sensitive
to the choice of optimizer, hyper-parameter, and the schedule of
training. Visformer revealed the gap between transformers
and CNNs with two different training settings. The first one is the
standard setting for CNNs, i.e., the training schedule is shorter
and the data augmentation only contains random cropping and
horizental flipping. The other one is the training setting used
in , i.e., the training schedule is longer and the data augmentation is stronger. changed the early visual processing of ViT
by replacing its embedding stem with a standard convolutional
stem, and found that this change allows ViT to converge faster
and enables the use of either AdamW or SGD without a significant
drop in accuracy. In addition to these two works, , also
choose to add convolutional stem on the top of the transformer.
Self-supervised Representation Learning
Generative Based Approach. Generative pre-training methods
for images have existed for a long time , , , .
Chen et al. re-examined this class of methods and combined
it with self-supervised methods. After that, several works ,
Accuracy (%)
EfficientNet
Throughput (image/s)
Accuracy (%)
EfficientNet
(a) Acc v.s. FLOPs.
(b) Acc v.s. throughput.
Fig. 6: FLOPs and throughput comparison of representative CNN
and vision transformer models.
 were proposed to extend generative based self-supervised
learning for vision transformer.
We briefly introduce iGPT to demonstrate its mechanism.
This approach consists of a pre-training stage followed by a finetuning stage. During the pre-training stage, auto-regressive and
BERT objectives are explored. To implement pixel prediction, a
sequence transformer architecture is adopted instead of language
tokens (as used in NLP). Pre-training can be thought of as a
favorable initialization or regularizer when used in combination
with early stopping. During the fine-tuning stage, they add a
small classification head to the model. This helps optimize a
classification objective and adapts all weights.
The image pixels are transformed into a sequential data by
k-means clustering. Given an unlabeled dataset X consisting of
high dimensional data x = (x1, · · · , xn), they train the model by
minimizing the negative log-likelihood of the data:
x∼X[−log p(x)],
where p(x) is the probability density of the data of images, which
can be modeled as:
p(xπi|xπ1, · · · , xπi−1, θ).
Here, the identity permutation πi = i is adopted for 1 ⩽i ⩽n,
which is also known as raster order. Chen et al. also considered the
BERT objective, which samples a sub-sequence M ⊂[1, n] such
that each index i independently has probability 0.15 of appearing
in M. M is called the BERT mask, and the model is trained by
minimizing the negative log-likelihood of the “masked” elements
xM conditioned on the “unmasked” ones x[1,n]\M:
[−log p(xi|x[1,n]\M)].
During the pre-training stage, they pick either LAR or LBERT
and minimize the loss over the pre-training dataset.
GPT-2 formulation of the transformer decoder block
is used. To ensure proper conditioning when training the AR
objective, Chen et al. apply the standard upper triangular mask
to the n × n matrix of attention logits. No attention logit masking
is required when the BERT objective is used: Chen et al. zero
out the positions after the content embeddings are applied to the
input sequence. Following the final transformer layer, they apply a
layer norm and learn a projection from the output to logits parameterizing the conditional distributions at each sequence element.
When training BERT, they simply ignore the logits at unmasked
positions.
During the fine-tuning stage, they average pool the output of
the final layer normalization layer across the sequence dimension
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
to extract a d-dimensional vector of features per example. They
learn a projection from the pooled feature to class logits and
use this projection to minimize a cross entropy loss. Practical
applications offer empirical evidence that the joint objective of
cross entropy loss and pretraining loss (LAR or LBERT ) works
even better. After iGPT, masked image modeling is proposed such
as MAE and SimMIM which achieves competitive
performance on downstream tasks.
iGPT and ViT are two pioneering works to apply transformer
for visual tasks. The difference of iGPT and ViT-like models
mainly lies on 3 aspects: 1) The input of iGPT is a sequence of
color palettes by clustering pixels, while ViT uniformly divided
the image into a number of local patches; 2) The architecture of iGPT is an encoder-decoder framework, while ViT only
has transformer encoder; 3) iGPT utilizes auto-regressive selfsupervised loss for training, while ViT is trained by supervised
image classification task.
Contrastive Learning Based Approach. Currently, contrastive
learning is the most popular manner of self-supervised learning for
computer vision. Contrastive learning has been applied on vision
transformer for unsupervised pretraining , , .
Chen et al. investigate the effects of several fundamental
components for training self-supervised ViT. The authors observe
that instability is a major issue that degrades accuracy, and these
results are indeed partial failure and they can be improved when
training is made more stable.
They introduce a “MoCo v3” framework, which is an incremental improvement of MoCo . Specifically, the authors take
two crops for each image under random data augmentation. They
are encodes by two encoders, fq and fk, with output vectors
q and k. Intuitively, q behaves like a “query” and the goal of
learning is to retrieve the corresponding “key”. This is formulated
as minimizing a contrastive loss function, which can be written as:
exp(q · k+/τ)
exp(q · k+/τ) + P
k−exp(q · k−/τ).
Here k+ is fk’s output on the same image as q, known as q’s
positive sample. The set k−consists of fk’s outputs from other
images, known as q’s negative samples. τ is a temperature hyperparameter for l2-normalized q, k. MoCo v3 uses the keys that naturally co-exist in the same batch and abandon the memory queue,
which they find has diminishing gain if the batch is sufficiently
large (e.g., 4096). With this simplification, the contrastive loss can
be implemented in a simple way. The encoder fq consists of a
backbone (e.g., ViT), a projection head and an extra prediction
head; while the encoder fk has the backbone and projection head,
but not the prediction head. fk is updated by the moving-average
of fq, excluding the prediction head.
MoCo v3 shows that the instability is a major issue of training
the self-supervised ViT, thus they describe a simple trick that can
improve the stability in various cases of the experiments. They
observe that it is not necessary to train the patch projection layer.
For the standard ViT patch size, the patch projection matrix is
complete or over-complete. And in this case, random projection
should be sufficient to preserve the information of the original
patches. However, the trick alleviates the issue, but does not solve
it. The model can still be unstable if the learning rate is too big and
the first layer is unlikely the essential reason for the instability.
Discussions
All of the components of vision transformer including multihead self-attention, multi-layer perceptron, shortcut connection,
layer normalization, positional encoding and network topology,
play key roles in visual recognition. As stated above, a number
of works have been proposed to improve the effectiveness and
efficiency of vision transformer. From the results in Figure 6,
we can see that combining CNN and transformer achieve the
better performance, indicating their complementation to each other
through local connection and global connection. Further investigation on backbone networks can lead to the improvement for the
whole vision community. As for the self-supervised representation
learning for vision transformer, we still need to make effort to
pursue the success of large-scale pretraining in the filed of NLP.
High/Mid-level Vision
Recently there has been growing interest in using transformer
for high/mid-level computer vision tasks, such as object detection , , , , , lane detection , segmentation , , and pose estimation , , , .
We review these methods in this section.
Generic Object Detection
Traditional object detectors are mainly built upon CNNs, but
transformer-based object detection has gained significant interest
recently due to its advantageous capability.
Some object detection methods have attempted to use transformer’s self-attention mechanism and then enhance the specific modules for modern detectors, such as feature fusion
module and prediction head . We discuss this in
the supplemental material. Transformer-based object detection
methods are broadly categorized into two groups: transformerbased set prediction methods , , , , and
transformer-based backbone methods , , as shown in
Fig. 7. Transformer-based methods have shown strong performance compared with CNN-based detectors, in terms of both
accuracy and running speed. Table 3 shows the detection results
for different transformer-based object detectors mentioned earlier
on the COCO 2012 val set.
Transformer
Transformer
Set prediction
Transformer
(b) Transformer-based backbone for detection
(a) Transformer-based set prediction for detection
Positional
Positional
Fig. 7: General framework of transformer-based object detection.
Transformer-based Set Prediction for Detection. As a pioneer
for transformer-based detection method, the detection transformer
(DETR) proposed by Carion et al. redesigns the framework
of object detection. DETR, a simple and fully end-to-end object
detector, treats the object detection task as an intuitive set prediction problem, eliminating traditional hand-crafted components
such as anchor generation and non-maximum suppression (NMS)
post-processing. As shown in Fig. 8, DETR starts with a CNN
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
backbone to extract features from the input image. To supplement
the image features with position information, fixed positional encodings are added to the flattened features before the features are
fed into the encoder-decoder transformer. The decoder consumes
the embeddings from the encoder along with N learned positional
encodings (object queries), and produces N output embeddings.
Here N is a predefined parameter and typically larger than the
number of objects in an image. Simple feed-forward networks
(FFNs) are used to compute the final predictions, which include
the bounding box coordinates and class labels to indicate the
specific class of object (or to indicate that no object exists). Unlike
the original transformer, which computes predictions sequentially,
DETR decodes N objects in parallel. DETR employs a bipartite
matching algorithm to assign the predicted and ground-truth
objects. As shown in Eq. 11, the Hungarian loss is exploited to
compute the loss function for all matched pairs of objects.
LHungarian(y, ˆy) =
−log ˆpˆσ(i)(ci) + 1{ci̸=∅}Lbox(bi,ˆbˆσ(i))
where ˆσ is the optimal assignment, ci and ˆpˆσ(i)(ci) are the target
class label and predicted label, respectively, and bi and ˆbˆσ(i) are
the ground truth and predicted bounding box, y = {(ci, bi)}
and ˆy are the ground truth and prediction of objects, respectively.
DETR shows impressive performance on object detection, delivering comparable accuracy and speed with the popular and wellestablished Faster R-CNN baseline on COCO benchmark.
set of image features
transformer
positional encoding
transformer
object queries
prediction heads
Fig. 8: The overall architecture of DETR (image from ).
DETR is a new design for the object detection framework
based on transformer and empowers the community to develop
fully end-to-end detectors. However, the vanilla DETR poses
several challenges, specifically, longer training schedule and poor
performance for small objects. To address these challenges, Zhu et
al. proposed Deformable DETR, which has become a popular
method that significantly improves the detection performance. The
deformable attention module attends to a small set of key positions
around a reference point rather than looking at all spatial locations
on image feature maps as performed by the original multi-head
attention mechanism in transformer. This approach significantly
reduces the computational complexity and brings benefits in terms
of fast convergence. More importantly, the deformable attention
module can be easily applied for fusing multi-scale features.
Deformable DETR achieves better performance than DETR with
10× less training cost and 1.6× faster inference speed. And by
using an iterative bounding box refinement method and two-stage
scheme, Deformable DETR can further improve the detection
performance.
There are also several methods to deal with the slow convergence problem of the original DETR. For example, Sun et al. 
investigated why the DETR model has slow convergence and
discovered that this is mainly due to the cross-attention module
in the transformer decoder. To address this issue, an encoder-only
version of DETR is proposed, achieving considerable improvement in terms of detection accuracy and training convergence. In
addition, a new bipartite matching scheme is designed for greater
training stability and faster convergence and two transformerbased set prediction models, i.e. TSP-FCOS and TSP-RCNN, are
proposed to improve encoder-only DETR with feature pyramids.
These new models achieve better performance compared with the
original DETR model. Gao et al. proposed the Spatially
Modulated Co-Attention (SMCA) mechanism to accelerate the
convergence by constraining co-attention responses to be high
near initially estimated bounding box locations. By integrating
the proposed SMCA module into DETR, similar mAP could be
obtained with about 10× less training epochs under comparable
inference cost.
Given the high computation complexity associated with
DETR, Zheng et al. proposed an Adaptive Clustering
Transformer (ACT) to reduce the computation cost of pre-trained
DETR. ACT adaptively clusters the query features using a locality
sensitivity hashing (LSH) method and broadcasts the attention
output to the queries represented by the selected prototypes. ACT
is used to replace the self-attention module of the pre-trained
DETR model without requiring any re-training. This approach
significantly reduces the computational cost while the accuracy
slides slightly. The performance drop can be further reduced by
utilizing a multi-task knowledge distillation (MTKD) method,
which exploits the original transformer to distill the ACT module
with a few epochs of fine-tuning. Yao et al.
 pointed out
that the random initialization in DETR is the main reason for
the requirement of multiple decoder layers and slow convergence.
To this end, they proposed the Efficient DETR to incorporate
the dense prior into the detection pipeline via an additional
region proposal network. The better initialization enables them
to use only one decoder layers instead of six layers to achieve
competitive performance with a more compact network.
Transformer-based Backbone for Detection. Unlike DETR
which redesigns object detection as a set prediction tasks via
transformer, Beal et al. proposed to utilize transformer as
a backbone for common detection frameworks such as Faster R-
CNN . The input image is divided into several patches and
fed into a vision transformer, whose output embedding features
are reorganized according to spatial information before passing
through a detection head for the final results. A massive pretraining transformer backbone could bring benefits to the proposed
ViT-FRCNN. There are also quite a few methods to explore versatile vision transformer backbone design , , , and
transfer these backbones to traditional detection frameworks like
RetinaNet and Cascade R-CNN . For example, Swin
Transformer obtains about 4 box AP gains over ResNet-50
backbone with similar FLOPs for various detection frameworks.
Pre-training for Transformer-based Object Detection. Inspired
by the pre-training transformer scheme in NLP, several methods
have been proposed to explore different pre-training scheme
for transformer-based object detection , , . Dai et
al. proposed unsupervised pre-training for object detection
(UP-DETR). Specifically, a novel unsupervised pretext task named
random query patch detection is proposed to pre-train the DETR
model. With this unsupervised pre-training scheme, UP-DETR
significantly improves the detection accuracy on a relatively small
dataset (PASCAL VOC). On the COCO benchmark with sufficient
training data, UP-DETR still outperforms DETR, demonstrating
the effectiveness of the unsupervised pre-training scheme.
Fang et al. explored how to transfer the pure ViT
structure that is pre-trained on ImageNet to the more challenging
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
TABLE 3: Comparison of different transformer-based object detectors on COCO 2017 val set. Running speed (FPS) is evaluated on an
NVIDIA Tesla V100 GPU as reported in . †Estimated speed according to the reported number in the paper. ‡ViT backbone is pre-trained
on ImageNet-21k. ∗ViT backbone is pre-trained on an private dataset with 1.3 billion images.
#Params (M)
FCOS 
Faster R-CNN + FPN 
CNN Backbone + Transformer Head
DETR-DC5 
Deformable DETR 
TSP-FCOS 
TSP-RCNN 
ACT+MKKD (L=32) 
SMCA 
Efficient DETR 
UP-DETR 
UP-DETR 
Transformer Backbone + CNN Head
ViT-B/16-FRCNN‡ 
ViT-B/16-FRCNN∗ 
PVT-Small+RetinaNet 
Twins-SVT-S+RetinaNet 
Swin-T+RetinaNet 
Swin-T+ATSS 
Pure Transformer based
PVT-Small+DETR 
TNT-S+DETR 
YOLOS-Ti 
YOLOS-S 
YOLOS-B 
object detection task and proposed the YOLOS detector. To cope
with the object detection task, the proposed YOLOS first drops
the classification tokens in ViT and appends learnable detection
tokens. Besides, the bipartite matching loss is utilized to perform
set prediction for objects. With this simple pre-training scheme
on ImageNet dataset, the proposed YOLOS shows competitive
performance for object detection on COCO benchmark.
Segmentation
Segmentation is an important topic in computer vision community,
which broadly includes panoptic segmentation, instance segmentation and semantic segmentation etc. Vision transformer has also
shown impressive potential on the field of segmentation.
Transformer for Panoptic Segmentation. DETR can be
naturally extended for panoptic segmentation tasks and achieve
competitive results by appending a mask head on the decoder.
Wang et al. proposed Max-DeepLab to directly predict
panoptic segmentation results with a mask transformer, without
involving surrogate sub-tasks such as box detection. Similar to
DETR, Max-DeepLab streamlines the panoptic segmentation tasks
in an end-to-end fashion and directly predicts a set of nonoverlapping masks and corresponding labels. Model training is
performed using a panoptic quality (PQ) style loss, but unlike prior
methods that stack a transformer on top of a CNN backbone, Max-
DeepLab adopts a dual-path framework that facilitates combining
the CNN and transformer.
Transformer for Instance Segmentation. VisTR, a transformerbased video instance segmentation model, was proposed by
Wang et al. to produce instance prediction results from
a sequence of input images. A strategy for matching instance
sequence is proposed to assign the predictions with ground truths.
In order to obtain the mask sequence for each instance, VisTR
utilizes the instance sequence segmentation module to accumulate
the mask features from multiple frames and segment the mask
sequence with a 3D CNN. Hu et al. proposed an instance
segmentation Transformer (ISTR) to predict low-dimensional
mask embeddings, and match them with ground truth for the set
loss. ISTR conducted detection and segmentation with a recurrent
refinement strategy which is different from the existing top-down
and bottom-up frameworks. Yang et al. investigated how
to realize better and more efficient embedding learning to tackle
the semi-supervised video object segmentation under challenging
multi-object scenarios. Some papers such as
 , also
discussed using Transformer to deal with segmentation task.
Transformer for Semantic Segmentation. Zheng et al. 
proposed a transformer-based semantic segmentation network
(SETR). SETR utilizes an encoder similar to ViT as the
encoder to extract features from an input image. A multi-level
feature aggregation module is adopted for performing pixel-wise
segmentation. Strudel et al. introduced Segmenter which
relies on the output embedding corresponding to image patches
and obtains class labels with a point-wise linear decoder or a mask
transformer decoder. Xie et al. proposed a simple, efficient
yet powerful semantic segmentation framework which unifies
Transformers with lightweight multilayer perception (MLP) decoders, which outputs multiscale features and avoids complex
Transformer for Medical Image Segmentation. Cao et al. 
proposed an Unet-like pure Transformer for medical image segmentation, by feeding the tokenized image patches into the
Transformer-based U-shaped Encoder-Decoder architecture with
skip-connections for local-global semantic feature learning. Valanarasu et al. explored transformer-based solutions and study
the feasibility of using transformer-based network architectures
for medical image segmentation tasks and proposed a Gated
Axial-Attention model which extends the existing architectures by
introducing an additional control mechanism in the self-attention
module. Cell-DETR , based on the DETR panoptic segmentation model, is an attempt to use transformer for cell instance segmentation. It adds skip connections that bridge features between
the backbone CNN and the CNN decoder in the segmentation
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
head in order to enhance feature fusion. Cell-DETR achieves
state-of-the-art performance for cell instance segmentation from
microscopy imagery.
Pose Estimation
Human pose and hand pose estimation are foundational topics that
have attracted significant interest from the research community.
Articulated pose estimation is akin to a structured prediction task,
aiming to predict the joint coordinates or mesh vertices from input
RGB/D images. Here we discuss some methods , , ,
 that explore how to utilize transformer for modeling the
global structure information of human poses and hand poses.
Transformer for Hand Pose Estimation. Huang et al. proposed a transformer based network for 3D hand pose estimation
from point sets. The encoder first utilizes a PointNet to
extract point-wise features from input point clouds and then adopts
standard multi-head self-attention module to produce embeddings.
In order to expose more global pose-related information to the
decoder, a feature extractor such as PointNet++ is used
to extract hand joint-wise features, which are then fed into the
decoder as positional encodings. Similarly, Huang et al. 
proposed HOT-Net (short for hand-object transformer network)
for 3D hand-object pose estimation. Unlike the preceding method
which employs transformer to directly predict 3D hand pose from
input point clouds, HOT-Net uses a ResNet to generate initial 2D
hand-object pose and then feeds it into a transformer to predict
the 3D hand-object pose. A spectral graph convolution network
is therefore used to extract input embeddings for the encoder.
Hampali et al. proposed to estimate the 3D poses of two
hands given a single color image. Specifically, appearance and
spatial encodings of a set of potential 2D locations for the joints
of both hands were inputted to a transformer, and the attention
mechanisms were used to sort out the correct configuration of the
joints and outputted the 3D poses of both hands.
Transformer for Human Pose Estimation. Lin et al. 
proposed a mesh transformer (METRO) for predicting 3D human
pose and mesh from a single RGB image. METRO extracts
image features via a CNN and then perform position encoding
by concatenating a template human mesh to the image feature. A
multi-layer transformer encoder with progressive dimensionality
reduction is proposed to gradually reduce the embedding dimensions and finally produce 3D coordinates of human joint and mesh
vertices. To encourage the learning of non-local relationships between human joints, METRO randomly mask some input queries
during training. Yang et al. constructed an explainable model
named TransPose based on Transformer architecture and low-level
convolutional blocks. The attention layers built in Transformer can
capture long-range spatial relationships between keypoints and explain what dependencies the predicted keypoints locations highly
rely on. Li et al. proposed a novel approach based on Token
representation for human Pose estimation (TokenPose). Each keypoint was explicitly embedded as a token to simultaneously learn
constraint relationships and appearance cues from images. Mao et
al. proposed a human pose estimation framework that solved
the task in the regression-based fashion. They formulated the pose
estimation task into a sequence prediction problem and solve it by
transformers, which bypass the drawbacks of the heatmap-based
pose estimator. Jiang et al. proposed a novel transformer
based network that can learn a distribution over both pose and
motion in an unsupervised fashion rather than tracking body parts
and trying to temporally smooth them. The method overcame
inaccuracies in detection and corrected partial or entire skeleton
corruption. Hao et al. proposed to personalize a human pose
estimator given a set of test images of a person without using
any manual annotations. The method adapted the pose estimator
during test time to exploit person-specific information, and used
a Transformer model to build a transformation between the selfsupervised keypoints and the supervised keypoints.
Other Tasks
There are also quite a lot different high/mid-level vision tasks
that have explored the usage of vision transformer for better
performance. We briefly review several tasks below.
Pedestrian Detection. Because the distribution of objects is very
dense in occlusion and crowd scenes, additional analysis and
adaptation are often required when common detection networks
are applied to pedestrian detection tasks. Lin et al. revealed
that sparse uniform queries and a weak attention field in the
decoder result in performance degradation when directly applying
DETR or Deformable DETR to pedestrian detection tasks. To
alleviate these drawbacks, the authors proposes Pedestrian Endto-end Detector (PED), which employs a new decoder called
Dense Queries and Rectified Attention field (DQRF) to support
dense queries and alleviate the noisy or narrow attention field
of the queries. They also proposed V-Match, which achieves
additional performance improvements by fully leveraging visible
annotations.
Lane Detection. Based on PolyLaneNet , Liu et al. 
proposed a method called LSTR, which improves performance
of curve lane detection by learning the global context with
a transformer network. Similar to PolyLaneNet, LSTR regards
lane detection as a task of fitting lanes with polynomials and
uses neural networks to predict the parameters of polynomials.
To capture slender structures for lanes and the global context,
LSTR introduces a transformer network into the architecture. This
enables processing of low-level features extracted by CNNs. In addition, LSTR uses Hungarian loss to optimize network parameters.
As demonstrated in , LSTR outperforms PolyLaneNet, with
2.82% higher accuracy and 3.65× higher FPS using 5-times fewer
parameters. The combination of a transformer network, CNN and
Hungarian Loss culminates in a lane detection framework that
is precise, fast, and tiny. Considering that the entire lane line
generally has an elongated shape and long-range, Liu et al. 
utilized a transformer encoder structure for more efficient context
feature extraction. This transformer encoder structure improves
the detection of the proposal points a lot, which rely on contextual
features and global information, especially in the case where the
backbone network is a small model.
Scene Graph. Scene graph is a structured representation of a
scene that can clearly express the objects, attributes, and relationships between objects in the scene . To generate scene
graph, most of existing methods first extract image-based object
representations and then do message propagation between them.
Graph R-CNN utilizes self-attention to integrate contextual
information from neighboring nodes in the graph. Recently, Sharifzadeh et al. employed transformers over the extracted
object embedding. Sharifzadeh et al. proposed a new
pipeline called Texema and employed a pre-trained Text-to-Text
Transfer Transformer (T5) to create structured graphs from
textual input and utilized them to improve the relational reasoning
module. The T5 model enables Texema to utilize the knowledge
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Tracking. Some researchers also explored to use transformer
encoder-decoder architecture in template-based discriminative
trackers, such as TMT , TrTr and TransT . All
these work use a Siamese-like tracking pipeline to do video
object tracking and utilize the encoder-decoder network to replace explicit cross-correlation operation for global and rich
contextual inter-dependencies. Specifically, the transformer encoder and decoder are assigned to the template branch and the
searching branch, respectively. In addition, Sun et al. proposed
TransTrack , which is an online joint-detection-and-tracking
pipeline. It utilizes the query-key mechanism to track pre-existing
objects and introduces a set of learned object queries into the
pipeline to detect new-coming objects. The proposed TransTrack
achieves 74.5% and 64.5% MOTA on MOT17 and MOT20 benchmark.
Re-Identification. He et al. proposed TransReID to investigate the application of pure transformers in the field of object
re-identification (ReID). While introducing transformer network
into object ReID, TransReID slices with overlap to reserve local
neighboring structures around the patches and introduces 2D bilinear interpolation to help handle any given input resolution. With
the transformer module and the loss function, a strong baseline
was proposed to achieve comparable performance with CNNbased frameworks. Moreover, The jigsaw patch module (JPM)
was designed to facilitate perturbation-invariant and robust feature
representation of objects and the side information embeddings
(SIE) was introduced to encode side information. The final framework TransReID achieves state-of-the-art performance on both
person and vehicle ReID benchmarks. Both Liu et al. and
Zhang et al. provided solutions for introducing transformer
network into video-based person Re-ID. And similarly, both of the
them utilized separated transformer networks to refine spatial and
temporal features, and then utilized a cross view transformer to
aggregate multi-view features.
Point Cloud Learning. A number of other works exploring
transformer architecture for point cloud learning , ,
 have also emerged recently. For example, Guo et al. 
proposed a novel framework that replaces the original selfattention module with a more suitable offset-attention module,
which includes implicit Laplace operator and normalization refinement. In addition, Zhao et al. designed a novel transformer
architecture called Point Transformer. The proposed self-attention
layer is invariant to the permutation of the point set, making it
suitable for point set processing tasks. Point Transformer shows
strong performance for semantic segmentation task from 3D point
Discussions
As discussed in the preceding sections, transformers have shown
strong performance on several high-level tasks, including detection, segmentation and pose estimation. The key issues that need to
be resolved before transformer can be adopted for high-level tasks
relate to input embedding, position encoding, and prediction loss.
Some methods propose improving the self-attention module from
different perspectives, for example, deformable attention ,
adaptive clustering and point transformer . Nevertheless, exploration into the use of transformers for high-level vision
tasks is still in the preliminary stages and so further research
may prove beneficial. For example, is it necessary to use feature
extraction modules such as CNN and PointNet before transformer
for potential better performance? How can vision transformer be
fully leveraged using large scale pre-training datasets as BERT
and GPT-3 do in the NLP field? And is it possible to pre-train a
single transformer model and fine-tune it for different downstream
tasks with only a few epochs of fine-tuning? How to design more
powerful architecture by incorporating prior knowledge of the
specific tasks? Several prior works have performed preliminary
discussions for the aforementioned topics and We hope more
further research effort is conducted into exploring more powerful
transformers for high-level vision.
Low-level Vision
Few works apply transformers on low-level vision fields, such
as image super-resolution and generation. These tasks often take
images as outputs (e.g., high-resolution or denoised images),
which is more challenging than high-level vision tasks such as
classification, segmentation, and detection, whose outputs are
labels or boxes.
Transformer
(a) Image Generation
(GAN-based)
(b) Image Generation
(Transformer-based)
Transformer
Training Only
Fig. 9: A generic framework for transformer in image generation.
Image Generation
An simple yet effective to apply transformer model to the image
generation task is to directly change the architectures from CNNs
to transformers, as shown in Figure 9 (a). Jiang et al. proposed
TransGAN, which build GAN using the transformer architecture. Since the it is difficult to generate high-resolution images
pixel-wise, a memory-friendly generator is utilized by gradually
increasing the feature map resolution at different stages. Correspondingly, a multi-scale discriminator is designed to handle the
varying size of inputs in different stages. Various training recipes
are introduced including grid self-attention, data augmentation,
relative position encoding and modified normalization to stabilize
the training and improve its performance. Experiments on various
benchmark datasets demonstrate the effectiveness and potential
of the transformer-based GAN model in image generation tasks.
Kwonjoon Lee et al. proposed ViTGAN, which introduce
several technique to both generator and discriminator to stabilize
the training procedure and convergence. Euclidean distance is
introduced for the self-attention module to enforce the Lipschitzness of transformer discriminator. Self-modulated layernorm
and implicit neural representation are proposed to enhance the
training for the generator. As a result, ViTGAN is the first work
to demonstrate transformer-based GANs can achieve comparable
performance to state-of-the-art CNN-based GANs.
Parmar et al. proposed Image Transformer, taking the first
step toward generalizing the transformer model to formulate image
translation and generation tasks in an auto-regressive manner.
Image Transformer consists of two parts: an encoder for extracting
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
image representation and a decoder to generate pixels. For each
pixel with value 0 −255, a 256 × d dimensional embedding
is learned for encoding each value into a d dimensional vector,
which is fed into the encoder as input. The encoder and decoder
adopt the same architecture as that in . Each output pixel
q′ is generated by calculating self-attention between the input
pixel q and previously generated pixels m1, m2, ... with position
embedding p1, p2, .... For image-conditioned generation, such as
super-resolution and inpainting, an encoder-decoder architecture is
used, where the encoder’s input is the low-resolution or corrupted
images. For unconditional and class-conditional generation (i.e.,
noise to image), only the decoder is used for inputting noise vectors. Because the decoder’s input is the previously generated pixels
(involving high computation cost when producing high-resolution
images), a local self-attention scheme is proposed. This scheme
uses only the closest generated pixels as input for the decoder,
enabling Image Transformer to achieve performance on par with
CNN-based models for image generation and translation tasks,
demonstrating the effectiveness of transformer-based models on
low-level vision tasks.
Since it is difficult to directly generate high-resolution images
by transformer models, Esser et al. proposed Taming Transformer. Taming Transformer consists of two parts: a VQGAN
and a transformer. VQGAN is a variant of VQVAE , which
uses a discriminator and perceptual loss to improve the visual
quality. Through VQGAN, the image can be represented by a
series of context-rich discrete vectors and therefore these vectors
can be easily predicted by a transformer model through an autoregression way. The transformer model can learn the long-range
interactions for generating high-resolution images. As a result, the
proposed Taming Transformer achieves state-of-the-art results on
a wide variety of image synthesis tasks.
Besides image generation, DALL·E proposed the transformer model for text-to-image generation, which synthesizes
images according to the given captions. The whole framework
consists of two stages. In the first stage, a discrete VAE is
utilized to learn the visual codebook. In the second stage, the
text is decoded by BPE-encode and the corresponding image
is decoded by dVAE learned in the first stage. Then an autoregression transformer is used to learn the prior between the
encoded text and image. During the inference procedure, tokens
of images are predicted by the transformer and decoded by the
learned decoder. The CLIP model is introduced to rank
generated samples. Experiments on text-to-image generation task
demonstrate the powerful ability of the proposed model. Note that
our survey mainly focus on pure vision tasks, we do not include
the framework of DALL·E in Figure 9. The image generation has
been pushed to a higher level with the introduction of diffusion
model , such as DALLE2 and Stable Diffusion .
Image Processing
A number of recent works eschew using each pixel as the input
for transformer models and instead use patches (set of pixels) as
input. For example, Yang et al. proposed Texture Transformer
Network for Image Super-Resolution (TTSR), using the transformer architecture in the reference-based image super-resolution
problem. It aims to transfer relevant textures from reference
images to low-resolution images. Taking a low-resolution image
and reference image as the query Q and key K, respectively, the
relevance ri,j is calculated between each patch qi in Q and ki in
A hard-attention module is proposed to select high-resolution
features V according to the reference image, so that the lowresolution image can be matched by using the relevance. The
hard-attention map is calculated as:
hi = arg max
The most relevant reference patch is ti = vhi, where ti in
T is the transferred features. A soft-attention module is then
used to transfer V to the low-resolution feature. The transferred
features from the high-resolution texture image and the lowresolution feature are used to generate the output features of
the low-resolution image. By leveraging the transformer-based
architecture, TTSR can successfully transfer texture information
from high-resolution reference images to low-resolution images in
super-resolution tasks.
Transformer Encoder
Multi-head
Multi-tail
Flatten features
Task embedding
Transformer Decoder
Fig. 10: Diagram of IPT architecture (image from ).
Different from the preceding methods that use transformer
models on single tasks, Chen et al. proposed Image Processing Transformer (IPT), which fully utilizes the advantages
of transformers by using large pre-training datasets. It achieves
state-of-the-art performance in several image processing tasks,
including super-resolution, denoising, and deraining. As shown
in Figure 10, IPT consists of multiple heads, an encoder, a
decoder, and multiple tails. The multi-head, multi-tail structure
and task embeddings are introduced for different image processing
tasks. The features are divided into patches, which are fed into
the encoder-decoder architecture. Following this, the outputs are
reshaped to features with the same size. Given the advantages
of pre-training transformer models on large datasets, IPT uses
the ImageNet dataset for pre-training. Specifically, images from
this dataset are degraded by manually adding noise, rain streaks,
or downsampling in order to generate corrupted images. The
degraded images are used as inputs for IPT, while the original
images are used as the optimization goal of the outputs. A selfsupervised method is also introduced to enhance the generalization
ability of the IPT model. Once the model is trained, it is finetuned on each task by using the corresponding head, tail, and
task embedding. IPT largely achieves performance improvements
on image processing tasks (e.g., 2 dB in image denoising tasks),
demonstrating the huge potential of applying transformer-based
models to the field of low-level vision.
Besides single image generation, Wang et al. proposed
SceneFormer to utilize transformer in 3D indoor scene generation.
By treating a scene as a sequence of objects, the transformer
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
decoder can be used to predict series of objects and their location,
category, and size. This has enabled SceneFormer to outperform
conventional CNN-based methods in user studies.
Transformer
Transformer
Fig. 11: A generic framework for transformer in image processing.
It should be noted that iGPT is pre-trained on an
inpainting-like task. Since iGPT mainly focus on the fine-tuning
performance on image classification tasks, we treat this work more
like an attempt on image classification task using transformer than
low-level vision tasks.
In conclusion, different to classification and detection tasks,
the outputs of image generation and processing are images. Figure 11 illustrates using transformers in low-level vision. In image
processing tasks, the images are first encoded into a sequence of
tokens or patches and the transformer encoder uses the sequence
as input, allowing the transformer decoder to successfully produce
desired images. In image generation tasks, the GAN-based models
directly learn a decoder to generated patches to outputting images
through linear projection, while the transformer-based models
train a auto-encoder to learn a codebook for images and use an
auto-regression transformer model to predict the encoded tokens.
A meaningful direction for future research would be designing a
suitable architecture for different image processing tasks.
Video Processing
Transformer performs surprisingly well on sequence-based tasks
and especially on NLP tasks. In computer vision (specifically,
video tasks), spatial and temporal dimension information is favored, giving rise to the application of transformer in a number
of video tasks, such as frame synthesis , action recognition , and video retrieval .
High-level Video Processing
Video Action Recognition. Video human action tasks, as the
name suggests, involves identifying and localizing human actions
in videos. Context (such as other people and objects) plays a
critical role in recognizing human actions. Rohit et al. proposed
the action transformer to model the underlying relationship
between the human of interest and the surrounding context.
Specifically, the I3D is used as the backbone to extract highlevel feature maps. The features extracted (using RoI pooling)
from intermediate feature maps are viewed as the query (Q), while
the key (K) and values (V) are calculated from the intermediate
features. A self-attention mechanism is applied to the three components, and it outputs the classification and regressions predictions.
Lohit et al. proposed an interpretable differentiable module,
named temporal transformer network, to reduce the intra-class
variance and increase the inter-class variance. In addition, Fayyaz
and Gall proposed a temporal transformer to perform action
recognition tasks under weakly supervised settings. In addition
to human action recognition, transformer has been utilized for
group activity recognition . Gavrilyuk et al. proposed an
actor-transformer architecture to learn the representation,
using the static and dynamic representations generated by the 2D
and 3D networks as input. The output of the transformer is the
predicted activity.
Video Retrieval. The key to content-based video retrieval is to
find the similarity between videos. Leveraging only the imagelevel of video-level features to overcome the associated challenges, Shao et al. suggested using the transformer to model
the long-range semantic dependency. They also introduced the
supervised contrastive learning strategy to perform hard negative
mining. The results of using this approach on benchmark datasets
demonstrate its performance and speed advantages. In addition,
Gabeur et al. presented a multi-modal transformer to learn
different cross-modal cues in order to represent videos.
Video Object Detection. To detect objects in a video, both
global and local information is required. Chen et al. introduced
the memory enhanced global-local aggregation (MEGA) 
to capture more content. The representative features enhance the
overall performance and address the ineffective and insufficient
problems. Furthermore, Yin et al. proposed a spatiotemporal transformer to aggregate spatial and temporal information.
Together with another spatial feature encoding component, these
two components perform well on 3D video object detection tasks.
Multi-task Learning. Untrimmed video usually contains many
frames that are irrelevant to the target tasks. It is therefore
crucial to mine the relevant information and discard the redundant
information. To extract such information, Seong et al. proposed
the video multi-task transformer network , which handles
multi-task learning on untrimmed videos. For the CoVieW dataset,
the tasks are scene recognition, action recognition and importance
score prediction. Two pre-trained networks on ImageNet and
Places365 extract the scene features and object features. The
multi-task transformers are stacked to implement feature fusion,
leveraging the class conversion matrix (CCM).
Low-level Video Processing
Frame/Video Synthesis. Frame synthesis tasks involve synthesizing the frames between two consecutive frames or after a
frame sequence while video synthesis tasks involve synthesizing
a video. Liu et al. proposed the ConvTransformer , which is
comprised of five components: feature embedding, position encoding, encoder, query decoder, and the synthesis feed-forward network. Compared with LSTM based works, the ConvTransformer
achieves superior results with a more parallelizable architecture.
Another transformer-based approach was proposed by Schatz et
al. , which uses a recurrent transformer network to synthetize
human actions from novel views.
Video Inpainting. Video inpainting tasks involve completing
any missing regions within a frame. This is challenging, as it
requires information along the spatial and temporal dimensions
to be merged. Zeng et al. proposed a spatial-temporal transformer
network , which uses all the input frames as input and fills
them in parallel. The spatial-temporal adversarial loss is used to
optimize the transformer network.
Discussions
Compared to image, video has an extra dimension to encode
the temporal information. Exploiting both spatial and temporal
information helps to have a better understanding of a video.
Thanks to the relationship modeling capability of transformer,
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
video processing tasks have been improved by mining spatial
and temporal information simultaneously. Nevertheless, due to
the high complexity and much redundancy of video data, how
to efficiently and accurately modeling both spatial and temporal
relationships is still an open problem.
Multi-Modal Tasks
Owing to the success of transformer across text-based NLP tasks,
many researches are keen to exploit its potential for processing
multi-modal tasks (e.g., video-text, image-text and audio-text).
One example of this is VideoBERT , which uses a CNNbased module to pre-process videos in order to obtain representation tokens. A transformer encoder is then trained on these tokens
to learn the video-text representations for downstream tasks, such
as video caption. Some other examples include VisualBERT 
and VL-BERT , which adopt a single-stream unified transformer to capture visual elements and image-text relationship for
downstream tasks such as visual question answering (VQA) and
visual commonsense reasoning (VCR). In addition, several studies
such as SpeechBERT explore the possibility of encoding
audio and text pairs with a transformer encoder to process autotext tasks such as speech question answering (SQA).
Fig. 12: The framework of the CLIP (image from ).
Apart from the aforementioned pioneering multi-modal transformers, Contrastive Language-Image Pre-training (CLIP) 
takes natural language as supervision to learn more efficient image
representation. CLIP jointly trains a text encoder and an image
encoder to predict the corresponding training text-image pairs.
The text encoder of CLIP is a standard transformer with masked
self-attention used to preserve the initialization ability of the pretrained language models. For the image encoder, CLIP considers
two types of architecture, ResNet and Vision Transformer. CLIP is
trained on a new dataset containing 400 million (image, text) pairs
collected from the Internet. More specifically, given a batch of N
(image, text) pairs, CLIP learns both text and image embeddings
jointly to maximize the cosine similarity of those N matched
embeddings while minimize N 2 −N incorrectly matched embeddings. On Zero-Shot transfer, CLIP demonstrates astonishing zeroshot classification performances, achieving 76.2% top-1 accuracy
on ImageNet-1K dataset without using any ImageNet training
labels. Concretely, at inference, the text encoder of CLIP first
computes the feature embeddings of all ImageNet Labels and the
image encoder then computes the embeddings of all images. By
calculating the cosine similarity of text and image embeddings,
the text-image pair with the highest score should be the image and
its corresponding label. Further experiments on 30 various CV
benchmarks show the zero-shot transfer ability of CLIP and the
feature diversity learned by CLIP.
While CLIP maps images according to the description in text,
another work DALL-E synthesizes new images of categories
described in an input text. Similar to GPT-3, DALL-E is a multimodal transformer with 12 billion model parameters autoregressively trained on a dataset of 3.3 million text-image pairs. More
specifically, to train DALL-E, a two-stage training procedure is
used, where in stage 1, a discrete variational autoencoder is used
to compress 256× 256 RGB images into 32×32 image tokens and
then in stage 2, an autoregressive transformer is trained to model
the joint distribution over the image and text tokens. Experimental
results show that DALL-E can generate images of various styles
from scratch, including photorealistic imagery, cartoons and emoji
or extend an existing image while still matching the description in
the text. Subsequently, Ding et al. proposes CogView , which
is a transformer with VQ-VAE tokenizer similar to DALL-E, but
supports Chinese text input. They claim CogView outperforms
DALL-E and previous GAN-bsed methods and also unlike DALL-
E, CogView does not need an additional CLIP model to rerank the
samples drawn from transformer, i.e. DALL-E.
Recently, a Unified Transformer (UniT) model is proposed to cope with multi-modal multi-task learning, which can
simultaneously handle multiple tasks across different domains,
including object detection, natural language understanding and
vision-language reasoning. Specifically, UniT has two transformer
encoders to handle image and text inputs, respectively, and then
the transformer decoder takes the single or concatenated encoder
outputs according to the task modality. Finally, a task-specific
prediction head is applied to the decoder outputs for different
tasks. In the training stage, all tasks are jointly trained by randomly
selecting a specific task within an iteration. The experiments show
UniT achieves satisfactory performance on every task with a
compact set of model parameters.
In conclusion, current transformer-based mutil-modal models
demonstrates its architectural superiority for unifying data and
tasks of various modalities, which demonstrates the potential of
transformer to build a general-purpose intelligence agents able to
cope with vast amount of applications. Future researches can be
conducted in exploring the effective training or the extendability
of multi-modal transformers (e.g., GPT-4 ).
Efficient Transformer
Although transformer models have achieved success in various
tasks, their high requirements for memory and computing resources block their implementation on resource-limited devices
such as mobile phones. In this section, we review the researches
carried out into compressing and accelerating transformer models
for efficient implementation. This includes including network
pruning, low-rank decomposition, knowledge distillation, network
quantization, and compact architecture design. Table 4 lists some
representative works for compressing transformer-based models.
Pruning and Decomposition
In transformer based pre-trained models (e.g., BERT), multiple
attention operations are performed in parallel to independently
model the relationship between different tokens , . However, specific tasks do not require all heads to be used. For
example, Michel et al. presented empirical evidence that a
large percentage of attention heads can be removed at test time
without impacting performance significantly. The number of heads
required varies across different layers — some layers may even
require only one head. Considering the redundancy on attention
heads, importance scores are defined to estimate the influence of
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
each head on the final output in , and unimportant heads can be
removed for efficient deployment. Dalvi et al. analyzed the
redundancy in pre-trained transformer models from two perspectives: general redundancy and task-specific redundancy. Following
the lottery ticket hypothesis , Prasanna et al. analyzed
the lotteries in BERT and showed that good sub-networks also
exist in transformer-based models, reducing both the FFN layers
and attention heads in order to achieve high compression rates.
For the vision transformer which splits an image to multiple
patches, Tang et al. proposed to reduce patch calculation
to accelerate the inference, and the redundant patches can be
automatically discovered by considering their contributions to the
effective output features. Zhu et al. extended the network
slimming approach to vision transformers for reducing
the dimensions of linear projections in both FFN and attention
In addition to the width of transformer models, the depth
(i.e., the number of layers) can also be reduced to accelerate the
inference process , . Differing from the concept that
different attention heads in transformer models can be computed in
parallel, different layers have to be calculated sequentially because
the input of the next layer depends on the output of previous
layers. Fan et al. proposed a layer-wisely dropping strategy
to regularize the training of models, and then the whole layers are
removed together at the test phase.
Beyond the pruning methods that directly discard modules in
transformer models, matrix decomposition aims to approximate
the large matrices with multiple small matrices based on the lowrank assumption. For example, Wang et al. decomposed the
standard matrix multiplication in transformer models, improving
the inference efficiency.
Knowledge Distillation
Knowledge distillation aims to train student networks by transferring knowledge from large teacher networks , ,
 . Compared with teacher networks, student networks usually
have thinner and shallower architectures, which are easier to
be deployed on resource-limited resources. Both the output and
intermediate features of neural networks can also be used to
transfer effective information from teachers to students. Focused
on transformer models, Mukherjee et al. used the pre-trained
BERT as a teacher to guide the training of small models,
leveraging large amounts of unlabeled data. Wang et al. train
the student networks to mimic the output of self-attention layers
in the pre-trained teacher models. The dot-product between values
is introduced as a new form of knowledge for guiding students. A
teacher’s assistant is also introduced in , reducing the
gap between large pre-trained transformer models and compact
TABLE 4: List of representative compressed transformerbased models. The data of the Table is from .
Compress Type #Layer
BERTBASE 
ALBERT 
Decomposition
Architecture
of-Theseus 
Q-BERT 
Quantization
Q8BERT 
TinyBERT 
Distillation
DistilBERT 
BERT-PKD 
45.7∼67M ×3.73∼1.64
MobileBERT 
student networks, thereby facilitating the mimicking process. Due
to the various types of layers in the transformer model (i.e., selfattention layer, embedding layer, and prediction layers), Jiao et
al. design different objective functions to transfer knowledge
from teachers to students. For example, the outputs of student
models’ embedding layers imitate those of teachers via MSE
losses. For the vision transformer, Jia et al. proposed a finegrained manifold distillation method, which excavates effective
knowledge through the relationship between images and the divided patches.
Quantization
Quantization aims to reduce the number of bits needed to represent
network weight or intermediate features , . Quantization
methods for general neural networks have been discussed at length
and achieve performance on par with the original networks ,
 , . Recently, there has been growing interest in how
to specially quantize transformer models , . For example, Shridhar et al. suggested embedding the input into
binary high-dimensional vectors, and then using the binary input
representation to train the binary neural networks. Cheong et
al. represented the weights in the transformer models by
low-bit (e.g., 4-bit) representation. Zhao et al. empirically
investigated various quantization methods and showed that kmeans quantization has a huge development potential. Aimed
at machine translation tasks, Prato et al. proposed a fully
quantized transformer, which, as the paper claims, is the first 8bit model not to suffer any loss in translation quality. Beside,
Liu et al. explored a post-training quantization scheme to
reduce the memory storage and computational costs of vision
transformers.
Compact Architecture Design
Beyond compressing pre-defined transformer models into smaller
ones, some works attempt to design compact models directly , . Jiang et al. simplified the calculation
of self-attention by proposing a new module — called spanbased dynamic convolution — that combine the fully-connected
layers and the convolutional layers. Interesting “hamburger” layers
are proposed in , using matrix decomposition to substitute
the original self-attention layers.Compared with standard selfattention operations, matrix decomposition can be calculated more
efficiently while clearly reflecting the dependence between different tokens. The design of efficient transformer architectures can
also be automated with neural architecture search (NAS) ,
 , which automatically searches how to combine different
components. For example, Su et al. searched patch size and
dimensions of linear projections and head number of attention
modules to get an efficient vision transformer. Li et al. 
explored a self-supervised search strategy to get a hybrid architecture composing of both convolutional modules and self-attention
The self-attention operation in transformer models calculates
the dot product between representations from different input tokens in a given sequence (patches in image recognition tasks ),
whose complexity is O(N), where N is the length of the sequence. Recently, there has been a targeted focus to reduce the
complexity to O(N) in large methods so that transformer models
can scale to long sequences , , . For example,
Katharopoulos et al. approximated self-attention as a linear
dot-product of kernel feature maps and revealed the relationship
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
Pruning & Decomposition
Knowledge Distillation
Quantization
Architecture Design
Fig. 13: Different methods for compressing transformers.
between tokens via RNNs. Zaheer et al. considered each token as a vertex in a graph and defined the inner product calculation
between two tokens as an edge. Inspired by graph theories ,
 , various sparse graph are combined to approximate the dense
graph in transformer models, and can achieve O(N) complexity.
Discussion. The preceding methods take different approaches
in how they attempt to identify redundancy in transformer models (see Figure 13). Pruning and decomposition methods usually
require pre-defined models with redundancy. Specifically, pruning
focuses on reducing the number of components (e.g., layers,
heads) in transformer models while decomposition represents an
original matrix with multiple small matrices. Compact models
also can be directly designed either manually (requiring sufficient
expertise) or automatically (e.g., via NAS). The obtained compact
models can be further represented with low-bits via quantization
methods for efficient deployment on resource-limited devices.
CONCLUSIONS AND DISCUSSIONS
Transformer is becoming a hot topic in the field of computer
vision due to its competitive performance and tremendous potential compared with CNNs. To discover and utilize the power of
transformer, as summarized in this survey, a number of methods
have been proposed in recent years. These methods show excellent
performance on a wide range of visual tasks, including backbone,
high/mid-level vision, low-level vision, and video processing.
Nevertheless, the potential of transformer for computer vision has
not yet been fully explored, meaning that several challenges still
need to be resolved. In this section, we discuss these challenges
and provide insights on the future prospects.
Challenges
Although researchers have proposed many transformer-based
models to tackle computer vision tasks, these works are only the
first steps in this field and still have much room for improvement.
For example, the transformer architecture in ViT follows
the standard transformer for NLP , but an improved version
specifically designed for CV remains to be explored. Moreover, it
is necessary to apply transformer to more tasks other than those
mentioned earlier.
The generalization and robustness of transformers for computer vision are also challenging. Compared with CNNs, pure
transformers lack some inductive biases and rely heavily on
massive datasets for large-scale training . Consequently, the
quality of data has a significant influence on the generalization
and robustness of transformers. Although ViT shows exceptional
performance on downstream image classification tasks such as
CIFAR and VTAB , directly applying the ViT backbone on object detection has failed to achieve better results than
CNNs . There is still a long way to go in order to better
generalize pre-trained transformers on more generalized visual
tasks. Practitioners concern the robustness of transformer (e.g.
the vulnerability issue ). Although the robustness has been
investigated in , , , it is still an open problem
waiting to be solved.
Although numerous works have explained the use of transformers in NLP , , it remains a challenging subject
to clearly explain why transformer works well on visual tasks.
The inductive biases, including translation equivariance and locality, are attributed to CNN’s success, but transformer lacks any
inductive bias. The current literature usually analyzes the effect
in an intuitive way , . For example, Dosovitskiy et
al. claim that large-scale training can surpass inductive
bias. Position embeddings are added into image patches to retain
positional information, which is important in computer vision
tasks. Inspired by the heavy parameter usage in transformers,
over-parameterization , may be a potential point to the
interpretability of vision transformers.
Last but not least, developing efficient transformer models for
CV remains an open problem. Transformer models are usually
huge and computationally expensive. For example, the base ViT
model requires 18 billion FLOPs to process an image. In
contrast, the lightweight CNN model GhostNet , can
achieve similar performance with only about 600 million FLOPs.
Although several methods have been proposed to compress transformer, they remain highly complex. And these methods, which
were originally designed for NLP, may not be suitable for CV.
Consequently, efficient transformer models are urgently needed
so that vision transformer can be deployed on resource-limited
Future Prospects
In order to drive the development of vision transformers, we
provide several potential directions for future study.
One direction is the effectiveness and the efficiency of transformers in computer vision. The goal is to develop highly effective and efficient vision transformers; specifically, transformers
with high performance and low resource cost. The performance
determines whether the model can be applied on real-world
applications, while the resource cost influences the deployment
on devices , . The effectiveness is usually correlated
with the efficiency, so determining how to achieve a better balance
between them is a meaningful topic for future study.
Most of the existing vision transformer models are designed to
handle only a single task. Many NLP models such as GPT-3 
have demonstrated how transformer can deal with multiple tasks in
one model. IPT in the CV field is also able to process multiple
low-level vision tasks, such as super-resolution, image denoising,
and deraining. Perceiver and Perceiver IO are the
pioneering models that can work on several domains including
images, audio, multimodal, point clouds. We believe that more
tasks can be involved in only one model. Unifying all visual
tasks and even other tasks in one transformer (i.e., a grand unified
model) is an exciting topic.
There have been various types of neural networks, such as
CNN, RNN, and transformer. In the CV field, CNNs used to
be the mainstream choice , , but now transformer is
becoming popular. CNNs can capture inductive biases such as
translation equivariance and locality, whereas ViT uses large-scale
training to surpass inductive bias . From the evidence currently
available , CNNs perform well on small datasets, whereas
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
transformers perform better on large datasets. The question for the
future is whether to use CNN or transformer.
By training with large datasets, transformers can achieve stateof-the-art performance on both NLP , and CV benchmarks . It is possible that neural networks need big data rather
than inductive bias. In closing, we leave you with a question:
Can transformer obtains satisfactory results with a very simple
computational paradigm (e.g., with only fully connected layers)
and massive data training?
ACKNOWLEDGEMENT
This research is partially supported by MindSpore (https://
mindspore.cn/) and CANN (Compute Architecture for Neural
Networks).
A1. General Formulation of Self-attention
The self-attention module for machine translation computes the
responses at each position in a sequence by estimating attention
scores to all positions and gathering the corresponding embeddings based on the scores accordingly. This can be viewed as a
form of non-local filtering operations , . We follow the
convention to formulate the self-attention module. Given an
input signal (e.g., image, sequence, video and feature) X ∈Rn×d,
where n = h × w (indicating the number of pixels in feature) and
d is the number of channels, the output signal is generated as:
f(xi, xj)g(xj),
where xi ∈R1×d and yi ∈R1×d indicate the ith position
(e.g., space, time and spacetime) of the input signal X and output
signal Y, respectively. Subscript j is the index that enumerates all
positions, and a pairwise function f(·) computes a representing
relationship (such as affinity) between i and all j. The function
g(·) computes a representation of the input signal at position j,
and the response is normalized by a factor C(xi).
Note that there are many choices for the pairwise function
f(·). For example, a simple extension of the Gaussian function
could be used to compute the similarity in an embedding space.
As such, the function f(·) can be formulated as:
f(xi, xj) = eθ(xi)ϕ(xj)T
where θ(·) and ϕ(·) can be any embedding layers. If we
consider the θ(·), ϕ(·), g(·) in the form of linear embedding:
θ(X) = XWθ, ϕ(X) = XWϕ, g(X) = XWg where
Wθ ∈Rd×dk, Wϕ ∈Rd×dk, Wg ∈Rd×dv, and set the
normalization factor as C(xi) = P
∀j f(xi, xj), the Eq. 14 can
be rewritten as:
j exiwθ,iwT
where wθ,i ∈Rd×1 is the ith row of the weight matrix Wθ. For a
given index i,
C(xi)f(xi, xj) becomes the softmax output along
the dimension j. The formulation can be further rewritten as:
Y = softmax(XWθWT
where Y ∈Rn×c is the output signal of the same size as X.
Compared with the query, key and value representations Q =
XWq, K = XWk, V = XWv from the translation module,
once Wq = Wθ, Wk = Wϕ, Wv = Wg, Eq. 17 can be
formulated as:
Y = softmax(QKT )V = Attention(Q, K, V),
The self-attention module proposed for machine translation
is, to some extent, the same as the preceding non-local filtering
operations proposed for computer vision.
Generally, the final output signal of the self-attention module
for computer vision will be wrapped as:
Z = YWo + X
where Y is generated through Eq. 17. If Wo is initialized as zero,
this self-attention module can be inserted into any existing model
without breaking its initial behavior.
A2. Revisiting Transformers for NLP
Before transformer was developed, RNNs ( e.g., GRU 
and LSTM ) with added attention empowered most of
the state-of-the-art language models. However, RNNs require the
information flow to be processed sequentially from the previous
hidden states to the next one. This rules out the possibility of using
acceleration and parallelization during training, and consequently
hinders the potential of RNNs to process longer sequences or build
larger models. In 2017, Vaswani et al. proposed transformer,
a novel encoder-decoder architecture built solely on multi-head
self-attention mechanisms and feed-forward neural networks. Its
purpose was to solve seq-to-seq natural language tasks (e.g.,
machine translation) easily by acquiring global dependencies. The
subsequent success of transformer demonstrates that leveraging
attention mechanisms alone can achieve performance comparable
with attentive RNNs. Furthermore, the architecture of transformer
lends itself to massively parallel computing, which enables training on larger datasets. This has given rise to the surge of large
pre-trained models (PTMs) for natural language processing.
RoBERTa ) are a series of PTMs built on the multi-layer
transformer encoder architecture. Two tasks are conducted on
BookCorpus and English Wikipedia datasets at the pretraining stage of BERT: 1) Masked language modeling (MLM),
which involves first randomly masking out some tokens in the
input and then training the model to predict; 2) Next sentence prediction, which uses paired sentences as input and predicts whether
the second sentence is the original one in the document. After
pre-training, BERT can be fine-tuned by adding an output layer
on a wide range of downstream tasks. More specifically, when
performing sequence-level tasks (e.g., sentiment analysis), BERT
uses the representation of the first token for classification; for
token-level tasks (e.g., name entity recognition), all tokens are fed
into the softmax layer for classification. At the time of its release,
BERT achieved the state-of-the-art performance on 11 NLP tasks,
setting a milestone in pre-trained language models. Generative
Pre-trained Transformer models (e.g., GPT , GPT-2 )
are another type of PTMs based on the transformer decoder
architecture, which uses masked self-attention mechanisms. The
main difference between the GPT series and BERT is the way
in which pre-training is performed. Unlike BERT, GPT models
are unidirectional language models pre-trained using Left-to-Right
(LTR) language modeling. Furthermore, BERT learns the sentence
separator ([SEP]) and classifier token ([CLS]) embeddings during
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
pre-training, whereas these embeddings are involved in only the
fine-tuning stage of GPT. Due to its unidirectional pre-training
strategy, GPT achieves superior performance in many natural
language generation tasks. More recently, a massive transformerbased model called GPT-3, which has an astonishing 175 billion
parameters, was developed . By pre-training on 45 TB of
compressed plaintext data, GPT-3 can directly process different
types of downstream natural language tasks without fine-tuning.
As a result, it achieves strong performance on many NLP datasets,
including both natural language understanding and generation.
Since the introduction of transformer, many other models have
been proposed in addition to the transformer-based PTMs mentioned earlier. We list a few representative models in Table 5 for
interested readers, but this is not the focus of our study.
TABLE 5: List of representative language models built on
transformer. Transformer is the standard encoder-decoder architecture. Transformer Enc. and Dec. represent the encoder
and decoder, respectively. Decoder uses mask self-attention to
prevent attending to the future tokens. The data of the Table
is from .
Architecture
# of Params
Fine-tuning
Transformer Dec.
GPT-2 
Transformer Dec.
117M-1542M
GPT-3 
Transformer Dec.
Transformer Enc.
RoBERTa 
Transformer Enc.
XLNet 
Two-Stream
Transformer Enc.
ELECTRA 
Transformer Enc.
UniLM 
Transformer Enc.
BART 
Transformer
110% of BERT
Transfomer
ERNIE (THU) 
Transformer Enc.
KnowBERT 
Transformer Enc.
Apart from the PTMs trained on large corpora for general NLP
tasks, transformer-based models have also been applied in many
other NLP-related domains and to multi-modal tasks.
BioNLP Domain. Transformer-based models have outperformed
many traditional biomedical methods. Some examples of such
models include BioBERT , which uses a transformer architecture for biomedical text mining tasks, and SciBERT ,
which is developed by training transformer on 114M scientific
articles (covering biomedical and computer science fields) with
the aim of executing NLP tasks in the scientific domain more
precisely. Another example is ClinicalBERT, proposed by Huang
et al. . It utilizes transformer to develop and evaluate continuous representations of clinical notes. One of the side effects
of this is that the attention map of ClinicalBERT can be used
to explain predictions, thereby allowing high-quality connections
between different medical contents to be discovered.
The rapid development of transformer-based models on a
variety of NLP-related tasks demonstrates its structural superiority
and versatility, opening up the possibility that it will become a
universal module applied in many AI fields other than just NLP.
The following part of this survey focuses on the applications of
transformer in a wide range of computer vision tasks that have
emerged over the past two years.
A3. Self-attention for Computer Vision
The preceding sections reviewed methods that use a transformer
architecture for vision tasks. We can conclude that self-attention
plays a pivotal role in transformer. The self-attention module can
also be considered a building block of CNN architectures, which
have low scaling properties concerning the large receptive fields.
This building block is widely used on top of the networks to
capture long-range interactions and enhance high-level semantic
features for vision tasks. In this section, we delve deeply into
the models based on self-attention designed for challenging tasks
in computer vision. Such tasks include semantic segmentation,
instance segmentation, object detection, keypoint detection, and
depth estimation. Here we briefly summarize the existing applications using self-attention for computer vision.
Image Classification. Trainable attention for classification
consists of two main streams: hard attention , , regarding the use of an image region, and soft attention , ,
 , generating non-rigid feature maps. Ba et al. 
first proposed the term “visual attention” for image classification
tasks, and used attention to select relevant regions and locations
within the input image. This can also reduce the computational
complexity of the proposed model regarding the size of the input
image. For medical image classification, AG-CNN was
proposed to crop a sub-region from a global image by the attention
heat map. And instead of using hard attention and recalibrating the
crop of feature maps, SENet was proposed to reweight the
channel-wise responses of the convolutional features using soft
self-attention. Jetley et al. used attention maps generated
by corresponding estimators to reweight intermediate features in
DNNs. In addition, Han et al. utilized the attribute-aware
attention to enhance the representation of CNNs.
Segmentation. PSANet , OCNet ,
DANet and CFNet are the pioneering works to propose
using the self-attention module in semantic segmentation tasks.
These works consider and augment the relationship and similarity , , , , , between the contextual
pixels. DANet simultaneously leverages the self-attention
module on spatial and channel dimensions, whereas A2Net 
groups the pixels into a set of regions, and then augments the
pixel representations by aggregating the region representations
with the generated attention weights. DGCNet employs a
dual graph CNN to model coordinate space similarity and feature
space similarity in a single framework. To improve the efficiency
of the self-attention module for semantic segmentation, several
works , , , , have been proposed,
aiming to alleviate the huge amount of parameters brought by
calculating pixel similarities. For example, CGNL applies
the Taylor series of the RBF kernel function to approximate the
pixel similarities. CCNet approximates the original selfattention scheme via two consecutive criss-cross attention modules. In addition, ISSA factorizes the dense affinity matrix as
the product of two sparse affinity matrices. There are other related
works using attention based graph reasoning modules , ,
 to enhance both the local and global representations.
Object Detection. Ramachandran et al. proposes an
attention-based layer and swapped the conventional convolution
layers to build a fully attentional detector that outperforms the
typical RetinaNet on COCO benchmark . GCNet 
assumes that the global contexts modeled by non-local operations
are almost the same for different query positions within an image,
A SUBMISSION TO IEEE TRANSACTION ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
and unifies the simplified formulation and SENet into a
general framework for global context modeling , ,
 , . Vo et al. designs a bidirectional operation
to gather and distribute information from a query position to
all possible positions. Zhang et al. suggests that previous
methods fail to interact with cross-scale features, and proposes
Feature Pyramid Transformer, based on the self-attention module,
to fully exploit interactions across both space and scales.
Conventional detection methods usually exploit a single visual
representation (e.g., bounding box and corner point) for predicting
the final results. Hu et al. proposes a relation module based
on self-attention to process a set of objects simultaneously through
interaction between their appearance features. Cheng et al. 
proposes RelationNet++ with the bridging visual representations
(BVR) module to combine different heterogeneous representations
into a single one similar to that in the self-attention module.
Specifically, the master representation is treated as the query input
and the auxiliary representations are regarded as the key input.
The enhanced feature can therefore bridge the information from
auxiliary representations and benefit final detection results.
Other Vision Tasks. Zhang et al. proposes a resolutionwise attention module to learn enhanced feature maps when
training multi-resolution networks to obtain accurate human keypoint locations for pose estimation task. Furthermore, Chang et
al. uses an attention-mechanism based feature fusion block
to improve the accuracy of the human keypoint detection model.
To explore more generalized contextual information for improving the self-supervised monocular trained depth estimation,
Johnston et al. directly leverages self-attention module.
Chen et al. also proposes an attention-based aggregation network to capture context information that differs in diverse scenes
for depth estimation. And Aich et al. proposes bidirectional
attention modules that utilize the forward and backward attention
operations for better results of monocular depth estimation.