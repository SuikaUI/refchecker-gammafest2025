The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Point2Sequence: Learning the Shape Representation of 3D
Point Clouds with an Attention-Based Sequence to Sequence Network
Xinhai Liu,1 Zhizhong Han,1,2 Yu-Shen Liu,1∗Matthias Zwicker2
1School of Software, Tsinghua University, Beijing 100084, China
Beijing National Research Center for Information Science and Technology (BNRist)
2Department of Computer Science, University of Maryland, College Park, USA
 , , , 
Exploring contextual information in the local region is important for shape understanding and analysis. Existing studies
often employ hand-crafted or explicit ways to encode contextual information of local regions. However, it is hard to
capture ﬁne-grained contextual information in hand-crafted
or explicit manners, such as the correlation between different areas in a local region, which limits the discriminative ability of learned features. To resolve this issue, we
propose a novel deep learning model for 3D point clouds,
named Point2Sequence, to learn 3D shape features by capturing ﬁne-grained contextual information in a novel implicit way. Point2Sequence employs a novel sequence learning model for point clouds to capture the correlations by
aggregating multi-scale areas of each local region with attention. Speciﬁcally, Point2Sequence ﬁrst learns the feature
of each area scale in a local region. Then, it captures the
correlation between area scales in the process of aggregating all area scales using a recurrent neural network (RNN)
based encoder-decoder structure, where an attention mechanism is proposed to highlight the importance of different
area scales. Experimental results show that Point2Sequence
achieves state-of-the-art performance in shape classiﬁcation
and segmentation tasks.
Introduction
3D point clouds, also called point sets, are considered as
one of the simplest 3D shape representations, since they
are composed of only raw coordinates in 3D space. A point
cloud can be acquired expediently by popular sensors such
as LiDAR, conventional cameras, or RGB-D cameras. Furthermore, this kind of 3D data is widely used in 3D modeling , autonomous
driving , indoor navigation 
and robotics . However, learning
features or shape representations based on point clouds by
deep learning models remains a challenging problem due to
the irregular nature of point clouds .
As a pioneering approach, PointNet resolves this challenge by directly applying deep learning on
point sets. PointNet individually computes a feature of each
point and then aggregates all the point features into a global
∗Corresponding author: Yu-Shen Liu
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
feature by a pooling layer. This leads to PointNet being limited by capturing contextual information of local regions.
Attempting to address this issue, several researches take
the aggregation of local regions into consideration. KC-Net
 employs a kernel correlation layer and a
graph-based pooling layer to capture the local information of
point clouds. SO-Net and DGCNN
 further explore the local structures by
building k-nearest neighbors (kNN) graphs and integrating
neighbors of a given point using learnable edge attributes
in the graph. PointNet++ ﬁrst extracts features for multi-scale local regions individually and aggregates these features by concatenation, where the two steps
are repeated to complete the hierarchical feature extraction.
Similarly, ShapeContextNet segments the
local region of a given point into small bins, then extracts the
feature for each bin individually, and ﬁnally concatenates the
features of all bins as the updated feature for the point. However, most of these previous methods employ hand-crafted
or explicit ways for encoding contextual information in local regions, which makes it hard to fully capture ﬁne-grained
contextual information, such as the correlation between different areas in the feature space. However, the correlation
between different areas in a local region is an important contextual information. Fully exploiting this information might
enhance the discriminability of learned features and improve
the performance in shape analysis tasks.
We address this issue by proposing a novel deep learning model for 3D point clouds, called Point2Sequence, to
encode ﬁne-grained contextual information in local regions
in a novel implicit way. Point2Sequence employs a novel
RNN-based sequence model for local regions in point clouds
to capture the correlation by aggregating multi-scale areas with attention. Speciﬁcally, each local region is ﬁrst
separated into multi-scale areas. Then, the feature of each
area scale is extracted by a shared Multi-Layer-Perceptron
(MLP) layer. Finally, our novel encoder-decoder based sequence model aggregates the features of all area scales,
where an attention mechanism is involved to highlight the
importance of different scale areas. Experimental results
show that Point2Sequence is able to learn more discriminative features from point clouds than existing methods in
shape classiﬁcation and part segmentation tasks.
Our contributions are summarized as follows.
• We propose Point2Sequence to learn features from point
clouds by capturing correlations between different areas
in a local region, which takes full advantage of encoding
ﬁne-grained contextual information in local regions.
• We introduce an attention mechanism to highlight the importance of different scale areas, and the feature extraction of local regions is enhanced by leveraging the correlation between different area scales.
• To the best of our knowledge, Point2Sequence is the ﬁrst
RNN-based model for capturing correlations between different areas in local regions of point clouds, and our outperforming results verify the feasibility of RNNs to effectively understand point clouds.
Related Work
Learning from point clouds by rasterization.
As an irregular type of 3D data, it is intuitive to rasterize the point
clouds into uniform sparse 3D grids and then apply volumetric convolutional neural networks. Some approaches represent each voxel
with a binary representation which indicates whether it is occupied in the space. Nevertheless, the performance is largely
limited by the time and memory consuming due to the data
sparsity of 3D shapes. Several improvements have been proposed to relief the
sparsity problem of the volumetric representation. However,
the sparsity of 3D shapes is an inherent drawback, which
still makes it difﬁcult to process very large point clouds.
Some recent methods have tried to project the 3D point clouds or
3D shapes into 2D views and then apply 2D CNNs to recognize them. Inﬂuenced by the great success of 2D CNNs
for images, such methods have achieved dominant results in
3D shape classiﬁcation and retrieval tasks . Due to the lack of depth information, it is nontrivial to extend view-based methods to perpoint processing tasks such as point classiﬁcation and shape
classiﬁcation.
Compared with uniform 3D grids, some latest studies utilize more scalable indexing techniques such as kd-tree and
octree to generate regular structures which can facilitate the
use of deep learning functions. To enable 3D convolutional
networks, OctNet build
a hierarchically partition of the space by generating a set
of unbalanced octrees in the regular grids, where each leaf
node stores a pooled feature representation. Kd-Net performs multiplicative transformations according to the subdivisions of point clouds based on
the kd-trees. However, in order to obtain rotation invariant of
shapes, these methods usually require extra operations such
as pre-alignment or excessive data argumentations.
Different from the above-mentioned methods, our method
directly learns from point clouds without pre-alignment and
voxelization.
Learning from point clouds directly.
As a pioneer,
PointNet achieves satisfactory performance
by directly applying deep learning methods on point sets.
local region
Figure 1: The left is an airplane point cloud with two sampled centroids p′
2 in the red color. The right is the
corresponding local regions of p′
1 (below) and p′
2 (above),
where different colors represent different area scales within
the local region. For example, there are four different scale
1} in the local region R1 centered by
PointNet individually computes the feature of each point
and then aggregates all the point features into a global feature by pooling. This leads to PointNet limited by capturing contextual information in local regions. Some enhancements are proposed to address this problem by combining
contextual information in local regions by hand-crafted or
explicit ways. PointNet++ has been proposed to group points into several clusters in pyramid-like
layers, where the feature of multi-scale local regions can
be extracted hierarchically. PointCNN and
SpiderCNN investigate the convolution-like
operations which aggregate the neighbors of a given point
by edge attributes in the local region graph. However, with
hand-crafted or explicit ways of encoding contextual information in local regions, it is hard for these methods to capture ﬁne-grained contextual information. In particular, the
correlation between different areas in feature space is an important contextual information, which limits the discriminative ability of learned features for point cloud understanding.
Correlation learning with RNN-based models.
To aggregate sequential feature vectors, recurrent neural networks have shown preeminent performance
in many popular tasks such as speech recognition or handwriting recognition . Inspired by the sequence to sequence architecture
 , RNN-based seq2seq models can capture
the ﬁne-grained contextual information from the input sequence and effectively convert it to another sequence. Furthermore, Sutskever, Vinyals, and Le engages an attention mechanism that intuitively allows neural networks
to focus on different parts of the input sequences, which is
more in line with the actual situation. In addition, several
kinds of attention mechanism have been presented to enhance the performance of networks in machine
translation (MT). To utilize the powerful ability of correla-
Segmentation
interpolating
MLP (256,128)
interpolating
MLP (32,64,128)
MLP (256, 512, 1024)
Classification
input points
MLP (128,128,128)
global feature
context vector
["#, $ , "%, $ , "&]
(b) Area feature extraction
(c) Encoder-decoder feature aggregation
(d) Local region feature aggregation
(a) Multi-scale area establishment
(e) Shape classification
(f) Shape part segmentation
Figure 2: Our Point2Sequence architecture.
Point2Sequence ﬁrst samples local regions from an input point cloud and establishes multi-scale areas in each local region in (a). Then, MLP layer is employed to extract the feature of each multi-scale area
in (b). Subsequently, the feature of each local region is extracted by attention-based seq2seq structure in (c). Finally, the global
feature of the point cloud is obtained by aggregating the features of all sampled local regions in (d). The learned global feature
can be used not only for shape classiﬁcation shown in (e) but also for part segmentation with some extension network shown in
tion learning from RNN-based sequence to sequence structure, Point2Sequence adopts a seq2seq encoder-decoder architecture to learn the correlation of different areas in each
local region with attention.
The Point2Sequence Model
In this section, we ﬁrst overview our Point2Sequence model,
and then detail the model including multi-scale area establishment, multi-scale area feature extraction, attention-based
sequence to sequence structure and model adjustments for
segmentation.
Figure 2 illustrates our Point2Sequence architecture. Our
model is formed by six parts: (a) Multi-scale area establishment, (b) Area feature extraction, (c) Encoder-decoder feature aggregation, (d) Local region feature aggregation, (e)
Shape classiﬁcation and (f) Shape part segmentation. The
input of our network is a raw point set P = {pi ∈R3, i =
1, 2, · · · , N}. We ﬁrst select M points P′ = {p′
j ∈R3, j =
1, 2, · · · , M} from P to deﬁne the centroids of local regions {R1, · · · , Rj, · · · , RM}. As illustrated in Figure 1,
T different scale areas {A1
j, · · · , At
j, · · · , AT
j } in each local region Rj centered by p′
j are established in the multiscale area establishment part, where T different scale areas
contain [K1, · · · , Kt, · · · , KT ] points, respectively. We then
extract a D-dimensional feature st
j for each scale area At
through the scale feature extraction part. In each local region Rj, a feature sequence Sj = {s1
j, · · · , st
j, · · · , sT
multi-scale areas is aggregated into a D-dimensional feature
vector rj by the encoder-decoder feature aggregation part.
Finally, a 1024-dimensional global feature g is aggregated
from the features rj of all local regions by the local region
feature aggregation part. Our model can be trained for shape
classiﬁcation or shape part segmentation by minimizing the
error according to the ground truth class labels or per point
part labels.
Multi-scale Area Establishment
Similar to PointNet++ and ShapeContextNet , we build the structure of multiscale areas in each local region on the point cloud. This part
is formed by three layers: sampling layer, searching layer
and grouping layer. The sampling layer selects M points
from the input point cloud as the centroids of local regions.
In each local region, searching layer searches the Kt points
in the input points and return the corresponding point indexes. According to the point indexes, grouping layer groups
the Kt points from P to form each multi-scale area.
As shown in Figure 2, a certain amount of points are ﬁrst
selected as the centroids of local regions by the sampling
layer. We adopt the farthest point sampling (FPS) to iteratively select M (M < N) points. The new added point p′
is always the farthest point from points {p′
2, · · · , p′
in the 3D space. Although random sampling is an optional
choice, the FPS can achieve a more uniform coverage of the
entire point cloud in the case of the same centroids.
Then the searching layer respectively ﬁnds the top
[K1, · · · , Kt, · · · , KT ] nearest neighbors for each local region Rj from the input points and returns the corresponding
indexes of these points. In the searching layer, we adopt kNN
to ﬁnd the neighbors of centroids according to the sorted Euclidean distance in the 3D space. Another optional search
method is ball search which selects all
points within a radius around the centroid. Compared with
ball search, kNN search guarantees the size of local regions
and makes it insensitive to the sampling density of point
Finally, by the grouping layer, these indexes of points are
used to extract the points in each area of local region Rj,
where we obtain the points with the size of M × Kt × 3 in
the scale area At
j of all local regions.
Multi-scale Area Feature Extraction
To extract the feature for each multi-scale area, a simple and
effective MLP layer is employed in our network. The MLP
layer ﬁrst abstracts the points in each scale area At
j into the
feature space and then the point features are aggregated into
a D-dimensional feature st
j by max pooling. Therefore, in
each local region Rj, a T × D feature sequence Sj of the
multi-scale areas is acquired.
In addition, similar to prior studies such as SO-Net , the coordinates of points in each area
j are converted to the relative coordinate system of the
centroid p′
j by a subtraction operation: pl = pl −p′
l is the index of point in the area. By using the relative coordinate, the learned features of point sets can be invariant
to transformations such as rotation and translation, which is
crucial for improving the learning ability of networks. Moreover, we also combine the area feature st
j with the coordinates of area centroid p′
j to enhance the association between
Attention-based Sequence to Sequence Structure
In this subsection, we propose an attention-based encoderdecoder network to capture the ﬁne-grained contextual information of local regions. Through the multi-scale area feature extraction, each local region Rj is abstracted into a feature sequence rj. To learn from the feature sequence, we
adopt a RNN-based model to aggregate the features Sj of
size T × D into a D dimension feature vector sj in each
local region by an implicit way. To further promote the correlation learning between different areas in local regions, we
employ an attention mechanism to focus on items of the feature sequence.
Multi-scale area encoder.
To learn the correlation between different areas, a RNN is employed as encoder to integrate the features of multi-scale areas in a local region.
The RNN encoder is consisted by a hidden state h and an
optional output y, which operates on the input feature sequence Sj = {s1
j, . . . , st
j, . . . , sT
j } of multi-scale areas in
each local region. Each item st
j of the sequence Sj is a Ddimensional feature vector and the length of Sj is T which is
also the steps of the encoder. At each time step t, the hidden
state ht of the RNN encoder is updated by
ht = f(ht−1, st
where f is a non-linear activation function which can be
a long short-term memory (LSTM) unit or a gated recurrent unit (Cho et al.
A RNN can learn the probability distribution over a sequence by being trained to predict the next item in the sequence. Similarly, at time t, the output yt of the encoder can
be represented as
yt = Waht,
where Wa is a learnable weight matrix.
After forwarding the entire input feature sequence at step
T, the last-step hidden state hT of the encoder is acquired,
which contains the context information of the entire input
Local region feature decoder.
To obtain the feature rj for
each local region Rj, we employ a RNN decoder to translate
the contextual information from the multi-scale areas in Rj.
Different from the models in machine translation, there is no
decoding target for the decoder in our case. To address this
issue, we employ hT as the decoding target which contains
contextual information of the entire input feature sequence.
Therefore, a one-step decoding process is adopt in our network to decode the feature rj for each local region Rj. Similar to the encoder, the decoder is also consisted by a hidden
state ¯h and output ¯y. We initialize the ¯h0 with a zero state
z0 and the current hidden state of the decoder at step one can
be updated by
¯h1 = f(z0, hT ),
where f is an activation function as shown in Eq. (1). Similarly, the output of decoder ¯y1 is computed by
¯y1 = Wb¯h1,
where Wb is a learnable matrix in the training.
To further enhance the decoding of the contextual information in the input feature sequence Sj, a context vector c
is generated to help the predict of feature ˜y1 of local region
with attention. Therefore, a new hidden state ˜h1 and output
˜y1 are computed in the decoder as introduced later, where
we employ the output ˜y1 to be the feature rj for each local
thought of focusing on parts of the source sentence in the
machine translation, we adopt an attention mechanism to
highlight the importance of different areas in each local region. The goal is to utilize the context vector c which is generated by
t=1α(t)ht,
where α is the attention vector and t is the time step.
The idea of our attention model is to consider all the hidden states of the encoder when generating the context vector
c. In this model, a ﬁxed-length attention vector α, whose
size is equal to the sequence length T of the input side, is
Table 1: The shape classiﬁcation accuracy (%) comparison on ModelNet10 and ModelNet40.
ModelNet10
ModelNet40
PointNet 
PointNet++ 
ShapeContextNet 
Kd-Net 
KC-Net 
PointCNN 
DGCNN 
SO-Net 
Table 2: The accuracies (%) of part segmentation on ShapeNet part segmentation dataset.
Intersection over Union (IoU)
PointNet++
ShapeContextNet
generated by comparing the current hidden state ¯h1 with
each source hidden state ht as
exp(score(¯h1, ht))
t′=1exp(score(¯h1, ht′))
Here, score is referred as
score(¯h1, ht) = ¯h⊤
which is a content-based function to show the correlation between these two vectors. Here, Wc is also a learnable weight
matrix in the training.
With the help of Eq. (5), we can acquire the new hidden
state ˜h1 based on the current hidden state ¯h1 in the decoder.
Speciﬁcally, with the current hidden state ¯h1 and the context
vector c, a simple concatenation layer combines the contextual information of the two vectors to generate the attentional new hidden state as follows,
˜h1 = tanh(Wd[c; ¯h1]).
And the output ˜y1 is similarly computed by
˜y1 = Ws˜h1,
where Wd and Ws are variables to be learned.
Our attention-based sequence to sequence structure aggregates the sequential features Sj of multi-scale areas in
each local region Rj by an implicit way. So far, the features
rj of all local regions with size of M × D are acquired. In
the subsequent network, a 1024-dimensional global feature
g of the input point cloud is extracted from the features of
all local regions by the global feature extraction part. As depicted in Figure 2, we apply the global feature g to shape
classiﬁcation and part segmentation tasks.
Model Adjustments for Segmentation
The goal of part segmentation is to predict a semantic label for each point in the point cloud. In Point2Sequence, a
global feature of the input point cloud is generated. Therefore, we need to acquire the per-point feature for each point
in the point cloud from the global feature. There are two
optional implementations, one is to duplicate the global feature with N times , and
the other is to perform upsampling by interpolation . In this paper, two interpolate layers are equipped in our networks as shown in Figure
2, which propagate the feature from shape level to point level
by upsampling. Compared with shape classiﬁcation, it is a
challenge task to distinguish the parts in the object, which
requires more ﬁne-grained contextual information of local
regions. We implement the feature φ propagation according
to the Euclidean distance between points in 3D space. The
feature is interpolated by inverse square Euclidean distance
wighted average based on k nearest neighbors as
i=1 w(pi)φ(pi)
where w(pi) =
(p−pi)2 is the inverse square Euclidean distance between p and pi.
To guide the interpolation process, the interpolated features are concatenated with the corresponding point features
in the abstraction side and several MLP layers are equipped
in our network to enhance the performance. Several shared
fully connected layers and ReLU layers are also applied to
promote the extraction of point features, like the branch in
shape classiﬁcation.
Table 3: The effect of the number of sampled points M on
ModelNet40.
Experiments
In this section, we ﬁrst investigate how some key parameters affect the performance of Point2Sequence in the shape
classiﬁcation task. Then, we compare our Point2Sequence
with several state-of-the-art methods in shape classiﬁcation
on ModelNet10 and ModelNet40 , respectively. Finally, the performance of our Point2Sequence is
evaluated in the part segmentation task on ShapeNet part
dataset .
Ablation Study
Network conﬁguration.
In Point2Sequence, we ﬁrst sample M = 384 points as the centroids of local regions by the
sampling layer. Then the searching layer and grouping layer
select T = 4 scale of areas with points in
each area of a local region. The points in each area are abstracted into a D = 128 dimensional feature by a 3-layer
MLP and then these abstracted features are aggregated by
max pooling. And the feature sequence of different areas in
each local region is aggregated by the RNN-based encoderdecoder structure with attention. Here, we initialize the RNN
encoder and decoder with h=128 dimensional hidden state,
where LSTM is used as the RNN cell. The rest setting of our
network is the same as in Figure 2. In addition, ReLU is used
after each fully connected layer with Batch-normalization,
and Dropout is also applied with drop ratio 0.4 in the fully
connected layers. In the experiment, we trained our network
on a NVIDIA GTX 1080Ti GPU using ADAM optimizer
with initial learning rate 0.001, batch size of 16 and batch
normalization rate 0.5. The learning rate and batch normalization rate are decreased by 0.3 and 0.5 for every 20 epochs,
respectively.
Parameters.
All the experiments in the ablation study
are evaluated on ModelNet40. ModelNet40 contains 12311
CAD models from 40 categories and is split into 9843 for
training and 2468 for testing. For each model, we adopt 1024
points which are uniformly sampled from mesh faces and are
normalized into a unit ball as input.
We ﬁrst explore the number of sampled points M which
inﬂuences the distribution of local regions in point clouds.
In the experiment, we keep the settings of our network as
depicted in the network conﬁguration section and modiﬁes
the number of sampled points M from 128 to 512. The results are shown in Table3, where the instance accuracies on
the benchmark of ModelNet40 have a tendency to rise ﬁrst
and then fall. The highest accuracy of 92.54% is reached
at M = 384 sampled points. This comparison implies that
Point2Sequence can extract the contextual information from
local regions effectively and M = 384 is a optimum number
of sampled points to coverage the whole point cloud.
Table 4: The effects of the type of RNN cell (CT) and hidden
state dimension h on ModelNet40.
Therefore, we employ the sampled points M = 384 as the
setting of our network in the following experiments. Then,
as shown in Table 4, we show the effect of the type of the
RNN cell (RT) and the dimension of the RNN hidden state
h, respectively. The accuracy degenerates to 92.18% when
replacing the LSTM cell as the GRU cell. Based on the setting of h = 128, we set h to 64 and 256, which reduce the
accuracy of h = 128 to 92.46% and 92.18%. The above results suggest that dimension of hidden state h = 128 and
the type of hidden state RT=LSTM is more suitable for our
Table 5: The effects of the attention mechanism (Att) and
decoder (Dec) on ModelNet40.
We also discuss the impact of the attention mechanism,
the decoder and the encoder-decoder structure on our network. We evaluate the performance of our network without
the attention mechanism (No Att), without decoder (No Dec)
and without encoder-decoder structure (No ED). In No ED,
we remove the encoder-decoder from our network and aggregate the features of multi-scale areas in each local region
by concatenating (Con) or max pooling (MP). In Table 5,
the result of the attention mechanism with encoder-decoder
(Att+ED) is better than the results of removing parts of the
attention mechanism and encoder-decoder. This comparison
shows that the decoder works better with the attention mechanism, and the decoder without attention will decrease the
capability of the network. And our RNN-based sequence to
sequence structure outperforms hand-crafted manners such
as concatenate (Con) and max pooling (MP) in aggregating
feature of multi-scale areas.
Table 6: The effect of the number of scales T on Model-
Moreover, we reduce the scale of local regions T from 4
to 1 and remains the largest scale with 128 points. As depicted in Table 6, we obtain a even better result of 92.62%
with T = 2 (K1 = 64, K2 = 128). The results with T > 1
are better than the result with T = 1, which shows that the
strategy of multi-scale areas can be better in capturing contextual information from local regions. Therefore, the number of areas T affects the performance of Point2Sequence in
extracting the information from local regions.
Finally, based on the setting of multi-scale areas T = 2,
Figure 3: Visualization of part segmentation results. In each shape pair, ﬁrst column is the ground truth (GT), and second
column is our predicted result, where parts with the same color have a consistent meaning. From left to right: bag, airplane, car,
Table 7: The effect of learning rate on ModelNet40.
we explore the effect of learning rate (LR) by setting it to
0.0005 and 0.002. As shown in Table 7, the highest accuracy is reached at LR = 0.001. Therefore, LR = 0.001 is the
optimal choice of Point2Sequence.
Shape Classiﬁcation
subsection,
performance
Point2Sequence on ModelNet10 and ModelNet40 benchmarks, where ModelNet10 contains 4899 CAD models
which is split into 3991 for training and 908 for testing.
Table 1 compares Point2Sequence with the state-of-the-art
methods in the shape classiﬁcation task on ModelNet10 and
ModelNet40. We compare our method with the results of
eight recently ranked methods on each benchmark in terms
of class average accuracy and instance average accuracy. For
fair comparison, all the results in Table 1 are obtained under the same condition, which handles with raw point sets
without the normal vector. By optimizing the cross entropy
loss function in the training process, on both benchmarks,
Point2Sequence outperforms other methods in class average
accuracies and instance average accuracies. In ModelNet40
shape classiﬁcation, our method achieves the instance accuracy of 92.6% which is 1.9% and 0.2% higher than Point-
Net++ and DGCNN , respectively. Experimental results show that Point2Sequence
outperforms other methods by extracting the contextual information of local regions.
Part Segmentation
To further validate that our approach is qualiﬁed for point
cloud analysis, we evaluate Point2Sequence on the semantic
part segmentation task. The goal of part segmentation is to
predict semantic part label for each point of the input point
cloud. As dipected in Figure 2, we build the part segmentation branch to implement the per-point classiﬁcation.
In part segmentation, ShapeNet part dataset is used as our
benchmark for the part segmentation task, the dataset contains 16881 models from 16 categories and is spit into train
set, validation set and test set following PointNet++. There
are 2048 points sampled from each 3D shape, where each
point in a point cloud object belongs to a certain one of 50
part classes and each point cloud contains 2 to 5 parts.
We employ the mean Intersection over Union (IoU) proposed in as the evaluation metric. For each
shape, the IoU is computed between groundtruth and the
prediction for each part type in the shape category. To calculate the mIoU for each shape category, we compute the
average of all shape mIoUs in the shape category. Overall mIoU is also calculated as the average mIoUs over all
test shapes. Similar to the shape classiﬁcation task, we optimized the cross entropy loss in the training process. We
compare our results with PointNet , Point-
Net++ , Kd-Net , SO-Net , KC-Net , ShapeContextNet and DGCNN
 . In Table 2, we report the performance
of Point2Sequence in each category and the mean IoU of
all testing shapes. Compared with the stated-of-the-art methods, Point2Sequence acquires the best mean instance IoU of
85.2% and achieves the comparable performances on many
categories. Figure 3 visualizes some examples of our predicted results, where our results are highly consistent with
the ground truth.
Conclusions
In this paper, we propose a novel representation learning
framework for point cloud processing in the shape classi-
ﬁcation and part segmentation tasks. An attention-based sequence to sequence model is proposed to utilize a sequence
of multi-scale areas, which focuses on learning the correlation of different areas in a local region. To enhance the performance, an attention mechanism is adopted to highlight
the importance of multi-scale areas in the local region. Experimental results show that our method achieves competitive performances with the state-of-the-art methods.
Acknowledgments
Yu-Shen Liu is the corresponding author. This work
was supported by National Key R&D Program of China
(2018YFB0505400), the National Natural Science Foundation of China (61472202), and Swiss National Science Foundation grant (169151). We thank all anonymous reviewers
for their constructive comments.