REVIEW ARTICLE
Neuroevolution: from architectures to learning
Dario Floreano Æ Peter Du¨rr Æ Claudio Mattiussi
Received: 5 October 2007 / Accepted: 17 October 2007 / Published online: 10 January 2008
 Springer-Verlag 2008
Artiﬁcial neural networks (ANNs) are applied
to many real-world problems, ranging from pattern classiﬁcation to robot control. In order to design a neural
network for a particular task, the choice of an architecture
(including the choice of a neuron model), and the choice of
a learning algorithm have to be addressed. Evolutionary
search methods can provide an automatic solution to these
problems. New insights in both neuroscience and evolutionary biology have led to the development of increasingly
powerful neuroevolution techniques over the last decade.
This paper gives an overview of the most prominent
methods for evolving ANNs with a special focus on recent
advances in the synthesis of learning architectures.
Neural networks  Evolution  Learning
1 Introduction
Over the last 50 years, researchers from a variety of ﬁelds
have used models of biological neural networks not only to
better understand the workings of biological nervous systems, but also to devise powerful tools for engineering
applications.
Artiﬁcial neural networks (ANNs) are computational
models implemented in software or specialized hardware
devices that attempt to capture the behavioral and adaptive
features of biological nervous systems. They are typically
composed of several interconnected processing units, or
‘neurons’ (see Fig. 1) which can have a number of inputs
and outputs. In mathematical terms, an ANN can be seen as
a directed graph where each node implements a neuron
model. In the simplest case, the neuron model is just a
weighted sum of the incoming signals transformed by a
(typically nonlinear) static transfer function. More sophisticated neuron models involve discrete-time or continuoustime dynamics (see Sect.
3). The connection strengths
associated with the edges of the graph connecting two
neurons are referred to as synaptic weights, the neurons
with connections to the external environment are often
called input or output neurons, respectively. The number
and type of neurons and the set of possible interconnections
between them deﬁne the architecture or topology of the
neural network.
In order to solve computational or engineering problems
with neural networks, learning algorithms are used to ﬁnd
parameters.
Evolutionary
algorithms
provide an interesting alternative, or complement, to the
commonly used learning algorithms, such as back-propagation . Evolutionary algorithms are a class of
population-based, stochastic search methods inspired by
the principles of Darwinian evolution. Instead of using a
conventional learning algorithm, the characteristics of
neural networks can be encoded in artiﬁcial genomes and
evolved according to a performance criterion. The advantages of using an evolutionary algorithm instead of another
learning method are that several deﬁning features of the
neural network can be genetically encoded and co-evolved
at the same time and that the deﬁnition of a performance
D. Floreano  P. Du¨rr (&)  C. Mattiussi
Ecole Polytechnique Fe´de´rale de Lausanne Laboratory
of Intelligent Systems, Station 11, 1015 Lausanne, Switzerland
e-mail: 
URL: 
D. Floreano
e-mail: 
C. Mattiussi
e-mail: 
Evol. Intel. 1:47–62
DOI 10.1007/s12065-007-0002-4
criterion is more ﬂexible than the deﬁnition of an energy or
error function. Furthermore, evolution can be coupled with
a learning algorithm such as Hebbian learning , or even
used to generate new learning algorithms. In the following
we give an overview of the recent methods for evolving
ANNs. Since to our knowledge there is no systematic
large-scale comparison of the different approaches we
focus on the most-widely used canonical methods and
some of the most promising recent developments. We ﬁrst
review approaches that evolve the network architecture and
its parameters. Then we give a short overview of the most
prominent dynamic neuron models used in neuroevolution
experiments. We then move on to describe different
methods for combining evolution and learning, including
the evolutionary synthesis of novel learning algorithms.
Finally we point to recent work aiming at evolving architectures capable of controlling learning events.
2 Evolution of neural architectures
The evolutionary synthesis of a neural network leads to
several design choices. Besides the choice of an appropriate ﬁtness function and the setting of appropriate
evolutionary parameters, e.g., population size or mutation
rates, the key problem is the choice of suitable genetic
representations that recombination and mutation operators
can manipulate to discover high-ﬁtness networks with high
probability. We can classify current representations into
three classes: direct, developmental and implicit. Although
all three representations can be used to evolve both the
network topology and its parameters, developmental and
implicit representations offer more ﬂexibility for evolving
neural topologies whereas direct representations have been
used mainly for evolving the parameter values of ﬁxed-size
2.1 Direct Representations
In a direct representation, there is a one-to-one mapping
between the parameter values of the neural network and the
genes that compose the genetic string. In the simplest case,
synaptic weights and other parameters (e.g., bias weights or
time-constants) of a ﬁxed network topology are directly
encoded in the genotype either as a string of real values or
as a string of characters, which are then interpreted as realvalues with a given precision. This can be done by interpreting the string of characters as a binary or Gray-coded
number. However, this strategy requires the knowledge of
the suitable intervals and precision of the encoded parameters. In order to make the mapping more adaptive,
Schraudolph and Belew suggested a dynamic encoding, where the bits allocated for each weight are used to
encode the most signiﬁcant part of the binary representation until the population has converged to a satisfactory
solution. At that point, those same bits are used to encode
the less signiﬁcant part of the binary representation in order
to narrow the search and reﬁne the performance of the
evolutionary network. Another possibility is to use a selfadaptive encoding such as the Center of Mass Encoding
suggested by Mattiussi et al. , where the characters of
the string are interpreted as a system of particles whose
center of mass determines the encoded value. The advantage of this method is that it automatically adapts the
granularity of the representation to the requirements of the
Recent benchmark experiments with Evolution Strategies , which use a ﬂoating-point representation of the
synaptic weights, have reported excellent performance with
direct encoding of a small, ﬁxed architecture . In these
experiments,
covariance matrix adaptation (CMA-ES). CMA-ES is an
evolutionary algorithm that generates new candidate solutions by sampling from a multivariate normal distribution
over the search space and changes this mutation distribution by adapting its covariance matrix .
In a classic study, Montana and Davis compared the
performance of synaptic weight evolution with a discrete
direct representation with that of the back-propagation
Fig. 1 A generic neural network architecture. It consists of Input
units and Output units which are connected to the external environment and Hidden units which connect to other neurons but are not
directly connected to the environment
Evol. Intel. 1:47–62
algorithm on a problem of sonar data classiﬁcation. The
results indicate that evolution ﬁnds much better networks
and in signiﬁcantly less computational cycles than backpropagation of error (the evaluation of one evolutionary
individual on the data set is equivalent to one set of training
cycles on the data set). These results have been conﬁrmed
for a different classiﬁcation task by other authors .
Evolved neural networks with direct encoding have also
been applied to a variety of other problems such as game
playing , data classiﬁcation or the control of robot
swarms .
It has been argued that evolving neural networks may not be trivial because the population may
individuals
conventions
Fig. 2). This refers to the situation where very different
genotypes (conventions) correspond to neural networks
with similar behavior. For example, two networks with
inverted hidden nodes may have very different genotypes,
but will produce exactly the same behavior. Since the two
genotypes correspond to quite different areas of the genetic
space, crossover among competing conventions may produce offspring with duplicated structures and low ﬁtness.
Another problem that evolutionary algorithms for ANNs
face is premature convergence. Difﬁcult ﬁtness landscapes
with local optima can cause a rapid decrease of the population diversity and thus render the search inefﬁcient.
To overcome these problems, Moriarty and Miikkulainen suggested to evolve individual neurons to
cooperate in networks. This has the advantage that population diversity is automatically maintained and competing
conventions can be avoided. The authors developed an
symbiotic,
neuro-evolution
(SANE). They encoded the neurons in binary chromosomes which contained a series of connection deﬁnitions
(see Fig. 3). The neurons connected only to the input and
the output layer. During the evaluation stage, random
neurons were selected to form a neural network of ﬁxed
size. The ﬁtness of each neuron was deﬁned as the average
ﬁtness of all the networks it had participated in. Gomez and
Miikkulainen extended this approach by segregating
the neurons in subpopulations with a method they called
enforced subpopulations (ESP). SANE could not evolve
recurrent neural networks as the neurons were selected
randomly from the population and thus could not rely on
being combined with similar neurons in different trials. The
subpopulations of ESP resolved this problem and additionally allowed for a higher amount of specialization
through constrained mating. ESP has been applied to various benchmark tasks, for example several different pole
balancing setups, however, more recent methods such as
Neuro-Evolution of Augmenting Topologies (see below)
and Analog Genetic Encoding (see Sect.
2.3) outperformed ESP on these benchmarks .
The topology of a neural network can signiﬁcantly affect
its ability to solve a problem. As mentioned above, direct
encoding is typically applied to ﬁxed network topologies,
however, it can also be used to evolve the architecture of an
ANN. One of the main challenges of evolving the topology
along with the weights and parameters of a neural network
is that changes in the topology usually lead to decreased
genotype space
CONVENTION 1
CONVENTION 2
Fig. 2 Competing conventions. Two different genotypes (a) may
encode networks that are behaviorally equivalent, but have inverted
hidden units (b). The two genotypes deﬁne two separate hills on the
ﬁtness landscape (c) and thus may make evolutionary search more
difﬁcult. Adapted from Schaffer et al. 
Fig. 3 The symbiotic, adaptive neuroevolution (SANE) encoding.
Each individual of the population encodes a neuron with a series of
connection deﬁnitions. The value of the label deﬁnes to which input
or output the neuron connects. The weight ﬁeld encodes the respective
connection weight. The ﬁnal neural network is formed by combining
randomly chosen neurons from the population. Redrawn from
Moriarty and Miikkulainen 
Evol. Intel. 1:47–62
ﬁtness even if they have the potential to further increase the
ﬁtness later in evolution .
Neuro-evolution of augmenting topologies (NEAT) is a
method for genetically encoding and evolving the architecture and the weights of neural networks . The
approach makes use of genetic operators that can introduce
new genes and disable old ones. NEAT was designed to
avoid the problem of competing conventions, allowing
meaningful crossover between individuals with different
genetic length, produce networks of increasing complexity
starting from simple ones, and protect topological innovations that may initially display lower ﬁtness but later
develop into powerful solutions.
The main insight of NEAT is that genes sharing the
same evolutionary origin are more likely to encode a
similar function. In order to keep a genetic historical
record, whenever a new gene is created, it is assigned a
marker that corresponds to its chronological order of
appearance in the evolving population (see Fig. 4). When
genes are reproduced and transmitted to offspring, they
retain their original markers. The marker number is used
to ﬁnd homologous genes that correspond to alignment
points between genotypes of different length, to prevent
crossover on competing conventions, and to detect the
genetic similarity of individuals in order to create subpopulations of similar individuals. Selective reproduction
operates on individuals within the same sub-population
and the ﬁtness of an individual is divided by a number
proportional to the number of individuals that are genetically similar.
This last feature is useful for preventing the competition
for reproduction between old individuals, which have a
relatively high ﬁtness, and newer individuals with topological innovations (genes with high innovation numbers),
which may display relatively low ﬁtness. Since the two
types of individuals will be genetically different, they will
compete separately for reproduction. NEAT starts with an
initial population where genotypes correspond to neural
networks of minimal size. The genetic operators can
modify the genotypes by inserting new genes that correspond to larger networks. If those larger networks provide a
competitive advantage, they are retained and compete with
networks of different size.
Neuro-evolution of augmenting topologies (NEAT) has
been applied to many problems such as pole balancing
 , robot control , computer games or an
automobile crash warning system .
Direct representations have been used with excellent
results for networks of relatively small size. However, they
may not work well for larger networks because the length
of the genetic string and its evolvability do not always scale
well with network size.
2.2 Developmental representations
In order to evolve large networks, some researchers suggested
genetically
speciﬁcation
developmental process which in turn constructs the neural
network. In a seminal paper, suggested a developmental encoding based on a set of rewriting rules encoded
in the genotype (see Fig. 5). He employed a genome
divided into blocks of ﬁve elements. Each block of ﬁve is
interpreted as a rewriting rule that determines how the ﬁrst
symbol is developed into a matrix containing the other four
symbols of the block. There are two types of symbols:
terminals and non-terminals. A terminal symbol develops
into a predetermined 2 9 2 matrix of 0 s and 1 s. A nonterminal symbol develops into a 2 9 2 matrix of symbols.
The ﬁrst block of the genotype builds the initial 2 9 2
matrix of symbols, each of which recursively develops
using the rules encoded in the genotype until a matrix of 0s
and 1s is built. This matrix represents the architecture and
connection pattern of the network. Kitano’s experiments
indicated that such a developmental encoding led to better
results than a direct encoding when used to evolve the
connectivity pattern of a neural network that was subsequently trained with back-propagation. However, Siddiqi
and Lucas showed that the inferior performance of the
direct representation in Kitano’s work did not result from
the difference in the network representations but from
different initial populations.
Among others, (see Yao for more examples), Gruau
 proposed a genetic encoding scheme for neural networks based on a cellular duplication and differentiation
process, i.e., a cellular encoding (CE). The genotype-tophenotype mapping starts with a single cell that undergoes
a number of duplication and transformation processes
resulting in a complete neural network. In this scheme the
Fig. 4 Genetic encoding of a network topology within NEAT.
Genetic operators can insert new genes or disable old genes. When a
new gene is inserted, it receives an innovation number that marks its
inception. Redrawn from Stanley and Miikkulainen 
Evol. Intel. 1:47–62
genotype is a collection of rules governing the process of
cell divisions (a single cell is replaced by two ‘‘daughter’’
cells) and transformations (new connections can be added
and the weights of the connections departing from a cell
can be modiﬁed).
In Gruau’s model, connection links are established
during the cellular duplication process. For this reason,
there are different forms of duplication, each determining
the way in which the connections of the mother cells are
inherited by the daughter cells. Additional operations
include the possibility to add or remove connections and to
modify the connection weights. The instructions contained
in the genotype are represented with a binary tree structure
and evolved using genetic programming . During the
genotype-to-phenotype mapping process, the genotype tree
is scanned starting from the top node of the tree and then
following each ramiﬁcation. Inspired by Koza’s work on
automatic discovery of reusable programs , Gruau also
considered the case of genotypes formed by many trees
where the terminal nodes of a tree may point to other trees.
This mechanism allows the genotype-to-phenotype process
to produce repeated phenotypical structures (e.g., repeated
sub-networks)
information. Trees that are pointed to more than once will
be executed more often. This encoding method has two
advantages: (a) compact genotypes can produce complex
phenotypical networks, and (b) evolution may exploit
phenotypes where repeated substructures are encoded in a
single part of the genotype. Since the identiﬁcation of
substructures that are read more than once is an emergent
result of the evolutionary process, Gruau called this cellular
neural subnetworks
(ADNS). Cellular encodings have been applied to different
control problems such as pole balancing or the control
of a hexapod robot .
In the cellular encoding, development occurs instantaneously before evaluating the fully formed phenotype.
Nolﬁet al. used a growing encoding scheme to evolve
the architecture and the connection weights of neural networks controlling a Khepera robot, where the growth of the
neural network occurred during the lifetime of the robot
while its ﬁtness was evaluated. These controllers were
composed of a collection of artiﬁcial neurons distributed
over a two-dimensional space with growing and branching
‘axons’ (see Fig. 6). The genotype speciﬁed instructions
that controlled the axonal growth and branching process of
Fig. 5 Kitano’s grammar
encoding. The genome is
divided into blocks of ﬁve
elements. Each block deﬁnes a
rewriting rule. Together with a
set of ﬁxed rules, this allows the
recursive development of a
matrix which deﬁnes the
connectivity of a neural
network. Redrawn from Kitano
Fig. 6 Growth of axonal connections in the 2D brain space, pruning of axons that do not connect to other neurons, and pruning of all neurons
that are not connected to input, output, or other parts of the network. From Nolﬁet al. 
Evol. Intel. 1:47–62
a set of neurons. When the axon growing from one neuron
reached another neuron, a connection between the two
neurons was established. Axons grew and branched only if
the neurons displayed an activation variability above a
genetically speciﬁed threshold. Axons that did not connect
to other neurons and neurons that remained unconnected
were pruned.
This activity-dependent growth was based on the idea
that the sensory information coming from the environment
plays a critical role in the maturation of the connectivity of
the biological nervous system. Indeed, it has been shown
that the maturation process is affected by the activity patterns of single neurons . The developmental process
of these individuals was therefore determined both by
genetic and by environmental factors.
Husbands et al. proposed a similar method where
the connections grew according to a set of differential
equations. The genotype encoded the properties of each
neuron (the type of neuron, the relative position with respect
to the neuron created previously, the initial direction of
growth of the dendrites and the parameters of the equations
governing the growth process). During the genotype-tophenotype process, the genetic string was scanned from left
to right until a particular marker was found. When a special
marker indicating the beginning of the description of a
neuron was encountered, the following bits were read and
interpreted as parameters for a new neuron. The presence of
an additional marker, however, could indicate that the
parameters of the current neuron were speciﬁed in a previous
potentially allow the emergence of phenotypes with repeated structures formed by re-expression of the same genetic
instructions, similarly to the approach described by Gruau.
Despite the ability of developmental approaches to
generate large networks from compact genetic representations, it is not clear to what extent the network size
contributes to solve a problem. Developmental representations tend to generate regular and modular architectures,
which can be very suitable for problems that require them,
such as the control of a multi-legged robot. However, these
methods require additional mechanisms to cope with
asymmetric
architectures
ﬁne-tuning
parameters.
2.3 Implicit encoding
A recent approach to representing neural networks is
inspired from the mechanisms of biological gene regulatory networks (GRNs). The so-called implicit encoding is
frequently used as a representation for GRN models 
and has also been considered as a representation for other
types of networks, such as neural networks.
In biological gene networks, the interaction between the
genes is not explicitly encoded in the genome, but follows
implicitly from the physical and chemical environment in
which the genome is immersed. The activation of a gene
results from interactions of proteins in the environment of
the gene with the so-called regulatory regions (see Fig. 7a).
These are sequences of nucleotides to which the proteins
can bind to promote or hinder the working of specialized
molecular machinery that is in charge of expressing the
gene. The expression of the gene corresponds to the synthesis of proteins speciﬁed in another sequence of genetic
characters, the so-called coding region of the gene. The
start and end of the coding region of a gene are marked by
special motifs (i.e., nucleotide patters), called promoter
and terminator regions. The proteins produced by a gene
can in turn interact with the regulatory regions of other
genes and inﬂuence their expression. This process can
be interpreted as a network of interactions between the
different genes.
...XOVJWPGNBMJHDBTEOODFODDPWXXTEKCMRSIZZKJUWPOXCGNJJYXXVISTEVUBYCPTESSOOXI...
Fig. 7 a In biological gene
networks, the link between
genes is realized by molecules
that are synthesized from the
coding region of one gene and
interact with the regulatory
region of another gene.
b Analog genetic encoding
abstracts this mechanism with
an interaction map that
transforms the coding and
regulatory regions into a
numerical value that represents
the strength of the link. From
Mattiussi et al. 
Evol. Intel. 1:47–62
Reil studied the structural properties of such
gene networks with a simple computational model. He
used a sequence of characters from a ﬁnite alphabet as a
genome. The presence of a gene in the genome is signaled by the presence of a promoter, that is, a predeﬁned
sequence of ﬁxed length. A gene is constituted by a ﬁxed,
predeﬁned number of characters following a promoter. In
Reil , an activated gene produces another sequence of
characters which is a transliterated version of the gene.
The new sequence is interpreted as a regulatory protein. If
a regulatory protein matches exactly a sequence of characters in the genome, it regulates the expression of the
gene that immediately follows the matched genome
sequence (see Fig. 8). The regulation can be positive or
negative, and is of the on-off type, with repression prevailing over activation. Geard and Wiles reﬁned this
model by complexifying the map that transforms a gene
into a regulator protein, adding a further level of regulation which mimics the action of small RNA regulation
in real genomes and deﬁning regulation in terms of a
weighted sum of regulator proteins effects. The simulation of such artiﬁcial GRNs produced behaviors featuring
many properties observed in real GRNs. Although these
early computational GRN models were not directly aimed
at the exploitation of the evolutionary properties of such a
system, they inspired other researchers such as Bongard
 to use GRN models to evolve autonomous agents.
Bongard used a GRN model that relied on the diffusion of
chemicals, so-called transcription factors, which could
either directly inﬂuence the phenotype of the agent or
regulate the expression of genes. The results of his
computational experiments showed that his simple model
allowed for the evolution of complex morphologies
capable of simple locomotion behaviors and indicated that
the evolution of modular GRNs was beneﬁcial for the
success of the agents.
This type of abstraction of the biochemical process of
gene regulation can be extended to a genetic representation
for any kind of analog network.1 Analog genetic encoding
(AGE) is such a representation which has been applied to
the evolution of ANNs . AGE is based on a
discrete genome composed of sequences of characters from
a ﬁnite alphabet (e.g., the ASCII uppercase alphabet) which
is decoded into a network of interconnected devices (see
Fig. 7b). In the case of an artiﬁcial neural network, the
different types of neurons of the network are encoded by
sequences of genome which are separated by predeﬁned
delimiter sequences. These artiﬁcial coding and regulatory
regions play a role analogous to the corresponding
sequences in biological genomes.
The biochemical process which determines the interaction between the proteins encoded by one gene and the
regulatory region of another gene is abstracted by the socalled interaction map. The interaction map is a mathematical function that takes two sequences of characters as
argument and outputs a numerical value representing the
synaptic weight between two neurons. The neural network
can be decoded from the genome by scanning it for the
sequences of characters which represent a neuron. Subsequently, the interaction map can be used to compute the
interaction strengths between these neurons (see Fig. 9).
The size and topology of the decoded network hence
depends on the number of neurons encoded in the genome
and the interaction strengths between them (which can be
zero, thus leading to no connection between the corresponding neurons).
In AGE, the sequences that deﬁne the interaction
between devices can have variable length, and the interaction map that is used to establish the connection between
the neurons is deﬁned so as to apply to sequences of
arbitrary length. The assemblies of sequences of characters
that represent a neuron can be located anywhere in the
genome and can be spaced by stretches of non-coding
genetic characters. In this way the structure of the genome
Fig. 8 A simple model of gene expression and regulation in artiﬁcial
genomes. The genes which are marked by promoter regions in the
genome, are translated into transcription factors. These transcription
factors are then used to regulate the expression of the following genes.
Redrawn from Reil 
1 Analog networks are collections of dynamical devices interconnected by links of varying strength. For example, genetic regulatory
networks, metabolic networks, neural networks, or electronic circuits
can be seen as analog networks.
Evol. Intel. 1:47–62
is not unduly constrained and tolerates a large class of
genetic operators, which can alter both the topology and
the sizing of the encoded network. In particular, the AGE
genome permits the insertion, deletion and substitution of
single characters, and the insertion, deletion, duplication,
and transposition of whole genome fragments. All these
genetic mutations are known to occur in biological genomes and to be instrumental to their evolution. In
particular, gene duplication and the insertion of fragments
of genome of foreign organisms are deemed to be crucial
mechanism for the evolutionary increase of complexity of
GRNs . Finally, the interaction map is deﬁned so as to
be highly redundant, so that many different pairs of character sequences produce the same numeric value. Thus,
many mutations have no effect, resulting potentially in a
high neutrality in the search space.
When compared to NEAT, AGE reported equal performance on a non-Markovian double-pole balancing
problem , while both algorithms performed better than
a developmental encoding (CE) and a coevolution method
(ESP). However, both AGE and NEAT performed worse
than an evolution strategy with direct encoding of a ﬁxed
topology ANN . This indicates that if a suitable
topology is known in advance, it is better to use simpler
representations. Reisinger and Miikkulainen showed
that an implicit encoding very similar to AGE outperforms
NEAT on a complex board-game task.
3 Neuroevolution with dynamic neuron models
Some applications, for example the detection or generation
of time sequences, require a dynamic behavior of the
neural networks. Feedback connections in ANNs with
static transfer functions can provide such dynamics.
Another possibility is the integration of dynamic neuron
models. Dynamic neural networks are more difﬁcult to
train with learning algorithms that use gradient descent,
correlation, and reinforcement learning. Artiﬁcial evolution
instead has been used with success on a variety of applications because it can discover suitable architectures, time
constants and even learning algorithms.
3.1 Continuous-time recurrent neural networks
One of the most widely-used dynamic neuron models is
continuous-time
CTRNN . In this model, the neural network can be seen
as a set of differential equations
wijr xjðtÞ þ hj
i ¼ 1; 2; :::; N
where xi(t) is the activation of neuron i at the time t, N is
the number of neurons in the network, si is a time constant
whose magnitude (for si [ 0) is inversely proportional to
the decay rate of the activation, wij is the connection weight
between neuron i and neuron j, r(x) = 1/(1 + e-x) is the
standard logistic activation function, hj is a bias term and Ii
represents
dynamics and represent a ﬁrst approximation of the timedependent processes that occur at the membrane of biological neurons. Sometimes these models are also called
leaky integrators, with reference to electrical circuits,
Fig. 9 A simple artiﬁcial neural
network represented with
analog genetic encoding. The
interaction strengths are
computed by the interaction
map I which takes sequences of
characters of arbitrary length as
Evol. Intel. 1:47–62
because the equation describing the neuron activation is
equivalent to that describing the voltage difference of a
capacitor, where the time constant of the exponential and
synaptic weights can be approximated by a set of resistors
 . While being one of the simplest nonlinear, continuous
dynamic network models, CTRNNs are universal dynamic
approximators . This means that they can approximate
the trajectories of any smooth dynamical system arbitrarily
well for any ﬁnite interval of time.
Evolution of CTRNNs has been applied to difﬁcult
problems such as robot control . An interesting property of CTRNNs is that they can display learning behavior
without synaptic plasticity because they can store intermediate states in the activation function of internal
neurons. For example, Yamauchi and Beer evolved
generating
sequences without synaptic plasticity. Blynel and Floreano
 evolved a recurrent neural architecture capable of
reinforcement learning-like behaviors for a robot required
to navigate a T-maze.
3.2 Spiking neuron models
Most biological neurons communicate through action
potentials, or spikes, which are punctual events that result
from a process taking place at the output of the neuron.
In spiking neurons the activation state, which corresponds to an analog value, can be approximated by the
ﬁring rate of the neuron. That is, a larger number of spikes
within a given time window would be an indicator of
higher activation of the neuron. However, if that is the
case, it means that spiking neurons require relatively longer
time to communicate information to post-synaptic neurons.
This hints at the fact that spiking neurons may use other
ways to efﬁciently encode information, such as the ﬁring
time of single spikes or the temporal coincidence of spikes
coming from multiple sources . It may therefore be
advantageous for engineering purposes to use models of
spiking neurons that exploit ﬁring time in order to encode
spatial and temporal structure of the input patterns with less
computational resources.
There are several models of spiking neurons that
describe in detail the electrochemical processes that produce spiking events by means of differential equations
 . A simple way of implementing a spiking neuron is to
take the dynamic model of a CTRNN neuron and substitute
the output function with an element that compares the
neuron activation with its threshold followed by a pulse
generator that takes the form of a Dirac function (see
Fig. 10). In other words, if the neuron activation is larger
than the threshold, a spike is emitted. In order to prevent
continuous spike emission, one must also add a strong
negative feedback so that the neuron activation goes below
threshold immediately after spike emission. This model is
known as an Integrate and Fire neuron.
Korkin et al. evolved spiking neural networks
which could produce time-dependent waveforms, such as
sinusoids. Floreano and Mattiussi evolved networks of
spiking neurons for solving a non-trivial vision-based
navigation task. They compared a neural network using the
spike response model with a conventional sigmoid
network which could not solve the problem. The authors
suggest that the intrinsic dynamics of the spiking neural
networks might provide more degrees of freedom that can
be exploited by evolution to create viable controllers.
Saggie et al. compared spiking neural networks with
conventional static ANNs in a simple setup where an agent
had to search food and avoid poison in a simulated grid
world. Their results indicate that for tasks with memory
dependent dynamics, spiking neural networks can provide
less complex solutions. Federici combined an integrate
and ﬁre neuron model with a correlation-based synaptic
plasticity model and a developmental encoding. His results
from experiments with a simple robot navigation task
indicate that such a system may allow for the efﬁcient
evolution of large networks.
4 Evolution and learning
One of the main advantages of using evolutionary algorithms for the design and tuning of neural networks is that
evolution can be combined with learning,2 as shown in this
The combination of evolution and supervised learning
provides a powerful synergy between complementary
search algorithms . For example, gradient-based learning algorithms such as back-propagation are
Fig. 10 The integrate and ﬁre neuron is a simple extension of the
CTRNN neuron. The weighted sum of the inputs is fed to an
integrator which is followed by a threshold function and a pulse
generator. If the neuron activation is larger than a threshold, a spike is
2 Algorithms which combine evolutionary search with some kinds of
local search are sometimes called memetic algorithms .
Evol. Intel. 1:47–62
signiﬁcantly affect the quality of the trained network. In
this case, evolutionary algorithms can be used to ﬁnd the
initial weight values of networks to be trained with backpropagation. The ﬁtness function is computed using the
residual error of the network after training with backpropagation on a given task (notice that the ﬁnal weights
after supervised training are not coded back into the
genotype, i.e., evolution is Darwinian, not Lamarckian).
Experimental results consistently indicate that networks
with evolved initial weights can be trained signiﬁcantly
faster and better (by two orders of magnitude) than networks with random initial weights. The genetic string can
also encode the values of the learning rate and of other
learning parameters, such as the momentum in the case of
back-propagation. In this case, Belew et al. found that
the best evolved networks employed learning rates ten
times higher than values suggested before (i.e., much less
than 1.0), but this result may depend on several factors,
such as the order of presentation of the patterns, the
number of learning cycles allowed before computing the
ﬁtness, and the initial weight values.
Evolutionary algorithms have been employed also to
evolve learning rules. In its general form, a learning rule can
be described as a function U of few variables, such as presynaptic activity xi, postsynaptic activity yj, the current value
of the synaptic connection wij, and the learning signal tj
Dwij ¼ U xj; yi; wij; ti
Chalmers suggested to describe this function as a
linear combination of the products between the variables
weighted by constants. These constants are encoded in a
genetic string and evolved. The neural network is trained
on a set of tasks using the decoded learning rule and its
performance is used to compute the ﬁtness of the corresponding learning rule. The initial synaptic weights are
always set to small random values centered around zero.
Chalmers employed a ﬁtness function based on the mean
square error. A neural network with a single layer of
connections was trained on eight linearly separable classiﬁcation tasks. The genetic algorithm evolved a learning
rule similar to the delta rule by Widrow and Hoff .
Similar results were obtained by Fontanari and Meir .
Dasdan and Oﬂazer employed a similar encoding
strategy as Chalmers to evolve unsupervised learning
rules for classiﬁcation tasks. The authors reported that
evolved rules were more powerful than comparable,
human-designed
architecture and whether a synapse could be modiﬁed by
a simple Hebb rule (the rule was predetermined). Floreano and Mondada allowed evolution to choose
among four Hebbian learning rules for each synaptic
connection and evaluated the approach for a mobile robot
requested to solve a sequential task involving multiple
sensory modalities and environmental landmarks. The
results indicated that the evolved robots had the ability to
adapt on-the-ﬂy to changed environmental conditions
(including spatial reorganization, textures and even robot
morphologies) without incremental evolution . On the
contrary, evolved neural networks with ﬁxed weights
could not adapt to those changed conditions. Floreano
and Urzelai also showed that a morphogenetic
approach can greatly beneﬁt from co-evolution of synaptic plasticity because the strengths of the growing
connections are developed by learning rules that are coevolved with the developmental rules. Finally, Floreano
and Urzelai showed that dynamic environments
favor the genetic expression of plastic connections over
static connections.
DiPaolo evolved spiking neural controllers modeled using the Spike Response Model and heterosynaptic
variants of a learning rule called spike-timing dependent
plasticity (STDP) for a robot. The learning rule was
described as a polynomial expression of the STDP rule
where the components of the rule were weighted by individual constants that were genetically encoded and evolved
(similar to the encoding proposed by Chalmers). The
author showed that evolved robots were capable of learning
suitable associations between environmental stimuli and
Nolﬁand Parisi evolved a neural controller for a
mobile robot whose output layer included two ‘‘teaching
neurons’’ that were used to modify the connection weights
from the sensory neurons to the motor neurons with backpropagation learning during the robot’s lifetime. This
special architecture allows for using the sensory information not only to generate behavior but also to generate
teaching signals that can modify the behavior. Analysis of
the evolved robots revealed that they developed two different behaviors that were adapted to the particular
environment where they happened to be ‘‘born’’. The
evolved networks did not inherit an effective control
strategy, but rather a predisposition to learn to control. This
predisposition to learn involved several aspects such as a
tendency to move so as to experience useful learning
experiences and a tendency to acquire useful adaptive
characters through learning .
It has been known for a long time that learning may
affect natural evolution . Empirical evidence shows that
this is the case also for artiﬁcial evolution when combined
with some form of learning . Hinton and Nowlan 
proposed a simple computational model that shows how
learning might facilitate and guide evolution. They considered the case where a neural network confers added
reproductive ﬁtness on an organism only if it is connected
in exactly the right way. In this worst case, there is no
reasonable path toward the good network and a pure
Evol. Intel. 1:47–62
evolutionary search can only discover which of the
potential connections should be present by trying possibilities at random. In the words of the authors, the good
network is ‘‘like a needle in a haystack’’. Hinton and
Nowlan’s results suggest that the addition of learning
produces a smoothing of the ﬁtness surface area around the
good combination of genes (weights), which can be discovered and easily climbed by the genetic algorithm (see
One of the main limitations of Hinton and Nowlan’s
model is that the learning space and the evolutionary space
are completely correlated.3 By systematically varying the
cost of learning and the correlation between the learning
space and the evolutionary space, Mayley showed
that: (1) the adaptive advantage of learning is proportional
to the correlation between the two search spaces; (2) the
assimilation of characters ﬁrst acquired through learning is
proportional to the correlation between the two search
spaces and to the cost of learning (i.e., to the ﬁtness lost
during the ﬁrst part of the lifetime in which individuals
have suboptimal performance); (3) in certain situations
learning costs may exceed learning beneﬁts.
In the computational literature, it is sometimes debated
whether Lamarckian evolution (i.e., an evolutionary process where characters acquired through learning are
directly coded back into the genotype and transmitted to
offspring) could be more effective than Darwinian evolution (i.e., an evolutionary process in which characters
acquired through learning are not coded back into the
genotype). Ackley and Littman for instance claimed
that in artiﬁcial evolution, where inherited characters can
easily be coded into the genotype given that the mapping
between genotype and phenotype is generally quite simple,
it is preferable to use Lamarckian evolution. Indeed the
authors showed that Lamarckian evolution is far more
effective than Darwinian evolution in a stationary environment
input-output
change). However, Sasaki and Tokoro showed that
outperforms
Lamarckian
evolution when the environment is not stationary or when
different individuals are exposed to different learning
experiences.
5 Evolution of learning architectures
Experimental data suggests that mammalian brains employ
learning mechanisms that result in behaviors similar to
those produced by Reinforcement learning .
Reinforcement learning is a class of learning algorithms
that attempts to estimate explicitly or implicitly the value
of the states experienced by the agents in order to favor the
choice of those actions that maximize the amount of
positive reinforcement received by the agent over time
 . The exact mechanism that implements this kind of
learning in biological neural system is the subject of
ongoing research and is not yet completely understood. The
existing evidence points to the combined action of evolved
value systems and neuromodulatory effects .
The value system has the task of discriminating the
behaviors according to their reinforcing, punishing, or
negligible consequences. This leads to the production of
neuromodulatory signals that can activate or inhibit synaptic learning mechanisms.
Algorithms inspired by this approach have been developed by the machine learning community. The actor-critic
model for example, is a simple neural architecture
that implements reinforcement learning with two modules,
the Actor and the Critic shown in Fig. 12. Both modules
receive information on the current sensory state. In addition, the Critic receives information on the current
reinforcement value (external reinforcement) from the
environment. The output of the Critic generates an estimate
of the weighted sum of future rewards. The output of the
Actor instead is a probability of executing a certain set of
actions. This allows the system to perform an exploration
of the state-action space.
Typically such models are implemented in ANNs with a
ﬁxed architecture where separate modules realize the
sensory preprocessing, the value system and the action-
Fig. 11 Fitness landscape with and without learning. In absence of
learning, the ﬁtness landscape is ﬂat, with a thin spike in correspondence of the good combinations of genes (thick line). When learning
is enabled (dotted line), the ﬁtness surface displays a smooth hill
around the spike corresponding to the gene combination which have
in part correct ﬁxed values and in part unspeciﬁed (learnable) values.
Redrawn from Hinton and Nowlan 
3 The two spaces are correlated if genotypes which are close in the
evolutionary space correspond to phenotypes which are also close in
the phenotype space.
Evol. Intel. 1:47–62
selection mechanism.4 This puts the load of ﬁnding the
optimal structure of the value and the action-selection
system on the human designer. The integration of neuromodulation into a neuroevolution framework, as described
in the next section, allows for the evolution of reinforcement
learning-like
architectures
intervention.
5.1 Evolution of neuromodulatory architectures
biological
organisms,
specialized
neuromodulatory
neurons control the amount of activity-dependent plasticity
in the connection strength of other neurons. This effect is
mediated by special neurotransmitters called neuromodulators
neuromodulators
dopamine, serotonine, noradrenaline and acetylcholine. A
simple model of neuromodulation can easily be integrated
into a conventional neuroevolution framework by introducing a second type of neuron. These modulatory neurons
affect the learning events at the postsynaptic site (see
implementation
approach requires a genetic representation that allows for
the evolution of neural architectures consisting of several
different types of neurons. In Soltoggio et al. this has
Sect. 2.3) and a parameterized, activity dependent Hebbian
learning model . The authors show how reinforcementlearning like behavior can evolve in a simulated foraging
experiment. A simulated bee has to collect nectar on a ﬁeld
that contains two types of ﬂowers (see Fig. 14). As the
amount of nectar associated with each of the ﬂowers
changes stochastically, the behavioral strategy to maximize
the amount of collected nectar has to be adapted. The
evolved networks (see Fig. 15) implemented a value-based
learning strategy which was found to generalize to scenarios different from those used to assess the quality of the
evolution,
outperforming
obtained with hand-designed value-based learning architectures .
Another approach to evolving neuromodulatory architectures was suggested by Husbands et al. . They
developed and evolved a class of neural networks called
GasNets, inspired by the modulatory effects of freely diffusing gases, such as nitric oxide, which affect the response
proﬁle of neurons in biological neural networks. In Gas-
Nets, some of the neurons, which are spatially distributed
over a 2D surface, emit ‘gases’ that diffuse through the
network and modulate the proﬁle of the activation function
of other neurons in their vicinity. The spatial distance
between the neurons determines both the gas concentration
(as a result of the diffusion dynamics) and, in combination
additional
parameters,
connectivity.
The authors showed that the modulation caused by gas
diffusion introduces a form of plasticity in the network
without synaptic modiﬁcation. A comparison with conventional neural networks in a robot control task indicated
better evolvability of the GasNets . In another set of
experiments, McHale and Husbands compared Gas-
Nets to CTRNNs and conventional ANNs with Hebbian
learning in a robot locomotion control tasks. From all
Fig. 12 The Actor–Critic architecture for reinforcement learning.
Redrawn from Mizutani and Dreyfus 
Fig. 13 Neuromodulatory neurons permit the implementation of a
value-based learning mechanism in artiﬁcial neural networks. The
basic learning mechanism changes the weight of the link between the
two neurons n1 and n2 according to their activity. The neuromodulatory neuron mod modulates (i.e., strengthens or suppresses) the
basic learning mechanism and permits the synthesis of networks
where the learning is activated only in particular circumstances. From
Mattiussi et al. 
4 An alternative approach to this are neural learning classiﬁer
systems. For example, Hurst and Bull addressed the control of a
simulated robot in a maze task. They used a population of neural
networks acting as ‘rules’ controlling the robot. As evolution favored
rules that led to succesful behavior, the set of rules adapted to the
requirements of the task.
Evol. Intel. 1:47–62
tested networks, only the GasNets achieved a cyclic locomotion, but the CTRNNs achieved a higher average ﬁtness.
This is in line with Magg and Philippides whose
results indicate that GasNets perform extremely well on
tasks that require some kind of neural pattern generator or
timer, while performing worse on tasks which do not
require different time scales in the network.
6 Closing remarks
The methods for evolving neural networks have greatly
advanced over the years. In particular, we have witnessed a
signiﬁcant progress in the ability to evolve architectures
and learning for dynamic neural networks, which are very
powerful systems for real-world problems such as robot
From a practical point of view, artiﬁcial evolution turns
out to be extremely efﬁcient, or even unique, when it is
used to explore both the topology and the adaptive properties of neural networks because other training algorithms
usually require stronger assumptions and constraints on the
choice of architecture, training data distribution, and optimization
comparisons between alternative methods, this claim is
supported by the fact that most experiments reported in the
literature with neural robots in real environments resort to
some sort of evolutionary procedure rather than to other
types of training algorithms.
From a scientiﬁc point of view, artiﬁcial evolution can
be used to test hypothesis of brain development and
dynamics because it can encompass multiple temporal and
spatial scales along which an organism evolves, such as
genetic, developmental, learning, and behavioral phenomena. The possibility to co-evolve both the neural system
and the morphological properties of agents (although this
latter aspect is not reviewed in this paper) adds an additional valuable perspective to the evolutionary approach
that cannot be matched by any other approach.
We think that neuroevolution will continue to provide
efﬁcient solutions to hard problems as well as new insights
into how biological brains became what they are in physical bodies interacting with dynamic environments. Within
this context, we believe that the evolution of neural
architectures with elements that gate or modulate learning
will generate novel understanding of reinforcement-learning like structures that are so important for biological and
robotic life.
Acknowledgements
National Science Foundation, grant no. 200021-112060. Thanks to
Daniel Marbach for the illustrations and the two anonymous
reviewers for their helpful comments.