Proceedings of the 11th International Workshop on Semantic Evaluations , pages 502–518,
Vancouver, Canada, August 3 - 4, 2017. c⃝2017 Association for Computational Linguistics
SemEval-2017 Task 4: Sentiment Analysis in Twitter
Sara Rosenthal♣, Noura Farra♦, Preslav Nakov♥
♥Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar
♦Department of Computer Science, Columbia University
♣IBM Research, USA
This paper describes the ﬁfth year of
the Sentiment Analysis in Twitter task.
SemEval-2017 Task 4 continues with a
rerun of the subtasks of SemEval-2016
Task 4, which include identifying the overall sentiment of the tweet, sentiment towards a topic with classiﬁcation on a twopoint and on a ﬁve-point ordinal scale, and
quantiﬁcation of the distribution of sentiment towards a topic across a number
of tweets: again on a two-point and on
a ﬁve-point ordinal scale.
Compared to
2016, we made two changes: (i) we introduced a new language, Arabic, for all
subtasks, and (ii) we made available information from the proﬁles of the Twitter
users who posted the target tweets. The
task continues to be very popular, with a
total of 48 teams participating this year.
Introduction
The identiﬁcation of sentiment in text is an important ﬁeld of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences. The task usually involves detecting whether a piece of text expresses
a POSITIVE, a NEGATIVE, or a NEUTRAL sentiment; the sentiment can be general or about a speciﬁc topic, e.g., a person, a product, or an event.
The Sentiment Analysis in Twitter task has been
run yearly at SemEval since 2013 ,
with the 2015 task introducing sentiment towards
a topic and the 2016 task
introducing tweet quantiﬁcation and ﬁve-point ordinal classiﬁcation .
SemEval is the International Workshop on Semantic Evaluation, formerly SensEval.
It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on
the Lexicon of the Association for Computational
Linguistics. Other related tasks at SemEval have
explored sentiment analysis of product review and
their aspects ,
sentiment analysis of ﬁgurative language on Twitter , implicit event polarity
 , detecting stance in tweets
 , out-of-context sentiment intensity of words and phrases , and emotion detection .
Some of these tasks featured languages other than English, such as Arabic
 ;
however, they did not target tweets, nor did they
focus on sentiment towards a topic.
This year, we performed a re-run of the subtasks
in SemEval-2016 Task 4, which, in addition to the
overall sentiment of a tweet, featured classiﬁcation, ordinal regression, and quantiﬁcation with
respect to a topic. Furthermore, we introduced a
new language, Arabic.
Finally, we made available to the participants demographic information
about the users who posted the tweets, which we
extracted from the respective public proﬁles.
Ordinal Classiﬁcation
As last year, SemEval-
2017 Task 4 includes sentiment analysis on a ﬁvepoint scale {HIGHLYPOSITIVE, POSITIVE, NEU-
TRAL, NEGATIVE, HIGHLYNEGATIVE}, which is
in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp.
In machine learning terms, moving from a categorical two-point scale to an ordered ﬁve-point scale
means moving from binary to ordinal classiﬁcation (aka ordinal regression).
Tweet Quantiﬁcation
SemEval-2017 Task 4 includes tweet quantiﬁcation tasks along with tweet
classiﬁcation tasks, also on 2-point and 5-point
scales. While the tweet classiﬁcation task is concerned with whether a speciﬁc tweet expresses a
given sentiment towards a topic, the tweet quantiﬁcation task looks at estimating the distribution of tweets about a given topic across the
different sentiment classes.
Most (if not all)
tweet sentiment classiﬁcation studies within political science , economics , social science ,
and market research , study Twitter with an interest in aggregate statistics about sentiment and are
not interested in the sentiment expressed in individual tweets. We should also note that quantiﬁcation is not a mere byproduct of classiﬁcation, as it
can be addressed using different approaches and it
also needs different evaluation measures .
Analysis in Arabic
This year, we added a new
language, Arabic, in order to encourage participants to experiment with multilingual and crosslingual approaches for sentiment analysis. Our objective was to expand the Twitter sentiment analysis resources available to the research community, not only for general multilingual sentiment
analysis, but also for multilingual sentiment analysis towards a topic, which is still a largely unexplored research direction for many languages
and in particular for morphologically complex languages such as Arabic.
Arabic has become an emergent language for
sentiment analysis, especially as more resources
and tools for it have recently become available. It
is also both interesting and challenging due to its
rich morphology and abundance of dialectal use in
Twitter. Early Arabic studies focused on sentiment
analysis in newswire , but recently there
has been a lot more work on social media, especially Twitter , where the challenges of sentiment analysis are compounded by the presence
of multiple dialects and orthographical variants,
which are frequently used in conjunction with the
formal written language.
Some work studied the utility of machine translation for sentiment analysis of Arabic texts
 , identiﬁcation of sentiment holders , and sentiment targets .
We believe
that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in
this regard.
User Information
Demographic information in
Twitter has been studied and analyzed using network analysis and natural language processing
(NLP) techniques . Recent work has shown that user information and information from the network can help sentiment
analysis in other corpora and in
Twitter . Thus, this year we encouraged participants
to use information from the public proﬁles of Twitter users such as demographics (e.g., age, location)
as well as information from the rest of the social
network (e.g., sentiment of the tweets of friends),
with the goal of analyzing the impact of this information on improving sentiment analysis.
The rest of this paper is organized as follows.
Section 2 presents in more detail the ﬁve subtasks
of SemEval-2017 Task 4. Section 3 describes the
English and the Arabic datasets and how we created them. Section 4 introduces and motivates the
evaluation measures for each subtask. Section 5
presents the results of the evaluation and discusses
the techniques and the tools that the participants
used. Finally, Section 6 concludes and points to
some possible directions for future work.
Task Deﬁnition
SemEval-2017 Task 4 consists of ﬁve subtasks,
each offered for both Arabic and English:
1. Subtask A: Given a tweet, decide whether
it expresses POSITIVE, NEGATIVE or NEU-
TRAL sentiment.
2. Subtask B: Given a tweet and a topic, classify the sentiment conveyed towards that
topic on a two-point scale: POSITIVE vs.
3. Subtask C: Given a tweet and a topic,
tweet towards that topic on a ﬁve-point
scale: STRONGLYPOSITIVE, WEAKLYPOS-
ITIVE, NEUTRAL, WEAKLYNEGATIVE, and
STRONGLYNEGATIVE.
4. Subtask D: Given a set of tweets about
a topic, estimate the distribution of tweets
across the POSITIVE and NEGATIVE classes.
5. Subtask E: Given a set of tweets about a
topic, estimate the distribution of tweets
across the ﬁve classes:
STRONGLYPOS-
WEAKLYPOSITIVE,
WEAKLYNEGATIVE, and STRONGLYNEG-
Languages: English and Arabic
Granularity
Classiﬁcation
Classiﬁcation
Classiﬁcation
Quantiﬁcation
Quantiﬁcation
Table 1: Summary of the subtasks.
Each subtask is run for both English and Arabic.
Subtask A has been run in all previous editions of
the task and continues to be the most popular one
(see section 5.) Subtasks B-E have all been run at
SemEval-2016 Task 4 , with
variants running in 2015 .
Table 1 shows a summary of the subtasks.
Our datasets consist of tweets annotated for sentiment on a 2-point, 3-point, and 5-point scales.
We made available to participants all the data from
previous years for the English training sets, and we collected new training
data for Arabic, as well as new test sets for both
English and Arabic. The annotation scheme remained the same as last year ,
with the key new contribution being to apply the
task and instructions to Arabic as well as providing a script to download basic user information.
All annotations were performed on CrowdFlower.
Note that we release all our datasets to the research
community to be used freely beyond SemEval.
Tweet Collection
We chose English and Arabic topics based on popular current events that were trending on Twitter, both internationally and in speciﬁc Arabicspeaking countries, using local and global Twitter
trends.1 The topics included a range of named entities (e.g., Donald Trump, iPhone), geopolitical
entities (e.g., Aleppo, Palestine), and other entities
(e.g., Syrian refugees, Dakota Access Pipeline,
Western media, gun control, and vegetarianism).
We then used the Twitter API to download tweets,
along with corresponding user information, containing mentions of these topics in the speciﬁed
We intentionally chose to use some
overlapping topics between the two languages in
order to encourage cross-language approaches.
We automatically ﬁltered the tweets for duplicates and we removed those for which the bag-ofwords cosine similarity exceeded 0.6. We then retained only the topics for which at least 100 tweets
The training tweets for Arabic were
collected over the period of September-November
2016 and all test tweets were collected over the
period of December 2016-January 2017.
For both English and Arabic, the topics for the
test dataset were different from those in the training and in the development datasets.
Annotation using CrowdFlower
We used CrowdFlower to annotate the new training and testing tweets. The annotators were asked
to indicate the overall polarity of the tweet (on a
ﬁve-point scale) as well as the polarity of the tweet
towards the given target topic (again, on a ﬁvepoint scale), as described in .
We also provided additional examples, some of
which are shown in Tables 2 and 3. In particular, we stressed that topic-level positive or negative
sentiment needed to express an opinion about the
topic itself rather than about a positive or a negative event occurring in the context of the topic (see
for example, the third row of Table 3).
Each tweet was annotated by at least ﬁve people, and we created many hidden tests for quality control, which we used to reject annotations
by contributors who missed a large number of the
hidden tests. We also created pilot runs, which
helped us adjust the annotation instructions until
we found, based on manual inspection, the quality
of the annotated tweets to be satisfactory.
1 
Overall Sentiment
Topic-level Sentiment
Who are you tomorrow? Will you make me smile or just
bring me sorrow? #HottieOfTheWeek Demi Lovato
Demi Lovato: POSITIVE
Saturday without Leeds United is like Sunday dinner it
doesn’t feel normal at all (Ryan)
WEAKLYNEGATIVE Leeds United: HIGHLYPOSITIVE
Apple releases a new update of its OS
Apple: NEUTRAL
Table 2: Some English example annotations that we provided to the annotators.
Overall Sentiment
Topic-level Sentiment
Ë@ ÐA ¢ JË éªK.@QË@ éJ
Qj. JË@ é j
Apple releases a fourth beta of its OS
@ Apple: NEUTRAL
®Ê mÌ'@ I. ªÊË@ ½ÊÓ PPY ¯ Qk. ðP èPñ¢B@ ... ðQ
éKA¢®Ë ÉÔg. @ áÓ
The maestro ... the legend Roger Federer king of the backhand game one of his best shots
HIGHLYPOSITIVE
PPY ¯ Federer: HIGHLYPOSITIVE
HAK.ñªË@ àñêk. @ñK
Refugees are facing difﬁculties
WEAKLYNEGATIVE àñ
Jk. CË@ Refugees : NEUTRAL
Table 3: Some Arabic example annotations that we provided to the annotators.
For Arabic, the contributors tended to annotate
somewhat conservatively, and thus a very small
number of HIGHLYPOSITIVE and HIGHLYNEG-
ATIVE annotations were consolidated, despite us
having provided examples of such annotations.
Consolidating the Annotations
As the annotations are on a ﬁve-point scale, where
the expected agreement is lower, we used a twostep procedure. If three out of the ﬁve annotators
agreed on a label, we accepted the label.
Otherwise, we ﬁrst mapped the categorical labels to
the integer values −2, −1, 0, 1, 2.
calculated the average, and ﬁnally we mapped
that average to the closest integer value. In order to counter-balance the tendency of the average to stay away from the extreme values −2 and
2, and also to prefer 0, we did not use rounding at ±0.5 and ±1.5, but at ±0.4 and ±1.4 instead. Finally, note that the values −2, −1, 0, 1,
2 are to be interpreted as STRONGLYNEGATIVE,
WEAKLYNEGATIVE, NEUTRAL, WEAKLYPOSI-
TIVE, and STRONGLYPOSITIVE, respectively.
Data Statistics
The English training and development data this
year consisted of the data from all previous editions of this task . Unlike in
previous years, we did not set aside data to assess
progress compared to prior years. Therefore, we
allowed all data to be used for training and development.
For evaluation, we used the newly-created data described in the previous subsection. Tables 4 and 5
show the statistics for the English and Arabic data.
For English, we only show the aggregate statistics for the training data; the breakdown from prior
years can be found in . Note
that the same tweets were annotated for multiple
subtasks, so there is overlap between the tweets
across the tasks. Duplicates may have occurred
where the same tweet was extracted for multiple
As Arabic is a new language this year, we created for it a default train-development split of
the Arabic data for the participants to use if they
wished to do so.
Data Distribution
As in previous years, we provided the participants
with a script2 to download the training tweets
given IDs. In addition, this year we also included
in the script the option to download basic user information for the author of each tweet: user id,
follower count, status count, description, friend
count, location, language, name, and time zone.
To ensure a fair evaluation, the test set was provided via download and included the tweets as
well as the basic user information provided by the
download script. The training and the test data is
available for downloaded on our task page.3
2 
3 
task4/index.php?id=data-and-tools
Table 4: Statistics about the English training and testing datasets. The training data is the aggregate of
all data from prior years, while the testing data is new.
Table 5: Statistics about the newly collected Arabic training and testing datasets.
Evaluation Measures
This section describes the evaluation measures for
our ﬁve subtasks. Note that for Subtasks B to E,
the datasets are each subdivided into a number of
topics, and the subtask needs to be carried out independently for each topic. As a result, each of
the evaluation measures will be “macroaveraged”
across the topics, i.e., we compute the measure individually for each topic, and we then average the
results across the topics.
Subtask A: Overall Sentiment of a Tweet
Our primary measure is AvgRec, or average recall, which is recall averaged across the POSITIVE
(P), NEGATIVE (N), and NEUTRAL (U) classes.
This measure has desirable theoretical properties
 , and is also the one we use as
primarily for Subtask B. It is computed as follows:
AvgRec = 1
3(RP + RN + RU)
where RP , RN and RU refer to recall with respect to the POSITIVE, the NEGATIVE, and the
NEUTRAL class, respectively. See for more detail.
AvgRec ranges in , where a value of 1 is
achieved only by the perfect classiﬁer (i.e., the
classiﬁer that correctly classiﬁes all items), a value
of 0 is achieved only by the perverse classiﬁer
(the classiﬁer that misclassiﬁes all items), while
0.3333 is both (i) the value for a trivial classiﬁer
(i.e., one that assigns all tweets to the same class
– be it POSITIVE, NEGATIVE, or NEUTRAL), and
(ii) the expected value of a random classiﬁer.
The advantage of AvgRec over “standard” accuracy is that it is more robust to class imbalance.
The accuracy of the majority-class classiﬁer is the
relative frequency (aka “prevalence”) of the majority class, that may be much higher than 0.5 if
the test set is imbalanced. Standard F1 is also sensitive to class imbalance for the same reason. Another advantage of AvgRec over F1 is that AvgRec
is invariant with respect to switching POSITIVE
with NEGATIVE, while F1 is not. See for more detail on AvgRec.
We further use two secondary measures: accuracy and F PN
. The latter was the primary evaluation measure for Subtask A in previous editions
of the task. It is macro-average F1, calculated over
the POSITIVE and the NEGATIVE classes (note the
exclusion of NEUTRAL). This year, we demoted
to a secondary evaluation measure. It is calculated as follows:
1 refer to F1 with respect to the
POSITIVE and the NEGATIVE class, respectively.
Subtask B: Topic-Based Classiﬁcation on
a 2-point Scale
As in 2016, our primary evaluation measure for
subtask B is average recall, or AvgRec (note that
there are only two classes for this subtask):
AvgRec = 1
2(RP + RN)
We further use accuracy and F1 as secondary
measures for subtask B. Finally, as subtask B is
topic-based, we computed each metric individually for each topic, and we then averaged the result
across the topics to yield the ﬁnal score.
Subtask C: Topic-based Classiﬁcation on
a 5-point Scale
Subtask C is an ordinal classiﬁcation (also known
as ordinal regression) task, in which each tweet
must be classiﬁed into exactly one of the classes
in C={HIGHLYPOSITIVE, POSITIVE, NEUTRAL,
NEGATIVE, HIGHLYNEGATIVE}, represented in
our dataset by numbers in {+2,+1,0,−1,−2},
with a total order deﬁned on C.
We adopt an evaluation measure that takes the
order of the ﬁve classes into account. For instance,
misclassifying a HIGHLYNEGATIVE example as
HIGHLYPOSITIVE is a bigger mistake than misclassifying it as NEGATIVE or as NEUTRAL.
As in SemEval-2016 Task 4, we use macroaverage mean absolute error (MAEM) as the
main ordinal classiﬁcation measure:
MAEM(h, Te) = 1
|h(xi)−yi|
where yi denotes the true label of item xi, h(xi)
is its predicted label, Tej denotes the set of test
documents whose true class is cj, |h(xi) −yi| denotes the “distance” between classes h(xi) and yi
(e.g., the distance between HIGHLYPOSITIVE and
NEGATIVE is 3), and the “M” superscript indicates
“macroaveraging”.
The advantage of MAEM over “standard”
mean absolute error, which is deﬁned as
MAEµ(h, Te) =
|h(xi) −yi|
is that it is robust to class imbalance (which
is useful, given the imbalanced nature of our
dataset). On perfectly balanced datasets MAEM
and MAEµ are equivalent.
MAEM is an extension of macro-average recall
for ordinal regression; yet, it is a measure of error, and thus lower values are better. We also use
MAEµ as a secondary measure, in order to provide better consistency with Subtasks A and B.
These measures are computed for each topic, and
the results are then averaged across all topics to
yield the ﬁnal score. See 
for more detail about MAEM and MAEµ.
Subtask D: Tweet Quantiﬁcation on a
2-point Scale
Subtask D assumes a binary quantiﬁcation setup,
in which each tweet is classiﬁed as POSITIVE
or NEGATIVE, and the distribution across classes
must be estimated.
The difference with binary
classiﬁcation is that errors of different polarity
(e.g., a false positive and a false negative for
the same class) can compensate for each other in
quantiﬁcation. Quantiﬁcation is thus a more lenient task than classiﬁcation, since a perfect classi-
ﬁer is also a perfect quantiﬁer, but a perfect quantiﬁer is not necessarily a perfect classiﬁer.
For evaluating binary quantiﬁcation, we keep
the Kullback-Leibler Divergence (KLD) measure
used in 2016 along with additive smoothing
 . KLD was
proposed as a quantiﬁcation measure in , and is deﬁned as follows:
KLD(ˆp, p, C) =
p(cj) loge
KLD is a measure of the error made in estimating a true distribution p over a set C of classes by
means of a predicted distribution ˆp. Like MAEM,
KLD is a measure of error, which means that
lower values are better. KLD ranges between 0
(best) and +∞(worst).
Note that the upper bound of KLD is not ﬁnite
since Equation 5 has predicted prevalences, and
not true prevalences, at the denominator: that is,
by making a predicted prevalence ˆp(cj) inﬁnitely
small we can make KLD inﬁnitely large. To solve
this problem, in computing KLD we smooth both
p(cj) and ˆp(cj) via additive smoothing, i.e.,
p(cj)) + ϵ · |C|
= p(cj) + ϵ
1 + ϵ · |C|
where ps(cj) denotes the smoothed version of
p(cj) and the denominator is just a normalizer
(same for the ˆps(cj)’s); the quantity ϵ =
used as a smoothing factor, where Te denotes the
test dataset.
The smoothed versions of p(cj) and ˆp(cj) are
used in place of their original versions in Equation
5; as a result, KLD is always deﬁned and still
returns a value of 0 when p and ˆp coincide.
Like MAEM, KLD is a measure of error,
which means that lower values are better.
further use two secondary error-based evaluation
measures: absolute error (AE), and relative absolute error (RAE).
Again, the measures are computed individually
for each topic, and the results are averaged across
the topics to yield the ﬁnal score.
Subtask E: Tweet Quantiﬁcation on a
5-point Scale
Subtask E is an ordinal quantiﬁcation task. As in
binary quantiﬁcation, the goal is to compute the
distribution across classes, this time assuming a
quantiﬁcation setup.
of the classes in C={HIGHLYPOSITIVE, POS-
ITIVE, NEUTRAL, NEGATIVE, HIGHLYNEGA-
TIVE}, where there is a total order on C. As in
binary quantiﬁcation, the task is to compute an estimate ˆp(cj) of the relative frequency p(cj) in the
test tweets of all the classes cj ∈C.
The measure we adopt for ordinal quantiﬁcation is the Earth Mover’s Distance , also known as the Vaser˘ste˘ın metric , a measure well-known in
the ﬁeld of computer vision. EMD is currently
the only known measure for ordinal quantiﬁcation. It is deﬁned for the general case in which
a distance d(c′, c′′) is deﬁned for each c′, c′′ ∈C.
When there is a total order on the classes in C and
d(ci, ci+1) = 1 for all i ∈{1, ..., (C −1)}, the
Earth Mover’s Distance is deﬁned as
EMD(ˆp, p) =
p(ci)| (7)
and can be computed in |C| steps from the estimated and true class prevalences.
Like KLD, EMD is a measure of error, so
lower values are better; EMD ranges between 0
(best) and |C| −1 (worst). See for more detail on EMD.
As before, EMD is computed individually for
each topic, and the results are then averaged across
all topics to yield the ﬁnal score. For more detail
on EMD, the reader is referred to and to last year’s task description
paper .
Participants and Results
A total of 48 teams participated in SemEval-2017
Task 4 this year. As in previous years, the most
popular subtask this year was Subtask A, with 38
teams participating in the English subtask A, and 8
teams participating in the Arabic subtask A. Overall, there were 46 teams who participated in some
English subtask and 9 teams that participated in
some Arabic subtask. There were 28 teams that
participated in a subtask other than subtask A.
Moreover, two teams (OMAM and ELiRF-UPV)
participated in all English and in all Arabic subtasks. There were 9 teams that participated in the
topic versions of the subtasks but not in subtask A,
reﬂecting a growing interest among researchers in
developing systems for topic-speciﬁc analysis.
Common Resources and Methods
In terms of methods, the use of deep learning
stands out in particular, and we also see an increase over the last year. There were at least 20
teams who used deep learning and neural network
methods such as CNN and LSTM networks. Supervised SVM and Liblinear were also very popular, with several participants combining SVM with
neural network methods or SVM with dense word
embedding features. Other teams used classiﬁers
such as Maximum Entropy, Logistic Regression,
Random Forest, Na¨ıve Bayes classiﬁer, and Conditional Random Fields.
Common software used included Python (with
the sklearn and numpy libraries), Java, Tensor-
ﬂow, Weka, NLTK, Keras, Theano, and Stanford
CoreNLP. The most common external datasets
used were sentiment140 as a lexicon, pre-trained
word2vec embeddings. Many teams further gathered additional tweets using the Twitter API that
were not annotated for sentiment. These were used
for distant supervision, lexicon building, and word
vector training.
In the following subsections, we present the results and the ranking for each subtask, and we
highlight the best-performing systems for each
DataStories
CrystalNest
Amobee-C-137
ej-za-2017
Neverland-THU
WarwickDCS
All POSITIVE
All NEGATIVE
All NEUTRAL
Table 6: Results for Subtask A “Message Polarity Classiﬁcation”, English. The systems are ordered by average recall AvgRec (higher is better).
In each column, the rankings according to the corresponding measure are indicated with a subscript.
Bx indicates a baseline.
Results for Subtask A: Overall Sentiment
in a Tweet
Tables 6 and 7 show the results for Subtask A in
English and Arabic, respectively, where the teams
are ranked by macro-average recall.
BB twtr and DataStories, both achieving a macroaverage recall of 0.681. Both top teams used deep
learning; BB twtr used an ensemble of LSTMs
and CNNs with multiple convolution operations,
while DataStories used deep LSTM networks with
an attention mechanism.
All POSITIVE
All NEGATIVE
All NEUTRAL
Table 7: Results for Subtask A “Message Polarity Classiﬁcation”, Arabic. The systems are ordered by average recall AvgRec (higher is better).
In each column, the rankings according to the corresponding measure are indicated with a subscript.
Bx indicates a baseline.
Both teams participated in all English subtasks
and were also ranked in ﬁrst (BB twtr) and second (DataStories) place for subtasks B-D; BB twtr
was also ranked ﬁrst for subtask E.
The top 5 teams for English were very closely
scored. The following four best-ranked teams all
used deep learning or deep learning ensembles.
Three of the top-10 scoring teams (INGEOTEC,
SiTAKA, and UCSC-NLP) used SVM classiﬁers
instead, with various surface, lexical, semantic,
and dense word embedding features.
of ensembles clearly stood out, with ﬁve of the
top-10 scoring systems (BB twtr, LIA, NNEMBs,
Tweester, and INGEOTEC) using ensembles, hybrid, stacking or some kind of mix of learning
methods. All teams beat the baseline on macroaverage recall; however, a few teams did not beat
the harsher average F-measure and accuracy baselines.
best-ranked
NileTMRG, and it achieved a score of 0.583.
They used a Na¨ıve Bayes classiﬁer with a
combination of lexical and sentiment features;
they further augmented the training dataset to
about 13K examples using external tweets. The
SiTAKA team was ranked second with a score of
0.55. Their system used a feature-rich SVM with
lexical features and embedding representations.
Except for EliRF-UPV, who used multi-layer
neural networks (CRNNs), the remaining teams
used SVM and Na¨ıve Bayes classiﬁers, genetic
algorithms, or conditional random ﬁelds (CRFs).
All teams managed to beat all baselines for all
The difference in the absolute scores for the two
languages is probably partially due to the difference in the amount of training data available for
Arabic, which was much smaller compared English, even when external datasets were taken into
The results also reﬂect the linguistic
complexity of Arabic as it is used in social media, which is characterized by the abundant use of
dialectal forms and spelling variants. Overall, participants preferred to focus on developing Arabicspeciﬁc systems (varying in the extent to which
they applied Arabic-speciﬁc preprocessing) rather
than trying to leverage cross-language models that
would enable them to use English data to augment
their Arabic models.
DataStories
TopicThunder
funSentiment
WarwickDCS
CrystalNest
Amobee-C-137
ej-za-2017
All POSITIVE
All NEGATIVE
Table 8: Results for Subtask B “Tweet classiﬁcation according to a two-point scale”, English.
The systems are ordered by average recall AvgRec
(higher is better). Bx indicates a baseline.
All POSITIVE
All NEGATIVE
Table 9: Results for Subtask B “Tweet classiﬁcation according to a two-point scale”, Arabic.
The systems are ordered by average recall AvgRec
(higher is better). Bx indicates a baseline.
Results for Subtasks B and C:
Topic-Based Classiﬁcation
The results of Subtasks B and C are shown in Tables 8–11. We can see that the system scores for
subtask B are higher than those for subtask A, with
the best team achieving 0.882 accuracy for English
(compared to 0.681 for subtask A) and 0.768 for
Arabic (compared to 0.583 for subtask A). However, this is primarily due to the fact there are two
classes for subtask B, while there are three classes
for subtask A.
DataStories
Amobee-C-137
CrystalNest
funSentiment
HIGHLYNEGATIVE
HIGHLYPOSITIVE
Table 10: Results for Subtask C “Tweet classiﬁcation according to a ﬁve-point scale”, English.
The systems are ordered by their MAEM score
(lower is better). Bx indicates a baseline.
HIGHLYNEGATIVE
HIGHLYPOSITIVE
Table 11: Results for Subtask C “Tweet classiﬁcation according to a ﬁve-point scale”, Arabic.
The systems are ordered by their MAEM score
(lower is better). Bx indicates a baseline.
For English
the BB twtr system, ranked ﬁrst,
modeled topics by concatenating the topical information at the word level. The second-best system,
DataStories, also accounted for topics by producing topic annotations and a context-aware attention mechanism.
DataStories
CrystalNest
funSentiment
10 THU HCSI IDU
0.12910 0.17910 2.42811
11 Amobee-C-137
0.14911 0.17910 2.16810
0.16412 0.20412 2.79012
13 SSK JNTUH
0.42113 0.31413 2.98313
14 ELiRF-UPV
1.06014 0.59315 7.99115
15 YNU-HPCC
1.14215 0.59214 7.85914
B2 macro-avg on 2016 data
B3 micro-avg on 2016 data
B4 macro-avg on 2015-6 data 0.534
B5 micro-avg on 2015-6 data 0.587
Table 12: Results for Subtask D “Tweet quantiﬁcation according to a two-point scale”, English.
The systems are ordered by their KLD
score (lower is better). Bx indicates a baseline.
B2 macro-avg on train-2017 0.296
B3 micro-avg on train-2017
Table 13: Results for Subtask D “Tweet quantiﬁcation according to a two-point scale”, Arabic. The systems are ordered by their KLD score
(lower is better). Bx indicates a baseline.
funSentiment, ranked 6th and 9th for subtasks B
and C, respectively, modeled the sentiment towards the topic using the left and the right context
around a topic mention in the tweet. WarwickDCS,
ranked 8th, used simple tweet-level classiﬁcation,
while ignoring the topic. Overall, almost all teams
managed to outperform the majority class baseline
for subtask B, but only two teams outperformed
the NEUTRAL class baseline for subtask C.
For Arabic
four teams participated in Subtask B
and two teams in Subtask C. NileTMRG was once
again ranked ﬁrst for Subtask B, with a system
based on ensembles of topic-speciﬁc and topicagnostic models. For subtask C, OMAM also used
combinations of such models applied in succession. All teams easily outperformed the baselines
for Subtask B, but only the OMAM team managed
to do so for Subtask C.
funSentiment
Amobee-C-137
THU HCSI IDU
DataStories
(0 0 0 1 0)
macro-avg on 2016 data
micro-avg on 2016 data
Table 14: Results for Subtask E “Tweet quantiﬁcation according to a ﬁve-point scale”, English. The systems are ordered by their EMD
score (lower is better). Bx indicates a baseline.
(0 0 1 0 0)
macro-avg on train-2017
micro-avg on train-2017
Table 15: Results for Subtask E “Tweet quantiﬁcation according to a ﬁve-point scale”, Arabic. The systems are ordered by their EMD score
(lower is better). Bx indicates a baseline.
Results for Subtasks D and E: Tweet
Quantiﬁcation
Tables 12–15 show the results for the tweet quantiﬁcation subtasks. The bottom of the tables report
the result of a baseline system, B1, that assigns
a prevalence of 1 to the majority class (which is
the POSITIVE class for subtask D, and the WEAK-
LYPOSITIVE/NEUTRAL class for subtask E, English/Arabic) and 0 to the other class(es).
We further show the results for a smarter “maximum likelihood” baseline, which assigns to each
test topic the distribution of the training tweets
(the union of TRAIN, DEV, DEVTEST) across
the classes.
This is the “smartest” among the
trivial policies that attempt to maximize KLD.
For this baseline, for English we use for training either (i) the 2016 data only, or (ii) data from
both 2015 and 2016; we also experiment with
(i) micro-averaging and (ii) macro-averaging over
the topics. It turns out that macro-averaging over
2015+2016 data is the strongest baseline in terms
of KLD. For Arabic, we use the train-2017 data,
and micro-averaging works better there.
There were 15 participating teams competing in
Subtask D: 15 for English and 3 for Arabic (these
3 teams all participated in English).
other subtasks, BB twtr was ranked ﬁrst in English. They achieved an improvement of .50 points
absolute in KLD over the best baseline, and a .01
improvement over the next best team, DataStories.
For Arabic, the best team was NileTMRG With improvement of .17 over the best baseline and of .08
over the next best team, OMAM. All but the last
two teams in English and the last team for Arabic
outperformed all baselines.
In Subtask E, there were 12 participating teams,
with OMAM and EliRF-UPV competing for both
English and Arabic. Once again, BB twtr was the
best for English, improving over the best baseline
by .31 EMD points absolute. Interestingly, this is
the ﬁrst subtask where DataStories was not the
second-ranked team.
BB twtr outperformed the
second-best team, TwiSe, by .02 points. For English, all but the last two teams outperformed the
baselines. However, for Arabic, none of the two
participating teams could do so.
User Information
This year, we encouraged teams to explore using
in their models information about the user who
wrote the tweet, which can be extracted from the
public user proﬁles of the respective Twitter users.
Participants could also try features about following relations and the structure of the social network in general, as well as could make use of other
tweets by the target user when analyzing one particular tweet. Four teams tried that: SINAI, ECNU,
TakeLab, and OMAM. OMAM and TakeLab did
not ﬁnd any improvements, and ultimately decided
not to use any user information. ECNU used pro-
ﬁle information such as favorited, favorite count,
retweeted, and retweet count. They ended up 15th
in Subtask A. SINAI used the last 200 tweets from
the person’s timeline. They ranked 12th in Subtask
B. They generated a user model from the timeline
of a given target user. They built a general SVM
model on word2vec embeddings. Then, for each
user in the test set, they downloaded the last 200
tweets published by the user and classiﬁed their
sentiment using that SVM classiﬁer. If the classiﬁed user tweets achieved an accuracy above a
threshold (0.7), the user model was applied on the
authored tweets from the test set. If not, the general SVM model was used.
It is difﬁcult to judge whether and by how much
user information could help the best approaches
as they did not try to use such information. However, we believe that building and using a Twitter user proﬁle is a promising research direction,
and that participants should learn how to make this
work in the future. Thus, we would like to encourage more teams to try to explore using this
information. We would also like to provide more
user information such as age and gender, which we
can predict automatically , when it is not directly available from
the user proﬁle. Another promising direction is to
make use of “conversations” in Twitter, i.e., take
into account the replies to tweets in Twitter. For
example, previous work has
shown that it is beneﬁcial to model the polarity detection problem as a sequential classiﬁcation task
over streams of tweets, where the stream is a “conversation” on Twitter containing tweets, replies to
these tweets, replies to these replies, etc.
Conclusion and Future Work
Sentiment Analysis in Twitter continues to be a
very popular task, attracting 48 teams this year.
The task provides immense value to the sentiment community by providing a large accessible
benchmark dataset containing over 70,000 tweets
across two languages for researchers to evaluate
and compare their method to the state of the art.
This year, we introduced a new language for the
ﬁrst time and also encouraged the use of user information. These additions drew new participants
and ideas to the task. The Arabic tasks drew nine
participants and four teams took advantage of user
information.
Although a respectable amount of
participants for its inaugural year, further exploration into both of these areas would be useful in
the future, such as collecting more training data for
Arabic and encouraging the use of cross-lingual
training data. In the future, we would like to include exploring additional languages, providing
further user information, and other related tasks
such as irony and emotion detection. Finally, deep
learning continues to be popular and employed by
the state of the art approaches.
We expect this
trend to continue in sentiment analysis research,
but also look forward to new innovative ideas that
are discovered.
Afﬁliation
Korea University
South Korea
 
Amobee C-137
 
Al-Imam Muhammad Ibn Saud Islamic University.
Saudi Arabia
 
Bogazici University
 
CrystalNest
Institute of High Performance Computing
 
DataStories
Data Science Lab at University of Piraeus
 
National Sun Yat-sen University
 
Democritus University of Thrace
 
East China Normal University
 
East China Normal University
 
ej-sa-2017
University of Evora
 
Universitat Polit´ecnica de Val´encia
 
funSentiment
Thomson Reuters
 
University of Pennsylvania
 
CONACYT-INFOTEC/CENTROGEO
 
 
Aix-Marseille University
 
Harbin Institute of Technology
 
Neverland-THU
Institute of Mathematics and Computer Science, University of So Paulo
 
Nile University
 
Peking University
 
National Research University Higher School
of Economics
 
American University of Beirut, Universiti
Teknologi Malaysia, Cairo University, New
York University Abu Dhabi, Qatar University
United Arab Emirates
 
 
Universidad de Ja´en
 
iTAKA, Universitat Rovira i Virgili; Hodeidah University
Spain, Yemen
 
BVRIT Hyderabad College of Engineering
Department of CSE, SSN College of Engineering
 
TakeLab, University of Zagreb
 
THU HCSI IDU
Human Computer Speech Interaction Research Group, Tsinghua University
TopicThunder
Infosys Limited
 
Selcuk University, Universit Libre de Bruxelles (ULB)
Belgium, Turkey
 
National Technical University of Athens, University of Athens, “Athena” Research and Innovation Center, Signal Analysis and Interpretation Laboratory (SAIL), USC
Greece, USA
 
University of Grenoble-Alps
 
Catholic University of the Most Holy Conception
 
WarwickDCS
Department of Computer Science, University
of Warwick
Xi’an JiaoTong University
 
Yunnan University
 
Yunnan University
 
Table 16: Alphabetical list of the participating teams, their afﬁliation, country, the subtasks they participated in, and the system description paper that they contributed to SemEval-2017. Teams whose
Afﬁliation column is typeset on more than one row include researchers from different institutions, which
have collaborated to build a joint system submission. An N/A entry for the Paper column indicates that
the team did not contribute a system description paper. Finally, the last row gives statistics about the total
number of system submissions for each subtask.