University of California, Berkeley
U.C. Berkeley Division of Biostatistics Working Paper Series
A General Framework for Statistical
Performance Comparison of Evolutionary
Computation Algorithms
David Shilane∗
Jarno Martikainen†
Sandrine Dudoit‡
Seppo Ovaska∗∗
∗Division of Biostatistics, School of Public Health, University of California, Berkeley, 
†Power Electronics Laboratory, Helsinki University of Technology, Espoo, Finland
‡Division of Biostatistics, School of Public Health, University of California, Berkeley, 
∗∗Power Electronics Laboratory, Helsinki University of Technology, Espoo, Finland
This working paper is hosted by The Berkeley Electronic Press (bepress) and may not be commercially reproduced without the permission of the copyright holder.
 
Copyright c⃝2006 by the authors.
A General Framework for Statistical
Performance Comparison of Evolutionary
Computation Algorithms
David Shilane, Jarno Martikainen, Sandrine Dudoit, and Seppo Ovaska
This paper proposes a statistical methodology for comparing the performance of
evolutionary computation algorithms. A two-fold sampling scheme for collecting
performance data is introduced, and these data are analyzed using bootstrap-based
multiple hypothesis testing procedures. The proposed method is sufﬁciently ﬂexible to allow the researcher to choose how performance is measured, does not
rely upon distributional assumptions, and can be extended to analyze many other
randomized numeric optimization routines. As a result, this approach offers a
convenient, ﬂexible, and reliable technique for comparing algorithms in a wide
variety of applications.
Introduction
Evolutionary algorithms (EAs) are used to estimate the solution to diﬃcult optimization problems.
EAs are often hand-crafted to meet the requirements of a particular problem because no single optimization
algorithm can solve all problems competitively . When alternative algorithms are proposed, their relative
eﬃcacies should be assessed. Because EAs follow a stochastic process, statistical analysis is appropriate for
algorithm comparison. This paper seeks to provide a general methodology for comparing the performance
of EAs based on statistical sampling and hypothesis testing.
In , Christensen and Wineberg explain the use of appropriate statistics in artiﬁcial intelligence and propose
non-parametric tests to verify the distribution of an EA’s estimate of a function’s optimal value. Flexer 
proposes general guidelines for statistical evaluation of neural networks that can also be applied to EAs.
Czarn et al. discuss the use of the analysis of variance (ANOVA) in comparing the performance of EAs.
However, this third procedure relies upon distributional assumptions that are not necessarily valid and limit
the class of performance metrics that can be used.
An EA’s initial population (Section 2) consists of a set of starting values for the evolution process. Most
previous EA performance comparisons have only considered results for a single initial population or even
provided diﬀerent inputs for each algorithm studied. Supplying diﬀerent single inputs to each EA may result
in a founder eﬀect, in which a population’s initial advantage is continually propagated to successive generations. Furthermore, relying upon a single choice of initial population can at best determine the plausibility
of preferring one candidate EA to another given suitable initial conditions. We can alleviate these issues by
assessing relative performance over each of a representative sample of initial populations.
For each particular initial population sampled, two EAs may be compared by testing the null hypothesis
of equal performance according to a speciﬁed performance metric. Student’s t-statistics are commonly
used to test the equality of two population means. However, the parametric t-test assumes that the data
are normally distributed. If this assumption is not valid, the resulting inference may not be meaningful.
Therefore, we require a more general and objective framework for statistical performance comparison of EAs.
This paper proposes a two-fold sampling scheme to perform repeated EA trials at each of a representative sample of possible inputs. The candidate EAs’ eﬃcacies are assessed in a multiple hypothesis testing
framework that relies upon bootstrap resampling to estimate the joint distribution of the test
statistics. This methodology establishes a procedure for fair comparison of EAs that can be considered
general in the following aspects. First, the results do not rely heavily on a single advantageous input. Second, the bootstrap-based testing procedure is applicable to any distribution and requires no a priori model
assumptions. Finally, this methodology can be applied to essentially any function of the data collected, so
the researcher is free to choose how performance should be evaluated.
The paper is organized as follows: Section 2 provides a brief introduction to EAs and presents a two-fold
sampling scheme for data collection.
Section 3 places performance comparison in a multiple hypothesis
testing framework. Section 4 shows how to use the bootstrap to estimate the test statistics’ underlying
distribution. Section 5 introduces a variety of multiple testing procedures. Section 6 provides an example
comparing the performance of two EAs seeking to minimize Ackley’s function. Section 7 discusses further
applications of statistics in EA performance comparison and concludes the paper.
Evolutionary Algorithms and Data Collection
An EA’s ﬁtness (or objective) function is a map f : RD →R to be optimized. Any candidate solution is
speciﬁed by an individual with a vector of genes (or traits, used interchangeably) y = (y1, . . . , yD). Each individual has a corresponding ﬁtness given by f(y). Given a population of individuals, an EA uses evolutionary
Hosted by The Berkeley Electronic Press
mechanisms to successively create oﬀspring, or new individuals. The evolutionary mechanisms often consist
of some combination of adaptation, selection, reproduction, and mutation. An adaptation process reﬁnes
the ﬁtness of existing individuals (perhaps by performing a local optimization). The selection mechanism
ranks individuals by ﬁtness, determines which individuals shall produce oﬀspring, and assigns individuals
to mating groups. Given a mating group, reproduction combines the genes of individuals within the mating
group one or more times to produce oﬀspring. Finally, the mutation mechanism randomly alters the genetic
proﬁle of oﬀspring immediately following conception.
An EA’s initial population (or input, used interchangeably) is a set of individuals that serve as starting values
for the algorithm, and its result is given by the optimum observed ﬁtness among all individuals produced
in G generations. Once the evolutionary mechanisms are speciﬁed, one ordered iteration of these processes
in sequence is considered one generation, and the evolution process proceeds for a user-speciﬁed number of
generations G ∈Z+.
An EA’s result is determined by a stochastic process with two sources of variation: the ﬁtness of the input
and the algorithm’s improvements to this ﬁtness produced by G generations of the random evolution process.
Because an EA’s result depends both on its input ﬁtness and its eﬃcacy given this initial population, a sample
of result data should be collected in a two-fold sampling scheme: we ﬁrst generate a representative sample
of initial populations, and then, for each of these inputs, we perform a number of trials of each candidate
EA. If we specify the number of generations G, the data are collected via the following algorithm:
1. Generate M initial populations of H individuals. Each individual is described by a D-dimensional
vector of genes.
The value of the dth gene of the hth individual of the mth population is labeled
ymhd. (When referring to an overall population ym or single individual ymh within that population,
the unnecessary indices will be dropped.) Populations of individuals are constructed from genes randomly generated from an associated D-dimensional distribution function P. The resulting sample of
individuals (and hence the population samples) are independent and identically distributed (i.i.d.).
2. Because an EA with a particular input follows a stochastic process, we sample results for each of
the inputs generated in Step 1.
For each initial population ym, perform na, a ∈{1, 2}, trials of
algorithm a, allowing each trial to iterate for G generations. Save the ith trial’s result as the [m, i]th
entry of an M × na data matrix Xa. (The number of generations G will be dropped from much of
the subsequent notation, but it should be emphasized that the data collected are realizations of an
experiment conducted by running an EA for G generations, and therefore this analysis is only valid
for the speciﬁed value of G.)
The values na specify the sample size, and M represents the number of hypotheses, each of which correspond
to an initial population. In general, one should collect as much data as possible given the computational
constraints of the problem.
Multiple Hypothesis Testing Framework
For any comparison, we must ﬁrst specify the theoretical parameter of interest µa(ym), which in this setting
is an EA’s measure of performance given initial population ym and the number of generations G. A typical
choice for µa(ym) is the EA’s expected result after G generations. This parameter is estimated by a statistic
ˆµa(ym), which is just a function of the observed data Xa. When the expected result is the parameter of
interest, the corresponding estimator is the sample mean:
ˆµa(ym) = 1
m = 1, . . . , M; a = 1, 2,
and the estimated variance of the result is
 
(Xa[m, i] −ˆµa(ym))2 ;
m = 1, . . . , M; a = 1, 2.
A multiple hypothesis testing framework is needed to compare algorithmic performance based on the data
collected in Section 2. The null hypothesis can take many forms depending on the researcher’s priorities.
For example, one may wish to show that a new algorithm’s expected optimal ﬁtness after G generations is
greater than that of an established standard or that its performance falls in a particular range. Typically
we wish to demonstrate that the EAs diﬀer signiﬁcantly in performance given an initial population, so a
skeptical null hypothesis would assume for each input that no diﬀerence in performance exists between the
two algorithms. This corresponds to the multiple null hypotheses
Hm : µ1(ym) −µ2(ym) = 0;
m = 1, . . . , M.
We then test (3) at multiple signiﬁcance level α (e.g. FWER 0.05 – Section 5). To do so, we must construct
test statistics and corresponding decision rules that reject the null hypotheses when the test statistics exceed
to-be-determined cut-oﬀs. We test each component null hypothesis using a two-sample t-statistic:
tm = ˆµ1(ym) −ˆµ2(ym)
m = 1, . . . , M.
In order to specify cut-oﬀs that probabilistically control a suitably deﬁned Type I error rate (Section 5),
we must estimate the underlying joint distribution of (4). When the data are assumed to follow a Normal
distribution, Student’s t-distribution is appropriate for the marginal distributions of (4). However, if this
assumption is not valid, the test statistics may not follow any mathematically simple distribution. Under
either of these circumstances, the joint distribution of (4) can be estimated using the bootstrap.
Using the Bootstrap in Hypothesis Testing
The bootstrap is a simulation-based resampling method that uses the data collected to estimate a statistic’s
distribution in a mathematically simple but computationally intensive way.
This estimate is consistent,
asymptotically eﬃcient, and does not rely upon parametric assumptions, so it is widely applicable to many
problems in statistical inference . In the setting of hypothesis testing, we can estimate the underlying
joint distribution of (4) via the following algorithm :
1. Specify a number B ∈Z+ (typically at least 10000 for multiple hypothesis testing) of bootstrap
iterations.
2. Let n = n1 + n2. Concatenate the columns of X1 and X2 to form an M × n data matrix X. For each
b ∈{1, . . . , B}, sample n columns at random with replacement from X and store this resampling in an
M × n matrix X#b.
3. For b = 1, . . . , B, compute bootstrap test statistics in a similar manner as for the original dataset X,
by applying (4) to the bootstrap dataset X#b of Step 2”. Store these values in an M × B matrix T.
The reader may refer to for further details.
4. Obtain an M × B matrix Z by shifting T about its row means and scaling by its row standard deviations for m = 1, . . . , M; b = 1, . . . , B:
Hosted by The Berkeley Electronic Press
T[m, b] −1
b=1 T[m, b]
T[m, b] −1
The estimate of (4)’s joint distribution is given by the empirical distribution of the columns of Z in (5).
For hypothesis testing applications, the bootstrap is implemented in the MTP function of the R statistical
programming environment’s multtest package .
Multiple Testing Proedures
The signiﬁcance level α, the observed test statistics tm (4), and the matrix of bootstrap test statistics Z
(5) constitute the input to a Multiple Testing Procedure (MTP). In this setting, a variety of methods that
reﬂect a diversity of attitudes toward risk are available. Statistical tests can generate two types of errors: a
Type I error (or false positive) occurs when a true null hypothesis is incorrectly rejected, and a Type II error
(or false negative) occurs when a false null is not rejected. When testing M hypotheses simultaneously, as
in (3), we deﬁne the following random variables: The number of Type I errors V , which is not observed, and
the number of rejected hypotheses R, which is observed. Classical MTPs seek to control the Family-Wise
Error Rate (FWER). More recent research has been developed to control the generalized Family-Wise Error
Rate (gFWER), False Discovery Rate (FDR), and the Tail Probability for the Proportion of False Positives
(TPPFP), which are deﬁned in Table 1.
Type I Error Rate
Parameter Controlled
Pr(V/R > q)
Table 1: Type I error rates.
As described in , Table 2 lists a selection of available MTPs for each Type I error rate. The
results of a multiple hypothesis test can be summarized in terms of rejection regions for the test statistics,
conﬁdence regions for the parameters of interest, and adjusted p-values . The rejection region provides a
set of values for which each hypothesis Hm of (3) is rejected while controlling the desired Type I error rate
at level α.
The adjusted p-value for null hypothesis Hm is deﬁned as the minimum value of the Type I error level α for
which Hm is rejected. Adjusted p-values from diﬀerent MTPs controlling the same Type I error rate may
be directly compared, with smaller values reﬂecting a less conservative test . The MTPs of Table 2 are
implemented in the MTP function of the R multtest package . The user need only supply the data,
the value of α, the form of the null hypothesis, the test statistic, the Type I error rate to control, and select
 
Type I Error Rate
Multiple Testing Procedures
Single Step (SS) max T, SS minP, Step Down (SD) maxT,
SD minP, Bonferroni, Holm, Hochberg, SS ˇSid´ak, SD ˇSid´ak
Augmentation Procedure (AMTP), SS Common Cut-oﬀ,
SS Common Quantile, Empirical Bayes
Conservative Augmentation, Restrictive Augmentation,
Benjamini-Yekutieli (BY), Benjamini-Hochberg (BH)
AMTP, Empirical Bayes
Table 2: MTPs by Type I error rate.
Example: Ackley’s Function Minimization
Deﬁning Ackley’s Function
We seek to compare two candidate EAs that approximate the minimum of a D = 10-dimensional Ackley
function . With ymh = (ymh1, . . . , ymhD) as in Section 2, Ackley’s multi-modal function, which achieves a
known minimum at the origin, is deﬁned as:
f(ymh) = −c1 exp
cos (c3ymhd)
+ c1 + exp(1)
with the following parameters supplied for this example:
c1 = 20, c2 = 0.2, c3 = 2π, D = 10, ymhd ∈(−20, 30).
Candidate EAs Ackley1 and Ackley2
The algorithms Ackley1 and Ackley2 were devised to estimate the minimum of (6). Each EA takes an input
population ym as described in Section 2. Each individual ymh of this population has associated ﬁtness f(ymh)
given by (6). At each generation, both algorithms include a selection, reproduction, and mutation phase; no
adaptation mechanism was used. Ackley1 and Ackley2 diﬀer only in the choice of the mutation rate and are
otherwise identical algorithms. The evolutionary mechanisms of these EAs are as follows:
Selection: For simplicity, the population size H is assumed to be a multiple of 4, though this method can
be generalized with ﬂoor and ceiling operators for other values of H. Sort and re-label the H individuals in
order of increasing f(ymh), h = 1, . . . , H. The H/2 best-ﬁt individuals – those with the smallest values of (6)
– are selected for reproduction, while the other members will not breed. For h = 1, . . . , H/4, pair individuals
y[m(2h−1)] and y[m(2h)] for mating. Although selection is the last phase of a generation, it is presented ﬁrst
because the initialization process that creates the 0th generation requires selection before the ﬁrst generation
of the evolution process may commence.
Reproduction: Selection in the previous generation pairs individuals y[m(2h−1)] and y[m(2h)], h = 1, . . . , H/4,
for mating. Each pair produces two oﬀspring to replace individuals not selected. For the ﬁrst child (c = 1), a
uniform random variable λ1 is generated on (0, 1), and the second child (c = 2) receives λ2 = 1 −λ1. Genes
are inherited (vector-wise) by the weighted average
y[m(H/2+2(h−1)+c)] = λcy[m(2h−1)] + (1 −λc) y[m(2h)].
Mutation: Each oﬀspring y[H/2+1], . . . , yH may randomly mutate in a single gene at birth with probability
θa. When mutation occurs, the gene is selected from a uniform random variable on {1, . . . , D}, and this trait
Hosted by The Berkeley Electronic Press
is assigned a uniform random variable on (−20, 30). In this example, mutation probabilities for Ackley1 and
Ackley2 are θ1 = 0.1 and θ2 = 0.8, respectively. Because only one of an individual’s D = 10 genes may
mutate, the expected proportions of mutating genes in Ackley1 and Ackley2 are 0.01 and 0.08.
Except for the mutation probability, Ackley1 and Ackley2 are identical EAs.
The initial population is
considered the completion of the reproduction and mutation phases for the 0th generation, and the ﬁrst
generation begins after selection of the initial population.
The process of reproduction, mutation, and
selection repeats a total of G generations, and the EAs’ results are given by
h∈{1,...,H} f(ymh)
The value of (8) observed for EA a after G generations on the ith trial given initial population ym is stored
as the [m, i]th entry of the data matrix Xa. Because the reproduction and mutation phases have random
components at each generation, the value Xa[m, i] is a random variable.
It should be noted that Ackley1 and Ackley2 were designed solely to provide an example of our comparison
methodology. Diﬀerent population sizes, reproduction schemes, or mutation rates may lead to improved
estimates of (6)’s minimum.
Study Design and Results
Using the two-fold sampling scheme of Section 2, we generated M = 100 initial populations y1, . . . , yM,
each consisting of H = 100 individuals with D = 10 genes apiece. Each individual’s traits were initialized
using pseudo-random number generation from a uniform distribution on the interval (-20,20). It should be
noted that subsequent mutations allowed genes to take any value in (-20,30), so only mutant genes and their
oﬀspring can reach the interval [20,30). The function (6) was used to assess each individual’s ﬁtness. Then,
for each initial population m = 1, . . . , M, we collected result data on n1 = n2 = 50 trials of the EAs. On
each trial, both Ackley1 and Ackley2 were allowed to evolve for G = 10000 generations.
The data for the Ackley1 and Ackley2 trials are displayed in Figure 1 as a function of initial population
index. Figure 2 shows the average performance of the EAs for each initial population. Though Ackley2
produces a better (i.e. smaller) mean value of (8) than Ackley1 at each initial population, Figure 1 shows
that Ackley1 is capable of producing competitive results for some trials across all inputs. Furthermore,
Ackley1 appears to exhibit greater variance than Ackley2 in its estimates. Therefore, it is not immediately
clear that Ackley2 does indeed perform better than Ackley1.
We conducted two-sided tests of the multiple null hypotheses (3) corresponding to no diﬀerence in mean
performance between Ackley1 and Ackley2 at each given input versus the alternative of unequal mean performance. Note that one could also perform one-sided tests that designate one candidate EA as superior to
the other in the null hypothesis.
The hypotheses (3) were tested using the multtest package of R based on the data collected and the
test statistic (4). We ﬁrst employed the FWER-controlling SS maxT MTP at nominal level α = 0.05. Figure
3 shows several summary plots of the SS maxT results. The ﬁrst plot shows how the number of rejected
hypotheses R grows as a function of α. The second plot shows the ordered SS maxT adjusted p-values.
This curve indicates that 92 hypotheses are rejected at level α = 0.05. The third plot shows how the SS
maxT adjusted p-values decrease with the absolute value of the test statistics. Here the adjusted p-values
approach 0.05 as the test statistics increase toward -2.75. The ﬁnal plot of Figure 3 shows the unordered SS
maxT adjusted p-values, which allow one to identify the initial populations that result in signiﬁcant (< 0.05)
performance diﬀerences.
 
Initial Population
Figure 1: Fitness data for Ackley1 and Ackley2 trials by initial population.
Initial Population
Average Fitness
Average Performance by Initial Population
Figure 2: Average ﬁtness of Ackley1 and Ackley2 by initial population.
Hosted by The Berkeley Electronic Press
Figure 3: Summary displays for SS maxT testing.
 
We then implemented a selection of the MTPs listed in Table 2 to test (3) under diﬀerent Type I error rates.
Table 3 displays the number of hypotheses rejected by each MTP at varying Type I error levels α. The
following procedures reject all 100 hypotheses at level α = 0.05: Holm, Hochberg, SD ˇSid´ak, Benjamini-
Yekutieli, and Benjamini-Hochberg.
For the gFWER and TPPFP-controlling augmentation procedures, the question remains whether the allowed number k or rate q of false positives is tolerable in testing EA performance diﬀerences. This question
is epistemological in nature and must be decided by subject matter specialists. In practice, a maximum
value for these parameters should be established before comparison takes place. Although the particular
benchmark is somewhat arbitrary (much like the choice of α = 0.05 in hypothesis testing), establishing a
uniform standard is necessary for future studies.
The results of the test of (3) suggest a performance diﬀerence between Ackley1 and Ackley2. On each of
the M = 100 sample input populations, Ackley2 achieved a smaller average observed minimum. All MTPs
rejected at least 86 of the M = 100 hypotheses at level α = 0.05, and a number of procedures rejected
all hypotheses at level α = 0.01. Therefore, based upon the data collected, we conclude that Ackley2 signiﬁcantly outperforms Ackley1 in estimating the minimum of (6) when the expected result obtained after
G generations of evolution is the parameter of interest. Because the two algorithms only diﬀered in their
mutation probabilities, it appears that increased mutation is beneﬁcial in this application.
Bonferroni
SS ˇSid´ak
SD ˇSid´ak
Conservative
Restricted
Table 3: The number of rejected hypotheses R as a function of α for a selection of MTPs.
Discussion
This paper’s methodology provides a general approach to EA performance comparison. The proposed framework allows the researcher to choose the parameter of interest in an EA comparison. When parameters other
than the expected optimum ﬁtness are used (such as the median, 75th percentile, or other quantiles), our
Hosted by The Berkeley Electronic Press
methodology is applicable provided that the necessary data are collected and appropriate estimators (1), null
hypotheses (3), and test statistics (4) are chosen. In crafting an EA for a particular optimization problem,
this paper’s approach can be used iteratively to select the best among a set of candidate parameter values
for quantities such as the mutation rate, population size, and selection proportion. When three or more EAs
are simultaneously compared, null hypotheses of equality in means may be tested using F-statistics.
For illustration purposes, we considered an example in Section 6 involving a simple objective function (6),
measure of performance µa(ym), and sampling scheme based upon i.i.d. inputs. However, this methodology is applicable for general choices of the objective function, parameters of interest, sampling scheme, null
hypotheses, test statistics, and number of algorithms to compare. Furthermore, although this paper studies
performance comparison within the ﬁeld of evolutionary computation, the general framework can be applied
to essentially any random numeric optimization routine.
The reader should be cautioned that issues of sample size cannot be neglected. In particular, the bootstrap
approximation of (4)’s joint distribution grows more accurate as the values B and na increase. In practice,
researchers may choose to collect as much data as a pre-speciﬁed time limit will allow. Data-adaptive study
designs may also be implemented to halt data collection once a pre-speciﬁed level of statistical power is
If competing algorithms draw from diﬀerent input sets, then the test of a single hypothesis (M = 1) concerning average results from representative input samples may be considered. When the input sets are identical,
an alternative to the approach of this paper may choose to average all trials in a single hypothesis test
provided that all inputs are i.i.d. The choice of which approach to use is philosophical: this paper assumes
that EAs should be compared using the same input sample. In this setting, the parameter of interest is
the expected result obtained in G generations given the initial population. This allows the algorithm to
be assessed solely on the merits of its evolutionary mechanisms without any possibility of a founder eﬀect.
However, if one views the input generation and resulting evolution as inextricably linked in the same algorithm, then a single hypothesis testing framework may be more appropriate, and this paper’s methodology
is otherwise applicable. In this scenario, the parameter of interest shifts to the unconditional expectation of
performance. Though a single test may simplify the interpretation of performance diﬀerences, this approach
lacks the appeal of direct performance comparison on the same trial inputs.
The researcher may also wish to compare EAs as a function of time by collecting data at regular generational
intervals. Displaying performance curves and conﬁdence regions graphically may allow one to quickly determine decision criteria and search for clues about an algorithm’s rate of convergence and asymptotic result.
Finally, an EA’s eﬃcacy should be considered in terms of both performance and computational complexity.
Researchers may consider performing a comparison in which each candidate algorithm is allowed to run for
the same amount of time instead of the same number of generations to satisfy both objectives simultaneously.
Acknowledgment
The authors gratefully acknowledge Frances Tong for her helpful comments and suggestions during the
process of preparing this manuscript.