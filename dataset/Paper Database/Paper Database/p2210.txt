HAL Id: hal-01113521
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
On Anomaly Ranking and Excess-Mass Curves
Nicolas Goix, Anne Sabourin, Stéphan Clémençon
To cite this version:
Nicolas Goix, Anne Sabourin, Stéphan Clémençon. On Anomaly Ranking and Excess-Mass Curves.
2014. ￿hal-01113521￿
On Anomaly Ranking and Excess-Mass Curves
Nicolas Goix
Anne Sabourin
St´ephan Cl´emen¸con
UMR LTCI No. 5141
Telecom ParisTech/CNRS
Institut Mines-Telecom
Paris, 75013, France
UMR LTCI No. 5141
Telecom ParisTech/CNRS
Institut Mines-Telecom
Paris, 75013, France
UMR LTCI No. 5141
Telecom ParisTech/CNRS
Institut Mines-Telecom
Paris, 75013, France
Learning how to rank multivariate unlabeled
observations depending on their degree of abnormality/novelty is a crucial problem in a
wide range of applications.
In practice, it
generally consists in building a real valued
”scoring” function on the feature space so
as to quantify to which extent observations
should be considered as abnormal. In the 1d situation, measurements are generally considered as ”abnormal” when they are remote
from central measures such as the mean or
the median. Anomaly detection then relies
on tail analysis of the variable of interest.
Extensions to the multivariate setting are far
from straightforward and it is precisely the
main purpose of this paper to introduce a
novel and convenient (functional) criterion
for measuring the performance of a scoring
function regarding the anomaly ranking task,
referred to as the Excess-Mass curve (EM
In addition, an adaptive algorithm
for building a scoring function based on unlabeled data X1, . . . , Xn with a nearly optimal EM is proposed and is analyzed from a
statistical perspective.
Introduction
In a great variety of applications (e.g.
fraud detection, distributed ﬂeet monitoring, system management
in data centers), it is of crucial importance to address anomaly/novelty issues from a ranking point of
view. In contrast to novelty/anomaly detection (e.g.
Appearing in Proceedings of the 18th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)
2015, San Diego, CA, USA.
JMLR: W&CP volume 38.
Copyright 2015 by the authors.
 ), novelty/anomaly ranking is very poorly
documented in the statistical learning literature (see
 for instance).
However, when confronted with
massive data, being enable to rank observations according to their supposed degree of abnormality may
signiﬁcantly improve operational processes and allow
for a prioritization of actions to be taken, especially
in situations where human expertise required to check
each observation is time-consuming.
When univariate, observations are usually considered as ”abnormal”
when they are either too high or else too small compared to central measures such as the mean or the median. In this context, anomaly/novelty analysis generally relies on the analysis of the tail distribution of
the variable of interest. No natural (pre-) order exists on a d-dimensional feature space, X ⊂Rd say, as
soon as d > 1. Extension to the multivariate setup
is thus far from obvious and, in practice, the optimal ordering/ranking must be learned from training
data X1, . . . , Xn, in absence of any parametric assumptions on the underlying probability distribution
describing the ”normal” regime. The most straightforward manner to deﬁne a preorder on the feature space
X is to transport the natural order on the real half-line
through a measurable scoring function s : X →R+:
the ”smaller” the score s(X), the more ”abnormal” the
observation X is viewed. Any scoring function deﬁnes
a preorder on X and thus a ranking on a set of new
observations.
An important issue thus concerns the
deﬁnition of an adequate performance criterion, C(s)
say, in order to compare possible candidate scoring
function and to pick one eventually: optimal scoring
functions s∗being then deﬁned as those optimizing C.
Throughout the present article, it is assumed that the
distribution F of the observable r.v. X is absolutely
continuous w.r.t. Lebesgue measure Leb on X, with
density f(x). The criterion should be thus deﬁned in
a way that the collection of level sets of an optimal
scoring function s∗(x) coincides with that related to
In other words, any nondecreasing transform of
the density should be optimal regarding the ranking
On Anomaly Ranking and Excess-Mass Curves
performance criterion C. According to the Empirical
Risk Minimization (ERM) paradigm, a scoring function will be built in practice by optimizing an empirical version Cn(s) of the criterion over an adequate
set of scoring functions S0 of controlled complexity
(e.g. a major class of ﬁnite VC dimension). Hence,
another desirable property to guarantee the universal
consistency of ERM learning strategies is the uniform
convergence of Cn(s) to C(s) over such collections S0
under minimal assumptions on the distribution F(dx).
In , a functional criterion referred to as the Mass-
Volume (MV) curve, admissible with respect to the
requirements listed above has been introduced, extending somehow the concept of ROC curve in the
unsupervised setup. Relying on the theory of minimum volume sets (see e.g. and the references
therein), it has been proved that the scoring functions
minimizing empirical and discretized versions of the
MV curve criterion are accurate when the underlying
distribution has compact support and a ﬁrst algorithm
for building nearly optimal scoring functions, based on
the estimate of a ﬁnite collection of properly chosen
minimum volume sets, has been introduced and analyzed. However, by construction, learning rate bounds
are rather slow (of the order n−1/4 namely) and cannot
be established in the unbounded support situation, unless very restrictive assumptions are made on the tail
behavior of F(dx). See Figure 3 and related comments
for an insight into the gain resulting from the concept
introduced in the present paper in contrast to the MV
curve minimization approach.
Given these limitations, it is the major goal of this
paper to propose an alternative criterion for anomaly
ranking/scoring, called the Excess-Mass curve (EM
curve in short) here, based on the notion of density
contour clusters . Whereas minimum volume
sets are solutions of volume minimization problems under mass constraints, the latter are solutions of mass
maximization under volume constraints. Exchanging
this way objective and constraint, the relevance of this
performance measure is thoroughly discussed and accuracy of solutions which optimize statistical counterparts of this criterion is investigated. More speciﬁcally,
rate bounds of the order n−1/2 are proved, even in the
case of unbounded support. Additionally, in contrast
to the analysis carried out in , the model bias issue
is tackled, insofar as the assumption that the level sets
of the underlying density f(x) belongs to the class of
sets used to build the scoring function is relaxed here.
The rest of this paper is organized as follows. Section 3
introduces the notion of EM curve and that of optimal
EM curve. Estimation in the compact support case is
covered by section 4, extension to distributions with
non compact support and control of the model bias are
tackled in section 5. A simulation study is performed
in section 6. All proofs are deferred to the Appendix
Background and related work
As a ﬁrst go, we ﬁrst provide a brief overview of the
scoring approach based on the MV curve criterion,
as a basis for comparison with that promoted in the
present paper.
Here and throughout, the indicator function of any
event E is denoted by 1E, the Dirac mass at any point
x by δx, A∆B the symmetric diﬀerence between two
sets A and B and by S the set of all scoring functions
s : X →R+ integrable w.r.t Lebesgue measure. Let
s ∈S. As deﬁned in , the MV-curve of s is the
plot of the mapping α ∈(0, 1) 7→MVs(α) = λs ◦
s (α), where αs(t) = P(s(X) ≥t), λs(t) = Leb({x ∈
X, s(x) ≥t}) and H−1 denotes the pseudo-inverse of
any cdf H : R →(0, 1). This induces a partial ordering
on the set of all scoring functions: s is preferred to s′
if MVs(α) ≤MVs′(α) for all α ∈(0, 1).
show that MV∗(α) ≤MVs(α) for all α ∈(0, 1) and
any scoring function s, where MV ∗(α) is the optimal
value of the constrained minimization problem
Γ borelian Leb(Γ) subject to P(X ∈Γ) ≥α.
Suppose now that F(dx) has a density f(x) satisfying
the following assumptions:
A1 The density f is bounded, i.e. ||f(X)||∞< +∞.
A2 The density f has no ﬂat parts: ∀c ≥0, P{f(X) =
c} = 0 . One may then show that the curve MV∗is
actually a MV curve, that is related to (any increasing
transform of) the density f namely: MV∗= MVf. In
addition, the minimization problem (1) has a unique
solution Γ∗
α of mass α exactly, referred to as minimum volume set (see ): MV∗(α) = Leb(Γ∗
Anomaly scoring can be then viewed
as the problem of building a scoring function s(x)
based on training data such that MVs is (nearly) minimum everywhere, i.e. minimizing ∥MVs−MV∗∥∞
supα∈ |MVs(α)−MV∗(α)|. Since F is unknown, a
minimum volume set estimate bΓ∗
α can be deﬁned as the
solution of (1) when F is replaced by its empirical version Fn = (1/n) Pn
i=1 δXi, minimization is restricted
to a collection G of borelian subsets of X supposed not
too complex but rich enough to include all density level
sets (or reasonable approximants of the latter) and α
is replaced by α −φn, where the tolerance parameter
φn is a probabilistic upper bound for the supremum
supΓ∈G |Fn(Γ)−F(Γ)|. Refer to for further details.
The set G should ideally oﬀer statistical and computational advantages both at the same time. Allowing
Nicolas Goix, Anne Sabourin, St´ephan Cl´emen¸con
for fast search on the one hand and being suﬃciently
complex to capture the geometry of target density level
sets on the other. In , a method consisting in preliminarily estimating a collection of minimum volume
sets related to target masses 0 < α1 < . . . < αK < 1
forming a subdivision of (0, 1) based on training data
so as to build a scoring function s = P
been proposed and analyzed. Under adequate assumptions (related to G, the perimeter of the Γ∗
αk’s and the
subdivision step in particular) and for an appropriate
choice of K = Kn either under the very restrictive assumption that F(dx) is compactly supported or else
by restricting the convergence analysis to [0, 1 −ǫ] for
ǫ > 0, excluding thus the tail behavior of the distribution F from the scope of the analysis, rate bounds of
the order OP(n−1/4) have been established to guarantee the generalization ability of the method.
Figure 3 illustrates the problems inherent to the use of
the MV curve as a performance criterion for anomaly
scoring in a ”non asymptotic” context, due to the prior
discretization along the mass-axis. In the 2-d situation
described by Fig. 3 for instance, given the training
sample and the partition of the feature space depicted,
the MV criterion leads to consider the sequence of
empirical minimum volume sets A1, A1 ∪A2, A1 ∪
A3, A1∪A2∪A3 and thus the scoring function s1(x) =
I{x ∈A1}+I{x ∈A1 ∪A2}+I{x ∈A1 ∪A3}, whereas
the scoring function s2(x) = I{x ∈A1} + I{x ∈A1 ∪
A3} is clearly more accurate.
In this paper, a diﬀerent functional criterion is proposed, obtained by exchanging objective and constraint functions in (1), and it is shown that optimization of an empirical discretized version of this performance measure yields scoring rules with convergence
rates of the order OP(1/√n). In addition, the results
can be extended to the situation where the support of
the distribution F is not compact.
The Excess-Mass curve
The performance criterion we propose in order to evaluate anomaly scoring accuracy relies on the notion
of excess mass and density contour clusters, as introduced in the seminal contribution . The main idea is
to consider a Lagrangian formulation of a constrained
minimization problem, obtained by exchanging constraint and objective in (1): for t > 0,
Ωborelian {P(X ∈Ω) −tLeb(Ω)} .
We denote by Ω∗
t any solution of this problem.
shall be seen in the subsequent analysis (see Proposition 3 below), compared to the MV curve approach,
this formulation oﬀers certain computational and theoretical advantages both at the same time: when letting
(a discretized version of) the Lagrangian multiplier t
increase from 0 to inﬁnity, one may easily obtain solutions of empirical counterparts of (2) forming a nested
sequence of subsets of the feature space, avoiding thus
deteriorating rate bounds by transforming the empirical solutions so as to force monotonicity.
Deﬁnition 1. (Optimal EM curve) The optimal
Excess-Mass curve related to a given probability distribution F(dx) is deﬁned as the plot of the mapping
t > 0 7→EM∗(t)
Ωborelian{P(X ∈Ω) −tLeb(Ω)}.
EM ∗(t) = P(X ∈Ω∗
t ) −tLeb(Ω∗
t ) for all t > 0.
Notice also that EM∗(t) = 0 for any t > ∥f∥∞
supx∈X |f(x)|.
Corresponding
distributions f :
ﬁnite support
ﬁnite support
inﬁnite support
heavy tailed
Figure 1: EM curves depending on densities
Lemma 1. (On existence and uniqueness) For
any subset Ω∗
t solution of (2), we have
{x, f(x) > t} ⊂Ω∗
t ⊂{x, f(x) ≥t}almost-everywhere,
and the sets {x, f(x) > t} and {x, f(x) ≥t} are both
solutions of (2). In addition, under assumption A2,
Figure 2: Comparison between MV ∗(α) and EM ∗(t)
On Anomaly Ranking and Excess-Mass Curves
the solution is unique:
t = {x, f(x) > t} = {x, f(x) ≥t}.
Observe that the curve EM∗is always well-deﬁned,
f≥t(f(x) −t)dx =
f>t(f(x) −t)dx. We also
point out that EM∗(t) = α(t) −tλ(t) for all t > 0,
where we set α = αf and λ = λf.
Proposition 1. (Derivative and convexity of
EM∗) Suppose that assumptions A1 and A2 are full-
ﬁlled. Then, the mapping EM∗is diﬀerentiable and
we have for all t > 0:
EM∗′(t) = −λ(t).
In addition, the mapping t > 0 7→λ(t) being decreasing, the curve EM ∗is convex.
We now introduce the concept of Excess-Mass curve
of a scoring function s ∈S.
Deﬁnition 2. (EM curves) The EM curve of s ∈S
w.r.t. the probability distribution F(dx) of a random
variable X is the plot of the mapping
EMs : t ∈[0, ∞[7→
A∈{(Ωs,l)l>0}
P(X ∈A) −tLeb(A),
where Ωs,t = {x ∈X, s(x) ≥t} for all t > 0. One may
also write: ∀t > 0, EMs(t) = supu>0 αs(u) −tλs(u).
Finally, under assumption A1, we have EMs(t) = 0
for every t > ∥f∥∞.
Regarding anomaly scoring, the concept of EM curve
naturally induces a partial order on the set of all scoring functions: ∀(s1, s2) ∈S2, s1 is said to be more
accurate than s2 when ∀t > 0, EMs1(t) ≥EMs2(t).
Observe also that the optimal EM curve introduced
in Deﬁnition 1 is itself the EM curve of a scoring function, the EM curve of any strictly increasing transform of the density f namely: EM∗= EMf. Hence,
in the unsupervised framework, optimal scoring functions are those maximizing the EM curve everywhere.
In addition, maximizing EMs can be viewed as recovering a collection of subsets (Ω∗
t )t>0 with maximum
mass when penalized by their volume in a linear fashion. An optimal scoring function is then any s ∈S
with the Ω∗
t ’s as level sets, for instance any scoring
function of the form
with a(t) > 0 (observe that s(x) = f(x) for a ≡1).
Proposition 2. (Nature of anomaly scoring)
Let s ∈S. The following properties hold true.
(i) The mapping EMs is non increasing on (0, +∞),
takes its values in and satisﬁes, EMs(t) ≤
EM∗(t) for all t ≥0.
(ii) For t ≥0, we have: 0 ≤EM∗(t) −EMs(t) ≤
∥f∥∞infu>0 Leb({s > u}∆{f > t}).
f −1({u}) 1/∥∇f(x)∥
dµ(x) is bounded,
where µ denotes the (d −1)-dimensional Hausdorﬀmeasure. Set ǫ1 := infT ∥f −T ◦s∥∞, where
the inﬁmum is taken over the set T of all borelian
increasing transforms T : R+ →R+. Then
t∈[ǫ+ǫ1,∥f∥∞]
|EM∗(t) −EMs(t)|
T ∈T ∥f −T ◦s∥∞
where C1 = C(ǫ1, f) is a constant independent
from s(x).
Assertion (ii) provides a control of the pointwise difference between the optimal EM curve and EMs in
terms of the error made when recovering a speciﬁc
minimum volume set Ω∗
t by a level set of s(x). Assertion (iii) reveals that, if a certain increasing transform of a given scoring function s(x) approximates
well the density f(x), then s(x) is an accurate scoring function w.r.t. the EM criterion. As the distribution F(dx) is generally unknown, EM curves must
be estimated.
Let s ∈S and X1, . . . , Xn be an
i.i.d. sample with common distribution F(dx) and set
bαs(t) = (1/n) Pn
i=1 1s(Xi)≥t. The empirical EM curve
of s is then deﬁned as
EMs(t) = sup
{bαs(u) −tλs(u)} .
In practice, it may be diﬃcult to estimate the volume
λs(u) and Monte-Carlo approximation can naturally
be used for this purpose.
A general approach to learn a
scoring function
The concept of EM-curve provides a simple way to
compare scoring functions but optimizing such a functional criterion is far from straightforward. As in ,
we propose to discretize the continuum of optimization problems and to construct a nearly optimal scoring function with level sets built by solving a ﬁnite
collection of empirical versions of problem (2) over a
subclass G of borelian subsets. In order to analyze the
accuracy of this approach, we introduce the following
additional assumptions.
A3 All minimum volume sets belong to G:
∀t > 0, Ω∗
Nicolas Goix, Anne Sabourin, St´ephan Cl´emen¸con
A4 The Rademacher average
is of order OP(n−1/2), where (ǫi)i≥1 is a Rademacher
chaos independent of the Xi’s.
Assumption A4 is very general and is fulﬁlled in particular when G is of ﬁnite VC dimension, see , whereas
the zero bias assumption A3 is in contrast very restrictive. It will be relaxed in section 5.
Let δ ∈(0, 1) and consider the complexity penalty
Φn(δ) = 2Rn +
. We have for all n ≥1:
(|P(G) −Pn(G)| −Φn(δ)) > 0
see for instance. Denote by Fn = (1/n) Pn
the empirical measure based on the training sample
X1, . . . , Xn. For t ≥0, deﬁne also the signed measures:
Ht( · ) = F( · ) −tLeb( · )
Hn,t( · ) = Fn( · ) −tLeb( · ).
Equipped with these notations, for any s ∈S, we point
out that one may write EM∗(t) = supu≥0 Ht({x ∈
u}) and EMs(t)
supu≥0 Ht({x
X, s(x) ≥u}).
Let K > 0 and 0 < tK < tK−1 <
. . . < t1. For k in {1, . . . , K}, let ˆΩtk be an empirical
tk-cluster, that is to say a borelian subset of X such
ˆΩtk ∈arg max
Ω∈G Hn,tk(Ω).
Hn,tk(ˆΩtk).
The following result reveals the bene-
ﬁt of viewing density level sets as solutions of (2)
rather than solutions of (1) (corresponding to a different parametrization of the thresholds).
Proposition
3. (Monotonicity) For any k in
{1, . . . , K}, the subsets ∪i≤k ˆΩti and ∩i≥k ˆΩti are still
empirical tk-clusters, just like ˆΩtk:
Hn,tk(∪i≤k ˆΩti) = Hn,tk(∩i≥k ˆΩti) = Hn,tk(ˆΩtk).
The result above shows that monotonous (regarding
the inclusion) collections of empirical clusters can always be built. Coming back to the example depicted
3, as t decreases, the ˆΩt’s are successively
equal to A1, A1 ∪A3, and A1 ∪A3 ∪A2, and are thus
monotone as expected. This way, one fully avoids the
problem inherent to the prior speciﬁcation of a subdivision of the mass-axis in the MV-curve minimization
approach (see the discussion in section 2).
Consider an increasing sequence of empirical tk clusters (ˆΩtk)1≤k≤K and a scoring function s ∈S of the
ak1x∈ˆΩtk ,
where ak > 0 for every k ∈{1, . . . , K}. Notice that
the scoring function (6) can be seen as a Riemann sum
approximation of (4) when ak = a(tk) −a(tk+1). For
simplicity solely, we take ak = tk −tk+1 so that the
ˆΩtk’s are tk-level sets of sK, i.e ˆΩtk = {s ≥tk} and
{s ≥t} = ˆΩtk if t ∈]tk+1, tk]. Observe that the results
established in this paper remain true for other choices.
In the asymptotic framework considered in the subsequent analysis, it is stipulated that K = Kn →∞as
n →+∞. We assume in addition that P∞
k=1 ak < ∞.
Remark 1. (Nested sequences) For L ≤K, we
have {ΩsL,l, l ≥0} = (ˆΩtk)0≤k≤L ⊂(ˆΩtk)0≤k≤K =
{ΩsK,l, l ≥0}, so that by deﬁnition, EMsL ≤EMsK.
Remark 2. (Related work) We point out that a
very similar result is proved in (see Lemma 2.2
therein) concerning the Lebesgue measure of the symmetric diﬀerences of density clusters.
Remark 3. (Alternative construction) It is
noteworthy that, in practice, one may solve the optimization problems ˜Ωtk ∈arg maxΩ∈G Hn,tk(Ω) and
next form ˆΩtk = ∪i≤k ˜Ωti.
The following theorem provides rate bounds describing
the performance of the scoring function sK thus built
with respect to the EM curve criterion in the case
where the density f has compact support.
Theorem 1. (Compact support case) Assume
that conditions A1, A2, A3 and A4 hold true, and
that f has a compact support.
Let δ ∈]0, 1[, let
(tk)k∈{1, ..., K} be such that sup1≤k<K(tk −tk+1) =
O(1/√n). Then, there exists a constant A independent from the tk’s, n and δ such that, with probability
at least 1 −δ, we have:
|EM∗(t) −EMsK(t)|
2 log(1/δ) + Leb(suppf)
Remark 4. (Localization) The problem tackled in
this paper is that of scoring anomalies, which correspond to observations lying outside of ”large” excess
mass sets, namely density clusters with parameter t
close to zero.
It is thus essential to establish rate
bounds for the quantity supt∈]0,C[ |EM∗(t)−EMsK(t)|,
where C > 0 depends on the proportion of the ”least
normal” data we want to score/rank.
On Anomaly Ranking and Excess-Mass Curves
Extensions - Further results
This section is devoted to extend the results of the
previous one. We ﬁrst relax the compact support assumption and next the one stipulating that all density
level sets belong to the class G, namely A3.
Distributions with non compact support
It is the purpose of this section to show that the algorithm detailed below produces a scoring function s
such that EMs is uniformly close to EM ∗(Theorem
2). See Figure 3 as an illustration and a comparaison
with the MV formulation as used as a way to recover
empirical minimum volume set ˆΓα .
Algorithm 1. Suppose that assumptions A1, A2,
A3, A4 hold true. Let t1 such that maxΩ∈G Hn,t1(Ω) ≥
0. Fix N > 0. For k = 1, . . . , N,
1. Find ˜Ωtk ∈arg maxΩ∈G Hn,tk(Ω) ,
2. Deﬁne ˆΩtk = ∪i≤k ˜Ωti
3. Set tk+1 =
√n )k for k ≤N −1.
In order to reduce the complexity, we may replace steps
1 and 2 with ˆΩtk ∈arg maxΩ⊃ˆΩtk−1 Hn,tk(Ω).
resulting piecewise constant scoring function is
(tk −tk+1)1x∈ˆΩtk .
n1, n2, n3 = 10, 9, 1
Sample of n = 20 points in a 2-d space, partitioned into three rectangles. As α increases, the minimum
volume sets ˆΓα are successively equal to A1, A1∪A2, A1∪
A3, and A1∪A3∪A2, whereas, in the EM-approach, as t decreases, the ˆΩt’s are successively equal to A1, A1∪A3, and
A1 ∪A3 ∪A2.
The main argument to extend the above results to the
case where suppf is not bounded is given in Lemma 2
in the ”Technical Details” section. The meshgrid (tk)
must be chosen adaptively, in a data-driven fashion.
Let h : R∗
+ →R+ be a decreasing function such that
limt→0 h(t) = +∞. Just like the previous approach,
the grid is described by a decreasing sequence (tk). Let
t1 ≥0, N > 0 and deﬁne recursively t1 > t2 > . . . >
tN > tN+1 = 0, as well as ˆΩt1, . . . , ˆΩtN , through
tk+1 = tk −(√n)−1
ˆΩtk = arg max
Ω∈G Hn,tk(Ω),
with the property that ˆΩtk+1 ⊃ˆΩtk. As pointed out
in Remark 3, it suﬃces to take ˆΩtk+1 = ˜Ωtk+1 ∪ˆΩtk,
where ˜Ωtk+1 = arg maxΩ∈G Hn,tk(Ω). This yields the
scoring function sN deﬁned by (7) such that by virtue
of Lemma 2 (see the Technical Deails), with probability at least 1 −δ,
|EM ∗(t) −EMsN (t)|
2 log(1/δ) +
Therefore, if we take h such that λ(t) = O(h(t)) as
t →0, we can assume that λ(t)/h(t) ≤B for t in
]0, t1] since λ is decreasing, and we obtain:
|EM ∗(t) −EMsN (t)|
2 log(1/δ)
On the other hand from tLeb({f > t}) ≤
we have λ(t) ≤1/t. Thus h can be chosen as h(t) :=
1/t for t ∈]0, t1]. In this case, (9) yields, for k ≥2,
Theorem 2. (Unbounded support case) Suppose
that assumptions A1, A2, A3, A4 hold true, let t1 > 0
and for k ≥2, consider tk as deﬁned by (11), Ωtk by
(8), and sN (7). Then there is a constant A independent from N, n and δ such that, with probability larger
than 1 −δ, we have:
|EM ∗(t) −EMsN (t)|
2 log(1/δ)
√n + oN(1),
where oN(1) = 1 −EM ∗(tN). In addition, sN(x) converges to s∞(x) := P∞
k=1(tk+1 −tk)1ˆΩtk+1 as N →∞
and s∞is such that, for all δ ∈(0, 1), we have with
probability at least 1 −δ:
|EM ∗(t) −EMs∞(t)| ≤
2 log(1/δ)
Nicolas Goix, Anne Sabourin, St´ephan Cl´emen¸con
Bias analysis
In this subsection, we relax assumption A3. For any
collection C of subsets of Rd, σ(C) denotes here the
σ-algebra generated by C.
Consider the hypothesis
˜A3 There exists a countable subcollection of G, F =
{Fi}i≥1 say, forming a partition of X and such that
Denote by fF the best approximation (for the L1norm) of f by piecewise functions on F,
Then, variants of Theorems 1 and 2 can be established
without assumption A3, as soon as ˜A3 holds true, at
the price of the additional term ∥f −fF ∥L1 in the
bound, related to the inherent bias. For illustration
purpose, the following result generalizes one of the inequalities stated in Theorem 2:
Theorem 3. (Biased empirical clusters) Suppose that assumptions A1, A2, ˜A3, A4 hold true, let
t1 > 0 and for k ≥2 consider tk deﬁned by (11), Ωtk
by (8), and sN by (7). Then there is a constant A
independent from N, n, δ such that, with probability
larger than 1 −δ, we have:
|EM ∗(t) −EMsN (t)|
2 log(1/δ)
√n + ∥f −fF ∥L1 + oN(1),
where oN(1) = 1 −EM ∗(tN).
Remark 5. (Hypercubes) In practice, one deﬁnes
a sequence of models Fl ⊂Gl indexed by a tuning parameter l controlling (the inverse of) model complexity, such that ∥f −fFl∥L1 →0 as l →0. For instance,
the class Fl could be formed by disjoint hypercubes of
side length l.
Simulation examples
Algorithm 1 is here implemented from simulated 2d heavy-tailed data with common density f(x, y) =
1/2 × 1/(1 + |x|)3 × 1/(1 + |y|)2. The training set is of
size n = 105, whereas the test set counts 106 points.
For l > 0, we set Gl = σ(F) where Fl = {F l
i }i∈Z2 and
i = [li1, li1 + 1] × [li2, li2 + 1] for all i = (i1, i2) ∈Z2.
The bias of the model is thus bounded by ∥f −fF ∥∞,
vanishing as l →0 (observe that the bias is at most
of order l as soon as f is Lipschitz for instance). The
scoring function s is built using the points located in
[−L, L]2 and setting s = 0 outside of [−L, L]2. Practically, one takes L as the maximum norm value of the
points in the training set, or such that an empirical estimate of P(X ∈[−L, L]2) is very close to 1 (here one
obtains 0.998 for L = 500). The implementation of our
algorithm involves the use of a sparse matrix to store
the data in the partition of hypercubes, such that the
complexity of the procedure for building the scoring
function s and that of the computation of its empirical EM-curve is very small compared to that needed
to compute fFl and EMfFl , which are given here for
the sole purpose of quantifying the model bias.
Fig. 4 illustrates as expected the deterioration of EMs
for large l, except for t close to zero: this corresponds
to the model bias. However, Fig. 5 reveals an ”over-
ﬁtting” phenomenon for values of t close to zero, when
l is fairly small. This is mainly due to the fact that
subsets involved in the scoring function are then tiny
in regions where there are very few observations (in
the tail of the distribution). On the other hand, for
the largest values of t, the smallest values of l give the
best results: the smaller the parameter l, the weaker
the model bias and no overﬁtting is experienced because of the high local density of the observations.
Recalling the notation EM ∗
G(t) = maxΩ∈G Ht(Ω) ≤
EM ∗(t) = maxΩmeas. Ht(Ω) so that the bias of our
model is EM ∗−EM ∗
G, Fig. 6 illustrates the variations
of the bias with the wealth of our model characterized
by l the width of the partition by hypercubes. Notice
that partitions with small l are not so good approximation for large t, but are performing as well as the
other in the extreme values, namely when t is close to
0. On the top of that, those partitions have the merit
not to overﬁt the extreme datas, which typically are
This empirical analysis demonstrates that introducing
a notion of adaptivity for the partition F, with progressively growing bin-width as t decays to zero and
as the hypercubes are being selected in the construction of s (which crucially depends on local properties
of the empirical distribution), drastically improves the
accuracy of the resulting scoring function in the EM
curve sense.
Conclusion
Prolongating the contribution of , this article provides an alternative view (respectively, an other parameterization) of the anomaly scoring problem, leading to another adaptive method to build scoring functions, which oﬀers theoretical and computational advantages both at the same time. This novel formulation yields a procedure producing a nested sequence of
empirical density level sets, and exhibits a good performance, even in the non compact support case. In
addition, the model bias has been incorporated in the
rate bound analysis.
On Anomaly Ranking and Excess-Mass Curves
Figure 4: Optimal and
realized EM curves
Figure 5: Zoom near 0
Figure 6: EMG for diﬀerent l
Technical Details
Proof of Theorem 1 (Sketch of) The proof results
from the following lemma, which does not use the compact support assumption on f and is the starting point
of the extension to the non compact support case (section 5.1).
Lemma 2. Suppose that assumptions A1, A2, A3
and A4 are fulﬁlled. Then, for 1 ≤k ≤K −1, there
exists a constant A independent from n and δ, such
that, with probability at least 1 −δ, for t in ]tk+1, tk],
|EM∗(t) −EMsK(t)| ≤
+ λ(tk+1)(tk −tk+1).
The detailed proof of this lemma is in the supplementary material, and is a combination on the two
following results, the second one being a straightforward consequence of the derivative property of EM ∗
(Proposition 1):
• With probability at least 1 −δ, for k ∈{1, ..., K},
0 ≤EM ∗(tk) −EMsK(tk) ≤2Φn(δ) .
• Let k in {1, ..., K −1}.
Then for every t in
]tk+1, tk],
0 ≤EM ∗(t) −EM ∗(tk) ≤λ(tk+1)(tk −tk+1) .
Proof of Theorem 2 (Sketch of) The ﬁrst assertion
is a consequence of (10) combined with the fact that
|EM ∗(t) −EMsN (t)| ≤1 −EMsN (tN)
≤1 −EM ∗(tN) + 2Φn(δ)
holds true with probability at least 1 −δ. For the second part, it suﬃces to observe that sN(x) (absolutely)
converges to s∞and that, as pointed out in Remark
1, EMsN ≤EMs∞.
Proof of Theorem 3 (Sketch of) The result
directly follows from the following lemma, which
establishes an upper bound for the bias, with the
notations EM∗
C(t) := maxΩ∈C Ht(Ω) ≤EM∗(t) =
maxΩmeas. Ht(Ω) for any class of measurable sets C,
and F := σ(F) so that by assumption A3, F ⊂G.
Details are omitted due to space limits.
Lemma 3. Under assumption ˜A3, we have for every
t in [0, ∥f∥∞],
0 ≤EM∗(t) −EM∗
F(t) ≤∥f −fF ∥L1 .
The model bias EM∗−EM∗
G is then uniformly bounded
by ∥f −fF ∥L1.
To prove this lemma (see the supplementary material
for details), one shows that:
EM∗(t) −EM∗
{f>t}\{fF >t}
{fF >t}\{f>t}
where we use the fact that for all t > 0, {fF > t} ∈F
and ∀F ∈F,
G fF . It suﬃces then to observe
that the second and the third term in the bound are
non-positive.
Nicolas Goix, Anne Sabourin, St´ephan Cl´emen¸con