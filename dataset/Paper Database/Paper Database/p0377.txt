LISA Data Analysis using MCMC methods
Neil J. Cornish and JeﬀCrowder
Department of Physics, Montana State University, Bozeman, MT 59717
The Laser Interferometer Space Antenna (LISA) is expected to simultaneously detect many thousands of low frequency gravitational wave signals. This presents a data analysis challenge that is
very diﬀerent to the one encountered in ground based gravitational wave astronomy. LISA data
analysis requires the identiﬁcation of individual signals from a data stream containing an unknown
number of overlapping signals. Because of the signal overlaps, a global ﬁt to all the signals has to be
performed in order to avoid biasing the solution. However, performing such a global ﬁt requires the
exploration of an enormous parameter space with a dimension upwards of 50,000. Markov Chain
Monte Carlo (MCMC) methods oﬀer a very promising solution to the LISA data analysis problem.
MCMC algorithms are able to eﬃciently explore large parameter spaces, simultaneously providing
parameter estimates, error analysis, and even model selection. Here we present the ﬁrst application
of MCMC methods to simulated LISA data and demonstrate the great potential of the MCMC approach. Our implementation uses a generalized F-statistic to evaluate the likelihoods, and simulated
annealing to speed convergence of the Markov chains. As a ﬁnal step we super-cool the chains to
extract maximum likelihood estimates, and estimates of the Bayes factors for competing models.
We ﬁnd that the MCMC approach is able to correctly identify the number of signals present, extract the source parameters, and return error estimates consistent with Fisher information matrix
predictions.
INTRODUCTION
The LISA observatory has incredible science potential, but that potential can only be fully realized by employing advanced data analysis techniques. LISA will explore the low frequency portion of the gravitational wave
spectrum, which is thought to be home to a vast number of sources. Since gravitational wave sources typically
evolve on timescales that are long compared to the gravitational wave period, individual low frequency sources
will be “on” for large fractions of the nominal three year
LISA mission lifetime.
Moreover, unlike a traditional
telescope, LISA can not be pointed at a particular point
on the sky. The upshot is that the LISA data stream will
contain the signals from tens of thousands of individual
sources, and ways must be found to isolate individual
voices from the crowd. This “Cocktail Party Problem”
is the central issue in LISA data analysis.
The types of sources LISA is expected to detect include
galactic and extra-galactic compact stellar binaries, super massive black hole binaries, and extreme mass ratio
inspirals of compact stars into supermassive black holes
(EMRIs). Other potential sources include intermediate
mass black hole binaries, cosmic strings, and a cosmic
gravitational wave background produced by processes in
the early universe.
In the case of compact stellar binaries and EMRIs , the number of
sources is likely to be so large that it will be impossible to resolve all the sources individually, so that there
will be a residual signal that is variously referred to as
a confusion limited background or confusion noise.
is important that this confusion noise be made as small
as possible so as not to hinder the detection of other
high value targets.
Several estimates of the confusion
noise level have been made , and they
all suggest that unresolved signals will be the dominant
source of low frequency noise for LISA. However, these
estimates are based on assumptions about the eﬃcacy of
the data analysis algorithms that will be used to identify
and regress sources from the LISA data stream, and it
is unclear at present how reasonable these assumptions
might be. Indeed, the very notion that one can ﬁrst clean
the data stream of one type of signal before moving on
to search for other targets is suspect as the gravitational
wave signals from diﬀerent sources are not orthogonal.
For example, when the signal from a supermassive black
hole binary sweeps past the signal from a white dwarf
binary of period T , the two signals will have signiﬁcant
overlap for a time interval equal to the geometric mean
of T and tc, where tc is the time remaining before the
black holes merge. Thus, by a process dubbed “the white
dwarf transform,” it is possible to decompose the signal
from a supermassive black hole binary into signals from
a collection of white dwarf binaries.
As described in §II, optimal ﬁltering of the LISA data
would require the construction of a ﬁlter bank that described the signals from every source that contributes
to the data stream. In principle one could construct a
vast template bank describing all possible sources and
look for the best match with the data. In practice the
enormous size of the search space and the presence of
unmodeled sources renders this direct approach impractical. Possible alternatives to a full template based search
include iterative reﬁnement of a source-by-source search,
ergodic exploration of the parameter space using Markov
Chain Monte Carlo (MCMC) algorithms , Darwinian optimization by genetic algorithms, and global iterative re-
ﬁnement using the Maximum Entropy Method (MEM).
Each approach has its strengths and weakness, and at
this stage it is not obvious which approach will prove
Here we apply the popular Markov Chain Monte
Carlo method to simulated LISA data. This is
not the ﬁrst time that MCMC methods have been applied to gravitational wave data analysis, but it is ﬁrst
outing with realistic simulated LISA data. Our simulated
data streams contain the signals from multiple galactic
binaries. Previously, MCMC methods have been used to
study the extraction of coalescing binary and spinning neutron star signals from terrestrial interferometers. More recently, MCMC methods have been applied to a simpliﬁed toy problem that shares some of
the features of the LISA cocktail party problem. These
studies have shown that MCMC methods hold considerable promise for gravitational wave data analysis, and
oﬀer many advantages over the standard template grid
For example, the EMRI data analysis problem is often cited as the greatest challenge facing LISA science.
Neglecting the spin of the smaller
body yields a 14 dimensional parameter space, which
would require ∼1040 templates to explore in a grid based
search . This huge computational cost arises because
grid based searches scale geometrically with the parameter space dimension D. In contrast, the computational
cost of MCMC based searches scale linearly with the D.
In ﬁelds such as ﬁnance, MCMC methods are routinely
applied to problems with D > 1000, making the LISA
EMRI problem seem trivial in comparison.
search on “Markov Chain Monte Carlo” returns almost
250,000 results, and a quick scan of these pages demonstrates the wide range of ﬁelds where MCMC methods
are routinely used.
We found it amusing that one of
the Google search results is a link to the PageRank 
MCMC algorithm that powers the Google search engine.
The structure of the paper follows the development
sequence we took to arrive at a fast and robust MCMC
algorithm. In §II we outline the LISA data analysis problem and the particular challenges posed by the galactic
background. A basic MCMC algorithm is introduced in
§III and applied to a full 7 parameter search for a single galactic binary. A generalized multi-channel, multisource F-statistic for reducing the search space from
D = 7N to D = 3N is described in §IV.
The performance of a basic MCMC algorithm that uses the Fstatistic is studied in §V and a number of problems with
this simple approach are identiﬁed.
A more advanced
mixed MCMC algorithm that incorporates simulated annealing is introduced in §VI and is successfully applied
to multi-source searches. The issue of model selection
is addressed in §VII, and approximate Bayes factor are
calculated by super-cooling the Markov Chains to extract maximum likelihood estimates. We conclude with
a discussion of future reﬁnements and extensions of our
approach in §VIII.
THE COCKTAIL PARTY PROBLEM
Space based detectors such as LISA are able to return several interferometer outputs .
The strains
registered in the interferometer in response to a gravitational wave pick up modulations due to the motion
of the detector.
The orbital motion introduces amplitude, frequency, and phase modulation into the observed
gravitational wave signal. The amplitude modulation results from the detector’s antenna pattern being swept
across the sky, the frequency modulation is due to the
Doppler shift from the relative motion of the detector
and source, and the phase modulation results from the
detector’s varying response to the two gravitational wave
polarizations . These modulations encode information about the location of the source. The modulations spread a monochromatic signal over a bandwidth
∆f ∼(9 + 6(f/mHz) sin θ)fm, where θ is the co-latitude
of the source and fm = 1/year is the modulation frequency.
In the low frequency limit, where the wavelengths are large compared to the armlengths of the detector, the interferometer outputs sα(t) can be combined
to simulate the response of two independent 90 degree
interferometers, sI(t) and sII(t), rotated by 45 degrees
with respect to each other . This allows LISA to
measure both polarizations of the gravitational wave simultaneously. A third combination of signals in the low
frequency limit yields the symmetric Sagnac variable ,
which is insensitive to gravitational waves and can be
used to monitor the instrument noise. When the wavelengths of the gravitational waves become comparable to
the size of the detector, which for LISA corresponds to
frequencies above 10 mHz, the interferometry signals can
be combined to give three independent time series with
comparable sensitivities .
The output of each LISA data stream can be written
sα(t) = hα(t,⃗λ) + nα(t) =
α(t,⃗λi) + nα(t) .
α(t,⃗λi) describes the response registered in detector channel α to a source with parameters ⃗λi. The quantity hα(t,⃗λ) denotes the combined response to a collection of N sources with total parameter vector ⃗λ = P
and nα(t) denotes the instrument noise in channel α. Extracting the parameters of each individual source from
the combined response to all sources deﬁnes the LISA
cocktail party problem.
In practice it will be impossible to resolve all of the millions of signals that contribute to the LISA data streams. For one, there will
not be enough bits of information in the entire LISA
data archive to describe all N sources in the Universe
with signals that fall within the LISA band. Moreover,
most sources will produce signals that are well below the
instrument noise level, and even after optimal ﬁltering
most of these sources will have signal to noise ratios below one. A more reasonable goal might be to provide
estimates for the parameters describing each of the N ′
sources that have integrated signal to noise ratios (SNR)
above some threshold (such as SNR > 5), where it is now
understood that the noise includes the instrument noise,
residuals from the regression of bright sources, and the
signals from unresolved sources.
While the noise will be neither stationary nor Gaussian, it is not unreasonable to hope that the departures
from Gaussianity and stationarity will be mild.
well know that matched ﬁltering is the optimal linear
signal processing technique for signals with stationary
Gaussian noise .
Matched ﬁltering is used extensively in all ﬁelds of science, and is a popular data
analysis technique in ground based gravitational wave
astronomy .
Switching to the Fourier domain, the signal can be written as ˜sα(f) = ˜hα(f,⃗λ′) + ˜nα(f), where ˜nα(f) includes
instrument noise and confusion noise, and the signals are
described by parameters ⃗λ′.
Using the standard noise
weighted inner product for the independent data channels over a ﬁnite observation time T ,
α(f)˜bα(f) + ˜aα(f)˜b∗
a Wiener ﬁlter statistic can be deﬁned:
(h(⃗λ)|h(⃗λ))
The noise spectral density Sn(f) is given in terms of the
autocorrelation of the noise
⟨n(f)n∗(f ′)⟩= T
2 δff ′Sn(f) .
Here and elsewhere angle brackets ⟨⟩denote an expectation value. An estimate for the source parameters ⃗λ′ can
be found by maximizing ρ(⃗λ). If the noise is Gaussian
and a signal is present, ρ(⃗λ) will be Gaussian distributed
with unit variance and mean equal to the integrated signal to noise ratio
SNR = ⟨ρ(⃗λ′)⟩=
(h(⃗λ′)|h(⃗λ′)) .
The optimal ﬁlter for the LISA signal (1) is a matched
template describing all N ′ resolvable sources. The number of parameters di required to describe a source ranges
from 7 for a slowly evolving circular galactic binary to
17 for a massive black hole binary. A reasonable estimate for N ′ is around 104, so the full parameter
space has dimension D = P
i di ∼105. Since the number of templates required to uniformly cover a parameter
space grows exponentially with D, a grid based search using the full optimal ﬁlter is out of the question. Clearly
an alternative approach has to be found. Moreover, the
number of resolvable sources N ′ is not known a priori, so
some stopping criteria must be found to avoid over-ﬁtting
Existing approaches to the LISA cocktail party problem employ iterative schemes. The ﬁrst such approach
was dubbed “gCLEAN” due to its similarity with
the “CLEAN” algorithm that is used for astronomical image reconstruction.
The “gCLEAN” procedure
identiﬁes and records the brightest source that remains
in the data stream, then subtracts a small amount of
this source. The procedure is iterated until a prescribed
residual is reached, at which time the individual sources
are reconstructed from the subtraction record. A much
faster iterative approach dubbed “Slice & Dice” was
recently proposed that proceeds by identifying and fully
subtracting the brightest source that remains in the data
stream. A global least squares re-ﬁt to all the current
list of sources is then performed, and the new parameter record is used to produce a regressed data stream for
the next iteration. Bayes factors are used to provide a
stopping criteria.
There is always the danger with iterative approaches
that the procedure “gets oﬀon the wrong foot,” and
is unable to ﬁnd its way back to the optimal solution.
This can happen when two signals have a high degree of
overlap. A very diﬀerent approach to the LISA source
confusion problem is to solve for all sources simultaneously using ergodic sampling techniques. Markov Chain
Monte Carlo (MCMC) is a method for estimating the posterior distribution, p(⃗λ|s), that can be used
with very large parameter spaces. The method is now in
widespread use in many ﬁelds, and is starting to be used
by astronomers and cosmologists. One of the advantages
of MCMC is that it combines detection, parameter estimation, and the calculation of conﬁdence intervals in
one procedure, as everything one can ask about a model
is contained in p(⃗λ|s). Another nice feature of MCMC
is that there are implementations that allow the number
of parameters in the model to be variable, with built in
penalties for using too many parameters in the ﬁt. In
an MCMC approach, parameter estimates from Wiener
matched ﬁltering are replaced by the Bayes estimator 
λi p(⃗λ|s) d⃗λ ,
which requires knowledge of p(⃗λ|s) - the posterior distribution of ⃗λ (i.e. the distribution of ⃗λ conditioned on the
data s). By Bayes theorem, the posterior distribution is
related to the prior distribution p(⃗λ) and the likelihood
p(s|⃗λ) by
p(⃗λ)p(s|⃗λ)
p(⃗λ′)p(s|⃗λ′)d⃗λ′ .
Until recently the Bayes estimator was little used in practical applications as the integrals appearing in (6) and (7)
are often analytically intractable. The traditional solution has been to use approximations to the Bayes estimator, such as the maximum likelihood estimator described
below, however advances in the Markov Chain Monte
Carlo technique allow direct numerical estimates to be
When the noise n(t) is a normal process with zero
mean, the likelihood is given by 
p(s|⃗λ) = C exp
(s −h(⃗λ))|(s −h(⃗λ))
where the normalization constant C is independent of
s. In the large SNR limit the Bayes estimator can be
approximated by ﬁnding the dominant mode of the posterior distribution, p(⃗λ|s), which Finn and Cutler &
Flannagan refer to as a maximum likelihood estimator. Other authors deﬁne the maximum likelihood estimator to be the value of ⃗λ that maximizes the
likelihood, p(s|⃗λ). The former has the advantage of incorporating prior information, but the disadvantage of not
being invariant under parameter space coordinate transformations. The latter deﬁnition corresponds to the standard deﬁnition used by most statisticians, and while it
does not take into account prior information, it is coordinate invariant. The two deﬁnitions give the same result
for uniform priors, and very similar results in most cases
(the exception being where the priors have a large gradient at maximum likelihood).
The standard deﬁnition of the likelihood yields an estimator that is identical to Wiener matched ﬁltering .
Absorbing normalization factors by adopting the inverted
relative likelihood L(⃗λ) = p(s|0)/p(s|⃗λ), we have
log L(⃗λ) = (s|h(⃗λ)) −1
2(h(⃗λ)|h(⃗λ)) .
In the gravitational wave literature the quantity log L(⃗λ)
is usually referred to as the log likelihood, despite the
inversion and rescaling. Note that
⟨log L(⃗λ′)⟩= 1
2⟨ρ(⃗λ′)⟩2 = 1
The maximum likelihood estimator (MLE), ⃗λML, is found
by solving the coupled set of equations ∂log L/∂λi = 0.
Parameter uncertainties can be estimated from the negative Hessian of log L, which yields the Fisher Information
Γij(⃗λ) = −
D∂2 log L(⃗λ)
= (h,i|h,j).
In the large SNR limit the MLE can be found by writing ⃗λ = ⃗λ′ + ∆⃗λ and Taylor expanding (9).
∂log L/∂∆λi = 0 yields the lowest order solution
ML = λ′i + ∆λi = λ′i + Γij(⃗λ′)(n|h,j) .
The expectation value of the maximum of the log likelihood is then
⟨log L(⃗λML)⟩= SNR2 + D
This value exceeds that found in (10) by an amount that
depends on the total number of parameters used in the
ﬁt, D, reﬂecting the fact that models with more parameters generally give better ﬁts to the data. Deciding how
many parameters to allow in the ﬁt is an important issue
in LISA data analysis as the number of resolvable sources
is not known a priori. This issue does not usually arise for
ground based gravitational wave detectors as most high
frequency gravitational wave sources are transient. The
relevant question there is whether or not a gravitational
wave signal is present in a section of the data stream, and
this question can be dealt with by the Neyman-Pearson
test or other similar tests that use thresholds on the likelihood L that are related to the false alarm and false
dismissal rates. Demanding that L > 1 - so it is more
likely that a signal is present than not - and setting a
detection threshold of ρ = 5 yields a false alarm probability of 0.006 and a detection probability of 0.994 (if
the noise is stationary and Gaussian). A simple acceptance threshold of ρ = 5 for each individual signal used
to ﬁt the LISA data would help restrict the total number
of parameters in the ﬁt, however there are better criteria that can be employed. The simplest is related to
the Neyman-Pearson test and compares the likelihoods of
models with diﬀerent numbers of parameters. For nested
models this ratio has an approximately chi squared distribution which allows the signiﬁcance of adding extra
parameters to be determined from standard statistical
tables. A better approach is to compute the Bayes factor,
BXY = pX(s)
which gives the relative weight of evidence for models X
and Y in terms of the ratio of marginal likelihoods
p(s|⃗λ, X)p(⃗λ, X)d⃗λ .
Here p(s|⃗λ, X) is the likelihood distribution for model X
and p(⃗λ, X) is the prior distribution for model X. The
diﬃculty with this approach is that the integral in (15)
is hard to calculate, though estimates can be made using the Laplace approximation or the Bayesian Information Criterion (BIC) . The Laplace approximation is
based on the method of steepest descents, and for uniform priors yields
pX(s) ≃p(s|⃗λML, X)
where p(s|⃗λML, X) is the maximum likelihood for the
model, VX is the volume of the model’s parameter space,
and ∆VX is the volume of the uncertainty ellipsoid (estimated using the Fisher matrix). Models with more parameters generally provide a better ﬁt to the data and
a higher maximum likelihood, but they get penalized by
the ∆VX/VX term which acts as a built in Occam’s razor.
MARKOV CHAIN MONTE CARLO
We begin by implementing a basic MCMC search
for galactic binaries that searches over the full D =
7N dimensional parameter space using the Metropolis-
Hastings algorithm. The idea is to generate a set of
samples, {⃗x}, that correspond to draws from the posterior distribution, p(⃗λ|s). To do this we start at a randomly chosen point ⃗x and generate a Markov chain according to the following algorithm: Using a proposal distribution q(·|⃗x), draw a new point ⃗y. Evaluate the Hastings ratio
H = p(⃗y)p(s|⃗y)q(⃗x|⃗y)
p(⃗x)p(s|⃗x)q(⃗y|⃗x) .
Accept the candidate point ⃗y with probability α =
min(1, H), otherwise remain at the current state ⃗x
(Metropolis rejection ). Remarkably, this sampling
scheme produces a Markov chain with a stationary distribution equal to the posterior distribution of interest, p(⃗λ|s), regardless of the choice of proposal distribution . A concise introduction to MCMC methods
can be found in the review paper by Andrieu et al .
On the other hand, a poor choice of the proposal distribution will result in the algorithm taking a very long
time to converge to the stationary distribution (known
as the burn-in time). Elements of the Markov chain produced during the burn-in phase have to be discarded as
they do not represent the stationary distribution. When
dealing with large parameter spaces the burn-in time can
be very long if poor techniques are used. For example,
the Metropolis sampler, which uses symmetric proposal
distributions, explores the parameter space with an ef-
ﬁciency of at most ∼0.3/D, making it a poor choice
for high dimension searches. Regardless of the sampling
scheme, the mixing of the Markov chain can be inhibited
by the presence of strongly correlated parameters. Correlated parameters can be dealt with by making a local
coordinate transformation at ⃗x to a new set of coordinates that diagonalises the Fisher matrix, Γij(⃗x).
We tried a number of proposal distributions and update schemes to search for a single galactic binary. The
results were very disappointing. Bold proposals that attempted large jumps had a very poor acceptance rate,
while timid proposals that attempted small jumps had a
good acceptance rate, but they explored the parameter
space very slowly, and got stuck at local modes of the posterior. Lorentzian proposal distributions fared the best
as their heavy tails and concentrated peaks lead to a mixture of bold and timid jumps, but the burn in times were
still very long and the subsequent mixing of the chain was
torpid. The MCMC literature is full of similar examples
of slow exploration of large parameter spaces, and a host
of schemes have been suggested to speed up the burn-in.
Many of the accelerated algorithms use adaptation to
tune the proposal distribution. This violates the Markov
nature of the chain as the updates depend on the history of the chain. More complicated adaptive algorithms
have been invented that restore the Markov property by
using additional Metropolis rejection steps. The popular Delayed Rejection Method and Reversible Jump
Method are examples of adaptive MCMC algorithms.
A simpler approach is to use a non-Markov scheme during burn-in, such as adaptation or simulated annealing,
then transition to a Markov scheme after burn-in. Since
the burn-in portion of the chain is discarded, it does not
matter if the MCMC rules are broken (the burn-in phase
is more like Las Vegas than Monte Carlo).
Before resorting to complex acceleration schemes we
tried a much simpler approach that proved to be very
successful.
When using the Metropolis-Hastings algorithm there is no reason to restrict the updates to a single
proposal distribution. For example, every update could
use a diﬀerent proposal distribution so long as the choice
of distribution is not based on the history of the chain.
The proposal distributions to be used at each update can
be chosen at random, or they can be applied in a ﬁxed
sequence. Our experience with single proposal distributions suggested that a scheme that combined a very bold
proposal with a very timid proposal would lead to fast
burn-in and eﬃcient mixing. For the bold proposal we
chose a uniform distribution for each of the source parameters ⃗λ →(A, f, θ, φ, ψ, ι, ϕ0). Here A is the amplitude, f
is the gravitational wave frequency, θ and φ are the ecliptic co-latitude and longitude, ψ is the polarization angle,
ι is the inclination of the orbital plane, and ϕ0 is the orbital phase at some ﬁducial time. The amplitudes were
restricted to the range A ∈[10−23, 10−21] and the frequencies were restricted to lie within the range of the data
snippet f ∈[0.999995, 1.003164] mHz (the data snippet
contained 100 frequency bins of width ∆f = 1/year). A
better choice would have been to use a cosine distribution for the co-latitude θ and inclination ι, but the choice
is not particularly important.
When multiple sources
were present each source was updated separately during the bold proposal stage. For the timid proposal we
used a normal distribution for each eigendirection of the
Fisher matrix, Γij(⃗x).
The standard deviation σˆk for
each eigendirection k was set equal to σˆk = 1/pαˆkD,
where αˆk is the corresponding eigenvalue of Γij(⃗x), and
D = 7N is the search dimension. The factor of 1/
ensures a healthy acceptance rate as the typical total
jump is then ∼1σ. All N sources were updated simultaneously during the timid proposal stage.
the timid proposal distributions are not symmetric since
Γij(⃗x) ̸= Γij(⃗y). One set of bold proposals (one for each
source) was followed by ten timid proposals in a repeating
cycle. The ratio of the number of bold to timid proposals impacted the burn-in times and the ﬁnal mixing rate,
but ratios anywhere from 1:1 to 1:100 worked well. We
used uniform priors, p(⃗x) = const., for all the parameters,
though once again a cosine distribution would have been
better for θ and ι. Two independent LISA data channels
were simulated directly in the frequency domain using the
method described in Ref. , with the sources chosen at
random using the same uniform distributions employed
TABLE I: 7 parameter MCMC search for a single galactic
A (10−22) f (mHz)
1.0005853 0.98
1.0005837 1.07
0.085 0.051 0.054 0.050 0.22
0.089 0.055 0.058 0.052 0.23
by the bold proposal. The data covers 1 year of observations, and the data snippet contains 100 frequency bins
(of width 1/year). The instrument noise was assumed to
be stationary and Gaussian, with position noise spectral
density Spos
= 4 × 10−22 m2Hz−1 and acceleration noise
spectral density Saccel
= 9 × 10−30 m2s−4Hz−1.
Table I summarizes the results of one MCMC run using a model with one source to search for a single source
in the data snippet. Burn-in lasted ∼2000 iterations,
and post burn-in the chain was run for 106 iterations
with a proposal acceptance rate of 77% (the full run
took 20 minutes on a Mac G5 2 GHz processor). The
chain was used to calculate means and variances for all
the parameters.
The parameter uncertainty estimates
extracted from the MCMC output are compared to the
Fisher matrix estimates evaluated at the mean values of
the parameters. The source had true SNR = 12.9, and
MCMC recovered SNR = 10.7. Histograms of the posterior parameter distributions are shown in Figure 1, where
they are compared to the Gaussian approximation to the
posterior given by the Fisher matrix. The agreement is
impressive, especially considering that the bandwidth of
the source is roughly 10 frequency bins, so there are very
few noise samples to work with.
Similar results were
found for other MCMC runs on the same source, and for
MCMC runs with other sources. Typical burn-in times
were of order 3000 iterations, and the proposal acceptance rate was around 75%.
The algorithm was run successfully on two and three
source searches (the model dimension was chosen to
match the number of sources in each instance), but on
occasions the chain would get stuck at a local mode of
the posterior for a large number of iterations.
attempting to cure this problem with a more reﬁned
MCMC algorithm, we decided to eliminate the extrinsic
parameters A, ι, ψ, ε0 from the search by using a multi-
ﬁlter generalized F-statistic. This reduces the search dimension to D = 3N, with the added beneﬁt that the
projection onto the (f, θ, φ) sub-space yields a softer target for the MCMC search.
GENERALIZED F-STATISTIC
The F-statistic was originally introduced in the
context of ground based searches for gravitational wave
signals from rotating Neutron stars. The F-statistic has
since been used to search for monochromatic galactic bi-
FIG. 1: Histograms showing the posterior distribution (grey)
of the parameters. Also shown (black line) is the Gaussian approximation to the posterior distribution based on the Fisher
information matrix. The mean values have been subtracted,
and the parameters have been scaled by the square root of
the variances calculated from the MCMC chains.
naries using simulated LISA data . By using multiple linear ﬁlters, the F-statistic is able to automatically
extremize the log likelihood over extrinsic parameters,
thus reducing the dimension of the search space (the parameter space dimension remains the same).
In the low-frequency limit the LISA response to a gravitational wave with polarization content h+(t), h×(t) can
be written as
h(t) = h+(t)F +(t) + h×(t)F ×(t) ,
F +(t) = 1
 cos 2ψ D+(t) −sin 2ψ D×(t)
F ×(t) = 1
 sin 2ψ D+(t) + cos 2ψ D×(t)
The detector pattern functions D+(t) and D×(t) are
given in equations (36) and (37) of Ref. . To leading
post-Newtonian order a slowly evolving, circular binary
has polarization components
h+(t) = A(1 + cos2 ι) cos(Φ(t) + ϕ0)
h×(t) = −2A cosι sin(Φ(t) + ϕ0).
The gravitational wave phase
Φ(t; f, θ, φ) = 2πft + 2πfAU sin θ cos(2πfmt −φ), (21)
couples the sky location and the frequency through the
term that depends on the radius of LISA’s orbit, 1 AU,
and the orbital modulation frequency, fm = 1/year. The
gravitational wave amplitude, A, is eﬀectively constant
for the low frequency galactic sources we are considering.
Using these expressions (18) can be written as
ai(A, ψ, ι, ϕ0)Ai(t; f, θ, φ) ,
where the time-independent amplitudes ai are given by
 (1 + cos2 ι) cos ϕ0 cos 2ψ −2 cosι sin ϕ0 sin 2ψ
 2 cosι sin ϕ0 cos 2ψ + (1 + cos2 ι) cos ϕ0 sin 2ψ
 2 cosι cos ϕ0 sin 2ψ + (1 + cos2 ι) sin ϕ0 cos 2ψ
 (1 + cos2 ι) sin ϕ0 sin 2ψ −2 cos ι cos ϕ0 cos 2ψ
and the time-dependent functions Ai(t) are given by
A1(t) = D+(t; θ, φ) cos Φ(t; f, θ, φ)
A2(t) = D×(t; θ, φ) cos Φ(t; f, θ, φ)
A3(t) = D+(t; θ, φ) sin Φ(t; f, θ, φ)
A4(t) = D×(t; θ, φ) sin Φ(t; f, θ, φ) .
Deﬁning the four constants N i = (s|Ai) and using (22)
yields a solution for the amplitudes ai:
ai = (M −1)ijN j ,
where M ij = (Ai|Aj).
The output of the four linear
ﬁlters, N i, and the 4 × 4 matrix M ij can be calculated
using the same fast Fourier space techniques used
to generate the full waveforms.
Substituting (22) and
(25) into expression (9) for the log likelihood yields the
F-statistic
F = log L = 1
2(M −1)ijN iN j .
The F-statistic automatically maximizes the log likelihood over the extrinsic parameters A, ι, ψ and ϕ0, and
reduces the search to the sub-space spanned by f, θ and
φ. The extrinsic parameters can be recovered from the
A+a4 −A×a1
−(A×a2 + A+a3)
ι = arccos
ϕ0 = arctan
 c(A+a4 −A×a1)
−c(A×a2 + A+a3)
(a1 + a4)2 + (a2 −a3)2
(a1 −a4)2 + (a2 + a3)2
(a1 + a4)2 + (a2 −a3)2
(a1 −a4)2 + (a2 + a3)2
c = sign(sin(2ψ)) .
The preceding description of the F-statistic automatically incorporates the two independent LISA channels
through the use of the dual-channel noise weighted inner
product (a|b). The basic F-statistic can easily be generalized to handle N sources. Writing i = 4K +l, where K
labels the source and l = 1 →4 labels the four ﬁlters for
each source, the F-statistic (26) keeps the same form as
before, but now there are 4N linear ﬁlters N i, and M ij
is a 4N × 4N dimensional matrix. For slowly evolving
galactic binaries, which dominate the confusion problem,
the limited bandwidth of each individual signal means
that the M ij is band diagonal, and thus easily inverted
despite its large size.
Since the search is now over the projected sub-space
{fJ, θJ, φJ} of the full parameter space, the full Fisher
matrix, Γij(⃗x), is replaced by the projected Fisher matrix, γij(⃗x). The projection of the kth parameter is given
where n denotes the dimension of the projected matrix. Repeated application of the above projection yields
Inverting γij yields the same uncertainty
estimates for the intrinsic parameters as one gets from
the full Fisher matrix, but the covariances are much
larger. The large covariances make it imperative that the
proposal distributions use the eigenvalues and eigenvectors of γij, as using the parameter directions themselves
would lead to a slowly mixing chain.
F-STATISTIC MCMC
We implemented an F-statistic based MCMC algorithm using the approach described in §III, but with
the full likelihood replaced by the F-statistic and the full
Fisher matrix replaced by the projected Fisher matrix.
Applying the F-MCMC search to the same data set as
before yields the results summarized in Figure 2 and Table II. The recovered source parameters and signal-tonoise ratio (SNR = 10.4) are very similar to those found
using the full 7-parameter search, but the F-MCMC estimates for the errors in the extrinsic parameters are very
diﬀerent. This is because the chain does not explore extrinsic parameters, but rather relies upon the F-statistic
to ﬁnd the extrinsic parameters that give the largest log
likelihood based on the current values for the intrinsic
TABLE II: F-MCMC search for a single galactic binary
A (10−22) f (mHz)
1.0005853 0.98
1.0005835 1.09
0.089 0.052 0.055 0.051 0.22
0.093 0.056 0.027 0.016 0.21
parameters.
The eﬀect is very pronounced in the histograms shown in Figure 2. Similar results were found
for other F-MCMC runs on the same source, and for F-
MCMC runs with other sources. Typical burn-in times
were of order 1000 iterations, and the proposal acceptance rate was around 60%. As expected, the F-MCMC
algorithm gave shorter burn-in times than the full parameter MCMC, and a comparable mixing rate.
FIG. 2: Histograms showing the posterior distribution (grey)
of the parameters. Also shown (black line) is the Gaussian approximation to the posterior distribution based on the Fisher
information matrix. The mean values have been subtracted,
and the parameters have been scaled by the square root of
the variances calculated from the F-MCMC chains.
It is interesting to compare the computational cost of
the F-MCMC search to a traditional F-Statistic based
search on a uniformly spaced template grid. To cover
the parameter space of one source (which for the current example extends over the full sky and 100 frequency
bins) with a minimal match of MM = 0.9 requires
39,000 templates . A typical F-MCMC run uses less
than 1000 templates to cover the same search space. The
comparison becomes even more lopsided if we consider simultaneous searches for multiple sources. A grid based
simultaneous search for two sources using the F-statistic
would take (39, 000)2 ≃1.5 × 109 templates, while the
basic F-MCMC algorithm typically converges on the two
sources in just 2000 steps.
As the number of sources
in the model increases the computation cost of the grid
based search grows geometrically while the cost of the
F-MCMC search grows linearly. It is hard to imagine
a scenario (other than quantum computers) where noniterative grid based searches could play a role in LISA
data analysis.
FIG. 3: Trace plots of the sky location parameters for two
F-MCMC runs on the same data set. Both chains initially
locked onto a secondary mode of the posterior, but one of the
chains (light colored line) transitioned to the correct mode
after 13,000 iterations.
While testing the F-MCMC algorithm on diﬀerent
chain became stuck at secondary modes of the posterior.
A good example occurred for a source with
parameters (A, f, θ, φ, ψ, ι, ϕ0)=(1.4e-22, 1.0020802 mHz,
0.399, 5.71, 1.3, 0.96, 1.0) and SNR
MCMC runs returned good ﬁts to the source parameters, with an average log likelihood of ln L
(1.0020809 mHz, 0.391, 5.75) and SNR = 16.26.
However, some runs locked into a secondary mode with average log likelihood ln L = 100, mean intrinsic parameter values (f, θ, φ) = (1.0020858 mHz, 2.876, 5.20) and
SNR = 14.15. It could sometimes take hundreds of thousands of iterations for the chain to discover the dominant
mode. Figure 4 shows plots of the (inverted) likelihood
L and the log likelihood ln L as a function of sky location for ﬁxed f = 1.0020802 mHz.
The log likelihood
plot reveals the problematic secondary mode near the
south pole, while the likelihood plot shows just how small
a target the dominant mode presents to the F-MCMC
search. Similar problems with secondary modes were encountered in the f −φ plane, where the chain would get
stuck a full bin away from the correct frequency. These
problems with the basic F-MCMC algorithm motivated
the embellishments described in the following section.
FIG. 4: Inverted likelihood and log likelihood as a function of
sky location at ﬁxed frequency.
MULTIPLE PROPOSALS AND HEATING
The LISA data analysis problem belongs to a particularly challenging class of MCMC problems known as
“mixture models.”
As the name suggests, a mixture
model contains a number of components, some or all of
which may be of the same type. In our present study
all the components are slowly evolving, circular binaries,
and each component is described by the same set of seven
parameters. There is nothing to stop two components in
the search model from latching on to the same source, nor
is there anything to stop one component in the search
model from latching on to a blend of two overlapping
In the former instance the likelihood is little
improved by using two components to model one source,
so over time one of the components will tend to wander
oﬀin search of another source. In the latter instance it
may prove impossible for any data analysis method to
de-blend the sources (the marginal likelihood for the single component ﬁt to the blended sources may exceed the
marginal likelihood of the “correct” solution).
The diﬃculties we encountered with the single source
searches getting stuck at secondary modes of the posterior are exacerbated in the multi-source case. Source
overlaps can create additional secondary modes that are
not present in the non-overlapping case. We employed
two techniques to speed burn-in and to reduce the chance
of the chain getting stuck at a secondary mode: simulated annealing and multiple proposal distributions. Simulated annealing works by softening the likelihood function, making it easier for the chain to move between
modes. The likelihood (8) can be thought of as a partition function Z = C exp(−βE) with the “energy” of the
system given by E = (s −h|s −h) and the “inverse temperature” equal to β = 1/2. Our goal is to ﬁnd the template h that minimizes the energy of the system. Heating up the system by setting β < 1/2 allows the Markov
Chain to rapidly explore the likelihood surface. We used
a standard power law cooling schedule:
0 < t < Tc
where t is the number of steps in the chain, Tc is the
cooling time and β0 is the initial inverse temperature. It
took some trial and error to ﬁnd good values of Tc and β0.
If some of the sources have very high SNR it is a good idea
to start at a high temperature β0 ∼1/50, but in most
cases we found β0 = 1/10 to be suﬃcient. The optimal
choice for the cooling time depends on the number of
sources and the initial temperature. We found that it
was necessary to increase Tc roughly linearly with the the
number of sources and the initial temperature. Setting
Tc = 105 for a model with N = 10 sources and an initial
temperature of β0 = 1/10 gave fairly reliable results, but
it is always a good idea to allow longer cooling times if
the computational resources are available. The portion
of the chain generated during the annealing phase has to
be discarded as the cooling introduces an arrow of time
which necessarily violates the reversibility requirement of
a Markov Chain.
After cooling to β = 1/2 the chain can explore the likelihood surface for the purpose of extracting parameter estimates and error estimates. Finally, we can extract maximum likelihood estimates by “super cooling” the chain
to some very low temperature (we used β ∼104).
The second ingredient in our advanced F-MCMC algorithm is a large variety of proposal distributions.
We used the following types of proposal distribution:
Uniform(·, ⃗x, i) - a uniform draw on all the parameters
that describe source i, using the full parameter ranges,
with all other sources held ﬁxed; Normal(·, ⃗x) - a multivariate normal distribution with variance-covariance matrix given by 3N × γ(⃗x); Sky(·, ⃗x, i) - a uniform draw on
the sky location for source i; σ-Uniform(·, ⃗x, i) - a uniform
draw on all the parameters that describe source i, using
a parameter range given by some multiple of the standard deviations given by γ(⃗x). The Uniform(·, ⃗x, i) and
Normal(·, ⃗x) proposal distributions are the same as those
used in the basic F-MCMC algorithm. The Sky(·, ⃗x, i)
proposal proved to be very useful at getting the chain
away from secondary modes like the one seen in Figure 4,
while the σ-Uniform(·, ⃗x, i) proposal helped to move the
chain from secondary modes in the f −φ or f −θ planes.
During the initial annealing phase the various proposal
distributions were used in a cycle with one set of the bold
distributions (Uniform, Sky and σ−Uniform) for every
10 draws from the timid multivariate normal distribution. During the main MCMC run at β = 1/2 the ratio
of timid to bold proposals was increased by a factor of
10, and in the ﬁnal super-cooling phase only the timid
multivariate normal distribution was used.
The current algorithm is intended to give a proof of
principle, and is certainly far from optimal. Our choice
of proposal mixtures was based on a few hundred runs
using several diﬀerent mixtures.
There is little doubt
that a better algorithm could be constructed that uses a
larger variety of proposal distributions in a more optimal
The improved F-MCMC algorithm was tested on a
variety of simulated data sets that included up to 10
sources in a 100 bin snippet (once again we are using one year of observations). The algorithm performed
very well, and was able to accurately recover all sources
with SNR > 5 so long as the degree of source correlation was not too large.
Generally the algorithm
could de-blend sources that had correlation coeﬃcients
C12 = (h1|h2)/
(h1|h1)(h2|h2) below 0.3. A full investigation of the de-blending of highly correlated sources is
deferred to a subsequent study. For now we present one
representative example from the 10 source searches.
FIG. 5: Simulated LISA data with 10 galactic binaries. The
solid lines show the total signal plus noise, while the dashed
lines show the instrument noise contribution.
A set of 10 galactic sources was randomly selected from
the frequency range f ∈[0.999995, 1.003164] mHz and
their signals were processed through a model of the LISA
instrument response. The root spectral densities in the
two independent LISA data channels are shown in Figure 5, and the source parameters are listed in Table III.
Note that one of the sources had a SNR below 5. The
data was then search using our improved F-MCMC al-
FIG. 6: Trace plots of the frequencies for two of the model
During the annealing phase (# < 105) the chain
explores large regions of parameter space. The inset shows a
zoomed in view of the chain during the MCMC run at β = 1/2
and the ﬁnal super cooling which starts at # = 1.2 × 105.
TABLE III: F-MCMC search for 10 galactic binaries using a
model with 10 sources. The frequencies are quoted relative to
1 mHz as f = 1 mHz + δf with δf in µHz.
SNR A (10−22)
0.623 1.18 4.15 2.24 2.31 1.45
0.619 1.13 4.16 1.86 2.07 1.01
0.725 0.80 0.69 0.18 0.21 2.90
MCMC ML 11.3
0.725 0.67 0.70 0.82 0.99 1.41
0.907 2.35 0.86 0.01 2.09 2.15
0.910 2.07 0.61 3.13 1.88 2.28
1.126 1.48 2.91 0.46 1.42 1.67
1.114 1.24 3.01 0.40 1.26 2.88
1.732 1.45 0.82 1.58 0.79 2.05
1.730 1.99 0.69 1.27 1.18 2.73
1.969 1.92 0.01 1.04 2.17 5.70
MCMC ML 12.8
1.964 1.97 6.16 0.97 2.00 6.15
2.057 2.19 1.12 1.04 2.13 3.95
1.275 0.57 2.81 0.57 1.82 3.93
2.186 2.21 4.65 3.13 2.01 4.52
MCMC ML 10.0
2.182 2.43 5.06 0.26 2.00 5.54
2.530 2.57 0.01 0.06 0.86 0.50
2.582 2.55 6.03 2.71 1.52 5.58
2.632 1.17 3.14 0.45 2.53 0.69
MCMC ML 13.5
2.627 1.55 3.07 3.08 1.94 6.07
gorithm using a model with 10 sources (70 parameters).
The annealing time was set at 105 steps, and this was followed by a short MCMC run of 2×104 steps and a super
cooling phase that lasted 2×104 steps. The main MCMC
run was kept short as we were mostly interested in extracting maximum likelihood estimates. Figure 6 shows
a trace plot of the chain that focuses on the frequencies
of two of the model sources. During the early hot phase
the chain moves all over parameter space, but as the system cools to β = 1/2 the chain settles down and locks
onto the sources. During the ﬁnal super cooling phase
the movement of the chain is exponentially damped as
the model is trapped at a mode of shrinking width and
increasing height.
The list of recovered sources can be found in Table III.
The low SNR source (SNR = 4.9) was not recovered, but
because the model was asked to ﬁnd 10 sources it instead
dug up a spurious source with SNR = 5.2. With two exceptions, the intrinsic parameters for the other 9 sources
were recovered to within 3σ of the true parameters (using the Fisher matrix estimate of the parameter recovery
errors). The two misses were the frequency of the source
at f = 1.00253 mHz (out by 19σ) and the co-latitude
of the the source at f = 1.002632 mHz (out by 6σ). It
is no co-incidence that these misses occurred for the two
most highly correlated sources (C9,10 = −0.23). The full
source cross-correlation matrix is listed in (31).
(hi|hi)(hj|hj)
0.01 0.01 -0.06
-0.03 0.03
0.01 0.03 -0.05
The MCMC derived maximum likelihood estimates for
the the source parameters can be used to regress the
sources from the data streams. Figure 7 compares the
residual signal to the instrument noise. The total residual power is below the instrument noise level as some of
the noise has been incorporated into the recovered signals.
MODEL SELECTION
In the preceding examples we used models that had
the same number of components as there were sources in
the data snippet. This luxury will not be available with
the real LISA data. A realistic data analysis procedure
will have to explore model space as well as parameter
space. It is possible to generalize the MCMC approach
to simultaneously explore both spaces by incorporating
trans-dimensional moves in the proposal distributions. In
other words, proposals that change the number of sources
FIG. 7: The LISA data channels with the sources regressed
using the maximum likelihood parameter estimates from the
F-MCMC search. The solid lines show the residuals, while
the dashed lines show the instrument noise contribution.
being used in the ﬁt. One popular method for doing this
is Reverse Jump MCMC , but there are other simpler methods that can be used. When trans-dimensional
moves are built into the MCMC algorithm the odds ratio
for the competing models is given by the fraction of the
time that the chain spends exploring each model. While
trans-dimensional searches provide an elegant solution to
the model determination problem in principle, they can
perform very poorly in practice as the chain is often reluctant to accept a trans-dimensional move.
A simpler alternative is to compare the outputs of
MCMC runs using models of ﬁxed dimension. The odds
ratio can then calculated using Bayes factors. Calculating the marginal likelihood of a model is generally very
diﬃcult as it involves an integral over all of parameter
p(s|⃗λ, X)p(⃗λ, X)d⃗λ .
Unfortunately, this integrand is not weighted by the posterior distribution, so we cannot use the output of the
MCMC algorithm to compute the integral. When the
likelihood distribution has a single dominant mode, the
integrand can be approximated using the Laplace approximation:
p(⃗λ, X)p(s|⃗λ, X) ≃p(⃗λML, X)p(s|⃗λML, X)
−(⃗λ −⃗λML) · F · (⃗λ −⃗λML)
where F is given by the Hessian
Fij = ∂2 ln(p(⃗λ, X)p(s|⃗λ, X))
TABLE IV: F-MCMC search for 10 galactic binaries using a
model with 9 sources. The frequencies are quoted relative to
1 mHz as f = 1 mHz + δf with δf in µHz.
SNR A (10−22)
0.623 1.18 4.15 2.24 2.31 1.45
0.619 1.12 4.16 1.86 2.06 1.02
0.725 0.80 0.69 0.18 0.21 2.90
MCMC ML 11.3
0.725 0.67 0.70 0.84 0.98 1.30
0.907 2.35 0.86 0.01 2.09 2.15
0.910 2.09 0.61 3.09 1.82 2.21
1.126 1.48 2.91 0.46 1.42 1.67
1.112 1.24 2.95 0.45 1.31 2.55
1.732 1.45 0.82 1.58 0.79 2.05
1.730 1.95 0.70 1.23 1.19 2.68
1.969 1.92 0.01 1.04 2.17 5.70
MCMC ML 12.9
1.965 1.97 6.17 0.99 1.99 6.11
2.057 2.19 1.12 1.04 2.13 3.95
2.186 2.21 4.65 3.13 2.01 4.52
MCMC ML 10.0
2.182 2.41 5.05 0.23 2.00 5.59
2.530 2.57 0.01 0.06 0.86 0.50
2.536 0.58 5.70 3.04 1.52 4.64
2.632 1.17 3.14 0.45 2.53 0.69
MCMC ML 13.2
2.631 1.41 2.97 0.46 2.02 0.50
When the priors p(⃗λ, X) are uniform or at least slowly
varying at maximum likelihood, Fij is equal to the Fisher
matrix Γij.
The integral is now straightforward and
pX(s) ≃p(⃗λML, X)p(s|⃗λML, X)(2π)D/2
With uniform priors p(⃗λML, X)=1/V , where V is the volume of parameter space, and (2π)D/2/detF=∆V , where
∆V is the volume of the error ellipsoid.
To illustrate how the Bayes factor can be used in model
selection, we repeated the F-MCMC search described in
the previous section, but this time using a model with
9 sources. The results of a typical run are presented in
Table IV. The parameters of the 9 brightest sources were
all recovered to within 3σ of the input values, save for the
sky location of the source with frequency f = 1.00253
mHz. It appears that confusion with the source at f =
1.002632 mHz may have caused the chain to favour a
secondary mode like the one seen in Figure 4.
(35) to estimate the marginal likelihoods for the 9 and
10 parameter models we found ln p9(s) = −384.3 and
ln p10(s) = −394.9, which gives an odds ratio of 1 : 4×104
in favour of the 9 parameter model. In contrast, a naive
comparison of log likelihoods, ln L9 = 413.1 and ln L10 =
425.7 would have favoured the 10 parameter model.
It is also interesting to compare the output of the 10
source MCMC search to the maximum likelihood one gets
by starting at the true source parameters then applying
the super cooling procedure (in other words, cheat by
starting in the neighborhood of the true solution). We
found pcheat(s) = −394.5, and ln Lcheat = 421.5, which
tells us that the MCMC solution, while getting two of the
source parameters wrong, provides an equally good ﬁt to
the data. In other words, there is no data analysis algorithm that can fully deblend the two highly overlapping
CONCLUSION
Our ﬁrst pass at applying the MCMC method to LISA
data analysis has shown the method to have considerable
promise. The next step is to push the existing algorithm
until it breaks. Simulations of the galactic background
suggest that bright galactic sources reach a peak density
of one source per ﬁve 1/year frequency bins . We have
shown that our current F-MCMC algorithm can handle
a source density of one source per ten frequency bins
across a one hundred bin snippet. We have yet to try
larger numbers of sources as the current version of the
algorithm employs the full D = 7N dimensional Fisher
matrix in many of the updates, which leads to a large
computational overhead. We are in the process of modifying the algorithm so that sources are ﬁrst grouped into
blocks that have strong overlap. Each block is eﬀectively
independent of the others. This allows each block to be
updated separately, while still taking care of any strongly
correlated parameters that might impede mixing of the
We have already seen some evidence that high
local source densities pose a challenge to the current algorithm. The lesson so far has been that adding new,
specially tailored proposal distributions to the mix helps
to keep the chain from sticking at secondary modes of
the posterior (it takes a cocktail to solve the cocktail
party problem). On the other hand, we have also seen
evidence of strong multi-modality whereby the secondary
modes have likelihoods within a few percent of the global
maximum. In those cases the chain tends to jump back
and forth between modes before being forced into a decision by the super-cooling process that follows the main
MCMC run. Indeed, we may already be pushing the limits of what is possible using any data analysis method.
For example, the 10 source search used a model with
70 parameters to ﬁt 400 pieces of data (2 channels × 2
Fourier components × 100 bins). One of our goals is to
better understand the theoretical limits of what can be
achieved so that we know when to stop trying to improve
the algorithm!
It would be interesting to compare the performance of
the diﬀerent methods that have been proposed to solve
the LISA cocktail party problem. Do iterative methods
like gCLEAN and Slice & Dice or global maximization
methods like Maximum Entropy have diﬀerent strengths
and weakness compared to MCMC methods, or do they
all fail in the same way as they approach the confusion
limit? It may well be that methods that perform better
with idealized, stationary, Gaussian instrument noise will
not prove to be the best when faced with real instrumental noise.
Acknowledgments
This work was supported by NASA Cooperative Agreement NCC5-579.
 P. Bender et al., LISA Pre-Phase A Report, .
 C. R. Evans, I. Iben & L. Smarr, ApJ 323, 129 .
 V. M. Lipunov, K. A. Postnov & M. E. Prokhorov, A&A
176, L1 .
 D. Hils, P. L. Bender & R. F. Webbink, ApJ 360, 75
 D. Hils & P. L. Bender, ApJ 537, 334 .
 G. Nelemans, L. R. Yungelson & S. F. Portegies Zwart,
A&A 375, 890 .
 L. Barack & C. Cutler, Phys. Rev. D69, 082005 .
 J. R. Gair, L. Barack, T. Creighton, C. Cutler, S. L.
Larson, E. S. Phinney & M. Vallisneri, Class. Quant.
Grav. 21, S1595 .
 A. J. Farmer & E. S. Phinney, Mon. Not. Roy. Astron.
Soc. 346, 1197 .
 S. Timpano, L. J. Rubbo & N. J. Cornish, gr-qc/0504071
 L. Barack & C. Cutler, Phys. Rev. D70, 122002 .
 N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A.
H. Teller & E. Teller, J. Chem. Phys. 21, 1087 .
 W. K. Hastings, Biometrics 57, 97 .
 N. Christensen & R. Meyer, Phys. Rev. D58, 082001
 ; N. Christensen & R. Meyer, Phys. Rev. D64,
022001 ; N. Christensen, R. Meyer & A. Libson,
Class. Qaunt. Grav. 21, 317 .
 N. Christensen, R. J. Dupuis, G. Woan & R. Meyer,
Phys. Rev. D70, 022001 ; R. Umstatter, R. Meyer,
R. J. Dupuis, J. Veitch, G. Woan & N. Christensen, grqc/0404025 .
 R. Umstatter, N. Christensen, M. Hendry, R. Meyer, V.
Simha, J. Veitch, S. Viegland & G. Woan, gr-qc/0503121
 L. Page, S. Brin, R. Motwani & T. Winograd, Stanford
Digital Libraries Working Paper .
 M. Tinto, J. W. Armstrong & F. B. Estabrook, Phys.
Rev. D63, 021101(R) .
 C. Cutler, Phys. Rev. D 57, 7089 .
 N. J. Cornish & L. J. Rubbo, Phys. Rev. D67, 022001
 T. A. Prince, M. Tinto, S. L. Larson & J. W. Armstrong,
Phys. Rev. D66, 122002 .
 C. W. Helstrom, Statistical Theory of Signal Detection,
2nd Edition .
 L.A. Wainstein and V.D. Zubakov, Extraction of Signals
from Noise .
 K.S. Thorne, in 300 Years of Gravitation, edited by S.W.
Hawking and W. Israel , p. 330.
 B.F. Schutz, in The Detection of Gravitational waves,
edited by D.G. Blair , p. 406.
 B.S. Sathyaprakash and S.V. Dhurandhar, Phys. Rev. D
44, 3819 .
 S.V. Dhurandhar and B.S. Sathyaprakash, Phys. Rev. D
49, 1707 .
 C. Cutler and ´E.´E. Flanagan, Phys. Rev. D 49, 2658
 R. Balasubramanian and S.V. Dhurandhar, Phys. Rev.
D 50, 6080 .
 B.S. Sathyaprakash, Phys. Rev. D 50, 7111 .
 T.A. Apostolatos, Phys. Rev. D 52, 605 .
 E. Poisson and C.M. Will, Phys. Rev. D 52, 848 .
 R. Balasubramanian, B.S. Sathyaprakash, and S.V. Dhurandhar, Phys. Rev. D 53, 3033 .
 B.J. Owen, Phys. Rev. D 53, 6749 .
 B.J. Owen and B.S. Sathyaprakash, Phys. Rev. D 60,
022002 .
 L. S. Finn, Phys. Rev. D 46 5236 .
 P. Jaranowski & A. Krolak, Phys. Rev. D49, 1723 .
 P. Jaranowski, A. Krolak & B. F. Schutz, Phys. Rev.
D58 063001 .
 M. H. A. Davis, in Gravitational Wave Data Analysis, edited by B. F. Schutz, .
 N.J. Cornish & S.L. Larson, Phys. Rev. D67, 103001
 J. H¨ogbom, Astr. J. Suppl. 15, 417 .
 N.J. Cornish, Talk given at GR17, Dublin, July ;
N.J. Cornish, L.J. Rubbo & R. Hellings, in preparation
 G. Schwarz, Ann. Stats. 5, 461 .
 Markov Chain Monte Carlo in Practice, Eds. W. R.
Gilks, S. Richardson & D. J. Spiegelhalter, .
 D. Gamerman, Markov Chain Monte Carlo:
Stochastic Simulation of Bayesian Inference, .
 C. Andrieu, N. De Freitas, A. Doucet & M. Jordan, Machine Learning 50, 5 .
 L. Tierney & A. Mira, Statistics in Medicine 18, 2507
 P. J. Green & A. Mira, Biometrika 88, 1035 .
 L. J. Rubbo, N. J. Cornish & O. Poujade, Phys. Rev.
D69 082003 .
 A. Krolak, M. Tinto & M. Vallisneri, Phys. Rev. D70
022003 .