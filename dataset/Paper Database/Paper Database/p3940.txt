HAL Id: hal-01640325
 
Submitted on 20 Nov 2017
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Anomaly Detection in Streams with Extreme Value
Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, Christine Largouët
To cite this version:
Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, Christine Largouët. Anomaly Detection in
Streams with Extreme Value Theory.
KDD 2017 - Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Aug 2017, Halifax, Canada.
￿10.1145/3097983.3098144￿. ￿hal-01640325￿
Anomaly Detection in Streams with Extreme Value Theory
Amossys, Inria, IRISA
 
P.A. Fouque
Univ. Rennes 1, IUF, IRISA
 
A. Termier
Univ. Rennes 1, IRISA
 
C. Largouet
AgroCampus, IRISA
 
Anomaly detection in time series has attracted considerable
attention due to its importance in many real-world applications including intrusion detection, energy management
and ﬁnance. Most approaches for detecting outliers rely on
either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar.
Here, we propose a new approach to detect outliers in
streaming univariate time series based on Extreme Value
Theory that does not require to hand-set thresholds and
makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but
more generally for automatically setting thresholds, making
it useful in wide number of situations. We also experiment
our algorithms on various real-world datasets which conﬁrm
its soundness and eﬃciency.
CCS Concepts
•Computing methodologies →Anomaly detection;
•Mathematics of computing →Time series analysis;
•Information systems →Data stream mining;
Outliers in time series, Extreme Value Theory, Streaming
INTRODUCTION
Anomaly detection is an important research area in data
mining. Many types of anomalies, or outliers, are described
in the literature . One of the most fundamental type of
anomalies are the extreme values (maximum and minimum).
Many work have been proposed for solving this problem.
However, they require some knowledges about the data: either they make assumptions on the underlying distribution
or they need manually set thresholds.
When the data is
static, or is a stream coming from an extremely controlled
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from .
c⃝2017 ACM. ISBN .
environment, such assumptions can safely be made. But in
the general case of streaming data from an open environment, these assumptions are no longer true. They may fail
in unexpected cases.
The issue is that nowadays, more and more critical applications rely on high throughput streaming numerical data like
energy management , cyber-security or ﬁnance .
For example, in intrusion detection, some network attack
techniques rely on intensive scans of the network, which are
characterized by an unusually high number of SYN packets . The main challenge is to learn “normality” in an
ever changing environment and to automatically adapt the
detection method accordingly.
The problem of detecting extreme values in streams can
be expressed as follows: Let (Xt)t≥0 be a streaming time
series of iid observations. Can we set a threshold zq such
that for any t ≥0, the probability to observe Xt > zq is
lower than q (for q as small as desired) ?
To solve this problem, we use the statistical powerful tool
of Extreme Value Theory (EVT). This theory was developed to study the law of extreme values in a distribution
function after the following dramatic event. In the night of
January 31 to February 1 of the year 1953, a set of rare
conditions occurred in the North Sea, leading to a “perfect
storm” scenario.
On the coast of Netherlands, the waves
generated overwhelmed the dikes, causing extensive ﬂooding. The ﬂooding led to the death of 1800+ people in the
Netherlands alone. In the dike case, Xt is the height of the
waves, and zq is the height of the dike.
In the aftermath of this disaster, scientists were tasked to
determine a minimal dike height such that the probability
for waves to exceed this height is extremely low. Statisticians devised an elegant theory for the study of such rare
events . One of the most elegant result of EVT is that
the distribution of the extreme values is almost independent
of the distribution of the data with a theorem similar to
the central limit theorem, for min, max instead of the mean
The main contribution of this paper is to propose an approach for outlier detection in high throughput streaming
univariate and unimodal time series. Thanks to EVT, our
approach makes no distribution assumption on the data: it
is thus a solution to “Research Issue 6” for outlier detection
in data streams as stated by Sadik and Gruenwald in
SIGKDD Explorations 2014. We decline our approach into
two algorithms: SPOT for streaming data having any stationary distribution, and DSPOT for streaming data that
can be subject to concept drift.
Through detailed exper-
iments on synthetic and real data, we show that our approach is accurate for detecting outliers, is computationally
eﬃcient, and for DSPOT reacts quickly to any change in
the stream. For instance, we decide to test our algorithm on
incoming streams without knowledge on their distribution.
We show that we detect very eﬃciently and accurately: (i)
network SYN attacks on a labeled data stream and (ii) peaks
that allows to take decision on stock market (quickly react
for buying or selling shares). Our experiments also conﬁrm
the EVT theory with accuracy and fast convergence.
Analyzing streaming data require the computation of EVT
to be fast and resilient: as a secondary contribution, we propose two improvements on the general method for solving
the EVT problem, that improve both its speed and its robustness. They are used in our algorithms, but they are not
speciﬁc to streaming data and can immediately be applied
to most algorithms using EVT.
RELATED WORK
Classically, anomaly detectors have to highlight what will
be considered as an anomaly, also called outlier. As we propose a statistical method to ﬁnd anomalies, we rely on the
assumption given by Chandola, Banerjee and Kumar in :
“Normal data instances occur in high probability regions of
a stochastic model, while anomalies occur in the low probability regions of the stochastic model”.
A great deal of algorithms for static outlier detection are
given in the literature. The main approaches are distance
based , nearest-neighbor based or clustering based 
and are very well detailed in . Nonetheless, as mentioned
in , most existing outlier detection methods need to scan
several times the data and/or have high time complexity,
thus they cannot be used in data streams.
In , Sadik details the speciﬁcities of the stream
environment and the hardships of outlier detection in this
context. The main constraints are the following: data cannot be scanned twice and new concepts may keep evolving.
Many works which address the streaming case present
distance based algorithms for outlier detection (STORM
 , CORM , DBOD-DS , attributes weighting ).
These methods are able to work on multidimensional streams
with categorical features but they need user deﬁned thresholds which could be a real hindrance in practice.
Current statistical approaches to perform outlier detection
in data stream suﬀer from the inherent problem, namely the
distribution assumption. In , Agarwal assumes a gaussian
model to detect anomalies in multidimensional arrays and
recommends a Box-Cox transformation if it is not the case.
In , a more general mixture model is presented by Eskin, based on a majority distribution and an anomaly one.
However both models need to be learned, so data of each
distribution are required. In , the authors use a probabilist threshold ϵ to discriminate normal or abnormal data
(observations with probability lower than ϵ are anomalies),
but its possible values are lower-bounded by 1/(k+1) where
k is the number of training elements. Thus it needs a huge
training sample if we want a very low false positive rate.
In our work, we do not assume the distribution of the
value we monitor but we rely on powerful theoretical results
to estimate accurately low probability areas and then discriminate outliers. With our single parameter algorithms,
we are able to detect outliers in both stationary and drifting
BACKGROUND
In this section we describe the theoretical background of
the Extreme Value Theory (EVT). We try to explain the
main results and how they could be used to address our
problem (the reader could refer to the rich reference of Beirlant et al. for more details). This part is not a prerequisite to understand the purpose of our algorithm but it
gathers some elements to precise its fundamental basis.
Many techniques allow the scientist to ﬁnd statistical thresholds (quantiles). For instance, we can compute them empirically or assume a distribution. However data do not necessarily follow well-known distributions (Gaussian, uniform,
exponential etc.) so the model step (the choice of the distribution) could be hard, even inappropriate. Moreover, if we
want to predict extreme events, like rare or unprecedented
events (as tidal waves), the empirical method will not give
accurate estimation (an unprecedented event would have a
probability equal to zero).
The extreme value theory addresses these problems by inferring the distribution of the
extreme events we might monitor, without strong hypothesis
on the original distribution.
Mathematically, X is a random variable and F its cumulative distribution function: F(x) = P(X ≤x). We denote by
¯F the “tail” of the distribution: ¯F(x) = 1 −F(x) = P(X >
x). We use Xi to denote both random variables and their
outcomes, however the context will precise their meanings.
For a random variable X and a given probability q we note
zq its quantile at level 1 −q, i.e. zq is the smallest value s.t.
P(X ≤zq) ≥1 −q i.e. P(X > zq) < q.
Extreme value distributions
The goal of the extreme value theory is to ﬁnd the law of
extreme events (e.g. the law of the daily maximum of temperature, or the law of the monthly maximal tide height).
A beautiful result from Fisher, Tippett and later Gnedenko states that, under a weak condition, these extreme events have the same kind of distribution, regardless
of the original one. For instance the maximum of temperatures or tide heights have more or less the same distribution
whereas the distributions of the temperatures and the tide
heights are not likely to be the same. This extreme laws
are called the Extreme Value Distributions (EVD) and they
have the following form :
Gγ : x 7→exp
−(1 + γx)−1
1 + γx > 0.
All the extremes of common standard distributions follow
such a distribution and the extreme value index γ depends
on this original law.
For example, if X1, . . . Xn are n iid
variables (e.g. gaussian N(0, 1)) then Mn = max1≤i≤n Xi is
likely to follow an EVD which extreme value index γ is given
by the initial distribution (for the Gaussian distribution γ =
This result may seem very counterintuitive but we can
give some elements to catch the idea. Indeed, we can easily imagine that for most distributions the probabilities decrease when events are extreme, ie P(X > x) →0 when x
increases. The function ¯F(x) = P(X > x) represents the tail
of the distribution of X. Actually, there are not many possible shapes for this tail and Gγ tries to ﬁt them. The table
1 presents the three possible shapes of the tail and the link
with the extreme value index γ. It gives also an example of
standard distribution which follows each tail behavior. The
parameter τ represents the bound of the initial distribution,
so it could be ﬁnite (ex: uniform cdf) or inﬁnite (ex: normal
cdf). The ﬁgure 1 depicts an example of the three behaviors.
Tail behavior (x →τ)
Heavy tail, P(X > x) ≃x−1
Exponential tail, P(X > x) ≃e−x
Bounded, P(X > x) =
Table 1: Relation between F and γ
Figure 1: Tail distribution ¯Gγ according to γ
Power of EVT
This phenomenon allows us to accurately compute probabilities without inferring the initial law that can be really
complex. It“regularizes”the initial distribution. Indeed, the
central limit theorem states that the mean of n iid random
variables converges in distribution to a normal distribution.
The EVT theorem states the same result for the maximum.
Figure 2: EVD ﬁt of an
unknown cdf
By ﬁtting an EVD to
the unknown input distribution tail (see ﬁgure 2), it
is then possible to evaluate
the probability of potential
extreme events. In particular, from a given probability q it is possible to calculate zq such that P(X >
zq) < q. To solve this problem, the natural way will be
to estimate γ.
Several estimates exist such as Hill’s
estimate and Pickands’ estimate but they give good
results only for certain tail behaviors. Its estimation is hard
and nowadays we do not know a general and eﬃcient method
to compute it (i.e. for all γ ∈R).
Another way exists to ﬁt the tail of the distribution: the
Peaks-Over-Threshold (POT) approach.
Peaks-Over-Threshold (POT) approach
The Peaks-Over-Threshold (POT) approach relies on the
Pickands-Balkema-de Haan theorem (also called second theorem in EVT in comparison to the initial result of
Fisher, Tippett and Gnedenko) given below.
(Pickands-Balkema-de Haan). The cumulative distribution function F ∈Dγ
1 if and only if a function σ exists, for all x ∈R s.t. 1 + γx > 0:
¯F (t + σ(t)x)
t→τ (1 + γx)−1
A clearer view of the theorem is the following:
¯Ft(x) = P (X −t > x | X > t) ∼
This result shows that the excess over a threshold t, written X −t, are likely to follow a Generalized Pareto Distribution (GPD) with parameters γ, σ. In fact, the GPD needs
a third parameter, the location µ, but it is null in our case.
Rather than ﬁtting an EVD to the extreme values of X, the
POT approach tries to ﬁt a GPD to the excesses X −t.
In the case we get estimates ˆγ and ˆσ (our method will be
described in 3.4), the quantile can be computed through :
zq ≃t + ˆσ
where t is a“high”threshold (details will be given in 4.3.3), q
the desired probability, n the total number of observations,
Nt the number of peaks i.e the number of Xi s.t. Xi > t.
Some classical methods can be used to perform the estimation of γ and σ, as the Method of Moments (MOM) or
the Probability Weigted Moments (PWM) but they are less
eﬃcient and robust than the maximum likelihood estimation
 that we describe below.
Maximum likelihood estimation
Likelihood expression
The maximum likelihood estimation remains a natural
way to evaluate the parameters through observations.
X1, . . . Xn are n independent realizations of the random variable X which density (noted fθ) is parametrized by θ (possibly a vector), the likelihood function is deﬁned by:
L (X1, . . . Xn; θ) =
It represents joint density of these n observations. As X1, . . .
Xn are ﬁxed in our context, we try to ﬁnd the parameter θ
such that the likelihood is maximized. It means that we are
looking for the value of θ which makes our observations the
most probable. Practically, we work on the log-likelihood,
so in our case (GPD ﬁt) we have to maximize :
log L(γ, σ) = −Nt log σ −
where Yi > 0 are the excesses of Xi over t (Yi = Xi −
t for Xi > t).
Unfortunately, the optimization must be
done numerically, implying the classical numerical issues.
To perform it, the procedure of Grimshaw can be used.
In a strict GPD case (if the Yi follow exactly a GPD), the
Maximum Likelihood Estimate (MLE) has some good convergence properties in comparison to other estimates (MOM
1It means that the extrema of the distribution of F converge
in distribution to Gγ.
or PWM). It converges in distribution to a Gaussian distribution when the number of peaks Nt →∞when γ > −1
(with a rate of consistency √Nt) and is supereﬃcient when
−1 < γ < −1
2 with a rate of consistency N −γ
The Grimshaw’s trick
The trick of the Grimshaw’s procedure is to reduce the two
variables optimization problem to a single variable equation.
Let us write ℓ(γ, σ) = log L(γ, σ). As we ﬁnd an extremum
of ℓ, we look for solutions of the system ∇ℓ(γ, σ) = 0.
Grimshaw has shown that if we get a solution (γ∗, σ∗) of
this system then the variable x∗= γ∗/σ∗is solution of the
scalar equation u(x)v(x) = 1 where:
v(x) = 1 + 1
log (1 + xYi) .
Moreover, by ﬁnding a solution x∗of this equation, we
can retrieve γ∗= v (x∗) −1 and σ∗= γ∗/x∗. Nevertheless,
the solutions of this equation give only possible candidates
for the maximum of ℓ, so we have to get all the roots, to calculate the corresponding likelihood and keep the best tuple
(ˆγ, ˆσ) as our ﬁnal estimates.
We have to pay attention to how this numerical root search
is done. In fact, the values 1+xYi must be strictly positives.
As the Yi are positive, we must ﬁnd x∗on
where YM = max Yi. Grimshaw calculates also an upperbound x∗
max for this root search:
max = 2Y −Ym
where Ym = min Yi and Y is the mean of the Yi. Finally,
the number of roots is not known and 0 is always a solution
so the implementation must ﬁnd all the solutions and pick
up those which maximizes the likelihood.
OUR CONTRIBUTION
The extreme value theory, through the POT approach,
gives us a way to estimate zq such that P(X > zq) < q
without any strong assumption on the distribution of X and
without any clear knowledge about its distribution.
In this section we use this result to build a streaming outlier detector. First we present the initialization step which
computes an threshold zq from n observations X1, . . . Xn.
Then, we detail our two streaming algorithms which update zq with the incoming data and use it as a decision
bound. We propose SPOT which works in stationary cases,
and DSPOT which takes into account a drift component. Finally we give some theoretical and technical improvements
making our bound update fast and sturdy.
Initialization step
Let us sum up the basic idea of our algorithm. We have n
observations X1, . . . Xn, and we have ﬁxed a risk q. The goal
is to compute a ﬁrst threshold zq verifying P(X > zq) < q.
The ﬁgure 3 shows what we do on this initial batch (calibration). The idea is to set a high threshold t (e.g. a high empirical quantile practically), retrieve the peaks (the excesses
over t) and ﬁt a GPD (Generalized Pareto Distribution) to
them. So that we infer the distribution of the extreme values
and we can compute the threshold zq.
This initialization step is summarized in the algorithm 1.
The choice of t will be discussed in 4.3.3. The set Yt is the
peaks set where we store the observed excesses over t. The
GPD ﬁt is performed with the Grimshaw trick (we detail our
likelihood optimization in 4.3.2) and then we can compute
zq with equation 1.
Algorithm 1 POT (Peaks-over-Threshold)
1: procedure POT(X1, . . . Xn,q)
t ←SetInitialThreshold(X1, . . . Xn)
Yt ←{Xi −t | Xi > t}
ˆγ, ˆσ ←Grimshaw(Yt)
zq ←CalcThreshold(q, ˆγ, ˆσ, n, Nt, t)
return zq, t
7: end procedure
Finding anomalies in a stream
The POT primitive returns a threshold zq which we use
to deﬁne a ”normality bound” (ﬁgure 3). In our streaming
algorithms the POT primitive (algorithm 1) is used as an
initialization step.
The POT primitive may be seen as a training step but
this is partly wrong because the initial batch X1, . . . Xn is
not labeled and is not considered as a ground truth in our
algorithm. The initialization is more a calibration step. Our
streaming anomaly detector uses the next observations to
both detect anomalies and reﬁne the anomaly threshold zq.
Stationary case
The way how the POT estimate is built is really streamready.
As we do not have to store the whole time series
(only the peaks), it requires low memory so we can use
it in a stream.
However, the stream must contain values
from the same distribution, so this distribution cannot been
time-dependent (what we call stationary). In case of timedependency, we will show that our algorithm can be adapted
to drifting cases (see 4.2.2).
The principle of the SPOT algorithm is the following :
we want to detect abnormal events in a stream (Xi)i>0 in
a blind way (without knowledge about the distribution).
Firstly, we perform a POT estimate on the n ﬁrst values
(n ∼1000) and we get an initial threshold zq (initialization). Then for all the next observed values we can ﬂag the
events or update the threshold (see ﬁgure 3). If a value exceeds our threshold zq then we consider it as abnormal (we
can retrieve this anomaly in a list A). The anomalies are not
taken into account for the model update. In the other cases,
either Xi is greater than the initial threshold (peak case)
either it is a “common” value (normal case). In the peak
case, we add the excess to the peaks set and we update the
threshold zq.
In this algorithm we perform the maximum number of
threshold updates but it is possible to do it oﬀ-line at ﬁxed
time interval.
Of course we illustrate the principle only
with upper-bound thresholds but the method is the same
for lower-bound ones and we can even combine both (performances will be presented in 5.4).
Drifting case
SPOT assumes that the distribution of the Xi does not
change over time but it might be restrictive. For instance, a
mid-term seasonality cannot be taken into account, making
Algorithm 2 SPOT (Streaming POT)
1: procedure SPOT((Xi)i>0, n, q)
▷set of the anomalies
zq, t ←POT(X1, . . . Xn, q)
for i > n do
if Xi > zq then
▷anomaly case
Add (i, Xi) in A
else if Xi > t then
▷real peak case
Add Yi in Yt
Nt ←Nt + 1
ˆγ, ˆσ ←Grimshaw(Yt)
zq ←CalcThreshold(q, ˆγ, ˆσ, k, Nt, t)
▷normal case
19: end procedure
calibration
Figure 3: Anomaly detection overview
local peaks undetectable. In this section we overcome this
issue by modeling an average local behavior and applying
SPOT on relative gaps.
We propose Drift SPOT (DSPOT) which makes SPOT
run not on the absolute values Xi but on the relative ones.
We use the variable change X′
i = Xi −Mi where Mi models
the local behavior at time i (see ﬁgure 4). In our implementation we used a moving average Mi = (1/d) · Pd
i−1, . . . X∗
i−d the last d ”normal” observations (so d is
a window parameter). In this new context we assume that
the local variations X′
i come from a same stationary distribution (the hypothesis assumed for Xi in SPOT is now
assumed for X′
This variant uses an additional parameter d, which can
be viewed as a window size. The distinctive features of this
window (noted W ∗) are the following: it might be non continuous and it does not contain abnormal values.
The algorithm 3 shows our method to capture the local
model and perform SPOT thresholding on local variations.
It contains some additional steps so as to compute variable
changes. For these stages, we principally use a sliding windows over normal observations W ∗(lines 3, 7, 24 and 28)
in order to calculate a local normal behavior Mi (lines 4,
8, 25 and 29) through averaging. We logically update the
local behavior only in normal or peak cases (lines 25 and
29). We can retrieve sequentially the “real” extreme quantiles by adding Mi to the calculated zq. Such a choice to
model the local behavior is a very eﬃcient way to adapt
Figure 4: Anomaly detection with drift
Algorithm 3 DSPOT (Streaming POT with drift)
1: procedure DSPOT((Xi)i>0, n, d, q)
▷set of the anomalies
W ∗←[X1, . . . Xd]
▷Last d normal values
Md+1 = W ∗
▷Local model with depth d
for i ∈[[d + 1, d + n]] do
i = Xi −Mi
W ∗←[Xi−d+1, . . . Xi]
zq, t ←POT(X′
d+1, . . . X′
for i > d + n do
i = Xi −Mi
▷variable change
i > zq then
▷anomaly case
Add (i, Xi) in A
▷no update
else if X′
i > t then
▷real peak case
Add Yi in Yt
Nt ←Nt + 1
ˆγ, ˆσ ←Grimshaw(Yt)
zq ←CalcThreshold(q, ˆγ, ˆσ, k, Nt, t)
W ∗←W ∗[1 :] ∪Xi
▷window slide
Mi+1 ←W ∗▷update of the local model
▷normal case
W ∗←W ∗[1 :] ∪Xi
▷window slide
Mi+1 ←W ∗▷update of the local model
32: end procedure
SPOT to drifting contexts. As mentioned in the previous
paragraph, DSPOT can be adapted to compute upper and
lower bounds.
Numerical optimization
The streaming context requests fast and resilient algorithms. In this part, we optimize the GPD ﬁt by reducing
the search of optimal parameters and making it more robust to common numerical stability problems (divergence,
absurd values). The proposition 1 gives a general result for
EVT, improving the Grimshaw’s trick. In 4.3.2, we detail
how we perform the likelihood optimization and ﬁnally we
give some details about the initial threshold t.
Reduction of the optimal parameters search
As we have seen in section 3.4.2, the Grimshaw’s method
for the maximum likelihood estimation requires a numerical
root search in a bounded interval. In this paragraph we show
that we can reduce this interval.
Gathering the following result and the previous bounds
(section 3.4.2), the possible solutions of u(x)v(x) = 1 stand
in the two intervals
Proposition 1. If x∗is a solution of u(x)v(x) = 1,
Proof. Since ∀x > −1, log(1 + x) ≥
v(x) ≥1 + 1
Then applying Jensen’s inequality on the convex function
1+x we get :
u(x)v(x) ≥
If x∗is a solution of the equation u(x)v(x) = 1, we must
Simplifying this inequality, we get
x∗ x∗YmY −2
And a simple sign study gives the result.
How can we maximize the likelihood function?
Finding the maximum of the likelihood boils down to apply a root search. But this is not a trivial task: we do not
know the number of roots and all the roots are potential
candidates to maximize this function.
In , Grimshaw gives an analytic-based routine using
root-ﬁnding algorithm is given however the needed condition
f(a)f(b) < 0 is not emphasized leading to uncertain results.
For this reason we decide to use another method to ﬁnd
these roots.
In our implementation, we set a very small ϵ > 0 (∼10−8)
and we look for the roots of the function w : x 7→u(x)v(x)−1
in both intervals
YM + ϵ, −ϵ
The real ϵ is used to avoid both cases x =
YM (where w is
not deﬁned) and x = 0 (which is always a solution).
Many methods exist to ﬁnd multiple roots of polynomials
(such as Sturm method) but not for the general case of scalar
functions. Furthermore, ﬁnding a root needs a sign change
which may be diﬃcult to detect. Thus we have reduced our
root search to a function minimization which requires less
assumptions.
To ﬁnd the zeros of w we solve numerically the following
optimization problem in both intervals (that we note I):
The minimization can be done with a classical algorithm
(e.g. L-BFGS-B ) starting with k points x0
10) distributed over I. We use this procedure for three reasons: the optimal conﬁguration (x∗
k) is likely to contain
the zeros of w in I, we can retrieve several roots (according
to k) and optimizing procedures do not require sign change
between the bounds.
We perform this optimization in both intervals so we get a
list of candidates to maximize the likelihood (the case x = 0
is also treated). We keep the best of them and we retrieve
the best parameters for the GPD ﬁt.
Initial threshold
We have detailed the Grimshaw procedure (3.4.2 and 4.3)
and how the ﬁnal threshold zq is calculated (equation 1) but
we have not dealt with the initial threshold t. In practice, its
value is not paramount except that it must be“high”enough.
The higher is t, the more relevant will be the GPD ﬁt (low
bias). However, if t is too high, the peaks set Yt would be
little ﬁlled in, so the model would be more variable (high
variance). The only important condition is to ensure that t
is lower than zq, meaning that the probability associated to
t must be lower than 1 −q. In practice we set t to a high
empirical quantile (98%).
A method based on the mean excess plot could be
used to set t in a smarter way but it is less stable and likely
to output absurd values.
EXPERIMENTS
In this section we apply both our algorithms SPOT and
DSPOT on several contexts. First, we compare the computed threshold zq with the theoretical ones through experiments on synthetic data. Then we use real world datasets
from several ﬁelds (network, physics, ﬁnance) to highlight
the properties of our algorithms.
Finally we present the
performance of our implementation.
The real world datasets used in these experiments are all
available on the Internet and we do our utmost to detail
experimental protocols making them totally reproducible.
Our python3 implementation is available at .
(D)SPOT reliability
In this section, we compare our computed threshold zq
to the theoretical one. In other words, we want to check if
zq is the desired threshold (which veriﬁes P(X > zq) < q).
In the same time we evaluate the impact of the number of
observations n in the initial batch.
In the following experiments we set q = 10−3 and we
run SPOT on Gaussian white noises of 15000 values, i.e.
15000 independent values from a standard normal distribution (µ = 0, σ2 = 1). For diﬀerent values of n , we run SPOT k = 100 times and
we retrieve the averaged error made in comparison to the
theoretical threshold:
error rate =
zSPOT −zth
Here, zth is the quantile of the standard normal distribution
at level 1−q, so zth ≃3.09. The ﬁgure 5 presents the results.
First of all the curves show that, for all initial batch sizes
n, the error is low and decreases when the number of observations increases. It means that the computed threshold
zSPOT is close to the theoretical one and tends to it.
Secondly, we have to notice that the error curves all converge to the same value regardless of n. Therefore, n is not
a paramount parameter. In our experiments, we just have
to ensure that n is not too small, otherwise the initialization
step is likely to fail because of a lack of peaks to perform
the GPD ﬁt. Generally, we use n ≃1000.
Number of observations
Error rate (%)
Figure 5: Error rate with the number of observations according to the batch size n
Finding anomalies with SPOT
Intrusion detection example
SPOT computes a robust threshold estimation (zq): the
more data we monitor, the more accurate the estimation
So having a lot of data from the same and unknown
distribution, SPOT can gradually adapt this threshold in
order to detect anomalies. Cyber-security is a typical ﬁeld
where such conﬁgurations appear.
To test our algorithm we use real data from the MAWI
repository which contains daily network captures (15 minutes a day stored in a .pcap ﬁle). In these captures, MAW-
Ilab ﬁnds anomalies and labels them with the taxonomy
proposed by Mazel et al. . The anomalies are referred
through detailed patterns. To be close to real monitoring
systems we converted raw .pcap ﬁles into NetFlow format,
which aggregates packets and retrieves meta-data only, and
is commonly used to measure network activity. Then we labeled the ﬂows according to the patterns given by the MAW-
Ilab. In this experiment we use the two captures from the
17/08/2012 and the 18/08/2012.
Classical attacks are network scans where many SYN packets are sent in order to ﬁnd open and potentially vulnerable
ports on several machines. A relevant feature to detect such
attack is the ratio of SYN packets in a given time window
 . From our NetFlow records we compute this feature on
successive 50 ms time windows and we try to ﬁnd extreme
events. To initialize SPOT we use the last 1000 values of
the 17/08 record and we let the algorithm working on the
18/08 capture.
ratio of SYN packets in 50 ms time window
Figure 6: SYN ﬂood detection at level q = 10−4
The ﬁgure 6 shows the alerts triggered by SPOT (red circles). We recall that each point represents a 50 ms window
gathering several ﬂows (possibly benign and malicious). The
computed threshold (dashed line) seems nearly constant but
this behavior is due to the stability of the measure we monitor (SPOT has quickly inferred the behavior of the feature).
By ﬂagging all the ﬂows in the triggered windows, we get a
true positive rate equal to 86% with less than 4% of false
positives.
The parameter q as a false-positive regulator
In the previous section we noticed that the size of the
initial batch n is not an important parameter insofar as it
does not aﬀect the overall behavior of SPOT. Here, we study
the impact of the main parameter q on the MAWI dataset.
On the ﬁgure 7, the ROC curve shows the eﬀect of q on
the False Positive rate (FPr). Values of q between 10−3 and
10−5 allow to have a high TPr while keeping a low FPr: this
leaves some room for error when setting q.
1. 10−5 5. 10−5
Figure 7: ROC curves on MAWI dataset (the markers give the corresponding value of q)
Finding anomalies with DSPOT
Measure of the magnetic ﬁeld
To show the wide variety of ﬁelds on which we can use
DSPOT, we apply our algorithm on astrophysics measures
from the SPIDR (Space Physics Interactive Data Resource)
 . SPIDR is an online platform which stores and manages
historical space physics data for integration with environment models and space weather forecasts.
Particularly we use a dataset available on Comp-Engine
Time Series which gathers some physical measures taken
by the ACE satellite between 1/1/1995 and 1/6/1995. In
the ﬁgure 8 we monitor a component of the magnetic ﬁeld
(in nT) during several minutes.
This time series is very noisy and contains diﬀerent complex behaviors. We calibrate DSPOT with the n = 2000
ﬁrst values and we run on the 15801 others with q = 10−3
and d = 450. The results are depicted on the ﬁgure 8.
Magnetic field (nT)
Calibration
Figure 8: DSPOT run with q = 10−3, d = 450
At the ﬁrst glance, the bounds are following the signal
and some alarms are triggered by high peaks. After 9000
minutes, the upper bound seems higher than expected.
To understand why, let us divide the signal into two parts:
before and after 8000 minutes. Before 8000, we can observe
that “peaks” lean upwards the trend although the opposite
phenomenon appears after 8000.
During these 8000 ﬁrst
minutes, DSPOT learns that peaks may lean upwards the
trend and it keeps this information in memory. Hence after
8000 minutes, the upper bound stays high in order to accommodate for possible peaks above the trend. DSPOT has
this behavior because it keeps a global memory of all peaks
encountered. If it was not desired, un easy modiﬁcation is
to keep only the last k peaks, for a ﬁxed k.
Stock prices
On Thursday the 9th of February 2017, an explosion happened at Flamanville nuclear plant, in northern France. This
power plant is managed by EDF, a French electricity provider.
The incident was not in the nuclear zone and did not hurt
people . This incident was oﬃcially declared at 11:00 a.m.
making the EDF stock price fall down. This recent event encouraged us to test DSPOT on EDF stock prices.
Obviously, retrieving ﬁnancial data with high resolution is
not within ours grasp. However, some websites like Google
Finance propose intraday ﬁnancial data with a record
per minute. Google Finance keeps these records during 15
days, so we retrieve the records from the 6th to the 8th of
February 2017 for calibration (1062 values) and ran DSPOT
on the explosion day (379 values).
On ﬁgure 9, we notice that DSPOT follows the average
behavior and ﬂags the drop around 11:00 a.m. This may help
a trading system to quickly take actions or warn experts.
EDF stock price ( )
Figure 9: DSPOT run with q = 10−3, d = 10
Performances
Here we give some details about the time and memory performances of our python3 implementation. All these experiments have been made on a laptop with an Intel i5-5300U
CPU @ 2.30GHz (4 cores) and 8 GB RAM.
Both algorithms, SPOT and DSPOT require a ﬁxed memory size for all the variables except for the peak set Yt which
may grow with the number of observations. To measure the
memory performance of our algorithm we report the number
of peaks we stored.
To test the performances of our algorithms we run each
of them on 100 Gaussian white noises of 15000 values (like
the experiment in section 5.1). At every run we measure
the averaged time to perform one iteration and the ratio of
stored peaks, i.e. the number of peaks over the number of
observations.
For these experiments we set q = 10−3 and we use n =
1000 values for the initial batch.
We record these measures in four contexts: SPOT, SPOT both sides (bi-SPOT),
DSPOT and DSPOT both sides (bi-DSPOT). The “both
sides” runs take into account upper and lower thresholds
updates. In drifting cases, we add a drift to the Gaussian
white noise and we use a depth d = 50.
The table 2 presents the averaged time to perform one
iteration (denoted T, measured in µs) and the ratio number
of peaks over number of observations at the end of the run
(denoted M, in %) in mean, best and worst cases.
Time (T, in µs) and Memory (M, in %)
performances
Our algorithm stores a little ratio of all the stream (few
percents).
Logically we store about twice more when we
compute upper and lower thresholds.
Even if the growth
speed of the peaks sets size is low it could be an hindrance
for a long term monitoring. However, the size of the peaks
set could be upper-bounded (with a high bound) making it
work like a wide sliding window (very old peaks would be
dropped) without loss of accuracy.
Finally the time performances of our algorithms show that
our implementation is able to work on streams with more
than 1000 values a second.
CONCLUSION
This paper has presented a novel approach to detect outliers in high throughput numerical time series.
points of our approach is that it does not assume the data
distribution, and it does not require manually set thresholds. It adapts on multiple and complex contexts, learning
how the interest measure behaves. It achieves these results
by using the Extreme Values Theory (EVT). To the best of
our knowledge, it is the ﬁrst time EVT has been used to
detect outliers in streaming data.
There are two kinds of perspectives. From the theoretical
side, in this paper we only exploited a small part of EVT,
we would like to extend this work to the multivariate case
and to non-iid observations.
From the practical side, one of the immediate application
of our approach is automatic thresholding: it can provide
thresholds with strong statistical guarantees that can adapt
to the evolution of a data stream. We wold like to explore
the use of our approach as a building block into more complex systems that require thresholding. Also the EVT on
multivariate cases could address our problem in more general contexts without independence condition