Received April 16, 2019, accepted May 5, 2019, date of publication May 14, 2019, date of current version June 5, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2916828
Insights Into LSTM Fully Convolutional Networks
for Time Series Classification
FAZLE KARIM1, (Graduate Student Member, IEEE), SOMSHUBRA MAJUMDAR
AND HOUSHANG DARABI
1, (Senior Member, IEEE)
1Department of Mechanical and Industrial Engineering, University of Illinois at Chicago, Chicago, IL 60607, USA
2Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607, USA
Corresponding author: Houshang Darabi ( )
The Research Open Access Publishing (ROAAP) Fund of the University of Illinois at Chicago ﬁnancially supported towards the open
access publishing fee for this article.
ABSTRACT Long short-term memory fully convolutional neural networks (LSTM-FCNs) and Attention
LSTM-FCN (ALSTM-FCN) have shown to achieve the state-of-the-art performance on the task of classifying time series signals on the old University of California-Riverside (UCR) time series repository.
However, there has been no study on why LSTM-FCN and ALSTM-FCN perform well. In this paper,
we perform a series of ablation tests (3627 experiments) on the LSTM-FCN and ALSTM-FCN to provide
a better understanding of the model and each of its sub-modules. The results from the ablation tests
on the ALSTM-FCN and LSTM-FCN show that the LSTM and the FCN blocks perform better when
applied in a conjoined manner. Two z-normalizing techniques, z-normalizing each sample independently and
z-normalizing the whole dataset, are compared using a Wilcoxson signed-rank test to show a statistical
difference in performance. In addition, we provide an understanding of the impact dimension shufﬂe that
has on LSTM-FCN by comparing its performance with LSTM-FCN when no dimension shufﬂe is applied.
Finally, we demonstrate the performance of the LSTM-FCN when the LSTM block is replaced by a gated
recurrent unit (GRU), basic neural network (RNN), and dense block.
INDEX TERMS Convolutional neural network, long short term memory recurrent neural network, time
series classiﬁcation.
I. INTRODUCTION
Time series classiﬁcation has recently received a lot of attention over the past three decades – . Such data is widely
available everywhere and collected with various sensors .
A variety of real world sensors capture time series information such as weather readings , stock market data , and
EEG / ECG , .Time series classiﬁcation is a supervised
learning task that classiﬁes a series of data points that are
commonly collected in equal intervals and depicted in a
sequential order . Typically, the input to a time series
classiﬁcation problem is a time series signal, X ∈RT×F,
such that Xt ∈RF is the input feature vector of length F
at time step t, where 0 < t ≤T. The maximum length
of each time series, T, may vary . The output of a time
series classiﬁcation problem, Y ∈{1, . . . , C}, is a discrete
class/category label that represents the input time series signals. The total number of classes, C, is dependent on the time
series classiﬁcation problem. The main challenges faced in
The associate editor coordinating the review of this manuscript and
approving it for publication was Xi Peng.
time series classiﬁcation are how to efﬁciently (speed and
space) and effectively (accurately) classify a time
Some of the earliest work that applies data mining techniques for time series classiﬁcation dates back to the early
1990s when authors would apply various algorithms onto
single artiﬁcial datasets , . Since the initial decade
of research in this ﬁeld, Chen et al. have graciously
helped the community by collecting and making 85 time
series datasets from various domains available online to the
public for research purposes. This has lead to rapid progress
in the ﬁeld of time series classiﬁcation and yielded a significant body of work. Recently, Dau et al. have updated
the repository with 43 datasets with time series datasets.
These datasets have a signiﬁcantly higher number of samples,
several of which have long time dependencies or incorporates variable sequence lengths, which makes the task of
sequence classiﬁcation far more exigent. Most of the new
datasets also have a signiﬁcantly larger test set and a few
have variable time series lengths to represent real-world
scenarios .
2019 IEEE. Translations and content mining are permitted for academic research only.
Personal use is also permitted, but republication/redistribution requires IEEE permission.
See for more information.
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
Several researchers have used the old archive benchmark
datasets to propose feature-based models – , ensembles , and deep learning models – to accurately classify the time series data. One of the current state-ofthe-art models that classify the time series datasets from the
repository developed by Chen et al. are the Long Short
Term Memory Fully Convolutional Network (LSTM-FCN)
and the Attention LSTM-FCN proposed by Karim et al. .
LSTM-FCN and ALSTM-FCN are deep learning models,
a Fully Convolutional Network (FCN) module augmented
with a Long Term Short Term Recurrent Neural Network
(LSTM) that classify time series datasets. LSTM-FCN and
ALSTM-FCN have received a lot of attention from the time
series classiﬁcation community due to their advantage over
other models. In terms of classiﬁcation accuracy, both the
models outperform several traditional time series classiﬁcation models, while requiring minimal pre-processing of the
data. A signiﬁcant advantage of utilizing these models is
their ability to compute features on their own, eliminating
the requirement for signiﬁcant domain expertise and manual feature extraction. Furthermore, both these models can
easily scale with a larger amount of time series data, which
is generated daily by automated processes. Finally, LSTM-
FCN has already been deployed in real world scenarios. One
such application is to efﬁciently classify pet dog sounds
using resource constrained sensors . The original models,
LSTM-FCN and ALSTM-FCN, lacked the explanation of
each sub-module. In this paper, we provide detailed ablation
tests to explain the sub-modules of the models.
The remainder of the paper is organized as follows.
Section III presents the parameters used in developing
the models and discusses the experiments performed.
Section IV compares two z-normalization schemes. Subsequently, Section V provides a detailed ablation test on the
deep learning models, LSTM-FCN and ALSTM. Finally,
Section VI concludes the paper.
II. BACKGROUND REVIEW
A. TEMPORAL CONVOLUTIONS NETWORKS
Temporal convolution network is a type of artiﬁcial neural
network whose input is generally a time series signal, X,
where Xt ∈RF is the input feature vector of length F for
time step t for 0 < t ≤T. T may vary for each time series
sequence .
In a temporal convolution network, 1D ﬁlters are applied
on each convolutional layer, L, that discovers the evolution of
the input signal over the course of an action. Lea et al. 
discusses each ﬁlter of each layer are parameterized by tensor
W (l) ∈RFl×d×Fl−1 and biases b(l) ∈RFl, where l ∈{1, ..., L}
is the layer index and d is the ﬁlter duration. The i-th element
of the activation ˆE(l)
∈RFl of the l-th layer is a function of the
activation matrix E(l−1) ∈RFl−1×Tl−1 of the previous layer,
such that,
i,t′,., E(l−1)
for each time t where f (·) is a Rectiﬁed Linear Unit.
Typically, a convolutional layer is followed by batch normalization . Subsequently, this is trailed by an activation
function (a Rectiﬁed Linear Unit or a Parametric Rectiﬁed
Linear Unit ).
B. RECURRENT NEURAL NETWORKS
Recurrent Neural Networks (RNN) are a type of artiﬁcial
neural network that demonstrates stateful temporal behavior
given a time sequence. Pascanu et al. proposed an RNN
to preserve a hidden vector h as a state that is updated at time
ht = tanh(Wht−1 + Ixt),
where tanh is the hyperbolic tangent function, xt is the input
vector at time step t, W is the recurrent weight matrix and I
is the projection matrix. The prediction, yt, is computed such
yt = softmax(Wht−1),
where h is a hidden state, W is a weight matrix and softmax
operation normalizes the output of the model to a valid probability distribution and the logistic sigmoid function is shown
as σ. Deep RNNs can be formed by stacking the output of
one RNN as the input to another, such that the hidden state,
hl−1 of a RNN layer l−1, is an input to the hidden state, hl
of another RNN layer l. In other words,
t−1 + Ihl−1
RNNs are prone to be affected by vanishing gradients.
This issue is addressed using a Long short-term memory
(LSTM) or a Gated Recurrent Unit (GRU).
C. LONG SHORT-TERM MEMORY RNNs
To solve the vanishing gradient problem, LSTM RNNs utilize
gating functions in their state dynamics . Each LSTM cell
contains a hidden vector, h, and a memory vector, m. At each
time step, the memory vector regulates the state updates and
outputs, such that the following computation is performed
computed as follows (ﬁrst depicted by Graves et al. ):
gu = σ(Wuht−1 + Iuxt)
gf = σ(Wf ht−1 + If xt)
go = σ(Woht−1 + Ioxt)
gc = tanh(Wcht−1 + Icxt)
mt = gf ⊙mt−1 + gu ⊙gc
ht = tanh(go ⊙mt)
where gu, gf , go, gc are the activation vectors of the input, forget, output and cell state gates respectively, Wu, Wf , Wo, Wc
are the recurrent weight matrices, Iu, If , Io, Ic portrays the
projection matrices, σ is the logistic sigmoid function, ⊙is an
elementwise multiplication, and ht is the hidden state vector
of the tth time step.
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
D. GATED RECURRENT UNIT
Cho et al. proposed a modiﬁcation to the LSTM RNN
that also solves the vanishing gradient problem using an
update and reset gate. Due to the simpler gating structure of
the model, reduced number of gates and thereby parameters,
it is considered to be an efﬁcient alternative to the LSTM
zt = σg(Wzxt + Uzht−1)
rt = σg(Wrxt + Urht−1)
ht = (1 −zt) ⊙ht−1
+zt ⊙σh(Whxt + Uh(rt ⊙ht−1))
where xt is the input vector at time step t, zt is the update gate
vector, rt is the reset gate vector, ht is the hidden state and
output vector, Wz and Wr are the trainable weight matrices
for the update and reset gate respectively, Uz and Ur are the
trainable recurrent weight matrices for the update and reset
gate respectively, σg is the logistic sigmoid function, σh is the
hyperbolic tangent function and ⊙is the Hadamard product
of the two inputs.
E. FULLY CONNECTED (DENSE) LAYER
A fully connected layer can be described as a dense matrix
multiplication of the input vector with a trainable weight
matrix, and optionally, the addition of a trainable bias vector
to the output. The output of each layer can be represented by:
output = a (Wx + b)
where W is a weight matrix, b is a bias vector, and a is a
non-linear activation function. Common activation functions
are the Rectiﬁed Linear Unit (ReLU), the logistic sigmoid
function, or a hyperbolic tangent function.
III. EXPERIMENTS
The LSTM-FCN and ALSTM-FCN models are trained on
various released UCR benchmark datasets. The benchmark
datasets include a train and test set which is used for model
training and validation. We utilize the same structure of the
models as the original models and perform grid search
to ﬁnd the optimal number of LSTM cells from the set
consisting of 8, 64 or 128 cells. All models are trained for
2000 epochs. The batch size of 128 is kept consistent for all
datasets. All LSTM or Attention LSTM layers are followed
by dropout layer with a probability of 80 percent to prevent
overﬁtting. Class imbalance is handled via a class weighing
scheme inspired by King and Zeng . All models are
trained using the Keras library with Tensorﬂow as
the backend and are made available publically.1
All models are trained via gradient descent using the Adam
optimizer . The initial learning rate was set to 1e-3 and is
reduced to a minimum of 1e-4. We reduced the learning rate
by a factor of 1/
2, whenever the training loss of 100 consecutive epochs do not improve. The model weights are
1The codes and weights of each model are made available at
 
TABLE 1. Performance comparison of LSTM-FCN and ALSTM-FCN with the
baseline models. Green cells designate instances where our performance
matches or exceeds state-of-the-art results. Bold values denote model
with the best performance.
updated only through the training loss. The accuracies we
report are based on the best models we ﬁnd. The methodology
we follow is common in various deep learning applications
 – . In addition, we utilize the initialization proposed
by He et al. for all convolutional layers. The input data
is z-normalized and the datasets with variable length time
series are padded with zeros at the end to match the longest
time series in that dataset. All models are evaluated using
classiﬁcation accuracy and mean-per-class-error (MPCE),
which is deﬁned as the average error of each class for all the
datasets and mathematically represented as following:
1 −accuracy
number of unique classes
IV. DATASET ABLATION TEST
Table 1 represents the accuracies obtained by applying
LSTM-FCN and ALSTM-FCN on the 43 new UCR benchmark datasets based on two z-normalization schemes when
normalizing the datasets prior to training. These 43 UCR
benchmark datasets are the only datasets in the repository
that are not padded, normalized or pre-processed in any way.
The dataset mean and standard deviation is calculated as
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
FIGURE 1. Ablation test–visual representation of the input signal after transformation through randomly selected filters
from each convolutional layer.
the mean and standard deviation of only the train set, and
then applied to both train and tests, whereas the sample mean
and standard deviation was calculated for each individual
sample separately. When using LSTM-FCN and ALSTM-
FCN, our results indicate that when the whole dataset is
z-normalized, it performs better on 34 datasets (LSTM-FCN)
and 30 datasets (ALSTM-FCN) than when each sample is
z-normalized separately. In addition, a Wilcoxon signedrank test was performed to compare this, yielding a
p-value of 4.57e-07. We chose the signiﬁcance level (alpha)
of 0.05 for all statistical tests. Since the p-value is less
than the Dunn-Sidak corrected signiﬁcance level (alpha)
of 0.025, we conclude that z-normalizing the whole dataset
performs differently than when z-normalizing each sample.
We recommend z-normalizing the whole dataset iff one
knows that the train set can sufﬁciently represent the global
population of the dataset. In other words, if no a priori information or domain knowledge is known about the train set,
it is safer to z-normalize each sample separately, as explained
by Dau et al. . They provide an example explaining why
it is safer to z-normalize each sample separately using the
dataset GunPoint, where a video is converted into a time
series. If another video is taken where ‘‘the camera is zoomed
in or out, or the actors stood a little closer to the camera, or that the female actor decided to wear new shoes with a
high heel’’ , the converted time series will be different. The
train set will not have this distribution as the validation or test
set, and the prediction made by this classiﬁer will be off.
In this scenario, it would be best to z-normalize each sample
separately. On the other hand, if a domain expert knows the
train set contains a wide range of samples that represent the
different types and amplitudes of time series, z-normalizing
via the dataset mean and standard deviation would be wiser
when using LSTM-FCN and ALSTM-FCN as classiﬁers.
V. MODEL ABLATION TESTS
We perform an ablation study on our model to provide an
understanding of the impact of each layer of our model and
show how signiﬁcantly they affect the performance measure.
The LSTM-FCN and ALSTM-FCN models are applied to
61 datasets from the UCR repository, such that each dataset
is sample z-normalized. Each dataset chosen were datasets
that outperform the SOTA non-ensemble classiﬁers, BOSS
 and WEASEL . We apply BOSS and WEASEL
on all UCR datasets based on code and default parameters
provided by the author online. It should be noted, this paper is
not comparing results with BOSS and WEASEL. BOSS and
WEASEL is only used to select datasets that would provide a
better understanding of LSTM-FCN and ALSTM-FCN when
it performs well.
In addition, the signiﬁcance level (alpha) of 0.05 is selected
for all statistical tests. The null hypothesis and alternative
hypothesis of all Wilcoxon signed-rank test are as follows:
Ho : Medianproposed model = Mediancompared model
Ha : Medianproposed model ̸= Mediancompared model.
An essential point of discussion concerning the working
of the LSTM-FCN and ALSTM-FCN model is the choice of
utilizing an LSTM Recurrent module in conjunction with the
FCN block. In the following ablation tests, we study the performance of the individual components which constitute the
LSTM-FCN and ALSTM-FCN models, their performance
compared to a linear baseline, as well as the empirical and
statistical analysis on the performance of the individual components and the ﬁnal model.
A. FULLY CONVOLUTIONAL BLOCK
LSTM-FCN and ALSTM-FCN comprise of a fully convolutional block and an LSTM/Attention LSTM block. The FCN
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
block has three stacked temporal convolutional blocks with
the number of ﬁlters deﬁned as 128, 256, and 128. Figure 1
depicts a visual representation of a single sample from the
UMD dataset after transformation via a random ﬁlter selected
from each of the convolutional blocks.
As can be noticed, a randomly selected ﬁlter from the
ﬁrst CNN block is applying a form of noise reduction that
is learned via gradient descent, whereas two subsequent randomly selected ﬁlters from the later layers are transforming
the data to be far more inconsistent. Based on our analysis of
a few ﬁlters on various datasets, we conclude that the CNN
ﬁlters in all layers act as feature extractors and transform the
data into separable classes. The model learns the parameters
of these transformations on its own via stochastic gradient
descent. If a dataset sample requires the removal of noise, it is
learned by a few ﬁlters of the ﬁrst CNN layer. It is challenging
to postulate what type of transformation is occurring in each
ﬁlter, as the model transforms the data differently for each
of the datasets, on the basis of random initialization of the
convolution kernels and order of stochastic gradient descent
updates. However, the ﬁlter parameters are learned such that
their objective is to transform the data into separable classes.
In order to empirically demonstrate that the LSTM-FCN
and ALSTM-FCN models are learning to separate the classes
better, we examine the features from the FCN block by
applying them to a tuned linear SVM classiﬁer. The results
are summarized in Table 2. The linear SVM classiﬁer that
is applied on the features extracted from the FCN block is
better in 41 datasets (LSTM-FCN model) and 45 datasets
(ALSTM-FCN model) as compared to when the tuned linear
SVM classiﬁer is applied on to the raw signal. Based on this
knowledge, we conclude that the FCN block is transforming
the data into separable classes.
B. LSTM/ALSTM RECURRENT BLOCK
Due to the dimensional shufﬂe that is applied before the
LSTM block, the features extracted by LSTM block by itself
do not contribute signiﬁcantly to the overall performance.
When these features are applied onto a tuned linear SVM
classiﬁer, the classiﬁer is better in only 19 datasets (for the
LSTM block) and 4 datasets (for the ALSTM block) as
compared to when the tuned linear SVM classiﬁer is applied
to the raw input dataset. The above indicates that the LSTM,
by itself, is not separating the data into linear separable
C. LSTM/ALSTM CONCATENATED WITH FCN BLOCK
Nevertheless, when the features of the LSTM block/ALSTM
block are concatenated with the CNN features, we obtain
a more robust set of features that can better separate the
classes of the dataset. The above insight is statistically validated by applying the concatenated features to a single layer
perceptron classiﬁer which accepts the extracted features as
input (due to the fact that the data is transformed into separable classes). The training scheme of all perceptron models
is kept consistent with how we train all LSTM-FCN and
TABLE 2. Ablation test - linear SVM performance comparison of
LSTM/ALSTM Block, FCN Block with the raw signals. Green cells and
orange cells designate instances where the linear SVM model on the
block exceeds the linear SVM on raw signals. Bold values denotes the
block with the best performance using the linear SVM classifier. Count∗
represents the number of bold values in that column.
ALSTM-FCN models, as detailed in Section III. Results,
shown in Table 3, show that the features from of the
LSTM/ALSTM block coupled with the features from the
FCN block improve the model performance.
For the ALSTM-FCN model, the ALSTM features joined
with the FCN features outperform the features from the
ALSTM block or the FCN block on 49 datasets, yielding to
a p-value of 1.34e-08 when a Wilcoxon Signed-rank test 
is applied. Similarly, the LSTM features joined with the FCN
features in the model LSTM-FCN outperform the features
from the LSTM block or the FCN block on 54 datasets, yielding to a p-value of 1.22e-08. The Dunn-Sidak corrected
signiﬁcant alpha value is 0.02.
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
TABLE 3. Ablation test - MLP performance comparison of LSTM/ALSTM
Block, FCN Block, LSTM/ALSTM-FCN Block and the raw signals. Green
cells and orange cells designate instances where the MLP model on the
block exceeds the MLP on raw signals. Bold values denotes the block
with the best performance using the MLP classifier. Count∗represents the
number of bold values in that column.
It is evident that when applying the LSTM block (with
dimension shufﬂe) and the FCN block parallelly, the blocks
augment each other, and force each other to detect a set
of features which when combined, yield an overall better
performing model. In other words, the LSTM block attached
with the FCN block statistically helps improve the overall
performance of the model providing informative features that
in conjunction with the FCN features, are useful in separating
the classes further.
D. DIMENSION SHUFFLE vs NO DIMENSION SHUFFLE
Another ablation test performed is to check the impact dimension shufﬂe has on the overall behavior of the model. The
dimension shufﬂe transposes the input univariate time series
of N time steps and 1 variable into a multivariate time
series of N variables and 1 time step. In other words, when
dimension shufﬂe is applied to the input before the LSTM
block, the LSTM block will process only 1 time step with N
variables.
In this ablation test, LSTM-FCN with dimension shufﬂe
is compared to LSTM-FCN without dimension shufﬂe on all
128 UCR datasets using a cell size of 8, 64, 128 (yielding
to a total of 128 × 3 = 384 experiments). LSTM-FCN
with dimension shufﬂe outperforms LSTM-FCN without
dimension shufﬂe on 258 experiments, ties in 27 experiments, and performs worse in 99 experiments. For the experiments when LSTM-FCN with dimension shufﬂe outperforms
improved on average by 6.00%. Conversely, for the experiments when LSTM-FCN with dimension shufﬂe performs
worse than LSTM-FCN without dimension shufﬂe, the accuracy is worse by an average of 5.26%. A Wilcoxson signedrank test results in a p-value of 3.69E −17, indicating a
statistical difference in performance where LSTM-FCN with
dimension shufﬂe performs better. This result is contrary
to what most people would hypothesize. LSTM-FCN without dimension shufﬂe overﬁts the UCR datasets in more
instances than LSTM-FCN with dimension shufﬂe. This is
because the LSTM block without dimension shufﬂe by itself
performs extremely well. The FCN block and LSTM block
without the dimension shufﬂe does not beneﬁt each other.
Another critical fact to note is that the LSTM-FCN with
dimension shufﬂe processes the univariate time series in
one time step. The gating mechanisms of the LSTM-FCN
is only being applied on a single time step. This attributes
to why LSTM with dimension shufﬂe by itself performs
poorly. However, as noticed in Section V-C, when applying
the LSTM block with dimension shufﬂe and the FCN block
parallelly, the blocks augment each other, while improving its
overall performance. To the best of our knowledge, we believe
the LSTM block with a dimension shufﬂe acts as a regularizer
to the FCN block, forcing the FCN block to improve its
performance.
E. REPLACING LSTM WITH GRU, RNN, AND A DENSE
Since the usage of the LSTM block when applying dimension
shufﬂe to the input is atypical, we replace the LSTM block
with a GRU block (8, 64, 128 cells), basic RNN block (8,
64, 128 cells), and a Dense block with a sigmoid activation function (8, 64, 128 units) on all 128 datasets (total
of 384 experiments on each model).The intuition behind
selecting an RNN block and a GRU block is that these blocks
have similar properties to an LSTM block, and differ only
in their capacity to learn long term temporal dependencies.
Furthermore, a dense layer is selected to compare against the
atypical usage of the LSTM block, so that we may analyze
whether the complex interaction within the recurrent gates
of the LSTM can be simpliﬁed into a single fully connected
layer. We chose the sigmoid activation function for the Dense
block, instead of the standard Rectifying Linear Unit (ReLU)
activation, as we wish to compare the effectiveness of the
VOLUME 7, 2019
F. Karim et al.: Insights Into LSTM Fully Convolutional Networks for Time Series Classification
TABLE 4. Ablation test - Wilcoxson signed-rank test comparing LSTM-FCN
with GRU-FCN, RNN-FCN, and Dense-FCN. The values in parenthesis
depicts the number of wins, ties, and losses the row index has with the
header. Red cell depicts when the test fails to reject the null hypothesis.
gating effect exhibited by the 3 gates of the LSTM. The
majority of the gates of the LSTM use the sigmoid activation
function. Therefore, we construct the Dense block to also use
the same. The input to the GRU block, RNN block, and Dense
block had a dimension shufﬂe applied onto it. Replacing the
LSTM block of LSTM-FCN with a GRU block was ﬁrst proposed by Elsayed et al. . Table 4 summarizes a Wilcoxson
signed-rank test when LSTM-FCN with dimension shufﬂe is
compared to GRU-FCN, RNN-FCN, and Dense-FCN.
The Wilcoxson signed-rank test depicts LSTM-FCN with
dimension shufﬂe to statistically outperform GRU-FCN,
RNN-FCN, Dense-FCN. Surprisingly, the model to perform most similar to LSTM-FCN with dimension shuf-
ﬂe is Dense-FCN. LSTM-FCN outperforms Dense-FCN
in 231 experiments, ties in 35 experiments and performs
worse in 118 experiments.
An interesting observation is that GRU-FCN does not statistically outperform Dense-FCN. Based on our 384 experiments, GRU-FCN outpeforms Dense-FCN in 160 experiments, ties in 49 experiments, while performing worse
in 175 experiments. As a disclaimer, we performed each of
these experiments only once, therefore there may be some
deviation when run multiple times due to the inherent variance of training using random initialization. However, due to
the sample size of 384, we believe the variance will not be
signiﬁcant to result in a different conclusion.
VI. CONCLUSION & FUTURE WORK
In this paper, we provide a better understanding of
LSTM-FCN, ALSTM-FCN and their sub-modules through
a series of ablation tests (3627 experiments). We show that
z-normalizing the whole dataset yields to results different
than z-normalizing each sample. For the model LSTM-FCN
and ALSTM-FCN, we recommend z-normalizing the whole
dataset only in situations when it is known that the training set
is a good representation of the global population. Moreover,
our ablation tests show that the LSTM/ALSTM block and
the FCN block yields to a better performing model when
applied in a conjoined manner. Further, the performance
of LSTM-FCN is enhanced only when dimension shufﬂe
is applied before the LSTM block. Finally, in this paper,
we substitute the LSTM block with either a GRU block,
a RNN block or a Dense block to observe the effect of such a
substitution. Our results indicate LSTM-FCN to outperform
GRU-FCN, RNN-FCN and Dense-FCN.
An exciting area for future work is to investigate why
LSTM-FCN and ALSTM-FCN underperform in a few UCR
datasets and to ascertain whether the models can be made
more robust to the various types of time series data.
Furthermore, integrating the models in both low-power systems and wearables for on-device classiﬁcation is of great
interest. Finally, further inroads can be made in streaming
time series classiﬁcation by the utilization of these models.In
the future, researchers that want to implement deep learning
models for time series classiﬁcation need to focus on generalization of the model on unseen sequences, and reduce over-
ﬁtting as the UCR repository contain small real world data
ACKNOWLEDGMENT
The authors would like to thank all the researchers that
helped create and clean the data available in the updated UCR
Time Series Classiﬁcation Archive. They would also like to
show their gratitude to the administrators of the UCR Time
Series Classiﬁcation Archive, Dau et al. Sustained research
in this domain would be much more challenging without their
Further, the authors would like to acknowledge the
Research Open Access Publishing (ROAAP) Fund of the
University of Illinois at Chicago for ﬁnancial support towards
the open access publishing fee for this article.
(Fazle Karim and Somshubra Majumdar contributed
equally to this work.)