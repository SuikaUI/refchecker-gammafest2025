The Template Update Problem
Iain Matthews, Takahiro Ishikawa, and Simon Baker
The Robotics Institute, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213, USA
{iainm,taka,simonb}@cs.cmu.edu
Template tracking is a well studied problem in computer vision which dates
back to the Lucas-Kanade algorithm of 1981. Since then the paradigm has
been extended in a variety of ways including: arbitrary parametric transformations of the template, and linear appearance variation. These extensions
have been combined, culminating in non-rigid appearance models such as
Active Appearance Models (AAMs) and Active Blobs. One question that
has received very little attention is how to update the template over time so
that it remains a good model of the object being tracked. This paper proposes an algorithm to update the template that avoids the “drifting” problem
of the naive update algorithm. Our algorithm can be interpreted as a heuristic
to avoid local minima. It can also be extended to templates with linear appearance variation. This extension can be used to convert (update) a generic,
person-independent AAM into a person speciﬁc AAM.
Introduction
Template tracking is a well studied problem in computer vision which dates back to .
An object is tracked through a video sequence by extracting an example image of the
object, a template, in the ﬁrst frame and then ﬁnding the region which matches the template as closely as possible in the remaining frames. Template tracking has been extended
in a variety of ways, including: (1) to allow arbitrary parametric transformations of the
template , (2) to allow linear appearance variation , and (3) to be efﬁcient .
Combining these extensions has resulted in non-rigid appearance models such as Active
Appearance Models (AAMs) and Active Blobs .
The underlying assumption behind template tracking is that the appearance of the
object remains the same throughout the entire video. This assumption is generally reasonable for a certain period of time, but eventually the template is no-longer an accurate
model of the appearance of the object. A naive solution to this problem is to update the
template every frame (or every n frames) with a new template extracted from the current
image at the current location of the template. The problem with this naive algorithm is
that the template “drifts.” Each time the template is updated, small errors are introduced in
the location of the template. With each update, these errors accumulate and the template
steadily drifts away from the object. See Figure 1 for an example.
In this paper we propose a template update algorithm that does not suffer from drift.
The template can be updated in every frame and yet still stays ﬁrmly attached to the
original object. The algorithm is a simple extension of the naive algorithm. As well as
maintaining a current estimate of the template, our algorithm also retains the ﬁrst template
from the ﬁrst frame. The template is ﬁrst updated as in the naive algorithm with the image
at the current template location. However, to eliminate drift, this updated template is then
aligned with the ﬁrst template to give the ﬁnal update. We ﬁrst evaluate this algorithm
qualitatively and show that it can update the template without introducing drift. Next, we
reinterpret the algorithm as a heuristic to avoid local minima and quantitatively evaluate
it as such.
We then consider the more general case of template tracking with linear appearance
variation. Speciﬁcally we generalize our template update algorithm to AAMs . In this
context our appearance update algorithm can also be interpreted as a heuristic to avoid
local minima and so we again quantitatively evaluate it as such. We also demonstrate
how our algorithm can be applied to automatically convert a generic person-independent
AAM into a person speciﬁc AAM.
Template Tracking
We begin by considering the original template tracking problem where the object is represented by a single template image. Suppose we are given a video sequence of images
In(x) where x = (x,y)T are the pixel coordinates and n = 0,1,2,... is the frame number. In
template tracking, a subregion of the initial frame I0(x) that contains the object of interest
is extracted and becomes the template T(x). (The template is not necessarily rectangular,
and might, for example, be a face shaped region .)
Let W(x;p) denote the parameterized set of allowed deformations of the template,
where p = (p1,... pk)T is a vector of parameters. The warp W(x;p) takes the pixel x in
the coordinate frame of the template T(x) and maps it to a sub-pixel location W(x;p) in
the coordinate frame of the video In(x). The set of allowed warps depends on the type of
motions we expect from the object being tracked. If the object is a roughly planar image
patch moving in 2D we might consider the set of similarity warps:
(1+ p1)·x −
+ (1+ p1)·y + p4
where there are 4 parameters p = (p1, p2, p3, p4)T. In general, the number of parameters
k may be arbitrarily large and W(x;p) can be arbitrarily complex. (A complex example
is the set of piecewise afﬁne warps used to model non-rigidly moving objects in Active
Appearance Models .)
The goal of template tracking is to ﬁnd the best match to the template in every subsequent frame in the video. The sum of squared error is normally used to measure the degree
of match between the template and the video frames. The goal is therefore to compute:
pn = argmin
[In(W(x;p))−T(x)]2
for n ≥1 and where the summation is over all of the pixels in the template. (Excuse
the abuse of terminology.) The original solution to the non-linear optimization in Equation (2) was the Lucas-Kanade algorithm . A variety of other algorithms have since
been proposed. See for a recent survey.
Template Update Strategies
In this paper we consider the problem of how to update the template T(x). Suppose that
a (potentially) different template is used in each frame. Denote the template that is used
in the nth frame Tn(x). Tracking then consists of computing:
pn = argmin
[In(W(x;p))−Tn(x)]2
and the template update problem consists of computing Tn+1(x) from I0(x),...,In(x) and
T1(x),...,Tn(x). The simplest strategy is not to update the template at all:
Strategy 1: No Update
Tn+1(x) = T1(x) for all n ≥1.
The simplest strategy for actually updating the template is to set the new template to be
the region of the input image that the template was tracked to in In(x):
Strategy 2: Naive Update
Tn+1(x) = In(W(x;pn)) for all n ≥1.
Neither of these strategies are very good. With the ﬁrst strategy, the template eventually, and inevitably, becomes out-of-date and no longer representative of the appearance
of the object being tracked. With the second strategy, the template eventually drifts away
from the object. Small errors in the warp parameters pn mean that the new template
In(W(x;pn)) is always a slighted shifted version of what it ideally should be. These errors accumulate and after a while the template drifts away from the object that it was
initialized to track. See Figure 1 for an example of the template drifting in this way.
(Note that simple variants of this strategy such as updating the template every few frames,
although more robust, also suffer from the same drifting problem.)
How can we update the template every frame and avoid it wandering off? One possibility is to keep the ﬁrst template T1(x) around and use it to correct the drift in Tn+1(x).
For example, we could take the estimate of Tn+1(x) computed in Strategy 2 and then align
Tn+1(x) to T1(x) to eliminate the drift. Since Tn+1(x) = In(W(x;pn)) this is the same as
ﬁrst tracking in image In(x) with template Tn(x) and then with template T1(x). If the nonlinear minimizations in Equations (2) and (3) are solved perfectly, this is theoretically
exactly the same as just tracking with T1(x). The non-linear minimizations are solved
using a gradient descent algorithm, however, and so this strategy is actually different. Let
us change the notation slightly to emphasize the point that a gradient descent algorithm is
used to solve Equation (3). In particular, re-write Equation (3) as:
pn = gd min
[In(W(x;p))−Tn(x)]2
where gdminpn−1 means “perform a gradient descent minimization” starting at p = pn−1.
To correct the drift in Strategy 2, we therefore propose to compute updated parameters:
n = gd min
[In(W(x;p))−T1(x)]2 .
Note that this is different from tracking with the constant template Tn = T1 using:
[In(W(x;p))−T1(x)]2
because the starting point of the gradient descent is different. To correct the drift, we use
n rather than pn to form the template for the next image. In summary (see also Figure 2),
we update the template using:
Strategy 3: Template Update with Drift Correction
n −pn∥≤ε then Tn+1(x) = In(W(x;p∗
else Tn+1(x) = Tn(x)
where ε > 0 is a small threshold that enforces the requirement that the result of the second
gradient descent does not diverge too far from the result of the ﬁrst. If it does, there must
be a problem and so we act conservatively by not updating the template in that step. (A
minor variant of this is to perform the drift-correcting alignment using the magnitudes
of the gradients of the image and the template rather than the raw images to increase
robustness to illumination variation.)
Qualitative Comparison
We now present a qualitative comparison of the three update strategies. Although we only
have room to include one set of results, these results are typical. A more principled quantitative evaluation is included in Section 2.4. We implemented each of the three strategies
and ran them on a 972 frame video of a car tracked using a 2D similarity transform.
Sample frames are shown in Figure 1 for each of the update algorithms. If the template
is not updated (Strategy 1), the car is no longer tracked correctly after frame 312. If we
update the template every frame using the naive approach (Strategy 2), by around frame
200 the template has drifted away from the car. With update Strategy 3 “Template Update
with Drift Correction”, the car is tracked throughout the entire sequence and the template
is updated correctly in every frame, without introducing any drift. See the accompanying
movie1 “car-track.mpg” for tracking results on the sequence.
Reinterpretation of Update Strategy 3
A schematic diagram of Strategy 3 is included in Figure 2(a). The image In(x) is ﬁrst
tracked with template Tn(x) starting from the previous parameters pn−1. The result is
the tracked image In(W(x;pn)) and the parameters pn. The new template Tn+1(x) =
n)) is then computed by tracking T1(x) in In(x) starting at parameters pn.
If we reorganize Figure 2(a) slightly we get Figure 2(b). The only change made in this
reorganization is that the “tracked output” is In(W(x;p∗
n)) rather than In(W(x;pn)). The
difference between Figure 2(a) and Figure 2(b) is not the computation (the two diagrams
result in the same sequence of parameters pn), but their interpretation. Figure 2(a) can
be interpreted as tracking with Tn(x) followed by updating Tn(x). Figure 2(b) can be
interpreted as tracking with Tn(x) to get an initial estimate to track with T1(x). This
initial estimate improves robustness because tracking with T1(x) is prone to local minima.
1Movies may be downloaded from 513.html.
Strategy 1
Strategy 2
Strategy 3
Figure 1: A qualitative comparison of update Strategies 1, 2, and 3. With Strategy 1 the template is
not updated and tracking eventually fails. With Strategy 2, the template is updated every frame and
the template “drifts”. With Strategy 3 the template is updated every frame, but a “drift correction”
step is added. With this strategy the object is tracked correctly.
Tracking with In−1(W(x;p∗
n−1)) is less prone to local minima and is used to initialize the
tracking with T1(x) and start it close enough to avoid local minima. In summary, there are
two equivalent ways to interpret Strategy 3:
1. The template can be updated every frame, but it must be re-aligned to the original
template T1(x) to remove drift.
2. Not updating the template and tracking using the constant template T1(x) is ﬁne, so
long as we ﬁrst initialize pn by tracking with Tn(x) = In−1(W(x;p∗
Quantitative Evaluation
We now present a quantitative evaluation of Strategy 3 in the context of the second interpretation above. We measure how much more robust tracking is if we initialize it by ﬁrst
tracking with In−1(W(x;p∗
n−1)); i.e. use Strategy 3 rather than Strategy 1.
Our goal is to track the car in the 972 frame video sequence shown in Figure 1. First,
using a combination of Lucas-Kanade tracking and hand re-initialization, we obtain a set
of ground-truth parameters pn for each frame. We then generate 50 test cases for each of
the 972 frames by randomly perturbing the ground-truth parameters pn. The perturbation
is computed using a normal distribution so that the root-mean-square template coordinate
locations in the image are displaced by a known spatial standard deviation. We then run
the two tracking algorithms starting with the same perturbed parameters and determine
which of the two algorithms converged by comparing the ﬁnal pn with the ground-truth.
This experiment is repeated for all frames over a range of perturbationstandard deviations.
The ﬁnal result is a graph plotting the frequency of convergence versus the perturbation
magnitude for each algorithm. The results of this comparison are shown in Figure 3. We
Update Step
Tracking Step
In(W(x;pn))
Tracked Output
Tracked Output
(a) Update Strategy 3
(b) Update Strategy 3 (Reorganized)
Figure 2: Two equivalent schematic diagrams for update Strategy 3. The diagrams are equivalent
in the sense that they result in exactly the same sequence of parameters pn. (a) Can be interpreted
as ﬁrst tracking with template Tn and then updating Tn. (b) Can be interpreted as tracking with
constant template T1, after ﬁrst tracking with Tn(x) = In−1(W(x;p∗
n−1)) to avoid local minima.
plot two curves, one for update Strategy 1 “No Update” and one for update Strategy 3
“Template Update with Drift Correction”. No results are shown for Strategy 2 because
after a few frames the template drifts and so none of the trials converge to the correct
location (although many trials do converge). The accompanying movie “car-exp.mpg”
shows example trials for both algorithms with the ground truth marked in yellow and
the perturbed position tracked in green. Figure 3 clearly demonstrates that updating the
template using Strategy 3 dramatically improves the tracking robustness.
Template Tracking With Appearance Variation
We now consider the problem of template tracking with linear appearance variation. Instead of tracking with a single template Tn(x) (for each frame n), we assume that a linear model of appearance variation is used; i.e. a set of appearance images Ai
n(x) where
i = 1,...,dn. Instead of the template Tn(x) appearing (appropriately warped) in the input
image In(x), we assume that:
appears instead for a unknown set of appearance parameters λ = (λ 1,...,λ dn)T. The
appearance images Ai
n(x) can be used to model either illumination variation or more
general linear appearance variation . In this paper, we focus particularly on Active
Appearance Models which combine a linear appearance model with a (low parametric) piecewise afﬁne warp to model the shape deformation W(x;p). The process of
tracking with such a linear appearance model then consists of minimizing:
(pn,λ n) = arg min
In(W(x;p))−Tn(x)−
Point location sigma
Percentage of trials converged
Strategy 1
Strategy 3
The frequency of convergence of
Strategies 1 and 3 plot against the magnitude of
the perturbation to the ground-truth parameters,
computed over 50 trials for each frame in the
sequence used in Figure 1. The results demonstrate that updating the template using Strategy 3
results in far more robust tracking.
Shape eigenvector sigma + 4.0 × point location sigma
Percentage of trials converged
Algorithm 1
Algorithm 2
Algorithm 3
Algorithm 4
Figure 4: A comparison of the frequency of
convergence of four template and appearance
model update algorithms. The three algorithms
which actually update the template and/or appearance model (Algorithms 2, 3, and 4) all dramatically outperform the algorithm which does
not update model (Algorithm 1).
Several efﬁcient gradient descent algorithms have been proposed to solve this non-linear
optimization problem including for translations, afﬁne warps, and 2D similarity transformations, for arbitrary warps that form a group, and for Active Appearance
Models. Denote the result:
(pn,λ n) = gd
(pn−1,λ n−1) ∑
In(W(x;p))−Tn(x)−
where the gradient descent is started at (pn−1,λ n−1).
Updating Both the Template and the Appearance Model
Assume that the initial template T1 and appearance model Ai
1 are given. The template
update problem with linear appearance variation then consists of estimating Tn+1 and Ai
from I0(x),...,In(x), T1(x),...,Tn(x), and Ai
n. Analogously to above, denote:
n) = gd min
(pn,λ n) ∑
In(W(x;p))−T1(x)−
One way to update the template and appearance model is then as follows:
Strategy 4: Template and Appearance Model Update with Drift Correction
n −pn∥≤ε then (Tn+1(x),Ai
n+1) = PCA(I1(W(x;p∗
1)),...,In(W(x;p∗
else Tn+1(x) = Tn(x), Ai
where PCA() means perform Principal Components Analysis setting Tn to be the mean
n to be the ﬁrst dn eigenvectors, where dn is chosen to keep a ﬁxed amount of
the energy, typically 95%. (Other variants of this exist, such as incrementally updating
appearance model Ai
n to include the new measurement In(W(x;p∗
n)).) If we reinterpret this
algorithm as in Section 2.3, we end up with the following two step tracking algorithm:
Step 1: Apply PCA to I1(W(x;p∗
1)),...,In−1(W(x;p∗
n−1)). Set Tn to be the mean vector
n to be the ﬁrst i = 1,...,dn eigenvectors. Once computed, track with template
Tn and appearance model Ai
Step 2: Track with the a priori template T1(x) and linear appearance model Ai
1(x), starting the gradient descent at the result of the ﬁrst step.
One way to interpret these two steps is as performing “progressive appearance complexity”, analogously to “progressive transformation complexity” the standard heuristic for
improving the robustness of tracking algorithms by increasing the complexity of the warp
W(x;p). For example, tracking with an afﬁne warp is often performed by ﬁrst tracking
with a translation, then a 2D similarity transformation, and ﬁnally a full afﬁne warp. Here,
tracking with one appearance model is used to initialize tracking with another. Based on
this analogy, we add another step to the algorithm above:
Step 0: Track using the template Tn(x) = In−1(W(x;p∗
n−1)) with no appearance model.
This step is performed before the two steps above and is used to initialize them.
Quantitative Evaluation
We evaluate Strategy 4 “Template and Appearance Model Update with Drift Correction”
in the same way that we evaluated Strategy 3 in Section 2.4. We use a 947 frame video
of a face and construct an initial AAM for it by hand-marking feature points in a random selection of 80 frames. We then generate ground-truth parameters by tracking the
AAM through the video using a combination of AAM ﬁtting , pyramid search, progressive transformation complexity, and re-initialization by hand. The accompanying movie
“face-gt.mpg” plots the ground-truth AAM feature points on all images in the video
sequence. The sequence shows a drivers face in a car and includes moderate face pose
and lighting variation. We generate 50 test cases for each of the 947 frames in the video
by randomly perturbing the AAM parameters. Similarly to Section 2.4, and following the
exact procedure in , we generate perturbations in both the similarity transform of the
AAM and the shape parameters. Speciﬁcally, the RMS similarity displacement standard
deviation is chosen to be 4 times the shape eigenvector standard deviation so that each
is weighted according to their relative importance. For each test case, we compared four
algorithms:
Algorithm 1: Step 2 (no update).
Algorithm 2: Step 1 followed by Step 2.
Algorithm 3: Step 0 followed by Step 1 followed by Step 2.
Algorithm 4: Step 0 followed by Step 2.
We plot the frequency of convergence of these four algorithms computed on average
across all 50 × 947 test cases against the magnitude of the perturbation to the AAM parameters in Figure 4. As for the single template tracking case in Section 2, the template
and appearance model update algorithms (Algorithms 2, 3, and 4) all outperform the algorithm which does not update the template and appearance mode (Algorithm 1). As one
might imagine, Algorithm 3 (Steps 0, 1, 2) marginally outperforms Algorithm 2 which
just uses Steps 1 and 2. Algorithm 4 performs signiﬁcantly worse than both Algorithms 2
and 3 indicating that Step 1 is essential for the best performance.
Converting a Generic AAM to a Person-Speciﬁc AAM
When we use Step 1 above, a new template and appearance model are computed online
as we track the face through the video. To illustrate this process we applied Algorithm 2
to track a video of a face using a generic, person-independent AAM. The accompanying
movie “face-app.mpg” shows the tracked face, T1(x) and the ﬁrst two Ai
1(x). Also
shown are the current Tn(x) and the ﬁrst two Ai
n(x) for each frame. The result is that at the
end of the sequence, the template and appearance model update algorithm has computed
a person speciﬁc appearance model.
This process is illustrated in Figure 5. Figure 5(a) shows 4 frames of the face that
is tracked. Note that no images of the person in the video were used to compute the
generic AAM. Figure 5(b) shows the appearance eigenvectors of the generic AAM. Note
that the appearance eigenvectors contain both identity and illumination variation. Figure 5(c) shows the appearance eigenvectors of the person-speciﬁc AAM computed using
our algorithm. Note that the eigenvectors mainly code illumination variation, and no identity variation. Figure 5(d) plots the appearance eigenvalues of both AAMs. There is far
less appearance variation in the person-speciﬁc AAM and it therefore requires far fewer
appearance parameters to provide the same representational power.
We have investigated the template update problem. We ﬁrst proposed a template update
algorithm that does not suffer from the “drift” inherent in the naive algorithm. Next,
we showed how this algorithm can be re-interpreted as a heuristic to avoid local minima and quantitatively evaluated it as such. The results show that updating the template
using “Template Update with Drift Correction” improves tracking robustness. We then
extended our algorithm to template tracking with linear appearance models and quantitatively compared four variants of the update strategy. The results again show that updating
both the template and the appearance model with drift correction results in more robust
ﬁtting. Finally, we showed that our linear appearance model update strategy can also
automatically compute a person-speciﬁc AAM while tracking with a generic AAM.
Acknowledgments
The research described in this report was partially supported by Denso Corporation, Japan, and was
conducted at Carnegie Mellon University Robotics Institute while Takahiro Ishikawa was a Visiting
Industrial Scholar from Denso Corporation. This research was also supported, in part, by the U.S.
Department of Defense through award number N41756-03-C4024. The Generic AAM model in
Section 3.3 was trained on the ViaVoiceTMAV database provided by IBM Research.
Eigenvector
Eigenvalue
All subjects
Computed on sequence
Figure 5: An illustration of the conversion of a generic AAM to a person-speciﬁc AAM. (a) Four
frames from the video that is tracked. (b) The appearance variation of the generic AAM. (c) The appearance variation of the person-speciﬁc AAM. (d) The appearance eigenvalues of the two AAMs.