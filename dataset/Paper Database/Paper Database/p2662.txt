FactSheets: Increasing Trust in AI Services
through Supplier’s Declarations of Conformity
M. Arnold,1 R. K. E. Bellamy,1 M. Hind,1 S. Houde,1 S. Mehta,2 A. Mojsilovi´c,1
R. Nair,1 K. Natesan Ramamurthy,1 D. Reimer,1 A. Olteanu,∗D. Piorkowski,1
J. Tsay,1 and K. R. Varshney1
IBM Research
1Yorktown Heights, New York, 2Bengaluru, Karnataka
Accuracy is an important concern for suppliers of artiﬁcial intelligence (AI) services, but considerations
beyond accuracy, such as safety (which includes fairness and explainability), security, and provenance,
are also critical elements to engender consumers’
trust in a service.
Many industries use transparent, standardized, but often not legally required documents called supplier’s declarations of conformity
(SDoCs) to describe the lineage of a product along
with the safety and performance testing it has undergone. SDoCs may be considered multi-dimensional
fact sheets that capture and quantify various aspects
of the product and its development to make it worthy of consumers’ trust. Inspired by this practice, we
propose FactSheets to help increase trust in AI services. We envision such documents to contain purpose, performance, safety, security, and provenance
information to be completed by AI service providers
for examination by consumers. We suggest a comprehensive set of declaration items tailored to AI and
provide examples for two ﬁctitious AI services in the
appendix of the paper.
Introduction
Artiﬁcial intelligence (AI) services, such as those containing predictive models trained through machine
learning, are increasingly key pieces of products and
decision-making workﬂows. A service is a function or
application accessed by a customer via a cloud infrastructure, typically by means of an application programming interface (API). For example, an AI ser-
∗A. Olteanu’s work was done while at IBM Research. Author is currently aﬃliated with Microsoft Research.
vice could take an audio waveform as input and return a transcript of what was spoken as output, with
all complexity hidden from the user, all computation
done in the cloud, and all models used to produce
the output pre-trained by the supplier of the service.
A second more complex example would provide an
audio waveform translated into a diﬀerent language
as output. The second example illustrates that a service can be made up of many diﬀerent models (speech
recognition, language translation, possibly sentiment
or tone analysis, and speech synthesis) and is thus
a distinct concept from a single pre-trained machine
learning model or library.
In many diﬀerent application domains today, AI
services are achieving impressive accuracy.
In certain areas, high accuracy alone may be suﬃcient,
but deployments of AI in high-stakes decisions, such
as credit applications, judicial decisions, and medical recommendations, require greater trust in AI services. Although there is no scholarly consensus on
the speciﬁc traits that imbue trustworthiness in people or algorithms , fairness, explainability, general safety, security, and transparency are some of the
issues that have raised public concern about trusting
AI and threatened the further adoption of AI beyond
low-stakes uses . Despite active research and development to address these issues, there is no mechanism yet for the creator of an AI service to communicate how they are addressed in a deployed version.
This is a major impediment to broad AI adoption.
Toward transparency for developing trust, we propose a FactSheet for AI Services. A FactSheet will
contain sections on all relevant attributes of an AI
service, such as intended use, performance, safety,
and security. Performance will include appropriate
accuracy or risk measures along with timing information. Safety, discussed in as the minimiza-
tion of both risk and epistemic uncertainty, will include explainability, algorithmic fairness, and robustness to dataset shift.
Security will include robustness to adversarial attacks. Moreover, the FactSheet
will list how the service was created, trained, and deployed along with what scenarios it was tested on,
how it may respond to untested scenarios, guidelines
that specify what tasks it should and should not be
used for, and any ethical concerns of its use. Hence,
FactSheets help prevent overgeneralization and unintended use of AI services by solidly grounding them
with metrics and usage scenarios.
A FactSheet is modeled after a supplier’s declaration of conformity (SDoC). An SDoC is a document to “show that a product, process or service conforms to a standard or technical regulation, in which
a supplier provides written assurance [and evidence]
of conformity to the speciﬁed requirements,” and is
used in many diﬀerent industries and sectors including telecommunications and transportation .
Importantly, SDoCs are often voluntary and tests reported in SDoCs are conducted by the supplier itself
rather than by third parties . This distinguishes
self-declarations from certiﬁcations that are mandatory and must have tests conducted by third parties.
We propose that FactSheets for AI services be voluntary initially; we provide further discussion on their
possible evolution in later sections.
Our proposal of AI service FactSheets is inspired
by, and builds upon, recent work that focuses on increased transparency for datasets and models , but is distinguished from these in that we
focus on the ﬁnal AI service. We take this focus for
three reasons:
1. AI services constitute the building blocks for
many AI applications.
Developers will query
the service API and consume its output.
AI service can be an amalgam of many models
trained on many datasets. Thus, the models and
datasets are (direct and indirect) components of
an AI service, but they are not the interface to
the developer.
2. Often, there is an expertise gap between the producer and consumer of an AI service. The production team relies heavily on the training and
creation of one or more AI models and hence will
mostly contain data scientists. The consumers of
the service tend to be developers. When such an
expertise gap exists, it becomes more crucial to
communicate the attributes of the artifact in a
standardized way, as with Energy Star or food
nutrition labels.
3. Systems composed of safe components may be
unsafe and, conversely, it may be possible to
build safe systems out of unsafe components, so
it is prudent to also consider transparency and
accountability of services in addition to datasets
and models. In doing so, we take a functional
perspective on the overall service, and can test
for performance, safety, and security aspects that
are not relevant for a dataset in isolation, such as
generalization accuracy, explainability, and adversarial robustness.
Loukides et al. propose a checklist that has some of
the elements we seek .
Our aim is not to give the ﬁnal word on the contents of AI service FactSheets, but to begin the conversation on the types of information and tests that
may be included.
Moreover, determining a single
comprehensive set of FactSheet items is likely infeasible as the context and industry domain will often
determine what items are needed. One would expect
higher stakes applications will require more comprehensive FactSheets. Our main goal is to help identify a common set of properties. A multi-stakeholder
approach, including numerous AI service suppliers
and consumers, standards bodies, and civil society
and professional organizations is essential to converge
onto standards.
It will only be then that we as a
community will be able to start producing meaningful FactSheets for AI services.
The remainder of the paper is organized as follows.
Section 2 overviews related work, including labeling,
safety, and certiﬁcation standards in other industries.
Section 3 provides more details on the key issues to
enable trust in AI systems. Section 4 describes the
AI service FactSheet in more detail, giving examples
of questions that it should include. In Section 5, we
discuss how FactSheets can evolve from a voluntary
process to one that could be an industry requirement.
Section 6 covers challenges, opportunities, and future
work needed to achieve the widespread usage of AI
service declarations of conformity. A proposed complete set of sections and items for a FactSheet is included in the appendix, along with sample FactSheets
for two exemplary ﬁctitious services, ﬁngerprint veriﬁcation and trending topics in social media.
Related Work
This section discusses related work in providing
transparency in the creation of AI services, as well
as a brief survey of ensuring trust in non-AI systems.
Transparency in AI
Within the last year, several research groups have advocated standardizing and sharing information about
training datasets and trained models. Gebru et al.
proposed the use of datasheets for datasets as a way
to expose and standardize information about public
datasets, or datasets used in the development of commercial AI services and pre-trained models . The
datasheet would include provenance information, key
characteristics, and relevant regulations, but also signiﬁcant, yet more subjective information, such as potential bias, strengths and weaknesses, and suggested
Bender and Friedman propose a data statement schema, as a way to capture and convey the
information and properties of a dataset used in natural language processing (NLP) research and development . They argue that data statements should be
included in most writing on NLP, including: papers
presenting new datasets, papers reporting experimental work with datasets, and documentation for NLP
Holland et al. outline the dataset nutrition label, a
diagnostic framework that provides a concise yet robust and standardized view of the core components of
a dataset . Academic conferences such as the International AAAI Conference on Web and Social Media are also starting special tracks for dataset papers
containing detailed descriptions, collection methods,
and use cases.
Subsequent to the ﬁrst posting of this paper ,
Mitchell et al. propose model cards to convey information that characterizes the evaluation of a machine
learning model in a variety of conditions and disclose
the context in which models are intended to be used,
details of the performance evaluation procedures, and
other relevant information . There is also budding
activity on auditing and labeling algorithms for accuracy, bias, consistency, transparency, fairness and
timeliness, in the industry , but this audit
does not cover several aspects of safety, security, and
Our proposal is distinguished from prior work in
that we focus on the ﬁnal AI service, a distinct concept from a single pre-trained machine learning model
or dataset.
Moreover, we take a broader view on
trustworthy AI that extends beyond principles, values and ethical purpose to also include technical robustness and reliability .
Enabling Trust in Other Domains
Enabling trust in systems is not unique to AI. This
section provides an overview of mechanisms used in
other domains and industries to achieve trust. The
goal is to understand existing approaches to help inspire the right directions for enabling trust in AI services.
Standards Organizations
Standardization organizations, such as the IEEE 
and ISO , deﬁne standards along with the requirements that need to be satisifed for a product or a process to meet the standard. The product developer can
self-report that a product meets the standard, though
there are several cases, especially with ISO standards,
where an independent accredited body will verify that
the standards are met and provide the certiﬁcation.
Consumer Products
The United States Consumer Product Safety Commission (CPSC) requires a manufacturer or importer to declare its product as compliant with applicable consumer product safety requirements in a
written or electronic declaration of conformity.
many cases, this can be self-reported by the manufacturer or importer, i.e. an SDoC. However, in the
case of children’s products, it is mandatory to have
the testing performed by a CPSC-accepted laboratory for compliance. Durable infant or toddler products must be marked with specialized tracking labels
and must have a postage-paid customer registration
card attached, to be used in case of a recall.
The National Parenting Center has a Seal of Approval program that conducts testing on a variety
of children’s products involving interaction with the
products by parents, children, and educators, who ﬁll
out questionnaires for the products they test. The
quality of a product is determined based on factors
like the product’s level of desirability, sturdiness, and
interactive stimulation. Both statistical averaging as
well as comments from testers are examined before
providing a Seal of Approval for the product.
In the ﬁnancial industry, corporate bonds are rated
by independent rating services to help an investor assess the bond issuer’s ﬁnancial strength or
its ability to pay a bond’s principal and interest in a
timely fashion. These letter-grade ratings range from
AAA or Aaa for safe, ‘blue-chip’ bonds to C or D for
‘junk’ bonds. On the other hand, common-stock investments are not rated independently. Rather, the
Securities and Exchange Commission (SEC) requires
potential issuers of stock to submit speciﬁc registration documents that discloses extensive ﬁnancial information about the company and risks associated
with the future operations of the company. The SEC
examines these documents, comments on them, and
expects corrections based on the comments. The ﬁnal
product is a prospectus approved by the SEC that is
available for potential buyers of the stock.
In the software area, there have been recent attempts
to certify digital data repositories as ‘trusted.’ Trustworthiness involves both the quality of the data and
sustainable, reliable access to the data. The goal of
certiﬁcation is to enhance scientiﬁc reproducibility.
The European Framework for Audit and Certiﬁcation has three levels of certiﬁcation, Core, Extended, and Formal (or Bronze, Silver, and Gold),
having diﬀerent requirements, mainly to distinguish
between the requirements of diﬀerent types of data,
e.g. research data vs. human health data vs. ﬁnancial
transaction data. The CoreTrustSeal , a private
legal entity, provides a Bronze level certiﬁcation to
an interested data repository, for a nominal fee.
There have been several proposals in the literature
for software certiﬁcations of various kinds.
and McGraw propose a certiﬁcation process for
testing software components for security properties.
Their technique involves a process and a set of whitebox and black-box testing procedures, that eventually results in a stamp of approval in the form of a
digital signature.
Schiller proposes a certiﬁcation process that starts with a checklist with yes/no
answers provided by the developer, and determines
which tests need to be performed on the software to
certify it. Currit et al. describe a procedure for
certifying the reliability of software before its release
to the users. They predict the performance of the
software on unseen inputs using the MTTF (mean
time to failure) metric. Port and Wilf describe
a procedure to certify the readiness for software release, understanding the tradeoﬀin cost of too early
a release due to failures in the ﬁeld, versus the cost
in personnel and schedule delay arising from more
extensive testing. Their technique involves the ﬁlling out of a questionnaire by the software developer
called the Software Review and Certiﬁcation Record
(SRCR), which is ‘credentialed’ with signatories who
approve the document prior to the release decision.
Heck et al. also describe a software product certiﬁcation model to certify legislative compliance or
acceptability of software delivered during outsourcing. The basis for certiﬁcation is a questionnaire to
be ﬁlled out by the developer. The only acceptable
answers to the questions are yes and n/a (not applicable).
A diﬀerent approach is taken in the CERT Secure Coding Standards of the Software Engineering Institute. Here the emphasis is on documenting
best practices and coding standards for security purposes. The secure coding standards consist of guidelines about the types of security ﬂaws that can be
injected through development with speciﬁc programming languages. Each guideline oﬀers precise information describing the cause and impact of violations,
and examples of common non-compliant (ﬂawed) and
compliant (ﬁxed) code. The organization also provides tools, which audits code to identify security
ﬂaws as indicated by violations of the CERT secure
coding standards.
Environmental Impact Statements
Environment law in the United States requires that
an environmental impact statement (EIS) should be
prepared prior to starting large constructions.
EIS is a document used as a tool for decision making that describes positive and negative environmental eﬀects of a proposed action. It is made available
both to federal agencies and to the public, and captures impacts to endangered species, air quality, water quality, cultural sites, and the socioeconomics of
local communities. The federal law, the National Environmental Policy Act, has inspired similar laws in
various jurisdictions and in other ﬁelds beyond the
environment. Selbst has proposed an algorithmic impact statement for AI that follows the form and purpose of EISs .
Human Subjects
In addition to products and technologies, another
critical endeavor requiring trust is research involving
human subjects. Institutional review boards (IRB)
have precise reviewing protocols and requirements
such as those presented in the Belmont Report .
Items to be completed include statement of purpose, participant selection, procedures to be followed,
harms and beneﬁts to subjects, conﬁdentiality, and
consent documents. As AI services increasingly make
inferences for people and about people , IRB requirements increasingly apply to them.
To ensure trust in products, industries have established a variety of practices to convey information
about how a product is expected to perform when
utilized by a consumer. This information usually includes how the product was constructed and tested.
Some industries allow product creators to voluntarily provide this information, whereas others explicitly
require it. When the information is required, some industries require the information to be validated by a
third party. One would expect the latter scenario to
occur in mature industries where there is conﬁdence
that the requirements strongly correlate with safety,
reliability, and overall trust in the product. Mandatory external validation of nascent requirements in
emerging industries may unnecessarily stiﬂe the development of the industry.
Elements of Trust in AI Systems
We drive cars trusting the brakes will work when the
pedal is pressed. We undergo laser eye surgery trusting the system to make the right decisions. We accept
that the autopilot will operate an airplane, trusting
that it will navigate correctly. In all these cases, trust
comes from conﬁdence that the system will err extremely rarely, leveraging system training, exhaustive
testing, experience, safety measures and standards,
best practices, and consumer education.
Every time new technology is introduced, it creates
new challenges, safety issues, and potential hazards.
As the technology develops and matures, these issues
are better understood, documented, and addressed.
Human trust in technology is developed as users overcome perceptions of risk and uncertainty , i.e., as
they are able to assess the technology’s performance,
reliability, safety, and security.
Consumers do not
yet trust AI like they trust other technologies because of inadequate attention given to the latter of
these issues . Making technical progress on safety
and security is necessary but not suﬃcient to achieve
trust in AI, however; the progress must be accompanied by the ability to measure and communicate the
performance levels of the service on these dimensions
in a standardized and transparent manner. One way
to accomplish this is to provide such information via
FactSheets for AI services.
Trust in AI services will come from: a) applying
general safety and reliability engineering methodologies across the entire lifecycle of an AI service, b)
identifying and addressing new, AI-speciﬁc issues and
challenges in an ongoing and agile way, and c) creating standardized tests and transparent reporting
mechanisms on how such a service operates and performs. In this section we outline several areas of concern and how they uniquely apply to AI. The crux of
this discussion is the manifestation of risk and uncertainty in machine learning, including that data distributions used for training are not always the ones
that ideally should be used.
Basic Performance and Reliability
Statistical machine learning theory and practice is
built around risk minimization. The particular loss
function, whose expectation over the data distribution is considered to be the risk, depends on the task,
e.g. zero-one loss for binary classiﬁcation and mean
squared error for regression. Diﬀerent types of errors
can be given diﬀerent costs. Abstract loss functions
may be informed by real-world quality metrics ,
including context-dependent ones . There is no
particular standardization on the loss function, even
broadly within application domains. Moreover, performance metrics that are not directly optimized are
also often examined, e.g. area under the curve and
normalized cumulative discounted gain.
The true expected value of the loss function can
never be known and must be estimated empirically.
There are several approaches and rules of thumb for
estimating the risk, but there is no standardization
here either. Diﬀerent groups make diﬀerent choices
(k-fold cross-validation, held-out samples, stratiﬁcation, bootstrapping, etc.).
Further notions of performance and reliability are the technical aspects of
latency, throughput, and availability of the service,
which are also not standardized for the speciﬁcs of
AI workloads.
To develop trust in AI services from a basic performance perspective, the choice of metrics and testing
conditions should not be left to the discretion of the
supplier (who may choose conditions which present
the service in a favorable light), but should be codiﬁed and standardized. The onerous requirement of
third-party testing could be avoided by ensuring that
the speciﬁcations are precise, i.e., that each metric
is precisely deﬁned to ensure consistency and enable
reproducibility by AI service consumers.
For each metric a FactSheet should report the values under various categories relevant to the expected
consumers, (e.g., performance for various age groups,
geographies, or genders) with the goal of providing
the right level of insight into the service, but still
preserving privacy. We expect some metrics will be
speciﬁc to a domain, (e.g., ﬁnance, healthcare, manufacturing), or a modality (e.g., visual, speech, text),
reﬂecting common practice of evaluation in that environment.
While typical machine learning performance metrics
are measures of risk (the ones described in the previous section), we must also consider epistemic uncertainty when assessing the safety of a service .
The main uncertainty in machine learning is an unknown mismatch between the training data distribution and the desired data distribution on which one
would ideally train. Usually that desired distribution
is the true distribution encountered in operation (in
this case the mismatch is known as dataset shift),
but it could also be an idealized distribution that
encodes preferred societal norms, policies, or regulations (imagine a more equitable world than what
exists in reality). One may map four general categories of strategies to achieve safety proposed in 
to machine learning : inherently safe design, safety
reserves, safe fail, and procedural safeguards, all of
which serve to reduce epistemic uncertainty. Interpretability of models is one example of inherently safe
Dataset Shift
As the statistical relationship between features and labels changes over time, known as
dataset shift, the mismatch between the training distribution and the distribution from which test samples are being drawn increases. A well-known reason
for performance degradation, dataset shif is a common cause of frustration and loss of trust for AI service consumers. Dataset shift can be detected and
corrected using a multitude of methods . The sensitivity of performance of diﬀerent models to dataset
shift varies and should be part of a testing protocol. To the best of our knowledge, there does not yet
exist any standard for how to conduct such testing.
To mitigate this risk a FactSheet should contain demographic information about the training and test
datasets that report the various outcomes for each
group of interest as speciﬁed in Section 3.1.
AI fairness is a rapidly growing topic of
inquiry .
There are many diﬀerent deﬁnitions
of fairness (some of which provably conﬂict) that
are appropriate in varying contexts. The concept of
fairness relies on protected attributes (also contextdependent) such as race, gender, caste, and religion.
For fairness, we insist on some risk measure being approximately equal in groups deﬁned by the protected
attributes.
Unwanted biases in training data, due
to either prejudice in labels or under-/over-sampling,
lead to unfairness and can be checked using statistical
tests on datasets or models . One can think of
bias as the mismatch between the training data distribution and a desired fair distribution. Applications
such as lending have legal requirements on fairness
in decision making, e.g. the Equal Credit Opportunity Act in the United States. Although the parity
deﬁnitions and computations in such applications are
explicit, the interpretation of the numbers is subjective: there are no immutable thresholds on fairness
metrics (e.g., the well-known 80% rule ) that are
aplied in isolation of context.
Explainability
interpretable
learning (in contrast to post hoc interpretation)
 , in which a person can look at a model and
understand what it does, reduces epistemic uncertainty and increases safety because quirks and
vagaries of training dataset distributions that will
not be present in distributions during deployment
can be identiﬁed by inspection .
Diﬀerent users
have diﬀerent needs from explanations, and there
is not yet any satisfactory quantitative deﬁnition
of interpretability (and there may never be) .
Recent regulations in the European Union require
‘meaningful’ explanations, but it is not clear what
constitutes a meaningful explanation.
AI services can be attacked by adversaries in various
Small imperceptible perturbations could
cause AI services to misclassify inputs to any label
that attackers desire; training data and models can be
poisoned, allowing attackers to worsen performance
(similar to concept drift but deliberate); and sensitive
information about data and models can be stolen by
observing the outputs of a service for diﬀerent inputs.
Services may be instrumented to detect such attacks
and may also be designed with defenses . New
research proposes certiﬁcations for defenses against
adversarial examples , but these are not yet practical.
Once performance, safety, and security are suﬃcient
to engender trust, we must also ensure that we track
and maintain the provenance of datasets, metadata,
models along with their hyperparameters, and test
results. Users, those potentially aﬀected, and third
parties, such as regulators, must be able to audit the
systems underlying the services. Appropriate parties
may need the ability to reproduce past outputs and
track outcomes. Speciﬁcally, one should be able to
determine the exact version of the service deployed
at any point of time in the past, how many times the
service was retrained and associated details like hyperparameters used for each training episode, training dataset used, how accuracy and safety metrics
have evolved over time, the feedback data received
by the service, and the triggers for retraining and
improvement. This information may span multiple
organizations when a service is built by multiple parties.
Items in a FactSheet
In this section we provide an overview of the items
that should be addressed in a FactSheet.
appendix for the complete list of items. To illustrate
how these items might be completed in practice, we
also include two sample FactSheets in the appendix:
one for a ﬁctitious ﬁngerprint veriﬁcation service and
one for a trending topics service.
The items are grouped into several categories
aligned with the elements of trust.
The categories
are: statement of purpose, basic performance, safety,
security, and lineage. They cover various aspects of
service development, testing, deployment and maintenance: from information about the data the service
is trained on, to underlying algorithms, test setup,
test results, and performance benchmarks, to the way
the service is maintained and retrained (including automatic adaptation).
The items are devised to aid the user in understanding how the service works, in determining if the
service is appropriate for the intended application,
and in comprehending its strengths and limitations.
The identiﬁed items are not intended to be deﬁnitive.
If a question is not applicable to a given service, it
can simply be ignored.
In some cases, the service
supplier may not wish to disclose details of the service for competitive reasons. For example, a supplier
of a commercial fraud detection service for healthcare insurance claims may choose not to reveal the
details of the underlying algorithm; nevertheless, the
supplier should be able to indicate the class of algorithm used, provide sample outputs along with explanations of the algorithmic decisions leading to the
outputs. More consequential applications will likely
require more comprehensive completion of items.
A few examples of items a FactSheet might include
• What is the intended use of the service output?
• What algorithms or techniques does this service
implement?
• Which datasets was the service tested on? (Provide links to datasets that were used for testing,
along with corresponding datasheets.)
• Describe the testing methodology.
• Describe the test results.
• Are you aware of possible examples of bias, ethical issues, or other safety risks as a result of using
the service?
• Are the service outputs explainable and/or interpretable?
• For each dataset used by the service: Was the
dataset checked for bias?
What eﬀorts were
made to ensure that it is fair and representative?
• Does the service implement and perform any bias
detection and remediation?
• What is the expected performance on unseen
data or data with diﬀerent distributions?
• Was the service checked for robustness against
adversarial attacks?
• When were the models last updated?
As such a declaration is reﬁned, and testing procedures for performance, robustness to concept drift,
explainability, and robustness to attacks are further
codiﬁed, the FactSheet may refer to standardized test
protocols instead of providing descriptive details.
Since completing a FactSheet can be laborious, we
expect most of the information to be populated as
part of the AI service creation process in a secure
auditable manner. A FactSheet will be created once
and associated with a service, but can continually be
augmented, without removing previous information,
i.e., results are added from more tests, but results
cannot be removed. Any changes made to the service
will prompt the creation of a new version of the Fact-
Sheet for the new model. Thus, these FactSheets will
be treated as a series of immutable artifacts.
This information can be used to more accurately
monitor a deployed service by comparing deployed
metrics with those that were seen during development
and taking appropriate action when unexpected behavior is detected.
The Evolution of FactSheet
We expect that AI will soon go through the same evolution that other technologies have gone through (cf.
 for an excellent review of the evolution of safety
standards in diﬀerent industries). We propose that
FactSheets be initially voluntary for several reasons.
First, discussion and feedback from multiple parties
representing suppliers and consumers of AI services is
needed to determine the ﬁnal set of items and format
of FactSheets. So, an initial voluntary period to allow this discussion to occur is needed. Second, there
needs to be a balance between the needs of AI service consumers with the freedom to innovate for AI
service producers. Although producing a FactSheet
will initially be an additional burden to an AI service
producer, we expect market feedback from AI service
consumers to encourage this creation.
Because of peer pressure to conform , Fact-
Sheets could become a de facto requirement similar to
Energy Star labeling of the energy eﬃciency of appliances. They will serve to reduce information asymmetry between supplier and consumer, where consumers are currently unaware of important properties
of a service, such as its intended use, its performance
metrics, and information about fairness, explainability, safety, and security. In particular, consumers in
many businesses do not have the requisite expertise to
evaluate various AI services available in the marketplace; uninformed or incorrect choices can result in
suboptimal business performance. By creating easily
consumable FactSheets, suppliers can accrue a competitive advantage by capturing consumers’ trust.
Moreover, with such transparency, FactSheets should
serve to allow better functioning of AI service marketplaces and prevent a so-called ‘market for lemons’
A counter-argument to voluntary compliance
and self-regulation argues that while participation of
industry is welcome, this should not stand in the way
of legislation and governmental regulation .
FactSheet adoption could potentially lead to an
eventual system of third-party certiﬁcation , but
probably only for services catering to applications
with the very highest of stakes, to regulated business processes and enterprise applications, and to
applications originating in the public sector .
Children’s toys are an example category of consumer
products in which an SDoC is not enough and certiﬁcation is required. If an AI service is already touching
on a regulation from a speciﬁc industry in which it
is being used, its FactSheet will serve as a tool for
better compliance.
Discussion and Future Work
One may wonder why AI should be held to a higher
standard (FactSheets) than non-AI software and services in the same domain. Non-AI software include
several artifacts beyond the code, such as design documents, program ﬂow charts, and test plans that can
provide transparency to concerned consumers. Since
AI services do not contain any of these, and the generated code may not be easily understandable, there
is a higher demand to enhance transparency through
FactSheets.
Although FactSheets enable AI services producers
to provide information about the intent and construction of their service so that educated consumers can
make informed decisions, consumers may still, innocently or maliciously, use the service for purposes
other than those intended. FactSheets cannot fully
protect against such use, but can form the basis of
service level agreements.
Some components of an AI service may be produced by organizations other than the service sup-
plier. For example, the dataset may be obtained from
a third party, or the service may be a composition
of models, some of which are produced by another
organization.
In such cases, the FactSheet for the
composed service would need to include information
from the supplying organizations. Ideally, those organizations would produce FactSheets for their components, enabling the composing organization to provide a complete FactSheet. This complete FactSheet
could include the component FactSheets along with
any necessary additional information. In some cases,
the demands for transparency on the composing organization may be greater than on the component organization; market forces will require the component
organization to provide more transparency to retain
their relation with the composing organization. This
is analogous to other industries, like retail, where retailers push demands on their suppliers to meet the
expectations of the retailers’ customers. In these situations the provenance of the information among organizations will need to be tracked.
Summary and Conclusion
In this paper, we continue in the research direction established by datasheets or nutrition labels for
datasets to examine trusted AI at the functional level
rather than at the component level. We discuss the
several elements of AI services that are needed for
people to trust them, including task performance,
safety, security, and maintenance of lineage. The ﬁnal piece to build trust is transparent documentation
about the service, which we see as a variation on declarations of conformity for consumer products. We
propose a starting point to a voluntary AI service
supplier’s declaration of conformity. Further discussion among multiple parties is required to standardize
protocols for testing AI services and determine the ﬁnal set of items and format that AI service FactSheets
will take.
We envision that suppliers will voluntarily populate and release FactSheets for their services to remain competitive in the market.
The evolution of
the marketplace of AI services may eventually lead
to an ecosystem of third party testing and veriﬁcation laboratories, services, and tools. We also envision the automation of nearly the entire FactSheet
as part of the build and runtime environments of AI
services. Moreover, it is not diﬃcult to imagine Fact-
Sheets being automatically posted to distributed, immutable ledgers such as those enabled by blockchain
technologies.
We see our work as a ﬁrst step at deﬁning which
questions to ask and metrics to measure towards development and adoption of broader industry practices
and standards. We see a parallel between the issue of
trusted AI today and the rise of digital certiﬁcation
during the Internet revolution. The digital certiﬁcation market ‘bootstrapped’ the Internet, ushering in a
new era of ‘transactions’ such as online banking and
beneﬁts enrollment that we take for granted today.
In a similar vein, we can see AI service FactSheets
ushering in a new era of trusted AI end points and
bootstrapping broader adoption.