Dealing with Limited Overlap in
Estimation of Average Treatment Effects
Crump, Richard K., V. Joseph Hotz, Guido W. Imbens, and Oscar A. Mitnik. 2009. Dealing
with limited overlap in estimation of average treatment effects. Biometrika 96(1): 187-199.
Published version
 
 
Terms of use
This article was downloaded from Harvard University’s DASH repository, and is made
available under the terms and conditions applicable to Open Access Policy Articles (OAP),
as set forth at
 
Accessibility
 
Share Your Story
The Harvard community has made this article openly available.
Please share how this access benefits you. Submit a story
IZA DP No. 2347
Moving the Goalposts:
Addressing Limited Overlap in Estimation of
Average Treatment Effects by Changing the Estimand
Richard K. Crump
V. Joseph Hotz
Guido W. Imbens
Oscar A. Mitnik
D I S C U S S I O N P A P E R S E R I E S
Forschungsinstitut
zur Zukunft der Arbeit
Institute for the Study
September 2006
Moving the Goalposts:
Addressing Limited Overlap in
Estimation of Average Treatment
Effects by Changing the Estimand
Richard K. Crump
University of California at Berkeley
V. Joseph Hotz
University of California at Los Angeles
Guido W. Imbens
Harvard University
Oscar A. Mitnik
University of Miami
and IZA Bonn
Discussion Paper No. 2347
September 2006
P.O. Box 7240
53072 Bonn
Phone: +49-228-3894-0
Fax: +49-228-3894-180
E-mail: 
This paper can be downloaded without charge at:
 
An index to IZA Discussion Papers is located at:
 
Any opinions expressed here are those of the author(s) and not those of the institute. Research
disseminated by IZA may include views on policy, but the institute itself takes no institutional policy
positions.
The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center
and a place of communication between science, politics and business. IZA is an independent nonprofit
company supported by Deutsche Post World Net. The center is associated with the University of Bonn
and offers a stimulating research environment through its research networks, research support, and
visitors and doctoral programs. IZA engages in (i) original and internationally competitive research in
all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research
results and concepts to the interested public.
IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion.
Citation of such a paper should account for its provisional character. A revised version may be
available directly from the author.
IZA Discussion Paper No. 2347
September 2006
Moving the Goalposts:
Addressing Limited Overlap in Estimation of
Average Treatment Effects by Changing the Estimand*
Estimation of average treatment effects under unconfoundedness or exogenous treatment
assignment is often hampered by lack of overlap in the covariate distributions. This lack of
overlap can lead to imprecise estimates and can make commonly used estimators sensitive
to the choice of specification. In such cases researchers have often used informal methods
for trimming the sample. In this paper we develop a systematic approach to addressing such
lack of overlap. We characterize optimal subsamples for which the average treatment effect
can be estimated most precisely, as well as optimally weighted average treatment effects.
Under some conditions the optimal selection rules depend solely on the propensity score. For
a wide range of distributions a good approximation to the optimal rule is provided by the
simple selection rule to drop all units with estimated propensity scores outside the range [0.1,
JEL Classification:
C14, C21, C52
average treatment effects, causality, unconfoundedness, overlap, treatment
effect heterogeneity
Corresponding author:
Guido W. Imbens
M-24 Littauer Center
Department of Economics
1830 Cambridge Street
Cambridge, MA 02138
E-mail: 
* We are grateful for helpful comments by Richard Blundell, Gary Chamberlain, Jinyong Hahn, Gary
King, Michael Lechner, Robert Moffitt, Geert Ridder and Don Rubin, and by participants in seminars at
the ASSA meetings in Philadelphia, University College London, UCLA, UC–Berkeley, UC–Riverside,
MIT–Harvard, Johns Hopkins University, the Malinvaud seminar at CREST, and the IAB Empirical
Evaluation of Labour Market Programmes conference.
Introduction
There is a large literature on estimating average treatment eﬀects (ATE) under assumptions
of unconfoundedness, ignorability, or exogeneity following the seminal work by Rubin and Rosenbaum and Rubin . Researchers have developed estimators based on
regression methods , matching , and methods based on the propensity score
 .
Related methods
for missing data problems are discussed in Robins, Rotnitzky and Zhao and Robins
and Rotnitzky .1
An important practical concern in implementing these methods is
that one needs overlap between covariate distributions in the two subpopulations, i.e., there
must be common support in the covariates across the two subpopulations. Even if there exists
overlap (common support), there may be parts of this common covariate space with limited
numbers of observations for one or the other treatment groups. Such areas of limited overlap
can lead to poor ﬁnite sample properties for many estimators of average treatment eﬀects.
In such cases, many of these estimators can have substantial bias, large variances, as well as
considerable sensitivity to the exact speciﬁcation of the treatment eﬀect regression functions
or of the propensity score. LaLonde , Heckman, Ichimura and Todd and Dehejia
and Wahba discuss the empirical relevance of this overlap issue.2
One strand of the literature has focused on assessing the robustness of existing estimators
to a variety of potential problems, including limited overlap.3
A second strand focuses on
developing new matching estimators of treatment eﬀects or modifying existing ones to reduce
their sensitivity and improve their precision in the face of the overlap problem. For example,
Rubin and Lee , in situations where there is a single discrete covariate, suggest
simply discarding all units with covariate values with either no treated or no control units.
Alternatively, Cochran and Rubin suggest caliper matching where potential matches
are dropped if the within-match diﬀerence in propensity scores exceeds some threshold level.
LaLonde creates subsamples of the control group by conditioning on covariate values
lying in ranges with substantial overlap. Ho, Imai, King and Stuart propose preprocessing the data by ﬁrst matching units and carrying out parametric inferences using only the
matched data. Heckman, Ichimura and Todd , Heckman, Ichimura, Smith and Todd
 , and Smith and Todd , who focus on estimating the average treatment eﬀect for
the treated (ATT), discard all observations in both the treated and non-treated groups for values of the estimated propensity scores that have zero or occur infrequently. Dehejia and Wahba
 , who also focus on estimating the ATT, discard those non-treated group observations
1See Rosenbaum , Heckman, LaLonde and Smith , Wooldridge , Blundell and Costa-Diaz
 , Imbens and Lee for surveys of this literature.
2Dehejia and Wahba write: “... our methods succeed for a transparent reason: They only use the
subset of the comparison group that is comparable to the treatment group, and discard the complement.”
Heckman, Ichimura and Todd write “A major ﬁnding of this paper is that comparing the incomparable—
i.e., violating the common support condition for the matching variables—is a major source of evaluation bias as
conventionally measured.”
3See, for example, Rosenbaum and Rubin , Rosenbaum , Imbens , and Ichino, Mealli,
and Nannicini .
for propensity scores that are less than the smallest value of the propensity score for those in
the treated group.
Although there are diﬀerences across these alternative strategies, they have several things
in common. First, all of them discard observations for which there is no overlap between the
treated and non-treated group based on either the propensity score or covariate distribution.
As a result, each strategy focuses, in essence, on average treatment eﬀect estimands that are
deﬁned for subsets of the sample observations and, thus, diﬀer from either the typical ATE or
ATT which are deﬁned over the full (population) covariate distributon. Second, each of these
strategies is somewhat arbitrary, i.e., the strategies used to discard or reweight observations in
forming new estimators are based on criteria with unknown properties.
In this paper, we propose a systematic approach to dealing with samples with limited overlap
in the covariates that have optimality properties with respect to the precision of estimating
treatment eﬀects, and which are straightforward to implement in practice. As with the previous
methods, our approaches also are based on characterizing diﬀerent estimands relative to the
traditional ATE or ATT. We return below to the implications of and some justiﬁcations for
this latter feature of our approach.
We consider the following two strategies. In the ﬁrst, we focus on average treatment eﬀects
within a selected subpopulation deﬁned in terms of covariate values. Inevitably, conditioning
on a subpopulation based on any selection criterion reduces the eﬀective sample size, which, all
else the same, increases the variance of the estimated average treatment eﬀect. However, if the
subpopulation is chosen appropriately, it may be possible to estimate the average treatment
within this subpopulation more precisely than the average eﬀect for the entire population despite
the smaller sample size. As we establish below, this tradeoﬀis, in general, well-deﬁned and,
under some conditions, leads to discarding units with propensity scores outside an interval
[α, 1 −α], where the optimal cutoﬀvalue of α is solely determined by the distribution of the
propensity score.
Our approach is consistent with the practice noted above of researchers
dropping units with extreme values of the propensity score, with two important distinctions.
First, the role of the propensity score in our procedure is not imposed from the outset; rather,
it emerges as a consequence of the criterion of variance minimization.
Second, we have a
systematic way of choosing the cutoﬀpoint, α.
We refer to the resulting estimand as the
Optimal Subpopulation Average Treatment Eﬀect (OSATE). We note that the determination
of the subset of observations that characterize a particular OSATE is based solely on the joint
distribution of covariates and the treatment indicator and not on the outcome data. As a result,
we avoid introducing deliberate bias with respect to the treatment eﬀects being analyzed.
In the second strategy, we formulate weighted average treatment eﬀects, where the weights
depend only on covariates. Note that the OSATE can be viewed as a special case of these
weighted treatment eﬀects, where the weight function is restricted to be an indicator function.
Within a broad class, we characterize the weight function that leads to the most precisely
estimated average treatment eﬀect. We note that this class of estimands includes the average
treatment eﬀect for the treated, where the weight function is proportional to the propensity
score. Under the same conditions as before, the optimal weight function turns out to be a
function of the propensity score; in fact, it is proportional to the product of the propensity
score and one minus the propensity score. We refer to this as the Optimally Weighted Average
Treatment Eﬀect (OWATE).
Although both strategies we consider are similiar to the more informal ones noted above, it
is still the case that both are somewhat uncommon in econometric analyses, precisely because
they entail focusing on estimands that depend on sample data.4 Typically, econometric analyses
of treatment eﬀects focus on estimands that are deﬁned a priori for populations of interest,
as is the case with the population average treatment eﬀect or the average treatment eﬀect for
the treated subpopulation. In these cases, estimates are produced that turn out to be more or
less precise, depending on the actual sample data. In contrast, we focus on average eﬀects for
a statistically deﬁned (weighted) subpopulation.5 This change of focus is not motivated, per
se, by an intrinsic interest in the subpopulation for which we ultimately estimate the average
causal eﬀect. Rather, it acknowledges and addresses the diﬃculties in making inferences about
the population of primary interest.
In our view this approach has several justiﬁcations. First, our approach of achieving precision in the estimation of treatment eﬀects has analogues in the statistics literature. In particular,
it is similar to the traditional motivation for medians rather than means as more precise measures of central tendency. In particular, by changing the sample from one that was potentially
representative of the population of interest, we can gain greater internal validity, although, in
doing so, we may sacriﬁce some of the external validity of the resulting estimates.6 Furthermore, our proposed approach of placing greater stress on internal versus external validity is
similar to that found in the design of randomized experiments which are often carried out on
populations unrepresentative of the population of interest in order to improve the precision of
the inferences to be drawn. More generally, the relative primacy of internal validity over external validity is advocated in many discussions of causal inference .
Second, our approach may be well-suited to situations where the primary interest is to determine whether a treatment may harm or beneﬁt at least some group in a broader population.
For example, one may be interested whether there is any evidence that a particular drug could
harm or have side eﬀects for some group of patients in a well-deﬁned population. In this context, obtaining greater precision in the estimation of a treatment eﬀect, even if it is not for the
entire population, is warranted. We note that the subpopulation for which these estimands are
valid are deﬁned in terms of the observed covariate values so that one can determine, for each
individual, whether they are in the relevant subpopulation or not.
Third, our approach can provide useful, albeit auxiliary, information when making inferences
about the treatment eﬀects for ﬁxed populations. Thus, instead of only reporting the potentially
imprecise estimate for the population average treatment eﬀect, one can also report the estimates
4We note that the local average treatment eﬀect introduced by Imbens and Angrist represents another
example in which a new estimand is introduced—one in which the average eﬀect of the treatment is deﬁned for
the subpopulation of compliers—to deal with a phenomenon quite similar to limited overlap.
5This is also true for the method proposed by Heckman, Ichimura and Todd .
6A separate issue is that in practice in many cases even the original sample is not representative of the
population of interest. For example, we are often interested in policies that would extend small pilot versions of
job training programs to diﬀerent locations and times.
for the subpopulations where we can make more precise inferences.
Fourth, focusing on estimands that discard or reweight observations from the treated and
non-treated group subsamples in order to improve precision tends to produce more balance in
the distribution of the covariates across these groups. As has been noted elsewhere , increasing the balance
in the covariate distributions tends to reduce the sensitivity of treatment eﬀect estimates to
changes in the speciﬁcation.
In the extreme case, where the selected sample is completely
balanced in covariates in the two treatment arms, one can simply use the average diﬀerence in
outcomes between treated and control units.
At the same time, our focus on strategies for improving the precision of treatment eﬀect
estimators in the face of limited overlap has its limitations. For example, one might seek to
devise strategies to deal with limited overlap of the covariate distributions that balance the
representativeness of that distributon with precision.
While exploring how to achieve such
objectives is desirable, we see the results in this paper as an important ﬁrst step in formulating
strategies to deal with the problem of limited overlap that have well-deﬁned properties and that
can be implemented on real data.
Finally, it is important to note that the properties we derive below concerning the precision
associated with both the OSATE and OWATE estimands are not tied to a speciﬁc estimator.
Rather, we focus on diﬀerences in the eﬃciency bounds for diﬀerent subpopulations. As a consequence, a range of eﬃcient estimators—including the ones proposed by Hahn , Hirano,
Imbens and Ridder , Imbens, Newey and Ridder , and Robins, and Rotnitzky
and Zhao —can potentially be used to estimate these estimands, especially the OWATE.
However, as we make clear below, these standard estimators are not readily applicable to the
estimation of the OSATE, due to the complications that arise from having to estimate the
optimal subsets of the covariate distribution for this estimand. Accordingly, we develop a new
estimator that deals with this case and derive its large sample properties.
We illustrate these methods using data from the non-experimental part of a data set on labor
market programs previously used by LaLonde , Heckman and Hotz , Dehejia and
Wahba , Smith and Todd and others. In this data set the overlap issue is a well
known problem, with the control and treatment group far apart on some of the most important
covariates including lagged values for the outcome of interest, yearly earnings. Here our OSATE
method suggests dropping 2363 out of 2675 observations (leaving only 312 observations, or just
12% of the original sample) in order to minimize the variance. Calculations suggest that this
lowers the variance by a factor 1/160, 000, reﬂecting the fact that most of the controls are very
diﬀerent from the treated and that it is essentially impossible to estimate the population average
treatment eﬀect. More relevant, given the fact that most of the researchers analyzing this data
set have focused on the average eﬀect for the treated, is that the variance for the optimal
subsample is only 40% of that for the propensity score weighted sample (which estimates the
eﬀect on the treated).
The remainder of the paper is organized as follows. In Section 2, we present a simple example
in which there is a single and scalar covariate used in the estimation of the average treatment
This example allows us to illustrate how the precision of the estimates varies with
changes in the estimand. Section 3 develops the general setup we use throughout the paper.
Section 4 reviews the previous approaches to dealing with limited overlap when estimating
treatment eﬀects. In Section 5, we develop new estimands and discuss their precision gains.
We also show that for a wide class of distributions the optimal set is well approximated by the
set of observations with propensity scores in the interval [0.1, 0.9]. In Section 6, we discuss the
properties of estimators for the OSATE and OWATE estimands. In Section 7, we present the
application to the LaLonde data. Section 8 concludes.
A Simple Example
To set the stage for the issues to be discussed in this paper, consider the following simpliﬁed
treatment eﬀect example in which the covariate of interest, X, is a scalar taking on one of two
values. In particular, suppose that X = f (female) or X = m (male), so that the covariate
space is X = {f, m}. For x = f, m, let Nx be the sample size for the subsample with X = x,
and let N = Nf + Nm be the total sample size. Let W ∈{0, 1} denote the indicator for the
treatment. Also, let p = E[W] be the population share of treated individuals, where ˆp = Nm/N
is the share of men in the sample. We denote the average treatment eﬀect, conditional on
X = x, as τx. Let Nxw be the number of observations with covariate Xi = x and treatment
indicator Wi = w. It follows that ex = Nx1/Nx is the propensity score for x = f, m. Finally,
let ¯yxw = !N
i=1 Yi · 1{Xi = x, Wi = w}/Nxw be the average within each of the four subsamples.
We assume that the distribution of the outcomes is homoskedastic, i.e., the variance of Y (w)
given Xi = x is σ2 for all x = f, m and w = 0, 1.
At the outset, consider the following two average treatment eﬀects that diﬀer in somewhat subtle ways. In particular, consider the average eﬀect that is averaged over the sample
distribution of the covariates,
τS = ˆp · τm + (1 −ˆp) · τf,
versus the average treatment eﬀect for the full population,
τP = p · τm + (1 −p) · τf.
where τx = E[yx1 −yx0], x = f, m. It is immediately obvious that for either τS and τP the
natural estimator is
ˆτX = ˆp · ˆτm + (1 −ˆp) · ˆτf.
However, as we develop below, which estimand is the object of interest makes a diﬀerence in
terms of the variance for this esimtator and this fact plays a crucial role in the results derived
in this paper.
To make things very simple, suppose that subjects are randomly assigned to one of the treatment statuses, Wi = 0 or 1, conditional on X. In this case, the natural, unbiased, estimators
for the average treatment eﬀects for each of the two subpopulations are
ˆτf = ¯yf1 −¯yf0,
ˆτm = ¯ym1 −¯ym0,
with variances (conditional on the covariates)
V(ˆτf) = E[(ˆτf −τf)2] = σ2 ·
N · (1 −ˆp) ·
ef · (1 −ef),
V(ˆτm) = E[(ˆτm −τm)2] = σ2 ·
em · (1 −em).
respectively. The estimator for the sample average treatment eﬀect, τS, is
ˆτX = ˆp · ˆτm + (1 −ˆp) · ˆτf.
Because the two estimates, ˆτf and ˆτm, are independent, it follows that the variance of this
estimator is
VS(ˆτX) = E[(ˆτX −τS)2] = ˆp2 · V (ˆτm) + (1 −ˆp)2 · V (ˆτf)
em · (1 −em) +
ef · (1 −ef)
It follows that the asymptotic variance of
N(ˆτX −τS) converges to
N(ˆτX −τS)
em · (1 −em) +
ef · (1 −ef)
eX · (1 −eX)
Note, however, that the asymptotic variance of
N(ˆτX −τP ) converges to
N(ˆτX −τP )
em · (1 −em) +
ef · (1 −ef)
+p·(τm−τP )2+(1−p)·(τf −τP )2
eX · (1 −eX)
+ E[(τX −τP )2].
where the extra term in this second variance arises because of the diﬀerence between the average
treatment eﬀect conditional on the sample distribution of X and the one for the full population.
The ﬁrst formal result of the paper concerns the comparison of VS(ˆτ), V(ˆτf), and V(ˆτm)
according to a variance minimization criterion. In particular, the optimal subset A∗⊂X that
Vmin = min(VS(ˆτX), V(ˆτf), V(ˆτm)) = V(ˆτA∗).
is given by
ef (1−ef )
ef (1−ef )
ef (1−ef ) .
Note that which estimator has the smallest variances crucially depends on the ratio of the
product of the propensity score and one minus the propensity score, em(1 −em)/(ef(1 −ef)).
If the propensity score for women is close to zero or one, we cannot estimate the average
treatment eﬀect for women precisely. In that case the ratio em(1−em)/(ef(1−ef)) will be high
and we may be able to estimate the average treatment eﬀect for men more accurately than the
average eﬀect for the sample as a whole, even though we may well lose a substantial number
of observations by discarding women. Similarly, if the propensity score for men is close to zero
or one, the ratio em(1 −em)/(ef(1 −ef)) is close to zero, and we may be able to estimate the
average treatment eﬀect for the women more accurately than for the sample as a whole. If the
ratio is close to one, we can estimate the average treatment eﬀect for the population as a whole
more accurately than for either of the two subpopulations. Put diﬀerently, based on the data,
and more speciﬁcally the distribution of (X, W), one might prefer to estimate τf (or τm), rather
than the overall average τ, if, a priori, it is clear that τ cannot be estimated precisely, and τf
(or τm) can be estimated with accuracy. In this case there is a second obvious advantage of
focusing on subpopulation average treatment eﬀects. Within the two subpopulations, we can
estimate the within-subpopulation average treatment eﬀect without bias by simply diﬀerencing
average treatment and control outcomes. As a result, our results are not sensitive to the choice
of estimator for the within-subpopulation treatment eﬀects. This need not be the case for
the population as a whole, where there is potentially substantial bias from simply diﬀerencing
average outcomes.
Note that we did not deﬁne A∗so as to minimize (AV (
N(ˆτX −τP ))/N, V(ˆτf), V(ˆτm)).
While doing so is, in principle, possible, it has two drawbacks, given our desire to determine the
estimator which has the smallest variance and that is implementable in practice. First, using
N(ˆτX −τP ))/N, V(ˆτf), V(ˆτm)) as the criteria for estimator selection would require
one to evaluate E[(τX −τP )2], which would necessarily be diﬃcult to do. Second, this criterion
depends on the value of the treatment eﬀect, and, as such, would require analyzing outcome
data, Y , before selecting the sample. This would open the door to introducing deliberate biases
of the sort avoided by a selection criterion that depends solely on the treatment and covariate
A second issue concerns knowledge of A∗. In an actual data set, one typically does not
know A∗and it would have to be estimated, using estimated values for the propensity score
and the covariate distribution. Call this estimate ˆA. In cases with continuous covariates, the
uncertainty stemming from the diﬀerence between ˆA and A∗is not neglible. As a result, in our
discussion of statistical inference below, we focus on the distribution of
N(ˆτˆA −τˆA), rather
than at the distribution of
N(ˆτˆA −τA∗). That is, we focus on the deviation of the estimated
average eﬀect relative to the average eﬀect in the selected subsample, not relative to the average
eﬀect in the subset that would be optimal in the population. To be clear, focusing on τˆA rather
than τA∗has consequences. For example, suppose that our estimate is ˆA = {m}, so that we
estimate the average treatment eﬀect using only data for the male subpopulation. It may well
be that, in fact, A∗= X so that the average treatment eﬀect should be estimated over the
population of men and women. Nevertheless, we focus on the distribution of ˆτˆA −τˆA = ˆτm −τm,
rather than on the asymptotic distribution of ˆτˆA −τA∗= ˆτˆA −τˆA + (τˆA −τA∗). Given that ˆA is
known, and A∗is not, the estimates would seem more interpretable that way.
The second result of the paper takes account of the fact that one need not limit the choice of
average treatment eﬀects to the three discussed so far. In particular, one may wish to consider
a weighted average treatment eﬀect of the form
τλ = λ · τm + (1 −λ) · τf,
for ﬁxed λ. It follows that τλ can be estimated by
ˆτλ = λ · ˆτm + (1 −λ) · ˆτf,
where the variance for this weighted average treatment eﬀect is given by
V(ˆτλ) = λ2 · V(ˆτm) + (1 −λ)2 · V(ˆτf)
em · (1 −em) + (1 −λ)2 ·
N · (1 −ˆp) ·
ef · (1 −ef).
It follows that the variance of this estimator is minimized by choosing λ to be
1/V(ˆτm) + 1/V(ˆτf) =
ˆp · em · (1 −em)
(1 −ˆp) · ef · (1 −ef) + ˆp · em · (1 −em).
with the minimum value for the variance equal to
V(τλ∗) = σ2
((1 −ˆp) · ef · (1 −ef) + ˆp · em · (1 −em)).
The ratio of the variance for the population average to the variance for the optimally weighted
average treatment eﬀect is
V(τP )/V(τλ∗) −→E
eX · (1 −eX)
· E[eX · (1 −eX)] = E
· E[V(W|X)]. (2.3)
By Jensen’s inequality this is greater than one if V(W|X) varies with X.
So what are some of the implications of focusing on a criterion of variance minimization for
selecting among alternative treatment eﬀect estimators derived from this simpliﬁed example?
Suppose one is interested in the sample average treatment eﬀect, τS. One may ﬁnd that the
eﬃcient estimator for this average eﬀect is likely to be imprecise, even before looking at the
outcome data. This would be consistent with two states of the world that correspond to very
diﬀerent sets of information about treatment eﬀects. In one state, the average eﬀect for both
of the subpopulations are imprecisely estimable, and, in eﬀect, one cannot say much about the
eﬀect of the treatment at all. In the other state of the world it is still possible to learn something
about the eﬀect of the treatment because one of the subpopulation average treatment eﬀects
can be estimated precisely. In that case, which corresponds to the propensity score for one of
the two subpopulations being close to zero or one, it may be useful to report also the estimator
for the precisely estimable average treatment eﬀect to convey the information the data contain
about the eﬀect of the treatment. It is important to stress that the message of the paper is not
that one should report only ˆτm or ˆτf in place of ˆτ. Rather, in cases where ˆτm or ˆτf are precisely
estimable and ˆτ is not, we propose one should report both.
In the remainder of the paper, we generalize the above analysis to the case with a vector
of potentially continuously distributed covariates. We study the existence and characterization
of a partition of the covariates space X into two subsets, A∗and X/A∗. For A∗, the average
treatment eﬀect is at least as accurately estimable as that for any other subset of the covariate
space. This leads to a generalization of (2.1). Under a certain set of assumptions, this problem
has a well-deﬁned solution and, under homoskedasticity, these subpopulations have a very
simple characterization, namely the set of covariates such that the propensity score is in the
closed interval [α, 1 −α], or A∗= {x ∈X|α ≤e(x) ≤1 −α}.
The optimal value of the
boundary point, α, is determined by the distribution of the propensity score and its calculation
is straightforward. Compared to the binary covariate case just considered, it will be diﬃcult to
argue in the general setting that this subpopulation is of intrinsic or substantive interest. We
will not attempt to do so. Instead, we view it as an interesting average treatment eﬀect because
of its statistical properties and, in particular, as a convenient summary measure of the full
distribution of conditional treatment eﬀects τ(x). In addition, we characterize the optimally
weighted average treatment eﬀect and its variance, the generalization of (2.2) and (2.3).
The framework we use is standard in this literature.7 We have a random sample of size N
from a large population. For each unit i in the sample, let Wi indicate whether the treatment
of interest was received, with Wi = 1 if unit i receives the treatment of interest, and Wi = 0
if unit i receives the control treatment. Using the potential outcome notation popularized by
Rubin , let Yi(0) denote the outcome for unit i under control and Yi(1) the outcome
under treatment. We observe Wi and Yi, where
Yi = Yi(Wi) = Wi · Yi(1) + (1 −Wi) · Yi(0).
In addition, we observe a vector of pre-treatment variables, or covariates, denoted by Xi.
Deﬁne the two conditional mean functions, µw(x) = E[Y (w)|X = x], the two conditional
variance functions, σ2
w(x) = Var(Y (w)|X = x), the conditional average treatment eﬀect τ(x) =
E[Y (1) −Y (0)|X = x] = µ1(x) −µ0(x), and the propensity score, the probability of selection
into the treatment, e(x) = Pr(W = 1|X = x) = E[W|X = x].
Initially, we focus on two average treatment eﬀects. The ﬁrst is the (super-)population
average treatment eﬀect
τP = E[Y (1) −Y (0)].
We also consider the sample average treatment eﬀect
7For example, see Rosenbaum and Rubin , Hahn , Heckman, Ichimura and Todd , and
Hirano, Imbens and Ridder .
where we condition on the observed set of covariates. The reason for focusing on the second one
is twofold. First, it is analogous to the conditioning on covariates commonly used in regression
analysis. Second, it can be estimated more precisely if there is variation in the treatment eﬀect
by covariates.
To solve the identiﬁcation problem, we maintain throughout the paper the unconfoundedness
assumption , which asserts that conditional on the
pre-treatment variables, the treatment indicator is independent of the potential outcomes:
Assumption 3.1 (Unconfoundedness)
W ⊥(Y (0), Y (1))
This assumption is widely used in this literature. See discussions in Hahn , Heckman,
Ichimura, and Todd , Hirano, Imbens, and Ridder , Lechner , and others.
In addition, we assume there is overlap in the covariate distributions:
Assumption 3.2 (Overlap) For some c > 0, and all x ∈X, where X is the support of X,
c ≤e(x) ≤1 −c.
In addition, one often needs smoothness conditions on the two regression functions µw(x) and
the propensity score e(x) for estimation. We make those assumptions explicit in Section 6.
Previous Approaches to Dealing with Limited Overlap
In empirical applications, there is often concern about the overlap assumption . As noted in the Introduction, researchers
have sometimes trimmed their sample by excluding observations with propensity scores close
to zero or one in order to ensure that there is suﬃcient overlap. Cochran and Rubin 
suggest using caliper matching where units whose match quality is too low according to the
distance in terms of the propensity score are left unmatched.
Dehejia and Wahba focus on the average eﬀect for the treated. They suggest dropping
all control units with an estimated propensity score lower than the smallest value for the
estimated propensity score among the treated units. Formally, they ﬁrst estimate the propensity
score. Let the estimated propensity score for unit i be ˆe(Xi). Then let e1 be the minimum of
the ˆe(Xi) among treated units. Dehejia and Wahba drop all control units such that ˆe(Xi) < e1.
Heckman, Ichimura and Todd , Heckman, Ichimura, Smith and Todd and
Smith and Todd also focus on the average eﬀect for the treated. They propose discarding
units with covariate values at which the estimated density is below some threshold. The precise
method is as follows.8 First, they estimate the propensity score ˆe(x). Next, they estimate
8See Heckman, Ichimura and Todd and Smith and Todd for details, and Smith and Todd 
and Ham, Li and Reagan for applications of this method.
the density of the estimated propensity score in both treatment arms. Let ˆfw(e) denote the
estimated density of the estimated propensity score. The speciﬁc estimator they use is a kernel
estimator, ˆfw(e) =
, with bandwidth h.9 First, Heckman, Ichimura
and Todd discard observations with ˆf0(ˆe(Xi)) or ˆf1(ˆe(Xi)) exactly equal to zero leaving J
observations.10
Next, they ﬁx a quantile q (Smith and Todd use q = 0.02).
Using the J
observations with positive densities, they rank the 2J values of ˆf0(ˆe(Xi)) and ˆf1(ˆe(Xi)). They
then drop units i with ˆf0(ˆe(Xi)) or ˆf1(ˆe(Xi)) less than or equal to cq, where cq is the largest
real number such that
1{ ˆf0(ˆe(Xi)) < cq} + 1{ ˆf1(ˆe(Xi)) < cq}
Ho, Imai, King and Stuart propose combining any speciﬁc parametric procedure
that the researcher may wish to employ with a nonparametric ﬁrst stage. In this ﬁrst stage, all
treated units are matched to the closest control unit. Only the treated units and their matches
are then used in the second stage. The ﬁrst stage leads to a data set that is more balanced in
terms of covariate distributions between treated and control. It thus reduces sensitivity of the
parametric model to speciﬁc modelling decisions such as the inclusion of covariates or functional
form assumptions.
All these methods tend to make the estimators more robust to speciﬁcation decisions. However, few formal results are available on the properties of these procedures. They typically also
depend on arbitrarily selected values for the “trimming” parameters.
Alternative Estimands
This section contains the main results of the paper. First, in Subsection 5.1, we review some
results on eﬃciency bounds and present one new result. These eﬃciency bounds are used to
motivate the estimands that we propose in the remainder of this section. In Subsection 5.2, we
discuss the choice of criteria for selecting estimands that have optimal properties with respect
to the estimation of average treatment eﬀects. In Subsection 5.3, we derive the optimal subset
of covariates over which to deﬁne the estimand, and in Subsection 5.4 we derive the optimal
weights. Finally we provide some numerical calculations based on the Beta distribution.
Eﬃciency Bounds
In this subsection, we discuss some results on eﬃciency bounds for average treatment eﬀects
that will be used to motivate the estimands proposed in this paper. In addition, we present a
new result on eﬃciency bounds.
Various bounds have been derived in the literature.11 Hahn derived the semiparametric eﬃciency bound for τP = E[Y (1) −Y (0)] under unconfoundedness and overlap use Silverman’s rule of thumb to choose the bandwidth.
10Observations with the estimated density exactly equal to zero may exist when the kernel has ﬁnite support.
For example, Smith and Todd use a quadratic kernel with K(u) = (u2 −1)2 for |u| ≤1 and zero elsewhere.
11See Hahn , Robins, Rotznitzky and Zhao , Robins, Mark, and Newey , and Hirano, Imbens
and Ridder .
some regularity conditions).12 The eﬃciency bound for τP is
1 −e(X) + (τ(X) −τP )2
A generalization of τP to the case of the weighted average treatment eﬀect given by
τP,ω = E [τ(X) · ω(X)]
with the weight function ω : X ,
where they establish that the eﬃciency bound for τP,ω is
E[ω(X)]2 · E
1 −e(X) + (τ(X) −τP,ω)2
Hirano, Imbens and Ridder propose the eﬃcient estimator,
ˆe(Xi) −Yi · (1 −Wi)
for τP,ω. The inﬂuence function for this estimator is
ψP,ω(y, w, x) =
w · y −µ1(x)
−(1 −w) · y −µ0(x)
1 −e(x) + µ1(x) −µ0(x) −τP,ω
ˆτω = τP,ω + 1
ψP,ω(Yi, Wi, Xi) + op
Note that ˆτω also can be interpreted as an estimator of the weighted sample average treatment
τ(Xi) · ω(Xi)
As an estimator for τS,ω, ˆτω satisﬁes
N · (ˆτω −τS,ω)
E[ω(X)]2 · E
Comparing the eﬃciency bound for τP,ω in (5.7) with the asymptotic variance in (5.9), it follows
that we can estimate τS,ω more accurately than τP,ω, so long as there is variation (with X) in
the treatment eﬀect τ(x).
Next we consider the case where the weights depend on the propensity score: ω(x) = λ(e(x)),
with λ : (→R known and the propensity score is unknown. (If the propensity score is
known, this is a special case of the previous result.) If the propensity score is unknown, the
eﬃciency bound changes. We establish what it is in the following theorem:
12See also Robins, Rotznitzky and Zhao for a related result in a missing data setting.
Theorem 5.1 (Weighted Average Treatment Effects with Weights Depending on
the Propensity Score) Suppose Assumptions 3.1 and 3.2 hold, and suppose that the weights
are a function of the propensity score: ω(x) = λ(e(x)) with λ(e) known and e(x) unknown.
Then the semiparametric eﬃciency bound for τP,λ is
E[λ(e(X))]2 · E
λ(e(X))2 ·
1 −e(X) + (τ(X) −τP,λ)2
E[λ(e(X))]2 · E
e(X)(1 −e(X)) ·
(τ(X) −τP,λ)2
Proof: See Appendix.
The diﬀerence between the known weight function case (5.7) and the case with the weight
function depending on the unknown propensity score established in Theorem 5.1 is the last term
in (5.10). The fact that this term depends on the derivative of the weight function with respect
to the propensity score will give rise to problems in formulating an implementable estimator.
We address this issue in Section 6 below.
A Criterion for Choosing the Estimand
We now consider the problem of selecting the estimand that minimize the asymptotic variance
in (5.9). Formally, we choose an estimand τS,ω by choosing the weight function ω(x) that
minimizes:
E[ω(X)]2 · E
Under the assumption that the distribution of Y is homoskedastic, the criterion is slightly
modiﬁed and one minimizes
E[ω(X)]2 · E
However, before implementing this approach, we oﬀer several comments in support of using
these variance-minimization criteria.
As we have discussed, one of the consequences of limited overlap in the covariate distributions is the imprecision with which average treatment eﬀects are estimated. Suppose, for now,
that the propensity score is known. In that case, no matter how unbalanced the sample is, there
is an estimator that is exactly unbiased as long as the propensity score is strictly between zero
and one. However, if the sample is severely imbalanced, the eﬃciency bound—and, in this case,
the exact variance of the associated estimator—will be large. Because of this imprecision, it is
desirable to try to modify the estimator to improve its precision. One possibility is to utilize
a mean-squared-error type criterion for this modiﬁcation. Unfortunately, implementing such a
criterion would be very diﬃcult in practice, as the biases of alternative estimators would be
diﬃcult to estimate. This is because these biases will depend on the entire function τ(x), which
is likely to be diﬃcult to estimate for some values of x. More generally, alternative estimators
for τP will tend to suﬀer from this same problem.
To get around this problem, we focus on alternative estimands to τP . In doing so, the
two questions naturally arise: What is the class of estimands and what is the criterion for
choosing an estimand within that class? A natural class of estimands would seem to be τP,ω =
E[ω(X)·τ(X)]/E[ω(X)] and to use a criterion of the asymptotic variance of associated estimators
in order to reduce the imprecision associated with limited overap. We do not pursue this class of
estimands because evaluating the asymptotic variance of estimators for such estimands requires
estimation of τ(x) over the entire support to deal with the term E[ω2(X) · (τ(X) −τP,ω)2] in
the expression of the asymptotic variance in (5.7), making it diﬃcult to implement in practice.
Furthermore, the resulting variance-minimization criterion associated with this estimand would
depend on values of the treatment eﬀect, introducing the potential bias discussed in Section 2.
For these two reasons, we limit ourselves to the class of estimands given by τS,ω = !
i ω(Xi) for diﬀerent ω(·). We note, however, that this is not the only possible approach. There may be alternative classes of estimands or alternative criteria that would lead to
eﬀective solutions. The key, however, is to use a systematic approach characterized by a class
of estimands and a formal criterion for choosing an estimand within that class that are easy to
implement.
The Optimal Subpopulation Average Treatment Eﬀect
In this section, we characterize the Optimal Subpopulation Averate Treatment Eﬀect (OSATE).
We do so by restricting attention to weight functions that are indicator functions: ω(x) =
1{x ∈A}, where A is some closed subset of the covariate space X. For a given set, A, we deﬁne
corresponding population and sample average treatment eﬀects τP,A and τS,A as
τP,A = E[τ(X)|X ∈A],
and τS,A =
respectively, where NA = !N
i=1 1{Xi ∈A} is the number of observations with covariate values
in the set A. Let q(A) = Pr(X ∈A) = E[1{X ∈A}]. With this class of weight functions, the
criterion given in (5.11) can be written as
We look for an optimal A, denoted by A∗, that minimizes the asymptotic variance (5.13) among
all closed subsets A.
As noted in the Introduction, focusing on estimands that discard observations to reduce the
variance of average treatment eﬀect estimators has two opposing eﬀects. First, by excluding
units with covariate values outside the set A, one reduces the eﬀective sample size from N to
N · q(A). This will increase the asymptotic variance by a factor 1/q(A). Second, by discarding
units with high values for σ2
1(X)/e(X)+σ2
0(X)/(1−e(X))—that is, units with covariate values
x such that it is diﬃcult to estimate the average treatment eﬀect τ(x)—one can lower the
conditional expectation E[σ2
1(X)/e(X) + σ2
0(X)/(1 −e(X))|X ∈A]. Optimally choosing A
involves balancing these two eﬀects.
The following theorem gives the formal result for the optimal A∗that minimizes the asymptotic variance. Deﬁne k(x) = σ2
1(x)/e(x) + σ2
0(x)/(1 −e(x)).
Theorem 5.2 (Optimal Overlap for the Average Treatment Effect)
Suppose Assumptions 3.1-3.2 hold. Let f ≤fX(x) ≤f, and σ2
w(x) ≤σ2 for w = 0, 1 and all
x ∈X. We consider τ(A) where A is a closed subset of X. Then the Optimal Subpopulation
Average Treatment Eﬀect (OSATE) is τS,A∗, where, if supx∈X k(x) ≤2 · E [k(X)], then A∗= X
and otherwise,
----k(x) ≤
α · (1 −α)
where α is a solution to
α · (1 −α) = 2 · E
----k(X) <
α · (1 −α)
Proof: See Appendix.
The result in this theorem simpliﬁes in an interesting way under homoskedasticity.
Corollary 5.1 Optimal Overlap for the Average Treatment Effect Under Homoskedasticity Suppose Assumptions 3.1-3.2 hold.
Let f ≤fX(x) ≤f.
Suppose that
w(x) = σ2 for all w ∈{0, 1} and x ∈X. Then the OSATE under homoskedasticity is τS,A∗,
H = {x ∈X |α ≤e(x) ≤1 −α} .
e(x) · (1 −e(x)) ≤2 · E
e(X) · (1 −e(X))
then α = 0. Otherwise α is a solution to
α · (1 −α) = 2 · E
e(X) · (1 −e(X))
e(X) · (1 −e(X)) ≤
α · (1 −α)
In Section 6, we focus on an estimator for the optimal estimand based on choosing weight
functions that minimizes the criterion in (5.12) which corresponds to the case where the Y
distribution is homoskedastic rather than using the criterion in (5.11). We use this criterion,
even though we do not presume that the true distribution of Y is homoskedastic and, in fact,
derive the asymptotic properties of this estimator under heteroskedasticity. We use the heteroskedastic criterion in (5.12) for three distinct reasons. The ﬁrst—a principled reason—is that
estimators of A∗
H do not require using outcome data. The entire analysis of selecting the sample
can be carried out without using the outcome data, thus avoiding any deliberate bias that may
result from selecting the sample based on outcome data. The second—a practical reason—is
that the entire analysis is motivated by the diﬃculty of estimating τ(x) for covariates in some
subset of the covariate space. For those values, it is even less likely that one can estimate the
conditional variances σ2
w(x) accurately, and, hence, methods that rely on nonparametrically
estimating these conditional variances are unlikely to be eﬀective using sample sizes found in
practice. Note that the imbalance that precludes accurate estimation of τ(x) and σ2
not necessarily preclude accurate estimation of the propensity score e(x). In fact, when it is
impossible to estimate τ(x) because there are no treated or no control units for a particular
value of X, it need not be diﬃcult at all to estimate the propensity score accurately. The third
reason is that it is rare in applications to ﬁnd diﬀerences of conditional variances that vary by an
order of magnitude. In contrast, it is common to ﬁnd considerable variation in the propensity
score so that the dependence of the optimal region on the conditional variances is likely to be
less important. For these reasons, we focus on A∗
H, optimal sets based under homoskedasticity,
even though all of the estimation used in making inferences will not maintain this assumption.
The ﬁnal result in this section concerns the case where we are interested only in the average
treatment eﬀect for the treated. In this case, it makes sense to limit the estimand to the average
over the subpopulation of the treated with suﬃcient overlap. Formally, we are interested in the
set A that minimizes
E[e(X) · 1{X ∈A}]2 · E
e2(X) · 1{X ∈A} ·
We only present the result under homoskedasticity.
Theorem 5.3 Optimal Overlap for the Average Effect for the Treated Under
Homoskedasticity Suppose Assumptions 3.1-3.2 hold. Let f ≤fX(x) ≤f. Then the set A∗
that minimizes
E[e(X) · 1{X ∈A}]2 · E
e2(X) · 1{X ∈A} ·
is equal to A∗
t = {x ∈X |e(x) ≤αt } , where αt = 1 if
1 −e(x) ≤2 · E
---- W = 1
and αt is a solution to
---- W = 1, e(X) ≤αt
otherwise.
Proof: See Appendix.
The Optimally Weighted Average Treatment Eﬀect
In this section, we consider weighted average treatment eﬀects of the form
ω(Xi) · τ(Xi)
without requiring ω(x) to be an indicator function.
The following theorem gives the most
precisely estimable weighted average treatment eﬀect.
Theorem 5.4 (Optimally Weighted Average Treatment Effect)
Suppose Assumptions 3.1-3.2 hold. Let f ≤fX(x) ≤f, and σ2
w(x) ≤σ2 for w = 0, 1 and all
x ∈X. Then the Optimal Weighted Average Treatment Eﬀect (OWATE) is τS,ω∗, where
Proof: See Appendix.
Again the result simpliﬁes under homoskedasticity to an estimand in which the weight
functions only depend on the propensity score.
Corollary 5.2 (Optimally Weighted Average Treatment Effect under Homoskedasticity)
Suppose Assumptions 3.1-3.2 hold.
Let f ≤fX(x) ≤f. Suppose that σ2
w(x) = σ2 for all
w ∈{0, 1} and x ∈X. Then the Optimally Weighted Average Treatment Eﬀect (OWATE) is
H(x) = e(x) · (1 −e(x)).
Numerical Simulations for Optimal Estimands when the Propensity Score
follows a Beta Distribution
In this section, we assess the implications of the results derived in the previous sections. We do
so by presenting simulations for the optimal estimands when the true propensity score follows
a Beta distribution. We study the homoskedastic case, where the optimal cutoﬀvalue as well
as the ratio of the variances depends only on the (true) marginal distribution of the propensity
score. The Beta distribution is characterized by two parameters, here denoted by β and γ, both
nonnegative. For a Beta distribution with parameters β and γ, denoted by B(β, γ), the mean is
β/(γ+β), ranging from zero to one. The corresponding variance is βγ/((γ+β)2(γ+β+1)), which
lies between zero and 1/4. The largest value that this variance takes on is for γ = β = 0, leading
to a binomial distribution with probability 1/2 for both zero and one. We focus on distributions
for the true propensity score, where β ∈{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4} and γ ∈{β, . . ., 4}.13 For a
given pair of values (β, γ), let VP (β, γ) denote the asymptotic variance of the eﬃcient estimator
for the sample average treatment eﬀect, τS, which is given by
VP (β, γ) = σ2 · E
---- e(X) ∼B(β, γ)
In addition, let VP,α(β, γ) denote the asymptotic variance for the sample average treatment
eﬀect, where we drop observations with the propensity score outside the interval [α, 1 −α].
This variance is given by
VP,α(β, γ) =
Pr (α ≤e(X) ≤1 −α| e(X) ∼B(β, γ))
13There is no diﬀerence from our perspective between a Beta distribution with parameters γ and β and one
with parameters β and γ.
---- α ≤e(X) ≤1 −α, e(X) ∼B(β, γ)
Finally, let α(β, γ) denote the optimal cutoﬀpoint for the case where the true propensity
score has a Beta distribution with parameters γ and β. We calculate the resulting variances,
VP,α(β, γ), for the optimal cutoﬀpoint and two ﬁxed cutoﬀvalues, 0.01, and 0.1. For each of
the Beta distributions we report the three ratios
VP,α(β,γ)(β, γ),
VP,0.01(β, γ)
VP,α(β,γ)(β, γ),
VP,0.10(β, γ)
VP,α(β,γ)(β, γ).
Table 1 presents results for this case. There are two main ﬁndings. First, the gain from
trimming the sample can be substantial, reducing the asymptotic variance of the average treatment eﬀect estimand by a factor of up to ten for some of the values of the propensity score
based on the Beta distribution. Second, discarding observations with a propensity score outside
the interval [0.1, 0.9] produces variances that are extremely close those produced with optimally
chosen cutoﬀvalues. In particular, the ratio of the asymptotic variance when using a cutoﬀ
value of 0.1 to the variance based on the optimal cutoﬀvalue is never larger than 1.04 over the
range of distributions we investigate. In contrast, using the smaller ﬁxed cutoﬀvalue of 0.01
can lead to considerably larger variances than using the optimal cutoﬀvalue.
In this section, we discuss inference for the estimands introduced in the previous sections. Two
issues arise with respect to the tractibility of forming estimators for some of these estimands.
First, as we have noted at the end of Section 5.1, there is an important diﬀerence in the
eﬃciency bounds for population average treatment eﬀects (τP ) and sample average treatment
eﬀects (τS) that complicate the formation of estimators for the former estimand. In particular,
the eﬃciency bound for τP requires one to evaluate τ(X) over its population distribution, which
implies that one must know this distribution or be able to estimate it non-parametrically in
order to determine this bound.14 In general, the distribution of τ(X) is not known and nonparametrically estimating it with any precision is complicated precisely because of the limited
overlap problem. Such complications do not arise if we focus on sample average treatment
eﬀects, τS. Accordingly, we restrict our attention to chracterizing estimators for the latter class
of OSATE and OWATE estimands.
A second issue arises in the case of making inferences concerning Optimal Subpopulation
Average Treatment Eﬀects (OSATE). In particular, for this class of estimands, the optimal set,
A∗, is generally unknown and must be estimated. Moreover, the eﬃciency bound in Theorem
5.1 implies that, in some cases, the average eﬀect over any subset of the covariate space deﬁned
in terms of the propensity score cannot be estimated at root−N rate. For example, suppose
that the set of interest is A = {x ∈X|e(x) ≤p}. Note that this corresponds to using the
14The same problems arise in the estimation of τP,ω.
weight function, λ(e(x)) = 1{e(x) ≤p}, in deﬁning the associated estimand. But, the eﬃciency
bound for this estimand given in (5.10) is a function of E[( ∂
∂eλ(e(X)))2] which diverges when
λ(e(x) is an indicator function so that variance VP,λ is unbounded and cannot be attained. In
contrast, such problems do not plague estimation if we focus on the subset ˆA, even though, as
noted in the discussion of our simple motivating example in Section 2, the sets A∗and ˆA can be
quite diﬀerent. Accordingly, we also restrict our attention to the subset ˆA when chracterizing
estimators for OSATE estimands.
Nonparametric Estimates for Regression Functions
The proposed estimators for the average treatment eﬀects rely on preliminary estimates of the
propensity score and the two conditional regression functions. For these conditional means,
various estimators have been proposed .
None of them exactly ﬁts the setting we consider here. Speciﬁcally, the previously developed
estimators do not allow for estimation of the set over which the treatment eﬀect is averaged. It
is possible to modify these estimators to allow for this complication, although doing so would
not be trivial. However, it is easier to use the generalized partial mean framework developed
by Newey and extended by Imbens and Ridder .
For simplicity, we use the same type of estimator for both conditional means, namely kernel
estimators, although it would be possible to use series estimators for the propensity score as
in Hirano, Imbens and Ridder . Let K : [−1, 1]L (→R be the kernel and b > 0 be the
bandwidth. Then the standard kernel estimators for the propensity score, the regression and
the variance functions are given by
˜µw,b(x) =
1{Wi = w} · Yi · K
1{Wi = w} · K
1{Wi = w} · (Yi −˜µw(Xi))2 · K
1{Wi = w} · K
respectively for w = 0, 1. To deal with technical boundary issues and to avoid trimming, it is
useful to modify this estimator close to the boundary of the covariate space, using the boundary correction suggested by Imbens and Ridder . The key idea behind this boundary
modiﬁcation is to modify the standard estimator for values of x that are close to the boundary,
relative to the bandwidth, by using a Taylor series expansion around the nearest point that
is suﬃciently far away from the boundary. Details for this modiﬁcation are presented in the
Appendix. The resulting estimators will be denoted by ˆem,b(x), and ˆµw,m,b(x), where m stands
for the degree of the Taylor series expansion.
Assumptions
Here we list three technical assumptions that will be used to control the convergence rate of
the nonparametric estimators. These are closely related to the assumptions used in Imbens and
Ridder . The ﬁrst assumption restricts the kernel.
Assumption 6.1 (Kernel)
(i) K : RL (→R,
(ii) K(u) = 0 for u /∈U, with U = [−1, 1]L,
(iii) K is r times continuously diﬀerentiable, with the r-th derivative bounded on the interior
(iv) K is a kernel of order s, so that 4
U K(u)du = 1 and 4
U uλK(u)du = 0 for all λ such that
0 < |λ| < s, for some s ≥1.
The second assumption requires suﬃcient smoothness of the distribution of (Y, W, X).
Assumption 6.2 (Distribution)
(i) (Y1, W1, X1), (Y2, W2, X2), . . ., are independent and identically distributed,
(ii) the support of Xi is X ⊂RL, X = 5L
l=1[xl, xl], xl < xl for all l = 1, . . ., L.
(iii), Xi is a random vector with probability density function fX(x), which is q times continuously diﬀerentiable on the interior of X, with the q-th derivative bounded,
(iv) supx∈X E[|Y |p|X = x] < ∞for some p.
(v) µw(x) = E[Y (w)|X = x] is q times continuously diﬀerentiable on the interior of X with the
qth derivative bounded for w = 0, 1,
w(x) = V[Y (w)|X = x] = E[(Y (w) −µw(X))2|X = x] is q times continuously diﬀerentiable on the interior of X with the qth derivative bounded for w = 0, 1,
(vii) e(x) = E[W|X = x] is q times continuously diﬀerentiable on the interior of X with the qth
derivative bounded,
(viii) e(X) has a continuous distribution on with the probability density function bounded
and continuously diﬀerentiable.
The third assumption puts restrictions on the bandwidth and the smoothness of the kernel and
the conditional mean functions.
Assumption 6.3 (Bandwidth and Smoothness)
(i) bN = N −δ,
(ii) p > 4,
(iii) s > max(L, (L + 2)/(1 −2/p)),
(iv) q ≥2s −1,
(v) r ≥s −1,
(vi) 1/(2s) < δ < min(1/(2L), (1−2/p)/(L + 2)).
The Optimally Selected Average Treatment Eﬀect
Deﬁne, for a given set A ⊂X, the estimator
(ˆµ1(Xi) −ˆµ0(Xi)) .
In this expression we drop the indexing of the kernel estimators ˆµ1(x) and ˆµ0(x) on the bandwidth bN and the degree of the Taylor series expansion in the boundary correction. The latter
will be assumed to be equal to s, the degree of the kernel, and subject to conditions given in
Assumption 6.3.
We ﬁrst characterize some preliminary results in order to deﬁne the estimator for the optimal
H. This involves ﬁrst estimating the propensity score, and then estimating the optimal
cutoﬀvalue α. First, deﬁne
i=1,...,N(ˆe(Xi)(1 −ˆe(Xi)))−1,
i=1,...,N(ˆe(Xi)(1 −ˆe(Xi)))−1.
By the support and smoothness conditions, ˆγ and ˆγ exist in large enough samples. For Γ =
[0, ∞), deﬁne the function ˆr : Γ (→R:
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
ˆe(Xi) · (1 −ˆe(Xi))·1
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
for γ > ˆγ, and 0 for 0 ≤γ ≤ˆγ. We are interested in the maximand of ˆr(γ). To deal with
nonuniqueness, we deﬁne the maximand as
-----ˆr(γ) ≤sup
and ˆγ = sup
Then let ˆα = 1/2 −
1 −4/ˆγ. Now we are in a position to deﬁne the estimator for the set A∗
ˆA = {x ∈X |ˆα ≤ˆe(x) ≤1 −ˆα} .
Theorem 6.1 (OSATE)
Suppose that Assumptions 3.1-3.2 and 6.1-6.3 hold. Then
N · (ˆτˆA −τS,ˆA)
---- X ∈A∗
Proof: See Appendix.
A key step in the proof is that
ˆτˆA −τS,ˆA
ˆτA∗−τS,A∗
which allows us to deal with—or, rather, avoid dealing with—the uncertainty in the estimated
Next we consider estimation of the OWATE, based on the weight function ω∗
H(x) = e(x) ·
(1 −e(x)). Deﬁne. for all functions ω : X (→R. the estimator
ω(Xi) · ˆτ(Xi)
The estimator we actually consider is ˆτˆω, where ˆω(x) = ˆe(x)·(1−ˆe(x)). We present two results
for this estimator. First, we consider the normalized diﬀerence between ˆτˆω and τS,ˆω. Second,
we consider the normalized diﬀerence between ˆτˆω and τP,ω∗
Theorem 6.2 (OWATE)
Suppose that Assumptions 3.1-3.2 and 6.1-6.3 hold. Then
N·(ˆτˆω−τS,ˆω)
E [e(X) · (1 −e(X))]2 · E
(e(X) · (1 −e(X)))2 ·
Proof: See Supplementary materials .
Theorem 6.3 (OWATE)
Suppose that Assumptions 3.1-3.2 and 6.1-6.3 hold. Then
N · (ˆτˆω −τP,ω∗
E [e(X) · (1 −e(X))]2 · E
(e(X) · (1 −e(X)))2 ·
E [e(X) · (1 −e(X))]2 · E
(e(X) · (1 −e(X)))2 ·
τ(X) −τP,ω∗
E [e(X) · (1 −e(X))]2 ·E
e(X) · (1 −e(X)) · (1 −2e(X))2(·τ(X) −τP,ω∗
Proof: See Supplementary materials .
The second term in the expression for VP,ω∗
H, equation (6.16), takes account of the uncertainty in estimating the population versus sample version, and corresponds to the normalized
variance of the diﬀerence τS,ω∗
H. The last term, equation (6.17), takes account of the uncertainty in estimating the weights, and corresponds to the normalized variance of the diﬀerence
τP,ˆω −τP,ω∗
Estimating the Asymptotic Variance
In this subsection, we propose consistent estimators for the asymptotic variances. Deﬁne, for
all sets A, the folloiwng estimators
ˆq(A) = NA
Theorem 6.4 Suppose that Assumptions 6.1-6.3 hold. Then
---- X ∈A∗
Proof: See Appendix.
i=1 ˆe(Xi) · (1 −ˆe(Xi))
(ˆe(Xi) · (1 −ˆe(Xi)))2·
ˆVP,ˆω = ˆVS,ˆω +
i=1 ˆe(Xi) · (1 −ˆe(Xi))
(ˆe(Xi) · (1 −ˆe(Xi)))2 · (ˆτ(Xi) −ˆτω)2
i=1 ˆe(Xi) · (1 −ˆe(Xi))
ˆe(Xi)·(1−ˆe(Xi))·(1 −2 · ˆe(Xi)))2·(ˆτ(Xi) −ˆτω)2 .
Theorem 6.5 Suppose that Assumptions 6.1-6.3 hold. Then
Proof: See Supplementary materials .
Theorem 6.6 Suppose that Assumptions 6.1-6.3 hold. Then
Proof: See Supplementary materials .
Some Illustrations Based on Real Data
In this section we apply the methods developed in this paper to data from a labor market
program. The data set we use was originally constructed by LaLonde and subsequently
used by, among others, Heckman and Hotz , Dehejia and Wahba and Smith
and Todd . The particular sample we use here is the one used by Dehejia and Wahba
 . The treatment of interest is a job training program. The trainees are drawn from an
experimental evaluation of this program. The control group is a sample drawn from the Panel
Study of Income Dynamics (PSID). The control and treatment group are very unbalanced.
Table 2 presents some summary statistics. The fourth and ﬁfth column present the averages for
each of the covariates separately for the control and treatment group. Consider, for example,
the average earnings in the year prior to the program, earn ’75. For the control group from the
PSID, this is 19.06, in thousands of dollars. For the treatment group, it is only 1.53. Given
that the standard deviation is 13.88, this is a very large diﬀerence of 1.26 standard deviations,
suggesting that simple covariance adjustments are unlikely to lead to credible inferences.
For these data, we compute and compare 9 diﬀerent estimands. The ﬁrst is the sample
average treatment eﬀect, τS (ATE). We then examine average treatment eﬀects derived over
three subsamples. In the ﬁrst, we drop all observations with an estimated propensity score
outside of the interval [0.01, 0.99] (ATE0.01). In the second, we drop all observations with an
estimated propensity score outside of the interval [0.10, 0.90] (ATE0.10). Finally, we calculate
the estimate of the OSATE with optimal cutoﬀpoint, α, using the results in Corollary 5.1.
The estimated optimal cutoﬀpoint is ˆα = 0.0660. For these calculations, we estimate the
propensity score using a logistic model with all nine covariates displayed in Table 2 entered
linearly. We also estimate the optimally weighted average treatment eﬀect (OWATE), with
weights ˆe(x) · (1 −ˆe(x)). The ﬁnal four estimates we consider are all versions of the average
treatment eﬀect for the treated.
We ﬁrst estimate the conventional average eﬀect for the
treated (ATT). We then form ATT estimates similiar to those in Dehejia and Wabha by
dropping observations which have estimated propensity scores greater than 0.99 (ATT0.01) and
0.90 (ATE0.10), respectively. Finally, we form estimates of the optimal subpopulation average
treatment eﬀect on the treated (OSATT) by dropping those observations with an estimated
propensity score greater than the optimal cutoﬀpoint of 0.73. For each of these cases, we
display, in Table 3, estimates of the associated estimands and their asymptotic standard errors.
Note that the standard errors are calculated separately for each estimator, implying that implicit
estimates of the conditional variance σ2 are diﬀerent. Hence, the optimal estimators need not
have smaller estimated asymptotic variances than the suboptimal ones.
For both the average treatment eﬀect and the average eﬀect for the treated estimands, it
makes a substantial diﬀerence to the standard errors of the estimators if we drop observations
with propensity scores close to their extreme values. For the average treatment eﬀects, the
gain in precision is huge. This is not surprising. There are many control observations whose
covariate values are so far from those for the treated that it makes little sense to attempt to
estimate the treatment eﬀect for those covariate values. Even for the average eﬀect for the
treated however, there is a substantial gain to discarding observations with outlying values for
the propensity score. This reduces the asymptotic standard error from 2.58 (with no sample
selection) to 1.82 (for the ﬁxed cutoﬀpoint of 0.10).
The number of observations that should be discarded according to the OSATE is substantial.
We report the number of observations dropped for this estimand in Table 4. Out of the original
2675 observations (2490 controls and 185 treated), only 312 are used in estimation (183 controls
and 129 treated). We also report in Table 4 the number of observations dropped in the various
categories for this criterion and for the suboptimal criteria based on the ﬁxed cutoﬀpoints 0.01
(ATE0.01) and 0.10 (ATE0.10), respectively, in the subsequent two panels of this table.
While not the primary focus of our analysis, we also note that the estimates of the various
estimands, themselves, vary substantially. This is not surprising, given that the deﬁnitions of
the underlying estimands are varying. They even diﬀer in sign. At the same time, we make two
observations about these estimates. First, the standard errors relative to the estimates tend
to be large for all of the alternative estimates, implying that the inferences drawn from them
would not diﬀer across the estimates. Second, the OSATE, OWATE and OSATT estimates
are all negative and tend to be closer in magnitude to one another compared to the other
estimators. One should not draw strong conclusions from either of this observations, given
that the theoretical results established in this paper are focused primarily on the precision of
alternative estimands.
Conclusion
Estimation of average treatment eﬀects under unconfoundedness or selection on observables
is often hampered by lack of overlap in the covariate distributions. This lack of overlap can
lead to imprecise estimates and can make commonly used estimators sensitive to the choice
of speciﬁcation. In such cases, researchers have often used informal methods for trimming the
sample. In this paper, we develop a systematic approach to addressing such lack of overlap
in which we sacriﬁce some external validity in exchange for improved internal validity. We
characterize optimal subsamples where the average treatment eﬀect can be estimated most
precisely, as well as optimally weighted average treatment eﬀects.
Under some simplifying
assumptions, the optimal rules depend solely on the propensity score. We ﬁnd that the precision
for average treatment eﬀects for the optimally selected samples can be much higher than for
the overall sample. In addition, we ﬁnd that a simple ad hoc selection rule based on discarding
all units with an estimated propensity score outside the interval [0.1, 0.9] can capture most of
the precision gains from selecting the sample optimally for a wide range of distributions.
Appendix A: The Kernel Estimator with Boundary Correction
In this appendix we present the details of the boundary correction we use for the kernel estimator.
This boundary correction was developed by Imbens and Ridder . We refer to this paper for more
details on the estimator. Let g(x) = E[Y |X = x] be the regression function of interest, and let fX(x)
be the probability density function of X, with the dimension of X equal to L.
Then we can write
g(x) = h1(x)/h2(x), where h1(x) = g(x) · fX(x), and h2(x) = fX(x). If we deﬁne Y1 = Y and Y2 = 1,
then we can write hk(x) = E[Yk|X = x] · fX(x), with the standard kernel estimator for hk(x) equal to
˜hk,b(x) = 1
bL Yki · K
Let ∂X be the boundary of X, and let XI be the “internal” region, more than bN away from the boundary
in all directions, XI = {x ∈X| minl=1,...,L infy∈∂X |yl −xl| ≥bN}. Then let rb(x) be the projection of x
onto the set XI: rb(x) = arg miny∈XI ∥x −y∥}. Let λ denote an L vector of nonnegative integers, with
l=1 λl, and λ! = %L
l=1 λl!. Deﬁne for a given, m −1 times diﬀerentiable function g : RL →R, a
point y ∈RL and an integer m, the m −1-th order polynomial function t : RL →R based on the Taylor
series expansion of order m −1 of g(·) around the point y:
t(x; g(·), y, m) =
∂xλ g(y) · (x −y)λ.
Now we deﬁne the boundary corrected estimators for hk(x):
ˆhk,m,b(x) =
& ˜hk,b(x)
x,˜hk,b, rb(x), m
elsewhere.
Finally the boundary corrected estimator for g(x) is
ˆgm,b(x) = ˆh1,m,b(x)/ˆh2,m,b(x).
Appendix B: Proofs
Proof of Theorem 5.1: The derivation of the eﬃciency bound follows that of Hahn and Hirano, Imbens
and Ridder . The density of (Y (0), Y (1), W, X) with respect to some σ-ﬁnite measure is
q(y(0), y(1), w, x)
f(y(0), y(1)|w,x) · f(w|x) · f(x)
f(y(0), y(1)|x) · f(w|x) · f(x)
f(y(0), y(1)|x) · e(x)w · (1 −e(x))1−w · f(x),
where in the second equality we used unconfoundedness. The density of the observed data (y, w, x) is
q(y, w,x) = f1(y|x)w · e(x)w · f0(y|x)1−w · (1 −e(x))1−w · f(x),
where fw(y|x) = fY (W )|X(y(w)|x) =
f(y(1 −w), y|x)dy(1 −w).
Consider a regular parametric submodel
indexed by θ, with density
q(y, w,x|θ) = f1(y|x, θ)w · e(x|θ)w · f0(y|x, θ)1−w · (1 −e(x|θ))1−w · f(x|θ),
which is equal to the true density q(y, w, x) for θ = θ0, or q(y, w,x) = q(y, w, x|θ0). The score for the parametric
model is given by
S(y, w,x|θ) = ∂
∂θ ln q(y, w, x|θ) = w · S1(y|x,θ) + (1 −w) · S0(y|x, θ) + Sx(x|θ) +
e(x|θ)(1 −e(x|θ)) · e′(x|θ)
S1(y|x,θ) = ∂
∂θ ln f1(y|x,θ),
and S0(y|x,θ) = ∂
∂θ ln f0(y|x,θ),
Sx(x|θ) = ∂
∂θ ln f(x|θ),
and e′(x|θ) = ∂
∂θ e(x|θ).
The tangent space of the model is the set of functions t(y, w,x) of the form
T = {w · S1(y, x) + (1 −w) · S0(y, x) + Sx(x) + a(x) · (w −e(x))}
where a(x) is any square-integrable measurable function of x and S1, S0, and Sx satisfy
S1(y, x)f1(y|x)dy = E [S1(Y (1), X)|X = x] = 0,
S0(y, x)f0(y|x)dy = E [S0(Y (0), X)|X = x] = 0,
Sx(x)f(x)dx = E [Sx(X)] = 0.
The parameter of interest is
λ(e(x))yf1(y|x)f(x)dydx −
λ(e(x))yf0(y|x)f(x)dydx
λ(e(x))f(x)dx
Thus, for the parametric submodel indexed by θ, the parameter of interest is
λ(e(x|θ))yf1(y|x, θ)f(x|θ)dydx −
λ(e(x|θ))yf0(y|x, θ)f(x|θ)dydx
λ(e(x|θ))f(x|θ)dx
We need to ﬁnd a function ψ(y, w, x) such that for all regular parametric submodels,
= E [ψ(Y, W, X) · S(Y, W, X| θ0)] .
First, we will calculate
∂θτP,λ(θ0). Let µλ =
λ(e(x))f(x)dx. Then,
∂θ τP,λ(θ0)
λ(e(x|θ0))y [S1(y|x,θ0)f1(y|x,θ0) −S0(y|x, θ0)f0(y|x,θ0)] f(x|θ0)dydx
λ(e(x|θ0)) [τ(x) −τP,λ] Sx(x|θ0)f(x|θ0)dx
λ′(e(x|θ0))e′(x|θ0) [τ(x) −τP,λ] f(x|θ0)dx
where λ′(e(x)) =
∂e(x)λ(e(x)).
The following choice for ψ(y, w, x) is shown in the supplementary materials
 to satisfy the condition:
w · λ(e(x))
µλ · e(x) (y −E[Y (1)|X = x]) −(1 −w) · λ(e(x))
µλ · (1 −e(x)) (y −E[Y (0)|X = x])
(τ(x) −τP,λ) + (w −e(x)) · λ′(e(x))
(τ(x) −τP,λ).
Then by Theorem 2 in Section 3.3 of Bickel, Klaassen, Ritov, and Wellner , the variance bound is the
expected square of the projection of ψ(Y, W, X) on the tangent space T . Since ψ(y, w, x) ∈T , the variance
E[ψ(Y, W, X)2]
# [λ(e(X))]2
(µλ)2 · e(X) · σ2
[λ(e(X))]2
(µλ)2 · (1 −e(X)) · σ2
#[λ(e(X)) + (W −e(X)) · λ′(e(X))]2
(τ(X) −τP,λ)2
# [λ(e(X))]2
(µλ)2 · e(X) · σ2
[λ(e(X))]2
(µλ)2 · (1 −e(X)) · σ2
#[λ(e(X))]2 + e(X)(1 −e(X)) · [λ′(e(X))]2
(τ(X) −τP,λ)2
For the special case of λ(e(x)) = e(x) the semiparametric eﬃciency bound is,
(µλ)2 · σ2
(µλ)2 · (1 −e(X)) · σ2
(µλ)2 (τ(X) −τP,λ)2
For the special case of λ(e(x)) = e(x)(1 −e(x)) the semiparametric eﬃciency bound is,
# e(X)(1 −e(X))2
#e(X)2(1 −e(X))
#e(X)2(1 −e(X))2 + e(X)(1 −e(X)) · (1 −2 · e(X))2
(τ(X) −τP,λ)2
which simpliﬁes to
# e(X)(1 −e(X))2
#e(X)2(1 −e(X))
#e(X)(1 −e(X))(3e(X)2 −3e(X) + 1)
(τ(X) −τP,λ)2
τ(Xi) · ω(Xi)
for nonnegative functions ω(·). For estimands of this type consider the criterion that encompasses Theorems 5.2
E[ω(X) · 1{X ∈A}]2 · E
ω(X)2 · 1{X ∈A} ·
We are interested in the choice of set A that minimizes (B.2) among the set of all closed subsets of X. The
following theorem provides the characterization.
Theorem B.1 (Weighted OSATE)
Let f ≤f(x) ≤f, and σ2(x) ≤σ2 for w = 0, 1 and all x ∈X, and let ω : X %→R+ be continuously diﬀerentiable.
The set A∗that minimizes (B.2) is equal to X if
E [ω(X)] ,
and otherwise,
****ω(x) ·
where γ is a positive solution to
. ***ω(X) ·
Proof: Deﬁne k(x) = σ2
1(x)/e(x) + σ2
0(x)/(1 −e(x)), ˜fX(x) = fX(x) · ω(x)/
z fX(z) · ω(z)dz, and ˜ω(x) =
z fX(z) · ω(z)dz, so that k(x) is bounded, bounded away from zero, and continuously diﬀerentiable on X.
X be a random vector with probability density function ˜fX(x) on X, and let ˜q(A) = Pr( ˜
X ∈A).15 Then
E [˜ω(X) · 1{X ∈A}] =
˜ω(x) · 1{x ∈A} · fX(x)dx
z ω(z) · fX(z)dz · 1{x ∈A} · fX(x)dx
15Note that
! ˜fX(x)dx = 1 by construction, so that ˜fX(x) is a valid probability density function.
1{x ∈A} · ˜fX(x)dx
and similarly,
˜ω(X)2 · 1{X ∈A} ·
˜ω(X)2 · 1{X ∈A} · k(X)
˜ω(x)2 · 1{x ∈A} · k(x) · fX(x)dx
z ω(z) · fX(z)dz · 1{x ∈A} · k(x) · fX(x)dx
˜ω(x) · 1{x ∈A} · k(x) · ˜fX(x)dx
X ∈A} · k( ˜
Because multiplying ω(x) by a constant does not change the value of the objective function in (B.2), we have
VS,ω(A) = VS,˜ω(A) =
E[˜ω(X) · 1{X ∈A}]2 · E
˜ω(X)2 · 1{X ∈A} ·
˜q2(A)) · E
X ∈A} · k( ˜
Thus the question now concerns the set A that minimizes (B.3).
We do the remainder of the proof of Theorem B.1 in two stages. First, suppose there is a closed set A such that
x ∈int(A), z /∈A, and ˜ω(z)·k(z) < ˜ω(x)·k(x). Then we will construct a closed set ˜A such that VS,˜ω(˜A) < VS,˜ω(A).
This implies that the optimal set has the form
A∗= {x ∈X|˜ω(x) · k(x) ≤γ},
for some γ. The second step consists of deriving the optimal value for γ.
For the ﬁrst step deﬁne a ball around x with volume ν,
Bν(x) = {z ∈X|∥z −x∥≤ν1/L2−1/Lπ−1/2Γ(L/2)1/L},
where Γ(a) =
xa−1 exp(−x)dx is the gamma function. Let Ac be the complement of A in X, and for sets A
and B let A/B = A ∩Bc. Let ν0 be small enough so that for ν ≤ν0 we have Bν/ ˜
fX (x)(x) ⊂A, Bν/ ˜
fX (z)(z) ⊂Ac
and ˜ω(z′) · k(z′) < ˜ω(x′) · k(x′) for all z′ ∈Bν/ ˜
fX (z)(z) and all x′ ∈Bν/ ˜
fX (x)(x). Also, because the volume of the
sets Bν/ ˜
fX (x)(x) and Bν/ ˜
fX (z)(z) is ν/ ˜fX(x) and ν/ ˜fX(z) respectively, it follows that
fX (x)(x)) −ν = o(ν),
fX (z)(z)) −ν = o(ν),
fX (x)(x)) −˜q(Bν/ ˜
fX (z)(z)) = o(ν).
Now we construct the set
fX (z)(z).
The objective function for this set is
VS,˜ω(˜Aν) =
˜q(A) + ˜q(Bν/ ˜
fX (z)(z)) −˜q(Bν/ ˜
fX (x)(x))
fX (z)(z)) · E
˜ω( ˜X) · k( ˜
*** ˜X ∈Bν/ ˜
fX (x)(x)) · E
˜q(A) + ˜q(Bν/ ˜
fX (z)(z)) −˜q(Bν/ ˜
fX (x)(x))
so that the diﬀerence relative to the value of the objective function for the original set A is
VS,˜ω(˜Aν)−VS,˜ω(A) =
˜ω( ˜X) · k( ˜
< 0 if ν ≤ν0, the diﬀerence VS,˜ω(˜Aν)−
VS,˜ω(A) is negative for small enough ν, which ﬁnishes the ﬁrst part of the proof.
The question now is to determine the optimal value for γ given that the optimal set has the form
Aγ = {x ∈X|˜ω(x) · k(x) ≤γ}.
Let Y = ˜ω( ˜X) · k( ˜
X), with probability density function fY (y). Then
γ)) = E[˜ω( ˜
= E[Y |Y < γ′]
Pr(Y < γ′) =
y · fY (y)dy
Denote the minimum and maximum value of the function k(x) over the set X by k and k. By assumption k > 0
and k < ∞.
Then limγ↓k →∞.
Because VS,˜ω(Ak) = VS,˜ω(X) which is ﬁnite by assumption, and because
VS,˜ω(Ak) is continuous as a function of γ, it follows that either VS,˜ω(Ak) is minimized at γ = k, or there is an
interior minimum where the ﬁrst order conditions are satisﬁed. Let γ′ denote the optimum.
The ﬁrst derivative with respect to γ is
∂γ VS,˜ω(Aγ) =
0 fY (y)dy
32 · γ · fY (γ) −2
0 fY (y)dy ·
0 yfY (y)dy · fY (γ)
0 fY (y)dy
This is zero if
fY (y)dy = 2
y · fY (y)dy,
y · fY (y)dy
= 2 · E[Y |Y < γ′]
= 2 · E[˜ω( ˜X) · k( ˜
Because ˜ω(x) = ω(x)/
z ω(z) · fX(z)dz,
γ′ = 2 · E
X) · k( ˜X) < γ′/
for γ = γ′ ·
ω(x) · fX(x)dx. This in turn implies
x ω(x) · k(x) · 1 {ω(x) · k(x) < γ} ˜fX(x)dx
x 1 {ω(x) · k(x) < γ} ˜fX(x)dx
x ω(x) · k(x) · 1 {ω(x) · k(x) < γ} ·
z ω(z)·fX (z)dz fX(x)dx
x 1 {ω(x) · k(x) < γ} ·
z ω(z)·fX (z)dz fX(x)dx
x ω2(x) · k(x) · 1 {ω(x) · k(x) < γ} fX(x)dx
x ω(x) · 1 {ω(x) · k(x) < γ} fX(x)dx
ω2(X) · k(X) · 1 {ω(X) · k(X) < γ}
E [ω(X) · 1 {ω(X) · k(X) < γ}]
ω2(X) · k(X) |ω(X) · k(X) < γ
E [ω(X) |ω(X) · k(X) < γ|]
Substituting back k(x) = σ2
1(x)/e(x) + σ2
0(x)/(1 −e(x)) this implies
. ***ω(X) ·
Proof of Theorem 5.2: Substituting ω(x) = 1 into Theorem B.1 implies that the optimal set A∗is equal to X
1 −e(x) ≤2 · E
and otherwise,
1 −e(x) ≤γ
where γ is a positive solution to
1 −e(X) < γ
Then deﬁne α = 1/2 −
1/4 −1/γ, so that γ = (α(1 −α))−1 and
α · (1 −α)
where α is a positive solution to
α · (1 −α) = 2 · E
α · (1 −α)
Proof of Theorem 5.3: Substituting ω(x) = e(x) and σ2
1(x) = σ2 into Theorem B.1 implies that the
optimal A∗is equal to X if
1 −e(x) ≤2 · E
# σ2 · e(X)
and otherwise,
1 −e(x) ≤γ
where γ is a positive solution to
1−e(X) < γ
1−e(X) < γ
Condition (B.4) is equivalent to
1 −e(x) ≤2 · E
E [e(X)] = 2 · E
**** W = 1
Let αt = 1 −σ2/γ, so that γ = σ2/(1 −α). Then (B.5) implies
1−e(X) |e(X) ≤αt
E [e(X) |e(X) ≤αt ]
**** W = 1, e(X) ≤αt
Proof of Theorem 5.4:
We are choosing ω : X →R to minimize
E[ω(X)]2 · E
Again let k(x) =
1−e(x), so that we minimize
E[ω(X)]2 · E
ω2(x)k(x)f(x)dx
ω(x)f(x)dx
If ˜ω(x) is a solution, than so is ˜ω(x)/c. Hence we can normalize ω(x) to satisfy
ω(x)f(x)dx = 1. Then the
problem is the minimization of
ω2(x)k(x)f(x)dx
ω(x)f(x)dx = 1.
The solution to this satisﬁes 0 = 2 · ω(x)k(x)f(x) −λf(x), so that for some constant c, ω(x) = c/k(x). Hence the
optimal weights ω∗(x) are proportional to 1/k(x), and since we do not care about the constant of proportionality
we can choose
e(x) · (1 −e(x))
(1 −e(x)) · σ2
1(x) + e(x) · σ2
Deﬁne for sets A ⊂X,
i=1 1{Xi ∈A} · (τ(Xi))
if NA > 0,
otherwise,
i=1 1{Xi ∈A} · (ˆµ1(Xi) −ˆµ0(Xi))
if NA > 0,
otherwise.
Lemma B.1 Suppose that supx∈X,w∈{0,1} |ˆµw(x) −µw(x)| = op(N −1/2+ε). Then for all A ⊂X,
ˆτA −τS,A = op
ˆτA −τS,A = 1{NA > 0} · 1
1{Xi ∈A} · (ˆτ(Xi) −τ(Xi)) ≤sup
|ˆτ(x) −τ(x)|
|ˆµ1(x) −µ1(x)| + sup
|ˆµ0(x) −µ0(x)| = op
x∈X(e(x)(1 −e(x)))−1,
(e(x)(1 −e(x)))−1,
i=1,...,N(e(Xi)(1 −e(Xi)))−1,
i=1,...,N(e(Xi)(1 −e(Xi)))−1,
For Γ = [0, ∞) deﬁne the functions r : Γ %→R and ˆr : Γ %→R:
e(x) · (1 −e(x)) ≤γ
e(x) · (1 −e(x)) ·1
e(x) · (1 −e(x)) ≤γ
for γ > γ, and 0 for 0 ≤γ ≤γ, and
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
ˆe(Xi) · (1 −ˆe(Xi)) · 1
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
for γ > ˆγ, and 0 for 0 ≤γ ≤ˆγ. Deﬁne
γ ∈Γ|r(γ) ≤sup
and γ∗= sup
γ ∈Γ|ˆr(γ) ≤sup
and ˆγ = sup
Lemma B.2 Suppose that supx∈X |e(x) −ˆe(x)| = op(N −α), and that infx∈X e(x) · (1 −e(x)) > 0. Suppose also
that if γ∗< γ, then Γ∗= {γ∗}, and
∂γ2 r(γ∗) < 0. Then for any δ > 0,
ˆγ −γ∗= op
Proof: Deﬁne
e(x) · (1 −e(x)) · 1
e(x) · (1 −e(x)) ≤γ
· fX(x)dx,
ˆe(Xi) · (1 −ˆe(Xi)) · 1
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
e(x) · (1 −e(x)) ≤γ
· fX(x)dx,
ˆe(Xi) · (1 −ˆe(Xi)) ≤γ
r(γ) = q2(γ)/p(γ),
ˆr(γ) = ˆq2(γ)/ˆp(γ),
The proof consists of three parts. First we show that
|ˆp(γ) −p(γ)| = Op
|ˆq(γ) −q(γ)| = Op
In the second step we show that this implies that
|ˆr(γ) −r(γ)| = Op
In the third step we show that this in turn implies for any δ > 0 that
ˆγ −γ∗= arg max
γ∈Γ ˆr(γ) −arg max
γ∈Γ r(γ) = op
First consider the convergence of ˆq(γ). Deﬁne k(x) =
e(x)·(1−e(x)) , and ˆk(x) =
ˆe(x)·(1−ˆe(x)) , so that q(γ) =
1 {k(x) ≤γ} · fX(x)dx, and ˆq(γ) =
. By the Triangle Inequality,
|ˆq(γ) −q(γ)| ≤sup
*****ˆq(γ) −1
1 {k(Xi) ≤γ}
1 {k(Xi) ≤γ} −q(γ)
As the diﬀerence between the distribution function and the empirical distribution function of k(X),
1 {k(Xi) ≤γ} −q(γ)
***** = Op
e.g., Billingsley . Next, consider the righthand side of (B.9):
1 {k(Xi) ≤γ}
−1 {k(Xi) ≤γ}
ˆk(Xi) ≤γ ≤k(Xi)
k(Xi) ≤γ ≤ˆk(Xi)
Deﬁne the indicator
|ˆk(x) −k(x)| ≥N −α
so that BN = op(1). Then (B.11) can be written as
ˆk(Xi) ≤γ ≤k(Xi)
k(Xi) ≤γ ≤ˆk(Xi)
+(1 −BN) · sup
ˆk(Xi) ≤γ ≤k(Xi)
k(Xi) ≤γ ≤ˆk(Xi)
Because BN is binary and op(1), it follows that (B.12) is op(N −α).
If BN = 0, then ˆk(x) ≤γ ≤k(x) or
k(x) ≤γ ≤ˆk(x) both imply |k(x) −γ| < N −α, so that (B.13) can be bounded by
(1 −BN) · 2 · sup
|k(Xi) −γ| < N −α;
|k(Xi) −γ| < N −α;
This is the sum of independent and identically distributed binary random variables with mean bounded by
C0 · N −α, implying by Markov’s inequality that (B.14) is Op(N −α):
N α · 2 · sup
|k(Xi) −γ| < N −α;
≤2N αC0 · N −α
= 2 · C0/C,
which can be made arbitrarily small by choosing C large. This ﬁnishes the proof that (B.9) is Op(N −α). and
thus that supγ∈Γ |ˆq(γ) −q(γ)| = Op(N −α). The proof for the claim that supγ∈Γ |ˆp(γ) −p(γ)| = Op(N −α) is
similar and is omitted.
Next consider (B.7). This follows directly from the convergence of ˆp(γ) and ˆq(γ) to p(γ) and q(γ) respectively.
Finally, consider (B.8). Let a = −∂2
∂γ2 r(γ∗) > 0. Let Γ0 = {γ ∈Γ| ∂2
∂γ2 r(γ) < −a/2}, so that γ∗∈int(Γ0), and
let ΓN = {γ ∈Γ||γ −γ∗| < N −α/2}. For N > N0, ΓN ⊂Γ0. Let r0 = supγ∈Γ/Γ0 r(γ). Then r0 < r(γ∗) =
supγ∈Γ r(γ). Deﬁne the two events
AN = 1{inf
γ∈Γ |ˆr(γ) −r(γ)| > |r0 −r(γ∗)|/2},
BN = 1{inf
γ∈Γ |ˆr(γ) −r(γ)| > (a/8)N −α}.
For N > N1, BN implies AN. Since Pr(BN = 1) →0, it follows that BN = op(N −α).
Let N > max(N0, N1), and consider γ ∈Γ0/ΓN. We will show that for such γ, |r(γ∗) −r(γ)| = r(γ∗) −r(γ) >
(a/4) · N −α. Suppose γ > γ∗. First note that for c ∈Γ0, c > γ∗, it follows that
∂γ r(c) = ∂2
∂γ2 r(˜c) · (c −γ∗) < −(a/2) · (c −γ∗).
Hence for γ > γ∗,
r(γ) = r(γ∗) +
γ∗(a/2) · (c −γ∗)dc
= r(γ∗) −a
4(γ −γ∗)2.
Because γ /∈ΓN, |γ −γ∗| ≥N −α/2 so that
r(γ) −r(γ∗) < −a
|r(γ) −r(γ∗)| = r(γ∗) −r(γ) > |a/4| · N −α.
Therefore, if BN = 0, it must be that γ /∈ΓN implies
ˆr(γ) ≤r(γ) + (a/8)N −α < r(γ∗) −(a/8)N −α ≤ˆr(γ∗),
and thus ˆγ ∈ΓN. Finally, write
ˆγ −γ∗= BN · (ˆγ −γ∗) + (1 −BN) · (ˆγ −γ∗) .
The ﬁrst term on the righthand side is op(N −α/2) because BN is binary and op(1), and the second term is
op(N −α/2+δ) because if BN = 0, then |ˆγ −γ∗| < N −α/2. Thus (B.15) is op(N −α/2+δ). □
ˆe(x) · (1 −ˆe(x)) −
e(x) · (1 −e(x))
**** ≤N −1/2+ϵ, |ˆγ −γ| ≤N −1/4+ε/2+δ
˜τ(A) = (1 −AN) · ˆτ(A).
Lemma B.3 Suppose that for some ε, δ > 0 supx∈X,w∈{0,1} |ˆµw(x) −µw(x)| = op(N −1/2+ϵ), supx∈X |ˆe(x) −
e(x)| = op(N −1/2+ϵ), and that infx∈X e(x) · (1 −e(x)) > 0. Then, for all sets A ⊂X,
˜τ(A) −ˆτ(A) = op
First we show that AN = op(1).
By the assumptions and Lemma B.2 it follows that ˆγ −γ =
N −1/2+ε/2+δ.
Pr(AN = 1)
ˆe(x) · (1 −ˆe(x)) −
e(x) · (1 −e(x))
**** > N −1/2+ε
|ˆγ −γ| > N −1/4+ε/2+δ.
N 1/2 · |˜τ(A) −ˆτ(A)| > C
N 1/2 · AN · |ˆτ(A)| > C
≤Pr(AN = 1) = o(1).
so that ˜τ(A) −ˆτ(A) = op(N −1/2). □
Lemma B.4 Suppose that supx∈X |ˆe(x)−e(x)| = op(N −1/2+ε), and that infx∈X e(x)·(1−e(x)) > 0. Then for any
δ > 0, (i),
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
Proof: We prove (i). The other claims follow the same argument. Deﬁne BNi = 1{|k(Xi)−γ| < 2N −1/4+ε/2+δ}.
Then for all N the random variables BNi and BNj are independent and identically distributed with mean bounded
by C · N −1/4+ε/2+δ. Hence 7N
i=1 BNi/N is Op(N −1/4+ε/2+δ). Now
' NˆA −NA∗
+ (1 −AN) ·
' NˆA −NA∗
N 1/2 · AN ·
' NˆA −NA∗
≤Pr(AN > 0) = o(1),
it follows that the ﬁrst term of the right hand side of (B.16) is op(N −1/2). Next, consider the second term of the
right hand side of (B.16):
**** = 1 −AN
1{ˆk(Xi) ≤ˆγ} −1{k(Xi) ≤γ}
***1{ˆk(Xi) ≤ˆγ} −1{k(Xi) ≤γ}
1{ˆk(Xi) ≤ˆγ, γ < k(Xi)} + 1{k(Xi) ≤γ, ˆγ < ˆk(Xi)}
(1 −AN) · BNi · 1{ˆk(Xi) ≤ˆγ, γ < k(Xi)}
(1 −AN) · (1 −BNi) · 1{ˆk(Xi) ≤ˆγ, γ < k(Xi)}
(1 −AN) · BNi · 1{k(Xi) ≤γ, ˆγ < ˆk(Xi)}
(1 −AN) · (1 −BNi) · 1{k(Xi) ≤γ, ˆγ < ˆk(Xi)}.
AN = 0 implies |ˆk(x) −k(x)| < N −1/2+ε and |γ −ˆγ| < N −1/4+ε/2+δ, so that ˆk(Xi) ≤ˆγ and γ < k(Xi) combined
with AN = 0 implies that γ < k(x) < γ + 2N −1/4+ε/2+δ and thus BNi = 1. Hence (B.18) is equal to zero, and
similarly (B.20) is equal to zero. Thus
(1 −AN) · BNi · 1{ˆk(Xi) ≤ˆγ, γ < k(Xi)}
(1 −AN) · BNi · 1{k(Xi) ≤γ, ˆγ < ˆk(Xi)}
BNi/N = Op(1) · Op
N −1/4+ε/2+δ.
N −1/4+ε/2+δ.
Combined with the fact that the ﬁrst term of the right hand side of (B.16) is op(N −1/2), this implies that
(NˆA −NA∗)/NA∗= Op
N −1/4+ε/2+δ.
The other parts of the Lemma follow by similar arguments. For that reason their proofs are omitted. □
Lemma B.5 Suppose that supx∈X |e(x)−ˆe(x)| = op(N −1/2+ε) for some ε < 1/6, and that infx∈X e(x)·(1−e(x)) >
0. Suppose also that if γ∗< γ, then Γ∗= {γ∗}, and
∂γ2 r(γ∗) < 0. Then
ˆτ(ˆA) −τ(ˆA)
−(ˆτ(A∗) −τ(A∗)) = op
Proof: Note that by Lemma B.2 for any δ > 0 we have ˆγ −γ∗= op(N −1/4+ε/2+δ). Because ε < 1/6, it follows
that −3/4 + ε(3/2) < −1/2, and so we can choose δ so that −3/4 + ε(3/2) + δ < −1/2. Next,
ˆτ(ˆA) −τ(ˆA)
−(ˆτ(A∗) −τ(A∗))
ˆτ(ˆA) −τ(ˆA)
−(ˆτ(A∗) −τ(A∗)) + N ∗
ˆτ(ˆA) −τ(ˆA)
ˆτ(ˆA) −˜τ(ˆA)
ˆτ(ˆA) −˜τ(ˆA)
−(ˆτ(A∗) −˜τ(A∗))
ˆτ(ˆA) −τ(ˆA)
˜τ(ˆA) −τ(ˆA)
−(˜τ(A∗) −τ(A∗)) .
By Lemma B.3 (B.21) and (B.23) are op(N −1/2). By Lemma B.3 the second factor in (B.22) is op(N −1/2), and
by Lemma B.4(i) the ﬁrst factor is Op(N −1/4+ε/2+δ), hence the product (B.22) is op(N −1/2). Next, consider
(B.24). The ﬁrst factor is Op(N −1/4+ε/2+δ) by Lemma B.4(i). The second factor is op(N −1/2+ε) by Lemma B.1,
so the product is op(N −3/4+ε(3/2)+δ) = op(N −1/2) because δ can be chosen to satisfy δ + ε(3/2) < 1/4.
Finally, consider (B.25):
˜τ(ˆA) −τ(ˆA)
−(˜τ(A∗) −τ(A∗))
˜τ(ˆA) −τ(ˆA)
−(˜τ(A∗) −τ(A∗))
+(1 −AN) ·
˜τ(ˆA) −τ(ˆA)
−(˜τ(A∗) −τ(A∗))
Because AN is binary and op(1), it follows that AN = op(N −1/2), and thus (B.26) is op(N −1/2). Next, (B.27) is
1{Xi ∈ˆA} · (ˆτ(Xi) −τ(Xi)) −
1{Xi ∈A∗} · (ˆτ(Xi) −τ(Xi))
1{Xi ∈ˆA/A∗} · (ˆτ(Xi) −τ(Xi))
1{Xi ∈A∗/ˆA} · (ˆτ(Xi) −τ(Xi)) .
Consider (B.28):
1{Xi ∈ˆA/A∗} · (ˆτ(Xi) −τ(Xi))
1{Xi ∈ˆA/A∗} · |ˆτ(Xi) −τ(Xi)|
1{Xi ∈ˆA/A∗} · sup
|ˆτ(x) −τ(x)|
|ˆτ(x) −τ(x)| = Op
N −1/4+ε/2+δ.
N −3/4+ε(3/2)+δ.
(B.29) is op(N −1/2) by the same argument. □
Lemma B.6 Suppose that for some 0 < ε < 1/4, supx∈X,w∈{0,1} |ˆµw(x)−µw(x)| = op(N −1/2+ε) and supx∈X |ˆe(x)−
e(x)| = op(N −1/2+ε), and that infx∈X e(x) · (1 −e(x)) > 0.
Then for λ(x) = e(x) · (1 −e(x)) and ˆλ(x) =
ˆe(x) · (1 −ˆe(x)),
(ˆτˆλ −τˆλ) −(ˆτλ −τλ) = op
Proof: By the rate of the convergence of ˆe(x) to e(x), it follows that supx∈X |ˆλ(x) −λ(x)| = op
in combination with the positive lower bound on λ(x) it follows that
By the rate of the convergence of ˆµw(x) to µw(x), it follows that supx∈X |ˆτ(x) −τ(x)| = op
|(ˆτˆλ −τˆλ) −(ˆτλ −τλ)| =
i=1 ˆλ(Xi) · (ˆτ(Xi) −τ(Xi))
i=1 ˆλ(Xi)
i=1 λ(Xi) · (ˆτ(Xi) −τ(Xi))
i=1 ˆλ(Xi) · (ˆτ(Xi) −τ(Xi))
i=1 ˆλ(Xi)
i=1 ˆλ(Xi) · (ˆτ(Xi) −τ(Xi))
i=1 ˆλ(Xi) · (ˆτ(Xi) −τ(Xi))
i=1 λ(Xi) · (ˆτ(Xi) −τ(Xi))
ˆλ(Xi) · (ˆτ(Xi) −τ(Xi))
ˆλ(Xi) −λ(Xi)
· (ˆτ(Xi) −τ(Xi))
λ(x) · sup
|ˆτ(x) −τ(x)| ·
x∈X λ(x)−1 · sup
***ˆλ(x) −λ(x)
|ˆτ(x) −τ(x)|
θ = E [λ(X) · (µ1(X) −µ0(X))] ,
λ(Xi) · (µ1(Xi) −µ0(Xi)) ,
λ(Xi) · (ˆµ1(Xi) −ˆµ0(Xi)) .
φ(y, w, x) =
e(x) −µ1(x)
' y · (1 −w)
· (w −e(x))
Lemma B.7 (Asymptotic Linearity)
Suppose Assumptions 3.1-3.2 and 6.1-6.3 hold. Then
φ(Yi, Wi, Xi) + op(1).
Proof: We apply Theorem 4.1 in Imbens and Ridder . Deﬁne the vector ˜Y as
Y · (1 −W)
and deﬁne the functions ω : X →R, g : X →R3, and m : R3 →R:
ω(x) = λ(x),
g(x) = E[ ˜Y |X = x] =
E[Y · W|X = x]
E[Y · (1 −W)|X = x]
E[W|X = x]
µ1(x) · e(x)
µ0(x) · (1 −e(x))
m(z) = z1/z3 −z2/(1 −z3).
θ = E[ω(X) · m(g(X))],
ω(Xi) · m(g(Xi)),
ω(Xi) · m(ˆg(Xi)).
Assumptions 3.1-3.2 and 6.1-6.3 imply Assumptions 3.2, 3.3, 4.1, and 4.2 in Imbens and Ridder . Then by
Theorem 4.1 in Imbens and Ridder we have
ω(Xi) · ∂m
∂g′ (g(X))
ω(Xi) (m(g(Xi)) −θ) + op(1).
N · (θ −θS) =
ω(Xi) · ∂m
∂g′ (g(X))
∂g (g(X)) =
−1/(1 −g3(X))
3(X) −g2(X)/(1 −g3(X))2
−1/(1 −e(X))
−µ1(X)/e(X)2 −µ0(X)/(1 −e(X))2
˜Y −g(X) =
Y · W −g1(X)
Y · (1 −W) −g2(X)
Y · W −µ1(X) · e(X)
Y · (1 −W) −µ0(X) · (1 −e(X))
'' Yi · Wi
e(Xi) −µ1(Xi)
' Yi · (1 −Wi)
· (Wi −e(Xi))
Lemma B.8 (Asymptotic Normality)
Suppose Assumptions 3.1-3.2 and 6.1-6.3 hold. Then
N · (ˆτ(A∗) −τ(A∗))
**** X ∈A∗
Proof: By Lemma B.7, independent sampling, and because the second moment of φ(Y, W, X) exists, it follows
N · (ˆθ −θS) →N
φ(Y, W, X)213
In addition, N ∗
p→q(A∗), so that
N · (ˆτ(A∗) −τ(A∗)) =
q2(A∗) · E
φ(Y, W, X)21(
@ '' Y · W
e(X) −µ1(X)
' Y · (1 −W)
· (W −e(X))
(2***** X ∈A∗
e(X) −µ1(X)
(2***** X ∈A∗
e(X) −µ1(X)
' Y · (1 −W)
(**** X ∈A∗
e(X) −µ1(X)
· (W −e(X))
**** X ∈A∗
@' Y · (1 −W)
(2***** X ∈A∗
# ' Y · (1 −W)
· (W −e(X))
**** X ∈A∗
· (W −e(X))
(2***** X ∈A∗
1(X) · 1 −e(X)
**** X ∈A∗
+E [2 · µ0(X) · µ1(X)| X ∈A∗]
2 · µ0(X) · µ1(X) + 2 · µ2
1(X) · 1 −e(X)
**** X ∈A∗
1 −e(X) + µ2
**** X ∈A∗
1 −e(X) + 2 · µ0(X) · µ1(X)
**** X ∈A∗
2 · µ0(X) · µ1(X) + µ2
1(X) · 1 −e(X)
**** X ∈A∗
**** X ∈A∗
it follows that
φ(Y, W, X)21
e(X) −µ1(X)
' Y · (1 −W)
· (W −e(X))
(2***** X ∈A∗
= q(A∗) · E
**** X ∈A∗
and the result in the Lemma follows. □
Proof of 6.1: This follows directly from Lemmas B.5 and B.8 □
The proofs of Theorems 6.2-6.6 are omitted here in the interest of space. They are available on the web .