TransBTS: Multimodal Brain Tumor
Segmentation Using Transformer
Wenxuan Wang1, Chen Chen2, Meng Ding3, Hong Yu1, Sen Zha1, Jiangyun
1 School of Automation and Electrical Engineering, University of Science and
Technology Beijing, China, ,
 , , 
2 Center for Research in Computer Vision, University of Central Florida, USA,
 
3 Scoop Medical, Houston, TX, USA, 
† Corresponding author: Jiangyun Li
Abstract. Transformer, which can beneﬁt from global (long-range) information modeling using self-attention mechanisms, has been successful in natural language processing and 2D image classiﬁcation recently.
However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper,
we for the ﬁrst time exploit Transformer in 3D CNN for MRI Brain
Tumor Segmentation and propose a novel network named TransBTS
based on the encoder-decoder structure. To capture the local 3D context information, the encoder ﬁrst utilizes 3D CNN to extract the volumetric spatial feature maps. Meanwhile, the feature maps are reformed
elaborately for tokens that are fed into Transformer for global feature
modeling. The decoder leverages the features embedded by Transformer
and performs progressive upsampling to predict the detailed segmentation map. Extensive experimental results on both BraTS 2019 and
2020 datasets show that TransBTS achieves comparable or higher results than previous state-of-the-art 3D methods for brain tumor segmentation on 3D MRI scans. The source code is available at https:
//github.com/Wenxuan-1119/TransBTS.
Keywords: Segmentation · Brain Tumor · MRI · Transformer · 3D CNN
Introduction
Gliomas are the most common malignant brain tumors with diﬀerent levels of
aggressiveness. Automated and accurate segmentation of these malignancies on
magnetic resonance imaging (MRI) is of vital importance for clinical diagnosis.
Convolutional Neural Networks (CNN) have achieved great success in various vision tasks such as classiﬁcation, segmentation and object detection. Fully
Convolutional Networks (FCN) realize end-to-end semantic segmentation
for the ﬁrst time with impressive results. U-Net uses a symmetric encoderdecoder structure with skip-connections to improve detail retention, becoming
 
Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li
the mainstream architecture for medical image segmentation. Many U-Net variants such as U-Net++ and Res-UNet further improve the performance
for image segmentation. Although CNN-based methods have excellent representation ability, it is diﬃcult to build an explicit long-distance dependence due
to limited receptive ﬁelds of convolution kernels. This limitation of convolution
operation raises challenges to learn global semantic information which is critical
for dense prediction tasks like segmentation.
Inspired by the attention mechanism in natural language processing, existing research overcomes this limitation by fusing the attention mechanism with
CNN models. Non-local neural networks design a plug-and-play non-local
operator based on the self-attention mechanism, which can capture the longdistance dependence in the feature map but suﬀers from the high memory and
computation cost. Schlemper et al. propose an attention gate model, which
can be integrated into standard CNN models with minimal computational overhead while increasing the model sensitivity and prediction accuracy. On the
other hand, Transformer is designed to model long-range dependencies in
sequence-to-sequence tasks and capture the relations between arbitrary positions
in the sequence. This architecture is proposed based solely on self-attention, dispensing with convolutions entirely. Unlike previous CNN-based methods, Transformer is not only powerful in modeling global context, but also can achieve
excellent results on downstream tasks in the case of large-scale pre-training.
Recently, Transformer-based frameworks have also reached state-of-the-art
performance on various computer vision tasks. Vision Transformer (ViT) 
splits the image into patches and models the correlation between these patches
as sequences with Transformer, achieving satisfactory results on image classiﬁcation. DeiT further introduces a knowledge distillation method for training
Transformer. DETR treats object detection as a set prediction task with the
help of Transformer. TransUNet is a concurrent work which employs ViT
for medical image segmentation. We will elaborate the diﬀerences between our
approach and TransUNet in Sec. 2.4.
Research Motivation. The success of Transformer has been witnessed
mostly on image classiﬁcation. For dense prediction tasks such as segmentation, both local and global (or long-range) information is important. However,
as pointed out by , local structures are ignored when directly splitting images into patches as tokens for Transformer. Moreover, for medical volumetric
data (e.g. 3D MRI scans) which is beyond 2D, local feature modeling among
continuous slices (i.e. depth dimension) is also critical for volumetric segmentation. We are therefore inspired to ask: How to design a neural network that
can eﬀectively model local and global features in spatial and depth dimensions of
volumetric data by leveraging the highly expressive Transformer?
In this paper, we present the ﬁrst attempt to exploit Transformer in 3D CNN
for 3D MRI Brain Tumor Segmentation (TransBTS). The proposed TransBTS
builds upon the encoder-decoder structure. The network encoder ﬁrst utilizes
3D CNN to extract the volumetric spatial features and downsample the input
3D images at the same time, resulting in compact volumetric feature maps that
TransBTS: Multimodal Brain Tumor Segmentation Using Transformer
eﬀectively captures the local 3D context information. Then each volume is reshaped into a vector (i.e. token) and fed into Transformer for global feature
modeling. The 3D CNN decoder takes the feature embedding from Transformer
and performs progressive upsampling to predict the full resolution segmentation map. Experiments on BraTS 2019 and 2020 datasets show that TransBTS
achieves comparable or higher results than previous state-of-the-art 3D methods
for brain tumor segmentation on 3D MRI scans. We also conduct comprehensive ablation study to shed light on architecture engineering of incorporating
Transformer in 3D CNN to unleash the power of both architectures. We hope
TransBTS can serve as a strong 3D baseline to facilitate future research on
volumetric segmentation.
Overall Architecture of TransBTS
An overview of the proposed TransBTS is presented in Fig. 1. Concretely, given
an input MRI scan X ∈RC×H×W ×D with a spatial resolution of H × W, depth
dimension of D (# of slices) and C channels (# of modalities), we ﬁrst utilize
3D CNN to generate compact feature maps capturing spatial and depth information, and then leverage the Transformer encoder to model the long distance
dependency in a global space. After that, we repeatedly stack the upsampling
and convolutional layers to gradually produce a high-resolution segmentation
result. The network details of TransBTS are provided in the Appendix. Next,
we will describe the components of TransBTS in detail.
Transformer Layer
Transformer Layer
Linear Projection
Downsample: Strided Convolution
Upsample: Deconvolution
(C, H, W, D)
(16, H, W, D)
(32, H/2, W/2, D/2)
(64, H/4, W/4, D/4)
(128, H/8, W/8, D/8)
(128, H/8, W/8, D/8)
(d=512, N)
Linear Projection
3×3×3 Convolution à Reshape
Position Embedding
Layer Norm
Layer Norm
Multi-Head
Feature Concatenation
Skip-Connection
Feature Mapping
Feature Mapping
Reshape à 3×3×3 Convolution
Element-wise Addition
Fig. 1. Overall architecture of the proposed TransBTS.
Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li
Network Encoder
As the computational complexity of Transformer is quadratic with respect to
the number of tokens (i.e. sequence length), directly ﬂattening the input image to a sequence as the Transformer input is impractical. Therefore, ViT 
splits an image into ﬁxed-size (16 × 16) patches and then reshapes each patch
into a token, reducing the sequence length to 162. For 3D volumetric data, the
straightforward tokenization, following ViT, would be splitting the data into 3D
patches. However, this simple strategy makes Transformer unable to model the
image local context information across spatial and depth dimensions for volumetric segmentation. To address this challenge, our solution is to stack the 3 × 3 × 3
convolution blocks with downsamping (strided convolution with stride=2) to
gradually encode input images into low-resolution/high-level feature representation F ∈RK× H
8 (K = 128), which is 1/8 of input dimensions of H, W
and D (overall stride (OS)=8). In this way, rich local 3D context features are effectively embedded in F. Then, F is fed into the Transformer encoder to further
learn long-range correlations with a global receptive ﬁeld.
Feature Embedding of Transformer Encoder. Given the feature map F,
to ensure a comprehensive representation of each volume, a linear projection (a
3×3×3 convolutional layer) is used to increase the channel dimension from K =
128 to d = 512. The Transformer layer expects a sequence as input. Therefore, we
collapse the spatial and depth dimensions into one dimension, resulting in a d×N
8 ) feature map f, which can be also regarded as N d-dimensional
tokens. To encode the location information which is vital in segmentation task,
we introduce the learnable position embeddings and fuse them with the feature
map f by direct addition, creating the feature embeddings as follows:
z0 = f + PE = W × F + PE
where W is the linear projection operation, PE ∈Rd×N denotes the position
embeddings, and z0 ∈Rd×N refers to the feature embeddings.
Transformer Layers. The Transformer encoder is composed of L Transformer
layers, each of them has a standard architecture, which consists of a Multi-Head
Attention (MHA) block and a Feed Forward Network (FFN). The output of the
ℓ-th (ℓ∈[1, 2, ..., L]) Transformer layer can be calculated by:
ℓ= MHA(LN(zℓ−1)) + zℓ−1
zℓ= FFN(LN(z
where LN(∗) denotes the layer normalization and zℓis the output of ℓ-th Transformer layer.
Network Decoder
In order to generate the segmentation results in the original 3D image space
(H × W × D), we introduce a 3D CNN decoder to perform feature upsampling
and pixel-level segmentation (see the right part of Fig. 1).
TransBTS: Multimodal Brain Tumor Segmentation Using Transformer
Feature Mapping. To ﬁt the input dimension of 3D CNN decoder, we ﬁrst
design a feature mapping module to project the sequence data back to a standard
4D feature map. Speciﬁcally, the output sequence of Transformer zL ∈Rd×N is
ﬁrst reshaped to d× H
8 . In order to reduce the computational complexity
of decoder, a convolution block is employed to reduce the channel dimension from
d to K. Through these operations, the feature map Z ∈RK× H
has the same dimension as F in the feature encoding part, is obtained.
Progressive Feature Upsampling. After the feature mapping, cascaded upsampling operations and convolution blocks are applied to Z to gradually recover
a full resolution segmentation result R ∈RH×W ×D. Moreover, skip-connections
are employed to fuse the encoder features with the decoder counterparts by
concatenation for ﬁner segmentation masks with richer spatial details.
Discussion
A very recent work TransUNet also employs Transformer for medical image segmentation. Here we want to highlight a few key distinctions between our
TransBTS and TransUNet. (1) TransUNet is a 2D network that processes each
3D medical image in a slice-by-slice manner. However, our TransBTS is based
on 3D CNN and processes all the image slices at a time, allowing the exploitation of better representations of continuous information between slices. In other
words, TransUNet only focuses on the spatial correlation between tokenized image patches, but our method can model the long-range dependencies in both
slice/depth dimension and spatial dimension simultaneously for volumetric segmentation. (2) As TransUNet adopts the ViT structure, it relies on pre-trained
ViT models on large-scale image datasets. In contrast, our TransBTS has a ﬂexible network design and is trained from scratch on task-speciﬁc dataset without
the dependence on pre-trained weights.
Experiments
Data and Evaluation Metric. The ﬁrst 3D MRI dataset used in the experiments is provided by the Brain Tumor Segmentation (BraTS) 2019 challenge
 . It contains 335 cases of patients for training and 125 cases for validation. Each sample is composed of four modalities of brain MRI scans, namely
native T1-weighted (T1), post-contrast T1-weighted (T1ce), T2-weighted (T2)
and Fluid Attenuated Inversion Recovery (FLAIR). Each modality has a volume
of 240×240×155 which has been aligned into the same space. The labels contain
4 classes: background (label 0), necrotic and non-enhancing tumor (label 1), peritumoral edema (label 2) and GD-enhancing tumor (label 4). The segmentation
accuracy is measured by the Dice score and the Hausdorﬀdistance (95%) metrics
for enhancing tumor region (ET, label 1), regions of the tumor core (TC, labels
1 and 4), and the whole tumor region (WT, labels 1,2 and 4). The second 3D
MRI dataset is provided by the Brain Tumor Segmentation Challenge (BraTS)
2020 . It consists of 369 cases for training, 125 cases for validation and
Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li
166 cases for testing. Except for the number of samples in the dataset, the other
information about these two datasets are the same.
Implementation Details. The proposed TransBTS is implemented in Pytorch
and trained with 8 NVIDIA Titan RTX GPUs (each has 24GB memory) for 8000
epochs from scratch using a batch size of 16. We adopt the Adam optimizer to
train the model. The initial learning rate is set to 0.0004 with a poly learning
rate strategy, in which the initial rate decays by each iteration with power 0.9.
The following data augmentation techniques are applied: (1) random cropping
the data from 240 × 240 × 155 to 128 × 128 × 128 voxels; (2) random mirror
ﬂipping across the axial, coronal and sagittal planes by a probability of 0.5;
(3) random intensity shift between [-0.1, 0.1] and scale between [0.9, 1.1]. The
softmax Dice loss is employed to train the network and L2 Norm is also applied
for model regularization with a weight decay rate of 10−5. In the testing phase,
we utilize Test Time Augmentation (TTA) to further improve the performance
of our proposed TransBTS.
Table 1. Comparison on BraTS 2019 validation set.
Dice Score (%) ↑
HausdorﬀDist. (mm) ↓
3D U-Net 
70.86 87.38 72.48 5.062
V-Net 
73.89 88.73 76.56 6.131
KiU-Net 
73.21 87.60 73.92 6.323
Attention U-Net 
75.96 88.81 77.20 5.202
Wang et al. 
73.70 89.40 80.70 5.994
Li et al. 
77.10 88.60 81.30 6.033
Frey et al. 
Myronenko et al. 
83.4 3.921
TransBTS w/o TTA 78.36 88.89 81.41 5.908
TransBTS w/ TTA
78.93 90.00 81.94 3.736 5.644
Table 2. Comparison on BraTS 2020 validation set.
Dice Score (%) ↑
HausdorﬀDist. (mm) ↓
3D U-Net 
50.983 13.366
Basic V-Net 
47.702 20.407
Deeper V-Net 
43.518 14.499
Residual 3D U-Net
37.422 12.337
TransBTS w/o TTA 78.50
81.36 16.716 6.469
TransBTS w/ TTA
78.73 90.09 81.73 17.947 4.964
Main Results
BraTS 2019. We ﬁrst conduct ﬁve-fold cross-validation evaluation on the training set – a conventional setting followed by many existing works. Our TransBTS
achieves average Dice scores of 78.69%, 90.98%, 82.85% respectively for ET, WT
and TC. We also conduct experiments on the BraTS 2019 validation set and
TransBTS: Multimodal Brain Tumor Segmentation Using Transformer
Attention U-Net
TransBTS (Ours)
Ground Truth
Fig. 2. The visual comparison of MRI brain tumor segmentation results.
compare TransBTS with state-of-the-art (SOTA) 3D approaches. The quantitative results are presented in Table 1. TransBTS achieves the Dice scores of
78.93%, 90.00%, 81.94% on ET, WT, TC, respectively, which are comparable
or higher results than previous SOTA 3D methods presented in Table 1. In
terms of Hausdorﬀdistance metric, a considerable improvement has also been
achieved for segmentation. Compared with 3D U-Net , TransBTS shows great
superiority in both metrics with signiﬁcant improvements. This clearly reveals
the beneﬁt of leveraging Transformer for modeling the global relationships. For
qualitative analysis, we also show a visual comparison of the brain tumor
segmentation results of various methods including 3D U-Net , V-Net , Attention U-Net and our TransBTS in Fig. 2. Since the ground truth for the
validation set is not available, we conduct ﬁve-fold cross-validation evaluation
on the training set for all methods. It is evident from Fig. 2 that TransBTS can
describe brain tumors more accurately and generate much better segmentation
masks by modeling long-range dependencies between each volume.
BraTS 2020. We also evaluate TransBTS on BraTS 2020 validation set and
the results are reported in Table 2. We directly adopt the hyperparameters on
BraTS19 for model training, our TransBTS achieves Dice scores of 78.73%,
90.09%, 81.73% and HD of 17.947mm, 4.964mm, 9.769mm on ET, WT, TC.
Compared with 3D U-Net , V-Net and Residual 3D U-Net, our TransBTS
shows great superiority in both metrics with signiﬁcant improvements. This
clearly reveals the beneﬁt of leveraging Transformer for modeling the global
relationships.
Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li
Model Complexity
TransBTS has 32.99M parameters and 333G FLOPs which is a moderate size
model. Besides, by reducing the number of stacked Transformer layers from 4 to
1 and halving the hidden dimension of the FFN, we reach a lightweight Trans-
BTS which only has 15.14M parameters and 208G FLOPs while achieving Dice
scores of 78.94%, 90.36%, 81.76% and HD of 4.552mm, 6.004mm, 6.173mm on
ET, WT, TC on BraTS2019 validation set. In other words, by reducing the
layers in Transformer as a simple and straightforward way to reduce complexity (54.11% reduction in parameters and 37.54% reduction in FLOPs of our
lightweight TransBTS), the performance only drops marginally. Compared with
3D U-Net which has 16.21M parameters and 1670G FLOPs, our lightweight
TransBTS shows great superiority in terms of model complexity. Note that eﬃcient Transformer variants can be used in our framework to replace the vanilla
Transformer to further reduce the memory and computation complexity while
maintaining the accuracy. But this is beyond the scope of this work.
Ablation Study
We conduct extensive ablation experiments to verify the eﬀectiveness of Trans-
BTS and justify the rationale of its design choices based on ﬁve-fold crossvalidation evaluations on the BraTS 2019 training set. (1) We investigate the
impact of the sequence length (N) of tokens for Transformer, which is controlled
by the overall stride (OS) of 3D CNN in the network encoder. (2) We explore
Transformer at various model scales (i.e. depth (L) and embedding dimension
(d)). (3) We also analyze the impact of diﬀerent positions of skip-connections.
Sequence length N. Table 3 presents the ablation study of various sequence
lengths for Transformer. The ﬁrst row (OS=16) and the second row (OS=8) both
reshape each volume of the feature map to a feature vector after downsampling.
It is noticeable that increasing the length of tokens, by adjusting the OS from 16
to 8, leads to a signiﬁcant improvement on performance. Speciﬁcally, 1.66% and
2.41% have been attained for the Dice score of ET and WT respectively. Due to
the memory constraint, after setting the OS to 4, we can not directly reshape
each volume to a feature vector. So we make a slight modiﬁcation to keep the
sequence length to 4096, which is unfolding each 2 × 2 × 2 patch into a feature
vector before passing to the Transformer. We ﬁnd that although the OS drops
from 8 to 4, without the essential increase of sequence length, the performance
does not improve or even gets worse.
Transformer Scale. Two hyper-parameters, the feature embedding dimension
(d) and the number of Transformer layers (depth L), mainly determines the scale
of Transformer. We conduct ablation study to verify the impact of Transformer
scale on the segmentation performance. For eﬃciency, we only train each model
conﬁguration for 1000 epochs. As shown in Table 4, the network with d = 512
and L = 4 achieves the best scores of ET and WT. Increasing the embedding
dimension (d) may not necessarily lead to improved performance (L = 4, d: 512
TransBTS: Multimodal Brain Tumor Segmentation Using Transformer
Table 3. Ablation study on sequence length (N).
OS Sequence
Dice score(%)
73.30 87.59 81.36
74.96 90.00 79.96
74.86 87.10 77.46
Table 4. Ablation study on Transformer.
Depth (L) Embedding dim (d)
Dice score(%)
68.95 83.31 66.89
73.72 88.02 73.14
69.38 83.54 74.16
70.11 85.84 70.95
66.48 79.16 67.22
Table 5. Ablation study on the positions of skip-connections (SC).
Number of SC
Position of SC
Dice score(%)
Transformer layer 74.96 90.00 79.96
3D Conv (Fig. 1) 78.92 90.23 81.19
vs. 768) yet brings extra computational cost. We also observe that L = 4 is a
“sweet spot” for the Transformer in terms of performance and complexity.
Positions of Skip-connections (SC). To improve the representation ability
of the model, we further investigate the positions for skip-connections (orange
dash lines “
” in Fig. 1). The ablation results are listed in Table 5. If skipconnections are attached to the ﬁrst three Transformer layers, it is more alike
to feature aggregation from adjacent layers without the compensation for loss of
spatial details. Following the traditional design of skip-connections from U-Net
(i.e. attach to the 3D Conv layers as shown in Fig. 1), considerable gains (3.96%
and 1.23%) have been achieved for the important ET and TC, thanks to the
recovery of low-level spatial detail information.
Conclusion
We present a novel segmentation framework that eﬀectively incorporates Transformer in 3D CNN for multimodal brain tumor segmentation in MRI. The resulting architecture, TransBTS, not only inherits the advantage of 3D CNN for
modeling local context information, but also leverages Transformer on learning
global semantic correlations. Experimental results on two datasets validate the eﬀectiveness of the proposed TransBTS. In future work,
we will explore computational and memory eﬃcient attention mechanisms in
Transformer to develop eﬃciency-focused models for volumetric segmentation.