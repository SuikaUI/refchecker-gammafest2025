PPFNet: Global Context Aware Local Features for Robust 3D Point Matching
Haowen Deng=,*,
Tolga Birdal=,*
Slobodan Ilic=,*
= Technische Universitat MÂ¨unchen, Germany,
* Siemens AG, MÂ¨unchen, Germany
National University of Defense Technology, China,
 , , 
We present PPFNet - Point Pair Feature NETwork
for deeply learning a globally informed 3D local feature
descriptor to ï¬nd correspondences in unorganized point
clouds. PPFNet learns local descriptors on pure geometry and is highly aware of the global context, an important cue in deep learning. Our 3D representation is computed as a collection of point-pair-features combined with
the points and normals within a local vicinity. Our permutation invariant network design is inspired by PointNet and
sets PPFNet to be ordering-free. As opposed to voxelization, our method is able to consume raw point clouds to
exploit the full sparsity. PPFNet uses a novel N-tuple loss
and architecture injecting the global information naturally
into the local descriptor. It shows that context awareness
also boosts the local feature representation. Qualitative and
quantitative evaluations of our network suggest increased
recall, improved robustness and invariance as well as a vital step in the 3D descriptor extraction performance.
1. Introduction
Local description of 3D geometry plays a key role in
3D vision as it precedes fundamental tasks such as correspondence estimation, matching, registration, object detection or shape retrieval.
Such wide application makes
the local features amenable for use in robotics , navigation (SLAM) and scene reconstruction for creation of
VR contents and digitalization. Developing such a generalpurpose tool motivated scholars to hand-craft their 3D feature descriptors/signatures for decades . Unfortunately, we now notice that this quest has not been very
fruitful in generating the desired repeatable and discriminative local descriptors for 3D point cloud data, especially
when the input is partial or noisy .
The recent trends carefully follow the paradigm shift to
Figure 1. PPFNet generates repeatable, discriminative descriptors
and can discover the correspondences simultaneously given a pair
of fragments. Point sets are colored by a low dimensional embedding of the local feature for visualization. 3D data and the
illustrative image are taken from 7-scenes dataset .
deep neural networks, but the latest works either base the
representation on a hand-crafted input encoding or try
to naively extend the networks from 2D domain to 3D .
Both approaches are sub-optimal, as they do not address an
end-to-end learning of the raw data, point sets.
In this paper, we present PPFNet network for deep learning of fast and discriminative 3D local patch descriptor with
increased tolerance to rotations.
To satisfy its desirable
properties, we ï¬rst represent the local geometry with an
augmented set of simple geometric relationships: points,
normals and point pair features (PPF) .
design a novel loss function, which we term as N-tuple
loss, to simultaneously embed multiple matching and nonmatching pairs into a Euclidean domain. Our loss resembles the contrastive loss , but instead of pairs, we consider an N-combination of the input points within two scene
 
fragments to boost the separability. Thanks to this many-tomany loss function, we are able to inject the global context
into the learning, i.e. PPFNet is aware of the other local
features when establishing correspondence for a single one.
Also because of such parallel processing, PPFNet is very
fast in inference. Finally, we combine all these contributions in a new pipeline, which trains our network from correspondences in 3D fragment pairs. PPFNet extends Point-
Net and thereby is natural for point clouds and neutral
to permutations. Fig. 1 visualizes our features and illustrates the robust matching we can perform.
Our extensive evaluations show that PPFNet achieves the
state of the art performance in accuracy, speed, robustness
to point density and tolerance to changes in 3D pose.
2. Related Work
Hand-crafted 3D Feature Descriptors
Similar to its 2D
counterpart, extracting meaningful and robust local descriptors from 3D data kept the 3D computer vision researchers
busy for a long period of time . Unfortunately, contrary to 2D, the repeatability and distinctiveness of 3D features were found to be way below expectations . Many of those approaches try to
discover a local reference frame (LRF), which is by its simplest deï¬nition, non-unique . This shifts the attention to
LRF-free methods such as the rotation invariant point pair
features (PPF) to be used as basis for creating powerful descriptors like PPFH and FPFH . PPFs are also
made semi-global to perform reasonably well under difï¬cult scenarios, such as clutter and occlusions .
Thus, they have been applied in many problems to estimate
3D poses or retrieve and recognize objects . Thanks
to the simplicity and invariance properties of PPFs, along
with the raw points and normals, we use it in PPFNet to describe the local geometry and learn a strong local descriptor.
Learned 3D Feature Descriptors
With the advent of
deep learning, several problems like 3D retrieval, recognition, segmentation and descriptor learning have been addressed using 3D data . A majority of them operate
on depth images , while just a handful of works target point clouds directly. As we address the problem of 3D
descriptor learning on point sets, we opt to review the data
representations regardless the application and then consider
learning local descriptors for point cloud matching.
There are many ways to represent sparse unstructured
3D data. Early works exploited the apparent dense voxels , usually in form of a binary-occupancy grid.
This idea is quickly extended to more informative encoding such as TDF , TSDF (Truncated Signed Distance Field), multi-label occupancy and other different
ones . Since mainly used in the context of 3D retrieval, entire 3D objects were represented with small voxel
grids 303 limited by the maximal size of 3D convolutions
kernels .
These representations have also been used
for describing the local neighbours in the context of 3D descriptor learning. One such contemporary work is 3DMatch
 . It is based on a robust volumetric TSDF encoding with
a contrastive loss to learn correspondences. Albeit straightforward, 3DMatch ignores the raw nature of the input: sparsity and unstructured-ness. It uses dense local grids and 3D
CNNs to learn the descriptor and thus can fall short in training/testing performance and recall.
A parallel track of works follow a view based
scheme , where the sub-spaces of 3D information
in form of projections or depth map are learned with well
studied 2D networks. Promising potential, these methods
do not cover for sparse point sets. Another spectrum of
research exploits graph networks also to represent
point sets . This new direction prospers in other domains but graph neural networks are not really suited to
point clouds, as they require edges, not naturally arising
from point sets. Khoury et. al. try to overcome the local
representation problem by a hand-crafted approach and use
a deep-network only for a dimensionality reduction. Their
algorithm also computes the non-unique LRF and taking
such a path deviates from the efforts of end-to-end learning.
The low hanging vital step is taken by PointNet , a
network designed for raw 3D point input. PointNet demonstrated that neural networks can be designed in a permutation invariant manner to learn segmentation, classiï¬cation
or keypoint extraction. It is then extended to PointNet++ to
better handle the variations in point density . However,
this work in its original form, cannot tackle the problem of
local geometry description as we successfully do.
As we will describe, PPFNet, trained on PPFs, points
and normals of local patches, boosts the notion of global
context in the semi-local features of PointNet by optimizing
combinatorial matching loss between multitudes of patches,
resulting in powerful features, outperforming the prior art.
3. Background
Motivation
Before explaining our local descriptor, let us
map out our setting. Consider two 3D point sets X âˆˆRnÃ—3
and Y âˆˆRnÃ—3. Let xi and yi denote the coordinates of
the ith points in the two sets, respectively. Momentarily, we
assume that for each xi, there exists a corresponding yi, a
bijective map. Then following and assuming rigidity,
the two sets are related by a correspondence and pose (motion), represented by a permutation matrix P âˆˆPn and a
rigid transformation T = {R âˆˆSO(3), t âˆˆR3}, respectively. Then the L2 error of point set registration reads:
d(X, Y|R, t, P) = 1
âˆ¥xi âˆ’Ryi(P) âˆ’tâˆ¥2
Global Feature
Concat Features
Concat Features
Concat Features
Patch Description - ğ…ğ‘Ÿ
Figure 2. PPFNET, our inference network, consists of multiple PointNets, each responsible for a local patch. To capture the global context
across all local patches, we use a max-pooling aggregation and fusing the output back into the local description. This way we are able to
produce stronger and more discriminative local representations.
where xi and yi(P) are matched under P. We assume X
and Y are of equal cardinality (|X| = |Y| = n). In form of
homogenized matrices, the following is equivalent:
d(X, Y|T, P) = 1
nâˆ¥X âˆ’PYTT âˆ¥2
Two sets are ideally matching if d(X, Y|T, P) â‰ˆ0. This
view suggests that, to learn an effective representation, one
should preserve a similar distance in the embedded space:
df(X, Y|T, P) = 1
nâˆ¥f(X) âˆ’f(PYTT )âˆ¥2
and df(X, Y|T, P) â‰ˆ0 also holds for matching points sets
under any action of (T, P). Thus, for invariance, it is desirable to have: f(Y) â‰ˆf(PYTT ). Ideally we would like to
learn f being invariant to permutations P and as intolerant
as possible to rigid transformations T. Therefore, in this
paper we choose to use a minimally handcrafted point set to
deeply learn the representation. This motivates us to exploit
PointNet architecture which intrinsically accounts for
unordered sets and consumes sparse input. To boost the tolerance to transformations, we will beneï¬t from point-pairfeatures, the true invariants under Euclidean isometry.
Point Pair Features (PPF)
Point pair features are antisymmetric 4D descriptors, describing the surface of a pair
of oriented 3D points x1 and x2, constructed as:
Ïˆ12 = (âˆ¥dâˆ¥2, âˆ (n1, d), âˆ (n2, d), âˆ (n1, n2))
where d denotes the difference vector between points, n1
and n2 are the surface normals at x1 and x2. âˆ¥Â·âˆ¥is the
Euclidean distance and âˆ is the angle operator computed in
a numerically robust manner as in :
âˆ (v1, v2) = atan2
 âˆ¥v1 Ã— v2âˆ¥, v1 Â· v2
âˆ (v1, v2) is guaranteed to lie in the range [0, Ï€). By construction, this feature is invariant under Euclidean transformations and reï¬‚ections as the distances and angles are preserved between every pair of points.
PointNet is an inspiring pioneer addressing the issue of consuming point clouds within a network
architecture. It is composed of stacking independent MLPs
anchored on points up until the last layers where a high dimensional descriptor is synthesized. This descriptor is weak
and used in max-pooling in order to aggregate to a global information, which is then fed into task speciï¬c losses. Use of
the max-pooling function makes the network inconsiderate
of the input ordering and that way extends notions of deep
learning to point sets. It showed potential on tasks like 3D
model classiï¬cation and segmentation. Yet, local features
of PointNet are only suitable for the tasks it targets and are
not generic. Moreover, the spatial transformer layer employed can bring only marginal improvement over the basic
architectures. It is one aspect of PPFNet to successfully
cure these drawbacks for the task of 3D matching. Note, in
our work, we use the vanilla version of PointNet.
We will begin by explaining our input preparation to compute a set describing the local geometry of a
3D point cloud. We then elaborate on PPFNet architecture,
which is designed to process such data with merit. Finally,
we explain our training method with a new loss function to
solve the combinatorial correspondence problem in a global
manner. In the end, the output of our network is a local descriptor per each sample point as shown in Fig. 2.
ğ‘ğ‘ğ‘“ğ±ğ‘Ÿ, ğ±0, ğ§ğ‘Ÿ, ğ§0
ğ‘ğ‘ğ‘“ğ±ğ‘Ÿ, ğ±1, ğ§ğ‘Ÿ, ğ§1
ğ‘ğ‘ğ‘“ğ±ğ‘Ÿ, ğ±ğ‘˜, ğ§ğ‘Ÿ, ğ§ğ‘˜
Figure 3. Simplistic encoding of a local patch.
Encoding of Local Geometry
Given a reference point
lying on a point cloud xr âˆˆX, we deï¬ne a local region
â„¦âŠ‚X and collect a set of points {mi} âˆˆâ„¦in this
local vicinity. We also compute the normals of the point
set . The associated local reference frame then
aligns the patches with canonical axes. Altogether, the oriented {xr âˆª{xi}} represent a local geometry, which we
term a local patch. We then pair each neighboring point
i with the reference r and compute the PPFs. Note that
complexity-wise, this is indifferent than using the points
themselves, as we omit the quadratic pairing thanks to ï¬xation of central reference point xr. As shown in Fig. 3, our
ï¬nal local geometry description and input to PPFNet is a
combined set of points normals and PPFs:
Fr = {xr, nr, xi, Â· Â· Â· , ni, Â· Â· Â· , Ïˆri, Â· Â· Â· }
architecture
architecture
PPFNet is shown in Fig.
Our input consists of N
local patches uniformly sampled from a fragment.
to sparsity of point-style data representation and efï¬cient
GPU utilization of PointNet, PPFNet can absorb those N
patches concurrently. The ï¬rst module of PPFNet is a group
of mini-PointNets, extracting features from local patches.
Weights and gradients are shared across all PointNets
during training.
A max pooling layer then aggregates
all the local features into a global one, summarizing the
distinct local information to the global context of the whole
fragment. This global feature is then concatenated to every
local feature. A group of MLPs are used to further fuse the
global and local features into the ï¬nal global-context aware
local descriptor.
N-tuple loss
Our goal is to use PPFNet to extract features
for local patches, a process of mapping from a high dimensional non-linear data space into a low dimensional linear
feature space. Distinctiveness of the resulting features are
closely related to the separability in the embedded space.
Ideally, the proximity of neighboring patches in the data
space should be preserved in the feature space.
(a) Pair Samples
(b) Triplet Samples
(c) N-Tuple Configuration
Figure 4. Illustration of N-tuple sampling in feature space. Green
lines link similar pairs, which are coerced to keep close. Red lines
connect non-similar pairs, pushed further apart. Without N-tuple
loss, there remains to be some non-similar patches that are close in
the feature space and some distant similar patches. Our novel Ntuple method pairs each patch with all the others guaranteeing that
all the similar patches remain close and non-similar ones, distant.
To this end, the state of the art seems to adopt two loss
functions: contrastive and triplet , which try to consider pairs and triplets respectively. Yet, a fragment consists
of more than 3 patches and in that case the widely followed
practice trains networks by randomly retrieving 2/3-tuples
of patches from the dataset. However, networks trained in
such manner only learn to differentiate maximum 3 patches,
preventing them from uncovering the true matching, which
is combinatorial in the patch count.
Generalizing these losses to N-patches, we propose Ntuple loss, an N-to-N contrastive loss, to correctly learn to
solve this combinatorial problem by catering for the manyto-many relations as depicted in Fig. 4. Given the ground
truth transformation T, N-tuple loss operates by constructing a correspondence matrix M âˆˆRNÃ—N on the points of
the aligned fragments. M = (mij) where:
mij = 1(âˆ¥xi âˆ’Tyjâˆ¥2 < Ï„)
1 is an indicator function. Likewise, we compute a featurespace distance matrix D âˆˆRNÃ—N and D = (dij) where
dij = âˆ¥f(xi) âˆ’f(yj)âˆ¥2
The N-tuple loss then functions on the two distance matrices solving the correspondence problem. For simplisity
of expression, we deï¬ne an operation Pâˆ—(Â·) to sum up all
the elements in a matrix. N-tuple loss can be written as:
+ Î±max(Î¸ âˆ’(1 âˆ’M) â—¦D, 0)
Here â—¦stands for Hadamard Product - element-wise multiplication. Î± is a hyper-parameter balancing the weight between matching and non-matching pairs and Î¸ is the lowerbound on the expected distance between non-correspondent
pairs. We train PPFNet via N-tuple loss, as shown in Fig.
5, by drawing random pairs of fragments instead of patches.
This also eases the preparation of training data.
Fragment 1
Fragment 2
Correspondence
Feature Space
Distance Matrix
N-tuple Loss
Gradient Backpropagation
Figure 5. Overall training pipeline of PPFNet. Local patches are sampled from a pair of fragments respectively, and feed into PPFNet to get
local features. Based on these features a feature distance matrix is computed for all the patch pairs. Meanwhile, a distance matrix of local
patches is formed based on the ground-truth rigid pose between the fragments. By binarizing the distance matrix, we get a correspondence
matrix to indicate all the matching and non-matching relationships between patches. N-tuple loss is then calculated by coupling the feature
distance matrix and correspondence matrix to guide the PPFNet to ï¬nd an optimal feature space.
Table 1. Our evaluations on the 3DMatch benchmark before RANSAC. Kitchen is from 7-scenes and the rest from SUN3D .
Spin Images 
PointNet 
3DMatch 
3DMatch-2K 
PPFNet (ours)
5. Results
Our input encoding uses a 17-point neighborhood
to compute the normals for the entire scene, using the well
accepted plane ï¬tting .
For each fragment, we anchor 2048 sample points distributed with spatial uniformity.
These sample points act as keypoints and within their 30cm
vicinity, they form the patch, from which we compute the
local PPF encoding. Similarly, we down-sample the points
within each patch to 1024 to facilitate the training as well as
to increase the robustness of features to various point density and missing part. For occasional patches with insufï¬cient points in the deï¬ned neighborhood, we randomly repeat points to ensure identical patch size. PPFNet extracts
compact descriptors of dimension 64.
PPFNet is implemented in the popular Tensorï¬‚ow .
The initialization uses random weights and ADAM 
optimizer minimizes the loss.
Our network operates simultaneously on all 2048 patches. Learning rate is set at
0.001 and exponentially decayed after every 10 epochs until 0.00001. Due to the hardware constraints, we use a batch
size of 2 fragment pairs per iteration, containing 8192 local
patches from 4 fragments already. This generates 2Ã—20482
combinations for the network per batch.
Real Datasets
We concentrate on real sets rather than
synthetic ones and therefore our evaluations are against the
diverse 3DMatch RGBD benchmark , in which 62 different real-world scenes retrieved from the pool of datasets
Analysis-by-Synthesis , 7-Scenes , SUN3D ,
RGB-D Scenes v.2 and Halber et . This collection
is split into 2 subsets, 54 for training and validation, 8 for
testing. The dataset typically includes indoor scenes like
living rooms, ofï¬ces, bedrooms, tabletops, and restrooms.
See for details. As our input consists of only point geometry, we solely use the fragment reconstructions captured
by Kinect sensor and not the color.
Can PPFNet outperform the baselines on real data?
We evaluate our method against hand-crafted baselines of
Inlier ratio threshold
Matching Percentage (%)
Spin Images
3DMatch-2K
(a) Recall on 3DMatch benchmark
Matching Fragements Accuracy
Spin Images
3DMatch-2K
(b) Robustness to sparsity
Inlier ratio threshold
Percentage of Matching Fragments (%)
Train Data
No Global No PPF
(c) Training with different inputs
Inlier ratio threshold
Percentage of Matching Fragments (%)
Validation Data
No Global No PPF
(d) Testing with different inputs
Figure 6. Evaluating PPFNet on real datasets: (a) Our method consistently outperforms the state-of-the-art on matching task (no RANSAC
is used) in terms of recall. (b) Thanks to its careful design, PPFNet clearly yields the highest robustness to change in the sparsity of the
input, even when only 6.25% of the input data is used. (c, d) Assessing different elements of the input on training and validation sets,
respectively. Note that combining cues of global information and point pair features help the network to achieve the top results.
Spin Images , SHOT , FPFH , USC , as
well as 3DMatch , the state of the art deep learning
based 3D local feature descriptor, the vanilla PointNet 
and CGF , a hybrid hand-crafted and deep descriptor
designed for compactness. To set the experiments more fair,
we also show a version of 3DMatch, where we use 2048 local patches per fragment instead of 5K, the same as in our
method, denoted as 3DMatch-2K. We use the provided pretrained weights of CGF . We keep the local patch size
same for all methods. Our evaluation data consists of fragments from 7-scenes and SUN3D datasets. We
begin by showing comparisons without applying RANSAC
to prune the false matches. We believe that this can show
the true quality of the correspondence estimator. Inspired
by , we accredit recall as a more effective measure for
this experiment, as the precision can always be improved
by better corresponding pruning . Our evaluation metric directly computes the recall by averaging the number of
matched fragments across the datasets:
 (xi âˆ’Tyj) < Ï„1
where M is the number of ground truth matching fragment
pairs, having at least 30% overlap with each other under
ground-truth transformation T and Ï„1 = 10cm. (i, j) denotes an element of the found correspondence set â„¦. x and
y respectively come from the ï¬rst and second fragment under matching. The inlier ratio is set as Ï„2 = 0.05. As
seen from Tab. 1, PPFNet outperforms all the hand crafted
counterparts in mean recall. It also shows consistent advantage over 3DMatch-2K, using an equal amount of patches.
Finally and remarkably, we are able to show âˆ¼2.7% improvement on mean recall over the original 3DMatch, using
only âˆ¼40% of the keypoints for matching. The performance boost from 3DMatch-2K to 3DMatch also indicates
that having more keypoints is advantageous for matching.
Our method expectedly outperforms both vanilla PointNet
and CGF by 15%. We show in Tab. 2 that adding more samples brings beneï¬t, but only up to a certain level (< 5K).
For PPFNet, adding more samples also increases the global
context and thus, following the advent in hardware, we have
the potential to further widen the performance gap over
3DMatch, by simply using more local patches. To show that
we do not cherry-pick Ï„2 but get consistent gains, we also
plot the recall computed with the same metric for different
inlier ratios in Fig. 6(a). There, for the practical choices of
Ï„2, PPFNet persistently remains above all others.
Table 2. Recall of 3DMatch for different sample sizes.
Samples 128
10K 20K 40K
0.24 0.32 0.40 0.47 0.51 0.59 0.59 0.56 0.60
Application to geometric registration
Similar to ,
we now use PPFNet in a broader context of transformation
estimation. To do so, we plug all descriptors into the well
established RANSAC based matching pipeline, in which
the transformation between fragments is estimated by running a maximum of 50,000 RANSAC iterations on the initial correspondence set. We then transform the source cloud
to the target by estimated 3D pose and compute the pointto-point error. This is a well established error metric .
Tab. 3 tabulates the results on the real datasets. Overall,
PPFNet is again the top performer, while showing higher
recall on a majority of the scenes and on the average. It
is noteworthy that we always use 2048 patches, while allowing 3DMatch to use its original setting, 5K. Even so,
we could get better recall on more than half of the scenes.
When we feed 3DMatch 2048 patches, to be on par with
our sampling level, PPFNet dominates performance-wise
on most scenes with higher average accuracy.
Robustness to point density
Changes in point density,
a.k.a. sparsity, is an important concern for point clouds, as it
Table 3. Our evaluations on the 3D-match benchmark after RANSAC. Kitchen is from 7-scenes and the rest from SUN3D .
Spin Images 
PointNet 
3DMatch 
3DMatch-2K
Red Kitchen
Study Room
Table 4. Average per-patch runtime of different methods.
input preparation
inference / patch
0.31ms on GPU
2.9ms on GPU
2.24ms on CPU
55Âµs on GPU
can change with sensor resolution or distance for 3D scanners. This motivates us to evaluate our algorithm against
others in varying sparsity levels. We gradually decrease
point density on the evaluation data and record the accuracy. Fig. 6(b) shows the signiï¬cant advantage of PPFNet,
especially under severe loss of density (only 6.5% of points
kept). Such robustness is achieved due to the PointNet backend and the robust point pair features.
How fast is PPFNet?
We found PPFNet to be lightning
fast in inference and very quick in data preparation since we
consume a very raw representation of data. Majority of our
runtime is spent in the normal computation and this is done
only once for the whole fragment. The PPF extraction is
carried out within the neighborhoods of only 2048 sample
points. Tab. 4 shows the average running times of different methods and ours on an NVIDIA TitanX Pascal GPU
supported by an Intel Core i7 3.2GhZ 8 core CPU. Such
dramatic speed-up in inference is enabled by the parallel-
PointNet backend and our simultaneous correspondence estimation during inference for all patches. Currently, to prepare the input for the network, we only use CPU, leaving
GPU idle for more work. This part can be easily implemented on GPU to gain even further speed boosts.
5.1. Ablation Study
N-tuple loss
We train and test our network with 3 different losses: contrastive (pair) , triplet and our
N-tuple loss on the same dataset with identical network
conï¬guration. Inter-distance distribution of correspondent
pairs and non-correspondent pairs are recorded for the
train/validation data respectively. Empirical results in Fig.
7 show that the theoretical advantage of our loss immediately transfers to practice: Features learned by N-tuple are
better separable, i.e. non-pairs are more distant in the embedding space and pairs enjoy a lower standard deviation.
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
Triplet (Train Data)
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
Pair (Train Data)
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
N-tuple (Train Data)
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
Triplet (Validation Data)
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
Pair (Validation Data)
0.2 0.4 0.6 0.8 1.0 1.2 1.4
percentage %
N-tuple (Validation Data)
No Correspondence
Correspondence
Figure 7. N-tuple Loss (c) lets the PPFNet better separate the
matching vs non-matching pairs w.r.t. the traditional contrastive
(a) and triplet (b) losses.
Table 5. Effect of different components in performance: Values
depict the number of correct matches found to be 5% inlier ratio.
Validation
Without points and normals
Vanilla PointNet 
Without global context
Without PPF
All combined
N-tuples loss repels non-pairs further in comparison to contrastive and triplet losses because of its better knowledge
of global correspondence relationships. Our N-tuple loss is
general and thus we strongly encourage the application also
to other domains such as pose estimation .
How useful is global context for local feature extraction?
We argue that local features are dependent on the context. A
corner belonging to a dining table should not share the similar local features of a picture frame hanging on the wall. A
table is generally not supposed to be attached vertically on
the wall. To assess the returns obtained from adding global
context, we simply remove the global feature concatenation,
keep the rest of the settings unaltered, and re-train and test
on two subsets of pairs of fragments. Our results are shown
in Tab. 5, where injecting global information into local features improves the matching by 18% in training and 7% in
Fragment 1
Fragment 2
(c) 3DMatch
(d) PPFNet
Figure 8. Visualization of estimated transformations. Thanks to its robustness and understanding of global information, PPFNet can operate
under challenging scenarios with confusing, repetitive structures as well as mostly planar scenes with less variation in geometry.
(a) Original Fragment
(b) Rotate 30o around z axis
(c) Rotate 60o around z axis
Without PPF
Figure 9. Inclusion of PPF makes the network more robust to rotational changes as shown, where the appearance across each row
is expected to stay identical, for a fully invariant feature.
validation set as opposed to our baseline version of Vanilla
PointNet â€¢, which is free of global context and PPFs. Such
signiï¬cance indicates that global features aid discrimination
and are valid cues also for local descriptors.
What does adding PPF bring?
We now run a similar experiment and train two versions of our network,
with/without incorporating PPF into the input. The contribution is tabulated in Tab. 5. There, a gain of 1% in training
and 5% in validation is achieved, justifying that inclusion of
PPF increases the discriminative power of the ï¬nal features.
While being a signiï¬cant jump, this is not the only bene-
ï¬t of adding PPF. Note that our input representation is composed of 33% rotation-invariant and 66% variant representations. This is already advantageous to the state of the art,
where rotation handling is completely left to the network
to learn from data. We hypothesize that an input guidance
of PPF would aid the network to be more tolerant to rigid
transformations. To test this, we gradually rotate fragments
around z-axis to 180â—¦with a step size of 30â—¦and then match
the fragment to the non-rotated one. As we can observe
from Tab. 6, with PPFs, the feature is more robust to rota-
â€¢Note that this doesnâ€™t 100% correspond to the original version, as we
modiï¬ed PointNet with task speciï¬c losses for our case.
Table 6. Effect of point pair features in robustness to rotations.
z-rotation
tion and the ratio in matching performance of two networks
opens as rotation increases. In accordance, we also show
a visualization of the descriptors at Fig. 9 under small and
large rotations. To assign each descriptor an RGB color, we
use PCA projection from high dimensional feature space to
3D color space by learning a linear map . It is qualitatively apparent that PPF can strengthen the robustness
towards rotations. All in all, with PPFs we gain both accuracy and robustness to rigid transformation, the best of
seemingly contradicting worlds. It is noteworthy that using
only PPF introduces full invariance besides the invariance
to permutations and renders the task very difï¬cult to learn
for our current network. We leave this as a future challenge.
A major limitation of PPFNet is quadratic memory footprint, limiting the number of used patches to 2K on our
hardware. This is, for instance, why we cannot outperform
3DMatch on fragments of Home-2. With upcoming GPUs,
we expect to reach beyond 5K, the point of saturation.
6. Conclusion
We have presented PPFNet, a new 3D descriptor tailored for point cloud input. By generalizing the contrastive
loss to N-tuple loss to fully utilize available correspondence
relatioships and retargeting the training pipeline, we have
shown how to learn a globally aware 3D descriptor, which
outperforms the state of the art not only in terms of recall but
also speed. Features learned from our PPFNet is more capable of dealing with some challenging scenarios, as shown
in Fig. 8. Furthermore, we have shown that designing our
network suitable for set-input such as point pair features are
advantageous in developing invariance properties.
Future work will target memory bottleneck and solving
the more general rigid graph matching problem.