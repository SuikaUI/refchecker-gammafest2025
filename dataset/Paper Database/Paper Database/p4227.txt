Wide Compression: Tensor Ring Nets
Wenqi Wang
Purdue University
 
Technicolor Research
 
Brian Eriksson
 
Wenlin Wang
Duke University
 
Vaneet Aggarwal
Purdue University
 
Deep neural networks have demonstrated state-of-theart performance in a variety of real-world applications. In
order to obtain performance gains, these networks have
grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The tradeoff is that these large architectures require an enormous
amount of memory, storage, and computation, thus limiting
their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which
signiﬁcantly compress both the fully connected layers and
the convolutional layers of deep neural networks. Our results show that our TR-Nets approach is able to compress
LeNet-5 by 11× without losing accuracy, and can compress
the state-of-the-art Wide ResNet by 243× with only 2.3%
degradation in Cifar10 image classiﬁcation. Overall, this
compression scheme shows promise in scientiﬁc computing and deep learning, especially for emerging resourceconstrained devices such as smartphones, wearables, and
IoT devices.
1. Introduction
Deep neural networks have made signiﬁcant improvements in a variety of applications, including recommender
systems , time series classiﬁcation , nature language processing , and image and video recognition . These accuracy improvements require developing
deeper and deeper networks, evolving from AlexNet 
(with P = 61 M parameters), VGG19 (P = 114 M),
and GoogleNet (P = 11 M) , to 32-layer ResNet (P =
0.46 M) , 28-layer WideResNet (P = 36.5 M),
and DenseNets . Unfortunately, with each evolution in
architecture comes a signiﬁcant increase in the number of
model parameters.
On the other hand, many modern use cases of deep neural networks are for resource-constrained devices, such as
mobile phones , wearables and IoT devices , etc. In
these applications, storage, memory, and test runtime complexity are extremely limited in resources, and compression
in these areas is thus essential.
After prior work observed redundancy in trained neural networks, a useful area of research has been compression
of network layer parameters (e.g., ). While a
vast majority of this research has been focused on the compression of fully connected layer parameters, the latest deep
learning architectures are almost entirely dominated by convolutional layers. For example, while only 5% of AlexNet
parameters are from convolutional layers, over 99% of Wide
ResNet parameters are from convolutional layers. This necessitates new techniques that can factorize and compress
the multi-dimensional tensor parameters of convolutional
We propose compressing deep neural networks using
Tensor Ring (TR) factorizations , which can be viewed
as a generalization of a single Canonical Polyadic (CP) decomposition , with two extensions:
1. the outer vector products are generalized to matrix
products, and
2. the ﬁrst and last matrix are additionally multiplied
along their outer edges, forming a “ring” structure.
The exact formulation is described in more detail in Section 3. Note that this is also a generalization of the Tensor
Train factorization , which only includes the ﬁrst extension. This is inspired by previous results in image processing , which demonstrate that this general factorization
technique is extremely expressive, especially in preserving
spatial features.
Speciﬁcally, we introduce Tensor Ring Nets (TRN), in
which layers of a deep neural network are compressed using tensor ring factorization. For fully connected layers,
we compress the weight matrix, and investigate different
merge/reshape orders to minimize real-time computation
and memory needs. For convolutional layers, we carefully
 
compress the ﬁlter weights such that we do not distort the
spatial properties of the mask. Since the mask dimensions
are usually very small (5×5, 3×3 or even 1×1) we do not
compress along these dimensions at all, and instead compress along the input and output channel dimensions.
To verify the expressive power of this formulation, we
train several compressed networks. First, we train LeNet-
300-100 and LeNet-5 on the MNIST dataset, compressing LeNet-5 by 11× without degradation and achiving 99.31% accuracy, and compressing LeNet-300-100 by
13× with a degrading of only 0.14% (obtaining overall accuracy of 97.36%). Additionally, we examine the state-ofthe-art 28-layer Wide-ResNet on Cifar10, and ﬁnd that
TRN can be used to effectively compress the Wide-ResNet
by 243× with only 2.3% decay in performance, obtaining
92.7% accuracy. The compression results demonstrates the
capability of TRN to compress state-of-the-art deep learning models for new resources constrained applications.
Section 2 discusses related work in neural network compression.
The compression model is introduced in Section 3, which discusses general tensor ring factorizations,
and their speciﬁc application to fully connected and convolutional layers. The compression method for convolutional layers is a key novelty, as few previous papers extend
factorization-based compression methods beyond fully connected layers. Finally, we show our experimental results
improve upon the state-of-the-art in compressibility without signiﬁcant performance degradation in Section 4.3 and
conclude with future work in Section 5
2. Related Work
Past deep neural network compression techniques have
largely applied to fully connected layers, which previously
have dominated the number of parameters of a model. However, since modern models like ResNet and WideResNet
are moving toward wider convolutional layers and omitting
fully connected layers altogether, it is important to consider
compression schemes that work on both fronts.
Many modern compression schemes focus on postprocessing techniques, such as hashing and quantization
 . A strength of these methods is that they can be applied in addition to any other compression scheme, and are
thus orthogonal to other methods. More similar to our work
are novel representations like circulant projections and
truncated SVD representations .
Low-rank tensor approximation of deep neural networks
has been widely investigated in the literature for effective model compression, low generative error, and fast prediction speed . Tensor Networks (TNs) have recently drawn considerable attention in multidimensional data representation , and deep
learning .
One of the most popular methods of tensor factorization
is the Tucker factorization , and has been shown to exhibit good performance in data representation and
in compressing fully connected layers in deep neural networks . In , a Tucker decomposition approach is
applied to compress both fully connected layers and convolution layers.
Tensor train (TT) representation is another example
of TNs that factorizes a tensor into boundary two matrices
and a set of 3rd order tensors, and has demonstrated its capability in data representation and deep learning
 . In , the TT model is compared against TR
for multi-dimensional data completion, showing that for the
same intermediate rank, TR can be far more expressive than
TT, motivating the generalization. In this paper, we investigate TR for deep neural network compression.
3. Tensor Ring Nets (TRN)
In this paper, X ∈RI1×···×Id is a d mode tensor with
i=1 Ii degrees of freedom.
A tensor ring decomposition factors such an X into d independent 3-mode tensors,
U(1), . . . , U(d) such that each entry inside the tensor X is
represented as
Xi1,··· ,id =
r1,··· ,rd
rd,i1,r1U(2)
r1,i2,r2 · · · U(d)
rd−1,id,rd,
where U(i) ∈RR×Ii×R, and R is the tensor ring rank. 1
Under this low-rank factorization, the number of free parameters is reduced to R2 Pd
i=1 Ii in the tensor ring factor
form, which is signiﬁcantly less than Qd
i=1 Ii in X.
For notational ease, let U = {U(1), · · · , U(d)}, and de-
ﬁne decomp(X; R, d) as the operation to obtain d factors
U(i) with tensor ring rank R from X, and construct(U) as
the operation to obtain X from U.
Additionally, for 1 ≤k < j ≤d, deﬁne the merge operation as M = merge(U, k, j) such that Uk, Uk+1, · · · , Uj
are merged into one single tensor M of dimension R×Ik ×
Ik+1 × · · · × Ij × R, and each entry in M is
Mrk−1,ik,ik+1,··· ,ij,rj =
rk,··· ,rj−1
rk−1,ik,rkU(k+1)
rk,ik+1,rk+1 · · · U(j)
rj−1,ij,rj.
Note that construct operator is the merge operation
merge(U, 1, d), which results in a tensor of shape R × I1 ×
I2 × · · · × Id × R, followed by summing along mode 1 and
mode d+2, resulting in a tensor of shape I1 ×I2 ×· · ·×Id;
construct(U) =
merge(U, 1, d)r,:,r.
1More generally, U(i) ∈RRi×Ii×Ri+1 and each Ri may not be the
same. For simplicity, we assume R1 = · · · = Rd = R.
Figure 1: Tensor diagrams. Left: A graphical representation of a length n vector x, a n×m matrix A, and a 3rd order
I1×I2×I3 tensor U. Right: factorized forms for a dot product xT y, matrix product AB where A and B have k rows
and columns respectively, and the tensor product of U and V
along a common axis. More explicitly, the tensor product on
the bottom right has 4 orders and the i1, i2, i3, i4-th element
j=1 Ui1,i2,jVi3,i4,j for ik = 1, . . . , Ik, k = 1, 2, 3, 4.
Tensor diagrams
Figure 1 introduces the popular tensor
diagram notation , which represents tensor objects as
nodes and their axes as edges of an undirected graph. An
edge connecting two nodes indicates multiplication along
that axis, and a “dangling” edge shows an axis in the remaining product, with the dimension given as the edge
weight. This compact notation is useful in representing various factorization methods (Figure 2).
Merge ordering
The computation complexity in this paper is measured in ﬂops (counting additions and multiplications). The number of ﬂops for a construct depends on the
sequence of merging U(i), i = 1, · · · , d. (See ﬁgure 3). A
detailed analysis of the two schemes is given in appendix A,
resulting in the following conclusions.
Theorem 1. Suppose I1 = · · · = Id ≥2 and I = Qd
1. any merge order costs between 2R3I and 4R3I ﬂops,
2. any merge order costs requires storing between R2I
and 2R2I ﬂoats, and
3. if d is a power of 2, then a hierarchical merge order
achieves the minimum ﬂop count.
Proof. See appendix A.
Several interpretations can be made from these observations. First, though different merge orderings give different
ﬂop counts, the worst choice is at most 2x more expensive
than the best choice. However, since we have to make some
kind of choice, we note that since every merge order is a
combination of hierarchical and sequential merges, striving
toward a hierarchical merging is a good heuristic to minimize ﬂop count. Thus, in our paper, we always use this
A Tensor Ring Network (TRN) is a tensor factorization
of either fully connected layers (FCL) or convolutional layers (ConvL), trained via back propagation. If a pre-trained
(a) CP Decomposition
(b) Tucker
(c) Tensor Train (TT)
(d) Tensor Ring(TR)
Figure 2: Tensor decompositions. Tensor diagrams for
four popular tensor factorization methods: (a) the CP decomposition (unnormalized), (b) the Tucker decomposition,
(c) the Tensor Train (TT) decomposition, and (d) the Tensor
Ring (TR) decomposition used in this paper. As shown, TR
can be viewed as a generalization of both CP (with r > 1)
and TT (with an added edge connecting the ﬁrst and last
tensors). In Section 4.3, we also compare against Tucker
decomposition compression schemes.
model is given, a good initialization can be obtained from
the tensor ring decomposition of the layers in the pre-trained
3.1. Fully Connected Layer Compression
In feed-forward neural networks, an input feature vector
x ∈RI is mapped to an output feature vector y = Ax ∈
RO via a fully connected layer A ∈RI×O. Without loss
of generality, x, A, and y can be reshaped into higher order
tensors X, A, and Y with
Yo1,...,o ˆ
Ai1,...,id,o1,...,o ˆ
dXi1,...,id
where d and ˆd are the modes of X and Y respectively, and
ik’s ad ok’s span from 1 to Ik and 1 to Ok respectively, and
To compress a feed-forward network, we decompose as
U = {U(1), . . . , U(d+ ˆd)} = decomp(A; R, d + ˆd) and replace A with its decomposed version in (3). A tensor diagram for this operation is given in Figure 4, which shows
Merge ordering.
A 4th order tensor is
merged from its factored form, either hierarchically via
(a)→(b)→(d), or sequentially via (a)→(c)→(d). Note that
the computational complexity of forming (b) is r3(I1I2 +
I3I4) and for (c) is r3(I1I2 + I1I2I4), and (c) is generally
more expensive (if I1 ≈I2 ≈I3 ≈I4). This is discussed
in detail in Appendix A.
Input of next layer
Figure 4: Fully connected layer. Tensor diagram of a fully
connected TRN, divided into input and weights. The composite tensor is the input into the next layer.
how each multiplication is applied and the resulting dimensions.
Computational cost
The computational cost again depends on the order of merging X and U. Note that there
is no need to fully construct the tensor A, and a tensor representation of A is sufﬁcient to obtain Y from X. To reduce
the computational cost, a layer separation approach is proposed by ﬁrst using hierarchical merging to obtain
F(1) = merge(U, 1, d) ∈RR×I1×···×Id×R
F(2) = merge(U, d + 1, d + ˆd) ∈RR×O1×···×O ˆ
which is upper bounded by 4R3(I + O) ﬂops. By replacing A in (3) with F(1) and F(2) and switching the order of
summation, we obtain
d,i1,··· ,id,rdXi1,...,id,
Yo1,...,o ˆ
rd,o1,··· ,o ˆ
The summation (5) is equivalent to a feed-forward layer of
shape (I1 · · · Id) × R2, which takes 2R2I ﬂops. Additionally, the summation over rd+ ˆd and rd is equivalent to another feed-forward layer of shape R2 × (O1 · · · O ˆd), which
takes 2R2O ﬂops. Such analysis demonstrates that the layer
separation approach to a FCL in a tensor ring net is equivalent to a low-rank matrix factorization to a fully-connected
layer, thus reducing the computational complexity when R
is relatively smaller than I and O.
Deﬁne PFC and CFC as the complexity saving in parameters and computation, respectively, for the tensor net decomposition over the typical fully connected layer forward
propagation. Thus we have
i Ii + P ˆd
(4R3 + 2BR2)(I + O),
where B is the batch size of testing samples. Here, we see
the compression beneﬁt in computation; when B is very
large, (8) converges to IO/(R2(I + O)), which for large I,
O and small R is signiﬁcant. Additionally, though the expensive reshaping step grows cubically with R (as before),
it does not grow with batch size; conversely, the multiplication itself (which grows linearly with batch size) is only
quadratic in R. In the paper, the parameter is selected by
picking small R and large d to achieve the optimal C since
R needs to be small enough for computation saving.
3.2. Convolutional Layer Compression
In convolutional neural networks(CNNs), an input tensor
X ∈RH×W ×I is convoluted with a 4th order kernel tensor
K ∈RD×D×I×O and mapped to a 3rd order tensor Y ∈
RH×W ×O, as follows
Xh′,w′,iKd1,d2,i,o,
h′ = (h −1)s + d1 −p,
w′ = (w −1)s + d2 −p,
where s is stride size, p is zero-padding size. Computed as
in (9), the ﬂop cost is D2 · IO · HW. 2
In TRN, tensor ring decomposition is applied onto the
kernel tensor K and factorizes the 4th order tensor into four
3rd tensors. With the purpose to maintain the spatial information in the kernel tensor, we do not factorize the spatial
dimension of K via merging the spatial dimension into one
4th order tensor V(1)
R1,D1,D2,R2, thus we have
Kd1,d2,i,o =
r1,r2,r3=1
Vr1,d1,d2,r2Ur2,i,r3 ˆUr3,o,r1.
In the scenario when I and O are large, the tensors U
and ˆU are further decomposed into U(1), . . . , U(d) and
U(d+1), . . . , U(d+ ˆd) respectively. (See also Figure 5.)
The kernel tensor factorization in (10) combined with the
convolution operation in (9) can be equivalently solved in
three steps:
Ph′,w′,r2,r3
Xh′,w′,iU(2)
Qh,w,r3,r1
Ph′,w′,r2,r3U(1)
r1,d1,d2,r2(12)
Qh,w,r3,r1U(3)
where (11) is a tensor multiplication along one slice, with
ﬂop count HWR2I, (12) is a 2-D convolution with ﬂop
count HWR3D2, and (13) is a tensor multiplication along
3 slices with ﬂop count HWR2O. This is also equivalent
to a three-layer convolutional networks without non-linear
transformations, where (11) is a convolutional layer from I
feature maps to R2 feature maps with a 1 × 1 patch, (12)
contains R convolutional layers from R feature maps to R
feature maps with a D × D patch, and (13) is a convolutional layer from R2 feature maps to O feature maps with
with a 1 × 1 patch.
This is a common sub-architecture
choice in other deep CNNs, like the inception module in
GoogleNets , but without nonlinearities between 1 × 1
and D × D convolution layers.
Complexity: We employ the ratio between complexity in CNN layer and the complexity in tensor ring layer
to quantify the capability of TRN in reducing computation
(Cconv) and parameter (Pconv) costs,
D2R2 + IR2 + OR2 ,
R2I + R3D2 + R2O.
2For small ﬁlter sizes D ≪log(HW), as is often the case in deep neural networks for image processing, often direct multiplication to compute
convolution is more efﬁcient than using an FFT, which for this problem
has order IO(HW(log(HW))) ﬂops. Therefore we only consider direct
multiplication as a baseline.
Figure 5: Convolutional layer. Dashed lines show the convolution operation (9). Here, U(1), U(2) and U(3) decompose U and U(4), U(5), and U(6) decompose ˆU in (10). The
dashed line between X and V represent the convolution operation as expressed in (9). Note that I1×I2×I3 decompose
the number of channels entering the layer (which is 1 at the
ﬁrst input), where in Figure 4 they decompose the feature
dimension entering the layer.
If, additionally, the tensors U(1) and U(2) are further decomposed to d and ˆd tensors, respectively, then
D2R2 + R2(Pd
i Ii + P ˆd
4R3(I + O) + BR2(I + O) + BR3D2 .
Note that in the second scenario, we have a further compression in storage requirements, but lose gain in computational
complexity, which is a design tradeoff. In our experiments,
we further factorize U(1) and U(3) in to higher order tensors
in order to achieve our gain in model compression.
Initialization
In general nonconvex optimization (and especially for deep learning) the choice of initial variables can
dramatically effect the quality of the model training. In particular, we have found that initializing each parameter randomly from a Gaussian distribution is effective, with a carefully chosen variance. If we initialize all tensor factors as
drawn i.i.d. from N(0, σ2), then after merging d factors
the merged tensor elements will have mean 0 and variance
Rdσ2d (See appendix B). By picking σ =
where N is the amount of parameters in the uncompressed
layer, the merged tensor will have mean 0, variance
and in the limit will also be Gaussian. Since this latter distribution works well in training the uncompressed models,
choosing this value of σ for initialization is well-motivated,
and observed to be necessary for good convergence.
4. Experiments
We now evaluate the effectiveness of TRN-based compression on several well-studied deep neural networks and
LeNet-300-100 and LeNet-5 on MNIST, and
ResNet and WideResNet on Cifar10 and Cifar100. These
networks are trained using Tensorﬂow . All the experiments on LeNet are implemented on Nvidia GTX 1070
GPUs, and all the experiments for ResNet and WideRes-
Net are implemented on Nvidia GTX Titan X GPUs. In all
cases, the same tensor ring rank r is used in the networks,
and all the networks are trained from randomly initialization using the the proposed initialization method. Overall,
we show that this compression scheme can give signiﬁcant
compression gains for small accuracy loss, and even negligible compression gains for no accuracy loss.
4.1. Fully connected layer compression
The goal of compressing the LeNet-300-100 network is
to assess the effectiveness of compressing fully connected
layers using TRNs; as the name suggests, LeNet-300-100
contains two hidden fully connected layers with output dimension 300 and 100, and an output layer with dimension
10 (= # classes). Table 1 gives the parameter settings for
LeNet-300-100, both in its original form (uncompressed)
and in its tensor factored form. A compression rate greater
than 1 is achieved for all r ≤54, and a reduction in computational complexity for all r ≤6; both are typical choices.
Table 2 shows the performance results on MNIST classiﬁcation for the original model (as reported in their paper), and compressed models using both matrix factorization and TRNs. For a 0.14% accuracy loss, TRN can compress up to 13×, and for no accuracy loss, can compress
1.2×. Note also that matrix factorization, at 16× compression, performs worse than TRN at 117× compression, suggesting that the high order structure is helpful. Note also
that low rank Tucker approximation in is equivalent
to low rank matrix approximation when compressing fully
connected layer.
4.2. Convolutional layer compression
We now investigate compression of convolutional layers
in a small network. LeNet-5 is a (relatively small) convolutional neural networks with 2 convolution layers, followed
by 2 fully connected layers, which achieves 0.79% error
rate on MNIST. The dimensions before and after compression are given in Table 3. In this wider network we see a
much greater potential for compression, with positive compression rate whenever r ≤57. However, the reduction in
complexity is more limited, and only occurs when r ≤4.
However, the performance on this experiment is still positive. By setting r = 20, we compress LeNet-5 by 11×
and a lower error rate than the original model as well as the
Tucker factorization approach. If we also require a reduction in ﬂop count, we incur an error of 2.24%, which is still
quite reasonable in many real applications.
r = 10 (train)
r = 10 (test)
r = 6 (train)
r = 6 (test)
r = 2 (train)
r = 2 (test)
Tucker (train)
Tucker (test)
Figure 6: Evolution. Evolution of training compressed 32
layer ResNet on Cifar100, using TRNs with different values
of r and the Tucker factorization method.
4.3. ResNet and Wide ResNet Compression
Finally, we evaluate the performance of tensor ring nets
(TRN) on the Cifar10 and Cifar100 image classiﬁcation
tasks .
Here, the input images are colored, of size
32 × 32 × 3, belonging to 10 and 100 object classes respectively. Overall there are 50000 images for training and
10000 images for testing.
Table 5 gives the dimensions of ResNet before and after compression. A similar reshaping scheme is used for
WideResNet. Note that for ResNet, we have compression
gain for any r ≤22; for WideResNet this bound is closer to
r ≤150, suggesting high compression potential.
The results are given in Table 6 demonstrates that
TRNs are able to signiﬁcantly compress both ResNet and
WideResNet for both tasks.
Picking r = 10 for TRN
on ResNet gives the same compression ratio as the Tucker
compression method , but with almost 3% performance
lift on Cifar10 and almost 10% lift on Cifar 100. Compared
to the uncompressed model, we see only a 2% performance
degradation on both datasets.
The compression of WideResNet is even more successful, suggesting that TRNs are well-suited for these extremely overparametrized models. At a 243× compression
TRNs give a better performance on Cifar10 than uncompressed ResNet (but with fewer parameters) and only a 2%
decay from the uncompressed WideResNet. For Cifar100,
this decay increases to 8%, but again TRN of WideResNet
achieves lower error than uncompressed ResNet, with overall fewer parameters. Compared against the Tucker compression method , at 5× compression rate TRNs incur
only 2-3% performance degradation on both datasets, while
Uncompressed dims.
TRN dimensions
shape of composite tensor
(4 × 7 × 4 × 7) × (3 × 4 × 5 × 5)
1177r3 + 1084r2
(3 × 4 × 5 × 5) × (4 × 5 × 5)
457r3 + 400r2
(4 × 5 × 5) × (2 × 5)
127r3 + 107r2
1761r3 + 1591r2
Table 1: Fully connected compression. Dimensions of the three-fully-connected layers in the uncompressed (left) and
TRN-compressed (right) models. The computational complexity includes tensor product merging (O(r3)) and feed-froward
multiplication (O(r2)).
Train (s/epoch)
LeNet-300-100 
0.011 ± 0.002
M-FC (r = 10)
0.016 ± 0.010
M-FC (r = 20)
0.014 ± 0.010
M-FC (r = 50)
0.021 ± 0.012
TRN (r = 3)
0.015 ± 0.007
TRN (r = 5)
0.015 ± 0.007
TRN (r = 15)
0.015 ± 0.007
TRN (r = 50)
0.022 ± 0.008
11.1 ± 1.4
Table 2: Fully connected results. LeNet-300-100 on MNIST datase, trained to 40 epochs, using a minibatch size 50.
Trained from random weight initialization. ADAM is used for optimization. Testing time is per 10000 samples. CR =
Compression ratio. LR = Learning rate.
Uncompressed dims.
TRN dimensions
5 × 5 × 1 × 20
5 × 5 × 1 × (4 × 5)
33408r2 + 39245r3
5 × 5 × 20 × 50
5 × 5 × (4 × 5) × (5 × 10)
17840r2 + 5095r3
1250 × 320
(5 × 5 × 5 × 10) × (5 × 8 × 8)
1570r2 + 1685r3
(5 × 8 × 8) × 10
330r2 + 360r3
53148r2 + 46385r3
Table 3: Small convolution compression. Dimensions of LeNet-5 layers in its original form (left) and TRN-compressed
(right). The computational complexity includes tensor product merging and convolution operation in (12) of O(r3), and
convolution in (11) (13) of O(r2).
Train (s/epoch)
LeNet-5 
0.038 ± 0.027
Tucker 
0.066 ± 0.025
TRN (r = 3)
0.058 ± 0.026
TRN (r = 5)
0.072 ± 0.039
10.6 ± 7.1
TRN (r = 10)
0.080 ± 0.025
15.6 ± 4.6
TRN (r = 15)
0.039 ± 0.019
20.1 ± 16.0
TRN (r = 20)
0.052 ± 0.028
27.8 ± 7.4
Table 4: Small convolution results. LeNet-5 on MNIST dataset, trained to 20 epochs, using a minibatch size 128. ADAM
 is used for optimization. Testing time is per 10000 samples. CR = Compression ratio. LR = Learning rate.
Tucker incurs 5% and 11% performance degradation. The
compressibility is even more signiﬁcant for WideResNet,
where to achieve the same performance as Tucker at
5× compression, TRNs can compress up to 243× on Cifar10 and 286× on Cifar100. The tradeoff is runtime; we
observe the Tucker model trains at about 2 or 3 times faster
than TRNs for the WideResNet compression. However, for
memory-constrained devices, this tradeoff may still be desirable.
Uncompressed dims.
TRN dimensions
shape of composite tensor
3 × 3 × 3 × 16
9 × 3 × (4 × 2 × 2)
ResBlock(3, 16, 16)
9 × (4 × 2 × 2) × (4 × 2 × 2)
ResBlock(3, 16, 16) × 4
9 × (4 × 2 × 2) × (4 × 2 × 2)
ResBlock(3, 16, 32)
9 × (4 × 2 × 2) × (4 × 4 × 2)
ResBlock(3, 32, 32) × 4
9 × (4 × 4 × 2) × (4 × 4 × 2)
ResBlock(3, 32, 64)
9 × (4 × 4 × 2) × (4 × 4 × 4)
ResBlock(3, 64, 64) × 4
9 × (4 × 4 × 4) × (4 × 4 × 4)
(4 × 4 × 4) × 10
Table 5: Large convolution compression. Dimensions of 32 layer ResNes on Cifar10 dataset. Each ResBlock(p,I,O)
includes a sequence: input →Batch Normalization →ReLU →p × p × I × O convolution layer →Batch Normalization
→ReLU →p × p × O × O convolution layer. The input of length I is inserted once at the beginning and again at the end of
each unit. See for more details.
ResNet(RN)-32L
Tucker-RN 
TT-RN(r = 13) 
TRN-RN (r = 2)
TRN-RN (r = 6)
TRN-RN (r = 10)
WideResNet(WRL)-28L
Tucker-WRN 
TT-RN(r = 13) 
TRN-WRN (r = 2)
TRN-WRN (r = 6)
TRN-WRN (r = 10)
TRN-WRN(r=15)
Table 6: Large convolution results. 32-layer ResNet (ﬁrst 5 rows) and 28-layer Wide-ResNet (last 4 rows) on Cifar10
dataset and Cifar100 dataset, trained to 200 epochs, using a minibatch size of 128. The model is trained using SGD with
momentum 0.9 and a decaying learning rate. CR = Compression ratio.
Figure 6 shows the train and test errors during
training of compressed ResNet on the Cifar100 classiﬁcation task, for various choices of r and also compared against
Tucker tensor factorization. In particular, we note that the
generalization gap (between train and test error) is particularly high for the Tucker tensor factorization method, while
for TRNs (especially smaller values of r) it is much smaller.
And, for r = 10, both the generalization error and ﬁnal train
and test errors improve upon the Tucker method, suggesting
that TRNs are easier to train.
5. Conclusion
We have introduced a tensor ring factorization approach
to compress deep neural networks for resource-limited devices. This is inspired by previous work that has shown tensor rings to have high representative power in image completion tasks. Our results show signiﬁcant compressibility
using this technique, with little or no hit in performance on
benchmark image classiﬁcation tasks.
One area for future work is the reduction of computational complexity. Because of the repeated reshaping needs
in both fully connected and convolutional layers, there is
computational overhead, especially when r is moderately
large. This tradeoff is reasonable, considering our considerable compressibility gains, and is appropriate in memorylimited applications, especially if training is ofﬂoaded to the
cloud. Additionally, we believe that the actual wall-clocktime will decrease as tensor-speciﬁc hardware and low-level
routines continue to develop–we observe, for example, that
numpy’s dot function is considerably more optimized than
Tensorﬂow’s tensordot. Overall, we believe this is a
promising compression scheme and can open doors to using
deep learning in a much more ubiquitous computing environment.