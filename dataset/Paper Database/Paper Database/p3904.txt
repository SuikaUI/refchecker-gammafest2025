A Study of Joint Graph Inference and Forecasting
Daniel Z¨ugner 1 2 Franc¸ois-Xavier Aubet 3 Victor Garcia Satorras 4 2 Tim Januschowski 3
Stephan G¨unnemann 1 Jan Gasthaus 3
We study a recent class of models which uses
graph neural networks (GNNs) to improve
forecasting in multivariate time series. The core
assumption behind these models is that there is
a latent graph between the time series (nodes)
that governs the evolution of the multivariate
time series.
By parameterizing a graph in a
differentiable way, the models aim to improve
forecasting quality.
We compare four recent
models of this class on the forecasting task. Further, we perform ablations to study their behavior
under changing conditions, e.g., when disabling
the graph-learning modules and providing the
ground-truth relations instead.
Based on our
ﬁndings, we propose novel ways of combining
the existing architectures.
1. Introduction
Forecasting multivariate time series is a core machine learning task both in science and in industry .
the individual time series (nodes), rich dependencies and
interactions (edges) govern how the time series evolves.
In the simplest case these could be (linear) correlations;
other examples include the road network underlying traf-
ﬁc ﬂows , or physical relations such as attraction or
repulsion affecting trajectories of objects in space .
Knowledge of the ‘true’ relations can be used to make more
accurate predictions of how the time series evolves in the
future, e.g., by using graph neural networks (GNNs) (e.g.,
 ) . Even more, the graph can reveal
fundamental insights into the system described by the time
series, and may thus be of value in itself, independent of an
improvement in the forecasting quality. Therefore, recent
works aim at jointly inferring relations between the time
series and learn to forecast in an end-to-end manner, some-
1Technical University of Munich 2Work done while being an
intern at AWS AI Labs, Amazon Web Services 3AWS AI Labs,
Amazon Web Services 4University of Amsterdam.
Correspondence to: Daniel Z¨ugner < >.
Time Series Workshop @ ICML 2021. Copyright by the author(s).
times without any prior information about the graph .
Besides potential beneﬁts in forecasting quality, inferring a
graph among N time series comes at an inherent computational complexity of O(N 2), which needs to be taken into
account when deciding whether to leverage joint graph inference and forecasting. Hence, we consider the following
research questions in this paper.
(R1) In which scenarios do joint graph inference and forecasting improve forecasting accuracy? Given the diverse
domains and settings of multivariate time series forecasting (e.g., underlying spatial relations of sensors in trafﬁc
forecasting, sets of sensors measuring different properties
of the same system, etc.) it is possible that graph inference
helps the forecasting task more in some use cases.
(R2) How do the existing architectures compare in forecasting performance? Are there certain architectural choices
that appear beneﬁcial for forecasting?
(R3) What are properties of the inferred graphs by the
model? Speciﬁcally, how consistent are the inferred graphs
across different training runs? How (dis-)similar are the inferred graphs to the “ground-truth” graphs (when known)?
2. Background
Forecasting with Multivariate Time Series In time series
forecasting we are interested in estimating a future series
zt+1:T given its past zt0:t and some context information
about the past xt0:t where variables t0 < t < T index over
time. For the multivariate case, we can consider N time series at a time zt0:T = {z1,t0:T , . . . , zN,t0:T } ∈RN×T −t0.
We model the following conditional distribution:
p(zi,t+1:T |zt0:t, xt0,t), 1 ≤i ≤N,
where i indexes over time series. Notice that we are conditioning on all N series in order to estimate the series i.
Time Series Forecasting for graph structured data
When conditioning over multivariate time series as in
Eq. (1), we may beneﬁt from modelling the relations between different multivariate time series.
An expressive
structure to capture such relations are graphs. We can de-
ﬁne a graph as a set of nodes vi ∈V and edges eij ∈E
A Study of Joint Graph Inference and Forecasting
that relate the nodes.
In our case each zi is associated
to a graph node vi. Edges eij may be given or unkown
depending on the dataset, in cases where the underlying
graph is latent/unkown we may jointly infer the graph while
estimating a forecasting model.
In this work we study
the performance of a variety of algorithms under different
assumptions of the graph structure (known, unkown, partially known). Note that even in the cases where we have
“ground-truth” knowledge (e.g., of spatial relations), there
may still be additional latent relations which could be discovered by the models.
3. Literature Review
Recent models perform joint graph learning and forecasting in multivariate timeseries.
These models are GTS
(“graph for timeseries”) , Graph Deviation Network
(GDN) , MTS forecasting with GNNs (MTGNN) ,
and Neural Relational Inference (NRI) .
brieﬂy introduce these four methods and their differences
and commonalities; for a more detailed overview, see
Appendix B.
All models can be decomposed into two main components:
the graph learning and the forecasting modules.
former outputs an adjacency matrix describing a graph
between the nodes (i.e., timeseries). The latter takes this
graph as well as the input timeseries window to forecast
the next timestep(s). Once the adjacency matrix has been
obtained from the graph learning module, there are many
ways of how to leverage it for forecasting the timeseries.
The core idea of the models of this study is that the
adjacency matrix construction step is differentiable and
jointly learned with the forecasting module.
intuition is that the model will learn graphs which help the
forecasting task.
3.1. Graph learning
The goal of the graph learning module is to output an adjacency matrix A ∈ N×N, where each entry Aij denotes the edge weight between nodes (i, j). Typically, we
aim for A to be sparse, which reﬂects the intuition that
there are only relatively few useful relations in the latent
graph. Each model ﬁrst represents each node i by a ﬁxedsize vector hi, followed by a pairwise similarity computation of any pair hi and hj, e.g., by using a fully connected
neural network or simply by taking the dot product.
Next, the models obtain the adjacency matrix from the
pairwise scores. MTGNN and GDN do so by taking the
K highest scores per node. An advantage of this is that
by choosing K appropriately A is guaranteed to be sparse.
On the other hand, the top-K operation is not continuously
differentiable, which may pose challenges to end-to-end
NRI and GTS ﬁrst map the pairwise scores into range 
(e.g., via softmax or sigmoid). The models use the Gumbel
softmax trick to sample a discrete adjacency matrix
from the edge probabilities in a differentiable way (though
gradients are biased); a downside is that we have to take
extra steps to obtain a sparse graph, e.g., by regularization.
Moreover, the models can can be broadly split into two
groups according to how they compute the ﬁxed-size representations hi per node: MTGNN and GDN simply learn
these representations as node embeddings; on the other
hand, NRI and GTS compute the vectors hi based on the
time series itself. That is, they apply some (shared) function to each timeseries to map it into a ﬁxed-size vector.
While NRI dynamically produces the representations per
individual window, GTS uses the whole training timeseries
for each node. The former has the advantage of being more
ﬂexible, though more expensive, since we need to compute
a [B×N×N] tensor to store the individual adjacency matrices, where B is the batch size. On the other hand, the graph
learned by GTS is global, i.e., shared for all time series. It
is thus more efﬁcient yet less ﬂexible, as the model cannot adjust the graph for changing inputs during inference
time. Moreover, in its current implementation, this leads
to GTS’s number of parameters growing linearly with the
length of the training time series (though this could in principle be resolved via dilated convolutions or pooling).
3.2. Graph-based forecasting
incorporate
forecasting
 ).
Each of the models in
this study has its own way of forecasting the time series
given the input timeseries window and the adjacency
matrix constructed by the graph learning module. For instance, MTGNN interchanges temporal convolution layers
with graph convolution layers, and GTS uses a Diffusion-
Convolutional Recurrent Neural Network (DCRNN) ,
where the hidden states of each node are diffused via graph
convolutions at each timestep. Again, the core idea is that
the adjacency matrix used in the graph-based forecasting
is itself constructed in a differentiable way and can thus be
adjusted by the model to improve forecasting results.
4. Experiments
To address our research questions (R1)-(R3), we perform
experiments on real-world and synthetic datasets. We repeat all runs ﬁve times and report the average; error bars
are provided in Table 4 (Appendix).
A Study of Joint Graph Inference and Forecasting
Table 1. Average forecasting MAE (over ﬁve runs) when disabling the graph-learning and forcing the model to use the ground-truth
graph. We also show the percentage change of the MAE (∆); e.g., −4% means error is reduced by 4% over the base scenario.
Random graph
Electricity
Solar Energy
Exchange Rate
Table 2. Average forecasting MAE (averaged over ﬁve runs) when forcing the model to use a (sparse) random graph (left) or when not
using a graph at all in the forecasting (right). Relative performance (∆) as explained in Fig. 1. ‘-’ indicates OOM/timeout after 24 hours.
4.1. Datasets
We brieﬂy describe here the datasets that we use; more details can be found in appendix section C. We scale each
timeseries to have zero mean and unit variance or to have
range (only SWaT and WADI, as in ). For training
and evaluation we compute MAE on the original scale.
PEMS-BAY and METR-LA are widely used trafﬁc
datasets where we do have knowledge about the underlying graph. To construct the sensor graph, we computed the
pairwise road network distances between sensors and build
the adjacency matrix using a thresholded Gaussian kernel.
We use a range of other multi-variate datasets for which no
graph structure is known: Electricity,1,2 Solar-energy,3,2
Exchange-rate2 and Trafﬁc.4,2 Further, SWaT and
WADI are datasets of sensors measuring watertreatment plants.
In the test split there are annotated
anomalies where the creators tampered with the water
treatment systems.
Therefore, SWaT and WADI were
originally proposed as anomaly detection datasets (and
e.g., used in the GDN paper); however, since the respective
training sets are free of anomalies, we use them for our
forecasting experiments.
Synthetic datasets. To enhance the real world datasets,
we create two synthetic datasets starting with a graph and
1archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014
2github.com/laiguokun/multivariate-time-series-data
3www.nrel.gov/grid/solar-power-data.html
4 
making sure that the graph has an impact on the connection
between the time series. This allows us to speculate that the
graph will be of importance for the forecasting of the time
series. We create the Diffusion dataset by using Personalized PageRank (PPR) to diffuse the multivariate timeseries. We create the DAG dataset using a directed acyclic
graph (DAG) and making all the children dimensions be a
weighted combination of its parents dimensions.
4.2. Results
(R1). Here we analyze the forecasting results on the different datasets at horizons 3, 6, and 12, respectively. For reference, we also add a vanilla LSTM baseline that jointly
forecasts all timeseries, as well LSTM-U, which consists of
N univariate LSTMs. Essentially, the LSTM uses information from all timeseries, though lacks typical GNN properties such as permutation equivariance and does not leverage
sparsity. LSTM-U is on the other end of the spectrum and
simply views all timeseries as completely independent. In
Table 4 (appendix) we present the results.
On the popular trafﬁc datasets METR-LA and PEMS-BAY,
the GNN models generally dominate the LSTMs. These
datasets have known spatial spatial relations among the
timeseries, thus this comes as no surprise. NRI’s results
on METR-LA is quite poor, which we attribute to the
relatively large number of nodes and to the fact that the
underlying relations are static, while NRI predicts a graph
per window.
On WADI, interestingly, LSTM-U performs on par with
A Study of Joint Graph Inference and Forecasting
MTGNN. The remaining gap to GTS is relatively small and
can potentially be explained by GTS’s more sophisticated
forecasting procedure. This indicates that on WADI, where
we do not have a “straightforward” spatial graph between
the nodes, the GNN-based models struggle to ﬁnd useful
relations in the data – or that there are no useful relations
in the data to begin with. Similarly, on SWaT, LSTM outperforms all GNN-based models except GTS.
In the synthetic diffusion-based dataset, GTS achieves
roughly 50% lower mean absolute error than LSTM. We
attribute this to the fact that the data-generating process
(graph diffusion) matches well with the DC-RNN architecture used by GTS in the forecasting module. Further, note
that on Trafﬁc, GTS ran OOM on a 16GB VRAM GPU for
batch size larger than 1, and therefore did not ﬁnish within
24 hours. NRI, which is even more computationally expensive, has additional missing values.
In summary, the GNN-based models’ edge over the nongraph baselines tends to be largest for datasets with an underlying spatial graph (trafﬁc datasets, Electricity, Solar Energy), and smaller for the datasets where the relations are
expected to be more subtle (WADI, SWaT). Future work
could compare the GNN-based models to state-of-the-art
non-graph forecasting methods in a benchmark study.
(R2). Next we perform ablation experiments on the GNNbased models to study their behavior when removing their
graph-learning module. For the forecasting modules, we
either provide the ground-truth graph (where known); provide a sparse random graph; or provide no graph. We compare results to the “vanilla” settings of the models, computing the relative change in MAE at horizon 12 in percent.
In Table 1 we show the results for providing the groundtruth graph to the forecasting modules.
Strikingly, MT-
GNN’s performance substantially increases, leading to almost 10% less MAE on METR-LA. On PEMS-BAY and
METR-LA, MTGNN’s results are on par with GTS’s. This
suggests that MTGNN’s forecasting module performs well,
and that GTS’s graph-learning module may be advantageous. GDN also beneﬁts from ground truth, though the
effect is not as pronounced. Interestingly, providing the
“true” graph to GTS leads to a slight performance drop
on all but one datasets, indicating that the model’s graphlearning module is effective at improving forecasting.
In Table 2, we see the results for providing a (sparse) random Erd˝os Renyi graph to the models (left), or completely
disabling the graph processing in the forecasting modules
(right). For the random graphs we set the edge probability p such that the expected degree is 30 (N ≥100), 10
(20 ≤N < 100), or 3 (N < 20). An interesting insight is
that for GTS, using a random graph has little or moderate
effect on most datasets; and that using no graph at all leads
Avg. corr.
Avg. corr. GT
GTS w/ reg.
GTS w/ reg.
Table 3. Average correlation of edge scores across different training runs (left), and with the ground-truth graph (right).
to strong performance drop, indicating that GTS’s forecasting module greatly beneﬁts from the sparsity of graphs.
Remarkably, for MTGNN we see relatively little effect
when using a random graph or even no graph at all. We hypothesize that this is due to MTGNN’s way of constructing
the adjacency matrix. It uses kNN-style approach, which
has sparse gradients. Further, the edge weights are the result of applying tanh to the pairwise scores, which may
lead to vanishing gradients. Thus, the node embeddings
may receive only very little training signal. In contrast,
GDN, which also uses node embeddings in the graph learning module, utilizes the node embeddings also in the forecasting task. This may be a way to address the issue of MT-
GNN. Another approach may be to replace the kNN graph
construction with differentiable sampling via the Gumbel
softmax trick (as in GTS and NRI). This is an interesting experiment to further investigate whether the strategy
of parameterizing the graph based on the time series, employed by NRI and GTS, is generally advantageous over
node-embedding-based approaches.
Finally, we measure how consistent the learned
edge scores are across training runs as well as how similar
the learned adjacency matrices are to the ground truth
adjacency matrices. For this we measure the correlation
of edge scores among re-runs and with the ground-truth
graph. Intuitively, high correlation means that the model
assigns large/small scores to the same node pairs. A subset
of the results is shown in Table 3; see Table 5 (app.) for
more details. We can see that (i) for GDN and GTS, the
learned adjacency matrices tend to be moderately similar
across training runs.
Interestingly, only GDN’s learned
graphs have a nontrivial correlation with the ground truth.
This indicates that the models learn a graph which is useful
for forecasting, which need not have much in common
with the “true” (e.g., spatial) graph. Note that for these
experiments we have disabled GTS’s regularization on
the ground-truth graph. When enabling the loss (GTS w/
reg.) we ﬁnd that, as expected, the learned graphs strongly
correlate with the input graph.
A Study of Joint Graph Inference and Forecasting
5. Conclusion
We present a study of recent models performing joint
graph inference and forecasting. We highlight key commonalities and differences among the architectures.
our experiments, we compare the forecasting results of the
models and study properties of the different graph-learning
modules. For instance, we ﬁnd MTGNN to be insensitive
as to whether the graph-learning module is active or not;
though it greatly beneﬁts from access to a ground-truth
graph. In general, learning a latent graph is a challenging
problem; improvements in terms of expressiveness and
computational efﬁciency could lead to broader applicability. We highlight potential ways of combining the existing
architectures.
A Study of Joint Graph Inference and Forecasting