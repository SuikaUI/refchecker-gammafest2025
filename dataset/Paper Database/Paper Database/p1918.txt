EAD: Elastic-Net Attacks to
Deep Neural Networks via Adversarial Examples
Pin-Yu Chen,1∗Yash Sharma,2∗† Huan Zhang,3† Jinfeng Yi,4‡ Cho-Jui Hsieh,3
1AI Foundations Lab, IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA
2The Cooper Union, New York, NY 10003, USA
3University of California, Davis, Davis, CA 95616, USA
4Tencent AI Lab, Bellevue, WA 98004, USA
 , , ,
 , 
Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually
indistinguishable adversarial image can easily be crafted to
cause a well-trained model to misclassify. Existing methods
for crafting adversarial examples are based on L2 and L∞
distortion metrics. However, despite the fact that L1 distortion
accounts for the total variation and encourages sparsity in the
perturbation, little has been developed for crafting L1-based
adversarial examples.
In this paper, we formulate the process of attacking DNNs via
adversarial examples as an elastic-net regularized optimization
problem. Our elastic-net attacks to DNNs (EAD) feature L1oriented adversarial examples and include the state-of-the-art
L2 attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set
of adversarial examples with small L1 distortion and attains
similar attack performance to the state-of-the-art methods in
different attack scenarios. More importantly, EAD leads to
improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging
L1 distortion in adversarial machine learning and security
implications of DNNs.
Introduction
Deep neural networks (DNNs) achieve state-of-the-art performance in various tasks in machine learning and artiﬁcial
intelligence, such as image classiﬁcation, speech recognition, machine translation and game-playing. Despite their
effectiveness, recent studies have illustrated the vulnerability of DNNs to adversarial examples . For instance, a
carefully designed perturbation to an image can lead a welltrained DNN to misclassify. Even worse, effective adversarial examples can also be made virtually indistinguishable to
human perception. For example, Figure 1 shows three adversarial examples of an ostrich image crafted by our algorithm,
∗Pin-Yu Chen and Yash Sharma contribute equally to this work.
†This work was done during the internship of Yash Sharma and
Huan Zhang at IBM T. J. Watson Research Center.
‡Part of the work was done when Jinfeng Yi was at AI Foundations Lab, IBM T. J. Watson Research Center.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
(b) Adversarial examples with target class labels
Figure 1: Visual illustration of adversarial examples crafted
by EAD (Algorithm 1). The original example is an ostrich
image selected from the ImageNet dataset (Figure 1 (a)). The
adversarial examples in Figure 1 (b) are classiﬁed as the
target class labels by the Inception-v3 model.
which are classiﬁed as “safe”, “shoe shop” and “vacuum” by
the Inception-v3 model , a state-of-theart image classiﬁcation model.
The lack of robustness exhibited by DNNs to adversarial
examples has raised serious concerns for security-critical
applications, including trafﬁc sign identiﬁcation and malware detection, among others. Moreover, moving beyond
the digital space, researchers have shown that these adversarial examples are still effective in the physical world at
fooling DNNs . Due to the robustness and security implications, the means of crafting adversarial examples are called attacks to DNNs. In particular, targeted attacks aim to craft adversarial examples that are misclassi-
ﬁed as speciﬁc target classes, and untargeted attacks aim
to craft adversarial examples that are not classiﬁed as the
original class. Transfer attacks aim to craft adversarial examples that are transferable from one DNN model to another. In addition to evaluating the robustness of DNNs,
adversarial examples can be used to train a robust model
that is resilient to adversarial perturbations, known as adversarial training . They have also
been used in interpreting DNNs .
Throughout this paper, we use adversarial examples to
attack image classiﬁers based on deep convolutional neural
networks. The rationale behind crafting effective adversarial
examples lies in manipulating the prediction results while
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
ensuring similarity to the original image. Speciﬁcally, in
the literature the similarity between original and adversarial
examples has been measured by different distortion metrics. One commonly used distortion metric is the Lq norm,
where ∥x∥q = (p
i=1 |xi|q)1/q denotes the Lq norm of a
p-dimensional vector x = [x1, . . . , xp] for any q ≥1. In
particular, when crafting adversarial examples, the L∞distortion metric is used to evaluate the maximum variation in
pixel value changes ,
while the L2 distortion metric is used to improve the visual
quality . However, despite the
fact that the L1 norm is widely used in problems related to
image denoising and restoration , as well as
sparse recovery , L1-based adversarial examples have not been rigorously explored. In the
context of adversarial examples, L1 distortion accounts for
the total variation in the perturbation and serves as a popular
convex surrogate function of the L0 metric, which measures
the number of modiﬁed pixels (i.e., sparsity) by the perturbation. To bridge this gap, we propose an attack algorithm
based on elastic-net regularization, which we call elasticnet attacks to DNNs (EAD). Elastic-net regularization is a
linear mixture of L1 and L2 penalty functions, and it has
been a standard tool for high-dimensional feature selection
problems . In the context of attacking
DNNs, EAD opens up new research directions since it generalizes the state-of-the-art attack proposed in based on L2 distortion, and is able to craft
L1-oriented adversarial examples that are more effective and
fundamentally different from existing attack methods.
To explore the utility of L1-based adversarial examples
crafted by EAD, we conduct extensive experiments on
MNIST, CIFAR10 and ImageNet in different attack scenarios.
Compared to the state-of-the-art L2 and L∞attacks ,
EAD can attain similar attack success rate when breaking
undefended and defensively distilled DNNs . More importantly, we ﬁnd that L1 attacks attain superior performance over L2 and L∞attacks in transfer attacks
and complement adversarial training. For the most difﬁcult
dataset (MNIST), EAD results in improved attack transferability from an undefended DNN to a defensively distilled
DNN, achieving nearly 99% attack success rate. In addition,
joint adversarial training with L1 and L2 based examples can
further enhance the resilience of DNNs to adversarial perturbations. These results suggest that EAD yields a distinct, yet
more effective, set of adversarial examples. Moreover, evaluating attacks based on L1 distortion provides novel insights
on adversarial machine learning and security implications
of DNNs, suggesting that L1 may complement L2 and L∞
based examples toward furthering a thorough adversarial
machine learning framework.
Related Work
Here we summarize related works on attacking and defending
DNNs against adversarial examples.
Attacks to DNNs
FGM and I-FGM: Let x0 and x denote the original and adversarial examples, respectively, and let t denote the target
class to attack. Fast gradient methods (FGM) use the gradient
∇J of the training loss J with respect to x0 for crafting adversarial examples .
For L∞attacks, x is crafted by
x = x0 −ϵ · sign(∇J(x0, t)),
where ϵ speciﬁes the L∞distortion between x and x0, and
sign(∇J) takes the sign of the gradient. For L1 and L2 attacks, x is crafted by
∥∇J(x0, t)∥q
for q = 1, 2, where ϵ speciﬁes the corresponding distortion. Iterative fast gradient methods (I-FGM) were proposed
in , which iteratively use FGM with a ﬁner distortion, followed by an ϵ-ball
clipping. Untargeted attacks using FGM and I-FGM can be
implemented in a similar fashion.
C&W attack: Instead of leveraging the training loss, Carlini
and Wagner designed an L2-regularized loss function based
on the logit layer representation in DNNs for crafting adversarial examples . Its formulation
turns out to be a special case of our EAD formulation, which
will be discussed in the following section. The C&W attack
is considered to be one of the strongest attacks to DNNs, as it
can successfully break undefended and defensively distilled
DNNs and can attain remarkable attack transferability.
JSMA: Papernot et al. proposed a Jacobian-based saliency
map algorithm (JSMA) for characterizing the input-output
relation of DNNs . It can be viewed as
a greedy attack algorithm that iteratively modiﬁes the most
inﬂuential pixel for crafting adversarial examples.
DeepFool: DeepFool is an untargeted L2 attack algorithm
 based on the
theory of projection to the closest separating hyperplane in
classiﬁcation. It is also used to craft a universal perturbation to mislead DNNs trained on natural images .
Black-box attacks: Crafting adversarial examples in the
black-box case is plausible if one allows querying of the
target DNN. In , JSMA is used to train
a substitute model for transfer attacks. In ,
an effective black-box C&W attack is made possible using
zeroth order optimization (ZOO). In the more stringent attack
scenario where querying is prohibited, ensemble methods
can be used for transfer attacks .
Defenses in DNNs
Defensive distillation: Defensive distillation defends against adversarial perturbations by using the
distillation technique in to
retrain the same network with class probabilities predicted
by the original network. It also introduces the temperature
parameter T in the softmax layer to enhance the robustness
to adversarial perturbations.
Adversarial training: Adversarial training can be implemented in a few different ways. A standard approach is augmenting the original training dataset with the label-corrected
adversarial examples to retrain the network. Modifying the
training loss or the network architecture to increase the robustness of DNNs to adversarial examples has been proposed
in .
Detection methods: Detection methods utilize statistical tests
to differentiate adversarial from benign examples . However, 10 different detection
methods were unable to detect the C&W attack .
EAD: Elastic-Net Attacks to DNNs
Preliminaries on Elastic-Net Regularization
Elastic-net regularization is a widely used technique in solving high-dimensional feature selection problems . It can be viewed as a regularizer that linearly
combines L1 and L2 penalty functions. In general, elastic-net
regularization is used in the following minimization problem:
minimizez∈Z f(z) + λ1∥z∥1 + λ2∥z∥2
where z is a vector of p optimization variables, Z indicates
the set of feasible solutions, f(z) denotes a loss function,
∥z∥q denotes the Lq norm of z, and λ1, λ2 ≥0 are the
L1 and L2 regularization parameters, respectively. The term
λ1∥z∥1 + λ2∥z∥2
2 in (3) is called the elastic-net regularizer
of z. For standard regression problems, the loss function
f(z) is the mean squared error, the vector z represents the
weights (coefﬁcients) on the features, and the set Z = Rp.
In particular, the elastic-net regularization in (3) degenerates
to the LASSO formulation when λ2 = 0, and becomes the
ridge regression formulation when λ1 = 0. It is shown in
 that elastic-net regularization is able to
select a group of highly correlated features, which overcomes
the shortcoming of high-dimensional feature selection when
solely using the LASSO or ridge regression techniques.
EAD Formulation and Generalization
Inspired by the C&W attack ,
we adopt the same loss function f for crafting adversarial
examples. Speciﬁcally, given an image x0 and its correct
label denoted by t0, let x denote the adversarial example
of x0 with a target class t ̸= t0. The loss function f(x) for
targeted attacks is deﬁned as
f(x, t) = max{max
j̸=t [Logit(x)]j −[Logit(x)]t, −κ}, (4)
where Logit(x) = [[Logit(x)]1, . . . , [Logit(x)]K] ∈RK is
the logit layer (the layer prior to the softmax layer) representation of x in the considered DNN, K is the number of
classes for classiﬁcation, and κ ≥0 is a conﬁdence parameter
that guarantees a constant gap between maxj̸=t[Logit(x)]j
and [Logit(x)]t.
It is worth noting that the term [Logit(x)]t is proportional
to the probability of predicting x as label t, since by the
softmax classiﬁcation rule,
Prob(Label(x) = t) =
exp([Logit(x)]t)
j=1 exp([Logit(x)]j)
Consequently, the loss function in (4) aims to render the label
t the most probable class for x, and the parameter κ controls
the separation between t and the next most likely prediction
among all classes other than t. For untargeted attacks, the
loss function in (4) can be modiﬁed as
f(x) = max{[Logit(x)]t0 −max
j̸=t [Logit(x)]j, −κ}.
In this paper, we focus on targeted attacks since they are more
challenging than untargeted attacks. Our EAD algorithm
(Algorithm 1) can directly be applied to untargeted attacks
by replacing f(x, t) in (4) with f(x) in (6).
In addition to manipulating the prediction via the loss
function in (4), introducing elastic-net regularization further
encourages similarity to the original image when crafting
adversarial examples. Our formulation of elastic-net attacks
to DNNs (EAD) for crafting an adversarial example (x, t)
with respect to a labeled natural image (x0, t0) is as follows:
minimizex c · f(x, t) + β∥x −x0∥1 + ∥x −x0∥2
subject to x ∈ p,
where f(x, t) is as deﬁned in (4), c, β ≥0 are the regularization parameters of the loss function f and the L1 penalty,
respectively. The box constraint x ∈ p restricts x to a
properly scaled image space, which can be easily satisﬁed by
dividing each pixel value by the maximum attainable value
(e.g., 255). Upon deﬁning the perturbation of x relative to x0
as δ = x −x0, the EAD formulation in (7) aims to ﬁnd an
adversarial example x that will be classiﬁed as the target class
t while minimizing the distortion in δ in terms of the elasticnet loss β∥δ∥1 + ∥δ∥2
2, which is a linear combination of L1
and L2 distortion metrics between x and x0. Notably, the
formulation of the C&W attack 
becomes a special case of the EAD formulation in (7) when
β = 0, which disregards the L1 penalty on δ. However, the
L1 penalty is an intuitive regularizer for crafting adversarial
examples, as ∥δ∥1 = p
i=1 |δi| represents the total variation of the perturbation, and is also a widely used surrogate
function for promoting sparsity in the perturbation. As will
be evident in the performance evaluation section, including
the L1 penalty for the perturbation indeed yields a distinct
set of adversarial examples, and it leads to improved attack
transferability and complements adversarial learning.
EAD Algorithm
When solving the EAD formulation in (7) without the L1
penalty (i.e., β = 0), Carlini and Wagner used a change-ofvariable (COV) approach via the tanh transformation on x
in order to remove the box constraint x ∈ p . When β > 0, we ﬁnd that the same COV
approach is not effective in solving (7), since the corresponding adversarial examples are insensitive to the changes in β
(see the performance evaluation section for details). Since
the L1 penalty is a non-differentiable, yet piece-wise linear,
function, the failure of the COV approach in solving (7) can
be explained by its inefﬁciency in subgradient-based optimization problems .
To efﬁciently solve the EAD formulation in (7) for crafting adversarial examples, we propose to use the iterative
shrinkage-thresholding algorithm (ISTA) . ISTA can be viewed as a regular ﬁrst-order optimization algorithm with an additional shrinkage-thresholding step
on each iteration. In particular, let g(x) = c·f(x)+∥x−x0∥2
and let ∇g(x) be the numerical gradient of g(x) computed
by the DNN. At the k+1-th iteration, the adversarial example
x(k+1) of x0 is computed by
x(k+1) = Sβ(x(k) −αk∇g(x(k))),
where αk denotes the step size at the k + 1-th iteration, and
Sβ : Rp →Rp is an element-wise projected shrinkagethresholding function, which is deﬁned as
[Sβ(z)]i =
 min{zi −β, 1},
if zi −x0i > β;
if |zi −x0i| ≤β;
max{zi + β, 0},
if zi −x0i < −β,
for any i ∈{1, . . . , p}. If |zi −x0i| > β, it shrinks the
element zi by β and projects the resulting element to the
feasible box constraint between 0 and 1. On the other hand,
if |zi −x0i| ≤β, it thresholds zi by setting [Sβ(z)]i = x0i.
The proof of optimality of using (8) for solving the EAD
formulation in (7) is given in the supplementary material1.
Notably, since g(x) is the attack objective function of the
C&W method , the ISTA operation in (8) can be viewed as a robust version of the C&W
method that shrinks a pixel value of the adversarial example
if the deviation to the original image is greater than β, and
keeps a pixel value unchanged if the deviation is less than β.
Our EAD algorithm for crafting adversarial examples is
summarized in Algorithm 1. For computational efﬁciency, a
fast ISTA (FISTA) for EAD is implemented, which yields the
optimal convergence rate for ﬁrst-order optimization methods
 . The slack vector y(k) in Algorithm
1 incorporates the momentum in x(k) for acceleration. In the
experiments, we set the initial learning rate α0 = 0.01 with a
square-root decay factor in k. During the EAD iterations, the
iterate x(k) is considered as a successful adversarial example
of x0 if the model predicts its most likely class to be the
target class t. The ﬁnal adversarial example x is selected
from all successful examples based on distortion metrics. In
this paper we consider two decision rules for selecting x:
the least elastic-net (EN) and L1 distortions relative to x0.
The inﬂuence of β, κ and the decision rules on EAD will be
investigated in the following section.
Performance Evaluation
In this section, we compare the proposed EAD with the
state-of-the-art attacks to DNNs on three image classiﬁcation
datasets - MNIST, CIFAR10 and ImageNet. We would like
to show that (i) EAD can attain attack performance similar
1 
Algorithm 1 Elastic-Net Attacks to DNNs (EAD)
Input: original labeled image (x0, t0), target attack class
t, attack transferability parameter κ, L1 regularization parameter β, step size αk, # of iterations I
Output: adversarial example x
Initialization: x(0) = y(0) = x0
for k = 0 to I −1 do
x(k+1) = Sβ(y(k) −αk∇g(y(k)))
y(k+1) = x(k+1) +
k+3(x(k+1) −x(k))
Decision rule: determine x from successful examples in
k=1 (EN rule or L1 rule).
to the C&W attack in breaking undefended and defensively
distilled DNNs, since the C&W attack is a special case of
EAD when β = 0; (ii) Comparing to existing L1-based FGM
and I-FGM methods, the adversarial examples using EAD
can lead to signiﬁcantly lower L1 distortion and better attack success rate; (iii) The L1-based adversarial examples
crafted by EAD can achieve improved attack transferability
and complement adversarial training.
Comparative Methods
We compare EAD with the following targeted attacks, which
are the most effective methods for crafting adversarial examples in different distortion metrics.
C&W attack: The state-of-the-art L2 targeted attack proposed by Carlini and Wagner ,
which is a special case of EAD when β = 0.
FGM: The fast gradient method proposed in . The FGM attacks using different
distortion metrics are denoted by FGM-L1, FGM-L2 and
I-FGM: The iterative fast gradient method proposed in . The I-FGM attacks
using different distortion metrics are denoted by I-FGM-L1,
I-FGM-L2 and I-FGM-L∞.
Experiment Setup and Parameter Setting
Our experiment setup is based on Carlini and Wagner’s framework2. For both the EAD and C&W attacks, we use the default setting1, which implements 9 binary search steps on
the regularization parameter c (starting from 0.001) and runs
I = 1000 iterations for each step with the initial learning rate
α0 = 0.01. For ﬁnding successful adversarial examples, we
use the reference optimizer1 (ADAM) for the C&W attack
and implement the projected FISTA (Algorithm 1) with the
square-root decaying learning rate for EAD. Similar to the
C&W attack, the ﬁnal adversarial example of EAD is selected by the least distorted example among all the successful
examples. The sensitivity analysis of the L1 parameter β and
the effect of the decision rule on EAD will be investigated
in the forthcoming paragraph. Unless speciﬁed, we set the
attack transferability parameter κ = 0 for both attacks.
2 robust attacks
Table 1: Comparison of the change-of-variable (COV) approach and EAD (Algorithm 1) for solving the elastic-net formulation
in (7) on MNIST. ASR means attack success rate (%). Although these two methods attain similar attack success rates, COV is
not effective in crafting L1-based adversarial examples. Increasing β leads to less L1-distorted adversarial examples for EAD,
whereas the distortion of COV is insensitive to changes in β.
Average case
Worst case
Optimization
We implemented FGM and I-FGM using the CleverHans
package3. The best distortion parameter ϵ is determined by
a ﬁne-grained grid search - for each image, the smallest ϵ in
the grid leading to a successful attack is reported. For I-FGM,
we perform 10 FGM iterations (the default value) with ϵ-ball
clipping. The distortion parameter ϵ′ in each FGM iteration
is set to be ϵ/10, which has been shown to be an effective
attack setting in . The range of the grid
and the resolution of these two methods are speciﬁed in the
supplementary material1.
The image classiﬁers for MNIST and CIFAR10 are trained
based on the DNN models provided by Carlini and Wagner1. The image classiﬁer for ImageNet is the Inception-v3
model . For MNIST and CIFAR10, 1000
correctly classiﬁed images are randomly selected from the
test sets to attack an incorrect class label. For ImageNet,
100 correctly classiﬁed images and 9 incorrect classes are
randomly selected to attack. All experiments are conducted
on a machine with an Intel E5-2690 v3 CPU, 40 GB RAM
and a single NVIDIA K80 GPU. Our EAD code is publicly
available for download4.
Evaluation Metrics
Following the attack evaluation criterion in , we report the attack success rate and distortion
of the adversarial examples from each method. The attack
success rate (ASR) is deﬁned as the percentage of adversarial examples that are classiﬁed as the target class (which is
different from the original class). The average L1, L2 and
L∞distortion metrics of successful adversarial examples are
also reported. In particular, the ASR and distortion of the
following attack settings are considered:
Best case: The least difﬁcult attack among targeted attacks
to all incorrect class labels in terms of distortion.
Average case: The targeted attack to a randomly selected
incorrect class label.
3 
4 
Worst case: The most difﬁcult attack among targeted attacks
to all incorrect class labels in terms of distortion.
Sensitivity Analysis and Decision Rule for EAD
We verify the necessity of using Algorithm 1 for solving the
elastic-net regularized attack formulation in (7) by comparing
it to a naive change-of-variable (COV) approach. In , Carlini and Wagner remove the box
constraint x ∈ p by replacing x with 1+tanh w
w ∈Rp and 1 ∈Rp is a vector of ones. The default ADAM
optimizer is then used to solve w and
obtain x. We apply this COV approach to (7) and compare
with EAD on MNIST with different orders of the L1 regularization parameter β in Table 1. Although COV and EAD
attain similar attack success rates, it is observed that COV
is not effective in crafting L1-based adversarial examples.
Increasing β leads to less L1-distorted adversarial examples
for EAD, whereas the distortion (L1, L2 and L∞) of COV
is insensitive to changes in β. Similar insensitivity of COV
on β is observed when one uses other optimizers such as
AdaGrad, RMSProp or built-in SGD in TensorFlow. We also
note that the COV approach prohibits the use of ISTA due
to the subsequent tanh term in the L1 penalty. The insensitivity of COV suggests that it is inadequate for elastic-net
optimization, which can be explained by its inefﬁciency in
subgradient-based optimization problems . For EAD, we also ﬁnd an interesting trade-off between
L1 and the other two distortion metrics - adversarial examples with smaller L1 distortion tend to have larger L2 and
L∞distortions. This trade-off can be explained by the fact
that increasing β further encourages sparsity in the perturbation, and hence results in increased L2 and L∞distortion.
Similar results are observed on CIFAR10 (see supplementary
material1).
In Table 1, during the attack optimization process the ﬁnal
adversarial example is selected based on the elastic-net loss of
all successful adversarial examples in {x(k)}I
k=1, which we
call the elastic-net (EN) decision rule. Alternatively, we can
Figure 2: Comparison of EN and L1 decision rules in EAD on MNIST with varying L1 regularization parameter β (average
case). Comparing to the EN rule, for the same β the L1 rule attains less L1 distortion but may incur more L2 and L∞distortions.
Table 2: Comparison of different attacks on MNIST, CIFAR10 and ImageNet (average case). ASR means attack success rate (%).
The distortion metrics are averaged over successful examples. EAD, the C&W attack, and I-FGM-L∞attain the least L1, L2,
and L∞distorted adversarial examples, respectively. The complete attack results are given in the supplementary material1.
Attack method
EAD (EN rule)
EAD (L1 rule)
select the ﬁnal adversarial example with the least L1 distortion, which we call the L1 decision rule. Figure 2 compares
the ASR and average-case distortion of these two decision
rules with different β on MNIST. Both decision rules yield
100% ASR for a wide range of β values. For the same β,
the L1 rule gives adversarial examples with less L1 distortion than those given by the EN rule at the price of larger
L2 and L∞distortions. Similar trends are observed on CI-
FAR10 (see supplementary material1). The complete results
of these two rules on MNIST and CIFAR10 are given in the
supplementary material1. In the following experiments, we
will report the results of EAD with these two decision rules
and set β = 10−3, since on MNIST and CIFAR10 this β
value signiﬁcantly reduces the L1 distortion while having
comparable L2 and L∞distortions to the case of β = 0 (i.e.,
without L1 regularization).
Attack Success Rate and Distortion on MNIST,
CIFAR10 and ImageNet
We compare EAD with the comparative methods in terms
of attack success rate and different distortion metrics on attacking the considered DNNs trained on MNIST, CIFAR10
and ImageNet. Table 2 summarizes their average-case performance. It is observed that FGM methods fail to yield successful adversarial examples (i.e., low ASR), and the corresponding distortion metrics are signiﬁcantly larger than other
methods. On the other hand, the C&W attack, I-FGM and
EAD all lead to 100% attack success rate. Furthermore, EAD,
the C&W method, and I-FGM-L∞attain the least L1, L2,
and L∞distorted adversarial examples, respectively. We note
that EAD signiﬁcantly outperforms the existing L1-based
method (I-FGM-L1). Compared to I-FGM-L1, EAD with the
EN decision rule reduces the L1 distortion by roughly 47%
on MNIST, 53% on CIFAR10 and 87% on ImageNet. We
also observe that EAD with the L1 decision rule can further
reduce the L1 distortion but at the price of noticeable increase
in the L2 and L∞distortion metrics.
Notably, despite having large L2 and L∞distortion metrics, the adversarial examples crafted by EAD with the L1
rule can still attain 100% ASRs in all datasets, which implies
the L2 and L∞distortion metrics are insufﬁcient for evaluating the robustness of neural networks. Moreover, the attack
results in Table 2 suggest that EAD can yield a set of distinct
adversarial examples that are fundamentally different from
L2 or L∞based examples. Similar to the C&W method and
I-FGM, the adversarial examples from EAD are also visually
indistinguishable (see supplementary material1).
Breaking Defensive Distillation
In addition to breaking undefended DNNs via adversarial
examples, here we show that EAD can also break defensively
distilled DNNs. Defensive distillation 
is a standard defense technique that retrains the network with
class label probabilities predicted by the original network,
soft labels, and introduces the temperature parameter T in
the softmax layer to enhance its robustness to adversarial
perturbations. Similar to the state-of-the-art attack (the C&W
method), Figure 3 shows that EAD can attain 100% attack
Figure 3: Attack success rate (average case) of the C&W
method and EAD on MNIST and CIFAR10 with respect to
varying temperature parameter T for defensive distillation.
Both methods can successfully break defensive distillation.
success rate for different values of T on MNIST and CI-
FAR10. Moreover, since the C&W attack formulation is a
special case of the EAD formulation in (7) when β = 0,
successfully breaking defensive distillation using EAD suggests new ways of crafting effective adversarial examples
by varying the L1 regularization parameter β. The complete
attack results are given in the supplementary material1.
Improved Attack Transferability
It has been shown in that the
C&W attack can be made highly transferable from an undefended network to a defensively distilled network by tuning
the conﬁdence parameter κ in (4). Following , we adopt the same experiment setting for
attack transferability on MNIST, as MNIST is the most dif-
ﬁcult dataset to attack in terms of the average distortion per
image pixel from Table 2.
Fixing κ, adversarial examples generated from the original (undefended) network are used to attack the defensively
distilled network with the temperature parameter T = 100
 . The attack success rate (ASR) of
EAD, the C&W method and I-FGM are shown in Figure 4.
When κ = 0, all methods attain low ASR and hence do not
produce transferable adversarial examples. The ASR of EAD
and the C&W method improves when we set κ > 0, whereas
I-FGM’s ASR remains low (less than 2%) since the attack
does not have such a parameter for transferability.
Notably, EAD can attain nearly 99% ASR when κ = 50,
whereas the top ASR of the C&W method is nearly 88% when
κ = 40. This implies improved attack transferability when
using the adversarial examples crafted by EAD, which can be
explained by the fact that the ISTA operation in (8) is a robust
version of the C&W attack via shrinking and thresholding.
We also ﬁnd that setting κ too large may mitigate the ASR
of transfer attacks for both EAD and the C&W method, as
the optimizer may fail to ﬁnd an adversarial example that
minimizes the loss function f in (4) for large κ. The complete
attack transferability results are given in the supplementary
material1.
Complementing Adversarial Training
To further validate the difference between L1-based and L2based adversarial examples, we test their performance in
Figure 4: Attack transferability (average case) from the undefended network to the defensively distilled network on
MNIST by varying κ. EAD can attain nearly 99% attack
success rate (ASR) when κ = 50, whereas the top ASR of
the C&W attack is nearly 88% when κ = 40.
Table 3: Adversarial training using the C&W attack and EAD
(L1 rule) on MNIST. ASR means attack success rate. Incorporating L1 examples complements adversarial training
and enhances attack difﬁculty in terms of distortion. The
complete results are given in the supplementary material1.
Adversarial
Average case
adversarial training on MNIST. We randomly select 1000 images from the training set and use the C&W attack and EAD
(L1 rule) to generate adversarial examples for all incorrect
labels, leading to 9000 adversarial examples in total for each
method. We then separately augment the original training
set with these examples to retrain the network and test its
robustness on the testing set, as summarized in Table 3. For
adversarial training with any single method, although both
attacks still attain a 100% success rate in the average case,
the network is more tolerable to adversarial perturbations, as
all distortion metrics increase signiﬁcantly when compared to
the null case. We also observe that joint adversarial training
with EAD and the C&W method can further increase the L1
and L2 distortions against the C&W attack and the L2 distortion against EAD, suggesting that the L1-based examples
crafted by EAD can complement adversarial training.
Conclusion
We proposed an elastic-net regularized attack framework for
crafting adversarial examples to attack deep neural networks.
Experimental results on MNIST, CIFAR10 and ImageNet
show that the L1-based adversarial examples crafted by EAD
can be as successful as the state-of-the-art L2 and L∞attacks
in breaking undefended and defensively distilled networks.
Furthermore, EAD can improve attack transferability and
complement adversarial training. Our results corroborate the
effectiveness of EAD and shed new light on the use of L1based adversarial examples toward adversarial learning and
security implications of deep neural networks.
Acknowledgment Cho-Jui Hsieh and Huan Zhang acknowledge the support of NSF via IIS-1719097.