IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Global Contrast based Salient Region Detection
Ming-Ming Cheng, Niloy J. Mitra, Xiaolei Huang, Philip H. S. Torr, and Shi-Min Hu
Abstract—Automatic estimation of salient object regions across images, without any prior assumption or knowledge of the contents
of the corresponding scenes, enhances many computer vision and computer graphics applications. We introduce a regional contrast
based salient object detection algorithm, which simultaneously evaluates global contrast differences and spatial weighted coherence
scores. The proposed algorithm is simple, efﬁcient, naturally multi-scale, and produces full-resolution, high-quality saliency maps.
These saliency maps are further used to initialize a novel iterative version of GrabCut, namely SaliencyCut, for high quality unsupervised
salient object segmentation. We extensively evaluated our algorithm using traditional salient object detection datasets, as well as a
more challenging Internet image dataset. Our experimental results demonstrate that our algorithm consistently outperforms 15 existing
salient object detection and segmentation methods, yielding higher precision and better recall rates. We also show that our algorithm
can be used to efﬁciently extract salient object masks from Internet images, enabling effective sketch-based image retrieval (SBIR) via
simple shape comparisons. Despite such noisy internet images, where the saliency regions are ambiguous, our saliency guided image
retrieval achieves a superior retrieval rate compared with state-of-the-art SBIR methods, and additionally provides important target
object region information.
Index Terms—Salient object detection, visual attention, saliency map, unsupervised segmentation, image retrieval.
INTRODUCTION
E, as humans, are experts at quickly and accurately identifying the most visually noticeable
foreground object in the scene, known as salient objects,
and adaptively focus our attention on such perceived
important regions. In contrast, computationally identifying such salient object regions , , that match
the human annotators’ behaviour when they have been
asked to pick a salient object in an image, is very
challenging. Being able to automatically, efﬁciently, and
accurately estimate salient object regions, however, is
highly desirable given the immediate ability to characterise the spatial support for feature extraction, isolate
the object from potentially confusing background, and
preferentially allocate ﬁnite computational resources for
subsequent image processing.
While essentially solving a segmentation problem,
salient object detection approaches segment only the
salient foreground object from the background, rather
than partition an image into regions of coherent properties as in general segmentation algorithms . Salient
object detection models also differ from eye ﬁxation
prediction models that predict a few ﬁxation points in
an image rather than uniformly highlighting the entire
salient object region . In practice, salient object detection methods are commonly used as a ﬁrst step of
many graphics/vision applications including object-ofinterest image segmentation , object recognition ,
• M.M. Cheng is with Oxford University. N.J. Mitra is with UCL. X.
Huang is with Lehigh University. P.H.S. Torr is with Oxford University.
Shi-Min Hu (corresponding author) is with TNList, Tsinghua University.
• A preliminary version of this work appeared at CVPR . The C++ source
code, benchmark dataset, results, highlighted applications and FAQs are
available via our project page: The major part
of this work was done in Tsinghua University.
Fig. 1. Given an input image (top), a global contrast
analysis is used to compute a high resolution saliency
map (middle), which can be used to produce an unsupervised segmentation mask (bottom) for an object of
adaptive compression of images , content-aware image editing , , image retrieval – , etc.
Although extraction of salient objects in a scene is
related to accurate image segmentation and object retrieval, interestingly, reliable saliency estimation is often feasible without any actual scene understanding.
Saliency, as widely believed, is a bottom-up process that
originates from visual distinctness, rarity, or surprise
and is often attributed to variations in image attributes
such as color, gradient, edges, and boundaries .
Visual saliency, being closely related to our perception
and processing of visual stimuli, is investigated across
many disciplines including cognitive psychology ,
 , neurobiology , , and computer vision –
 . Based on our observed reaction times and estimated
signal transmission times along biological pathways,
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
human attention theories hypothesize that the human
vision system processes only parts of an image in detail,
while leaving the rest nearly unprocessed. Early work
by Treisman and Gelade , Koch and Ullman ,
and subsequent attention theories proposed by Itti, Wolfe
and others, suggest two stages of visual attention: a fast,
pre-attentive, bottom-up, data driven saliency extraction;
followed by a slower, task dependent, top-down, goal
driven saliency extraction.
We focus on bottom-up data driven salient object
detection using image contrast (see Fig. 1) under the
assumption that a salient object exists in an image .
The proposed method is simple, fast, and produces high
quality results on benchmark datasets. Motivated by the
popular belief that human cortical cells may be hard wired
to preferentially respond to high contrast stimulus in
their receptive ﬁelds , we propose contrast analysis
for extracting high-resolution, full-ﬁeld saliency maps
based on the following considerations:
• A global contrast based method, which separates
a large-scale object from its surroundings, is desirable over local contrast based methods producing
high saliency values at or near object boundaries.
Global considerations enable assignment of comparable saliency values across similar image regions,
and can uniformly highlight entire objects.
• Saliency of a region mainly depends on its contrast
with respect to its nearby regions, while contrasts to
distant regions are less signiﬁcant (see also ).
• In man-made photographs, object are often concentrated towards the inner regions of the images, away
from image boundaries (see ).
• Saliency maps should be fast, accurate, have low
memory footprints, and easy to generate to allow
processing of large image collections, and facilitate
efﬁcient image classiﬁcation and retrieval.
We propose a histogram-based contrast method (HC) to
measure saliency. HC-maps assign pixel-wise saliency
values based simply on color separation from all other
image pixels to produce full resolution saliency maps.
We use a histogram-based approach for efﬁcient processing, while employing a smoothing procedure to reduce
quantization artifacts.
As an improvement over HC-maps, we incorporate
spatial relations to produce region-based contrast (RC)
maps where we ﬁrst segment the input image into
regions, and then assign saliency values to them. The
saliency value of a region is then calculated using a
global contrast score, measured by the region’s contrast
and spatial distances to other regions in the image. Note
that this approach better acknowledges the relation between image segmentation and saliency determination.
Segmenting regions of interest in still images is of
great practical importance in many computer vision
and graphics applications. Researchers have devoted
signiﬁcant efforts to minimize user interaction during
this process. GrabCut , which iteratively optimizes
the energy function and considers both texture and
edge information, has successfully simpliﬁed the user
interaction to simply dragging a rectangle around the
desired object. We propose SaliencyCut, an improved
iterative version of GrabCut, and combine it with our
saliency detection method to achieve superior performance compared to state-of-the-art unsupervised salient
object extraction methods.
In order to evaluate the proposed algorithms and
compare with state-of-the-art alternatives, we build
a database with 10,000 pixel-accurate human-labeled
ground truth images (see also Sec. 6.1.1), which is an
order of magnitude bigger than previous largest public
available dataset of its kind . We have extensively
evaluated our methods on this dataset, and compared
our methods with 15 state-of-the-art saliency methods
as well as with manually created ground truth annotations. The experiments show signiﬁcant improvements
over previous methods both in terms of precision and
recall rates. Overall, compared with HC-maps, RC-maps
produce better precision and recall rates, but at the cost
of increased computational overheard. In our extensive
empirical evaluations, we observe that the unsupervised segmentation results produced by our SaliencyCut
method are, in most cases, are comparable to the manually annotated ground truths. We also demonstrate applications of the extracted saliency maps to segmentation
and sketch-based image retrieval.
RELATED WORK
Our work belongs to the active research ﬁeld of visual
attention modeling, for which a comprehensive discussion is beyond the scope of this paper. We refer readers
to recent survey papers for a detailed discussion of 65
models , as well as quantitative analysis of different
methods in the two major research directions: ﬁxation
prediction , and salient object detection .
We focus on relevant literature targeting pre-attentive
bottom-up saliency region detection, which are biologically motivated, or purely computational, or involve
both aspects. Such methods utilize low-level processing
to determine the contrast of image regions to their surroundings, and use feature attributes such as intensity,
color, and edges . We broadly classify the algorithms
into local and global schemes. Note that the classiﬁcation
is not strict as some of the research efforts can be listed
under both categories.
Local contrast based methods investigate the rarity
of image regions with respect to (small) local neighborhoods. Based on the highly inﬂuential biologically inspired early representation model introduced by Koch and
Ullman , Itti et al. deﬁne image saliency using
central-surrounded differences across multi-scale image
features. Ma and Zhang propose an alternate local
contrast analysis for generating saliency maps, which
is then extended using a fuzzy growth model. Harel
et al. propose a bottom-up visual saliency model
to normalize the feature maps of Itti et al. to highlight
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
(a) original
(b) IT 
(c) AIM 
(d) IM 
(e) MSS 
(f) SEG 
(g) SeR 
(h) SUN 
(i) SWD 
(j) GB 
(k) SR 
(l) AC 
(m) CA 
(n) FT 
(o) LC 
(p) CB 
Fig. 2. Saliency maps computed by different state-of-the-art methods (b-p), and with our proposed HC (q) and RC
methods (r). Most results highlight edges, or are of low resolution. See also Fig. 9 and our project webpage.
conspicuous parts and permit combination with other
importance maps. The model is simple, biologically
plausible, and easy to parallelize. Liu et al. ﬁnd
multi-scale contrast by linearly combining contrast in
a Gaussian image pyramid. More recently, Goferman
et al. simultaneously model local low-level clues,
global considerations, visual organization rules, and
high-level features to highlight salient objects along with
their contexts. Such methods using local contrast tend
to produce higher saliency values near edges instead
of uniformly highlighting salient objects (see Fig. 2).
Note that Reinagel et al. observe that humans tend
to focus attention in image regions with high spatial
contrast and local variance in pixel correlation.
Global contrast based methods evaluate saliency of an
image region using its contrast with respect to the entire
image. Zhai and Shah deﬁne pixel-level saliency
based on a pixel’s contrast to all other pixels. However, for efﬁciency they use only luminance information,
thus ignoring distinctiveness clues in other channels.
Achanta et al. propose a frequency tuned method
that directly deﬁnes pixel saliency using a pixel’s color
difference from the average image color. The elegant approach, however, only considers ﬁrst order average color,
which can be insufﬁcient to analyze complex variations
common in natural images. In Figures 9 and 12, we
show qualitative and quantitative weaknesses of such
approaches. Furthermore, these methods ignore spatial
relationships across image parts, which can be critical
for reliable and coherent saliency detection (see Sec. 6).
Saliency maps are widely employed for unsupervised
object segmentation: Ma and Zhang ﬁnd rectangular
salient regions by fuzzy region growing on their saliency
maps. Ko and Nam select salient regions using
a support vector machine trained on image segment
features, and then cluster these regions to extract salient
objects. Han et al. model color, texture, and edge
features in a Markov random ﬁeld framework to grow
salient object regions from seed values in the saliency
maps. More recently, Achanta et al. average saliency
values within image segments produced by mean-shift
segmentation, and then ﬁnd salient objects via adaptive
thresholding. We propose a different approach that extends GrabCut method and automatically initialize
it using our saliency detection methods. Experiments on
our 10, 000 images dataset (see Sec. 6.1.1) demonstrate
the signiﬁcant advantages of our method compared to
other state-of-the-art methods.
Subsequent to our preliminary results , Jiang et
al. propose a comparable method also making use of
region level contrast to model image saliency. In the segmentation step, their method also expands and shrinks
the initial trimap and iteratively applies graphcut and
histogram appearance model. Since GrabCut is an iterative process of using graphcut and GMM appearance
mode, the two segmentation methods share a strong similarity. Compared to the CB method , experimental
results show that our RC salient object region detection
and segmentation is more accurate (Fig. 12(a)(c)), 20×
faster (Fig. 7), and more robust to center-bias (Fig. 12(b)).
HISTOGRAM BASED CONTRAST
Our biological vision system is highly sensitive to contrast in visual signal. Based on this observation, we propose a histogram-based contrast (HC) method to deﬁne
saliency values for image pixels using color statistics of
the input image. Speciﬁcally, the saliency of a pixel is
deﬁned using its color contrast to all other pixels in the
image, i.e., the saliency value of a pixel Ik in image I is,
∀Ii∈I D(Ik, Ii),
where D(Ik, Ii) is the color distance metric between pixels Ik and Ii in the L∗a∗b∗space for perceptual accuracy.
(1) can be expanded by pixel order as,
S(Ik) = D(Ik, I1) + D(Ik, I2) + · · · + D(Ik, IN),
where N is the number of pixels in image I. It is
easy to see that pixels with the same color have the
same saliency under this deﬁnition, since the measure is
oblivious to spatial relations. Thus, rearranging (2) such
that the terms with the same color value cj are grouped
together, we get saliency value for each color as,
S(Ik) = S(cl) =
j=1 fjD(cl, cj),
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 3. Given an input image (left), we compute its color
histogram (middle). Corresponding histogram bin colors
are shown in the lower bar. The quantized image (right)
uses only 43 histogram bin colors and still retains sufﬁcient visual quality for saliency detection.
where cl is the color value of pixel Ik, n is the number
of distinct pixel colors, and fj is the probability of pixel
color cj in image I.
Histogram based speed up
Naively evaluating the saliency value for each image
pixel using (1) takes O(N 2) time, which is computationally too expensive even for medium sized images. The
equivalent representation in (3), however, takes O(N) +
O(n2) time, implying that computational efﬁciency can
be improved to O(N) if O(n2) ≤O(N). Thus, the key
to speed up is to reduce the number of pixel colors in
the image. However, the true-color space contains 2563
possible colors, which is typically larger than the number
of image pixels.
Zhai and Shah reduce the number of colors, n, by
only using luminance. In this way, n2 = 2562 (typically
2562 ≪N). The method, however, ignores distinctiveness of color information. In this work, we use the full
color space instead of luminance only. To reduce the
number of colors needed to consider, we ﬁrst quantize
each color channel to have 12 different values, which
reduces the number of colors to 123 = 1728. Considering
that color in a natural image typically covers only a small
portion of the full color space, we further reduce the
number of colors by ignoring less frequently occurring
colors. By choosing more frequently occurring colors and
ensuring these colors cover the colors of more than 95%
of the image pixels, we typically are left with around
n = 85 colors (see Sec. 6 for experimental details).
The colors of the remaining pixels, which comprise
fewer than 5% of the image pixels, are replaced by the
closest colors in the histogram. A typical example of
such quantization is shown in Fig. 3. Note that due to
efﬁciency considerations we select the simple histogram
based quantization instead of optimizing for an image
speciﬁc color palette.
Color space smoothing
Although we can efﬁciently compute color contrast by
building a compact color histogram using color quantization and choosing more frequent colors, the quantization itself may introduce artifacts. Some similar colors
may be quantized to different values. In order to reduce
Fig. 4. Saliency of each color before (left) and after (right)
color space smoothing. Corresponding saliency maps are
shown in the respective insets.
noisy saliency results caused by such randomness, we
use a smoothing procedure to reﬁne the saliency value
for each color. We replace the saliency value of each color
by the weighted average of the saliency values of similar
colors.This is actually a smoothing process in the color
feature space. We choose m = n/4 nearest colors to reﬁne
the saliency value of color c by,
i=1(T −D(c, ci))S(ci)
where T = Pm
i=1 D(c, ci) is the sum of distances between
color c and its m nearest neighbors ci, and the normalization factor comes from Pm
i=1(T −D(c, ci)) = (m −1)T.
Note that we use a linearly-varying smoothing weight
(T −D(c, ci)) to assign larger weights to colors closer
to c in the color feature space. In our experiments, we
found that such linearly-varying weights are better than
Gaussian weights, which fall off too sharply. Fig. 4 shows
the typical effect of color space smoothing with the
corresponding histograms sorted by decreasing saliency
values. Note that similar histogram bins are closer to
each other after such smoothing, indicating that similar
colors is more likely to be assigned similar saliency
values, thus reducing quantization artifacts (see Fig. 12).
Implementation details
To quantize the color space into 123 different colors, we
uniformly divide each color channel into 12 levels. While
the quantization of colors is performed in the RGB color
space, we measure color differences in the L∗a∗b∗color
space given its perceptual accuracy. We do not, however,
perform quantization directly in the L∗a∗b∗color space
since not all colors in the range L∗∈ , and
a∗, b∗∈[−127, 127] necessarily correspond to real colors.
Experimentally we observed worse quantization artifacts
using direct L∗a∗b∗color space quantization. Best results
were obtained by quantization in the RGB space while
measuring distance in the L∗a∗b∗color space, as opposed
to performing both quantization and distance calculation
in a single color space, either RGB or L∗a∗b∗.
REGION BASED CONTRAST
Humans pay more attention to image regions with high
contrast to their surroundings . Besides contrast,
spatial relationships are important in human attention.
High contrast to ones surrounding regions is usually
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 5. Region based contrast computation: (a) input image, (b) image regions generated by Felzenszwalb and
Huttenlocher’s segmentation method , (c) region contrast without distance weighting and spatial prior ((5)),
(d) region contrast with distance weighting, (e) region contrast further considering spatial prior ((7)), (f) region contrast
after improvement by border region estimation and color space smoothing, (g) using our SaliencyCut (Sec. 5), we get
a high quality cut that is comparable to human labeled ground truth.
stronger evidence for saliency of a region than comparable contrast to far-away regions. Since directly introducing spatial relationships when computing pixel-level
contrast is computationally expensive, we introduce a
contrast analysis method, region contrast (RC), so as to
integrate spatial relationships into region-level contrast
computation. In RC, we ﬁrst segment the input image
into regions, then compute color contrast at the region
level, and ﬁnally deﬁne saliency for each region as
the weighted sum of the region’s contrasts to all other
regions in the image. The weights are set according to
the spatial distances with farther regions being assigned
smaller weights.
Region contrast by histogram comparison
We ﬁrst segment the input image into regions using a
graph-based image segmentation method . Then we
build the color histogram for each region as in Sec. 3. For
a region rk, we compute its saliency value by measuring
its color contrast to all other regions in the image,
rk̸=ri w(ri)Dr(rk, ri),
where w(ri) is the weight of region ri and Dr(·, ·) is
the color distance metric between the two regions. We
weight the distances by the number of pixels in ri as
w(ri) to emphasize color contrast to bigger regions. The
color distance between two regions r1 and r2 is,
Dr(r1, r2) =
j=1 f(c1,i)f(c2,j)D(c1,i, c2,j)
where f(ck,i) is the probability of the i-th color ck,i
among all nk colors in the k-th region rk, k = {1, 2}. Note
that we use the probability of a color in the probability
density function (i.e., normalized color histogram) of the
region as the weight for this color to further emphasize
the color differences between dominant colors.
Storing and calculating the regular matrix format histogram for each region is inefﬁcient since each region
typically contains a small number of colors in the color
histogram of the whole image. Instead, we use a sparse
histogram representation for efﬁcient computation.
Spatially weighted region contrast
We further incorporate spatial information by introducing a spatial weighting term in (5) to increase the effects
of closer regions and decrease the effects of farther
regions. Speciﬁcally, for any region rk, the spatially
weighted region contrast based saliency is:
S(rk) = ws(rk)
w(ri)Dr(rk, ri)
where Ds(rk, ri) is the spatial distance between regions
rk and ri, σs controls the strength of spatial distance
weighting, w(ri) is the weight of region ri deﬁned by
the number of pixels in ri, and ws(rk) is a spatial prior
weighting term similar to center bias (CB ). We use
ws(rk) = exp(−9d2
k), where dk is the average distance
between pixels in region rk and the center of the image,
with pixel coordinates normalized to . Thus, ws(rk)
gives a high value if region rk is close to the center
of the image and it gives a low value if the region is
a border region away from the center.
For σs, larger
values of σs reduce the effect of spatial weighting so
that contrast to farther regions would contribute more
to the saliency of the current region. The spatial distance
between two regions is deﬁned as the Euclidean distance
between their centroids. In our implementation, we use
s = 0.4 with pixel coordinates normalized to .
Further improvement of RC saliency maps
We further reﬁne our RC saliency maps in two steps.
First, we use the spatial prior to explicitly estimate the
non-salient (background) region. Second, we apply the
color space smoothing as described in Sec. 3.2.
We observe that regions with long borders overlapping with image borders are typically non-salient
background regions, which we call border regions. We
incorporate them as another spatial prior (ws(·) in (7))
to detect non-salient regions. In our implementation, we
normalize the number of pixels located in the 15 pixelwide image-border area by the region size, and consider
regions with this value higher than a threshold to be
border regions. In practice, this hard constraint improves
both the saliency maps as well as the convergence speed
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 6. Demonstration of SaliencyCut: (a) original image, (b) initial segmentation by ﬁxed thresholding the saliency map,
(c) trimap after ﬁrst iteration, (d) trimap after second iteration, (e) ﬁnal segmentation, and (f) manually labeled ground
truth. In the segmented images (e), blue is foreground, gray is background, while in the trimaps (b–d), foreground is
red, background is green, and unknown regions are left unchanged.
of SaliencyCut (Sec. 5) by improving the initial condition.
Our border region estimation aims at high precision,
rather than high recall. A strict ﬁxed threshold, which
on average corresponds to 2% miss alarm rate in our
dataset, is chosen to detect border regions.
In order to uniformly highlight the entire saliency
region of the image, we get the average saliency of each
color in the color histogram and adopt the color space
smoothing (Sec. 3.2) to improve our RC saliency map.
After smoothing, some border region pixels may get
non-zero saliency values. We reset the saliency of border
region to zero and re-estimate the saliency of each region
as the average saliency value of its corresponding pixels.
Since initial RC maps are typically more uniformly highlighted compared to HC saliency maps without color
space smoothing, we typically choose smaller number
of nearest colors (m = n/10 in this part). Fig. 5(f)
demonstrates such an example. The jumping man region
is more uniformly highlighted compared to Fig. 5(e).
SALIENCYCUT: AUTOMATIC SALIENT RE-
GION EXTRACTION
In a highly inﬂuential work, GrabCut made critical
changes to the graphcut formulation to allow processing
of noisy initialization. This enabled users to roughly
annotate (e.g., using a rectangle) a region of interest, and
then use GrabCut to extract a precise image mask. Using
our estimated saliency masks, we remove even the need
for user annotated rectangular regions. In this section,
we introduce SaliencyCut, which uses the computed
saliency map to assist in automatic salient object segmentation. This immediately enables automatic analysis
of large internet image repositories. Speciﬁcally, we make
two enhancements to GrabCut : “iterative reﬁne” and
“adaptive ﬁtting”, which together handle considerably
more noisy initializations. Thanks to the robustness of
the new approach, we are able to automatically initialize
the segmentation according to the detected saliency map.
Algorithm initialization
Instead of manually selecting a rectangular region to
initialize the process, as in classical GrabCut, we automatically initialize using a segmentation obtained by
binarizing the saliency map using a ﬁxed threshold Tb.
Similar to GrabCut, we use incomplete trimap for the
initialization. For image pixels with saliency value bigger
than Tb, the largest connected region is considered as
initial candidate region of the most dominate salient
object. This candidate region is labeled as unknown
part of the trimap, while other regions are labeled as
background. Note that we do not initialize any hard
foreground labeling. These unknown regions are initially
used to train foreground color models thus helps the
algorithm to identify the foreground pixels.
Since the initial background regions are retained while
other regions may be changed during the GrabCut optimization, we give preference to conﬁdent background
labels in the trimaps. Thus we initialize the GrabCut
algorithm using threshold given high recall of potential
foreground region and let the iterative optimization
process to increase its precision. In our experiments,
the threshold is chosen empirically to be the threshold
that gives 95% recall rate in our ﬁxed thresholding
experiments (see Sec. 6.2). When initialized using RC
saliency maps, we use Tb = 70 with saliency values
normalized to .
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Matlab & C
Fig. 7. Average time taken to compute a saliency map for images in the MSRA10K database (most have resolution
400 × 300). We use parallel computing environment for all Matlab functions for efﬁcient computation.
Segmentation by iterative ﬁtting
Once initialized, we iteratively run GrabCut to
improve the SaliencyCut result (maximum of 4 iterations
in our experiments). After each iteration, we use dilation
and erosion operations on the current segmentation result to get a new trimap for the next GrabCut iteration.
As shown in Fig. 6(c, d), the region outside the dilated
region is set to background, the region inside the eroded
region is set to foreground, and the remaining areas are
set to unknown in the trimap. GrabCut, which by itself
is an iterative process using Gaussian mixture models
and graph-cut , helps to reﬁne salient object regions
at each step.
Different from one-pass GrabCut or the even simpler
graph cut based saliency segmentation , the new
scheme in SaliencyCut iteratively reﬁnes the initial salient
regions. Such an iterative design is important to handle
noisy initializations supplied by the saliency detection
algorithm rather than human annotations. In case of
incorrect initialization as shown in ﬂower example in
Fig. 6 (b), the initial background region incorrectly contains foreground object(s). Although we can still get a
segmentation result containing many parts of the ﬂower
using GrabCut, the remaining ﬂowers in the initial background region would never be correctly extracted using
GrabCut since the background gets a hard labeling. One
may consider relaxing the hard constrain of GrabCut to
solve this problem. However, experimental results show
this would make the method not stable, often producing
results containing all foreground or all background.
We iteratively reﬁne the initial segmentation and adaptively change the initial condition to ﬁt with newly segmented salient region. The adaptive ﬁtting is based on an
important observation: regions closer to an initial salient
object region are more likely to be part of that salient
object than far-away regions. Thus, our new initialization
enables GrabCut to include nearby salient regions, and
exclude non-salient regions according to color feature
dissimilarity. After each GrabCut iteration, SaliencyCut
incorporates the constraints given by the newly obtained
trimap, and train a better appearance model according
to previous results.
Fig. 6 shows three examples. In the ﬂower example,
SaliencyCut successfully expanded the initial salient regions (obtained directly from the saliency map) and converged to an accurate segmentation result. In the excavator and teapot examples, unwanted regions are correctly
excluded during GrabCut iterations. The intermediate
steps show how SaliencyCut successfully extracted the
object regions of interest in these challenging examples.
A comprehensive quantitative evaluation of different
saliency segmentation methods is presented in Sec. 6.3.
EXPERIMENTAL COMPARISONS
In this work, we extensively evaluated our saliency
detection method on three different types of benchmark
datasets, and compared it against 15 alternate methods
— SR , IT , IM , SUN , AC , SeR ,
AIM , GB , LC , CA , FT , SWD ,
SEG , MSS , LP 
and CB , respectively.
Following , we selected these methods according to:
number of citations (IT, SR, SUN, AIM and FT), recency
(SeR, MSS, SEG, IM, CA
and SWD), variety (IT is
biologically-motivated, LC is purely computational, GB
and LP are hybrid, SR works in the frequency domain,
AC and FT output full resolution saliency maps), and
being related to our approach (LC and CB).
Fig. 7 compares the average time taken by each
method on a Dual Core 2.6 GHz machine with 2GB
RAM. Our algorithms, HC and RC, are implemented
in C++. For the other methods namely IT, AIM, IM,
MSS, SEG, SeR, SUN, GB, SR, AC, CA, FT and CB, we
used the authors’ implementations, while for LC, we
implemented the algorithm in C++ since we failed to
obtain the authors’ implementation. For typical natural
images, our HC method runs in O(N), which is sufﬁcient
for real-time applications. In contrast, our RC variant
Fig. 8. Ground truth examples: (ﬁrst row) original images
with ground truth rectangles from , (second row) our
ground truth, which have more precisely marked important regions at pixel level accuracy.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
(a) original
(e) HC-maps
(f) RC-maps
Fig. 9. Visual comparison of saliency maps. (a) original images, saliency maps produced using (b) Zhai and Shah ,
(c) Goferman et al. , (d) Achanta et al. , (e) our HC and (f) RC methods, and (g) SaliencyCut. Our methods
generate uniformly highlighted salient regions (see our project webpage for all results on the full benchmark dataset).
is slower as it requires image segmentation , but
produces superior quality saliency maps.
The true effectiveness of a saliency detection method
depends on the applications . We evaluated our
method on three core computer vision and graphics applications, namely salient region segmentation by ﬁxed
thresholding, object of interest image segmentation, and
sketch based image retrieval (SBIR).
Benchmark datasets for saliency detection
Images with unambiguous salient objects
Similar to existing salient object region detection methods , , , , we ﬁrst evaluate our methods
on images with unambiguous salient object. The largest
dataset of this kind is provided by Liu et al. . This
dataset contains 20,000+ images (mostly at 400×300 resolution), with bounding box labeling by 3-9 users. These
images are selected from an initial set of 130,099 images,
such that each image contains a clear, unambiguous
object of interest. Since objects can still be recognized at
low resolution, the dataset has limited scale and location
variations of salient objects, i.e., implicitly the images
have scale and location priors (Flickr like).
Although an invaluable recourse to evaluate saliency
detection algorithms, the database with the marked
bounding boxes, however, is often too coarse for ﬁne
grained evaluation as observed by Wang and Li ,
and Achanta et al. . In order to do more extensive
and accurate evaluation, we randomly selected 10,000
images with consistent bounding box labeling in MSRA
database provided by Liu et al. and the consistent
measure is the same as choosing image dataset B in their
paper. As shown in Fig. 8, we accurately marked pixels
in salient object regions. We call this dataset MSRA10K
because it contains 10,000 images with pixel-level saliency
labeling (publicly available on our project page). Our
dataset is 10 times bigger than what was previously the
largest public available database of its kind . In our
experiments, we ﬁnd that saliency detection methods
using pixel level contrast (FT, HC, LC, MSS) do not scale
well on this lager benchmark (see Fig. 12(a)), suggesting
the importance of region level analysis.
Randomly-selected internet images
While state-of-the-art methods consistently produce excellent results when evaluated using the traditional
benchmark dataset (see Fig. 12(c)), ordinary users
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
often report less satisfactory experiences when using
their own images. This encourage us to think about
two questions: ‘How would these methods deal with
random internet images?’ and ‘When can we trust the
results of these methods?’ To better explore these issues,
we evaluated salient object segmentation methods on
a dataset with randomly-selected internet images .
This benchmark dataset, namely THUR15K , contains
about 3000 images downloaded from Flickr for each of
the 5 keywords: “butterﬂy,” “coffee mug,” “dog jump,”
“giraffe,” and “plane.” Salient regions in THUR15K images are marked at pixel accuracy. Note that not every
image in the THUR15K dataset contains a salient region
label, as some images do not have any salient object
region. Besides saliency detection, this dataset can also
be used to evaluate the performance of sketch based
image retrieval (SBIR).
Human ﬁxation dataset
While our algorithm targets salient object detection, it is
also interesting to test its performance on human ﬁxation
prediction benchmarks. We use the most widely adopted
human ﬁxation benchmark for such evaluation.
Segmentation by ﬁxed thresholding
The simplest way to get a binary segmentation of salient
objects is to threshold the saliency map with a threshold
∈ . To reliably compare how well various
saliency detection methods highlight salient regions in
images, we vary the threshold Tf from 0 to 255. Fig. 12(a)
shows the resulting precision vs. recall curves. Typical
qualitative comparison of saliency maps obtained by the
various methods are presented in Fig. 2 and Fig. 9.
Unlike most other methods, both the CB method and
our RC method use the center location prior of the
man-made photographs. However, for a fair comparison,
Fig. 12(b) shows comparisons while disabling such a location prior. Speciﬁcally, RC1 shows the effect disabling
the center location weighting ((7)) of RC method, while
RC2 shows the effect of further disabling border region
estimation (Sec. 4.3). Other methods in Fig. 12(b) also
improve when we use the same segmentation, as used
in RC, to average saliency values within each segment
and re-normalize to by uniform scaling. Note that
many of these methods aim to predict human eye movements rather than perform salient object segmentation,
as is our focus.
The precision and recall curves clearly show that our
RC method outperforms the other methods. We observe
a signiﬁcant loss in precision Fig. 12(b) for the CB
method (which has best performance in the benchmark
paper ) indicating that the method heavily relies on
location prior. The extremities of the precision vs. recall
curve are interesting: At maximum recall where Tf = 0,
all pixels are retained as positives, i.e., considered to be
foreground, so all the methods have the same precision
and recall values; precision 0.22 and recall 1.0 at this
point indicate that, on average, there are 22% image
pixels belonging to the ground truth salient regions. At
the other end, the minimum recall values of our RC
method are higher than those of the other methods,
because the saliency maps computed by our RC method
are smoother and contain more pixels with the saliency
value 255. Our HC method also has better precision and
recall compared to methods with similar computational
efﬁciency (SR, FT, and LC). After comparison of a large
number of saliency detection models, Borji et al. 
proposed a combined model and show that integration
of the few best models (with the initial version of our
method as one of them) outperforms all models. We believe that the combined model of will further beneﬁt
from performance improvement due to our method.
As also observed in the survey papers , , ,
 , center-bias naturally exists in human captured photos. Judd et al. further found that a simple Gaussian blob performs better than many saliency detection
methods when evaluated in famous eye ﬁxation dataset.
We experimentally ﬁnd that such simple Gaussian blob,
represented by ‘Gau’ in Fig. 12(a)(c), also performs better
than many existing models for saliency region detection
task. However, in the absence of explicit information,
we prefer not to use such a strong prior that can
potentially produce biased results, e.g., in automated
imaging systems. When disabling the center bias term,
our method still produces better results than other alternatives Fig. 12(b).
In the context of ﬁxation prediction, the CA and
methods report the best performance. Although it avoids the heavy learning for combining multisaliency models and object detectors, the CA method
still needs about 1 min to calculate a saliency map even
for small images. Fig. 11 and Fig. 7 shows that our
method, although initially designed for saliency region
detection, has only slightly lower performance to stateof-the-art methods for predicting human ﬁxation points,
while being 200+ times more efﬁcient. Readers can refer
to , , for more comparisons. Notice that the
good performance of our RC method for predicting eye
ﬁxation points shown in Fig. 11 is achieved by disabling
Fig. 10. From left to right, we show source image, ground
truth eye ﬁxation map by human observer, our RC result
with the term encouraging similar appearance region
receive similar saliency (Sec. 4.3) disabled, and result by
our full RC method.
Fig. 11. Comparison on human ﬁxation dataset .
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
(a) Original methods
(b) Variants of these methods
(c) Saliency segmentation methods
(i) Achanta et al. dataset (1, 000 images)
(ii) MSRA10K dataset (10, 000 images)
Fig. 12. Statistical comparison results of (a) different saliency detection methods, (b) their variants, and (c) object of
interest region segmentation methods, using largest public available dataset (i) and (ii) our MSRA10K dataset.
the term encouraging similar appearance region receive
similar saliency value, thus improves human ﬁxation
point prediction as demonstrated in Fig. 10. Although
disabling the process explained in Sec. 4.3 improves
eye ﬁxation prediction performance, we argue that uniformly highlighting the entire object region is better in
many applications, including content aware image resizing , non-photorealist rendering , adaptive image
compression , and image mosaic . Thus, although
their own method achieves best performance on eye
ﬁxation dataset , Margolin et al. still choose to
integrate our RC saliency maps to achieve better effects
for various of image manipulation applications.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Object of interest image segmentation
To objectively evaluate our new SaliencyCut method
using our RC-map as initialization, we compare our
results with results obtained by other state-of-the-art
methods for object of interest segmentation, i.e., FT ,
SEG , GrabCut (initialized using 5 pixel wide
image boundary), and CB 
(best parameters are
selected for these methods). Average precision, recall,
and F-Measure are compared against the entire groundtruth database , with the F-Measure deﬁned as:
Fβ = (1 + β2)Precision × Recall
β2 × Precision + Recall .
We use β2 = 0.3 as in Achanta et al. to weigh
precision more than recall. As can be seen from the
comparison (see Fig. 12(c)), SaliencyCut using our RC
saliency maps signiﬁcantly outperforms other methods.
As discussed by Liu et al. , recall rate is not as important as precision for attention detection. For example,
a 100% recall rate can be achieved by simply selecting
the whole image. Our approach reduced 57.2%, 50.9%,
46.5%, and 23.7% overall error rates on F-measure, compared with FT , SEG , GrabCut , and CB ,
respectively when evaluated using large accurate dataset
(MSRA10K). Besides producing higher F-Measure and
robustness to location prior, our SaliencyCut is about 60
times faster (see Fig. 14) compared to CB . Although
several new methods , have been developed
since the initial version of this work , to the best of our
knowledge, our salient object segmentation results are
still the best results reported on the most widely used
benchmark .
Although producing quite promising results for simple images as evaluated in Fig. 12, evaluation results
for randomly-selected internet images Fig. 13 shows that
Fig. 13. Comparison of average Fβ for different saliency
segmentation methods: FT , SEG , and ours, on
THUR15K dataset .
Matlab & C
Matlab & C
Fig. 14. Comparison of average time taken for different
saliency segmentation methods. Segmentation results for
MSRA10K dataset are available via our project page.
there is still a need to develop more robust methods. For
both datasets, our SaliencyCut’s performance is the best.
Sketch based image retrieval
Outline sketches are typically easier and faster for users
to generate than a complete color description of the
desired image. Sketch based image retrieval (SBIR) techniques become vital for users to leverage the increasing
volumes of available image database. A large majority
of potential users fail to precisely express ﬁne details
in their drawings. Thus most SBIR systems, which employ global descriptors, are unsatisfactory as they are
unreliable under afﬁne variations. To overcome such
drawbacks, Eitz et al. , use local descriptors to
achieve state-of-the-art retrieval performance. The success of their methods is mainly attributed to translation
invariance of local descriptors while using large local
feature size (in the order of 20 −25% of the image’s diagonal) to still retain large scale characteristics. However,
for such large window sizes, there is simply not much
space left for translating the sketch, thus limiting the
translation invariance. SBIR still suffers from relatively
low accuracy thus restricting its commercial potential.
Matching object shapes with clean background, however, is a relatively mature ﬁeld. Even for the very
challenging MPEG-7 dataset, state-of-the-art methods
can achieve 91.61% retrieval rates . Classical shape
methods such as Shape Contexts (SC) and Chamfer Matching are mostly successful when dealing
with limited background clutter. Selecting clean object
outlines without inﬂuence from irrelevant image edges
has great potential to improve current SBIR systems.
Based on the observation that good results cannot be
achieved without selection of segments, Bai et al. use
a shape band model to coarsely select candidate of edge
segments while using Shape Context distance to decide
the optimal matching. However, the shape band model
requires user sketch for further detection thus does not
allow preprocessing. It needs a few minutes to process
a single image making it unsuited for real-time image
retrieval applications.
Our SaliencyCut algorithm provides another possibility for automatically ﬁnding the outlines of object
of interest on large scale image datasets. After such
preprocessing, it becomes possible to make use of proven
shape matching algorithms. We simply rank the images
by SC distance between their salient region outlines
and user input sketches and compare with a state-of-theart SBIR method using SHoG .
Experiments indicate that although our SaliencyCut
method may produce less optimal results for noisy internet images, the shape matching method is very efﬁcient
in selecting those well segmented results. A quantitative
evaluation in our THUR15K dataset is shown in Fig. 16.
One can see that our retrieval method is more effective
than SHoG in terms of selecting user-desired candidates. Sample qualitative results are shown in Fig. 15.
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
Fig. 15. Sketch based image comparison: ﬁrst row shows images downloaded from Flickr using keyword ‘giraffe’,
second row shows our retrieval results obtained by comparing user input sketch with SaliencyCut result using shape
context measure ; third row shows corresponding sketch based retrieval results using SHoG .
Coffee mug
Fig. 16. True positive ratios (TPR) among top 50 and 100 retrieval results. Results for SHOG are supplied by original
authors. An image is considered as true positive if it contains a target object speciﬁed by the keywords.
Compared with SHoG, our method gives results that
are more relevant to user input sketches. Moreover, our
method produces the precise boundary of the desired
object, which makes it possible to reuse these segmented
image components in many applications. Note that the
extracted salient region features are complementary to
other features like color, texture, and local edges.
DISCUSSION
We presented global contrast based saliency computation methods, namely Histogram based Contrast (HC)
and spatial information-enhanced Region based Contrast (RC). While the HC method is fast and generates
results with ﬁne details, the RC method generates spatially coherent high quality saliency maps at the cost of
reduced computational efﬁciency. Based on the proposed
saliency detection method, we introduced a novel unsupervised segmentation algorithm, namely SaliencyCut,
to automatically segment the most salient object in an
image, without requiring expensive training data. The
proposed methods were evaluated on several large scale
publicly available benchmarks. The experimental results
show that our methods consistently outperform other
state-of-the-art methods in terms of precision and recall,
while still being simple, fast, and efﬁcient. For noisy
internet images, although our saliency detection and
segmentation methods cannot guarantee robust performance on individual images, their efﬁciency and simplicity makes it possible to automatically process a large
number of images, which can then be further ﬁltered for
reliability and accuracy.
Limitations. Our methods aim at ﬁnding the most
salient object in an image. It might produce sub-optimal
results for images with multiple objects (e.g., Fig. 17),
especially if the objects occlude each other (e.g., PASCAL
Fig. 17. Salient object detection results for benchmark
images with multiple objects.
VOC images ), for which even specialized object
detectors fail to reliably generate good results for most
object classes. Rather than making hard decisions early
on, proposing some candidate object regions – 
can be useful for those applications requiring high detection rate, e.g., object detection in cluttered scenes.
Future work. We further discuss possible applications
and extensions by highlighting a few of the many exciting works using the preliminary version of our work:
maps is essential for many applications including
content-aware image manipulation, , , nonphotorealist rendering , , image scene analysis – adaptive compression , forgery
detection , , etc.
• Unsupervised segmentation of the entire salient
object, without extensive training data annotation,
naturally beneﬁts applications like auto-cropping
 , scene classiﬁcation , semantic manipulation
 – , and data-driven image synthesis , .
• A tool to retrieve internet images and get precise
object of interest regions is powerful to explore
this big data for image composition , semantic
colorization , information discovery , ,
image retrieval – , etc.
• The proposed saliency measure has already been
IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX,NO. XX, XXX. XXXX
used to produce state-of-the-art results on cosegmentation benchmarks without using cosegmentation , or simultaneously analyze multiple images
for better salient object extraction , .
ACKNOWLEDGMENTS
We would like to thank Prof. Z. Tu (for the suggestion of region level analysis), Prof. C. Rother, Prof.
A. Zisserman, and the anonymous reviewers for their
many useful feedbacks. This research was supported by
the 973 Program (2011CB302205), NSFC (61120106007,
61133008), the 863 Program (2012AA011802), Research
program of Beijing Education council, EPSRC, Leverhulme Trust, ERC , and ERC
Starting Grant SmartGeometry.