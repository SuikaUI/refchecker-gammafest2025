Proactive Resource Management for LTE in
Unlicensed Spectrum: A Deep Learning
Perspective
Ursula Challita∗, Li Dong∗, and Walid Saad†
∗School of Informatics, The University of Edinburgh, Edinburgh, UK,
Emails: {ursula.challita, li.dong}@ed.ac.uk.
†Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA,
USA. Email: .
LTE in unlicensed spectrum using licensed assisted access LTE (LTE-LAA) is a promising approach
to overcome the wireless spectrum scarcity. However, to reap the beneﬁts of LTE-LAA, a fair coexistence
mechanism with other incumbent WiFi deployments is required. In this paper, a novel deep learning
approach is proposed for modeling the resource allocation problem of LTE-LAA small base stations
(SBSs). The proposed approach enables multiple SBSs to proactively perform dynamic channel selection,
carrier aggregation, and fractional spectrum access while guaranteeing fairness with existing WiFi
networks and other LTE-LAA operators. Adopting a proactive coexistence mechanism enables future
delay-tolerant LTE-LAA data demands to be served within a given prediction window ahead of their
actual arrival time thus avoiding the underutilization of the unlicensed spectrum during off-peak hours
while maximizing the total served LTE-LAA trafﬁc load. To this end, a noncooperative game model is
formulated in which SBSs are modeled as Homo Egualis agents that aim at predicting a sequence of
future actions and thus achieving long-term equal weighted fairness with WLAN and other LTE-LAA
operators over a given time horizon. The proposed deep learning algorithm is then shown to reach a
mixed-strategy Nash equilibrium (NE), when it converges. Simulation results using real data traces show
that the proposed scheme can yield up to 28% and 11% gains over a conventional reactive approach
and a proportional fair coexistence mechanism, respectively. The results also show that the proposed
framework prevents WiFi performance degradation for a densely deployed LTE-LAA network.
Index Terms
Licensed assisted access LTE (LTE-LAA); LTE-U; small cell; unlicensed band; long short term
memory (LSTM); deep reinforcement learning; game theory; proactive resource allocation
A preliminary version of this work was published at European Wireless 2017 .
This research was supported by The University of Edinburgh Principal’s Career Development PhD Scholarship and the U.S.
National Science Foundation under Grants IIS-1633363 and CNS-1460316.
 
I. INTRODUCTION
Licensed assisted access LTE (LTE-LAA) has emerged as an effective solution to overcome
the scarcity of the radio spectrum . Using LTE-LAA, a cellular small base station (SBS) can
improve its performance by simultaneously accessing licensed and unlicensed bands. However, to
achieve the promised quality-of-service (QoS) improvements from LTE-LAA, many challenges
must be addressed ranging from effective co-existence with existing WiFi networks to resource
allocation, multiple access, and inter-operator spectrum sharing .
If not properly deployed, LTE-LAA can signiﬁcantly degrade the performance of WiFi .
There has been a number of recent works – that study the problem of enhanced LTE-
LAA and WiFi coexistence. This existing body of works can be categorized into two groups:
channel access – and channel selection – . The authors in – propose different
channel access mechanisms based on listen-before-talk (LBT) that rely on either an exponential
backoff , a ﬁxed/random contention window (CW) size , or an adaptive CW size .
Nevertheless, an exponential backoff approach leads to unnecessary retransmissions while a ﬁxed
CW size cannot handle time-varying trafﬁc loads thus yielding unfair outcomes. The authors
in develop a holistic approach for both trafﬁc ofﬂoading and resource sharing across the
licensed and unlicensed bands but considering one SBS. In , the authors study the problem of
resource allocation with uplink-downlink decoupling for LTE-LAA. The authors in propose
an inter-network coordination scheme with a centralized radio resource management for the
LTE-WiFi coexistence. However, this prior art is limited to one unlicensed channel and does not
jointly account for channel selection and channel access. In other words, these works do not
analyze the potential gains that can be obtained upon aggregating or switching between different
unlicensed channels. Operating on a ﬁxed unlicensed channel limits the amount of cellular data
trafﬁc that can be ofﬂoaded to the unlicensed band and leads to an increase in the interference
level caused to neighboring WiFi access points (WAPs) operating on that same channel.
In terms of LTE-LAA channel selection, the authors in propose a distributed approach
based on Q-learning. A matching-based solution approach is proposed in , which is both
distributed and cooperative. Moreover, the work in combines channel selection along with
channel access. Despite the promising results, all of these works – consider a reactive
approach in which data requests are ﬁrst initiated and, then, resources are allocated based on
their corresponding delay tolerance value. Nevertheless, this sense-and-avoid approach can cause
an underutilization of the spectrum due to the impulsive reconﬁguration of the spectrum usage
that does not account for the future dynamics of the network. Despite the predominance of
the reactive LTE-WiFi coexistence solutions, cellular data trafﬁc networks are known to exhibit
statistically ﬂuctuating and periodic demand patterns, especially applications such as ﬁle transfer,
video streaming and browsing , therefore providing an opportunity for the network to exploit
the predictable behavior of the users to smooth out the trafﬁc over time and reduce the difference
between the peak and the average load. Therefore, in a proactive approach, rather than reactively
responding to incoming demands and serving them when requested, an SBS can predict trafﬁc
patterns and determine future off-peak times so that incoming trafﬁc demand can be properly
allocated over a given time window.
Therefore, the main motivation for adopting a proactive LTE-WiFi coexistence scheme is to
avoid the underutilization of the unlicensed spectrum. This is mainly accomplished by either
serving a fraction of the LTE-LAA trafﬁc when requested or shifting part of it to the future,
over a given time window, so as to balance the occupancy of the unlicensed spectrum usage
across time and, consequently, improve its degree of utilization. From the LTE-LAA network
perspective, this will increase its transmission opportunities on the unlicensed spectrum, reduce
the collision probability with WAPs and other SBSs and, hence, provide a boost for its throughput.
Moreover, a proactive resource allocation scheme can exploit the inherent predictability of the
future channel availability status so as to allocate resources in a window of time slots based on
the predicted requests. This, in turn, can lead to a decrease in the probability of occurrence of a
congestion event while ensuring a degree of fairness to the wireless local area network (WLAN).
The main contribution of this paper is a novel deep reinforcement learning algorithm based
on long short-term memory (RL-LSTM) cells for proactively allocating LTE-LAA resources
over the unlicensed spectrum. The LTE-LAA resource allocation problem is formulated as a
noncooperative game in which the players are the SBSs. To solve this game, we propose an
RL-LSTM framework using which the SBSs can autonomously learn which unlicensed channels
to use along with the corresponding channel access probability on each channel taking into
account future environmental changes, in terms of WLAN activity on the unlicensed channels
and LTE-LAA trafﬁc loads. Unlike previous studies which are either centralized or rely
on the coordination among SBSs , our approach is based on a self-organizing proactive
resource allocation scheme in which the SBSs utilize past observations of the network state to
build predictive models on spectrum availability and to intelligently plan channel usage over
Operator C
Operator B
Operator A
Fig. 1: Illustration of the system model. In the above example, 3 SBSs belonging to different operators and 3
unlicensed channels are only shown for simplicity. The channel selection vector over a time window of 3 epochs
is also shown.
a ﬁnite time window. The use of long short term memory (LSTM) cells enables the SBSs to
predict a sequence of interdependent actions over a long-term time horizon thus achieving longterm fairness among different underlying technologies. We show that, upon convergence, the
proposed algorithm reaches to a mixed-strategy distribution which constitutes a mixed-strategy
Nash equilibrium (NE) for the studied game. We also show that the gain of the proposed
proactive resource allocation scheme and the optimal size of the prediction time window is
a function of the trafﬁc pattern of the dataset under study. To the best of our knowledge, this
is the ﬁrst work that exploits the framework of LSTMs for proactive resource allocation in
LTE-LAA networks. Simulation results show that the proposed approach yields signiﬁcant rate
improvements compared to conventional reactive solutions such as instantaneous equal weighted
fairness, proportional fairness and total network throughput maximization. The results also show
that the proposed scheme prevents disruption to WLAN operation in the case large number of LTE
operators selﬁshly deploy LTE-LAA in the unlicensed spectrum. In terms of priority fairness,
results show that an efﬁcient utilization of the unlicensed spectrum is guaranteed when both
technologies are given equal weighted priorities for transmission on the unlicensed spectrum.
The rest of this paper is organized as follows. In Section II, we present the system model.
Section III describes the proposed coexistence game model. The LSTM-based algorithm is
proposed in Section IV. In Section V, simulation results are analyzed. Finally, conclusions are
drawn in Section VI.
II. SYSTEM MODEL
Consider the downlink of an LTE-LAA network composed of a set J of J LTE-LAA SBSs
belonging to different LTE operators, a set W of W WAPs, and a set C of C unlicensed channels
as shown in Fig. 1. Each SBS j ∈J has a set Kj of Kj LTE-LAA UEs associated with it. We
consider a network scenario corresponding to environments such as work ofﬁces, a university
campus, and airports in which the trafﬁc load of a given WAP or SBS can be characterized
through a particular model that typically remains unchanged over coarse periods of time (e.g.,
one day) . We focus on the operation of the SBSs over the unlicensed band, while the
licensed spectrum resources are assumed to be allocated in a conventional way . Both SBSs
and WAPs adopt the LBT access scheme and, thus, at a particular time, a given unlicensed
channel is occupied by either an SBS or a WAP. We consider the LTE carrier aggregation
feature using which the SBSs can aggregate up to ﬁve component carriers belonging to the same
or different operating frequency bands . This, in turn, would enable the SBSs to operate on
multiple unlicensed channels simultaneously thus maximizing their data rate during a particular
transmission opportunity.
Our goal is to jointly determine the dynamic channel selection, carrier aggregation, and
fractional spectrum access for each SBS, while guaranteeing long-term airtime fairness with
WLAN and other LTE-LAA operators. The main motivation for adopting a long-term fairness
approach is to avoid the underutilization of the unlicensed spectrum by either serving part of the
LTE-LAA trafﬁc when requested or shifting part of it in the future over a given time window
in a way that would balance the occupancy of the unlicensed spectrum usage across time and,
consequently, improve its degree of utilization. This will subsequently result in an increase in
the transmission opportunities for LTE-LAA as well as a decrease in the collision probability
for the WLAN. To realize this, we need to dynamically analyze the usage of various unlicensed
channels over a particular time window. To this end, we divide our time domain into multiple
time windows of duration T, each of which consists of multiple time epochs t, as shown in Fig. 2.
Our objective is to proactively determine the spectrum allocation vector for each SBS at t = 0
over T while guaranteeing long-term equal weighted airtime share with WLAN. To guarantee a
fair spectrum allocation among SBSs belonging to different operators, we consider inter-operator
interference along with inter-technology interference. In fact, inter-operator interference is the
consequence of the selﬁsh behavior of different operators and could result in a degradation in
Prediction window T1
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
Prediction window T2
Fig. 2: The division of the time domain into multiple time windows T, each of which consists of multiple time
the spectral efﬁciency if not managed. Next, we deﬁne xj,c,t = 1 if channel c is selected by SBS
j during time epoch t, and 0, otherwise, and αj,c,t ∈ . xj,c,t determines the channel c that
is used by SBS j during time t and αj,c,t is the channel access probability of SBS j on the
unlicensed channel c at time t.
Since the 3rd Generation Partnership Project (3GPP) has identiﬁed LBT as an access mechanism for standardizing a global solution for the operation of LTE in the unlicensed spectrum,
we consider a contention-based protocol for our proposed channel access mechanism . In
this protocol, prior to transmission, an SBS applies clear channel assessment for the duration
of DIFS to detect the state of the channel (idle or busy) based on the detected energy level. If
the channel is idle, the SBS would backoff for a random number between [0, CW] and if the
medium was still free, it gets a transmit opportunity for up to 10 LTE sub-frames (considering
priority class 1 devices ); it sends a reservation signal, e.g., clear-to-send (CTS), with the
duration of its transmission period along with the remaining time period until the beginning of
its next subframe. This allows prevention of other competing devices from getting access to the
unlicensed channel until the beginning of the next subframe of the corresponding SBS and hence
reserving the channel for transmission. On the other hand, if the channel was busy, the SBS keeps
monitoring the channel until it becomes idle. Here, we note that our proposed algorithm is not
fully compliant with the regulations in terms of CW size adjustment. In particular, we consider an
exponential backoff scheme for WiFi while the SBSs adjust their CW size (and thus the channel
access probability) on each of the selected channels in a way that would guarantee a long-term
equal weighted fairness with WLAN and other SBSs. In essence, the exponential backoff access
method that has been adopted by 3GPP for SBSs can lead to short-term unfairness . This
results from the fact that, after each collision, the colliding hosts double their CWs and, thus,
have higher probability of choosing a larger backoff during which other hosts may beneﬁt from
channel access. This also means increased delay for hosts that doubled their CW. Therefore, the
standard distributed coordination function (DCF) method controls the load on the channel by
reducing the number of contending hosts, because the hosts that have failed their transmission
are likely to attempt to access the channel in the future. Moreover, hosts consider all failed
transmissions as collisions in DCF, however, this is not always the case. Thus, DCF bases its
load control on a biased indicator, which can potentially lead to lower performance and increased
unfairness . On the other hand, by having a ﬁxed CW size for each SBS during each time
epoch t, we can alleviate these problems and, more importantly, we can decouple the load control
from handling failed transmissions. It is also worth noting that small CW sizes lead to an increase
in the collision probability while large CW sizes result in too much time spent waiting in idle
slots. Therefore, an efﬁcient access method should adapt the value of the CW of each SBS to
the trafﬁc conditions of the network.
To derive the throughput achieved by an LTE-LAA user equipment (UE) and a WAP, we ﬁrst
deﬁne the stationary probability of each WAP w and each SBS j, τw and τj,c,t respectively. The
stationary probability is the probability with which a given base station attempts to transmit in
a randomly chosen slot. Considering an exponential backoff scheme for WiFi, the stationary
probability with which WAPs transmit a packet during a particular WiFi time slot, τw, will be
given by :
(1 −2qw)(CWmin + 1) + qwCWmin(1 −(2qw)m),
where qw is the collision probability of a WAP, m is the maximum backoff stage where CWmax =
2mCWmin. CWmin and CWmax are the minimum and maximum contention window size, respectively. For LTE-LAA, m=0 since no exponential backoff is considered, and, thus, the
stationary probability of an SBS on a given unlicensed channel c during time epoch t will
be τj,c,t =
CWj,c,t+1, where CWj,c,t is the contention window size of SBS j on channel c during
time epoch t. Therefore, we do not consider a contention stage for LTE-LAA and, thus, the CW
size of the SBSs is not doubled after each unsuccessful transmission. Instead, the SBSs consider
a ﬁxed value for the CW for each time epoch t and this value is adjusted adaptively from one
time epoch t to another in order to control the corresponding channel access probabilities over
Fig. 3: An illustrative example for computing the actual data transmission time of an LTE-LAA SBS.
the unlicensed band for different time epochs. The collision probability of a WAP is deﬁned
as qw = 1 −QW
v=1,v̸=w(1 −τv) QJ
j=1(1 −τj,c,t), where c is the channel used by WAP w. The
throughput Rw of a WAP w during a particular WiFi time slot will be:
Pw,succ · E[Dw]
Pw,idle · θ + Pw,busy · Tb
where E[Dw] is the expected payload size for WAP w, Pw,succ = τw
v=1,v̸=w(1 −τv) QJ
τj,c,t) is the probability of a successful transmission, Pw,idle = QJ
j=1(1−τj,c,t) QW
w=1(1−τw) is the
probability of an idle slot, and Pw,busy = 1−QJ
j=1(1−τj,c,t) QW
w=1(1−τw) is the probability of a
busy slot, regardless of whether it corresponds to a collision or a successful transmission. θ and
Tb are, respectively, the average durations of an idle and a busy slot and, thus, the denominator
in (2) corresponds to the mean duration of a WiFi medium access control (MAC) slot.
The achievable airtime fraction for an LTE-LAA SBS j on channel c at time t is:
αj,c,t = τj,c,t
(1 −τi,c,t)
The airtime fraction represents the time allocated for an SBS on channel c during time t, which
essentially accounts for both the data transmission time as well as the reservation signal overhead.
Here, it is important to account for the reservation signal overhead during a transmission burst
of an SBS when computing the throughput, as done in . As such, we let ξj,c,t be the average
fraction of time of αj,c,t during which LTE-LAA SBS is transmitting data. Fig. 3 provides an
illustrative example for computing the fraction of time of t during which LTE-LAA SBS is
transmitting data. Thus, the total throughput of all Kj,t UEs that are served by SBS j during
time epoch t is:
αj,c,tξj,c,trj,c,t,
1 + Pj,c,thj,k,c,t
Ij,c,t + BcN0
Here, Ij,c,t = PW
w=1 Pw,c,thw,k,c,t + PJ
i=1,i̸=j Pi,c,thi,k,c,t is the interference level on SBS j when
operating on channel c during time t and Bc is the bandwidth of channel c. Pj,c,t is the transmit
power of SBS j on channel c during time t. hj,k,c,t is the channel gain between SBS j and UE
k on channel c during time t. N0 is the power spectral density of additive white Gaussian noise.
Since SBSs and WAPs both adopt LBT, then one cell may occupy the entire channel at a given
time thus transmitting exclusively on a given channel c. However, hidden and exposed terminals
could be present on a given channel which can result in interference and thus a degradation in
the throughput.
Given this system model, next, we develop an effective spectrum allocation scheme that
can allocate the appropriate unlicensed channels along with the corresponding channel access
probabilities to each SBS simultaneously over T, at t = 0.
III. PROACTIVE RESOURCE ALLOCATION SCHEME FOR UNLICENSED LTE
A. Proactive Resource Allocation Game
We formulate the resource allocation problem as a noncooperative game G=(J , Aj, uj) with
the SBSs in J being the players, each of which must choose a channel selection and channel
access pair aj,c,t =(xj,c,t,αj,c,t) ∈Aj at t = 0 for each t of the next time window T. The objective
of each SBS j is to maximize its total throughput over the selected channels:
uj(aj, a−j) =
αj,c,tξj,c,trj,c,t,
where aj = [(aj,1,1, · · · , aj,1,T), · · · , (aj,C,1, · · · , aj,C,T)] and a−j correspond, respectively to the
action vector of SBS j and all other SBSs, over all the channels C during T. Note that the utility
function (6) of SBS j depends on its actions as well as those of other SBSs which makes the
formulation of a game model suitable for this problem. This is mainly due to the interference
from other SBSs transmitting on the same channel as SBS j as it was shown previously in the
deﬁnition of the rate expression in (5). The goal of each SBS j is to maximize its own utility:
aj∈Aj uj(aj, a−j) ∀j ∈J ,
αj,c,t ≤xj,c,t
xj,c,t ≤min(Mc, C)
αj,c,tT Bc ≤
αw,c,t + αj,c,t +
αi,c,t ≤tmax
xj,c,t ∈{0, 1}, αj,c,t ∈ 
where Mc is the total number of unlicensed channels which an SBS can aggregate. (8) allows
the allocation of a channel access proportion for SBS j on channel c during t only if SBS j
transmits on channel c at time t. (9) guarantees that each SBS can aggregate a maximum of Mc
channels at a given time t. (10) limits the amount of allocated bandwidth to the required demand
where f(Lj,t) captures the relationship between bandwidth requirement and offered load. (11)
captures coupling constraints which limit the proportion of time used by SBSs and WLAN on
a given unlicensed band to the maximum fraction of time an unlicensed channel can be used,
tmax1. (12) represents the feasibility constraints.
Given the fact that different operators and technologies have equal priorities on the unlicensed
spectrum, we incorporate the Homo Egualis (HE) anthropological model, an inequity-averse
based fairness model, into the strategy design of the agents .
Deﬁnition 1. Inequity aversion is the preference for fairness and resistance to incidental inequalities. In other words, it refers to the willingness of giving up some material payoff in order to
move in the direction of more equitable outcomes.
In an HE society, agents focus not only on maximizing their own payoffs, but also become
aware of how their payoffs are compared to other agents’ payoffs , . Therefore, their
utility function is inﬂuenced not only by their own reward, but also by envy and altruism. An
agent is altruistic to others if its payoff is above an equitable benchmark and is envious of the
others if its payoff exceeds that benchmark and therefore, an unfair distribution of resources
among agents results in disutility for inequity-averse agents. The HE concept comes from the
anthropological literature in which Homo sapiens evolved in small hunter-gatherer groups without
a centralized governance .
In fact, we incorporate the notion of airtime fairness in the modeling of our HE agents. The
average airtime per radio system is considered as one of the most important fairness metrics in
the unlicensed band and is the focus of this work . Our motivation for considering a time-fair
1tmax depends on the channel access method in the unlicensed band and should be strictly less than 1 in the case of LBT,
otherwise, the channel will always be sensed busy and devices would not be able to access it.
channel allocation scheme is to overcome the rate anomaly problem that arises when different
nodes use distinct data rates, which leads to the slowest link limiting the system performance ,
 , and . Therefore, to model our players as HE agents, we consider the following two
coupling constraints for the allocated airtime fraction on each channel c for each SBS j:
t=1 αj,c,t
t=1 αi,c,t
∀c ∈bCj, i ∈bSj,c(i ̸= j),
n∈Sc,t αn,c,t
n∈Sc,t ¯Ln,t
t=1 αw,c,t
t=1 Lw,c,t
where bCj is the subset of channels used by SBS j during T. Sc,t is the subset of SBSs that are
transmitting over channel c, c ∈bCj, during time t and bSj,c is the subset of other neighboring
SBSs, i ̸= j, that are using the same channel c ∈bCj as SBS j during T. ¯Lj,t = Lj,t−P
c′ f(αj,c′,t)
corresponds to the remaining trafﬁc that needs to be served by SBSs j with Lj,t being the total
aggregate trafﬁc demand of SBS j on channel c during time epoch t. f(.) corresponds to the
served trafﬁc load as a function of the airtime allocation. c′ represents all the set of channels
except channel c. αw,c,t = min(f(Lw,c,t), tmax −αj,c,t −P
i∈Sj,c,t αi,c,t) is the airtime allocated for
WLAN transmissions over channel c during time t. PWiFi and PLTE correspond to the priority
metric deﬁned for each technology when operating on the unlicensed band. These parameters
allow adaptation of the level of fairness between LTE-LAA and WLAN.
Constraint (13) represents inter-operator fairness which guarantees an equal weighted airtime
allocation among SBSs belonging to different operators on a given channel c. The adopted
notion of fairness is based on a long-term weighted equality over T, as opposed to instantaneous
weighted equality. wj,c = PT
t=1 xj,c,t is the weight of SBS j on channel c during T and thus
different SBSs are assigned different weights on each channel c based on the number of time
epochs t a given SBS j uses that particular channel. (14) deﬁnes an inter-technology fairness
metric to guarantee a long-term equal weighted airtime allocation over T for both LTE-LAA
and WiFi. Therefore, (13) and (14) reﬂect the inequity aversion property of the SBSs.
In fact, the optimal value of T, which corresponds to the time window size that allows the
maximum achievable throughput for LTE-LAA as compared to the reactive approach, is dataset
dependent. Next, we characterize the optimal value of T under a uniform trafﬁc distribution.
Proposition 1. For a uniform trafﬁc distribution, the optimal value of T is equal to 1.
Proof. Under a uniform demand model, the trafﬁc load for each of SBS j and WAP w is an
independent and identically distributed (i.i.d.) sequence of random variables which implies that
all requests of the same user are statistically indistinguishable over time. In our model, WAPs are
passive in that their channel selection action is ﬁxed and, thus, the activity on a given channel
is characterized by the level of activity of WAPs operating on that channel. In that case, the
WLAN trafﬁc load on each channel also follows a uniform distribution. At the convergence
point, (8)-(14) are satisﬁed and, hence, the average airtime allocated to the LTE-LAA network
on channel c over the time window T will be:
αj,c,t = PLTE
j∈Sc,t ¯Lj,t
t=1 Lw,c,t
αw,c,t ∀c ∈C,
However, for the case of uniform trafﬁc demand, the channel selection vector over T is the
same for each SBS because the network state is the same for every t in T. Moreover, if an
SBS aggregates multiple channels, then its load on each channel is the same for each t in T.
This implies that ¯Lj,t for each SBS j is uniform over T and thus
j∈Sc,t ¯Lj,t
t=1 Lw,c,t
j∈Sc,t ¯Lj,t
Consequently, (15) can be written as:
αj,c,t = PLTE
j∈Sc,t ¯Lj,t
αw,c,t ∀c ∈C,
When T = 1, the airtime allocated to the LTE-LAA network on channel c will be:
αj,c,t = PLTE
j∈Sc,t ¯Lj,t
Over a ﬁxed time window T, the average airtime allocated to the LTE-LAA network on
channel c can be written as:
αj,c,t = 1
j∈Sc,t ¯Lj,t
j∈Sc,t ¯Lj,t
αw,c,t ∀c ∈C.
(19) is equivalent to (16) and, hence, our proposed framework does not offer any gain for the
LTE-LAA network when considering a time window T larger than 1 in the case of a uniform
trafﬁc pattern. This completes the proof.
From Proposition 1, we conclude that the gain of our proposed long-term fairness notion is
evident in the case of trafﬁc ﬂuctuations. Under a uniform trafﬁc distribution, the SBSs cannot
make use of future off-peak times to shift part of their trafﬁc forward in time and, hence, the gain
is limited to predicting the network state for the next time epoch only. It is also worth noting
that the gain of the proactive scheduling approach decreases in the case of a highly congested
WLAN network. This is mainly due to the fact that the system becomes more congested with
incoming requests, thereby restricting the opportunities of shifting part of the LTE-LAA load in
the future.
B. Equilibrium Analysis
Our game G is a generalized Nash equilibrium problem (GNEP) in which both the objective
functions and the action spaces are coupled. To solve the GNEP, we incorporate the Lagrangian
penalty method into the utility functions thus reducing it to a simpler Nash equilibrium problem
(NEP). The resulting penalized utility function will be, ∀(j ∈J ):
buj(aj, a−j) =
αj,c,trj,c,t−ρ1,j
min(0, tmax −αw,c,t −αj,c,t −
Sj,c(i̸=j)
t=1 αj,c,t
t=1 αi,c,t
n∈Sc,t αn,c,t
n∈Sc,t ¯Ln,t
t=1 αw,c,t
t=1 Lw,c,t
where ρ1,j, ρ2,j and ρ3,j are positive penalty coefﬁcients corresponding to constraints (11), (13),
and (14), respectively. Here, we consider equal penalty coefﬁcients for all players for each
coupled constraint, ρ1,j = ρ1, ρ2,j = ρ2 and ρ3,j = ρ3. This allows all SBSs to have equal
incentives to give up some payoff in order to satisfy the coupled constraints. To determine the
values of ρ1, ρ2 and ρ3, we adopt the incremental penalty algorithm in that guarantees the
existence of penalty parameters ρ∗
3] that satisfy the coupled constraints.
In our game G, αj,c,t is a continuous variable bounded between 0 and 1, however, for a
particular network state, we are interested only in a certain region of the continuous space
where the optimal actions are expected to be. Therefore, we will propose a sampling-based
approach to discretize αj,c,t in Section IV. Under such a discretization of the action space, we
turn our attention to mixed strategies in which players choose their strategies probabilistically.
Such a mixed-strategy approach enables us to analyze the frequency with which players choose
different channels and channel access combinations. In fact, the optimal policy is often stochastic
and therefore requires the selection of different actions with speciﬁc probabilities . This, in
turn, validates our choice of adopting a mixed strategy approach as opposed to a pure strategy
one that is oriented towards ﬁnding deterministic policies. A player can possibly choose different
possible actions with different probabilities which enables it to play a combination of strategies
over time. Moreover, unlike pure strategies that might not exist for a particular game, there
always exists at least one equilibrium in mixed strategies .
Let ∆(A) be the set of all probability distributions over the action space A and pj =
[pj,a1 · · · , pj,a|Aj|] be a probability distribution with which SBS j selects a particular action
from Aj. Therefore, our objective is to maximize the expected value of the utility function,
uj(pj, p−j) = Epj [buj (aj, a−j)] = P
a∈A buj(aj, a−j) QJ
j=1 pj,aj.
Deﬁnition 2. A mixed strategy p∗=(p∗
1, · · · , p∗
−j) constitutes a mixed-strategy Nash
equilibrium if, ∀j ∈J and ∀pj ∈∆(Aj), uj(p∗
−j) ≥uj(pj, p∗
Here, we note that any ﬁnite noncooperative game will admit at least one mixed-strategy
Nash equilibrium . To solve for the mixed-strategy NE of our game G, we ﬁrst consider the
simpler scenario in which the number of SBSs is less than the number of unlicensed channels.
Then, we develop a learning algorithm to handle the more realistic scenario in which the number
of SBSs is much larger than the number of unlicensed channels.
Remark 1. If the number of SBSs is less than the number of available unlicensed channels
(i.e., J ≤C), then the mixed-strategy NE solution will simply reduce to a pure strategy that is
reached when all SBSs occupy disjoint channels during each time epoch of the time window T.
To show this, we consider two cases depending on whether or not carrier aggregation is
enabled. Let Mc = 1. Consider the state in which each SBS is operating on a different unlicensed
channel. If SBS j changes its channel from c to c′ on which SBS i is transmitting, then it would
have to share channel c′ with SBS i in an equal weighted manner (based on the inter-operator
fairness constraint). This leads to a decrease in the reward function of SBS i on channel c′ (and
potentially for SBS j), which makes SBS i deviate to another channel that is less occupied (e.g.,
c). Therefore, a given strategy cannot be a best response (BR) strategy for SBS i in case it results
in its transmission on the same channel as SBS j. Therefore, all strategies that result in more
than one SBS occupying the same channel are dominated by the alternative where different SBSs
transmit on disjoint channels and hence cannot correspond to BR strategies. Consequently, at the
NE point, all SBSs play their BR strategies that would result in each SBS occupying a disjoint
channel. Similarly for Mc > 1. If SBS j transmits on multiple channels, then aggregating a
channel that is already occupied by SBS i would make SBS i change its operating channel to
a less congested one. This implies that an SBS would not aggregate more channels unless they
are not occupied by other SBSs.
Therefore, we can conclude that our proposed scheme results in having less number of SBSs
on each of the unlicensed bands. This leads to a lower collision probability on each channel and
a better coexistence with WLAN. Moreover, enabling carrier aggregation does not necessarily
allow LTE to ofﬂoad more trafﬁc to the unlicensed band. On the other hand, our proposed
scheme can avoid causing performance degradation to WLAN in case a large number of LTE
operators deploy LTE-LAA in the unlicensed bands.
Now, when J > C, multiple SBSs will then potentially have to share the same channel. In
this case, the mixed-strategy NE is challenging to characterize, and therefore, next, we propose
a learning-based approach for solving our game G. Given the fact that each SBS needs to learn
a sequence of actions over the time window T at t = 0 based on a sequence of previous network
states, the proposed learning algorithm must be capable of generating data that is sequential in
nature. This necessitates the knowledge of historical trafﬁc values as well as future network states
for all the time epochs of the following time window T. Moreover, in order to satisfy the longterm fairness constraints (13) and (14), future actions cannot be assumed to be independent due
to the long-term temporal dependence among these actions. Conventional reinforcement learning
algorithms such as Q-learning and multi-armed bandit take as an input the current state of the
network and enable the prediction of the next state only and therefore do not account for the
interdependence of future actions . To learn several steps ahead in time, recursive learning
can be adopted. However, such an approach uses values already predicted, instead of measured
past values which produces an accumulation of errors that may grow very fast. In contrast, deep
learning techniques, such as time series prediction algorithms, are capable of learning long-term
temporal dependence sequences based on input sequences , . This is viable due to their
adaptive memory that allows them to store necessary previous state information to predict future
events. Therefore, next, we develop a novel time series prediction algorithm based on deep
learning techniques for solving the mixed-strategy NE of our game.
IV. RL-LSTM FOR SELF-ORGANIZING RESOURCE ALLOCATION
The proposed game requires each SBS to learn a sequence of actions over the prediction time
window T, at t = 0, without any knowledge of future network states. This necessitates a learning
approach with memory for storing previous states whenever needed while being able to learn a
sequence of future network states. Employing LSTMs is therefore an obvious choice for learning
as they are capable of generating data that is sequential in nature – . Consequently, we
propose a novel sequence level training algorithm based on RL-LSTM that allows SBSs to learn
a sequence of future actions at operation time based on a sequence of historic trafﬁc load thus
maximizing the sum of their future rewards.
LSTMs are a special kind of “deep” recurrent neural networks (RNNs) capable of storing
information for long periods of time to learn the long-term dependency within a sequence .
LSTMs process a variable-length sequence y = (y1, y2, ..., ym) by incrementally adding new
content into a single memory slot, with gates controlling the extent to which new content should
be memorized, old content should be erased, and current content should be exposed. Unlike
conventional one-step RL techniques (e.g., Q-learning), LSTM networks are capable of predicting
a sequence of future actions , . Predictions at a given time step are inﬂuenced by the
network activations at previous time steps thus making LSTMs suitable for our application. The
total number of parameters W in a standard LSTM network with one cell in each memory block
is given by:
W = nc × nc × 4 + ni × nc × 4 + nc × no + nc × 3
where nc is the number of memory cells, ni is the number of input units, and no is the number of
output units. The computational complexity of learning LSTM models per weight and time step
is linear i.e., O(1). Therefore, the learning computational complexity per time step is O(W) .
Consequently, we consider an end-to-end RL-LSTM based approach to train the network to
ﬁnd a mixed-strategy NE of the game G. LSTMs have three types of layers, one input and one
output layer as well as a varying number of hidden layers depending on the dataset under study.
For our dataset, adding more hidden layers does not improve performance and thus one layer is
sufﬁcient. Moreover, in order to allow a sequence to sequence mapping, we consider an encoderdecoder model. The encoder network takes the input sequence and maps it to a vector of a ﬁxed
dimensionality. The encoded representation is then used by the decoder network to decode the
target sequence from the vector. Fig. 4 summarizes the proposed approach. The trafﬁc encoder
Traffic encoder
Action decoder
Fig. 4: Proposed framework.
takes as an input the historical trafﬁc loads and learns a vector representation of the input timeseries. The multi-layer perceptron (MLP) summarizes the input vectors into one vector. In our
scheme, an MLP is required to encode all the vectors together since a particular action at time
t depends on the values of all other input vectors (i.e., trafﬁc values of all SBSs and WLAN
on all the unlicensed channels). The action decoder takes as an input the summarized vector to
reconstruct the predicted action sequence. All SBSs have the same input vector for the trafﬁc
encoders and thus they share the same trafﬁc encoders. On the other hand, SBSs learn different
action sequences and thus different SBSs use different action decoders.
In the ﬁrst step, we need to train the neural networks in order to learn the parameters of the
algorithm that would maximize the proposed utility function. Therefore, the proposed algorithm
is divided into two phases, the training phase followed by the testing phase. In the former, SBSs
are trained ofﬂine before they become active in the network using the architecture given in
Fig. 4. The input dataset represents the WiFi trafﬁc load distribution on the unlicensed channels
as well as the SBSs trafﬁc load collected over several days. On the other hand, the testing
phase corresponds to the actual execution of the algorithm after which the parameters have been
optimized and is implemented on each SBS for execution during run time.
For the training phase, we train the weights of our neural network using a policy gradient
approach that aims at maximizing the expected return of a policy. This is achieved by representing
the policy by its own function approximator and updating it according to the gradient of the
expected reward with respect to the policy parameters . Consider the set M of M history
trafﬁc sequences corresponding to either an SBS or WiFi on each unlicensed channel, where
M = J + C. Let hm,t ∈Rn and hj,t ∈Rn be, respectively, the hidden vectors of the trafﬁc
encoder m and action decoder of SBS j at time t. hm,t and hj,t are then computed by:
hm,t=φ (vm,t, hm,t−1) , hj,t=φ (vj,t, hj,t−1) ,
where φ refers to the LSTM cell function being used, and vm,t is the input vector. For the
encoder, vm,t =
is the history trafﬁc value. For the decoder, vj,t = [W de(xj,t−1)||αj,c,t−1]
is the vector of the previous predicted action where e() maps discrete value to a one-hot vector,
W d ∈Rn×Nx is a matrix that is used to transform the discrete actions of each of the unlicensed
channels into a vector, and Nx is the number of discrete actions. In our approach, we learn the
channel selection vector for all the channels simultaneously and thus xj,t = [xj,1,t, · · · , xj,C,t].
To learn the mixed strategy of our proposed game, we need to initialize the action space
with a subset of the continuous action space of αj,c,t. A naive approach for working with
continuous action spaces is to discretize the action space; however, this approach would lead to
combinatorial explosion and thus the well known problem of “curse of dimensionality” when
highly discretizing our space and a loss in the accuracy of the predicted action when considering
less discretized values. Therefore, we consider a sampling-based approach where we ﬁrst deﬁne
a probability distribution for the continuous variable αj,c,t and for the discrete variable xj,c,t in
order to deal with the large discrete action space as T increases. We use a softmax classiﬁer to
predict the distribution for the discrete variable xj,t and a Gaussian policy for the distribution of
the continuous variable αj,c,t. For the Gaussian policy, the probability of an action is proportional
to a Gaussian distribution with a parameterized mean and a ﬁxed value for the variance in our
implementation. The variance of the Gaussian distribution deﬁnes the area around the mean from
which we explore the action space. For our implementation, the initial value of the variance is set
to 0.06 in order to increase exploration and then is decreased linearly towards 0.02. Therefore,
deﬁning probability distributions for our variables allows the initialization of the action space
Aj by sampling Z actions from the proposed distributions. This enables the SBSs to learn more
accurate transmission probabilities for αj,c,t, as opposed to ﬁxed discretization, thus satisfying
the fairness constraints. The hidden vector hj,t in the decoder is used to predict the t-th output
actions xj,t and αj,c,t. The probability vector over xj,t and αj,c,t can be deﬁned, respectively, as:
xj,t|xj,<t, αj,c,<t, bLt ∼σ (W xhj,t),
µj,c,t = S (W µhj,t), αj,c,t ∼N(µj,c,t, Var(αj,c,t)),
where µj,c,t and Var(αj,c,t) correspond to the mean value and variance of the Gaussian policy
respectively, W x ∈R|Va|×n, W µ ∈Rn are parameters, σ(.) is the softmax function σ(b)q =
o=1 ebo for q = 1, · · · , O, and S(.) is the sigmoid function where S(b) =
1+e−b and is used
to normalize the value to (0, 1). αj,c,t is computed only when xj,c,t = 1. The probability of the
whole action sequence for SBS j, given a historic trafﬁc sequence bL, pj,aj|bL, is given by:
pj,aj|bL =
(xj,t, αj,c,t)|xj,<t, αj,c,<t, bLt
where bLt=(bL1,t, · · · , bLM,t), xj,<t=[xj,1, · · · , xj,t−1], and µj,c,<t=[µj,c,1, · · · , µj,c,t−1].
Our goal is to maximize the exact expectation of the reward buj(aj, a−j) over the action space
for the training dataset. Therefore, the objective function can be deﬁned as:
uj(pj, p−j),
where D is the training dataset. For this objective function, the REINFORCE algorithm can
be used to compute the gradient of the expected reward with respect to the policy parameters,
and then standard gradient descent optimization algorithms can be adopted to allow the
model to generate optimal action sequences for input history trafﬁc values. Speciﬁcally, Monte
Carlo sampling is adopted to compute the expectation.
In particular, we adopt the RMSprop gradient descent optimization algorithm for the update
rule . The learning rate of a particular weight is divided by a running average of the
magnitudes of recent gradients for that weight. The RMSprop update rule is given by:
E[g2]t = γE[g2]t−1 + (1 −γ)g2
θt+1 = θt −
E[g2]t + ϵ
where θt corresponds to the model parameters at time t, gt is the gradient of the objective function
with respect to the parameter θ at time step t, E[g2]t is the expected value of the magnitudes of
recent gradients, γ is the discount factor, λ is the learning rate and ϵ is a smoothing parameter.
On the other hand, the testing phase corresponds to the actual execution of the algorithm on
each SBS. Based on historical trafﬁc values, each SBS learns the future sequence of actions
based on the learned parameters from the training phase. For practicality, we assume knowledge
of historical measurements of the WiFi activity on each of the unlicensed channels using simple
network management protocol (SNMP) statistics with accurate calibration and of other
SBSs by exchanging past trafﬁc information via the X2 interface as done in and . For
our proposed scheme, the SBSs are trained over a large training dataset taking into account
the trafﬁc load over multiple days. The likeliness that an error occurs at the same time over
multiple days is thus very rare. Moreover, our proposed scheme takes into account a sequence
of history trafﬁc values. Therefore, in case of non-ideal information, the impact of this error
Algorithm 1 Training phase of the proposed approach.
Input: J ; W; C; bLj,t∀j ∈J , t; bLw,c,t∀c ∈C , t.
Initialization: The weights of all LSTMs are initialized following a uniform distribution with arbitrarily small values.
Training: Each SBS j is modeled as an LSTM network.
while Any of the coupled constraints is not satisﬁed do
for Number of training epochs do
for Size of the training dataset do
Step 1. Run Algorithm 2 to compute the best actions for all SBSs.
for j=1:J do
Step 2. Sample actions for SBS j based on the best expected actions of other SBSs.
Step 3. Use REINFORCE to update rule and compute the gradient of the expected value of the reward function.
Step 4. Update model parameters with back-propagation algorithm .
Step 5. Using the incremental penalty algorithm, check the feasibility of the coupled constraints and update the values of ρl accordingly.
Algorithm 2 Testing phase of the proposed approach.
Input: J ; W; C; bLj,t∀j ∈J , t; bLw,c,t∀c ∈C , t.
for For each SBS j do
Step 1. Trafﬁc history encoding: The history trafﬁc of each SBS and WLAN activity on each channel is fed into each of the M LSTM
trafﬁc encoders.
Step 2. Vector summarization: The encoded vectors are transformed to initialize action decoders.
Step 3. Action decoding: Action sequence is decoded for each SBS j.
can be considered to be negligible. The proposed approach can also be combined with online
machine learning to accommodate changes in the trafﬁc model, by properly re-training the
developed learning mechanism. Consequently, the proposed algorithm offers a practical solution
that is amenable to implementation. Here, we note that one practical challenge for deploying
this algorithm in a real-world network is synchronization between SBSs and WAPs. In essence,
such synchronization can be achieved by inter-operator cooperation, using mechanisms such as
in . The training and the testing phases are given in Algorithms 1 and 2 respectively.
Note that guaranteeing the convergence of the proposed algorithm is challenging as it is highly
dependent on the hyperparameters used during the training phase. It has been shown in that
the learning rate and the hidden layer size are the two most important hyperparameters for
the convergence of LSTMs. For instance, using too few neurons in the hidden layers results
in underﬁtting which could make it hard for the neural network to detect the signals in a
complicated data set. On the other hand, using too many neurons in the hidden layers can
result in either overﬁtting or an increase in the training time. Therefore, in this work, we
limit our contribution to providing simulation results (see Section V) to show that, under a
reasonable choice of the hyperparameters, convergence is observed for our proposed game, as
per the following theorem:
Theorem 1. If Algorithm 1 converges, then the convergence strategy proﬁle corresponds to a
mixed-strategy NE of game G.
Proof. In order to prove this theorem, we ﬁrst need to show that the solution of the adopted multiagent learning algorithm converges to an equilibrium point. In fact, every strict Nash equilibrium
is a local optimum for a gradient descent learning approach but the reverse is not always true
(Theorems 2 and 3 in ). Therefore, to show that a gradient-based learning method guarantees
convergence of our proposed game to an equilibrium point, we deﬁne the following lemma.
Lemma 1. The square of a linear function is convex. It follows that the payoff function of player
j deﬁned in (20) is an afﬁne combination of convex functions, and hence is convex. Therefore,
a gradient-based learning algorithm for our game G allows the convergence to an equilibrium
point of that game.
Lemma 1 is the consequence of the convexity of the players’ payoffs where it has been
shown in that under certain convexity assumptions about the shape of payoff functions,
the gradient-descent process converges to an equilibrium point. However, convergence is only
guaranteed under a decreasing step-size sequence . Therefore, given the fact that we employ
an adaptive learning rate method satisfying the Robbins-Monro conditions (λ > 0, P∞
t=0 λ(t) =
t=0 λ2(t) < +∞), one can guarantee that under suitable initial conditions, our proposed
algorithm converges to an equilibrium point.
Moreover, following the penalized reformulation of our game G, one can easily show that
a strategy that violates the coupled constraints cannot be a best response strategy. From ,
there exists ρ∗
l such that the incremental penalty algorithm terminates. Therefore, there exists
a mixed strategy for which the coupled constraints are satisﬁed at ρ∗
l . In that case, there is no
incentive for an SBS to violate any of the coupled constraints, otherwise, its reward function
would be penalized by the corresponding penalty function. Hence, all strategies that violate the
coupled constraints are dominated by the alternative of complying with these constraints. Since
in the proposed algorithm, the optimal strategy proﬁle results in maximizing Epj [buj (aj, a−j)],
SYSTEM PARAMETERS
Parameters
Parameters
Transmit power (Pt)
BW (channel)
CCA threshold
Noise variance
15.3 + 50 log10(m)
Hidden size (encoder)
Hidden size (decoder)
time epoch (t)
1023 slots
Action sampling (Z)
100 samples
History trafﬁc size
7 time epochs
PLTE, PWiFi
Learning rate (λ)
LSTM layers
Learning rate decay (γ)
we can conclude that the converged mixed-strategy NE is guaranteed not to violate the coupled
constraints and hence it corresponds to a mixed-strategy NE for the game G. Therefore, our
proposed learning algorithm learns a mixed strategy of the game G, by using a deep neural
network function approximator to represent strategies, and by averaging those strategies via
gradient descent machine learning techniques.
V. SIMULATION RESULTS AND ANALYSIS
For our simulations, we consider a 300 m × 300 m square area in which we randomly deploy a
number of SBSs and 7 WAPs that share 7 unlicensed channels. We use real data for trafﬁc loads
from the dataset provided in and divide it as 80% for training and 20% for testing. During
the training phase, we randomly shufﬂe examples in the training dataset in order to prevent cycles
when approximating the reward function. Table I summarizes the main simulation parameters.
All statistical results are averaged over a large number of independent runs.
Fig. 5 shows the average throughput gain, compared to a reactive approach, achieved by the
proposed approach for different values of T under three different network scenarios. Here, we
note that, in Fig. 5, the case in which T = 1 corresponds to other proactive schemes such as
exponential smoothing and conventional reinforcement learning algorithms (e.g., Q-learning and
multi-armed bandit) . Intuitively, a larger T provides the framework additional opportunities
to beneﬁt over the reactive approach, which does not account for future trafﬁc loads. First,
evidently, for very small time windows, our proposed approach does not yield any signiﬁcant
gains. However, as T increases, LTE-LAA network utilizes statistical predictions for allocating
Time window T
Average airtime allocation for LTE−LAA
Proposed scheme - 2 SBSs, 2 channels
Proposed scheme - 4 SBSs, 4 channels
Proposed scheme - 7 SBSs, 7 channels
Reactive approach - 2 SBSs, 2 channels
Reactive approach - 4 SBSs, 4 channels
Reactive approach - 7 SBSs, 7 channels
Fig. 5: The average throughput gain for LTE-LAA upon applying a proactive approach (with varying T) as compared
to a reactive approach.
Time window T
Served LTE−LAA traffic load (%)
2 SBSs, 2 channels
4 SBSs, 4 channels
7 SBSs, 7 channels
Fig. 6: The proportion of load served over LTE-LAA as a function of T.
resources and thus the gains start to become more pronounced as compared to the reactive
approach as well as to other proactive approaches at T = 1. For example, from Fig. 5, we can
see that, for 4 SBSs and 4 channels, our proposed scheme achieves an increase of 17% and 20%
in the average airtime allocation for LTE-LAA as compared to other proactive schemes and the
reactive approach, respectively. Eventually, as T grows, the gain of our proposed framework
remains almost constant at the maximum achievable value. This corresponds to the minimum
value of T required to allow the LTE-LAA network smooth out its load over time and thus
achieve maximum gain while guaranteeing fairness to WLAN.
Fig. 6 shows the proportion of LTE-LAA served load for different values of T. Clearly, as T
increases, the proportion of LTE-LAA served trafﬁc increases. For example, the proportion of
Time window T
Average airtime allocation for LTE−LAA
Proposed scheme - 2 SBSs, 2 channels
Proposed scheme - 4 SBSs, 4 channels
PF - 2 SBSs, 2 channels
PF - 4 SBSs, 4 channels
Time window T
Average airtime allocation for WLAN
Proposed scheme - 2 SBSs, 2 channels
Proposed scheme - 4 SBSs, 4 channels
PF - 2 SBSs, 2 channels
PF - 4 SBSs, 4 channels
Time window T
Served total network traffic load (%)
Time window T
Jain's fairness index
Fig. 7: The (a) average airtime allocated for LTE-LAA, (b) average airtime allocated for WLAN, (c) proportion of
served total network trafﬁc load, and (d) Jain’s fairness index resulting from our proposed scheme as well as from
a centralized proportional fairness utility maximization scheme (with varying T).
served load increases from 82% to 97% for the case of 4 SBSs and 4 channels. The gain of the
LTE-LAA network stems from the ﬂexibility of choosing actions over a large time horizon T.
In contrast to the myopic reactive approach, our proposed proactive scheme takes into account
future predictions of the network state along with the current state. Therefore, the optimal policy
will balance the instantaneous reward and the available information for future use and thus
maximizing the total load served over time.
Fig. 7 shows the (a) average airtime allocated for the LTE-LAA network, (b) average airtime
allocated for WLAN, (c) proportion of served total network trafﬁc load, and (d) Jain’s fairness
index as a function of T resulting from our proposed scheme as well as from a centralized
solution considering a proportional fairness (PF) utility function that is widely used for resource
allocation , subject to constraints (8)-(12) with T = 1. Here, we compute the Jain’s index
based on the proportion of served trafﬁc load for each network using J (lo) = (PO
o=1 l2o , where
lo is the proportion of served trafﬁc load for network o and O is number of networks .
The centralized solution of the PF resource allocation is obtained using the branch-and-bound
algorithm in . From Fig. 7 (a), we can see that for small values of T, the PF allocation
offers higher airtime allocation for the LTE-LAA network. For example, for the scenario of
4 SBSs and 4 channels, PF offers airtime gains of 7% and 5% as compared to our proposed
approach for T = 1 and 2 respectively. However, as T increases, our proposed scheme achieves
more transmission opportunities for the LTE-LAA network as compared to the PF solution. For
instance, for the scenario of 2 SBSs and 2 channels, our proposed scheme achieves an increase
of 11% in the transmission opportunities for T ≥8. This gain stems from the proactive resource
allocation approach that allows more ﬂexibility in spectrum allocation as T increases. From
Figs. 7 (b) and (c), we can see that, although the average airtime allocation for WiFi resulting
from our proposed scheme is less than that of the PF scheme for T > 4, the proportion of the
total network trafﬁc load served by our proposed scheme is higher than that of the PF scheme
for all values of T (e.g., 84% for our proposed scheme as compared to 74% for PF for the case
of 2 SBSs and 2 channels and for T > 6). Moreover, from Fig. 7 (d), we can conclude that,
as T increases, our proposed scheme achieves similar fairness performance as that of PF. This
is due to the fact that, for our proposed scheme, as T increases, the proportion of LTE served
trafﬁc load increases while that of WiFi decreases eventually, converging to a constant value
for T > 7. In particular, a relatively large time window allows SBSs to exploit future off-peak
hours on the unlicensed band and thus increasing their transmission opportunities. Therefore, at
the convergence point, the proportion of served trafﬁc load of both technologies is almost the
same. In summary, our proposed scheme allows more transmission opportunities for LTE-LAA,
increases the proportion of the total network served load while also preserving fairness with
WiFi. It offers better tradeoff in terms of efﬁciency and fairness compared to the centralized
PF allocation scheme. Note that the resulting problem for the PF solution is a mixed integer
nonlinear optimization problem (MINLP) and therefore, ﬁnding its solution becomes challenging
for larger network scenarios due to the polynomial computational complexity.
Fig. 8 shows the (a) average airtime allocated for the LTE-LAA network, (b) average airtime
allocated for WLAN, (c) proportion of served total network trafﬁc load, and (d) Jain’s fairness
index as a function of T resulting from our proposed scheme as well as a centralized solution
considering a total network throughput (TNT) utility function subject to constraints (8)-(12)
with T = 1. From Fig. 8 (a), we can see that our proposed resource allocation scheme offers
Time window T
Average airtime allocation for LTE−LAA
Proposed scheme - 2 SBSs, 2 channels
Proposed scheme - 4 SBSs, 4 channels
Proposed scheme - 7 SBSs, 7 channels
TNT - 2 SBSs, 2 channels
TNT - 4 SBSs, 4 channels
TNT - 7 SBSs, 7 channels
Time window T
Average airtime allocation for WLAN
Proposed scheme - 2 SBSs, 2 channels
Proposed scheme - 4 SBSs, 4 channels
Proposed scheme - 7 SBSs, 7 channels
TNT - 2 SBSs, 2 channels
TNT - 4 SBSs, 4 channels
TNT - 7 SBSs, 7 channels
Time window T
Served total network traffic load (%)
Time window T
Jain's fairness index
Fig. 8: The (a) average airtime allocated for LTE-LAA, (b) average airtime allocated for WLAN, (c) proportion of
served total network trafﬁc load, and (d) Jain’s fairness index resulting from our proposed scheme as well as from
a centralized total network throughput utility maximization scheme (with varying T).
higher transmission opportunities for LTE-LAA for all values of T as compared to the centralized
solution considering a TNT utility function. For example, for the case of 4 SBSs and 4 channels,
the gain for our proposed approach can reach up to 52% for T ≥8. Similarly, from Figs. 8 (b)
and (c), we can observe that, although the average airtime allocation for WLAN for our proposed
scheme is less than that of the TNT scheme for all T, the proportion of the total served network
trafﬁc load for our proposed scheme is higher than that of the TNT scheme. From Fig. 8 (d),
we can also conclude that, as T increases, our scheme achieves similar fairness to TNT due
to the fact that, as T increases, the proportion of LTE served trafﬁc load increases while that
of WiFi decreases for our proposed scheme, converging to a constant value for T > 7. At this
convergence point, the proportion of served trafﬁc load of both technologies is almost the same.
In summary, our proposed scheme offers a better tradeoff in terms of efﬁciency and fairness as
compared to the centralized TNT allocation scheme.
LTE/WLAN traffic ratio
LTE/WLAN airtime ratio
LTE/WLAN traffic ratio
LTE/WLAN airtime ratio
Fig. 9: LTE/WLAN airtime ratio as a function of the LTE/WLAN trafﬁc ratio for 3 different values of Mc (Mc = 1,
2 and 3). The LTE and WLAN airtime fraction correspond to the average airtime allocated per SBS and per WAP,
respectively. Moreover, the number of unlicensed channels is ﬁxed to 7 and the number of SBSs is equal to 2 and
7 in (a) and (b) respectively.
Fig. 9 shows the value of the LTE/WLAN airtime ratio under varying LTE/WLAN trafﬁc ratio
and for different values of Mc. Note here that the LTE and WLAN airtime fraction correspond
to the average airtime allocated per SBS and per WAP, respectively. We consider two different
scenarios with varying number of SBSs (2 and 7 SBSs for scenarios (a) and (b) respectively),
while the number of unlicensed channels is ﬁxed to 7. Fig. 9 shows that inter-technology fairness
is satisﬁed. This can be clearly seen in scenario (b) for the case of Mc = 1. For instance, when
the trafﬁc ratio is 1, LTE/WLAN airtime ratio is 1 and thus equal weighted airtime is allocated
for each technology (given that PLTE = 1 and PWiFi = 1). From Fig. 9, we can also see that
enabling carrier aggregation impacts the resource allocation outcome. In fact, we can see that a
considerable gain in terms of spectrum access time can be achieved with carrier aggregation. For
instance, in the case of 2 SBSs, the LTE/WLAN airtime ratio increases from 0.84 for Mc = 1
to 1.7 and 2.4 for Mc = 2 and 3 respectively for the value of 0.6 for LTE/WLAN trafﬁc ratio.
On the other hand, this gain decreases as more SBSs are deployed and for a densely deployed
LTE-LAA network, there is no need to aggregate more channels. This can be seen from (b)
where the LTE-LAA network gets the same airtime share for Mc = 1, 2 and 3 (as also shown
in Remark 1).
Moreover, Fig. 9 shows that deploying more SBSs does not necessarily allow more airtime
for the LTE-LAA network. For example, LTE/WLAN airtime ratio of scenarios (a) and (b)
corresponding to 0.6 LTE/WLAN trafﬁc ratio is equal to 0.84 and 0.6 respectively for Mc = 1.
Number of SBSs
Served LTE−LAA traffic load (%)
Fig. 10: The proportion of LTE-LAA served trafﬁc load as a function of the number of SBSs and for different
number of unlicensed channels (C = 2, 4, and 7).
Consequently, the proposed scheme can avoid causing performance degradation to WLAN in
the case LTE operators selﬁshly deploy a high number of SBSs.
Fig. 10 investigates the proportion of served LTE-LAA trafﬁc for different network parameters.
From Fig. 10, we can see that, as the number of SBSs increases, the proportion of LTE-LAA
served trafﬁc, relative to its corresponding offered load decreases. Moreover, reducing the number
of unlicensed channels leads to a decrease in the proportion of LTE-LAA served trafﬁc. Although
the number of available unlicensed channels are not players in the game, they affect spectrum
allocation action selection for each SBS. As the number of channels increases, the action space
for the channel selection vector increases, thus giving more opportunities for an SBS to serve
more of its offered load.
Fig. 11 shows the total network served trafﬁc load as well as that of LTE-LAA and WiFi as
a function of the priority fairness ratio on the unlicensed band (PLTE/PWiFi) for three different
network scenarios considering T = 6. From Figs. 11 (b) and (c), we can see that more LTE-
LAA and less WiFi trafﬁc load is served as PLTE/PWiFi increases and thus the priority fairness
parameters PLTE and PWiFi can be regarded as network design parameters that can be adjusted in
a way that would avoid LTE-LAA from aggressively ofﬂoading trafﬁc to the unlicensed bands.
Moreover, from Fig. 11 (a), we can see that the served total network trafﬁc load is maximized
at PLTE/PWiFi = 1 thus allowing an efﬁcient utilization of the unlicensed spectrum. On the other
hand, from Fig. 11 (c), we can see that the served WiFi trafﬁc load for our proposed scheme is
greater than or equal to the served WiFi trafﬁc load for the case in which LTE-LAA is replaced
Priority ratio (PLTE/PWiFi)
Served total network traffic load (%)
Priority ratio (PLTE/PWiFi)
Served LTE-LAA traffic load (%)
Priority ratio (PLTE/PWiFi)
Served WiFi traffic load (%)
Fig. 11: The proportion of the (a) total network served trafﬁc load (b) LTE-LAA served trafﬁc load and (c) WiFi
served trafﬁc load as a function of the priority fairness ratio on the unlicensed band, (PLTE/PWiFi). The straight
line in (c) represents the proportion of WiFi served trafﬁc load for the case when the LTE network is replaced by
an equivalent WiFi network.
by an equivalent WiFi network for values of PLTE/PWiFi less than 0.8. From Fig. 11 (c), we can
conclude that the WiFi performance for our proposed spectrum sharing scheme, when considering
equal weighted airtime share (i.e., PLTE/PWiFi = 1), achieves very close performance to the case
when only WLAN is operating over the unlicensed spectrum. For instance, the proportion of
WiFi served trafﬁc load corresponds to 68% for the WiFi-LTE scenario as opposed to 70% for
the WiFi-WiFi scenario in the case of 4 SBS and 4 channels. This slight decrease is mainly
due to the differences in the MAC layers of both technologies. For instance, LTE adopts a more
efﬁcient scheduling mechanism and has less overhead as compared to WiFi. In particular, the
DCF protocol of WiFi results in the channel being unused for some period of time and, thus,
WiFi should be given a slightly larger priority in that case. In summary, we can deduce that the
values of PLTE and PWiFi can be regarded as tuning parameters that allow the network operator
to achieve a tradeoff between efﬁciency and fairness.
Fig. 12 shows the average value of airtime allocated to the LTE-LAA network as a function
of the number of epochs required for the network to converge while considering different values
for the learning rate. The learning rate determines the step size the algorithm takes to reach the
minimizer and thus has an impact on the convergence rate of our proposed framework. Moreover,
an epoch, which consists of multiple iterations, is a single pass through the entire training set,
followed by testing of the veriﬁcation set. From Fig. 12, we can see that for λ = 0.1, our
proposed algorithm requires more than 50 epochs to approximate the reward function, while, for
Number of epochs
Average airtime allocation for LTE−LAA
λ = 0.0005
λ = 0.0001
λ = 0.00005
Fig. 12: The average airtime allocated for LTE-LAA as a function of the number of epochs for different values of
the learning rate.
λ = 0.01, it only needs 20 epochs. In fact, for λ = 0.1, we can see that our proposed algorithm
ﬂuctuates around a different region of the optimization space. Clearly, a learning rate that is too
large can cause the algorithm to diverge from the optimal solution. This is because too large
initial learning rates will decay the loss function faster and thus make the model get stuck at a
particular region of the optimization space instead of better exploring it. On the other hand, a
learning rate that is too small results in a low speed of convergence. For instance, for λ = 0.0001
and λ = 0.00005, our proposed algorithm requires ∼40 epochs to converge. Therefore, although
we use an adaptive learning rate approach, the optimization algorithm relies heavily on a good
choice of an initial learning rate . In other words, the initial value of the learning rate should
be within a particular range in order to have good performance. Choosing a proper learning rate
is an important key aspect that has an impact on the solution as well as the convergence speed.
The optimal value of the initial learning rate is dependent on the dataset under study, where for
each dataset, there exists an interval of good learning rates at which the performance does not
vary much . This in turn necessitates the need for experimental studies in order to search
for good problem-speciﬁc learning rates . A typical range of the learning rate for the dataset
under study falls approximately between 0.0005 and 0.01, requiring ∼20 epochs.
VI. CONCLUSION
In this paper, we have proposed a novel resource allocation framework for the coexistence
of LTE-LAA and WiFi in the unlicensed band. We have formulated a game model where each
SBS seeks to maximize its rate over a given time horizon while achieving long-term equal
weighted fairness with WLAN and other LTE-LAA operators transmitting on the same channel.
To solve this problem, we have developed a novel deep learning algorithm based on LSTMs. The
proposed algorithm enables each SBS to decide on its spectrum allocation scheme autonomously
with limited information on the network state. Simulation results have shown that the proposed approach yields signiﬁcant performance gains in terms of rate compared to conventional
approaches that considers only instantaneous network parameters such as instantaneous equal
weighted fairness, proportional fairness and total network throughput maximization. Results have
also shown that our proposed scheme prevents disruption to WLAN operation in the case large
number of LTE operators selﬁshly deploy LTE-LAA in the unlicensed spectrum. For future
work, we will consider a multi-mode SBS that operates over the sub-6 GHz licensed band, sub-
6 GHz unlicensed band, and the high-frequency mmWave band. Here, we aim at addressing the
problem of coexistence of a multi-mode SBS with WiFi on the 5 GHz band and with WiGig on
the mmWave band, simultaneously.