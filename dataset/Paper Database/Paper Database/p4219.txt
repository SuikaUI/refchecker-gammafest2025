Comparative Deep Learning of Hybrid Representations
for Image Recommendations
Chenyi Lei, Dong Liu, Weiping Li, Zheng-Jun Zha, Houqiang Li
CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System,
University of Science and Technology of China, Hefei 230027, China
 , {dongeliu,wpli,zhazj,lihq}@ustc.edu.cn
In many image-related tasks, learning expressive and
discriminative representations of images is essential, and
deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such
as image recommendations, call for effective representations of not only images but also preferences and intents of
users over images. Such representations are termed hybrid
and addressed via a deep learning approach in this paper.
We design a dual-net deep network, in which the two subnetworks map input images and preferences of users into a
same latent semantic space, and then the distances between
images and users in the latent space are calculated to make
decisions. We further propose a comparative deep learning
(CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their
relative distances. The CDL embraces much more training
data than naive deep learning, and thus achieves superior
performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data
sets for image recommendations have shown the proposed
dual-net network and CDL greatly outperform other stateof-the-art image recommendation solutions.
1. Introduction
With the increasing abundance of images, ﬁnding out images that satisfy user needs from a huge collection is more
and more required, which emphasizes the importance of image search and image recommendations working as ﬁlters
for users. Such tasks are not trivial, however, due to the
gap in understanding the semantics of images as well as
the gap in understanding the intents or preferences of users
This work was supported by the National Program on Key Basic Research Projects (973 Program) under Grant 2015CB351800, by the Natural
Science Foundation of China (NSFC) under Grants 61303149, 61331017,
61390512, and 61472392, and by the Fundamental Research Funds for the
Central Universities under Grants WK2100060011 and WK3490000001.
over images. Compared to their counterparts for structured
data, such as search of text and recommendations of book
or movie, image search and recommendations raise more
challenges since images lack an immediately effective representation.
How to represent images both expressively and discriminatively is of essential importance in many image-related
tasks including detection, registration, recognition, classi-
ﬁcation, and retrieval. This problem had been extensively
studied, and many kinds of hand-crafted features had been
designed and adopted in different tasks . Most of
previous work focuses on low-level visual features of images, but for image search and recommendations, it is often
not clear how to represent the intents or preferences of users
within the framework of low-level features.
One feasible solution that has been studied is to utilize
the users’ information as constraints to reﬁne the image representations, making them consistent with both semantic labels and user provided hints . For example, Liu
et al. proposes to learn an image distance metric by
combining the images’ visual similarity and their “social
similarity,” deﬁned from users’ interests in images that are
mined from user data in online social networks. Nonetheless, visual content of images and users’ intents/preferences
on images are of two different modalities, simply combining them may not turn out efﬁcient enough.
Recently, deep network models have attracted much attention of researchers in the image processing ﬁeld. One
signiﬁcant advantage of deep networks is the automated
learning of image representations, which are demonstrated
to be more effective than hand-crafted features, especially
in semantic level image understanding .
deep networks have achieved great success in processing
other forms of data such as speech and text . Promisingly, multimodal data, such as images and users’ intents/preferences, may be efﬁciently handled by a single integrated deep network.
In this paper, we study a dual-net deep network model
for the purpose of making recommendations of images to
 
users. The network consists of two sub-networks, which
map an image and the preferences of a user into a same latent semantic space, respectively. Therefore, the network
achieves representations of both images and users, termed
hybrid representations hereafter, and these hybrid representations are directly comparable to make decisions of recommendations.
Moreover, we propose a comparative deep learning
(CDL) method to train the designed deep network.
Instead of a naive learning, e.g. learning a distance between
a user and an image, the CDL uses two images compared against one user, and learns the relative distances
among them. Our key idea is depicted in Fig. 1, where
for a query user, her historical data used for learning consist of “positive” images, e.g. her favorites, and “negative” images, e.g. her dislikes; the objective of CDL is
that the distance between the user and a positive image
shall be less than the distance between the user and a negative image. Thus, training data for CDL are triplets of
(user, positive image, negative image) and these data
are fed into a triple-net deep network consisting of three
sub-networks, one of which is for user, and the other two
are for positive and negative images and are actually identical, as shown in Fig. 2. Note that after training, we need
only two sub-networks for user and image, respectively.
The designed dual-net network and CDL method have
been veriﬁed on an image recommendation task with realworld data sets. Experimental results display that the proposed CDL achieves superior performance than naive learning, and our proposed solution outperforms other state-ofthe-art image recommendation methods signiﬁcantly.
The remainder of this paper is organized as follows. Related work is discussed in Section 2. Then our proposed
CDL-based image recommendation solution is described,
the objective of CDL is formulated in Section 3, followed
by detailed description of the deep network model in Section 4, and details of making image recommendations in
Section 5. Experimental results are reported in Section 6,
and concluding remarks in Section 7.
2. Related Work
We give brief overview of related work at two aspects:
learning of image representations and personalized image
recommendations.
2.1. Learning of Image Representations
In view of the limitation of hand-crafted image features
such as those designed in , more and more research focuses on designing effective deep learning models
to extract image representations automatically .
Karpathy et al. proposes a supervised hashing method
with deep learning architecture, followed by a stage of simultaneous learning of hash function and image represen-
Tags: Flowers, Animals, Cat
Friend List: Alice, Bob
Group List: Animal Lover
Positive Images
Negative Images
Latent Space
User Preferences
Figure 1. This ﬁgure depicts the key idea of our proposed comparative deep learning (CDL). One user’s preferences can be described
by her frequently used tags as well as her friends’ preferences and
her joined groups’ preferences. These preferences, together with
images, are mapped into a same latent semantic space. In that
space, the distance between the user and a “positive” image (e.g.
favorite image) shall be less than the distance between the user
and a “negative” image (e.g. disliked image), which is taken as the
objective of CDL.
tations. Furthermore, it is noticed that middle-layer outputs
in deep learning models can be seamlessly utilized as image
representations, though the deep network is not trained for
that . For example, Krizhevsky et al. proposes a deep learning architecture to perform image classi-
ﬁcation, and the outputs of the 7th full-connection layer are
also veriﬁed to be kind of robust image representations.
The abovementioned work mainly focuses on low-level
visual features of images. But recently, along with the development of user-centric applications such as image recommendations, it is worthwhile to learn not only visual information but also intents or preferences of users for image representations. A paucity of work has made attempts
at this aspect . Pan et al. proposes an embedding method to study the cross-view (i.e. text to image views) search problem with analyses of user click log.
Liu et al. consider jointly the users’ social relationship
and images’ visual similarity to learn a new image distance
metric. But such work relies heavily on carefully designed
hand-crafted features. Liu et al. employ deep learning
architecture to capture user intent and image visual information, where user intent is described by only similarity
between a pair of users. But in practice, there is multimodal information for drawing upon user intents, such as
tags, browsing history and social groups. Moreover, the
deep architecture in considers only one image at each
training round. To the contrary, recent studies 
indicate that deep ranking models perform much better by
forming training data as triplets. To summary, how to design an effective deep learning architecture to capture both
visual information and the intents or preferences of users
over images is still a challenging open problem.
2.2. Personalized Image Recommendations
Personalized recommendations for structured data such
as book, movie, and music have been studied for a long
while . Typical technologies include content-based ﬁltering, collaborative ﬁltering, and hybrid of both . However, it is difﬁcult to directly adopt these technologies for
image recommendations, possibly due to several difﬁculties: images are highly unstructured and lack an immediate representations, user-image interaction data are often
too sparse, users rarely provide ratings on images but rather
give implicit feedback. Nevertheless, mature technologies
in recommender systems are still inspiring, for example,
matrix factorization can be perceived as to learn latent
representations of users and items in a same semantic space.
With the development of social networks, recent research
starts to leverage social data to improve the performance of
recommendations . Most of existing work on image recommendations also follows this line .
For example, Jing et al. propose a novel probabilistic matrix factorization framework that combines the ratings
of local community users for recommending Flickr photos.
Cui et al. propose a regularized dual-factor regression
method based on matrix factorization to capture the social
attributes for recommendations. These methods ignore the
visual information of images, instead, they focus solely on
modeling users by discovering user proﬁles and behavior
patterns. The representations of images and users are still
isolated due to semantic gap and the sparsity of user-image
interactions.
Only a few recent work is concentrated on joint modeling of users and images for making recommendations . Sang et al. propose a topic sensitive model
that concerns user preferences and user uploaded images to
study users’ inﬂuences in social networks. Liu et al. 
propose to recommend images by voting strategy according to learnt social embedded image representations. Till
now, the existing methods often perform separate processing of user information and image and then simply combining them. A fully integrated solution is to be investigated.
Formulation
Comparative
We address the hybrid representations, i.e. simultaneous
representations of both users and images in a same latent
space, via a deep learning approach. For this learning, how
to prepare training data is not obvious. Given the fact that
users rarely provide ratings on images due to the abundance
of online images, we shall be able to utilize users’ implicit
feedback on images. Such feedback, however, is still sparse
and severely unbalanced, usually negative feedback is almost none . A naive learning, e.g. learning a distance
between a user and an image, will probably fail due to the
training data.
Motivated by previous efforts on deep ranking models , we propose a comparative learning method
to tackle the imperfect training data. Several symbols are
deﬁned as follows. Let an image be I and a user be U, we
have deﬁned functions π(I) and φ(U) that map I and U
to a same latent space, respectively. Another function D is
used to measure the distance between any two vectors in the
learnt latent space. Note that instead of learning a distance
between user and image, we propose to learn comparatively
the relative distances between a user and two images. That
is, the learning algorithm is given a sequence of triplets,
{Tt = (Ut, I+
t ), t = 1, 2, ..., T},
where T is the total amount of triplets, Ut, I+
t indicate
the triple input elements, i.e. query user Ut prefers image I+
than image I−
t . Then, the learning is to ﬁnd such mapping
functions π(·) and φ(·) and such a distance function D(·, ·),
to satisfy
D(π(Ut), φ(I+
t )) < D(π(Ut), φ(I−
To fulﬁll this learning, we may perceive Eq. (2) as a
binary classiﬁcation problem (the former distance is less
or more than the latter), and thus can reuse the 0-1 loss
function, or its better alternatives such as hinge loss function.
However, in order to make the distance measure
more discriminative (in Eq.
(2) the difference between
the two distances should be as large as possible), we may
also adopt cross entropy as loss function. Speciﬁcally, let
ij = D(π(Ut), φ(i)) −D(π(Ut), φ(j)), and
we further deﬁne
t , j = I−
t , j = I+
then our learning objective is deﬁned by cross entropy as,
π,φ,DL({Tt}) =
ij log(P Ut
ij ) −(1 −¯P Ut
ij ) log(1 −P Ut
In this paper, we are interested in learning representations of users and images and thus we may assume the distance function D to be quite simple, for example the Euclidean. Then, the comparative learning leads to solutions
Convolution
Convolution
Convolution
Convolution
Convolution
information
Image visual
information
Image visual
information
Max Pooling
Max Pooling
Max Pooling
Convolution
Convolution
Convolution
Convolution
Convolution
Max Pooling
Max Pooling
Max Pooling
Figure 2. This ﬁgure depicts the deep network used for comparative deep learning (CDL). There are three sub-networks that all output
1024-dim vectors as representations of images and users, respectively. The top and bottom sub-networks processing images are identical.
The middle sub-network is processing users. Following these sub-networks are two distance calculating nets. The difference between
distances is fed into the ﬁnal cross-entropy loss function for comparison with label. The numbers shown above each arrow give the size of
the corresponding output. The numbers shown above each box indicate the size of kernel and size of stride for the corresponding layer.
to the mapping functions π(·) and φ(·) that generate representations seamlessly. Traditionally, such learning problems were solved by hand-crafted shallow models, but our
case raises more difﬁculties, since it is required to learn two
mapping functions at the same time and the two functions
are dealing with quite different modalities but shall embed
into a same space. We turn to deep learning to solve this
4. Comparative Deep Learning (CDL)
As illustrated in Fig. 2, we design a deep network to
perform the proposed comparative deep learning (CDL).
This network architecture takes triplets as inputs, i.e.
t ) with a query user Ut having relatively shorter
distance from image I+ than from image I−. There are
three sub-networks in the CDL architecture. The top and
bottom sub-networks are two convolutional neural networks
(CNNs) with identical conﬁguration and shared parameters,
they are designed to capture image visual information. The
middle sub-network is a full-connection neural network that
is designed for user’s information.
The two kinds of sub-networks in our architecture correspond to mapping functions for image I : π(I) ∈Rd
and for user U : φ(U) ∈Rd, respectively, where Rd is the
target latent space. The outputs of these sub-networks are
indeed hybrid representations of images and users (FC1 3,
FC2 4 and FC3 3 in Fig. 2). To guarantee that the learnt
functions π(·) and φ(·) can embed multimodal information
into the same latent space, we add two distance calculating
nets that outputs two distances (FC top and FC bottom in
Fig. 2), and the difference between distances, i.e. oUt
Eq. (3), is fed into the ﬁnal cross entropy loss function to
be veriﬁed by the label. In the rest of this section, we will
describe each part of the architecture in more details.
In the top/bottom sub-network, there are 5 convolutional
layers, 3 max-pooling layers and 3 full-connection layers.
These conﬁgurations including the sizes of convolution kernels in the convolution layers and the numbers of neurons
in the full-connection layers are remarked in Fig. 2. The
architecture and settings of this sub-network are inspired
by AlexNet , which achieves great success in modeling
image visual information. Input to this sub-network are the
pixel data of RGB channels of an image, and output of this
sub-network is a 1024-dim vector (FC1 3 and FC3 3).
The middle sub-network is designed for capturing user’s
information. Users’ preferences/intents can be described in
various forms and different kinds of data. However, normally neural networks accept only numerical vector inputs.
We adopt a traditional full-connection network to map an
input user vector to the representation, and leave the process
of converting practical data into user vectors to be deﬁned in
Section 5. This sub-network also outputs 1024-dim vectors
(FC2 4) to be comparable with the image representations.
Afterwards, the deep network performs distance calculation. As the focus of this paper is on effective hybrid representations, we assume the distance function shall be quite
simple, yet we still design a sub-network for calculating distance. It is completed by ﬁrst calculating the element-wise
difference vector (DIFF1 and DIFF2 shown in Fig. 2), then
calculating the element-wise square (Square1 and Square2
in Fig. 2), and ﬁnally using a full-connection layer to derive the distance. A special note is that we adopt the idea of
dropout (at rate 0.5) to bring in some randomization factors to select partial dimensions of the learnt representations. The full-connection layer acts as weighting factors
on the different dimensions of squared difference vector,
and thus the distance calculating sub-network is equivalent
to weighted l2-norm distance function. Many complicated
distance calculating networks can be adopted herein, but we
leave them for future exploration.
5. CDL for Image Recommendations
Since our proposed CDL learns hybrid representations,
it is well suitable for user-centric image processing tasks.
In this paper, we take personalized image recommendation
task as an example to discuss on the utility of CDL. We will
restrict our discussions to recommending new images to a
user based on her browsing history and will not dive into
details of practice. There are two key issues to be solved
before applying the CDL. First, how to preprocess user data
to generate user vectors as inputs to the deep network. Second, how to prepare triplets as training data.
There are several intuitive methods to generate user vectors. A straightforward method is bag-of-words, for example, using a vector whose dimension is equal to the amount
of possible tags, and entries of this vector correspond to the
interest levels of this user in these tags. Such interest levels can be estimated from the user’s browsing history and
tagging history, and so on. This method faces two challenges. First, tags may be too many and accordingly the
vector may be too sparse. Second, the method cannot deal
with synonyms of tags. In this paper, we use the well-known
word2vector as a remedy for these problems. Tags are
converted to vectors1 and then vectors are clustered by kmeans into 1024 semantic clusters. Then, tags are replaced
by clusters and the bag-of-words method works on these
clusters. Fig. 3 shows the distribution of clusters, where we
observe the clusters have variant frequencies and bear topical polymerism to some degree.
downloaded
 
The amount of users
Figure 3. This ﬁgure shows the distribution of clusters. The x-axis
displays 1024 clusters and the y-axis is the number of users having
interests in this cluster. A user can be described by bag of words
where words are indeed clusters.
Since the input is a set of triplets in our proposed CDL, it
is desirable to generate a set of pairwise images (a positive
image and a negative image) for each user. Positive images
for users are often handy since users’ behavior data such
as “add to favorites” and “like” give such information explicitly. However, negative images are not obvious . An
image is not “liked” by a user dose not necessarily indicate
the user is not interested in the image, but rather the user
never saw it. We utilize social data to help solve this problem. In general, a user has friendship with another usually
indicates that both users have similar interests, and users
of the same social group have similar interests also. For a
speciﬁc user, we deﬁne the set of potentially liked images
as her friends’ favorite images and the images “liked” by
users in her joined groups. We then assign the images to be
negative, which have no tag of the user’s interests and are
not belonging to the set of potentially liked images. Due
to abundance of negative images assigned in this manner,
random sampling can be performed to generate a subset of
triplets for training purpose.
Last but not the least question is how to make recommendations for users. This is performed in the following
steps. First, a set of candidate images are selected, where
each candidate shall have at least one tag of the user’s interests. Second, the representations of these candidate images
as well as of the user are calculated; these representations
can be calculated and stored in advance, or can be calculated in parallel to accelerate. Third, distances are calculated among the images and the user. Finally, K nearest
neighboring images having minimum distances are chosen
as recommendations.
6. Experiments
In this section, we report the conducted experiments to
evaluate the efﬁcacy of the proposed CDL of hybrid representations. We ﬁrst introduce our experimental settings.
Then, we evaluate the performance of our learnt hybrid rep-
The amount of users (in logs)
The amount of user favorite images
The amount of users (in logs)
The amount of user tags
Figure 4. Distributions of the amounts of user favorite images and
user tags. Note the logarithm scale of y-axis.
resentations in personalized image recommendation task.
Finally, we give some insights of our proposed approach.
6.1. Experimental Settings
In this paper, we use the same dataset as reported in .
The images and users’ information in this dataset are
crawled from Flickr through its API. There are 101, 496 images, 54, 173 users, 6, 439 groups and 35, 844 tags in this
dataset. The details of crawling can be found in . On
average, there are 23.5 tags and 5.8 favorite images for each
user. Due to the sparsity of user-image interactions, this
dataset is not quite suitable for traditional recommendation
algorithms, especially collaborative ﬁltering. Therefore, we
do not compare our method with them.
The distributions of the amounts of user favorite images
and user tags are shown in Fig. 4. Both of them are typical
long-tail distributions. Users having modest favorite images
and tags usually have most valuable and robust information . Too few favorites indicate inactivity of user, and
too many favorites indicate quite diverse interests of user.
Thus, we ﬁlter out users that have less than 40 or more than
200 favorite images from test . Furthermore, according
to statistics shown in Fig. 3, we further ﬁlter out users that
have interests in less than 80 or more than 280 clusters from
training data, so as to improve the accuracy of training, but
keep them for test. Finally, we have 8, 616 users for training
and 15, 023 users for test.
For each user, 20 images are randomly selected from her
favorite images and “concealed” for test. Training data are
then generated by randomly sampling the rest favorite images as well as assigned negative images (c.f. Section 5). 20
triplets are sampled for each user for training. Finally, there
are 72, 161 distinct images in training data. After training,
the concealed favorite images are retrieved and mixed with
other 80 images (for each user) for test.
Compared Approaches
As discussed in Section 3, our proposed CDL allows the
choice of different loss functions and we have used cross entropy for better performance. We also tested the use of hinge
loss in replacement of cross entropy. Moreover, we compare
our method with several state-of-the-art approaches.
Borda Count with SIDL . Social embedding Image Distance Learning (SIDL) is a novel image distance
learning method that embeds the similarity of collective
social and behavioral information into visual space. After learning the social embedding image distance metric, it
can be adopted together with Borda Count method to
perform personalized image recommendations, as detailed
Borda Count with BoW, ImageNet , LMNN ,
Social+LMNN . Bag of Words (BoW) feature is a traditional hand-crafted visual representation, and ImageNet
feature stands for deep learning based representation, both
can be used to measure image similarity with e.g. Euclidean
Large Margin Nearest Neighbor (LMNN) is a
metric learning method to reduce the margins of the nearest neighbors. Liu et al. proposes to embed social similarity into LMNN, termed Social+LMNN. We then use BoW,
ImageNet, LMNN, and Social+LMNN with Borda Count
method to evaluate the performance of personalized image
recommendations.
TwoNets. In this paper, we propose the CDL instead
of naive deep learning to learn hybrid representations. To
demonstrate the effectiveness of CDL, we also perform
the naive learning experiment called TwoNets.
Speciﬁcally, TwoNets is similar to the CDL but it has only two
sub-networks, which process user and image, respectively;
the output representations of the two sub-networks are directly compared to calculate a distance, and the distance
is re-scaled by a logistic sigmoid function and then compared with the ground-truth by cross entropy loss function;
note that in TwoNets, training data consist of doublets of
(user, image) and the ground-truth is 0 or 1 indicating negative or positive.
Implementation
We implement the CDL and TwoNets methods based on the
open source deep learning software Caffe . In our experiments, all images are resized to 256 × 256. The structure
and parameters of sub-networks are illustrated in Fig. 2, and
all probabilities of dropout are set to 0.5 . The learning rate starts from 0.001 for all layers and the momentum
is 0.9. The mini-batch size of images is 128. The weight
decay parameter is 0.0005. Training was done on a single
GeForce Tesla K20c GPU with 5GB graphical memory, and
it took about 4 days to ﬁnish training.
6.2. Overall Performance
In our personalized image recommendation task, the target is to recommend 20 images out of 100 candidates for
Precision@K
ImageNet Features
Social+LMNN
CDL+Hinge-Loss
CDL+Cross-Entropy
Figure 5. Precision@K for different K values of compared image
recommendation methods.
each user. To make a fair comparison, we implement every comparative method to return top K recommended images where K is adjustable. Precision@K and Recall@K
are used to evaluate the performance of each method, which
are shown in Figs. 5 and 6, respectively. It can be seen that
our approach performs the best in both precision and recall for all K values, which demonstrates the effectiveness
of our proposed CDL of hybrid representations. Note that
using cross entropy as loss function has obvious advantage
compared to using hinge loss function in our image recommendation task (c.f. Section 3).
The approaches based solely on hand-crafted visual representations, i.e. BoW and LMNN, perform poorly in making recommendations. The Precision result of BoW is near
to random guess (random guess for recommending 20 out
of 100 achieves precision 0.2). ImageNet Features lead to
much better results, almost the third best after our CDL
methods and SIDL, which shows the advantage of deep
learning based representations.
If we add social factors to constrain LMNN (i.e. Social+LMNN), the performance will be improved a lot, due
to the utilization of extra information besides visual features. The SIDL performs better than Social+LMNN, indicating the importance of carefully designed features to
capture visual information and embedding functions to integrate multimodal information. Compared to SIDL, our
approach leads signiﬁcant gains of average 42.58% and
46.50% for precision and recall, respectively. It owes to the
superiority of deep network models over traditional handcrafted models especially in capturing visual information.
It should be noted that TwoNets, also adopting deep
network model, has very poor performance. It is slightly
better than BoW but the latter is near to random guess.
Thus, deep network models do not guarantee great success especially when the task is complicated (learning hybrid representations) and the training data are imperfect
(unreliable negative samples). The proposed CDL outperforms TwoNets signiﬁcantly and consistently, which further
ImageNet Features
Social+LMNN
CDL+Hinge-Loss
CDL+Cross-Entropy
Figure 6. Recall@K for different K values of compared image recommendation methods.
Figure 8. Exemplar input and output of the user sub-network in our
designed dual-net deep network. Left: pre-processed user vector
(input). Right: learnt user representation (output).
demonstrates the effectiveness of the proposed comparative
learning method.
6.3. Case Study and Insights
In this section, we present a case for comprehensive
study to give some insights of our proposed approach. For
the selected user whose word cloud of frequent tags can be
found in Fig. 7 (Middle), we prepare a set of images for
training, illustrated in Fig. 7 (Left). It can be observed that
positive images match the user’s preferences as described
by the word cloud, e.g. portrait, woman and mood. Obviously, there are large differences between positive images
and negative images, which veriﬁes the effectiveness of our
designed process for assigning negative images for training
(c.f. Section 5).
Our approach’s recommendation results for this user are
shown in Fig. 7 (Right). Precision@20 is as high as 70%
in this case. Given a closeup view, most of correct recommendations made by our approach are portraits with
darker tone and gloomy atmosphere, in the similar topics
and styles of the user’s word cloud and the positive images
in training data. Interestingly, the 1st image in the 2nd row
in Fig. 7 (Right) is not belonging to the styles mentioned
above. Such images are not easy to be recommended if using purely tags. But we may compare this image with the
2nd image in the 2nd row in Fig. 7 (Left), and observe their
similarity in the sense of color, bokeh, and theme. This is
Figure 7. (Best view in color.) Case study of making recommendations to a selected user. Left: some samples of training images for this
user, 10 positive and 10 negative, separated by the red line; unlike positive images that are indeed favorite images of this user, negative
images are “assigned” by the process discussed in Section 5. Middle: the word cloud of this user’s frequent tags retrieved from her
tagging history and browsing history. Right: recommendation results sorted in relevance (ascending order of distance calculated by hybrid
representations), where correct results are highlighted by red borders.
Figure 9. Exemplar learnt representations of positive images (top
row) and negative images (bottom row). Note the similarity between positive images and dissimilarity between positive and negative images, especially in the circled areas.
where image representations help.
Several images in the recommendation results are not
“correct” according to ground-truth, but we cannot say
ﬁrmly that the user dislikes these images since we do not
know whether the user has ever seen them. Especially, the
2nd image in the 2nd row and the last image in Fig. 7 (Right)
are probably what user may like. Both images match the
word cloud and the user’s positive images in the training
data. However, the 1st image in the 4th row is probably a
mistake of recommendation. This photo has darker background and a human-like object (which is actually a skeleton), which interprets its being selected, but per view of the
user’s positive images, skeleton may not be his/her favorite.
In such cases, making ﬁner discrimination of similar objects
may help improve the recommendation accuracy.
Fig. 8 illustrates the input and output of the user subnetwork learnt by CDL. Fig. 8 (Left) is the input vector, indeed a bag-of-words vector spanned over clusters of
word2vector results, such vector is very sparse, dominated
by several interests. Fig. 8 (Right) is the learnt user representation, not sparse any more. It shall be noted that such
dense vectors are due to the following distance calculation
(weighted l2-norm distance).
Fig. 9 illustrates several examples of learnt image representations, where the top row shows positive images and
the bottom row shows negative images.
It is interesting
to ﬁnd that the representations of two positive images are
quite similar, while they are very different from the representations of two negative images. Some obvious similarity and dissimilarity are highlighted by circles in the ﬁgure. However, such information is not easily perceived from
the images themselves. Therefore, simultaneous learning of
hybrid representations is indeed quite different from only
learning the representations of images.
7. Conclusions
In this paper, we explore learning of hybrid representations to capture both visual information and intents or preferences of users over images, and utilizing such representations for user-centric tasks such as personalized image recommendations. A dual-net deep network model is proposed
to learn representations in a latent semantic space. We also
propose a comparative deep learning method to train the designed deep network, in which triplets of users and positive/negative images are taken as inputs and the relative distances are the objective of learning. The empirical evaluations on personalized image recommendation task show that
our proposed approach achieves much better performance
than naive deep learning as well as several state-of-the-art
image recommendation solutions. The proposed comparative deep learning can be applied to many other user-centric
applications, such as image search and image editing. We
will further explore along these directions.