Computational Optimal Transport
Gabriel Peyré
CNRS and DMA, ENS
Marco Cuturi
Google and CREST, ENSAE
 
@article{COTFNT,
year = {2019},
volume = {11},
journal = {Foundations and Trends in Machine Learning},
title = {Computational Optimal Transport},
number = {5-6},
pages = {355--607}
author = {Gabriel Peyr\’e and Marco Cuturi}
Introduction
Theoretical Foundations
Histograms and Measures . . . . . . . . . . . . . . . . . . . . . . . . . . .
Assignment and Monge Problem . . . . . . . . . . . . . . . . . . . . . . .
Kantorovich Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Metric Properties of Optimal Transport
. . . . . . . . . . . . . . . . . . .
Dual Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Special Cases
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Algorithmic Foundations
The Kantorovich Linear Programs
. . . . . . . . . . . . . . . . . . . . . .
C-Transforms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Complementary Slackness . . . . . . . . . . . . . . . . . . . . . . . . . . .
Vertices of the Transportation Polytope
. . . . . . . . . . . . . . . . . . .
A Heuristic Description of the Network Simplex . . . . . . . . . . . . . . .
Dual Ascent Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Auction Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Entropic Regularization of Optimal Transport
Entropic Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Sinkhorn’s Algorithm and Its Convergence . . . . . . . . . . . . . . . . . .
Speeding Up Sinkhorn’s Iterations
. . . . . . . . . . . . . . . . . . . . . .
Stability and Log-Domain Computations . . . . . . . . . . . . . . . . . . .
Regularized Approximations of the Optimal Transport Cost . . . . . . . . .
Generalized Sinkhorn
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Semidiscrete Optimal Transport
c-Transform and ¯c-Transform . . . . . . . . . . . . . . . . . . . . . . . . .
Semidiscrete Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . .
Entropic Semidiscrete Formulation . . . . . . . . . . . . . . . . . . . . . .
Stochastic Optimization Methods . . . . . . . . . . . . . . . . . . . . . . .
W1 Optimal Transport
W1 on Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
W1 on Euclidean Spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . .
W1 on a Graph
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Dynamic Formulations
Continuous Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
Discretization on Uniform Staggered Grids . . . . . . . . . . . . . . . . . . 105
Proximal Solvers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Dynamical Unbalanced OT . . . . . . . . . . . . . . . . . . . . . . . . . . 108
More General Mobility Functionals . . . . . . . . . . . . . . . . . . . . . . 110
Dynamic Formulation over the Paths Space
. . . . . . . . . . . . . . . . . 111
Statistical Divergences
ϕ-Divergences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Integral Probability Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . 120
Wasserstein Spaces Are Not Hilbertian . . . . . . . . . . . . . . . . . . . . 125
Empirical Estimators for OT, MMD and ϕ-divergences
. . . . . . . . . . . 128
Entropic Regularization: Between OT and MMD . . . . . . . . . . . . . . . 131
Variational Wasserstein Problems
Diﬀerentiating the Wasserstein Loss
. . . . . . . . . . . . . . . . . . . . . 134
Wasserstein Barycenters, Clustering and Dictionary Learning . . . . . . . . 138
Gradient Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
Minimum Kantorovich Estimators . . . . . . . . . . . . . . . . . . . . . . . 155
10 Extensions of Optimal Transport
10.1 Multimarginal Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
10.2 Unbalanced Optimal Transport . . . . . . . . . . . . . . . . . . . . . . . . 162
10.3 Problems with Extra Constraints on the Couplings . . . . . . . . . . . . . . 165
10.4 Sliced Wasserstein Distance and Barycenters . . . . . . . . . . . . . . . . . 166
10.5 Transporting Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . 169
10.6 Gromov–Wasserstein Distances . . . . . . . . . . . . . . . . . . . . . . . . 172
References
Optimal transport (OT) theory can be informally described using the words of the
French mathematician Gaspard Monge (1746–1818): A worker with a shovel in hand
has to move a large pile of sand lying on a construction site. The goal of the worker is
to erect with all that sand a target pile with a prescribed shape (for example, that of a
giant sand castle). Naturally, the worker wishes to minimize her total eﬀort, quantiﬁed
for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability
distributions—two diﬀerent piles of sand of the same volume. They consider all of the
many possible ways to morph, transport or reshape the ﬁrst pile into the second, and
associate a “global” cost to every such transport, using the “local” consideration of
how much it costs to move a grain of sand from one place to another. Mathematicians
are interested in the properties of that least costly transport, as well as in its eﬃcient
computation. That smallest cost not only deﬁnes a distance between distributions, but
it also entails a rich geometric structure on the space of probability distributions. That
structure is canonical in the sense that it borrows key geometric properties of the underlying “ground” space on which these distributions are deﬁned. For instance, when
the underlying space is Euclidean, key concepts such as interpolation, barycenters, convexity or gradients of functions extend naturally to the space of distributions endowed
with an OT geometry.
OT has been (re)discovered in many settings and under diﬀerent forms, giving it
a rich history. While Monge’s seminal work was motivated by an engineering problem,
Tolstoi in the 1920s and Hitchcock, Kantorovich and Koopmans in the 1940s established its signiﬁcance to logistics and economics. Dantzig solved it numerically in 1949
within the framework of linear programming, giving OT a ﬁrm footing in optimization.
OT was later revisited by analysts in the 1990s, notably Brenier, while also gaining
fame in computer vision under the name of earth mover’s distances. Recent years have
witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to large problem dimensions. As a consequence, OT is
being increasingly used to unlock various problems in imaging sciences (such as color
or texture processing), graphics (for shape manipulation) or machine learning (for regression, classiﬁcation and generative modeling).
This paper reviews OT with a bias toward numerical methods, and covers the
theoretical properties of OT that can guide the design of new algorithms. We focus in
particular on the recent wave of eﬃcient algorithms that have helped OT ﬁnd relevance
in data sciences. We give a prominent place to the many generalizations of OT that
have been proposed in but a few years, and connect them with related approaches
originating from statistical inference, kernel methods and information theory. All of
the ﬁgures can be reproduced using code made available in a companion website1. This
website hosts the book project Computational Optimal Transport. You will also ﬁnd
slides and computational resources.
now Publishers Inc.. Computational Optimal Transport. Foundations and Trends
⃝in Computer
Graphics and Vision, vol. XX, no. XX, pp. 1–205, 2020.
DOI: 10.1561/XXXXXXXXXX.
1 
Introduction
The shortest path principle guides most decisions in life and sciences: When a commodity, a person or a single bit of information is available at a given point and needs to be
sent at a target point, one should favor using the least possible eﬀort. This is typically
reached by moving an item along a straight line when in the plane or along geodesic
curves in more involved metric spaces. The theory of optimal transport generalizes that
intuition in the case where, instead of moving only one item at a time, one is concerned
with the problem of moving simultaneously several items (or a continuous distribution
thereof) from one conﬁguration onto another. As schoolteachers might attest, planning
the transportation of a group of individuals, with the constraint that they reach a given
target conﬁguration upon arrival, is substantially more involved than carrying it out
for a single individual. Indeed, thinking in terms of groups or distributions requires a
more advanced mathematical formalism which was ﬁrst hinted at in the seminal work
of Monge . Yet, no matter how complicated that formalism might look at ﬁrst
sight, that problem has deep and concrete connections with our daily life. Transportation, be it of people, commodities or information, very rarely involves moving only
one item. All major economic problems, in logistics, production planning or network
routing, involve moving distributions, and that thread appears in all of the seminal references on optimal transport. Indeed Tolstoı , Hitchcock and Kantorovich
 were all guided by practical concerns. It was only a few years later, mostly after
the 1980s, that mathematicians discovered, thanks to the works of Brenier and
others, that this theory provided a fertile ground for research, with deep connections
to convexity, partial diﬀerential equations and statistics. At the turn of the millenium,
researchers in computer, imaging and more generally data sciences understood that op-
Introduction
timal transport theory provided very powerful tools to study distributions in a diﬀerent
and more abstract context, that of comparing distributions readily available to them
under the form of bags-of-features or descriptors.
Several reference books have been written on optimal transport, including the two
recent monographs by Villani , those by Rachev and Rüschendorf and more recently that by Santambrogio . As exempliﬁed by these books,
the more formal and abstract concepts in that theory deserve in and by themselves
several hundred pages. Now that optimal transport has gradually established itself as
an applied tool (for instance, in economics, as put forward recently by Galichon ),
we have tried to balance that rich literature with a computational viewpoint, centered
on applications to data science, notably imaging sciences and machine learning. We
follow in that sense the motivation of the recent review by Kolouri et al. but
try to cover more ground. Ultimately, our goal is to present an overview of the main
theoretical insights that support the practical eﬀectiveness of OT and spend more time
explaining how to turn these insights into fast computational schemes. The main body
of Chapters 2, 3, 4, 9, and 10 is devoted solely to the study of the geometry induced by
optimal transport in the space of probability vectors or discrete histograms. Targeting
more advanced readers, we also give in the same chapters, in light gray boxes, a more
general mathematical exposition of optimal transport tailored for discrete measures.
Discrete measures are deﬁned by their probability weights, but also by the location
at which these weights are deﬁned. These locations are usually taken in a continuous
metric space, giving a second important degree of freedom to model random phenomena.
Lastly, the third and most technical layer of exposition is indicated in dark gray boxes
and deals with arbitrary measures that need not be discrete, and which can have in
particular a density w.r.t. a base measure. This is traditionally the default setting for
most classic textbooks on OT theory, but one that plays a less important role in general
for practical applications. Chapters 5 to 8 deal with the interplay between continuous
and discrete measures and are thus targeting a more mathematically inclined audience.
The ﬁeld of computational optimal transport is at the time of this writing still an
extremely active one. There are therefore a wide variety of topics that we have not
touched upon in this survey. Let us cite in no particular order the subjects of distributionally robust optimization [Shaﬁeezadeh Abadeh et al., 2015, Esfahani and Kuhn,
2018, Lee and Raginsky, 2018, GAO et al., 2018], in which parameter estimation is
carried out by minimizing the worst posssible empirical risk of any data measure taken
within a certain Wasserstein distance of the input data; convergence of the Langevin
Monte Carlo sampling algorithm in the Wasserstein geometry [Dalalyan and Karagulyan, 2017, Dalalyan, 2017, Bernton, 2018]; other numerical methods to solve OT
with a squared Euclidian cost in low-dimensional settings using the Monge-Ampère
equation [Froese and Oberman, 2011, Benamou et al., 2014, Sulman et al., 2011] which
are only brieﬂy mentioned in Remark 2.25.
• JnK: set of integers {1, . . . , n}.
• 1n,m: matrix of Rn×m with all entries identically set to 1. 1n: vector of ones.
• In: identity matrix of size n × n.
• For u ∈Rn, diag(u) is the n × n matrix with diagonal u and zero otherwise.
• Σn: probability simplex with n bins, namely the set of probability vectors in Rn
• (a, b): histograms in the simplices Σn × Σm.
• (α, β): measures, deﬁned on spaces (X, Y).
dβ : relative density of a measure α with respect to β.
dx: density of a measure α with respect to Lebesgue measure.
i aiδxi, β = P
j bjδyj): discrete measures supported on x1, . . . , xn ∈X and
y1, . . . , ym ∈Y.
• c(x, y): ground cost, with associated pairwise cost matrix Ci,j = (c(xi, yj))i,j
evaluated on the support of α, β.
• π: coupling measure between α and β, namely such that for any A ⊂X, π(A ×
Y) = α(A), and for any subset B ⊂Y, π(X × B) = β(B). For discrete measures
i,j Pi,jδ(xi,yj).
• U(α, β): set of coupling measures, for discrete measures U(a, b).
• R(c): set of admissible dual potentials; for discrete measures R(C).
• T : X →Y: Monge map, typically such that T♯α = β.
t=0: dynamic measures, with αt=0 = α0 and αt=1 = α1.
• v: speed for Benamou–Brenier formulations; J = αv: momentum.
• (f, g): dual potentials, for discrete measures (f, g) are dual variables.
= (ef/ε, eg/ε): Sinkhorn scalings.
= e−C/ε: Gibbs kernel for Sinkhorn.
Introduction
• s: ﬂow for W1-like problem (optimization under divergence constraints).
• LC(a, b) and Lc(α, β): value of the optimization problem associated to the OT
with cost C (histograms) and c (arbitrary measures).
• Wp(a, b) and Wp(α, β): p-Wasserstein distance associated to ground distance
matrix D (histograms) and distance d (arbitrary measures).
• λ ∈ΣS: weight vector used to compute the barycenters of S measures.
• ⟨·, ·⟩: for the usual Euclidean dot-product between vectors; for two matrices of
the same size A and B, ⟨A, B⟩
= tr(A⊤B) is the Frobenius dot-product.
• f ⊕g(x, y)
= f(x) + g(y), for two functions f : X →R, g : Y →R, deﬁnes
f ⊕g : X × Y →R.
m + 1ng⊤∈Rn×m for two vectors f ∈Rn, g ∈Rm.
• α ⊗β is the product measure on X × Y, i.e.
X×Y g(x, y)d(α ⊗β)(x, y)
X×Y g(x, y)dα(x)dβ(y).
= ab⊤∈Rn×m.
• u ⊙v = (uivi) ∈Rn for (u, v) ∈(Rn)2.
Theoretical Foundations
This chapter describes the basics of optimal transport, introducing ﬁrst the related
notions of optimal matchings and couplings between probability vectors (a, b), generalizing gradually this computation to transport between discrete measures (α, β), to
cover lastly the general setting of arbitrary measures. At ﬁrst reading, these last nuances
may be omitted and the reader can only focus on computations between probability
vectors, namely histograms, which is the only requisite to implement algorithms detailed in Chapters 3 and 4. More experienced readers will reach a better understanding
of the problem by considering the formulation that applies to arbitrary measures, and
will be able to apply it for more advanced problems (e.g. in order to move positions of
clouds of points, or in a statistical setting where points are sampled from continuous
densities).
Histograms and Measures
We will use interchangeably the terms histogram and probability vector for any element
a ∈Σn that belongs to the probability simplex
A large part of this review focuses exclusively on the study of the geometry induced by
optimal transport on the simplex.
Theoretical Foundations
Remark 2.1 (Discrete measures). A discrete measure with weights a and locations
x1, . . . , xn ∈X reads
where δx is the Dirac at position x, intuitively a unit of mass which is inﬁnitely
concentrated at location x. Such a measure describes a probability measure if,
additionally, a ∈Σn and more generally a positive measure if all the elements of
vector a are nonnegative. To avoid degeneracy issues where locations with no mass
are accounted for, we will assume when considering discrete measures that all the
elements of a are positive.
Remark 2.2 (General measures). A convenient feature of OT is that it can deal
with measures that are either or both discrete and continuous within the same
framework. To do so, one relies on the set of Radon measures M(X) on the space
X. The formal deﬁnition of that set requires that X is equipped with a distance,
usually denoted d, because one can access a measure only by “testing” (integrating)
it against continuous functions, denoted f ∈C(X).
Integration of f ∈C(X) against a discrete measure α computes a sum
f(x)dα(x) =
More general measures, for instance on X = Rd (where d ∈N∗is the dimension),
can have a density dα(x) = ρα(x)dx w.r.t. the Lebesgue measure, often denoted
dx, which means that
∀h ∈C(Rd),
Rd h(x)dα(x) =
Rd h(x)ρα(x)dx.
An arbitrary measure α ∈M(X) (which need not have a density nor be a sum
of Diracs) is deﬁned by the fact that it can be integrated against any continuous
function f ∈C(X) and obtain
X f(x)dα(x) ∈R. If X is not compact, one should
also impose that f has compact support or at least has 0 limit at inﬁnity. Measures
are thus in some sense “less regular” than functions but more regular than distributions (which are dual to smooth functions). For instance, the derivative of a Dirac
is not a measure. We denote M+(X) the set of all positive measures on X. The
set of probability measures is denoted M1
+(X), which means that any α ∈M1
is positive, and that α(X) =
X dα = 1. Figure 2.1 oﬀers a visualization of the
diﬀerent classes of measures, beyond histograms, considered in this work.
2.2. Assignment and Monge Problem
Discrete d = 1
Discrete d = 2
Density d = 1
Density d = 2
Figure 2.1: Schematic display of discrete distributions α = Pn
i=1 aiδxi (red corresponds to empirical
uniform distribution ai = 1/n, and blue to arbitrary distributions) and densities dα(x) = ρα(x)dx (in
purple), in both one and two dimensions. Discrete distributions in one-dimension are displayed as stem
plots (with length equal to ai) and in two dimensions using point clouds (in which case their radius
might be equal to ai or, for a more visually accurate representation, their area).
Assignment and Monge Problem
Given a cost matrix (Ci,j)i∈JnK,j∈JmK, assuming n = m, the optimal assignment problem
seeks for a bijection σ in the set Perm(n) of permutations of n elements solving
One could naively evaluate the cost function above using all permutations in the set
Perm(n). However, that set has size n!, which is gigantic even for small n. Consider,
for instance, that such a set has more than 10100 elements [Dantzig, 1983] when n is as
small as 70. That problem can therefore be solved only if there exist eﬃcient algorithms
to optimize that cost function over the set of permutations, which is the subject of §3.7.
Remark 2.3 (Uniqueness). Note that the optimal assignment problem may have several
optimal solutions. Suppose, for instance, that n = m = 2 and that the matrix C is the
pairwise distance matrix between the four corners of a 2-D square of side length 1, as
represented in the left plot of Figure 2.2. In that case only two assignments exist, and
they are both optimal.
Remark 2.4 (Monge problem between discrete measures). For discrete measures
the Monge problem seeks a map that associates to each point xi a single
point yj and which must push the mass of α toward the mass of β, namely, such a
Theoretical Foundations
Figure 2.2:
Left: blue dots from measure α and red dots from measure β are pairwise equidistant.
Hence, either matching σ = (1, 2) (full line) or σ = (2, 1) (dotted line) is optimal. Right: a Monge map
can associate the blue measure α to the red measure β. The weights αi are displayed proportionally
to the area of the disk marked at each location. The mapping here is such that T(x1) = T(x2) = y2,
T(x3) = y3, whereas for 4 ≤i ≤7 we have T(xi) = y1.
map T : {x1, . . . , xn} →{y1, . . . , ym} must verify that
i:T(xi)=yj
which we write in compact form as T♯α = β. Because all the elements of b are
positive, that map is necessarily surjective. This map should minimize some transportation cost, which is parameterized by a function c(x, y) deﬁned for points
(x, y) ∈X × Y,
c(xi, T(xi)) : T♯α = β
Such a map between discrete points can be of course encoded, assuming all x’s
and y’s are distinct, using indices σ : JnK →JmK so that j = σ(i), and the mass
conservation is written as
where the inverse σ−1(j) is to be understood as the preimage set of j. In the special
case when n = m and all weights are uniform, that is, ai = bj = 1/n, then the
mass conservation constraint implies that T is a bijection, such that T(xi) = yσ(i),
and the Monge problem is equivalent to the optimal matching problem (2.2), where
the cost matrix is
= c(xi, yj).
When n ̸= m, note that, optimality aside, Monge maps may not even exist between
a discrete measure to another. This happens when their weight vectors are not
2.2. Assignment and Monge Problem
compatible, which is always the case when the target measure has more points
than the source measure, n < m. For instance, the right plot in Figure 2.2 shows
an (optimal) Monge map between α and β, but there is no Monge map from β to
Remark 2.5 (Push-forward operator). For a continuous map T : X →Y, we deﬁne
its corresponding push-forward operator T♯: M(X) →M(Y). For discrete measures (2.1), the push-forward operation consists simply in moving the positions of
all the points in the support of the measure
For more general measures, for instance, for those with a density, the notion of
push-forward plays a fundamental role to describe the spatial modiﬁcation (or
transport) of a probability measure. The formal deﬁnition reads as follows.
Deﬁnition 2.1 (Push-forward). For T : X →Y, the push-forward measure β =
T♯α ∈M(Y) of some α ∈M(X) satisﬁes
h(y)dβ(y) =
h(T(x))dα(x).
Equivalently, for any measurable set B ⊂Y, one has
β(B) = α({x ∈X : T(x) ∈B}) = α(T −1(B)).
Note that T♯preserves positivity and total mass, so that if α ∈M1
Intuitively, a measurable map T : X →Y can be interpreted as a function
moving a single point from a measurable space to another. T♯is an extension of
T that can move an entire probability measure on X toward a new probability
measure on Y. The operator T♯pushes forward each elementary mass of a measure
α on X by applying the map T to obtain then an elementary mass in Y. Note that
a push-forward operator T♯: M1
+(Y) is linear in the sense that for two
measures α1, α2 on X, T♯(α1 + α2) = T♯α1 + T♯α2.
Remark 2.6 (Push-forward for multivariate densities). Explicitly doing the change
of variables in formula (2.6) for measures with densities (ρα, ρβ) on Rd (assuming T is smooth and bijective) shows that a push-forward acts on densities linearly
Theoretical Foundations
as a change of variables in the integration formula. Indeed, one has
ρα(x) = | det(T ′(x))|ρβ(T(x)),
where T ′(x) ∈Rd×d is the Jacobian matrix of T (the matrix formed by taking the
gradient of each coordinate of T). This implies
| det(T ′(x))| =
Remark 2.7 (Monge problem between arbitrary measures). The
problem (2.5) can be extended to the case where two arbitrary probability measures
(α, β), supported on two spaces (X, Y) can be linked through a map T : X →Y
that minimizes
c(x, T(x))dα(x) : T♯α = β
The constraint T♯α = β means that T pushes forward the mass of α to β, using
the push-forward operator deﬁned in Remark 2.5
Remark 2.8 (Push-forward vs. pull-back). The push-forward T♯of measures should
not be confused with the pull-back of functions T ♯: C(Y) →C(X) which corresponds to “warping” between functions, deﬁned as the linear map which to g ∈C(Y)
associates T ♯g = g ◦T. Push-forward and pull-back are actually adjoint to one another, in the sense that
∀(α, g) ∈M(X) × C(Y),
Note that even if (α, β) have densities (ρα, ρβ) with respect to a ﬁxed measure (e.g.
Lebesgue on Rd), T♯α does not have T ♯ρβ as density, because of the presence of
the Jacobian in (2.8). This explains why OT should be used with caution to perform image registration, because it does not operate as an image warping method.
Figure 2.3 illustrates the distinction between these push-forward and pull-back
operators.
Remark 2.9 (Measures and random variables). Radon measures can also be viewed
as representing the distributions of random variables. A random variable X on X
is actually a map X : Ω→X from some abstract (often unspeciﬁed) probability
space (Ω, P), and its distribution α is the Radon measure α ∈M1
+(X) such that
P(X ∈A) = α(A) =
A dα(x). Equivalently, it is the push-forward of P by X,
α = X♯P. Applying another push-forward β = T♯α for T : X →Y, following (2.6),
2.3. Kantorovich Relaxation
Push-forward of measures
Pull-back of functions
Figure 2.3: Comparison of the push-forward operator T♯, which can take as an input any measure,
and the pull-back operator T ♯, which operates on functions, notaly densities.
is equivalent to deﬁning another random variable Y = T(X) : ω ∈Ω→T(X(ω)) ∈
Y , so that β is the distribution of Y . Drawing a random sample y from Y is thus
simply achieved by computing y = T(x), where x is drawn from X.
Kantorovich Relaxation
The assignment problem, and its generalization found in the Monge problem laid out in
Remark 2.4, is not always relevant to studying discrete measures, such as those found
in practical problems. Indeed, because the assignment problem is formulated as a permutation problem, it can only be used to compare uniform histograms of the same size.
A direct generalization to discrete measures with nonuniform weights can be carried
out using Monge’s formalism of push-forward maps, but that formulation may also be
degenerate in the absence of feasible solutions satisfying the mass conservation constraint (2.4) (see the end of Remark 2.4). Additionally, the assignment problem (2.5) is
combinatorial, and the feasible set for the Monge problem (2.9), despite being continuously parameterized as the set consisting in all push-forward measures that satisfy the
mass conservation constraint, is nonconvex. Both are therefore diﬃcult to solve when
approached in their original formulation.
The key idea of Kantorovich is to relax the deterministic nature of transportation, namely the fact that a source point xi can only be assigned to another point
or location yσi or T(xi) only. Kantorovich proposes instead that the mass at any point
xi be potentially dispatched across several locations. Kantorovich moves away from the
idea that mass transportation should be deterministic to consider instead a probabilistic
transport, which allows what is commonly known now as mass splitting from a source
toward several targets. This ﬂexibility is encoded using, in place of a permutation σ or a
map T, a coupling matrix P ∈Rn×m
, where Pi,j describes the amount of mass ﬂowing
Theoretical Foundations
from bin i toward bin j, or from the mass found at xi toward yj in the formalism of
discrete measures (2.3). Admissible couplings admit a far simpler characterization than
Monge maps,
where we used the following matrix-vector notation:
The set of matrices U(a, b) is bounded and deﬁned by n + m equality constraints, and
therefore is a convex polytope (the convex hull of a ﬁnite set of matrices) [Brualdi,
2006, §8.1].
Additionally, whereas the Monge formulation (as illustrated in the right plot of
Figure 2.2) was intrisically asymmetric, Kantorovich’s relaxed formulation is always
symmetric, in the sense that a coupling P is in U(a, b) if and only if PT is in U(b, a).
Kantorovich’s optimal transport problem now reads
P∈U(a,b) ⟨C, P⟩
This is a linear program (see Chapter 3), and as is usually the case with such programs,
its optimal solutions are not necessarily unique.
Remark 2.10 (Mines and factories). The Kantorovich problem ﬁnds a very natural
illustration in the following resource allocation problem (see also Hitchcock ).
Suppose that an operator runs n warehouses and m factories. Each warehouse contains
a valuable raw material that is needed by the factories to run properly. More precisely,
each warehouse is indexed with an integer i and contains ai units of the raw material.
These raw materials must all be moved to the factories, with a prescribed quantity bj
needed at factory j to function properly. To transfer resources from a warehouse i to
a factory j, the operator can use a transportation company that will charge Ci,j to
move a single unit of the resource from location i to location j. We assume that the
transportation company has the monopoly to transport goods and applies the same
linear pricing scheme to all actors of the economy: the cost of shipping a units of the
resource from i to j is equal to a × Ci,j.
Faced with the problem described above, the operator chooses to solve the linear
program described in Equation (2.11) to obtain a transportation plan P⋆that quantiﬁes
for each pair i, j the amount of goods Pi,j that must transported from warehouse i to
factory j. The operator pays on aggregate a total of ⟨P⋆, C⟩to the transportation
company to execute that plan.
2.3. Kantorovich Relaxation
Permutation matrices as couplings.
For a permutation σ ∈Perm(n), we write Pσ
for the corresponding permutation matrix,
∀(i, j) ∈JnK2,
otherwise.
One can check that in that case
⟨C, Pσ⟩= 1
which shows that the assignment problem (2.2) can be recast as a Kantorovich problem (2.11) where the couplings P are restricted to be exactly permutation matrices:
σ∈Perm(n) ⟨C, Pσ⟩.
Next, one can easily check that the set of permutation matrices is strictly included in
the Birkhoﬀpolytope U(1n/n, 1n/n). Indeed, for any permutation σ we have Pσ1 =
1n and PσT1 = 1n, whereas 1n1nT/n2 is a valid coupling but not a permutation
matrix. Therefore, the minimum of ⟨C, P⟩is necessarily smaller when considering all
transportation than when considering only permutation matrices:
LC(1n/n, 1n/n) ≤
σ∈Perm(n) ⟨C, Pσ⟩.
The following proposition shows that these problems result in fact in the same
optimum, namely that one can always ﬁnd a permutation matrix that minimizes Kantorovich’s problem (2.11) between two uniform measures a = b = 1n/n. The Kantorovich relaxation is therefore tight when considered on assignment problems. Figure 2.4 shows on the left a 2-D example of optimal matching corresponding to this
special case.
Proposition 2.1 (Kantorovich for matching). If m = n and a = b = 1n/n, then there
exists an optimal solution for Problem (2.11) Pσ⋆, which is a permutation matrix
associated to an optimal permutation σ⋆∈Perm(n) for Problem (2.2).
Proof. Birkhoﬀ’s theorem states that the set of extremal points of U(1n/n, 1n/n)
is equal to the set of permutation matrices. A fundamental theorem of linear programming [Bertsimas and Tsitsiklis, 1997, Theorem 2.7] states that the minimum of a linear
objective in a nonempty polyhedron, if ﬁnite, is reached at an extremal point of the
polyhedron.
Theoretical Foundations
Figure 2.4:
Comparison of optimal matching and generic couplings. A black segment between xi
and yj indicates a nonzero element in the displayed optimal coupling Pi,j solving (2.11). Left: optimal
matching, corresponding to the setting of Proposition 2.1 (empirical measures with the same number
n = m of points). Right: these two weighted point clouds cannot be matched; instead a Kantorovich
coupling can be used to associate two arbitrary discrete measures.
Remark 2.11 (Kantorovich problem between discrete measures). For discrete measures α, β of the form (2.3), we store in the matrix C all pairwise costs between
points in the supports of α, β, namely Ci,j
= c(xi, yj), to deﬁne
= LC(a, b).
Therefore, the Kantorovich formulation of optimal transport between discrete measures is the same as the problem between their associated probability weight vectors
a, b except that the cost matrix C depends on the support of α and β. The notation Lc(α, β), however, is useful in some situations, because it makes explicit the
dependency with respect to both probability weights and supporting points, the
latter being exclusively considered through the cost function c.
Remark 2.12 (Using optimal assignments and couplings). The
plan itself (either as a coupling P or a Monge map T when it exists) has found
many applications in data sciences, and in particular image processing. It has,
for instance, been used for contrast equalization [Delon, 2004] and texture synthesis Gutierrez et al. . A signiﬁcant part of applications of OT to imaging
sciences is for image matching [Zhu et al., 2007, Wang et al., 2013, Museyko et al.,
2009, Li et al., 2013], image fusion [Courty et al., 2016], medical imaging [Wang
et al., 2011] and shape registration [Makihara and Yagi, 2010, Lai and Zhao, 2017,
2.3. Kantorovich Relaxation
Semidiscrete
Continuous
Figure 2.5: Schematic viewed of input measures (α, β) and couplings U(α, β) encountered in the three
main scenarios for Kantorovich OT. Chapter 5 is dedicated to the semidiscrete setup.
Su et al., 2015], and image watermarking [Mathon et al., 2014]. In astrophysics,
OT has been used for reconstructing the early universe [Frisch et al., 2002]. OT has
also been used for music transcription [Flamary et al., 2016], and ﬁnds numerous
applications in economics to interpret matching data [Galichon, 2016]. Lastly, let us
note that the computation of transportation maps computed using OT techniques
(or inspired from them) is also useful to perform sampling [Reich, 2013, Oliver,
2014] and Bayesian inference [Kim et al., 2013, El Moselhy and Marzouk, 2012].
Remark 2.13 (Kantorovich problem between arbitrary measures). Deﬁnition (2.13)
of Lc is extended to arbitrary measures by considering couplings π ∈M1
which are joint distributions over the product space. The discrete case is a
special situation where one imposes this product measure to be of the form
i,j Pi,jδ(xi,yj). In the general case, the mass conservation constraint (2.10)
should be rewritten as a marginal constraint on joint probability distributions
+(X × Y) : PX♯π = α
Here PX♯and PY♯are the push-forwards (see Deﬁnition 2.1) of the projections
PX (x, y) = x and PY(x, y) = y. Figure 2.5 shows how these coupling constraints translate for diﬀerent classes of problems (discrete measures and densities). Using (2.7), these marginal constraints are equivalent to imposing that
π(A × Y) = α(A) and π(X × B) = β(B) for sets A ⊂X and B ⊂Y. The Kan-
Theoretical Foundations
torovich problem (2.11) is then generalized as
c(x, y)dπ(x, y).
This is an inﬁnite-dimensional linear program over a space of measures. If (X, Y)
are compact spaces and c is continuous, then it is easy to show that it always
has solutions. Indeed U(α, β) is compact for the weak topology of measures (see
Remark 2.2), π 7→
R cdπ is a continuous function for this topology and the constraint set is nonempty (for instance, α ⊗β ∈U(α, β)). Figure 2.6 shows examples
of discrete and continuous optimal coupling solving (2.15). Figure 2.7 shows other
examples of optimal 1-D couplings, involving discrete and continuous marginals.
Figure 2.6: Left: “continuous” coupling π solving (2.14) between two 1-D measures with density. The
coupling is localized along the graph of the Monge map (x, T(x)) (displayed in black). Right: “discrete”
coupling T solving (2.11) between two discrete measures of the form (2.3). The positive entries Ti,j are
displayed as black disks at position (i, j) with radius proportional to Ti,j.
Figure 2.7:
Four simple examples of optimal couplings between 1-D distributions, represented as
maps above (arrows) and couplings below. Inspired by Lévy and Schwindt .
2.4. Metric Properties of Optimal Transport
Remark 2.14 (Probabilistic interpretation). Kantorovich’s problem can be reinterpreted through the prism of random variables, following Remark 2.9. Indeed, Problem (2.15) is equivalent to
Lc(α, β) = min
E(X,Y )(c(X, Y )) : X ∼α, Y ∼β
where (X, Y ) is a couple of random variables over X ×Y and X ∼α (resp., Y ∼β)
means that the law of X (resp., Y ), represented as a measure, must be α (resp.,
β). The law of the couple (X, Y ) is then π ∈U(α, β) over the product space X ×Y.
Metric Properties of Optimal Transport
An important feature of OT is that it deﬁnes a distance between histograms and probability measures as soon as the cost matrix satisﬁes certain suitable properties. Indeed,
OT can be understood as a canonical way to lift a ground distance between points to
a distance between histogram or measures.
We ﬁrst consider the case where, using a term ﬁrst introduced by Rubner et al.
 , the “ground metric” matrix C is ﬁxed, representing substitution costs between
bins, and shared across several histograms we would like to compare. The following
proposition states that OT provides a valid distance between histograms supported on
these bins.
Proposition 2.2. We suppose n = m and that for some p ≥1, C = Dp = (Dp
Rn×n, where D ∈Rn×n
is a distance on JnK, i.e.
(i) D ∈Rn×n
is symmetric;
(ii) Di,j = 0 if and only if i = j;
(iii) ∀(i, j, k) ∈JnK3, Di,k ≤Di,j + Dj,k.
= LDp(a, b)1/p
(note that Wp depends on D) deﬁnes the p-Wasserstein distance on Σn, i.e. Wp is
symmetric, positive, Wp(a, b) = 0 if and only if a = b, and it satisﬁes the triangle
inequality
∀a, b, c ∈Σn,
Wp(a, c) ≤Wp(a, b) + Wp(b, c).
Proof. Symmetry and deﬁniteness of the distance are easy to prove: since C = Dp
has a null diagonal, Wp(a, a) = 0, with corresponding optimal transport matrix P⋆=
diag(a); by the positivity of all oﬀ-diagonal elements of Dp, Wp(a, b) > 0 whenever
a ̸= b (because in this case, an admissible coupling necessarily has a nonzero element
outside the diagonal); by symmetry of Dp, Wp(a, b) is itself a symmetric function.
Theoretical Foundations
To prove the triangle inequality of Wasserstein distances for arbitrary measures,
Villani [2003, Theorem 7.3] uses the gluing lemma, which stresses the existence of
couplings with a prescribed structure. In the discrete setting, the explicit constuction
of this glued coupling is simple. Let a, b, c ∈Σn. Let P and Q be two optimal solutions
of the transport problems between a and b, and b and c, respectively. To avoid issues
that may arise from null coordinates in b, we deﬁne a vector ˜b such that ˜bj
bj > 0, and ˜bj
= 1 otherwise, to write
= P diag(1/˜b)Q ∈Rn×n
and notice that S ∈U(a, c) because
S1n = P diag(1/˜b)Q1n = P(b/˜b) = P1Supp(b) = a,
where we denoted 1Supp(b) the vector of size n with ones located at those indices j
where bj > 0 and zero otherwise, and we use the fact that P1Supp(b) = P1 = a because
necessarily Pi,j = 0 for those j where bj = 0. Similarly one veriﬁes that ST1n = c. The
triangle inequality follows then from
Wp(a, c) =
P∈U(a,c)⟨P, Dp⟩
≤⟨S, Dp⟩1/p
(Dij + Djk)p PijQjk
The ﬁrst inequality is due to the suboptimality of S, the second is the triangle inequality
for elements in D, and the third comes from Minkowski’s inequality. One thus has
Wp(a, c) ≤
= Wp(a, b) + Wp(b, c),
which concludes the proof.
Remark 2.15 (The cases 0 < p ≤1). Note that if 0 < p ≤1, then Dp is itself distance.
This implies that while for p ≥1, Wp(a, b) is a distance, in the case p ≤1, it is actually
Wp(a, b)p which deﬁnes a distance on the simplex.
2.4. Metric Properties of Optimal Transport
Remark 2.16 (Applications of Wasserstein distances). The fact that the OT distance automatically “lifts” a ground metric between bins to a metric between
histograms on such bins makes it a method of choice for applications in computer
vision and machine learning to compare histograms. In these ﬁelds, a classical approach is to “pool” local features (for instance, image descriptors) and compute
a histogram of the empirical distribution of features (a so-called bag of features)
to perform retrieval, clustering or classiﬁcation; see, for instance, [Oliva and Torralba, 2001]. Along a similar line of ideas, OT distances can be used over some lifted
feature spaces to perform signal and image analysis [Thorpe et al., 2017]. Applications to retrieval and clustering were initiated by the landmark paper [Rubner
et al., 2000], with renewed applications following faster algorithms for threshold
matrices C that ﬁt for some applications, for example, in computer vision [Pele and
Werman, 2008, 2009]. More recent applications stress the use of the earth mover’s
distance for bags-of-words, either to carry out dimensionality reduction [Rolet et al.,
2016] and classify texts [Kusner et al., 2015, Huang et al., 2016], or to deﬁne an
alternative loss to train multiclass classiﬁers that output bags-of-words [Frogner
et al., 2015]. Kolouri et al. provides a recent overview of such applications
to signal processing and machine learning.
Remark 2.17 (Wasserstein distance between measures). Proposition
generalized to deal with arbitrary measures that need not be discrete.
Proposition 2.3. We assume X = Y and that for some p ≥1, c(x, y) = d(x, y)p,
where d is a distance on X, i.e.
(i) d(x, y) = d(y, x) ≥0;
(ii) d(x, y) = 0 if and only if x = y;
(iii) ∀(x, y, z) ∈X 3, d(x, z) ≤d(x, y) + d(y, z).
Then the p-Wasserstein distance on X,
= Ldp(α, β)1/p
(note that Wp depends on d), is indeed a distance, namely Wp is symmetric,
nonnegative, Wp(α, β) = 0 if and only if α = β, and it satisﬁes the triangle
inequality
∀(α, β, γ) ∈M1
Wp(α, γ) ≤Wp(α, β) + Wp(β, γ).
Theoretical Foundations
Proof. The proof follows the same approach as that for Proposition 2.2 and relies on
the existence of a coupling between (α, γ) obtained by “gluing” optimal couplings
between (α, β) and (β, γ).
Remark 2.18 (Geometric intuition and weak convergence). The
Wasserstein
distance Wp has many important properties, the most important being that it is
a weak distance, i.e. it allows one to compare singular distributions (for instance,
discrete ones) whose supports do not overlap and to quantify the spatial shift
between the supports of two distributions. In particular, “classical” distances (or
divergences) are not even deﬁned between discrete distributions (the L2 norm can
only be applied to continuous measures with a density with respect to a base measure, and the discrete ℓ2 norm requires that positions (xi, yj) take values in a predetermined discrete set to work properly). In sharp contrast, one has that for any
p(δx, δy) = d(x, y). Indeed, it suﬃces to notice that U(δx, δy) = {δx,y} and
therefore the Kantorovich problem having only one feasible solution, Wp
is necessarily (d(x, y)p)1/p = d(x, y). This shows that Wp(δx, δy) →0 if x →y.
This property corresponds to the fact that Wp is a way to quantify the weak
convergence, as we now deﬁne.
Deﬁnition 2.2 (Weak convergence). On a compact domain X, (αk)k converges
weakly to α in M1
+(X) (denoted αk ⇀α) if and only if for any continuous function
X gdα. One needs to add additional decay conditions on
g on noncompact domains. This notion of weak convergence corresponds to the
convergence in the law of random vectors.
This convergence can be shown to be equivalent to Wp(αk, α) →0 [Villani,
2009, Theorem 6.8] (together with a convergence of the moments up to order p for
unbounded metric spaces).
Remark 2.19 (Translations). A nice feature of the Wasserstein distance over a Euclidean space X = Rd for the ground cost c(x, y) = ∥x −y∥2 is that one can factor
out translations; indeed, denoting Tτ : x 7→x −τ the translation operator, one has
W2(Tτ♯α, Tτ ′♯β)2 = W2(α, β)2 −2⟨τ −τ ′, mα −mβ⟩+
X xdα(x) ∈Rd is the mean of α. In particular, this implies the nice
decomposition of the distance as
W2(α, β)2 = W2(˜α, ˜β)2 + ∥mα −mβ∥2 ,
where (˜α, ˜β) are the “centered” zero mean measures ˜α = Tmα♯α.
2.5. Dual Problem
Remark 2.20 (The case p = +∞). Informally, the limit of Wp
p as p →+∞is
(x,y)∈Supp(π)
where the sup should be understood as the essential supremum according to the
measure π on X 2. In contrast to the cases p < +∞, this is a nonconvex optimization
problem, which is diﬃcult to solve numerically and to study theoretically. The W∞
distance is related to the Hausdorﬀdistance between the supports of (α, β); see
§ 10.6.1. We refer to [Champion et al., 2008] for details.
Dual Problem
The Kantorovich problem (2.11) is a constrained convex minimization problem, and as
such, it can be naturally paired with a so-called dual problem, which is a constrained
concave maximization problem. The following fundamental proposition explains the
relationship between the primal and dual problems.
Proposition 2.4. The Kantorovich problem (2.11) admits the dual
LC(a, b) =
(f,g)∈R(C) ⟨f, a⟩+ ⟨g, b⟩,
where the set of admissible dual variables is
= {(f, g) ∈Rn × Rm : ∀(i, j) ∈JnK × JmK, f ⊕g ≤C} .
Such dual variables are often referred to as “Kantorovich potentials.”
Proof. This result is a direct consequence of the more general result on the strong
duality for linear programs [Bertsimas and Tsitsiklis, 1997, p. 148, Theo. 4.4]. The
easier part of the proof, namely, establishing that the right-hand side of Equation (2.20)
is a lower bound of LC(a, b), is discussed in Remark 3.2 in the next section. For the
sake of completeness, let us derive our result using Lagrangian duality. The Lagangian
associated to (2.11) reads
(f,g)∈Rn×Rm ⟨C, P⟩+ ⟨a −P1m, f⟩+ ⟨b −PT1n, g⟩.
We exchange the min and the max above, which is always possible when considering
linear programs (in ﬁnite dimension), to obtain
(f,g)∈Rn×Rm ⟨a, f⟩+ ⟨b, g⟩+ min
P≥0 ⟨C −f1mT −1ngT, P⟩.
We conclude by remarking that
P≥0 ⟨Q, P⟩=
Theoretical Foundations
so that the constraint reads C −f1mT −1ngT = C −f ⊕g ≥0.
The primal-dual optimality relation for the Lagrangian (2.22) allows us to locate
the support of the optimal transport plan (see also §3.3)
{(i, j) ∈JnK × JmK : Pi,j > 0} ⊂
(i, j) ∈JnK × JmK : fi + gj = Ci,j
Remark 2.21. Following the interpretation given to the Kantorovich problem in Remark 2.10, we follow with an intuitive presentation of the dual. Recall that in that
setup, an operator wishes to move at the least possible cost an overall amount of resources from warehouses to factories. The operator can do so by solving (2.11), follow
the instructions set out in P⋆, and pay ⟨P⋆, C⟩to the transportation company.
Outsourcing logistics. Suppose that the operator does not have the computational
means to solve the linear program (2.11). He decides instead to outsource that task to
a vendor. The vendor chooses a pricing scheme with the following structure: the vendor
splits the logistic task into that of collecting and then delivering the goods and will
apply a collection price fi to collect a unit of resource at each warehouse i (no matter
where that unit is sent to) and a price gj to deliver a unit of resource to factory j
(no matter from which warehouse that unit comes from). On aggregate, since there
are exactly ai units at warehouse i and bj needed at factory j, the vendor asks as a
consequence of that pricing scheme a price of ⟨f, a⟩+ ⟨g, b⟩to solve the operator’s
logistic problem.
Setting prices. Note that the pricing system used by the vendor allows quite naturally for arbitrarily negative prices. Indeed, if the vendor applies a price vector f for
warehouses and a price vector g for factories, then the total bill will not be changed
by simultaneously decreasing all entries in f by an arbitrary number and increasing all
entries of g by that same number, since the total amount of resources in all warehouses
is equal to those that have to be delivered to the factories. In other words, the vendor
can give the illusion of giving an extremely good deal to the operator by paying him to
collect some of his goods, but compensate that loss by simply charging him more for
delivering them. Knowing this, the vendor, wishing to charge as much as she can for
that service, sets vectors f and g to be as high as possible.
Checking prices. In the absence of another competing vendor, the operator must
therefore think of a quick way to check that the vendor’s prices are reasonable. A
possible way to do so would be for the operator to compute the price LC(a, b) of the
most eﬃcient plan by solving problem (2.11) and check if the vendor’s oﬀer is at the
very least no larger than that amount. However, recall that the operator cannot aﬀord
such a lengthy computation in the ﬁrst place. Luckily, there is a far more eﬃcient way
for the operator to check whether the vendor has a competitive oﬀer. Recall that fi
is the price charged by the vendor for picking a unit at i and gj to deliver one at j.
Therefore, the vendor’s pricing scheme implies that transferring one unit of the resource
2.5. Dual Problem
from i to j costs exactly fi + gj. Yet, the operator also knows that the cost of shipping
one unit from i to j as priced by the transporting company is Ci,j. Therefore, if for any
pair i, j the aggregate price fi + gj is strictly larger that Ci,j, the vendor is charging
more than the fair price charged by the transportation company for that task, and the
operator should refuse the vendor’s oﬀer.
Figure 2.8: Consider in the left plot the optimal transport problem between two discrete measures α
and β, represented respectively by blue dots and red squares. The area of these markers is proportional
to the weight at each location. That plot also displays the optimal transport P⋆using a quadratic
Euclidean cost. The corresponding dual (Kantorovich) potentials f⋆and g⋆that correspond to that
conﬁguration are also displayed on the right plot. Since there is a “price” f⋆
i for each point in α (and
conversely for g and β), the color at that point represents the obtained value using the color map on
the right. These potentials can be interpreted as relative prices in the sense that they indicate the
individual cost, under the best possible transport scheme, to move a mass away at each location in α,
or on the contrary to send a mass toward any point in β. The optimal transport cost is therefore equal
to the sum of the squared lengths of all the arcs on the left weighted by their thickness or, alternatively,
using the dual formulation, to the sum of the values (encoded with colors) multiplied by the area of
each marker on the right plot.
Optimal prices as a dual problem. It is therefore in the interest of the operator to
check that for all pairs i, j the prices oﬀered by the vendor verify fi+gj ≤Ci,j. Suppose
that the operator does check that the vendor has provided price vectors that do comply
with these n×m inequalities. Can he conclude that the vendor’s proposal is attractive?
Doing a quick back of the hand calculation, the operator does indeed conclude that it
is in his interest to accept that oﬀer. Indeed, since any of his transportation plans P
would have a cost ⟨P, C⟩= P
i,j Pi,jCi,j, the operator can conclude by applying these
n × m inequalities that for any transport plan P (including the optimal one P⋆), the
marginal constraints imply
Pi,jCi,j ≥
= ⟨f, a⟩+ ⟨g, b⟩,
and therefore observe that any attempt at doing the job by himself would necessarily
be more expensive than the vendor’s price.
Theoretical Foundations
Knowing this, the vendor must therefore ﬁnd a set of prices f, g that maximize
⟨f, a⟩+ ⟨g, b⟩but that must satisfy at the very least for all i, j the basic inequality
that fi +gj ≤Ci,j for his oﬀer to be accepted, which results in Problem (2.20). One can
show, as we do later in §3.1, that the best price obtained by the vendor is in fact exactly
equal to the best possible cost the operator would obtain by computing LC(a, b).
Figure 2.8 illustrates the primal and dual solutions resulting from the same transport
problem. On the left, blue dots represent warehouses and red dots stand for factories; the
areas of these dots stand for the probability weights a, b, links between them represent
an optimal transport, and their width is proportional to transfered amounts. Optimal
prices obtained by the vendor as a result of optimizing Problem (2.20) are shown on
the right. Prices have been chosen so that their mean is equal to 0. The highest relative
prices come from collecting goods at an isolated warehouse on the lower left of the
ﬁgure, and delivering goods at the factory located in the upper right area.
Remark 2.22 (Dual problem between arbitrary measures). To extend this primaldual construction to arbitrary measures, it is important to realize that measures are
naturally paired in duality with continuous functions (a measure can be accessed
only through integration against continuous functions). The duality is formalized
in the following proposition, which boils down to Proposition 2.4 when dealing with
discrete measures.
Proposition 2.5. One has
Lc(α, β) =
(f,g)∈R(c)
f(x)dα(x) +
g(y)dβ(y),
where the set of admissible dual potentials is
= {(f, g) ∈C(X) × C(Y) : ∀(x, y), f(x) + g(y) ≤c(x, y)} .
Here, (f, g) is a pair of continuous functions and are also called, as in the discrete
case, “Kantorovich potentials.”
The discrete case (2.20) corresponds to the dual vectors being samples of the
continuous potentials, i.e. (fi, gj) = (f(xi), g(yj)). The primal-dual optimality conditions allow us to track the support of the optimal plan, and (2.23) is generalized
Supp(π) ⊂{(x, y) ∈X × Y : f(x) + g(y) = c(x, y)} .
Note that in contrast to the primal problem (2.15), showing the existence of
solutions to (2.24) is nontrivial, because the constraint set R(c) is not compact and
the function to minimize noncoercive. Using the machinery of c-transform detailed
2.5. Dual Problem
in § 5.1, in the case c(x, y) = d(x, y)p with p ≥1, one can, however, show that
optimal (f, g) are necessarily Lipschitz regular, which enables us to replace the
constraint by a compact one.
Remark 2.23 (Unconstrained dual). In the case
Y dβ = 1, the constrained
dual problem (2.24) can be replaced by an unconstrained one,
Lc(α, β) =
(f,g)∈C(X)×C(Y)
X⊗Y (c −f ⊕g),
where we denoted (f ⊕g)(x, y) = f(x) + g(y). Here the minimum should be considered as the essential supremum associated to the measure α⊗β, i.e., it does not
change if f or g is modiﬁed on sets of zero measure for α and β. This alternative
dual formulation was pointed out to us by Francis Bach. It is obtained from the
primal problem (2.15) by adding the redundant constraint
Remark 2.24 (Monge–Kantorovich equivalence—Brenier theorem). The
theorem is often attributed to Brenier and ensures that in Rd for p = 2, if
at least one of the two input measures has a density, and for measures with second
order moments, then the Kantorovich and Monge problems are equivalent. The
interested reader should also consult variants of the same result published more
or less at the same time by Cuesta and Matran , Rüschendorf and Rachev
 , including notably the original result in [Brenier, 1987] and a precursor
by Knott and Smith .
Theorem 2.1 (Brenier). In the case X = Y = Rd and c(x, y) = ∥x −y∥2, if at
least one of the two input measures (denoted α) has a density ρα with respect to
the Lebesgue measure, then the optimal π in the Kantorovich formulation (2.15)
is unique and is supported on the graph (x, T(x)) of a “Monge map” T : Rd →Rd.
This means that π = (Id, T)♯α, i.e.
∀h ∈C(X × Y),
h(x, y)dπ(x, y) =
h(x, T(x))dα(x).
Furthermore, this map T is uniquely deﬁned as the gradient of a convex function ϕ,
T(x) = ∇ϕ(x), where ϕ is the unique (up to an additive constant) convex function
such that (∇ϕ)♯α = β. This convex function is related to the dual potential f
solving (2.24) as ϕ(x) = ∥x∥2
Proof. We sketch the main ingredients of the proof; more details can be found, for
instance, in [Santambrogio, 2015]. We remark that
R cdπ = Cα,β−2
R ⟨x, y⟩dπ(x, y),
Theoretical Foundations
where the constant is Cα,β =
R ∥x∥2 dα(x)+
R ∥y∥2 dβ(y). Instead of solving (2.15),
one can thus consider the problem
⟨x, y⟩dπ(x, y),
whose dual reads
ψdβ : ∀(x, y),
ϕ(x) + ψ(y) ≥⟨x, y⟩
The relation between these variables and those of (2.25) is (ϕ, ψ) = (∥·∥2
2 −f, ∥·∥2
g). One can replace the constraint by
ψ(y) ≥ϕ∗(y)
⟨x, y⟩−ϕ(x).
Here ϕ∗is the Legendre transform of ϕ and is a convex function as a supremum
of linear forms (see also (4.54)). Since the objective appearing in (2.31) is linear
and the integrating measures positive, one can minimize explicitly with respect to
ψ and set ψ = ϕ∗in order to consider the unconstrained problem
see also §3.2 and §5.1, where that idea is applied respectively in the discrete
setting and for generic costs c(x, y). By iterating this argument twice, one
can replace ϕ by ϕ∗∗, which is a convex function, and thus impose in (2.31)
that ϕ is convex. Condition (2.26) shows that an optimal π is supported on
{(x, y) : ϕ(x) + ϕ∗(y) = ⟨x, y⟩}, which shows that such a y is optimal for the
minimization (2.30) of the Legendre transform, whose optimality condition reads
y ∈∂ϕ(x). Since ϕ is convex, it is diﬀerentiable almost everywhere, and since α has
a density, it is also diﬀerentiable α-almost everywhere. This shows that for each
x, the associated y is uniquely deﬁned α-almost everywhere as y = ∇ϕ(x), and it
shows that necessarily π = (Id, ∇ϕ)♯α.
This result shows that in the setting of W2 with no-singular densities, the
Monge problem (2.9) and its Kantorovich relaxation (2.15) are equal (the relaxation
is tight). This is the continuous counterpart of Proposition 2.1 for the assignment
case (2.1), which states that the minimum of the optimal transport problem is
achieved at a permutation matrix (a discrete map) when the marginals are equal
and uniform. Brenier’s theorem, stating that an optimal transport map must be
the gradient of a convex function, provides a useful generalization of the notion
of increasing functions in dimension more than one. This is the main reason why
2.5. Dual Problem
optimal transport can be used to deﬁne quantile functions in arbitrary dimensions,
which is in turn useful for applications to quantile regression problems [Carlier
et al., 2016].
Note also that this theorem can be extended in many directions. The condition
that α has a density can be weakened to the condition that it does not give mass
to “small sets” having Hausdorﬀdimension smaller than d−1 (e.g. hypersurfaces).
One can also consider costs of the form c(x, y) = h(x −y), where h is a strictly
convex function.
Remark 2.25 (Monge–Ampère equation). For measures with densities, using (2.8),
one obtains that ϕ is the unique (up to the addition of a constant) convex function
which solves the following Monge–Ampère-type equation:
det(∂2ϕ(x))ρβ(∇ϕ(x)) = ρα(x)
where ∂2ϕ(x) ∈Rd×d is the Hessian of ϕ. The Monge–Ampère operator det(∂2ϕ(x))
can be understood as a nonlinear degenerate Laplacian. In the limit of small displacements, ϕ = Id + εψ, one indeed recovers the Laplacian ∆as a linearization
since for smooth maps
det(∂2ϕ(x)) = 1 + ε∆ψ(x) + o(ε).
The convexity constraint forces det(∂2ϕ(x)) ≥0 and is necessary for this equation
to have a solution. There is a large body of literature on the theoretical analysis of
the Monge–Ampère equation, and in particular the regularity of its solution—see,
for instance, [Gutiérrez, 2016]; we refer the interested read to the review paper
by Caﬀarelli . A major diﬃculty is that in full generality, solutions need not
be smooth, and one has to resort to the machinery of Alexandrov solutions when the
input measures are arbitrary (e.g. Dirac masses). Many solvers have been proposed
in the simpler case of the Monge–Ampère equation det(∂2ϕ(x)) = f(x) for a ﬁxed
right-hand-side f; see, for instance, [Benamou et al., 2016b] and the references
therein. In particular, capturing anisotropic convex functions requires special care,
and usual ﬁnite diﬀerences can be inaccurate. For optimal transport, where f
actually depends on ∇ϕ, the discretization of Equation (2.32), and the boundary
condition result in technical challenges outlined in [Benamou et al., 2014] and the
references therein. Note also that related solvers based on ﬁxed-point iterations
have been applied to image registration [Haker et al., 2004].
Theoretical Foundations
Special Cases
In general, computing OT distances is numerically involved. Before detailing in §§3,4,
and 7 diﬀerent numerical solvers, we ﬁrst review special favorable cases where the
resolution of the OT problem is relatively easy.
Remark 2.26 (Binary cost matrix and 1-norm). One can easily check that when the
cost matrix C is 0 on the diagonal and 1 elsewhere, namely, when C = 1n×n −In,
the 1-Wasserstein distance between a and b is equal to the 1-norm of their diﬀerence,
LC(a, b) = ∥a −b∥1.
Remark 2.27 (Kronecker cost function and total variation). In
Remark 2.26 above, one can also easily check that this result extends to arbitrary
measures in the case where c(x, y) is 0 if x = y and 1 when x ̸= y. The OT
distance between two discrete measures α and β is equal to their total variation
distance (see also Example 8.2).
Remark 2.28 (1-D case—Empirical measures). Here X
R. Assuming α
i=1 δxi and β =
j=1 δyj, and assuming (without loss of generality) that
the points are ordered, i.e. x1 ≤x2 ≤· · · ≤xn and y1 ≤y2 ≤· · · ≤yn, then one
has the simple formula
Wp(α, β)p = 1
|xi −yi|p,
i.e. locally (if one assumes distinct points), Wp(α, β) is the ℓp norm between two
vectors of ordered values of α and β. That statement is valid only locally, in the
sense that the order (and those vector representations) might change whenever
some of the values change. That formula is a simple consequence of the more
general setting detailed in Remark 2.30. Figure 2.9, top row, illustrates the 1-D
transportation map between empirical measures with the same number of points.
The bottom row shows how this monotone map generalizes to arbitrary discrete
It is also possible to leverage this 1-D computation to also compute eﬃciently
OT on the circle as shown by Delon et al. . Note that if the cost is a concave
function of the distance, notably when p < 1, the behavior of the optimal transport
plan is very diﬀerent, yet eﬃcient solvers also exist [Delon et al., 2012].
2.6. Special Cases
Figure 2.9:
1-D optimal couplings: each arrow xi →yj indicates a nonzero Pi,j in the optimal
coupling. Top: empirical measures with same number of points (optimal matching). Bottom: generic
case. This corresponds to monotone rearrangements, if xi ≤xi′ are such that Pi,j ̸= 0, Pi′,j′ ̸= 0, then
necessarily yj ≤yj′.
Remark 2.29 (Histogram equalization). One-dimensional optimal transport can be
used to perform histogram equalization, with applications to the normalization of
the palette of grayscale images, see Figure 2.10. In this case, one denotes (¯xi)i and
(¯yj)j the gray color levels (0 for black, 1 for white, and all values in between) of all
pixels of the two input images enumerated in a predeﬁned order (i.e. columnwise).
Assuming the number of pixels in each image is the same and equal to n×m, sorting
these color levels deﬁnes xi = ¯xσ1(i) and yj = ¯yσ2(j) as in Remark 2.28, where
σ1, σ2 : {1, . . . , nm} →{1, . . . , nm} are permutations, so that σ
optimal assignment between the two discrete distributions. For image processing
applications, (¯yσ(i))i deﬁnes the color values of an equalized version of ¯x, whose
empirical distribution matches exactly the one of ¯y. The equalized version of that
image can be recovered by folding back that nm-dimensional vector as an image
of size n × m. Also, t ∈ 7→(1 −t)¯xi + t¯yσ(i) deﬁnes an interpolation between
the original image and the equalized one, whose empirical distribution of pixels is
the displacement interpolation (as deﬁned in (7.7)) between those of the inputs.
Remark 2.30 (1-D case—Generic case). For a measure α on R, we introduce the
cumulative distribution function from R to → deﬁned as
and its pseudoinverse C−1
: →R ∪{−∞}
∀r ∈ ,
α (r) = min
{x ∈R ∪{−∞} : Cα(x) ≥r} .
That function is also called the generalized quantile function of α. For any p ≥1,
Theoretical Foundations
Figure 2.10:
Histogram equalization for image processing, where t parameterizes the displacement
interpolation between the histograms.
Wp(α, β)p =
Lp( ) =
α (r) −C−1
β (r)|pdr.
This means that through the map α 7→C−1
α , the Wasserstein distance is isometric
to a linear space equipped with the Lp norm or, equivalently, that the Wasserstein
distance for measures on the real line is a Hilbertian metric. This makes the geometry of 1-D optimal transport very simple but also very diﬀerent from its geometry
in higher dimensions, which is not Hilbertian as discussed in Proposition 8.1 and
more generally in §8.3. For p = 1, one even has the simpler formula
W1(α, β) = ∥Cα −Cβ∥L1(R) =
|Cα(x) −Cβ(x)|dx
which shows that W1 is a norm (see §6.2 for the generalization to arbitrary dimensions). An optimal Monge map T such that T♯α = β is then deﬁned by
Figure 2.11 illustrates the computation of 1-D OT through cumulative functions.
It also displays displacement interpolations, computed as detailed in (7.7); see also
Remark 9.6. For a detailed survey of the properties of optimal transport in one
dimension, we refer the reader to [Santambrogio, 2015, Chapter 2].
Remark 2.31 (Distance between Gaussians). If
2.6. Special Cases
(tT + (1 −t)Id)♯α
Figure 2.11: Computation of OT and displacement interpolation between two 1-D measures, using
cumulant function as detailed in (2.39).
N(mβ, Σβ) are two Gaussians in Rd, then one can show that the following map
T : x 7→mβ + A(x −mα),
is such that T♯ρα = ρβ. Indeed, one simply has to notice that the change of variables
formula (2.8) is satisﬁed since
ρβ(T(x)) = det(2πΣβ)−1
2 exp(−⟨T(x) −mβ, Σ−1
β (T(x) −mβ)⟩)
= det(2πΣβ)−1
2 exp(−⟨x −mα, ATΣ−1
β A(x −mα)⟩)
= det(2πΣβ)−1
2 exp(−⟨x −mα, Σ−1
α (x −mα)⟩),
and since T is a linear map we have that
| det T ′(x)| = det A =
and we therefore recover ρα = | det T ′|ρβ meaning T♯α = β. Notice now that T
is the gradient of the convex function ψ : x 7→1
2⟨x −mα, A(x −mα)⟩+ ⟨mβ, x⟩
to conclude, using Brenier’s theorem (see Remark 2.24), that T is optimal.
Theoretical Foundations
Both that map T and the corresponding potential ψ are illustrated in Figures 2.12
With additional calculations involving ﬁrst and second order moments of ρα,
we obtain that the transport cost of that map is
2(α, β) = ∥mα −mβ∥2 + B(Σα, Σβ)2,
where B is the so-called Bures metric between positive deﬁnite matrices (see
also Forrester and Kieburg ),
B(Σα, Σβ)2 def.
Σα + Σβ −2(Σ1/2
where Σ1/2 is the matrix square root. One can show that B is a distance on covariance matrices and that B2 is convex with respect to both its arguments. In the
case where Σα = diag(ri)i and Σβ = diag(si)i are diagonals, the Bures metric is
the Hellinger distance
B(Σα, Σβ) =
For 1-D Gaussians, W2 is thus the Euclidean distance on the 2-D plane plotting the mean and the standard deviation of a Gaussian (m,
Σ), as illustrated
in Figure 2.14. For a detailed treatment of the Wasserstein geometry of Gaussian distributions, we refer to Takatsu , and for additional considerations
on the Bures metric the reader can consult the very recent references [Malagò
et al., 2018, Bhatia et al., 2018]. One can also consult [Muzellec and Cuturi, 2018]
for a a recent application of this metric to compute probabilistic embeddings for
words, [Shaﬁeezadeh Abadeh et al., 2018] to see how it is used to compute a robust
extension to Kalman ﬁltering, or [Mallasto and Feragen, 2017] in which it is applied
to covariance functions in reproducing kernel Hilbert spaces.
Remark 2.32 (Distance between elliptically contoured distributions). Gelbrich
provides a more general result than that provided in Remark 2.31: the Bures metric between Gaussians extends more generally to elliptically contoured distributions . In a nutshell, one can ﬁrst show that for two measures with given
mean and covariance matrices, the distance between the two Gaussians with these
respective parameters is a lower bound of the Wasserstein distance between the
two measures [Gelbrich, 1990, Theorem 2.1]. Additionally, the closed form (2.41)
extends to families of elliptically contoured densities: If two densities ρα and ρβ
belong to such a family, namely when ρα and ρβ can be written for any point x
using a mean and positive deﬁnite parameter,
2.6. Special Cases
Figure 2.12: Two Gaussians ρα and ρβ, represented using the contour plots of their densities, with
respective mean and variance matrices mα = (−2, 0), Σα =
and mβ = (3, 1), Σβ =
. The arrows originate at random points x taken on the plane and end at the corresponding
mappings of those points T(x) = mβ + A(x −mα).
det(A)h(⟨x −mα, A−1(x −mα)⟩)
det(B)h(⟨x −mβ, B−1(x −mβ)⟩),
for the same nonnegative valued function h such that the integral
Rd h(⟨x, x⟩)dx = 1,
then their optimal transport map is also the linear map (2.40) and their Wasserstein
distance is also given by the expression (2.41), with a slightly diﬀerent scaling of the
Bures metric that depends only the generator function h. For instance, that scaling
is 1 for Gaussians (h(t) = e−t/2) and 1/(d+2) for uniform distributions on ellipsoids
(h the indicator function for ). This result follows from the fact that the
covariance matrix of an elliptic distribution is a constant times its positive deﬁnite
parameter [Gómez et al., 2003, Theo. 4(ii)] and that the Wasserstein distance
between elliptic distributions is a function of the Bures distance between their
covariance matrices [Gelbrich, 1990, Cor. 2.5].
Theoretical Foundations
Figure 2.13: Same Gaussians ρα and ρβ as deﬁned in Figure 2.12, represented this time as surfaces.
The surface above is the Brenier potential ψ deﬁned up to an additive constant (here +50) such that
T = ∇ψ. For visual purposes, both Gaussian densities have been multiplied by a factor of 100.
Computation of displacement interpolation between two 1-D Gaussians. Denoting
2πse−(x−m)2
interpolation
G(1−t)m0+tm1,(1−t)σ0+tσ1.
Algorithmic Foundations
This chapter describes the most common algorithmic tools from combinatorial optimization and linear programming that can be used to solve the discrete formulation
of optimal transport, as described in the primal problem (2.11) or alternatively its
dual (2.20).
The origins of these algorithms can be traced back to World War II, either right
before with Tolstoı’s seminal work or during the war itself, when Hitchcock
 and Kantorovich formalized the generic problem of dispatching available
resources toward consumption sites in an optimal way. Both of these formulations, as
well as the later contribution by Koopmans , fell short of providing a provably correct algorithm to solve that problem (the cycle violation method was already proposed
as a heuristic by Tolstoı ). One had to wait until the ﬁeld of linear programming
fully blossomed, with the proposal of the simplex method, to be at last able to solve
rigorously these problems.
The goal of linear programming is to solve optimization problems whose objective
function is linear and whose constraints are linear (in)equalities in the variables of interest. The optimal transport problem ﬁts that description and is therefore a particular
case of that wider class of problems. One can argue, however, that optimal transport is
truly special among all linear program. First, Dantzig’s early motivation to solve linear
programs was greatly related to that of solving transportation problems [Dantzig, 1949,
p. 210]. Second, despite being only a particular case, the optimal transport problem
remained in the spotlight of optimization, because it was understood shortly after that
optimal transport problems were related, and in fact equivalent, to an important class
of linear programs known as minimum cost network ﬂows [Korte and Vygen, 2012, p.
Algorithmic Foundations
213, Lem. 9.3] thanks to a result by Ford and Fulkerson . As such, the OT problem has been the subject of particular attention, ever since the birth of mathematical
programming [Dantzig, 1951], and is still widely used to introduce optimization to a
new audience [Nocedal and Wright, 1999, §1, p. 4].
The Kantorovich Linear Programs
We have already introduced in Equation (2.11) the primal OT problem:
LC(a, b) =
i∈JnK,j∈JmK
To make the link with the linear programming literature, one can cast the equation
above as a linear program in standard form, that is, a linear program with a linear
objective; equality constraints deﬁned with a matrix and a constant vector; and nonnegative constraints on variables. Let In stand for the identity matrix of size n and let
⊗be Kronecker’s product. The (n + m) × nm matrix
∈R(n+m)×nm
can be used to encode the row-sum and column-sum constraints that need to be satisﬁed
for any P to be in U(a, b). To do so, simply cast a matrix P ∈Rn×m as a vector p ∈Rnm
such that the i + n(j −1)’s element of p is equal to Pij (P is enumerated columnwise)
to obtain the following equivalence:
P ∈Rn×m ∈U(a, b) ⇔p ∈Rnm
Therefore we can write the original optimal transport problem as
LC(a, b) =
where the nm-dimensional vector c is equal to the stacked columns contained in the
cost matrix C.
Remark 3.1. Note that one of the n + m constraints described above is redundant
or that, in other words, the line vectors of matrix A are not linearly independent.
Indeed, summing all n ﬁrst lines and the subsequent m lines results in the same vector
 = 1nmT). One can show that removing a line in A and
the corresponding entry in
 yields a properly deﬁned linear system. For simplicity,
and to avoid treating asymmetrically a and b, we retain in what follows a redundant
formulation, keeping in mind that degeneracy will pop up in some of our computations.
3.1. The Kantorovich Linear Programs
The dual problem corresponding to Equation (3.2) is, following duality in linear
programming [Bertsimas and Tsitsiklis, 1997, p. 143] deﬁned as
LC(a, b) =
Note that this program is exactly equivalent to that presented in Equation (2.4).
Remark 3.2. We provide a simple derivation of the duality result above, which can be
seen as a direct formulation of the arguments developed in Remark 2.21. Strong duality,
namely the fact that the optima of both primal (3.2) and dual (3.3) problems do indeed
coincide, requires a longer proof [Bertsimas and Tsitsiklis, 1997, §4.10]. To simplify
notation, we write q =
. Consider now a relaxed primal problem of the optimal
transport problem, where the constraint Ap = q is no longer necessarily enforced
but bears instead a cost hT(Ap −q) parameterized by an arbitrary vector of costs
h ∈Rn+m. This relaxation, whose optimum depends directly on the cost vector h, can
be written as
cTp −hT(Ap −q).
Note ﬁrst that this relaxed problem has no marginal constraints on p. Because that
minimization allows for many more p solutions, we expect H(h) to be smaller than
¯z = LC(a, b). Indeed, writing p⋆for any optimal solution of the primal problem (3.1),
cTp −hT(Ap −q) ≤cTp⋆−hT(Ap⋆−q) = cTp⋆= ¯z.
The approach above deﬁnes therefore a problem which can be used to compute an
optimal upper bound for the original problem (3.1), for any cost vector h; that function
is called the Lagrange dual function of L. The goal of duality theory is now to compute
the best lower bound z by maximizing H over any cost vector h, namely
H(h) = max
The second term involving a minimization on p can be easily shown to be −∞if any
coordinate of cT −ATh is negative. Indeed, if for instance for a given index i ≤n + m
we have ci −(ATh)i < 0, then it suﬃces to take for p the canonical vector ei multiplied
by any arbitrary large positive value to obtain an unbounded value. When trying to
maximize the lower bound H(h) it therefore makes sense to restrict vectors h to be
such that ATh ≤c, in which case the best possible lower bound becomes
We have therefore proved a weak duality result, namely that z ≤¯z.
Algorithmic Foundations
C-Transforms
We present in this section an important property of the dual optimal transport problem (3.3) which takes a more important meaning when used for the semidiscrete optimal
transport problem in §5.1. This section builds upon the original formulation (2.20) that
splits dual variables according to row and column sum constraints:
LC(a, b) =
(f,g)∈R(C) ⟨f, a⟩+ ⟨g, b⟩.
Consider any dual feasible pair (f, g). If we “freeze” the value of f, we can notice that
there is no better vector solution for g than the C-transform vector of f, denoted
f C ∈Rm and deﬁned as
(f C)j = min
i∈JnK Cij −fi,
since it is indeed easy to prove that (f, f C) ∈R(C) and that f C is the largest possible
vector such that this constraint is satisﬁed. We therefore have that
⟨f, a⟩+ ⟨g, b⟩≤⟨f, a⟩+ ⟨f C, b⟩.
This result allows us ﬁrst to reformulate the dual problem as a piecewise aﬃne concave
maximization problem expressed in a single variable f as
LC(a, b) = max
f∈Rn ⟨f, a⟩+ ⟨f C, b⟩.
Putting that result aside, the same reasoning applies of course if we now “freeze”
the values of g and consider instead the ¯C-transform of g, namely vector g ¯C ∈Rn
¯C)i = min
j∈JmK Cij −gj,
with a diﬀerent increase in objective
⟨f, a⟩+ ⟨g, b⟩≤⟨g
¯C, a⟩+ ⟨g, b⟩.
Starting from a given f, it is therefore tempting to alternate C and ¯C transforms several
times to improve f. Indeed, we have the sequence of inequalities
⟨f, a⟩+ ⟨f C, b⟩≤⟨f C ¯C, a⟩+ ⟨f C, b⟩≤⟨f C ¯C, a⟩+ ⟨f C ¯CC, b⟩≤. . .
One may hope for a strict increase in the objective at each of these iterations. However,
this does not work because alternating C and ¯C transforms quickly hits a plateau.
Proposition 3.1. The following identities, in which the inequality sign between vectors
should be understood elementwise, hold:
(i) f ≤f ′ ⇒f C ≥f ′ C,
3.3. Complementary Slackness
(ii) f C ¯C ≥f, g ¯CC ≥g,
(iii) f C ¯CC = f C.
Proof. The ﬁrst inequality follows from the deﬁnition of C-transforms. Expanding the
deﬁnition of f C ¯C we have
j∈JmK Cij −f C
j∈JmK Cij −min
i′∈JnK Ci′j −fi′.
Now, since −mini′∈JnK Ci′j −fi′ ≥−(Cij −fi), we recover
j∈JmK Cij −Cij + fi = fi.
The relation g ¯CC ≥g is obtained in the same way. Now, set g = f C. Then, g ¯C =
f C ¯C ≥f. Therefore, using result (i) we have f C ¯CC ≤f C. Result (ii) yields f C ¯CC ≥f C,
proving the equality.
Complementary Slackness
Primal (3.2) and dual (3.3), (2.20) problems can be solved independently to obtain
optimal primal P⋆and dual (f⋆, g⋆) solutions. The following proposition characterizes
their relationship.
Proposition 3.2. Let P⋆and f⋆, g⋆be optimal solutions for the primal (2.24) and
dual (2.11) problems, respectively. Then, for any pair (i, j) ∈JnK × JmK, P⋆
i,j(Ci,j −
j) = 0 holds. In other words, if P⋆
i,j > 0, then necessarily f⋆
j = Ci,j; if
j < Ci,j then necessarily P⋆
Proof. We have by strong duality that ⟨P⋆, C⟩= ⟨f⋆, a⟩+⟨g⋆, b⟩. Recall that P⋆1m =
a and P⋆T1n = b; therefore
⟨f⋆, a⟩+ ⟨g⋆, b⟩= ⟨f⋆, P⋆1m⟩+ ⟨g⋆, P⋆T1n⟩
= ⟨f⋆1mT, P⋆⟩+ ⟨1ng⋆T, P⋆⟩,
which results in
⟨P⋆, C −f⋆⊕g⋆⟩= 0.
Because (f⋆, g⋆) belongs to the polyhedron of dual constraints (2.21), each entry of the
matrix C −f⋆⊕g⋆is necessarily nonnegative. Therefore, since all the entries of P are
nonnegative, the constraint that the dot-product above is equal to 0 enforces that, for
any pair of indices (i, j) such that Pi,j > 0, Ci,j −(fi + gj) must be zero, and for any
pair of indices (i, j) such that Ci,j > fi + gj that Pi,j = 0.
The converse result is also true. We deﬁne ﬁrst the idea that two variables for the
primal and dual problems are complementary.
Algorithmic Foundations
Deﬁnition 3.1. A matrix P ∈Rn×m and a pair of vectors (f, g) are complementary
w.r.t. C if for all pairs of indices (i, j) such that Pi,j > 0 one also has Ci,j = fi + gj.
If a pair of feasible primal and dual variables is complementary, then we can conclude
they are optimal.
Proposition 3.3. If P and (f, g) are complementary and feasible solutions for the primal (2.24) and dual (2.11) problems, respectively, then P and (f, g) are both primal
and dual optimal.
Proof. By weak duality, we have that
LC(a, b) ≤⟨P, C⟩= ⟨P, f ⊕g⟩= ⟨a, f⟩+ ⟨b, g⟩≤LC(a, b)
and therefore P and (f, g) are respectively primal and dual optimal.
Vertices of the Transportation Polytope
Recall that a vertex or an extremal point of a convex set is formally a point x in that
set such that, if there exiss y and z in that set with x = (y + z)/2, then necessarily
x = y = z. A linear program with a nonempty and bounded feasible set attains its
minimum at a vertex (or extremal point) of the feasible set [Bertsimas and Tsitsiklis,
1997, p. 65, Theo. 2.7]. Since the feasible set U(a, b) of the primal optimal transport
problem (3.2) is bounded, one can restrict the search for an optimal P to the set of
extreme points of the polytope U(a, b). Matrices P that are extremal in U(a, b) have
an interesting structure that has been the subject of extensive research [Brualdi, 2006,
§8]. That structure requires describing the transport problem using the formalism of
bipartite graphs.
Tree Structure of the Support of All Vertices of U(a, b)
Let V = (1, 2, . . . , n) and V ′ = (1′, 2′, . . . , m′) be two sets of nodes. Note that we add a
prime to the labels of set V ′ to disambiguate them from those of V . Consider their union
V ∪V ′, with n+m nodes, and the set E of all nm directed edges {(i, j′), i ∈JnK, j ∈JmK}
between them (here we just add a prime to an integer j ≤m to form j′ in V ′). To
each edge (i, j′) we associate the corresponding cost value Cij. The complete bipartite
graph G between V and V ′ is (V ∪V ′, E). A transport plan is a ﬂow on that graph
satisfying source (ai ﬂowing out of each node i) and sink (bj ﬂowing into each node
j′) constraints, as described informally in Figure 3.1. An extremal point in U(a, b) has
the following property [Brualdi, 2006, p. 338, Theo. 8.1.2].
Proposition 3.4 (Extremal solutions). Let P be an extremal point of the polytope
U(a, b). Let S(P) ⊂E be the subset of edges {(i, j′), i ∈JnK, j ∈JmK such that Pij >
3.4. Vertices of the Transportation Polytope
Figure 3.1: The optimal transport problem as a bipartite network ﬂow problem. Here n = 3, m = 4.
All coordinates of the source histogram, a, are depicted as source nodes on the left labeled 1, 2, 3,
whereas all coordinates of the target histogram b are labeled as nodes 1′, 2′, 3′, 4′. The graph is bipartite
in the sense that all source nodes are connected to all target nodes, with no additional edges. To each
edge (i, j′) is associated a cost Cij. A feasible ﬂow is represented on the right. Proposition 3.4 shows
that this ﬂow is not extremal since it has at least one cycle given by ((1, 1′), (2, 1′), (2, 4′), (1, 4′)).
Figure 3.2:
A solution P with a cycle in the graph of its support can be perturbed to obtain two
feasible solutions Q and R such that P is their average, therefore disproving that P is extremal.
0}. Then the graph G(P)
= (V ∪V ′, S(P)) has no cycles. In particular, P cannot have
more than n + m −1 nonzero entries.
Proof. We proceed by contradiction. Suppose that P is an extremal point of the polytope U(a, b) and that its corresponding set S(P) of edges, denoted F for short, is such
that the graph G = (V ∪V ′, F) contains a cycle, namely there exists k > 1 and a
sequence of distinct indices i1, . . . , ik−1 ∈JnK and j1, . . . , jk−1 ∈JmK such that the set
of edges H given below forms a subset of F.
1), (i2, j′
1), (i2, j′
2), . . . , (ik, j′
k), (i1, j′
We now construct two feasible matrices Q and R such that P = (Q + R)/2. To
do so, consider a directed cycle ¯H corresponding to H, namely the sequence of pairs
1 →i2, i2 →j′
2, . . . , ik →j′
k →i1, as well as the elementary amount of ﬂow
Algorithmic Foundations
ε < min(i,j′)∈F Pij. Consider a perturbation matrix E whose (i, j) entry is equal to ε
if i →j′ ∈¯H, −ε if j →i′ ∈¯H, and zero otherwise. Deﬁne matrices Q = P + E and
R = P−E as illustrated in Figure 3.2. Because ε is small enough, all elements in Q and
R are nonnegative. By construction, E has either lines (resp., columns) with all entries
equal to 0 or exactly one entry equal to ε and another equal to −ε for those indexed
by i1, . . . , ik (resp., j1, . . . , jk). Therefore, E is such that E1m = 0n and ET1n = 0m,
and we have that Q and R have the same marginals as P, and are therefore feasible.
Finally P = (Q + R)/2 which, since Q, R ̸= P, contradicts the fact that P is an
extremal point. Since a graph with k nodes and no cycles cannot have more than k −1
edges, we conclude that S(P) cannot have more than n + m −1 edges, and therefore
P cannot have more than n + m −1 nonzero entries.
The North-West Corner Rule
The north-west (NW) corner rule is a heuristic that produces a vertex of the polytope
U(a, b) in up to n + m operations. This heuristic can play a role in initializing any
algorithm working on the primal, such as the network simplex outlined in the next
The rule starts by giving the highest possible value to P1,1 by setting it to
min(a1, b1). At each step, the entry Pi,j is chosen to saturate either the row constraint at i, the column constraint at j, or both if possible. The indices i, j are then
updated as follows: i is incremented in the ﬁrst case, j is in the second, and both i and
j are in the third case. The rule proceeds until Pn,m has received a value.
Formally, the algorithm works as follows: i and j are initialized to 1, r ←a1, c ←b1.
While i ≤n and j ≤m, set t ←min(r, c), Pi,j ←t, r ←r −t, c ←s −t; if r = 0 then
increment i, and update r ←ai if i ≤n; if c = 0 then increment j, and update c ←bj
if j ≤n; repeat. Here is an example of this sequence assuming a = [0.2, 0.5, 0.3] and
b = [0.5, 0.1, 0.4]:
We write NW(a, b) for the unique plan that can be obtained through this heuristic.
Note that there is, however, a much larger number of NW corner solutions that
can be obtained by permuting arbitrarily the order of a and b ﬁrst, computing
the corresponding NW corner table, and recovering a table of U(a, b) by inverting again the order of columns and rows: setting σ = (3, 1, 2), σ′ = (3, 2, 1) gives
3.5. A Heuristic Description of the Network Simplex
aσ = [0.3, 0.2, 0.5], bσ′ = [0.4, 0.1, 0.5], and σ−1 = (2, 3, 1), σ′ = (3, 2, 1). Observe that
NW(aσ, bσ′) =
∈U(aσ, bσ′),
NWσ−1σ′−1(aσ, bσ′) =
∈U(a, b).
Let N(a, b) be the set of all NW corner solutions that can be produced this way:
= {NWσ−1σ′−1(rσ, cσ′), σ, σ′ ∈Sd}.
All NW corner solutions have by construction up to n + m −1 nonzero elements. The
NW corner rule produces a table which is by construction unique for aσ and b′
there is an exponential number of pairs or row/column permutations (σ, σ′) that may
yield the same table [Stougie, 2002, p. 2]. N(a, b) forms a subset of (usually strictly
included in) the set of extreme points of U(a, b) [Brualdi, 2006, Cor. 8.1.4].
A Heuristic Description of the Network Simplex
Consider a feasible matrix P whose graph G(P) = (V ∪V ′, S(P)) has no cycles. P has
therefore no more than n + m −1 nonzero entries and is a vertex of U(a, b) by Proposition 3.4. Following Proposition 3.3, it is therefore suﬃcient to obtain a dual solution
(f, g) which is feasible (i.e. C−f⊕g has nonnegative entries) and complementary to P
(pairs of indices (i, j′) in S(P) are such that Ci,j = fi +gj), to prove that P is optimal.
The network simplex relies on two simple principles: to each feasible primal solution
P one can associate a complementary pair (f, g). If that pair is feasible, then we have
reached optimality. If not, one can consider a modiﬁcation of P that remains feasible
and whose complementary pair (f, g) is modiﬁed so that it becomes closer to feasibility.
Obtaining a Dual Pair Complementary to P
The simplex proceeds by associating ﬁrst to any extremal solution P a pair of (f, g)
complementary dual variables. This is simply carried out by ﬁnding two vectors f and
g such that for any (i, j′) in S(P), fi + gj is equal to Ci,j. Note that this, in itself, does
not guarantee that (f, g) is feasible.
Let s be the cardinality of S(P). Because P is extremal, s ≤n + m −1. Because
G(P) has no cycles, G(P) is either a tree or a forest (a union of trees), as illustrated
in Figure 3.3. Aiming for a pair (f, g) that is complementary to P, we consider the
Algorithmic Foundations
{1, 2, 10, 20, 30},
{1, 10}, {1, 20}, {2, 20}, {2, 30}
{3, 4, 40, 50},
{3, 40}, {4, 40}, {4, 50}
{1, 10}, {1, 20}, {2, 20}, {2, 30},
{3, 40}, {4, 40}, {4, 50}, {5, 60}
Figure 3.3: A feasible transport P and its corresponding set of edges S(P) and graph G(P). As can
be seen, the graph G(P) = ({1, . . . , 5, 1′, . . . , 6′}, S(P)) is a forest, meaning that it can be expressed as
the union of tree graphs, three in this case.
following set of s linear equality constraints on n + m variables:
where the elements of S(P) are enumerated as (i1, j′
1), . . . , (is, j′
Since s ≤n+m−1 < n+m, the linear system (3.6) above is always undetermined.
This degeneracy can be interpreted in part because the parameterization of U(a, b)
with n + m constraints results in n + m dual variables. A more careful formulation,
outlined in Remark 3.1, would have resulted in an equivalent formulation with only
n + m −1 constraints and therefore n + m −1 dual variables. However, s can also be
strictly smaller than n + m −1: This happens when G(P) is the disjoint union of two
or more trees. For instance, there are 5 + 6 = 11 dual variables (one for each node) in
Figure 3.3, but only 8 edges among these 11 nodes, namely 8 linear equations to deﬁne
(f, g). Therefore, there will be as many undetermined dual variables under that setting
as there will be connected components in G(P).
Consider a tree among those listed in G(P). Suppose that tree has k nodes i1, . . . , ik
among source nodes and l nodes j′
1, . . . , j′
l among target nodes, resulting in r
and r −1 edges, corresponding to k variables in f and l variables in g, linked with
r −1 linear equations. To lift an indetermination, we can choose arbitrarily a root node
in that tree and assign the value 0 to its corresponding dual variable. From there, we
can traverse the tree using a breadth-ﬁrst or depth-ﬁrst search to obtain a sequence
of simple variable assignments that determines the values of all other dual variables in
that tree, as illustrated in Figure 3.4. That procedure can then be repeated for all trees
in the graph of P to obtain a pair of dual variables (f, g) that is complementary to P.
3.5. A Heuristic Description of the Network Simplex
g1 := C1,1 −f1
f2 := C2,1 −g1
g2 := C2,2 −f2
g3 := C2,3 −f2
Figure 3.4: The ﬁve dual variables f1, f2, g1, g2, g3 corresponding to the ﬁve nodes appearing in the
ﬁrst tree of the graph G(P) illustrated in Figure 3.3 are linked through four linear equations that
involve corresponding entries in the cost matrix C. Because that system is degenerate, we choose a
root in that tree (node 1 in this example) and set its corresponding variable to 0 and proceed then by
traversing the tree (either breadth-ﬁrst or depth-ﬁrst) from the root to obtain iteratively the values of
the four remaining dual variables.
Network Simplex Update
The dual pair (f, g) obtained previously might be feasible, in the sense that for all i, j
we have fi + gj ≤Ci,j, in which case we have reached the optimum by Proposition 3.3.
When that is not the case, namely when there exists i, j such that fi + gj > Ci,j, the
network simplex algorithm kicks in. We ﬁrst initialize a graph G to be equal to the
graph G(P) corresponding to the feasible solution P and add the violating edge (i, j′)
to G. Two cases can then arise:
(a) G is (still) a forest, which can happen if (i, j′) links two existing subtrees. The
approach outlined in §3.5.1 can be used on graph G to recover a new complementary dual vector (f, g). Note that this addition simply removes an indetermination
among the n + m dual variables and does not result in any change in the primal
variable P. That update is usually called degenerate in the sense that (i, j′) has
now entered graph G although Pi,j remains 0. G(P) is, however, contained in G.
(b) G now has a cycle. In that case, we need to remove an edge in G to ensure that G
is still a forest, yet also modify P so that P is feasible and G(P) remains included
in G. These operations can all be carried out by increasing the value of Pi,j and
modifying the other entries of P appearing in the detected cycle, in a manner
very similar to the one we used to prove Proposition 3.4. To be more precise, let
us write that cycle (i1, j′
1, i2), (i2, j′
2), . . . , (il, j′
l, il+1) with the convention
that i1 = il+1 = i to ensure that the path is a cycle that starts and ends at i,
whereas j1 = j, to highlight the fact that the cycle starts with the added edge
{i, j}, going in the right direction. Increase now the ﬂow of all “positive” edges
k) (for k ≤l), and decrease that of “negative” edges (j′
k, ik+1) (for k ≤l), to
obtain an updated primal solution ˜P, equal to P for all but the following entries:
˜Pik,jk := Pik,jk + θ;
˜Pik+1,jk := Pik+1,jk −θ.
Here, θ is the largest possible increase at index i, j using that cycle. The value
Algorithmic Foundations
of θ is controlled by the smallest ﬂow negatively impacted by the cycle, namely
mink Pik+1,jk. That update is illustrated in Figure 3.5. Let k⋆be an index that
achieves that minimum. We then close the update by removing (ik⋆+1, jk⋆) from
G, to compute new dual variables (f, g) using the approach outlined in §3.5.1.
Improvement of the Primal Solution
Although this was not necessarily our initial motivation, one can show that the manipulation above can only improve the cost of P. If the added edge has not created a
cycle, case (a) above, the primal solution remains unchanged. When a cycle is created,
case (b), P is updated to ˜P, and the following equality holds:
⟨˜P, C⟩−⟨P, C⟩= θ
We now use the dual vectors (f, g) computed at the end of the previous iteration. They
are such that fik + gik = Cik,jk and fik+1 + gik = Cik+1,jk for all edges initially in G,
resulting in the identity
Cik+1,jk = Ci,j +
fik + gjk −
fik+1 + gjk
= Ci,j −(fi + gj).
That term is, by deﬁnition, negative, since i, j were chosen because Ci,j < fi −gj.
Therefore, if θ > 0, we have that
⟨˜P, C⟩= ⟨P, C⟩+ θ (Ci,j −(fi −fg)) < ⟨P, C⟩.
If θ = 0, which can happen if G and G(P) diﬀer, the graph G is simply changed, but
The network simplex algorithm can therefore be summarized as follows: Initialize
the algorithm with an extremal solution P, given for instance by the NW corner rule
as covered in §3.4.2. Initialize the graph G with G(P). Compute a pair of dual variables (f, g) that are complementary to P using the linear system solve using the tree
structure(s) in G as described in §3.5.1. (i) Look for a violating pair of indices to the
constraint C −f ⊕g ≥0; if none, P is optimal and stop. If there is a violating pair
(i, j′), (ii) add the edge (i, j′) to G. If G still has no cycles, update (f, g) accordingly;
if there is a cycle, direct it making sure (i, j′) is labeled as positive, and remove a negative edge in that cycle with the smallest ﬂow value, updating P, G as illustrated in
Figure 3.5, then build a complementary pair f, g accordingly; return to (i). Some of the
operations above require graph operations (cycle detection, tree traversals) which can
be implemented eﬃciently in this context, as described in .
3.6. Dual Ascent Methods
(a) {3, 30} added
(b.1) {1, 30} added
(b.2) {1, 10} removed
Figure 3.5:
Adding an edge {i, j} to the graph G(P) can result in either (a) the graph remains a
forest after this addition, in which case f, g can be recomputed following the approach outlined in §3.5.1;
(b.1) the addition of that edge creates a cycle, from which we can deﬁne a directed path; (b.2) the path
can be used to increase the value of Pi,j and propagate that change along the cycle to maintain the
ﬂow feasibility constraints, until the ﬂow of one of the edges that is negatively impacted by the cycle
is decreased to 0. This removes the cycle and updates P.
Orlin was the ﬁrst to prove the polynomial time complexity of the
network simplex. Tarjan provided shortly after an improved bound in
O ( (n + m)nm log(n + m) log ((n + m)∥C∥∞) ) which relies on more eﬃcient data
structures to help select pivoting edges.
Dual Ascent Methods
Dual ascent methods precede the network simplex by a few decades, since they can be
traced back to work by Borchardt and Jocobi and later König and Egerváry, as
recounted by Kuhn . The Hungarian algorithm is the best known algorithm in
that family, and it can work only in the particular case when a and b are equal and are
both uniform, namely a = b = 1n/n. We provide in what follows a concise description
of the more general family of dual ascent methods. This requires the knowledge of
the maximum ﬂow problem . By contrast to the
network simplex, presented above in the primal, dual ascent methods maintain at each
iteration dual feasible solutions whose objective is progressively improved by adding a
sparse vector to f and g. Our presentation is mostly derived from that of and starts with the following deﬁnition.
Deﬁnition 3.2. For S ⊂JnK, S′ ⊂JmK′ def.
= {1′, . . . , m′} we write 1S for the vector in Rn
of zeros except for ones at the indices enumerated in S, and likewise for the vector 1S′
in Rm with indices in S′.
In what follows, (f, g) is a feasible dual pair in R(C). Recall that this simply means
that for all pairs (i, j′) ∈JnK × JmK′, fi + gj ≤Cij. We say that (i, j′) is a balanced pair
(or edge) if fi + gj = Cij and inactive otherwise, namely if fi + gj < Cij. With this
Algorithmic Foundations
convention, we start with a simple result describing how a feasible dual pair (f, g) can
be perturbed using sparse vectors indexed by sets S and S′ and still remain feasible.
Proposition 3.5. (˜f, ˜g)
= (f, g) + ε(1S, −1S′) is dual feasible for a small enough ε > 0
if for all i ∈S, the fact that (i, j′) is balanced implies that j′ ∈S′.
Proof. For any i ∈S, consider the set Ii of all j′ ∈JmK′ such that (i, j′) is inactive,
namely such that fi + gj < Cij. Deﬁne εi
= minj∈Ii Ci,j −fi −gj, the smallest margin
by which fi can be increased without violating the constraints corresponding to j′ ∈Ii.
Indeed, one has that if ε ≤εi then ˜fi + ˜gj < Ci,j for any j′ ∈Ii. Consider now the set
Bi of balanced edges associated with i. Note that Bi = JmK′ \Ii. The assumption above
is that j′ ∈Bi ⇒j′ ∈S′. Therefore, one has that for j′ ∈Bi, ˜fi + ˜gj = fi +gj = Ci,j. As
a consequence, the inequality ˜fi + ˜gj ≤Ci,j is ensured for any j ∈JmK′. Choosing now
an increase ε smaller than the smallest possible allowed, namely mini∈S εi, we recover
that (˜f, ˜g) is dual feasible.
The main motivation behind the iteration of the network simplex presented in §3.5.1
is to obtain, starting from a feasible primal solution P, a complementary feasible dual
pair (f, g). To reach that goal, P is progressively modiﬁed such that its complementary
dual pair reaches dual feasibility. A symmetric approach, starting from a feasible dual
variable to obtain a feasible primal P, motivates dual ascent methods. The proposition below is the main engine of dual ascent methods in the sense that it guarantees
(constructively) the existence of an ascent direction for (f, g) that maintains feasibility. That direction is built, similarly to the network simplex, by designing a candidate
primal solution P whose infeasibility guides an update for (f, g).
Proposition 3.6. Either (f, g) is optimal for Problem (3.4) or there exists S ⊂JnK, S′ ⊂
JmK′ such that (˜f, ˜g)
= (f, g) + ε(1S, −1S′) is feasible for a small enough ε > 0 and has
a strictly better objective.
Proof. We consider ﬁrst a complementary primal variable P to (f, g). To that eﬀect, let
B be the set of balanced edges, namely all pairs (i, j′) ∈JnK × JmK′ such that fi + gj =
Ci,j, and form the bipartite graph whose vertices {1, . . . , n, 1′, . . . , m′} are linked with
edges in B only, complemented by a source node s connected with capacitated edges to
all nodes i ∈JnK with respective capacities ai, and a terminal node t also connected
to all nodes j′ ∈JmK′ with edges of respective capacities bj, as seen in Figure 3.6.
The Ford–Fulkerson algorithm can be used to
compute a maximal ﬂow F on that network, namely a family of n+m+|B| nonnegative
values indexed by (i, j′) ∈B as fsi ≤ai, fij′, fj′t ≤bj that obey ﬂow constraints and
such that P
i fsi is maximal. If the throughput of that ﬂow F is equal to 1, then a feasible
primal solution P, complementary to f, g by construction, can be extracted from F by
deﬁning Pi,j = fij′ for (i, j′) ∈B and zero elsewhere, resulting in the optimality of (f, g)
3.6. Dual Ascent Methods
and P by Proposition 3.3. If the throughput of F is strictly smaller than 1, the labeling
algorithm proceeds by labeling (identifying) those nodes reached iteratively from s for
which F does not saturate capacity constraints, as well as those nodes that contribute
ﬂow to any of the labeled nodes. Labeled nodes are stored in a nonempty set Q, which
does not contain the terminal node t per optimality of F . Q can be split into two sets
S = Q ∩JnK and S′ = Q ∩JmK′. Because we have assumed that the total throughput
is strictly smaller than 1, S ̸= ∅. Note ﬁrst that if i ∈S and (i, j) is balanced, then j′
is necessarily in S′. Indeed, since all edges (i, j′) have inﬁnite capacity by construction,
the labeling algorithm will necessarily reach j′ if it includes i in S. By Proposition 3.5,
there exists thus a small enough ε to ensure the feasibility of ˜f, ˜g. One still needs to
prove that 1STa −1S′Tb > 0 to ensure that (˜f, ˜g) has a better objective than (f, g).
Let ¯S = JnK \ S and ¯S′ = JmK′ \ S′ and deﬁne
The total maximal ﬂow starts from s and is therefore equal to A + B, but also arrives
at t and is therefore equal to C +D. Flow conservation constraints also impose that the
very same ﬂow is equal to B + C, therefore A = C. On the other hand, by deﬁnition
of the labeling algorithm, we have for all i in S that fsi < ai, whereas fj′t = bj for
j′ ∈¯S′ because t cannot be in S′ by optimality of the considered ﬂow. We therefore
have A < 1STa and C = 1′
Tb. Therefore 1STa −1′
Tb > A −C = 0.
The dual ascent method proceeds by modifying any feasible solution (f, g) by any
vector generated by sets S, S′ that ensure feasibility and improve the objective. When
the sets S, S′ are those given by construction in the proof of Proposition 3.6, and
the steplength ε is deﬁned as in the proof of Proposition 3.5, we recover a method
known as the primal-dual method. That method reduces to the Hungarian algorithm
for matching problems. Dual ascent methods share similarities with the dual variant
of the network simplex, yet they diﬀer in at least two important aspects. Simplex-type
methods always ensure that the current solution is an extreme point of the feasible
set, R(C) for the dual, whereas dual ascent as presented here does not make such
an assumption, and can freely produce iterates that lie in the interior of the feasible
set. Additionally, whereas the dual network simplex would proceed by modifying (f, g)
to produce a primal solution P that satisﬁes linear (marginal constraints) but only
nonnegativity upon convergence, dual ascent builds instead a primal solution P that is
always nonnegative but which does not necessarily satisfy marginal constraints.
Algorithmic Foundations
1 capacity
S0 = {20, 30, 40, 60}
(b) Maxﬂow = 0.74
(a) Given balanced edges, what is the maximal ﬂow possible?
(c) the labeling algorithm identiﬁes sets S, S0
S = {2, 3, 4, 5}
(d) total ﬂows A, B, C, D through nodes S, ¯S, S0, ¯S0
Figure 3.6: Consider a transportation problem involving the marginals introduced ﬁrst in Figure 3.3,
with n = 5, m = 6. Given two feasible dual vectors f, g, we try to obtain the “best” ﬂow matrix P that
is complementary to (f, g). Recall that this means that P can only take positive values on those edges
(i, j′) corresponding to indices for which fi + gj = Ci,j, here represented with dotted lines in plot (a).
The best ﬂow that can be achieved with that graph structure can be formulated as a max-ﬂow problem
in a capacitated network, starting from an abstract source node s connected to all nodes labeled i ∈JnK,
terminating at an abstract terminal node t connected to all nodes labeled j′, where j ∈Jm′K, and such
that the capacities of edge (s, i), (j′, t), i ∈JnK, j ∈JmK are respectively ai, bj and all others inﬁnite.
The Ford–Fulkerson algorithm can be applied to compute such
a max-ﬂow, which, as represented in plot (b), only achieves 0.74 units of mass out of 1 needed to solve
the problem. One of the subroutines used by max-ﬂow algorithms, the labeling algorithm , can be used to identify nodes that receive an unsaturated ﬂow from s
(and recursively, all of its successors), denoted by orange lines in plot (c). The labeling algorithm also
adds by default nodes that send a positive ﬂow to any labeled node, which is the criterion used to select
node 3, which contributes with a red line to 3′. Labeled nodes can be grouped in sets S, S′ to identify
nodes which can be better exploited to obtain a higher ﬂow, by modifying f, g to obtain a diﬀerent
graph. The proof involves partial sums of ﬂows described in plot (d)
Auction Algorithm
The auction algorithm was originally proposed by Bertsekas and later reﬁned
in [Bertsekas and Eckstein, 1988]. Several economic interpretations of this algorithm
have been proposed (see e.g. Bertsekas ). The algorithm can be adapted for arbitrary marginals, but we present it here in its formulation to solve optimal assignment
3.7. Auction Algorithm
Complementary slackness.
Notice that in the optimal assignment problem, the
primal-dual conditions presented for the optimal transport problem become easier to
formulate, because any extremal solution P is
necessarily a permutation matrix Pσ
for a given σ (see Equation (3.3)). Given primal Pσ⋆and dual f⋆, g⋆optimal solutions
we necessarily have that
Recall also that, because of the principle of C-transforms enunciated in §3.2, that one
can choose f⋆to be equal to g ¯C. We therefore have that
On the contrary, it is easy to show that if there exists a vector g and a permutation σ
Ci,σi −gσi = min
holds, then they are both optimal, in the sense that σ is an optimal assignment and
g ¯C, g is an optimal dual pair.
Partial assignments and ε-complementary slackness.
The goal of the auction algorithm is to modify iteratively a triplet S, ξ, g, where S is a subset of JnK, ξ a partial
assignment vector, namely an injective map from S to JnK, and g a dual vector. The
dual vector is meant to converge toward a solution satisfying an approximate complementary slackness property (3.8), whereas S grows to cover JnK as ξ describes a
permutation. The algorithm works by maintaining the three following properties after
each iteration:
(a) ∀i ∈S,
Ci,ξi −gξi ≤ε + minj Ci,j −gj (ε-CS).
(b) The size of S can only increase at each iteration.
(c) There exists an index i such that gi decreases by at least ε.
Auction algorithm updates.
Given a point j the auction algorithm uses not only the
optimum appearing in the usual C-transform but also a second best,
i ∈argminj Ci,j −gj,
i ∈argminj̸=j1
i Ci,j −gj,
to deﬁne the following updates on g for an index i /∈S, as well as on S and ξ:
1. update g: Remove to the j1
i th entry of g the sum of ε and the diﬀerence between
the second lowest and lowest adjusted cost {Ci,j −gj}j,
i ) −(Ci,j1
Algorithmic Foundations
2. update S and ξ: If there exists an index i′ ∈S such that ξi′ = j1
i , remove it by
updating S ←S \ {i′}. Set ξi = j1
i and add i to S, S ←S ∪{i}.
Algorithmic properties.
The algorithm proceeds by starting from an empty set of
assigned points S = ∅with no assignment and empty partial assignment vector ξ,
and g = 0n, terminates when S = JnK, and loops through both steps above until
it terminates. The fact that properties (b) and (c) are valid after each iteration is
made obvious by the nature of the updates (it suﬃces to look at Equation (3.9)). εcomplementary slackness is easy to satisfy at the ﬁrst iteration since in that case S = ∅.
The fact that iterations preserve that property is shown by the following proposition.
Proposition 3.7. The auction algorithm maintains ε-complementary slackness at each
iteration.
Proof. Let g, ξ, S be the three variables at the beginning of a given iteration. We therefore assume that for any i′ ∈S the relationship
Ci,ξi′ −gξi′ ≤ε + min
holds. Consider now the particular i /∈S considered in an iteration. Three updates
happen: g, ξ, S are updated to gn, ξn, Sn using indices j1
i . More precisely, gn is
equal to g except for element j1
i , whose value is equal to
i ) −(Ci,j1
, ξn is equal to ξ except for its ith element equal to j1
i , and Sn is equal to the union of
{i} with S (with possibly one element removed). The update of gn can be rewritten
therefore we have
i = ε + (Ci,j2
i ) = ε + min
(Ci,j −gj).
Since −g ≤−gn this implies that
i = ε + min
(Ci,j −gj) ≤ε + min
and since the inequality is also obviously true for j = j1
i we therefore obtain the εcomplementary slackness property for index i. For other indices i′ ̸= i, we have again
that since gn ≤g the sequence of inequalities holds,
i′ = Ci,ξi′ −gξi′ ≤ε + min
Ci′,j −gj ≤ε + min
3.7. Auction Algorithm
Proposition 3.8. The number of steps of the auction algorithm is at most N =
Proof. Suppose that the algorithm has not stopped after T > N steps. Then there
exists an index j which is not in the image of ξ, namely whose price coordinate gj has
never been updated and is still gj = 0. In that case, there cannot exist an index j′ such
that gj′ was updated n times with n > ∥C∥∞/ε. Indeed, if that were the case then for
any index i
gj′ ≤−nε < −∥C∥∞≤−Ci,j = gj −Ci,j,
which would result in, for all i,
Ci,j′ −gj′ > Ci,j + (Ci,j −gj),
which contradicts ε-CS. Therefore, since there cannot be more than ∥C∥∞/ε updates
for each variable, the total number of iterations T cannot be larger than n∥C∥∞/ε =
Remark 3.3. Note that this result yields a naive number of operations of N3∥C∥∞/ε
for the algorithm to terminate. That complexity can be reduced to N3 log ∥C∥∞when
using a clever method known as ε-scaling, designed to decrease the value of ε with each
iteration .
Proposition 3.9. The auction algorithm ﬁnds an assignment whose cost is nε suboptimal.
Proof. Let σ, g⋆be the primal and dual optimal solutions of the assignment problem
of matrix C, with optimum
Let ξ, g be the solutions output by the auction algorithm upon termination. The ε-CS
conditions yield that for any i ∈S,
Ci,j −gj ≥Ci,ξi −gξi −ε.
Therefore by simple suboptimality of g we ﬁrst have
Ci,ξi −gξi
gj = −nε +
Ci,ξj ≥−nε + t⋆.
where the second inequality comes from ε-CS, the next equality by cancellation of the
sum of terms in gξi and gj, and the last inequality by the suboptimality of ξ as a
permutation.
Algorithmic Foundations
The auction algorithm can therefore be regarded as an alternative way to use the
machinery of C-transforms. Next we explore another approach grounded on regularization, the so-called Sinkhorn algorithm, which also bears similarities with the auction
algorithm as discussed in [Schmitzer, 2016b].
Note ﬁnally that, on low-dimensional regular grids in Euclidean space, it is possible
to couple these classical linear solvers with multiscale strategies, to obtain a signiﬁcant
speed-up [Schmitzer, 2016a, Oberman and Ruan, 2015].
Entropic Regularization of Optimal Transport
This chapter introduces a family of numerical schemes to approximate solutions to
Kantorovich formulation of optimal transport and its many generalizations. It operates
by adding an entropic regularization penalty to the original problem. This regularization has several important advantages, which make it, when taken altogether, a very
useful tool: the minimization of the regularized problem can be solved using a simple
alternate minimization scheme; that scheme translates into iterations that are simple
matrix-vector products, making them particularly suited to execution of GPU; for some
applications, these matrix-vector products do not require storing an n×m cost matrix,
but instead only require access to a kernel evaluation; in the case where a large group
of measures share the same support, all of these matrix-vector products can be cast as
matrix-matrix products with signiﬁcant speedups; the resulting approximate distance
is smooth with respect to input histogram weights and positions of the Diracs and can
be diﬀerentiated using automatic diﬀerentiation.
Entropic Regularization
The discrete entropy of a coupling matrix is deﬁned as
Pi,j(log(Pi,j) −1),
with an analogous deﬁnition for vectors, with the convention that H(a) = −∞if one of
the entries aj is 0 or negative. The function H is 1-strongly concave, because its Hessian
is ∂2H(P) = −diag(1/Pi,j) and Pi,j ≤1. The idea of the entropic regularization
of optimal transport is to use −H as a regularizing function to obtain approximate
Entropic Regularization of Optimal Transport
solutions to the original transport problem (2.11):
P∈U(a,b) ⟨P, C⟩−εH(P).
Since the objective is an ε-strongly convex function, Problem (4.2) has a unique optimal
solution. The idea to regularize the optimal transport problem by an entropic term can
be traced back to modeling ideas in transportation theory [Wilson, 1969]: Actual traﬃc
patterns in a network do not agree with those predicted by the solution of the optimal
transport problem. Indeed, the former are more diﬀuse than the latter, which tend
to rely on a few routes as a result of the sparsity of optimal couplings for (2.11).
To mitigate this sparsity, researchers in transportation proposed a model, called the
“gravity” model [Erlander, 1980], that is able to form a more “blurred” prediction of
traﬃc given marginals and transportation costs.
Figure 4.1 illustrates the eﬀect of the entropy to regularize a linear program over the
simplex Σ3 (which can thus be visualized as a triangle in two dimensions). Note how
the entropy pushes the original LP solution away from the boundary of the triangle.
The optimal Pε progressively moves toward an “entropic center” of the triangle. This
is further detailed in the proposition below. The convergence of the solution of that
regularized problem toward an optimal solution of the original linear program has been
studied by Cominetti and San Martín , with precise asymptotics.
Figure 4.1:
Impact of ε on the optimization of a linear function on the simplex, solving Pε =
argminP∈Σ3⟨C, P⟩−εH(P) for a varying ε.
Proposition 4.1 (Convergence with ε). The unique solution Pε of (4.2) converges to
the optimal solution with maximal entropy within the set of all optimal solutions of
the Kantorovich problem, namely
{−H(P) : P ∈U(a, b), ⟨P, C⟩= LC(a, b), }
so that in particular
C(a, b) ε→0
−→LC(a, b).
One also has
−→a ⊗b = abT = (aibj)i,j.
4.1. Entropic Regularization
Proof. We consider a sequence (εℓ)ℓsuch that εℓ→0 and εℓ> 0. We denote Pℓthe
solution of (4.2) for ε = εℓ. Since U(a, b) is bounded, we can extract a sequence (that
we do not relabel for the sake of simplicity) such that Pℓ→P⋆. Since U(a, b) is closed,
P⋆∈U(a, b). We consider any P such that ⟨C, P⟩= LC(a, b). By optimality of P
and Pℓfor their respective optimization problems (for ε = 0 and ε = εℓ), one has
0 ≤⟨C, Pℓ⟩−⟨C, P⟩≤εℓ(H(Pℓ) −H(P)).
Since H is continuous, taking the limit ℓ→+∞in this expression shows that ⟨C, P⋆⟩=
⟨C, P⟩so that P⋆is a feasible point of (4.3). Furthermore, dividing by εℓin (4.5) and
taking the limit shows that H(P) ≤H(P⋆), which shows that P⋆is a solution of (4.3).
Since the solution P⋆
0 to this program is unique by strict convexity of −H, one has
0, and the whole sequence is converging. In the limit ε →+∞, a similar proof
shows that one should rather consider the problem
P∈U(a,b) −H(P),
the solution of which is a ⊗b.
Formula (4.3) states that for a small regularization ε, the solution converges to the
maximum entropy optimal transport coupling. In sharp contrast, (4.4) shows that for
a large regularization, the solution converges to the coupling with maximal entropy between two prescribed marginals a, b, namely the joint probability between two independent random variables distributed following a, b. A reﬁned analysis of this convergence
is performed in Cominetti and San Martín , including a ﬁrst order expansion in
ε (resp., 1/ε) near ε = 0 (resp., ε = +∞). Figures 4.2 and 4.3 show visually the eﬀect
of these two convergences. A key insight is that, as ε increases, the optimal coupling
becomes less and less sparse (in the sense of having entries larger than a prescribed
threshold), which in turn has the eﬀect of both accelerating computational algorithms
(as we study in §4.2) and leading to faster statistical convergence (as shown in §8.5).
Deﬁning the Kullback–Leibler divergence between couplings as
−Pi,j + Ki,j,
the unique solution Pε of (4.2) is a projection onto U(a, b) of the Gibbs kernel associated to the cost matrix C as
Indeed one has that using the deﬁnition above
Pε = ProjKL
Entropic Regularization of Optimal Transport
Figure 4.2:
Impact of ε on the couplings between two 1-D densities, illustrating Proposition 4.1.
Top row: between two 1-D densities. Bottom row: between two 2-D discrete empirical densities with
the same number n = m of points (only entries of the optimal (Pi,j)i,j above a small threshold are
displayed as segments between xi and yj).
Figure 4.3:
Impact of ε on coupling between two 2-D discrete empirical densities with the same
number n = m of points (only entries of the optimal (Pi,j)i,j above a small threshold are displayed as
segments between xi and yj).
Remark 4.1 (Entropic regularization between discrete measures). For discrete measures of the form (2.1), the deﬁnition of regularized transport extends naturally
with cost Ci,j = c(xi, yj), to emphasize the dependency with respect to the positions (xi, yj) supporting the input measures.
4.1. Entropic Regularization
Remark 4.2 (General formulation). One can consider arbitrary measures by replacing the discrete entropy by the relative entropy with respect to the product measure
dα⊗dβ(x, y)
= dα(x)dβ(y), and propose a regularized counterpart to (2.15) using
c(x, y)dπ(x, y) + ε KL(π|α ⊗β),
where the relative entropy is a generalization of the discrete Kullback–Leibler divergence (4.6)
(dξ(x, y) −dπ(x, y)),
and by convention KL(π|ξ) = +∞if π does not have a density dπ
dξ with respect
to ξ. It is important to realize that the reference measure α ⊗β chosen in (4.9)
to deﬁne the entropic regularizing term KL(·|α ⊗β) plays no speciﬁc role; only its
support matters, as noted by the following proposition.
Proposition 4.2. For any π ∈U(α, β), and for any (α′, β′) having the same 0
measure sets as (α, β) (so that they have both densities with respect to one another)
KL(π|α ⊗β) = KL(π|α′ ⊗β′) −KL(α ⊗β|α′ ⊗β′).
This proposition shows that choosing KL(·|α′ ⊗β′) in place of KL(·|α ⊗β)
in (4.9) results in the same solution.
Formula (4.9) can be refactored as a projection problem
π∈U(α,β) KL(π|K),
where K is the Gibbs distributions dK(x, y)
= e−c(x,y)
dα(x)dβ(y). This problem is
often referred to as the “static Schrödinger problem” [Léonard, 2014, Rüschendorf
and Thomsen, 1998], since it was initially considered by Schrödinger in statistical
physics [Schrödinger, 1931]. As ε →0, the unique solution to (4.11) converges to
the maximum entropy solution to (2.15); see [Léonard, 2012, Carlier et al., 2017].
Section 7.6 details an alternate “dynamic” formulation of the Schrödinger problem
over the space of paths connecting the points of two measures.
Remark 4.3 (Mutual entropy). Similarly to (2.16), one can rephrase (4.9) using
Entropic Regularization of Optimal Transport
random variables
c(α, β) = min
E(X,Y )(c(X, Y )) + εI(X, Y ) : X ∼α, Y ∼β
where, denoting π the distribution of (X, Y ), I(X, Y )
= KL(π|α ⊗β) is the socalled mutual information between the two random variables. One has I(X, Y ) ≥0
and I(X, Y ) = 0 if and only if the two random variables are independent.
Remark 4.4 (Independence and couplings). A coupling π ∈U(α, β) describes the
distribution of a couple of random variables (X, Y ) deﬁned on (X, Y), where X
(resp., Y ) has law α (resp., β). Proposition 4.1 carries over for generic (nonnecessary
discrete) measures, so that the solution πε of (4.9) converges to the tensor product
coupling α⊗β as ε →+∞. This coupling α⊗β corresponds to the random variables
(X, Y ) being independent. In contrast, as ε →0, πε convergence to a solution π0 of
the OT problem (2.15). On X = Y = Rd, if α and β have densities with respect to
the Lebesgue measure, as detailed in Remark 2.24, then π0 is unique and supported
on the graph of a bijective Monge map T : Rd →Rd. In this case, (X, Y ) are in
some sense fully dependent, since Y = T(X) and X = T −1(Y ). In the simple 1-D
case d = 1, a convenient way to visualize the dependency structure between X and
Y is to use the copula ξπ associated to the joint distribution π. The cumulative
function deﬁned in (2.34) is extended to couplings as
∀(x, y) ∈R2,
The copula is then deﬁned as
∀(s, t) ∈ 2,
α (s), C−1
where the pseudoinverse of a cumulative function is deﬁned in (2.35). For independent variables, ε = +∞, i.e. π = α ⊗β, one has ξπ+∞(s, t) = st. In contrast, for
fully dependent variables, ε = +∞, one has ξπ0(s, t) = min(s, t). Figure 4.4 shows
how entropic regularization generates copula ξπε interpolating between these two
extreme cases.
Sinkhorn’s Algorithm and Its Convergence
The following proposition shows that the solution of (4.2) has a speciﬁc form, which can
be parameterized using n + m variables. That parameterization is therefore essentially
dual, in the sense that a coupling P in U(a, b) has nm variables but n+m constraints.
4.2. Sinkhorn’s Algorithm and Its Convergence
ε = 0.5 · 10−1
Figure 4.4: Top: evolution with ε of the solution πε of (4.9). Bottom: evolution of the copula function
Proposition 4.3. The solution to (4.2) is unique and has the form
∀(i, j) ∈JnK × JmK,
Pi,j = uiKi,jvj
for two (unknown) scaling variable (u, v) ∈Rn
Proof. Introducing two dual variables f ∈Rn, g ∈Rm for each marginal constraint, the
Lagrangian of (4.2) reads
E(P, f, g) = ⟨P, C⟩−εH(P) −⟨f, P1m −a⟩−⟨g, PT1n −b⟩.
First order conditions then yield
∂E(P, f, g)
= Ci,j + ε log(Pi,j) −fi −gj = 0,
which result, for an optimal P coupling to the regularized problem, in the expression
Pi,j = efi/εe−Ci,j/εegj/ε, which can be rewritten in the form provided above using
nonnegative vectors u and v.
Regularized OT as matrix scaling.
The factorization of the optimal solution exhibited in Equation (4.12) can be conveniently rewritten in matrix form as P =
Entropic Regularization of Optimal Transport
diag(u)K diag(v). The variables (u, v) must therefore satisfy the following nonlinear
equations which correspond to the mass conservation constraints inherent to U(a, b):
diag(u)K diag(v)1m = a,
diag(v)K⊤diag(u)1n = b.
These two equations can be further simpliﬁed, since diag(v)1m is simply v, and the
multiplication of diag(u) times Kv is
u ⊙(Kv) = a
v ⊙(KTu) = b,
where ⊙corresponds to entrywise multiplication of vectors. That problem is known in
the numerical analysis community as the matrix scaling problem . An intuitive way to handle these equations
is to solve them iteratively, by modifying ﬁrst u so that it satisﬁes the left-hand side
of Equation (4.14) and then v to satisfy its right-hand side. These two updates deﬁne
Sinkhorn’s algorithm,
u(ℓ+1) def.
v(ℓ+1) def.
KTu(ℓ+1) ,
initialized with an arbitrary positive vector v(0) = 1m. The division operator used above
between two vectors is to be understood entrywise. Note that a diﬀerent initialization
will likely lead to a diﬀerent solution for u, v, since u, v are only deﬁned up to a
multiplicative constant (if u, v satisfy (4.13) then so do λu, v/λ for any λ > 0). It turns
out, however, that these iterations converge (see Remark 4.8 for a justiﬁcation using
iterative projections, and see Remark 4.14 for a strict contraction result) and all result
in the same optimal coupling diag(u)K diag(v). Figure 4.5, top row, shows the evolution
of the coupling diag(u(ℓ))K diag(v(ℓ)) computed by Sinkhorn iterations. It evolves from
the Gibbs kernel K toward the optimal coupling solving (4.2) by progressively shifting
the mass away from the diagonal.
Remark 4.5 (Historical perspective). The iterations (4.15) ﬁrst appeared in [Yule, 1912,
Kruithof, 1937]. They were later known as the iterative proportional ﬁtting procedure
(IPFP) Deming and Stephan and RAS [Bacharach, 1965] methods [Idel, 2016].
The proof of their convergence is attributed to Sinkhorn , hence the name of the
algorithm. This algorithm was later extended in inﬁnite dimensions by Ruschendorf
 . This regularization was used in the ﬁeld of economics to obtain approximate
solutions to optimal transport problems, under the name of gravity models [Wilson,
1969, Erlander, 1980, Erlander and Stewart, 1990]. It was rebranded as “softassign”
by Kosowsky and Yuille in the assignment case, namely when a = b = 1n/n,
and used to solve matching problems in economics more recently by Galichon and
Salanié . This regularization has received renewed attention in data sciences (including machine learning, vision, graphics and imaging) following [Cuturi, 2013], who
4.2. Sinkhorn’s Algorithm and Its Convergence
showed that Sinkhorn’s algorithm provides an eﬃcient and scalable approximation to
optimal transport, thanks to seamless parallelization when solving several OT problems
simultaneously (notably on GPUs; see Remark 4.16), and that this regularized quantity
also deﬁnes, unlike the linear programming formulation, a diﬀerentiable loss function
(see §4.5). There exist countless extensions and generalizations of the Sinkhorn algorithm (see for instance §4.6). For instance, when a = b, one can use averaged projection
iterations to maintain symmetry [Knight et al., 2014].
Figure 4.5:
Top: evolution of the coupling π(ℓ)
= diag(u(ℓ))K diag(v(ℓ)) computed at iteration ℓ
of Sinkhorn’s iterations, for 1-D densities on X = , c(x, y) = |x −y|2, and ε = 0.1. Bottom:
impact of ε the convergence rate of Sinkhorn, as measured in term of marginal constraint violation
ε 1m −b∥1).
Remark 4.6 (Overall complexity). By doing a careful convergence analysis (assuming n = m for the sake of simplicity), Altschuler et al. showed that by
setting ε = 4 log(n)
∞log(n)τ −3) Sinkhorn iterations (with an additional
rounding step to compute a valid coupling ˆP ∈U(a, b)) are enough to ensure that
⟨ˆP, C⟩≤LC(a, b)+τ. This implies that Sinkhorn computes a τ-approximate solution of the unregularized OT problem in O(n2 log(n)τ −3) operations. The rounding
scheme consists in, given two vectors u ∈Rn, v ∈Rm to carry out the following
Entropic Regularization of Optimal Transport
updates :
u ⊙(Kv), 1n
v ⊙(KTu′), 1n
= a −u′ ⊙(Kv′), ∆b
= b −v′ ⊙(KTu),
= diag(u′)K diag(v′) + ∆a(∆b)T/ ∥∆a∥1 .
This yields a matrix
U(a, b) such that the 1-norm between
diag(u)K diag(v) is controlled by the marginal violations of diag(u)K diag(v),
ˆP −diag(u)K diag(v)
1 ≤∥a −u ⊙(Kv)∥1 +
b −v ⊙(KTu)
This ﬁeld remains active, as shown by the recent improvement on the result above
by Dvurechensky et al. .
Remark 4.7 (Numerical stability of Sinkhorn iterations). As we discuss in Remarks 4.14
and 4.15, the convergence of Sinkhorn’s algorithm deteriorates as ε →0. In numerical
practice, however, that slowdown is rarely observed in practice for a simpler reason:
Sinkhorn’s algorithm will often fail to terminate as soon as some of the elements of
the kernel K become too negligible to be stored in memory as positive numbers, and
become instead null. This can then result in a matrix product Kv or KTu with ever
smaller entries that become null and result in a division by 0 in the Sinkhorn update
of Equation (4.15). Such issues can be partly resolved by carrying out computations
on the multipliers u and v in the log domain. That approach is carefully presented in
Remark 4.23 and is related to a direct resolution of the dual of Problem (4.2).
Remark 4.8 (Relation with iterative projections). Denoting
= {P : P1m = a}
P : PT1m = b
the rows and columns constraints, one has U(a, b) = C1
b. One can use Bregman
iterative projections [Bregman, 1967],
P(ℓ+1) def.
C1a (P(ℓ))
P(ℓ+2) def.
b (P(ℓ+1)).
Since the sets C1
b are aﬃne, these iterations are known to converge to the solution
of (4.7); see [Bregman, 1967]. These iterates are equivalent to Sinkhorn iterations (4.15)
since deﬁning
P(2ℓ) def.
= diag(u(ℓ))K diag(v(ℓ)),
P(2ℓ+1) def.
= diag(u(ℓ+1))K diag(v(ℓ))
P(2ℓ+2) def.
= diag(u(ℓ+1))K diag(v(ℓ+1)).
4.2. Sinkhorn’s Algorithm and Its Convergence
In practice, however, one should prefer using (4.15), which only requires manipulating
scaling vectors and multiplication against a Gibbs kernel, which can often be accelerated
(see Remarks 4.17 and 4.19 below).
Remark 4.9 (Proximal point algorithm). In order to approximate a solution of the unregularized (ε = 0) problem (2.11), it is possible to use iteratively the Sinkhorn algorithm, using the so-called proximal point algorithm for the KL metric. We denote
= ⟨P, π⟩+ ιU(a,b)(P) the unregularized objective function. The proximal point
iterations for the KL divergence computes a minimizer of F, and hence a solution of
the unregularized OT problem (2.11), by computing iteratively
P(ℓ+1) def.
ε F (P(ℓ))
KL(P|P(ℓ)) + 1
starting from an arbitrary P(0) (see also (4.52)). The proximal point algorithm is the
most basic proximal splitting method. Initially introduced for the Euclidean metric
 ), it extends to any Bregman divergence [Censor
and Zenios, 1992], so in particular it can be applied here for the KL divergence (see
Remark 8.1). The proximal operator is usually not available in closed form, so some
form of subiterations are required. The optimization appearing in (4.17) is very similar
to the entropy regularized problem (4.2), with the relative entropy KL(·|P(ℓ)) used in
place of the negative entropy −H. Proposition 4.3 and Sinkhorn iterations (4.15) carry
over to this more general setting when deﬁning the Gibbs kernel as K = e−C
i,j )i,j. Iterations (4.17) can thus be implemented by running the Sinkhorn
algorithm at each iteration. Assuming for simplicity P(0) = 1n1⊤
m, these iterations thus
have the form
P (ℓ+1) = diag(u(ℓ))(e−C
ε ⊙P(ℓ)) diag(v(ℓ))
= diag(u(ℓ) ⊙· · · ⊙u(0))e−(ℓ+1)C
⊙P(ℓ)) diag(v(ℓ) ⊙· · · ⊙v(0)).
The proximal point iterates apply therefore iteratively Sinkhorn’s algorithm with a
kernel e−C
ε/ℓ, i.e., with a decaying regularization parameter ε/ℓ. This method is thus
tightly connected to a series of works which combine Sinkhorn with some decaying
schedule on the regularization; see, for instance, [Kosowsky and Yuille, 1994]. They
are eﬃcient in small spacial dimension, when combined with a multigrid strategy to
approximate the coupling on an adaptive sparse grid [Schmitzer, 2016b].
Remark 4.10 (Other regularizations). It is possible to replace the entropic term −H(P)
in (4.2) by any strictly convex penalty R(P), as detailed, for instance, in [Dessein et al.,
2018]. A typical example is the squared ℓ2 norm
i,j + ιR+(Pi,j);
Entropic Regularization of Optimal Transport
see [Essid and Solomon, 2017]. Another example is the family of Tsallis entropies [Muzellec et al., 2017]. Note, however, that if the penalty function is deﬁned even when entries
of P are nonpositive, which is, for instance, the case for a quadratic regularization (4.18),
then one must add back a nonnegativity constraint P ≥0, in addition to the marginal
constraints P1m = a and P⊤1n = b. Indeed, one can aﬀord to ignore the nonnegativity constraint using entropy because that penalty incorporates a logarithmic term
which forces the entries of P to stay in the positive orthant. This implies that the set
of constraints is no longer aﬃne and iterative Bregman projections do not converge
anymore to the solution. A workaround is to use instead Dykstra’s algorithm , as detailed in [Benamou et al., 2015]. This
algorithm uses projections according to the Bregman divergence associated to R. We
refer to Remark 8.1 for more details regarding Bregman divergences. An issue is that in
general these projections cannot be computed explicitly. For the squared norm (4.18),
this corresponds to computing the Euclidean projection on (C1
b) (with the extra
positivity constraints), which can be solved eﬃciently using projection algorithms on
simplices [Condat, 2015]. The main advantage of the quadratic regularization over entropy is that it produces sparse approximation of the optimal coupling, yet this comes at
the expense of a slower algorithm that cannot be parallelized as eﬃciently as Sinkhorn
to compute several optimal transports simultaneously (as discussed in §4.16). Figure 4.6
contrasts the approximation achieved by entropic and quadratic regularizers.
ε = 0.5 · 10−1
ε = 5 · 103
Figure 4.6:
Comparison of entropic regularization R = −H (top row) and quadratic regularization
R = ∥·∥2 + ιR+ (bottom row). The (α, β) marginals are the same as for Figure 4.4.
4.2. Sinkhorn’s Algorithm and Its Convergence
Remark 4.11 (Barycentric projection). Consider again the setting of Remark 4.1 in
which we use entropic regularization to approximate OT between discrete measures.
The Kantorovich formulation in (2.11) and its entropic regularization (4.2) both
yield a coupling P ∈U(a, b). In order to deﬁne a transportation map T : X →Y,
in the case where Y = Rd, one can deﬁne the so-called barycentric projection map
T : xi ∈X 7−→1
Pi,jyj ∈Y,
where the input measures are discrete of the form (2.3). Note that this map is only
deﬁned for points (xi)i in the support of α. In the case where T is a permutation
matrix (as detailed in Proposition 2.1), then T is equal to a Monge map, and as
ε →0, the barycentric projection progressively converges to that map if it is unique.
For arbitrary (not necessarily discrete) measures, solving (2.15) or its regularized
version (4.9) deﬁnes a coupling π ∈U(α, β). Note that this coupling π always has
dα(x)dβ(y) with respect to α ⊗β. A map can thus be retrieved by the
T : x ∈X 7−→
dα(x)dβ(y)dβ(y).
In the case where, for ε = 0, π is supported on the graph of the Monge map
(see Remark 2.24), then using ε > 0 produces a smooth approximation of this
map. Such a barycentric projection is useful to apply the OT Monge map to solve
problems in imaging; see Figure 9.6 for an application to color modiﬁcation. It has
also been used to compute approximations of principal geodesics in the space of
probability measures endowed with the Wasserstein metric; see [Seguy and Cuturi,
Remark 4.12 (Hilbert metric). As initially explained by [Franklin and Lorenz,
1989], the global convergence analysis of Sinkhorn is greatly simpliﬁed using the
Hilbert projective metric on Rn
+,∗(positive vectors), deﬁned as
∀(u, u′) ∈(Rn
It can be shown to be a distance on the projective cone Rn
+,∗/ ∼, where u ∼u′
means that ∃r > 0, u = ru′ (the vectors are equal up to rescaling, hence the
name “projective”). This means that dH satisﬁes the triangular inequality and
dH(u, u′) = 0 if and only if u ∼u′. This is a projective version of Hilbert’s original
distance on bounded open convex sets [Hilbert, 1895]. The projective cone Rn
is a complete metric space for this distance. By a logarithmic change of variables,
Entropic Regularization of Optimal Transport
the Hilbert metric on the rays of the positive cone is isometric to the variation
seminorm (it is a norm between vectors that are deﬁned up to an additive constant)
dH(u, u′) =
log(u) −log(u′)
This variation seminorm is closely related to the ℓ∞norm since one always has
∥f∥var ≤2 ∥f∥∞. If one imposes that fi = 0 for some ﬁxed i, then a converse inequality also holds since ∥f∥∞≤∥f∥var. These bounds are especially useful to analyze
Sinkhorn convergence (see Remark 4.14 below), because dual variables f = log(u)
solving (4.14) are deﬁned up to an additive constant, so that one can impose that
fi = 0 for some i. The Hilbert metric was introduced independently by [Birkhoﬀ,
1957] and [Samelson et al., 1957]. They proved the following fundamental theorem,
which shows that a positive matrix is a strict contraction on the cone of positive
Theorem 4.1. Let K ∈Rn×m
+,∗; then for (v, v′) ∈(Rm
dH(Kv, Kv′) ≤λ(K)dH(v, v′), where
η(K)+1 < 1,
Figure 4.7 illustrates this theorem.
u = {ru ; r > 0}
Figure 4.7: Left: the Hilbert metric dH is a distance over rays in cones (here positive vectors). Right:
visualization of the contraction induced by the iteration of a positive matrix K.
Remark 4.13 (Perron–Frobenius). A typical application of Theorem 4.1 is to provide a quantitative proof of the Perron–Frobenius theorem, which, as explained
in Remark 4.15, is linked to a local linearization of Sinkhorn’s iterates. A matrix
with K⊤1n = 1n maps Σn into Σn. If furthermore K > 0, then according to Theorem 4.1, it is strictly contractant for the metric dH, hence there exists
a unique invariant probability distribution p⋆∈Σn with Kp⋆= p⋆. Furthermore,
4.2. Sinkhorn’s Algorithm and Its Convergence
for any p0 ∈Σn, dH(Kℓp0, p⋆) ≤λ(K)ℓdH(p0, p⋆), i.e. one has linear convergence
of the iterates of the matrix toward p⋆. This is illustrated in Figure 4.8.
Figure 4.8:
Evolution of KℓΣ3 →{p⋆} the invariant probability distribution of K ∈R3×3
K⊤13 = 13.
Remark 4.14 (Global convergence). The following theorem, proved by [Franklin
and Lorenz, 1989], makes use of Theorem 4.1 to show the linear convergence of
Sinkhorn’s iterations.
Theorem 4.2. One has (u(ℓ), v(ℓ)) →(u⋆, v⋆) and
dH(u(ℓ), u⋆) = O(λ(K)2ℓ),
dH(v(ℓ), v⋆) = O(λ(K)2ℓ).
One also has
dH(u(ℓ), u⋆) ≤dH(P(ℓ)1m, a)
dH(v(ℓ), v⋆) ≤dH(P(ℓ),⊤1n, b)
where we denoted P(ℓ) def.
= diag(u(ℓ))K diag(v(ℓ)). Last, one has
∥log(P(ℓ)) −log(P⋆)∥∞≤dH(u(ℓ), u⋆) + dH(v(ℓ), v⋆),
where P⋆is the unique solution of (4.2).
Proof. One notices that for any (v, v′) ∈(Rm
+,∗)2, one has
dH(v, v′) = dH(v/v′, 1m) = dH(1m/v, 1m/v′).
Entropic Regularization of Optimal Transport
This shows that
dH(u(ℓ+1), u⋆) = dH
= dH(Kv(ℓ), Kv⋆) ≤λ(K)dH(v(ℓ), v⋆),
where we used Theorem 4.1. This shows (4.22). One also has, using the triangular
inequality,
dH(u(ℓ), u⋆) ≤dH(u(ℓ+1), u(ℓ)) + dH(u(ℓ+1), u⋆)
Kv(ℓ) , u(ℓ)
+ λ(K)2dH(u(ℓ), u⋆)
a, u(ℓ) ⊙(Kv(ℓ))
+ λ(K)2dH(u(ℓ), u⋆),
which gives the ﬁrst part of (4.23) since u(ℓ) ⊙(Kv(ℓ)) = P(ℓ)1m (the second one
being similar). The proof of (4.24) follows from [Franklin and Lorenz, 1989, Lem.
The bound (4.23) shows that some error measures on the marginal constraints
violation, for instance, ∥P(ℓ)1m −a∥1 and ∥P(ℓ)T1n −b∥1, are useful stopping
criteria to monitor the convergence. Note that thanks to (4.21), these Hilbert metric
rates on the scaling variable (u(ℓ), v(ℓ)) give a linear rate on the dual variables
(f(ℓ), g(ℓ))
= (ε log(u(ℓ)), ε log(v(ℓ))) for the variation norm ∥·∥var.
Figure 4.5, bottom row, highlights this linear rate on the constraint violation
and shows how this rate degrades as ε →0. These results are proved in [Franklin
and Lorenz, 1989] and are tightly connected to nonlinear Perron–Frobenius theory [Lemmens and Nussbaum, 2012]. Perron–Frobenius theory corresponds to the
linearization of the iterations; see (4.25). This convergence analysis is extended
by [Linial et al., 1998], who show that each iteration of Sinkhorn increases the
permanence of the scaled coupling matrix.
Remark 4.15 (Local convergence). The global linear rate (4.24) is often quite pessimistic, typically in X = Y = Rd for cases where there exists a Monge map when
ε = 0 (see Remark 2.7). The global rate is in contrast rather sharp for more dif-
ﬁcult situations where the cost matrix C is close to being random, and in these
cases, the rate scales exponentially bad with ε, 1 −λ(K) ∼e−1/ε. To obtain a
ﬁner asymptotic analysis of the convergence (e.g. if one is interested in a highprecision solution and performs a large number of iterations), one usually rather
studies the local convergence rate. One can write a Sinkhorn update as iterations
4.3. Speeding Up Sinkhorn’s Iterations
of a ﬁxed-point map f(ℓ+1) = Φ(f(ℓ)), where
Φ1(f) = ε log KT(ef/ε) −log(b),
Φ2(g) = ε log K(eg/ε) −log(a).
For optimal (f, g) solving (4.30), denoting P = diag(ef/ε)K diag(eg/ε) the optimal
coupling solving (4.2), one has the following Jacobian:
∂Φ(f) = diag(a)−1 ⊙P ⊙diag(b)−1 ⊙PT.
This Jacobian is a positive matrix with ∂Φ(f)1n = 1n, and thus by the Perron–
Frobenius theorem, it has a single dominant eigenvector 1m with associated eigenvalue 1. Since f is deﬁned up to a constant, it is actually the second eigenvalue
1 −κ < 1 which governs the local linear rate, and this shows that for ℓlarge
∥f(ℓ) −f∥= O((1 −κ)ℓ).
Numerically, in “simple cases” (such as when there exists a smooth Monge map
when ε = 0), this rate scales like κ ∼ε. We refer to [Knight, 2008] for more details
in the bistochastic (assignment) case.
Speeding Up Sinkhorn’s Iterations
The main computational bottleneck of Sinkhorn’s iterations is the vector-matrix multiplication against kernels K and K⊤, with complexity O(nm) if implemented naively. We
now detail several important cases where the complexity can be improved signiﬁcantly.
Remark 4.16 (Parallel and GPU friendly computation). The simplicity of Sinkhorn’s
algorithm yields an extremely eﬃcient approach to compute simultaneously several
regularized Wasserstein distances between pairs of histograms. Let N be an integer,
a1, . . . , aN be histograms in Σn, and b1, . . . , bN be histograms in Σm. We seek to
compute all N approximate distances Lε
C(a1, b1), . . . , Lε
C(aN, bN). In that case, writing
A = [a1, . . . , aN] and B = [b1, . . . , bN] for the n × N and m × N matrices storing all
histograms, one can notice that all Sinkhorn iterations for all these N pairs can be
carried out in parallel, by setting, for instance,
U(ℓ+1) def.
V(ℓ+1) def.
KTU(ℓ+1) ,
initialized with V(0) = 1m×N. Here ·
· corresponds to the entrywise division of matrices. One can further check that upon convergence of V and U, the (row) vector of
regularized distances simpliﬁes to
1nT(U ⊙log U ⊙((K ⊙C)V) + U ⊙((K ⊙C)(V ⊙log V))) ∈RN.
Entropic Regularization of Optimal Transport
Note that the basic Sinkhorn iterations described in Equation (4.15) are intrinsically
GPU friendly, since they only consist in matrix-vector products, and this was exploited,
for instance, to solve matching problems in Slomp et al. ). However, the matrixmatrix operations presented in Equation (4.26) present even better opportunities for
parallelism, which explains the success of Sinkhorn’s algorithm to compute OT distances
between histograms at large scale.
Remark 4.17 (Speed-up for separable kernels). We consider in this section an important
particular case for which the complexity of each Sinkhorn iteration can be signiﬁcantly
reduced. That particular case happens when each index i and j considered in the costmatrix can be described as a d-uple taken in the cartesian product of d ﬁnite sets
Jn1K, . . . , JndK,
k=1, j = (jk)d
k=1 ∈Jn1K × · · · × JndK.
In that setting, if the cost Cij between indices i and j is additive along these sub-indices,
namely if there exists d matrices C1, . . . , Cd, each of respective size n1 ×n, . . . , nd ×nd,
then one obtains as a direct consequence that the kernel appearing in the Sinkhorn
iterations has a separable multiplicative structure,
Such a separable multiplicative structure allows for a very fast (exact) evaluation of
Ku. Indeed, instead of instantiating K as a matrix of size n × n, which would have
a prohibitive size since n = Q
k nk is usually exponential in the dimension d, one can
instead recover Ku by simply applying Kk along each “slice” of u. If n = m, the
complexity reduces to O(n1+1/d) in place of O(n2).
An important example of this speed-up arises when X = Y = d; the ground
cost is the q-th power of the q-norm,
c(x, y) = ∥x −y∥q
|xi −yi|q, q > 0;
and the space is discretized using a regular grid in which only points xi
(i1/n1, . . . , id/nd) for i = (i1, . . . , id) ∈Jn1K × · · · × JndK are considered. In that case a
multiplication by K can be carried out more eﬃciently by applying each 1-D nk × nk
convolution matrix
4.3. Speeding Up Sinkhorn’s Iterations
to u reshaped as a tensor whose ﬁrst dimension has been permuted to match the k-th
set of indices. For instance, if d = 2 (planar case) and q = 2 (2-Wasserstein, resulting in
Gaussian convolutions), histograms a and as a consequence Sinkhorn multipliers u can
be instantiated as n1×n2 matrices. We write U to underline the fact that the multiplier
u is reshaped as a n1×n2 matrix, rather than a vector of length n1n2. Then, computing
Ku, which would naively require (n1n2)2 operations with a naive implementation, can
be obtained by applying two 1-D convolutions separately, as
(K2(K1U)T )T = K1UK2,
to recover a n1 × n2 matrix in (n2
1)n2 + n1(n2
2) operations instead of n2
2 operations.
Note that this example agrees with the exponent (1 + 1/d) given above. With larger d,
one needs to apply these very same 1-D convolutions to each slice of u (reshaped as a
tensor of suitable size) an operation which is extremely eﬃcient on GPUs.
This important observations underlies many of the practical successes found when
applying optimal transport to shape data in 2-D and 3-D, as highlighted in [Solomon
et al., 2015, Bonneel et al., 2016], in which distributions supported on grids of sizes as
large as 2003 = 8 × 106 are handled.
Remark 4.18 (Approximated convolutions). The main computational bottleneck of
Sinkhorn’s iterations (4.15) lies in the multiplication of a vector by K or by its adjoint. Besides using separability (4.27), it is also possible to exploit other special structures in the kernel. The simplest case is for translation invariant kernels Ki,j = ki−j,
which is typically the case when discretizing the measure on a ﬁxed uniform grid in
Euclidean space X = Rd. Then Kv = k ⋆v is a convolution, and there are several
algorithms to approximate the convolution in nearly linear time. The most usual one
is by Fourier transform F, assuming for simplicity periodic boundary conditions, because F(k ⋆v) = F(k) ⊙F(v). This leads, however, to unstable computations and is
often unacceptable for small ε. Another popular way to speed up computation is by
approximating the convolution using a succession of autoregressive ﬁlters, using, for
instance, the Deriche ﬁltering method Deriche . We refer to [Getreuer, 2013] for
a comparison of various fast ﬁltering methods.
Remark 4.19 (Geodesic in heat approximation). For nonplanar domains, the kernel K
is not a convolution, but in the case where the cost is Ci,j = dM(xi, yj)p where dM
is a geodesic distance on a surface M (or a more general manifold), it is also possible
to perform fast approximations of the application of K = e−dM
to a vector. Indeed,
Varadhan’s formulas assert that this kernel is close to the Laplacian kernel (for
p = 1) and the heat kernel (for p = 2). The ﬁrst formula of Varadhan states
2 log(Pt(x, y)) = dM(x, y) + o(t)
= (Id −t∆M)−1,
Entropic Regularization of Optimal Transport
where ∆M is the Laplace–Beltrami operator associated to the manifold M (which is
negative semideﬁnite), so that Pt is an integral kernel and g =
M Pt(x, y)f(y)dy is the
solution of g −t∆Mg = f. The second formula of Varadhan states
−4t log(Ht(x, y)) = dM(x, y) + o(t),
where Ht is the integral kernel deﬁned so that gt =
M Ht(x, y)f(y)dy is the solution
at time t of the heat equation
= (∆Mgt)(x).
The convergence in these formulas (4.28) and (4.29) is uniform on compact manifolds.
Numerically, the domain M is discretized (for instance, using ﬁnite elements) and
∆M is approximated by a discrete Laplacian matrix L. A typical example is when
using piecewise linear ﬁnite elements, so that L is the celebrated cotangent Laplacian
 . These formulas
can be used to approximate eﬃciently the multiplication by the Gibbs kernel Ki,j =
. Equation (4.28) suggests, for the case p = 1, to use ε =
2 and to replace
the multiplication by K by the multiplication by (Id −tL)−1, which necessitates the
resolution of a positive symmetric linear system. Equation (4.29), coupled with R steps
of implicit Euler for the stable resolution of the heat ﬂow, suggests for p = 2 to trade
the multiplication by K by the multiplication by (Id−t
RL)−R for 4t = ε, which in turn
necessitates R resolutions of linear systems. Fortunately, since these linear systems
are supposed to be solved at each Sinkhorn iteration, one can solve them eﬃciently
by precomputing a sparse Cholesky factorization. By performing a reordering of the
rows and columns of the matrix [George and Liu, 1989], one obtains a nearly linear
sparsity for 2-D manifolds and thus each Sinkhorn iteration has linear complexity (the
performance degrades with the dimension of the manifold). The use of Varadhan’s
formula to approximate geodesic distances was initially proposed in [Crane et al., 2013]
and its use in conjunction with Sinkhorn iterations in [Solomon et al., 2015].
Remark 4.20 (Extrapolation acceleration). Since the Sinkhorn algorithm is a ﬁxed-point
algorithm (as shown in Remark 4.15), one can use standard linear or even nonlinear
extrapolation schemes to enhance the conditioning of the ﬁxed-point mapping near
the solution, and improve the linear convergence rate. This is similar to the successive
overrelaxation method , so that the local linear
rate of convergence is improved from O((1 −κ)ℓ) to O((1 −√κ)ℓ) for some κ > 0 (see
Remark 4.15). We refer to [Peyré et al., 2017] for more details.
4.4. Stability and Log-Domain Computations
Stability and Log-Domain Computations
As brieﬂy mentioned in Remark 4.7, the Sinkhorn algorithm suﬀers from numerical
overﬂow when the regularization parameter ε is small compared to the entries of the cost
matrix C. This concern can be alleviated to some extent by carrying out computations
in the log domain. The relevance of this approach is made more clear by considering
the dual problem associated to (4.2), in which these log-domain computations arise
naturally.
Proposition 4.4. One has
f∈Rn,g∈Rm ⟨f, a⟩+ ⟨g, b⟩−ε⟨ef/ε, Keg/ε⟩.
The optimal (f, g) are linked to scalings (u, v) appearing in (4.12) through
(u, v) = (ef/ε, eg/ε).
Proof. We start from the end of the proof of Proposition 4.3, which links the optimal
primal solution P and dual multipliers f and g for the marginal constraints as
Pi,j = efi/εe−Ci,j/εegj/ε.
Substituting in the Lagrangian E(P, f, g) of Equation (4.2) the optimal P as a function
of f and g, we obtain that the Lagrange dual function equals
f, g 7→⟨ef/ε, (K ⊙C) eg/ε⟩−εH(diag(ef/ε)K diag(eg/ε)).
The neg-entropy of P scaled by ε, namely ε⟨P, log P −1n×m⟩, can be stated explicitly
as a function of f, g, C,
⟨diag(ef/ε)K diag(eg/ε), f1mT + 1ngT −C −ε1n×m⟩
= −⟨ef/ε, (K ⊙C) eg/ε⟩+ ⟨f, a⟩+ ⟨g, b⟩−ε⟨ef/ε, Keg/ε⟩;
therefore, the ﬁrst term in (4.32) cancels out with the ﬁrst term in the entropy above.
The remaining terms are those appearing in (4.30).
Remark 4.21 (Sinkhorn as a block coordinate ascent on the dual problem). A simple
approach to solving the unconstrained maximization problem (4.30) is to use an exact
block coordinate ascent strategy, namely to update alternatively f and g to cancel the
respective gradients in these variables of the objective of (4.30). Indeed, one can notice
after a few elementary computations that, writing Q(f, g) for the objective of (4.30),
∇|f Q(f, g) = a −ef/ε ⊙
∇|g Q(f, g) = b −eg/ε ⊙
Entropic Regularization of Optimal Transport
Block coordinate ascent can therefore be implemented in a closed form by applying
successively the following updates, starting from any arbitrary g(0), for l ≥0:
f(ℓ+1) = ε log a −ε log
g(ℓ+1) = ε log b −ε log
KTef(ℓ+1)/ε
Such iterations are mathematically equivalent to the Sinkhorn iterations (4.15) when
considering the primal-dual relations highlighted in (4.31). Indeed, we recover that at
any iteration
(f(ℓ), g(ℓ)) = ε(log(u(ℓ)), log(v(ℓ))).
Remark 4.22 (Soft-min rewriting). Iterations (4.35) and (4.36) can be given an alternative interpretation, using the following notation. Given a vector z of real numbers we
write minε z for the soft-minimum of its coordinates, namely
minε z = −ε log
Note that minε (z) converges to min z for any vector z as ε →0. Indeed, minε can be
interpreted as a diﬀerentiable approximation of the min function, as shown in Figure 4.9.
Figure 4.9: Display of the function minε (z) in 2-D, z ∈R2, for varying ε.
Using this notation, Equations (4.35) and (4.36) can be rewritten
(f(ℓ+1))i = minε (Cij −g(ℓ)
j )j + ε log ai,
(g(ℓ+1))j = minε (Cij −f(ℓ)
i )i + ε log bj.
Here the term minε (Cij −g(ℓ)
j )j denotes the soft-minimum of all values of the jth
column of matrix (C−1n(g(ℓ))⊤). To simplify notations, we introduce an operator that
takes a matrix as input and outputs now a column vector of the soft-minimum values
of its columns or rows. Namely, for any matrix A ∈Rn×m, we deﬁne
minε (Ai,j)j
 minε (Ai,j)i
4.4. Stability and Log-Domain Computations
Note that these operations are equivalent to the entropic c-transform introduced in §5.3
(see in particular (5.11)). Using this notation, Sinkhorn’s iterates read
f(ℓ+1) = Minrow
(C −1ng(ℓ)T) + ε log a,
g(ℓ+1) = Mincol
(C −f(ℓ)1mT) + ε log b.
Note that as ε →0, minε converges to min, but the iterations do not converge anymore
in the limit ε = 0, because alternate minimization does not converge for constrained
problems, which is the case for the unregularized dual (2.20).
Remark 4.23 (Log-domain Sinkhorn). While mathematically equivalent to the Sinkhorn
updates (4.15), iterations (4.38) and (4.39) suggest using the log-sum-exp stabilization
trick to avoid underﬂow for small values of ε. Writing z = min z, that trick suggests
evaluating minε z as
minε z = z −ε log
e−(zi−z)/ε.
Instead of substracting z to stabilize the log-domain iterations as in (4.42), one can actually substract the previously computed scalings. This leads to the stabilized iteration
f(ℓ+1) = Minrow
(S(f(ℓ), g(ℓ))) + f(ℓ) + ε log(a),
g(ℓ+1) = Mincol
(S(f(ℓ+1), g(ℓ))) + g(ℓ) + ε log(b),
where we deﬁned
Ci,j −fi −gj
In contrast to the original iterations (4.15), these log-domain iterations (4.43) and (4.44)
are stable for arbitrary ε > 0, because the quantity S(f, g) stays bounded during the
iterations. The downside is that it requires nm computations of exp at each step. Computing a Minrow
is typically substantially slower than matrix multiplications
and requires computing line by line soft-minima of matrices S. There is therefore no
eﬃcient way to parallelize the application of Sinkhorn maps for several marginals simultaneously. In Euclidean domains of small dimension, it is possible to develop eﬃcient
multiscale solvers with a decaying ε strategy to signiﬁcantly speed up the computation
using sparse grids [Schmitzer, 2016b].
Remark 4.24 (Dual for generic measures). For generic and not necessarily discrete
input measures (α, β), the dual problem (4.30) reads
(f,g)∈C(X)×C(Y)
−c(x,y)+f(x)+g(y)
dα(x)dβ(y).
This corresponds to a smoothing of the constraint R(c) appearing in the original
problem (2.24), which is retrieved in the limit ε →0. Proving existence (i.e. the
Entropic Regularization of Optimal Transport
sup is actually a max) of these Kantorovich potentials (f, g) in the case of entropic
transport is less easy than for classical OT, because one cannot use the c-transform
and potentials are not automatically Lipschitz. Proof of existence can be done using
the convergence of Sinkhorn iterations; see [Chizat et al., 2018b] for more details.
Remark 4.25 (Unconstrained entropic dual). As
Y dβ = 1, one can consider an alternative dual formulation
(f,g)∈C(X)×C(Y)
gdβ + minε (c −f ⊕g),
which achieves the same optimal value as (4.45). Similarly to (4.37), the softminimum (here on X × Y) is deﬁned as
∀S ∈C(X × Y),
dα(x)dβ(y)
(note that it depends on (α, β)). As ε →0, minε →min, as used in the unregularized and unconstrained formulation (2.27). Note that while both (4.45) and (4.46)
are unconstrained problems, a chief advantage of (4.46) is that it is better conditioned, in the sense that the Hessian of the functional is uniformly bounded by ε.
Another way to obtain such a conditioning improvement is to consider semidual
problems; see §5.3 and in particular Remark 5.1. A disadvantage of this alternative dual formulation is that the presence of a log prevents the use of stochastic
optimization methods as detailed in §5.4; see in particular Remark 5.3.
Regularized Approximations of the Optimal Transport Cost
The entropic dual (4.30) is a smooth unconstrained concave maximization problem,
which approximates the original Kantorovich dual (2.20), as detailed in the following
proposition.
Proposition 4.5. Any pair of optimal solutions (f⋆, g⋆) to (4.30) are such that (f⋆, g⋆) ∈
R(C), the set of feasible Kantorovich potentials deﬁned in (2.21). As a consequence,
we have that for any ε,
⟨f⋆, a⟩+ ⟨g⋆, b⟩≤LC(a, b).
Proof. Primal-dual optimality conditions in (4.4) with the constraint that P is a probability and therefore Pi,j ≤1 for all i, j yields that exp(−(f⋆
j −Ci,j)/ε) ≤1 and
therefore that f⋆
A chief advantage of the regularized transportation cost Lε
C deﬁned in (4.2) is that
it is smooth and convex, which makes it a perfect ﬁt for integrating as a loss function
4.5. Regularized Approximations of the Optimal Transport Cost
in variational problems (see Chapter 9).
Proposition 4.6. Lε
C(a, b) is a jointly convex function of a and b for ε ≥0. When
ε > 0, its gradient is equal to
where f⋆and g⋆are the optimal solutions of Equation (4.30) chosen so that their
coordinates sum to 0.
In [Cuturi, 2013], lower and upper bounds to approximate the Wasserstein distance
between two histograms were proposed. These bounds consist in evaluating the primal
and dual objectives at the solutions provided by the Sinkhorn algorithm.
Deﬁnition 4.1 (Sinkhorn divergences). Let f⋆and g⋆be optimal solutions to (4.30)
and P⋆be the solution to (4.2). The Wasserstein distance is approximated using the
following primal and dual Sinkhorn divergences:
= ⟨C, P⋆⟩= ⟨e
ε , (K ⊙C)e
= ⟨f⋆, a⟩+ ⟨g⋆, b⟩,
where ⊙stands for the elementwise product of matrices,
Proposition 4.7. The following relationship holds:
C(a, b) ≤Lε
C(a, b) ≤Pε
Furthermore
C(a, b) −Dε
C(a, b) = ε(H(P⋆) + 1).
Proof. Equation (4.47) is obtained by writing that the primal and dual problems have
the same values at the optima (see (4.30)), and hence
C(a, b) = Pε
C(a, b) −εH(P⋆) = Dε
C(a, b) −ε⟨ef⋆/ε, Keg⋆/ε⟩
The ﬁnal result can be obtained by remarking that ⟨ef⋆/ε, Keg⋆/ε⟩= 1, since the latter
amounts to computing the sum of all entries of P⋆.
The relationships given above suggest a practical way to bound the actual OT
distance, but they are, in fact, valid only upon convergence of the Sinkhorn algorithm
and therefore never truly useful in practice. Indeed, in practice Sinkhorn iterations are
always terminated after a certain accuracy threshold is reached. When a predetermined
number of L iterations is set and used to evaluate Dε
C using iterates f(L) and g(L) instead
of optimal solutions f⋆and g⋆, one recovers, however, a lower bound: Using notation
Entropic Regularization of Optimal Transport
appearing in Equations (4.43) and (4.44), we thus introduce the following ﬁnite step
approximation of Lε
= ⟨f(L), a⟩+ ⟨g(L), b⟩.
This “algorithmic” Sinkhorn functional lower bounds the regularized cost function as
soon as L ≥1.
Proposition 4.8 (Finite Sinkhorn divergences). The following relationship holds:
C (a, b) ≤Lε
Proof. Similarly to the proof of Proposition 4.5, we exploit the fact that after even just
one single Sinkhorn iteration, we have, following (4.35) and (4.36), that f(L) and g(L)
are such that the matrix with elements exp(−(f(L)
−Ci,j)/ε) has column sum
b and its elements are therefore each upper bounded by 1, which results in the dual
feasibility of (f(L)
Remark 4.26 (Primal infeasibility of the Sinkhorn iterates). Note that the primal iterates
provided in (4.8) are not primal feasible, since, by deﬁnition, these iterates are designed
to satisfy upon convergence marginal constraints. Therefore, it is not valid to consider
⟨C, P(2L+1)⟩as an approximation of LC(a, b) since P(2L+1) is not feasible. Using the
rounding scheme of Altschuler et al. laid out in Remark 4.6 one can, however,
yield an upper bound on Lε
C(a, b) that can, in addition, be conveniently computed
using matrix operations in parallel for several pairs of histograms, in the same fashion
as Sinkhorn’s algorithm [Lacombe et al., 2018].
Remark 4.27 (Nonconvexity of ﬁnite dual Sinkhorn divergence). Unlike the regularized
expression Lε
C in (4.30), the ﬁnite Sinkhorn divergence D(L)
C (a, b) is not, in general, a
convex function of its arguments (this can be easily checked numerically). D(L)
is, however, a diﬀerentiable function which can be diﬀerentiated using automatic diﬀerentiation techniques (see Remark 9.1.3) with respect to any of its arguments, notably
C, a, or b.
Generalized Sinkhorn
The regularized OT problem (4.2) is a special case of a structured convex optimization
problem of the form
Ci,jPi,j −εH(P) + F(P1m) + G(PT1n).
Indeed, deﬁning F = ι{a} and G = ι{b}, where the indicator function of a closed convex
otherwise,
4.6. Generalized Sinkhorn
one retrieves the hard marginal constraints deﬁning U(a, b). The proof of Proposition 4.3 carries to this more general problem (4.49), so that the unique solution of (4.49)
also has the form (4.12).
As shown in [Peyré, 2015, Frogner et al., 2015, Chizat et al., 2018b, Karlsson and
Ringh, 2016], Sinkhorn iterations (4.15) can hence be extended to this problem, and
where the proximal operator for the KL divergence is
F (u) = argmin
KL(u′|u) + F(u′).
For some functions F, G it is possible to prove the linear rate of convergence for iterations (4.51), and these schemes can be generalized to arbitrary measures; see [Chizat
et al., 2018b] for more details.
Iterations (4.51) are thus interesting in the cases where ProxKL
and ProxKL
computed in closed form or very eﬃciently. This is in particular the case for separable
functions of the form F(u) = P
i Fi(ui) since in this case
Computing each ProxKL
is usually simple since it is a scalar optimization problem.
Note that, similarly to the initial Sinkhorn algorithm, it is also possible to stabilize the
computation using log-domain computations [Chizat et al., 2018b].
This algorithm can be used to approximate the solution to various generalizations
of OT, and in particular unbalanced OT problems of the form (10.7) (see §10.2 and in
particular iterations (10.9)) and gradient ﬂow problems of the form (9.26) (see §9.3).
Remark 4.28 (Duality and Legendre transform). The dual problem to (4.49) reads
−F ∗(f) −G∗(g) −ε
fi+gj−Ci,j
so that (u, v) = (ef/ε, eg/ε) are the associated scalings appearing in (4.12). Here,
F ∗and G∗are the Fenchel–Legendre conjugate, which are convex functions deﬁned
a∈Rn ⟨f, a⟩−F(a).
The generalized Sinkhorn iterates (4.51) are a special case of Dykstra’s algorithm [Dykstra, 1983, 1985] and is an alternate maximization scheme on the dual problem (4.53).
The formulation (4.49) can be further generalized to more than two functions and
more than a single coupling; we refer to [Chizat et al., 2018b] for more details. This
includes as a particular case the Sinkhorn algorithm (10.2) for the multimarginal problem, as detailed in §10.1. It is also possible to rewrite the regularized barycenter problem (9.15) this way, and the iterations (9.18) are in fact a special case of this generalized
Semidiscrete Optimal Transport
This chapter studies methods to tackle the optimal transport problem when one of the
two input measures is discrete (a sum of Dirac masses) and the other one is arbitrary,
including notably the case where it has a density with respect to the Lebesgue measure.
When the ambient space has low dimension, this problem has a strong geometrical
ﬂavor because one can show that the optimal transport from a continuous density
toward a discrete one is a piecewise constant map, where the preimage of each point
in the support of the discrete measure is a union of disjoint cells. When the cost is
the squared Euclidean distance, these cells correspond to an important concept from
computational geometry, the so-called Laguerre cells, which are Voronoi cells oﬀset by
a constant. This connection allows us to borrow tools from computational geometry to
obtain fast computational schemes. In high dimensions, the semidescrete formulation
can also be interpreted as a stochastic programming problem, which can also beneﬁt
from a bit of regularization, extending therefore the scope of applications of the entropic
regularization scheme presented in Chapter 4. All these constructions rely heavily on
the notion of the c-transform, this time for general cost functions and not only matrices
as in §3.2. The c-transform is a generalization of the Legendre transform from convex
analysis and plays a pivotal role in the theory and algorithms for OT.
c-Transform and ¯c-Transform
Recall that the dual OT problem (2.24) reads
f(x)dα(x) +
g(y)dβ(y) + ιR(c)(f, g),
Semidiscrete Optimal Transport
where we used the useful indicator function notation (4.50). Keeping either dual potential f or g ﬁxed and optimizing w.r.t. g or f, respectively, leads to closed form solutions
that provide the deﬁnition of the c-transform:
x∈X c(x, y) −f(x),
y∈Y c(x, y) −g(y),
where we denoted ¯c(y, x)
= c(x, y). Indeed, one can check that
fc ∈argmax
g¯c ∈argmax
Note that these partial minimizations deﬁne maximizers on the support of respectively
α and β, while the deﬁnitions (5.1) actually deﬁne functions on the whole spaces X
and Y. This is thus a way to extend in a canonical way solutions of (2.24) on the
whole spaces. When X = Rd and c(x, y) = ∥x −y∥p
i=1 |xi −yi|)p/2, then the ctransform (5.1) fc is the so-called inf-convolution between −f and ∥·∥p. The deﬁnition
of fc is also often referred to as a “Hopf–Lax formula.”
The map (f, g) ∈C(X) × C(Y) 7→(g¯c, fc) ∈C(X) × C(Y) replaces dual potentials
by “better” ones (improving the dual objective E). Functions that can be written in
the form fc and g¯c are called c-concave and ¯c-concave functions. In the special case
c(x, y) = ⟨x, y⟩in X = Y = Rd, this deﬁnition coincides with the usual notion of
concave functions. Extending naturally Proposition 3.1 to a continuous case, one has
the property that
fc¯cc = fc
g¯cc¯c = g¯c,
where we denoted fc¯c = (fc)¯c. This invariance property shows that one can “improve”
only once the dual potential this way. Alternatively, this means that alternate maximization does not converge (it immediately enters a cycle), which is classical for functionals
involving a nonsmooth (a constraint) coupling of the optimized variables. This is in
sharp contrast with entropic regularization of OT as shown in Chapter 4. In this case,
because of the regularization, the dual objective (4.30) is smooth, and alternate maximization corresponds to Sinkhorn iterations (4.43) and (4.44). These iterates, written
over the dual variables, deﬁne entropically smoothed versions of the c-transform, where
min operations are replaced by a “soft-min.”
Using (5.3), one can reformulate (2.24) as an unconstrained convex program over a
single potential,
Lc(α, β) =
f(x)dα(x) +
fc(y)dβ(y)
g¯c(x)dα(x) +
g(y)dβ(y).
5.2. Semidiscrete Formulation
Since one can iterate the map (f, g) 7→(g¯c, fc), it is possible to add the constraint that
f is ¯c-concave and g is c-concave, which is important to ensure enough regularity on
these potentials and show, for instance, existence of solutions to (2.24).
Semidiscrete Formulation
A case of particular interest is when β = P
j bjδyj is discrete (of course the same
construction applies if α is discrete by exchanging the role of α, β). One can adapt the
deﬁnition of the ¯c transform (5.1) to this setting by restricting the minimization to the
support (yj)j of β,
∀g ∈Rm, ∀x ∈X,
j∈JmK c(x, yj) −gj.
This transform maps a vector g to a continuous function g¯c ∈C(X). Note that this
deﬁnition coincides with (5.1) when imposing that the space X is equal to the support
of β. Figure 5.1 shows some examples of such discrete ¯c-transforms in one and two
dimensions.
Crucially, using the discrete ¯c-transform in the semidiscrete problem (5.4) yields a
ﬁnite-dimensional optimization,
Lc(α, β) = max
g¯c(x)dα(x) +
The Laguerre cells associated to the dual weights g
x ∈X : ∀j′ ̸= j, c(x, yj) −gj ≤c(x, yj′) −gj′
induce a disjoint decomposition of X = S
j Lj(g). When g is constant, the Laguerre cells
decomposition corresponds to the Voronoi diagram partition of the space. Figure 5.1,
bottom row, shows examples of Laguerre cells segmentations in two dimensions.
This allows one to conveniently rewrite the minimized energy as
c(x, yj) −gj
dα(x) + ⟨g, b⟩.
The gradient of this function can be computed as follows:
∇E(g)j = −
dα(x) + bj.
Figure 5.2 displays iterations of a gradient descent to minimize E. Once the optimal g
is computed, then the optimal transport map T from α to β is mapping any x ∈Lj(g)
toward yj, so it is piecewise constant.
In the special case c(x, y) = ∥x −y∥2, the decomposition in Laguerre cells is also
known as a “power diagram.” The cells are polyhedral and can be computed eﬃciently
Semidiscrete Optimal Transport
Figure 5.1: Top: examples of semidiscrete ¯c-transforms g¯c in one dimension, for ground cost c(x, y) =
|x −y|p for varying p (see colorbar). The red points are at locations (yj, −gj)j. Bottom: examples of
semidiscrete ¯c-transforms g¯c in two dimensions, for ground cost c(x, y) = ∥x −y∥p
i=1 |xi −yi|)p/2
for varying p. The red points are at locations yj ∈R2, and their size is proportional to gj. The regions
delimited by bold black curves are the Laguerre cells (Lj(g))j associated to these points (yj)j.
using computational geometry algorithms; see [Aurenhammer, 1987]. The most widely
used algorithm relies on the fact that the power diagram of points in Rd is equal to the
projection on Rd of the convex hull of the set of points ((yj, ∥yj∥2 −gj))m
j=1 ⊂Rd+1.
There are numerous algorithms to compute convex hulls; for instance, that of Chan
 in two and three dimensions has complexity O(m log(Q)), where Q is the number
of vertices of the convex hull.
The initial idea of a semidiscrete solver for Monge–Ampère equations was proposed
by Oliker and Prussner , and its relation to the dual variational problem was
shown by Aurenhammer et al. . A theoretical analysis and its application to the
reﬂector problem in optics is detailed in [Caﬀarelli et al., 1999]. The semidiscrete formulation was used in [Carlier et al., 2010] in conjunction with a continuation approach
based on Knothe’s transport. The recent revival of this methods in various ﬁelds is due
to Mérigot , who proposed a quasi-Newton solver and clariﬁed the link with concepts from computational geometry. We refer to [Lévy and Schwindt, 2018] for a recent
5.3. Entropic Semidiscrete Formulation
Figure 5.2:
Iterations of the semidiscrete OT algorithm minimizing (5.8) (here a simple gradient
descent is used). The support (yj)j of the discrete measure β is indicated by the colored points, while
the continuous measure α is the uniform measure on a square. The colored cells display the Laguerre
partition (Lj(g(ℓ)))j where g(ℓ) is the discrete dual potential computed at iteration ℓ.
overview. The use of a Newton solver which is applied to sampling in computer graphics
is proposed in [De Goes et al., 2012]; see also [Lévy, 2015] for applications to 3-D volume
and surface processing. An important area of application of the semidiscrete method
is for the resolution of the incompressible ﬂuid dynamic (Euler’s equations) using Lagrangian methods [de Goes et al., 2015, Gallouët and Mérigot, 2017]. The semidiscrete
OT solver enforces incompressibility at each iteration by imposing that the (possibly
weighted) points cloud approximates a uniform distribution inside the domain. The
convergence (with linear rate) of damped Newton iterations is proved in [Mirebeau,
2015] for the Monge–Ampère equation and is reﬁned in [Kitagawa et al., 2016] for optimal transport. Semidiscrete OT ﬁnds important applications to illumination design,
notably reﬂectors; see [Meyron et al., 2018].
Entropic Semidiscrete Formulation
The dual of the entropic regularized problem between arbitrary measures (4.9) is a
smooth unconstrained optimization problem:
(f,g)∈C(X)×C(Y)
where we denoted (f ⊕g)(x, y)
= f(x) + g(y).
Similarly to the unregularized problem (5.1), one can minimize explicitly with respect to either f or g in (5.9), which yields a smoothed c-transform
−c(x,y)+f(x)
−c(x,y)+g(y)
In the case of a discrete measure β = Pm
j=1 bjδyj, the problem simpliﬁes as with (5.7)
to a ﬁnite-dimensional problem expressed as a function of the discrete dual potential
Semidiscrete Optimal Transport
−c(x,yj)+gj
One deﬁnes similarly f¯c,ε in the case of a discrete measure α. Note that the rewriting (4.40) and (4.41) of Sinkhorn using the soft-min operator minε corresponds to the
alternate computation of entropic smoothed c-transforms,
= g¯c,ε(xi)
= f c,ε(yj).
Instead of maximizing (5.9), one can thus solve the following ﬁnite-dimensional
optimization problem:
g∈Rn Eε(g)
g¯c,ε(x)dα(x) + ⟨g, b⟩.
Note that this optimization problem is still valid even in the unregularized case ε = 0
and in this case g¯c,ε=0 = g¯c is the ¯c-transform deﬁned in (5.6) so that (5.12) is in
fact (5.8). The gradient of this functional reads
∇Eε(g)j = −
j(x)dα(x) + bj,
j is a smoothed version of the indicator χ0
j of the Laguerre cell Lj(g),
−c(x,yj)+gj
−c(x,yℓ)+gℓ
Note once again that this formula (5.13) is still valid for ε = 0. Note also that the
family of functions (χε
j)j is a partition of unity, i.e. P
j = 1 and χε
j ≥0. Figure 5.3,
bottom row, illustrates this.
Remark 5.1 (Second order methods and connection with logistic regression). A crucial
aspect of the smoothed semidiscrete formulation (5.12) is that it corresponds to the
minimization of a smooth function. Indeed, as shown in [Genevay et al., 2016], the
Hessian of Eε is upper bounded by 1/ε, so that ∇Eε is
ε-Lipschitz continuous. In
fact, that problem is very closely related to a multiclass logistic regression problem
(see Figure 5.3 for a display of the resulting fuzzy classiﬁcation boundary) and enjoys
the same favorable properties , which are generalizations of self-concordance; see [Bach, 2010]. In particular, the Newton method converges
quadratically, and one can use in practice quasi-Newton techniques, such as L-BFGS,
as advocated in [Cuturi and Peyré, 2016]. Note that [Cuturi and Peyré, 2016] studies the more general barycenter problem detailed in §9.2, but it is equivalent to this
semidiscrete setting when considering only a pair of input measures. The use of second
5.3. Entropic Semidiscrete Formulation
order schemes (Newton or L-BFGS) is also advocated in the unregularized case ε = 0
by [Mérigot, 2011, De Goes et al., 2012, Lévy, 2015]. In [Kitagawa et al., 2016, Theo.
5.1], the Hessian of E0(g) is shown to be uniformly bounded as long as the volume
of the Laguerre cells is bounded by below and α has a continuous density. Kitagawa
et al. proceed by showing the linear convergence of a damped Newton algorithm with
a backtracking to ensure that the Laguerre cells never vanish between two iterations.
This result justiﬁes the use of second order methods even in the unregularized case.
The intuition is that, while the conditioning of the entropic regularized problem scales
like 1/ε, when ε = 0, this conditioning is rather driven by m, the number of samples of
the discrete distribution (which controls the size of the Laguerre cells). Other methods
exploiting second order schemes were also recently studied by [Knight and Ruiz, 2013,
Sugiyama et al., 2017, Cohen et al., 2017, Allen-Zhu et al., 2017].
Figure 5.3:
Top: examples of entropic semidiscrete ¯c-transforms g¯c,ε in one dimension, for ground
cost c(x, y) = |x −y| for varying ε (see colorbar). The red points are at locations (yj, −gj)j. Bottom:
examples of entropic semidiscrete ¯c-transforms g¯c,ε in two dimensions, for ground cost c(x, y) = ∥x −y∥2
for varying ε. The black curves are the level sets of the function g¯c,ε, while the colors indicate the
smoothed indicator function of the Laguerre cells χε
j. The red points are at locations yj ∈R2, and their
size is proportional to gj.
Semidiscrete Optimal Transport
Remark 5.2 (Legendre transforms of OT cost functions). As stated in Proposition 4.6,
C(a, b) is a convex function of (a, b) (which is also true in the unregularized case
ε = 0). It is thus possible to compute its Legendre–Fenchel transform, which is deﬁned
in (4.54). Denoting Fa(b) = Lε
C(a, b), one has, for a ﬁxed a, following Cuturi and Peyré
a(g) = −εH(a) +
aig¯c,ε(xi).
Here g¯c,ε is the entropic-smoothed c-transform introduced in (5.10). In the unregularized case ε = 0, and for generic measures, Carlier et al. show, denoting
= Lc(α, β),
g¯c(x)dα(x),
where the ¯c-transform g¯c ∈C(X) of g is deﬁned in §5.1. Note that here, since M(X)
is in duality with C(X), the Legendre transform is a function of continuous functions.
Denoting now G(a, b)
C(a, b), one can derive as in [Cuturi and Peyré, 2016, 2018]
the Legendre transform for both arguments,
∀(f, g) ∈Rn × Rm,
G∗(f, g) = −ε log
−Ci,j+fi+gj
which can be seen as a smoothed version of the Legendre transform of G(α, β)
∀(f, g) ∈C(X) × C(Y),
G∗(f, g) =
(x,y)∈X×Y c(x, y) −f(x) −g(y).
Stochastic Optimization Methods
The semidiscrete formulation (5.8) and its smoothed version (5.12) are appealing because the energies to be minimized are written as an expectation with respect to the
probability distribution α,
Eε(g, x)dα(x) = EX(Eε(g, X))
= g¯c,ε(x) −⟨g, b⟩,
and X denotes a random vector distributed on X according to α. Note that the gradient
of each of the involved functional reads
∇gEε(x, g) = (χε
j(x) −bj)m
One can thus use stochastic optimization methods to perform the maximization, as proposed in Genevay et al. . This allows us to obtain provably convergent algorithms
5.4. Stochastic Optimization Methods
without the need to resort to an arbitrary discretization of α (either approximating α
using sums of Diracs or using quadrature formula for the integrals). The measure α is
used as a black box from which one can draw independent samples, which is a natural
computational setup for many high-dimensional applications in statistics and machine
learning. This class of methods has been generalized to the computation of Wasserstein
barycenters (as described in §9.2) in [Staib et al., 2017b].
Stochastic gradient descent.
Initializing g(0) = 0P , the stochastic gradient descent
algorithm (SGD; used here as a maximization method) draws at step ℓa point xℓ∈X
according to distribution α (independently from all past and future samples (xℓ)ℓ) to
form the update
g(ℓ+1) def.
= g(ℓ) + τℓ∇gEε(g(ℓ), xℓ).
The step size τℓshould decay fast enough to zero in order to ensure that the “noise”
created by using ∇gEε(xℓ, g) as a proxy for the true gradient ∇Eε(g) is canceled in the
limit. A typical choice of schedule is
where ℓ0 indicates roughly the number of iterations serving as a warmup phase. One
can prove the convergence result
Eε(g⋆) −E(Eε(g(ℓ))) = O
where g⋆is a solution of (5.12) and where E indicates an expectation with respect to
the i.i.d. sampling of (xℓ)ℓperformed at each iteration. Figure 5.4 shows the evolution
of the algorithm on a simple 2-D example, where α is the uniform distribution on 2.
Stochastic gradient descent with averaging.
SGD is slow because of the fast decay of
the stepsize τℓtoward zero. To improve the convergence speed, it is possible to average
the past iterates, which is equivalent to running a “classical” SGD on auxiliary variables
˜g(ℓ+1) def.
= ˜g(ℓ) + τℓ∇gEε(˜g(ℓ), xℓ),
where xℓis drawn according to α (and all the (xℓ)ℓare independent) and output as
estimated weight vector the average
This deﬁnes the stochastic gradient descent with averaging (SGA) algorithm. Note that
it is possible to avoid explicitly storing all the iterates by simply updating a running
Semidiscrete Optimal Transport
Figure 5.4:
Evolution of the energy Eε(g(ℓ)), for ε = 0 (no regularization) during the SGD iterations (5.14). Each colored curve shows a diﬀerent randomized run. The images display the evolution of
the Laguerre cells (Lj(g(ℓ)))j through the iterations.
average as follows:
ℓ+ 1˜g(ℓ+1) +
In this case, a typical choice of decay is rather of the form
Notice that the step size now goes much slower to 0 than for (5.15), at rate ℓ−1/2.
Bach proves that SGA leads to a faster convergence (the constants involved are
smaller) than SGD, since in contrast to SGD, SGA is adaptive to the local strong
convexity (or concavity for maximization problems) of the functional.
Remark 5.3 (Continuous-continuous problems). When neither α nor β is a discrete measure, one cannot resort to semidiscrete strategies involving ﬁnite-dimensional dual variables, such as that given in Problem (5.7). The only option is to use stochastic optimization methods on the dual problem (4.45), as proposed in [Genevay et al., 2016].
A suitable regularization of that problem is crucial, for instance by setting an entropic
regularization strength ε > 0, to obtain an unconstrained problem that can be solved by
stochastic descent schemes. A possible approach to revisit Problem (4.45) is to restrict
that inﬁnite-dimensional optimization problem over a space of continuous functions to a
5.4. Stochastic Optimization Methods
much smaller subset, such as that spanned by multilayer neural networks [Seguy et al.,
2018]. This approach leads to nonconvex ﬁnite-dimensional optimization problems with
no approximation guarantees, but this can provide an eﬀective way to compute a proxy
for the Wasserstein distance in high-dimensional scenarios. Another solution is to use
nonparametric families, which is equivalent to considering some sort of progressive re-
ﬁnement, as that proposed by Genevay et al. using reproducing kernel Hilbert
spaces, whose dimension is proportional to the number of iterations of the SGD algorithm.
W1 Optimal Transport
This chapter focuses on optimal transport problems in which the ground cost is equal
to a distance. Historically, this corresponds to the original problem posed by Monge
in 1781; this setting was also that chosen in early applications of optimal transport in
computer vision [Rubner et al., 2000] under the name of “earth mover’s distances”.
Unlike the case where the ground cost is a squared Hilbertian distance (studied
in particular in Chapter 7), transport problems where the cost is a metric are more
diﬃcult to analyze theoretically. In contrast to Remark 2.24 that states the uniqueness
of a transport map or coupling between two absolutely continuous measures when
using a squared metric, the optimal Kantorovich coupling is in general not unique
when the cost is the ground distance itself. Hence, in this regime it is often impossible
to recover a uniquely deﬁned Monge map, making this class of problems ill-suited for
interpolation of measures. We refer to works by Trudinger and Wang , Caﬀarelli
et al. , Sudakov , Evans and Gangbo for proofs of existence of optimal
W1 transportation plans and detailed analyses of their geometric structure.
Although more diﬃcult to analyze in theory, optimal transport with a linear ground
distance is usually more robust to outliers and noise than a quadratic cost. Furthermore, a cost that is a metric results in an elegant dual reformulation involving local
ﬂow, divergence constraints, or Lipschitzness of the dual potential, suggesting cheaper
numerical algorithms that align with minimum-cost ﬂow methods over networks in
graph theory. This setting is also popular because the associated OT distances deﬁne
a norm that can compare arbitrary distributions, even if they are not positive; this
property is shared by a larger class of so-called dual norms (see §8.2 and Remark 10.6
for more details).
6.1. W1 on Metric Spaces
W1 on Metric Spaces
Here we assume that d is a distance on X = Y, and we solve the OT problem with the
ground cost c(x, y) = d(x, y). The following proposition highlights key properties of the
c-transform (5.1) in this setup. In the following, we denote the Lipschitz constant of a
function f ∈C(X) as
|f(x) −f(y)|
: (x, y) ∈X 2, x ̸= y
We deﬁne Lipschitz functions to be those functions f satisfying Lip(f) < +∞; they
form a convex subset of C(X).
Proposition 6.1. Suppose X = Y and c(x, y) = d(x, y). Then, there exists g such that
f = gc if and only Lip(f) ≤1. Furthermore, if Lip(f) ≤1, then fc = −f.
Proof. First, suppose f = gc. Then, for x, y ∈X,
|f(x) −f(y)| =
z∈X d(x, z) −g(z) −
z∈X d(y, z) −g(z)
|d(x, z) −d(y, z)| ≤d(x, y).
The ﬁrst equality follows from the deﬁnition of gc, the next inequality from the identity
| inf f −inf g| ≤sup |f −g|, and the last from the triangle inequality. This shows that
Lip(f) ≤1.
Now, suppose Lip(f) ≤1, and deﬁne g
= −f. By the Lipschitz property, for all
x, y ∈X, f(y) −d(x, y) ≤f(x) ≤f(y) + d(x, y). Applying these inequalities,
gc(y) = inf
x∈X [d(x, y) + f(x)] ≥inf
x∈X [d(x, y) + f(y) −d(x, y)] = f(y),
gc(y) = inf
x∈X [d(x, y) + f(x)] ≤inf
x∈X [d(x, y) + f(y) + d(x, y)] = f(y).
Hence, f = gc with g = −f. Using the same inequalities shows
fc(y) = inf
x∈X [d(x, y) −f(x)] ≥inf
x∈X [d(x, y) −f(y) −d(x, y)] = −f(y),
fc(y) = inf
x∈X [d(x, y) −f(x)] ≤inf
x∈X [d(x, y) −f(y) + d(x, y)] = −f(y).
This shows fc = −f.
Starting from the single potential formulation (5.4), one can iterate the construction
and replace the couple (g, gc) by (gc, (gc)c). The last proposition shows that one can
thus use (gc, −gc), which in turn is equivalent to any pair (f, −f) such that Lip(f) ≤1.
This leads to the following alternative expression for the W1 distance:
W1(α, β) = max
f(x)(dα(x) −dβ(x)) : Lip(f) ≤1
W1 Optimal Transport
This expression shows that W1 is actually a norm, i.e. W1(α, β) = ∥α −β∥W1, and
that it is still valid for any measures (not necessary positive) as long as
This norm is often called the Kantorovich and Rubinstein norm .
For discrete measures of the form (2.1), writing α−β = P
k mkδzk with zk ∈X and
k mk = 0, the optimization (6.1) can be rewritten as
W1(α, β) = max
fkmk : ∀(k, ℓ), |fk −fℓ| ≤d(zk, zℓ),
which is a ﬁnite-dimensional convex program with quadratic-cone constraints. It can
be solved using interior point methods or, as we detail next for a similar problem, using
proximal methods.
When using d(x, y) = |x −y| with X = R, we can reduce the number of constraints
by ordering the zk’s via z1 ≤z2 ≤. . .. In this case, we only have to solve
W1(α, β) = max
fkmk : ∀k, |fk+1 −fk| ≤zk+1 −zk
which is a linear program. Note that furthermore, in this 1-D case, a closed form
expression for W1 using cumulative functions is given in (2.37).
Remark 6.1 (Wp with 0 < p ≤1). If 0 < p ≤1, then ˜d(x, y)
= d(x, y)p satisﬁes the
triangular inequality, and hence ˜d is itself a distance. One can thus apply the results and
algorithms detailed above for W1 to compute Wp by simply using ˜d in place of d. This
is equivalent to stating that Wp is the dual of p-Hölder functions {f : Lipp(f) ≤1},
|f(x) −f(y)|
: (x, y) ∈X 2, x ̸= y
W1 on Euclidean Spaces
In the special case of Euclidean spaces X = Y = Rd, using c(x, y) = ∥x −y∥, the global
Lipschitz constraint appearing in (6.1) can be made local as a uniform bound on the
gradient of f,
W1(α, β) = max
Rd f(x)(dα(x) −dβ(x)) : ∥∇f∥∞≤1
Here the constraint ∥∇f∥∞≤1 signiﬁes that the norm of the gradient of f at any
point x is upper bounded by 1, ∥∇f(x)∥2 ≤1 for any x.
Considering the dual problem to (6.3), one obtains an optimization problem under
ﬁxed divergence constraint
W1(α, β) = min
Rd ∥s(x)∥2 dx : div(s) = α −β
6.3. W1 on a Graph
which is often called the Beckmann formulation [Beckmann, 1952]. Here the vectorial
function s(x) ∈R2 can be interpreted as a ﬂow ﬁeld, describing locally the movement
of mass. Outside the support of the two input measures, div(s) = 0, which is the
conservation of mass constraint. Once properly discretized using ﬁnite elements, Problems (6.3) and (6.4) become nonsmooth convex optimization problems. It is possible to
use an oﬀ-the-shelf interior points quadratic-cone optimization solver, but as advocated
in §7.3, large-scale problems require the use of simpler but more adapted ﬁrst order
methods. One can thus use, for instance, Douglas–Rachford (DR) iterations (7.14) or
the related alternating direction method of multipliers method. Note that on a uniform
grid, projecting on the divergence constraint is conveniently handled using the fast
Fourier transform. We refer to Solomon et al. [2014a] for a detailed account for these
approaches and application to OT on triangulated meshes. See also Li et al. [2018a],
Ryu et al. [2017b,a] for similar approaches using primal-dual splitting schemes. Approximation schemes that relax the Lipschitz constraint on the dual potentials f have
also been proposed, using, for instance, a constraint on wavelet coeﬃcients leading to
an explicit formula [Shirdhonkar and Jacobs, 2008], or by considering only functions f
parameterized as multilayer neural networks with “rectiﬁed linear” max(0, ·) activation
function and clipped weights [Arjovsky et al., 2017].
W1 on a Graph
The previous formulations (6.3) and (6.4) of W1 can be generalized to the setting where
X is a geodesic space, i.e. c(x, y) = d(x, y) where d is a geodesic distance. We refer
to Feldman and McCann for a theoretical analysis in the case where X is a
Riemannian manifold. When X = J1, nK is a discrete set, equipped with undirected
edges (i, j) ∈E ⊂X 2 labeled with a weight (length) wi,j, we recover the important
case where X is a graph equipped with the geodesic distance (or shortest path metric):
K≥0,(ik)k:i→j
wik,ik+1 : ∀k ∈J1, K −1K, (ik, ik+1) ∈E
where i →j indicates that i1 = i and iK = j, namely that the path starts at i and
ends at j.
We consider two vectors (a, b) ∈(Rn)2 deﬁning (signed) discrete measures on the
graph X such that P
i bi (these weights do not need to be positive). The
goal is now to compute W1(a, b), as introduced in (2.17) for p = 1, when the ground
metric is the graph geodesic distance. This computation should be carried out without
going as far as having to compute a “full” coupling P of size n × n, to rely instead on
local operators thanks to the underlying connectivity of the graph. These operators are
discrete formulations for the gradient and divergence diﬀerential operators.
W1 Optimal Transport
A discrete dual Kantorovich potential f ∈Rn is a vector indexed by all vertices of
the graph. The gradient operator ∇: Rn →RE is deﬁned as
∀(i, j) ∈E,
A ﬂow s = (si,j)i,j is deﬁned on edges, and the divergence operator div : RE →Rn,
which is the adjoint of the gradient ∇, maps ﬂows to vectors deﬁned on vertices and is
∀i ∈J1, nK,
(si,j −sj,i) ∈Rn.
Problem (6.3) becomes, in the graph setting,
W1(a, b) = max
fi(ai −bi) : ∀(i, j) ∈E, |(∇f)i,j| ≤wi,j
The associated dual problem, which is analogous to Formula (6.4), is then
W1(a, b) = min
wi,jsi,j : div(s) = a −b
This is a linear program and more precisely an instance of min-cost ﬂow problems.
Highly eﬃcient dedicated simplex solvers have been devised to solve it; see, for instance, [Ling and Okada, 2007]. Figure 6.1 shows an example of primal and dual solutions. Formulation (6.6) is the so-called Beckmann formulation [Beckmann, 1952] and
has been used and extended to deﬁne and study traﬃc congestion models; see, for
instance, [Carlier et al., 2008].
6.3. W1 on a Graph
(a, b) and s
Figure 6.1: Example of computation of W1(a, b) on a planar graph with uniform weights wi,j = 1.
Left: potential f solution of (6.5) (increasing value from red to blue). The green color of the edges is
proportional to |(∇f)i,j|. Right: ﬂow s solution of (6.6), where bold black edges display nonzero si,j,
which saturate to wi,j = 1. These saturating ﬂow edge on the right match the light green edge on the
left where |(∇f)i,j| = 1.
Dynamic Formulations
This chapter presents the geodesic (also called dynamic) point of view of optimal transport when the cost is a squared geodesic distance. This describes the optimal transport
between two measures as a curve in the space of measures minimizing a total length.
The dynamic point of view oﬀers an alternative and intuitive interpretation of optimal
transport, which not only allows us to draw links with ﬂuid dynamics but also results
in an eﬃcient numerical tool to compute OT in small dimensions when interpolating
between two densities. The drawback of that approach is that it cannot scale to largescale sparse measures and works only in low dimensions on regular domains (because
one needs to grid the space) with a squared geodesic cost.
In this chapter, we use the notation (α0, α1) in place of (α, β) in agreement with
the idea that we start at time t = 0 from one measure to reach another one at time
Continuous Formulation
In the case X = Y = Rd, and c(x, y) = ∥x −y∥2, the optimal transport distance
2(α, β) = Lc(α, β) as deﬁned in (2.15) can be computed by looking for a minimal
length path (αt)1
t=0 between these two measures. This path is described by advecting
the measure using a vector ﬁeld vt deﬁned at each instant. The vector ﬁeld vt and the
path αt must satisfy the conservation of mass formula, resulting in
∂t + div(αtvt) = 0
αt=0 = α0, αt=1 = α1,
7.1. Continuous Formulation
where the equation above should be understood in the sense of distributions on Rd. The
inﬁnitesimal length of such a vector ﬁeld is measured using the L2 norm associated to
the measure αt, that is deﬁned as
∥vt∥L2(αt) =
Rd ∥vt(x)∥2 dαt(x)
This deﬁnition leads to the following minimal-path reformulation of W2, originally
introduced by Benamou and Brenier :
2(α0, α1) =
(αt,vt)t sat. (7.1)
Rd ∥vt(x)∥2 dαt(x)dt,
where αt is a scalar-valued measure and vt a vector-valued measure. Figure 7.1 shows
two examples of such paths of measures.
Figure 7.1:
Displacement interpolation αt satisfying (7.2). Top: for two measures (α0, α1) with
densities with respect to the Lebesgue measure. Bottom: for two discrete empirical measures with the
same number of points (bottom).
The formulation (7.2) is a nonconvex formulation in the variables (αt, vt)t because
of the constraint (7.1) involving the product αtvt. Introducing a vector-valued measure
(often called the “momentum”)
Benamou and Brenier showed in their landmark paper that it is instead convex
in the variable (αt, Jt)t when writing
2(α0, α1) =
(αt,Jt)t∈C(α0,α1)
Rd θ(αt(x), Jt(x))dxdt,
Dynamic Formulations
where we deﬁne the set of constraints as
(αt, Jt) : ∂αt
∂t + div(Jt) = 0, αt=0 = α0, αt=1 = α1
and where θ :→R+ ∪{+∞} is the following lower semicontinuous convex function
∀(a, b) ∈R+ × Rd,
(a, b) = 0,
otherwise.
This deﬁnition might seem complicated, but it is crucial to impose that the momentum
Jt(x) should vanish when αt(x) = 0. Note also that (7.3) is written in an informal way
as if the measures (αt, Jt) were density functions, but this is acceptable because θ is a
1-homogeneous function (and hence deﬁned even if the measures do not have a density
with respect to Lebesgue measure) and can thus be extended in an unambiguous way
from density to functions.
Remark 7.1 (Links with McCann’s interpolation). In the case (see Equation (2.28))
where there exists an optimal Monge map T : Rd →Rd with T♯α0 = α1, then αt is
equal to McCann’s interpolation
αt = ((1 −t)Id + tT)♯α0.
In the 1-D case, using Remark 2.30, this interpolation can be computed thanks to
the relation
αt = (1 −t)C−1
see Figure 2.11. We refer to Gangbo and McCann for a detailed review on
the Riemannian geometry of the Wasserstein space. In the case that there is “only”
an optimal coupling π that is not necessarily supported on a Monge map, one can
compute this interpolant as
Pt : (x, y) ∈Rd × Rd 7→(1 −t)x + ty.
For instance, in the discrete setup (2.3), denoting P a solution to (2.11), an interpolation is deﬁned as
Pi,jδ(1−t)xi+tyj.
Such an interpolation is typically supported on n + m −1 points, which is the
maximum number of nonzero elements of P. Figure 7.2 shows two examples of
such displacement interpolation of discrete measures. This construction can be
generalized to geodesic spaces X by replacing Pt by the interpolation along geodesic
7.2. Discretization on Uniform Staggered Grids
paths. McCann’s interpolation ﬁnds many applications, for instance, color, shape,
and illumination interpolations in computer graphics [Bonneel et al., 2011].
Figure 7.2:
Comparison of displacement interpolation (7.8) of discrete measures. Top: point clouds
(empirical measures (α0, α1) with the same number of points). Bottom: same but with varying weights.
For 0 < t < 1, the top example corresponds to an empirical measure interpolation αt with N points,
while the bottom one deﬁnes a measure supported on 2N −1 points.
Discretization on Uniform Staggered Grids
For simplicity, we describe the numerical scheme in dimension d = 2; the extension to
higher dimensions is straightforward. We follow the discretization method introduced
by Papadakis et al. , which is inspired by staggered grid techniques which are
commonly used in ﬂuid dynamics. We discretize time as tk = k/T ∈ and assume
the space is uniformly discretized at points xi = (i1/n1, i2/n2) ∈X = 2. We
use a staggered grid representation, so that αt is represented using a ∈R(T+1)×n1×n2
associated to half grid points in time, whereas J is represented using J = (J1, J2),
where J1 ∈RT×(n1+1)×n2 and J1 ∈RT×n1×(n2+1) are stored at half grid points in each
space direction. Using this representation, for (k, i1, i2) ∈J1, TK × J1, n1K × J1, n2K, the
time derivative is computed as
= ak+1,i −ak,i
and spatial divergence as
k,i1+1,i2 −J1
k,i1,i2 + J2
k,i1,i2+1 −J2
which are both deﬁned at grid points, thus forming arrays of RT×n1×n2.
Dynamic Formulations
In order to evaluate the functional to be optimized, one needs interpolation operators from midgrid points to grid points, for all (k, i1, i2) ∈J1, TK × J1, n1K × J1, n2K,
= I(ak+1,i, ak,i),
k,i1+1,i2, J1
k,i1,i2), I(J2
k,i1,i2+1, J2
k,i1,i2)).
The simplest choice is to use a linear operator I(r, s) =
2 , which is the one we
consider next. The discrete counterpart to (7.3) reads
(a,J)∈C(a0,a1) Θ(Ia(a), IJ(J)),
θ(˜ak,i, ˜Jk,i),
and where the constraint now reads
= {(a, J) : ∂ta + div(J) = 0, (a0,·, aT,·) = (a0, a1)} ,
where a ∈R(T+1)×n1×n2, J = (J1, J2) with J1 ∈RT×(n1+1)×n2, J2 ∈RT×n1×(n2+1).
Figure 7.3 shows an example of evolution (αt)t approximated using this discretization
Remark 7.2 (Dynamic formulation on graphs). In the case where X is a graph and
c(x, y) = dX (x, y)2 is the squared geodesic distance, it is possible to derive faithful
discretization methods that use a discrete divergence associated to the graph structure
in place of the uniform grid discretization (7.10). In order to ensure that the heat
equation has a gradient ﬂow structure (see §9.3 for more details about gradient ﬂows)
for the corresponding dynamic Wasserstein distance,
Maas and later
 proposed to use a logarithmic mean I(r, s) .
Proximal Solvers
The discretized dynamic OT problem (7.11) is challenging to solve because it requires
us to minimize a nonsmooth optimization problem under aﬃne constraints. Indeed,
the function θ is convex but nonsmooth for measures with vanishing mass ak,i. When
interpolating between two compactly supported input measures (a0, a1), one typically
expects the mass of the interpolated measures (ak)T
k=1 to vanish as well, and the diﬃcult
part of the optimization process is indeed to track this evolution of the support. In
particular, it is not possible to use standard smooth optimization techniques.
There are several ways to recast (7.11) into a quadratic-cone program, either by
considering the dual problem or simply by replacing the functional θ(ak,i, Jk,i) by a
7.3. Proximal Solvers
linear function under constraints,
Θ(˜a, ˜J) = min
˜zk,i : ∀(k, i), (zk,i, ˜ak,i, ˜Ji,j) ∈L
which thus requires the introduction of an extra variable ˜z. Here L
= {(z, a, J) ∈
R × R+ × Rd
∥J∥2 ≤za} is a rotated Lorentz quadratic-cone. With this extra
variable, it is thus possible to solve the discretized problem using standard interior point
solvers for quadratic-cone programs [Nesterov and Nemirovskii, 1994]. These solvers
have fast convergence rates and are thus capable of computing a solution with high
precision. Unfortunately, each iteration is costly and requires the resolution of a linear
system of dimension that scales with the number of discretization points. They are
thus not applicable for large-scale multidimensional problems encountered in imaging
applications.
An alternative to these high-precision solvers are low-precision ﬁrst order methods,
which are well suited for nonsmooth but highly structured problems such as (7.11).
While this class of solvers is not new, it has recently been revitalized in the ﬁelds of
imaging and machine learning because they are the perfect ﬁt for these applications,
where numerical precision is not the driving goal. We refer, for instance, to the monograph [Bauschke and Combettes, 2011] for a detailed account on these solvers and their
use for large-scale applications. We here concentrate on a speciﬁc solver, but of course
many more can be used, and we refer to [Papadakis et al., 2014] for a study of several
such approaches for dynamical OT. Note that the idea of using a ﬁrst order scheme for
dynamical OT was initially proposed by Benamou and Brenier .
The DR algorithm [Lions and Mercier, 1979] is speciﬁcally tailored to solve nonsmooth structured problems of the form
x∈H F(x) + G(x),
where H is some Euclidean space, and where F, G : H →R ∪{+∞} are two closed
convex functions, for which one can “easily ” (e.g. in closed form or using a rapidly
converging scheme) compute the so-called proximal operator
ProxτF (x)
for a parameter τ > 0. Note that this corresponds to the proximal map for the Euclidean
metric and that this deﬁnition can be extended to more general Bregman divergence in
place of ∥x −x′∥2; see (4.52) for an example using the KL divergence. The iterations of
the DR algorithm deﬁne a sequence (x(ℓ), w(ℓ)) ∈H2 using an initialization (x(0), w(0)) ∈
w(ℓ+1) def.
= w(ℓ) + α(ProxγF (2x(ℓ) −w(ℓ)) −x(ℓ)),
x(ℓ+1) def.
= ProxγG(w(ℓ+1)).
Dynamic Formulations
If 0 < α < 2 and γ > 0, one can show that x(ℓ) →z⋆, where z⋆is a solution of (7.12);
see [Combettes and Pesquet, 2007] for more details. This algorithm is closely related to
another popular method, the alternating direction method of multipliers [Gabay and
Mercier, 1976, Glowinski and Marroco, 1975] ,
which can be recovered by applying DR on a dual problem; see [Papadakis et al., 2014]
for more details on the equivalence between the two, ﬁrst shown by [Eckstein and
Bertsekas, 1992].
There are many ways to recast Problem (7.11) in the form (7.12), and we refer
to [Papadakis et al., 2014] for a detailed account of these approaches. A simple way to
achieve this is by setting x = (a, J, ˜a, ˜J) and letting
= Θ(˜a, ˜J) + ιC(a0,a1)(a, J)
G(x) = ιD(a, J, ˜a, ˜J),
(a, J, ˜a, ˜J) : ˜a = Ia(a), ˜J = IJ(J)
The proximal operator of these two functions can be computed eﬃciently. Indeed, one
ProxτF (x) = (ProxτΘ(˜a, ˜J), ProjC(a0,a1)(a, J)).
The proximal operator ProxτΘ is computed by solving a cubic polynomial equation at
each grid position. The orthogonal projection on the aﬃne constraint C(a0, a1) involves
the resolution of a Poisson equation, which can be achieved in O(N log(N)) operations
using the fast Fourier transform, where N = Tn1n2 is the number of grid points.
Lastly, the proximal operator ProxτG is a linear projector, which requires the inversion
of a small linear system. We refer to Papadakis et al. for more details on these
computations. Figure 7.3 shows an example in which that method is used to compute
a dynamical interpolation inside a complicated planar domain. This class of proximal
methods for dynamical OT has also been used to solve related problems such as mean
ﬁeld games [Benamou and Carlier, 2015].
Dynamical Unbalanced OT
In order to be able to match input measures with diﬀerent mass α0(X) ̸= α1(X) (the
so-called “unbalanced” settings, the terminology introduced by Benamou ), and
also to cope with local mass variation, several normalizations or relaxations have been
proposed, in particular by relaxing the ﬁxed marginal constraint; see §10.2. A general
methodology consists in introducing a source term st(x) in the continuity equation (7.4).
We thus consider
¯C(α0, α1)
(αt, Jt, st) : ∂αt
∂t + div(Jt) = st, αt=0 = α0, αt=1 = α1
The crucial question is how to measure the cost associated to this source term and
introduce it in the original dynamic formulation (7.3). Several proposals appear in the
7.4. Dynamical Unbalanced OT
Figure 7.3: Solution αt of dynamic OT computed with a proximal splitting scheme.
literature, for instance, using an L2 cost Piccoli and Rossi . In order to avoid
having to “teleport” mass (mass which travels at inﬁnite speed and suddenly grows
in a region where there was no mass before), the associated cost should be inﬁnite. It
turns out that this can be achieved in a simple convex way, by also allowing st to be
an arbitrary measure (e.g. using a 1-homogeneous cost) by penalizing st in the same
way as the momentum Jt,
WFR2(α0, α1) =
(αt,Jt,st)t∈¯C(α0,α1) Θ(α, J, s),
Θ(α, J, s)
Rd (θ(αt(x), Jt(x)) + τθ(αt(x), st(x))) dxdt,
where θ is the convex 1-homogeneous function introduced in (7.5), and τ is a weight
controlling the trade-oﬀbetween mass transportation and mass creation/destruction.
This formulation was proposed independently by several authors [Liero et al., 2016,
Chizat et al., 2018c, Kondratyev et al., 2016]. This “dynamic” formulation has a “static”
counterpart; see Remark 10.5. The convex optimization problem (7.15) can be solved
using methods similar to those detailed in §7.3. Figure 7.4 displays a comparison of
several unbalanced OT dynamic interpolations. This dynamic formulation resembles
“metamorphosis” models for shape registration [Trouvé and Younes, 2005], and a more
precise connection is detailed in [Maas et al., 2015, 2016].
As τ →0, and if α0(X) = α1(X), then one retrieves the classical OT problem,
WFR(α0, α1) →W(α0, α1). In contrast, as τ →+∞, this distance approaches the
Dynamic Formulations
Hellinger metric over densities
τ WFR(α0, α1)2
ρα1(x)|2dx
(x)|2dα0(x).
Figure 7.4: Comparison of Hellinger (ﬁrst row), Wasserstein (row 2), partial optimal transport (row
3), and Wasserstein–Fisher–Rao (row 4) dynamic interpolations.
More General Mobility Functionals
It is possible to generalize the dynamic formulation (7.3) by considering other “mobility
functions” θ in place of the one deﬁned in (7.5). A possible choice for this mobility
functional is proposed in Dolbeault et al. ,
∀(a, b) ∈R+ × Rd,
θ(a, b) = as−p ∥b∥p ,
where the parameter should satisfy p ≥1 and s ∈[1, p] in order for θ to be convex.
Note that this deﬁnition should be handled with care in the case 1 < s ≤p because θ
does not have a linear growth at inﬁnity, so that solutions to (7.3) must be constrained
to have a density with respect to the Lebesgue measure.
The case s = 1 corresponds to the classical OT problem and the optimal value
of (7.3) deﬁnes Wp(α, β). In this case, θ is 1-homogeneous, so that solutions to (7.3)
can be arbitrary measures. The case (s = 1, p = 2) is the initial setup considered in (7.3)
to deﬁne W2.
7.6. Dynamic Formulation over the Paths Space
The limiting case s = p is also interesting, because it corresponds to a dual Sobolev
norm W −1,p and the value of (7.3) is then equal to
W −1,p(Rd) = min
Rd fd(α −β) :
Rd ∥∇f(x)∥q dx ≤1
for 1/q + 1/p = 1. In the limit (p = s, q) →(1, ∞), one recovers the W1 norm. The
case s = p = 2 corresponds to the Sobolev H−1(Rd) Hilbert norm deﬁned in (8.15).
Dynamic Formulation over the Paths Space
There is a natural dynamical formulation of both classical and entropic regularized
(see §4) formulations of OT, which is based on studying abstract optimization problems
on the space ¯
X of all possible paths γ : →X (i.e. curves) on the space X. For
simplicity, we assume X = Rd, but this extends to more general spaces such as geodesic
spaces and graphs. Informally, the dynamic of “particles” between two input measures
α0, α1 at times t = 0, 1 is described by a probability distribution ¯π ∈M1
X). Such a
distribution should satisfy that the distributions of starting and end points must match
(α0, α1), which is formally written using push-forward as
¯U(α0, α1)
X) : ¯P0♯¯π = α0, ¯P1♯¯π = α1
where, for any path γ ∈¯
X, P0(γ) = γ(0), P1(γ) = γ(1).
OT over the space of paths.
The dynamical version of classical OT (2.15), formulated
over the space of paths, then reads
W2(α0, α1)2 =
¯π∈¯U(α0,α1)
L(γ)2d¯π(γ),
where L(γ) =
0 |γ′(s)|2ds is the kinetic energy of a path s ∈ 7→γ(s) ∈X. The
connection between optimal couplings π⋆and ¯π⋆solving respectively (7.17) and (2.15)
is that ¯π⋆only gives mass to geodesics joining pairs of points in proportion prescribed
by π⋆. In the particular case of discrete measures, this means that
Pi,jδ(xi,yj)
Pi,jδγxi,yj ,
where γxi,yj is the geodesic between xi and yj. Furthermore, the measures deﬁned by
the distribution of the curve points γ(t) at time t, where γ is drawn following ¯π⋆, i.e.
t ∈ 7→αt
Pt(γ) = γ(t) ∈X,
is a solution to the dynamical formulation (7.3), i.e. it is the displacement interpolation.
In the discrete case, one recovers (7.9).
Dynamic Formulations
Entropic OT over the space of paths.
We now turn to the re-interpretation of entropic OT, deﬁned in Chapter 4, using the space of paths. Similarly to (4.11), this is
deﬁned using a Kullback–Leibler projection, but this time of a reference measure over
the space of paths ¯K which is the distribution of a reversible Brownian motion (Wiener
process), which has a uniform distribution at the initial and ﬁnal times
¯π∈¯U(α0,α1) KL(¯π| ¯K).
We refer to the review paper by Léonard for an overview of this problem and an
historical account of the work of Schrödinger . One can show that the (unique)
solution ¯π⋆
ε to (7.19) converges to a solution of (7.17) as ε →0. Furthermore, this
solution is linked to the solution of the static entropic OT problem (4.9) using Brownian
bridge ¯γε
X (which are similar to fuzzy geodesic and converge to δγx,y as ε →0).
In the discrete setting, this means that
ε,i,jδ(xi,yj)
ε,i,j can be computed using Sinkhorn’s algorithm. Similarly to (7.18), one then
can deﬁne an entropic interpolation as
Since the law Pt♯¯γε
x,y of the position at time t along a Brownian bridge is a Gaussian
Gt(1−t)ε2(· −γx,y(t)) of variance t(1 −t)ε2 centered at γx,y(t), one can deduce that αε,t
is a Gaussian blurring of a set of traveling Diracs
ε,i,jGt(1−t)ε2(· −γxi,yj(t)).
The resulting mixture of Brownian bridges is displayed on Figure 7.5.
Figure 7.5: Samples from Brownian bridge paths associated to the Schrödinger entropic interpolation (7.20) over path space. Blue corresponds to t = 0 and red to t = 1.
7.6. Dynamic Formulation over the Paths Space
Another way to describe this entropic interpolation (αt)t is using a regularization
of the Benamou–Brenier dynamic formulation (7.2), namely
(αt,vt)t sat. (7.1)
∥vt(x)∥2 + ε
4 ∥∇log(αt)(x)∥2
see [Gentil et al., 2015, Chen et al., 2016a].
Statistical Divergences
We study in this chapter the statistical properties of the Wasserstein distance. More
speciﬁcally, we compare it to other major distances and divergences routinely used
in data sciences. We quantify how one can approximate the distance between two
probability distributions when having only access to samples from said distributions.
To introduce these subjects, §8.1 and §8.2 review respectively divergences and integral
probability metrics between probability distributions. A divergence D typically satisﬁes
D(α, β) ≥0 and D(α, β) = 0 if and only if α = β, but it does not need to be symmetric
or satisfy the triangular inequality. An integral probability metric for measures is a
dual norm deﬁned using a prescribed family of test functions. These quantities are
sound alternatives to Wasserstein distances and are routinely used as loss functions
to tackle inference problems, as will be covered in §9. We show ﬁrst in §8.3 that the
optimal transport distance is not Hilbertian, i.e. one cannot approximate it eﬃciently
using a Hilbertian metric on a suitable feature representation of probability measures.
We show in §8.4 how to approximate D(α, β) from discrete samples (xi)i and (yj)j
drawn from α and β. A good statistical understanding of that problem is crucial when
using the Wasserstein distance in machine learning. Note that this section will be chieﬂy
concerned with the statistical approximation of optimal transport between distributions
supported on continuous sets. The very same problem when the ground space is ﬁnite
has received some attention in the literature following the work of Sommerfeld and
Munk , extended to entropic regularized quantities by Bigot et al. [2017a].
8.1. ϕ-Divergences
ϕ-Divergences
Before detailing in the following section “weak” norms, whose construction shares similarities with W1, let us detail a generic construction of so-called divergences between
measures, which can then be used as loss functions when estimating probability distributions. Such divergences compare two input measures by comparing their mass
pointwise, without introducing any notion of mass transportation. Divergences are functionals which, by looking at the pointwise ratio between two measures, give a sense of
how close they are. They have nice analytical and computational properties and build
upon entropy functions.
Deﬁnition 8.1 (Entropy function). A function ϕ : R →R∪{∞} is an entropy function if
it is lower semicontinuous, convex, dom ϕ ⊂[0, ∞[, and satisﬁes the following feasibility
condition: dom ϕ ∩]0, ∞[ ̸= ∅. The speed of growth of ϕ at ∞is described by
x→+∞ϕ(x)/x ∈R ∪{∞} .
∞= ∞, then ϕ grows faster than any linear function and ϕ is said superlinear.
Any entropy function ϕ induces a ϕ-divergence as follows.
Deﬁnition 8.2 (ϕ-Divergences). Let ϕ be an entropy function. For α, β ∈M(X), let
dβ β + α⊥be the Lebesgue decomposition1 of α with respect to β. The divergence Dϕ
is deﬁned by
if α, β are nonnegative and ∞otherwise.
The additional term ϕ′
∞α⊥(X) in (8.1) is important to ensure that Dϕ deﬁnes a
continuous functional (for the weak topology of measures) even if ϕ has a linear growth
at inﬁnity, as this is, for instance, the case for the absolute value (8.8) deﬁning the TV
norm. If ϕ as a superlinear growth, e.g. the usual entropy (8.4), then ϕ′
that Dϕ(α|β) = +∞if α does not have a density with respect to β.
In the discrete setting, assuming
are supported on the same set of n points (xi)n
i=1 ⊂X, (8.1) deﬁnes a divergence on
i/∈Supp(b)
1The Lebesgue decomposition theorem asserts that, given β, α admits a unique decomposition as
the sum of two measures αs + α⊥such that αs is absolutely continuous with respect to β and α⊥and
β are singular.
Statistical Divergences
where Supp(b)
= {i ∈JnK : bi ̸= 0}.
The proof of the following proposition can be found in [Liero et al., 2018, Thm 2.7].
Proposition 8.1. If ϕ is an entropy function, then Dϕ is jointly 1-homogeneous, convex
and weakly* lower semicontinuous in (α, β).
Figure 8.1: Example of entropy functionals.
Remark 8.1 (Dual expression). A ϕ-divergence can be expressed using the Legendre
of ϕ (see also (4.54)) as
f(x)dα(x) −
ϕ∗(f(x))dβ(x);
see Liero et al. for more details.
We now review a few popular instances of this framework. Figure 8.1 displays the
associated entropy functionals, while Figure 8.2 reviews the relationship between them.
Example 8.1 (Kullback–Leibler divergence). The Kullback–Leibler divergence KL
DϕKL, also known as the relative entropy, was already introduced in (4.10) and (4.6).
It is the divergence associated to the Shannon–Boltzman entropy function ϕKL, given
s log(s) −s + 1
for s > 0,
for s = 0,
otherwise.
8.1. ϕ-Divergences
TV 6 W1/dmin
W1 6 dmaxTV
KL 6 log(1 + χ2)
Figure 8.2: Diagram of relationship between divergences (inspired by Gibbs and Su ). For X a
metric space with ground distance d, dmax = sup(x,x′) d(x, x′) is the diameter of X. When X is discrete,
= minx̸=x′ d(x, x′).
Remark 8.1 (Bregman divergence). The discrete KL divergence, KL
= DϕKL, has the
unique property of being both a ϕ-divergence and a Bregman divergence. For discrete
vectors in Rn, a Bregman divergence [Bregman, 1967] associated to a smooth strictly
convex function ψ : Rn →R is deﬁned as
= ψ(a) −ψ(b) −⟨∇ψ(b), a −b⟩,
where ⟨·, ·⟩is the canonical inner product on Rn. Note that Bψ(a|b) is a convex function
of a and a linear function of ψ. Similarly to ϕ-divergence, a Bregman divergence satisﬁes
Bψ(a|b) ≥0 and Bψ(a|b) = 0 if and only if a = b. The KL divergence is the Bregman
divergence for minus the entropy ψ = −H deﬁned in (4.1)), i.e. KL = B−H. A Bregman
divergence is locally a squared Euclidean distance since
Bψ(a + ε|a + η) = ⟨∂2ψ(a)(ε −η), ε −η⟩+ o(∥ε −η∥2)
and the set of separating points
a : Bψ(a|b) = Bψ(a|b′)
is a hyperplane between
b and b′. These properties make Bregman divergence suitable to replace Euclidean
distances in ﬁrst order optimization methods. The best know example is mirror gradient
descent [Beck and Teboulle, 2003], which is an explicit descent step of the form (9.32).
Bregman divergences are also important in convex optimization and can be used, for
instance, to derive Sinkhorn iterations and study its convergence in ﬁnite dimension;
see Remark 4.8.
Remark 8.2 (Hyperbolic geometry of KL). It is interesting to contrast the geometry of
the Kullback–Leibler divergence to that deﬁned by quadratic optimal transport when
comparing Gaussians. As detailed, for instance, by Costa et al. , the Kullback–
Leibler divergence has a closed form for Gaussian densities. In the univariate case,
Statistical Divergences
d = 1, if α = N(mα, σ2
α) and β = N(mβ, σ2
β), one has
KL(α|β) = 1
+ |mα −mβ|
This expression shows that the divergence between α and β diverges to inﬁnity as σβ
diminishes to 0 and β becomes a Dirac mass. In that sense, one can say that singular
Gaussians are inﬁnitely far from all other Gaussians in the KL geometry. That geometry
is thus useful when one wants to avoid dealing with singular covariances. To simplify
the analysis, one can look at the inﬁnitesimal geometry of KL, which is obtained by
performing a Taylor expansion at order 2,
KL(N(m + δm, (σ + δσ)2)|N(m, σ2)) = 1
This local Riemannian metric, the so-called Fisher metric, expressed over (m/
R × R+,∗, matches exactly that of the hyperbolic Poincaré half plane. Geodesics over
this space are half circles centered along the σ = 0 line and have an exponential speed,
i.e. they only reach the limit σ = 0 after an inﬁnite time. Note in particular that if
σα = σβ but mα ̸= mα, then the geodesic between (α, β) over this hyperbolic half plane
does not have a constant standard deviation.
The KL hyperbolic geometry over the space of Gaussian parameters (m, σ) should be
contrasted with the Euclidean geometry associated to OT as described in Remark 2.31,
since in the univariate case
2(α, β) = |mα −mβ|2 + |σα −σβ|2.
Figure 8.3 shows a visual comparison of these two geometries and their respective
geodesics. This interesting comparison was suggested to us by Jean Feydy.
Example 8.2 (Total variation). The total variation distance TV
= DϕTV is the divergence associated to
otherwise.
It actually deﬁnes a norm on the full space of measure M(X) where
TV(α|β) = ∥α −β∥TV ,
∥α∥TV = |α|(X) =
If α has a density ρα on X = Rd, then the TV norm is the L1 norm on functions,
X |ρα(x)|dx = ∥ρα∥L1. If α is discrete as in (8.2), then the TV norm is the
ℓ1 norm of vectors in Rn, ∥α∥TV = P
i |ai| = ∥a∥ℓ1.
8.1. ϕ-Divergences
Figure 8.3:
Comparisons of interpolation between Gaussians using KL (hyperbolic) and OT (Euclidean) geometries.
Remark 8.2 (Strong vs. weak topology). The total variation norm (8.9) deﬁnes the socalled “strong” topology on the space of measure. On a compact domain X of radius
R, one has
W1(α, β) ≤R ∥α −β∥TV
so that this strong notion of convergence implies the weak convergence metrized by
Wasserstein distances. The converse is, however, not true, since δx does not converge
strongly to δy if x →y (note that ∥δx −δy∥TV = 2 if x ̸= y). A chief advantage is that
+(X) (once again on a compact ground space X) is compact for the weak topology,
so that from any sequence of probability measures (αk)k, one can always extract a converging subsequence, which makes it a suitable space for several optimization problems,
such as those considered in Chapter 9.
Example 8.3 (Hellinger). The Hellinger distance h
ϕH is the square root of the
divergence associated to
otherwise.
As its name suggests, h is a distance on M+(X), which metrizes the strong topology
as ∥·∥TV. If (α, β) have densities (ρα, ρβ) on X = Rd, then h(α, β) = ∥√ρα −√ρβ∥L2.
If (α, β) are discrete as in (8.2), then h(α, β) = ∥√a −
b∥. Considering ϕLp(s) =
|s1/p −1|p generalizes the Hellinger (p = 2) and total variation (p = 1) distances and
ϕLp is a distance which metrizes the strong convergence for 0 < p < +∞.
Statistical Divergences
Example 8.4 (Jensen–Shannon distance). The KL divergence is not symmetric and,
while being a Bregman divergence (which are locally quadratic norms), it is not the
square of a distance. On the other hand, the Jensen–Shannon distance JS(α, β), deﬁned
JS(α, β)2 def.
2 (KL(α|ξ) + KL(β|ξ))
is a distance [Endres and Schindelin, 2003, Österreicher and Vajda, 2003]. JS2 can be
shown to be a ϕ-divergence for ϕ(s) = t log(t) −(t + 1) log(t + 1). In sharp contrast
with KL, JS(α, β) is always bounded; more precisely, it satisﬁes 0 ≤JS(α, β)2 ≤ln(2).
Similarly to the TV norm and the Hellinger distance, it metrizes the strong convergence.
Example 8.5 (χ2). The χ2-divergence χ2 def.
= Dϕχ2 is the divergence associated to
otherwise.
If (α, β) are discrete as in (8.2) and have the same support, then
Integral Probability Metrics
Formulation (6.3) is a special case of a dual norm. A dual norm is a convenient way to
design “weak” norms that can deal with arbitrary measures. For a symmetric convex
set B of measurable functions, one deﬁnes
f(x)dα(x) : f ∈B
These dual norms are often called “integral probability metrics’; see [Sriperumbudur
et al., 2012].
Example 8.6 (Total variation). The total variation norm (Example 8.2) is a dual norm
associated to the whole space of continuous functions
B = {f ∈C(X) : ∥f∥∞≤1} .
The total variation distance is the only nontrivial divergence that is also a dual norm;
see [Sriperumbudur et al., 2009].
Remark 8.3 (Metrizing the weak convergence). By using smaller “balls” B, which typically only contain continuous (and sometimes regular) functions, one deﬁnes weaker
dual norms. In order for ∥·∥B to metrize the weak convergence (see Deﬁnition 2.2), it is
8.2. Integral Probability Metrics
suﬃcient for the space spanned by B to be dense in the set of continuous functions for
the sup-norm ∥·∥∞(i.e. for the topology of uniform convergence); see [Ambrosio et al.,
2006, para. 5.1].
Figure 8.4 displays a comparison of several such dual norms, which we now detail.
(α, β) = (δ0, δt)
(α, β) = (δ0, 1
2(δ−t/2 + δt/2))
Figure 8.4: Comparison of dual norms.
W1 and Flat Norm
If the set B is bounded, then ∥·∥B is a norm on the whole space M(X) of measures.
This is not the case of W1, which is only deﬁned for α such that
X dα = 0 (otherwise
∥α∥B = +∞). This can be alleviated by imposing a bound on the value of the potential
f, in order to deﬁne for instance the ﬂat norm.
Example 8.7 (W1 norm). W1 as deﬁned in (6.3), is a special case of dual norm (8.10),
B = {f : Lip(f) ≤1}
the set of 1-Lipschitz functions.
Example 8.8 (Flat norm and Dudley metric). The ﬂat norm is deﬁned using
B = {f : ∥∇f∥∞≤1
It metrizes the weak convergence on the whole space M(X). Formula (6.2) is extended
to compute the ﬂat norm by adding the constraint |fk| ≤1. The ﬂat norm is sometimes
called the “Kantorovich–Rubinstein” norm [Hanin, 1992] and has been used as a ﬁdelity
term for inverse problems in imaging [Lellmann et al., 2014]. The ﬂat norm is similar
to the Dudley metric, which uses
B = {f : ∥∇f∥∞+ ∥f∥∞≤1} .
Statistical Divergences
Dual RKHS Norms and Maximum Mean Discrepancies
It is also possible to deﬁne “Euclidean” norms (built using quadratic functionals) on
measures using the machinery of kernel methods and more speciﬁcally reproducing
kernel Hilbert spaces , of which we recall ﬁrst some basic deﬁnitions.
Deﬁnition 8.3. A symmetric function k (resp., ϕ) deﬁned on a set X × X is said to be
positive (resp., negative) deﬁnite if for any n ≥0, family x1, . . . , xn ∈Z, and vector
r ∈Rn the following inequality holds:
rirjk(xi, xj) ≥0,
rirjϕ(xi, xj) ≤0
The kernel is said to be conditionally positive if positivity only holds in (8.12) for zero
mean vectors r (i.e. such that ⟨r, 1n⟩= 0).
If k is conditionally positive, one deﬁnes the following norm:
k(x, y)dα(x)dα(y).
These norms are often referred to as “maximum mean discrepancy” (MMD) and have also been called “kernel norms” in shape analysis [Glaunes
et al., 2004]. This expression (8.13) can be rephrased, introducing two independent
random vectors (X, X′) on X distributed with law α, as
k = EX,X′(k(X, X′)).
One can show that ∥·∥2
k is the dual norm in the sense of (8.10) associated to the unit
ball B of the RKHS associated to k. We refer to [Berlinet and Thomas-Agnan, 2003,
Hofmann et al., 2008, Schölkopf and Smola, 2002] for more details on RKHS functional
Remark 8.4 (Universal kernels). According to Remark 8.3, the MMD norm ∥·∥k metrizes
the weak convergence if the span of the dual ball B is dense in the space of continuous functions C(X). This means that ﬁnite sums of the form Pn
i=1 aik(xi, ·) (for arbitrary choice of n and points (xi)i) are dense in C(X) for the uniform norm ∥·∥∞. For
translation-invariant kernels over X = Rd, k(x, y) = k0(x −y), this is equivalent to
having a nonvanishing Fourier transform, ˆk0(ω) > 0.
In the special case where α is a discrete measure of the form (2.3), one thus has the
simple expression
aiai′ki,i′ = ⟨ka, a⟩
= k(xi, xi′).
8.2. Integral Probability Metrics
In particular, when α = Pn
i=1 aiδxi and β = Pn
i=1 biδxi are supported on the same set
of points, ∥α −β∥2
k = ⟨k(a −b), a −b⟩, so that ∥·∥k is a Euclidean norm (proper if
k is positive deﬁnite, degenerate otherwise if k is semideﬁnite) on the simplex Σn. To
compute the discrepancy between two discrete measures of the form (2.3), one can use
aiai′k(xi, xi′) +
bjbj′k(yj, yj′) −2
aibjk(xi, yj).
Example 8.9 (Gaussian RKHS). One of the most popular kernels is the Gaussian one
k(x, y) = e−∥x−y∥2
, which is a positive universal kernel on X = Rd. An attractive
feature of the Gaussian kernel is that it is separable as a product of 1-D kernels,
which facilitates computations when working on regular grids (see also Remark 4.17).
However, an important issue that arises when using the Gaussian kernel is that one
needs to select the bandwidth parameter σ. This bandwidth should match the “typical
scale” between observations in the measures to be compared. If the measures have
multiscale features (some regions may be very dense, others very sparsely populated),
a Gaussian kernel is thus not well adapted, and one should consider a “scale-free” kernel
as we detail next. An issue with such scale-free kernels is that they are global (have
slow polynomial decay), which makes them typically computationally more expensive,
since no compact support approximation is possible. Figure 8.5 shows a comparison
between several kernels.
ED(R2, ∥·∥)
Figure 8.5:
Top row: display of ψ such that ∥α −β∥k = ∥ψ ⋆(α −β)∥L2(R2), formally deﬁned over
Fourier as ˆψ(ω) =
pˆk0(ω), where k(x, x′) = k0(x −x′). Bottom row: display of ψ ⋆(α −β). (G,σ)
stands for Gaussian kernel of variance σ2. The kernel for ED(R2, ∥·∥) is ψ(x) = 1/
Statistical Divergences
Example 8.10 (H−1(Rd)). Another important dual norm is H−1(Rd), the dual (over
distributions) of the Sobolev space H1(Rd) of functions having derivatives in L2(Rd).
It is deﬁned using the primal RKHS norm ∥∇f∥2
L2(Rd). It is not deﬁned for singular
measures (e.g. Diracs) unless d = 1 because functions in the Sobolev space H1(Rd) are
in general not continuous. This H−1 norm (deﬁned on the space of zero mean measures
with densities) can also be formulated in divergence form,
H−1(Rd) = min
Rd ∥s(x)∥2
2 dx : div(s) = α −β
which should be contrasted with (6.4), where an L1 norm of the vector ﬁeld s was used
in place of the L2 norm used here. The “weighted” version of this Sobolev dual norm,
Rd ∥s(x)∥2
can be interpreted as the natural “linearization” of the Wasserstein W2 norm, in the
sense that the Benamou–Brenier dynamic formulation can be interpreted inﬁnitesimally
W2(α, α + ερ) = ε ∥ρ∥H−1(α) + o(ε).
The functionals W2(α, β) and ∥α −β∥H−1(α) can be shown to be equivalent [Peyre,
2011]. The issue is that ∥α −β∥H−1(α) is not a norm (because of the weighting by α),
and one cannot in general replace it by ∥α −β∥H−1(Rd) unless (α, β) have densities. In
this case, if α and β have densities on the same support bounded from below by a > 0
and from above by b < +∞, then
b−1/2 ∥α −β∥H−1(Rd) ≤W2(α, β) ≤a−1/2 ∥α −β∥H−1(Rd) ;
see [Santambrogio, 2015, Theo. 5.34], and see [Peyre, 2011] for sharp constants.
Example 8.11 (Negative Sobolev spaces). One can generalize this construction by considering the Sobolev space H−r(Rd) of arbitrary negative index, which is the dual of
the functional Sobolev space Hr(Rd) of functions having r derivatives (in the sense of
distributions) in L2(Rd). In order to metrize the weak convergence, one needs functions
in Hr(Rd) to be continuous, which is the case when r > d/2. As the dimension d increases, one thus needs to consider higher regularity. For arbitrary α (not necessarily
integers), these spaces are deﬁned using the Fourier transform, and for a measure α
with Fourier transform ˆα(ω) (written here as a density with respect to the Lebesgue
measure dω)
Rd ∥ω∥−2r |ˆα(ω)|2dω.
This corresponds to a dual RKHS norm with a convolutive kernel k(x, y) = k0(x −y)
with ˆk0(ω) = ± ∥ω∥−2r. Taking the inverse Fourier transform, one sees that (up to
8.3. Wasserstein Spaces Are Not Hilbertian
constant) one has
Example 8.12 (Energy distance). The energy distance (or Cramer distance when d =
1) [Székely and Rizzo, 2004] associated to a distance d is deﬁned as
∥α −β∥ED(X,dp)
= ∥α −β∥kED
kED(x, y) = −d(x, y)p
for 0 < p < 2.
It is a valid MMD norm over measures if d is negative deﬁnite (see
Deﬁnition 8.3), a typical example being the Euclidean distance d(x, y) = ∥x −y∥. For
X = Rd, d(x, y) = ∥·∥, using (8.18), one sees that the energy distance is a Sobolev norm
∥·∥ED(Rd,∥·∥p) = ∥·∥
A chief advantage of the energy distance over more usual kernels such as the Gaussian
(Example 8.9) is that it is scale-free and does not depend on a bandwidth parameter
σ. More precisely, one has the following scaling behavior on X = Rd, when denoting
fs(x) = sx the dilation by a factor s > 0,
∥fs♯(α −β)∥ED(Rd,∥·∥p) = s
2 ∥α −β∥ED(Rd,∥·∥p) ,
while the Wasserstein distance exhibits a perfect linear scaling,
Wp(fs♯α, fs♯β)) = s Wp(α, β)).
Note, however, that for the energy distance, the parameter p must satisfy 0 < p < 2,
and that for p = 2, it degenerates to the distance between the means
∥α −β∥ED(Rd,∥·∥2) =
Rd x(dα(x) −dβ(x))
so it is not a norm anymore. This shows that it is not possible to get the same linear
scaling under fs♯with the energy distance as for the Wasserstein distance.
Wasserstein Spaces Are Not Hilbertian
Some of the special cases of the Wasserstein geometry outlined earlier in §2.6 have
highlighted the fact that the optimal transport distance can sometimes be computed
in closed form. They also illustrate that in such cases the optimal transport distance is
a Hilbertian metric between probability measures, in the sense that there exists a map
φ from the space of input measures onto a Hilbert space, as deﬁned below.
Deﬁnition 8.4. A distance d deﬁned on a set Z × Z is said to be Hilbertian if there
exists a Hilbert space H and a mapping φ : Z →H such that for any pair z, z′ in Z we
have that d(z, z′) = ∥φ(z) −φ(z′)∥H.
Statistical Divergences
For instance, Remark 2.30 shows that the Wasserstein metric is a Hilbert norm
between univariate distributions, simply by deﬁning φ to be the map that associates
to a measure its generalized quantile function. Remark 2.31 shows that for univariate
Gaussians, as written in (8.7) in this chapter, the Wasserstein distance between two
univariate Gaussians is simply the Euclidean distance between their mean and standard
deviation.
Hilbertian distances have many favorable properties when used in a data analysis
context [Dattorro, 2017]. First, they can be easily cast as radial basis function kernels:
for any Hilbertian distance d, it is indeed known that e−dp/t is a positive deﬁnite kernel
for any value 0 ≤p ≤2 and any positive scalar t as shown in [Berg et al., 1984,
Cor. 3.3.3, Prop. 3.2.7]. The Gaussian (p = 2) and Laplace (p = 1) kernels are simple
applications of that result using the usual Euclidean distance. The entire ﬁeld of kernel
methods [Hofmann et al., 2008] builds upon the positive deﬁniteness of a kernel function
to deﬁne convex learning algorithms operating on positive deﬁnite kernel matrices.
Points living in a Hilbertian space can also be eﬃciently embedded in lower dimensions
with low distortion factors [Johnson and Lindenstrauss, 1984], [Barvinok, 2002, §V.6.2]
using simple methods such as multidimensional scaling [Borg and Groenen, 2005].
Because Hilbertian distances have such properties, one might hope that the Wasserstein distance remains Hilbertian in more general settings than those outlined above,
notably when the dimension of X is 2 and more. This can be disproved using the
following equivalence.
Proposition 8.1. A distance d is Hilbertian if and only if d2 is negative deﬁnite.
Proof. If a distance is Hilbertian, then d2 is trivially negative deﬁnite. Indeed, given n
points in Z, the sum P rirjd2(zi, zj) can be rewritten as P rirj∥φ(zi) −φ(zj)∥2
can be expanded, taking advantage of the fact that P ri = 0 to −2 P rirj⟨φ(zi), φ(zj)⟩H
which is negative by deﬁnition of a Hilbert dot product. If, on the contrary, d2 is negative
deﬁnite, then the fact that d is Hilbertian proceeds from a key result by Schoenberg
 outlined in .
It is therefore suﬃcient to show that the squared Wasserstein distance is not negative
deﬁnite to show that it is not Hilbertian, as stated in the following proposition.
Proposition 8.2. If X = Rd with d ≥2 and the ground cost is set to d(x, y) = ∥x −y∥2,
then the p-Wasserstein distance is not Hilbertian for p = 1, 2.
Proof. It suﬃces to prove the result for d = 2 since any counterexample in that dimension suﬃces to obtain a counterexample in any higher dimension. We provide a
nonrandom counterexample which works using measures supported on four vectors
x1, x2, x3, x4 ∈R2 deﬁned as follows: x1 = , x2 = , x3 = , x4 = . We
now consider all points on the regular grid on the simplex of four dimensions, with
8.3. Wasserstein Spaces Are Not Hilbertian
increments of 1/4. There are 35 =
 such points in the simplex. Each
probability vector ai on that grid is such that for j ≤4, we have that ai
j is in the set
4, 1} and such that P4
j = 1. For a given p, the 35 × 35 pairwise Wasserstein distance matrix Dp between these histograms can be computed. Dp is not negative
deﬁnite if and only if its elementwise square D2
p is such that JD2
pJ has positive eigenvalues, where J is the centering matrix J = In −1
n1n,n, which is the case as illustrated
in Figure 8.6.
p parameter to deﬁne p-Wasserstein
Max. Eig. of
Centered Distance Matrix
Figure 8.6: One can show that a distance is not Hilbertian by looking at the spectrum of the centered
matrix JD2
pJ corresponding to the pairwise squared-distance matrix D2
p of a set of points. The spectrum
of such a matrix is necessarily non-positive if the distance is Hilbertian. Here we plot the values of the
maximal eigenvalue of that matrix for points selected in the proof of Proposition 8.2. We do so for
varying values of p, and display the maximal eigenvalues we obtain. These eigenvalues are all positive,
which shows that for all these values of p, the p-Wasserstein distance is not Hilbertian.
Embeddings and Distortion
An important body of work quantiﬁes the hardness of approximating Wasserstein distances using Hilbertian embeddings. It has been shown that embedding measures in
ℓ2 spaces incurs necessarily an important
distortion (Naor and Schechtman ,
Andoni et al. ) as soon as X = Rd with d ≥2.
It is possible to embed quasi-isometrically p-Wasserstein spaces for 0 < p ≤1 in ℓ1
 , but the equivalence constant between the distances grows fast with the dimension d. Note also that
for p = 1 the embedding is true only for discrete measures (i.e. the embedding constant
depends on the minimum distance between the spikes). A closely related embedding
Statistical Divergences
technique consists in using the characterization of W1 as the dual of Lipschitz functions
f (see §6.2) and approximating the Lipschitz constraint ∥∇f∥1 ≤1 by a weighted ℓ1
ball over the wavelets coeﬃcients; see [Shirdhonkar and Jacobs, 2008]. This weighted ℓ1
ball of wavelet coeﬃcients deﬁnes a so-called Besov space of negative index [Leeb and
Coifman, 2016]. These embedding results are also similar to the bound on the Wasserstein distance obtained using dyadic partitions; see [Weed and Bach, 2017, Prop. 1]
and also [Fournier and Guillin, 2015]. This also provides a quasi-isometric embedding
in ℓ1 (this embedding being given by rescaled wavelet coeﬃcients) and comes with the
advantage that this embedding can be computed approximately in linear time when
the input measures are discretized on uniform grids. We refer to [Mallat, 2008] for
more details on wavelets. Note that the idea of using multiscale embeddings to compute Wasserstein-like distances has been used extensively in computer vision; see, for
instance, [Ling and Okada, 2006, Grauman and Darrell, 2005, Cuturi and Fukumizu,
2007, Lazebnik et al., 2006].
Negative/Positive Deﬁnite Variants of Optimal Transport
We show later in §10.4 that the sliced approximation to Wasserstein distances, essentially a sum of 1-D directional transportation distance computed on random pushforwards of measures projected on lines, is negative deﬁnite as the sum of negative
deﬁnite functions [Berg et al., 1984, §3.1.11]. This result can be used to deﬁne a positive deﬁnite kernel [Kolouri et al., 2016]. Another way to recover a positive deﬁnite
kernel is to cast the optimal transport problem as a soft-min problem (over all possible
transportation tables) rather than a minimum, as proposed by Kosowsky and Yuille
 to introduce entropic regularization. That soft-min deﬁnes a term whose negexponential (also known as a generating function) is positive deﬁnite [Cuturi, 2012].
Empirical Estimators for OT, MMD and ϕ-divergences
In an applied setting, given two input measures (α, β) ∈M1
+(X)2, an important statistical problem is to approximate the (usually unknown) divergence D(α, β) using
only samples (xi)n
i=1 from α and (yj)m
j=1 from β. These samples are assumed to be
independently identically distributed from their respective distributions.
Empirical Estimators for OT and MMD
For both Wasserstein distances Wp (see 2.18) and MMD norms (see §8.2), a straightforward estimator of the unknown distance between distriubtions is compute it directly
between the empirical measures, hoping ideally that one can control the rate of con-
8.4. Empirical Estimators for OT, MMD and ϕ-divergences
vergence of the latter to the former,
D(α, β) ≈D(ˆαn, ˆβm)
Note that here both ˆαn and ˆβm are random measures, so D(ˆαn, ˆβm) is a random
number. For simplicity, we assume that X is compact (handling unbounded domain
requires extra constraints on the moments of the input measures).
For such a dual distance that metrizes the weak convergence (see Deﬁnition 2.2),
since there is the weak convergence ˆαn →α, one has D(ˆαn, ˆβn) →D(α, β) as n →+∞.
But an important question is the speed of convergence of D(ˆαn, ˆβn) toward D(α, β),
and this rate is often called the “sample complexity” of D.
Note that for D(α, β) = ∥·∥TV, since the TV norm does not metrize the weak
convergence, ∥ˆαn −ˆβn∥TV is not a consistent estimator, namely it does not converge
toward ∥α −β∥TV. Indeed, with probability 1, ∥ˆαn−ˆβn∥TV = 2 since the support of the
two discrete measures does not overlap. Similar issues arise with other ϕ-divergences,
which cannot be estimated using divergences between empirical distributions.
Rates for OT.
For X = Rd and measure supported on bounded domain, it is shown
by [Dudley, 1969] that for d > 2, and 1 ≤p < +∞,
E(| Wp(ˆαn, ˆβn) −Wp(α, β)|) = O(n−1
where the expectation E is taken with respect to the random samples (xi, yi)i. This
rate is tight in Rd if one of the two measures has a density with respect to the Lebesgue
measure. This result was proved for general metric spaces [Dudley, 1969] using the
notion of covering numbers and was later reﬁned, in particular for X = Rd in [Dereich
et al., 2013, Fournier and Guillin, 2015]. This rate can be reﬁned when the measures are
supported on low-dimensional subdomains: Weed and Bach show that, indeed,
the rate depends on the intrinsic dimensionality of the support. Weed and Bach also
study the nonasymptotic behavior of that convergence, such as for measures which are
discretely approximated (e.g. mixture of Gaussians with small variances). It is also
possible to prove concentration of Wp(ˆαn, ˆβn) around its mean Wp(α, β); see [Bolley
et al., 2007, Boissard, 2011, Weed and Bach, 2017].
Rates for MMD.
For weak norms ∥·∥2
k which are dual of RKHS norms (also called
MMD), as deﬁned in (8.13), and contrary to Wasserstein distances, the sample complexity does not depend on the ambient dimension
E(|∥ˆαn −ˆβn∥k −∥α −β∥k |) = O 2 def.
k(xi, xi′) +
k(yj, yj′)
k(xi, yj),
which should be compared to (8.14). It satisﬁes E(MMDk(ˆαn, ˆβn)2) = ∥α −β∥2
see [Gretton et al., 2012].
Energy distance ∥·∥H−1
Figure 8.7:
Decay of log10(D(ˆαn, ˆα′
n)) as a function of log10(n) for D being the energy distance
D = ∥·∥H−1 (i.e. the H−1 norm) as deﬁned in Example 8.12 (left) and the Wasserstein distance D = W2
(right). Here (ˆαn, ˆα′
n) are two independent empirical distributions of α, the uniform distribution on
the unit cube d, tested for several value of d ∈{2, 3, 5}. The shaded bar displays the conﬁdence
interval at ± the standard deviation of log(D(ˆαn, α)).
Empirical Estimators for ϕ-divergences
It is not possible to approximate Dϕ(α|β), as deﬁned in (8.2), from discrete samples using Dϕ(ˆαn|ˆβn). Indeed, this quantity is either +∞(for instance, for the KL divergence)
or is not converging to Dϕ(α|β) as n →+∞(for instance, for the TV norm). Instead,
it is required to use a density estimator to somehow smooth the discrete empirical
measures and replace them by densities; see [Silverman, 1986]. In a Euclidean space
X = Rd, introducing hσ = h(·/σ) with a smooth windowing function and a bandwidth
σ > 0, a density estimator for α is deﬁned using a convolution against this kernel,
ˆαn ⋆hσ = 1
hσ(· −xi).
One can then approximate the ϕ divergence using
ϕ(ˆαn|ˆβn)
i hσ(yj −xi)
j′ hσ(yj −yj′),
8.5. Entropic Regularization: Between OT and MMD
where σ should be adapted to the number n of samples and to the dimension d. It is also
possible to devise nonparametric estimators, bypassing the choice of a ﬁxed bandwidth
σ to select instead a number k of nearest neighbors. These methods typically make use
of the distance between nearest neighbors [Loftsgaarden and Quesenberry, 1965], which
is similar to locally adapting the bandwidth σ to the local sampling density. Denoting
∆k(x) the distance between x ∈Rd and its kth nearest neighbor among the (xi)n
density estimator is deﬁned as
|Bd|∆k(x)r ,
where |Bd| is the volume of the unit ball in Rd. Instead of somehow “counting” the
number of sample falling in an area of width σ in (8.20), this formula (8.21) estimates
the radius required to encapsulate k samples. Figure 8.8 compares the estimators (8.20)
and (8.21). A typical example of application is detailed in (4.1) for the entropy functional, which is the KL divergence with respect to the Lebesgue measure. We refer
to [Moon and Hero, 2014] for more details.
σ = 2.5 · 10−3
σ = 15 · 10−3
σ = 25 · 10−3
Figure 8.8:
Comparison of kernel density estimation ˆαn ⋆hσ (top, using a Gaussian kernel h) and
k-nearest neighbors estimation ρk
ˆαn (bottom) for n = 200 samples from a mixture of two Gaussians.
Entropic Regularization: Between OT and MMD
Following Proposition 4.7, we recall that the Sinkhorn divergence is deﬁned as
= ⟨P⋆, C⟩= ⟨e
ε , (K ⊙C)e
where P⋆is the solution of (4.2) while (f⋆, g⋆) are solutions of (4.30). Assuming Ci,j =
d(xi, xj)p for some distance d on X, for two discrete probability distributions of the
form (2.3), this deﬁnes a regularized Wasserstein cost
Wp,ε(α, β)p def.
Statistical Divergences
This deﬁnition is generalized to any input distribution (not necessarily discrete) as
Wp,ε(α, β)p def.
d(x, y)pdπ⋆(x, y),
where π⋆is the solution of (4.9).
In order to cancel the bias introduced by the regularization (in particular,
Wp,ε(α, α) ̸= 0), we introduce a corrected regularized divergence
˜Wp,ε(α, β)p def.
= 2 Wp,ε(α, β)p −Wp,ε(α, α)p −Wp,ε(β, β)p.
It is proved in [Feydy et al., 2019] that if e−c/ε is a positive kernel, then a related
corrected divergence (obtained by using Lε
C in place of Pε
C) is positive. Note that it is
possible to deﬁne other renormalization schemes using regularized optimal transport,
as proposed, for instance, by Amari et al. .
Figure 8.9:
Decay of E(log10( ˜
Wp,ε(ˆαn, ˆα′
n))), for p = 3/2 for various ε, as a function of log10(n)
where α is the same as in Figure 8.7.
The following proposition, whose proof can be found in [Ramdas et al., 2017], shows
that this regularized divergence interpolates between the Wasserstein distance and the
energy distance deﬁned in Example 8.12.
Proposition 8.3. One has
˜Wp,ε(α, β) ε→0
−→2 Wp(α, β)
˜Wp,ε(α, β)p ε→+∞
where ∥·∥ED(X,d) is deﬁned in (8.19).
Figure 8.9 shows numerically the impact of ε on the sample complexity rates. It is
proved in Genevay et al. , in the case of c(x, y) = ∥x −y∥2 on X = Rd, that these
rates interpolate between the ones of OT and MMD.
Variational Wasserstein Problems
In data analysis, common divergences between probability measures (e.g. Euclidean,
total variation, Hellinger, Kullback–Leibler) are often used to measure a ﬁtting error or
a loss in parameter estimation problems. Up to this chapter, we have made the case that
the optimal transport geometry has a unique ability, not shared with other information
divergences, to leverage physical ideas (mass displacement) and geometry (a ground
cost between observations or bins) to compare measures. These two facts combined
make it thus very tempting to use the Wasserstein distance as a loss function. This
idea was recently explored for various applied problems. However, the main technical
challenge associated with that idea lies in approximating and diﬀerentiating eﬃciently
the Wasserstein distance. We start this chapter with a few motivating examples and
show how the diﬀerent numerical schemes presented in the ﬁrst chapters of this book
can be used to solve variational Wasserstein problems.
In image processing, the Wasserstein distance can be used as a loss to synthesize
textures [Tartavel et al., 2016], to account for the discrepancy between statistics of
synthesized and input examples. It is also used for image segmentation to account
for statistical homogeneity of image regions [Swoboda and Schnörr, 2013, Rabin and
Papadakis, 2015, Peyré et al., 2012, Ni et al., 2009, Schmitzer and Schnörr, 2013b,
Li et al., 2018b]. The Wasserstein distance is also a very natural ﬁdelity term for inverse problems when the measurements are probability measures, for instance, image
restoration [Lellmann et al., 2014], tomographic inversion [Abraham et al., 2017], density regularization [Burger et al., 2012], particle image velocimetry [Saumier et al.,
2015], sparse recovery and compressed sensing [Indyk and Price, 2011], and seismic
inversion [Métivier et al., 2016]. Distances between measures (mostly kernel-based as
Variational Wasserstein Problems
shown in §8.2.2) are routinely used for shape matching (represented as measures over
a lifted space, often called currents) in computational anatomy [Vaillant and Glaunès,
2005], but OT distances oﬀer an interesting alternative [Feydy et al., 2017]. To reduce the dimensionality of a dataset of histograms, Lee and Seung have shown that the
nonnegative matrix factorization problem can be cast using the Kullback–Leibler divergence to quantify a reconstruction loss [Lee and Seung, 1999]. When prior information
is available on the geometry of the bins of those histograms, the Wasserstein distance
can be used instead, with markedly diﬀerent results [Sandler and Lindenbaum, 2011,
Zen et al., 2014, Rolet et al., 2016].
All of these problems have in common that they require access to the gradients of
Wasserstein distances, or approximations thereof. We start this section by presenting
methods to approximate such gradients, then follow with three important applications
that can be cast as variational Wasserstein problems.
Diﬀerentiating the Wasserstein Loss
In statistics, text processing or imaging, one must usually compare a probability distribution β arising from measurements to a model, namely a parameterized family of
distributions {αθ, θ ∈Θ}, where Θ is a subset of a Euclidean space. Such a comparison
is done through a “loss” or a “ﬁdelity” term, which is the Wasserstein distance in this
section. In the simplest scenario, the computation of a suitable parameter θ is obtained
by minimizing directly
= Lc(αθ, β).
Of course, one can consider more complicated problems: for instance, the barycenter
problem described in §9.2 consists in a sum of such terms. However, most of these more
advanced problems can be usually solved by adapting tools deﬁned for the basic case
above, either using the chain rule to compute explicitly derivatives or using automatic
diﬀerentiation as advocated in §9.1.3.
Convexity.
The Wasserstein distance between two histograms or two densities is convex with respect to its two inputs, as shown by (2.20) and (2.24), respectively. Therefore,
when the parameter θ is itself a histogram, namely Θ = Σn and αθ = θ, or more generally when θ describes K weights in the simplex, Θ = ΣK, and αθ = PK
i=1 θiαi is a
convex combination of known atoms α1, . . . , αK in ΣN, Problem (9.1) remains convex
 . However,
for more general parameterizations θ 7→αθ, Problem (9.1) is in general not convex.
9.1. Diﬀerentiating the Wasserstein Loss
Simple cases.
For those simple cases where the Wasserstein distance has a closed
form, such as univariate (see §2.30) or elliptically contoured (see §2.31) distributions,
simple workarounds exist. They consist mostly in casting the Wasserstein distance as
a simpler distance between suitable representations of these distributions (Euclidean
on quantile functions for univariate measures, Bures metric for covariance matrices
for elliptically contoured distributions of the same family) and solving Problem (9.1)
directly on such representations.
In most cases, however, one has to resort to a careful discretization of αθ to compute a local minimizer for Problem (9.1). Two approaches can be envisioned: Eulerian
or Lagrangian. Figure 9.1 illustrates the diﬀerence between these two fundamental discretization schemes. At the risk of oversimplifying this argument, one may say that
a Eulerian discretization is the most suitable when measures are supported on a lowdimensional space (as when dealing with shapes or color spaces), or for intrinsically
discrete problems (such as those arising from string or text analysis). When applied
to ﬁtting problems where observations can take continuous values in high-dimensional
spaces, a Lagrangian perspective is usually the only suitable choice.
Figure 9.1:
Increasing ﬁne discretization of a continuous distribution having a density (violet, left)
using a Lagrangian representation
i δxi (blue, top) and an Eulerian representation P
i aiδxi with
xi representing cells on a grid of increasing size (red, bottom). The Eulerian perspective starts from
a pixelated image down to one with such ﬁne resolution that it almost matches the original density.
Weights ai are directly proportional to each pixel-cell’s intensity.
Eulerian Discretization
A ﬁrst way to discretize the problem is to suppose that both distributions β =
j=1 bjδyj and αθ = Pn
i=1 a(θ)iδxi are discrete distributions deﬁned on ﬁxed locations (xi)i and (yj)j. Such locations might stand for cells dividing the entire space of
observations in a grid, or a ﬁnite subset of points of interest in a continuous space . The parameterized measure αθ is in that case entirely represented
through the weight vector a : θ 7→a(θ) ∈Σn, which, in practice, might be very sparse
if the grid is large. This setting corresponds to the so-called class of Eulerian discretization methods. In its original form, the objective of Problem (9.1) is not diﬀerentiable.
In order to obtain a smooth minimization problem, we use the entropic regularized OT
and approximate (9.1) using
C(a(θ), b)
= c(xi, yj).
We recall that Proposition 4.6 shows that the entropic loss function is diﬀerentiable
and convex with respect to the input histograms, with gradient.
Proposition 9.1 (Derivative with respect to histograms). For ε > 0, (a, b) 7→Lε
is convex and diﬀerentiable. Its gradient reads
C(a, b) = (f, g),
where (f, g) is the unique solution to (4.30), centered such that P
j gj = 0. For
ε = 0, this formula deﬁnes the elements of the sub-diﬀerential of Lε
C, and the function
is diﬀerentiable if they are unique.
The zero mean condition on (f, g) is important when using gradient descent to
guarantee conservation of mass. Using the chain rule, one thus obtains that EE is
smooth and that its gradient is
∇EE(θ) = [∂a(θ)]⊤(f ),
where ∂a(θ) ∈Rn×dim(Θ) is the Jacobian (diﬀerential) of the map a(θ), and where
f ∈Rn is the dual potential vector associated to the dual entropic OT (4.30) between
a(θ) and b for the cost matrix C (which is ﬁxed in a Eulerian setting, and in particular
independent of θ). This result can be used to minimize locally EE through gradient
Lagrangian Discretization
A diﬀerent approach consists in using instead ﬁxed (typically uniform) weights and
approximating an input measure α as an empirical measure αθ =
i δx(θ)i for a
point-cloud parameterization map x : θ 7→x(θ) = (x(θ)i)n
i=1 ∈X n, where we assume
here that X is Euclidean. Problem (9.1) is thus approximated as
C(x(θ))(1n/n, b)
= c(x(θ)i, yj).
Note that here the cost matrix C(x(θ)) now depends on θ since the support of αθ
changes with θ. The following proposition shows that the entropic OT loss is a smooth
function of the cost matrix and gives the expression of its gradient.
9.1. Diﬀerentiating the Wasserstein Loss
Proposition 9.2 (Derivative with respect to the cost). For ﬁxed input histograms (a, b),
for ε > 0, the mapping C 7→R(C)
C(a, b) is concave and smooth, and
∇R(C) = P,
where P is the unique optimal solution of (4.2). For ε = 0, this formula deﬁnes the set
of upper gradients.
Assuming (X, Y) are convex subsets of Rd, for discrete measures (α, β) of the
form (2.3), one obtains using the chain rule that x = (xi)n
i=1 ∈X n 7→F(x)
LC(x)(1n/n, b) is smooth and that
Pi,j∇1c(xi, yj)
where ∇1c is the gradient with respect to the ﬁrst variable. For instance, for X = Y =
Rd, for c(s, t) = ∥s −t∥2 on X = Y = Rd, one has
where ai = 1/n here. Note that, up to a constant, this gradient is Id −T, where T is
the barycentric projection deﬁned in (4.19). Using the chain rule, one thus obtains that
the Lagrangian discretized problem (9.4) is smooth and its gradient is
∇EL(θ) = [∂x(θ)]⊤(∇F(x(θ))),
where ∂x(θ) ∈Rdim(Θ)×(nd) is the Jacobian of the map x(θ) and where ∇F is implemented as in (9.6) or (9.7) using for P the optimal coupling matrix between αθ and
β. One can thus implement a gradient descent to compute a local minimizer of EL, as
used, for instance, in [Cuturi and Doucet, 2014].
Automatic Diﬀerentiation
The diﬃculty when applying formulas (9.3) and (9.8) is that one needs to compute
the exact optimal solutions f or P for these formulas to be valid, which can only be
achieved with acceptable precision using a very large number of Sinkhorn iterates. In
challenging situations in which the size and the quantity of histograms to be compared
are large, the computational budget to compute a single Wasserstein distance is usually
limited, therefore allowing only for a few Sinkhorn iterations. In that case, and rather
than approximating the gradient (4.30) using the value obtained at a given iterate,
it is usually better to diﬀerentiate directly the output of Sinkhorn’s algorithm, using
reverse mode automatic diﬀerentiation. This corresponds to using the “algorithmic”
Variational Wasserstein Problems
Sinkhorn divergences as introduced in (4.48), rather than the quantity Lε
C in (4.2)
which incorporates the entropy of the regularized optimal transport, and diﬀerentiating
it directly as a composition of simple maps using the inputs, either the histogram in the
Eulerian case or the cost matrix in the Lagrangian cases. Using deﬁnitions introduced
in §4.5, this is equivalent to diﬀerentiating
C (a(θ), b)
C(x(θ))(a, b)
with respect to θ, in, respectively, the Eulerian and the Lagrangian cases for L large
The cost for computing the gradient of functionals involving Sinkhorn divergences
is the same as that of computation of the functional itself; see, for instance, [Bonneel et al., 2016, Genevay et al., 2018] for some applications of this approach. We also
refer to [Adams and Zemel, 2011] for an early work on diﬀerentiating Sinkhorn iterations with respect to the cost matrix (as done in the Lagrangian framework), with
applications to learning rankings. Further details on automatic diﬀerentiation can be
found in [Griewank and Walther, 2008, Rall, 1981, Neidinger, 2010], in particular on the
“reverse mode,” which is the fastest way to compute gradients. In terms of implementation, all recent deep-learning Python frameworks feature state-of-the-art reverse-mode
diﬀerentiation and support for GPU/TPU computations [Al-Rfou et al., 2016, Abadi
et al., 2016, Pytorch, 2017], they should be adopted for any large-scale application
of Sinkhorn losses. We strongly encourage the use of such automatic diﬀerentiation
techniques, since they have the same complexity as computing (9.3) and (9.8), these
formulas being mostly useful to obtain a theoretical understanding of what automatic
diﬀerentation is computing. The only downside is that reverse mode automatic differentation is memory intensive (the memory grows proportionally with the number
of iterations). There exist, however, subsampling strategies that mitigate this problem [Griewank, 1992].
Wasserstein Barycenters, Clustering and Dictionary Learning
A basic problem in unsupervised learning is to compute the “mean” or “barycenter” of
several data points. A classical way to deﬁne such a weighted mean of points (xs)S
X S living in a metric space (X, d) (where d is a distance or more generally a divergence)
is by solving a variational problem
λsd(x, xs)p
for a given family of weights (λs)s ∈ΣS, where p is often set to p = 2. When X = Rd and
d(x, y) = ∥x −y∥2, this leads to the usual deﬁnition of the linear average x = P
9.2. Wasserstein Barycenters, Clustering and Dictionary Learning
for p = 2 and the more evolved median point when p = 1. One can retrieve various
notions of means (e.g. harmonic or geometric means over X = R+) using this formalism.
This process is often referred to as the “Fréchet” or “Karcher” mean (see Karcher
 for a historical account). For a generic distance d, Problem (9.9) is usually a
diﬃcult nonconvex optimization problem. Fortunately, in the case of optimal transport
distances, the problem can be formulated as a convex program for which existence can
be proved and eﬃcient numerical schemes exist.
Fréchet means over the Wasserstein space.
Given input histogram {bs}S
s=1, where
bs ∈Σns, and weights λ ∈ΣS, a Wasserstein barycenter is computed by minimizing
λsLCs(a, bs),
where the cost matrices Cs ∈Rn×ns need to be speciﬁed. A typical setup is “Eulerian,”
so that all the barycenters are deﬁned on the same grid, ns = n, Cs = C = Dp is set
to be a distance matrix, to solve
The barycenter problem (9.10) was introduced in a more general form involving
arbitrary measures in Agueh and Carlier following earlier ideas of Carlier and
Ekeland . That presentation is deferred to Remark 9.1. The barycenter problem
for histograms (9.10) is in fact a linear program, since one can look for the S couplings
(Ps)s between each input and the barycenter itself, which by construction must be
constrained to share the same row marginal,
a∈Σn,(Ps∈Rn×ns)s
λs⟨Ps, Cs⟩: ∀s, P⊤
s 1ns = a, P⊤
Although this problem is an LP, its scale forbids the use of generic solvers for mediumscale problems. One can resort to using ﬁrst order methods such as subgradient descent
on the dual [Carlier et al., 2015].
Remark 9.1 (Barycenter of arbitrary measures). Given a set of input measure (βs)s
deﬁned on some space X, the barycenter problem becomes
λsLc(α, βs).
In the case where X = Rd and c(x, y) = ∥x −y∥2, Agueh and Carlier show
that if one of the input measures has a density, then this barycenter is unique.
Variational Wasserstein Problems
Problem (9.11) can be viewed as a generalization of the problem of computing
barycenters of points (xs)S
s=1 ∈X S to arbitrary measures. Indeed, if βs = δxs is
a single Dirac mass, then a solution to (9.11) is δx⋆, where x⋆is a Fréchet mean
solving (9.9). Note that for c(x, y) = ∥x −y∥2, the mean of the barycenter α⋆is
necessarily the barycenter of the mean, i.e.
and the support of α⋆is located in the convex hull of the supports of the (αs)s. The
consistency of the approximation of the inﬁnite-dimensional optimization (9.11)
when approximating the input distribution using discrete ones (and thus solving (9.10) in place) is studied in Carlier et al. . Let us also note that it is
possible to recast (9.11) as a multimarginal OT problem; see Remark 10.2.
Remark 9.2 (k-means as a Wasserstein variational problem). When the family of
input measures (βs)s is limited to but one measure β, this measure is supported on
a discrete ﬁnite subset of X = Rd, and the cost is the squared Euclidean distance,
then one can show that the barycenter problem
k(X) Lc(α, β),
where α is constrained to be a discrete measure with a ﬁnite support of size up
to k, is equivalent to the usual k-means problem taking β. Indeed, one can easily
show that the centroids output by the k-means problem correspond to the support
of the solution α and that its weights correspond to the fraction of points in β
assigned to each centroid. One can show that approximating Lc using entropic
regularization results in smoothed out assignments that appear in soft-clustering
variants of k-means, such as mixtures of Gaussians [Dessein et al., 2017].
Remark 9.3 (Distribution of distributions and consistency). It is possible to generalize (9.11) to a possibly inﬁnite collection of measures. This problem is described
by considering a probability distribution M over the space M1
+(X) of probability
distributions, i.e. M ∈M1
+(X)). A barycenter is then a solution of
+(X) EM(Lc(α, β)) =
Lc(α, β)dM(β),
where β is a random measure distributed according to M. Drawing uniformly at
random a ﬁnite number S of input measures (βs)S
s=1 according to M, one can then
deﬁne ˆβS as being a solution of (9.11) for uniform weights λs = 1/S (note that
9.2. Wasserstein Barycenters, Clustering and Dictionary Learning
here ˆβS is itself a random measure). Problem (9.11) corresponds to the special case
of a “discrete” measure M = P
s λsδβs. The convergence (in expectation or with
high probability) of Lc(ˆβS, α) to zero (where α is the unique solution to (9.13))
corresponds to the consistency of the barycenters, and is proved in [Bigot and
Klein, 2012a, Le Gouic and Loubes, 2016, Bigot and Klein, 2012b]. This can be
interpreted as a law of large numbers over the Wasserstein space. The extension of
this result to a central limit theorem is an important problem; see [Panaretos and
Zemel, 2016] and [Agueh and Carlier, 2017] for recent formulations of that problem
and solutions in particular cases (1-D distributions and Gaussian measures).
Remark 9.4 (Fixed-point map). When dealing with the Euclidean space X = Rd
with ground cost c(x, y) = ∥x −y∥2, it is possible to study the barycenter problem
using transportation maps. Indeed, if α has a density, according to Remark 2.24,
one can deﬁne optimal transportation maps Ts between α and αs, in particular
such that Ts,♯α = αs. The average map
T (α) def.
(the notation above makes explicit the dependence of this map on α) is itself an
optimal map between α and T (α)
α (a positive combination of optimal maps is
equal by Brenier’s theorem, Remark 2.24, to the sum of gradients of convex functions, equal to the gradient of a sum of convex functions, and therefore optimal
by Brenier’s theorem again). As shown in [Agueh and Carlier, 2011], ﬁrst order
optimality conditions of the barycenter problem (9.13) actually read T (α⋆) = IRd
(the identity map) at the optimal measure α⋆(the barycenter), and it is shown
in [Álvarez-Esteban et al., 2016] that the barycenter α⋆is the unique to the ﬁxed-point
Under mild conditions on the input measures, Álvarez-Esteban et al. 
and Zemel and Panaretos have shown that α 7→G(α) strictly decreases
the objective function of (9.13) if α is not the barycenter and that the ﬁxed-point
iterations α(ℓ+1)
= G(α(ℓ)) converge to the barycenter α⋆. This ﬁxed point algorithm can be used in cases where the optimal transportation maps are known
in closed form (e.g. for Gaussians). Adapting this algorithm for empirical measures of the same size results in computing optimal assignments in place of Monge
maps. For more general discrete measures of arbitrary size the scheme can also be
Variational Wasserstein Problems
adapted [Cuturi and Doucet, 2014] using barycentric projections (4.19).
Special cases.
In general, solving (9.10) or (9.11) is not straightforward, but there
exist some special cases for which solutions are explicit or simple.
Remark 9.5 (Barycenter of Gaussians). It is shown in [Agueh and Carlier, 2011]
that the barycenter of Gaussians distributions αs = N(ms, Σs), for the squared
Euclidean cost c(x, y) = ∥x −y∥2, is itself a Gaussian N(m⋆, Σ⋆). Making use
of (2.41), one sees that the barycenter mean is the mean of the inputs
while the covariance minimizes
λsB(Σ, Σs)2,
where B is the Bure metric (2.42). As studied in [Agueh and Carlier, 2011], the
ﬁrst order optimality condition of this convex problem shows that Σ⋆is the unique
positive deﬁnite ﬁxed point of the map
2 is the square root of positive semideﬁnite matrices. This result was
known from [Knott and Smith, 1994, Rüschendorf and Uckelmann, 2002] and is
proved in [Agueh and Carlier, 2011]. While Ψ is not strictly contracting, iterating
this ﬁxed-point map, i.e. deﬁning Σ(ℓ+1)
= Ψ(Σ(ℓ)) converges in practice to the
solution Σ⋆. This method has been applied to texture synthesis in [Xia et al., 2014].
Álvarez-Esteban et al. have also proposed to use an alternative map
for which the iterations Σ(ℓ+1) def.
= ¯Ψ(Σ(ℓ)) converge. This is because the ﬁxed-point
map G deﬁned in (9.14) preserves Gaussian distributions, and in fact,
G(N(m, Σ)) = N(m⋆, ¯Ψ(Σ)).
Figure 9.2 shows two examples of computations of barycenters between four 2-D
Gaussians.
9.2. Wasserstein Barycenters, Clustering and Dictionary Learning
Figure 9.2: Barycenters between four Gaussian distributions in 2-D. Each Gaussian is displayed using
an ellipse aligned with the principal axes of the covariance, and with elongations proportional to the
corresponding eigenvalues.
Remark 9.6 (1-D cases). For 1-D distributions, the Wp barycenter can be computed almost in closed form using the fact that the transport is the monotone
rearrangement, as detailed in Remark 2.30. The simplest case is for empirical measures with n points, i.e. βs =
i=1 δys,i, where the points are assumed to be
sorted ys,1 ≤ys,2 ≤. . .. Using (2.33) the barycenter αλ is also an empirical measure on n points
xλ,i = Aλ(xs,i)s,
where Aλ is the barycentric map
λs|x −xs|p.
For instance, for p = 2, one has xλ,i = PS
s=1 λsxs,i. In the general case, one needs
to use the cumulative functions as deﬁned in (2.34), and using (2.36), one has
∀r ∈ ,
αλ (r) = Aλ(C−1
which can be used, for instance, to compute barycenters between discrete measures
supported on less than n points in O(n log(n)) operations, using a simple sorting
procedure.
Remark 9.7 (Simple cases). Denoting by Tr,u : x 7→rx + u a scaling and translation, and assuming that αs = Trs,us,♯α0 is obtained by scaling and translating an
initial template measure, then a barycenter αλ is also obtained using scaling and
Variational Wasserstein Problems
translation
αλ = Tr⋆,u⋆,♯α0
s λs/rs)−1,
Remark 9.8 (Case S = 2). In the case where X = Rd and c(x, y) = ∥x −y∥2 (this
can be extended more generally to geodesic spaces), the barycenter between S =
2 measures (α0, α1) is the McCann interpolant as already introduced in (7.6).
Denoting T♯α0 = α1 the Monge map, one has that the barycenter αλ reads αλ =
(λ1Id + λ2T)♯α0. Formula (7.9) explains how to perform the computation in the
discrete case.
Entropic approximation of barycenters.
One can use entropic smoothing and approximate the solution of (9.10) using
for some ε > 0. This is a smooth convex minimization problem, which can be tackled
using gradient descent [Cuturi and Doucet, 2014, Gramfort et al., 2015]. An alternative
is to use descent methods (typically quasi-Newton) on the semi-dual [Cuturi and Peyré,
2016], which is useful to integrate additional regularizations on the barycenter, to impose, for instance, some smoothness w.r.t a given norm. A simpler yet very eﬀective
approach, as remarked by Benamou et al. is to rewrite (9.15) as a (weighted) KL
projection problem
λsεKL(Ps|Ks) : ∀s, PsT1m = bs, P111 = · · · = PS1S,
where we denoted Ks
= e−Cs/ε. Here, the barycenter a is implicitly encoded in the
row marginals of all the couplings Ps ∈Rn×ns as a = P111 = · · · = PS1S. As detailed
by Benamou et al. , one can generalize Sinkhorn to this problem, which also
corresponds to iterative projections. This can also be seen as a special case of the
generalized Sinkhorn detailed in §4.6. The optimal couplings (Ps)s solving (9.16) are
computed in scaling form as
Ps = diag(us)K diag(vs),
9.2. Wasserstein Barycenters, Clustering and Dictionary Learning
and the scalings are sequentially updated as
∀s ∈J1, SK,
∀s ∈J1, SK,
a(ℓ+1) def.
An alternative way to derive these iterations is to perform alternate minimization on
the variables of a dual problem, which is detailed in the following proposition.
Proposition 9.1. The optimal (us, vs) appearing in (9.17) can be written as (us, vs) =
(efs/ε, egs/ε), where (fs, gs)s are the solutions of the following program (whose value
matches the one of (9.15)):
⟨gs, bs⟩−ε⟨Ksegs/ε, efs/ε⟩
Proof. Introducing Lagrange multipliers in (9.16) leads to
(Ps)s,a max
εKL(Ps|Ks) + ⟨a −Ps1m, fs⟩
+⟨bs −PsT1m, gs⟩
Strong duality holds, so that one can exchange the min and the max, to obtain
⟨gs, bs⟩+ min
Ps εKL(Ps|Ks) −⟨Ps, fs ⊕gs⟩
The explicit minimization on a gives the constraint P
s λsfs = 0 together with
λs⟨gs, bs⟩−εKL∗
where KL∗(·|Ks) is the Legendre transform (4.54) of the function KL∗(·|Ks). This
Legendre transform reads
KL∗(U|K) =
Ki,j(eUi,j −1),
which shows the desired formula. To show (9.22), since this function is separable, one
needs to compute
∀(u, k) ∈R2
ur −(r log(r/k) −r + k)
whose optimality condition reads u = log(r/k), i.e. r = keu, hence the result.
Variational Wasserstein Problems
Minimizing (9.21) with respect to each gs, while keeping all the other variables
ﬁxed, is obtained in closed form by (9.18). Minimizing (9.21) with respect to all the
(fs)s requires us to solve for a using (9.20) and leads to the expression (9.19).
Figures 9.3 and 9.4 show applications to 2-D and 3-D shapes interpolation. Figure 9.5 shows a computation of barycenters on a surface, where the ground cost is the
square of the geodesic distance. For this ﬁgure, the computations are performed using the geodesic in heat approximation detailed in Remark 4.19. We refer to [Solomon
et al., 2015] for more details and other applications to computer graphics and imaging
Figure 9.3: Barycenters between four input 2-D shapes using entropic regularization (9.15). To display
a binary shape, the displayed images shows a thresholded density. The weights (λs)s are bilinear with
respect to the four corners of the square.
The eﬃcient computation of Wasserstein barycenters remains at this time an active
research topic [Staib et al., 2017a, Dvurechenskii et al., 2018]. Beyond their methodological interest, Wasserstein barycenters have found many applications outside the ﬁeld
of shape analysis. They have been used for image processing [Rabin et al., 2011], in
particular color modiﬁcation [Solomon et al., 2015] (see Figure 9.6); Bayesian computations [Srivastava et al., 2015a,b] to summarize measures; and nonlinear dimensionality
reduction, to express an input measure as a Wasserstein barycenter of other known
measures [Bonneel et al., 2016]. All of these problems result in involved nonconvex
objective functions which can be accurately optimized using automatic diﬀerentiation
(see Remark 9.1.3). Problems closely related to the computation of barycenters include
the computation of principal components analyses over the Wasserstein space (see, for
9.2. Wasserstein Barycenters, Clustering and Dictionary Learning
Figure 9.4:
Barycenters between four input 3-D shapes using entropic regularization (9.15). The
weights (λs)s are bilinear with respect to the four corners of the square. Shapes are represented as
measures that are uniform within the boundaries of the shape and null outside.
instance, [Seguy and Cuturi, 2015, Bigot et al., 2017b]) and the statistical estimation
of template models [Boissard et al., 2015]. The ability to compute barycenters enables
more advanced clustering methods such as the k-means on the space of probability
measures [del Barrio et al., 2016, Ho et al., 2017].
Figure 9.5: Barycenters interpolation between two input measures on surfaces, computed using the
geodesic in heat fast kernel approximation (see Remark 4.19). Extracted from [Solomon et al., 2015].
Remark 9.9 (Wasserstein propagation). As studied in Solomon et al. [2014b], it is
possible to generalize the barycenter problem (9.10), where one looks for distributions (bu)u∈U at some given set U of nodes in a graph G given a set of ﬁxed input
distributions (bv)v∈V on the complementary set V of the nodes. The unknown are
determined by minimizing the overall transportation distance between all pairs of
Variational Wasserstein Problems
Figure 9.6: Interpolation between the two 3-D color empirical histograms of two input images (here
only the 2-D chromatic projection is visualized for simplicity). The modiﬁed histogram is then applied
to the input images using barycentric projection as detailed in Remark 4.11. Extracted from [Solomon
et al., 2015].
nodes (r, s) ∈G forming edges in the graph
(bu∈Σnu)u∈U
LCr,s(br, bs),
where the cost matrices Cr,s ∈Rnr×ns need to be speciﬁed by the user. The
barycenter problem (9.10) is a special case of this problem where the considered
graph G is “star shaped,” where U is a single vertex connected to all the other
vertices V (the weight λs associated to bs can be absorbed in the cost matrix).
Introducing explicitly a coupling Pr,s ∈U(br, bs) for each edge (r, s) ∈G, and
using entropy regularization, one can rewrite this problem similarly as in (9.16),
and one extends Sinkhorn iterations (9.18) to this problem (this can also be derived by recasting this problem in the form of the generalized Sinkhorn algorithm
detailed in §4.6). This discrete variational problem (9.23) on a graph can be generalized to deﬁne a Dirichlet energy when replacing the graph by a continuous domain [Solomon et al., 2013]. This in turn leads to the deﬁnition of measure-valued
harmonic functions which ﬁnds application in image and surface processing. We
refer also to Lavenant for a theoretical analysis and to Vogt and Lellmann
 for extensions to nonquadratic (total-variation) functionals and applications
to imaging.
9.3. Gradient Flows
Gradient Flows
Given a smooth function a 7→F(a), one can use the standard gradient descent
a(ℓ+1) def.
= a(ℓ) −τ∇F(a(ℓ)),
where τ is a small enough step size. This corresponds to a so-called “explicit” minimization scheme and only applies for smooth functions F. For nonsmooth functions, one
can use instead an “implicit” scheme, which is also called the proximal-point algorithm
(see, for instance, Bauschke and Combettes )
a(ℓ+1) def.
2 + τF(a).
Note that this corresponds to the Euclidean proximal operator, already encountered
in (7.13). The update (9.24) can be understood as iterating the explicit operator Id −
τ∇F, while (9.25) makes use of the implicit operator (Id + τ∇F)−1. For convex F,
iterations (9.25) always converge, for any value of τ > 0.
If the function F is deﬁned on the simplex of histograms Σn, then it makes sense to
use an optimal transport metric in place of the ℓ2 norm ∥·∥in (9.25), in order to solve
a(ℓ+1) def.
Wp(a, a(ℓ))p + τF(a).
Remark 9.10 (Wasserstein gradient ﬂows). Equation (9.26) can be generalized to
arbitrary measures by deﬁning the iteration
α(ℓ+1) def.
Wp(α, α(ℓ))p + τF(α)
for some function F deﬁned on M1
+(X). This implicit time stepping is a useful tool
to construct continuous ﬂows, by formally taking the limit τ →0 and introducing
the time t = τℓ, so that α(ℓ) is intended to approximate a continuous ﬂow t ∈
R+ 7→αt. For the special case p = 2 and X = Rd, a formal calculus shows that αt
is expected to solve a PDE of the form
∂t = div(αt∇(F ′(αt))),
where F ′(α) denotes the derivative of the function F in the sense that it is a
continuous function F ′(α) ∈C(X) such that
F(α + εξ) = F(α) + ε
F ′(α)dξ(x) + o(ε).
A typical example is when using F = −H, where H(α) = KL(α|LRd) is the relative
Variational Wasserstein Problems
entropy with respect to the Lebesgue measure LRd on X = Rd
Rd ρα(x)(log(ρα(x)) −1)dx
(setting H(α) = −∞when α does not have a density), then (9.28) shows that the
gradient ﬂow of this neg-entropy is the linear heat diﬀusion
where ∆is the spatial Laplacian. The heat diﬀusion can therefore be interpreted
either as the “classical” Euclidian ﬂow (somehow performing “vertical” movements
with respect to mass amplitudes) of the Dirichlet energy
Rd ∥∇ρα(x)∥2dx or, alternatively, as the entropy for the optimal transport ﬂow (somehow a “horizontal”
movement with respect to mass positions). Interest in Wasserstein gradient ﬂows
was sparked by the seminal paper of Jordan, Kinderlehrer and Otto [Jordan et al.,
1998], and these evolutions are often called “JKO ﬂows” following their work. As
shown in detail in the monograph by Ambrosio et al. , JKO ﬂows are a
special case of gradient ﬂows in metric spaces. We also refer to the recent survey
paper [Santambrogio, 2017]. JKO ﬂows can be used to study in particular nonlinear evolution equations such as the porous medium equation [Otto, 2001], total
variation ﬂows [Carlier and Poon, 2019], quantum drifts [Gianazza et al., 2009],
or heat evolutions on manifolds [Erbar, 2010]. Their ﬂexible formalism allows for
constraints on the solution, such as the congestion constraint (an upper bound on
the density at any point) that Maury et al. used to model crowd motion [Maury
et al., 2010] .
Remark 9.11 (Gradient ﬂows in metric spaces). The implicit stepping (9.27) is a
special case of a more general formalism to deﬁne gradient ﬂows over metric spaces
(X, d), where d is a distance, as detailed in [Ambrosio et al., 2006]. For some function F(x) deﬁned for x ∈X, the implicit discrete minmization step is then deﬁned
x(ℓ+1) ∈argmin
d(x(ℓ), x)2 + τF(x).
The JKO step (9.27) corresponds to the use of the Wasserstein distance on the
space of probability distributions. In some cases, one can show that (9.31) admits
a continuous ﬂow limit xt as τ →0 and kτ = t. In the case that X also has a
Euclidean structure, an explicit stepping is deﬁned by linearizing F
x(ℓ+1) = argmin
d(x(ℓ), x)2 + τ⟨∇F(x(ℓ)), x⟩.
9.3. Gradient Flows
In sharp contrast to the implicit formula (9.31) it is usually straightforward to
compute but can be unstable. The implicit step is always stable, is also deﬁned for
nonsmooth F, but is usually not accessible in closed form. Figure 9.7 illustrates
this concept on the function F(x) = ∥x∥2 on X = R2 for the distances d(x, y) =
∥x −y∥p = (|x1−y1|p+|x2−y2|p)
p for several values of p. The explicit scheme (9.32)
is unstable for p = 1 and p = +∞, and for p = 1 it gives axis-aligned steps
(coordinatewise descent). In contrast, the implicit scheme (9.31) is stable. Note in
particular how, for p = 1, when the two coordinates are equal, the following step
operates in the diagonal direction.
Figure 9.7: Comparison of explicit and implicit gradient ﬂow to minimize the function f(x) = ∥x∥2
on X = R2 for the distances d(x, y) = ∥x −y∥p for several values of p.
Remark 9.12 (Lagrangian discretization using particles systems). The
ﬁnitedimensional problem in (9.26) can be interpreted as the Eulerian discretization
of a ﬂow over the space of measures (9.27). An alternative way to discretize
the problem, using the so-called Lagrangian method using particles systems, is
to parameterize instead the solution as a (discrete) empirical measure moving
with time, where the locations of that measure (and not its weights) become
the variables of interest. In practice, one can consider a dynamic point cloud
of particles αt =
i=1 δxi(t) indexed with time. The initial problem (9.26) is
then replaced by a set of n coupled ODE prescribing the dynamic of the points
X(t) = (xi(t))i ∈X n. If the energy F is ﬁnite for discrete measures, then one
can simply deﬁne F(X) = F( 1
i=1 δxi). Typical examples are linear functions
X V (x)dα(x) and quadratic interactions F(α) =
X 2 W(x, y)dα(x)dα(y),
in which case one can use respectively
W(xi, xj).
Variational Wasserstein Problems
For functions such as generalized entropy, which are only ﬁnite for measures having
densities, one should apply a density estimator to convert the point cloud into a
density, which allows us to also deﬁne function F(x) consistent with F as n →+∞.
A typical example is for the entropy F(α) = H(α) deﬁned in (9.29), for which a
consistent estimator (up to a constant term) can be obtained by summing the
logarithms of the distances to nearest neighbors
log(dX(xi))
x′∈X,x′̸=x
see Beirlant et al. for a review of nonparametric entropy estimators. For
small enough step sizes τ, assuming X
= Rd, the Wasserstein distance W2
matches the Euclidean distance on the points, i.e. if |t −t′| is small enough,
W2(αt, αt′) = ∥X(t) −X(t′)∥. The gradient ﬂow is thus equivalent to the Euclidean
ﬂow on positions X′(t) = −∇F(X(t)), which is discretized for times tk = τk similarly to (9.24) using explicit Euler steps
X(ℓ+1) def.
= X(ℓ) −τ∇F(X(ℓ)).
Figure 9.8 shows an example of such a discretized explicit evolution for a linear plus
entropy functional, resulting in a discretized version of a Fokker–Planck equation.
Note that for this particular case of linear Fokker–Planck equation, it is possible
also to resort to stochastic PDEs methods, and it can be approximated numerically
by evolving a single random particle with a Gaussian drift. The convergence of
these schemes (so-called Langevin Monte Carlo) to the stationary distribution can
in turn be quantiﬁed in terms of Wasserstein distance; see, for instance, [Dalalyan
and Karagulyan, 2017].
If the function F is not smooth, one should discretize
similarly to (9.25) using implicit Euler steps, i.e. consider
X(ℓ+1) def.
2 + τF(Z).
In the simplest case of a linear function F(α) =
X V (x)dα(x), the ﬂow operates
independently over each particule xi(t) and corresponds to a usual Euclidean ﬂow
for the function V , x′
i(t) = −∇V (xi(t)) (and is an advection PDEs of the density
along the integral curves of the ﬂow).
Remark 9.13 (Geodesic convexity). An important concept related to gradient ﬂows
is the convexity of the functional F with respect to the Wasserstein-2 geometry, i.e.
the convexity of F along Wasserstein geodesics (i.e. displacement interpolations as
shown in Remark 7.1). The Wasserstein gradient ﬂow (with a continuous time) for
9.3. Gradient Flows
Figure 9.8:
Example of gradient ﬂow evolutions using a Lagrangian discretization, for the function
V dα−H(α), for V (x) = ∥x∥2. The entropy is discretized using (9.33). The limiting stationary
distribution is a Gaussian.
such a function exists, is unique, and is the limit of the discrete stepping (9.27)
as τ →0. It converges to a ﬁxed stationary distribution as t →+∞. The entropy
is a typical example of geodesically convex function, and so are linear functions
of the form F(α) =
X V (x)dα(x) and quadratic interaction functions F(α) =
X×X W(x, y)dα(x)dα(y) for convex functions V : X →R, W : X × X →R. Note
that while linear functions are convex in the classical sense, quadratic interaction
functions might fail to be. A typical example is W(x, y) = ∥x −y∥2, which is a
negative semi-deﬁnite kernel (see Deﬁnition 8.3) and thus corresponds to F(α)
being a concave function in the usual sense (while it is geodesically convex). An
important result of McCann is that generalized “entropy” functions of the
form F(α) =
Rd ϕ(ρα(x))dx on X = Rd are geodesically convex if ϕ is convex,
with ϕ(0) = 0, ϕ(t)/t →+∞as t →+∞and such that s 7→sdϕ(s−d) is convex
There is important literature on the numerical resolution of the resulting discretized
ﬂow, and we give only a few representative publications. For 1-D problems, very precise
solvers have been developed because OT is a quadratic functional in the inverse cumulative function (see Remark 2.30): Kinderlehrer and Walkington , Blanchet et al.
 , Agueh and Bowles , Matthes and Osberger , Blanchet and Carlier
 . In higher dimensions, it can be tackled using ﬁnite elements and ﬁnite volume
schemes: Carrillo et al. , Burger et al. . Alternative solvers are obtained
using Lagrangian schemes (i.e. particles systems): Carrillo and Moll , Benamou
et al. [2016a], Westdickenberg and Wilkening . Another direction is to look for
discrete ﬂows (typically on discrete grids or graphs) which maintain some properties of
their continuous counterparts; see Mielke , Erbar and Maas , Chow et al.
 , Maas .
An approximate approach to solve the Eulerian discretized problem (9.24) relying
on entropic regularization was initially proposed in Peyré , reﬁned in Chizat et al.
Variational Wasserstein Problems
[2018b] and theoretically analyzed in Carlier et al. . With an entropic regularization, Problem (9.26) has the form (4.49) when setting G = ιa(ℓ) and replacing F
by τF. One can thus use the iterations (4.51) to approximate a(ℓ+1) as proposed initially in Peyré . The convergence of this scheme as ε →0 is proved in Carlier
et al. . Figure 9.9 shows an example of evolution computed with this method.
An interesting application of gradient ﬂows to machine learning is to learn the underlying function F that best models some dynamical model of density. This learning
can be achieved by solving a smooth nonconvex optimization using entropic regularized
transport and automatic diﬀerentiation (see Remark 9.1.3); see Hashimoto et al. .
Analyzing the convergence of gradient ﬂows discretized in both time and space is
diﬃcult in general. Due to the polyhedral nature of the linear program deﬁning the
distance, using too-small step sizes leads to a “locking” phenomena . We refer to [Matthes and Osberger, 2014, 2017] for a
convergence analysis of a discretization method for gradient ﬂows in one dimension.
Figure 9.9:
Examples of gradient ﬂows evolutions, with drift V and congestion terms (from Peyré
 ), so that F(α) = R
X V (x)dα(x) + ι≤κ(ρα).
It is also possible to compute gradient ﬂows for unbalanced optimal transport distances as detailed in §10.2. This results in evolutions allowing mass creation or destruction, which is crucial to model many physical, biological or chemical phenomena.
An example of unbalanced gradient ﬂow is the celebrated Hele-Shaw model for cell
growth [Perthame et al., 2014], which is studied theoretically in [Gallouët and Monsaingeon, 2017, Di Marino and Chizat, 2017]. Such an unbalanced gradient ﬂow also
9.4. Minimum Kantorovich Estimators
can be approximated using the generalized Sinkhorn algorithm [Chizat et al., 2018b].
Minimum Kantorovich Estimators
Given some discrete samples (xi)n
i=1 ⊂X from some unknown distribution, the goal is
to ﬁt a parametric model θ 7→αθ ∈M(X) to the observed empirical input measure β
θ∈Θ L(αθ, β)
where L is some “loss” function between a discrete and a “continuous” (arbitrary)
distribution (see Figure 9.10).
In the case where αθ as a density ρθ
= ραθ with respect to the Lebesgue measure
(or any other ﬁxed reference measure), the maximum likelihood estimator (MLE) is
obtained by solving
LMLE(αθ, β)
log(ρθ(xi)).
This corresponds to using an empirical counterpart of a Kullback–Leibler loss since,
assuming the xi are i.i.d. samples of some ¯β, then
LMLE(α, β) n→+∞
−→KL(α|¯β).
Figure 9.10: Schematic display of the density ﬁtting problem 9.34.
This MLE approach is known to lead to optimal estimation procedures in many
cases (see, for instance, Owen ). However, it fails to work when estimating singular
distributions, typically when the αθ does not have a density (so that LMLE(αθ, β) =
+∞) or when (xi)i are samples from some singular ¯β (so that the αθ should share the
same support as β for KL(αθ|¯β) to be ﬁnite, but this support is usually unknown).
Another issue is that in several cases of practical interest, the density ρθ is inaccessible
(or too hard to compute).
A typical setup where both problems (singular and unknown densities) occur is for
so-called generative models, where the parametric measure is written as a push-forward
of a ﬁxed reference measure ζ ∈M(Z)
αθ = hθ,♯ζ
hθ : Z →X,
Variational Wasserstein Problems
where the push-forward operator is introduced in Deﬁnition 2.1. The space Z is usually
low-dimensional, so that the support of αθ is localized along a low-dimensional “manifold” and the resulting density is highly singular (it does not have a density with respect
to Lebesgue measure). Furthermore, computing this density is usually intractable, while
generating i.i.d. samples from αθ is achieved by computing xi = hθ(zi), where (zi)i are
i.i.d. samples from ζ.
In order to cope with such a diﬃcult scenario, one has to use weak metrics in place
of the MLE functional LMLE, which needs to be written in dual form as
(f,g)∈C(X)2
f(x)dα(x) +
g(x)dβ(x) : (f, g) ∈R
Dual norms shown in §8.2 correspond to imposing
R = {(f, −f) : f ∈B} ,
while optimal transport (2.24) sets R = R(c) as deﬁned in (2.25).
For a ﬁxed θ, evaluating the energy to be minimized in (9.34) using such a loss
function corresponds to solving a semidiscrete optimal transport, which is the focus
of Chapter 5. Minimizing the energy with respect to θ is much more involved and is
typically highly nonconvex.
Denoting fθ a solution to (9.35) when evaluating E(θ) = L(αθ, β), a subgradient is
obtained using the formula
[∂hθ(x)]⊤∇fθ(x)dαθ(x),
where ∂hθ(x) ∈Rdim(Θ)×d is the diﬀerential (with respect to θ) of θ ∈Rdim(Θ) 7→hθ(x),
while ∇fθ(x) is the gradient (with respect to x) of fθ. This formula is hard to use
numerically, ﬁrst because it requires ﬁrst computing a continuous function fθ, which
is a solution to a semi-discrete problem. As shown in §8.5, for OT loss, this can be
achieved using stochastic optimization, but this is hardly applicable in high dimension.
Another option is to impose a parametric form for this potential, for instance expansion
in an RKHS (Genevay et al. ) or a deep-network approximation . This, however, leads to important approximation errors that are not yet analyzed
theoretically. A last issue is that it is unstable numerically because it requires the
computation of the gradient ∇fθ of the dual potential fθ.
For the OT loss, an alternative gradient formula is obtained when one rather computes a primal optimal coupling for the following equivalent problem:
Lc(αθ, β) =
c(hθ(z), x)dγ(z, x) : γ ∈U(ζ, β)
Note that in the semidiscrete case considered here, the objective to be minimized can
9.4. Minimum Kantorovich Estimators
be actually decomposed as
c(hθ(z), xi)dγi(z)
dγi(z) = 1
where each γi ∈M1
+(Z). Once an optimal (γθ,i)i solving (9.38) is obtained, the gradient
of E(θ) is computed as
[∂hθ(z)]⊤∇1c(hθ(z), xi)dγi(z),
where ∇1c(x, y) ∈Rd is the gradient of x 7→c(x, y). Note that as opposed to (9.36),
this formula does not involve computing the gradient of the potentials being solutions
of the dual OT problem.
The class of estimators obtained using L = Lc, often called “minimum Kantorovich
estimators,” was initially introduced in [Bassetti et al., 2006]; see also [Canas and
Rosasco, 2012]. It has been used in the context of generative models by [Montavon et al.,
2016] to train restricted Boltzmann machines and in [Bernton et al., 2017] in conjunction
with approximate Bayesian computations. Approximations of these computations using
Deep Network are used to train deep generative models for both GAN [Arjovsky et al.,
2017] and VAE [Bousquet et al., 2017]; see also [Genevay et al., 2018, 2017, Salimans
et al., 2018]. Note that the use of Sinkhorn divergences for parametric model ﬁtting
is used routinely for shape matching and registration, see [Gold et al., 1998, Chui and
Rangarajan, 2000, Myronenko and Song, 2010, Feydy et al., 2017].
Remark 9.14 (Metric learning and transfer learning). Let us insist on the fact that,
for applications in machine learning, the success of OT-related methods very much
depends on the choice of an adapted cost c(x, y) which captures the geometry of the
data. While it is possible to embed many kinds of data in Euclidean spaces , in many cases, some sort of
adaptation or optimization of the metric is needed. Metric learning for supervised
tasks is a classical problem and it has been extended to the learning of the ground metric c(x, y) when
some OT distance is used in a learning pipeline [Cuturi and Avis, 2014] . Let us also mention the
related inverse problem of learning the cost matrix from the observations of an
optimal coupling P, which can be regularized using a low-rank prior [Dupuy et al.,
2016]. Related problems are transfer learning [Pan and Yang, 2010] and domain
adaptation [Glorot et al., 2011], where one wants to transfer some trained machine
learning pipeline to adapt it to some new dataset. This problem can be modeled
Variational Wasserstein Problems
and solved using OT techniques; see [Courty et al., 2017b,a].
Extensions of Optimal Transport
This chapter details several variational problems that are related to (and share the same
structure of) the Kantorovich formulation of optimal transport. The goal is to extend
optimal transport to more general settings: several input histograms and measures,
unnormalized ones, more general classes of measures, and optimal transport between
measures that focuses on local regularities (points nearby in the source measure should
be mapped onto points nearby in the target measure) rather than a total transport
cost, including cases where these two measures live in diﬀerent metric spaces.
Multimarginal Problems
Instead of coupling two input histograms using the Kantorovich formulation (2.11),
one can couple S histograms (as)S
s=1, where as ∈Σns, by solving the following multimarginal problem:
P∈U(as)s ⟨C, P⟩
Ci1,...,iSPi1,...,iS,
where the set of valid couplings is
P ∈Rn1×...×nS : ∀s, ∀is,
Pi1,...,iS = as,is
The entropic regularization scheme (4.2) naturally extends to this setting
P∈U(as)s ⟨P, C⟩−εH(P),
Extensions of Optimal Transport
and one can then apply Sinkhorn’s algorithm to compute the optimal P in scaling form,
where each entry indexed by a multi-index vector i = (i1, . . . , iS)
where us ∈Rns
+ are (unknown) scaling vectors, which are iteratively updated, by cycling
repeatedly through s = 1, . . . , S,
r̸=s uℓ,ir
Remark 10.1 (General measures). The discrete multimarginal problem (10.1) is
generalized to measures (αs)s on spaces (X1, . . . , XS) by computing a coupling
c(x1, . . . , xS)dπ(x1, . . . , xS),
where the set of couplings is
+(X1 × . . . × XS) : ∀s = 1, . . . , S, Ps,♯π = αs,
where Ps : X1 × . . . × XS
→Xs is the projection on the sth component,
Ps(x1, . . . , xS) = xs; see, for instance, [Gangbo and Swiech, 1998]. We refer to [Pass,
2015, 2012] for a review of the main properties of the multimarginal OT problem.
A typical application of multimarginal OT is to compute approximation of solutions to quantum chemistry problems, and in particular, in density functional
theory [Cotar et al., 2013, Gori-Giorgi et al., 2009, Buttazzo et al., 2012]. This
problem is obtained when considering the singular Coulomb interaction cost
c(x1, . . . , xS) =
Remark 10.2 (Multimarginal formulation of the barycenter). It is possible to recast
the linear program optimization (9.11) as an optimization over a single coupling
over X S+1 where the last marginal is the barycenter and the other ones are the
input measure (αs)S
λsc(x, xs)d¯π(x1, . . . , xs, x)
subject to
∀s = 1, . . . , S,
Ps,♯¯π = αs.
This stems from the “gluing lemma,” which states that given couplings (πs)S
10.1. Multimarginal Problems
where πs ∈U(αs, α), one can construct a higher-dimensional coupling ¯π ∈
+(XS+1) with marginals πs, i.e. such that Qs♯¯π = πs, where Qs(x1, . . . , xS, x)
(xs, x) ∈X 2. By explicitly minimizing in (10.4) with respect to the last marginal
(associated to x ∈X), one obtains that solutions α of the barycenter problem (9.11)
can be computed as α = Aλ,♯π, where Aλ is the “barycentric map” deﬁned as
Aλ : (x1, . . . , xS) ∈X S 7→argmin
λsc(x, xs)
(assuming this map is single-valued), where π is any solution of the multimarginal
problem (10.3) with cost
c(x1, . . . , xS) =
λℓc(xℓ, Aλ(x1, . . . , xS)).
For instance, for c(x, y) = ∥x −y∥2, one has, removing the constant squared terms,
c(x1, . . . , xS) = −
λrλs⟨xr, xs⟩,
which is a problem studied in Gangbo and Swiech . We refer to Agueh and
Carlier for more details. This formula shows that if all the input measures
are discrete βs = Pns
is=1 as,isδxs,is, then the barycenter α is also discrete and is
obtained using the formula
(i1,...,iS)
P(i1,...,iS)δAλ(xi1,...,xiS ),
where P is an optimal solution of (10.1) with cost matrix Ci1,...,iS = c(xi1, . . . , xiS)
as deﬁned in (10.5). Since P is a nonnegative tensor of Q
s ns dimensions obtained
as the solution of a linear program with P
s ns −S + 1 equality constraints, an
optimal solution P with up to P
s ns −S + 1 nonzero values can be obtained. A
barycenter α with a support of up to P
s ns−S+1 points can therefore be obtained.
This result and other considerations in the discrete case can be found in Anderes
et al. .
Remark 10.3 (Relaxation of Euler equations). A convex relaxation of Euler equations of incompressible ﬂuid dynamics has been proposed by Brenier and [Ambrosio and Figalli, 2009]. Similarly to the setting exposed in §7.6, it corresponds to the problem of ﬁnding a probability distribution
X) over the set
X of all paths γ : →X, which describes the
movement of particules in the ﬂuid. This is a relaxed version of the initial partial
diﬀerential equation model because, as in the Kantorovich formulation of OT, mass
Extensions of Optimal Transport
can be split. The evolution with time does not necessarily deﬁne a diﬀemorphism
of the underlying space X. The dynamic of the ﬂuid is obtained by minimizing as
in (7.17) the energy
0 ∥γ′(t)∥2 dt of each path. The diﬀerence with OT over the
space of paths is the additional incompressibilty of the ﬂuid. This incompressibilty
is taken care of by imposing that the density of particules should be uniform at
any time t ∈ (and not just imposed at initial and ﬁnal times t ∈{0, 1} as in
classical OT). Assuming X is compact and denoting ρX the uniform distribution
on X, this reads ¯Pt,♯¯π = ρX where ¯Pt : γ ∈¯
X →γ(t) ∈X. One can discretize
this problem by replacing a continuous path (γ(t))t∈ by a sequence of S points
(xi1, xi2, . . . , xiS) on a grid (xk)n
k=1 ⊂X, and ¯Π is represented by an S-way coupling P ∈RnS ∈U(as)s, where the marginals are uniform as = n−11n. The cost of
the corresponding multimarginal problem is then
Ci1,...,iS =
xis −xis+1
xσ(i1) −xiS
Here R is a large enough penalization constant, which is here to enforce the movement of particules between initial and ﬁnal times, which is prescribed by a permutation σ : JnK →JnK. This resulting multimarginal problem is implemented
eﬃciently in conjunction with Sinkhorn iterations (10.2) using the special structure of the cost, as detailed in [Benamou et al., 2015]. Indeed, in place of the O(nS)
cost required to compute the denominator appearing in (10.2), one can decompose
it as a succession of S matrix-vector multiplications, hence with a low cost Sn2.
Note that other solvers have been proposed, for instance, using the semidiscrete
framework shown in §5.2; see [de Goes et al., 2015, Gallouët and Mérigot, 2017].
Unbalanced Optimal Transport
A major bottleneck of optimal transport in its usual form is that it requires the two
input measures (α, β) to have the same total mass. While many workarounds have
been proposed (including renormalizing the input measure, or using dual norms such
as detailed in § 8.2), it is only recently that satisfying unifying theories have been
developed. We only sketch here a simple but important particular case.
Following Liero et al. , to account for arbitrary positive histograms (a, b) ∈
+, the initial Kantorovich formulation (2.11) is “relaxed” by only penalizing
marginal deviation using some divergence Dϕ, deﬁned in (8.3). This equivalently cor-
10.2. Unbalanced Optimal Transport
responds to minimizing an OT distance between approximate measures
C(a, b) = min
LC(a, b) + τ1Dϕ(a, ˜a) + τ2Dϕ(b, ˜b)
⟨C, P⟩+ τ1Dϕ(P1m|a) + τ2Dϕ(P⊤1m|b),
where (τ1, τ2) controls how much mass variations are penalized as opposed to transportation of the mass. In the limit τ1 = τ2 →+∞, assuming P
“balanced” case), one recovers the original optimal transport formulation with hard
marginal constraint (2.11).
This formalism recovers many diﬀerent previous works, for instance introducing
for Dϕ an ℓ2 norm [Benamou, 2003] or an ℓ1 norm as in partial transport [Figalli,
2010, Caﬀarelli and McCann, 2010]. A case of particular importance is when using
Dϕ = KL the Kulback–Leibler divergence, as detailed in Remark 10.5. For this cost,
in the limit τ = τ1 = τ2 →0, one obtains the so-called squared Hellinger distance (see
also Example 8.3)
C(a, b) τ→0
−→h2(a, b) =
Sinkhorn’s iterations (4.15) can be adapted to this problem by making use of the
generalized algorithm detailed in §4.6. The solution has the form (4.12) and the scalings
are updated as
Remark 10.4 (Generic measure). For (α, β) two arbitrary measures, the unbalanced version (also called “log-entropic”) of (2.15) reads
c(x, y)dπ(x, y)
+ τDϕ(P1,♯π|α) + τDϕ(P2,♯π|β),
where divergences Dϕ between measures are deﬁned in (8.1). In the special case
c(x, y) = ∥x −y∥2, Dϕ = KL, Lτ
c(α, β)1/2 is the Gaussian–Hellinger distance [Liero
et al., 2018], and it is shown to be a distance on M1
Remark 10.5 (Wasserstein–Fisher–Rao). For the particular choice of cost
c(x, y) = −log cos(min(d(x, y)/κ, π/2)),
Extensions of Optimal Transport
where κ is some cutoﬀdistance, and using Dϕ = KL, then
is the so-called Wasserstein–Fisher–Rao or Hellinger–Kantorovich distance. In the
special case X = Rd, this static (Kantorovich-like) formulation matches its dynamical counterparts (7.15), as proved independently by Liero et al. , Chizat
et al. [2018a]. This dynamical formulation is detailed in §7.4.
The barycenter problem (9.11) can be generalized to handle an unbalanced setting
by replacing Lc with Lτ
c. Figure 10.1 shows the resulting interpolation, providing a
good illustration of the usefulness of the relaxation parameter τ. The input measures
are mixtures of two Gaussians with unequal mass. Classical OT requires the leftmost
bump to be split in two and gives a nonregular interpolation. In sharp contrast, unbalanced OT allows the mass to vary during interpolation, so that the bumps are not
split and local modes of the distributions are smoothly matched. Using ﬁnite values
for τ (recall that OT is equivalent to τ = ∞) is thus important to prevent irregular
interpolations that arise because of mass splitting, which happens because of a “hard”
mass conservation constraint. The resulting optimization problem can be tackled numerically using entropic regularization and the generalized Sinkhorn algorithm detailed
In practice, unbalanced OT techniques seem to outperform classical OT for applications (such as in imaging or machine learning) where the input data is noisy or
not perfectly known. They are also crucial when the signal strength of a measure, as
measured by its total mass, must be accounted for, or when normalization is not meaningful. This was the original motivation of Frogner et al. , whose goal was to
compare sets of word labels used to describe images. Unbalanced OT and the corresponding Sinkhorn iterations have also been used for applications to the dynamics of
cells in [Schiebinger et al., 2017].
Remark 10.6 (Connection with dual norms). A particularly simple setup to account
for mass variation is to use dual norms, as detailed in §8.2. By choosing a compact
set B ⊂C(X) one obtains a norm deﬁned on the whole space M(X) (in particular,
the measures do not need to be positive). A particular instance of this setting is the
ﬂat norm (8.11), which is recovered as a special instance of unbalanced transport,
when using Dϕ(α|α′) = ∥α −α′∥TV to be the total variation norm (8.9); see, for instance, [Hanin, 1992, Lellmann et al., 2014]. We also refer to [Schmitzer and Wirth,
2017] for a general framework to deﬁne Wasserstein-1 unbalanced transport.
10.3. Problems with Extra Constraints on the Couplings
Classical OT (τ = +∞)
Ubalanced OT (τ = 1)
Figure 10.1:
Inﬂuence of relaxation parameter τ on unbalanced barycenters. Top to bottom: the
evolution of the barycenter between two input measures.
Problems with Extra Constraints on the Couplings
Many other OT-like problems have been proposed in the literature. They typically
correspond to adding extra constraints C on the set of feasible couplings appearing in
the original OT problem (2.15)
c(x, y)dπ(x, y) : π ∈C
Let us give two representative examples. The optimal transport with capacity constraint [Korman and McCann, 2015] corresponds to imposing that the density ρπ (for
instance, with respect to the Lebesgue measure) is upper bounded
C = {π : ρπ ≤κ}
for some κ > 0. This constraint rules out singular couplings localized on Monge maps.
The martingale transport problem (see, for instance, Galichon et al. , Dolinsky
and Soner , Tan and Touzi , Beiglböck et al. ), which ﬁnds many
applications in ﬁnance, imposes the so-called martingale constraint on the conditional
mean of the coupling, when X = Y = Rd:
π : ∀x ∈Rd,
dα(x)dβ(y)dβ(y) = x
Extensions of Optimal Transport
This constraint imposes that the barycentric projection map (4.20) of any admissible
coupling must be equal to the identity. For arbitrary (α, β), this set C is typically empty,
but necessary and suﬃcient conditions exist (α and β should be in “convex order”) to
ensure C ̸= ∅so that (α, β) satisfy a martingale constraint. This constraint can be
diﬃcult to enforce numerically when discretizing an existing problem. It also forbids
the solution to concentrate on a single Monge map, and can lead to couplings concentrated on the union of several graphs (a “multivalued” Monge map), or even more
complicated support sets. Using an entropic penalization as in (4.9), one can solve approximately (10.10) using the Dykstra algorithm as explained in Benamou et al. ,
which is a generalization of Sinkhorn’s algorithm shown in §4.2. This requires computing the projection onto C for the KL divergence, which is straightforward for (10.11)
but cannot be done in closed form (10.12) and thus necessitates subiterations; see [Guo
and Obloj, 2017] for more details.
Sliced Wasserstein Distance and Barycenters
One can deﬁne a distance between two measures (α, β) deﬁned on Rd by aggregating
1-D Wasserstein distances between their projections onto all directions of the sphere.
This deﬁnes
SW(α, β)2 def.
Sd W2(Pθ,♯α, Pθ,♯β)2dθ,
where Sd = {θ ∈Rd : ∥θ∥= 1} is the d-dimensional sphere, and Pθ : x ∈Rd →R is the
projection. This approach is detailed in [Bonneel et al., 2015], following ideas from Marc
Bernot. It is related to the problem of Radon inversion over measure spaces [Abraham
et al., 2017].
Lagrangian discretization and stochastic gradient descent.
The advantage of this
functional is that 1-D Wasserstein distances are simple to compute, as detailed in §2.6.
In the speciﬁc case where m = n and
this is achieved by simply sorting points
SW(α, β)2 =
|⟨xσθ(i) −yκθ(i), θ⟩|2
where σθ, κθ ∈Perm(n) are the permutation ordering in increasing order, respectively,
(⟨xi, θ⟩)i and (⟨yi, θ⟩)i.
Fixing the vector y, the function Eβ(x)
= SW(α, β)2 is smooth, and one can use
this function to deﬁne a mapping by gradient descent
x ←x −τ∇Eβ(x)
10.4. Sliced Wasserstein Distance and Barycenters
∇Eβ(x)i = 2
⟨xi −yκθ◦σ−1
using a small enough step size τ > 0. To make the method tractable, one can use a
stochastic gradient descent (SGD), replacing this integral with a discrete sum against
randomly drawn directions θ ∈Sd (see §5.4 for more details on SGD). The ﬂow (10.15)
can be understood as (Langrangian implementation of) a Wasserstein gradient ﬂow (in
the sense of §9.3) of the function α 7→SW(α, β)2. Numerically, one ﬁnds that this ﬂow
has no local minimizer and that it thus converges to α = β. The usefulness of the
Lagrangian solver is that, at convergence, it deﬁnes a matching (similar to a Monge
map) between the two distributions. This method has been used successfully for color
transfer and texture synthesis in [Rabin et al., 2011] and is related to the alternate
minimization approach detailed in [Pitié et al., 2007].
It is simple to extend this Lagrangian scheme to compute approximate “sliced”
barycenters of measures, by mimicking the Frechet deﬁnition of Wasserstein barycenters (9.11) and minimizing
λs SW(α, βs)2,
given a set (βs)S
s=1 of ﬁxed input measure. Using a Lagrangian discretization of the
form (10.14) for both α and the (βs)s, one can perform the nonconvex minimization
over the position x = (xi)i
λs∇Eβs(x),
by gradient descent using formula (10.15) to compute ∇Eβs(x) (coupled with a random
sampling of the direction θ).
Eulerian discretization and Radon transform.
A related way to compute an approximated sliced barycenter, without resorting to an iterative minimization scheme, is to
use the fact that (10.13) computes a distance between the Radon transforms R(α) and
R(β) where
= (Pθ,♯α)θ∈Sd.
A crucial point is that the Radon transform is invertible and that its inverse can be
computed using a ﬁltered backprojection formula. Given a collection of measures ρ =
(ρθ)θ∈Sd, one deﬁnes the ﬁltered backprojection operator as
R+(ρ) = Cd∆
where ξ = B(ρ) ∈M(Rd) is the measure deﬁned through the relation
∀g ∈C(Rd),
Rd g(x)dξ(x) =
g(rθ + Uθz)dρθ(r)dzdθ,
Extensions of Optimal Transport
where Uθ is any orthogonal basis of θ⊥, and where Cd ∈R is a normalizing constant
which depends on the dimension. Here ∆
is a fractional Laplacian, which is the
high-pass ﬁlter deﬁned over the Fourier domain as ˆ∆
2 (ω) = ∥ω∥d−1. The deﬁnition
of the backprojection (10.19) adds up the contribution of all the measures (ρθ)θ by
extending each one as being constant in the directions orthogonal to θ. One then has
the left-inverse relation R+ ◦R = IM(Rd), so that R+ is a valid reconstruction formula.
Figure 10.2:
Example of sliced barycenters computation using the Radon transform (as deﬁned
in (10.20)). Top: barycenters αt for S = 2 two input and weights (λ1, λ2) = (1 −t, t). Bottom: their
Radon transform R(αt) (the horizontal axis being the orientation angle θ).
In order to compute barycenters of input densities, it makes sense to replace formula (9.11) by its equivalent using Radon transform, and thus consider independently
for each θ the 1-D barycenter problem
λs W2(ρθ, Pθ,♯βs)2.
Each 1-D barycenter problem is easily computed using the monotone rearrangement as
detailed in Remark 9.6. The Radon approximation αR
= R+(ρ⋆) of a sliced barycenter solving (9.11) is then obtained by the inverse Radon transform R+. Note that in
general, αR is not a solution to (9.11) because the Radon transform is not surjective,
so that ρ⋆, which is obtained as a barycenter of the Radon transforms R(βs) does not
necessarily belong to the range of R. But numerically it seems in practice to be almost
the case [Bonneel et al., 2015]. Numerically, this Radon transform formulation is very
eﬀective for input measures and barycenters discretized on a ﬁxed grid (e.g. a uniform
grid for images), and R and well as R+ are computed approximately on this grid using
fast algorithms . Figure 10.2 illustrates this
10.5. Transporting Vectors and Matrices
computation of barycenters (and highlights the way the Radon transforms are interpolated), while Figure 10.3 shows a comparison of the Radon barycenters (10.20) and the
ones obtained by Lagrangian discretization (10.17).
Lagrangian
Wasserstein
Figure 10.3:
Comparison of barycenters computed using Radon transform (10.20) (Eulerian discretization), Lagrangian discretization (10.17), and Wasserstein OT (computed using Sinkhorn iterations (9.18)).
Sliced Wasserstein kernels.
Beside its computational simplicity, another advantage of
the sliced Wasserstein distance is that it is isometric to a Euclidean distance (it is thus
a “Hilbertian” metric), as detailed in Remark 2.30, and in particular formula (2.36).
As highlighted in §8.3, this should be contrasted with the Wasserstein distance W2 on
Rd, which is not Hilbertian in dimension d ≥2. It is thus possible to use this sliced
distance to equip the space of distributions M+
1 (Rd) with a reproducing kernel Hilbert
space structure (as detailed in §8.3). One can, for instance, use the exponential and
energy distance kernels
k(α, β) = e−SW(α,β)p
k(α, β) = −SW(α, β)p
for 1 ≤p ≤2 for the exponential kernels and 0 < p < 2 for the energy distance kernels.
This means that for any collection (αi)i of input measures, the matrix (k(αi, αj))i,j is
symmetric positive semideﬁnite. It is possible to use these kernels to perform a variety of
machine learning tasks using the “kernel trick,” for instance, in regression, classiﬁcation
(SVM and logistic), clustering (K-means) and dimensionality reduction (PCA) [Hofmann et al., 2008]. We refer to Kolouri et al. for details and applications.
Transporting Vectors and Matrices
Real-valued measures α ∈M(X) are easily generalized to vector-valued measures α ∈
M(X; V), where V is some vector space. For notational simplicity, we assume V is
Extensions of Optimal Transport
Euclidean and equipped with some inner product ⟨·, ·⟩(typically V = Rd and the inner
product is the canonical one). Thanks to this inner product, vector-valued measures
are identiﬁed with the dual of continuous functions g : X →V, i.e. for any such g, one
deﬁnes its integration against the measure as
g(x)dα(x) ∈R,
which is a linear operation on g and α. A discrete measure has the form α = P
where (xi, ai) ∈X × V and the integration formula (10.21) simply reads
g(x)dα(x) =
⟨ai, g(xi)⟩∈R.
Equivalently, if V = Rd, then such an α can be viewed as a collection (αs)d
“classical” real-valued measures (its coordinates), writing
g(x)dα(x) =
gs(x)dαs(x),
where g(x) = (gs(x))d
s=1 are the coordinates of g in the canonical basis.
Dual norms.
It is nontrivial, and in fact in general impossible, to extend OT distances
to such a general setting. Even coping with real-valued measures taking both positive
and negative values is diﬃcult. The only simple option is to consider dual norms, as
deﬁned in §8.2. Indeed, formula (6.3) readily extends to M(X; V) by considering B to
be a subset of C(X; V). So in particular, W1, the ﬂat norm and MMD norms can be
computed for vector-valued measures.
OT over cone-valued measures.
It is possible to deﬁne more advanced OT distances
when α is restricted to be in a subset M(X; V) ⊂M(X; V). The set V should be a
positively 1-homogeneous convex cone of V
λu : λ ∈R+, u ∈V0,
where V0 is a compact convex set. A typical example is the set of positive measures
where V = Rd
+. Dynamical convex formulations of OT over such a cone have been proposed; see [Zinsl and Matthes, 2015]. This has been applied to model the distribution
of chemical components. Another important example is the set of positive symmetric
matrices V = Sd
+ ⊂Rd×d. It is of course possible to use dual norms over this space, by
treating matrices as vectors; see, for instance, [Ning and Georgiou, 2014]. Dynamical
convex formulations for OT over such a cone have been provided [Chen et al., 2016b,
Jiang et al., 2012]. Some static (Kantorovich-like) formulations also have been proposed [Ning et al., 2015, Peyré et al., 2017], but a mathematically sound theoretical
10.5. Transporting Vectors and Matrices
framework is still missing. In particular, it is unclear if these static approaches deﬁne
distances for vector-valued measures and if they relate to some dynamical formulation.
Figure 10.4 is an example of tensor interpolation obtained using the method detailed
in [Peyré et al., 2017], which proposes a generalization of Sinkhorn algorithms using
quantum relative entropy (10.22) to deal with tensor ﬁelds.
OT over positive matrices.
A related but quite diﬀerent setting is to replace discrete
measures, i.e. histograms a ∈Σn, by positive matrices with unit trace A ∈S+
that tr(A) = 1. The rationale is that the eigenvalues λ(A) ∈Σn of A play the role of
a histogram, but one also has to take care of the rotations of the eigenvectors, so that
this problem is more complicated.
One can extend several divergences introduced in §8.1 to this setting. For instance,
the Bures metric (2.42) is a generalization of the Hellinger distance (deﬁned in Remark 8.3), since they are equal on positive diagonal matrices. One can also extend
the Kullback–Leibler divergence (4.6) (see also Remark 8.1), which is generalized to
positive matrices as
= tr (P log(P) −P log(Q) −P + Q, )
where log(·) is the matrix logarithm. This matrix KL is convex with both of its arguments.
It is possible to solve convex dynamic formulations to deﬁne OT distances between
such matrices [Carlen and Maas, 2014, Chen et al., 2016b, 2017]. There also exists
an equivalent of Sinkhorn’s algorithm, which is due to Gurvits and has been
extensively studied in [Georgiou and Pavon, 2015]; see also the review paper [Idel,
2016]. It is known to converge only in some cases but seems empirically to always work.
Figure 10.4: Interpolations between two input ﬁelds of positive semideﬁnite matrices (displayed at
times t ∈{0, 1} using ellipses) on some domain (here, a 2-D planar square and a surface mesh), using
the method detailed in Peyré et al. . Unlike linear interpolation schemes, this OT-like method
transports the “mass” of the tensors (size of the ellipses) as well as their anisotropy and orientation.
Extensions of Optimal Transport
Gromov–Wasserstein Distances
For some applications such as shape matching, an important weakness of optimal transport distances lies in the fact that they are not invariant to important families of invariances, such as rescaling, translation or rotations. Although some nonconvex variants
of OT to handle such global transformations have been proposed [Cohen and Guibas,
1999, Pele and Taskar, 2013] and recently applied to problems such as cross-lingual
word embeddings alignments [Grave et al., 2019, Alvarez-Melis et al., 2019, Grave et al.,
2019], these methods require specifying ﬁrst a subset of invariances, possibly between
diﬀerent metric spaces, to be relevant. We describe in this section a more general and
very natural extension of OT that can deal with measures deﬁned on diﬀerent spaces
without requiring the deﬁnition of a family of invariances.
HausdorﬀDistance
The Hausdorﬀdistance between two sets A, B ⊂Z for some metric dZ is
b∈B dZ(a, b), sup
a∈A dZ(a, b)
see Figure 10.5. This deﬁnes a distance between compact sets K(Z) of Z, and if Z is
compact, then (K(Z), HZ) is itself compact; see [Burago et al., 2001].
Figure 10.5: Computation of the Hausdorﬀdistance in R2.
Following Mémoli , one remarks that this distance between sets (A, B) can
be deﬁned similarly to the Wasserstein distance between measures (which should be
somehow understood as “weighted” sets). One replaces the measures couplings (2.14)
by sets couplings
R ∈X × Y :
∀a ∈A, ∃b ∈B, (a, b) ∈R
∀b ∈B, ∃a ∈A, (a, b) ∈R
With respect to Kantorovich problem (2.15), one should replace integration (since one
10.6. Gromov–Wasserstein Distances
does not have access to measures) by maximization, and one has
HZ(A, B) =
Note that the support of a measure coupling π ∈U(α, β) is a set coupling between
the supports, i.e. Supp(π) ∈R(Supp(α), Supp(β)). The Hausdorﬀdistance is thus
connected to the ∞-Wasserstein distance (see Remark 2.20) and one has H(A, B) ≤
W∞(α, β) for any measure (α, β) whose supports are (A, B).
Gromov–Hausdorﬀdistance
The Gromov–Hausdorﬀ(GH) distance [Gromov, 2001] is a
way to measure the distance between two metric spaces (X, dX ), (Y, dY) by quantifying
how far they are from being isometric to each other, see Figure 10.6. It is deﬁned as the
minimum Hausdorﬀdistance between every possible isometric embedding of the two
spaces in a third one,
GH(dX , dY)
HZ(f(X), g(Y)) :
f : X isom
g : Y isom
Here, the constraint is that f must be an isometric embedding, meaning that
dZ(f(x), f(x′)) = dX (x, x′) for any (x, x′) ∈X 2 (similarly for g). One can show that
GH deﬁnes a distance between compact metric spaces up to isometries, so that in particular GH(dX , dY) = 0 if and only if there exists an isometry h : X →Y, i.e. h is
bijective and dY(h(x), h(x′)) = dX (x, x′) for any (x, x′) ∈X 2.
Figure 10.6: The GH approach to compare two metric spaces.
Similarly to (10.23) and as explained in [Mémoli, 2011], it is possible to rewrite
equivalently the GH distance using couplings as follows:
GH(dX , dY) = 1
((x,y),(x′,y′))∈R2 |dX (x, x′) −dX (y, y′)|.
For discrete spaces X = (xi)n
i=1, Y = (yj)m
j=1 represented using a distance matrix D =
(dX (xi, xi′))i,i′ ∈Rn×n, D′ = (dY(yj, yj′))j,j′ ∈Rm×m, one can rewrite this optimization
Extensions of Optimal Transport
using binary matrices R ∈{0, 1}n×m indicating the support of the set couplings R as
GH(D, D′) = 1
R1>0,R⊤1>0
(i,i′,j,j′) Ri,jRj,j′|Di,i′ −D′
The initial motivation of the GH distance is to deﬁne and study limits of metric spaces,
as illustrated in Figure 10.7, and we refer to [Burago et al., 2001] for details. There is an
explicit description of the geodesics for the GH distance [Chowdhury and Mémoli, 2016],
which is very similar to the one in Gromov–Wasserstein spaces, detailed in Remark 10.8.
Figure 10.7: GH limit of sequences of metric spaces.
The underlying optimization problem (10.24) is highly nonconvex, and computing
the global minimum is untractable. It has been approached numerically using approximation schemes and has found applications in vision and graphics for shape matching [Mémoli and Sapiro, 2005, Bronstein et al., 2006].
It is often desirable to “smooth” the deﬁnition of the Hausdorﬀdistance by replacing the maximization by an integration. This in turn necessitates the introduction of
measures, and it is one of the motivations for the deﬁnition of the GW distance in the
next section.
Gromov–Wasserstein Distance
Optimal transport needs a ground cost C to compare histograms (a, b) and thus cannot
be used if the bins of those histograms are not deﬁned on the same underlying space,
or if one cannot preregister these spaces to deﬁne a ground cost between any pair of
bins in the ﬁrst and second histograms, respectively. To address this limitation, one
can instead only assume a weaker assumption, namely that two matrices D ∈Rn×n
and D′ ∈Rm×m quantify similarity relationships between the points on which the
histograms are deﬁned. A typical scenario is when these matrices are (power of) distance
matrices. The GW problem reads
GW((a, D), (b, D′))2 def.
P∈U(a,b) ED,D′(P)
10.6. Gromov–Wasserstein Distances
|Di,i′ −D′
j,j′|2Pi,jPi′,j′,
see Figure 10.8. This problem is similar to the GH problem (10.24) when replacing
maximization by a sum and set couplings by measure couplings. This is a nonconvex
problem, which can be recast as a quadratic assignment problem [Loiola et al., 2007]
and is in full generality NP-hard to solve for arbitrary inputs. It is in fact equivalent
to a graph matching problem [Lyzinski et al., 2016] for a particular cost.
|Di,i0 −D0
Figure 10.8: The GW approach to comparing two metric measure spaces.
One can show that GW satisﬁes the triangular inequality, and in fact it deﬁnes a
distance between metric spaces equipped with a probability distribution, here assumed
to be discrete in deﬁnition (10.25), up to isometries preserving the measures. This distance was introduced and studied in detail by Mémoli . An in-depth mathematical
exposition (in particular, its geodesic structure and gradient ﬂows) is given in [Sturm,
2012]. See also [Schmitzer and Schnörr, 2013a] for applications in computer vision. This
distance is also tightly connected with the GH distance [Gromov, 2001] between metric
spaces, which have been used for shape matching [Mémoli, 2007, Bronstein et al., 2010].
Remark 10.7 (Gromov–Wasserstein distance). The general setting corresponds to
computing couplings between metric measure spaces (X, dX , αX ) and (Y, dY, αY),
where (dX , dY) are distances, while αX and αY are measures on their respective
spaces. One deﬁnes
GW((αX , dX ), (αY, dY))2 def.
π∈U(αX ,αY)
X 2×Y2 |dX (x, x′) −dY(y, y′)|2dπ(x, y)dπ(x′, y′).
GW deﬁnes a distance between metric measure spaces up to isometries, where
one says that (X, αX , dX ) and (Y, αY, dY) are isometric if there exists a bijection
ϕ : X →Y such that ϕ♯αX = αY and dY(ϕ(x), ϕ(x′)) = dX (x, x′).
Extensions of Optimal Transport
Remark 10.8 (Gromov–Wasserstein geodesics). The space of metric spaces (up to
isometries) endowed with this GW distance (10.26) has a geodesic structure. Sturm
 shows that the geodesic between (X0, dX0, α0) and (X1, dX1, α1) can be chosen
to be t ∈ 7→(X0 × X1, dt, π⋆), where π⋆is a solution of (10.26) and for all
((x0, x1), (x′
1)) ∈(X0 × X1)2,
dt((x0, x1), (x′
= (1 −t)dX0(x0, x′
0) + tdX1 detailed below.
Entropic Regularization
To approximate the computation of GW, and to help convergence of minimization
schemes to better minima, one can consider the entropic regularized variant
P∈U(a,b) ED,D′(P) −εH(P).
As proposed initially in [Gold and Rangarajan, 1996, Rangarajan et al., 1999], and later
revisited in [Solomon et al., 2016a] for applications in graphics, one can use iteratively
Sinkhorn’s algorithm to progressively compute a stationary point of (10.27). Indeed,
successive linearizations of the objective function lead to consider the succession of
P(ℓ+1) def.
P∈U(a,b) ⟨P, C(ℓ)⟩−εH(P)
= ∇ED,D′(P(ℓ)) = −DP(ℓ)D′,
which can be interpreted as a mirror-descent scheme [Solomon et al., 2016a]. Each
update can thus be solved using Sinkhorn iterations (4.15) with cost C(ℓ). Figure 10.9
displays the evolution of the algorithm. Figure 10.10 illustrates the use of this entropic
GW to compute soft maps between domains.
10.6. Gromov–Wasserstein Distances
Figure 10.9: Iterations of the entropic GW algorithm (10.28) between two shapes (xi)i and (yj)j in
R2, initialized with P(0) = a ⊗b. The distance matrices are Di,i′ = ∥xi −xi′∥and D′
j,j′ = ∥yj −yj′∥.
Top row: coupling P(ℓ) displayed as a 2-D image. Bottom row: matching induced by P(ℓ) (each point
xi is connected to the three yj with the three largest values among {P(ℓ)
i,j }j). The shapes have the same
size, but for display purposes, the inner shape (xi)i has been reduced.
Figure 10.10:
Example of fuzzy correspondences computed by solving GW problem (10.27) with
Sinkhorn iterations (10.28). Extracted from [Solomon et al., 2016a].
Acknowledgements
We would like to thank the many colleagues, collaborators and students who have
helped us at various stages when preparing this survey. Some of their inputs have
shaped this work, and we would like to thank in particular Jean-David Benamou,
Yann Brenier, Guillaume Carlier, Vincent Duval and the entire MOKAPLAN team at
Inria; Francis Bach, Espen Bernton, Mathieu Blondel, Nicolas Courty, Rémi Flamary,
Alexandre Gramfort, Young-Heon Kim, Daniel Matthes, Philippe Rigollet, Filippo Santambrogio, Justin Solomon, Jonathan Weed; as well as the feedback by our current and
former students on these subjects, in particular Gwendoline de Bie, Lénaïc Chizat,
Aude Genevay, Hicham Janati, Théo Lacombe, Boris Muzellec, Francois-Pierre Paty,
Vivien Seguy.
References
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, et al. Tensorﬂow: large-scale
machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,
Isabelle Abraham, Romain Abraham, Maıtine Bergounioux, and Guillaume Carlier.
Tomographic reconstruction from a few views: a multi-marginal optimal transport approach. Applied Mathematics & Optimization, 75(1):55–73, 2017.
Ryan Prescott Adams and Richard S Zemel. Ranking via sinkhorn propagation. arXiv preprint
 
Martial Agueh and Malcolm Bowles. One-dimensional numerical algorithms for gradient ﬂows
in the p-Wasserstein spaces. Acta Applicandae Mathematicae, 125(1):121–134, 2013.
Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal
on Mathematical Analysis, 43(2):904–924, 2011.
Martial Agueh and Guillaume Carlier. Vers un théorème de la limite centrale dans l’espace de
Wasserstein? Comptes Rendus Mathematique, 355(7):812–818, 2017.
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermüller, Dzmitry Bahdanau,
and Nicolas Ballas et al. Theano: A python framework for fast computation of mathematical
expressions. CoRR, abs/1605.02688, 2016.
Syed Mumtaz Ali and Samuel D Silvey. A general class of coeﬃcients of divergence of one
distribution from another. Journal of the Royal Statistical Society. Series B (Methodological),
28(1):131–142, 1966.
Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Much faster algorithms for
matrix scaling. arXiv preprint arXiv:1704.02315, 2017.
Jason Altschuler, Jonathan Weed, and Philippe Rigollet. Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration.
arXiv preprint arXiv:1705.09634,
References
Pedro C Álvarez-Esteban, E del Barrio, JA Cuesta-Albertos, and C Matrán. A ﬁxed-point
approach to barycenters in Wasserstein space. Journal of Mathematical Analysis and Applications, 441(2):744–762, 2016.
David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. Towards optimal transport with
global invariances. 2019.
Shun-ichi Amari, Ryo Karakida, and Masafumi Oizumi.
Information geometry connecting
Wasserstein distance and Kullback-Leibler divergence via the entropy-relaxed transportation problem. Information Geometry, (1):13–37, 2018.
L. Ambrosio, N. Gigli, and G. Savaré. Gradient Flows in Metric Spaces and in the Space of
Probability Measures. Springer, 2006.
Luigi Ambrosio and Alessio Figalli. Geodesics in the space of measure-preserving maps and
plans. Archive for Rational Mechanics and Analysis, 194(2):421–462, 2009.
Ethan Anderes, Steﬀen Borgwardt, and Jacob Miller. Discrete Wasserstein barycenters: optimal
transport for discrete data. Mathematical Methods of Operations Research, 84(2):389–409,
Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Earth mover distance over highdimensional spaces.
In Proceedings of the nineteenth annual ACM-SIAM Symposium on
Discrete Algorithms, pages 343–352. Society for Industrial and Applied Mathematics, 2008.
Alexandr Andoni, Assaf Naor, and Ofer Neiman. Snowﬂake universality of Wasserstein spaces.
Annales scientiﬁques de l’École normale supérieure, 51:657–700, 2018.
Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. Proceedings of the 34th International Conference on Machine Learning, 70:214–223,
Franz Aurenhammer. Power diagrams: properties, algorithms and applications. SIAM Journal
on Computing, 16(1):78–96, 1987.
Franz Aurenhammer, Friedrich Hoﬀmann, and Boris Aronov. Minkowski-type theorems and
least-squares clustering. Algorithmica, 20(1):61–76, 1998.
Amir Averbuch, Ronald Coifman, David Donoho, Moshe Israeli, and Johan Walden. Fast slant
stack: a notion of radon transform for data in a cartesian grid which is rapidly computible,
algebraically exact, geometrically faithful and invertible. Tech. Rep., Stanford University,
Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics,
4:384–414, 2010.
Francis R Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity
for logistic regression. Journal of Machine Learning Research, 15(1):595–627, 2014.
Michael Bacharach. Estimating nonnegative matrices from marginal data. International Economic Review, 6(3):294–310, 1965.
Alexander Barvinok. A Course in Convexity. Graduate Studies in Mathematics. American
Mathematical Society, 2002. ISBN 9780821829684.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum kantorovich distance
estimators. Statistics & Probability Letters, 76(12):1298–1302, 2006.
References
Heinz H Bauschke and Patrick L Combettes. Convex analysis and monotone operator theory
in Hilbert spaces. Springer-Verlag, New York, 2011.
Heinz H Bauschke and Adrian S Lewis.
Dykstra’s algorithm with Bregman projections: a
convergence proof. Optimization, 48(4):409–427, 2000.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods
for convex optimization. Operations Research Letters, 31(3):167–175, 2003.
Martin Beckmann. A continuous model of transportation. Econometrica, 20:643–660, 1952.
Mathias Beiglböck, Pierre Henry-Labordère, and Friedrich Penkner. Model-independent bounds
for option prices: a mass transport approach. Finance and Stochastics, 17(3):477–501, 2013.
Jan Beirlant, Edward J Dudewicz, Laszlo Gyorﬁ, and Edward C Van der Meulen. Nonparametric entropy estimation: an overview. International Journal of Mathematical and Statistical
Sciences, 6(1):17–39, 1997.
Jean-David Benamou.
Numerical resolution of an “unbalanced” mass transport problem.
ESAIM: Mathematical Modelling and Numerical Analysis, 37(05):851–868, 2003.
Jean-David Benamou and Yann Brenier.
A computational ﬂuid mechanics solution to the
Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–393, 2000.
Jean-David Benamou and Guillaume Carlier. Augmented lagrangian methods for transport
optimization, mean ﬁeld games and degenerate elliptic equations. Journal of Optimization
Theory and Applications, 167(1):1–26, 2015.
Jean-David Benamou, Brittany D Froese, and Adam M Oberman. Numerical solution of the optimal transportation problem using the Monge–Ampere equation. Journal of Computational
Physics, 260:107–126, 2014.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyré. Iterative Bregman projections for regularized transportation problems. SIAM Journal on Scientiﬁc Computing, 37(2):A1111–A1138, 2015.
Jean-David Benamou, Guillaume Carlier, Quentin Mérigot, and Edouard Oudet. Discretization
of functionals involving the Monge–Ampère operator. Numerische Mathematik, 134(3):611–
636, 2016a.
Jean-David Benamou, Francis Collino, and Jean-Marie Mirebeau. Monotone and consistent
discretization of the Monge-Ampere operator. Mathematics of Computation, 85(302):2743–
2775, 2016b.
Christian Berg, Jens Peter Reus Christensen, and Paul Ressel. Harmonic Analysis on Semigroups. Number 100 in Graduate Texts in Mathematics. Springer Verlag, 1984.
Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability
and Statistics. Kluwer Academic Publishers, 2003.
Espen Bernton.
Langevin Monte Carlo and JKO splitting.
In Sébastien Bubeck, Vianney
Perchet, and Philippe Rigollet, editors, Proceedings of the 31st Conference On Learning Theory, volume 75 of Proceedings of Machine Learning Research, pages 1777–1798. PMLR, 2018.
Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Christian P Robert. Inference in generative models using the Wasserstein distance. arXiv preprint arXiv:1701.05146, 2017.
References
Dimitri P Bertsekas. A new algorithm for the assignment problem. Mathematical Programming,
21(1):152–171, 1981.
Dimitri P Bertsekas. Auction algorithms for network ﬂow problems: a tutorial introduction.
Computational Optimization and Applications, 1(1):7–66, 1992.
Dimitri P Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc,
Dimitri P Bertsekas and Jonathan Eckstein. Dual coordinate step methods for linear network
ﬂow problems. Mathematical Programming, 42(1):203–243, 1988.
Dimitris Bertsimas and John N Tsitsiklis. Introduction to Linear Optimization. Athena Scientiﬁc, 1997.
Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. On the bures-wasserstein distance between
positive deﬁnite matrices. Expositiones Mathematicae, to appear, 2018.
Jérémie Bigot and Thierry Klein.
Consistent estimation of a population barycenter in the
Wasserstein space. arXiv Preprint arXiv:1212.2562, 2012a.
Jérémie Bigot and Thierry Klein. Characterization of barycenters in the Wasserstein space by
averaging optimal transport maps. arXiv preprint arXiv:1212.2562, 2012b.
Jérémie Bigot, Elsa Cazelles, and Nicolas Papadakis.
Central limit theorems for sinkhorn
divergence between probability distributions on ﬁnite spaces and statistical applications.
arXiv preprint arXiv:1711.08947, 2017a.
Jérémie Bigot, Raúl Gouet, Thierry Klein, and Alfredo López. Geodesic PCA in the Wasserstein
space by convex pca. Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, 53
(1):1–26, 2017b.
Garrett Birkhoﬀ. Tres observaciones sobre el algebra lineal. Universidad Nacional de Tucumán
Revista Series A, 5:147–151, 1946.
Garrett Birkhoﬀ. Extensions of jentzsch’s theorem. Transactions of the American Mathematical
Society, 85(1):219–227, 1957.
Adrien Blanchet and Guillaume Carlier. Optimal transport and Cournot-Nash equilibria. Mathematics of Operations Research, 41(1):125–145, 2015.
Adrien Blanchet, Vincent Calvez, and José A Carrillo.
Convergence of the mass-transport
steepest descent scheme for the subcritical Patlak-Keller-Segel model.
SIAM Journal on
Numerical Analysis, 46(2):691–721, 2008.
Emmanuel Boissard. Simple bounds for the convergence of empirical and occupation measures
in 1-Wasserstein distance. Electronic Journal of Probability, 16:2296–2333, 2011.
Emmanuel Boissard, Thibaut Le Gouic, and Jean-Michel Loubes. Distribution’s template estimate with Wasserstein metrics. Bernoulli, 21(2):740–759, 2015.
Franccois Bolley, Arnaud Guillin, and Cédric Villani. Quantitative concentration inequalities
for empirical measures on non-compact spaces. Probability Theory and Related Fields, 137
(3):541–593, 2007.
References
Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement
interpolation using lagrangian mass transport. ACM Transactions on Graphics, 30(6):158,
Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pﬁster. Sliced and Radon Wasserstein barycenters of measures. Journal of Mathematical Imaging and Vision, 51(1):22–45,
Nicolas Bonneel, Gabriel Peyré, and Marco Cuturi. Wasserstein barycentric coordinates: histogram regression using optimal transport. ACM Transactions on Graphics, 35(4):71:1–71:10,
CW Borchardt and CGJ Jocobi. De investigando ordine systematis aequationum diﬀerentialium
vulgarium cujuscunque. Journal für die reine und angewandte Mathematik, 64:297–320, 1865.
Ingwer Borg and Patrick JF Groenen. Modern Multidimensional Scaling: Theory and Applications. Springer Science & Business Media, 2005.
Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and Bruno Lévy. Polygon mesh processing. Taylor & Francis, 2010.
Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bernhard
Schoelkopf. From optimal transport to generative modeling: the VEGAN cookbook. arXiv
 
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
Distributed
optimization and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3(1):1–122, January 2011.
Lev M Bregman.
The relaxation method of ﬁnding the common point of convex sets and
its application to the solution of problems in convex programming. USSR Computational
Mathematics and Mathematical Physics, 7(3):200–217, 1967.
Yann Brenier. Décomposition polaire et réarrangement monotone des champs de vecteurs. C.
R. Acad. Sci. Paris Sér. I Math., 305(19):805–808, 1987.
Yann Brenier. The least action principle and the related concept of generalized ﬂows for incompressible perfect ﬂuids. Journal of the AMS, 2:225–255, 1990.
Yann Brenier.
Polar factorization and monotone rearrangement of vector-valued functions.
Communications on Pure and Applied Mathematics, 44(4):375–417, 1991.
Yann Brenier. The dual least action problem for an ideal, incompressible ﬂuid. Archive for
Rational Mechanics and Analysis, 122(4):323–351, 1993.
Yann Brenier. Minimal geodesics on groups of volume-preserving maps and generalized solutions
of the Euler equations. Communications on Pure and Applied Mathematics, 52(4):411–452,
Yann Brenier. Generalized solutions and hydrostatic approximation of the Euler equations.
Physica D. Nonlinear Phenomena, 237(14-17):1982–1988, 2008.
Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized multidimensional
scaling: a framework for isometry-invariant partial surface matching.
Proceedings of the
National Academy of Sciences, 103(5):1168–1172, 2006.
References
Alexander M Bronstein, Michael M Bronstein, Ron Kimmel, Mona Mahmoudi, and Guillermo
Sapiro. A Gromov-Hausdorﬀframework with diﬀusion geometry for topologically-robust nonrigid shape matching. International Journal on Computer Vision, 89(2-3):266–286, 2010.
Richard A Brualdi. Combinatorial Matrix Classes, volume 108. Cambridge University Press,
Dmitri Burago, Yuri Burago, and Sergei Ivanov. A Course in Metric Geometry, volume 33.
American Mathematical Society Providence, RI, 2001.
Donald Bures. An extension of Kakutani’s theorem on inﬁnite product measures to the tensor
product of semiﬁnite w∗-algebras. Transactions of the American Mathematical Society, 135:
199–212, 1969.
Martin Burger, José Antonio Carrillo de la Plata, and Marie-Therese Wolfram. A mixed ﬁnite
element method for nonlinear diﬀusion equations. Kinetic and Related Models, 3(1):59–83,
Martin Burger, Marzena Franek, and Carola-Bibiane Schönlieb. Regularised regression and
density estimation based on optimal transport. Applied Mathematics Research Express, 2:
209–253, 2012.
Giuseppe Buttazzo, Luigi De Pascale, and Paola Gori-Giorgi. Optimal-transport formulation
of electronic density-functional theory. Physical Review A, 85(6):062502, 2012.
Luis Caﬀarelli. The Monge-Ampere equation and optimal transportation, an elementary review.
Lecture Notes in Mathematics, Springer-Verlag, pages 1–10, 2003.
Luis Caﬀarelli, Mikhail Feldman, and Robert McCann. Constructing optimal maps for Monge’s
transport problem as a limit of strictly convex costs. Journal of the American Mathematical
Society, 15(1):1–26, 2002.
Luis A Caﬀarelli and Robert J McCann. Free boundaries in optimal transport and Monge-
Ampère obstacle problems. Annals of Mathematics, 171(2):673–730, 2010.
Luis A Caﬀarelli, Sergey A Kochengin, and Vladimir I Oliker. Problem of reﬂector design with
given far-ﬁeld scattering data.
In Monge Ampère equation: applications to geometry and
optimization, volume 226, page 13, 1999.
Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal
transport metrics. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors,
Advances in Neural Information Processing Systems 25, pages 2492–2500. 2012.
Eric A Carlen and Jan Maas. An analog of the 2-Wasserstein metric in non-commutative probability under which the fermionic Fokker–Planck equation is gradient ﬂow for the entropy.
Communications in Mathematical Physics, 331(3):887–926, 2014.
Guillaume Carlier and Ivar Ekeland. Matching for teams. Economic Theory, 42(2):397–418,
Guillaume Carlier and Clarice Poon. On the total variation Wasserstein gradient ﬂow and the
TV-JKO scheme. to appear in ESAIM: COCV, 2019.
Guillaume Carlier, Chloé Jimenez, and Filippo Santambrogio. Optimal transportation with
traﬃc congestion and Wardrop equilibria. SIAM Journal on Control and Optimization, 47
(3):1330–1350, 2008.
References
Guillaume Carlier, Alfred Galichon, and Filippo Santambrogio. From knothe’s transport to Brenier’s map and a continuation method for optimal transport. SIAM Journal on Mathematical
Analysis, 41(6):2554–2576, 2010.
Guillaume Carlier, Adam Oberman, and Edouard Oudet. Numerical methods for matching
for teams and Wasserstein barycenters.
ESAIM: Mathematical Modelling and Numerical
Analysis, 49(6):1621–1642, 2015.
Guillaume Carlier, Victor Chernozhukov, and Alfred Galichon. Vector quantile regression beyond correct speciﬁcation. arXiv preprint arXiv:1610.06833, 2016.
Guillaume Carlier, Vincent Duval, Gabriel Peyré, and Bernhard Schmitzer. Convergence of
entropic schemes for optimal transport and gradient ﬂows. SIAM Journal on Mathematical
Analysis, 49(2):1385–1418, 2017.
José A Carrillo and J Salvador Moll. Numerical simulation of diﬀusive and aggregation phenomena in nonlinear continuity equations by evolving diﬀeomorphisms. SIAM Journal on
Scientiﬁc Computing, 31(6):4305–4329, 2009.
José A Carrillo, Alina Chertock, and Yanghong Huang. A ﬁnite-volume method for nonlinear nonlocal equations with a gradient ﬂow structure. Communications in Computational
Physics, 17:233–258, 1 2015.
Yair Censor and Simeon Reich. The Dykstra algorithm with Bregman projections. Communications in Applied Analysis, 2:407–419, 1998.
Yair Censor and Stavros Andrea Zenios. Proximal minimization algorithm with d-functions.
Journal of Optimization Theory and Applications, 73(3):451–464, 1992.
Thierry Champion, Luigi De Pascale, and Petri Juutinen. The ∞-wasserstein distance: local
solutions and existence of optimal transport maps. SIAM Journal on Mathematical Analysis,
40(1):1–20, 2008.
Timothy M Chan. Optimal output-sensitive convex hull algorithms in two and three dimensions.
Discrete & Computational Geometry, 16(4):361–368, 1996.
Yongxin Chen, Tryphon T Georgiou, and Michele Pavon. On the relation between optimal
transport and Schrödinger bridges: A stochastic control viewpoint. Journal of Optimization
Theory and Applications, 169(2):671–691, 2016a.
Yongxin Chen, Tryphon T Georgiou, and Allen Tannenbaum. Matrix optimal mass transport:
a quantum mechanical approach. arXiv preprint arXiv:1610.03041, 2016b.
Yongxin Chen, Wilfrid Gangbo, Tryphon T Georgiou, and Allen Tannenbaum. On the matrix
Monge-Kantorovich problem. arXiv preprint arXiv:1701.02826, 2017.
Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and Franccois-Xavier Vialard. Unbalanced
optimal transport: geometry and Kantorovich formulation. Journal of Functional Analysis,
274(11):3090–3123, 2018a.
Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and Franccois-Xavier Vialard. Scaling algorithms for unbalanced transport problems. Mathematics of Computation, 87:2563–2609,
References
Lenaic Chizat, Gabriel Peyré, Bernhard Schmitzer, and Franccois-Xavier Vialard. An interpolating distance between optimal transport and Fisher–Rao metrics. Foundations of Computational Mathematics, 18(1):1–44, 2018c.
Shui-Nee Chow, Wen Huang, Yao Li, and Haomin Zhou. Fokker-Planck equations for a free energy functional or Markov process on a graph. Archive for Rational Mechanics and Analysis,
203(3):969–1008, 2012.
Shui-Nee Chow, Wuchen Li, and Haomin Zhou. A discrete Schrodinger equation via optimal
transport on graphs. arXiv preprint arXiv:1705.07583, 2017a.
Shui-Nee Chow, Wuchen Li, and Haomin Zhou. Entropy dissipation of Fokker-Planck equations
on graphs. arXiv preprint arXiv:1701.04841, 2017b.
Samir Chowdhury and Facundo Mémoli. Constructing geodesics on the space of compact metric
spaces. arXiv preprint arXiv:1603.02385, 2016.
Haili Chui and Anand Rangarajan. A new algorithm for non-rigid point matching. In Computer
Vision and Pattern Recognition, 2000. Proceedings. IEEE Conference on, volume 2, pages
44–51. IEEE, 2000.
Imre Ciszár. Information-type measures of diﬀerence of probability distributions and indirect
observations. Studia Scientiarum Mathematicarum Hungarica, 2:299–318, 1967.
Michael B Cohen, Aleksander Madry, Dimitris Tsipras, and Adrian Vladu. Matrix scaling and
balancing via box constrained Newton’s method and interior point methods. arXiv preprint
 
Scott Cohen and Leonidas Guibas. The earth mover’s distance under transformation sets. In
Proceedings of the Seventh IEEE International Conference on Computer vision, volume 2,
pages 1076–1083. IEEE, 1999.
Patrick L Combettes and Jean-Christophe Pesquet. A Douglas-Rachford splitting approach to
nonsmooth convex variational signal recovery. IEEE Journal of Selected Topics in Signal
Processing, 1(4):564 –574, 2007.
Roberto Cominetti and Jaime San Martín. Asymptotic analysis of the exponential penalty
trajectory in linear programming. Mathematical Programming, 67(1-3):169–187, 1994.
Laurent Condat. Fast projection onto the simplex and the ℓ1 ball. Math. Programming, Ser.
A, pages 1–11, 2015.
Sueli IR Costa, Sandra A Santos, and João E Strapasson.
Fisher information distance: a
geometrical reading. Discrete Applied Mathematics, 197:59–69, 2015.
Codina Cotar, Gero Friesecke, and Claudia Klüppelberg. Density functional theory and optimal
transportation with Coulomb cost. Communications on Pure and Applied Mathematics, 66
(4):548–599, 2013.
Nicolas Courty, Rémi Flamary, Devis Tuia, and Thomas Corpetti. Optimal transport for data
fusion in remote sensing. In 2016 IEEE International Geoscience and Remote Sensing Symposium, pages 3571–3574. IEEE, 2016.
References
Nicolas Courty, Rémi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In I. Guyon, U. V. Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 3730–3739. 2017a.
Nicolas Courty, Rémi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for
domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39
(9):1853–1865, 2017b.
Keenan Crane, Clarisse Weischedel, and Max Wardetzky. Geodesics in heat: a new approach to
computing distance based on heat ﬂow. ACM Transaction on Graphics, 32(5):152:1–152:11,
October 2013.
Juan Antonio Cuesta and Carlos Matran. Notes on the wasserstein metric in hilbert spaces.
The Annals of Probability, 17(3):1264–1276, 07 1989.
Marco Cuturi. Positivity and transportation. arXiv preprint 1209.2655, 2012.
Marco Cuturi. Sinkhorn distances: lightspeed computation of optimal transport. In Advances
in Neural Information Processing Systems 26, pages 2292–2300, 2013.
Marco Cuturi and David Avis. Ground metric learning. Journal of Machine Learning Research,
15:533–564, 2014.
Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Proceedings
of ICML, volume 32, pages 685–693, 2014.
Marco Cuturi and Kenji Fukumizu. Kernels on structured objects through nested histograms.
In P. B. Schölkopf, J. C. Platt, and T. Hoﬀman, editors, Advances in Neural Information
Processing Systems 19, pages 329–336. MIT Press, 2007.
Marco Cuturi and Gabriel Peyré. A smoothed dual approach for variational Wasserstein problems. SIAM Journal on Imaging Sciences, 9(1):320–343, 2016.
Marco Cuturi and Gabriel Peyré. Semidual regularized optimal transport. SIAM Review, 60
(4):941–965, 2018.
Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin
monte carlo and gradient descent. In Proceedings of the 2017 Conference on Learning Theory,
volume 65 of Proceedings of Machine Learning Research, pages 678–689. PMLR, 2017.
Arnak S Dalalyan and Avetik G Karagulyan. User-friendly guarantees for the Langevin Monte
Carlo with inaccurate gradient. arXiv preprint arXiv:1710.00095, 2017.
George B. Dantzig. Programming of interdependent activities: II mathematical model. Econometrica, 17(3/4):200–211, 1949.
George B Dantzig. Application of the simplex method to a transportation problem. Activity
Analysis of Production and Allocation, 13:359–373, 1951.
George B. Dantzig.
Reminiscences Aabout the origins of linear programming, pages 78–86.
Springer, 1983.
George B. Dantzig. Linear programming. In J. K. Lenstra, A. H. G. Rinnooy Kan, and A. Schrijver, editors, History of mathematical programming: a collection of personal reminiscences,
pages 257–282. Elsevier Science Publishers, 1991.
References
Jon Dattorro. Convex Optimization & Euclidean Distance Geometry. Meboo Publishing, 2017.
Fernando De Goes, Katherine Breeden, Victor Ostromoukhov, and Mathieu Desbrun. Blue
noise through optimal transport. ACM Transactions on Graphics, 31(6):171, 2012.
Fernando de Goes, Corentin Wallez, Jin Huang, Dmitry Pavlov, and Mathieu Desbrun. Power
particles: an incompressible ﬂuid solver based on power diagrams. ACM Transaction Graphics, 34(4):50:1–50:11, July 2015.
Eustasio del Barrio, JA Cuesta-Albertos, C Matrán, and A Mayo-Íscar. Robust clustering tools
based on optimal transportation. arXiv preprint arXiv:1607.01179, 2016.
Julie Delon. Midway image equalization. Journal of Mathematical Imaging and Vision, 21(2):
119–134, 2004.
Julie Delon, Julien Salomon, and Andrei Sobolevski. Fast transport optimization for Monge
costs on the circle. SIAM Journal on Applied Mathematics, 70(7):2239–2258, 2010.
Julie Delon, Julien Salomon, and Andrei Sobolevski. Local matching indicators for transport
problems with concave costs. SIAM Journal on Discrete Mathematics, 26(2):801–827, 2012.
Edwards Deming and Frederick F Stephan. On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. Annals of Mathematical Statistics,
11(4):427–444, 1940.
Steﬀen Dereich, Michael Scheutzow, and Reik Schottstedt.
Constructive quantization: Approximation by empirical measures. In Annales de l’Institut Henri Poincaré, Probabilités et
Statistiques, volume 49, pages 1183–1203, 2013.
Rachid Deriche.
Recursively implementating the Gaussian and its derivatives.
PhD thesis,
INRIA, 1993.
Arnaud Dessein, Nicolas Papadakis, and Charles-Alban Deledalle.
Parameter estimation in
ﬁnite mixture models by regularized optimal transport: a uniﬁed framework for hard and
soft clustering. arXiv preprint arXiv:1711.04366, 2017.
Arnaud Dessein, Nicolas Papadakis, and Jean-Luc Rouas. Regularized optimal transport and
the rot mover’s distance. Journal of Machine Learning Research, 19(15):1–53, 2018.
Simone Di Marino and Lenaic Chizat. A tumor growth model of Hele-Shaw type as a gradient
ﬂow. Arxiv, 2017.
Khanh Do Ba, Huy L Nguyen, Huy N Nguyen, and Ronitt Rubinfeld. Sublinear time algorithms
for earth mover’s distance. Theory of Computing Systems, 48(2):428–442, 2011.
Jean Dolbeault, Bruno Nazaret, and Giuseppe Savaré.
A new class of transport distances
between measures. Calculus of Variations and Partial Diﬀerential Equations, 34(2):193–231,
Yan Dolinsky and H Mete Soner. Martingale optimal transport and robust hedging in continuous
time. Probability Theory and Related Fields, 160(1-2):391–427, 2014.
Richard M. Dudley. The speed of mean Glivenko-Cantelli convergence. Annals of Mathematical
Statistics, 40(1):40–50, 1969.
Arnaud Dupuy, Alfred Galichon, and Yifei Sun. Estimating matching aﬃnity matrix under
low-rank constraints. Arxiv:1612.09585, 2016.
References
Pavel Dvurechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich.
Decentralize and randomize: Faster algorithm for wasserstein barycenters.
In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances
in Neural Information Processing Systems 31, pages 10783–10793. 2018.
Pavel Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational optimal transport: Complexity by accelerated gradient descent is better than by sinkhorn’s algorithm. In
Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1367–
1376. PMLR, 2018.
Richard L Dykstra. An algorithm for restricted least squares regression. Journal American
Statistical Association, 78(384):839–842, 1983.
Richard L Dykstra. An iterative procedure for obtaining I-projections onto the intersection of
convex sets. Annals of Probability, 13(3):975–984, 1985.
Jonathan Eckstein and Dimitri P Bertsekas. On the Douglas-Rachford splitting method and
the proximal point algorithm for maximal monotone operators. Mathematical Programming,
55:293–318, 1992.
David A Edwards. The structure of superspace. In Studies in topology, pages 121–133. Elsevier,
Tarek A El Moselhy and Youssef M Marzouk. Bayesian inference with optimal maps. Journal
of Computational Physics, 231(23):7815–7850, 2012.
Dominik Maria Endres and Johannes E Schindelin. A new metric for probability distributions.
IEEE Transactions on Information theory, 49(7):1858–1860, 2003.
Matthias Erbar. The heat equation on manifolds as a gradient ﬂow in the Wasserstein space.
Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, 46(1):1–23, 2010.
Matthias Erbar and Jan Maas. Gradient ﬂow structures for discrete porous medium equations.
Discrete and Continuous Dynamical Systems, 34(4):1355–1374, 2014.
Sven Erlander. Optimal Spatial Interaction and the Gravity Model, volume 173. Springer-Verlag,
Sven Erlander and Neil F Stewart. The Gravity Model in Transportation Analysis: Theory and
Extensions. 1990.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(1-2):115–166, 2018.
Montacer Essid and Justin Solomon. Quadratically-regularized optimal transport on graphs.
arXiv preprint arXiv:1704.08200, 2017.
Lawrence C. Evans and Wilfrid Gangbo.
Diﬀerential Equations Methods for the Monge-
Kantorovich Mass Transfer Problem, volume 653. American Mathematical Society, 1999.
Mikhail Feldman and Robert McCann. Monge’s transport problem on a Riemannian manifold.
Transaction AMS, 354(4):1667–1697, 2002.
Jean Feydy, Benjamin Charlier, Francois-Xavier Vialard, and Gabriel Peyré. Optimal transport
for diﬀeomorphic registration. In Proceedings of MICCAI’17, pages 291–299. Springer, 2017.
References
Jean Feydy, Thibault Séjourné, Franccois-Xavier Vialard, Shun-Ichi Amari, Alain Trouvé, and
Gabriel Peyré. Interpolating between optimal transport and mmd using sinkhorn divergences.
In Proceedings of the 22th International Conference on Artiﬁcial Intelligence and Statistics,
Alessio Figalli. The optimal partial transport problem. Archive for Rational Mechanics and
Analysis, 195(2):533–560, 2010.
Rémi Flamary, Cédric Févotte, Nicolas Courty, and Valentin Emiya. Optimal spectral transportation with application to music transcription. In Advances in Neural Information Processing Systems, pages 703–711, 2016.
Lester Randolph Ford and Delbert Ray Fulkerson. Flows in Networks. Princeton University
Press, 1962.
Peter J Forrester and Mario Kieburg. Relating the Bures measure to the Cauchy two-matrix
model. Communications in Mathematical Physics, 342(1):151–187, 2016.
Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein distance of
the empirical measure. Probability Theory and Related Fields, 162(3-4):707–738, 2015.
Joel Franklin and Jens Lorenz. On the scaling of multidimensional matrices. Linear Algebra
and its Applications, 114:717–735, 1989.
Uriel Frisch, Sabino Matarrese, Roya Mohayaee, and Andrei Sobolevski. A reconstruction of
the initial conditions of the universe by optimal mass transportation. Nature, 417(6886):
260–262, 2002.
Brittany D Froese and Adam M Oberman. Convergent ﬁnite diﬀerence solvers for viscosity
solutions of the elliptic monge–ampère equation in dimensions two and higher. SIAM Journal
on Numerical Analysis, 49(4):1692–1714, 2011.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio.
Learning with a Wasserstein loss. In Advances in Neural Information Processing Systems,
pages 2053–2061, 2015.
Daniel Gabay and Bertrand Mercier. A dual algorithm for the solution of nonlinear variational
problems via ﬁnite element approximation. Computers & Mathematics with Applications, 2
(1):17–40, 1976.
Alfred Galichon. Optimal Transport Methods in Economics. Princeton University Press, 2016.
Alfred Galichon and Bernard Salanié. Matching with trade-oﬀs: revealed preferences over competing characteristics. Technical report, Preprint SSRN-1487307, 2009.
Alfred Galichon, Pierre Henry-Labordère, and Nizar Touzi. A stochastic control approach to
no-arbitrage bounds given marginals, with an application to lookback options. Annals of
Applied Probability, 24(1):312–336, 2014.
Thomas O Gallouët and Quentin Mérigot. A lagrangian scheme à la brenier for the incompressible euler equations. Foundations of Computational Mathematics, 18:1–31, 2017.
Thomas O Gallouët and Leonard Monsaingeon.
A JKO splitting scheme for Kantorovich–
Fisher–Rao gradient ﬂows. SIAM Journal on Mathematical Analysis, 49(2):1100–1130, 2017.
Wilfrid Gangbo and Robert J McCann. The geometry of optimal transportation. Acta Mathematica, 177(2):113–161, 1996.
References
Wilfrid Gangbo and Andrzej Swiech.
Optimal maps for the multidimensional Monge-
Kantorovich problem. Communications on Pure and Applied Mathematics, 51(1):23–45, 1998.
RUI GAO, Liyan Xie, Yao Xie, and Huan Xu. Robust hypothesis testing using wasserstein
uncertainty sets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 7913–7923.
Matthias Gelbrich. On a formula for the l2 wasserstein metric between measures on euclidean
and hilbert spaces. Mathematische Nachrichten, 147(1):185–203, 1990.
Aude Genevay, Marco Cuturi, Gabriel Peyré, and Francis Bach. Stochastic optimization for
large-scale optimal transport. In Advances in Neural Information Processing Systems, pages
3440–3448, 2016.
Aude Genevay, Gabriel Peyré, and Marco Cuturi. GAN and VAE from an optimal transport
point of view. (arXiv preprint arXiv:1706.01807), 2017.
Aude Genevay, Gabriel Peyré, and Marco Cuturi. Learning generative models with Sinkhorn
divergences. In Proceedings of the 21st International Conference on Artiﬁcial Intelligence
and Statistics, pages 1608–1617, 2018.
Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyré. Sample complexity of sinkhorn divergences. In Proceedings of the 22th International Conference on Artiﬁcial
Intelligence and Statistics, 2019.
Ivan Gentil, Christian Léonard, and Luigia Ripani. About the analogy between optimal transport and minimal entropy. arXiv preprint arXiv:1510.08230, 2015.
Alan George and Joseph WH Liu. The evolution of the minimum degree ordering algorithm.
SIAM Review, 31(1):1–19, 1989.
Tryphon T Georgiou and Michele Pavon. Positive contraction mappings for classical and quantum Schrödinger systems. Journal of Mathematical Physics, 56(3):033301, 2015.
Pascal Getreuer. A survey of Gaussian convolution algorithms. Image Processing On Line,
2013:286–310, 2013.
Ugo Gianazza, Giuseppe Savaré, and Giuseppe Toscani. The Wasserstein gradient ﬂow of the
Fisher information and the quantum drift-diﬀusion equation. Archive for Rational Mechanics
and Analysis, 194(1):133–220, 2009.
Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International Statistical Review, 70(3):419–435, 2002.
Joan Glaunes, Alain Trouvé, and Laurent Younes. Diﬀeomorphic matching of distributions:
a new approach for unlabelled point-sets and sub-manifolds matching.
In Proceedings of
the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
volume 2, 2004.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classiﬁcation: A deep learning approach.
In Proceedings of the 28th International
Conference on Machine Learning, pages 513–520, 2011.
References
Roland Glowinski and A. Marroco.
Sur l’approximation, par éléments ﬁnis d’ordre un, et
la résolution, par pénalisation-dualité d’une classe de problèmes de Dirichlet non linéaires.
ESAIM: Mathematical Modelling and Numerical Analysis, 9(R2):41–76, 1975.
Steven Gold and Anand Rangarajan. A graduated assignment algorithm for graph matching.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(4):377–388, April 1996.
Steven Gold, Anand Rangarajan, Chien-Ping Lu, Suguna Pappu, and Eric Mjolsness. New
algorithms for 2d and 3d point matching: pose estimation and correspondence.
Recognition, 31(8):1019–1031, 1998.
Eusebio Gómez, Miguel A Gómez-Villegas, and J Miguel Marín. A survey on continuous elliptical vector distributions. Rev. Mat. Complut, 16:345–361, 2003.
Paola Gori-Giorgi, Michael Seidl, and Giovanni Vignale. Density-functional theory for strongly
interacting electrons. Physical Review Letters, 103(16):166402, 2009.
Alexandre Gramfort, Gabriel Peyré, and Marco Cuturi. Fast optimal transport averaging of
neuroimaging data. In Information Processing in Medical Imaging - 24th International Conference, IPMI 2015, pages 261–272, 2015.
Kristen Grauman and Trevor Darrell. The pyramid match kernel: discriminative classiﬁcation
with sets of image features. In Tenth IEEE International Conference on Computer Vision,
volume 2, pages 1458–1465. IEEE, 2005.
Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings
with wasserstein procrustes. In Proceedings of the 22th International Conference on Artiﬁcial
Intelligence and Statistics, 2019.
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex J Smola.
A kernel method for the two-sample-problem. In Advances in Neural Information Processing
Systems, pages 513–520, 2007.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Schölkopf, and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723–773,
Andreas Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse
automatic diﬀerentiation. Optimization Methods and Software, 1(1):35–54, 1992.
Andreas Griewank and Andrea Walther. Evaluating Derivatives: Principles and Techniques of
Algorithmic Diﬀerentiation. SIAM, 2008.
Mikhail Gromov. Metric Structures for Riemannian and Non-Riemannian Spaces. Progress in
Mathematics. Birkhäuser, 2001.
Gaoyue Guo and Jan Obloj. Computational methods for martingale optimal transport problems. arXiv preprint arXiv:1710.07911, 2017.
Leonid Gurvits. Classical complexity and quantum entanglement. Journal of Computer and
System Sciences, 69(3):448–484, 2004.
Cristian E Gutiérrez. The Monge-Ampere Equation. Springer, 2016.
Jorge Gutierrez, Julien Rabin, Bruno Galerne, and Thomas Hurtut. Optimal patch assignment
for statistically constrained texture synthesis. In International Conference on Scale Space
and Variational Methods in Computer Vision, pages 172–183. Springer, 2017.
References
A Hadjidimos. Successive overrelaxation (SOR) and related methods. Journal of Computational
and Applied Mathematics, 123(1):177–199, 2000.
Steven Haker, Lei Zhu, Allen Tannenbaum, and Sigurd Angenent. Optimal mass transport for
registration and warping. International Journal of Computer Vision, 60(3):225–240, 2004.
Leonid G Hanin. Kantorovich-Rubinstein norm and its application in the theory of Lipschitz
spaces. Proceedings of the American Mathematical Society, 115(2):345–352, 1992.
Tatsunori Hashimoto, David Giﬀord, and Tommi Jaakkola. Learning population-level diﬀusions
with generative RNNs. In International Conference on Machine Learning, pages 2417–2426,
David Hilbert. Über die gerade linie als kürzeste verbindung zweier punkte. Mathematische
Annalen, 46(1):91–96, 1895.
Frank L Hitchcock. The distribution of a product from several sources to numerous localities.
Studies in Applied Mathematics, 20(1-4):224–230, 1941.
Nhat Ho, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh, and Dinh Phung.
Multilevel clustering via wasserstein means. In International Conference on Machine Learning, pages 1501–1509, 2017.
Thomas Hofmann, Bernhard Schölkopf, and Alexander J Smola. Kernel methods in machine
learning. Annals of Statistics, 36(3):1171–1220, 2008.
David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. Applied Logistic Regression,
volume 398. John Wiley & Sons, 2013.
Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger. Supervised
word mover’s distance. In Advances in Neural Information Processing Systems, pages 4862–
4870, 2016.
Martin Idel. A review of matrix scaling and Sinkhorn’s normal form for matrices and positive
maps. arXiv preprint arXiv:1609.06349, 2016.
Piotr Indyk and Eric Price. K-median clustering, model-based compressive sensing, and sparse
recovery for earth mover distance. In Proceedings of the forty-third annual ACM Symposium
on Theory of Computing, pages 627–636. ACM, 2011.
Piotr Indyk and Nitin Thaper.
Fast image retrieval via embeddings.
In 3rd International
Workshop on Statistical and Computational Theories of Vision, 2003.
Xianhua Jiang, Lipeng Ning, and Tryphon T Georgiou. Distances and Riemannian metrics for
multivariate spectral densities. IEEE Transactions on Automatic Control, 57(7):1723–1735,
William B Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert
space. In Conference in Modern Analysis and Probability , volume 26 of Contemporary Mathematics, pages 189–206. American Mathematical Society, 1984.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-
Planck equation. SIAM Journal on Mathematical Analysis, 29(1):1–17, 1998.
Leonid Kantorovich. On the transfer of masses (in russian). Doklady Akademii Nauk, 37(2):
227–229, 1942.
References
LV Kantorovich and G.S. Rubinstein. On a space of totally additive functions. Vestn Leningrad
Universitet, 13:52–59, 1958.
Hermann Karcher. Riemannian center of mass and so called Karcher mean. arXiv preprint
 
Johan Karlsson and Axel Ringh. Generalized Sinkhorn iterations for regularizing inverse problems using optimal mass transport. arXiv preprint arXiv:1612.02273, 2016.
Sanggyun Kim, Rui Ma, Diego Mesa, and Todd P Coleman. Eﬃcient bayesian inference methods via convex optimization and optimal transport. In IEEE International Symposium on
Information Theory, pages 2259–2263. IEEE, 2013.
David Kinderlehrer and Noel J Walkington. Approximation of parabolic equations using the
Wasserstein metric. ESAIM: Mathematical Modelling and Numerical Analysis, 33(04):837–
852, 1999.
Jun Kitagawa, Quentin Mérigot, and Boris Thibert.
A Newton algorithm for semi-discrete
optimal transport. arXiv preprint arXiv:1603.05579, 2016.
Philip A Knight. The Sinkhorn–Knopp algorithm: convergence and applications. SIAM Journal
on Matrix Analysis and Applications, 30(1):261–275, 2008.
Philip A Knight and Daniel Ruiz. A fast algorithm for matrix balancing. IMA Journal of
Numerical Analysis, 33(3):1029–1047, 2013.
Philip A Knight, Daniel Ruiz, and Bora Uccar. A symmetry preserving algorithm for matrix
scaling. SIAM Journal on Matrix Analysis and Applications, 35(3):931–955, 2014.
Martin Knott and Cyril S Smith. On the optimal mapping of distributions. Journal of Optimization Theory and Applications, 43(1):39–49, 1984.
Martin Knott and Cyril S Smith. On a generalization of cyclic monotonicity and distances
among random vectors. Linear Algebra and Its Applications, 199:363–371, 1994.
Soheil Kolouri, Yang Zou, and Gustavo K Rohde.
Sliced Wasserstein kernels for probability distributions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 5258–5267, 2016.
Soheil Kolouri, Se Rim Park, Matthew Thorpe, Dejan Slepcev, and Gustavo K Rohde. Optimal
mass transport: signal processing and machine-learning applications. IEEE Signal Processing
Magazine, 34(4):43–59, 2017.
Stanislav Kondratyev, Léonard Monsaingeon, and Dmitry Vofnikov. A new optimal transport
distance on the space of ﬁnite Radon measures.
Advances in Diﬀerential Equations, 21
(11/12):1117–1164, 2016.
Tjalling C Koopmans. Optimum utilization of the transportation system. Econometrica: Journal of the Econometric Society, pages 136–146, 1949.
Jonathan Korman and Robert McCann.
Optimal transportation with capacity constraints.
Transactions of the American Mathematical Society, 367(3):1501–1521, 2015.
Bernhard Korte and Jens Vygen. Combinatorial Optimization. Springer, 2012.
JJ Kosowsky and Alan L Yuille. The invisible hand algorithm: Solving the assignment problem
with statistical physics. Neural networks, 7(3):477–490, 1994.
References
J. Kruithof. Telefoonverkeersrekening. De Ingenieur, 52:E15–E25, 1937.
Harold W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics
Quarterly, 2:83–97, 1955.
Brian Kulis. Metric learning: a survey. Foundations and Trends in Machine Learning, 5(4):
287–364, 2012.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to
document distances. In International Conference on Machine Learning, pages 957–966, 2015.
Theo Lacombe, Marco Cuturi, and Steve Oudot. Large scale computation of means and clusters
for persistence diagrams using optimal transport. Advances in Neural Information Processing
Systems 31, pages 9792–9802, 2018.
Rongjie Lai and Hongkai Zhao. Multiscale nonrigid point cloud registration using rotationinvariant sliced-wasserstein distance via laplace–beltrami eigenmap. SIAM Journal on Imaging Sciences, 10(2):449–483, 2017.
Hugo Lavenant. Harmonic mappings valued in the Wasserstein space. Preprint cvgmt 3649,
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, volume 2, pages 2169–2178. IEEE, 2006.
Thibaut Le Gouic and Jean-Michel Loubes. Existence and consistency of Wasserstein barycenters. Probability Theory and Related Fields, 168:901–917, 2016.
Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999.
Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,
Advances in Neural Information Processing Systems 31, pages 2692–2701. 2018.
William Leeb and Ronald Coifman. Hölder–Lipschitz norms and their duals on spaces with
semigroups, with applications to earth mover’s distance. Journal of Fourier Analysis and
Applications, 22(4):910–953, 2016.
Jan Lellmann, Dirk A Lorenz, Carola Schönlieb, and Tuomo Valkonen.
Imaging with
Kantorovich–Rubinstein discrepancy. SIAM Journal on Imaging Sciences, 7(4):2833–2859,
Bas Lemmens and Roger Nussbaum. Nonlinear Perron-Frobenius Theory, volume 189. Cambridge University Press, 2012.
Christian Léonard. From the Schrödinger problem to the Monge–Kantorovich problem. Journal
of Functional Analysis, 262(4):1879–1920, 2012.
Christian Léonard. A survey of the Schrödinger problem and some of its connections with
optimal transport. Discrete Continuous Dynamical Systems Series A, 34(4):1533–1574, 2014.
Bruno Lévy.
A numerical algorithm for l2 semi-discrete optimal transport in 3d.
Mathematical Modelling and Numerical Analysis, 49(6):1693–1715, 2015.
References
Bruno Lévy and Erica L Schwindt. Notions of optimal transport theory and how to implement
them on a computer. Computers & Graphics, 72:135–148, 2018.
Peihua Li, Qilong Wang, and Lei Zhang.
A novel earth mover’s distance methodology for
image matching with Gaussian mixture models. In Proceedings of the IEEE International
Conference on Computer Vision, pages 1689–1696, 2013.
Wuchen Li, Ernest K. Ryu, Stanley Osher, Wotao Yin, and Wilfrid Gangbo. A parallel method
for Earth Mover’s distance. Journal of Scientiﬁc Computing, 75(1):182–197, 2018a.
Yupeng Li, Wuchen Li, and Guo Cao. Image segmentation via l1 monge-kantorovich problem.
CAM report 17-73, 2018b.
Matthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal transport in competition
with reaction: the Hellinger–Kantorovich distance and geodesic curves. SIAM Journal on
Mathematical Analysis, 48(4):2869–2911, 2016.
Matthias Liero, Alexander Mielke, and Giuseppe Savaré. Optimal entropy-transport problems
and a new hellinger–kantorovich distance between positive measures. Inventiones Mathematicae, 211(3):969–1117, 2018.
Haibin Ling and Kazunori Okada.
Diﬀusion distance for histogram comparison.
Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages
246–253. IEEE, 2006.
Haibin Ling and Kazunori Okada. An eﬃcient earth mover’s distance algorithm for robust
histogram comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence,
29(5):840–853, 2007.
Nathan Linial, Alex Samorodnitsky, and Avi Wigderson. A deterministic strongly polynomial
algorithm for matrix scaling and approximate permanents. In Proceedings of the Thirtieth
Annual ACM Symposium on Theory of Computing, pages 644–652. ACM, 1998.
Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear
operators. SIAM Journal on Numerical Analysis, 16:964–979, 1979.
Don O Loftsgaarden and Charles P Quesenberry. A nonparametric estimate of a multivariate
density function. Annals of Mathematical Statistics, 36(3):1049–1051, 1965.
Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Oswaldo Boaventura-Netto, Peter Hahn,
and Tania Querido.
A survey for the quadratic assignment problem.
European Journal
Operational Research, 176(2):657–690, 2007.
Vince Lyzinski, Donniell E Fishkind, Marcelo Fiori, Joshua T Vogelstein, Carey E Priebe, and
Guillermo Sapiro. Graph matching: relax at your own risk. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 38(1):60–73, 2016.
Gradient ﬂows of the entropy for ﬁnite Markov chains.
Journal of Functional
Analysis, 261(8):2250–2292, 2011.
Jan Maas, Martin Rumpf, Carola Schönlieb, and Stefan Simon. A generalized model for optimal
transport of images including dissipation and density modulation. ESAIM: Mathematical
Modelling and Numerical Analysis, 49(6):1745–1769, 2015.
Jan Maas, Martin Rumpf, and Stefan Simon.
Generalized optimal transport with singular
sources. arXiv preprint arXiv:1607.01186, 2016.
References
Yasushi Makihara and Yasushi Yagi. Earth mover’s morphing: Topology-free shape morphing
using cluster-based EMD ﬂows. In Asian Conference on Computer Vision, pages 202–215.
Springer, 2010.
Luigi Malagò, Luigi Montrucchio, and Giovanni Pistone. Wasserstein riemannian geometry of
positive-deﬁnite matrices. arXiv preprint arXiv:1801.09269, 2018.
Anton Mallasto and Aasa Feragen. Learning from uncertain curves: The 2-wasserstein metric
for gaussian processes. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
pages 5660–5670. 2017.
Stephane Mallat. A Wavelet Tour of Signal Processing: the Sparse Way. Academic press, 2008.
Benjamin Mathon, Francois Cayre, Patrick Bas, and Benoit Macq. Optimal transport for secure
spread-spectrum watermarking of still images. IEEE Transactions on Image Processing, 23
(4):1694–1705, 2014.
Daniel Matthes and Horst Osberger. Convergence of a variational Lagrangian scheme for a
nonlinear drift diﬀusion equation. ESAIM: Mathematical Modelling and Numerical Analysis,
48(3):697–726, 2014.
Daniel Matthes and Horst Osberger. A convergent lagrangian discretization for a nonlinear
fourth-order equation. Foundations of Computational Mathematics, 17(1):73–126, 2017.
Bertrand Maury and Anthony Preux. Pressureless Euler equations with maximal density constraint: a time-splitting scheme. Topological Optimization and Optimal Transport: In the
Applied Sciences, 17:333, 2017.
Bertrand Maury, Aude Roudneﬀ-Chupin, and Filippo Santambrogio.
A macroscopic crowd
motion model of gradient ﬂow type. Mathematical Models and Methods in Applied Sciences,
20(10):1787–1821, 2010.
Robert J McCann. A convexity principle for interacting gases. Advances in Mathematics, 128
(1):153–179, 1997.
Facundo Mémoli. On the use of Gromov–Hausdorﬀdistances for shape comparison. In Symposium on Point Based Graphics, pages 81–90. 2007.
Facundo Mémoli. Gromov–Wasserstein distances and the metric approach to object matching.
Foundations of Computational Mathematics, 11(4):417–487, 2011.
Facundo Mémoli and Guillermo Sapiro. A theoretical and computational framework for isometry
invariant recognition of point cloud data. Foundations of Computational Mathematics, 5(3):
313–347, 2005.
Quentin Mérigot. A multiscale approach to optimal transport. Computer Graphics Forum, 30
(5):1583–1592, 2011.
Ludovic Métivier, Romain Brossier, Quentin Merigot, Edouard Oudet, and Jean Virieux. An
optimal transport approach for seismic tomography: Application to 3D full waveform inversion. Inverse Problems, 32(11):115008, 2016.
Jocelyn Meyron, Quentin Mérigot, and Boris Thibert. Light in power: a general and parameterfree algorithm for caustic design. In SIGGRAPH Asia 2018 Technical Papers, page 224. ACM,
References
Alexander Mielke.
Geodesic convexity of the relative entropy in reversible Markov chains.
Calculus of Variations and Partial Diﬀerential Equations, 48(1-2):1–31, 2013.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeﬀrey Dean.
Eﬃcient estimation of word
representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Jean-Marie Mirebeau. Discretization of the 3D Monge-Ampere operator, between wide stencils
and power diagrams. ESAIM: Mathematical Modelling and Numerical Analysis, 49(5):1511–
1523, 2015.
Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Histoire de l’Académie
Royale des Sciences, pages 666–704, 1781.
Grégoire Montavon, Klaus-Robert Müller, and Marco Cuturi. Wasserstein training of restricted
Boltzmann machines. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett,
editors, Advances in Neural Information Processing Systems 29, pages 3718–3726. 2016.
Kevin Moon and Alfred Hero. Multivariate f-divergence estimation with conﬁdence. In Advances in Neural Information Processing Systems, pages 2420–2428, 2014.
Oleg Museyko, Michael Stiglmayr, Kathrin Klamroth, and Günter Leugering. On the application of the Monge–Kantorovich problem to image registration. SIAM Journal on Imaging
Sciences, 2(4):1068–1097, 2009.
Boris Muzellec and Marco Cuturi. Generalizing point embeddings using the wasserstein space
of elliptical distributions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31,
pages 10258–10269. 2018.
Boris Muzellec, Richard Nock, Giorgio Patrini, and Frank Nielsen. Tsallis regularized optimal
transport and ecological inference. In AAAI, pages 2387–2393, 2017.
Andriy Myronenko and Xubo Song. Point set registration: coherent point drift. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(12):2262–2275, 2010.
Assaf Naor and Gideon Schechtman. Planar earthmover is not in l1. SIAM Journal on Computing, 37(3):804–826, 2007.
Richard D Neidinger.
Introduction to automatic diﬀerentiation and Matlab object-oriented
programming. SIAM Review, 52(3):545–563, 2010.
Arkadi Nemirovski and Uriel Rothblum. On complexity of matrix scaling. Linear Algebra and
its Applications, 302:435–460, 1999.
Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex programming, volume 13. SIAM, 1994.
Kangyu Ni, Xavier Bresson, Tony Chan, and Selim Esedoglu. Local histogram based segmentation using the Wasserstein distance. International Journal of Computer Vision, 84(1):97–111,
Lipeng Ning and Tryphon T Georgiou. Metrics for matrix-valued measures via test functions.
In 53rd IEEE Conference on Decision and Control, pages 2642–2647. IEEE, 2014.
Lipeng Ning, Tryphon T Georgiou, and Allen Tannenbaum.
On matrix-valued Monge–
Kantorovich optimal mass transport. IEEE Transactions on Automatic Control, 60(2):373–
382, 2015.
References
Jorge Nocedal and Stephen J Wright. Numerical Optimization. Springer-Verlag, 1999.
Adam M Oberman and Yuanlong Ruan. An eﬃcient linear programming method for optimal
transportation. arXiv preprint arXiv:1509.03668, 2015.
Vladimir Oliker and Laird D Prussner. On the numerical solution of the equation ∂2z
= f and its discretizations, I. Numerische Mathematik, 54(3):271–293, 1989.
Aude Oliva and Antonio Torralba. Modeling the shape of the scene: a holistic representation
of the spatial envelope. International Journal of Computer Vision, 42(3):145–175, 2001.
Dean S Oliver. Minimization for conditional simulation: Relationship to optimal transport.
Journal of Computational Physics, 265:1–15, 2014.
James B. Orlin. A polynomial time primal network simplex algorithm for minimum cost ﬂows.
Mathematical Programming, 78(2):109–129, 1997.
Ferdinand Österreicher and Igor Vajda. A new class of metric divergences on probability spaces
and its applicability in statistics. Annals of the Institute of Statistical Mathematics, 55(3):
639–653, 2003.
Felix Otto.
The geometry of dissipative evolution equations: the porous medium equation.
Communications in Partial Diﬀerential Equations, 26(1-2):101–174, 2001.
Art B Owen. Empirical Likelihood. Wiley Online Library, 2001.
Sinno Jialin Pan and Qiang Yang.
A survey on transfer learning.
IEEE Transactions on
knowledge and data engineering, 22(10):1345–1359, 2010.
Victor M Panaretos and Yoav Zemel. Amplitude and phase variation of point processes. Annals
of Statistics, 44(2):771–812, 2016.
Nicolas Papadakis, Gabriel Peyré, and Edouard Oudet. Optimal transport with proximal splitting. SIAM Journal on Imaging Sciences, 7(1):212–238, 2014.
Brendan Pass. On the local structure of optimal measures in the multi-marginal optimal transportation problem. Calculus of Variations and Partial Diﬀerential Equations, 43(3-4):529–
536, 2012.
Brendan Pass. Multi-marginal optimal transport: theory and applications. ESAIM: Mathematical Modelling and Numerical Analysis, 49(6):1771–1790, 2015.
Oﬁr Pele and Ben Taskar.
The tangent earth mover’s distance.
In Geometric Science of
Information, pages 397–404. Springer, 2013.
Oﬁr Pele and Michael Werman. A linear time histogram metric for improved sift matching.
Computer Vision–ECCV 2008, pages 495–508, 2008.
Oﬁr Pele and Michael Werman. Fast and robust earth mover’s distances. In IEEE 12th International Conference on Computer Vision, pages 460–467, 2009.
Benoît Perthame, Fernando Quirós, and Juan Luis Vázquez. The Hele-Shaw asymptotics for
mechanical models of tumor growth. Archive for Rational Mechanics and Analysis, 212(1):
93–127, 2014.
Gabriel Peyré. Entropic approximation of Wasserstein gradient ﬂows. SIAM Journal on Imaging
Sciences, 8(4):2323–2351, 2015.
References
Gabriel Peyré, Jalal Fadili, and Julien Rabin.
Wasserstein active contours.
In 19th IEEE
International Conference on Image Processing, pages 2541–2544. IEEE, 2012.
Gabriel Peyré, Marco Cuturi, and Justin Solomon. Gromov-Wasserstein averaging of kernel
and distance matrices. In International Conference on Machine Learning, pages 2664–2672,
Gabriel Peyré, Lenaic Chizat, Francois-Xavier Vialard, and Justin Solomon. Quantum entropic
regularization of matrix-valued optimal transport. to appear in European Journal of Applied
Mathematics, 2017.
Rémi Peyre. Comparison between w2 distance and h−1 norm, and localisation of Wasserstein
distance. arXiv preprint arXiv:1104.4631, 2011.
Benedetto Piccoli and Francesco Rossi. Generalized Wasserstein distance and its application
to transport equations with source. Archive for Rational Mechanics and Analysis, 211(1):
335–358, 2014.
Franccois Pitié, Anil C Kokaram, and Rozenn Dahyot. Automated colour grading using colour
distribution transfer. Computer Vision and Image Understanding, 107(1):123–137, 2007.
Pytorch. Pytorch library. 2017.
Julien Rabin and Nicolas Papadakis. Convex color image segmentation with optimal transport
distances. In Proceedings of SSVM’15, pages 256–269, 2015.
Julien Rabin, Gabriel Peyré, Julie Delon, and Marc Bernot. Wasserstein barycenter and its
application to texture mixing. In International Conference on Scale Space and Variational
Methods in Computer Vision, pages 435–446. Springer, 2011.
Svetlozar T Rachev and Ludger Rüschendorf. Mass Transportation Problems: Volume I: Theory.
Springer Science & Business Media, 1998a.
Svetlozar T Rachev and Ludger Rüschendorf. Mass Transportation Problems: Volume II: Applications. Springer Science & Business Media, 1998b.
Louis B Rall. Automatic Diﬀerentiation: Techniques and Applications. Springer, 1981.
Aaditya Ramdas, Nicolás García Trillos, and Marco Cuturi. On Wasserstein two-sample testing
and related families of nonparametric tests. Entropy, 19(2):47, 2017.
Anand Rangarajan, Alan L Yuille, Steven Gold, and Eric Mjolsness. Convergence properties
of the softassign quadratic assignment algorithm.
Neural Computation, 11(6):1455–1474,
August 1999.
Sebastian Reich. A nonparametric ensemble transform method for bayesian inference. SIAM
Journal on Scientiﬁc Computing, 35(4):A2013–A2024, 2013.
R Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal
on Control and Optimization, 14(5):877–898, 1976.
Antoine Rolet, Marco Cuturi, and Gabriel Peyré. Fast dictionary learning with a smoothed
Wasserstein loss. In Proceedings of the 19th International Conference on Artiﬁcial Intelligence
and Statistics, volume 51 of Proceedings of Machine Learning Research, pages 630–638, 2016.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric
for image retrieval. International Journal of Computer Vision, 40(2):99–121, 2000.
References
Ludger Ruschendorf. Convergence of the iterative proportional ﬁtting procedure. Annals of
Statistics, 23(4):1160–1174, 1995.
Lüdger Rüschendorf and Svetlozar T Rachev.
A characterization of random variables with
minimum l2-distance. Journal of Multivariate Analysis, 32(1):48–54, 1990.
Ludger Rüschendorf and Wolfgang Thomsen. Closedness of sum spaces and the generalized
Schrodinger problem. Theory of Probability and its Applications, 42(3):483–494, 1998.
Ludger Rüschendorf and Ludger Uckelmann. On the n-coupling problem. Journal of Multivariate Analysis, 81(2):242–258, 2002.
Ernest K. Ryu, Yongxin Chen, Wuchen Li, and Stanley Osher. Vector and matrix optimal mass
transport: theory, algorithm, and applications. SIAM Journal on Scientiﬁc Compututing, 40
(5):A3675–A3698, 2017a.
Ernest K. Ryu, Wuchen Li, Penghang Yin, and Stanley Osher.
Unbalanced and partial l1
Monge-Kantorovich problem: a scalable parallel ﬁrst-order method.
Journal of Scientiﬁc
Computing, 75(3):1596–1613, 2017b.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal
transport. In International Conference on Learning Representations, 2018.
Hans Samelson et al. On the perron-frobenius theorem. Michigan Mathematical Journal, 4(1):
57–59, 1957.
Roman Sandler and Michael Lindenbaum. Nonnegative matrix factorization with earth mover’s
distance metric for image analysis. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 33(8):1590–1602, 2011.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkhauser, 2015.
Filippo Santambrogio. {Euclidean, metric, and Wasserstein} gradient ﬂows: an overview. Bulletin of Mathematical Sciences, 7(1):87–154, 2017.
Filippo Santambrogio. Crowd motion and population dynamics under density constraints. GMT
 
Louis-Philippe Saumier, Boualem Khouider, and Martial Agueh. Optimal transport for particle
image velocimetry. Communications in Mathematical Sciences, 13(1):269–296, 2015.
Geoﬀrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh
Solomon, Siyan Liu, Stacie Lin, Peter Berube, Lia Lee, et al. Reconstruction of developmental landscapes by optimal-transport analysis of single-cell gene expression sheds light on
cellular reprogramming. bioRxiv, page 191056, 2017.
Bernhard Schmitzer. A sparse multiscale algorithm for dense optimal transport. Journal of
Mathematical Imaging and Vision, 56(2):238–259, 2016a.
Bernhard Schmitzer.
Stabilized sparse scaling algorithms for entropy regularized transport
problems. arXiv preprint arXiv:1610.06519, 2016b.
Bernhard Schmitzer and Christoph Schnörr. Modelling convex shape priors and matching based
on the Gromov-Wasserstein distance. Journal of Mathematical Imaging and Vision, 46(1):
143–159, 2013a.
References
Bernhard Schmitzer and Christoph Schnörr.
Object segmentation by shape matching with
Wasserstein modes. In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, pages 123–136. Springer, 2013b.
Bernhard Schmitzer and Benedikt Wirth. A framework for Wasserstein-1-type metrics. arXiv
 
Isaac J Schoenberg. Metric spaces and positive deﬁnite functions. Transactions of the American
Mathematical Society, 38:522–356, 1938.
Bernhard Schölkopf and Alexander J Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT Press, 2002.
Erwin Schrödinger. Über die Umkehrung der Naturgesetze. Sitzungsberichte Preuss. Akad.
Wiss. Berlin. Phys. Math., 144:144–153, 1931.
Vivien Seguy and Marco Cuturi. Principal geodesic analysis for probability measures under the
optimal transport metric. In Advances in Neural Information Processing Systems 28, pages
3294–3302. 2015.
Vivien Seguy, Bharath Bhushan Damodaran, Rémi Flamary, Nicolas Courty, Antoine Rolet, and
Mathieu Blondel. Large-scale optimal transport and mapping estimation. In Proceedings of
ICLR 2018, 2018.
Soroosh Shaﬁeezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic regression. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 1576–
1584. 2015.
Soroosh Shaﬁeezadeh Abadeh, Viet Anh Nguyen, Daniel Kuhn, and Peyman Mohajerin Mohajerin Esfahani. Wasserstein distributionally robust kalman ﬁltering. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural
Information Processing Systems 31, pages 8483–8492. 2018.
Sameer Shirdhonkar and David W Jacobs. Approximate earth mover’s distance in linear time.
In IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2008.
Bernard W Silverman. Density Estimation for Statistics and Data Analysis, volume 26. CRC
press, 1986.
Richard Sinkhorn. A relationship between arbitrary positive matrices and doubly stochastic
matrices. Annals of Mathematical Statististics, 35:876–879, 1964.
Marcos Slomp, Michihiro Mikamo, Bisser Raytchev, Toru Tamaki, and Kazufumi Kaneda. Gpubased softassign for maximizing image utilization in photomosaics. International Journal of
Networking and Computing, 1(2):211–229, 2011.
Justin Solomon, Leonidas Guibas, and Adrian Butscher.
Dirichlet energy for analysis and
synthesis of soft maps.
In Computer Graphics Forum, volume 32, pages 197–206. Wiley
Online Library, 2013.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Earth mover’s distances on discrete surfaces. Transaction on Graphics, 33(4), 2014a.
References
Justin Solomon, Raif Rustamov, Guibas Leonidas, and Adrian Butscher. Wasserstein propagation for semi-supervised learning. In Proceedings of the 31st International Conference on
Machine Learning, pages 306–314, 2014b.
Justin Solomon, Fernando De Goes, Gabriel Peyré, Marco Cuturi, Adrian Butscher, Andy
Nguyen, Tao Du, and Leonidas Guibas. Convolutional Wasserstein distances: eﬃcient optimal
transportation on geometric domains.
ACM Transactions on Graphics, 34(4):66:1–66:11,
Justin Solomon, Gabriel Peyré, Vladimir G Kim, and Suvrit Sra. Entropic metric alignment
for correspondence problems. ACM Transactions on Graphics, 35(4):72:1–72:13, 2016a.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Continuous-ﬂow graph
transportation distances. arXiv preprint arXiv:1603.06927, 2016b.
Max Sommerfeld and Axel Munk. Inference for empirical wasserstein distances on ﬁnite spaces.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80(1):219–238,
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On integral probability metrics,ϕ-divergences and binary classiﬁcation. arXiv
 
Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG
Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of
Statistics, 6:1550–1599, 2012.
Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. WASP: Scalable Bayes via
barycenters of subset posteriors. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth International Conference on Artiﬁcial Intelligence and Statistics,
volume 38 of Proceedings of Machine Learning Research, pages 912–920, San Diego, California, USA, 2015a. PMLR. URL 
Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. WASP: scalable bayes
via barycenters of subset posteriors. In Artiﬁcial Intelligence and Statistics, pages 912–920,
Matthew Staib, Sebastian Claici, Justin M Solomon, and Stefanie Jegelka. Parallel streaming
wasserstein barycenters. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2647–2658. 2017a.
Matthew Staib, Sebastian Claici, Justin M Solomon, and Stefanie Jegelka. Parallel streaming
wasserstein barycenters. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 2647–2658. 2017b.
Leen Stougie. A polynomial bound on the diameter of the transportation polytope. Technical
report, TU/e, Technische Universiteit Eindhoven, Department of Mathematics and Computing Science, 2002.
Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient ﬂows on the space
of metric measure spaces. Preprint 1208.0434, arXiv, 2012.
References
Zhengyu Su, Yalin Wang, Rui Shi, Wei Zeng, Jian Sun, Feng Luo, and Xianfeng Gu. Optimal
mass transport for shape matching and comparison. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 37(11):2246–2259, 2015.
Vladimir N Sudakov. Geometric Problems in the Theory of Inﬁnite-dimensional Probability
Distributions. Number 141. American Mathematical Society, 1979.
Mahito Sugiyama, Hiroyuki Nakahara, and Koji Tsuda. Tensor balancing on statistical manifold. arXiv preprint arXiv:1702.08142, 2017.
Mohamed M Sulman, JF Williams, and Robert D Russell. An eﬃcient approach for the numerical solution of the monge–ampère equation. Applied Numerical Mathematics, 61(3):298–307,
Paul Swoboda and Christoph Schnörr. Convex variational image restoration with histogram
priors. SIAM Journal on Imaging Sciences, 6(3):1719–1735, 2013.
Gábor J Székely and Maria L Rizzo. Testing for equal distributions in high dimension. InterStat,
5(16.10), 2004.
Asuka Takatsu. Wasserstein geometry of Gaussian measures. Osaka Journal of Mathematics,
48(4):1005–1026, 2011.
Xiaolu Tan and Nizar Touzi. Optimal transportation under controlled stochastic dynamics.
Annals of Probability, 41(5):3201–3240, 2013.
Robert E. Tarjan. Dynamic trees as search trees via euler tours, applied to the network simplex
algorithm. Mathematical Programming, 78(2):169–177, 1997.
Guillaume Tartavel, Gabriel Peyré, and Yann Gousseau. Wasserstein loss for image synthesis
and restoration. SIAM Journal on Imaging Sciences, 9(4):1726–1755, 2016.
Matthew Thorpe, Serim Park, Soheil Kolouri, Gustavo K Rohde, and Dejan Slepčev. A transportation lp distance for signal analysis. Journal of Mathematical Imaging and Vision, 59
(2):187–210, 2017.
AN Tolstoı. Metody nakhozhdeniya naimen’shego summovogo kilome-trazha pri planirovanii
perevozok v prostranstve (russian; methods of ﬁnding the minimal total kilometrage in cargo
transportation planning in space). TransPress of the National Commissariat of Transportation, pages 23–55, 1930.
AN Tolstoı. Metody ustraneniya neratsional’nykh perevozok priplanirovanii [russian; methods
of removing irrational transportation in planning]. Sotsialisticheskiı Transport, 9:28–51, 1939.
Alain Trouvé and Laurent Younes. Metamorphoses through Lie group action. Foundations of
Computational Mathematics, 5(2):173–198, 2005.
Neil S Trudinger and Xu-Jia Wang. On the monge mass transfer problem. Calculus of Variations
and Partial Diﬀerential Equations, 13(1):19–31, 2001.
Marc Vaillant and Joan Glaunès. Surface matching via currents. In Information Processing in
Medical Imaging, pages 1–5. Springer, 2005.
Sathamangalam R Srinivasa Varadhan. On the behavior of the fundamental solution of the
heat equation with variable coeﬃcients. Communications on Pure and Applied Mathematics,
20(2):431–455, 1967.