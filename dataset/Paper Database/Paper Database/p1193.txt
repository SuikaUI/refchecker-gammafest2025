ESRGAN: Enhanced Super-Resolution
Generative Adversarial Networks
Xintao Wang1, Ke Yu1, Shixiang Wu2, Jinjin Gu3, Yihao Liu4,
Chao Dong2, Chen Change Loy5, Yu Qiao2, Xiaoou Tang1
1CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong
2SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology,
Chinese Academy of Sciences 3The Chinese University of Hong Kong, Shenzhen
4University of Chinese Academy of Sciences 5Nanyang Technological University, Singapore
{wx016,yk017,xtang}@ie.cuhk.edu.hk, {sx.wu,chao.dong,yu.qiao}@siat.ac.cn
 , , 
Abstract. The Super-Resolution Generative Adversarial Network (SR-
GAN) is a seminal work that is capable of generating realistic textures
during single image super-resolution. However, the hallucinated details
are often accompanied with unpleasant artifacts. To further enhance the
visual quality, we thoroughly study three key components of SRGAN –
network architecture, adversarial loss and perceptual loss, and improve
each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without
batch normalization as the basic network building unit. Moreover, we
borrow the idea from relativistic GAN to let the discriminator predict
relative realness instead of the absolute value. Finally, we improve the
perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery.
Beneﬁting from these improvements, the proposed ESRGAN achieves
consistently better visual quality with more realistic and natural textures
than SRGAN and won the ﬁrst place in the PIRM2018-SR Challenge1 .
The code is available at 
Introduction
Single image super-resolution (SISR), as a fundamental low-level vision problem, has attracted increasing attention in the research community and AI companies. SISR aims at recovering a high-resolution (HR) image from a single
low-resolution (LR) one. Since the pioneer work of SRCNN proposed by Dong
et al. , deep convolution neural network (CNN) approaches have brought prosperous development. Various network architecture designs and training strategies
have continuously improved the SR performance, especially the Peak Signal-to-
Noise Ratio (PSNR) value . However, these PSNR-oriented
approaches tend to output over-smoothed results without suﬃcient high-frequency
details, since the PSNR metric fundamentally disagrees with the subjective evaluation of human observers .
1 We won the ﬁrst place in region 3 and got the best perceptual index.
 
Xintao Wang et al.
Ground Truth
Fig. 1: The super-resolution results of ×4 for SRGAN2, the proposed ESRGAN
and the ground-truth. ESRGAN outperforms SRGAN in sharpness and details.
Several perceptual-driven methods have been proposed to improve the visual
quality of SR results. For instance, perceptual loss is proposed to optimize super-resolution model in a feature space instead of pixel space. Generative
adversarial network is introduced to SR by to encourage the network
to favor solutions that look more like natural images. The semantic image prior
is further incorporated to improve recovered texture details . One of the
milestones in the way pursuing visually pleasing results is SRGAN . The basic
model is built with residual blocks and optimized using perceptual loss in a
GAN framework. With all these techniques, SRGAN signiﬁcantly improves the
overall visual quality of reconstruction over PSNR-oriented methods.
However, there still exists a clear gap between SRGAN results and the
ground-truth (GT) images, as shown in Fig. 1. In this study, we revisit the
key components of SRGAN and improve the model in three aspects. First, we
improve the network structure by introducing the Residual-in-Residual Dense
Block (RDDB), which is of higher capacity and easier to train. We also remove
Batch Normalization (BN) layers as in and use residual scaling 
and smaller initialization to facilitate training a very deep network. Second, we
improve the discriminator using Relativistic average GAN (RaGAN) , which
learns to judge “whether one image is more realistic than the other” rather than
“whether one image is real or fake”. Our experiments show that this improvement
helps the generator recover more realistic texture details. Third, we propose an
improved perceptual loss by using the VGG features before activation instead of
after activation as in SRGAN. We empirically ﬁnd that the adjusted perceptual
loss provides sharper edges and more visually pleasing results, as will be shown
2 We use the released results of original SRGAN paper – 
box.com/s/lcue6vlrd01ljkdtdkhmfvk7vtjhetog.
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
Perceptual Index
EnhanceNet
EnhanceNet
Results on PIRM self val dataset
Fig. 2: Perception-distortion plane on PIRM self validation dataset. We show
the baselines of EDSR , RCAN and EnhanceNet , and the submitted
ESRGAN model. The blue dots are produced by image interpolation.
in Sec. 4.4. Extensive experiments show that the enhanced SRGAN, termed ES-
RGAN, consistently outperforms state-of-the-art methods in both sharpness and
details (see Fig. 1 and Fig. 7).
We take a variant of ESRGAN to participate in the PIRM-SR Challenge .
This challenge is the ﬁrst SR competition that evaluates the performance in a
perceptual-quality aware manner based on , where the authors claim that
distortion and perceptual quality are at odds with each other. The perceptual
quality is judged by the non-reference measures of Ma’s score and NIQE ,
i.e., perceptual index = 1
2((10−Ma)+NIQE). A lower perceptual index represents
a better perceptual quality.
As shown in Fig. 2, the perception-distortion plane is divided into three
regions deﬁned by thresholds on the Root-Mean-Square Error (RMSE), and the
algorithm that achieves the lowest perceptual index in each region becomes the
regional champion. We mainly focus on region 3 as we aim to bring the perceptual
quality to a new high. Thanks to the aforementioned improvements and some
other adjustments as discussed in Sec. 4.6, our proposed ESRGAN won the ﬁrst
place in the PIRM-SR Challenge (region 3) with the best perceptual index.
In order to balance the visual quality and RMSE/PSNR, we further propose
the network interpolation strategy, which could continuously adjust the reconstruction style and smoothness. Another alternative is image interpolation, which
directly interpolates images pixel by pixel. We employ this strategy to participate in region 1 and region 2. The network interpolation and image interpolation
strategies and their diﬀerences are discussed in Sec. 3.4.
Related Work
We focus on deep neural network approaches to solve the SR problem. As a
pioneer work, Dong et al. propose SRCNN to learn the mapping from LR
Xintao Wang et al.
to HR images in an end-to-end manner, achieving superior performance against
previous works. Later on, the ﬁeld has witnessed a variety of network architectures, such as a deeper network with residual learning , Laplacian pyramid
structure , residual blocks , recursive learning , densely connected network , deep back projection and residual dense network . Speciﬁcally,
Lim et al. propose EDSR model by removing unnecessary BN layers in
the residual block and expanding the model size, which achieves signiﬁcant improvement. Zhang et al. propose to use eﬀective residual dense block in SR,
and they further explore a deeper network with channel attention , achieving the state-of-the-art PSNR performance. Besides supervised learning, other
methods like reinforcement learning and unsupervised learning are also
introduced to solve general image restoration problems.
Several methods have been proposed to stabilize training a very deep model.
For instance, residual path is developed to stabilize the training and improve the
performance . Residual scaling is ﬁrst employed by Szegedy et al. 
and also used in EDSR. For general deep networks, He et al. propose a robust
initialization method for VGG-style networks without BN. To facilitate training
a deeper network, we develop a compact and eﬀective residual-in-residual dense
block, which also helps to improve the perceptual quality.
Perceptual-driven approaches have also been proposed to improve the visual
quality of SR results. Based on the idea of being closer to perceptual similarity , perceptual loss is proposed to enhance the visual quality by minimizing the error in a feature space instead of pixel space. Contextual loss is
developed to generate images with natural image statistics by using an objective
that focuses on the feature distribution rather than merely comparing the appearance. Ledig et al. propose SRGAN model that uses perceptual loss and
adversarial loss to favor outputs residing on the manifold of natural images. Sajjadi et al. develop a similar approach and further explored the local texture
matching loss. Based on these works, Wang et al. propose spatial feature
transform to eﬀectively incorporate semantic prior in an image and improve the
recovered textures.
Throughout the literature, photo-realism is usually attained by adversarial
training with GAN . Recently there are a bunch of works that focus on developing more eﬀective GAN frameworks. WGAN proposes to minimize a
reasonable and eﬃcient approximation of Wasserstein distance and regularizes
discriminator by weight clipping. Other improved regularization for discriminator includes gradient clipping and spectral normalization . Relativistic
discriminator is developed not only to increase the probability that generated data are real, but also to simultaneously decrease the probability that real
data are real. In this work, we enhance SRGAN by employing a more eﬀective
relativistic average GAN.
SR algorithms are typically evaluated by several widely used distortion measures, e.g., PSNR and SSIM. However, these metrics fundamentally disagree with
the subjective evaluation of human observers . Non-reference measures are
used for perceptual quality evaluation, including Ma’s score and NIQE ,
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
both of which are used to calculate the perceptual index in the PIRM-SR Challenge . In a recent study, Blau et al. ﬁnd that the distortion and perceptual
quality are at odds with each other.
Proposed Methods
Our main aim is to improve the overall perceptual quality for SR. In this section, we ﬁrst describe our proposed network architecture and then discuss the
improvements from the discriminator and perceptual loss. At last, we describe
the network interpolation strategy for balancing perceptual quality and PSNR.
Upsampling
Basic Block
Basic Block
Basic Block
Fig. 3: We employ the basic architecture of SRResNet , where most computation is done in the LR feature space. We could select or design “basic blocks”
(e.g., residual block , dense block , RRDB) for better performance.
Network Architecture
In order to further improve the recovered image quality of SRGAN, we mainly
make two modiﬁcations to the structure of generator G: 1) remove all BN layers; 2) replace the original basic block with the proposed Residual-in-Residual
Dense Block (RRDB), which combines multi-level residual network and dense
connections as depicted in Fig. 4.
Residual Block (RB)
Residual in Residual Dense Block (RRDB)
Fig. 4: Left: We remove the BN layers in residual block in SRGAN. Right:
RRDB block is used in our deeper model and β is the residual scaling parameter.
Removing BN layers has proven to increase performance and reduce computational complexity in diﬀerent PSNR-oriented tasks including SR and
deblurring . BN layers normalize the features using mean and variance in a
batch during training and use estimated mean and variance of the whole training dataset during testing. When the statistics of training and testing datasets
diﬀer a lot, BN layers tend to introduce unpleasant artifacts and limit the generalization ability. We empirically observe that BN layers are more likely to bring
Xintao Wang et al.
artifacts when the network is deeper and trained under a GAN framework. These
artifacts occasionally appear among iterations and diﬀerent settings, violating
the needs for a stable performance over training. We therefore remove BN layers
for stable training and consistent performance. Furthermore, removing BN layers
helps to improve generalization ability and to reduce computational complexity
and memory usage.
We keep the high-level architecture design of SRGAN (see Fig. 3), and use a
novel basic block namely RRDB as depicted in Fig. 4. Based on the observation
that more layers and connections could always boost performance , the
proposed RRDB employs a deeper and more complex structure than the original
residual block in SRGAN. Speciﬁcally, as shown in Fig. 4, the proposed RRDB
has a residual-in-residual structure, where residual learning is used in diﬀerent
levels. A similar network structure is proposed in that also applies a multilevel residual network. However, our RRDB diﬀers from in that we use dense
block in the main path as , where the network capacity becomes higher
beneﬁting from the dense connections.
In addition to the improved architecture, we also exploit several techniques
to facilitate training a very deep network: 1) residual scaling , i.e., scaling
down the residuals by multiplying a constant between 0 and 1 before adding them
to the main path to prevent instability; 2) smaller initialization, as we empirically
ﬁnd residual architecture is easier to train when the initial parameter variance
becomes smaller. More discussion can be found in the supplementary material.
The training details and the eﬀectiveness of the proposed network will be
presented in Sec. 4.
Relativistic Discriminator
Besides the improved structure of generator, we also enhance the discriminator
based on the Relativistic GAN . Diﬀerent from the standard discriminator D
in SRGAN, which estimates the probability that one input image x is real and
natural, a relativistic discriminator tries to predict the probability that a real
image xr is relatively more realistic than a fake one xf, as shown in Fig. 5.
ܦݔ௥ൌߪሺܥሺ ሻሻ→1
ܦݔ௙ൌߪሺܥሺ ሻሻ→0
ܦோ௔ݔ௥, ݔ௙ൌߪሺܥ
െॱሾܥሺ ሻሿሻ→1
More realistic
than fake data?
Less realistic
than real data?
b) Relativistic GAN
a) Standard GAN
ܦோ௔ݔ௙, ݔ௥ൌߪሺܥ
െॱሾܥሺ ሻሿሻ→0
Fig. 5: Diﬀerence between standard discriminator and relativistic discriminator.
Speciﬁcally, we replace the standard discriminator with the Relativistic average Discriminator RaD , denoted as DRa. The standard discriminator in
SRGAN can be expressed as D(x) = σ(C(x)), where σ is the sigmoid function
and C(x) is the non-transformed discriminator output. Then the RaD is formulated as DRa(xr, xf) = σ(C(xr) −Exf [C(xf)]), where Exf [·] represents the
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
operation of taking average for all fake data in the mini-batch. The discriminator
loss is then deﬁned as:
D = −Exr[log(DRa(xr, xf))] −Exf [log(1 −DRa(xf, xr))].
The adversarial loss for generator is in a symmetrical form:
G = −Exr[log(1 −DRa(xr, xf))] −Exf [log(DRa(xf, xr))],
where xf = G(xi) and xi stands for the input LR image. It is observed that the
adversarial loss for generator contains both xr and xf. Therefore, our generator
beneﬁts from the gradients from both generated data and real data in adversarial
training, while in SRGAN only generated part takes eﬀect. In Sec. 4.4, we will
show that this modiﬁcation of discriminator helps to learn sharper edges and
more detailed textures.
Perceptual Loss
We also develop a more eﬀective perceptual loss Lpercep by constraining on features before activation rather than after activation as practiced in SRGAN.
Based on the idea of being closer to perceptual similarity , Johnson
et al. propose perceptual loss and it is extended in SRGAN . Perceptual
loss is previously deﬁned on the activation layers of a pre-trained deep network,
where the distance between two activated features is minimized. Contrary to
the convention, we propose to use features before the activation layers, which
will overcome two drawbacks of the original design. First, the activated features
are very sparse, especially after a very deep network, as depicted in Fig. 6.
For example, the average percentage of activated neurons for image ‘baboon’
after VGG19-543 layer is merely 11.17%. The sparse activation provides weak
supervision and thus leads to inferior performance. Second, using features after
activation also causes inconsistent reconstructed brightness compared with the
ground-truth image, which we will show in Sec. 4.4.
Therefore, the total loss for the generator is:
LG = Lpercep + λLRa
where L1 = Exi||G(xi) −y||1 is the content loss that evaluate the 1-norm distance between recovered image G(xi) and the ground-truth y, and λ, η are the
coeﬃcients to balance diﬀerent loss terms.
We also explore a variant of perceptual loss in the PIRM-SR Challenge. In
contrast to the commonly used perceptual loss that adopts a VGG network
trained for image classiﬁcation, we develop a more suitable perceptual loss for
SR – MINC loss. It is based on a ﬁne-tuned VGG network for material recognition , which focuses on textures rather than object. Although the gain of
perceptual index brought by MINC loss is marginal, we still believe that exploring perceptual loss that focuses on texture is critical for SR.
3 We use pre-trained 19-layer VGG network , where 54 indicates features obtained
by the 4th convolution before the 5th maxpooling layer, representing high-level features and similarly, 22 represents low-level features.
Xintao Wang et al.
a) activation map of VGG19-22
b) activation map of VGG19-54
2nd Channel
4th Channel
before activation
after activation
48th Channel
13th Channel
Fig. 6: Representative feature maps before and after activation for image ‘baboon’. With the network going deeper, most of the features after activation
become inactive while features before activation contains more information.
Network Interpolation
To remove unpleasant noise in GAN-based methods while maintain a good perceptual quality, we propose a ﬂexible and eﬀective strategy – network interpolation. Speciﬁcally, we ﬁrst train a PSNR-oriented network GPSNR and then obtain
a GAN-based network GGAN by ﬁne-tuning. We interpolate all the corresponding parameters of these two networks to derive an interpolated model GINTERP,
whose parameters are:
= (1 −α) θPSNR
where θINTERP
are the parameters of GINTERP, GPSNR and
GGAN, respectively, and α ∈ is the interpolation parameter.
The proposed network interpolation enjoys two merits. First, the interpolated model is able to produce meaningful results for any feasible α without
introducing artifacts. Second, we can continuously balance perceptual quality
and ﬁdelity without re-training the model.
We also explore alternative methods to balance the eﬀects of PSNR-oriented
and GAN-based methods. For instance, one can directly interpolate their output
images (pixel by pixel) rather than the network parameters. However, such an
approach fails to achieve a good trade-oﬀbetween noise and blur, i.e., the interpolated image is either too blurry or noisy with artifacts (see Sec. 4.5). Another
method is to tune the weights of content loss and adversarial loss, i.e., the parameter λ and η in Eq. (3). But this approach requires tuning loss weights and
ﬁne-tuning the network, and thus it is too costly to achieve continuous control
of the image style.
Experiments
Training Details
Following SRGAN , all experiments are performed with a scaling factor of
×4 between LR and HR images. We obtain LR images by down-sampling HR
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
images using the MATLAB bicubic kernel function. The mini-batch size is set to
16. The spatial size of cropped HR patch is 128 × 128. We observe that training
a deeper network beneﬁts from a larger patch size, since an enlarged receptive
ﬁeld helps to capture more semantic information. However, it costs more training
time and consumes more computing resources. This phenomenon is also observed
in PSNR-oriented methods (see supplementary material).
The training process is divided into two stages. First, we train a PSNRoriented model with the L1 loss. The learning rate is initialized as 2 × 10−4 and
decayed by a factor of 2 every 2 × 105 of mini-batch updates. We then employ
the trained PSNR-oriented model as an initialization for the generator. The
generator is trained using the loss function in Eq. (3) with λ = 5×10−3 and η =
1×10−2. The learning rate is set to 1×10−4 and halved at [50k, 100k, 200k, 300k]
iterations. Pre-training with pixel-wise loss helps GAN-based methods to obtain
more visually pleasing results. The reasons are that 1) it can avoid undesired
local optima for the generator; 2) after pre-training, the discriminator receives
relatively good super-resolved images instead of extreme fake ones (black or
noisy images) at the very beginning, which helps it to focus more on texture
discrimination.
For optimization, we use Adam with β1 = 0.9, β2 = 0.999. We alternately
update the generator and discriminator network until the model converges. We
use two settings for our generator – one of them contains 16 residual blocks,
with a capacity similar to that of SRGAN and the other is a deeper model with
23 RRDB blocks. We implement our models with the PyTorch framework and
train them using NVIDIA Titan Xp GPUs.
For training, we mainly use the DIV2K dataset , which is a high-quality (2K
resolution) dataset for image restoration tasks. Beyond the training set of DIV2K
that contains 800 images, we also seek for other datasets with rich and diverse
textures for our training. To this end, we further use the Flickr2K dataset 
consisting of 2650 2K high-resolution images collected on the Flickr website,
and the OutdoorSceneTraining (OST) dataset to enrich our training set.
We empirically ﬁnd that using this large dataset with richer textures helps the
generator to produce more natural results, as shown in Fig. 8.
We train our models in RGB channels and augment the training dataset
with random horizontal ﬂips and 90 degree rotations. We evaluate our models on widely used benchmark datasets – Set5 , Set14 , BSD100 ,
Urban100 , and the PIRM self-validation dataset that is provided in the
PIRM-SR Challenge.
Qualitative Results
We compare our ﬁnal models on several public benchmark datasets with state-ofthe-art PSNR-oriented methods including SRCNN , EDSR and RCAN ,
and also with perceptual-driven approaches including SRGAN and EnhanceNet
Xintao Wang et al.
face from Set14
EnhanceNet
baboon from Set14
43074 from BSD100
102061 from BSD100
ESRGAN(ours)
（22.44 / 6.70）
（22.73 / 5.73）
（23.04 / 4.89）
（23.12 / 4.20）
（20.87 / 2.68）
（21.15 / 2.62）
（20.35 / 1.98）
（PSNR / Percpetual Index）
（PSNR / Percpetual Index）
（PSNR / Percpetual Index）
（PSNR / Percpetual Index）
EnhanceNet
ESRGAN(ours)
（31.49 / 8.37）
（32.33 / 6.84）
（32.82 / 6.31）
（32.93 / 6.89）
（30.33 / 3.60）
（30.28 / 4.47）
（30.50 / 3.64）
EnhanceNet
ESRGAN(ours)
（25.12 / 6.84）
（25.83 / 5.93）
（26.62 / 5.22）
（26.86 / 4.43）
（24.73 / 2.06）
（25.28 / 1.93）
（24.83 / 1.96）
EnhanceNet
ESRGAN(ours)
（29.29 / 7.35）
（29.62 / 6.46）
（29.76 / 6.25）
（29.79 / 6.22）
（27.69 / 3.00）
（27.29 / 2.74）
（27.69 / 2.76）
Fig. 7: Qualitative results of ESRGAN. ESRGAN produces more natural textures, e.g., animal fur, building structure and grass texture, and also less unpleasant artifacts, e.g., artifacts in the face by SRGAN.
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
 . Since there is no eﬀective and standard metric for perceptual quality, we
present some representative qualitative results in Fig. 7. PSNR (evaluated on
the luminance channel in YCbCr color space) and the perceptual index used in
the PIRM-SR Challenge are also provided for reference.
It can be observed from Fig. 7 that our proposed ESRGAN outperforms
previous approaches in both sharpness and details. For instance, ESRGAN can
produce sharper and more natural baboon’s whiskers and grass textures (see
image 43074) than PSNR-oriented methods, which tend to generate blurry results, and than previous GAN-based methods, whose textures are unnatural and
contain unpleasing noise. ESRGAN is capable of generating more detailed structures in building (see image 102061) while other methods either fail to produce
enough details (SRGAN) or add undesired textures (EnhanceNet). Moreover,
previous GAN-based methods sometimes introduce unpleasant artifacts, e.g.,
SRGAN adds wrinkles to the face. Our ESRGAN gets rid of these artifacts and
produces natural results.
Ablation Study
In order to study the eﬀects of each component in the proposed ESRGAN, we
gradually modify the baseline SRGAN model and compare their diﬀerences.
The overall visual comparison is illustrated in Fig. 8. Each column represents
a model with its conﬁgurations shown in the top. The red sign indicates the
main improvement compared with the previous model. A detailed discussion is
provided as follows.
BN removal. We ﬁrst remove all BN layers for stable and consistent performance without artifacts. It does not decrease the performance but saves the
computational resources and memory usage. For some cases, a slight improvement can be observed from the 2nd and 3rd columns in Fig. 8 (e.g., image 39).
Furthermore, we observe that when a network is deeper and more complicated,
the model with BN layers is more likely to introduce unpleasant artifacts. The
examples can be found in the supplementary material.
Before activation in perceptual loss. We ﬁrst demonstrate that using features before activation can result in more accurate brightness of reconstructed
images. To eliminate the inﬂuences of textures and color, we ﬁlter the image with
a Gaussian kernel and plot the histogram of its gray-scale counterpart. Fig. 9a
shows the distribution of each brightness value. Using activated features skews
the distribution to the left, resulting in a dimmer output while using features
before activation leads to a more accurate brightness distribution closer to that
of the ground-truth.
We can further observe that using features before activation helps to produce
sharper edges and richer textures as shown in Fig. 9b (see bird feather) and Fig. 8
(see the 3rd and 4th columns), since the dense features before activation oﬀer a
stronger supervision than that a sparse activation could provide.
RaGAN. RaGAN uses an improved relativistic discriminator, which is shown
to beneﬁt learning sharper edges and more detailed textures. For example, in
Xintao Wang et al.
39 from PIRM self_val
43074 from BSD100
69015 from BSD100
6 from PIRM self_val
20 from PIRM self_val
208001 from BSD100
baboon from Set14
baboon from Set14
Activation?
Deeper with RRDB?
Standard GAN
Standard GAN Standard GAN
More data?
Fig. 8: Overall visual comparisons for showing the eﬀects of each component in
ESRGAN. Each column represents a model with its conﬁgurations in the top.
The red sign indicates the main improvement compared with the previous model.
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
before activation
after activation
175032 from BSD100
Comparison of grayscale histogram
Pixel Value
Number of Pixels
(a) brightness inﬂuence
after activation
before activation
163085 from BSD100
(b) detail inﬂuence
Fig. 9: Comparison between before activation and after activation.
the 5th column of Fig. 8, the generated images are sharper with richer textures
than those on their left (see the baboon, image 39 and image 43074).
Deeper network with RRDB. Deeper model with the proposed RRDB can
further improve the recovered textures, especially for the regular structures like
the roof of image 6 in Fig. 8, since the deep model has a strong representation
capacity to capture semantic information. Also, we ﬁnd that a deeper model can
reduce unpleasing noises like image 20 in Fig. 8.
In contrast to SRGAN, which claimed that deeper models are increasingly
diﬃcult to train, our deeper model shows its superior performance with easy
training, thanks to the improvements mentioned above especially the proposed
RRDB without BN layers.
Network Interpolation
We compare the eﬀects of network interpolation and image interpolation strategies in balancing the results of a PSNR-oriented model and GAN-based method.
We apply simple linear interpolation on both the schemes. The interpolation
parameter α is chosen from 0 to 1 with an interval of 0.2.
As depicted in Fig. 10, the pure GAN-based method produces sharp edges
and richer textures but with some unpleasant artifacts, while the pure PSNRoriented method outputs cartoon-style blurry images. By employing network
interpolation, unpleasing artifacts are reduced while the textures are maintained.
By contrast, image interpolation fails to remove these artifacts eﬀectively.
Interestingly, it is observed that the network interpolation strategy provides
a smooth control of balancing perceptual quality and ﬁdelity in Fig. 10.
The PIRM-SR Challenge
We take a variant of ESRGAN to participate in the PIRM-SR Challenge .
Speciﬁcally, we use the proposed ESRGAN with 16 residual blocks and also empirically make some modiﬁcations to cater to the perceptual index. 1) The MINC
loss is used as a variant of perceptual loss, as discussed in Sec. 3.3. Despite the
marginal gain on the perceptual index, we still believe that exploring perceptual
loss that focuses on texture is crucial for SR. 2) Pristine dataset , which is
Xintao Wang et al.
net interp
image interp
net interp
image interp
PSNR-oriented
Perceptual-driven, GAN-based
79 from PIRM self_val
3 from PIRM self_val
Fig. 10: The comparison between network interpolation and image interpolation.
used for learning the perceptual index, is also employed in our training; 3) a
high weight of loss L1 up to η = 10 is used due to the PSNR constraints; 4) we
also use back projection as post-processing, which can improve PSNR and
sometimes lower the perceptual index.
For other regions 1 and 2 that require a higher PSNR, we use image interpolation between the results of our ESRGAN and those of a PSNR-oriented
method RCAN . The image interpolation scheme achieves a lower perceptual
index (lower is better) although we observed more visually pleasing results by
using the network interpolation scheme. Our proposed ESRGAN model won the
ﬁrst place in the PIRM-SR Challenge (region 3) with the best perceptual index.
Conclusion
We have presented an ESRGAN model that achieves consistently better perceptual quality than previous SR methods. The method won the ﬁrst place in
the PIRM-SR Challenge in terms of the perceptual index. We have formulated
a novel architecture containing several RDDB blocks without BN layers. In addition, useful techniques including residual scaling and smaller initialization are
employed to facilitate the training of the proposed deep model. We have also
introduced the use of relativistic GAN as the discriminator, which learns to
judge whether one image is more realistic than another, guiding the generator
to recover more detailed textures. Moreover, we have enhanced the perceptual
loss by using the features before activation, which oﬀer stronger supervision and
thus restore more accurate brightness and realistic textures.
ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks
Acknowledgement. This work is supported by SenseTime Group Limited, the
General Research Fund sponsored by the Research Grants Council of the Hong
Kong SAR (CUHK 14241716, 14224316. 14209217), National Natural Science
Foundation of China (U1613211) and Shenzhen Research Program
(JCYJ20170818164704758, JCYJ20150925163005055).