Machine Learning, 54, 153–178, 2004
c⃝2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
Active Sampling for Class Probability Estimation
and Ranking
MAYTAL SAAR-TSECHANSKY
 
Department of Management Science and Information Systems, Red McCombs School of Business,
The University of Texas at Austin, Austin, Texas 78712, USA
FOSTER PROVOST
 
Department of Information Operations & Management Sciences, Leonard N. Stern School of Business, New York
University, 44 West Fourth Street, New York, NY 10012, USA
Editor: Douglas Fisher
In many cost-sensitive environments class probability estimates are used by decision makers to evaluate the expected utility from a set of alternatives. Supervised learning can be used to build class probability
estimates; however, it often is very costly to obtain training data with class labels. Active learning acquires
data incrementally, at each phase identifying especially useful additional data for labeling, and can be used to
economize on examples needed for learning. We outline the critical features of an active learner and present a
sampling-based active learning method for estimating class probabilities and class-based rankings. BOOTSTRAP-LV
identiﬁes particularly informative new data for learning based on the variance in probability estimates, and uses
weighted sampling to account for a potential example’s informative value for the rest of the input space. We show
empirically that the method reduces the number of data items that must be obtained and labeled, across a wide
variety of domains. We investigate the contribution of the components of the algorithm and show that each provides
valuable information to help identify informative examples. We also compare BOOTSTRAP-LV with UNCERTAINTY
SAMPLING, an existing active learning method designed to maximize classiﬁcation accuracy. The results show
that BOOTSTRAP-LV uses fewer examples to exhibit a certain estimation accuracy and provide insights to the
behavior of the algorithms. Finally, we experiment with another new active sampling algorithm drawing from both
UNCERTAINTY SAMPLING and BOOTSTRAP-LV and show that it is signiﬁcantly more competitive with BOOTSTRAP-
LV compared to UNCERTAINTY SAMPLING. The analysis suggests more general implications for improving existing
active sampling algorithms for classiﬁcation.
active learning, cost-sensitive learning, class probability estimation, ranking, supervised learning,
decision trees, uncertainty sampling, selective sampling
Introduction
Supervised classiﬁer learning requires data with class labels. In many applications, procuring class labels can be costly. For example, to train diagnostic models experts may need to
read many historical cases. To train document classiﬁers experts may need to read many
documents and assign them labels. To train customer response models, consumers may have
to be given costly incentives to reveal their preferences.
Active learning acquires labeled data incrementally, using the model learned “so far”
to select particularly helpful additional training examples for labeling. When successful,
M. SAAR-TSECHANSKY AND F. PROVOST
active learning methods reduce the number of instances that must be labeled to achieve a
particular level of accuracy. Most existing methods and particularly empirical approaches
for active learning address classiﬁcation problems—they assume the task is to assign cases
to one class (from a ﬁxed set of classes).
Many applications, however, require more than simple classiﬁcation. In particular, probability estimates are central in decision theory, allowing a decision maker to incorporate
costs/beneﬁts for evaluating alternatives. For example, in targeted marketing the estimated
probability that a customer will respond to an offer is combined with the estimated proﬁt
 to evaluate various offer propositions. Other applications require
ranking cases by the likelihood of class membership, to improve the response rate to offer propositions, or to add ﬂexibility for user processing.1 For example, documents can be
ranked by their probability of being of interest to the user, and offers to consumers may be
presented/proposed in order of the probability of purchase or of the expected beneﬁt to the
seller. For these reasons we focus on learning class probability estimation (CPE) models.
In this paper we consider active learning to produce accurate CPEs and class-based rankings from fewer labeled training examples. We assume a (unspeciﬁed) cost is associated
with acquiring labels speciﬁcally rather than with the generation or the obtaining of training
examples. Figure 1 shows the desired behavior of an active learner. The horizontal axis represents the information needed for learning, i.e., the number of labeled training examples, and
the vertical axis represents the error rate of the probabilities produced by the learned model.
Each learning curve shows how error rate decreases as more training data are used. The
upper curve represents the decrease in error from sampling examples randomly for labeling
and training; the lower curve represents sampling actively. The two curves form a “banana”
shape: very early on, the curves are comparable because a model is not yet available to
guide the active sampling. The active sampling curve soon accelerates, because of the careful choice of training examples. Given enough data, random sampling eventually catches up.
We introduce a new sampling-based active learning technique, BOOTSTRAP-LV, for learning CPEs. BOOTSTRAP-LV uses bootstrap samples of available
Training set size
Random Sampling
Active Sampling
Learning curves for active sampling vs. random sampling.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
labeled data to examine the variance in the probability estimates for not-yet-labeled data,
and employs a weight-sampling procedure to select particularly informative examples for
labeling and learning. We show empirically across a range of data sets that BOOTSTRAP-LV
decreases the number of labeled instances needed to achieve accurate probability estimates, or alternatively that it increases the accuracy of the probability estimates for a ﬁxed
number of training data. An analysis of the algorithm’s characteristics and performance
reveals the contributions of its components. The results of the analysis lead to the design of a new algorithm for active sampling that is more competitive with BOOTSTRAP-LV
than a popular existing method and has computational advantages over BOOTSTRAP-LV.
This ﬁnal result further demonstrates how the components of the BOOTSTRAP-LV algorithm
contribute to its efﬁcacy and highlights why existing algorithms do not perform well for
Active learning and the Bootstrap-LV algorithm
The fundamental notion of active sampling has a long history in machine learning. To our
knowledge, the ﬁrst to discuss it explicitly were Simon and Lea and Winston .
Simon and Lea describe how machine learning is different from other types of problem
solving, because learning involves the simultaneous search of two spaces: the hypothesis
space and the instance space. The results of searching the hypothesis space can affect how
the instance space will be sampled. Porter and Kibler address the symbiosis between
learning and problem solving, and propose a learning apprentice system that learns problemsolving rules. Their method reduces reliance on the teacher to provide examples by acting
only when the system is unable to determine what to do next. Winston discusses how
the best examples to select next for learning are “near misses,” instances that miss being
class members for only a few reasons. Subsequently, theoretical results showed that the
number of training data can be reduced substantially if they are selected carefully . The term active learning was coined later to describe induction where the algorithm
controls the selection of potential unlabeled training examples .
A generic algorithm for active learning is shown in ﬁgure 2. A learner ﬁrst is applied to
an initial set L of labeled examples (usually selected at random or provided by an expert).
Subsequently, sets of M examples are selected in phases from a set of unlabeled examples
Generic active learning algorithm.
M. SAAR-TSECHANSKY AND F. PROVOST
UL, until some predeﬁned condition is met (e.g., the labeling budget is exhausted). If UL is
very large a subset of randomly sampled examples from UL may be used as a substitute for
the complete set . In each phase, each candidate example xi ∈UL is
assignedaneffectivenessscore ESi basedonanobjectivefunction,reﬂectingitscontribution
to subsequent learning. Examples then are selected for labeling based on their effectiveness
scores. Often, multiple examples, rather than a single example, are selected in each phase
due to computational constraints. Once examples are selected, their labels are obtained (e.g.,
by querying an expert) before being added to L, to which the learner is applied next.
The objective of active learning is to select examples that will reduce the generalization
error of the model the most. The generalization error is the expected error across the entire
example space. Therefore when evaluating a training example an optimal active learning
approach must evaluate the expected reduction in generalization error if the example were
to be added to the training set from which the model would be induced . The example that is expected to reduce the generalization error the most should
be added to the training set. Unfortunately, as we discuss below, assessing the expected
reduction of CPE generalization error is not straightforward.
We are interested in an active learning scheme that will apply to arbitrary learners,
thus computational considerations may prohibit us from examining the models resulting
from adding each potential unlabeled example to the training set ). We therefore resort to an indirect estimation of potential training
examples’informativevalue.Also,weconsiderthepotentialofeachtrainingexampletohelp
improve the estimation of other examples in the space, which we describe in detail below.
Given the generic framework presented in ﬁgure 2, BOOTSTRAP-LV embodies a particular instantiation of steps 3 and 4. The description we provide here pertains to binary
classiﬁcation problems.
Since our goal is to reduce the class probability estimation (CPE) error, it is useful
to understand the error’s sources. A model’s estimation ˆf (x | T ) for a particular input x
depends upon the sample T from which the model is induced, and therefore can be treated
as a random variable. Let f (x) be the underlying function describing the probability of class
membership for a case described by input x. One indication of the quality of the current
class probability estimate ˆf (x | T ) for example x given a training set T is the expected
estimation (absolute) error, reﬂecting the discrepancy between the estimated probability
and the true probability, i.e., | f (x)−ˆf (x | T )|. We may infer from the discrepancy whether
additional information is needed to improve the model’s estimation. Note that unfortunately
in our inductive learning setting we typically do not know the class probability, f (x), for an
input x, even when we do know the true class of a particular instance described by x.
A common formulation of the estimation error decomposes the expected squared estimation error into the sum of two terms: ET [( f (x) −ˆf (x | T )]2 =
ET [ ˆf (x | T ) −ET ˆf (x | T )]2 + [ f (x) −ET ˆf (x | T )]2; ET (·) represents expectation across
training sets T . The ﬁrst term in the sum is referred to as the variance of the estimation and
reﬂects the sensitivity of the estimation to the training sample. The second term is referred
to as the (squared) bias, reﬂecting the extent to which the induced model can approximate
the target function f (x) . Calculating both the estimation error and the
estimation bias requires knowing the actual probability function, f (x), which as mentioned
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
Input: an initial labeled set
L sampled at random, an unlabeled set UL, an inducer , a stopping criterion, and a sample size M.
2 for (s=1;until stopping criterion is met; s++)
3 Generate k bootstrap subsamples
4 Apply inducer I on each subsample
B and induce estimator
5 For all examples {
Sample from the probability distribution
sD, a subset N of M examples from UL
without replacement
7 Remove N from UL, label examples in N, and add them to L
Output: estimator E induced with I from L
The BOOTSTRAP-LV algorithm.
above is not available for an inductive learning algorithm to consider. Therefore it is impossible to compute them directly. The estimation variance, however, reﬂects a behavior of the
estimation procedure without reference to the underlying probability function. Therefore,
in order to reduce the estimation error the BOOTSTRAP-LV algorithm estimates and then tries
to reduce the estimation variance. The estimation variance for a certain input is referred to
as the “local variance” (LV) to differentiate it from the model’s expected variance over the
entire input space. We ignore the bias, or alternatively assume the bias is zero.
The BOOTSTRAP-LV algorithm, shown in ﬁgure 3, ﬁrst estimates the local variance of
each potential training example. If the LV is high, the algorithm infers that this input is not
well captured by the model given the available data. The local variance also reﬂects the
potential error reduction if this variance were reduced as more examples become available
for the learner. BOOTSTRAP-LV then employs the LV estimations together with a specialized
sampling procedure to identify the examples that are particularly likely to reduce the average
estimation error across the entire example space (i.e., the generalization error) the most.
We ﬁrst describe the estimation of the local variance. We then will discuss the sampling
procedure.
Giventhatanefﬁcientclosed-formestimationofthelocalvariancemaynotbeobtainedfor
arbitrary learners, we estimate it empirically. The variance stems from the estimation being
induced from a random sample. We therefore emulate a series of samples by generating a set
of k bootstrap subsamples B j, j = 1, . . . , k from L. We generate
a set of models by applying the inducer I to each bootstrap sample B j, resulting in k estimators E j, j = 1, . . . , k. To calculate the estimated variance, for each example in xi ∈UL, we
estimate the variance among CPEs predicted by the estimators {E j}. Finally, each example
in xi ∈UL is assigned an effectiveness score that is proportional to its local variance.
The local variance provides an indication of the potential error reduction for each individual training example. However, it does not necessarily provide an indication of how
much would be learned about other examples in the space. Recall that our objective is to
reduce the generalization error by training a model with fewer, particularly informative examples; a training example therefore must affect the estimation error of other examples in
the example space. It may be that an example with a very high variance is not well captured
by the model, but is an outlier, not similar to any other examples in the space.
M. SAAR-TSECHANSKY AND F. PROVOST
Many existing active learning algorithms select examples in order of their effectiveness
score, such that the examples with the highest scores are selected for labeling ﬁrst. Let us
refer to this approach as Direct Selection. Direct selection ignores information about how
the class probability estimation error of other examples in the space may be affected by
adding the example to the training set. This information, however, is essential to evaluate
the expected effect an example may have on the generalization error.
Random sampling is often referred to in the active learning literature as “non-informed”
learning . Nevertheless, random
sampling is powerful because it allows the incorporation of information about the distribution of examples even when this information is not known explicitly. For example, consider
the case when examples for labeling are sampled at random. An example may inform the
learning about other examples in the space if it is similar to these examples. Consider a set
of similar examples. With random sampling, the larger this set the more likely it is that an
example from this set is sampled, providing information about a larger number of examples.
Note that this property is obtained without having to capture explicitly how examples are
similar to each other.
In order to reduce the error across the example space, BOOTSTRAP-LV incorporates sampling into the selection of training examples by weight sampling examples for labeling. In
particular, the probability of each example to be sampled is proportional to its effectiveness score, i.e., its local variance. Speciﬁcally, the distribution from which examples are
sampled is given by Ds(xi) =
j=1 [(p j(xi)−¯pi)2]}/ ¯pi,min
, where p j(xi) denotes the estimated
probability an estimator E j assigns to the event that example xi belongs to one of the two
classes (the choice of performing the calculation for either class is arbitrary because the
variance for both classes is equal); ¯pi =
j=1 p j(xi)
; ¯pi,min is the average probability estimation assigned to the minority class by the estimators {E j}, and R is a normalizing factor
R = size(UL)
j=1 [(p j(xi) −¯pi)2]}/ ¯pi,min, so that Ds is a distribution.
There is one additional technical point of note. Consider the case where the classes are
not represented equally in the training data. When high variance exists in regions of the
domain for which the minority class is assigned high probability, it is likely that the region is
relatively better understood than regions with the same variance but for which the majority
class is assigned high probability. In the latter case, the class probability estimation may be
exhibiting high variance due simply to lack of representation of the minority class in the
training data, and would beneﬁt from sampling more from this subset of examples. Therefore
the estimated variance is divided by the average value of the minority-class probability
estimates ¯pi,min. The minority class is determined once from the initial random sample.
Related work
Cohn, Ghahramani, and Jordan propose an active learning approach for statistical
learning models, generating queries (i.e., training examples) from the input space to be
used as inputs to the learning algorithm. This approach directly evaluates the effectiveness
score, i.e., the informative contribution of each example to the learning task. At each phase
the expectation of the variance of the model over the example space is used to generate
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
the example that minimizes this variance. Since it requires a computation in closed form
of the learner’s variance, this approach is impracticable for arbitrary models. In addition,
queries are generated whereas here we are interested in identifying informative examples from an existing set of available unlabeled examples propose this approach for building classiﬁers. At each phase
they update the current model with each additional training example for each possible
label and calculate an effectiveness score, measured as class entropy, as an estimate of
the improvement in classiﬁcation error. They then select the example bringing about the
greatest expected reduction of entropy. The algorithm was shown to be effective, reducing
the number of examples needed to obtain a certain level of accuracy. For many learning
algorithms, however, the induction of a new model for each possible training example may
be prohibitively expensive. A critical requirement for their approach, therefore, allowing it
to be computationally tractable, is that the learning algorithm allow efﬁcient incremental
updates of the model, such as the na¨ıve Bayes algorithm used to classify text documents in
their experiments .
When an efﬁcient closed-form computation of the error or incremental model updating is
not possible, various active learning approaches compute alternative effectiveness scores.
For example, the QUERY BY COMMITTEE (QBC) algorithm was proposed to select training examples actively for training a binary classiﬁer.
Examples are sampled at random, generating a “stream” of potential training examples, and
each example is considered informative (and thus is labeled) if classiﬁers sampled from the
current version space disagree regarding its class prediction. The QBC algorithm employs
disagreement as a binary effectiveness score, designed to capture whether or not uncertainty
exists regarding class prediction given the current labeled examples.
McCallum and Nigam note that a disadvantage of the “stream-based” QBC approach lies in the decision as to whether to label an example being “made on each document
(i.e., example) individually, irrespective of the alternatives.” An attractive method would be
to compare the estimation uncertainty of all the unlabeled training examples, allowing one to
select at each phase the example(s) with the largest classiﬁcation uncertainty. Various other
approaches have been developed within the Query By Committee framework that identify
informative examples for constructing classiﬁers and which use a variety of measures that
quantify the level of uncertainty or the likelihood of classiﬁcation error given the current
labeled data. In particular, these effectiveness scores quantify the estimated informative
value of each example and thereby obtain a ranking of the examples’ informative values.
Subsequently the example(s) with the highest effectiveness score(s) is (are) selected.
For instance, Abe and Mamitsuka use bagging and boosting to generate a committee of classiﬁers and quantify disagreement as the margin (i.e., the difference in weight
assigned to either class). Examples with the minimum margin are selected for labeling. The
ﬁnal classiﬁer is composed of an ensemble of classiﬁers whose votes are used for class prediction. UNCERTAINTY SAMPLING was designed to select informative
M. SAAR-TSECHANSKY AND F. PROVOST
examples to construct binary classiﬁers by adopting the uncertainty notion underlying the
QBC approach, but instead of generating a committee of hypotheses to estimate uncertainty
the algorithm employs a single probabilistic classiﬁer. Examples whose probabilities of
class membership are closest to 0.5 are selected for labeling ﬁrst. UNCERTAINTY SAMPLING
has several attractive properties, which we return to below.
These methods are not designed to improve CPEs or rankings, which is our concern in this
paper; as indicated by their effectiveness scores, most are designed to improve classiﬁcation.
In addition, as opposed to the approach we propose in this paper, these methods do not
incorporate the effect of a potential additional training example on other examples in the
example space. Particularly, they disregard the potential of a training example to reduce
the error of the estimation for other examples. Examples for which the current estimation
is most uncertain may have no signiﬁcant contribution to reducing the estimation error of
other examples in the instance space. The failure to account for this effect was noted by
Argamon-Engelson and Dagan as well as by McCallum and Nigam who
proposed to incorporate an instance density measure explicitly into the effectiveness score,
where the density measure reﬂects how similar are other examples in the space to the one
examined. The underlying assumption is that the proposed similarity measure captures the
relative effect an example would have on reducing the classiﬁcation error of other examples
in the space. The approach was shown to be effective in selecting informative examples for
document classiﬁcation. Yet the proposed density measure is speciﬁc to document items,
where similarity measures are available (e.g., TF/IDF). It is not clear what an appropriate
density measure would be for an arbitrary domain.2
Our approach uses weight sampling, by which we argue it implicitly incorporates properties of the domain to support the selection of examples more likely to be informative
regarding other examples in the space. Note that weight sampling also is employed in the
AdaBoost algorithm on which Iyengar, Apte, and Zhang 
base their active learning approach. Their algorithm results in an ensemble of classiﬁers
where weight sampling is used both to select examples from which successive classiﬁers in
the ensemble are generated as well as to select examples for labeling. Iyengar et al. note that
better results were obtained when examples were sampled compared to when examples are
selected by order of their error measure. They propose to study this phenomenon further
and hypothesize that sampling allows their approach to avoid selecting the same examples
repeatedly. We argue that in addition weight sampling acts to increase the likelihood of
selecting examples that are particularly informative for reducing the generalization error.
As we discuss in the previous paragraph, selecting examples should address the relevance
of each training example to other examples in order to identify examples that will better
decrease the average estimation error (i.e., the generalization error). Moreover, whereas
the domain-speciﬁc approach of McCallum and Nigam modeled the example space explicitly and incorporated a measure of space density into the effectiveness score , the weight-sampling mechanism can be applied seamlessly for arbitrary
In sum, BOOTSTRAP-LV employs an effectiveness score that identiﬁes examples whose
CPE has large variance with respect to the training data used. It uses this measure to indicate
the potential improvement in class probability estimation error, rather than classiﬁcation
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
accuracy. BOOTSTRAP-LV estimates local variance empirically, enabling its computation
with an arbitrary modeling scheme. Lastly, we use a sampling mechanism to complement
the selection of examples for learning. We argue that weight sampling can help account for
the informative value an example confers to other examples in the space.
Experimental evaluation
The experiments we describe here examine BOOTSTRAP-LV’s performance over a range
of domains in order to assess its general ability to identify particularly useful examples
for learning. In Section 4.1 we present our experimental setting. Sections 4.2 and 4.3
present and discuss our results when learning simple probability estimation trees, and when
learning bagged probability estimation trees, respectively. We discuss additional evaluation
measures in Section 4.4. In Section 4.5 we compare BOOTSTRAP-LV with UNCERTAINTY
SAMPLING, an active learning approach designed to improve classiﬁcation accuracy, in
order to provide insight into the operation of the algorithm and its advantage compared to
existing approaches. Finally, we present experiments with a new active learning algorithm
inspired by the empirical investigation that provide further insight into the elements of the
BOOTSTRAP-LV algorithm.
Experimental setting
We applied BOOTSTRAP-LV to 20 data sets, 17 from the UCI machine learning repository
 and 3 used previously to evaluate rule-learning algorithms . Data sets with more than two classes were mapped into two-class problems.
For these data sets the minority class was associated with one class and all remaining classes
were mapped to the second class.
For these experiments we use tree induction to produce class probability estimates.3
In particular, for the experiments presented here, the underlying probability estimator is
a Probability Estimation Tree (PET), an unpruned C4.5 decision tree for
which the Laplace correction is applied at the leaves. Not pruning and using
the Laplace correction had been shown to improve the CPEs produced by PETs .
As models are learned from more data, performance typically improves as a learning curve; BOOTSTRAP-LV aims to obtain comparable performance with fewer labeled
data (recall ﬁgure 1). To evaluate the predictive quality of the CPE models induced by
BOOTSTRAP-LV it would be desirable to compare against the true class probability values,
for example, computing the mean absolute error with respect to the actual probabilities.
However, these data sets contain only class membership information; the true class probabilities are unknown. Instead, we compare the probabilities assigned by the model induced
with BOOTSTRAP-LV at each phase with those assigned by a “best” estimator, EB, as surrogates to the true probabilities, where EB is induced from the entire set of available training
examples L ∪UL (where the labels of all examples are known to us). In particular, we induce EB using bagged PETs, which have shown to produce superior probability estimates
M. SAAR-TSECHANSKY AND F. PROVOST
compared to individual PETs . We then calculate the
mean absolute error, denoted BMAE (Best-estimate Mean Absolute Error), for an estimator
E with respect to EB’s estimation. BMAE is given by BMAE =
i=1 |pEB (xi)−pE(xi)|
pEB(xi) is the estimated probability given by EB; pE(xi) is the probability estimated by E,
and N is the number of (test) examples examined.
We compare the performance of BOOTSTRAP-LV with a method, denoted RANDOM, where
estimators are induced with the same inducer and the same training-set size, but for which
examples are sampled at random. We compare across different sizes of the labeled set L. In
order not have very large sample sizes, M, for large data sets and very small ones for small
data sets, we applied different numbers of sampling phases for different data sets, varying
between 10 and 30; for a given data set at each phase the same number of examples was
added to L. Results are averaged over 10 random, three-way partitions of a data set into an
initial labeled set, an unlabeled set, and a test set against which the estimators are evaluated.
For control, the same partitions were used by both RANDOM and BOOTSTRAP-LV.
The banana curve in ﬁgure 4 shows the relative performance for the Car data set. The
ﬁgure shows that the error of the estimator induced with BOOTSTRAP-LV decreases faster
initially, exhibiting lower error for fewer examples. This demonstrates that examples actively added to the labeled set are more informative (on average), allowing the inducer to
construct a better estimator for a certain number of training examples. Note that for visibility
the algorithms’ performances with the initial labeled set (for which all algorithms perform
identically) are not shown.
Evaluations of active learning algorithms often present only the initial part of the learning curve to demonstrate the efﬁcacy of the algorithm. We summarize the comparative
performance of the competing algorithms instead across the entire leaning curve. In particular, the objective of BOOTSTRAP-LV is to enable learning with fewer examples in order
to obtain a certain level of CPE accuracy. For each data set we calculate a set of measures
Training set size
Random Sampling
Active Sampling
Examples saved
Error reduction
Learning behavior of BOOTSTRAP-LV and RANDOM for the Car data set.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
pertaining to the saving obtained with BOOTSTRAP-LV in terms of the number of examples
that did not need to be labeled when using BOOTSTRAP-LV instead of RANDOM. The number
of examples saved by BOOTSTRAP-LV for a certain performance level is demonstrated in
ﬁgure 4. For each sampling phase of the algorithm we calculate the difference in the number
of examples needed by BOOTSTRAP-LV to obtain the exhibited error level and the number
needed by RANDOM to obtain the same error level. We calculate the average saving across
all sampling phases, referred to as “average saving,” as well as the saving as a percentage
of the number of examples needed by RANDOM (i.e., the percentage of examples saved if
BOOTSTRAP-LV is used instead of RANDOM), referred to as “average relative saving.” For
instance, in the Car domain (ﬁgure 4) the average saving is 155 examples and the average
relative saving is 23.3% of the examples needed by RANDOM
Because of the natural banana shape of the learning curves, even for the ideal case the
performance of estimators induced from any two samples cannot be considerably different at
theﬁnalsamplingphases,asmostoftheavailableexampleshavebeenusedbybothsampling
methods and therefore the samples obtained by the methods become increasingly similar.
An average across all phases provides an indication of whether BOOTSTRAP-LV produces
superior estimations. However, it is even more telling to examine the improvement at the
“fat” part of the banana (where the beneﬁt of active learning is concentrated). To allow
a stable assessment, instead of presenting the saving exhibited by BOOTSTRAP-LV in the
single, best sampling phase, we present the average saving of the top 20% of the sampling
phases. We call this “top-20% saving.” We also present the top-20% saving as a percentage
of the examples needed by RANDOM, referred to as “top-20% relative saving.” For instance,
in the Car domain the top-20% saving is 281 examples or 35.4% of the examples needed
by RANDOM. We also present the percentage of the sampling phases in which a saving was
obtained, that is, where RANDOM needed more examples to obtain the error level exhibited
by BOOTSTRAP-LV. We refer to this as the percentage of phases with savings.
Finally, for each data set we also present the error reduction achieved by BOOTSTRAP-LV
withrespecttoRANDOMforthesamenumberoftrainingexamples.Thisalsoisdemonstrated
in ﬁgure 4. We calculate the average error reduction for the 20% of the phases in which the
largest error reduction is observed, and we refer to the latter as the top-20% error reduction.
For the Car domain the top-20% error reduction is 31.3%.
Results: Bootstrap-LV versus random sampling
For some data sets BOOTSTRAP-LV exhibits even more dramatic results than those presented
for the Car data set above; ﬁgure 5 shows results for the Pendigits data set (the most
impressive “win”). BOOTSTRAP-LV achieves its almost minimal level of error at about
4000 examples. RANDOM requires more than 9300 examples to obtain this error level. It is
important to note that an active learning algorithm’s performance is particularly interesting
in the initial sampling phases demonstrating the performance that can be obtained for a
relatively small portion of the data and therefore a small labeling cost. Similarly to the
results presented in ﬁgure 4, in the initial phases the error exhibited by the model induced
from BOOTSTRAP-LV’s selection of training examples is reduced substantially faster than
when examples are sampled randomly.
M. SAAR-TSECHANSKY AND F. PROVOST
Training set size
Bootstrap-LV
CPE learning curves for the Pendigits data set. BOOTSTRAP-LV accelerates error reduction considerably
in the initial sampling phases.
Training set size
Bootstrap-LV
CPE learning curves for the weather data set where Bootstrap-lv does not provide improvement in CPE.
For 5 of the 20 data sets, BOOTSTRAP-LV did not succeed in accelerating learning much or
at all, as is shown for the Weather data set in ﬁgure 6. Note that the accuracy was comparable
to that obtained with random sampling: neither curve consistently resides above the other.
This is discussed further below.
Table 1 presents a summary of our results for all the data sets. The second column
shows the percentage of phases with savings. The third and fourth columns show the top-
20% relative saving and the top-20% saving (respectively). The ﬁfth and sixth columns of
Table 1 show the average relative saving and the average saving across all sampling phases
by applying BOOTSTRAP-LV. The seventh Column presents the top-20% error reduction.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
Improvement in examples needed and improvement in error using BOOTSTRAP-LV versus RANDOM.
Phases with
Top-20% relative
Avg. relative
Avg. saving
Top-20% relative
savings (%)
saving (%)
saving (#)
saving (%)
error reduction (%)
Breast cancer-w
Contraceptive
Hypothyroid
Letter-a∗∗
Letter-vowel†
Sick-euthyroid
Solar-ﬂare
∗German credit database.
∗∗letter-recognition, letter a.
†letter-recognition, vowels.
In summarizing these results, to be conservative we regard the two methods to be comparable if the percent of phases with saving is 50% ± 15%. Thus our ﬁrst condition for
BOOTSTRAP-LV to be deemed superior is that it exhibits superior performance in at least
65% of the phases examined. In addition, in order for BOOTSTRAP-LV to be superior we
require that the average relative saving be at least 5% or higher (and symmetrically for
RANDOM to be superior the average percentage gain must be –5% or lower). As can be
seen in Table 1 (in bold), in 15 out of the 20 data sets BOOTSTRAP-LV exhibited superior
performance. Particularly, in all but one of these data sets the percentage of phases with
savings is 75% or above. In 13 of those the top-20% relative saving was 30% or more,
and in 9 data sets BOOTSTRAP-LV used 50% or less of the number of examples needed by
RANDOM to achieve the same accuracy level. For the Sick-euthyroid data set, for example,
BOOTSTRAP-LV gradually improves until it is saving more than 70% of the examples (i.e.,
needing fewer than 30% of the examples required by RANDOM to obtain the same level
of accuracy). Since the latter results pertain to the average improvement obtained for the
top-20% phases, the maximal savings are even greater.
M. SAAR-TSECHANSKY AND F. PROVOST
The measures pertaining to the number of examples saved and the error reduction complement each other and can provide interesting insight. For instance, the number of examples
saved can help evaluate the “difﬁculty” of error reduction, as reﬂected by the number of examples required by RANDOM to obtain such reduction. For example, although the top-20%
relative error reduction for Connect-4 is less than 10%, Table 1 shows that RANDOM needs
984 additional examples on average to obtain the same improvement.
For a single data set (Weather) BOOTSTRAP-LV exhibited a negative average saving.
However, the percentage of phases with savings, showing that BOOTSTRAP-LV uses fewer
examples in 41% of phases examined, and ﬁgure 6, both indicate that the two methods
indeed exhibit comparable learning curves for this data set.
An examination of the learning curves for the data sets in which BOOTSTRAP-LV exhibits
insigniﬁcant or no improvement reveals that training examples chosen at random seem to
contribute to error reduction at an almost constant rate. As shown for the Weather data set
in ﬁgure 6 and for the data sets in ﬁgure 7, the learning curves for all these data sets but one
(letter-vowel) have an atypical shape, where additional examples bring an almost constant
reduction in error, rather than the expected decreasing marginal error reduction. This may
indicate that training examples are equally informative regardless of what or how many
examples have been already used for training. An intelligent selection of training examples,
Training set size
Bootstrap-LV
Training set size
Bootstrap-LV
Solar-flare
Training set size
Bootstrap-LV
Letter-vowel
Training set size
Bootstrap-LV
Learning curves for data sets where BOOTSTRAP-LV and RANDOM show comparable performance.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
therefore, is not likely to improve learning, and will produce results comparable to those
obtained with random selection.
Experiments with bagged-PETs
InordertoverifythatBOOTSTRAP-LViseffectivenotsolelywithPETs,wealsoexperimented
with a different CPE learner. Bagged-PETs creates an ensemble of bagged 
trees, where each tree is induced from a different bootstrap 
sample. The trees are used to estimate the class probability of an instance by averaging the
CPEs of the individual PETs in the ensemble. Bagged-PETs are substantially more complex
than simple PETs, but have been shown generally to produce superior CPEs compared
to simple PETs .
BOOTSTRAP-LV’s performance for bagged-PETs concurs with the results obtained for
individual PETs. Particularly, for 15 of the data sets BOOTSTRAP-LV exhibited a percentage
of phases with savings of more than 65% (in 13 of those the percentage of phases with
savings is more than 75%). The top-20% relative saving was 25% or greater in 11 of those
data sets. Only in two data sets is the percentage of phases with savings less than 40%.
Figure 8 shows a comparison between BOOTSTRAP-LV and RANDOM for simple PETs
and for bagged-PETs. The overall error exhibited by the bagged-PETs is lower than for the
simplePETs,andforbothmodelsBOOTSTRAP-LVachievesitslowesterrorwithconsiderably
fewer examples than are required for RANDOM.
Other evaluation criteria
We also evaluated BOOTSTRAP-LV using alternative performance measures: the mean
squared error measure used by Bauer and Kohavi , as well as the area under the
Training set size
BPET Random
BPET Bootstrap-LV
PET Random
PET Bootstrap-LV
CPE learning curves for the Hypothyroid data set showing the performance of BOOTSTRAP-LV and
RANDOM with bagged PETs and with simple PETs.
M. SAAR-TSECHANSKY AND F. PROVOST
Training set size
Bootsrap-LV
Training set size
Bootstrap-LV
Training set size
Bootstrap-LV
Training set size
Bootsrap-LV
Hypothyroid
Training set size
Bootstrap-LV
Hypothyroid
Training set size
Bootstrap-LV
Learning curves for area under the ROC curve and MSE, comparing BOOTSTRAP-LV and RANDOM.
ROC curve (denoted AUC) , which speciﬁcally evaluates ranking accuracy. The results for these measures agree with those obtained with BMAE. For example,
BOOTSTRAP-LV generally leads to fatter ROC curves with fewer examples. Figure 9 presents
learning curves of both measures for the Car, Pendigits and Hypothyroid data sets, whose
learning curves using BMAE were presented earlier.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
Comparisons with uncertainty sampling
We now compare BOOTSTRAP-LV with an active learning algorithm previously shown to
improve classiﬁcation accuracy; improved classiﬁcation accuracy may also result from
improved class probability estimation error. The comparison shows that focusing on improving CPEs indeed adds value, and also provides interesting insight into the properties
of the algorithms.
For the comparison we selected the well-known UNCERTAINTY SAMPLING algorithm
 , proposed for the active learning of binary classiﬁers. Our choice was
based on the generality of the algorithm, allowing it to be applied with an arbitrary modeling
scheme(thatproducesCPEs)andanarbitrarydataset.Inaddition,UNCERTAINTYSAMPLING
focuses on identifying training examples and does not change the classiﬁer architecture.
In contrast, some active learning algorithms result in an ensemble of classiﬁers . Comparing these to active learning for
single classiﬁers (with active or random sampling) confounds the effects of active learning
and producing ensembles.4 UNCERTAINTY SAMPLING allows us to compare the selection
mechanism of the two algorithms over a wide range of domains. We present a summary of
the comparison results in Table 2, where all the measures are the same as in Table 1, except
that the baseline comparison is UNCERTAINTY SAMPLING rather than RANDOM.
BOOTSTRAP-LV exhibits markedly superior performance compared to UNCERTAINTY
SAMPLING. Particularly, BOOTSTRAP-LV is superior for 13 of the data sets (bold), and for
6 data sets the methods exhibit comparable performance, where savings were exhibited in
50% to 60% of the phases. UNCERTAINTY SAMPLING exhibits superior performance for
one data set, Solar-ﬂare, for which it produces better probability estimations (in the prior
comparison for this data set BOOTSTRAP-LV was not considerably better than RANDOM).
Several factors contribute to the weak performance of UNCERTAINTY SAMPLING for CPE
compared to BOOTSTRAP-LV. To understand them, recall the differences between UNCER-
TAINTY SAMPLING and BOOTSTRAP-LV: the effectiveness score each algorithm assigns to
potential training examples and the mechanisms they employ to sample/select examples
for labeling. Consider the latter ﬁrst. Because it uses direct selection, UNCERTAINTY SAM-
PLING does not account for the potential relevance of a training example for improving the
estimation of other examples in the space. It therefore is susceptible to selecting examples
with little contribution to the average error across the example space. This may degrade its
performance, particularly compared to random sampling. Second, its effectiveness score
causes UNCERTAINTY SAMPLING to prefer examples whose CPE is close to 0.5. Thus examples whose true class probability is close to 0.5 and that are captured correctly by the model
(hence their CPE is close to 0.5 as well) are more likely to be selected; yet they provide
little or no new information for learning. Similarly, UNCERTAINTY SAMPLING is less likely
to select examples whose CPEs are close to either 1 or 0, even when these estimations are
erroneous. Note that because UNCERTAINTY SAMPLING was designed for classiﬁcation, this
is reasonable. A CPE that is on the “correct” side of the decision boundary is sufﬁcient to
make a correct classiﬁcation, even though it may exhibit a large estimation error. Hence, this
policy is likely to be productive for selecting examples to improve classiﬁcation accuracy,
but will deny information important for the learner to improve the model’s CPEs.
M. SAAR-TSECHANSKY AND F. PROVOST
Improvement in number of training examples required to achieve a certain accuracy level and improvement in error for a given number of training examples using BOOTSTRAP-LV versus UNCERTAINTY SAMPLING.
Phases with
Top-20% relative
Avg. relative
Avg. saving
Top-20% relative
savings (%)
saving (%)
saving (#)
saving (%)
error reduction (%)
Breast cancer-w
Contraceptive
Hypothyroid
Letter-vowel
Sick-euthyroid
Solar-ﬂare
Note that when CPEs are extreme but on the “correct side” of the decision boundary, an
effort to select examples to improve CPE may undermine an improvement in classiﬁcation
accuracy. This may be inferred from Friedman’s analysis of classiﬁcation error . In particular, binary classiﬁcation error is minimized if the class most likely to occur
is predicted. The probability that due to erroneous CPE the predicted class ˆy is not the most
likely class, denoted yL, is given by
P(ˆy ̸= yL) = I( f < 1/2)
p( ˆf ) d ˆf + I( f ≥1/2)
p( ˆf ) d ˆf
where I(A) = 1 , if A is true, and I(A) = 0 otherwise. Assuming that p( ˆf ) is approximated
with a standard normal distribution. This probability then is given by:
P(ˆy ̸= yL) = 
sign( f −1/2) E ˆf −1/2
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
where  is the upper tail area of the standard normal distribution, and E denotes a statistical
expectation.5 Given a certain estimation variance, when the true class probability f and
the expected probability estimation, E ˆf , are on the same “side” of the decision boundary,
the farther E ˆf is from 0.5, the more the probability of a classiﬁcation error is reduced,
because it is less probable for the estimated class probability to be on the “wrong” side of
the decision boundary.
Therefore, for an active learning algorithm aiming to improve classiﬁcation accuracy, it
may not always be beneﬁcial to improve CPEs. For instance, consider a true class probability
of 0.6 and a mean estimation of 0.8. An attempt to alter the procedure to reduce the mean
estimation to 0.6 increases the likelihood of an estimation that is below 0.5, particularly
when the estimation variance is large, thus increasing the likelihood of a classiﬁcation error.
The effect of weight sampling
We argued earlier for using weight sampling to reduce generalization error. Particularly,
we argued for its ability to account for an example’s potential for reducing the error of
other examples in the example space. Figure 10 shows for the Pendigits data set the error
obtained with weight sampling (viz., using BOOTSTRAP-LV), BOOTSTRAP-LV using direct
selection instead (with the same effectiveness score), and random sampling. For readability
we present the ﬁrst 10 samples. As can be seen in ﬁgure 10, in the initial and most critical
sampling phases for active learning, weight sampling results in lower error compared to
direct selection and to random sampling. This phenomenon can be seen for most of our data
The superiority of BOOTSTRAP-LV over random sampling demonstrates that the weights
assigned to examples in BOOTSTRAP-LV, and which underlie the sampling process, provide
Training set size
Direct Selection
Bootstrap-LV
Figure 10.
Learning curves for weight-sampling, direct selection with BOOTSTRAP-LV’s effectiveness score, and
M. SAAR-TSECHANSKY AND F. PROVOST
useful information for selecting more informative training examples. The models induced
when these weights are ignored and examples are sampled at random (i.e., all weights are
equal) are inferior to those induced when the assigned weights are incorporated to direct
the sampling process.
Additionally, weight sampling also is important because direct selection often provides
results inferior to BOOTSTRAP-LV. As discussed in Section 2, we argue that weight sampling
is important in order to select examples more likely to affect other examples in the space,
and to avoid selecting training examples that, although not well captured by the model
(and hence their estimation may be improved), will not provide information about (many)
other examples in the space—and therefore are not likely to reduce the generalization error
signiﬁcantly. These considerations may result in smaller error reduction for Direct Selection,
as observed in the comparison to BOOTSTRAP-LV. Apparently, choosing examples based on
the potential of reducing the error of a single example, as some methods do, is not sufﬁcient.
It is important to consider the effect of each training example on the general population of
examples in the space.
4.6.1. Improving uncertainty sampling for CPE.
To demonstrate the effect weight sampling has on identifying informative examples, we propose an improvement to UNCER-
TAINTY SAMPLING by incorporating weights which reﬂect the UNCERTAINTY SAMPLING
rationale (for its effectiveness score) and then to weight sample examples according to their
weights. We will show how the performance of this algorithm improves CPE and compare
it to BOOTSTRAP-LV.
Since UNCERTAINTY SAMPLING selects examples whose CPE is close to 0.5, we assign to
each example a weight that reﬂects this distance. In particular, at each sampling phase s, the
weight assigned to example xi is given by Ws(xi) = (0.5−|0.5−pi|)
, where R is a normalization
factor such that W is a distribution. The probability of an example being sampled increases
the closer its CPE is to 0.5. The algorithm denoted WEIGHTED UNCERTAINTY SAMPLING
(WUS), is described in ﬁgure 11 below.
Comparing the new WUS algorithm with BOOTSTRAP-LV for CPE we see that WUS is
much more competitive with BOOTSTRAP-LV than UNCERTAINTY SAMPLING is. A summary
of the results is presented in Table 3. BOOTSTRAP-LV outperforms WUS for 8 data sets (in
Figure 11.
The WEIGHTED UNCERTAINTY SAMPLING algorithm.
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
Improvement in examples needed and improvement in error using BOOTSTRAP-LV versus WEIGHTED
UNCERTAINTY SAMPLING.
Phases with
Top-20% relative
Avg relative
Avg saving
Top-20% relative
savings (%)
saving (%)
saving (#)
saving (%)
error reduction (%)
Breast cancer-w
Contraceptive
Hypothyroid
Letter-vowel
Sick-euthyroid
Solar-ﬂare
bold), BOOTSTRAP-LV and WUS are comparable for 10 data sets and WUS is superior in
two (italicized). In comparison BOOTSTRAP-LV provides superior CPEs compared to UN-
CERTAINTY SAMPLING for 14 out of 20 data sets. For six data sets in which UNCERTAINTY
SAMPLING is inferior to BOOTSTRAP-LV, WUS exhibits comparable performance to that of
BOOTSTRAP-LV. Overall BOOTSTRAP-LV remains superior, yet the new WEIGHTED UNCER-
TAINTY SAMPLING algorithm exhibits improved performance compared to UNCERTAINTY
Figure 12 shows CPE learning curves for BOOTSTRAP-LV, UNCERTAINTY SAMPLING
and WUS for the Connect-4 data set. Whereas UNCERTAINTY SAMPLING is inferior to
BOOTSTRAP-LV for the Connect-4 data set, WUS’s performance is comparable to that of
BOOTSTRAP-LV. We assert that this can be attributed primarily to WUS accounting for a
broader set of considerations when selecting examples, particularly WUS’s consideration
of the potential error reduction effect an example may have on other examples in the space.
Figure 13 shows learning curves of the three algorithms for the sick-euthyroid data set,
where similarly, WUS’s performance is considerably better than that of UNCERTAINTY SAM-
PLING, but the CPE generalization error of BOOTSTRAP-LV still is better than that obtained
M. SAAR-TSECHANSKY AND F. PROVOST
Training set size
Bootstrap-LV
Uncertainty Sampling
Figure 12.
An example where WUS is superior to UNCERTAINTY SAMPLING and achieves performance comparable to that of BOOTSTRAP-LV.
Sick-euthyroid
Training set size
Bootstrap-LV
Uncertainty
Figure 13.
BOOTSTRAP-LV remains superior but WUS shows signiﬁcant improvements compared to UNCER-
TAINTY SAMPLING.
with WUS. The improved performance of BOOTSTRAP-LV in 8 data sets demonstrates that
the weights assigned by BOOTSTRAP-LV better support the sampling mechanism in identifying informative examples to improve CPE. As we discussed in section 4.2, weights assigned
to examples in WUS may not always be adequate to improve CPEs. Particularly, the focus
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
on selecting examples whose CPE is closer to 0.5 and avoiding examples whose CPE is
closer to either 0 or 1 sometimes hinders the reduction of CPE generalization error.
The above results suggest that given an informative effectiveness score, weight sampling
indeed provides important additional information, improving the selection of informative
training examples. Given the performance of the two algorithms, the effectiveness score
computed in BOOTSTRAP-LV is superior to the score assigned to examples by WUS, yet
the effectiveness score in WUS is informative. As we discussed earlier in the paper, by
preferring examples whose CPE is close to 0.5 WUS identiﬁes examples whose class is
uncertain; however, such uncertainty apparently also implies some uncertainty regarding
CPE and can beneﬁt from gaining more relevant evidence. Yet BOOTSTRAP-LV produces
better results because WUS may fail to identify all CPE uncertainties, particularly when
these uncertainties do not imply class uncertainty. In addition, as we mentioned above, a
CPE that is close to 0.5 does not necessarily imply CPE uncertainty when the true CPE is
also close to 0.5 and is correctly estimated by the model.
Our results with WUS further suggest that algorithms for improving classiﬁcation accuracy can capitalize on weight sampling. For example, WUS may also exhibit improved
performance compared to UNCERTAINTY SAMPLING for classiﬁcation accuracy. Similarly,
other effectiveness scores proposed to identify examples to increase classiﬁcation accuracy,
such as entropy, and that do not incorporate additional measures to capture the effect of a
training example on other examples in the space are likely to beneﬁt from weight sampling.
Limitations
The advantages gained by BOOTSTRAP-LV come with computational cost. At each phase of
the algorithm k models are induced from bootstrap samples. If n is the number of training
examples, the cost of generating each bootstrap sample is kO(n); for an arbitrary modeling
scheme whose computational complexity of inducing a model from n examples is C(n),
the added complexity from inducing these k models is kC(n). In order to compute the
weights for weight sampling, the model is applied to estimate the class probability for
all examples in UL. Let the average complexity of applying the model for a particular
input example be A, which depends on the type of model. For each phase, BOOTSTRAP-
LV subsequently samples M examples from UL, a procedure whose generic complexity
is O(M log M) + |UL|, which constitutes the cost of sorting the list of selected random
numbers and of scanning UL for the corresponding examples. Therefore the computation
cost at each phase is k[O(n) + C(n)] + (A + 1) · |UL| + O(M log M), where C(n) and A
are dependent on the modeling scheme used.
Given that the number of examples sampled in each phase, M, is relatively small, the
dominant computational components are C(n), the cost of generating a model (which must
be done k times), and A, the cost of applying a model, which must be done for all of UL. As
mentioned earlier, for a very large unlabeled set, a sample from |UL| can be used instead.
In addition, because of the typical shape of the learning curve, beyond a certain trainingset size the marginal error reduction is insigniﬁcant, whether active learning or random
sampling is employed. Thus, intelligent selection of examples for learning is critical only
in the early part of the curve (where n is small). If the n remains relatively small, multiple
M. SAAR-TSECHANSKY AND F. PROVOST
model induction from bootstrap samples does not constitute a considerable computational
Moreover, BOOTSTRAP-LV provides an appropriate solution whenever labeling costs are
more important than computational costs, such as when the primary concern is to obtain
accurate CPE or ranking with minimal costly labeling.
BOOTSTRAP-LV does not address computational concerns explicitly, as do Lewis and
Catlett . However, while UNCERTAINTY SAMPLING is simpler computationally, its
performance is signiﬁcantly inferior to that of BOOTSTRAP-LV and in the initial sampling
phases is often inferior to random sampling as well. BOOTSTRAP-LV’s performance also
surpasses the performance of WEIGHTED UNCERTAINTY SAMPLING. Yet, since WEIGHTED
UNCERTAINTY SAMPLING also incorporates a CPE uncertainty measure and is computationally simpler it should be considered for active learning of CPEs when computational
concerns are particularly critical.
Lastly, BOOTSTRAP-LV relies on detecting variance in CPEs to infer what examples are
useful for obtaining more accurate estimation. Its performance may be hampered, therefore,
when a low-variance model such as logistic regression is used for learning.
Conclusions
BOOTSTRAP-LV was designed to use fewer labeled training data to produce accurate class
probability estimates. The algorithm addresses two key components of active learning: an
effectiveness score and a selection procedure, which complement each other to identify
particularly informative examples for learning class probability estimates. BOOTSTRAP-LV
is domain independent and is not restricted to a particular learning algorithm.
An empirical evaluation of the approach shows that it performs well, indeed using fewer
training data. The evaluation encompasses a wide range of benchmark domains providing
evidence for the general efﬁcacy of the algorithm. In particular, the results show how the
information provided by the effectiveness scores improves upon random sampling (i.e.,
when all weights are equal). They also show that BOOTSTRAP-LV outperforms an existing
active learning method, UNCERTAINTY SAMPLING. We investigate the properties of the
algorithms to explain these results. For example, we demonstrate how both the weights
assigned to potential training examples and the weight sampling procedure combine to
produce superior CPEs.
Lastly, we use the results of this investigation to propose yet another active learning algorithm, WEIGHTED UNCERTAINTY SAMPLING, which assigns effectiveness scores reﬂecting the rationale of UNCERTAINTY SAMPLING’s effectiveness score, but which in addition,
employs the scores to weight sample examples for training (as does BOOTSTRAP-LV). A
comparison with BOOTSTRAP-LV reveals that BOOTSTRAP-LV still is superior for improving
CPEs, demonstrating the value of BOOTSTRAP-LV’s effectiveness score, but also demonstrates the advantages conferred by weight sampling. The improvement over direct selection
suggests the application of weight sampling with other effectiveness scores proposed in the
literature for the active learning of classiﬁers.
Making decisions in cost-sensitive environments often takes a decision-theoretic approach to evaluating alternatives, requiring the estimation of probabilities of events or
ACTIVE SAMPLING FOR CLASS PROBABILITY ESTIMATION AND RANKING
classes in order to assess alternative decisions. In such environments labeling costs often
also must be taken into account. We have shown that active sampling can be effective
for reducing the cost of labeling necessary to build accurate models for class-probability
estimation and ranking.
Acknowledgments
We thank Vijay Iyengar for helpful comments. We also thank IBM for a Faculty Partnership
Award and The Penn State eBusiness Research Center for its support.
This work is sponsored in part by the Defense Advanced Research Projects Agency
(DARPA) and Air Force Research Laboratory, Air Force Materiel Command, USAF, under
agreement number F30602-01-2-585. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either
expressed or implied, of the Defense Advanced Research Projects Agency (DARPA), the
Air Force Research Laboratory, or the U.S. Government.
1. Classiﬁcation accuracy has been criticized previously as a metric for machine learning research .
3. Probability estimation trees are easy to build, fast computationally, robust across data sets, comprehensible to
human experts, and produce surprisingly good probability-based rankings .
4. Ensembles usually improve learning curves even with random selection.
5. Note that with respect to an active leaning algorithm the estimation procedure whose variance and expectation
appear in the formulation above also incorporates the choice of training examples.