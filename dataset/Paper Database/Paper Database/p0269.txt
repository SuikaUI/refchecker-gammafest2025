warwick.ac.uk/lib-publications
Original citation:
Sirinukunwattana, Korsuk, Pluim, Josien P. W., Chen, Hao, Qi, Xiaojuan, Heng, Pheng-Ann,
Guo, Yun Bo, Wang, Li Yang, Matuszewski, Bogdan J., Brunie, Elia, Sanchez, Urko, Bohm,
Anton, Ronnebergerf, Olaf, Ben Cheikhh, Bassem, Racoceanu, Daniel, Kainz, Philipp, Pfeiffer,
Michael, Urschlerk, Martin, Snead, David R. J. and Rajpoot, Nasir M. (Nasir Mahmood).
 Gland segmentation in colon histology images : the GlaS challenge contest. Medical
Image Analysis, 35. pp. 489-502.
Permanent WRAP URL:
 
Copyright and reuse:
The Warwick Research Archive Portal (WRAP) makes this work by researchers of the
University of Warwick available open access under the following conditions. Copyright ©
and all moral rights to the version of the paper presented here belong to the individual
author(s) and/or other copyright owners. To the extent reasonable and practicable the
material made available in WRAP has been checked for eligibility before being made
available.
Copies of full items can be used for personal research or study, educational, or not-for-profit
purposes without prior permission or charge. Provided that the authors, title and full
bibliographic details are credited, a hyperlink and/or URL is given for the original metadata
page and the content is not changed in any way.
Publisher’s statement:
© 2016, Elsevier. Licensed under the Creative Commons Attribution-NonCommercial-
NoDerivatives 4.0 International 
A note on versions:
The version presented here may differ from the published version or, version of record, if
you wish to cite this item you are advised to consult the publisher’s version. Please see the
‘permanent WRAP URL’ above for details on accessing the published version and note that
access may require a subscription.
For more information, please contact the WRAP Team at: 
Gland Segmentation in Colon Histology Images: The
GlaS Challenge Contest
Korsuk Sirinukunwattanaa,∗, Josien P. W. Pluimb, Hao Chenc, Xiaojuan
Qic, Pheng-Ann Hengc, Yun Bo Guod, Li Yang Wangd, Bogdan J.
Matuszewskid, Elia Brunie, Urko Sancheze, Anton B¨ohmf, Olaf
Ronnebergerf,g, Bassem Ben Cheikhh, Daniel Racoceanuh, Philipp Kainzi,j,
Michael Pfeiﬀerj, Martin Urschlerk,l, David R. J. Sneadm, Nasir M.
Rajpoota,∗
aDepartment of Computer Science, University of Warwick, Coventry, UK, CV4 7AL
bDepartment of Biomedical Engineering, Eindhoven University of Technology,
Eindhoven, Netherlands
cDepartment of Computer Science and Engineering,
The Chinese University of Hong Kong.
dSchool of Engineering, University of Central Lancashire, Preston, UK
eExB Research and Development
fComputer Science Department, University of Freiburg, Germany
gBIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany and
Google-DeepMind, London, UK
hSorbonne Universit´es, UPMC Univ Paris 06, CNRS, INSERM,
Biomedical Imaging Laboratory (LIB), Paris, France
iInstitute of Biophysics, Center for Physiological Medicine, Medical University of Graz,
Graz, Austria
jInstitute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich,
Switzerland
kInstitute for Computer Graphics and Vision, BioTechMed, Graz University of
Technology, Graz, Austria
lLudwig Boltzmann Institute for Clinical Forensic Imaging, Graz, Austria
mDepartment of Pathology, University Hospitals Coventry and Warwickshire,
Walsgrave, Coventry, CV2 2DX, UK
Colorectal adenocarcinoma originating in intestinal glandular structures is
∗Corresponding authors
Email addresses: (Korsuk Sirinukunwattana),
 (Nasir M. Rajpoot)
 
August 30, 2016
the most common form of colon cancer. In clinical practice, the morphology
of intestinal glands, including architectural appearance and glandular formation, is used by pathologists to inform prognosis and plan the treatment
of individual patients.
However, achieving good inter-observer as well as
intra-observer reproducibility of cancer grading is still a major challenge in
modern pathology. An automated approach which quantiﬁes the morphology
of glands is a solution to the problem.
This paper provides an overview to the Gland Segmentation in Colon
Histology Images Challenge Contest (GlaS) held at MICCAI’2015. Details
of the challenge, including organization, dataset and evaluation criteria, are
presented, along with the method descriptions and evaluation results from
the top performing methods.
Histology Image Analysis, Segmentation, Colon Cancer,
Intestinal Gland, Digital Pathology
1. Introduction
Cancer grading is the process of determining the extent of malignancy and
is one of the primary criteria used in clinical practice to inform prognosis and
plan the treatment of individual patients. However, achieving good repro-
ducibility in grading most cancers remains one of the challenges in pathology
practice . With
digitized images of histology slides becoming increasingly ubiquitous, digital
pathology oﬀers a viable solution to this problem . Analysis of
histology images enables extraction of quantitative morphological features,
which can be used for computer-assisted grading of cancer making the grad-
ing process more objective and reproducible than it currently is . This has led to the recent surge in development of algorithms
for histology image analysis.
In colorectal cancer, morphology of intestinal glands including architec-
tural appearance and gland formation is a key criterion for cancer grading
 . Glands are
important histological structures that are present in most organ systems as
the main mechanism for secreting proteins and carbohydrates. An intestinal
gland (colonic crypt) found in the epithelial layer of the colon, is made up
of a single sheet of columnar epithelium, forming a ﬁnger-like tubular struc-
ture that extends from the inner surface of the colon into the underlying
connective tissue . There
are millions of glands in the human colon. Intestinal glands are responsible
for absorption of water and nutrients, secretion of mucus to protect the ep-
ithelium from a hostile chemical and mechanical environment , as well as being a niche for epithelial cells to regenerate . Due to the hostile
environment, the epithelial layer is continuously regenerating and is one of
the fastest regenerating surface in human body . This renewal process requires coordination between cell proliferation,
diﬀerentiation, and apoptosis. The loss of integrity in the epithelial cell re-
generation, through a mechanism that is not yet clearly understood, results
in colorectal adenocarcinoma, the most common type of colon cancer.
Manual segmentation of glands is a laborious process. Automated gland
segmentation will allow extraction of quantitative features associated with
gland morphology from digitized images of CRC tissue slides. Good quality
gland segmentation will pave the way for computer-assisted grading of CRC
and increase the reproducibility of cancer grading. However, consistent good
quality gland segmentation for all the diﬀerentiation grades of cancer has
remained a challenge. This was a main reason for organizing this challenge
The Gland Segmentation in Colon Histology Images (GlaS) challenge1
brought together computer vision and medical image computing researchers
to solve the problem of gland segmentation in digitized images of Hema-
toxylin and Eosin (H&E) stained tissue slides. Participants developed gland
segmentation algorithms, which were applied to benign tissue and to colonic
carcinomas. A training dataset was provided, together with ground truth
annotations by an expert pathologist. The participants developed and op-
timized their algorithms on this dataset. The results were judged on the
performance of the algorithms on test datasets. Success was measured by
how closely the automated segmentation matched the pathologist’s.
2. Related Work
Recent papers indicate the increas-
1 
ing interest in histology image analysis applied to intestinal gland segmenta-
tion. In this section, we review some of these methods.
Wu et al. presented a region growing method, which ﬁrst thresh-
olds an image, in order to separate nuclei from other tissue components.
Large empty regions, which potentially correspond to lumen found in the
middle of glands, are then used to initialize the seed points for region grow-
ing. The expanding process for each seed is terminated when a surround-
ing chain of epithelial nuclei is reached, and subsequently false regions are
removed. Although this method performs well in segmenting healthy and
benign glands, it is less applicable to cancer cases, where the morphology of
glands can be substantially deformed.
In contrast to the above method, which mainly uses pixel-level informa-
tion, Gunduz-Demir et al. represented each tissue component as a
disk. Each disk is represented by a vertex of a graph, with nearby disks
joined by an edge between the corresponding vertices. They proposed an al-
gorithm, using graph connectivity to identify initial seeds for region growing.
To avoid an excessive expansion beyond the glandular region, caused, for ex-
ample, by large gaps in the surrounding epithelial boundary, edges between
nuclear objects are used as a barrier to halt region growing. Those regions
that do not show glandular characteristics are eliminated at the last step.
The validation of this method was limited only to the dataset with healthy
and benign cases.
Fu et al. introduced a segmentation algorithm based on polar
coordinates. A neighborhood of each gland and a center chosen inside the
gland were considered. Using this center to deﬁne polar coordinates, the
neighborhood is displayed in (r, θ) coordinates with the r-axis horizontal
and the θ-axis vertical. One obtains a vertical strip, periodic with period
2π in the vertical direction.
As a result, the closed glandular boundary
is transformed into an approximately vertical periodic path, allowing fast
inference of the boundary through a conditional random ﬁeld model. Support
vector regression is later deployed to verify whether the estimated boundary
corresponds to the true boundary.
The algorithm performs well in both
benign and malignant cases stained by Hematoxylin and DAB. However, the
validation on routine H&E stained images was limited only to healthy cases.
Sirinukunwattana et al. recently formulated a segmentation ap-
proach based on Bayesian inference, which allows prior knowledge of the
spatial connectivity and the arrangement of neighboring nuclei on the ep-
ithelial boundary to be taken into account. This approach treats each glan-
Table 1: Details of the dataset.
Histologic Grade
Number of Images (Width x Height in Pixels)
Training Part
Test Part A
Test Part B
(574 × 433)
(589 × 453)
(775 × 522)
(574 × 433)
(589 × 453)
(775 × 522)
4 (775 × 522)
(567 × 430)
(589 × 453)
(775 × 522)
(578 × 433)
(581 × 442)
(775 × 522)
16 (775 × 522)
dular structure as a polygon made of a random number of vertices. The
idea is based on the observation that a glandular boundary is formed from
closely arranged epithelial nuclei. Connecting edges between these epithelial
nuclei gives a polygon that encapsulates the glandular structure. Inference of
the polygon is made via Reversible-Jump Markov Chain Monte Carlo. The
approach shows favorable segmentation results across all histologic grades
(except for the undiﬀerentiated grade) of colorectal cancers in H&E stained
images. This method is slow but eﬀective.
Most of the works for intestinal gland segmentation have used diﬀer-
ent datasets and/or criteria to assess their algorithms, making it diﬃcult to
objectively compare their performance. There have been many previous ini-
tiatives that provided common datasets and evaluation measures to validate
algorithms on various medical imaging modalities . This not only allows a
meaningful comparison of diﬀerent algorithms but also allows the algorithms
to be implemented and conﬁgured thoroughly to obtain optimal performance
 . Following these successful initiatives, we therefore or-
ganized the Gland Segmentation in Colon Histology Images (GlaS) challenge.
This challenge was a ﬁrst attempt to address the issues of reproducibility and
comparability of the results of intestinal gland segmentation algorithms. It
was also aimed at speeding up even further the development of algorithms for
gland segmentation. Note that none of above methods for intestinal gland
segmentation participated in this competition.
Figure 1: Example images of diﬀerent histologic grades in the dataset: (a) benign and (b)
malignant.
3. Materials
The dataset used in this challenge consists of 165 images derived from
16 H&E stained histological sections of stage T3 or T42 colorectal adenocar-
cinoma. Each section belongs to a diﬀerent patient, and sections were pro-
cessed in the laboratory on diﬀerent occasions. Thus, the dataset exhibits
high inter-subject variability in both stain distribution and tissue architec-
ture. The digitization of these histological sections into whole-slide images
(WSIs) was accomplished using a Zeiss MIRAX MIDI Slide Scanner with a
pixel resolution of 0.465µm. The WSIs were subsequently rescaled to a pixel
resolution of 0.620µm (equivalent to 20× objective magniﬁcation).
A total of 52 visual ﬁelds from both malignant and benign areas across
the entire set of the WSIs were selected in order to cover as wide a vari-
ety of tissue architectures as possible. An expert pathologist (DRJS) then
graded each visual ﬁeld as either ‘benign’ or ‘malignant’, according to the
overall glandular architecture. The pathologist also delineated the boundary
of each individual glandular object on that visual ﬁeld. We used this manual
annotation as ground truth for automatic segmentation. Note that diﬀerent
glandular objects in an image may be part of the same gland. This is because
a gland is a 3-dimensional structure that can appear as separated objects on
a single tissue section. The visual ﬁelds were further separated into smaller,
non-overlapping images, whose histologic grades (i.e. benign or malignant)
were assigned the same value as the larger visual ﬁeld. Representative exam-
ple images of the two grades can be seen in Figure 1. This dataset was also
previously used in the gland segmentation study by Sirinukunwattana et al.
In the challenge, the dataset was separated into Training Part, Test
Part A, and Test Part B. Note that the data were stratiﬁed according
to the histologic grade and the visual ﬁeld before splitting. This was done
to ensure that none of the images from the same visual ﬁeld appears in
diﬀerent parts of the dataset (i.e. Training, Test Part A, or Test Part B).
However, since the data were not stratiﬁed based on patient, diﬀerent visual
2The T in TNM cancer staging refers to the spread of the primary tumour. In colorectal
cancer, stage T3 means the tumour has grown into the outer lining of the bowel wall,
whereas stage T4 means the tumour has grown through the outer lining of the bowel wall.
The cancer stage is diﬀerent from the tumour histologic grade, as the latter indicates the
aggressiveness of the tumour.
ﬁelds from the same slide can appear in diﬀerent parts of the dataset. A
breakdown of the details of the dataset is shown in Table 1. The ground
truth as well as the histologic grade which reﬂects morphology of glandular
structures were provided for every image in the Training Part at the time of
release. We used Test Part A and Test Part B as oﬀ-site and on-site test
datasets respectively. Furthermore, to ensure blindness of evaluation, the
ground truth and histologic grade of each image in the test parts were not
released to the participants.
4. Challenge Organization
The GlaS challenge contest was oﬃcially launched by the co-organizers
(KS, JPWP, DRJS, NMR) on April 21st, 2015, and was widely publicized
through several channels. At the same point, a challenge website3 was set up
to disseminate challenge-related information and to serve as a site for reg-
istration, submission of results, and communication between the organizers
and contestants. The challenge involved 4 stages, as detailed below:
Stage 1: Registration and Release of the Training Data. The registration
was open for a period of about two months .
Interested individuals or groups of up to 3 people that were aﬃliated with an
academic institute or an industrial organization could register and download
the training data (Training Part, see Section 3 for details) to start developing
their gland segmentation algorithms. From this point forward, we will refer
to a separate individual or a group of registrants as a ‘team’.
Stage 2: Submission of a Short Paper. In order to gain access to the ﬁrst
part of the test data, each registered team was required to submit a 2-page
document containing a general description of their segmentation algorithms
and some preliminary results obtained from running each algorithm on the
training data.
Each team could submit up to 3 diﬀerent methods.
intention of this requirement was for the organizers to identify teams who
were serious about participating in the challenge. The organizers based their
reviews on two criteria: clarity of the method description and soundness of
the validation strategy. Segmentation performance was not considered in this
review. The submission of this document was due by July 17th, 2015.
3 
Stage 3: Release of the Test Data Part A and Submission of Segmentation
Results. The ﬁrst part of the test data (Test Part A, see Section 3 for de-
tails) was released on August 14th, 2015 to those teams selected from the
previous stage which also agreed to participate in the GlaS contest. The
teams were given a month to further adjust and optimize their segmentation
algorithms, and carry out segmentation on Part A of the test data. Each
team could hand-in up to 3 sets of results per method submitted in Stage
2. The submission of the segmentation results was due by September 14th,
2015. Evaluation of the submitted results was not disclosed to the teams
until after the challenge event.
Stage 4: GlaS’2015 Challenge Event. The event was held in conjunction
with MICCAI’2015 on October 5th, 2015. All teams were asked to produce
segmentation results on the second part of the test data (Test Part B, see Sec-
tion 3) within 45 minutes. The teams could either bring their own machines
or conduct an experiment remotely. There was no restriction on the num-
ber of machines that the teams could use to produce results. Those teams
that could not be present at the event provided implementations of their
algorithms with which the organizers carried out the segmentation on their
behalf. Each team was also asked to give a short presentation, discussing
their work. At the end of the event, the complete evaluation of segmentation
results across both parts of the test data was announced, which included a
ﬁnal ranking of the submitted methods. This information is also available
on the challenge website.
4.1. Challenge Statistics
By the end of Stage 1, a total of 110 teams from diﬀerent academic and
industrial institutes had registered. A total of 21 teams submitted the 2-page
document for review in Stage 2, and 20 teams were invited to participate in
the GlaS competition event. In Stage 3, only 13 teams submitted results
on Part A of the test data in time. Late entries were neither evaluated nor
considered in the next stage of the competition. On the day of the challenge
event, 11 of the 13 teams that submitted the results on time in Stage 3
attended the on-site competition and presented their work. The organizers
carried out the segmentation on behalf of the other two teams that could not
be present.
5. Evaluation
The performance of each segmentation algorithm was evaluated based on
three criteria: 1) accuracy of the detection of individual glands; 2) volume-
based accuracy of the segmentation of individual glands; and 3) boundary-
based similarity between glands and their corresponding segmentation. It
may seem that volume-based segmentation accuracy would entail boundary-
based segmentation accuracy between a gland and its segmentation. How-
ever, in practice, this is not always the case. The volume-based metric for
segmentation accuracy used in this challenge, was deﬁned and calculated us-
ing the label that the algorithm had assigned to each pixel, but the boundary-
based metric used the position assigned by the algorithm to the boundary
of each gland.
Pixels labels may be fairly accurate, while the boundary
curves are very diﬀerent. The remainder of this section describes all metrics
employed in the evaluation.
We use the concept of a pair of corresponding segmented and ground
truth objects as proposed in Sirinukunwattana et al. . Let S denote a
set of all segmented objects and G denote a set of all ground truth objects.
We also include in each of these sets the empty object ∅. We deﬁne a function
G∗: S →G, by setting, for each segmented object S ∈S, G∗(S) = G ∈G
where G has the largest possible overlapping area with S. Although there
could be more than one G ∈G that maximally overlaps S, this in practice
is extremely rare, and it is good enough to consider one of these G as the
value of G∗(S). If there is no overlapping G, we set G∗(S) = ∅. (However, in
the context of Hausdorﬀdistance – see Section 5.3 – G∗will be extended in
a diﬀerent way.) Similarly, we deﬁne S∗: G →S, by setting, for each G ∈G,
S∗(G) = S ∈S, where S has the largest possible overlapping area with G.
Note that G∗and S∗are, in general, neither injective, nor surjective. Nor
are they inverse to each other, in general. They do, however, assign to each
G an S = S∗(G), and to each S a G = G∗(S).
5.1. Detection Accuracy
The F1 score is employed to measure the detection accuracy of individual
glandular objects.
A segmented glandular object that intersects with at
least 50% of its ground truth object is counted as true positive, otherwise it
is counted as false positive. The number of false negatives is calculated as
the diﬀerence between the number of ground truth objects and the number
of true positives. Given these deﬁnitions, the F1 score is deﬁned by
F1score = 2 · Precision · Recall
Precision + Recall ,
Precision =
and TP, FP, and FN denote respectively the number of true positives, false
positives, and false negatives from all images in the dataset.
5.2. Volume-Based Segmentation Accuracy
5.2.1. Object-Level Dice Index
The Dice index is a measure of agreement or similarity be-
tween two sets of samples. Given G, a set of pixels belonging to a ground
truth object, and S, a set of pixels belonging to a segmented object, the Dice
index is deﬁned as follows:
Dice(G, S) = 2|G ∩S|
|G| + |S|,
where | · | denotes set cardinality. The index ranges over the interval ,
where the higher the value, the more concordant the segmentation result
and the ground truth. A Dice index of 1 implies a perfect agreement. It
is conventional that the segmentation accuracy on an image is calculated by
Dice(Gall, Sall), where Gall denotes the set of pixels of all ground truth objects
and Sall denotes the set of pixels of all segmented objects. The calculation
made in this way measures the segmentation accuracy only at the pixel level,
not at the gland level, which was the main focus of the competition.
To take the notion of an individual gland into account, we employ the
object-level Dice index . Let nG be the number
of non-empty ground truth glands, as annotated by the expert pathologist.
Similarly let nS be the number of glands segmented by the algorithm, that
is the number of non-empty segmented objects. Let Gi ∈G denote the ith
ground truth object, and let Sj ∈S denote the jth segmented object. The
object-level Dice index is deﬁned as
Diceobj(G, S) = 1
γiDice(Gi, S∗(Gi)) +
σjDice(G∗(Sj), Sj)
γi = |Gi|/
σj = |Sj|/
On the right hand side of (4), the ﬁrst summation term reﬂects how well each
ground truth object overlaps its segmented object, and the second summa-
tion term reﬂects how well each segmented object overlaps its ground truth
objects. Each term is weighted by the relative area of the object, giving less
emphasis to small segmented and small ground truth objects.
In the competition, the object-level Dice index of the whole test dataset
was calculated by including all the ground truth objects from all images in
G and all the segmented objects from all images in S.
5.2.2. Adjusted Rand Index
We also included the adjusted Rand index as
another evaluation measure of segmentation accuracy. This index was used
for additional assessment of the algorithm performance in Section 8.3.
The adjusted Rand index measures similarity between the set of all ground
truth objects G and the set of all segmented objects S, based on how pixels
in a pair are labeled. Two possible scenarios for the pair to be concordant
are that (i) they are placed in the same ground truth object in G and the
same segmented object in S, and (ii) they are placed in diﬀerent ground
truth objects in G and in diﬀerent segmented objects in S. Deﬁne nij as the
number of pixels that are common to both the ith ground truth object and
the jth segmented object, ni,· as the total number of pixels in the ith ground
truth object, n·,j as the total number of pixels in the jth segmented object,
and n as the total number of pixels. Following a simple manipulation, it can
be shown that the probability of agreement is equal to
Pagreement =
Here, the numerator term corresponds to the total number of agreements,
while the denominator term corresponds to the total number of all possi-
ble pairs of pixels. Under the assumption that the partition of pixels into
ground truth objects in G and segmented objects in S follows a generalized
hypergeometric distribution, the adjusted Rand index can be formulated as
ARI(G, S) =
The adjusted Rand index is bounded above by 1, and it can be negative.
5.3. Boundary-Based Segmentation Accuracy
We measure the boundary-based segmentation accuracy between the seg-
mented objects in S and the ground truth objects in G using the object-level
Hausdorﬀdistance. The usual deﬁnition of a Hausdorﬀdistance between
ground truth object G and segmented object S is
H(G, S) = max{sup
y∈S d(x, y), sup
x∈G d(x, y)}
where d(x, y) denotes the distance between pixels x ∈G and y ∈S. In this
work, we use the Euclidean distance. According to (8), Hausdorﬀdistance is
the most extreme value from all distances between the pairs of nearest pixels
on the boundaries of S and G. Thus, the smaller the value of the Hausdorﬀ
distance, the higher the similarity between the boundaries of S and G, and
S = G if their Hausdorﬀdistance is zero.
To calculate the overall segmentation accuracy between a pair of corre-
sponding segmented and ground truth objects, we now introduce object-level
Hausdorﬀdistance by imitating the deﬁnition of object-level Dice index (4).
The object-level Hausdorﬀdistance is deﬁned as
Hobj(G, S) = 1
γiH(Gi, S∗(Gi)) +
σjH(G∗(Sj), Sj)
where the meaning of the mathematical notation is similar to that given in
Section 5.2.1. In case a ground truth object G does not have a corresponding
segmented object (i.e.
S∗(G) = ∅), the Hausdorﬀdistance is calculated
between G and the nearest segmented object S ∈S to G (in the Hausdorﬀ
distance) in that image instead. The same applies for a segmented object
that does not have a corresponding ground truth object.
6. Ranking Scheme
Each submitted entry was assigned one ranking score per evaluation met-
ric and set of test data. Since there were 3 evaluation metrics (F1 score
for gland detection, object-level Dice index for volume-based segmentation
accuracy, and object-level Hausdorﬀindex for boundary-based segmentation
accuracy) and 2 sets of test data, the total number of ranking scores was
6. The best performing entry was assigned ranking score 1, the second best
was assigned ranking score 2, and so on. In care of a tie, the standard com-
petition ranking was applied. For instance, F1 score 0.8, 0.7, 0.7, and 0.6
would result in the ranking scores 1, 2, 2, and 4. The ﬁnal ranking was then
obtained by adding all 6 ranking scores (rank sum). The entry with smallest
sum was placed top in the ﬁnal ranking.
7. Methods
The top ranking methods are described in this section. They are selected
from the total of 13 methods that participated in all stages of the challenge.
The cut-oﬀfor the inclusion in this section was made where there was a
substantial gap in the rank sums (see Appendix A, Figure A.5). Of the 7
selected methods, only 6 preferred to have their methods described here.
7.1. CUMedVision4
A novel deep contour-aware network was presented.
This method explored the multi-level feature representations with fully con-
volutional networks (FCN) . The network outputted seg-
mentation probability maps and depicted the contours of gland objects simul-
taneously. The network architecture consisted of two parts: a down-sampling
path and an up-sampling path. The down-sampling path contained convo-
lutional and max-pooling layers while the up-sampling path contained con-
volutional and up-sampling layers, which increased the resolutions of feature
maps and outputted the prediction masks. In total, there were 5 max-pooling
layers and 3 up-sampling layers. Each layer with learned parameters was fol-
lowed by a non-linear mapping layer (element-wise rectiﬁed linear activation).
4Department of Computer Science and Engineering, The Chinese University of Hong
In order to separate touching glands, the feature maps from hierarchical
layers were up-sampled with two diﬀerent branches to output the segmented
object and contour masks respectively. The parameters of the down-sampling
path were shared and updated for these two kinds of masks.
This could
be viewed as a multi-task learning framework with feature representations,
simultaneously encoding the information of segmented objects and contours.
To alleviate the problem of insuﬃcient training data ,
an oﬀ-the-shelf model from DeepLab , trained on the
2012 PASCAL VOC dataset5, was used to initialize the weights for layers in
the down-sampling path. The parameters of the network were obtained by
minimizing the loss function with standard back-propagation 6.
The team submitted two entries for evaluation. CUMedVision1 was
produced by FCN with multi-level feature representations relying only on
gland object masks, while CUMedVision2 was the results of the deep
contour-aware network, which considers gland object and contour masks si-
multaneously.
7.2. CVML7
In the ﬁrst, preprocessing, stage the images were corrected to compensate
for variations in the appearance due to a variability of the tissue staining pro-
cess. This was implemented through histogram matching, where the target
histogram was calculated from the whole training data, and the individual
image histograms were used as inputs. The main processing stage was based
on two methods: a convolutional neural network (CNN) for a supervised pixel classiﬁcation, and a level set segmentation for
grouping pixels into spatially coherent structures. The employed CNN used
an architecture with two convolutional, pooling and fully connected layers.
The network was trained with three target classes. The classes were designed
to represent (1) the tubular interior of the glandular structure (inner class),
(2) epithelial cells forming boundary of the glandular structure (boundary
class) and (3) inter-gland tissue (outer class). The inputs to the CNN were
19 × 19 pixel patches sliding across the adjusted RGB input image. The two
convolutional layers used 6×6 and 4×4 kernels with 16 and 36 feature maps
5 
6More details will be available at: 
2015miccai_gland.html
7School of Engineering, University of Central Lancashire, Preston, UK.
respectively. The pooling layers, implementing the mean function, used 2×2
receptive ﬁelds and 2 × 2 stride. The ﬁrst and second fully connected layers
used the rectiﬁed linear unit and softmax functions respectively. The outputs
from the CNN were two probability maps representing the probability of each
image pixel belonging to the inner and boundary classes. These two prob-
ability maps were normalized between -1 and 1 and used as a propagation
term, along with an advection term and a curvature ﬂow term. These terms
were part of the hybrid level set model described in Zhang et al. . In
the post-processing stage, a sequence of morphological operations was per-
formed to removed small objects, ﬁll holes and disconnect weakly connected
objects. Additionally, if an image boundary intersecting an object forms a
hole, the corresponding pixels was labeled as part of that object. The team
submitted a single entry for evaluation, henceforth referred to as CVML.
This method ﬁrst preprocessed the data by performing per channel zero
mean and unit variance normalization, where the mean and variance were
computed from the training data. The method then exploited the local invari-
ance properties of the task by applying a set of transformations to the data.
At training time, the dataset was augmented by applying aﬃne transforma-
tions, Gaussian blur and warping. During testing, both image mirroring and
rotation were applied.
The main segmentation algorithm consisted of a multi-path convolutional
neural network. Each path was equipped with a diﬀerent set of convolutional
layers and conﬁgured to capture features from diﬀerent views in a local-global
fashion. All the diﬀerent paths were connected to a set of two fully connected
layers. A leaky rectiﬁed linear unit was used as a default activation function
between layers, and a softmax layer was used after the last fully connected
layer. Every network was trained via stochastic gradient descent with mo-
mentum, using a step-wise learning rate schedule .
The network was randomly initialized such that unit variance was preserved
across layers. It was found that using more than three paths led to heavy
over-ﬁtting – this was due to insuﬃcient training data.
Simple-path networks were trained to detect borders of glands.
ground truth for these networks was constructed using a band of width
8ExB Research and Development.
K ∈ pixels along a real gland border. These values of K were found
to produce optimal and equivalent quantitative results, measured by the F1
score and the object-Dice index. The output of these networks was used to
better calibrate the ﬁnal prediction.
In the post-processing step, a simple method was applied to clean noise
and ﬁll holes in the structures. Thresholding was applied to remove spurious
structures with diameter smaller than a certain epsilon. Filling-hole criteria
based on diameter size was also used.
Using the initial class discrimination (benign and malignant), a simple
binary classiﬁer constructed from a convolutional neural network with 2 con-
volutional and 1 fully connected layers was trained. This binary classiﬁer
used the raw image pixels as input. The output of the classiﬁer was used
together with the border networks and the post-processing method to apply
a diﬀerent set of parameters/thresholds depending on the predicted class.
The hyperparameters for the entire pipeline, including post-processing and
border networks, were obtained through cross-validation.
For this method, the team submitted 3 entries. ExB 1 was a two-path
network including both the border network for detecting borders of glands
and the binary classiﬁcation to diﬀerentiate between the post-processing pa-
rameters. ExB 2 was similar to ExB 1 without the use of the border network.
ExB 3 used a two-path network without any post-processing.
7.4. Image Analysis Lab Uni Freiburg9
The authors applied a u-shaped deep convolutional network “u-net”10
 for the segmentation.
The input was the raw
RGB image and the output was a binary segmentation map (glands and
background). The network consisted of an analysis-path constructed from a
sequence of convolutional layers and max-pooling layers, followed by a synthe-
sis path with a sequence of up-convolutional layers and convolutional layers,
resulting in 23 layers in total. Additional shortcut-connections propagated
the feature maps at all detail levels from the analysis to the synthesis path.
The network was trained from scratch in an end-to-end fashion with only the
9Computer Science Department and BIOSS Centre for Biological Signalling Studies,
University of Freiburg, Germany.
10The implementation of the u-net is freely available at 
uni-freiburg.de/people/ronneber/u-net/.
images and ground truth segmentation maps provided by the challenge orga-
nizers. To teach the network the desired invariances and to avoid overﬁtting,
the training data were augmented with randomly transformed images and
the correspondingly transformed segmentation maps. The applied transfor-
mations were random elastic deformations, rotations, shifts, ﬂips, and blurs.
The color transformations were random multiplications applied in the HSV
color space. To avoid accidentally joining touching objects, a high pixel-wise
loss weight was introduced for pixels in thin gaps between objects in the
training dataset ). The exact same u-net lay-
out with the same hyperparameters as in Ronneberger et al. was used
for the challenge. The only diﬀerence were more training iterations and a
slower decay of the learning rate.
The team submitted two entries. The ﬁrst entry Freiburg1 was a con-
nected component labelling applied to the raw network output. The second
entry Freiburg2 post-processed the segmentation maps with morphological
hole-ﬁlling and deletion of segments smaller than 1000 pixels.
7.5. LIB11
Intestinal glands were divided according to their appearance into three
categories: hollow, bounded, and crowded. A hollow gland was composed of
lumen and goblet cells and it could be a hole in the tissue surface. A bounded
gland had the same composition, but in addition, it was surrounded by a thick
epithelial layer. A crowded gland was composed of bunches of epithelial cells
clustered together and it might have shown necrotic debris.
The tissue was ﬁrst classiﬁed into one of the above classes before beginning
the segmentation.
The classiﬁcation relied on the characterization of the
spatial distribution of cells and the topology of the tissue.
Therefore, a
closing map was generated with a cumulative sum of morphological closing
by a disk of increasing radius (1 to 40 pixels) on the binary image of nuclear
objects, which were segmented by the k-means algorithm in the RGB colour
space. The topological features were calculated from a normalized closing
map in MSER fashion ) as the number of regions below three diﬀerent thresholds (25%, 50%
and 62.5%) and above one threshold (90%), their sizes and the mean of
11Sorbonne Universit´es, UPMC Univ Paris 06, CNRS, INSERM, Biomedical Imaging
Laboratory (LIB), Paris, France.
their corresponding values in the closing map.
The ﬁrst three thresholds
characterized the holes and the fourth one characterized the thickness of
nuclear objects.
After classifying the tissue with a Naive Bayes classiﬁer
trained on these features, a speciﬁc segmentation algorithm was applied.
Three segmentation algorithms were presented, one for each category.
Hollow glands were delineated by morphological dilation on regions below
50%. Bounded gland candidates were ﬁrst detected as hollow glands, then
the thickness of nuclear objects surrounding the region was evaluated by gen-
erating a girth map and a solidity map , then after
classifying nuclear objects, the epithelial layer was added or the candidate
was removed. Crowded glands were identiﬁed as populous regions (regions
above 90%), and then morphological ﬁltering was applied for reﬁnement. The
team submitted a single entry labeled as LIB for evaluation.
7.6. vision4GlaS12
Given an H&E-stained RGB histopathological section, the gland segmen-
tation method was based on a pixel-wise classiﬁcation and an active contour
model, and it proceeded in three steps . In a ﬁrst prepro-
cessing step the image was rescaled to half the spatial resolution, and color
deconvolution separated the stained tissue components. The red channel of
the deconvolved RGB image represented the tissue structure best and was
therefore considered for further processing.
Next, two convolutional neu-
ral networks (CNNs) of seven layers each were trained
for pixel-wise classiﬁcation on a set of image patches. Each network was
trained with ReLU nonlinearities, and stochastic gradient descent with mo-
mentum, weight decay, and dropout regularization to minimize a negative
log-likelihood loss function. The ﬁrst CNN, called Object-Net, was trained
to distinguish four classes: (i) benign background, (ii) benign gland, (iii)
malignant background, and (iv) malignant gland. For each image patch the
probability distribution over the class labels was predicted, using a softmax
function. The Object-Net consisted of three convolutional layers followed
by max-pooling, a ﬁnal convolutional layer and three fully connected layers.
12Institute of Biophysics, Center for Physiological Medicine, Medical University of Graz,
Graz, Austria; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich,
Switzerland; Institute for Computer Graphics and Vision, BioTechMed, Graz University
of Technology, Graz, Austria; Ludwig Boltzmann Institute for Clinical Forensic Imaging,
Graz, Austria.
The second – architecturally similar – CNN called Separator-Net, learned to
predict pixels of gland-separating structures in a binary classiﬁcation task.
Ground truth was generated by manually labeling image locations, close
to two or more gland borders, as gland-separating structures. In the ﬁnal
step the segmentation result was obtained by combining the outputs of the
two CNNs. Predictions for benign and malignant glands were merged, and
predictions of gland-separating structures were subtracted to emphasize the
foreground probabilities. Background classes were handled similarly. Using
these reﬁned foreground and background maps, a ﬁgure-ground segmentation
based on weighted total variation was employed to ﬁnd a globally optimal
solution. This approach optimized a geodesic active contour energy, which
minimized contour length while adhering to the reﬁned CNN predictions
 . The team submitted a single entry, referred to as
vision4GlaS.
8. Results and Discussion
8.1. Summary of the Methods
The methods described above take one of the following two approaches
to segmentation: (a) they start by identifying pixels corresponding to glands
which are then grouped together to form separated, spatially coherent ob-
jects; (b) they begin with candidate objects that are then classiﬁed as glands
or non-glands. All methods that are based on CNNs (CUMedVision, CVML,
ExB, Freiburg, and vision4GlaS) follow the former approach. CVML, ExB,
and vision4GlaS built CNN classiﬁers that assign a gland-related or non-
gland-related label to every pixel in an image, by taking patch(es) centered
at the pixel as input. ExB, in particular, use multi-path networks into which
patches at diﬀerent sizes are fed, in order to capture contextual informa-
tion at multiple scales. CUMedVision and Freiburg, on the other hand, base
their pixel classiﬁer on a fully convolutional network architecture , allowing simultaneous pixel-wise label assignment at multiple pixel lo-
cations. To separate gland-related pixels into individual objects, CVML and
vision4GlaS deploy contour based approaches. ExB trains additional net-
works for glandular boundary, while CUMedVision and Freiburg explicitly
include terms for boundary in the training loss function of their networks.
The only method that follows the latter approach for object segmentation
is LIB. In this method, candidate objects forming part of a gland (i.e., lu-
men, epithelial boundary) are ﬁrst identiﬁed, and then classiﬁed into diﬀerent
types, followed by the ﬁnal step of segmentation.
A variety of data transformation and augmentation were employed to deal
with variation within the data. In order to counter the eﬀect of stain vari-
ation, CVML and ExB performed transformations of the RGB color chan-
nels, vision4GlaS used a stain deconvolution technique to obtain only the
basophilic channel in their preprocessing step. By contrast, Freiburg tackled
the issue of stain variability through data augmentation, which implicitly
forces the networks to be robust to stain variation to some extent. As is
common among methods using CNNs, spatial transformations, such as aﬃne
transformations (e.g. translation, rotation, ﬂip), elastic deformations (e.g.
pincushion and barrel distortions), and blurring, were also used in the data
augmentation to teach the network to learn features that are spatially invari-
ant. The other beneﬁt of data augmentation is it provides, to some extent,
avoidance of over-ﬁtting.
ExB, LIB, and vision4GlaS incorporated histologic grades of glands in
their segmentation approach. In ExB, procedures and/or parameter values
used in boundary detection and post-precessing steps were diﬀerent, subject
to the predicted histologic grade of an image. vision4GlaS classiﬁed pixels
based on histological information. Although not explicit, LIB categorized
candidate objects forming glands according to their appearance, related to
histologic grades, before treating them in diﬀerent ways.
As a post-processing step, many segmentation algorithms employed sim-
ple criteria and/or a sequence of morphological operations to improve their
segmentation results. A common treatment was to eliminate small spurious
segmented objects. Imperfections in pixel labelling can result in the appear-
ance of one or more holes in the middle of an object. Filling such holes is often
necessary. In addition to these operations, CVML performed morphological
operations to separate accidentally joined objects.
8.2. Evaluation Results
Table 2 summarizes the overall evaluation scores and ranks achieved by
each entry from each test part. We list the entries according to the order
of their rank sum, which indicates the overall performance across evaluation
measures and tasks of the entries. The lower the rank sum, the more favor-
able the performance. The top three entries according to the overall rank
sum in descending order are CUMedVision2, ExB1, and ExB3. However,
Table 2: Summary results. The evaluation is carried out according to the challenge criteria
described in Section 6. A ranking score is assigned to each algorithm according to its
performance in each evaluation measure, obtained from each test part. The entries are
listed in a descending order based on their rank sum
Score Rank Score Rank Score Rank Score Rank
CUMedVision2 0.912
CUMedVision1 0.868
vision4GlaS
if rank sum is considered with respect to the test part, the three best en-
tries are CUMedVision2, ExB2, and ExB3 for part A; whereas in part B,
CUMedVision1, ExB1, and Freiburg2 come at the top. A summary of the
ranking results from the competition can be found in Appendix A. Some seg-
mentation results and their corresponding evaluation scores are illustrated in
Figure 2 to give a better idea of how the evaluation scores correlate with the
quality of the segmentation.
8.3. Additional Experiments
In the challenge, the split of the test data into two parts – Part A (60
images) for oﬀ-site test and Part B (20 images) for on-site test – to some
extent introduces bias into the performance evaluation of the segmentation
algorithms due to equal weight given to performance on the two test parts.
The algorithms that perform particularly well on Test Part B would therefore
get a better evaluation score even though they may not have performed as
well on Test Part A, where the majority of the test dataset is to be found.
In addition, the imbalance between the benign and malignant classes in Test
Part B, only 4 benign (20%) and 16 malignant (80%) images, would also favor
algorithms that perform well on the malignant class. In order to alleviate
these issues, we merged the two test parts and re-evaluated the performance
of all the entries. In addition, as suggested by one of the participating teams,
F1score = 1.000
Diceobj = 0.969
Hobj = 10.322
F1score = 0.546
Diceobj = 0.661
Hobj = 107.580
F1score = 0.875
Diceobj = 0.961
Hobj = 11.480
F1score = 0.615
Diceobj = 0.715
Hobj = 183.726
Figure 2: Example images showing segmentation results from some submitted entries. In
each row, (left) ground truth, (middle) the best segmentation result, and (right) the worst
segmentation result. For each image, the corresponding set of evaluation scores for the
segmentation result is reported underneath the image.
Object.Dice
Object.Hausdorff
vision4GlaS
CUMedVision2
CUMedVision1
vision4GlaS
CUMedVision2
CUMedVision1
vision4GlaS
CUMedVision2
CUMedVision1
Figure 3: Performance scores achieved by diﬀerent entries on the combined test data.
Evaluation is conducted on three subsets of the data: (1st row) the whole test data, (2nd
row) benign, and (3rd row) malignant.
Table 3: Ranking results of the entries when the two parts of test data are combined.
Two set of ranking scheme are considered: a) F1score + Diceobj+Hobj and b) F1score +
ARI + Hobj. In addition to the evaluation on the whole test data (overall), the entries
are evaluated on a subset of the data according to the histologic labels, i.e. benign and
malignant.
Final Ranking
F1score + Diceobj+Hobj
F1score + ARI + Hobj
Overall Benign Malignant Overall Benign Malignant
CUMedVision1
CUMedVision2
vision4GlaS
the adjusted Rand index is included as another performance measurement
for segmentation.
The evaluation scores calculated from the combined two test parts are
presented as bar chart in Figure 3. The ﬁnal rankings based on the rank
sums of evaluation scores calculated from the combined two test parts are
reported in Table 3. Here, two set of rank sums are considered: one calculated
according to the criteria of the competition (i.e., F1score+Diceobj+Hobj), and
the other where the adjusted Rand index is used instead of the object-level
Dice index to evaluate segmentation accuracy (i.e., F1score + ARI + Hobj).
For both sets of rank sums, the new ranking orders are largely similar to
those reported in Section 8.2, with a few swaps in the order, while the top
three entries remaining the same, namely CUMedVision2, ExB1, ExB3.
The main factors that negatively aﬀect the performance of the methods
are a number of challenges presented by the dataset. Firstly, large white
empty areas corresponding to the lumen of the gastrointestinal tract which
are not in the interior of intestinal glands can easily confuse the segmentation
algorithms (Figure 4a). Secondly, characteristics of non-glandular tissue can
sometimes resemble that of the glandular tissue. For instance, connective tis-
sue in muscularis mucosa or sub-mucosa layers of the colon is stained white
and pinkish and has less dense nuclei, thus resembling the inner part of glands
(Figure 4b). In the case where there is less stain contrast between nuclei and
cytoplasm due to elevated levels of Hematoxylin stain, non-glandular tissue
with dense nuclei can look similar to malignant epithelial tissue (Figure 4c).
Thirdly, small glandular objects are blended into the surrounding tissue and
can be easily mis-detected (Figure 4d). A careful inspection of the segmenta-
tion results generated by each entry showed that methods by CUMedVision,
ExB, and Freiburg better avoid over-segmentation or under-segmentation
when facing the above-mentioned pitfalls.
The performance of each entry with respect to the histologic grade of
cancer was also examined.
Their evaluation scores based on benign and
malignant samples are reported in the second and the third rows of Figure
3 respectively, and the ranking orders derived from the rank sums of the
scores are shown in Table 3. Based on these results, one can get a better
contrast between the performance of the entries that enforce border separa-
tion and those that do not. By applying a predicted border mask to separate
clumped segmented objects, CUMedVision2 performs better than CUMedVi-
sion1, which tends to produce segmentation results that merge neighboring
glands together, in both benign and malignant cases.
Similarly, ExB1 is
able to segment malignant glands better than ExB2 and ExB3 that do not
utilize border separation. However, this can have an adverse eﬀect if the al-
gorithm already yields segmentation results that separate individual objects
well, such as in the case of ExB1 which under-segments benign glandular
objects as compared to its counterparts ExB2 and ExB3.
8.4. General Discussion
The objectives of this challenge were to raise the research community’s
awareness of the existence of the intestinal gland segmentation problem in
routine stained histology images, and at the same time to provide a plat-
form for a standardized comparison of the performance of automatic and
semi-automatic algorithms. The challenge attracted a lot of attention from
researchers, as can be seen from the number of registered teams/individuals
and the number of submissions at each stage of the competition. Interest-
ingly, some of the teams had no experience in working with histology images
before. We would like to emphasize that ﬁnding the best performing ap-
proach is not the main objective of the competition, but rather pushing the
Figure 4: Example images showing some challenging features in the dataset: (a) lumen
of the gastrointestinal tract, (b) sub-mucosa layer, (c) area with dense nuclei in mucosa
layer, and (d) small glands. Each example is shown with (left) the original image and
(right) the overlaid image highlighting the area with challenging characteristic.
boundaries of the-state-of-the-art approaches. Already, we have seen quite
interesting developments from many participating teams and the leading al-
gorithms have produced excellent results, both qualitatively and quantita-
As noted in the Introduction, morphometric analysis of the appearance
of cells and tissues, especially those forming glands from which tumors origi-
nate, is one of the key components towards precision medicine, and segmen-
tation is the ﬁrst step to attain morphological information. Some may have
argued that there is no need to perform segmentation, but instead, to fol-
low conventional pattern recognition approaches by extracting mathematical
features which normally capture local and/or global tissue architecture and
then identifying features that are most suited to the objective of the study.
It is true that there are a number of successful works that follow such an
approach . However, because these extracted features are
often physically less interpretable in the eyes of practitioners, it is diﬃcult to
adopt such an approach in clinical settings. On the other hand, the appear-
ance of glands such as size and shape obtained through segmentation is easy
to interpret. Segmentation also helps to localize other type of information
(e.g., texture, spatial arrangement of cells) that is speciﬁc to the glandular
Even though the dataset used in the challenge included images of diﬀerent
histologic grades taken from several patients, it lacked other aspects. First of
all, inter-observer variability was not taken into account as the ground truth
was generated by a single expert. This is because the intricate and arduous
nature of the problem makes it diﬃcult to ﬁnd several volunteer experts to
perform manual segmentation. Considerable experience is required in order
to delineate boundaries of malignant glands, which are not so well-deﬁned
as those of the benign ones. Moreover, a single image can contain a large
number of glands to be segmented, making the task very laborious. Sec-
ondly, digitization variability was also not considered in this dataset. It is, in
fact, very important to evaluate the robustness of algorithms when the data
are scanned by diﬀerent instruments. As whole-slide scanners are becoming
increasingly available, this type of real-world problem should be expected.
The choice of evaluation measures would also aﬀect the comparative re-
sults. In this challenge, we emphasized object segmentation and accordingly
deﬁned the object-level Dice index and the object-level Hausdorﬀdistance to
measure segmentation accuracy at the object level rather than at the pixel
level. Nonetheless, it has been suggested that these measures are too strict,
as they put a severe penalty on mismatch of the objects. One could replace
these measures by less conservative ones, for example, adjusted Rand in-
dex or a topology preserving warping error for a volume-based metric and elastic distance for a boundary-based metric. For this reason, we included
adjusted rand index as an alternative to object-level Dice index in Section
8.3. As we have already pointed out, this results in only a minor change in the
ranking order of the entries. Another aspect that was not explicitly included
in the evaluation was execution times. Nevertheless, all the algorithms were
capable of completing the segmentation task on the on-site test data (Part
B) in the given amount of time with or without limitation of resources. Time
eﬃciency is required to process large scale data, such as whole-slide images,
whose volume is growing by the day as slides are routinely scanned. Still, in
medical practice, accuracy is far more important than speed.
It is worth noting that the used evaluation metrics used here are clini-
cally relevant. As mentioned in the Introduction, morphology of intestinal
glands is the key criterion for colorectal cancer grading. This includes shape,
size, and formation of the glands. Thus, in terms of clinical relevance, the
object-Hausdorﬀdistance is used in accessing the shape similarity between
the segmentation results and the ground truth. The object-Dice index is used
in assessing the closeness between the volume of the segmentation results and
that of the ground truth, which is important in estimating the size of individ-
ual glands. Although not directly clinically relevant, F1 score is important
in assessing the accuracy of the identiﬁed glands. Since the morphological
assessment is done on the basis of tissue slide including several thousands of
glands, an algorithm with high value of F1 score is more preferable as it can
detect a larger number of glands.
Gland segmentation algorithms presented here are not ready for deploy-
ment into clinic in their present form. Although some of the top algorithms
produce good segmentation results for the contest dataset and will probably
fare well in the real world, there needs to be a large-scale validation involving
data from multiple centers annotated by multiple pathologists before any of
these algorithms can be deployed in a diagnostic application.
The challenge is now completed, but the dataset will remain available
for research purposes so as to continually attract newcomers to the problem
and to encourage development of state-of-the-art methods. Extension of the
dataset to address inter-observer and inter-scanner variability seems to be the
most achievable aim in the near future. Beyond the scope of segmentation,
there lie various extremely interesting future research directions. Previous
studies have shown the strong association between the survival of colorectal
cancer patients and tumor-related characteristics, including lymphocytic in-
ﬁltration , desmoplasia , tumor budding , and necrosis . A systematic analysis of these characteristics with the help
of gland segmentation as part of automatic image analysis framework could
lead to a better understanding of the relevant cancer biology as well as bring
precision and accuracy into assessment and prediction of the outcome of the
9. Conclusions
This paper presented a summary of the Gland Segmentation in Colon
Histology Images (GlaS) Challenge Contest which was held in conjunction
with the 18th International Conference on Medical Image Computing and
Computer Assisted Interventions . The goal of the challenge
was to bring together researchers interested in the gland segmentation prob-
lem, to validate the performance of their existing or newly invented algo-
rithms on the same standard dataset. In the ﬁnal round, the total num-
ber of submitted entries for evaluation was 19, and we presented here in
this paper 10 of the leading entries. The dataset used in the challenge has
been made publicly available and can be accessed at the challenge website
( Those who are in-
terested in developing or improving their own approaches are encouraged to
use this dataset for quantitative evaluation.
10. Acknowledgements
This paper was made possible by NPRP grant number NPRP5-1345-1-228
from the Qatar National Research Fund (a member of Qatar Foundation).
The statements made herein are solely the responsibility of the authors. Kor-
suk Sirinukunwattana acknowledges the partial ﬁnancial support provided by
the Department of Computer Science, University of Warwick, UK. CUMed-
Vision team acknowledges Hong Kong RGC General Research Fund (Project
No. CUHK 412513). The work of Olaf Ronneberger was supported by the
Excellence Initiative of the German Federal and State Governments , and Martin Urschler acknowledges
funding by the Austrian Science Fund (FWF): P 28078-N33.
The authors thank Professor David Epstein for his extensive help with
the wording and mathematical notation in this paper, and Nicholas Trahearn
for his help with the wording.
Appendix A. The Complete Contest Results
A summary of the ranking results from the contest is given in Figure A.5.