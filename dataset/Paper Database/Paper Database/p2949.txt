An RKHS for Multi-View Learning and Manifold Co-Regularization
Vikas Sindhwani
 
Mathematical Sciences, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 USA
David S. Rosenberg
 
Department of Statistics, University of California Berkeley, CA 94720 USA
Inspired by co-training, many multi-view
semi-supervised kernel methods implement
the following idea: ﬁnd a function in each of
multiple Reproducing Kernel Hilbert Spaces
(RKHSs) such that (a) the chosen functions
make similar predictions on unlabeled examples, and (b) the average prediction given
by the chosen functions performs well on
labeled examples.
In this paper, we construct a single RKHS with a data-dependent
“co-regularization” norm that reduces these
approaches to standard supervised learning. The reproducing kernel for this RKHS
can be explicitly derived and plugged into
any kernel method, greatly extending the
theoretical and algorithmic scope of coregularization. In particular, with this development, the Rademacher complexity bound
for co-regularization given in follows easily from wellknown results.
Furthermore, more reﬁned
bounds given by localized Rademacher complexity can also be easily applied. We propose a co-regularization based algorithmic alternative to manifold regularization that
leads to major empirical improvements on
semi-supervised tasks.
Unlike the recently
proposed transductive approach of , our RKHS formulation is truly semisupervised and naturally extends to unseen
test data.
Appearing in Proceedings of the 25 th International Conference on Machine Learning, Helsinki, Finland, 2008. Copyright 2008 by the author(s)/owner(s).
1. Introduction
In semi-supervised learning, we are given a few labeled examples together with a large collection of unlabeled data from which to estimate an unknown target function. Suppose we have two hypothesis spaces,
H1 and H2, each of which contains a predictor that
well-approximates the target function. We know that
predictors that agree with the target function also agree
with each other on unlabeled examples. Thus, any predictor in one hypothesis space that does not have an
“agreeing predictor” in the other can be safely eliminated from consideration. Due to the resulting reduction in the complexity of the joint learning problem,
one can expect improved generalization performance.
These conceptual intuitions and their algorithmic instantiations together constitute a major line of work
in semi-supervised learning.
One of the earliest approaches in this area was “co-training” , in which H1 and H2 are deﬁned
over diﬀerent representations, or “views”, of the data,
and trained alternately to maximize mutual agreement on unlabeled examples.
More recently, several papers have formulated these intuitions as joint
complexity regularization, or co-regularization, between H1 and H2 which are taken to be Reproducing
Kernel Hilbert Spaces (RKHSs) of functions deﬁned
on the input space X.
Given a few labeled examples {(xi, yi)}i∈L and a collection of unlabeled data
{xi}i∈U, co-regularization learns a prediction function,
⋆(x) + f 2
⋆∈H1 and f 2
⋆∈H2 are obtained by solving
the following optimization problem,
f 1∈H1,f 2∈H2 γ1||f 1||2
H1 + γ2||f 2||2
[f 1(xi) −f 2(xi)]2 +
V (yi, f(xi))
An RKHS for Multi-View Learning and Manifold Co-Regularization
In this objective function, the ﬁrst two terms measure
complexity by the RKHS norms ∥· ∥2
H1 and ∥· ∥2
in H1 and H2 respectively, the third term enforces
agreement among predictors on unlabeled examples,
and the ﬁnal term evaluates the empirical loss of the
mean function f = (f 1 + f 2)/2 on the labeled data
with respect to a loss function V (·, ·). The real-valued
parameters γ1, γ2, and µ allow diﬀerent tradeoﬀs between the regularization terms. L and U are index sets
over labeled and unlabeled examples respectively.
Several variants of this formulation have been proposed independently and explored in diﬀerent contexts: linear logistic regression , regularized least squares classiﬁcation , regression ,
support vector classiﬁcation ,
Bayesian co-training , and generalization theory .
The main theoretical contribution of this paper is the
construction of a new “co-regularization RKHS,” in
which standard supervised learning recovers the solution to the co-regularization problem of Eqn. 2.
Theorem 2.2 presents the RKHS and gives an explicit formula for its reproducing kernel.
This “coregularization kernel” can be plugged into any standard kernel method giving convenient and immediate
access to two-view semi-supervised techniques for a
wide variety of learning problems. Utilizing this kernel, in Section 3 we give much simpler proofs of the
results of concerning
bounds on the Rademacher complexity and generalization performance of co-regularization. As a more
algorithmic application, in Section 4 we consider the
semi-supervised learning setting where examples live
near a low-dimensional manifold embedded in a high
dimensional ambient euclidean space. Our approach,
manifold co-regularization (CoMR), gives major empirical improvements over the manifold regularization
(MR) framework of .
The recent work of considers a similar
reduction. However, this reduction is strictly transductive and does not allow prediction on unseen test
examples. By contrast, our formulation is truly semisupervised and provides a principled out-of-sample extension.
2. An RKHS for Co-Regularization
We start by reformulating the co-regularization optimization problem, given in Eqn. 1 and Eqn. 2, in the
following equivalent form where we directly solve for
the ﬁnal prediction function f⋆:
f⋆= argmin
f 1∈H1,f 2∈H2
2 ||f 1||2
2 ||f 2||2
[f 1(xi) −f 2(xi)]2 + 1
Consider the sum space of functions, ˜H, given by,
{f|f(x) = f 1(x) + f 2(x), f 1 ∈H1, f 2 ∈H2}
and impose on it a data-dependent norm,
f 1∈H1,f 2∈H2
H1 + γ2∥f 2∥2
f 1(xi) −f 2(xi)
The minimization problem in Eqn. 3 can then be posed
as standard supervised learning in ˜H as follows,
f⋆= argmin
Of course, this reformulation is not
really useful unless ˜H itself is a valid new RKHS. Let
us recall the deﬁnition of an RKHS.
Deﬁnition 2.1 (RKHS). A reproducing kernel Hilbert
space (RKHS) is a Hilbert Space F that possesses a
reproducing kernel, i.e., a function k : X × X →R
for which the following hold: (a) k(x, .) ∈F for all
x ∈X, and (b) ⟨f, k(x, .)⟩F = f(x) for all x ∈X and
f ∈F, where ⟨·, ·⟩F denotes inner product in F.
In Theorem 2.2, we show that ˜H is indeed an RKHS,
and moreover we give an explicit expression for its reproducing kernel. Thus, it follows that although the
domain of optimization in Eqn. 6 is nominally a function space, by the Representer Theorem we can express
it as a ﬁnite-dimensional optimization problem.
2.1. Co-Regularization Kernels
Let H1, H2 be RKHSs with kernels given by k1, k2 respectively, and let ˜H = H1 ⊕H2 as deﬁned in Eqn. 4.
We have the following result.
Theorem 2.2. There exists an inner product on ˜H
for which ˜H is a RKHS with norm deﬁned by Eqn. 5
and reproducing kernel ˜k : X × X →R given by,
˜k(x, z) = s(x, z) −µdT
An RKHS for Multi-View Learning and Manifold Co-Regularization
where s(x, z) is the (scaled) sum of kernels given by,
s(x, z) = γ−1
1 k1(x, z) + γ−1
2 k2(x, z),
and dx is a vector-valued function that depends on the
diﬀerence in views measured as,
ki(x, xj), j ∈U
T , and H is a positivedeﬁnite matrix given by H = (I+µS)−1. Here, S is the
gram matrix of s(·, ·), i.e., S =
UU = ki(U, U) denotes the Gram matrices of
ki over unlabeled examples.
We give a rigorous proof in Appendix A.
2.2. Representer Theorem
Theorem 2.2 says that ˜H is a valid RKHS with kernel
˜k. By the Representer Theorem, the solution to Eqn.6
is given by
αi˜k(xi, x)
The corresponding components in H1, H2 can also be
retrieved as,
 k1(xi, x) −µdT
 k2(xi, x) + µdT
Note that H1 and H2 are deﬁned on the same domain X so that taking the mean prediction is meaningful. In a two-view problem one may begin by deﬁning
H1, H2 on diﬀerent view spaces X 1, X 2 respectively.
Such a problem can be mapped to our framework by
extending H1, H2 to X = X 1 × X 2 by re-deﬁning
f 1(x1, x2) = f 1(x1), f 1 ∈H1; similarly for H2. While
we omit these technical details, it is important to note
that in such cases, Eqns. 9 and 10 can be reinterpreted
as predictors deﬁned on X 1, X 2 respectively.
3. Bounds on Complexity and
Generalization
By eliminating all predictors that do not collectively
agree on unlabeled examples, co-regularization intuitively reduces the complexity of the learning problem. It is reasonable then to expect better test performance for the same amount of labeled training data.
In , the size of the coregularized function class is measured by its empirical Rademacher complexity, and tight upper and lower
bounds are given on the Rademacher complexity of the
co-regularized hypothesis space. This leads to generalization bounds in terms of the Rademacher complexity.
In this section, we derive these complexity bounds in
a few lines using Theorem 2.2 and a well-known result
on RKHS balls.
Furthermore, we present improved
generalization bounds based on the theory of localized
Rademacher complexity.
3.1. Rademacher Complexity Bounds
Deﬁnition 3.1. The empirical Rademacher complexity of a function class A = {f : X →R} on a sample
x1, . . . , xℓ∈X is deﬁned as
ˆRℓ(A) = Eσ
expectation
{σ1, . . . , σℓ}, and the σi are i.i.d.
Rademacher random variables, that is, P(σi = 1) = P(σi = −1) = 1
Let H be an arbitrary RKHS with kernel k(·, ·), and
denote the standard RKHS supervised learning objective function by Q(f) = P
i∈L V (yi, f(xi)) + λ||f||2
Let f⋆= argminf∈H Q(f).
Then Q(f⋆) ≤Q(0) =
i∈L V (yi, 0). It follows that ∥f⋆∥2
H ≤Q(0)/λ. Thus
if we have some control a priori on Q(0), then we
can restrict the search for f⋆to a ball in H of radius
well-known
Rademacher complexity of a ball in an RKHS ). Let Hr := {f ∈H :
||f||H ≤r} denote the ball of radius r in H. Then we
have the following:
Lemma 3.2. The empirical Rademacher complexity
on the sample x1, . . . , xℓ∈X for the RKHS ball Hr is
bounded as follows:
trK ≤ˆRℓ(Hr) ≤2r
 k(xi, xj)
i,j=1 is the kernel matrix.
co-regularization
Eqns. 3 and 6, we have f⋆
˜Hr where r2
ℓsupy V (0, y), where ℓis number of labeled examples.
We now state and prove bounds on the Rademacher
complexity of ˜Hr. The bounds here are exactly the
same as those given in .
However, while they have a lengthy “bare-hands” approach, here we get the result as a simple corollary of
Theorem 2.2 and Lemma 3.2.
Theorem 3.3. The empirical Rademacher complexity
on the labeled sample x1, . . . , xℓ∈X for the RKHS
ball ˜Hr is bounded as follows:
tr ˜K ≤ˆRℓ( ˜Hr) ≤2r
An RKHS for Multi-View Learning and Manifold Co-Regularization
UL (I + µS)−1 DUL and
Proof. Note that ˜K is just the kernel matrix for the coregularization kernel ˜k(·, ·) on the labeled data. Then
the bound follows immediately from Lemma 3.2.
3.2. Co-Regularization Reduces Complexity
The co-regularization parameter µ controls the extent
to which we enforce agreement between f 1 and f 2. Let
˜H(µ) denote the co-regularization RKHS for a particular value of µ. From Theorem 3.3, we see that the
Rademacher complexity for a ball of radius r in ˜H(µ)
decreases with µ by an amount determined by
UL (I + µS)−1 DUL
where ρ(·, ·) is a metric on R|U| deﬁned by
ρ2(s, t) = µ(γ−1
2 t)′ (I + µS)−1 (γ−1
We see that the complexity reduction, ∆(µ), grows
with the ρ-distance between the two diﬀerent representations of the labeled points. Note that the metric
ρ is determined by S, which is the weighted sum of the
gram matrices of the two kernels on unlabeled data.
3.3. Generalization Bounds
With Theorem 2.2 allowing us to express multi-view
co-regularization problems as supervised learning in a
data-dependent RKHS, we can now bring a large body
of theory to bear on the generalization performance
of co-regularization methods.
We start by quoting
the theorem proved in .
Next, we state an improved bound based on localized
Rademacher complexity. Below, we denote the unit
ball in ˜H by ˜H1.
Condition 1. The loss V (·, ·) is Lipschitz in its ﬁrst
argument, i.e., there exists a constant A such that
∀y, ˆy1, ˆy2: |V (ˆy1, y) −V (ˆy2, y)| ≤A |ˆy1 −ˆy2|
Theorem 3.4. Suppose V : Y2 → satisﬁes Condition 1. Then conditioned on the unlabeled data, for
any δ ∈(0, 1), with probability at least 1 −δ over
the sample of labeled points (x1, y1), . . . , (xℓ, yℓ) drawn
i.i.d. from P, we have for any predictor f ∈˜H1 that
P[V (ϕ(x), y)] ≤1
V (ϕ(xi), yi) + 2B ˆRℓ( ˜H1)
We need two more conditions for the localized bound:
Condition 2. For any probability distribution P,
there exists f⋆
˜H1 satisfying P[V (f⋆(x), y)] =
H1 P[V (f(x), y)]
Condition 3. There is a constant B ≥1 such that
for every probability distribution P and every f ∈˜H1
we have, P(f −f∗)2 ≤BP(V [f(x), y] −V [f⋆(x), y)])
In the following theorem, let Pℓdenote the empirical
probability measure for the labeled sample of size ℓ.
Theorem 3.5. [Cor. 6.7 from ]
Assume that supx∈X k(x, x) ≤1 and that V satisﬁes
the 3 conditions above. Let ˆf be any element of ˜H1
satisfying PℓV [ ˆf(x), y] = inff∈˜
H1 PℓV [f(x), y]. There
exist a constant c depending only on A and B s.t. with
probability at least 1 −6e−ν,
V [ ˆf(x), y] −V [f⋆(x), y]
where ˆr∗≤min0≤h≤ℓ
λ1, . . . , λℓare the eigenvalues of the labeled-data kernel
matrix ˜KLL in decreasing order.
Note that while Theorem 3.4 bounds the gap between
expected and empirical performance of an arbitrary
˜H1, Theorem 3.5 bounds the gap between the
empirical loss minimizer over ˜H1 and true risk minimizer in ˜H1.
Since the localized bound only needs
to account for the capacity of the function class in the
neighborhood of f∗, the bounds are potentially tighter.
Indeed, while the bound in Theorem 3.4 is in terms of
the trace of the kernel matrix, the bound in Theorem 3.5 involves the tail sum of kernel eigenvalues. If
the eigenvalues decay very quickly, the latter is potentially much smaller.
4. Manifold Co-Regularization
Consider the picture shown in Figure 1(a) where there
are two classes of data points in the plane (R2) lying
on one of two concentric circles. The large, colored
points are labeled while the smaller, black points are
unlabeled. The picture immediately suggests two notions of distance that are very natural but radically
diﬀerent. For example, the two labeled points are close
in the ambient euclidean distance on R2, but inﬁnitely
apart in terms of intrinsic geodesic distance measured
along the circles.
Suppose for this picture one had access to two kernel
functions, k1, k2 that assign high similarity to nearby
points according to euclidean and geodesic distance
respectively. Because of the diﬀerence in ambient and
intrinsic representations, by co-regularizing the associated RKHSs one can hope to get good reductions in
An RKHS for Multi-View Learning and Manifold Co-Regularization
complexity, as suggested in section 3.2. In Figure 1,
we report the value of complexity reduction (Eqn. 12)
for four point clouds generated at increasing levels of
noise oﬀthe two concentric circles. When noise becomes large, the ambient and intrinsic notions of distance converge and the amount of complexity reduction decreases.
Figure 1. Complexity Reduction ∆(µ = 1) (Eqn. 12) with
respect to noise level ρ. The choice of k1, k2 is discussed in
the following subsections.
(a) ∆= 7957, ρ = 0
(b) ∆= 7569, ρ = 0.1
(c) ∆= 3720, ρ = 0.2
(d) ∆= 3502, ρ = 0.5
The setting where data lies on a low-dimensional
submanifold M embedded in a higher dimensional
ambient space X, as in the concentric circles case
above, has attracted considerable research interest recently, almost orthogonal to multi-view eﬀorts. The
main assumption underlying manifold-motivated semisupervised learning algorithms is the following: two
points that are close with respect to geodesic distances
on M should have similar labels.
Such an assumption may be enforced by an intrinsic regularizer that
emphasizes complexity along the manifold.
Since M is truly unknown, the intrinsic regularizer is
empirically estimated from the point cloud of labeled
and unlabeled data.
In the graph transduction approach, an nn-nearest neighbor graph G is constructed
which serves as an empirical substitute for M. The
vertices V of this graph are the set of labeled and unlabeled examples. Let HI denote the space of all functions mapping V to R, where the subscript I implies
“intrinsic.”
Any function f ∈HI can be identiﬁed
with the vector f = [f(xi), xi ∈V]T . One can impose
a norm ∥f∥2
ij Wij [f(xi) −f(xj)]2 on HI that
provides a natural measure of smoothness for f with
respect to the graph. Here, W denotes the adjacency
matrix of the graph. When X is a euclidean space, a
typical W is given by Wij = exp(−∥xi−xj∥2
) if i and
j are nearest neighbors and 0 otherwise. In practice,
one may use a problem dependent similarity matrix to
set these edge weights. This norm can be conveniently
written as a quadratic form f T Mf, where M is the
graph Laplacian matrix deﬁned as M = D−W, and D
is a diagonal degree matrix with entrees Dii = P
It turns out that HI with the norm ∥· ∥I is an
RKHS whose reproducing kernel kI
R is given by kI(xi, xj) = M †
ij, where M † denotes the pseudo-inverse of the Laplacian.
HI with its reproducing kernel, graph transduction
solves the standard RKHS regularization problem,
f⋆= argminf∈HI γ∥f∥2
i∈L V (yi, f(xi)), where
yi is the label associated with the node xi. Note that
the solution f⋆is only deﬁned over V, the set of labeled and unlabeled examples. Since graph transduction does not provide a function whose domain is the
ambient space X, it is not clear how to make predictions on unseen test points x ∈X. Possessing a principled “out-of-sample extension” distinguishes semisupervised methods from transductive procedures.
4.1. Ambient and Intrinsic Co-Regularization
We propose a co-regularization solution for out-ofsample prediction.
Conceptually, one may interpret
the manifold setting as a multi-view problem where
each labeled or unlabeled example appears in two
“views”: (a) an ambient view in X in terms of euclidean co-ordinates x and (b) an intrinsic view in G
as a vertex index i.
Let HA : X × X →R be an
RKHS deﬁned over the ambient domain with an associated kernel kA : X × X →R. We can now combine
ambient and intrinsic views by co-regularizing HA, HI.
This can be done by setting k1 = kA and k2 = kI in
Eqn. 7 and solving Eqn. 6. The combined prediction
function f⋆given by Eqn. 8 is the mean of an ambient
component f 1
⋆given by Eqn. 9 and an intrinsic component f 2
⋆given by Eqn. 10. Even though f⋆is transductive and only deﬁned on labeled and unlabeled examples, the ambient component f 1
⋆can be used for outof-sample prediction.
Due to co-regularization, this
ambient component is (a) smooth in HX and (b) in
agreement with a smooth function on the data manifold. We call this approach manifold co-regularization,
and abbreviate it as CoMR.
4.2. Manifold Regularization
regularization
of , the
An RKHS for Multi-View Learning and Manifold Co-Regularization
following optimization problem is solved:
f ⋆= argmin
HA + γ2f T Mf
V (yi, f(xi))
where f = [f(xi), i ∈L, U]T .
The solution, f ⋆is
deﬁned on X, and can therefore be used for out-ofsample prediction.
In Figure 2, we show a simple two-dimensional dataset
where MR provably fails when HA is the space of linear functions on R2. The LINES dataset consists of
two classes spread along perpendicular lines. In MR
the intrinsic regularizer is enforced directly on HA.
It can be easily shown that the intrinsic norm of a
linear function f(x) = wT x along the perpendicular
lines is exactly the same as the ambient norm, i.e.,
HA = wT w. Due to this, MR simply ignores unlabeled data and reduces to supervised training with the regularization parameter γ1 + γ2.
The linear function that gives maximally smooth predictions on one line also gives the maximally nonsmooth predictions on the other line.
One way to
remedy this restrictive situation is to introduce slack
variables ξ = (ξi)i∈L∪U in Eqn. 13 with an ℓ2 penalty,
and instead solve: f ⋆= argminf∈HA,ξ γ1∥f∥2
γ2(f + ξ)T M(f + ξ) + µ∥ξ∥2 + P
i∈L V (yi, f(xi)).
Re-parameterizing g = f + ξ, we can re-write the
above problem as, f ⋆= argminf∈HA,g∈HI γ1∥f∥2
HI + µ∥f −g∥2 + P
i∈L V (yi, f(xi)), which may
be viewed as a variant of the co-regularization problem in Eqn. 2 where empirical loss is measured for f
alone. Thus, this motivates the view that CoMR adds
extra slack variables in the MR objective function to
better ﬁt the intrinsic regularizer. Figure 2 shows that
CoMR achieves better separation between classes on
the LINES dataset.
Figure 2. Decision boundaries of MR and CoMR (using the
quadratic hinge loss) on the LINES dataset
4.3. Experiments
In this section, we compare MR and CoMR. Similar to
our construction of the co-regularization kernel, provide a data-dependent kernel
that reduces manifold regularization to standard supervised learning in an associated RKHS. We write the
manifold regularization kernel in the following form,
˜kmr(x, z) = ¯s(x, z) −¯dT
where we have, ¯s = γ−1
1 k1(x, z), ¯dx = γ−1
¯K2−1, where ¯
K1 is the Gram Matrix of k1 = kA over labeled and unlabeled examples,
K2 = M †. We use the notation ¯s, ¯d, ¯H so that
the kernel can be easily compared with corresponding
quantities in the co-regularization kernel Eqn. 7. In
this section we empirically compare this kernel with
the co-regularization kernel of Eqn. 7 for exactly the
same choice of k1, k2.
Semi-supervised classiﬁcation
experiments were performed on 5 datasets described
in table 1.
Table 1. Datasets with d features and c classes. 10 random
data splits were created with l labeled, u unlabeled, t test,
and v validation examples.
The LINES dataset is a variant of the two-dimensional
problem shown in Figure 2 where we added random
noise around the two perpendicular lines. The G50C,
USPST, COIL20, and PCMAC datasets are well
known and have frequently been used for empirical
studies in semi-supervised learning literature.
were used for benchmarking manifold regularization
in against a number of competing methods. g50c is an artiﬁcial dataset generated from two unit covariance normal distributions
with equal probabilities. The class means are adjusted
so that the Bayes error is 5%.
COIL20 consists of
32 × 32 gray scale images of 20 objects viewed from
varying angles. USPST is taken from the test subset
of the USPS dataset of images containing 10 classes
of handwritten digits. PCMAC is used to setup binary text categorization problems drawn from the 20newsgroups dataset.
For each of the 5 datasets, we constructed random
splits into labeled, unlabeled, test and validation sets.
The sizes of these sets are given in table 1.
all datasets except LINES, we used Gaussian am-
An RKHS for Multi-View Learning and Manifold Co-Regularization
Table 2. Error Rates (in percentage) on Test Data
18.2 (1.5)
14.1 (1.6)
23.8 (11.1)
14.8 (8.8)
11.9 (3.4)
Table 3. Error Rates (in percentage) on Unlabeled Data
18.6 (1.4)
13.3 (1.0)
37.5 (6.0)
14.8 (3.3)
11.0 (2.4)
bient kernels k1(x, z) = exp(−∥x−z∥2
), and intrinsic graph kernel whose gram matrix is of the form
K2 = (M p + 10−6I)−1. Here, M is the normalized
Graph Laplacian constructed using nn nearest neighbors and p is an integer. These parameters are tabulated in Table 4 for reproducibility. For more details
on these parameters see .
We chose squared loss for V (·, ·). Manifold regularization with this choice is also referred to as Laplacian
RLS and empirically performs as well as Laplacian
For multi-class problems, we used the oneversus-rest strategy.
γ1, γ2 were varied on a grid of
values: 10−6, 10−4, 10−2, 1, 10, 100 and chosen with respect to validation error. The chosen parameters are
also reported in Table 4. Finally, we evaluated the MR
solution and the ambient component of CoMR on an
unseen test set. In Tables 2 and 3 we report the mean
and standard deviation of error rates on test and unlabeled examples with respect to 10 random splits. We
performed a paired t-test at 5% signiﬁcance level to assess the statistical signiﬁcance of the results. Results
shown in bold are statistically signiﬁcant.
Our experimental protocol makes MR and CoMR exactly comparable.
We ﬁnd that CoMR gives major
empirical improvements over MR on all datasets except G50C where both methods approach the Bayes
error rate.
5. Conclusion
In this paper, we have constructed a single, new RKHS
in which standard supervised algorithms are immediately turned into multi-view semi-supervised learners.
This construction brings about signiﬁcant theoretical
simpliﬁcations and algorithmic beneﬁts, which we have
demonstrated in the context of generalization analysis
and manifold regularization respectively.
Table 4. Parameters Used. Note µ = 1 for CoMR. Linear
kernel was used for LINES dataset.
0.01, 10−6
0.01, 0.01
10−6, 10−4
10−6, 10−6