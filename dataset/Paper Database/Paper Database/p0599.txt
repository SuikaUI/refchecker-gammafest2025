Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651–3657
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
What does BERT learn about the structure of language?
Ganesh Jawahar
Benoˆıt Sagot
Djam´e Seddah
Inria, France
{ﬁrstname.lastname}@inria.fr
BERT is a recent language representation
model that has surprisingly performed well in
diverse language understanding benchmarks.
This result indicates the possibility that BERT
networks capture structural information about
language. In this work, we provide novel support for this claim by performing a series of
experiments to unpack the elements of English
language structure learned by BERT. We ﬁrst
show that BERT’s phrasal representation captures phrase-level information in the lower layers. We also show that BERT’s intermediate
layers encode a rich hierarchy of linguistic information, with surface features at the bottom,
syntactic features in the middle and semantic
features at the top. BERT turns out to require
deeper layers when long-distance dependency
information is required, e.g. to track subjectverb agreement. Finally, we show that BERT
representations capture linguistic information
in a compositional way that mimics classical,
tree-like structures.
Introduction
BERT (Bidirectional Encoder Representations
from Transformers) is a
bidirectional variant of Transformer networks
 trained to jointly predict
a masked word from its context and to classify
whether two sentences are consecutive or not. The
trained model can be ﬁne-tuned for downstream
NLP tasks such as question answering and language inference without substantial modiﬁcation.
BERT outperforms previous state-of-the-art models in the eleven NLP tasks in the GLUE benchmark by a signiﬁcant margin.
This remarkable result suggests that BERT could
“learn” structural information about language.
Can we unveil the representations learned by
BERT to proto-linguistics structures? Answering
this question could not only help us understand
the reason behind the success of BERT but also its
limitations, in turn guiding the design of improved
architectures. This question falls under the topic of
the interpretability of neural networks, a growing
ﬁeld in NLP . An important step forward in this direction is Goldberg
 , which shows that BERT captures syntactic phenomena well when evaluated on its ability
to track subject-verb agreement.
In this work, we perform a series of experiments
to probe the nature of the representations learned
by different layers of BERT. 1 We ﬁrst show that
the lower layers capture phrase-level information,
which gets diluted in the upper layers. Second, we
propose to use the probing tasks deﬁned in Conneau et al. to show that BERT captures a
rich hierarchy of linguistic information, with surface features in lower layers, syntactic features in
middle layers and semantic features in higher layers. Third, we test the ability of BERT representations to track subject-verb agreement and ﬁnd that
BERT requires deeper layers for handling harder
cases involving long-distance dependencies.
Finally, we propose to use the recently introduced
Tensor Product Decomposition Network (TPDN)
 to explore different hypotheses about the compositional nature of BERT’s representation and ﬁnd that BERT implicitly captures
classical, tree-like structures.
BERT builds on Transformer
networks to pre-train bidirectional representations by conditioning on both
left and right contexts jointly in all layers. The
representations are jointly optimized by predicting
randomly masked words in the input and classify-
1The code to reproduce our experiments is publicly accessible at 
interpret_bert
(a) Layer 1
(b) Layer 2
(c) Layer 11
(d) Layer 12
Figure 1: 2D t-SNE plot of span embeddings computed from the ﬁrst and last two layers of BERT.
Table 1: Clustering performance of span representations obtained from different layers of BERT.
ing whether the sentence follows a given sentence
in the corpus or not. The authors of BERT claim
that bidirectionality allows the model to swiftly
adapt for a downstream task with little modiﬁcation to the architecture. Indeed, BERT improved
the state-of-the-art for a range of NLP benchmarks
 by a signiﬁcant margin.
In this work, we investigate the linguistic structure implicitly learned by BERT’s representations.
We use the PyTorch implementation of BERT,
which hosts the models trained by (Devlin et al.,
All our experiments are based on the
bert-base-uncased variant,2 which consists of
12 layers, each having a hidden size of 768 and 12
attention heads (110M parameters). In all our experiments, we seek the activation of the ﬁrst input
token (‘[CLS]’) (which summarizes the information from the actual tokens using a self-attention
mechanism) at every layer to compute BERT representation, unless otherwise stated.
Phrasal Syntax
Peters et al. have shown that the representations underlying LSTM-based language models can capture phrase-level (or span-level) information.3 It
remains unclear if this holds true for models not
trained with a traditional language modeling objective, such as BERT. Even if it does, would the
information be present in multiple layers of the
To investigate this question we extract
span representations from each layer of BERT.
2We obtained similar results in preliminary experiments
with the bert-large-uncased variant.
3Peters et al. experimented with ELMo-style CNN
and Transformer but did not report this ﬁnding for these models.
Following Peters et al. , for a token sequence si, . . . , sj, we compute the span representation s(si,sj),l at layer l by concatenating the
ﬁrst (hsi,l) and last hidden vector (hsj,l), along
with their element-wise product and difference.
We randomly pick 3000 labeled chunks and 500
spans not labeled as chunks from the CoNLL 2000
chunking dataset .
As shown in Figure 1, we visualize the span representations obtained from multiple layers using t-
SNE , a non-linear dimensionality reduction algorithm for visualizing
high-dimensional data.
We observe that BERT
mostly captures phrase-level information in the
lower layers and that this information gets gradually diluted in higher layers. The span representations from the lower layers map chunks (e.g. ‘to
demonstrate’) that project their underlying category (e.g. VP) together. We further quantify this
claim by performing a k-means clustering on span
representations with k = 10, i.e. the number
of distinct chunk types. Evaluating the resulting
clusters using the Normalized Mutual Information
(NMI) metric shows again that the lower layers encode phrasal information better than higher layers
(cf. Table 1).
Probing Tasks
Probing (or diagnostic) tasks help
in unearthing the linguistic features possibly encoded in neural models. This is achieved by setting up an auxiliary classiﬁcation task where the
ﬁnal output of a model is used as features to predict a linguistic phenomenon of interest. If the
auxiliary classiﬁer can predict a linguistic prop-
(Syntactic)
(Syntactic)
(Syntactic)
(Semantic)
(Semantic)
(Semantic)
(Semantic)
(Semantic)
93.9 (2.0)
24.9 (24.8)
35.9 (6.1)
63.6 (9.0)
50.3 (0.3)
82.2 (18.4)
77.6 (10.2)
76.7 (26.3)
49.9 (-0.1)
53.9 (3.9)
95.9 (3.4)
65.0 (64.8)
40.6 (11.3)
71.3 (16.1)
55.8 (5.8)
85.9 (23.5)
82.5 (15.3)
80.6 (17.1)
53.8 (4.4)
58.5 (8.5)
96.2 (3.9)
66.5 (66.0)
39.7 (10.4)
71.5 (18.5)
64.9 (14.9)
86.6 (23.8)
82.0 (14.6)
80.3 (16.6)
55.8 (5.9)
59.3 (9.3)
94.2 (2.3)
69.8 (69.6)
39.4 (10.8)
71.3 (18.3)
74.4 (24.5)
87.6 (25.2)
81.9 (15.0)
81.4 (19.1)
59.0 (8.5)
58.1 (8.1)
92.0 (0.5)
69.2 (69.0)
40.6 (11.8)
81.3 (30.8)
81.4 (31.4)
89.5 (26.7)
85.8 (19.4)
81.2 (18.6)
60.2 (10.3)
64.1 (14.1)
88.4 (-3.0)
63.5 (63.4)
41.3 (13.0)
83.3 (36.6)
82.9 (32.9)
89.8 (27.6)
88.1 (21.9)
82.0 (20.1)
60.7 (10.2)
71.1 (21.2)
83.7 (-7.7)
56.9 (56.7)
40.1 (12.0)
84.1 (39.5)
83.0 (32.9)
89.9 (27.5)
87.4 (22.2)
82.2 (21.1)
61.6 (11.7)
74.8 (24.9)
82.9 (-8.1)
51.1 (51.0)
39.2 (10.3)
84.0 (39.5)
83.9 (33.9)
89.9 (27.6)
87.5 (22.2)
81.2 (19.7)
62.1 (12.2)
76.4 (26.4)
80.1 (-11.1)
47.9 (47.8)
38.5 (10.8)
83.1 (39.8)
87.0 (37.1)
90.0 (28.0)
87.6 (22.9)
81.8 (20.5)
63.4 (13.4)
78.7 (28.9)
77.0 (-14.0)
43.4 (43.2)
38.1 (9.9)
81.7 (39.8)
86.7 (36.7)
89.7 (27.6)
87.1 (22.6)
80.5 (19.9)
63.3 (12.7)
78.4 (28.1)
73.9 (-17.0)
42.8 (42.7)
36.3 (7.9)
80.3 (39.1)
86.8 (36.8)
89.9 (27.8)
85.7 (21.9)
78.9 (18.6)
64.4 (14.5)
77.6 (27.9)
69.5 (-21.4)
49.1 (49.0)
34.7 (6.9)
76.5 (37.2)
86.4 (36.4)
89.5 (27.7)
84.0 (20.2)
78.7 (18.4)
65.2 (15.3)
74.9 (25.4)
Table 2: Probing task performance for each BERT layer. The value within the parentheses corresponds to the
difference in performance of trained vs. untrained BERT.
Table 3: Subject-verb agreement scores for each BERT
layer. The last ﬁve columns correspond to the number of nouns intervening between the subject and the
verb (attractors) in test instances. The average distance
between the subject and the verb is enclosed in parentheses next to each attractor category.
erty well, then the original model likely encodes
that property. In this work, we use probing tasks
to assess individual model layers in their ability to
encode different types of linguistic features. We
evaluate each layer of BERT using ten probing
sentence-level datasets/tasks created by Conneau
et al. , which are grouped into three categories. Surface tasks probe for sentence length
(SentLen) and for the presence of words in the
sentence (WC). Syntactic tasks test for sensitivity
to word order (BShift), the depth of the syntactic tree (TreeDepth) and the sequence of toplevel constituents in the syntax tree (TopConst).
Semantic tasks check for the tense (Tense), the
subject (resp. direct object) number in the main
clause (SubjNum, resp. ObjNum), the sensitivity
to random replacement of a noun/verb (SOMO) and
the random swapping of coordinated clausal conjuncts (CoordInv). We use the SentEval toolkit
 along with the recommended hyperparameter space to search for the
best probing classiﬁer. As random encoders can
surprisingly encode a lot of lexical and structural
information , we also
evaluate the untrained version of BERT, obtained
by setting all model weights to a random number.
Table 2 shows that BERT embeds a rich hierarchy of linguistic signals: surface information at
the bottom, syntactic information in the middle,
semantic information at the top. BERT has also
surpassed the previously published results for two
tasks: BShift and CoordInv. We ﬁnd that the
untrained version of BERT corresponding to the
higher layers outperforms the trained version in
the task of predicting sentence length (SentLen).
This could indicate that untrained models contain
sufﬁcient information to predict a basic surface
feature such as sentence length, whereas training
the model results in the model storing more complex information, at the expense of its ability to
predict such basic surface features.
Subject-Verb Agreement
Subject-verb agreement is a proxy task to probe
whether a neural model encodes syntactic structure . The task of predicting
the verb number becomes harder when there are
more nouns with opposite number (attractors) intervening between the subject and the verb. Goldberg has shown that BERT learns syntactic phenomenon surprisingly well using various
stimuli for subject-verb agreement.
his work by performing the test on each layer of
BERT and controlling for the number of attractors. In our study, we use the stimuli created by
Linzen et al. and the SentEval toolkit to build the binary classiﬁer
with the recommended hyperparameter space, using as features the activations from the (masked)
verb at hand.
Role scheme \ Layer
Left-to-right
Right-to-left
Bag-of-words
Bidirectional
Tree (random)
Table 4: Mean squared error between TPDN and BERT representation for a given layer and role scheme on SNLI
test instances. Each number corresponds to the average across ﬁve random initializations.
The keys to the cabinet are on the table
Figure 2: Dependency parse tree induced from attention head #11 in layer #2 using gold root (‘are’) as
starting node for maximum spanning tree algorithm.
Results in Table 3 show that the middle layers perform well in most cases, which supports
the result in Section 4 where the syntactic features
were shown to be captured well in the middle layers. Interestingly, as the number of attractors increases, one of the higher BERT layers (#8) is
able to handle the long-distance dependency problems caused by the longer sequence of words intervening between the subject and the verb, better than the lower layer (#7). This highlights the
need for BERT to have deeper layers to perform
competitively on NLP tasks.
Compositional Structure
Can we understand the compositional nature of
representation learned by BERT, if any? To investigate this question, we use Tensor Product
Decomposition Networks (TPDN) , which explicitly compose the input token
(“ﬁller”) representations based on the role scheme
selected beforehand using tensor product sum. For
instance, a role scheme for a word can be based on
the path from the root node to itself in the syntax tree (e.g. ‘LR’ denotes the right child of left
child of root). The authors assume that, for a given
role scheme, if a TPDN can be trained well to approximate the representation learned by a neural
model, then that role scheme likely speciﬁes the
compositionality implicitly learned by the model.
For each BERT layer, we work with ﬁve different role schemes. Each word’s role is computed
based on its left-to-right index, its right-to-left index, an ordered pair containing its left-to-right and
right-to-left indices, its position in a syntactic tree
 with no unary nodes
and no labels) and an index common to all the
words in the sentence (bag-of-words), which ignores its position. Additionally, we also deﬁne a
role scheme based on random binary trees.
Following McCoy et al. , we train our
TPDN model on the premise sentences in the
SNLI corpus . We initialize the ﬁller embeddings of the TPDN with the
pre-trained word embeddings from BERT’s input
layer, freeze it, learn a linear projection on top of
it and use a Mean Squared Error (MSE) loss function. Other trainable parameters include the role
embeddings and a linear projection on top of tensor product sum to match the embedding size of
BERT. Table 4 displays the MSE between representation from pretrained BERT and representation from TPDN trained to approximate BERT. We
discover that BERT implicitly implements a treebased scheme, as a TPDN model following that
scheme best approximates BERT’s representation
at most layers. This result is remarkable, as BERT
encodes classical, tree-like structures despite relying purely on attention mechanisms.
Motivated by this study, we perform a case
study on dependency trees induced from self attention weight following the work done by Raganato and Tiedemann . Figure 2 displays
the dependencies inferred from an example sentence by obtaining self attention weights for every word pairs from attention head #11 in layer
#2, ﬁxing the gold root as the starting node and
invoking the Chu-Liu-Edmonds algorithm . We observe that determiner-noun
dependencies (“the keys”, “the cabinet” and “the
table”) and subject-verb dependency (“keys” and
“are”) are captured accurately. Surprisingly, the
predicate-argument structure seems to be partly
modeled as shown by the chain of dependencies
between “key”,“cabinet” and “table”.
Related Work
Peters et al. studies how the choice of neural architecture such as CNNs, Transformers and
RNNs used for language model pretraining affects the downstream task accuracy and the qualitative properties of the contextualized word representations that are learned. They conclude that
all architectures learn high quality representations
that outperform standard word embeddings such
as GloVe for challenging NLP tasks. They also show that these architectures hierarchically structure linguistic information, such that morphological, (local) syntactic
and (longer range) semantic information tend to be
represented in, respectively, the word embedding
layer, lower contextual layers and upper layers. In
our work, we observe that such hierarchy exists as
well for BERT models that are not trained using
the standard language modelling objective. Goldberg shows that the BERT model captures
syntactic information well for subject-verb agreement. We build on this work by performing the test
on each layer of BERT controlling for the number of attractors and then show that BERT requires
deeper layers for handling harder cases involving
long-distance dependency information.
Tenney et al. is a contemporaneous work
that introduces a novel edge probing task to investigate how contextual word representations encode sentence structure across a range of syntactic, semantic, local and long-range phenomena.
They conclude that contextual word representations trained on language modeling and machine
translation encode syntactic phenomena strongly,
but offer comparably small improvements on semantic tasks over a non-contextual baseline. Their
result using BERT model on capturing linguistic hierarchy conﬁrms our probing task results although using a set of relatively simple probing
Liu et al. is another contemporaneous work that studies the features of language
captured/missed by contextualized vectors, transferability across different layers of the model and
the impact of pretraining on the linguistic knowledge and transferability. They ﬁnd that (i) contextualized word embeddings do not capture ﬁnegrained linguistic knowledge, (ii) higher layers of
RNN to be task-speciﬁc (with no such pattern for
a transformer) and (iii) pretraining on a closely related task yields better performance than language
model pretraining. Hewitt and Manning is
a very recent work which showed that we can recover parse trees from the linear transformation of
contextual word representation consistently, better
than with non-contextual baselines. They focused
mainly on syntactic structure while our work additionally experimented with linear structures (leftto-right, right-to-left) to show that the compositionality modelling underlying BERT mimics traditional syntactic analysis.
The recent burst of papers around these questions illustrates the importance of interpreting contextualized word embedding models and our work
complements the growing literature with additional evidences about the ability of BERT in
learning syntactic structures.
Conclusion
With our experiments, which contribute to a currently bubbling line of work on neural network
interpretability, we have shown that BERT does
capture structural properties of the English language.
Our results therefore conﬁrm those of
Goldberg ; Hewitt and Manning ;
Liu et al. ; Tenney et al. on BERT
who demonstrated that span representations constructed from those models can encode rich syntactic phenomena. We have shown that phrasal
representations learned by BERT reﬂect phraselevel information and that BERT composes a hierarchy of linguistic signals ranging from surface to
semantic features. We have also shown that BERT
requires deeper layers to model long-range dependency information. Finally, we have shown that
BERT’s internal representations reﬂect a compositional modelling that shares parallels with traditional syntactic analysis.
It would be interesting to see if our results transfer to other domains with higher variability in syntactic structures (such as noisy user generated content) and
with higher word order ﬂexibility as experienced
in some morphologically-rich languages.
Acknowledgments
We thank Grzegorz Chrupała and our anonymous
reviewers for providing insightful comments and
suggestions. This work was funded by the ANR
projects ParSiTi (ANR-16-CE33-0021), SoSweet
(ANR15-CE38-0011-01) and the French-Israeli
PHC Maimonide cooperation program.