“© 2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be
obtained for all other uses, in any current or future media, including reprinting/republishing this
material for advertising or promotional purposes, creating new collective works, for resale or
redistribution to servers or lists, or reuse of any copyrighted component of this work in other
Applications of Deep Reinforcement Learning in
Communications and Networking: A Survey
Nguyen Cong Luong, Dinh Thai Hoang, Member, IEEE, Shimin Gong, Member, IEEE, Dusit Niyato, Fellow,
IEEE, Ping Wang, Senior Member, IEEE, Ying-Chang Liang, Fellow, IEEE, and Dong In Kim, Fellow, IEEE
Abstract—This paper presents a comprehensive literature review on applications of deep reinforcement learning in communications and networking. Modern networks, e.g., Internet
of Things (IoT) and Unmanned Aerial Vehicle (UAV) networks,
become more decentralized and autonomous. In such networks,
network entities need to make decisions locally to maximize the
network performance under uncertainty of network environment.
Reinforcement learning has been efﬁciently used to enable the
network entities to obtain the optimal policy including, e.g.,
decisions or actions, given their states when the state and
action spaces are small. However, in complex and large-scale
networks, the state and action spaces are usually large, and the
reinforcement learning may not be able to ﬁnd the optimal policy
in reasonable time. Therefore, deep reinforcement learning, a
combination of reinforcement learning with deep learning, has
been developed to overcome the shortcomings. In this survey,
we ﬁrst give a tutorial of deep reinforcement learning from
fundamental concepts to advanced models. Then, we review deep
reinforcement learning approaches proposed to address emerging
issues in communications and networking. The issues include
dynamic network access, data rate control, wireless caching,
data ofﬂoading, network security, and connectivity preservation
which are all important to next generation networks such as
5G and beyond. Furthermore, we present applications of deep
reinforcement learning for trafﬁc routing, resource sharing,
and data collection. Finally, we highlight important challenges,
open issues, and future research directions of applying deep
reinforcement learning.
Keywords- Deep reinforcement learning, deep Q-learning, networking, communications, spectrum access, rate control, security,
caching, data ofﬂoading, data collection.
I. INTRODUCTION
Reinforcement learning is one of the most important
research directions of machine learning which has signiﬁcant
impacts to the development of Artiﬁcial Intelligence (AI) over
the last 20 years. Reinforcement learning is a learning process
in which an agent can periodically make decisions, observe
the results, and then automatically adjust its strategy to achieve
the optimal policy. However, this learning process, even though
proved to converge, takes a lot of time to reach the best policy
N. C. Luong and D. Niyato are with School of Computer Science
and Engineering, Nanyang Technological University, Singapore. E-mails:
 , .
D. T. Hoang is with the Faculty of Engineering and Information Technology,
University of Technology Sydney, Australia. E-mail: .
S. Gong is with School of Intelligent Systems Engineering, Sun Yat-sen
University, Guangzhou, 510275, China. E-mail: .
P. Wang is with Department of Electrical Engineering & Computer Science,
York University, Canada. E-mail: .
Y.-C. Liang is with Center for Intelligent Networking and Communications
(CINC), with University of Electronic Science and Technology of China,
Chengdu, China. E-mail: .
D. I. Kim is with School of Information and Communication Engineering,
Sungkyunkwan University, Korea. Email: .
as it has to explore and gain knowledge of an entire system,
making it unsuitable and inapplicable to large-scale networks.
Consequently, applications of reinforcement learning are very
limited in practice. Recently, deep learning has been introduced as a new breakthrough technique. It can overcome the
limitations of reinforcement learning, and thus open a new era
for the development of reinforcement learning, namely Deep
Reinforcement Learning (DRL). DRL embraces the advantage
of Deep Neural Networks (DNNs) to train the learning process,
thereby improving the learning speed and the performance of
reinforcement learning algorithms. As a result, DRL has been
adopted in numerous applications of reinforcement learning in
practice such as robotics, computer vision, speech recognition,
and natural language processing . One of the most famous
applications of DRL is AlphaGo , the ﬁrst computer program which can beat a human professional without handicaps
on a full-sized 19×19 board.
In the areas of communications and networking, DRL
has been recently used as an emerging tool to effectively
address various problems and challenges. In particular, modern
networks such as Internet of Things (IoT), Heterogeneous
Networks (HetNets), and Unmanned Aerial Vehicle (UAV)
network become more decentralized, ad-hoc, and autonomous
in nature. Network entities such as IoT devices, mobile users,
and UAVs need to make local and autonomous decisions, e.g.,
spectrum access, data rate selection, transmit power control,
and base station association, to achieve the goals of different
networks including, e.g., throughput maximization and energy
consumption minimization. Under uncertain and stochastic
environments, most of the decision-making problems can be
modeled by a so-called Markov Decision Process (MDP) .
Dynamic programming , and other algorithms such
as value iteration, as well as reinforcement learning techniques can be adopted to solve the MDP. However, the
modern networks are large-scale and complicated, and thus the
computational complexity of the techniques rapidly becomes
unmanageable. As a result, DRL has been developing to be
an alternative solution to overcome the challenge. In general,
the DRL approaches provide the following advantages:
• DRL can obtain the solution of sophisticated network
optimizations. Thus, it enables network controllers, e.g.,
base stations, in modern networks to solve non-convex
and complex problems, e.g., joint user association, computation, and transmission schedule, to achieve the optimal solutions without complete and accurate network
information.
• DRL allows network entities to learn and build knowl-
edge about the communication and networking environment. Thus, by using DRL, the network entities, e.g., a
mobile user, can learn optimal policies, e.g., base station
selection, channel selection, handover decision, caching
and ofﬂoading decisions, without knowing channel model
and mobility pattern.
• DRL provides autonomous decision-making. With the
DRL approaches, network entities can make observation
and obtain the best policy locally with minimum or
without information exchange among each other. This not
only reduces communication overheads but also improves
security and robustness of the networks.
• DRL improves signiﬁcantly the learning speed, especially
in the problems with large state and action spaces. Thus,
in large-scale networks, e.g., IoT systems with thousands
of devices, DRL allows network controller or IoT gateways to control dynamically user association, spectrum
access, and transmit power for a massive number of IoT
devices and mobile users.
• Several other problems in communications and networking such as cyber-physical attacks, interference management, and data ofﬂoading can be modeled as games, e.g.,
the non-cooperative game. DRL has been recently used
as an efﬁcient tool to solve the games, e.g., ﬁnding the
Nash equilibrium, without the complete information.
Although there are some surveys related to machine learning, they do not discuss the applications of DRL in communications and networking. Speciﬁcally, there are surveys
that discuss the applications of DRL such as and ,
but they are speciﬁcally for computer vision and natural
language processing. Also, there are surveys discussing the
applications of machine learning for networking such as
 , , , , and . However, they mostly focus
on deep learning approaches. In particular, the survey in 
discusses deep learning approaches for network cybersecurity, the survey in reviews deep learning approaches
for network trafﬁc control, the survey in presents deep
learning approaches for physical layer modulation, network
access/resource allocation, and network routing, and the survey
in presents deep learning approaches for emerging issues
including edge caching and computing, multiple radio access
and interference management. In summary, the existing surveys either consider applications of DRL for computer vision
and natural language processing or discuss applications of
deep learning for networking. There is no survey speciﬁcally
discussing the applications of DRL for communications and
networking. This motivates us to deliver the survey with the
tutorial of DRL and the comprehensive literature review on the
applications of DRL to address issues in communications and
networking. For convenience, the related works in this survey
are classiﬁed based on issues in communications and networking as shown in Fig. 2. The major issues include network
access, data rate control, wireless caching, data ofﬂoading,
network security, connectivity preservation, trafﬁc routing, and
data collection. Also, the percentages of DRL related works
for different networks and different issues in the networks are
shown in Figs. 1(a) and 1(b), respectively. From the ﬁgures,
communications
Network access
Computational
offloading
Network security
Connectivity
prevservation
scheduling
Data collection
Fig. 1: Percentages of related work for (a) different networks and
(b) different issues in the networks.
we observe that the majority of the related works are for
the cellular networks. Also, the related works to the wireless
caching and ofﬂoading have received more attention than the
other issues.
TABLE I: List of abbreviations
Abbreviation
Description
Artiﬁcial Neural Network/Artiﬁcial Potential Field
Asynchronous Advantage Actor- Critic
Cognitive Radio Network
Convolutional Neural Network
Deep Reinforcement Learning/Deep Q-Learning
Deep Neural Network
DQN/DDQN/DRQN
Q-Network/Double
Dynamic Adaptive Streaming over HTTP
Denial-of-Service
Echo State Network
Feedforward Neural Network/Recurrent Neural Network
Finite-State Markov Channel
High Volume Flexible Time
Intelligent Transportation System
Liquid State Machine/Long Short-Term Memory
Mobile Edge Computing
Markov Decision Process/Partially Observable MDP
Neural Fictitious Self-Play
Network Function Virtualization
Recurrent Deterministic Policy Gradient
Recursive Convolutional Neural Network
Remote Radio Head/BaseBand Unit
Received Signal Strength Indicators
Sequential Prisoner’s Dilemma
Small Base Station/Base Station
Software-Deﬁned Network
Secondary User/Primary User
Ultra-Density Network/Underwater Acoustic Network
Unmanned Aerial Vehicle
Vehicular Ad hoc Network/Vehicle-to-Vehicle
Applications of deep reinforcement learning
for communications and networking
Network access and rate
Miscellaneous issues
Security and
connectivity preservation
Caching and offloading
rate control
offloading
Connectivity
preservation
scheduling
collection
Fig. 2: A taxonomy of the applications of deep reinforcement learning for communications and networking.
The rest of this paper is organized as follows. Section II
presents the introduction of reinforcement learning and discusses DRL techniques as well as their extensions. Section III
reviews the applications of DRL for dynamic network access
and adaptive data rate control. Section IV discusses the applications of DRL for wireless caching and data ofﬂoading.
Section V presents DRL related works for network security
and connectivity preservation. Section VI considers how to use
DRL to deal with other issues in communications and networking. Important challenges, open issues, and future research
directions are outlined in Section VII. Section VIII concludes
the paper. The list of abbreviations commonly appeared in
this paper is given in Table I. Note that DRL consists of two
different algorithms which are Deep Q-Learning (DQL) and
policy gradients . In particular, DQL is mostly used for
the DRL related works. Therefore, in the rest of the paper, we
use “DRL” and “DQL” interchangeably to refer to the DRL
algorithms.
II. DEEP REINFORCEMENT LEARNING: AN OVERVIEW
In this section, we ﬁrst present fundamental knowledge of
Markov decision processes, reinforcement learning, and deep
learning techniques which are important branches of machine
learning theory. We then discuss DRL technique that can
capitalize on the capability of the deep learning to improve
efﬁciency and performance in terms of the learning rate for
reinforcement learning algorithms. Afterward, advanced DRL
models and their extensions are reviewed.
A. Markov Decision Processes
MDP is a discrete time stochastic control process. MDP
provides a mathematical framework for modeling decisionmaking problems in which outcomes are partly random and
under control of a decision maker or an agent. MDPs are useful
for studying optimization problems which can be solved by
dynamic programming and reinforcement learning techniques.
Typically, an MDP is deﬁned by a tuple (S, A, p, r) where
S is a ﬁnite set of states, A is a ﬁnite set of actions, p is
a transition probability from state s to state s′ after action
a is executed, and r is the immediate reward obtained after
action a is performed. We denote π as a “policy” which is a
mapping from a state to an action. The goal of an MDP is to
ﬁnd an optimal policy to maximize the reward function. An
MDP can be ﬁnite or inﬁnite time horizon. For the ﬁnite time
horizon MDP, an optimal policy π∗to maximize the expected
total reward is deﬁned by max
rt(st, π(st))
at = π(st). For the inﬁnite time horizon MDP, the objective
can be to maximize the expected discounted total reward or
to maximize the average reward. The former is deﬁned by
γrt(st, π(st))
, while the latter is expressed by
rt(st, π(st))
, where γ ∈ is the discount factor. The discount factor γ determines the importance
of future rewards compared with the current reward. If γ = 0,
the agent is “myopic”, i.e., it only considers to maximize
its current reward, i.e., immediate reward. In contrast, if γ
approaches one, the agent will strive for a long-term higher
1) Partially Observable Markov Decision Process:
MDPs, we assume that the system state is fully observable by the agent. However, in many cases, the agent only
can observe a part of the system state, and thus Partially
Observable Markov Decision Processes (POMDPs) can
be used to model the decision-making problems. A typical
POMDP model is deﬁned by a 6-tuple (S, A, p, r, Ω, O),
where S, A, p, r are deﬁned the same as in the MDP model, Ω
and O are deﬁned as the set of observations and observation
probabilities, respectively. At each time epoch, the agent is
at state s, selects an action a based on its belief about the
current state s, i.e., b(s), and observes the immediate reward
r and current observation o. Based on the observation o and
its belief about the current state b(s), the agent then updates
its belief about the new state s′, i.e., b(s′), as follows :
O(o|s, a, s′) P
s∈S p(s′|s, a)b(s)
s′∈S O(o|s, a, s′) P
s∈S p(s′|s, a)b(s),
where O(o|s, a, s′) is the probability that the agent obtains
observation o after action a is taken at state s and the
agent moves to state s′. p(s′|s, a) is deﬁned the same as
that of the MDP model, i.e., the transition probability from
state s to state s′ after action a is performed at state s.
Finally, the agent receives an immediate reward r that is
equal to r(s, a) in the MDP. Similar to the MDP model, the
agent in POMDP also aims to ﬁnd the optimal policy π∗in
order to maximize its expected long-term discounted reward
γrt(st, π∗(st)). Fig. 3 illustrates and compares an MDP
and a POMDP.
Agent s actions
Agent s observations
Agent s belief state updates
System state transition
Fig. 3: An illustration of MDP and POMDP.
2) Markov Games: In game theory, a Markov game, or
a stochastic game , is a dynamic game with probabilistic transitions played by multiple players, i.e., agents.
A typical Markov game model is deﬁned by a tuple
(I, S, {Ai}i∈I, p, {ri}i∈I), where
• I ≜{1, . . . , i, . . . , I} is a set of agents,
• S ≜{S1, . . . , Si, . . . , SI} is the global state space of all
agents with Si being the state space of agent i,
• {Ai}i∈I are sets of action spaces of the agents with Ai
being the action space of agent i,
• p ≜S×A1×· · ·×AI → is the transition probability
function of the system.
• {ri}i∈I are payoff functions of the agents with
ri ≜S × A1 × · · · × AI →R, i.e., the payoff of agent i
obtained after all actions of the agents are executed.
In a Markov game, the agents start at some initial state
s0 ∈S. After observing the current state, all the agents
simultaneously select their actions a = {a1, . . . , aI} and they
will receive their corresponding rewards together with their
own new observations. At the same time, the system will
transit to a new state s′ ∈S with probability p(s′|s, a). The
procedure is repeated at the new state and continues for a ﬁnite
or inﬁnite number of stages. In this game, all the agents try
to ﬁnd their optimal policies to maximize their own expected
long-term average rewards, i.e.,
i (st)), ∀i. The
set of all optimal policies of this game, i.e., {π∗
1, . . . , π∗
known to be the equilibrium of this game. If there is a ﬁnite
number of players and the sets of states and actions are ﬁnite,
then the Markov game always has a Nash equilibrium 
under a ﬁnite number of stages. The same is true for Markov
games with inﬁnite stages, but the total payoff of agents is the
discounted sum .
B. Reinforcement Learning
Reinforcement learning, an important branch of machine
learning, is an effective tool and widely used in the literature
to address MDPs . In a reinforcement learning process, an
agent can learn its optimal policy through interaction with its
environment. In particular, the agent ﬁrst observes its current
state, and then takes an action, and receives its immediate
reward together with its new state as illustrated in Fig. 4(a).
The observed information, i.e., the immediate reward and new
state, is used to adjust the agent’s policy, and this process
will be repeated until the agent’s policy approaches to the
optimal policy. In reinforcement learning, Q-learning is the
most effective method and widely used in the literature. In the
following, we will discuss the Q-learning algorithm and its
extensions for advanced MDP models.
1) Q-Learning Algorithm: In an MDP, we aim to ﬁnd an
optimal policy π∗: S →A for the agent to maximize the expected long-term reward function for the system. Accordingly,
we ﬁrst deﬁne value function Vπ : S →R that represents
the expected value obtained by following policy π from each
state s ∈S. The value function V for policy π quantiﬁes
the goodness of the policy through an inﬁnite horizon and
discounted MDP that can be expressed as follows:
Vπ(s) = Eπ
γrt(st, at)|s0 = s
rt(st, at) + γVπ(st+1)|s0 = s
Since we aim to ﬁnd the optimal policy π∗, an optimal action at each state can be found through the optimal value function expressed by V∗(s) = max
rt(st, at)+γVπ(st+1)
If we denote Q∗(s, a) ≜rt(st, at)+γEπ
optimal Q-function for all state-action pairs, then the optimal
value function can be written by V∗(s) = max
Now, the problem is reduced to ﬁnd optimal values of Qfunction, i.e., Q∗(s, a), for all state-action pairs, and this
can be done through iterative processes. In particular, the Qfunction is updated according to the following rule:
Qt+1(s, a) =Qt(s, a)+
rt(s, a) + γ max
a′ Qt(s, a′) −Qt(s, a)
The core idea behind this update is to ﬁnd the Temporal
Difference (TD) between the predicted Q-value, i.e., rt(s, a)+
a′ Qt(s, a′) and its current value, i.e., Qt(s, a). In (3),
the learning rate αt is used to determine the impact of new
information to the existing Q-value. The learning rate can
be chosen to be a constant, or it can be adjusted dynamically during the learning process. However, it must satisfy
Assumption 1 to guarantee the convergence for the Q-learning
algorithm.
Environment
Immediate reward r
Observed state s
Controller
Input layer
Hidden layer
Output layer
Environment
Immediate reward r
Observed state s
Controller policy π
Fig. 4: (a) Reinforcement learning, (b) Artiﬁcial neural network, and (c) Deep Q-learning.
Assumption 1. The step size αt is deterministic, nonnegative
and satisﬁes the following conditions: αt ∈ ,
(αt)2 < ∞.
The step size adaptation αt = 1
t is one of the most common
examples used in reinforcement learning. More discussions
for selecting an appropriate step size can be found in .
The details of the Q-learning algorithm are then provided in
Algorithm 1.
Algorithm 1 The Q-learning algorithm
Input: For each state-action pair (s, a), initialize the table
entry Q(s, a) arbitrarily, e.g., to zero. Observe the current
state s, initialize a value for the learning rate α and the
discount factor γ.
for t := 1 to T do
From the current state-action pair (s, a), execute action
a and obtain the immediate reward r and a new state s′.
Select an action a′ based on the state s′ and then update
the table entry for Q(s, a) as follows:
Qt+1(s, a) ←Qt(s, a) + αt
a′ Qt(s′, a′) −Qt(s, a)
Replace s ←s′.
Output: π∗(s) = arg maxa Q∗(s, a).
Once either all Q-values converge or a certain number
of iterations is reached, the algorithm will terminate. The
algorithm then yields the optimal policy indicating an action
to be taken at each state such that Q∗(s, a) is maximized for
all states in the state space, i.e., π∗(s) = arg max
a Q∗(s, a).
Under the assumption of the step size (i.e., Assumption 1), it
is proved in that the Q-learning algorithm converges to
the optimum action-values with probability one.
It is worth noting that unlike value function Vπ, Q-function
is an example of model-free learning algorithm that does not
require the agent to know the system model parameters, e.g.,
the state-transition and reward models, in advance to estimate
the pairs of state-action values. Speciﬁcally, the core idea
of Q-function is to approximate the values of state-action
pairs through samples obtained during the interactions with
the environment. Furthermore, while the value function takes
the expectation of all actions according to the policy π, the
Q-function only focuses on a particular action at a particular
state. As a result, learning algorithms using Q-function are
less complex than those of using the value function. However,
from the sampling perspective, the dimension of Q-function is
higher than that of value function, and thus Q-function might
be more difﬁcult to get enough samples, i.e., state-action pairs,
to learn. Therefore, if the system model is available in advance,
the value function is usually preferable.
2) SARSA: An Online Q-Learning Algorithm: Although the
Q-learning algorithm can ﬁnd the optimal policy for the agent
without requiring knowledge about the environment, this algorithm works in an ofﬂine fashion. In particular, Algorithm 1
can obtain the optimal policy only after all Q-values converge.
Therefore, this section presents an alternative online learning
algorithm, i.e., the SARSA algorithm, which allows the agent
to approach the optimal policy in an online fashion.
Different from the Q-learning algorithm, the SARSA algorithm is an online algorithm which allows the agent to
choose optimal actions at each time step in a real-time fashion
without waiting until the algorithm converges. In the Qlearning algorithm, the policy is updated according to the
maximum reward of available actions regardless of which
policy is applied, i.e., an off-policy method. In contrast, the
SARSA algorithm interacts with the environment and updates
the policy directly from the actions taken, i.e., an on-policy
method. Note that the SARSA algorithm updates Q-values
from the quintuple Q(s, a, r, s′, a′).
3) Q-Learning for Markov Games: To apply Q-learning
algorithm to the Markov game context, we ﬁrst deﬁne the
Q-function for agent i by Qi(s, ai, a−i), where a−i
{a1, . . . , ai−1, ai+1, . . . , aI} denotes the set of actions of all
agents except i. Then, the Nash Q-function of agent i is
deﬁned by:
i (s, ai, a−i) = ri(s, ai, a−i)+
p(s′|s, ai, a−i)Vi(s′, π∗
1, . . . , π∗
1, . . . , π∗
I) is the joint Nash equilibrium strategy,
ri(s, ai, a−i) is agent i’s immediate reward in state s under
the joint action (ai, a−i), and Vi(s′, π∗
1, . . . , π∗
I) is the total
discounted reward over an inﬁnite time horizon starting from
state s′ given that all the agents follow the equilibrium
strategies.
In , the authors propose a multi-agent Q-learning algorithm for general-sum Markov games which allows the agents
to perform updates based on assuming Nash equilibrium
behavior over the current Q-values. In particular, agent i will
learn its Q-values by forming an arbitrary guess from starting
time of the game. At each time step t, agent i observes the
current state and takes an action ai. Then, it observes its
immediate reward ri, actions taken by others a−i, others’
immediate rewards, and the new system state s′. After that,
agent i calculates a Nash equilibrium (π1(s′), . . . , πI(s′)) for
the state game (Qt
1(s′), . . . , Qt
I(s′)), and updates its Q-values
according to:
(s, ai, a−i) = (1−αt)Qt
i(s, ai, a−i)+αt[ri
where αt ∈(0, 1) is the learning rate and N i
t (s′) ≜Qt
π1(s′) × · · · × πI(s′).
In order to calculate the Nash equilibrium, agent i needs to
1(s′), . . . , Qt
I(s′)). However, the information about
other agents’ Q-values is not given, and thus agent i must learn
this information too. To do so, agent i will set estimations
about others’ Q-values at the beginning of the game, e.g.,
0(s, ai, a−i) = 0, ∀j, s. As the game proceeds, agent i
observes other agents’ immediate rewards and previous actions. That information can then be used to update agent i’s
conjectures on other agents’ Q-functions. Agent i updates its
beliefs about agent j’s Q-function, according to the same
updating rule in (6). Then, the authors prove that under
some highly restrictive assumptions on the form of the state
games during learning, the proposed multi-agent Q-learning
algorithm is guaranteed to be converged.
C. Deep Learning
Deep learning is composed of a set of algorithms and
techniques that attempt to ﬁnd important features of data and
to model its high-level abstractions. The main goal of deep
learning is to avoid manual description of a data structure (like
hand-written features) by automatic learning from the data. Its
name refers to the fact that typically any neural network with
two or more hidden layers is called DNN. Most deep learning
models are based on an Artiﬁcial Neural Network (ANN), even
though they can also include propositional formulas or latent
variables organized layer-wise in deep generative models such
as the nodes in Deep Belief Networks and Deep Boltzmann
An ANN is a computational nonlinear model based on the
neural structure of the brain that is able to learn to perform
tasks such as classiﬁcation, prediction, decision-making, and
visualization. An ANN consists of artiﬁcial neurons and is
organized into three interconnected layers: input, hidden, and
output as illustrated in Fig. 4(b). The input layer contains
input neurons that send information to the hidden layer. The
hidden layer sends data to the output layer. Every neuron has
weighted inputs (synapses), an activation function, and one
output. Synapses are the adjustable parameters that convert
a neural network to a parameterized system. The activation
function of a node deﬁnes the outputs of that node given the
inputs. In particular, the activation function will map the input
values into target ranges depending on the selected activation
function. For example, the logistic activation function will map
all inputs in the real number domain into the range of 0 to 1.
During the training phase, ANNs use backpropagation as an
effective learning algorithm to compute quickly a gradient descent with respect to the weights. Backpropagation is a special
case of automatic differentiation. In the context of learning,
backpropagation is commonly used by the gradient descent
optimization algorithm to adjust the weights of neurons by
calculating the gradient of the loss function. This technique is
also sometimes called backward propagation of errors, because
the error is calculated at the output and distributed back
through the network layers.
Recurrent Neural Network (RNN)
Feed-Forward Neural Network (FNN)
Fig. 5: FNN and RNN.
A DNN is deﬁned as an ANN with multiple hidden layers.
There are two typical DNN models, i.e., Feedforward Neural
Network (FNN) and Recurrent Neural Network (RNN). In the
FNN, the information moves in only one direction, i.e., from
the input nodes, through the hidden nodes and to the output
nodes, and there are no cycles or loops in the network as shown
in Fig. 5. In FNNs, Convolutional Neural Network (CNN) is
the most well known model with a wide range of applications
especially in image and speech recognition. The CNN contains
one or more convolutional layers, pooling or fully connected,
and uses a variation of multilayer perceptrons discussed above.
In general, CNNs have two main components, i.e., feature
extraction and classiﬁcation, as illustrated in Fig. 6. The
feature extraction component is placed at the hidden layers
with the aim of performing a series of convolutions and pooling operations during which the features are being detected.
After that, the classiﬁcation component, placed at the fully
connected layers, will assign a probability for the object, e.g.,
on the image, to what we need to predict.
Unlike FNNs, the RNN is a variant of a recursive artiﬁcial
neural network in which connections between neurons make
directed cycles. It means that an output depends not only
on its immediate inputs, but also on the previous further
step’s neuron state. The RNNs are designed to utilize sequential data, when the current step has some relation with the
previous steps. This makes the RNNs ideal for applications
with a time component, e.g., time-series data, and natural
language processing. However, all RNNs have feedback loops
in the recurrent layer. This lets RNNs maintain information
Convolution and
nonlinearity
Convolution and pooling layers
Max pooling
Fully connected layers
Classification
Fig. 6: An illustration of CNN.
in memory over time. Nevertheless, it can be difﬁcult to train
standard RNNs to solve problems that require learning longterm temporal dependencies. The reason is that the gradient
of the loss function decays exponentially with time, which
is called the vanishing gradient problem. Thus, Long Short-
Term Memory (LSTM) is often used in RNNs to address
this issue. LSTMs are designed to model temporal sequences
and their long-range dependencies are more accurate than
conventional RNNs. In particular, LSTMs provide a solution
by incorporating memory units that allow the network to learn
when to forget previous hidden states and when to update
hidden states given new information. Usually, LSTM units are
implemented in “blocks” which have three or four “gates”,
e.g., input gate, forget gate, output gate and input modulation
gate, as illustrated in Fig. 7 to control information
ﬂow drawing on the logistic function. Unlike the RNN, the
LSTM memory cell is composed of three components, i.e.,
the previous memory cell ct, current input xt and previous
hidden state ht−1. Here, the input gate and forget gate will be
used to selectively forget its previous memory or consider its
current input. Likewise, the output gate learns how much of the
memory cell to transfer to the hidden state. These additional
blocks enable the LSTM to learn extremely complex and longterm temporal dynamics that the RNN is unable to do.
Fig. 7: An illustration of a basic (a) LSTM and (b) RNN cell.
D. Deep Q-Learning
The Q-learning algorithm can efﬁciently obtain an optimal
policy when the state space and action space are small.
However, in practice, with complicated system models, these
spaces are usually large. As a result, the Q-learning algorithm
may not be able to ﬁnd the optimal policy. Thus, Deep Q-
Learning (DQL) algorithm is introduced to overcome this
shortcoming. Intuitively, the DQL algorithm implements a
Deep Q-Network (DQN), i.e., a DNN, instead of the Q-table to
derive an approximate value of Q∗(s, a) as shown in Fig. 4(c).
As stated in , the average reward obtained by reinforcement learning algorithms may not be stable or even
diverge when a nonlinear function approximator is used.
This stems from the fact that a small change of Q-values
may greatly affect the policy. Thus, the data distribution and
the correlations between the Q-values and the target values
R + γ maxa′ Q(s′, a′) are varied. To address this issue, two
mechanisms, i.e., experience replay and target Q-network, can
• Experience replay mechanism: The algorithm ﬁrst initializes a replay memory D, i.e., the memory pool, with
transitions (st, at, rt, st+1), i.e., experiences, generated
randomly, e.g., through using ϵ-greedy policy. Then, the
algorithm randomly selects samples, i.e., minibatches,
of transitions from D to train the DNN. The Q-values
obtained by the trained DNN will be used to obtain new
experiences, i.e., transitions, and these experiences will
then be stored in the memory pool D. This mechanism
allows the DNN trained more efﬁciently by using both
old and new experiences. In addition, by using the experience replay, the transitions are more independent and
identically distributed, and thus the correlations between
observations can be removed.
• Fixed target Q-network: In the training process, the Qvalue will be shifted. Thus, the value estimations can
be out of control if a constantly shifting set of values
is used to update the Q-network. This leads to the
destabilization of the algorithm. To address this issue,
the target Q-network is used to update frequently but
slowly the primary Q-networks’ values. In this way,
the correlations between the target and estimated Qvalues are signiﬁcantly reduced, thereby stabilizing the
algorithm.
The DQL algorithm with experience replay and ﬁxed target
Q-network is presented in Algorithm 2. DQL inherits and
promotes advantages of both reinforcement and deep learning
techniques, and thus it has a wide range of applications in
practice such as game development , transportation ,
and robotics . Table II summarizes how different the
DQL approach and other techniques, i.e., reinforcement learning, deep learning, and traditional combinatorial optimization
methods, solve optimization problems.
E. Advanced Deep Q-Learning Models
1) Double Deep Q-Learning: In some stochastic environments, the Q-learning algorithm performs poorly due to
the large over-estimations of action values . These overestimations result from a positive bias that is introduced
because Q-learning uses the maximum action value as an
approximation for the maximum expected action value as
shown in Eq. (4). The reason is that the same samples are used
to decide which action is the best, i.e., with highest expected
reward, and the same samples are also used to estimate that
action-value. Thus, to overcome the over-estimation problem
of the Q-learning algorithm, the authors in introduce a
TABLE II: Comparisons among optimization techniques
Techniques
Problem-solving
Non-linear programming
Used to address static optimization problems, i.e., optimizes the objective function for one time instant
only. To address this optimization, we can analyze the objective function and adopt suitable techniques.
For example, if the objective function is quadratic and the constraints are linear, quadratic programming
techniques can be used.
Dynamic programming
Used to address complex problem by breaking it down into a set of simpler subproblems over multiple
steps, solving each of those subproblems just once at at a time, and storing their solutions in a memory.
Thus, in the future, if the same subproblem occurs, instead of recomputing its solution, one simply
looks up the previously computed solution, thereby saving computation time.
Reinforcement Learning (RL)
This is a branch of machine learning used to help an agent to learn the optimal policy when the agent
has no information about the surrounding environment. Speciﬁcally, the agent ﬁrst observes its current
state, and then takes an action, and receives its immediate reward together with its new state. The
observed information, i.e., the immediate reward and new state, is used to adjust the agent’s policy,
and this process is repeated until the agent’s policy approaches the optimal policy.
Deep Learning (DL)
This is a branch of machine learning used to help an agent to learn the optimal policy when the agent
has some information about surrounding environment in advance. In particular, based on the obtained
information (i.e., stored in a database), the agent will train the neural network and ﬁnd the optimal
parameters for the network. The trained neural network will be then implemented on the agent to help
the agent make decisions in an online fashion.
Deep Reinforcement Learning (DRL)
This is an advanced model of reinforcement learning technique in which deep learning is utilized as
an effective tool to improve learning rate for reinforcement learning algorithms. In particular, during
real-time learning process, the obtained experiences will be stored and used as the data to train the
neural network. The trained neural network will be then used to help the agent make optimal decisions
in a real-time manner. Note that unlike deep learning technique, the neural network in the DRL will be
trained frequently based on new experiences obtained during the real-time interactions with surrounding
environment.
Algorithm 2 The DQL Algorithm with Experience Replay
and Fixed Target Q-Network
1: Initialize replay memory D.
2: Initialize the Q-network Q with random weights θ.
3: Initialize the target Q-network ˆQ with random weights θ′.
4: for episode=1 to T do
With probability ϵ select a random action at, otherwise
select at = arg max Q∗(st, at, θ).
Perform action at and observe immediate reward rt and
next state st+1.
Store transition (st, at, rt, st+1) in D.
Select randomly samples c(sj, aj, rj, sj+1) from D.
The weights of the neural network then are optimized
by using stochastic gradient descent with respect to the
network parameter θ to minimize the loss:
rj + γ max
ˆQ(sj+1, aj+1; θ′) −Q(sj, aj; θ)
Reset ˆQ = Q after every a ﬁxed number of steps.
11: end for
solution using two Q-value functions, i.e., Q1 and Q2, to
simultaneously select and evaluate action values. In particular,
the selection of an action is still due to the online weights θ1.
This means that, as in Q-learning, we are still estimating the
value of the greedy policy according to the current values, as
deﬁned by θ1. However, the second set of weights θ2 is used
to evaluate fairly the value of this policy. This second set of
weights can be updated symmetrically by switching the roles
of θ1 and θ2. Inspired by this idea, the authors in then
develop Double Deep Q-Learning (DDQL) model using
a Double Deep Q-Network (DDQN) with the loss function
updated as follows:
sj+1, arg max
 sj+1, aj+1; θ
−Q(sj, aj; θ)
Unlike double Q-learning, the weights of the second network θ2 are replaced with the weights of the target networks
θ′ for the evaluation of the current greedy policy as shown
in Eq. (8). The update to the target network stays unchanged
from DQN, and remains a periodic copy of the online network.
Due to the effectiveness of DDQL, there are some applications
of DDQL introduced recently to address dynamic spectrum
access problems in multichannel wireless networks and
resource allocation in heterogeneous networks .
2) Deep Q-Learning with Prioritized Experience Replay:
Experience replay mechanism allows the reinforcement learning agent to remember and reuse experiences, i.e., transitions,
from the past. In particular, transitions are uniformly sampled
from the replay memory D. However, this approach simply
replays transitions at the same frequency as that the agent
was originally experienced, regardless of their signiﬁcance.
Therefore, the authors in develop a framework for prioritizing experiences, so as to replay important transitions
more frequently, and therefore learn more efﬁciently. Ideally,
we want to sample more frequently those transitions from
which there is much to learn. In general, the DQL with the
Prioritized Experience Replay (PER) samples transitions with
a probability related to the last encountered absolute error
 . New transitions are inserted into the replay buffer with
maximum priority, providing a bias towards recent transitions.
Note that stochastic transitions may also be favoured, even
when there is little left to learn about them. Through real
experiments on many Atari games, the authors demonstrate
that DQL with PER outperforms DQL with uniform replay on
41 out of 49 games. However, this solution is only appropriate
to implement when we can ﬁnd and deﬁne the important
experiences in the replay memory D.
3) Dueling Deep Q-Learning: The Q-values, i.e., Q(s, a),
used in the Q-learning algorithm, i.e., Algorithm 1, are to
express how good it is to take a certain action at a given state.
The value of an action a at a given state s can actually be
decomposed into two fundamental values. The ﬁrst value is
the state-value function, i.e., V (s), to estimate the importance
of being in a particular state s. The second value is the
action-value function, i.e., A (a), to estimate the importance of
selecting an action a compared with other actions. As a result,
the Q-value function can be expressed by two fundamental
value functions as follows: Q(s, a) = V (s) + A (a).
Stemming from the fact that in many MDPs, it is unnecessary to estimate both values, i.e., action and state values of
Q-function Q(s, a), at the same time. For example, in many
racing games, moving left or right matters if and only if the
agent meets the obstacles or enemies. Inspired by this idea, the
authors in introduce an idea of using two streams, i.e.,
two sequences, of fully connected layers instead of using a
single sequence with fully connected layers for the DQN. The
two streams are constructed such that they are able to provide
separate estimations on the action and state value functions,
i.e., V (s) and A (a). Finally, the two streams are combined
to generate a single output Q(s, a) as follows:
Q(s, a; α, β) = V (s; β) +
A (s, a; α) −
a′ A (s, a′; α)
where β and α are the parameters of the two streams V (s; β)
and A (s, a′; α), respectively. Here, |A| is the total number
of actions in the action space A. Then, the loss function is
derived in the similar way to (7). Through the simulation, the
authors show that the proposed dueling DQN can outperform
DDQN in 50 out of 57 learned Atari games. However,
the proposed dueling architecture only clearly beneﬁts for
MDPs with large action spaces. For small state spaces, the
performance of dueling DQL is even not as good as that of
double DQL as shown in simulation results in .
4) Asynchronous Multi-step Deep Q-Learning: Most of the
Q-learning methods such as DQL and dueling DQL rely
on the experience replay method. However, such kind of
method has several drawbacks. For example, it uses more
memory and computation resources per real interaction, and
it requires off-policy learning algorithms that can update from
data generated by an older policy. This limits the applications
of DQL. Therefore, the authors in introduce a method
using multiple agents to train the DNN in parallel. In particular, the authors propose a training procedure which utilizes
asynchronous gradient decent updates from multiple agents at
once. Instead of training one single agent that interacts with
its environment, multiple agents are interacting with their own
version of the environment simultaneously. After a certain
amount of timesteps, accumulated gradient updates from an
agent are applied to a global model, i.e., the DNN. These
updates are asynchronous and lock free. In addition, to tradeoff
between bias and variance in the policy gradient, the authors
adopt n-step updates method to update the reward function.
In particular, the truncated n-step reward function can be
deﬁned by r(n)
γ(k)rt+k+1. Thus, the alternative loss
for each agent will be derived by:
ˆQ(sj+n, a′; θ′) −Q(sj, aj; θ)
The effects of training speed and quality of the proposed
asynchronous DQL with multi-step learning are analyzed
for various reinforcement learning methods, e.g., 1-step Qlearning, 1-step SARSA, and n-step Q-learning. They show
that asynchronous updates have a stabilizing effect on policy
and value updates. Also, the proposed method outperforms the
current state-of-the-art algorithms on the Atari games while
training for half of the time on a single multi-core CPU
instead of a GPU. As a result, some recent applications of
asynchronous DQL have been developed for handover control
problems in wireless systems 
5) Distributional Deep Q-learning:
All aforementioned
methods use the Bellman equation to approximate the expected
value of future rewards. However, if the environment is
stochastic in nature and the future rewards follow multimodal
distribution, choosing actions based on expected value may
not lead to the optimal outcome. For example, we know
that the expected transmission time of a packet in a wireless
network is 20 minutes. However, this information may not be
so meaningful because it may overestimate the transmission
time most of the time. For example, the expected transmission
time is calculated based on the normal transmissions (without
collisions) and the interference transmissions (with collisions).
Although the interference transmissions are very rare to happen, but it takes a lot of time. Then, the estimation about the
expected transmission is overestimated most of the time. This
makes estimations not useful for the DQL algorithms.
Thus, the authors in introduce a solution using distributional reinforcement learning to update Q-value function based
on its distribution rather than its expectation. In particular,
let Z(s, a) be the return obtained by starting from state s,
executing action a, and following the current policy, then
Q(s, a) = E[Z(s, a)]. Here, Z represents the distribution of
future rewards, which is no longer a scalar quantity like Qvalues. Then we obtain the distributional version of Bellman
equation as follows: Z(s, a) = r + γZ(s′, a′). Although the
proposed distributional deep Q-learning is demonstrated to
outperform the conventional DQL on many Atari 2600
Games (45 out of 57 games), its performance relies much
on the distribution function Z. If Z is well deﬁned, the
performance of distributional deep Q-learning is much more
signiﬁcant than that of the DQL. Otherwise, its performance
is even worse than that of the DQL.
Deep Q-learning with Noisy Nets: In , the authors
introduce Noisy Net, a type of neural network whose bias
and weights are iteratively perturbed during training by a
parametric function of the noise. This network basically adds
the Gaussian noise to the last (fully-connected) layers of the
network. The parameters of this noise can be adjusted by the
model during training, which allows the agent to decide when
and in what proportion it wants to introduce the uncertainty to
its weights. In particular, to implement the noisy network, we
ﬁrst replace the ϵ-greedy policy by a randomized action-value
function. Then, the fully connected layers of the value network
are parameterized as a noisy network, where the parameters
are drawn from the noisy network parameter distribution
after every replay step. For replay, the current noisy network
parameter sample is held ﬁxed across the batch. Since the
DQL takes one step of optimization for every action step, the
noisy network parameters are re-sampled before every action.
Through experimental results, the authors demonstrate that
by adding the Gaussian noise layer to the DNN, the performance of conventional DQL , dueling DQL , and
asynchronous DQL can be signiﬁcantly improved for a
wide range of Atari games. However, the impact of noise to
the performance of the deep DQL algorithms is still under
debating in the literature, and thus analysis on the impact of
noise layer requires further investigations.
7) Rainbow Deep Q-learning: In , the authors propose
a solution which integrates all advantages of seven aforementioned solutions (including DQL) into a single learning agent,
called Rainbow DQL. In particular, this algorithm ﬁrst deﬁnes
the loss function based on the asynchronous multi-step and
distributional DQL. Then, the authors combine the multi-step
distributional loss with double Q-learning by using the greedy
action in st+n selected according to the Q-network as the
bootstrap action a∗
t+n, and evaluate the action by using the
target network.
In standard proportional prioritized replay technique,
the absolute TD-error is used to prioritize the transitions.
Here, TD-error at a time slot is the error in the estimate
made at the time slot. However, in the proposed Rainbow
DQL algorithm, all distributional Rainbow variants prioritize
transitions by the Kullbeck-Leibler (KL) loss because this
loss may be more robust to noisy stochastic environment.
Alternatively, the dueling architecture of DNNs is presented
in . Finally, the Noisy Net layer is used to replace
all linear layers in order to reduce the number of independent
noise variables. Through simulation, the authors show that this
is the most advanced technique which outperforms almost all
current DQL algorithms in the literature over 57 Atari 2600
In Table III, we summarize the DQL algorithms and their
performance under the parameter settings used in . As
observed in Table III, all of the DQL algorithms have been
developed by Google DeepMind based on the original work
in . So far, through experimental results on Atari 2600
games, the Rainbow DQL presents very impressive results over
all other DQL algorithms. However, more experiments need
to be further conducted in different domains to conﬁrm the
real efﬁciency of the Rainbow DQL algorithm.
F. Deep Q-Learning for Extensions of MDPs
1) Deep Deterministic Policy Gradient Q-Learning for
Continuous Action: Although DQL algorithm can solve problems with high-dimensional state spaces, it can only handle
discrete and low-dimensional action spaces. However, systems
in many applications have continuous, i.e., real values, and
high dimensional action spaces. The DQL algorithms cannot
be straightforwardly applied to continuous actions since they
rely on choosing the best action that maximizes the Q-value
function. In particular, a full search in a continuous action
space to ﬁnd the optimal action is often infeasible.
In , the authors introduce a model-free off-policy actorcritic algorithm using deep function approximators that can
learn policies in high-dimensional, continuous action spaces.
The key idea is based on the deterministic policy gradient
(DPG) algorithm proposed in . In particular, the DPG
algorithm maintains a parameterized actor function µ(s; θµ)
with parameter vector θ which speciﬁes the current policy
by deterministically mapping states to a speciﬁc action. The
critic Q(s, a) is learned by using the Bellman equation as in
Q-learning. The actor is updated by applying the chain rule
to the expected return from the start distribution with respect
to the actor parameters.
Based on this update rule, the authors then introduce Deep
DPG (DDPG) algorithm which can learn competitive policies
by using low-dimensional observations, e.g. cartesian coordinates or joint angles, under the same hyper-parameters and
network structure. The algorithm makes a copy of the actor
and critic networks Q′(s, a; θQ′) and µ′(s; θµ′), respectively,
to calculate the target values. The weights of these target
networks are then updated with slowly tracking on the learned
networks, i.e., θ′ ←τθ + (1 −τ)θ′ with τ ≪1. This means
that the target values are constrained to change slowly, greatly
improving the stability of learning. Note that the major challenge of learning in continuous action spaces is exploration.
Therefore, in the proposed algorithm, the exploration policy µ′
is constructed by adding noise sampled from a noise process
N to the actor policy.
2) Deep Recurrent Q-Learning for POMDPs: To tackle
problems with partially observable environments by deep
reinforcement learning, the authors in propose a framework called Deep Recurrent Q-Learning (DRQN) in which an
LSTM layer was used to replace the ﬁrst post-convolutional
fully-connected layer of the conventional DQN. The recurrent
structure is able to integrate an arbitrarily long history to
better estimate the current state instead of utilizing a ﬁxedlength history as in DQNs. Thus, DRQNs estimate the function
Q(ot, ht−1; θ) instead of Q(st, at); θ), where θ denotes the
parameters of entire network, ht−1 denotes the output of the
LSTM layer at the previous step, i.e., ht = LSTM(ht−1, ot).
DRQN matches DQN’s performance on standard MDP problems and outperforms DQN in partially observable domains.
Regarding the training process, DRQN only considers the
convolutional features of the observation history instead of
explicitly incorporating the actions. Through the experiments,
the authors demonstrate that DRQN is capable of handling
partial observability, and recurrency confers beneﬁts when the
quality of observations changes during evaluation time.
3) Deep SARSA Learning: In , the authors introduce
a DQL technique based on SARSA learning to help the
agent determine optimal policies in an online fashion. In this
algorithm, given the current state s, the CNN is used to
obtain the current state-action value Q(s, a). Then, the current
TABLE III: Performance comparison among DQL algorithms by Google DeepMind.
algorithms
Utilize the
Suitable to
DNN to train
convergence
estimation
implement on
and easy to
on action values
MDPs with small
number of actions
Use 2 Q-value
Applicable
functions to
implement and
considering
to almost MDPs
simultaneously
specially features
select and evaluate
convergence
action values
Prioritized
Prioritize
Require information
Especially
experiences
convergence
about important
effective for MDPs
experiences in
with prioritized
replay memory
experiences
Using 2 DNNs to
Much faster
High complexity
Especially
simultaneously
and less efﬁcient
effective to deal
estimate the
on MDPs with
action and state
prioritized
small action
with large action
value functions
and state spaces
and state spaces
Asynchronous
Using multiple
Learning speed
High complexity
Especially
agents to train
is extremely
with intensive
effective for
the DNN in
faster by using
requirements on
MDPs with very
multiple agents
hardware devices
large state and
for training
for training
action spaces
Distributional
Use a distribution
Need to know
Suitable to
function (instead
accuracy in
the distribution
implement on
of expectation)
evaluating
of reward function
MDPs with available
over state and
distribution
Q-value function
action spaces
reward function
Noisy Nets
Adding the
Efﬁciency of
Especially
Gaussian noise
efﬁciency in
adding Gaussian
effective for
layer to the DNN
exploring the
noise layer is
MDPs with very
for training
environment
still debating
large state and
action spaces
Rainbow 
Inherit all
Extremely high
Only suitable for
features of all
advantages of
complexity with
MDPs with large
aforementioned
aforementioned
many requirements
state/action spaces
algorithms
algorithms
and some properties
in advance
known in advance
action a is selected by the ϵ-greedy algorithm. After that, the
immediate reward r and the next state s′ can be observed.
To estimate the current Q(s, a), the next state-action value
Q(s′, a′) is obtained. Here, when the next state s′ is used as
the input of the CNN, Q(s′, a′) can be obtained as the output.
Then, a label vector related to Q(s, a) is deﬁned as Q(s′, a′)
which represents the target vector. The two vectors only have
one different component, i.e., r + γQ(s′, a′) →Q(s, a). It
should be noted that during the training phase, the next action
a′ for estimating the current state-action value is never greedy.
On the contrary, there is a small probability that a random
action is chosen for exploration.
4) Deep Q-Learning for Markov Games:
In , the
authors introduce the general notion of sequential prisoner’s
dilemma (SPD) to model real world prisoner’s dilemma (PD)
problems. Since SPD is more complicated than PD, existing
approaches addressing learning in matrix PD games cannot be
directly applied in SPD. Thus, the authors propose a multiagent DRL approach for mutual cooperation in SDP games.
The deep multi-agent reinforcement learning towards mutual
cooperation consists of two phases, i.e., ofﬂine and online
phases. The ofﬂine phase generates policies with varying cooperation degrees. Since the number of policies with different
cooperation degrees is inﬁnite, it is computationally infeasible
to train all the policies from scratch. To address this issue,
the algorithm ﬁrst trains representative policies using actorcritic until it converges, i.e., cooperation and defection baseline
policy. Second, the algorithm synthesizes the full range of
policies from the above baseline policies. Another task is to
detect effectively the cooperation degree of the opponent. The
algorithm divides this task into two steps. First, the algorithm
trains an LSTM-based cooperation degree detection network
ofﬂine, which will be then used for real-time detection during
the online phase. In the online phase, the agent plays against
the opponents by reciprocating with a policy of a slightly
higher cooperation degree than that of the opponent. On one
hand, intuitively the algorithm is cooperation-oriented and
seeks for mutual cooperation whenever possible. On the other
hand, the algorithm is also robust against selﬁsh exploitation
and resorts to defection strategy to avoid being exploited
whenever necessary.
Unlike which considers a repeated normal form game
with complete information, in , the authors introduce
an application of DRL for extensive form games with imperfect information. In particular, the authors in introduce Neural Fictitious Self-Play (NFSP), a DRL method for
learning approximate Nash equilibria of imperfect-information
games. NFSP combines FSP with neural network function
approximation. An NFSP agent has two neural networks.
The ﬁrst network is trained by reinforcement learning from
memorized experience of play against fellow agents. This
network learns an approximate best response to the historical
behaviour of other agents. The second network is trained by
supervised learning from memorized experience of the agent’s
own behaviour. This network learns a model that averages
over the agent’s own historical strategies. The agent behaves
according to a mixture of its average strategy and best response
In the NSFP, all players of the game are controlled by
separate NFSP agents that learn from simultaneous play
against each other, i.e., self-play. An NFSP agent interacts
with its fellow agents and memorizes its experience of game
transitions and its own best response behaviour in two memories, MRL and MSL. NFSP treats these memories as two
distinct datasets suitable for DRL and supervised classiﬁcation,
respectively. The agent trains a neural network, Q(s, a; θQ),
to predict action values from data in MRL using off-policy
reinforcement learning. The resulting network deﬁnes the
agent’s approximate best response strategy, β = ϵ-greedy(Q),
which selects a random action with probability ϵ and otherwise
chooses the action that maximizes the predicted action values.
The agent trains a separate neural network Π(s, a; θΠ) to
imitate its own past best response behavior by using supervised
classiﬁcation on the data in MSL. NFSP also makes use of
two technical innovations in order to ensure the stability of the
resulting algorithm as well as to enable simultaneous self-play
learning. Through experimental results, the authors show that
the NFSP can converge to approximate Nash equilibria in a
small poker game.
Summary: In this section, we have presented the basics of
reinforcement learning, deep learning, and DQL. Furthermore,
we have discussed various advanced DQL techniques and their
extensions. Different DQL techniques can be used to solve
different problems in different network scenarios. In the next
sections, we review DQL related works for various problems
in communications and networking.
III. NETWORK ACCESS AND RATE CONTROL
Modern networks such as IoT become more decentralized
and ad-hoc in nature. In such networks, entities such as
sensors and mobile users need to make independent decisions,
e.g., channel and base station selections, to achieve their
own goals, e.g., throughput maximization. However, this is
challenging due to the dynamic and the uncertainty of network
status. Learning algorithms such as DQL allow to learn and
build knowledge about the networks that are used to enable
the network entities to make their optimal decisions. In this
section, we review the applications of DQL for the following
• Dynamic spectrum access: Dynamic spectrum access
allows users to locally select channels to maximize their
throughput. However, the users may not have full observations of the system, e.g., channel states. Thus, DQL
can be used as an effective tool for dynamic spectrum
Fig. 8: Channel selection based on DQL in IoT.
• Joint user association and spectrum access: User association is implemented to determine which user to be
assigned to which Base Station (BS). The joint user
association and spectrum access problems are studied in
 and . However, the problems are typically combinatorial and non-convex which require nearly complete
and accurate network information to obtain the optimal
strategy. DQL is able to provide distributed solutions
which can be effectively used for the problems.
• Adaptive rate control: This refers to bitrate/data rate control in dynamic and unpredictable environments such as
Dynamic Adaptive Streaming over HTTP (DASH). Such
a system allows clients or users to independently choose
video segments with different bitrates to download. The
client’s objective is to maximize its Quality of Experience
(QoE). DQL can be adopted to effectively solve the
problem instead of dynamic programming which has high
complexity and demands complete information.
A. Network Access
This section discusses how to use DQL to solve the spectrum access and user association in networks.
1) Dynamic Spectrum Access: The authors in propose
a dynamic channel access scheme of a sensor based on the
DQL for IoT. The model is shown in Fig. 8. At each time
slot, the sensor selects one of M channels for transmitting its
packet. The channel state is either in low interference, i.e.,
successful transmission, or in high interference, i.e., transmission failure. Since the sensor only knows the channel state
after selecting the channel, the sensor’s optimization decision
problem can be formulated as a POMDP. In particular, the
action of sensor is to select one of M channels. The sensor
receives a positive reward “+1” if the selected channel is in
low interference, and a negative reward “-1” otherwise. The
objective is to ﬁnd an optimal policy which maximizes the
sensor’s expected accumulated discounted reward over time
slots. In fact, the objective can be obtained by the myopic
policy . However, the myopic policy requires the prior
knowledge of the system transition matrix which is hard to
obtain. DQL allows the sensor to ﬁnd the optimal policy from
its experiences, and thus it can be adopted to solve the sensor’s
problem. In particular, the DQL uses a DQN1 with experience
replay . The input of the DQN is a state of the sensor
which is the combination of actions and observations, i.e., the
rewards, in the past time slots. The output includes Q-values
corresponding to the actions. To balance the exploration of
the current best Q-value with the exploration of the better
one, the ϵ-greedy policy is adopted for the action selection
mechanism. The simulation results based on the real data
1Remind that DQN is the core of the DQL algorithms.
(neighbor)
(neighbor)
Fig. 9: Joint channel selection and packet forwarding in IoT.
from show that the proposed scheme achieves an average
reward of 4.4 that is close to the myopic policy with an
average reward of 4.5. Note that the myopic policy requires
the knowledge of the system transition matrix.
 can be considered to be a pioneer work using the DQL
for the channel access. However, the DQL keeps following the
learned policy over time slots and stops learning a suitable
policy. Actual IoT environments are dynamic, and the DQN
in the DQL needs to be re-trained. An adaptive DQL scheme
is proposed in which evaluates the accumulated reward
of the current policy for every period. When the reward is
reduced by a given threshold, the DQN is re-trained to ﬁnd a
new good policy. The simulation results show that when
the states of the channels change, the adaptive DQL scheme
can detect the change and start re-learning to obtain the high
The models in and are constrained to only one
sensor. Consider a multi-sensor scenario, the authors in 
address the joint channel selection and packet forwarding
using the DQL. The model is shown in Fig. 9 in which
one sensor as a relay forwards packets received from its
neighboring sensors to the sink. The sensor is equipped with
a buffer to store the received packets. At each time slot, the
sensor selects a set of channels for the packet forwarding so
as to maximize its utility, i.e., the ratio of the number of
transmitted packets to the transmit power. Similar to , the
sensor’s problem can be formulated as an MDP. The action is
to select a set of channels, the number of packets transmitted
on the channels, and a modulation mode. To avoid packet loss,
the state is deﬁned as the combination of the buffer state and
channel state. The MDP is then solved by the DQL in which
the input is the state and the output is the action selection.
The DQL uses the stacked autoencoder to reduce the massive
calculation and storage in the Q-learning phase. The sensor’s
utility function is proved to be bounded which can guarantee
the convergence of the algorithm. The analysis shows that
the computational complexity of the proposed algorithm is
O(KM(J + 1)) that is lower than that of the strategy iteration algorithm with the computational complexity of
O(KM(J + 1))(L + 1)KCM), where K is the number of
buffers, L is the buffer length, M and C respectively are the
numbers of channels and channel states, and J is the number
of possible transmission modes. The simulation results show
that the proposed scheme signiﬁcantly improves the system
utility compared with the random action selection scheme. In
Base station
Fig. 10: Channel access in energy harvesting-enabled IoT systems.
particular, the average system utility of the proposed scheme
is 0.63, while that obtained by the random policy is 0.37.
However, as the packet arrival rate increases, the system utility
of the proposed scheme decreases since the sensor needs to
consume more power to transmit all packets.
Consuming more power leads to poor sensor’s performance
due to its energy constraint, i.e., a shorter IoT system lifetime.
The channel access problem in the energy harvesting-enabled
IoT system is investigated in . The model consists of one
BS and energy harvesting-based sensors (see Fig. 10). The
BS as a controller allocates channels to the sensors. However,
the uncertainty of ambient energy availability at the sensors
may make the channel allocation inefﬁcient. For example, the
channel allocated to the sensor with low available energy may
not be fully utilized since the sensor cannot communicate later.
Therefore, the BS’s problem is to predict the sensors’
battery states and select sensors for the channel access so as to
maximize the total rate. To solve the BS’s problem, the optimal
approaches such as the uplink resource allocation scheme 
can be adopted. However, the scheme requires the BS to have
perfect non-causal knowledge of all the random processes.
The perfect knowledge may not be available since the sensors
are distributed randomly over a geographical area. Thus, the
DQL is used to solve the problem of the BS, i.e., the agent.
The DQL uses a DQN consisting of two LSTM-based neural
network layers. The ﬁrst layer generates the predicted battery
states of sensors, and the second layer uses the predicted states
along with Channel State Information (CSI) to determine the
channel access policy. The state space consists of (i) channel
access scheduling history, (ii) the history of predicted battery
information, (iii) the history of the true battery information,
and (iv) the current CSI of the sensors. The action space
contains all sets of sensors to be selected for the channel
access, and the reward is the difference between the total rate
and the prediction error. As shown in the simulation results,
the proposed scheme is close to the optimal approach 
and outperforms the myopic policy in terms of total rate.
In particular, the total rates obtained by the proposed scheme,
the myopic policy, and the optimal scheme are 6.8, 6.5, and
7.0 kbps, respectively. Moreover, the battery prediction error
obtained from the proposed scheme is close to zero.
The above schemes, e.g., and , focus on the
rate maximization. In IoT systems such as Vehicle-to-Vehicle
(V2V) communications, latency also needs to be considered
due to the mobility of V2V transmitters/receivers and vital
applications in the trafﬁc safety. One of the problems of each
WiFi traffic
Fig. 11: Unlicensed channel access in LTE networks.
V2V transmitter is to select a channel and a transmit power
level to maximize its capacity under a latency constraint. Given
the decentralized network, a DQN is adopted to make optimal
decisions as proposed in . The model consists of V2V
transmitters, i.e., agents, which share a set of channels. The
actions of each V2V transmitter include choosing channels
and transmit power levels. The reward is a function of the
V2V transmitter’s capacity and latency. The state observed
by the V2V transmitter consists of (i) the instantaneous
CSI of the corresponding V2V link, (ii) the interference to
the V2V link in the previous time slot, (iii) the channels
selected by the V2V transmitter’ neighbors in the previous
time slot, and (iv) the remaining time to meet the latency
constraint. The state is also an input of the DQN. The output
includes Q-values corresponding to the actions. As shown in
the simulation results, by dynamically adjusting the power
and channel selection when V2V links are likely to violate
the latency constraint, the proposed scheme has more V2V
transmitters meeting the latency constraint compared with the
random channel allocation.
To reduce spectrum cost, the above IoT systems often use
unlicensed channels. However, this may cause the interference
to existing networks, e.g., WLANs. The authors in 
propose to use the DQN to jointly address the dynamic channel
access and interference management. The model consists of
Small Base Stations (SBSs) which share unlicensed channels
in an LTE network (see Fig. 11). At each time slot, the SBS
selects one of channels for transmitting its packet. However,
there may be WLAN trafﬁcs on the selected channel, and
thus the SBS accesses the selected channel with a probability.
The actions of the SBS include pairs of channel selection
and channel access probability. The problem of the SBS is
to determine an action vector so as to maximize its total
throughput, i.e., its utility, over all channels and time slots.
The resource allocation problem can be formulated as a noncooperative game, and the DQN using LSTM can be adopted
to solve the game. The input of the DQN is the history trafﬁc of
the SBSs and the WLAN on the channels. The output includes
predicted action vectors of the SBSs. The utility function of
each SBS is proved to be convex, and thus the DQN-based
algorithm converges to a Nash equilibrium of the game. The
analysis shows that the computational complexity per time step
of the proposed scheme is O(n2
c + ncni + ncn0 + nc), where
nc, ni, and no are the numbers of memory cells, input units,
and output units, respectively. The simulation results based
on real trafﬁc data from show that the proposed scheme
can improve the average throughput up to 28% compared with
the standard Q-learning. Moreover, deploying more SBSs in
the LTE network does not allow more airtime fraction for
the network. This implies that the proposed scheme can avoid
causing performance degradation to the WLAN. However, the
proposed scheme requires synchronization between the SBSs
and the WLAN which is challenging in real networks.
In the same cellular network context, the authors in 
address the dynamic spectrum access problem for multiple
users sharing K channels. At a time slot, the user selects a
channel with a certain attempt probability or chooses not to
transmit at all. The state is the history of the user’s actions
and its local observations, and the user’s strategy is mapping
from the history to an attempt probability. The problem of the
user is to ﬁnd a vector of the strategies, i.e., the policy, over
time slots to maximize its expected accumulated discounted
data rate of the user.
The above problem is solved by training a DQN. The
input of the DQN includes past actions and the corresponding
observations. The output includes estimated Q-values of the
actions. To avoid the overestimation in the Q-learning, the
DDQN is used. Moreover, the dueling DQN is
employed to improve the estimated Q-value. The DQN is
then ofﬂine trained at a base station. Similar to , the
multichannel random access is modeled as a non-cooperative
game. As proved in , the game has a subgame perfect
equilibrium. Note that some users can keep increasing their
attempt probability to increase their rates. This makes the
equilibrium point inefﬁcient, and thus the strategy space of
the users is restricted to avoid the situation. The simulation
results show that the proposed scheme can achieve twice the
channel throughput compared with the slotted-Aloha . The
reason is that in the proposed scheme, each user only learns
from its local observation without an online coordination or
carrier sensing. However, the proposed scheme requires the
central unit which may raise the message exchanges as the
training is frequently updated.
In the aforementioned models, the number of users is ﬁxed
in all time slots, and the arrival of new users is not considered.
The authors in address the channel allocation to new
arrival users in a multibeam satellite system. The multibeam
satellite system generates a geographical footprint subdivided
into multiple beams which provide services to ground User
Terminals (UTs). The system has a set of channels. If there
exist available channels, the system allocates a channel to the
new arrived UT, i.e., the new service is satisﬁed. Otherwise, the
service is blocked. The system’s problem is to ﬁnd a channel
allocation decision to minimize the total service blocking
probability of the new UT over time slots without causing
the interference to the current UTs.
The system’s problem can be viewed as a temporal correlated sequential decision-making optimization problem which
is effectively solved by the DQN. Here, the satellite system
is the agent. The action is an index indicating which channel
is allocated to the new arrived UT. The reward is positive
when the new service is satisﬁed and is negative when the
service is blocked. The state includes the set of current UTs,
the current channel allocation matrix, and the new arrived UT.
Note that the state has the spatial correlation feature due to
Macro base
Femto base
Fig. 12: Joint user association and channel selection based on DQL
in HetNets.
the co-channel interference, and thus it can be represented
in an image-like fashion, i.e., an image tensor. Therefore,
the DQN adopts the CNN to extract useful features of the
state. The simulation results show that the proposed DQN
algorithm converges after a certain number of training steps.
Also, by allocating available channels to the new arrived UTs,
the proposed scheme can improve the system trafﬁc up to
24.4% compared with the ﬁxed channel allocation scheme.
However, as the number of current UTs increases, the number
of available channels is low or even zero. Therefore, the
dynamic channel allocation decisions of the proposed scheme
become meaningless, and the performance difference between
the two schemes becomes insigniﬁcant. For the future work,
a joint channel and power allocation algorithm based on the
DQL can be investigated.
2) Joint User Association and Spectrum Access: Joint user
association and spectrum access problems are typically nonconvex. To solve the problems, traditional approaches such as
linear programming are developed to obtain the optimal
solution. However, the approaches require nearly complete
and accurate network information that is usually not available.
Learning techniques such as Q-learning can be used. However,
it is challenging to obtain an optimal solution due to the large
state and action spaces of the joint optimization problems.
By combining DNN with Q-learning, DQL is effectively used
to solve the joint optimization problems as proposed in 
The authors in consider a HetNet which consists of
multiple users and BSs including macro base stations and
femto base stations (see Fig. 12). The BSs share a set of
orthogonal channels, and the users are randomly located in the
network. The problem of each user is to select one BS and a
channel to maximize its data rate while guaranteeing that the
Signal-to-Interference-plus-Noise Ratio (SINR) of the user is
higher than a minimum Qualtiy of Service (QoS) requirement.
The DQL is adopted to solve the problem in which each user
is an agent, and its state is a vector including QoS states of
all users, i.e., the global state. Here, the QoS state of the
user refers to whether its SINR exceeds the minimum QoS
requirement or not. At each time slot, the user takes an action.
If the QoS is satisﬁed, the user receives utility as its immediate
reward. Otherwise, it receives a negative reward, i.e., an action
selection cost. Note that the cumulative reward of one user
depends on actions of other users, then the user’s problem can
be deﬁned as an MDP. Similar to , the DDQN and the
dueling DQN are used to learn the optimal policy, i.e., the
joint BS and channel selections, for the user to maximize its
cumulative reward. The simulation results from show that
the proposed scheme outperforms the Q-learning implemented
in in terms of convergence speed and system capacity. The
simulation comparisons imply that the DQN can be effectively
used to solve the complex problems, e.g., joint optimization
problems, in the large-scale systems such as HetNets and IoT.
The scheme proposed in is considered to be the
ﬁrst work using the DQL for the joint user association and
spectrum access problem. Inspired by this work, the authors
in propose to use the DQL for a joint user association,
spectrum access, and content caching problem. The network
model is an LTE network which consists of UAVs serving
ground users. The UAVs are equipped with storage units and
can act as cached-enabled LTE-BSs. The UAVs are able to
access both licensed and unlicensed bands in the network.
The UAVs are controlled by a cloud-based server, and the
transmissions from the cloud to the UAVs are implemented by
using the licensed cellular band. The problem of each UAV is
to determine (i) its optimal user association, (ii) the bandwidth
allocation indicators on the licensed band, (iii) the time slot
indicators on the unlicensed band, and (iv) a set of popular
contents that the users can request to maximize the number
of users with stable queue, i.e., users satisﬁed with content
transmission delay.
The UAV’s problem is combinatorial and non-convex, and
the DQL can be used to solve it. The UAVs do not know
the users’ content requests, and thus the Liquid State Machine
approach (LSM) is adopted to predict the content request
distribution of the users and to perform resource allocation.
In particular, predicting the content request distribution is
implemented at the cloud based on an LSM-based prediction
algorithm. Then, given the request distributions, each UAV as
an agent uses an LSM-based learning algorithm to ﬁnd its
optimal users association. Speciﬁcally, the input of the LSMbased learning algorithm consists of actions, i.e., UAV-user
association schemes, that other UAVs take, and the output
includes the expected numbers of users with stable queues
corresponding to actions that the UAV can take. After the user
association is done, the optimal content caching is determined
based on the results of [62, Theorem 2], and the optimal spectrum allocation is done by using linear programming. Based
on the Gordon’s Theorem , the proposed DQL is proved to
converge with probability one. The simulation results using the
content request data from show that the proposed DQL
can converge within 400 iterations. Compared with the Qlearning, the proposed DQN improves the convergence time up
to 33%. Moreover, the proposed DQL signiﬁcantly improves
the number of users with stable queues up to 50% compared
with the Q-learning without cache. In fact, energy efﬁciency is
also important for the UAVs, and thus applying the DQL for a
joint user association, spectrum access, and power allocation
problem needs to be investigated.
(Video segment & bitrate)
(Video segment)
Content delivery
Fig. 13: A dynamic adaptive streaming system based on HTTP
B. Adaptive Rate Control
Dynamic Adaptive Streaming over HTTP (DASH) becomes
the dominant standard for video streaming . DASH is able
to leverage existing content delivery network infrastructure
and is compatible with a multitude of client-side applications.
A general DASH system is shown in Fig. 13 in which the
videos are stored in servers as multiple segments, i.e., chunks.
Each segment is encoded at different compression levels to
generate representations with different bitrates, i.e., different
video visual quality. At each time slot, the client chooses
a representation, i.e., a segment with a certain bitrate, to
download. The client’s problem is to ﬁnd an optimal policy
which maximizes its QoE such as maximizing average bitrate
and minimizing rebuffering, i.e., the time which the video
playout freezes.
As presented in , the above problem can be modeled
as an MDP in which the agent is the client and the action
is choosing a representation to download. To maximize the
QoE, the reward is deﬁned as a function of (i) visual quality
of the video, (ii) video quality stability, (iii) rebuffering event,
and (iv) buffer state. Given the reward formulation, the state
of the client should include (i) the video quality of the last
downloaded segment, (ii) the current buffer state, (iii) the
rebuffering time, and (iv) the channel capacities experienced
during downloading of segments in the past time slots. The
MDP can be solved by using dynamic programming, but the
computational complexity rapidly becomes unmanageable as
the size of the problem increases. Thus, the authors in 
adopt the DQL to solve the problem. Similar to , the
LSTM networks are used in which the input is the state of
the client, and the output includes Q-values corresponding to
the client’s possible actions. To improve the performance of
the standard LSTM, peephole connections are added into the
LSTM networks. The simulation results based on the dataset
from show that the proposed DQL algorithm can converge
much faster than Q-learning. In particular, the DQL algorithm
converges in around 3 episodes, while the Q-learning converges in around 180 episodes. The fast convergence suggests
that DQL can provide efﬁcient solution to problems in realtime applications. Moreover, the proposed DQL improves the
video quality and reduces the rebuffering since it is able to
dynamically manage the buffer by considering the buffer state
and channel capacity.
The network model and the optimization problem in 
are also found in . However, different from , the
authors in adopt the Asynchronous Advantage Actor-
Critic (A3C) method for the DQL to further enhance
and speed up the training. As presented in Section II-F1,
A3C includes two neural networks, namely, actor network and
critic network. The actor network is to choose bitrates for the
client, and the critic network helps train the actor network.
For the actor network, the input is the client’s state, and the
output is a policy, i.e., a probability distribution over possible
actions given states that the client can take. Here, the action is
choosing the next representation, i.e., the next segment with a
certain bitrate, to download. For the critic network, the input
is the client’s state, and the output is the expected total reward
when following the policy obtained from the actor network.
The simulation results based on the mobile dataset from 
show that the proposed DQL can improve the average QoE up
to 25% compared with the bitrate control scheme . Also,
by having sufﬁcient buffer to handle the network’s throughput
ﬂuctuations, the proposed DQL reduces the rebuffering around
32.8% compared with the baseline scheme.
In practice, the DQL algorithm proposed in can be
easily deployed in a multi-client network since A3C is able
to support parallel training for multiple agents. Accordingly,
each client, i.e., an agent, is conﬁgured to observe its reward.
Then, the client sends a tuple including its state, action, and
reward to a server. The server uses the actor-critic algorithm
to update its actor network model. The server then pushes the
newest model to the agent. This update process can happen
asynchronously among all agents which improves quality and
speeds up the training. Although the parallel training scheme
may incur a Round-Trip Time (RTT) between the clients and
the server, the simulation results in show that the RTT
between the clients and the server reduces the average QoE by
only 3.5%. The performance degradation is small, and thus the
proposed DQL can be implemented in real network systems.
In and , the input of the DQL, i.e., the client’s
state, includes the video quality of the last downloaded video
segment. The video segment is raw which may cause “state
explosion” to the state space . To reduce the state space
and to improve the QoE, the authors in propose to use
a video quality prediction network. The prediction network
extracts useful features from the raw video segments using
CNN and RNN. Then, the output of the prediction network,
i.e., the predicted video quality, is used as one of the inputs of
the DQL which is proposed in . Simulation results based
on the broadband dataset from show that the proposed
DQL can improve the average QoE up to 25% compared with
the Google Hangout, i.e., a communication platform developed
by Google. Moreover, the proposed DQL can reduce the
average latency of video transmission around 45% due to the
small state space. This means that in the scenarios that the
state space is large, the CNN should be used to improve the
user QoE and the convergence time.
Apart from the DASH systems, the DQL can be effectively used for the rate control in High Volume Flexible
Time (HVFT) applications. HVFT applications use cellular
networks to deliver IoT trafﬁc as shown in Fig. 14. The HVFT
applications have a large volume of trafﬁc, and the trafﬁc
scheduling, e.g., data rate control, in the HVFT applications is
necessary. One common approach is to assign static priority
Base station
Fig. 14: Data rate control based on DQL in HVFT applications.
classes per trafﬁc type, and then trafﬁc scheduling is based
on its priority class. However, such an approach does not
evolve to accommodate new trafﬁc classes. Thus, learning
methods such as DQL should be used to provide adaptive
rate control mechanisms as proposed in . The network
model is a single cell including one BS as a central controller
and multiple mobile users. The problem at the BS is to ﬁnd
a proper policy, i.e., data rate for the users, to maximize
the amount of transmitted HVFT trafﬁc while minimizing
performance degradation to existing data trafﬁcs. It is shown
in that the problem can be formulated as an MDP. The
agent is the BS, and the state includes the current network
state and the useful features extracted from network states in
the past time slots. The network state at a time slot includes
(i) the congestion metric, i.e., the cell’s trafﬁc load, at the
time slot, (ii) the total number of network connections, and
(iii) the cell efﬁciency, i.e., the cell quality. The action that
the BS takes is a combination of the trafﬁc rate for the
users. To achieve the BS’ objective, the reward is deﬁned as
a function of (i) the sum of HVFT trafﬁc, (ii) trafﬁc loss
to existing applications due to the presence of the HVFT
trafﬁc, and (iii) the amount of bytes served below desired
minimum throughput. The DQL using the actor and critic
networks with LSTM is then adopted. By using the real
network data collected in Melbourne, the simulation results
show that the proposed DQL scheme increases the HVFT
trafﬁc up to 2 times compared with the heuristic control
scheme. The proposed DQL is thus expected to be applied in
modern networks in large-scale cities with a large population
In the aforementioned approaches, the maximum number of
objectives is constrained, e.g., to 3 in . The authors in 
show that the DQL can be used for the rate control to achieve
multiple objectives in complex communication systems. The
network model is a future space communication system which
is expected to operate in unpredictable environments, e.g.,
orbital dynamics, atmospheric and space weather, and dynamic channels. In the system, the transmitter needs to be
conﬁgured with several transmit parameters, e.g., symbol rate
and encoding rate, to achieve multiple conﬂict objectives, e.g.,
low Bit Error Rate (BER), throughput improvement, power
and spectral efﬁciency. The adaptive coding and modulation
schemes, i.e., , can be used. However, the methods allow to
achieve only limited number objectives. Learning algorithms
such as the DQL can be thus used. The agent is the transmitter
in the system. The action is a combination of (i) symbol rate,
(ii) energy per symbol, (iii) modulation mode, (iv) number
of bits per symbol, and (v) encoding rate. The objective is to
maximize the system performance. Thus, the reward is deﬁned
as a ﬁtness function of performance parameters including (i)
BER estimated at the receiver, (ii) throughput, (iii) spectral
efﬁciency, (iv) power consumption, and (v) transmit power
efﬁciency. The state is the system performance measured by
the transmitter, and thus the state is the reward. To achieve
multiple objectives, the DQL is implemented by using a set of
multiple neural networks in parallel. The input of the DQL is
the current state and the channel conditions, and the output is
the predicted action. The neural networks are trained by using
the Levenberg-Marquardt backpropagation algorithm . The
simulation results show that the proposed DQL can achieve the
ﬁtness score, i.e., the weighted sum of different objectives,
close to the ideal, i.e., the exhaustive search approach. This
implies that the DQL is able to select near-optimal actions
and learn the relationship between rewards and actions given
dynamic channel conditions.
Summary: This section reviews applications of DQL for
the dynamic network access and adaptive rate control. The
reviewed approaches are summarized along with the references
in Table IV. We observe that the problems are mostly modeled
as an MDP. Moreover, DQL approaches for the IoT and DASH
systems receive more attentions than other networks. Future
networks, e.g., 5G networks, involve multiple network entities
with multiple conﬂicting objectives, e.g., provider’s revenue
versus users’ utility maximization. This poses a number of
challenges to the traditional resource management mechanisms
that deserve in-depth investigation. In the next section, we
review the adoption of DQL for the emerging services, i.e.,
ofﬂoading and caching.
IV. CACHING AND OFFLOADING
As one of the key features of information-centric networking, in-network caching can efﬁciently reduce duplicated
content transmissions. The studies on wireless caching has
shown that access delays, energy consumption, and the total
amount of trafﬁc can be reduced signiﬁcantly by caching
contents in wireless devices. Big data analytics also
demonstrate that with limited cache size, proactive caching
at network edge nodes can achieve 100% user satisfaction
while ofﬂoading 98% of the backhaul trafﬁc. Joint content
caching and ofﬂoading can address the gap between the
mobile users’ large data demands and the limited capacities
in data storage and processing. This motivates the study on
Mobile Edge Computing (MEC). By deploying both computational resources and caching capabilities close to end
users, MEC signiﬁcantly improves energy efﬁciency and QoS
for applications that require intensive computations and low
latency. A uniﬁed study on caching, ofﬂoading, networking,
and transmission control in MEC scenarios involves very complicated system analysis because of strong couplings among
mobile users with heterogeneities in application demand, QoS
TABLE IV: A summary of approaches using DQL for network access and adaptive rate control.
ALGORITHMS
Network access
Past channel selections and
observations
Channel selection
Score +1 or -1
Current buffer state and
channel state
Channel, packets,
and modulation
mode selection
Ratio of number of
transmitted packets
to transmit power
Base station
Channel access history,
predicted and true battery
information history, and
current CSI
Sensor selection
for channel access
Total rate and
prediction error
transmitter
Current CSI, past
interference, past channel
selections, and remaining time
to meet the latency constraints
Channel and
transmit power
Capacity and latency
Small base
Trafﬁc history of small base
stations and the WLAN
Channel selection
and channel access
probability
Throughput
dueling DQN
Mobile user
Past channel selections and
observations
Channel selection
Current user terminals,
channel allocation matrix, and
the new arrival user
Channel selection
Score +1 or -1
dueling DQN
Mobile user
QoS states
Base station and
channel selection
Content request distribution
Base station
Users with stable
Rate control
connections
Last segment quality, current
buffer state, rebuffering time,
and channel capacities
Bitrate selection
for segment
Video quality,
rebuffering even, and
buffer state
Last segment quality, current
buffer state, rebuffering time,
and channel capacities
Bitrate selection
for segment
Video quality,
rebuffering even, and
buffer state
CNN and RNN
Predicted video quality,
current buffer state,
rebuffering time, and channel
capacities
Bitrate selection
for segment
Video quality,
rebuffering even, and
buffer state
Base station
Congestion metric, current
network connections, and cell
Trafﬁc rate
decisions for
mobile users
HVFT trafﬁc, trafﬁc
loss to existing
applications, and the
amount of served
application
Base station
Measurements of BER,
throughput, spectral
efﬁciency, power
consumption, and transmit
power efﬁciency
Symbol rate,
energy per symbol,
modulation mode,
number of bits per
symbol, and
encoding rate
Same as the state
Space communication
provisioning, mobility pattern, radio access interface, and
wireless resources. A learning-based and model-free approach
becomes a promising candidate to manage huge state space
and optimization variables, especially by using DNNs. In this
section, we review the modeling and optimization of caching
and ofﬂoading policies in wireless networks by leveraging the
DRL framework.
A. Wireless Proactive Caching
Wireless proactive caching has attracted great attentions
from both academia and industry. Statistically, a few popular contents are usually requested by many users during a
short time span, which accounts for most of the trafﬁc load.
Therefore, proactively caching popular contents can avoid the
heavy trafﬁc burden of the backhaul links. In particular, this
technique aims at pre-caching the contents from the remote
content servers at the edge devices or BSs that are close to
the end users. If the requested contents are already cached
locally, the BS can directly serve the end users with small
delay. Otherwise, the BS requests these contents from the
original content server and updates the local cache based on
the caching policy, which is one of the main design problem
for wireless proactive caching.
1) QoS-Aware Caching:
Content popularity is the key
factor used to solve the content caching problem. With a
large number of contents and their time-varying popularities,
DQL is an attractive strategy to tackle this problem with
high-dimensional state and action spaces. The authors in 
present a DQL scheme to improve the caching performance.
The system model consists of a single BS with a ﬁxed cache
size. For each request, the BS as an agent makes a decision
on whether or not to store the currently requested content in
the cache. If the new content is kept, the BS determines which
local content will be replaced. The state is the feature space of
the cached contents and the currently requested content. The
feature space consists of the total number of requests for each
content in a speciﬁc short-, medium-, and long-term. There
are two types of actions: (i) to ﬁnd a pair of contents and
exchange the cache states of the two contents and (ii) to keep
the cache states of the contents unchanged. The aim of the BS
is to maximize the long-term cache hit rate, i.e., reward.
The DQL scheme in trains the policy by using
the DDPG method and employs Wolpertinger architecture to reduce the size of the action space and avoid missing an optimal policy. The Wolpertinger architecture consists
of three main parts: an actor network, K-Nearest Neighbors
(K-NN), and a critic network. The actor network is to avoid a
large action space. The critic network is to correct the decision
made by the actor network. The DDPG method is applied
to update both critic and actor networks. K-NN can help to
explore a set of actions to avoid poor decisions. The actor and
critic networks are then implemented by using FNNs. The
simulation results show that the proposed DQL scheme outperforms the ﬁrst-in ﬁrst-out scheme in terms of long-term
cache hit rate. In particular, the cache hit rate obtained by the
DQL scheme is 0.5, while that obtained by the ﬁrst-in ﬁrst-out
scheme is 0.4. The performance comparisons demonstrate that
the proposed DQL scheme can achieve competitive cache hit
rates while effectively reducing the run-time. This makes the
proposed framework efﬁcient and suitable for handling largescale data.
Maximizing the long-term cache hit rate in implies
that the cache stores the most popular contents. In a dynamic
environment, contents stored in a cache have to be replaced
according to the users’ dynamic requests. An optimization of
the placement or replacement of cached contents is studied
in by a deep learning method. The optimization algorithm
is trained by a DNN in advance and then used for realtime caching or scheduling with minimum delay. The authors
in propose an optimal caching policy to learn the cache
expiration times, i.e., Time-To-Live (TTL), for dynamically
changing requests in content delivery networks. The system
includes a cloud database server and multiple mobile devices
that can issue queries and update entries in a single database.
The query results can be cached for a speciﬁed time interval
at server-controlled caches. All cached queries will become
invalid if one of the cached records has been updated. A large
TTL will strain cache capacities while a small TTL increases
latencies signiﬁcantly if the database server is physically
Unlike the DDPG approach used in , the authors in 
propose to utilize Normalized Advantage Functions (NAFs)
for continuous DQL scheme to learn optimal cache expiration
duration. The key problem in continuous DQL is to select an
action maximizing the Q-function, while avoiding performing
a costly numerical optimization at each step. The use of NAFs
obviates a second actor network that needs to be trained separately. Instead, a single neural network is used to output both
a value function and an advantage term. The DQL agent at the
cloud database uses an encoding of a query itself and the query
miss rates as the system states, which allows for an easier
generalization. The system reward is linearly proportional to
the current load, i.e., the number of cached queries divided
by the total capacity. This reward function can encourage
longer TTLs when fewer queries are cached, and shorter TTLs
when the load is close to the system capacity. Considering
incomplete measurements for rewards and next-states at runtime, the authors introduce the Delayed Experience Injection
(DEI) approach that allows the DQL agent to keep track of
incomplete transitions when measurements are not immediately available. The authors evaluate the learning algorithm
by Yahoo! cloud serving benchmark with customized web
workloads . The simulation results verify that the learning
approach based on NAFs and DEI outperforms a statistical
estimator.
2) Joint Caching and Transmission Control: The caching
policies determine where to store and retrieve the requested
content efﬁciently, e.g., by learning the contents’ popularities and cache expiration time . Another important aspect of caching design is the transmission control of
the content delivery from caches to end users, especially
for wireless systems with dynamic channel conditions. To
avoid mutual interference in multi-user wireless networks,
the transmission control decides which cached contents can
be transmitted concurrently as well as the most appropriate
control parameters, e.g., transmit power, precoding, data rate,
and channel allocation. Hence, the joint design of caching
and transmission control is required to enable efﬁcient content
delivery in multi-user wireless networks.
Recently, some approaches, e.g., , are proposed for the
joint caching and interference alignment in wireless systems.
However, most of the approaches assume that the channel
state information is invariant that may not be the case in
dynamic wireless systems. The authors in – propose
to use the DQL framework to address the joint caching
and interference alignment to tackle mutual interference in
multi-user wireless networks. The authors consider an MIMO
system with limited backhaul capacity and the caches at the
transmitter. The precoding design for interference alignment
requires the global CSI at each transmitter. A central scheduler
is responsible for collecting CSI and cache status from each
user via the backhaul, scheduling the users’ transmission,
and optimizing the resource allocation. By enabling content
caching at individual transmitters, we can decrease the demand
for data transfer and thus save more backhaul capacity for realtime CSI update and sharing. Using the DQL-based approach
at the central scheduler can reduce the explicit demand for
CSI and the computational complexity in matrix optimization,
especially with time-varying channel conditions. The DQL
agent implements the DNN to approximate the Q-function
with experience replay in training. To make the learning
process more stable, the target Q-network parameter is updated
by the Q-network for every a few time instants. The collected
information is assembled into a system state and sent to the
DQL agent, which feeds back an optimal action for the current
time instant. The action indicates which users to be active,
and the resource allocation among active users. The system
reward represents the total throughput of multiple users. An
extended work of and with a similar DQL framework
is presented in , in which a CNN-based DQN is adopted
and evaluated in a more practical conditions with imperfect
or delayed CSI. Simulation results show that the performance
of the MIMO system is signiﬁcantly improved compared with
the baseline scheme in terms of the total throughput and
energy efﬁciency. In particular, at SNR = 15 dB, the total rate
obtained by the proposed DQN scheme is 240 Mbps, while
that obtained by the baseline scheme is 200 Mbps.
Interference management is an important requirement of
wireless systems. The application-related QoS or user experience is also an essential metric. Different from – , the
authors in propose a DQL approach to maximize Quality
of Experience (QoE) of IoT devices by jointly optimizing
the cache allocation and transmission rate in content-centric
wireless networks. The system state is speciﬁed by the nodes’
caching conditions, e.g., the service information and cached
contents, as well as the transmission rates of the cached
contents. The aim of the DQL agent is to minimize continuously the network cost or maximize the QoE. The proposed
DQL framework is further enhanced with the use of PER and
DDQN. PER replays important transitions more frequently so
that DQN can learn from samples more efﬁciently. The use
of DDQN can stabilize the learning by providing two value
functions in separated neural networks. This avoids an overestimation of the DQN with the increasing number of actions.
These two neural networks are not completely decoupled as
the target network is a periodic copy of estimation network. A
discrete simulator ccnSim is used to model the caching
behavior in various graph structures. The output data trace of
the simulator is then imported to Matlab and used to evaluate
the learning algorithm. As shown in the simulation results,
the proposed DQL framework can achieve a QoE value of 4
that is double to that of the standard penetration test scheme.
Moreover, the computational complexity of the DDQN is
O(lg n) that is lower than that of the standard penetration test
scheme with the computational complexity of O(lsn3), where
n is the number of content-centric computing nodes, s is the
number of service nodes, and l is the number of transmission
rate values.
The QoE can be used to characterize the users’ perception of Virtual Reality (VR) services. The authors in 
address the joint content caching and transmission strategy
in a wireless VR network, where UAVs capture videos on live
games and transmit them to small-cell BSs servicing the VR
users. Millimeter wave (mmWave) downlink backhaul links
are used for VR content transmission from the UAVs to BSs.
The BSs can also cache the popular contents that may be
requested frequently by end users. The joint content caching
and transmission problem is formulated as an optimization to
maximize the users’ reliability, i.e., the probability that the
content transmission delay satisﬁes the instantaneous delay
target. The maximization involves the control of transmission
format, users’ association, the set and format of cached contents. A DQL framework combining the Liquid State Machine
(LSM) and Echo State Network (ESN) is proposed for each
BS to ﬁnd the optimal transmission and caching strategies. As
a randomly generated spiking neural network , LSM can
store information about the network environment over time
and adjust the users’ association policy, cached contents and
formats according to the users’ content requests. It has been
used in to predict the users’ content request distribution
while having only limited information regarding the network
and different users. Conventional LSM uses FNNs as the
output function, which demands high complexity in training
due to the computation of gradients for all of the neurons.
Conversely, the proposed DQL framework uses an ESN as the
output function, which uses historical information to ﬁnd the
relationship between the users’ reliability, caching, and content
transmission. It also has a lower complexity in training and
a better memory for network information. Simulation results
show that the proposed DQL framework can yield 25.4%
gain in terms of users’ reliability compared to the baseline
Q-learning.
3) Joint Caching, Networking, and Computation: Caching
and transmission control will become more involved in a Het-
Net that integrates different communication technologies, e.g.,
cellular system, device-to-device network, vehicular network,
and networked UAVs, to support various application demands.
The network heterogeneity raises the problem of complicated
system design that needs to address challenging issues such
as mutual interference, differentiated QoS provisioning, and
resource allocation, hopefully in a uniﬁed framework. Obviously this demands a joint optimization far beyond the extent
of joint caching and transmission control.
Accordingly, the authors in propose a DQL framework
for energy-efﬁcient resource allocation in green wireless networks, jointly considering the couplings among networking,
in-network caching and computation. The system consists
of a Software-Deﬁned Network (SDN) with multiple virtual
networks and mobile users requesting for video on-demand
ﬁles that require a certain amount of computational resource
at either the content server or at local devices. In each virtual
network, an authorized user issues a request to download ﬁles
from a set of available SBSs in its neighborhood area. The
wireless channels between each mobile user and the SBSs
are characterized as Finite-State Markov Channels (FSMC).
The states are the available cache capacity at the SBSs,
the channel conditions between mobile users and SBSs, the
computational capability of the content servers and mobile
users. The DQL agent at each SBS decides an association
between each mobile user and SBS, where to perform the
computational task, and how to schedule the transmissions
of SBSs to deliver the required data. The objective is to
minimize the total energy consumption of the system from data
caching, wireless transmission, and computation. Simulation
results show that the total energy consumption in different
testing scenarios is very high at the beginning of the learning
process and gradually decreases a stable value when the
learning converges. Moreover, the energy consumption of the
uniﬁed DRL framework considering caching, networking, and
computing is signiﬁcantly lower than those of other DRL
frameworks that only focus on part of the control variables.
The DQL scheme proposed in has been applied to
improve the performance of Vehicular Ad doc NETworks
(VANETs) in – . The network model includes multiple
BSs, Road Side Units (RSUs), MEC servers, and content
servers. All devices are controlled by a mobile virtual network
operator. The vehicles request for video contents that can be
cached at the BSs or retrieved from remote content servers.
The authors in formulate the resource allocation problem
as a joint optimization of caching, networking, and computing, e.g., compressing and encoding operations of the video
contents. The system states include the CSI from each BS, the
computational capability, and cache size of each MEC/content
server. The network operator feeds the system state to the
FNN-based DQN and gets the optimal policy that determines
the resource allocation for each vehicle. To exploit spatial
correlations in learning, the authors in enhance Q-learning
by using CNNs in DQN. This makes it possible to extract
high-level features from raw input data. Two schemes have
been introduced in to improve stability and performance
of the ordinary DQN method. Firstly, DDQN is designed to
avoid over-estimation of Q-value in ordinary DQN. Hence, the
action can be decoupled from the target Q-value generation.
This makes the training process faster and more reliable.
Secondly, the dueling DQN approach is also integrated in the
design with the intuition that it is not always necessary to
estimate the reward by taking some action. The state-action Qvalue in dueling DQN is decomposed into one value function
representing the reward in the current state, and the advantage
function that measures the relative importance of a certain
action compared with other actions. Simulation results show
that the proposed DQL scheme outperforms the existing static
scheme in terms of total utility. In particular, the total utility
obtained by the DQL scheme is 8000, while that obtained by
the existing static scheme is 5000.
Considering the huge action space and high complexity with
the vehicle’s mobility and service delay deadline Td, a multitime scale DQN framework is proposed in to minimize
the system cost by the joint design of communication, caching
and computing in VANET. The policy design accounts for
limited storage capacities and computational resources at the
vehicles and the RSUs. The small timescale DQN is for every
time slot and aims to maximize the exact immediate reward.
Additionally, the large timescale DQN is designed for every
Td time slots within the service delay deadline, and used to
estimate the reward considering the vehicle’s mobility in a
large timescale. Simulation results show that the proposed
framework can reduce the cost up to 30% compared with the
random resource allocation scheme.
The aforementioned DQL framework for VANETs, e.g.,
 – , has also been generalized to smart city applications in , which necessitates dynamic orchestration of
networking, caching, and computation to meet different servicing requirements. Through Network Function Virtualization
(NFV) , the physical wireless network in smart cities
can be divided logically into several virtual ones by the
network operator, which is responsible for network slicing
and resource scheduling, as well as allocation of caching
and computing capacities. The use cases in smart cities are
presented in , , which apply the generalized DQL
framework to improve the security and efﬁciency for trustbased data exchange, sharing, and delivery in mobile social
networks through the resource allocation and optimization
of MEC allocation, caching, and D2D (Device-to-Device)
Cellular BS
Small cell BS
Joint design of caching, networking,
and transmission control strategies
Fig. 15: Joint caching, networking, and transmission control to
optimize cache hit rate , cache expiration time , interference alignment – , Quality of Experience , , energy
efﬁciency , resource allocation – , trafﬁc latency, or redundancy , .
networking.
B. Data and Computation Ofﬂoading
With limited computation, memory and power supplies,
IoT devices such as sensors, wearable devices, and handheld
devices become the bottleneck to support advanced applications such as interactive online gaming and face recognition.
To address such a challenge, IoT devices can ofﬂoad the
computational tasks to nearby MEC servers, integrated with
the BSs, Access Points (APs), and even neighboring Mobile
Users (MUs). As a result, data and computation ofﬂoading can
potentially reduce the processing delay, save the battery energy, and even enhance security for computation-intensive IoT
applications. However, the critical problem in the computation
ofﬂoading is to determine the ofﬂoading rate, i.e., the amount
of computational workload, and choose the MEC server from
all available servers. If the chosen MEC server experiences
heavy workloads and degraded channel conditions, it may
take even longer time for the IoT devices to ofﬂoad data
and receive the results from the MEC server. Hence, the
design of an ofﬂoading policy has to take into account the
time-varying channel conditions, user mobility, energy supply,
computation workload and the computational capabilities of
different MEC servers. Some optimal ofﬂoading approaches,
e.g., the dynamic programming based ofﬂoading algorithm
and heuristic ofﬂoading algorithm , are proposed. However, the approaches assume that the mobility patterns of
the mobile users are given in advance. Without knowing the
mobility pattern in advance, the DQL can be used for each
mobile user to learn the optimal ofﬂoading policy from past
experiences such as the approaches proposed in and
 . The authors in focus on minimizing the mobile
user’s cost and energy consumption by ofﬂoading cellular
trafﬁc to WLAN. Each mobile user can either access the
cellular network, or the complimentary WLAN as illustrated
in Fig. 16(a), but with different monetary costs. The mobile
user also has to pay a penalty if the data transmission does not
ﬁnish before the deadline. The mobile user’s data ofﬂoading
decision can be modeled as an MDP. The system state includes
the mobile user’s location and the remaining ﬁle size of all
data ﬂows. The mobile user will choose to transmit data
through either WLAN or cellular network, and decide how
to allocate channel capacities to concurrent ﬂows. CNNs are
employed in the DQL to predict a continuous value of the
mobile user’s remaining data. Simulation results reveal that
the DQN-based scheme generally outperforms the dynamic
programming algorithm for the MDP in terms of cost and
energy consumption. In particular, the DQL-based scheme can
reduce the energy consumption up to 500 Joules compared
with that of the dynamic programming algorithm. The reason
is that the DQN can learn from experience while the dynamic
programming algorithm cannot obtain the optimal policy with
incorrect transition probability.
The allocation of limited computational resources at the
MEC server is critical for cost and energy minimization. The
authors in consider an MEC-enabled cellular system, in
which multiple mobile users can ofﬂoad their computational
tasks via wireless channels to one MEC server, co-located with
the cellular BS as shown in Fig. 16(b). Each mobile user has
a computational-intensive task, characterized by the required
computational resources, CPU cycles, and the maximum tolerable delay. The capacity of the MEC server is limited to
accommodate all mobile users’ task loads. The bandwidth
sharing between different mobile users’ ofﬂoading also affects
the overall delay performance and energy consumptions. The
DQL is used to minimize the cost of delay and power
consumptions for all mobile users, by jointly optimizing the
ofﬂoading decision and computational resource allocation. The
system states include the sum of cost of the entire system
and the available computational capacity of the MEC server.
The action of BS is to determine the resource allocation and
ofﬂoading decision for each mobile user. To limit the size of
action space, a pre-classiﬁcation step is proposed to check the
mobile users’ feasible set of actions. Simulation results show
that the proposed scheme can reduce the sum cost up to 55%
compared with the static allocation strategies.
In contrast to , multiple BSs in an ultra-dense network
is considered in and , as shown in Fig. 16(c),
with the objective of minimizing the long-term cost of delay in computation ofﬂoading. All computational tasks are
ofﬂoaded to the shared MEC server via different BSs. Besides
the allocation of computational resources and transmission
control, the ofﬂoading policy also has to optimize the association between mobile users and the BSs. With dynamic
network conditions, the mobile users’ decision-making can
be formulated as an MDP. The system states are the channel
conditions between the mobile user and the BSs, the states
of energy and task queues. The cost function is deﬁned
as a weighted sum of the execution delay, the handover
delay and the computational task dropping cost. The authors
in ﬁrstly propose a DDQN-based DQL algorithm to learn
the optimal ofﬂoading policy without knowing the network
dynamics. By leveraging the additive structure of the utility
function, the Q-function decomposition combined with the
DDQN further leads to a novel online SARSA-based DRL
algorithm. Numerical experiments show that the new algorithm
achieves a signiﬁcant improvement in computation ofﬂoading
performance compared with the baseline policies, e.g., the
DQN-based DQL algorithm and some heuristic ofﬂoading
strategies without learning. The high density of SBSs can
relieve the data ofﬂoading pressure in peak trafﬁc hours but
consume a large amount of energy in off-peak time. Therefore,
the authors in , , and propose a DQL-based
strategy for controlling the (de)activation of different SBSs
to minimize the energy consumption without compromising
the quality of provisioning. In particular, in , the on/off
decision framework uses a DQL scheme to approximate both
the policy and value functions in an actor-critic method.
The reward of the DQL agent is deﬁned as a cost function
relating to energy consumption, QoS degradation, and the
switching cost of SBSs. The DDPG approach is also employed
together with an action reﬁnement scheme to expedite the
training process. Through extensive numerical simulations, the
proposed scheme is shown to greatly outperform other baseline
methods in terms of both energy and computational efﬁciency.
With a similar model to that in , computation of-
ﬂoading ﬁnds a proper application for cloud-based malware
detection in . A review of the threat models and the RLbased solutions for security and privacy protection in mobile
ofﬂoading and caching are discussed in . With limited
energy supply, computational resources, and channel capacity,
mobile users cannot always update the local malware database
and process all application data in time and thus are vulnerable
to zero-day attacks . By leveraging the remote MEC
server, all mobile users can ofﬂoad their application data
and detection tasks via different BSs to the MEC/security
server with larger and more sophisticated malware database,
more computational capabilities, and powerful security services. This can be modeled by a dynamic malware detection
game in which multiple mobile users interact with each
other in resource competition, e.g., the allocation of wireless
channel capacities and the computational capabilities of the
MEC/security server. A DQL scheme is proposed for each mobile user to learn its ofﬂoading data rate to the MEC/security
server. The system states include the channel state and the size
of application traces. The objective is to optimize the detection
accuracy of the security server, which is deﬁned as a concave
function in the total amount of malware samples. The Q-value
is estimated by using a CNN in the DQL framework. The
authors also propose the hotbooting Q-learning technique that
provides a better initialization for Q-learning by exploiting
the ofﬂoading experiences in similar scenarios. It can save
exploration time at the initial stage and accelerate the learning
speed compared with a standard Q-learning algorithm with
all-zero initialization of the Q-value . The proposed
DQL scheme not only improves the detection speed and
accuracy, but also increases the mobile users’ battery life. The
simulation results reveal that compared with the hotbooting
Q-learning and standard Q-learning schemes, the DQL-based
malware detection has faster learning rate, higher accuracy,
and lower detection delay. For example, the detection delay
of the proposed DQL scheme reduces by 24.6% and 35.3%
at time slot 2000, respectively, compared with those of the
hotbooting Q-learning and the standard Q-learning schemes.
Multiple MEC servers have been considered in , ,
as illustrated in Fig. 16(d). The authors in aim to
design optimal ofﬂoading policy for IoT devices with energy
harvesting capabilities. The system consists of multiple MEC
servers, such as BSs and APs, with different capabilities
in computation and communications. The IoT devices are
equipped with energy storage and energy harvesters. They
can execute computational tasks locally and ofﬂoad the tasks
to the MEC servers. The IoT device’s ofﬂoading decision
can be formulated as an MDP. The system states include the
battery status, the channel capacity, and the predicted amount
of harvested energy in the future. The IoT device evaluates
the reward based on the overall delay, energy consumption,
the task drop loss and the data sharing gains in each time
slot. Similar to , the authors in enhance Q-learning
by the hotbooting technique to save the random exploration
time at the beginning of learning. The authors also propose a
fast DQL ofﬂoading scheme that uses hotbooting to initialize
the CNN and accelerates the learning speed.
The authors
in view the MEC-enabled BSs as different physical
machines constituting a part of the cloud resources. The cloud
optimizes the MUs’ computation ofﬂoading to different virtual
machines residing on the physical machines. A two-layered
DQL algorithm is proposed for the ofﬂoading problem to
maximize the utilization of cloud resources. The system state
relates to the waiting time of each computational task and the
number of virtual machines. The ﬁrst layer is implemented by
a CNN-based DQL framework to estimate an optimal cluster
for each computational task. Different clusters of physical
machines are generated based on the K-NN algorithm. The
second layer determines the optimal serving physical machine
within the cluster by Q-learning method.
The aforementioned works all focus on data or computation
ofﬂoading in cellular system via BSs to remote MEC servers,
e.g., – , , , . In and ,
the authors study QoS-aware computation ofﬂoading in an
ad-hoc mobile network. By making a certain payment, the
mobile user can ofﬂoad its computational tasks to nearby
mobile users constituting a mobile cloudlet, as shown in
Fig. 16(d). Each mobile user has a ﬁrst-in-ﬁrst-out queue with
limited buffer size to store the arriving tasks arriving as a
Poisson process. The mobile user selects nearby cloudlets
within D2D communication range for task ofﬂoading. The
ofﬂoading decision depends on the states including the number
of remaining tasks, the quality of the links between mobile
users and the cloudlet, and the availability of the cloudlet’s
resources. The objective is to maximize a composite utility
function, subject to the mobile user’s QoS requirements, e.g.,
energy consumption and processing delay. The utility function
is ﬁrstly an increasing function of the total number of tasks
that have been processed either locally or remotely by the
cloudlets. It is also related to the user’s beneﬁt such as energy
efﬁciency and payment for task ofﬂoading. This problem is
formulated as an MDP and solved by linear programming
and Q-learning approaches, depending on the availability of
information about the state transition probabilities. This work
Cellular BS
MEC Server
MEC Server
Cellular BS
Mobile user
Data/Energy
Fig. 16: Data/computation ofﬂoading models in cellular networks:
(a) Ofﬂoading cellular trafﬁc to WLAN , (b) Ofﬂoading to a
single MEC-enabled BS , (c) Ofﬂoading to one shared MEC
server via multiple BSs , , , (d) Ofﬂoading to multiple
MEC-enabled BSs , and mobile cloudlets , .
is further enhanced by leveraging DNN or DQN to learn the
decision strategy more efﬁciently. A similar model is studied
in , where the computation ofﬂoading is formulated as
an MDP to minimize the cost of computation ofﬂoading. The
solution to the MDP can be used to train a DNN by supervised
learning. The well-trained DNN is then applied to unseen
network conditions for real-time decision-making. Simulation
results show that the use of deep supervised learning achieves
signiﬁcant performance gain in ofﬂoading accuracy and cost
Data and computation ofﬂoading is also used in fog computing. The mobile application demanding a set of data and
computational resources can be hosted in a container, e.g.,
virtual machine of a fog node. With user’s mobility, the
container has to be migrated or ofﬂoaded to other nodes and
dynamically consolidated. With the container migration, some
nodes with low resource utilization can be switched off to
reduce power consumption. The authors in model the
container migration as a multi-dimensional MDP, which is
solved by the DQL. The system states consist of the delay,
the power consumption and the migration cost. The action
includes the selection policy that selects the containers to be
emigrated from each source node, and the allocation policy
that determines the destination node of each container. The
action space can be optimized for more efﬁcient exploration by
dividing fog nodes into under-utilization, normal-utilization,
and over-utilization groups. By powering off under-utilization
nodes, all their containers will be migrated to other nodes
to reduce power consumption. The training process is also
optimized by using DDQN and PER which assigns different
priorities to the transitions in experience memory. This helps
the DQL agent at each fog node to perform better in terms of
faster learning speed and more stability. The analysis presents
that the proposed scheme can be executed in polynomial
time. Simulation results reveal that the DQL scheme achieves
fast decision-making and outperforms the existing baseline
approaches signiﬁcantly in terms of delay, power consumption,
and migration cost.
Summary: This section reviews the applications of the
DQL for wireless caching and data/computation ofﬂoading,
which are inherently coupled with networking and allocation
of channel capacity, computational resources, and caching
capabilities, etc. We observe that the DQL framework for
caching is typically centralized and mostly implemented at
the network controller, e.g., the BS, service provider, and
central scheduler, which is more powerful in information collection and cross-layer policy design. On the contrary, the end
users have more control over their ofﬂoading decisions, and
hence we observe more popular implementation of the DQL
agent at local devices, e.g., mobile users, IoT devices, and
fog nodes. Though an orchestration of networking, caching,
data and computation ofﬂoading in one uniﬁed DQL framework is promising for network performance maximization,
we face many challenges in designing highly-stable and fastconvergent learning algorithms, due to excessive delay and
unsynchronized information collection from different network
V. NETWORK SECURITY AND CONNECTIVITY
PRESERVATION
Future networks become more decentralized and ad-hoc in
nature which are vulnerable to various attacks such as Denialof-Service (DoS) and cyber-physical attack. Recently, the DQL
has been used as an effective solution to avoid and prevent the
attacks. In this section, we review the applications of DQL in
addressing the following security issues:
• Jamming attack: In the jamming attack, attackers as
jammers transmit Radio Frequency (RF) jamming signals
with high power to cause interference to the legitimate
communication channels, thus reducing the SINR at
legitimate receivers. Anti-jamming techniques such as the
frequency hopping and user mobility, i.e., moving
out from the heavy jamming area, have been commonly
used. However, without being aware of the radio channel
model and the jamming methods, it is challenging for
the users to choose an appropriate frequency channel as
well as to determine how to leave and avoid the attack.
DQL enables the users to learn an optimal policy based
on their past observations, and thus DQL can be used to
address the above challenge.
• Cyber-physical attack: The cyber-physical attack is an
integrity attack in which an attacker manipulates data
to alter control signals in the system. This attack often happens in autonomous systems such as Intelligent
Transportation Systems (ITSs) and increases the risk
of accidents to Autonomous Vehicles (AVs). The DQL
allows the AVs to learn optimal actions based on their
time-varying observations of the attacker’ activities. Thus,
Fig. 17: Jamming attack in cognitive radio network .
the DQL can be used to achieve robust and dynamic
control of the AV to the attacks.
• Connectivity preserving: This refers to maintaining the
connectivity among the robots, e.g., UAVs, to support
the communication and exchange of information among
them. The system and network environment is generally
dynamic and complex, and thus the DQL which allows
each robot to make dynamic decisions based on its state
can be effectively used to preserve the connectivity in the
A. Network Security
This section discusses the applications of DQL to address
the jamming attack and the cyber-physical attack.
1) Jamming Attack: A pioneer work using the DQL for
the anti-jamming is . The network model is a Cognitive
Radio Network (CRN) as shown in Fig. 17 which consists of
one Secondary User (SU), multiple Primary Users (PUs), and
multiple jammers. The network has a set of frequency channels
for hopping. At each time slot, each jammer can arbitrarily
select one of the channels to send its jamming signal, and
the SU, i.e., the agent, needs to choose a proper action based
on the SU’s current state. The action is (i) selecting one of
the channels to send its signals or (ii) leaving the area to
connect to another BS. The jammers are assumed to avoid
causing interference to the PUs, and thus the SU’s current
state consists of the number of PUs and the discretized SINR
of the SU signal at the last time slot. The objective of the SU
is to maximize its expected discounted utility over time slots.
Note that when the SU chooses to leave the area to connect
to another BS, it spends a mobility cost. Thus, the utility is
deﬁned as a function of the SINR of the SU signal and the
mobility cost. Since the number of frequency channels may
be large that results in a large action set, the CNN is used for
the DQL to quickly learn the optimal policy. As shown in the
simulation results, the proposed DQL has a faster convergence
speed than that of the Q-learning algorithm. In particular, the
utility of the SU increases from 2.73 at the beginning to 3.39
at time slot 1000 that is 8.3% higher than that of the Qlearning algorithm. Moreover, considering the scenario with
two jammers, the proposed DQL outperforms the frequencyhopping method in terms of the SINR and the mobility cost.
The model in is constrained to two jammers. As the
number of jammers in the network increases, the proposed
TABLE V: A summary of approaches using DQL for caching and ofﬂoading.
ALGORITHMS
Wireless proactive caching
actor-critic,
Base station
Cached contents and
requested content
Replace selected
content or not
Cache hit rate
(score 1 or 0)
Base station
Channel states and
computational capabilities
User association,
computational
unit, content
Energy consumption
Encoding of a query, query
cache miss rate
Cache expiration
Cache hit rates, CDN
utilization
Channel coefﬁcients, cache
Active users and
resource allocation
Network throughput
Channel coefﬁcients, cache
Active users and
resource allocation
Network throughput
Conditions of cache nodes,
transmission rates of content
The content
chunks to cache
and to remove
Network cost, QoE
centric IoT
LSM and ESN
Base station
Historical content request
User association,
cached contents
and formats
Reliability
Available BS, MEC, and
User association,
caching, and
Composite revenue
Available BS, MEC, and
User association,
caching, and
Composite revenue
dueling DQN
Available BS, MEC, and
User association,
caching, and
Composite revenue
Base station
Channel state, computational
capability, content/version
indicator, and the trust value
User association,
caching, and
Data and computation ofﬂoading
Mobile user
User’s location and remaining
Idle, transmit via
WLAN or cellular
Total data rate
Base station
Sum of cost and
computational capacity of the
MEC server
decision and
resource allocation
Sum of cost of delay
and energy
consumption
Mobile user
Channel qualities, states of
energy and task queues
Ofﬂoading and
resource allocation
Long term cost
Mobile user
Channel qualities, states of
energy and task queues
decision and
computational
resource allocation
Long term cost
hotbooting
Q-learning
Mobile user
Channel states, size of App
Ofﬂoading rate
Utility related to
detection accuracy,
response speed, and
the transmission cost
Delay, container’s location
and resource allocation
Container’s next
Composite utility
related to delay,
power consumption,
and migration cost
scheme may not be effective. The reason is that it becomes
hard for the SU to ﬁnd good actions when the number
of jammed channels increases. An appropriate solution, as
proposed in , allows the receiver of the SU to leave its
current location. Since the leaving incurs the mobility cost, the
receiver, i.e., the agent, needs an optimal policy, i.e., staying
at or leaving the current location, to maximize its utility. In
this scenario, the DQL based on CNN can be used for the
receiver to ﬁnd the optimal action to maximize its expected
utility. Here, the utility and state of the receiver are essentially
deﬁned similarly to that of the agent in . In particular,
the state includes the discretized SINR of the signal measured
by the receiver at the last time slot. Simulation results show
that the proposed DQL converges to SINR and utility values
that are higher than those obtained by the Q-learning and
random schemes. In particular, the SINR value obtained by the
proposed DQL is 3.4, while those obtained by the Q-learning
and random schemes are 3.3 and 2.8, respectively.
The above approaches, i.e., in and , deﬁne states
of the agents based on raw SINR values of the signals. In
practical wireless environments, the number of SINR values
may be large and even inﬁnite. Moreover, the raw SINR can be
inaccurate and noisy. To cope with the challenge of the inﬁnite
number of states, the DQL can use a recursive Convolutional
Neural Network (RCNN) as proposed in . By using
the pre-processing layer and recursive convolution layers, the
RCNN is able to remove noise from the network environment
Serving BS
Server (receiver)
Compromised
Fig. 18: Anti-jamming scheme based on UAV .
and extract useful features of the SINR, i.e., discrete spectrum
sample values greater than a noise threshold, thus reducing
the computational complexity. The network model and the
problem formulation considered in are similar to those
in . However, instead of directly using the raw SINR, the
state of the SU is the extracted features of the SINR. Also,
the action of the SU includes only frequency-hopping decision.
The simulation results show that the proposed DQL based on
the RCNN can converge in both ﬁxed and dynamic jamming
scenarios while the Q-learning cannot converge in the dynamic
jamming one. Furthermore, the proposed DQL can achieve
the average throughput close to that of the optimal scheme,
i.e., an anti-jamming scheme with completely known jamming
Instead of ﬁnding the frequency-hopping decisions, the
authors in propose the use of DQL to ﬁnd an optimal
power control policy for the anti-jamming. The model is an
IoT network including IoT devices and one jammer. The
jammer can observe the communications of the transmitter and
chooses a jamming strategy to reduce the SINR at the receiver.
Thus, the transmitter chooses an action, i.e., transmit power
level, to maximize its utility. Here, the utility is the difference
between the SINR and the energy consumption cost due to the
transmission. Note that choosing the transmit power impacts
the future jamming strategy, and thus the interaction between
the transmitter and the jammer can be formulated as an MDP.
The transmitter is the agent, and the state is SINR measured
at its receiver at the last time slot. The DQN using the CNN
is then adopted to ﬁnd an optimal power control policy for the
transmitter to maximize its expected accumulated discounted
reward, i.e., the utility, over time slots. The simulation results
show that the proposed DQL can improve the utility of the
transmitter up to 17.7% compared with the Q-learning. Also,
the proposed DQL reduces the utility of the jammer around
18.1% compared with the Q-learning. Moreover, the proposed
DQL has faster convergence speed than that of the Q-learning.
Speciﬁcally, the proposed DQL converges at time slot 210,
while the Q-learning converges at time slot 240.
To prevent the jammer’s observations of communications,
the transmitter can change its communication strategy, e.g., by
using relays that are far from the jamming area. The relays can
be UAVs as proposed in . The model consists of one UAV,
i.e., a relay, one jammer, one mobile user and its serving BS
(see Fig. 18). The mobile user transmits messages to its server
via the serving BS. In the case that the serving BS is heavily
jammed, the UAV helps the mobile user to relay the messages
to the server through a backup BS. In particular, depending
on the SINR and Bit Error Rate (BER) values sent from the
serving BS, the UAV as an agent decides the relay power level
to maximize its utility, i.e., the difference between the SINR
and the relay cost. The relay power level can be considered to
be the UAV’s actions, and the SINR and BER are its states. As
such, the next state observed by the UAV is independent of all
the past states and actions. The problem is formulated as an
MDP. To quickly achieve the optimal relay policy for the UAV,
the DQL based on CNN is then adopted. The simulation results
in show that the proposed DQL scheme takes only 200
time slots to converge to the optimal policy, which is 83.3%
less than that of the relay scheme based on Q-learning .
Moreover, the proposed DQL scheme reduces the BER of the
user by 46.6% compared with the hill climbing-based UAV
relay scheme .
The scheme proposed in assumes that the relay UAV is
sufﬁciently far from the jamming area. However, as illustrated
in Fig. 18, the attacker can use a compromised UAV close
to the relay UAV to launch the jamming attack to the relay
UAV. In such a scenario, the authors in show that the
DQL can still be used to address the attack. The system model
is based on physical layer security and consists of one UAV
and one attacker. The attacker is assumed to be “smarter” than
that in the model in . This means that the attacker can
observe channels that the UAV uses to communicate with the
BS in the past time slots and then chooses jamming power
levels on the target channels. Therefore, the UAV needs to
ﬁnd a power allocation policy, i.e., transmit power levels on
the channels, to maximize the secrecy capacity of the UAV-
BS communication. Similar to , the DQL based on CNN
is used which enables the UAV to choose its actions, i.e.,
transmit power levels on the channels, based on its state, i.e.,
the attacker’s jamming power level in the last time slot. The
reward is the difference between the secrecy capacity of the
UAV and BS and the energy consumption cost.
The simulation results in show that the proposed DQL
can improve the UAV’s utility up to 13% compared with the
baseline scheme which uses the Win or Learn Faster-
Policy Hill Climbing (WoLF-PHC) to prevent the attack. Also,
the safe rate of the UAV, i.e., the probability that the UAV is
attacked, obtained by the proposed DQL is 7% higher than
that of the baseline. However, the proposed DQL has higher
computational complexity and takes longer time to make a
decision in each time epoch compared with the WoLH-PHC.
Thus, the proposed DQL is applied only to a single-UAV
system. For the future work, scenarios with multiple UAVs
need to be considered. In such a scenario, more computational
overhead is expected and multi-agent DQL algorithms can be
Faulty data
Safe spacing
Fig. 19: Car-following model with cyber-physical attack.
2) Cyber-Physical Attack: In autonomous systems such as
ITSs, the attacker can seek to inject faulty data to information
transmitted from the sensors to the AVs. The AVs which
receive the injected information may inaccurately estimate
the safe spacing among them. This increases the risk of
AV accidents. Vehicular communication security algorithms,
e.g., , can be used to minimize the spacing deviation.
However, the attacker’s actions in these algorithms are assumed to be stable which may not be applicable in practical
systems. The DQL that enables the AVs to learn optimal actions based on their time-varying observations of the attacker’
actions can be thus used.
The ﬁrst work using the DQL for the cyber-physical attack
in an ITS can be found in . The system is a carfollowing model of the General Motors as shown in
Fig. 19. In the model, each AV updates its speed based
on measurement information received from the closest road
smart sensors. The attacker attempts to inject faulty data to
the measurement information. However, the attacker cannot
inject the measurements of different sensors equally due to
its resource constraint. Thus, the AV can choose less-faulty
measurements by selecting a vector of measurement weights.
The objective of the attacker is to maximize the deviation, i.e.,
the utility, from the safe spacing between the AV and its nearby
AV while that of the AV is to minimize the deviation. The
interaction between the attacker and the AV can be modeled
as a zero-sum game. The authors in show that the DQL
can be used to ﬁnd the equilibrium strategies. In particular,
the action of the AV is to choose a weight vector. Its state
includes the past actions, i.e., the weight vectors, and the
past deviation values. Since the actions and deviations have
continuous values, the state space is inﬁnite. Thus, LSTM
units that are able to extract useful features are adopted for the
DQL to reduce the state space. The simulation results show
that by using the past actions and deviations for learning the
attacker’s action, the proposed DQL scheme can guarantee
a lower steady-state deviation than the Kalmar ﬁlter-based
scheme . Moreover, by using the LSTM units, the results
show that the proposed DQL scheme can converge much faster
than the baseline scheme.
Another work that uses the LSTM to extract useful features
from the measurement information to detect the cyber-physical
attack is proposed in . The model is an IoT system
including a cloud and a set of IoT devices. The IoT devices
Probs. of attacking IoT devices
Actions on
IoT devices
Fig. 20: Cyber-physical detection in IoT systems using DQL.
generate signals and transmit the signals to the cloud (see
Fig. 20). The cloud uses the received signals for estimation and
control of the IoT devices’ operation. An attacker can launch
the cyber-physical attack by manipulating the IoT devices’
output signals that causes control errors at the cloud and
degrades the performance of the IoT system. To detect the
attack, the cloud uses LSTM units to extract stochastic features
or ﬁngerprints such as ﬂatness, skewness, and kurtosis, of the
IoT devices’ signals. The cloud sends the ﬁngerprints back to
the IoT devices, and the IoT devices embed, i.e., watermark,
the ﬁngerprints inside the signals. The cloud uses the ﬁngerprints to authenticate the IoT devices’ signals to detect the
attack. The computational complexity of the proposed signal
authentication method is O(df i
s), where d is the delay that the
cloud authenticates any IoT signal, and f i
s is the sampling rate
of IoT device i.
The algorithm proposed in is also called dynamic
watermarking which is able to detect the cyber-physical
attack and to prevent eavesdropping attacks. However, the
algorithm requires large computational resources at the cloud
for the IoT device signal authentication. Consequently, the
cloud can only authenticate a limited number of vulnerable
IoT devices. The cloud can choose the vulnerable IoT devices
by observing their security status. However, this can be impractical since the IoT devices may not report their security
status. Thus, the authors in propose to use the DQL that
enables the cloud to decide which IoT devices to authenticate
with the incomplete information. Since IoT devices with more
valuable data are likely to be attacked, the reward is deﬁned
as a function of data values of IoT devices. The cloud’s state
includes attack actions of the attacker on the IoT devices in
the past time slots. The actions of the attacker on the IoT
devices can be obtained by using the dynamic watermarking
algorithm in (see Fig. 20). The DQL then uses an LSTM
unit to ﬁnd the optimal policy. The input of the LSTM unit
is the state of the cloud, and the output includes probabilities
of attacking the IoT devices. By using a real dataset from the
accelerometers, the simulation results show that the proposed
DQL can improve the cloud’s utility up to 30% compared with
the case in which the cloud chooses the IoT devices with equal
probability.
Base station
Connectivity
Connectivity
Base station
Fig. 21: Connectivity preservation of a multi-UAV network.
B. Connectivity Preservation
Multi-robot systems such as multi-UAV cooperative networks have been widely applied in many ﬁelds such as
military, e.g., enemy detecting. In the cooperative multi-robot
system, the connectivity among the robots, e.g., UAVs in
Fig 21, is required to enable the communication and exchange
of information. To tackle the connectivity preservation problem, the Artiﬁcial Potential Field (APF) algorithm is
used. However, the algorithm cannot be directly adopted when
the robots are undertaking missions in dynamic and complex
environments. The DQL which allows each robot to make
dynamic decisions based on its own state can be effectively
applied to preserve the connectivity in the multirobot system.
Such an approach is proposed in .
The model in consists of two robots or UAVs, i.e., one
leader robot and one follower robot. In the model, a central
control, i.e., a ground BS, adjusts the velocity of the follower
such that the follower stays in the communication range of the
leader at all time (see Fig 21). The connectivity preservation
problem can be thus formulated as an MDP. The agent is the
BS, and the states are the relative position and the velocity
of the leader with respect to the follower. The action space
consists of possible velocity values of the follower. Taking
an action returns a reward which is +1 if the follower is
in the range of the leader, and -1 otherwise. A DQN using
FNN is used which enables the BS to learn an optimal policy
to maximize the expected discounted cumulative reward. The
input of the DQN includes the states of the two robots, and
the output is the action space of the follower. The simulation
results show that for different locations of the leader and the
follower, the score obtained by the proposed scheme is always
100, while the score of the APF method can be occasionally
less than 100. This means that the proposed scheme achieves
better connectivity between the two robots than that of the
APF method. However, a general scenario with more than one
leader and one follower needs to be investigated.
Considering the general scenario, the authors in 
address the connectivity preservation between multiple leaders and multiple followers. The robot system is deﬁnitely
connected if any two robots are connected via a direct link
or multi-hop link. To express the connectivity in such a
robot system, the authors introduce the concept of algebraic
connectivity which is the second smallest eigenvalue
of a Laplacian matrix. The robot system is connected if the
algebraic connectivity of the system is positive. Thus, the
problem is to adjust the velocity of the followers such that the
algebraic connectivity is positive over time slots. This problem
can be formulated as an MDP in which the agent is the ground
BS, the state is a combination of the states of all robots, the
action is a set of possible velocity values for the followers.
The reward is +1 if the algebraic connectivity of the system
increases or holds, and becomes a penalty of -1 if the algebraic
connectivity decreases. Similar to , a DQN is adopted.
Due to the large action space of the followers, the actor-critic
neural network is used. The simulation results show that
the followers always follow the motion of the leaders even if
the leaders’ trajectory dynamically changes. This demonstrates
the capability of DQN to tackle the connectivity preservation
problem for multi-robot systems. However, the proposed DQN
requires more time to converge than that in because of
the presence of more followers.
The proposed schemes in and do not consider
a minimum distance between the leaders and followers. The
leaders and followers can collide with each other if the
distance between them is too short. Thus, the BS needs to
guarantee the minimum distance between them. One solution
is to have the minimum distance in the reward as proposed
in . In particular, if the leader is too close to its follower,
the reward of the system is penalized regarding the minimum
distance. The DQL algorithm proposed in is then used
such that the BS learns proper actions, e.g., turning left and
right, to maximize the cumulative reward.
When BSs are densely deployed, the UAVs or mobile
users need to trigger a frequent handover to preserve the
connectivity. The frequent handover increases communication
overhead and energy consumption of the mobile users, and
interrupts data ﬂows. Thus, it is essential to maintain an
appropriate handover rate. The authors in address the
handover decision problem in an ultra-density network. The
network model consists of multiple mobile users, SBSs, and
one central controller. At each time slot, the user needs to
decide its serving SBS. The handover decision process can
be modeled as an MDP, and the DQL is adopted to ﬁnd an
optimal handover policy for each user to minimize the number
of handover occurrences while ensuring certain throughput.
The state of the user, i.e., the agent, includes reference signal
quality received from candidate SBSs and the last action of
the user. The reward is deﬁned as the difference between
the data rate of the user and its energy consumption for
the handover process. Given a high density of users, the
DQL using A3C and LSTM is adopted to ﬁnd the optimal
policy in short training time. The simulation results show
that the proposed DQL can achieve higher throughput and
lower handover rate than those of the upper conﬁdence bandit
algorithm with similar training time. Speciﬁcally, the
throughput and the handover rate of the DQL are 0.7 bit/s/Hz
and 0.0003, respectively, while those of the upper conﬁdence
bandit algorithm are 0.67 bit/s/Hz and 0.00049, respectively.
To enhance the reliability of the communication between
the SBSs and the mobile users, the SBSs should be able
to handle network faults and failure automatically as selfhealing. The DQL can be applied as proposed in to make
optimal parameter adjustments based on the observation of the
network performance. The model is the 5G network including
one MBS. The MBS as an agent needs to handle network
faults such as transmit diversity faults and antenna azimuth
change, e.g., because of wind. These faults are represented as
the MBS’s state that is the number of active alarms. Based on
the alarms, the MBS can take actions including (i) enabling
the transmit diversity and (ii) setting the antenna azimuth
to default value. The reward that the MBS receives is the
scores, e.g., -1, 0, and +1, depending on the number of faults
happening. The DQL is used to learn the optimal policy. The
simulation results show that the proposed DQL can achieve
network throughput close to that of the oracle-based selfhealing, i.e., the upper-performance bound, but incurs less fault
message passing overhead. In particular, the message passing
complexity of the proposed DQL is O(N), and that of the
oracle-based self-healing is O(N 2), where N is the number
of SBSs in the network.
Summary: This section reviews applications of DQL for the
network security and connectivity preservation. The reviewed
approaches are summarized along with the references in Table VI. We observe that the CNN is mostly used for the DQL to
enhance the network security. Moreover, DQL approaches for
the anonymous system such as robot systems and ITS receive
more attentions than other networks. However, the applications
of DQL for the cyber-physical security are relatively few and
need to be investigated.
VI. MISCELLANEOUS ISSUES
In previous sections, we have reviewed and analyzed the
applications of DRL framework in different aspects (with
different technological focuses) of communications and networking, i.e., network access and rate control, caching and
ofﬂoading, security and connectivity preservation. However,
the optimization of communications and networking actually
involves cross-layer co-design and interactions among different network entities. For example, the trafﬁc engineering
problem may require joint optimization of routing, transmit
power control, channel access control, resource allocation and
so on. The problem can become more complicated for the
emerging communications systems, e.g., UAV and vehicular
networks. In this section, we review the other uses of DRL
in communications and networking. These issues include
(i) trafﬁc engineering and routing, (ii) resource sharing and
scheduling, and (iii) crowdsensing and social networking.
Due to its model-free nature, DRL provides a ﬂexible tool
for dynamic and diversiﬁed applications, typically involving
high-dimensional, cross-layer, and multi-agent interactions.
All these imply a huge space of state transitions and actions.
A. Trafﬁc Engineering and Routing
Trafﬁc Engineering (TE) in communication networks refers
to Network Utility Maximization (NUM) by optimizing a path
to forward the data trafﬁc, given a set of network ﬂows from
source to destination nodes. Traditional NUM problems are
mostly model-based. However, with the advances of wireless communication technologies, the network environment
becomes more complicated and dynamic, which makes it hard
to model, predict, and control. The recent development of
DQL methods provides a feasible and efﬁcient way to design
experience-driven and model-free schemes that can learn and
adapt to the dynamic wireless network from past observations.
Routing optimization is one of the major control problems
in trafﬁc engineering. The authors in present the ﬁrst
attempt to use the DQL for the routing optimization. Through
the interaction with the network environment, the DQL agent
at the network controller determines the paths for all sourcedestination pairs. The system state is represented by the
bandwidth request between each source-destination pair, and
the reward is a function of the mean network delay. The DQL
agent leverages the actor-critic method for solving the routing
problem that minimizes the network delay, by adapting routing
conﬁgurations automatically to current trafﬁc conditions. The
DQL agent is trained using the trafﬁc information generated by
a gravity model . The routing solution is then evaluated
by OMNet+ discrete event simulator . Simulation results
show that the well-trained DQL agent can produce a nearoptimal routing conﬁguration in a single step and thus the
agent is agile for real-time network control. The proposed
approach is attractive as the traditional optimization-based
techniques require a large number of steps to produce a new
conﬁguration. The authors in consider a similar network
model with multiple end-to-end communication sessions. Each
source-destination pair has a set of candidate paths that can
transport the trafﬁc load. Experimental results show that the
conventional DDPG method does not work well for the
continuous control problem in . One possible explanation
is that DDPG utilizes uniform sampling for experience replay,
which ignores different signiﬁcance of the transition samples.
The authors in also combine two new techniques to
optimize DDPG particularly for trafﬁc engineering problems,
i.e., TE-aware exploration and actor-critic-based PER methods. The TE-aware exploration leverages the shortest path
algorithm and NUM-based solution as the baseline during
exploration. The PER method is conventionally used in DQL,
e.g., and , while the authors in integrate
the PER method with the actor-critic framework for the ﬁrst
time. The proposed scheme assigns different priorities to
transitions in the experience replay. Based on the priority, the
proposed scheme samples the transitions in each epoch. The
system state consists of throughput and delay performance of
each communication session. The action speciﬁes the amount
of trafﬁc load going through each of the paths. By learning the dynamics of network environment, the DQL agent
aims to maximize the total utility of all the communication
sessions, which is deﬁned based on end-to-end throughput
and delay . Packet-level simulations using NS-3 ,
tested on well-known network topologies as well as random
topologies generated by BRITE , reveal that the proposed
DQL scheme signiﬁcantly reduces the end-to-end delay and
improves the network utility, compared with the baseline
schemes including DDPG and the NUM-based solutions.
The networking and routing optimization become more
TABLE VI: A summary of approaches using DQL for network security and connectivity preservation.
ALGORITHMS
Network security
Number of PUs and signal
Channel selection
and leaving
SINR and mobility
transducer
Signal SINR
Staying and
leaving decisions
SINR and mobility
Underwater
Signal SINR
Channel selection
SINR and mobility
IoT device
Signal SINR
Channel selection
SINR and energy
consumption cost
Signal SINR and BER
Relay power
SINR and relay cost
Jamming power
Transmit power
Secrecy capacity and
energy consumption
LSTM units
Autonomous
Deviation values
Measurement
weight selection
Safe spacing
LSTM units
Attack actions on IoT devices
IoT device set
IoT devices’ data
Connectivity preservation
base station
Relative positions and the
velocity of robots
Velocity decision
Sore +1 and -1
base station
Relative positions and the
velocity of robots
Velocity decision
Sore +1 and -1
base station
Information of distances
among robots
Turning left and
turning right
Sore +1 and -1
Reference signal received
quality and the last action
Serving SBS
Data rate and energy
consumption
Ultra-dense
The number of active alarms
Enabling transmit
diversity and
changing antenna
Score -1, 0, +1, and
Selforganization
complicated in the UAV-based wireless communications. The
authors in model autonomous navigation of one single
UAV in a large-scale unknown complex environment as a
POMDP, which can be solved by actor-critic-based DRL
method. The system state includes its distances and orientation
angles to nearby obstacles, the distance and angle between its
present position and the destination. The UAV’s action is to
turn left or right or keep ahead. The reward is composed of
four parts: an exponential penalty term if it is too close to
any obstacles, a linear penalty term to encourage minimum
time delay, the transition and direction rewards if the UAV is
getting close to the target position in a proper direction. Instead of using conventional DDPG for continuous control, the
Recurrent Deterministic Policy Gradient (RDPG) is proposed
for the POMDP by approximating the actor and critic using
RNNs. Considering that RDPG is not suitable for learning
using memory replay, the authors in propose the fast-
RDPG method by utilizing the actor-critic framework with
function approximation . The proposed method derives
policy update for POMDP by directly maximizing the expected
long-term accumulated discounted reward.
Path planning for multiple UAVs connected via cellular
systems is studied in and . Each UAV aims to
achieve a tradeoff between maximizing energy efﬁciency and
minimizing both latency and interference caused to the ground
network along its path. The network state observable by each
UAV includes its distances and orientation angles to cellular
BSs, the orientation angle to its destination, and the horizontal
coordinates of all UAVs. The action of each UAV includes an
optimal path, transmit power, and cell association along its
path. The interaction among UAVs is cast as a dynamic game
and solved by a multi-agent DRL framework. The use of ESN
in the DRL framework allows each UAV to retain previous
memory states and make a decision for unseen network states,
based on the reward obtained from previous states. ESN is a
new type of RNNs with feedback connections, consisting of
the input, recurrent, and output weight matrices. ESN training
is typically quick and computationally efﬁcient compared with
other RNNs. Deep ESNs can exploit the advantages of a
hierarchical temporal feature representation at different levels
of abstraction, hence disentangling the difﬁculties in modeling
complex tasks. The analysis shows that the computational
complexity of the proposed DRL is O(A3), where A is the
number of discretized areas. Simulation results show that
the proposed scheme improves the tradeoff between energy
efﬁciency, wireless latency, and the interference caused to the
ground network. Results also show that each UAV’s altitude
is a function of the ground network density and the UAV’s
objective function is an important factor in achieving the
UAV’s target.
Besides networked UAVs, vehicle-to-infrastructure also constitutes an important part and provides rich application implications in 5G ecosystem. The authors in adopt the DQL
to achieve an optimal control policy in communication-based
train control system, which is supported by bidirectional trainground communications. The control problem aims to optimize
the handoff decision and train control policy, i.e., accelerate or
decelerate, based on the states of stochastic channel conditions
and real-time information including train position, speed,
measured SNR from APs, and handoff indicator. The objective
of the DQL agent is to minimize a weighted combination of
operation proﬁle tracking error and energy consumption.
B. Resource Sharing and Scheduling
System capacity is one of the most important performance
metrics in wireless communication networks. System capacity
enhancements can be based on the optimization of resource
sharing and scheduling among multiple wireless nodes. The
integration of DRL into 5G systems would revolutionize the
resource sharing and scheduling schemes from model-based to
model-free approaches and meet various application demands
by learning from the network environment.
The authors in study the user scheduling in a multiuser massive MIMO system. User scheduling is responsible for
allocating resource blocks to BSs and mobile users, taking into
account the channel conditions and QoS requirements. Based
on this user scheduling strategy, a DRL-based coverage and
capacity optimization is proposed to obtain dynamically the
scheduling parameters and a uniﬁed threshold of QoS metric.
The performance indicators are calculated as the average spectrum efﬁciency of all the users. The system state is an indicator
of the average spectrum efﬁciency. The action of the scheduler
is a set of scheduling parameters to maximize the reward as a
function of the average spectrum efﬁciency. The DRL scheme
uses policy gradient method to learn a policy function (instead
of a Q-function) directly from trajectories generated by the
current policy. The policy network is trained with a variant
of the REINFORCE algorithm . The simulation results
in show that compared with the optimization-based
algorithms that suffer from incomplete network information,
the policy gradient method achieves much better performance
in terms of network coverage and capacity.
In , the authors focus on dynamic resource allocation
in a cloud radio access network and present a DQL-based
framework to minimize the total power consumption while
fulﬁlling mobile users’ QoS requirements. The system model
contains multiple Remote Radio Heads (RRHs) connected to
a cloud BaseBand Unit (BBU). The information of RRHs
can be shared in a centralized manner. The system state
contains information about the mobile users’ demands and
the RRHs’ working states, e.g., active or sleep. According
to the system state and the result of last execution, the
DQL agent at the BBU decides whether to turn on or off
certain RRH(s), and how to allocate beamforming weight
for each active RRH. The objective is to minimize the total
expected power consumption. The authors propose a two-step
decision framework to reduce the size of action space. In
the ﬁrst step, the DQL agent determines the set of active
RRHs by Q-learning and DNNs. In the second step, the
BBU derives the optimal resource allocation for the active
RRHs by solving a convex optimization problem. Through
the combination of DQL and optimization techniques, the
proposed framework results in a relatively small action space
and low online computational complexity. Simulation results
show that the framework achieves signiﬁcant power savings
while satisfying user demands and is robust in highly dynamic
network environment. The aforementioned works mostly focus
on simulations and numerical comparisons. With one step
further, the authors in implement a multi-objective DQL
framework as the radio-resource-allocation controller for space
communications. The implementation uses modular software
architecture to encourage re-use and easy modiﬁcation for
different algorithms, which is integrated into the real spaceground system developed by NASA Glenn Research Center.
In emerging and future wireless networks, BSs are deployed
with a high density that introduces a number of challenges to
resource management such as the power allocation and interference management. Traditional power allocation approaches,
e.g., the iterative algorithm based on closed-form factional programming can be used. However, the approaches always
require full CSI that may be not available as the networks
become larger and more complicated. Learning algorithms
such as DRL can be used for the power allocation. The authors
in propose to use a DQL scheme which allows the BSs
to learn their optimal power control policy. In the proposed
scheme, each BS is an agent, the action is choosing power
levels, and the state includes interference that the BS caused to
its neighbors in the last time slot. The objective is to maximize
the BS’s data rate. The DQN using FNN is then adopted to
implement the DQL algorithm. The DQN using FNN is then
adopted to implement the DQL algorithm. In the simulation,
the authors model the channel variations in the Jake’s model.
With random CSI and delay, the DQL algorithm is shown to
achieve near-optimal power allocation in real time. In certain
scenarios, it even outperforms the centralized algorithms. This
veriﬁes that the DQL algorithm can be fast and competitive,
especially for the scenarios with inaccurate CSI and nonnegligible delay. For the future work, a joint power control
and channel selection can be considered.
Network slicing and NFV are two emerging
concepts for resource allocation in the 5G ecosystem to
provide cost-effective services with better performance. The
network infrastructure, e.g., cache, computation, and radio
resources, is comparatively static while the upper-layer Virtualized Network Functions (VNFs) are dynamic to support
time-varying application-speciﬁc service requests. The concept
of network slicing is to divide the network resources into
multi-layer slices, managed by different service renderers
independently with minimal conﬂicts. The concept of Service
Function Chaining (SFC) is to orchestrate different VNFs to
provide required functionalities and QoS provisioning.
The authors in propose a DQL scheme for QoS/QoEaware SFC in NFV-enabled 5G systems. Typical QoS metrics
are bandwidth, delay, throughput, etc. The evaluation of QoE
normally involves the end-user’s participation in rating the service based on direct user perception. The authors quantify QoE
by measurable QoS metrics without end-user involvements,
according to the Weber-Fechner Law (WFL) and exponential interdependency of QoE and QoS hypothesis .
These two principles actually deﬁne nonlinear relationship between QoE and QoS. The system state represents the network
environment including network topology, QoS/QoE status of
the VNF instances, and the QoS requirements of the SFC request. The DQL agent selects a certain direct successive VNF
instance as an action. The reward is a composite function of the
QoE gain, the QoS constraint penalty, and the OPEX penalty.
A DQL based on CNNs is implemented to approximate the
action-value function. The authors in review the application of a DQL framework in two typical resource management
scenarios using network slicing. For radio resource slicing,
the authors simulate a scenario containing one single BS with
different types of services. Due to limited spectrum resource,
the radio resource slicing aims to optimize the allocation of
resource blocks to each slice. The reward can be deﬁned as a
weighted sum of spectrum efﬁciency and QoE. For prioritybased core network slicing, the authors simulate a scenario
with 3 SFCs demanding different computational resources and
waiting time. The aim is to minimize the scheduling delay by
optimizing the common or dedicated VNFs. The reward can
be deﬁned as a weighted sum of spectrum efﬁciency and QoE.
For priority-based core network slicing, the authors simulate
a scenario with 3 SFCs demanding different computational
resources and waiting time. The reward is the sum of waiting
time in different SFCs. Simulation results in both scenarios
show that the DQL framework could exploit more implicit
relationship between user activities and resource allocation in
resource constrained scenarios, and enhance the effectiveness
and agility for network slicing.
Mobile data trafﬁc is anticipated to signiﬁcantly increase
that may cause congestion at SBSs in 5G wireless networks.
The conventional technique adjusts the uplink-downlink ratio
based on the current network trafﬁc condition and lacks knowledge of future trafﬁc patterns. The conventional technique is
thus prone to repeated congestion. The deep learning technique
based on LSTM can be used as proposed in that allows
the SBS to make local prediction of its trafﬁc load based on
the past and current trafﬁc load. The input of the LSTM is
the trafﬁc information of the SBS in the past and current
time slots, and the output is the trafﬁc load predictions in the
next time slots. Based on the prediction, the SBS proactively
adjusts the uplink-downlink ratio to avoid the congestion. The
simulation results show that the proposed scheme outperforms
the conventional methods in terms of network throughput and
packet loss rate.
Resource allocation and scheduling problems are also important for computer clusters or database systems. This usually
leads to an online decision-making problem depending on
the information of workload and environment. The authors
in propose a DRL-based solution, DeepRM, by employing policy gradient methods to manage resources in
computer systems directly from experience. The same policy
gradient method is also used in for user scheduling
and resource management in wireless systems. DeepRM is
a multi-resource cluster scheduler that learns to optimize
various objectives such as minimizing average job slowdown
or completion time. The system state is the current allocation
of cluster resources and the resource proﬁles of jobs in
the queue. The action of the scheduler is to decide how
to schedule the pending jobs. By simulations with synthetic
dataset, DeepRM is shown to perform comparably or better
than state-of-the-art heuristics, e.g., Shortest-Job-First (SJF). It
adapts to different conditions and converges quickly, without
any prior knowledge of system behavior. In , the authors
use the actor-critic method to address the scheduling problem in a general-purpose distributed data stream processing
systems, which deal with processing of continuous data ﬂow
in real time or near-real-time. The system model contains
multiple threads, processes, and machines. The system state
consists of the current scheduling decision and the workload
of each data source. The scheduling problem is to assign each
thread to a process of a machine. The agent at the scheduler
determines the assignment of each thread, with the objective of
minimizing the average processing time. The DRL framework
includes three components, i.e., an actor network, an optimizer
producing a K-NN set of the actor network’s output action,
and the critic network predicting the Q-value for each action
in the set. The action is selected from the K-NN set with the
maximum Q-value. The use of optimizer may avoid unstable
learning and divergence problems in conventional actor-critic
methods . Simulation results show that the proposed
scheme can reduce the average processing time by 25.1%
compared to the default scheduler and 9% compared to the
model-based method.
C. Power Control and Data Collection
With the prevalence of IoT and smart mobile devices,
mobile crowdsensing becomes a cost-effective solution for
network information collection to support more intelligent
operations of wireless systems. The authors in consider spectrum sensing and power control in non-cooperative
cognitive radio networks. There is no information exchange
between PUs and SUs. As such, the SU outsources the sensing
task to a set of spatially distributed sensing devices to collect
information about the PU’s power control strategy. The SU’s
power control can be formulated as an MDP. The system
state is determined by the Received Signal Strength (RSS)
at individual sensing devices. The SU chooses its transmit
power from the set of pre-speciﬁed power levels based on the
current state. A reward is obtained if both primary and SUs can
fulﬁll their SNR requirements. Considering the randomness in
RSS measurements, the authors propose a DQL scheme for the
SU to learn and adjust its transmit power. The DQL is then
implemented by a DQN by using FNN. The simulation results
show that the proposed DQL scheme is able to converge to a
close-to-optimal solution.
In massive MIMO networks, each base station is able to
serve a number of users simultaneously. Thus, one critical
problem is to allocate power to the users. To address the problem, the authors in propose to use the deep learning. The
model consists of multiple base stations that serve multiple
users. The problem is to determine a power allocation vector
for the users to maximize the global energy efﬁciency of the
network. The global energy efﬁciency is generally proportional
to the data rates of the users and inversely proportionally to
their total power consumption. The FNN is trained in which
the input includes the signals received at the users, and the
output includes the predicted power allocation vector. The loss
function is formulated from the predicted power allocation
vector and the optimal power allocation vector. Here, the
optimal power allocation vector is obtained off-line by using
the Sequential Fractional Programming (SFP) algorithm .
The simulation results show that the proposed deep learning
scheme can achieve the energy efﬁciency close to that of the
SFP algorithm while using signiﬁcantly less CPU time.
Max-min fairness is also a performance metric in the
massive MIMO network to enhance the channel capacity of
the users. The authors in adopt a deep learning model
for the power allocation to achieve the objective. The model
consists of multiple cells, and each cell consists of a base
station that serves the base station’s users. The problem of
the base station is to determine the power allocation vector
for its serving users to achieve the max-min fairness. Here,
the max-min fairness is a max-min optimization problem in
which the objective function is the ergodic channel capacity
of each user. The FNN is trained in which the input includes
geographical positions of the users in the whole network and
the output includes the predicted power allocation vector of
the users served by the base station. The loss function is
a function of the optimal power allocation vector and the
predicted power allocation vector. In particular, the optimal
power allocation vector is obtained by ofﬂine solving the
base station’s optimization problem based on the traditional
bisection approach . The simulation results show that the
proposed deep learning scheme matches well the Minimum
Mean Square Error (MMSE) scheme that is optimal but
has high computational complexity.
The authors in leverage the DQL framework for
sensing and control problems in a Wireless Sensor and Actor
Network (WSAN), which is a group of wireless devices with
the ability to sense events and to perform actions based on the
sensed data shared by all sensors. The system state includes
processing power, mobility abilities, and functionalities of
the actors and sensors. The mobile actor can choose its
moving direction, networking, sensing and actuation policies
to maximize the number of connected actor nodes and the
number of sensing events.
The authors in focus on mobile crowdsensing
paradigm, where data inference is incorporated to reduce
sensing costs while maintaining the quality of sensing. The
target sensing area is split into a set of cells. The objective of a
sensing task is to collect data, e.g., temperature and air quality,
in all the cells. Here, the sensed data from different cells of
the target sensing area may has different quality. The problem
for each sensor in the mobile crowdsensing paradigm is how
to choose cells to collect data to minimize the sensing cost
while guaranteeing the sensing data quality. The optimal cell
selection algorithms, e.g., , need to know the true data
of each cell in advance that is usually impossible in practice.
DQL is able to provide optimal decisions without a complete
knowledge of the true data. Thus, it can be adopted for the
cell selection of the mobile sensors to decide which cell is
better to perform sensing tasks. The system state includes the
selection matrices for a few past decision epochs. The reward
function is determined by the sensing quality and cost in the
chosen cells. To extract temporal correlations in learning, the
authors propose the DRQN that uses LSTM layers in DQL
to capture the hidden patterns in state transitions. Considering
inter-data correlations, the authors use the transfer learning
method to reduce the amount of data in training. That is, the
cell selection strategy learned for one task can beneﬁt another
correlated task. Hence, the parameters of DRQN can be initialized by another DRQN with rich training data. Simulations
are conducted based on two real-life datasets collected from
sensor networks, i.e., the Sensor-Scope dataset in the
EPFL campus and the U-Air dataset of air quality readings
in Beijing . The experiments verify that DRQN reduces
up to 15% of the sensed cells with the same inference quality
guarantee. The authors in combine UAV and unmanned
vehicle in mobile crowdsensing for smart city applications.
The UAV cruises in the above of the target region for city-level
data collection. Meanwhile, the unmanned vehicle carrying
mobile charging stations moves on the ground and can charge
the UAV at a preset charging point.
The target region is divided into multiple subregions and
each subregion has a different sample priority. The authors
in propose a DQL-based control framework for the
unmanned vehicle to schedule its data collection, constrained
by limited energy supply. The system state includes information about the sample priority of each subregion, the
location of charging point, and the moving trace of the UAV
and unmanned vehicle. The UAV and unmanned vehicle can
choose the moving direction. The DQL framework utilizes
CNNs for extracting the correlation of adjacent subregions,
which can increase the convergence speed in training. The
DQL algorithm can be enhanced by using a feasible control
solution as the baseline during exploration. The PER method
is also used in DQL to assign higher priorities to important
transitions so that the DQL agent can learn from samples more
efﬁciently. The proposed scheme is evaluated by using real
dataset of taxi traces in Rome . Simulation results reveal
that the proposed DQL algorithm can obtain the highest data
collection rate compared with the MDP and other heuristic
baselines.
Mobile crowdsensing is vulnerable to faked sensing attacks,
as selﬁsh users may report faked sensing results to save
their sensing costs and avoid compromising their privacy.
The authors in formulate the interactions between the
server and a number of crowdsensing users as a Stackelberg
game. The server is the leader that sets and broadcasts its
payment policy for different sensing accuracy. In particular,
the higher payment is set for more sensing accuracy. Based
on the server’s sensing policy, each user as a follower then
chooses its sensing effort and thus the sensing accuracy to
receive the payment. The payment motivates the users to put
in sensing efforts, and thus the payment decision process can
be modeled as an MDP. The server can apply Q-learning to
optimize payment policy without requiring the sensing model.
However, in the presence of a large state space, Q-learning has
a slow learning rate. Therefore, the server can use the DQL to
obtain the optimal payment to maximize its utility, based on
the system state consisting of the past sensing quality and the
payment policy. The DQL uses a deep CNN to accelerate the
learning process and improve the crowdsensing performance
against selﬁsh users. Simulation results show that the DQLbased scheme produces a higher sensing quality, lower attack
rate, and higher utility of the server, exceeding those of both
the Q-learning and the random payment strategies. Moreover,
the DQL-based scheme can converge at 200 time slots that is
225% faster than that of the Q-learning.
Social networking is an important component of smart
city applications. The authors in aim to extract useful
information by observing and analyzing the users’ behaviors in
social networking. One of the main difﬁculties is that the social
behaviors are usually fuzzy and divergent. The authors model
pervasive social networking as a monopolistically competitive
market, which contains different users as data providers selling
information at a certain price. Given the market model, the
DQL can be used to estimate the users’ behavior patterns
and ﬁnd the market equilibrium. Considering the costly deep
learning structure, the authors in propose a Decentralized
DRL (DDRL) framework that decomposes the costly deep
component from the RL algorithms at individual users. The
deep component can be a feature extractor integrated with
the network infrastructure and provide mutual knowledge for
all individuals. Multiple RL agents can purchase the most
desirable data from the mutual knowledge. The authors combine well-known RL algorithms, i.e., Q-learning and learning
automata, to estimate users’ patterns which are described by
vectors of probabilities representing the users’ preferences
or altitudes to different information. In social networking
and smart city applications with human involvement, there
can be both labeled and unlabeled data and hence a semisupervised DRL framework can be designed, by combining
the strengths of DNNs and statistical modeling to improve
the performance and accuracy in learning. Then, the authors
in introduce the semi-supervised DRL framework that
utilizes variational auto-encoders as an inference engine
to infer the classiﬁcation of unlabeled data. As a case study,
the proposed DRL framework is customized to provide indoor
localization based on the RSS from Bluetooth devices. The
positioning environment contains a set of positions. Each
position is associated with the set of RSS values from the
set of anchor devices with known positions. The system state
includes a vector of RSS values, the current location, and the
distance to the target. The DQL agent, i.e., the positioning
algorithm itself, chooses a moving direction to minimize the
error distance to the target point. Simulations tested on realworld dataset show an improvement of 23% in terms of the
error distance to the target compared with the supervised DRL
D. Direction-of-Arrival (DoA) Estimation
Massive MIMO will be deployed for 5G to achieve highspeed communications at Gbps. For this, the Direction-of-
Arrival (DoA) estimation is the prerequisite for realizing
massive MIMO. However, the required DoA estimation is very
challenging since it is difﬁcult to exploit the characteristics of
the channel and sparsity of the massive MIMO systems. Deep
learning is able to learn useful features, and it can be used
for the DoA estimation as proposed in . In particular, the
DNN is trained based on a training dataset including DoAs
and received signals. The DoAs are randomly generated, and
the received signals are obtained by using different wireless
channel models. The input of the DNN includes the DoAs
and the received signals, and the output includes the estimated
DoAs. The simulation results show that the high accuracy of
the DoA estimation can be achieved with a mean square error
E. Signal Detection
Apart from improving the accuracy of the DoA estimation,
enhancing the accuracy of the symbol detection needs to be
considered to reduce Bit Error Ratio (BER). The authors
in propose to use the deep learning for the symbol
detection in OFDM systems. Speciﬁcally, the DNN is used
in which the input includes each pair of pilot symbol and
the corresponding received signal. The output of the DNN
includes the predicted pilot symbol. The simulation results
show that the proposed scheme outperforms the least square
technique in terms of BER. These results imply that the DNN
can learn the characteristics of the wireless channel accurately.
F. User Association and Load Balancing
In the complicated future networks such as HetNets and
massive MIMO, user association is one of critical problems
to assign users to base stations to maximize the total data
rate. The existing approaches generally incur a considerable
complexity overhead that impairs the real-time user association. Thus, deep learning can be used to perform the user
association in real-time in realistic massive MIMO networks
as proposed in . The model is similar to that in .
The problem is to determine the user-association vector to
maximize the total data rate of all users in the network. The
FNN is trained to output the user association vector given
the user positions. The training set consists of multiple pairs
of user positions and optimal user association vector. Here,
the optimal association vector is obtained ofﬂine by using the
branch-and-cut algorithm . Simulation results show that
the total data rate obtained by the deep learning scheme is
close to that obtained by the optimal user association scheme
from . This implies that the structure of the optimal
association can be learned by the deep learning.
G. User Localization
Channel State Information (CSI) has been recently used for
the user localization since the CSI values contain information
related to locations of the users. However, in the complicated wireless networks such as massive MIMO, measuring
the wireless channel characteristics, i.e., the CSI values, is
challenging. Deep learning that is able to learn useful features
from complicated environments can be used for the user
localization as proposed in . The deep learning algorithm
consists of two phases, i.e., the ofﬂine training phase and
online localization phase. In the ofﬂine training phase, a
mobile device transmits packets from known locations to a
base station. At each location, the base station measures the
corresponding CSI values. The measured CSI values as a
ﬁngerprint of the location are used to train the FNN. The input
of the FNN includes the measured CSI values, and the output
includes the reconstructed CSI values. The gready learning
algorithm is used to train the FNN that minimizes the
error between the measured CSI values and the reconstructed
CSI values. In the online localization phase, the reconstructed
CSI values are used to estimate the location of the mobile
device based on the probabilistic methods. Simulation results
show that with the proposed deep learning algorithm, 60%
of the positions in the test have errors under 1m, while the
baseline algorithm only ensures that 25% of the tested
positions have errors under 1m.
The FNN can work well as the number of CSI values is
small. However, it may be computationally intractable with a
large number of CSI values. To address the issue, CNNs can be
used to learn the features of the wireless channels for the user
localization as proposed in . However, CNN is suitable
to process inputs with grid-like structures, e.g., an image.
Therefore, the CSI values must be ﬁrst transformed into socalled channel snapshots. Then, the CNN is trained to output
the user locations given the channel snapshots. The analysis
shows that the overall complexity of the proposed CNN
algorithm is O(K2MLNF S1S2), where K is the number
of convolution Kernels, L is the number of convolutionalactivation-pooling layers, M is the number of the base station’s
antennas, NF is the number of frequency points, and S1 and
S2 are the sizes of the Kernels, respectively. As such, the
complexity does not depend on the training set size that is one
main advantage of using the CNN for the user localization.
H. Access Device Detection
Massive Machine-Type Communication (mMTC) is expected to be one of key technologies in 5G. mMTC enables
tens of billions of machine-type devices to communicate with
each other with high availability, low latency, and high reliability. However, the mMTC systems only allow the devices
to transmit small packets with minimum signaling overhead.
Thus, it is challenging for the base stations to detect and
identify active devices, i.e., devices to access the base stations. The traditional detection algorithms such as iterative
algorithm can be used. However, they fail to consider
the time constraints. Deep learning that is able to provide
online decisions can be an alternative solution as proposed
in . The system model consists of a base station and
multiple machine-type devices. Each device is assigned with
a unique pilot sequence. The signal received at the base
station is the sum of signals transmitted from active users.
Given the received signal, the base station needs to determine
which users are active. The base station uses the FNN in
which the input is the received signal, and the output contains
the user activity matrix. The FNN is trained in which the
training data is randomly generated and the loss function is
formulated from the received signal, the user activity matrix
and the channel estimations of the users, and the pilot matrix
of the users. Simulation results show that the proposed deep
learning scheme outperforms the traditional iterative algorithm
from in terms of active user detection success rate.
Summary: In this section, we review miscellaneous uses
of DRL in wireless and networked systems. DRL provides
a ﬂexible tool in rich and diversiﬁed applications, conventionally involving dynamic system modeling and multi-agent
interactions. All these imply a huge space of state transitions
and actions. These approaches are summarized along with the
references in Table VII. We observe that the NUM problems
in 5G ecosystem for trafﬁc engineering and resource allocation face very diversiﬁed control variables, including discrete
indicators, e.g., for BS (de)activation, user/cell association,
and path selection, as well as continuous variables such as
bandwidth allocation, transmit power, and beamforming optimization. Hence, both DQL and policy gradient methods are
used extensively for discrete and continuous control problems,
respectively.
VII. CHALLENGES, OPEN ISSUES, AND FUTURE
RESEARCH DIRECTIONS
Different approaches reviewed in this survey evidently show
that DRL can effectively address various emerging issues
in communications and networking. There are existing challenges, open issues, and new research directions which are
discussed as follows.
A. Challenges
1) State Determination in Density Networks: The DRL
approaches, e.g., , allow the users to ﬁnd an optimal access policy without having complete and/or accurate network
information. However, the DRL approaches often require the
users to report their local states at every time slot. To observe
the local state, the user needs to monitor Received Signal
Strength Indicators (RSSIs) from its neighboring BSs, and then
it temporarily connects to the BS with the maximum RSSI.
However, the future networks will deploy a high density of the
BSs, and the RSSIs from different BSs may not be different.
Thus, it is challenging for the users to determine the temporary
2) Knowledge of Jammers’ Channel Information: The DRL
approach for wireless security as proposed in enables
the UAV to ﬁnd optimal transmit power levels to maximize
the security capacity of the UAV and the BS. However,
to formulate the reward of the UAV, a perfect knowledge
of channel information of the jammers is required. This is
challenging and even impossible in practice.
3) Multi-agent DRL in Dynamic HetNets: Most of the
existing works focus on the customizations of DRL framework
for individual network entities, based on locally observed
or exchanged network information. Hopefully, the network
environment is relatively static to ensure convergent learning
results and stable policies. This requirement may be challenged
in a dynamic heterogenous 5G network, which consists of
hierarchically nested IoT devices/networks with fast changing
service requirements and networking conditions. In such a
situation, the DQL agents for individual entities have to be
light-weighted and agile to the change of network conditions.
TABLE VII: A summary of applications of DQL for trafﬁc engineering, resource scheduling, and data collection.
ALGORITHMS
Trafﬁc engineering and routing
actor-critic
controller
Bandwidth request of each
Trafﬁc load split
on different paths
Mean network delay
5G network
actor-critic
controller
Throughput and delay
performance
Trafﬁc load split
on different paths
α-fairness utility
5G network
actor-critic
Local sensory information,
e.g., distances and angles
Turn left or right
Composite reward
navigation
Coordinates, distances, and
orientation angles
Path, transmit
power, and cell
association
Weighted sum of
energy efﬁciency,
latency, and
interference
Cellularconnected
Channel conditions, train
position, speed, SNR, and
handoff indicator
Making handoff of
connection, or
accelerate or
decelerate the train
Tracking error and
energy consumption
Vehicle-toinfrastructure
Resource sharing and scheduling
MUs’ demands and the
RRHs’ working states
Turn on or off
certain RRH(s),
and beamforming
allocation
Expected power
consumption
controller
Network topology, QoS/QoE
status, and the QoS
requirements
Successive VNF
Composite function
of QoE gain, QoS
constraints penalty,
and OPEX penalty
controller
The number of arrived
packets/the priority and
time-stamp of ﬂows
Bandwidth/SFC
allocation
Weighted sum of
spectrum efﬁciency
and QoE/waiting
time in SFCs
5G network
actor-critic
Current scheduling decision
and the workload
Assignment of
each thread
Average processing
Distributed
stream data
processing
Data collection
Received signal strength at
individual sensors
Transmit power
Fixed reward if QoS
DRQN, LSTM,
Cell selection matrices
Next cell for
A function of the
sensing quality and
Subregions’ sample priority,
charging point’s location, and
trace of the UAV and
unmanned vehicle
Moving direction
of the UAV and
unmanned vehicle
Fixed reward related
to subregions’
sample priority
Crowdsensing
Previous sensing quality and
payment policy
Current payment
crowdsensing
Current preferences
Positive or
negative altitude
This implies a reduce to the state and action spaces in learning,
which however may compromise the performance of the
convergent policy. The interactions among multiple agents also
complicate the network environment and cause a considerable
increase to the state space, which inevitably slows down the
learning algorithms.
4) Training and Performance Evaluation of DRL Framework: The DRL framework requires large amounts of data for
both training and performance evaluation. In wireless systems,
such data is not easily accessible as we rarely have referential
data pools as other deep learning scenarios, e.g., computer
vision. Most of the existing works rely on simulated dataset,
which undermines the conﬁdence of the DRL framework in
practical system. The simulated data set is usually generated
by a speciﬁc stochastic model, which is a simpliﬁcation of the
real system and may overlook the hidden patterns. Hence, a
more effective way for generating simulation data is required
to ensure that the training and performance evaluation of the
DRL framework are more consistent with practical system.
B. Open Issues
1) Distributed DRL Framework in Wireless Networks: The
DRL framework requires large amounts of training for DNNs.
This may be implemented at a centralized network controller,
which has sufﬁcient computational capacity and the capability
for information collection. However, for massive end users
with limited capabilities, it becomes a meaningful task to
design distributed implementation for the DRL framework that
decomposes resource-demanding basic functionalities, e.g.,
information collection, sharing, and DNN training, from reinforcement learning algorithms at individual devices. The basic
functionalities can be integrated with the network controller. It
remains an open issue for the design of network infrastructure
that supports these common functionalities for distributed
DRL. The overhead of information exchange between end
users and network controller also has to be well controlled.
Balance between Information Quality and Learning
Performance: The majority of the existing works consider the
orchestration of networking, transmission control, ofﬂoading,
and caching decisions in one DRL framework to derive the
optimal policy, e.g., – , , . However, from a
practical viewpoint, the network system will have to pay substantially increasing cost for information gathering. The cost
is incurred from large delay, pre-processing of asynchronous
information, excessive energy consumption, reduced learning
speed, etc. Hence, an open issue is to ﬁnd the optimal balance
between information quality and learning performance so that
the DQL agent does not consume too much resources only
to achieve insigniﬁcantly marginal increase in the learning
performance.
C. Future Research Directions
1) DRL for Channel Estimation in Wireless Systems: We
expect that the combination of Wireless Power Transfer (WPT)
and Mobile Crowd Sensing (MCS), namely Wireless-Powered
Crowd Sensing (WPCS) will be a promising technique for the
emerging IoT services. To this end, a higher power transfer
efﬁciency of WPT is very critical to enable the deployment of
WPCS in low-power wide area network. A "large-scale array
antenna based WPT" will achieve this goal of higher WPT
efﬁciency, but the channel estimation should be performed
with minimal power consumption at a sensor node. This is
because of that the sensor must operate with self-powering via
WPT from the dedicated energy source, e.g., power beacon,
Wi-Fi or small-cell access point, and/or ambient RF sources,
e.g., TV tower, Wi-Fi AP and cellular BS. In this regard, the
channel estimation based on the receive power measurements
at the sensor node is one viable solution, because the receive
power can be measured by the passive-circuit power meter
with negligible power consumption. DRL can be used for
the time-varying wireless channels with temporal correlations
over time by taking the receive power measurements from
the sensor node as the input for DRL, which will enable the
channel estimation for WPT efﬁciently.
2) DRL for Crowdsensing Service Optimization: In MCS,
mobile users contribute sensing data to a crowdsensing service
provider and receive an incentive in return. However, due to
limited resources, e.g., bandwidth and energy, the mobile user
has to decide on whether and how much data to be uploaded
to the provider. Likewise, the provider aiming to maximize its
proﬁt has to determine the amount of incentive to be given.
The provider’s decision depends on the actions of the mobile
users. For example, with many mobile users contributing data
to the crowdsensing service provider, the provider can lower
the incentive. Due to a large state space of a large number of
users and dynamic environment, DRL can be applied to obtain
an optimal crowdsensing policy similar to .
3) DRL for Cryptocurrency Management in Wireless Networks: Pricing and economic models have been widely applied to wireless networks , . For example, wireless
users pay money to access radio resources or mobile services.
Alternatively, the users can receive money if they contribute to
the networks, e.g., offering a relay or cache function. However,
using real money and cash in such scenarios faces many
issues related to accounting, security, and privacy. Recently,
the concept of cryptocurrency based on the blockchain technology has been introduced and adopted in wireless networks,
e.g., , which has been shown to be a secure and efﬁcient
solution. However, the value of cryptocurrency, i.e., token
or coin, can be highly dynamic depending on many market
factors. The wireless users possessing the tokens can decide
to keep or spend the tokens, e.g., for radio resource access
and service usage or exchange into real money. In the random
cryptocurrency market environment, DRL can be applied to
achieve the maximum long-term reward of the cryptocurrency
management for wireless users as in .
4) DRL for Auction:
An auction has been effectively
used for radio resource management, e.g., spectrum allocation . However, obtaining the solution of the auction,
e.g., a winner determination problem, can be complicated and
intractable when the number of participants, i.e., bidders and
sellers, become very large. Such a scenario is typical in nextgeneration wireless networks such as 5G highly-dense heterogeneous networks. DRL appears to be an efﬁcient approach
for solving different types of auctions such as in .
VIII. CONCLUSIONS
This paper has presented a comprehensive survey of the
applications of deep reinforcement learning to communications and networking. First, we have presented an overview of
reinforcement learning, deep learning, and deep reinforcement
learning. Then, we have introduced various deep reinforcement learning techniques and their extensions. Afterwards,
we have provided detailed reviews, analyses, and comparisons of the deep reinforcement learning to solve different
issues in communications and networking. The issues include
dynamic network access, data rate control, wireless caching,
data ofﬂoading, network security, connectivity preservation,
trafﬁc routing, and data collection. Finally, we have outlined
important challenges, open issues as well as future research
directions.
ACKNOWLEDGEMENTS
This work was supported in part by A*STAR-NTU-SUTD
Joint Research Grant Call on Artiﬁcial Intelligence for the Future of Manufacturing RGANS1906, WASP/NTU M4082187
(4080), Singapore MOE Tier 1 under Grant 2017-T1-002-
007 RG122/17, MOE Tier 2 under Grant MOE2014-T2-2-015
ARC4/15, Singapore NRF2015-NRF-ISF001-2277, and Singapore EMA Energy Resilience under Grant NRF2017EWT-
EP003-041, in part by the National Research Foundation of
Korea (NRF) Grant funded by the Korean Government under
Grants 2014R1A5A1011478 and 2017R1A2B2003953, and in
part by the National Natural Science Foundation of China
under Grants 61631005, U1801261, and 61571100.