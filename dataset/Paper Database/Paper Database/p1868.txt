UCRL-ID-148494
U.S. Department of Energy
Lab oratory
A Survey of Dimension
Reduction Techniques
I. K. Fodor
May 9,2002
Approved for public release; further dissemination unlimited
DISCLAIMER
This document was prepared as an account of work sponsored by an agency of the United States
Government. Neither the United States Government nor the University of California nor any of their
employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for
the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or
represents that its use would not infringe privately owned rights. Reference herein to any specific
commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not
necessarily constitute or imply its endorsement, recommendation, or favoring by the United States
Government or the University of California. The views and opinions of authors expressed herein do not
necessarily state or reflect those of the United States Government or the University of California, and
shall not be used for advertising or product endorsement purposes.
This work was performed under the auspices of the U. S. Department of Energy by the University of
California, Lawrence Livennore National Laboratory under Contract No. W-7405-Eng-48.
This report has been reproduced directly from the best available copy.
Available electronically at htb: / /www.doe.gov/bridce
Available for a processing fee to U.S. Department of Energy
and its contractors in paper from
U.S. Department of Energy
Office of scientific and Technical Information
P.O. Box 62
Oak Ridge, TN 37831-0062
Telephone: (865) 576-8401
Facsimile: (865) 576-5728
E-mail: reDorts@adonis .ostl.Pg
Available for the sale to the public from
U.S. Department of Commerce
National Technical Information Service
5285 Port Royal Road
Springfield, VA 22161
Telephone: (800) 553-6847
Facsimile: (703) 605-6900
E-mail. orders% tis.fedworld.mv
Online ordering: hm:/ /www.ntis.Pov/orde rine.htm
Lawrence Livermore National Laboratory
Technical Information Department’s Digital Library
http:/ /www.llnl.gov/ tid/Library.html
A survey of dimension reduction techniques
Imola K. Fodor
May 9, 2002
1 Introduction
Advances in data collection and storage capabilities during the past decades have
led to an information overload in most sciences. Researchers working in domains
as diverse as engineering, astronomy, biology, remote sensing, economics, and
consumer transactions, face larger and larger observations and simulations on a
daily basis. Such datasets, in contrast with smaller, more traditional datasets
that have been studied extensively in the past, present new challenges in data
analysis. Traditional statistical methods break down partly because of the in-
crease in the number of observations, but mostly because of the increase in the
number of variables associated with each observation. The dimension of the
data, is the number of variables that are measured on each observation.
High-dimensional datasets present many mathematical challenges as well as
some opportunities, and are bound to give rise to new theoretical developments
[ll]. One of the problems with high-dimensional datasets is that, in many cases,
not all the measured variables are “important” for understanding the underlying
phenomena of interest. While certain computationally expensive novel methods
 can construct predictive models with high accuracy from high-dimensional
data, it is still of interest in many applications to reduce the dimension of the
original data prior to any modeling of the data.
In mathematical terms, the problem we investigate can be stated as fol-
lows: given the pdimensional random variable x = ( 2 1 , . . . , z ~ ) ~ ,
find a lower
dimensional representation of it, s = (SI,. . . , s k ) T with k 5 p , that captures
the content in the original data, according to some criterion. The components
of s are sometimes called the hidden components. Different fields use differ-
ent names for the p multivariate vectors: the term “variable” is mostly used
in statistics, while “feature” and “attribute” are alternatives commonly used in
the computer science and machine learning literature.
Throughout this paper, we assume that we have n observations, each being
a realization of the pdimensional random variable x = (51,. . . , zp)T
E(x) = 1-1 = (PI,. . . , pp)T and covariance matrix E{ (x - p)(x - 1 - 1 ) ~ ) = X p x p .
We denote such an observation matrix by X = {xi,j : 1 5 i 5 p,l 5 j 5 n}.
If pi and C T ~ = &
denote the mean and the standard deviation of the ith
random variable, respectively, then we will often standardize the observations
i-,., by (s,,, -/ii)/cfj, where Li, = S, = l / n E;=’=,
si.,, and cf, = l / n E;=’=,
We distinguish two major types of cliniension reduction met hods: linear arid
non-linear. Linear techniques result in each of the k 5 p components of the new
variable being a linear combination of the original variables:
.si = wl.l-rl + . . . iu,.psp, for
i = 1,. . . . k , or
relationship as
is the linear transformation weight matrix. Expressing the same
with A p x k r we note that the new variables s are also called the hidden or the
latent variables. In ternis of an n x p observation matrix X, we have
S,,, = i i l , . ’ = , SI,,
+ . . . I ~ ~ , . ~ . Y ~ . ,
i = 1. . . . . k , and j = 1. . . . . 11,
where j indicates the jth realization. or. cquivalcntly.
< I 1 = WL u p X p x r 1 .
X p x r i = A p x k S k
Snch lincw t echniqiies are siniplrr and c>;isier to irriplcment t hari more recent
met hods coiisidcring non-linear t rmsfornis.
In this paper. w
wvimv tratlitioiial a i d currwit st ate-of-thc-art dimension
rtduction mcthotls published in tho statistics. signal prowssing and niacliinc
learning litrraturc. Thew are ~iiiiii(~oiis
hooks and articles [41. 17, 5 . 14, 19, 46,
131 in tlir statistkal literaturr on tcdiriiqiic’s for ;inalyzirig riiii1tiv;iriate datasrts.
=\clvanc~>s
in cwnipiitrr scicwc.cI. riiadiiric. lwrning [43. 50. 44, 21. Earlier si1rvc.y
papers. I wirws-s sc.v(~al met hods, iric.liidiiig principal cwIiiponc’nt,s analysis,
pro.jwt ion pursuit. pi incipal (’111 ves. self-organizing maps. as w ~ l l as provides
~iciiral wtn-oik inipltwicmt at ions of soul(’ of thr rwicwecl statistical ~notlcls. 
siirveys r ( ~
trit rrsiilts iri iritlepc~ritl(mt (winpoiiwt analysis, in the cwntest, of othcr
dirrit~nsion rcdiictiori nirt hods.
This si1rvc.y is organized as follows. Srct ions 2 ant1 3 revimv principal cwrnpo-
ricrit aiialysis and fact or analysis. rcwpc~c’tivc~ly,
t he two most widely used linrar
diniension reduction met hods 1)ascd 0 1 1 sccwrid-order st at ist ics. For riorrrial vari-
ables (with mean zero), t lie covariancc~ mat rix cant airis all the inforniation about
the data. Secwitl-order niethods arc’ relativc1ly simple to cod(., as they require
classical matrix nianipulations. However. many dat astits of interest arc riot re-
alizat,ioIis from Gaussian distri1)ut ions. For t liosc. cases, higher-order dimension
reduction rncthods, using inforniation not contained in thc covariance matrix,
are more appropriatc.. Such a liwar higher-order rnethod, projection pursuit is
I eviewed in Section 4. Sect ion 5 siiiiiiiiarizw another liighw-order linear method
called independent component analysis. Xlthoiigh non-lincvir principal compo-
ncrit analysis can be considercd as a spcc.ial case of independent cornponrnt
analysis, Section 5.1.4, it is reviewed separately in Section 6. It uses non-linear
objective functions to determine the optimal weights, but the resulting compo-
nents are still linear combinations of the original variables. Section 7 explains
the method of random projections. Section 8 presents some extensions and
non-linear dimension reduction techniques.
2 Principal component analysis
Principal component analysis (PCA) is the best, in the mean-square error sense,
linear dimension reduction technique [25, 281. Being based on the covariance
matrix of the variables, it is a second-order method. In various fields, it is also
known as the singular value decomposition (SVD) , the Karhunen-Lohe trans-
form, the Hotelling transform, and the empirical orthogonal function (EOF)
In essence, PCA seeks to reduce the dimension of the data by finding a
few orthogonal linear combinations (the PCs) of the original variables with the
largest variance. The first PC, SI, is the linear combination with the largest
variance. We have .SI = xTw1 , where the pdimensional coefficient vector w1 =
( q , I , . . . ,
w1 = arg maxllw,,IIVar{xTw}.
The second PC is the linear combination with the second largest variance and
orthogonal to the first PC, and so on. There are as many PCs as the num-
ber of the original variables. For many datasets, the first several PCs explain
most of the variance, so that the rest can be disregarded with minimal loss of
information.
Since the variance depends on the scale of the variables, it is customary to
first standardize each variable to have mean zero and standard deviation one.
After the standardization, the original variables with possibly different units of
measurement are all in comparable units. Assuming a standardized data with
the empirical covariance matrix
xpxp = -XXT,
we can use the spectral decomposition theorem to write
where 11 = diag(A1,. . . , A,)
is the diagonal matrix of the ordered eigenvalues
A1 5 . . . 5 A,,
and U is a p x p orthogonal matrix containing the eigenvectors.
It can be shown that the PCs are given by the p rows of the p x n matrix
Comparing (10) to (5), we see that the weight matrix W is given by UT. It
can be shown that the subspace spanned by the first k eigenvectors has the
smallest mean square deviation from X among all subspaces of dimension k.
As briefly indicated in Section 8.5, PCs can also be obtained by using neural
Another property of the eigenvalue decomposition is that the total variation
networks with specific architectures and learning algorithms.
is equal to the sum of the eigenvalues of the covariance matrix,
and that the fraction
gives t,he cumulative proportion of the variance explained by the first I; PCs.
By plotting the ciirriulative proportions in (12) as a fiinction of k, one can select
the appropriate minibrr of PCs to keep in order to explain a given percentage of
thr overall variation. Snch plots arc called scree diagram plot,s in the statistical
literature . The number of PCs to keep can also be detcrniincd by first fixing
a t~hreshold Xo. t hrn only keeping t he cigcnvertors such that, t,heir corresponding
eigcnvalues are greater than A”.
This lat ter nirthod was found preferable in
[26, 271. where t h(> aiitlior also siiggwted keeping at least four variablrs.
Although they
arc’ uncorrrlatrd variables cmistriictctl as lincw conibiIiatioIis of tlie original
variahlcs, and have sonic desirable propcrt icxs. t hey do not ricressarily correspond
to nieaningful physical quantiticw. In SOIIW c.asc’s. sur11 loss of iIitwpretability is
not s,itisfac.tory to the domain scientists.
X i 1 altcrnativv n-ay to retliiw tlie dinitwsion of a dat<isct using PCA is siig-
gcstctl in . Iristclad of using the. PCs :is tho w n - varia1)lvs. this Iricthod u s w
thc information in the PCs to find important \.arial)lrs in thr’ original dat aset.
X s lwforr. oil(’ first ralrulatcs thci PC‘s. thcw studiw thr scrw plot to clcterniine
thcl nurnl)c~r k of iinport ant viix ial)lw to krq). Scst , one cwnsidcrs tlie eigen-
vector c.orrcisI)oIi(liIig to thcx sniall(~st c~igcmvaliit~
(thv Icast import ant PC). and
discards thv v;irial)l(. that has t lit. Iargt’st (a1)solutc~ value) cwc+ficitwt in that vec-
tor. Tlic~n. on(’ considws the‘ c4gcmw.toi c.orrcisl)oiitliiig to the sc~mnd smallest
c~gcwvalut~.
an(l (lis(.ards t lit. vai ial)l(i cant r i h t ing t hc. largc.st (al)solut,r value)
cwficitmt to that tigenvcctor. arno~ig the. varia1)los not discxdcd wr1it.r. The
pi o ( w s is rtycwtcd until only k varialh rclrnain.
Tht. intcrI)rc.tation of the PCs can be difficult at times.
Factor analysis
This srction follows . Liktx PC‘X. factor analysis (FA) is also d linear nitthotl.
based on the sworitl-ordcr data sunirnariw. First siiggtsttd by psychologists,
FA assiinies that the ~neasiirt~l
vitriahlw d~pcmd on soin(’ unknown, and often
uxirncasural~lt~.
(~)ninion factors. Typical c~sanipl~w
inrliidr variables defined as
various trst scores of individiials, as siic.11 s(’or(~
arc thoiight to l)c> related to a
(~)niinon “ixitt~lligc~~ic~~“
factor. Thc. goal of FAA is to iincov(~ such relat ions, and
thus can be iisc~l to rediic~. thr dinicnsion of dat ascts follo\ving thci factor model.
The zero-mean pdimensional random vector xpx
1 with covariance matrix
satisfies the k-factor model if
x = A f + u ,
where h p x k is a matrix of constants, and fkxl and upxl
are the random com-
mon factors and specific factors, respectively. In addition, the factors are all
uncorrelated and the common factors are standardized to have variance one:
E(f) = 0, Var(f) = I,
C ~ v ( u i , u j ) = O for
Cov(f,u) = 0.
Under these assumptions, the diagonal covariance matrix of u can be written
as Cov(u) = * = diag(&~,.--,+,,).
If the data covariance matris can be decomposed as
then it can be shown that the k-factor model holds. Since zi can be written as
xi = C A i j f j +ui, i = 1 ,..., p,
its variance may be decomposed as
Afj + +ii,
where the first part h: = Eftl ATj is called the communality and represents
the variance of xi common to all variables, while the second part $ii is called
the specific or unique variance and it is the contribution in the variability of
xi due to its specific ui part, not shared by the other variables. The term A%
measures the magnitude of the dependence of xi on the common factor fj. If
several variables zi have high loadings X i j on a given factor f j , the implication is
that those variables measure the same unobservable quantity, and are therefore
redundant.
Unlike PCA, the factor model does not depend on the scale of the variables.
However, the factor model also holds for orthogonal rotations of the factors.
Given the orthogonal matrix G, given the model (13), the new model
x = (hG)(GTf) + U,
also holds, with new factors GTf and corresponding loadings AG. Therefore,
the factors are generally rotated to satisfy some additional constraints, such as
is diagonal,
ATD-'h is diagonal, D = diag(a11,. . . , opPp),
where the diagonal elenients are in decreasing order. There are techniques,
such as the varirnax method, to rotate the factors to obtain a parsimonious
representation with few significantly non-zero loadings (Le. sparse matrix A).
As explained in , ICA (see Section 5) can be thought of as another fac-
tor rotation method, where the goal is to find rotations that maximize certain
independence criteria.
In many cases, a k-order factor model in (17) provides a better explanation
for the data than the alternative full covariance ~notlel Var(x) = C. In such
cases, it is possible to derive paranieter estimates A and 9.
Let X, R, and S denote the sample incan, covariance matrix, and correlation
matrix, respect,ively, of the observed data matrix X. Then, starting with
2 = AAT + *,
A?, + G,,.
TW) (lifferent possi1)ilities to (lcrivc. wt iniat ~s A ant^ & for t lie niotlel pa-
rainetrrs in (13)-(16) arc’ detailrd in Swtions 3.1 arid 3.2.
3.1 Principal factor analysis
Suppose thv (lata is standardizcd. so that its covariaxic.e matrix is cqual to the
correlation matrix. To obtain cstiniatcs A and 8 for the standardized variables,
of the niiiltipl[> (.orrelation corfficicnts of the it11 variable with all the ot,hrr
varial)lrs, ant1 t he largwt coi rclat ion corfficient l)t.tv,-een t he it h variable and
one of thr otliw varia1)lw Scst. form the ?~,diiced (‘07Td1Ltj071 .rricitrix R - \E,
wlitrc. the tliagonal c~lcrnc~Ilts
of 1 in R arc’ rt>pl;tc~1
tlir elrrnents it;’ = 1 - i~,,.
Then, clt~.ornpostl t lie rc~liic~~l
corrtht ion mat ris in t ernis of t hc> c+,tinvalucs
( 1 1 2 . . . 2 (iP and orthoriorni;tl oigcwwtors 2 ( l l . . . . . -,(p) as
first tstimatci i1;l for i = 1,. . . .p. c1oxIiI~loI1 estiniatw il; irirlu(1e the square
( ~ , - , ( i ) - , ( ~ )
If thti first k c~igcIiviiliiw arc’ positivth. wtiniatc thr i t h coliinin of A by
J(i) = ( I ,
i = 1,. . . .k.
Equivalently,
A = J?IA;”.
whtw rl = (-j(i), . . . . -,(k)), and AI = tliag(ul.. . . , ( [ A ) . The eigenvectors are
orthogonal, so the cwnstr’aint in (22) holds.
Finally, the specific variance estimates are updated as
The k-factor model is permissible if all the p terms in (29) are non-negative.
In practice, the number of factors may be determined by looking at the
eigenvalues ai of the reduced correlation matrix, and choosing k as the index
where there is a sharp drop in the eigenvalue magnitudes.
As its name suggests, principal factor analysis (PFA) is related to principal
component analysis. When the specific variances are all zero, Q = 0, comparing
Equations (17) and (26) to Section 2 indicates that PFA is equivalent to PCA.
3.2 Maximum likelihood factor analysis
If, in addition to the factor model specified in (13)-( 16), we also assume that the
factors f and u are distributed as multivariate normal variables, then parameters
of the model can also be estimated by maximizing the likelihood. In such cases,
one can also test the hypothesis that the k-factor model describes the data more
accurately than the unconstrained variance model.
The log-likelihood function can be written as
I = -- n logl2n~l- - n trX-'S,
and the goal is to maximize it with respect to the parameters A and \k, subject
to the constraint in (22) on A. Under the factor model, X = AnT + Q.
The optimization is carried out by noting that the function
F(A, Q) = F(A, Q; S) = trX-'S - loglX-'SI - p
is a linear function of the log-likelihood I, with a masimum in I corresponding
to a minimum in F. Also, in terms of the arithmetic mean a and the geometric
mean g of the eigenvalues of X-lS, we have
F = p(a - logg - 1).
Minimizing F(A, Q) proceeds in two stages: first, the minimization over A for
a fixed \k has an analytical solution, then, the minimization over \k is carried
out numerically.
4 Projection pursuit
Projection pursuit (PP) is a linear method that, unlike PCA and FA, can incor-
porate higher than second-order information, and thus is useful for non-Gaussian
datasets. It is more computationally intensive than second-order methods.
Given a projection index that defines the “i~iterestingncss” of a direction, PP
looks for the directions that optimize that index. .As the Gaussian distribution is
the least interesting distribution (having the least structure), projection indices
iisually measure some aspect of non-Gaiissianity.
If, however, one uses the
second-order Inaxirnum variance, subject that the projections be orthogonal,
as the projection index, PP yields the familiar PCX. IYriting the optimization
criterion as
Q(X,W) = I-ar{x‘w),
according to (7), the direction w1 of the first PC solves arg ~nasII,,lIIQ(x,w),
and the corresponding first PC is sl = xTw1.
A commonly used higher-ordrr projection index is based 011 the negative
Shannon entropy . Given the random variable x with probability distribution
j, its negative entropy is defined as
C2(X) = / f(x)logf(x)4x).
The Gaussian distribution rninirnizes this ~neasiire, so it, ~riakes sense to find
tlircct,ions w that rnasiniize the cmtropy of tlw projected data Q(x, w) with
respect to w, su1)jrc.t to liaving constant variance of xTw.
Ot hcr projection indices incliide indiccxs 1);isc~l on higher-ordcr ciimulants
and 011 the Fisher information [7. 221. Hou-wtr. all of these ni(’asiir(s depend on
tlie unknown protxi1)ility (1istriI)iition of X”W. wliich can \)e (Iifficult to rstirnat,e.
Xlt crnativr indicm based on aI)I)rosiinatioIis. ;iritl on diff(wnt mmiir(’s of non-
normality have also been proposed in thc literature .
Thc. Fast ICX algorithni for incltpcwdmt corripownts ill Stiction 5.3 can also
ustd to find projection piirsuit dirc\ct ions.
5 Independent component analysis
This stirtion is based on . ii rcwnt s u r v q on iridq)cwlcrit cmriponcnt analysis
(ICX). Alorv inforrriatioii (arid software) oil this ciiirciit Iy v c ~ y
popular nit~thotl
(mi t)t. founcl at various w4)sitw. incliitling [G. 24, 491. Books surrirnarizing thc.
r c w I i t adv-ancw in the throry and applic.ation of ICX inrliicle [l, 48, 15, 381.
ICA is a 1iiglic.r-orc1c.r rncthod that swks lincvir projt>ctions, not ncwssarily
ort hogorial to cw.h other. that i i w as ncvirly statist icxlly inclcpericlent as possi-
Statistical irid[.I)r.ndrnt.e is R niudi stroiigrr rondit ion than unc.orrt’latdriess.
IVliilt> the lat trr only involves tlir sc~o~itl-order
statist its. t liv fornicr depends 011
all thti 1iight.r-ordcr statistics. Forrrially, thc rantlorri vciri:il)lcs x = {SI, . . . , sP}
uric.orrt~latrd. if for Vi # j. 1 5 i , j 5 p. we h a v ~
Cov(s,..r,)
= E{(s, - p,)(s, - 1 1 , ) ) = E(s,.r,) - E(.r,)E(s,) = 0.
1x1 contrast, ixidrI)[’ntlenc.t~ rcyiiirc>s that the niu1tiv;iriattx pro1)at)ility density
fiiiiction factorizes. and (’an 1 ) ~
writ tcm as
Independence always implies uncorrelatedness, but not vice versa in general.
Only if the distribution f(q, . . . , zP) is multivariate normal, are the two equiv-
alent. For Gaussian distributions, the PCs are independent components. Fol-
lowing , the noise-free ICA model for the p-dimensional random vector x
seeks to estimate the components of the k-dimensional vector s and the p x k
full column rank mixing matrix A in (3),
(xi,. . . , z ~ ) ~
= Apxk(S1,. . . , ~
such that the components of s are as independent as possible, according to some
definition of independence. At least one of the hidden independent components
si has to be non-Gaussian to ensure the identifiability of the model . The
noisy ICA contains an additive random noise component,
but estimation of such models is still an open research issue . In this survey,
we only consider the noiseless model as specified in (37).
There are overcomplete versions of ICA, where the number k of ICs is larger
than the number of original variables p . In this paper, we will assume that
there are as many independent components as there are original variables, i.e.
k = p . In contrast with PCA, the goal of ICA is not necessarily dimension
reduction. To find k < p independent components, one needs to first reduce the
dimension of the original data p to k, by a method such as PCA.
As the problem is stated, there is no order among the ICs. Once they are
estimated, they can be ordered according to the norms of the columns of the
mixing matrix (similar to the ordering in PCA), or according to some non-
Gaussianity measure (similar to ordering in PP).
ICA can be considered a generalization of the PCA and the PP concepts.
While PCA seeks uncorrelated variables, ICA seeks independent variables. The
noise-free ICA is a special case of PP, with independence being the “interest-
ingness” in the projection pursuit index definition. The noisy ICA model is
equivalent to the FA model in (13) assuming non-Gaussian data.
ICA has been applied to many different problems, including exploratory data
analysis, blind source separation, blind deconvolution, and feature extraction.
In the feature extraction context, the columns of the matrix A represent features
in the data, and the components si give the coefficient of the ith feature in the
data. Several authors used ICA to extract meaningful features from natural
images .
Estimation of the model in (37) consists of two steps: specifying the ob-
jective function (also called the contrast, the loss function, the cost function),
and the algorithm to optimize the objective function. Objective functions can
be categorized into two groups: “multi-unit” contrast functions that estimate
all p independent components at once, and “one-unit” contrast functions that
estimate a single independent component at a time . They are detailed in
Section 5.1 and in Section 5.2, respectively. Section 5.3 lists several optimization
algorithms.
5.1 Multi-unit objective functions
There are many different ways to specify objective functions. This section lists
several possibilities. It has been shown, that despite their different formulations,
they all closely related, arid under certain conditions, sorne are equivalent .
Under certain conditions (the distribution of the independent components
is known with sufficient accuracy), the mutual information method is essen-
tially equivalent to maximum likelihood principle, and so is the non-linear PCA
method. Under the Sam’ conditions, cumulant-based methods are approxima-
tions to the mutual information.
Curnulant and general contrast-based methods, however, can be used for any
non-Gaussian data, without knon-ing the underlying distributions.
This method specifies the likelihood of the noisc-free ICX rnotlel, and uses the
Iriasirniim likelihood principle to estimate the pararneters. Under some condi-
tions, it is equivalerit t o the “infoniax” rietwork entropy maximization concept
iri the neural rietwork literatiire.
The adxintagcs of this method iriclutlc. the asymptotic cficiency of niaximuni
likelihood est ilnatcs unt1t.r rclgularity conditions. Howevc~, it, requires knowledge
of the tlistri1)ution of thc indepc~nderit conipon(wt.s, it is stnsitive to outliers, and
it is cornput;itionally iritensivc. which niakc it iindrsira1)le in many practical
sit iintioiis.
Maximum likelihood and network entropy
5.1.2 Mutual information and Kullback-Leibler divergence
llutiial inforinat ion I iii(xasiir(’s thtk tlcpcwdrnc~~
I I I rando~n variables g1
I(!/l.-.-.!/m)
= C“!/J -H(Y).
xli(~re H is thc tlifftmmtial c’ritropy. H(y) = -Q(y) in (31). Thc rnutual iri-
forrnation is always riori-ricgativv, arid is wro if and orily if tlic variables are
statistically iIidtycwlcnt. It t1ic.i (.fore makes smsc to find thr varia1)lcs that
rriininiizcl thv niut ual inforniat ion aniong the cwrriporicIit s.
l I u t iial inforni;ition is also qiia1 t o thr ~~iilll)ack-Lci~)lm
tlivw grncr
bc>twccri thrjoint clwisity f(y) ;ind the factoriztd j(y) = f~
(!/I) . . . f , l l ( y , l l ) . The
Iiiill1)ac.k-Ltii1)lc.r t1ivergc~ric.c. iii(’asiirw a .bclos(w(ss‘q of two distrihitions. If the
cwiriporicnts are iridcpend(~1it. tlic artiial dcnsity f(y) factorizes just likc f(y),
and results in zoro divcrgence.
lliitual informat ion is hard to estimate. iInposing dific.iilties on using it as
iiri objwtivc fiirict ion. A s suniniariztd in . wvckral api)rosiniations, 1)ased 011
polynomials, on higher-order cumulants, and on the maximum entropy principle,
have been proposed.
5.1.3 Non-linear cross-correlations
This principle is based on canceling non-linear cross-correlations of the form
E{gl(yj)g2(yj)}, where g1 and g 2 are non-linearities specified by the user. As-
suming that yi and y j are independent, such cross-correlations are zero. Of-
tentimes, there are no explicit objective functions associated with the chosen
cross-correlation, so that they are only implicitly specified.
5.1.4 Non-Linear PCA
This method indicates the strong connection between ICA and non-linear PCA.
By introducing non-linearities g based on the probability densities of the inde-
pendent components into the PCA objective function in (7), we obtain the ICA
w1 = arg max~~,=,~~var{g(x*w)).
As with the non-linear cross-correlation method, there might not exist explicit
contrast functions.
5.1.5 Higher-order cumulant tensors
The ICA model can also be estimated by solving for the eigenvectors of eigen-
matrices corresponding to the linear operator T defined by the fourth-order
cumulant as
T ( K ~ ~ )
= C c u m ( z , , z j , s r ; , s r ) ~ k r .
The linear operator T maps the space of k x k matrices to itself, and has k2
eigenvalues corresponding to eigenmatrices. This procedure does not need to
know the probability densities of the independent components, but suffers from
suboptimal statistical properties characteristic to cumulant-based estimators.
5.2 One-unit objective functions
One-unit contrast functions seek a single vector w such that the linear combi-
nation xTw is equal to one of the independent components si. It is desirable
when not all the PCs are needed, it can be used iteratively to find more PCs,
and it tends to result in computationally simple solutions.
The contrast functions in this section are closely related. Both cumulants
and general contrast functions can be used to approximate the negentropy. The.
objective function based on the kurtosis (fourth-order cumulant) is a special
case of general contrast functions.
5.2.1 Negentropy
Differential entropy is not invariant under scale transforrnations. The negen-
t,ropy, or negative normalized c>ritropy
J(Y) = H(Ygau.i,) - H(Y)
where H is the tlifferential entropy, H(y) = -Q(y) in (34): and ygauss is a
Gaussian random vector with the same covariance matrix as y, is a linearly
invariant version of the entropy. It is non-negative, arid zero if and only if y
is Gaussian. Finding the direction of masiniurn negentropy is equivalent to
finding the representation with minimum Inutual information. The directions
that rnasirnize the negentropy ( x i also be foiind by using differential entropy
as a projection index in PP.
Negcnt ropy is difficult to (1st irriat e. Xpprosirnat ions 1)ased on higher-order
curnulants are explaincd in 5.2.2. arid ones based on general contrast functions
5.2.2 Higher-order cumulants
01ic higher-or(1t.r cuinulant oft (’11 used as a ~~i(wsiire
of Iion-Gaussianity is the
fourth-order curnularit . also c.allrd tliv kurtosis. By cldinitiori. the kurtosis
of a ranclorxi variable .r is
kurt(.r) = E(.r’) - 3[E(.r2)]’.
The kurtosis is zero for a Gaussiari varial)l(>. it is positive for heavy-tailed super-
Gaussian (list ri1)utioiis. arid it is iityyitivc for light-tailrtl suh-Gaussian (listribti-
t ions. InclcpcwlcIit cwniponmt s (xii be tlerivcd hy Inasiniizing t hr rnocluliis of
tho kiirt osis.
C‘ii~riulant-l~;ts(’(1
cvtiiiiatoi s (’it11 1x1 poor in tcwns of rohistIirss and asyrnp-
tot ic vai ianw. They only c*oIisitl(ir tlic tails of tho distri1)ution. and art’ scnsitive
to outlicw.
5.2.3 General contrast functions
Iri contrast with thv conti ast fiiiict ions introtluc.c~d cwrlic.r, gmcml contmst func-
t ions a 1 (’ forrriulatcd to havc good statistic;il propcirtirs n-ithoiit requiring knowl-
edgr of thr distri1)ut ions, and to allow sirnple inttxrprctation and algorithmic ini-
plcmrrit ation. Such contrast furic.tioris J r~ieasiire riori-Gaussianity of t,lic stan-
tlartlized rando~n variahlt. !J by cwrnparing it to a st;imlartl Gaussian variable v
via ii sinooth Iiori-quadrat ic. ( x v ~ ~ i
fiirictiori G by
J<;(.Y) = IE,[G(!/)] - Ev[G(v)II”,
wlicr~ 1) is iisually taken to IN. 1 or 2. Taking G(y) = y’, .J<; is simply the
kurtosis. For siiitahle c-hoiws of G. siic*h as
G(y) = log c.osli(ril u ) or G(y) = c3p-<12u2/2),
with constants a1 , a2 2 1, estimators based on optimizing generalized contrast
functions have superior statistical properties than cumulant-based estimators.
Being the log-density of a super-Gaussian distribution, GI is related to maxi-
mum likelihood estimation.
5.3 Optimization algorithms
Most optimization algorithms either require that the data be sphered, or they
converge better for sphered data. Sphering is a linear transformation that maps
x into a new variable v with unit covariance matrix:
E(vvT) = I.
In terms of v, the ICA model in (37) can be written as
Assuming unit-variance independent components, we have I = E(vvT) = BE(ssT)BT =
BBT, and therefore B is orthogonal. The problem then translates to finding an
appropriate orthogonal matrix B from the sphered v. Once such a B is found,
the independent components are obtained via
Several algorithms have been proposed to estimate independent compo-
nents. As summarizes, there are two major types: adaptive and batch-mode
(block) algorithms.
Adaptive methods use stochastic gradient-type algorithms. Likelihood or
other multi-unit contrast functions are optimized using gradient ascent of the
objective function. One-unit implementations use straightforward stochastic
gradient methods that. optimize negentropy or approximations of it.
Examples of adaptive algorithms include the Jutten-Herault algorithm, which
is based on canceling non-linear cross-correlations and converges only under
harsh restrictions; other algorithms based on non-linear decorrelations that are
more stable and computationally tractable than the Jutten-Herault method; al-
gorithms for maximum likelihood estimation; non-linear PCA algorithms; neural
one-unit learning rules; and exploratory projection pursuit algorithms.
Batch-mode algorithms are much more computationally efficient than adap-
tive algorithms, and are more desirable in many practical situations where there
is no need for adaptation. The FastICA is such a batch-mode algorithm using
fixed-point iteration. It was introduced in using the kurtosis, but was
subsequently extended to general contrast functions in . A hIATLAB im-
plementation is available from . It can also be used for projection pursuit
analysis described in Section 4.
6 Non-linear principal component analysis
Non-linear PCA introduces non-linearity in the objective function, but the re-
sulting components are still linear combinations of the original variables. This
method can also be thought of as a special case of independent component anal-
ysis, Section 5.1.4. As indicated in , there are different formulations of the
non-linear PCA.
A non-linear PCX criterion for the data vector x = (TI,.
. . , s,,)‘ searches
for the components s = (SI, . . . , .sp)’ in the form of s = W‘x by miriiniizing
J(W) = E{\\x - Wg(W’x)((‘}
with respect, to the p x p weight rnatris W , where g(y) denotes the component-
wise application of the non-linear function g ( ) to the elements of the vector y.
Commonly used such Iion-linear functions are odd functions like g(y) = tanh(y)
The opt,irnization in (50) can be carried out (>ither hy the stochastic gradient
clrsceiit algorithm with the learning pararncter c and the 1 W update matrix of
w l)elo\v,
or by an approximate r(viirsiv(> least sqiiarrs (RLS) algorithm . The RLS
nicthod c-onvcrgcs faster t han the c.orrt.spoIi(liIig gradient clescent method. has
good final accuracy. but slightly highcir c.oniputationa1 load.
Brfore applying the algorithnis. the data needs t o he pre-w-hitcnrd by v =
Vx, wliere E{vv“} = I. By dcnotiiig
and g(g) = .lp.
I W = r[x - Wg(W’Tx)]g(x‘W),
y = W’v = W7’Vx = Bx.
-1w = r[v - Wg(y)]g(y’j.
thcx optimization in (51) ( x i lw u-ritteii a s
wlicw. after (wnvergenw. y c.ontains tlic soiiglit s vector.
Xssurning that t he (wnpon(wts of s d l 11;ivt~ vari;iric.c’ tyiial to one, the final
y (IstiInates are stan(inr(Iizc>(i to ~iavc. ~ { y y ’ } = I. rtmilting in the niatris W
1)cing ortliogonal ( ~ { y y ” }
= W’”E{VV“}W
= I). ~ r i d ~ i
this condition, it can
lw shown that
J(W) = E { ~ ~ v - W g ( W ’ r v ) ~ ~ 2 }
= E{IIY-dY)I12} = C E{[Y~-!I(!/~)I~}- (54)
A s indicated in Scctioii 8.5. proposcvl a nc~ural iic>twoik aic.hitrc*ture with
iioIi-liricar activation finict ions in t hci hiddm layus to cwt i m i t e non-liIirar PCAs.
7 Random projections
The‘ nirt hod of random projtvt ions is ii siniplr ycit powc’rfiil dirnrnsion rcdiiction
tc~linique that iisw ramlorn projection niatriccs to projrct the data into lower
dim.iisiona1 spaces [47. 32. 33. 351. The. original data X E 72” is transformed to
thc lower ciiriic~nsional S E xk. with k << p. via
where the columns of R are realizations of independent and identically dis-
tributed (i.i.d.) zero-mean normal variables, scaled to have unit length. The
method was proposed in the context of clustering text documents, where the
initial dimension p can be on the order of 6000, and the final dimension IC is still
relatively large, on the order of 100. Under such circumstances, even PCA, the
simplest alternative linear dimension reduction technique, can be computation-
ally too expensive. Random projections are applied as a data pre-processing
step, then, the resulting lower dimensional data is clustered. It has been shown
empirically that results with the random projection method are comparable
with results obtained with PCA, and take a fraction of the time PCA requires
 . To reduce the computational burden of the random projection method,
at a slight loss in accuracy, the random normal projection matrix R may be re-
placed by thresholding its values to -1 and +1, or by matrices whose rows have
a fixed number of Is (at random locations) and the rest Os .
If the similarity between two vectors is measured by their inner product
(giving the cosine of their angle for unit-length vectors), showed that if the
dimension of the reduced space d is large, random projection matrices preserve
the similarity measure: on the average, the distortion of the inner products is
zero, and its variance is at most the inverse of twice d.
8 Non-linear methods and extensions
8.1 Non-linear independent component analysis
Non-linear methods, such as principal curves, self organizing maps and topo-
graphic maps, can, in principle, be incorporated into ICA.
Given the pdimensional zero-mean non-Gaussian variable x, the non-linear
ICA model replaces the linear transformation in (3) by
(21,. . . , ZP)T = f ( S 1 , . . . , s k y ,
where f is an unknown real-valued pcomponent vector function. In general,
The identifiability and other properties of the general non-linear ICA model
makes its estimation difficult. A few publications considering special cases are
mentioned in . An overview of the problem, along with a maximum likeli-
hood and a Bayesian ensemble learning method for estimation can be found in
8.2 Principal curves
Principal curves are smooth curves that pass through the “middle” of multi-
dimensional data sets [18, 40, 71. Linear principal curves are in fact principal
components, while non-linear principal curves generalize the concept.
Given the pdimensional random vector y = (y1,. . . , yp) with density g(y),
and the smooth curve f(s) = (fl (A), . . . , fp(X)) E R p parameterized by the real-
valued A, define the projection index &(y) to be value of X corresponding to the
point on f(s) that is closest (in Euclidean distance) to y. The set of principal
curves is defined in [lS] as the curves that (lo not intersect themselves and are
self-consistent. with respect to the data. By definition. a curve is self-consistent
if each point f ( X ) is the mean of all points in the support of g that are projected
It was shown in [lS] that a curve f is a principal ciirve if and only if it solves
E[ylXf(y) = XI = f(X).
where y, is tlie ith instance of the ydirnensional vector, and the composition
of funct,ions f (Xf(y,) gives the ydiniensional coordinatcs of the projection of yi
onto thc ciirve f.
To estimate f and A, [lS] proposed an iterative algorithni. It starts with
f(X) = E(y) + dX, where d is the first eigrnvrc-tor of the covariance matrix of
y and X = Xf(y). Then it iterates thr two steps
1. For a fixed A. niiniiriize Elly - f(X)ll‘ by setting f,(X) = E(!j,IXf(y) = A)
for (w.h j
2. Fix f arid set X = Xf(y) for cw.h y
until the change in Elly - fxl]’’ is lrss than a thrc&ol(l.
An alternative formulation of thc principal (’iirws rnc~tliod. along with a
gcmcmlizcd ELI algorithni for its wtiriiation untlt~ Gaussian distrihtion of g ( ) ,
is prescwtttl in .
In grri~ral. for tht. niodel y = f ( X ) + E . wlit~rt~
f is srnootli and E(€) = 0,
f is not iie( (warily a pi-incipd ciirv~. Eswpt for a fmv spwial cascs, it is not
k~iown in grric~al for what typv of distributions (lo print ipa1 ciirvm exist, how
inmy principal mrvcs thcre arc. and what thrir propc’rtiw i.irc .
Thv c.onc.q)t of prinripal ciirv(~s
riin 1)c t>st cwd(~1
to 1iiglic.r dinicwsional prin-
ciplt. siirfaws. Init t hri est iiiiat ion procrdiirc~ gvt s inor(’ winplicat wl.
8.3 Multidimensional scaling
Givm T I itciiis in <i yclirntwsional space and a11 I I x I I matrix of proximity nic’a-
siires among t he it ~nis,
rriiiltitlirnc.nsio~i~i1 scaling (LIDS) pi odiicrs a k-dinit~nsional,
k 5 p. icI)rcsciIitatioIi of the itcins such that thv clistanc.cs miorig the point,s in
spac~’ reflrrt thc prosinlitits in the. (lata [S, 7. -111. The proximity
niwsiirw the ((1is)similarities miong tlic itenis. and in gcmwal, it is a distance
ni(>asiir(’: the inore similar two it rnis are. thc. srria1lt.r t heir (listanre is. Popular
clistanrr nieasiires iricliitlc thc. Euclidtwi (list iiiic~ (L’ iior111). tlie niarihattan
c1istanc.r (L1. a1)soliite norm). and tht niasirnurri ~ioini. Rcsults of &IDS are
inclctc’r~ninatc with rcyrct to translation. rotat ion, aiid rrflt~ctiori.
AIDS has bcwi typically uscd to transform the data into two or three tli-
~iictiisio~is.
arid visualizing the rcwilt to ii~icov~r
lii(l(1cvi structure in the data. .A
rule of thumb to determine the maximum number of k, is to ensure that there
are at least twice as many pairs of items then the number of parameters to be
estimated, resulting in p 2 4k + 1 .
Given the n items {xi};==, E RP and a symmetric distance matrix A =
{&j, i, j = 1, . . . , n}, the result of a k-dimensional MDS will be the set of points
{yi};=, E Rk
such that the distances dij = d(yi,yj) are as close as possible to
a function f of the corresponding proximities f(&j).
In , MDS methods that incorporate the given distances &j into their
calculations are called metric methods, while the ones that only use the rank
ordering of the distances are called non-metric methods. In contrast, states
that depending on whether f is linear or non-linear, MDS is called either metric
or non-metric, correspondingly.
Following , the steps for the most general estimation procedure are as
follows. First, define the stress as an objective function to be minimized by f
Ci,j[f(Sij) - dij12
scale factor
stressf(A, X, f) =
where the scale factor is usually based on Ci,j[f(Sij)]' or on
a given X = {xi}:==,, find f that minimizes (59),
dij. Nest, for
stress(A, X, f) = minj stressj(A, X,
stress(A, X, f) = minx stress(A, X, f).
then determine the optimal X by
The special case of using Euclidean distance and f as the identity in (59)
leads to the principal coordinates of X in k dimensions as the solution, which
are equivalent to the first k principal components of X (without re-scaling to
correlations) [-Ill.
An alternative to &IDS is FastMap , a computationally efficient algo-
rithm that maps high-dimensional data into lower-dimensional spaces, while
preserving distances between objects.
8.4 Topologically continuous maps
There are several methods based on finding a continuous map to transform a
high-dimensional data into a lower-dimensional lattice (latent) space of fised
dimension . Such techniques could be called self-organizing maps, but that
name is most often associated with one particular such method, namely, Koho-
nen's self-organizing maps. To avoid confusion, we follow the review , and
refer to these methods collectively as methods that use topologically continuous
8.4.1 Kohonen’s self-organizing maps
Given the data vector { tn}f=, E R”,
Kohonen’s self-organizing maps (KSOhI)
E361 learn, in an unsupervised way, a map hetween the data space and a 2-
dirnensional lattice. The method can be estentled to L-dimensional topological
arrangement s as well. Let df, and dn denote distances (typically Euclidean)
in the lattice and in the data space, respectively, and define a neighborhood
fiirictiori h,, on t,hr lattice space, siich that it is symmetric, has values in the
[O, 11 interval, h,, = 1 for any node i in the lattice, and the further node j is from
i in thc lattice, thc srrialler IL,, is. The neighborhood of node i consists of the
iod des for which h ,, greater than a threshold. Typical neighborhood function is
11,) = tisp( -dfA(i, j ) / 2 / 0 2 ) , whcw n specifies the range of the neighborhood.
Kohonen’s rule uses an initially random set of reference vectors { p, };El
in the data space 72”. then updatcs them iteratively according to the data
distri\)ution such that the final refercwce vector will be drnse in regions of R”
where t lie data is comnion. Kohontw’s rule iterates the following procedure over
all thc data points until (.o~iv(~rgeriw
0 For a given data t,,, find the dosest, vector p,. to it in the lattiw space:
i’ = arg I l l a S , E , ~ t t i ( r ~ ~ l l ) ( p ~ .
0 Thc~i, ;it iteration f arid lrarriing rat(’ o(‘) E [O. 11. upclatt> thr rc>ft.rexice
vwtor by nioving it a t1istwnc.c p = ( k ( ‘ ) h , . ,
towards t:
p y v = p,
old + o(‘)I,;!;(t,,
- ppI‘1) = (1 - p)pp’(l + pt,,.
Altlioiigh IiSOlIs ar(’ useful in many applications. t h y h a v ~
s(3veral draw-
I)iicks: tlirre is 110 implirit crit(>iid that t h y tiy to optiniize. thcw are no rules
to optinially s c k t o(‘) ;inti ] I ( ‘ ) . arid thew is 110 proof that, t h y cwnvergc. in
gc~rlc~ral.
8.4.2 Density networks
Density wtwoiks assiinii’ it proh1)ility t1istril)utiori for thc data givcvi the pa-
rmiet(’is, as ~ w l l
as prioi clistrilmtioris foi thr param~ttn. thcri apply Baycsian
lwrning tcdiriiqiic~s to riiockl tht’ data in tc’rnis of latcrit varialh.
Gr~ic’r atiw topographic mapping (GTlI) is a spc>c.ial dmsity rictwork 1)asrd
c.onstraincd Gaussian niisturcs that iism the c.si,ec.tation-rnasilliizatiori (Ehl)
algorithm to cxstirriate thci paraiiirttrs by Iriasiriiizing the likelihood fiinction. It
was introdiic.ec1 in . arid, iinlikti the IiSOlIs in Srction 8.4.1. it provides a
rigorous trcatnicnt of SOlIs uiitlcr wrtain assumptions.
Neural networks
iYcvira1 networks (NSs) nioclc~l the stit of output variables { !j, };=,
in terms of
t h c k inpiit vxia1)lcs x = { J - , } ~ = ~
! / , ( x - w ) 3
where the functions y j (x,
w) specify the network architecture, and the weights w
are determined by training (learning) the NN using a set of known examples and
an error function . Many, traditional and more recent, linear and non-linear,
dimension reduction techniques can be implemented using neural networks with
different architectures and learning algorithms [2, 46, 40, 51, 71.
The simplest NN has three layers: the input layer, one hidden (bottleneck)
layer, and the output layer. First, to obtain the data at node h of the hidden
layer, the inputs x i are combined through weights Wih along with a thresh-
old (bias) term ah, then they are passed through the corresponding activation
function f j h . In the second step, the output is obtained in a similar way from
the data on the hidden nodes, using the weights W h j , the threshold a j , and a
possibly different output function do:
The first part of the network reduces the input data into a lower-dimensional
space, while the second decodes the reduced data into the original domain.
Frequently used activation and output functions include the linear (identity)
function, sigmoidal (S-shaped) functions such as the logistic function, and the
Heaviside thresholding function [2, 531. NNs with a single hidden layer net-
works and the threshold activation function are also called perceptrons. Net-
works that try to learn the identity mapping, i.e. the outputs y j are identical
to the inputs zi, are called auto-associative (auto-encoders, bottlenecks, p-k-p
networks). Hetero-associative neura,l nets have different number of input- and
output layers, and are used, for example, in classification.
As summarized in , there are many types of NN architectures that can
extract principal components. hilore complete details can be found in . For ex-
ample, a linear, one hidden layer auto-associative perceptron with p input units,
k < p hidden units, and p output units, can be trained with back-propagation to
find a basis of the subspace spanned by the first k PCs, if the error metric used
is the minimum squared sum of differences between the input and the output
units. Other networks, based on Oja's rule and various de-correlating devices
can also be used to find principal components.
By adding two more hidden layers with nonlinear activation functions, one
between the input and the bottleneck, the other between the bottleneck and the
output layer, the PCA network can be generalized to obtain non-linear principal
components. Starting from the feed-forward neural network implementation of
PCA [40, 71, extended the idea to include non-linear activation functions
in the hidden layers. In this framework, the non-linear PCA network can be
thought of as an auto-associative neural network with five layers: input (l),
hidden (2), bottleneck (3), hidden (4), and output (5). If Xf : R p + Rk
the function modeled by layers (l), (2), and, (3), and f : Rk + R p denotes
the function modeled by layers (3), (4), and (5), it can be shown [do] that the
weights of the non-linear PCA network are determined such that
I IX, - f(~f(x,)
where x2 denotes the ith instance of the pdirnensional vector x. Note the close
connection to principal surfaces (58) in Section 8.2. Both lead to PCA in case
of linear sf and f.
The thesis of [5 11 compares principle component analysis, vector quantiza-
tion, and five layer neural networks, for reducing the dimension of iniages. It also
provides a C software package called NeuralCam iniplcrrierit,ing those methods.
8.6 Vector quantization
A s explained in [;i 11, introthiced a hybrid non-linear tlinierision reduction
t diniqiie 1)asc~l on cmInbiniIig vector quantization for first clustering t he data,
then applying loc.al PCX on the resulting I-oronoi cell clusters. On the im-
age' data set iisctl in . tiotli non-linear techIiiqiies (vector quantization, VQ,
and non-linear PCA using five layer riciiral network irnplenicntation, NLPCX)
out pc3rforrncd t he linear PCX. Xrnong tlir Iion-linear trc.hniqiic.s, \‘Q acliicwd
brtt,rr rrsults than SLPCA.
8.7 Genetic and evolutionary algorithms
Grwet ic and cw)lutioIiary algorithIns (GEXs) are opt iriiization tcchicliiw basc~l
on Darwinian tliwry of clvolution that iisc natural sclwtiori arid genetics to find
tlic bcwt solutioii a~iio~ig
rncwiI)t.1rs of a cwrnpt.ting popiilatiou [ 161. Thcw are
ni~riy refw(wws dwc.ril)irig how GEXs can IF iisod in tliriimsioIi I (duct ion. In
ws(ww. givm ii .sot of ciindidatv solutions. iiii ol)jrc.tivc> fiirictiori to (>valuate t hc.
fit iiws of cantlidatcxs. and the valiics for thv paraiiict (w of t lit> c.lioscw algorithni,
GEXs scwrli t lit c.aiiditl;ite spac~x for tlic iriwi1)ci- with t liv optimil fitwss. For
cxarnpl(~. lis(’ GXs iri c.orril)i~iatiori x-itli a k-iicvirtst nc4glil)or (kiln) c.1assifit.r
to r ~ l i i w
tlir clirricwsion of a fwtiirc st>t : starting with a poplation of rantlorn
transfoririatioxi iriatiices { W k x p } ( ’ ) . tlicy i i s ~
GXs to firid tlir traIisforinatioxi
W k ,’ sudi that t hc. kriii classifier using the I ~ ( Y ftiat i i r ~ s
SI. ,, = W k ,,X, ,,
rlassifit’s tlir training data niost accw-atdy.
8.8 Regression
Rtyqwsiori riict hods (’an be used for diiricwsiori rrc1iic.t ion whcn t hc. goal is to
niodrl a rcspoiisc’ variahlc y in tcvris of ii set of x, varia1)lcs. The regression
fiiriction (‘an be liwar. or non-lincwr. TraditioIially. the. x, varia1)lcs have 1)een
cdctl t 1 1 t h iridq)cwdmt. or c.splaiiat ory variables in st at ist i c y wliik y was t,he
rwponscx, or the dtycndwt varia1)lo. In this rcypssiori cwritcst,, it, is gc~ierally
assunlcYl that tlw XIS \V(W’ cart~flllly seltY.tec1, unc.orrclattd, axid rclrvant to ex-
plaining tlir variation in y . In c.urrrIit (lata iriiriirig applications, however, those
assumptions rarely hold. Variable selection, or dimension reduction, is therefore
needed for such cases.
A well-known statistical variable selection method is step-wise regression,
where different models are fit using different subsets of the explanatory variables.
The results are then compared by calculating various goodness-of-fit measures,
and the subset with the best measure is chosen as the explanatory variables with
the reduced dimension. A similar approach, selecting the most relevant features
by evaluating random subsets of the original features, is called the wrapper
method in the machine learning community .
Dimension reduction methods related to regression include projection pur-
suit regression [20, 71, generalized linear [42, 101 and additive models, neural
network models, and sliced inverse regression and principal hessian directions
In this paper, we described several dimension reduction methods.