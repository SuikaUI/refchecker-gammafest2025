Contemporary Physics, Vol. 00, No. 00, Month–Month 2006, 1–41
Bayes in the sky:
Bayesian inference and model selection in cosmology
Roberto Trotta∗
Oxford University, Astrophysics Department
Denys Wilkinson Building, Keble Rd, Oxford, OX1 3RH, UK
 
The application of Bayesian methods in cosmology and astrophysics has ﬂourished over the past decade, spurred
by data sets of increasing size and complexity. In many respects, Bayesian methods have proven to be vastly superior
to more traditional statistical tools, oﬀering the advantage of higher eﬃciency and of a consistent conceptual basis
for dealing with the problem of induction in the presence of uncertainty. This trend is likely to continue in the future,
when the way we collect, manipulate and analyse observations and compare them with theoretical models will assume
an even more central role in cosmology.
This review is an introduction to Bayesian methods in cosmology and astrophysics and recent results in the ﬁeld.
I ﬁrst present Bayesian probability theory and its conceptual underpinnings, Bayes’ Theorem and the role of priors.
I discuss the problem of parameter inference and its general solution, along with numerical techniques such as Monte
Carlo Markov Chain methods. I then review the theory and application of Bayesian model comparison, discussing
the notions of Bayesian evidence and eﬀective model complexity, and how to compute and interpret those quantities.
Recent developments in cosmological parameter extraction and Bayesian cosmological model building are summarized,
highlighting the challenges that lie ahead.
Keywords: Bayesian methods; model comparison; cosmology; parameter inference; data analysis; statistical methods.
Introduction
At ﬁrst glance, it might appear surprising that a trivial mathematical result obtained by an obscure
minister over 200 hundred years ago ought still to excite so much interest across so many disciplines,
from econometrics to biostatistics, from ﬁnancial risk analysis to cosmology. Published posthumously
thanks to Richard Price in 1763, “An essay towards solving a problem in the doctrine of chances” by the
rev. Thomas Bayes (1701(?)–1761) had nothing in it that could herald the growing importance and
enormous domain of application that the subject of Bayesian probability theory would acquire more than
two centuries afterwards. However, upon reﬂection there is a very good reason why Bayesian methods
are undoubtedly on the rise in this particular historical epoch: the exponential increase in computational
power of the last few decades made massive numerical inference feasible for the ﬁrst time, thus opening
the door to the exploitation of the power and ﬂexibility of a rich set of Bayesian tools. Thanks to fast and
cheap computing machines, previously unsolvable inference problems became tractable, and algorithms for
numerical simulation ﬂourished almost overnight.
Historically, the connections between physics and Bayesian statistics have always been very strong.
Many ideas were developed because of related physical problems, and physicists made several distinguished
contributions. One has only to think of people like Laplace, Bernouilli, Gauss, Metropolis, Jeﬀreys, etc.
Cosmology is perhaps among the latest disciplines to have embraced Bayesian methods, a development
mainly driven by the data explosion of the last decade, as Figure 1 indicates. However, motivated by
diﬃcult and computationally intensive inference problems, cosmologists are increasingly coming up with
new solutions that add to the richness of a growing Bayesian literature.
∗Email: 
Contemporary Physics
ISSN 0010-7514 print/ISSN 1366-5812 online c⃝2006 Taylor & Francis
 
DOI: 10.1080/0010751YYxxxxxxxx
Roberto Trotta
Some cosmologists are sceptic regarding the usefulness of employing more advanced statistical methods,
perhaps because they think with Mark Twain that there are “lies, damned lies and statistics”. One
argument that is often heard is that there is no point in bothering too much about reﬁned statistical
analyses, as better data will in the future resolve the question one way or another, be it the nature of
dark energy or the initial conditions of the Universe. I strongly disagree with this view, and would instead
argue that sophisticated statistical tools will be increasingly central for modern cosmology. This opinion
is motivated by the following reasons:
(i) The complexity of the modelling of both our theories and observations will always increase, thus
requiring correspondingly more reﬁned statistical and data analysis skills. In fact, the scientiﬁc return
of the next generation of surveys will be limited by the level of sophistication and eﬃciency of our
inference tools.
(ii) The discovery zone for new physics is when a potentially new eﬀect is seen at the 3–4 σ level. This is
when tantalizing suggestion for an eﬀect starts to accumulate but there is no ﬁrm evidence yet. In this
potential discovery region a careful application of statistics can make the diﬀerence between claiming
or missing a new discovery.
(iii) If you are a theoretician, you do not want to waste your time trying to explain an eﬀect that is not
there in the ﬁrst place. A better appreciation of the interpretation of statistical statements might help
in identifying robust claims from spurious ones.
(iv) Limited resources mean that we need to focus our eﬀorts on the most promising avenues. Experiment
forecast and optimization will increasingly become prominent as we need to use all of our current
knowledge (and the associated uncertainty) to identify the observations and strategies that are likely
to give the highest scientiﬁc return in a given ﬁeld.
(v) Sometimes there will be no better data! This is the case for the many problems associated with cosmic
variance limited measurements on large scales, for example in the cosmic background radiation, where
the small number of independent directions on the sky makes it impossible to reduce the error below
a certain level.
This review focuses on Bayesian methodologies and related issues, presenting some illustrative results
where appropriate and reviewing the current state–of–the art of Bayesian methods in cosmology. The
emphasis is on the innovative character of Bayesian tools. The level is introductory, pitched for graduate
students who are approaching the ﬁeld for the ﬁrst time, aiming at bridging the gap between basic textbook
examples and application to current research. In the last sections we present some more advanced material
that we hope might be useful for the seasoned practitioner, too. A basic understanding of cosmology and
of the interplay between theory and cosmological observations (at the level of the introductory chapters
in ) is assumed. A full list of references is provided as a comprehensive guidance to relevant literature
across disciplines.
This paper is organized in two main parts. The ﬁrst part, sections 2–4, focuses on probability theory,
methodological issues and Bayesian methods generally. In section 2 we present the fundamental distinction
between probability as frequency or as degree of belief, we introduce Bayes’ Theorem and discuss the
meaning and role of priors in Bayesian theory. Section 3 is devoted to Bayesian parameter inference and
related issues in parameter extraction. Section 4 deals with the topic of Bayesian model comparison from
a conceptual and technical point of view, covering Occam’s razor principle, its practical implementation in
the form of the Bayesian evidence, the eﬀective number of model parameters and information criteria for
approximate model comparison. The second part presents applications to cosmological parameter inference
and related topics (section 5) and to Bayesian cosmological model building (section 6), including multi–
model inference and model comparison forecasting. Section 7 gives our conclusions.
Bayesian probability theory
In this section we introduce the basic concepts and the notation we employ. After a discussion of what
probability is, we turn to the central formula for Bayesian inference, namely Bayes theorem. The whole of
Bayes in the sky
Number of Bayesian papers in cosmology and astrophysics
All papers (incl. conference proceedings)
Journal articles only
The evolution of the B–word: number of articles in astronomy and cosmology with “Bayesian” in the title, as a function of
publication year. The number of papers employing one form or another of Bayesian methods is of course much larger than that. Up
until about 1995, Bayesian papers were concerned mostly with image reconstruction techniques, while in subsequent years the domain
of application grew to include signal processing, parameter extraction, object detection, cosmological model building, decision theory
and experiment optimization, and much more. It appears that interest in Bayesian statistics began growing around 2002 (source:
NASA/ADS).
Bayesian inference follows from this extremely simple cornerstone. We then present some views about the
meaning of priors and their role in Bayesian theory, an issue which has always been (wrongly) considered
a weak point of Bayesian statistics.
There are many excellent textbooks on Bayesian statistics: the works by Sir Harold Jeﬀreys and
Bruno de Finetti are classics, while an excellent modern introduction with an extensive reading list
is given by . A good textbook is . Worth reading as a source of inspiration is the though–provoking
monograph by E.T. Jaynes . Computational aspects are treated in , while MacKay has a quite
unconventional but inspiring choice of topics with many useful exercices. Two very good textbooks on
the subject written by physicists are . A nice introductory review aimed at physicists is (see
also ). Tom Loredo has some masterfully written introductory material, too . A good source
expanding on many of the topics covered here is Ref. .
What is probability?
Probability as frequency. The classical approach to statistics deﬁnes the probability of an event
“the number of times the event occurs over the total number of trials, in the limit of an inﬁnite series of
equiprobable repetitions.”
This is the so–called frequentist school of thought. This deﬁnition of probability is however unsatisfactory
in many respects.
(i) Strikingly, this deﬁnition of probability in terms of relative frequency of outcomes is circular, i.e. it
assumes that repeated trials have the same probability of outcomes – but it was the the very notion
of probability that we were trying to deﬁne in the ﬁrst place!
(ii) It cannot handle with unrepeatable situations, such as the probability that I will be overrun by a
Roberto Trotta
car when crossing the street, or, in the cosmological context, questions concerning the properties of
the observable Universe as a whole, of which we have exactly one sample. Indeed, perfectly legitimate
questions such as “what is the probability that it was raining in Oxford when William I was crowned?”
cannot even be formulated in classical statistics.
(iii) The deﬁnition only holds exactly for an inﬁnite sequence of repetitions. In practice we always handle
with a ﬁnite number of measurements, sometimes with actually only a very small number of them. How
can we assess when “how many repetitions” are suﬃcient? And what shall we do when we have only
a handful of repetitions? Frequentist statistics does not say, except sometimes devising complicated
ad-hockeries to correct for “small sample size” eﬀects. In practice, physicists tend to forget about the
“inﬁnite series” requirement and use this deﬁnitions and the results that go with it (for example, about
asymptotic distributions of test statistics) for whatever number of samples they happen to be working
Another, more subtle aspects has to do with the notion of “randomness”. Restricting ourselves to classical
(non–chaotic) physical systems for now, let us consider the paradigmatic example of a series of coin tosses.
From an observed sequence of heads and tails we would like to come up with a statistical statement about
the fairness of the coin, which is deemed to be “fair” if the probability of getting heads is pH = 0.5.
At ﬁrst sight, it might appear plausible that the task is to determine whether the coin possesses some
physical property (for example, a tensor of intertia symmetric about the plane of the coin) that will
ensure that the outcome is indiﬀerent with respect to the interchange of heads and tails. As forcefully
argued by Jaynes , however, the probability of the outcome of a sequence of tosses has nothing to
do with the physical properties of the coin being tested! In fact, a skilled coin–tosser (or a purpose–built
machine, see ) can inﬂuence the outcome quite independently of whether the coin is well–balanced (i.e.,
symmetric) or heavily loaded. The key to the outcome is in fact the deﬁnition of random toss. In a loose,
intuitive fashion, we sense that a carefully controlled toss, say in which we are able to set quite precisely
the spin and speed of the coin, will spoil the “randomness” of the experiment — in fact, we might well call
it “cheating”. However, lacking a precise operational deﬁnition of what a “random toss” means, we cannot
meaningfully talk of the probability of getting heads as of a physical property of the coin itself. It appears
that the outcome depends on our state of knowledge about the initial conditions of the system (angular
momentum and velocity of the toss): an lack of precise information about the initial conditions results in
a state of knowledge of indiﬀerence about the possible outcome with respect to the speciﬁcation of heads
or tails. If however we insist on deﬁning probability in terms of the outcome of random experiments, we
immediately get locked up in a circularity when we try to specify what “random” means. For example,
one could say that
“a random toss is one for which the sequence of heads and tails is compatible with assuming the hypothesis
pH = 0.5”.
But the latter statement is exactly what we were trying to test in the ﬁrst place by using a sequence of
random tosses! We are back to the problem of circular deﬁnition we highlighted above.
Probability as degree of belief. Many of the limitations above can be avoided and paradoxes
resolved by taking a Bayesian stance about probabilities. The Bayesian viewpoint is based on the simple
and intuitive tenet that
“probability is a measure of the degree of belief about a proposition”.
It is immediately clear that this deﬁnition of probability applies to any event, regardless whether we are
considering repeated experiments (e.g., what is the probability of obtaining 10 heads in as many tosses of
a coin?) or one–oﬀsituations (e.g., what is the probability that it will rain tomorrow?). Another advantage
is that it deals with uncertainty independently of its origin, i.e. there is no distinction between “statistical
uncertainty” coming from the ﬁnite precision of the measurement apparatus and the associated random
noise and “systematic uncertainty”, deriving from deterministic eﬀects that are only partially known
(e.g., calibration uncertainty of a detector). From the coin tossing example above we learn that it makes
good sense to think of probability as a state of knowledge in presence of partial information and that
Bayes in the sky
“randomness” is really a consequence of our lack of information about the exact conditions of the system
(if we knew the precise way the coin is ﬂipped we could predict the outcome of any toss with certainty.
The case of quantum probabilities is discussed below). The rule for manipulating states of belief is given
by Bayes’ Theorem, which is introduced in Eq. (5) below.
It seems to us that the above arguments strongly favour the Bayesian view of probability (a more
detailed discussion can be found in ). Ultimately, as physicists we might as well take the pragmatic
view that the approach that yields demonstrably superior results ought to be preferred. In many real–life
cases, there are several good reasons to prefer a Bayesian viewpoint:
(i) Classic frequentist methods are often based on asymptotic properties of estimators. Only a handful of
cases exist that are simple enough to be amenable to analytic treatment (in physical problems one most
often encounters the Normal and the Poisson distribution). Often, methods based on such distributions
are employed not because they accurately describe the problem at hand, but because of the lack of
better tools. This can lead to serious mistakes. Bayesian inference is not concerned by such problems:
it can be shown that application of Bayes’ Theorem recovers frequentist results (in the long run) for
cases simple enough where such results exist, while remaining applicable to questions that cannot even
be asked in a frequentist context.
(ii) Bayesian inference deals eﬀortlessly with nuisance parameters. Those are parameters that have an in-
ﬂuence on the data but are of no interest for us. For example, a problem commonly encountered in
astrophysics is the estimation of a signal in the presence of a background rate (see ). The
particles of interest might be photons, neutrinos or cosmic rays. Measurements of the source s must
account for uncertainty in the background, described by a nuisance parameter b. The Bayesian procedure is straightforward: infer the joint probability of s and b and then integrate over the uninteresting
nuisance parameter b (“marginalization”, see Eq. (16)). Frequentist methods oﬀer no simple way of
dealing with nuisance parameters (the very name derives from the diﬃculty of accounting for them in
classical statistics). However neglecting nuisance parameters or ﬁxing them to their best–ﬁt value can
result in a very serious underestimation of the uncertainty on the parameters of interest (see for
an example involving galaxy evolution models).
(iii) In many situations prior information is highly relevant and omitting it would result in seriously wrong
inferences. The simplest case is when the parameters of interest have a physical meaning that restricts
their possible values: masses, count rates, power and light intensity are examples of quantities that
must be positive. Frequentist procedures based only on the likelihood can give best–ﬁt estimates that
are negative, and hence meaningless, unless special care is taken (for example, constrained likelihood
methods). This often happens in the regime of small counts or low signal to noise. The use of Bayes’
Theorem ensures that relevant prior information is accounted for in the ﬁnal inference and that physically meaningless results are weeded out from the beginning.
(iv) Bayesian statistics only deals with the data that were actually observed, while frequentist methods focus
on the distribution of possible data that have not been obtained. As a consequence, frequentist results
can depend on what the experimenter thinks about the probability of data that have not been observed.
(this is called the “stopping rule” problem). This state of aﬀairs is obviously absurd. Our inferences
should not depend on the probability of what could have happened but should be conditional on
whatever has actually occurred. This is built into Bayesian methods from the beginning since inferences
are by construction conditional on the observed data.
However one looks at the question, it is fair to say that the debate among statisticians is far from settled
(for a discussion geared for physicists, see ). Louis Lyons neatly summarized the state of aﬀairs by
saying that 
“Bayesians address the question everyone is interested in by using assumptions no–one believes, while frequentists use impeccable logic to deal with an issue of no interest to anyone”.
Roberto Trotta
Quantum probability. Ultimately the quantum nature of the microscopic world ensures that the
fundamental meaning of probability is to be found in a theory of quantum measurement. The fundamental
problem is how to make sense of the indeterminism brought about when the wavefunction collapses into
one or other of the eigenstates being measured. The classic textbook view is that the probability of each
outcome is given by the square of the amplitude, the so–called “Born rule”. However the question remains
— where do quantum probabilities come from?
Recent developments have scrutinized the scenario of the Everettian many–worlds interpretation of
quantum mechanics, in which the collapse never happens but rather the world “splits” in disconnected
“branches” every time a quantum measurement is performed. Although all outcomes actually occur, uncertainty comes into the picture because an observer is unsure about which outcome will occur in her branch.
David Deutsch suggested to consider the problem from a decision–theoretic point of view. He proposed
to consider a formal system in which the probabilities (“weights”) to be assigned to each quantum branch
are expressed in terms of the preferences of a rational agent playing a quantum game. Each outcome of
the game (i.e., weights assignment) has an associated utility function that determines the payoﬀof the
rational agent. Being rational, the agent will act to maximise the expectation value of her utility. The
claim is that it is possible to ﬁnd a simple and plausible set or rules for the quantum game that allow
to derive unique outcomes (i.e., probability assignments) in agreement with the Born rule, independently
of the utility function chosen, i.e. the payoﬀs. In such a way, one obtains a bridge between subjective
probabilities (the weight assignment by the rational agent) and quantum chance (the weights attached to
the branches according to the Born rule).
This approach to the problem of quantum measurement remains highly controversial. For an introduction
and further reading, see e.g. and references therein. An interesting comparison between classical
and quantum probabilities can be found in .
Bayes’ Theorem
Bayes’ Theorem is a simple consequence of the axioms of probability theory, giving the rules by which
probabilities (understood as degree of belief in propositions) should be manipulated. As a mathematical
statement it is not controversial — what is a matter of debate is whether it should be used as a basis for
inference and in general for dealing with uncertainty. In the previous section we have given some arguments
why we strongly believe this to be the case. An important result is that Bayes’ Theorem can be derived
from a set of basic consistency requirements for plausible reasoning, known as Cox axioms . Therefore,
Bayesian probability theory can be shown to be the unique generalization of boolean logic into a formal
system to manipulate propositions in the presence of uncertainty . In other words, Bayesian inference
is the unique generalization of logical deduction when the available information is incomplete.
We now turn to the presentation of the actual mathematical framework. We adopt a fairly relaxed
notation, as mathematical rigour is not the aim of this review. For a more formal introduction, see e.g. .
Let us consider a proposition A, which could be a random variable (e.g., the probability of obtaining 12
when rolling two dices) or a one–oﬀproposition , and its negation, A. The sum rule reads
p(A|I) + p(A|I) = 1,
where the vertical bar means that the probability assignment is conditional on assuming whatever information is given on its right. Above, I represents any relevant information that is assumed to be true. The
product rule is written as
p(A, B|I) = p(A|B, I)p(B|I),
which says that the joint probability of A and B equals to the probability of A given that B occurs times
the probability of B occurring on its own (both conditional on information I). If we are interested in the
Bayes in the sky
probability of B alone, irrespective of A, the sum and product rules together imply that
p(A, B|I),
where the sum runs over the possible outcomes for proposition A. The quantity on the left–hand–side is
called marginal probability of B. Since obviously p(A, B|I) = p(B, A|I), the product rule can be rewritten
to give Bayes’ Theorem:
p(B|A, I) = p(A|B, I)p(B|I)
(Bayes theorem).
The interpretation of this simple result is more illuminating if one replaces for A the observed data d and
for B the hypothesis H we want to assess, obtaining
p(H|d, I) = p(d|H, I)p(H|I)
On the left–hand side, p(H|d, I) is the posterior probability of the hypothesis taking the data into account.
This is proportional to the sampling distribution of the data p(d|H, I) assuming the hypothesis is true, times
the prior probability for the hypothesis, p(H|I) (“the prior”, conditional on whatever external information
we have, I), which represents our state of knowledge before seeing the data. The sampling distribution
encodes how the degree of plausibility of the hypothesis changes when we acquire new data. Considered
as a function of the hypothesis, for ﬁxed data (the ones that have been observed), it is called the likelihood
function and we will often employ the shortcut notation L(H) ≡p(d|H, I). Notice that as a function of the
hypothesis the likelihood is not a probability distribution. The normalization constant on the right–hand–
side in the denominator is the marginal likelihood (in cosmology often called the “Bayesian evidence”)
p(d|H, I)p(H|I)
(Bayesian evidence).
where the sum runs over all the possible outcomes for the hypothesis H. This is the central quantity for
model comparison purposes, and it is further discussed in section 4. The posterior is the relevant quantity
for Bayesian inference as it represents our state of belief about the hypothesis after we have considered the
information in the data (hence the name). Notice that there is a logical sequence in going from the prior
to the posterior, not necessarily a temporal one, i.e. a scientist might well specify the prior after the data
have been gathered provided that the prior reﬂects her state of knowledge irrespective of the data. Bayes’
Theorem is therefore a prescription as to how one learns from experience. It gives a unique rule to update
one’s beliefs in the light of the observed data.
The need to specify a prior describing a “subjective” state of knowledge has exposed Bayesian inference
to the criticism that it is not objective, and hence unﬁt for scientiﬁc reasoning. Exactly the contrary is
true — a thorny issue to which we now brieﬂy turn out attention.
Subjectivity, priors and all that
The prior choice is a fundamental ingredient of Bayesian statistics. Historically, it has been regarded as
problematic, since the theory does not give guidance about how the prior should be selected. Here we
argue that this issue has been given undue emphasis and that prior speciﬁcation should be regarded as a
feature of Bayesian statistics, rather than a limitation.
The guiding principle of Bayesian probability theory is that there can be no inference without assumptions, and thus the prior choice ought to reﬂect as accurately as possible one’s assumptions and state of
knowledge about the problem in question before the data come along. Far from undermining objectivity,
Roberto Trotta
this is obviously a positive feature, because Bayes’ Theorem gives a univoque procedure to update diﬀerent
degrees of beliefs that diﬀerent scientist might have held before seeing the data. Furthermore, there are
many cases where prior (i.e., external) information is relevant and it is sensible to include it in the inference
procedure1.
It is only natural that two scientists might have diﬀerent priors as a consequence of their past scientiﬁc
experiences, theoretical outlook and based on the outcome of previous observations they might have
performed. As long as the prior p(H|I) (where the extra conditioning on I denotes the external information
of the kind listed above) has a support that is non–zero in regions where the likelihood is large, repeated
application of Bayes theorem, Eq. (5), will lead to a posterior pdf that converges to a common, i.e. objective
inference on the hypothesis. As an example, consider the case where the inference concerns the value of
some physical quantity θ, in which case p(θ|d, I) has to be interpreted as the posterior probability density
and p(θ|d, I)dθ is the probability of θ to take on a value between θ and θ+dθ. Alice and Bob have diﬀerent
prior beliefs regarding the possible value of θ, perhaps based on previous, independent measurements of
that quantity that they have performed. Let us denote their priors by p(θ|Ii) (i = A, B) and let us assume
that they are described by two Gaussian distributions of mean µi and variance Σ2
i , i = A, B representing
the state of knowledge of Alice and Bob, respectively. Alice and Bob go together in the lab and perform a
measurement of θ with an apparatus subject to Gaussian noise of known variance σ2. They obtain a value
m1, hence their likelihood is
L(θ) ≡p(m1|θ) = L0 exp
Replacing the hypothesis H by the continuous variable θ in Bayes’ Theorem1, we obtain for their respective
posterior pdf’s after the new datum
p(θ|m1, Ii) = L(θ)p(θ|Ii)
(i = A, B).
It is easy to see that the posterior pdf’s of Alice and Bob are again Gaussians with means
µi = m1 + (σ/Σi)2µi
1 + (σ/Σi)2
and variance
1 + (σ/Σi)2
(i = A, B).
Thus if the likelihood is more informative than the prior, i.e. for (σ/Σi) ≪1 the posterior means of Bob
and Alice will converge towards the measured value, m1. As more and more data points are gathered,
one can simply replace m1 in the above equations by the mean m of the observations and σ2 by σ2/N,
with N the number of data points. Thus we can see that the initial prior means µi of Alice and Bob will
progressively be overridden by the data. This process is illustrated in Figure 2.
Finally, objectivity is ensured by the fact that two scientists in the same state of knowledge should
assign the same prior, hence their posterior are identical if they observe the same data. The fact that the
prior assignment eventually becomes irrelevant as better and better data make the posterior likelihood–
dominated in uncontroversial in principle but may be problematic in practice. Often the data are not
strong enough to override the prior, in which case great care must be given in assessing how much of
1As argued above, often it would be a mistake not to do so, for example when trying to estimate a mass m from some data one should
enforce it to be a positive quantity by requiring that p(m) = 0 for m < 0.
1Strictly speaking, Bayes’ Theorem holds for discrete probabilities and the passage to hypotheses represented by continuous variables
ought to be performed with some mathematical care. Here we simply appeal to the intuition of physicists without being too much
concerned by mathematical rigour.
Bayes in the sky
Converging views in Bayesian inference. Two scientists having diﬀerent prior believes p(θ|Ii) about the value of a quantity θ
(panel (a), red and green pdf’s) observe one datum with likelihood L(θ) (panel (b)), after which their posteriors p(θ|m1) (panel (c),
obtained via Bayes Theorem, Eq. (8)) represent their updated states of knowledge on the parameter. After observing 100 data points,
the two posteriors have become essentially indistinguishable (d).
the ﬁnal inference depends on the prior choice. This might occur for small sample sizes, or for problems
where the dimensionality of the hypothesis space is larger than the number of observations (for example,
in image reconstruction). Even in such a case, if diﬀerent prior choices lead to diﬀerent posteriors we can
still conclude that the data are not informative enough to completely override our prior state of knowledge
and hence we have learned something useful about the constraining power (or lack thereof) of the data.
The situation is somewhat diﬀerent for model comparison questions. In this case, it is precisely the
available prior volume that is important in determining the penalty that more complex models with more
free parameters should incur into (this is discussed in detail in section 4). Hence the impact of the prior
choice is much stronger when dealing with model selection issues, and care should be exercised in assessing
how much the outcome would change for physically reasonable changes in the prior.
There is a vast literature on priors that we cannot begin to summarize here. Important issues concern the
determination of “ignorance priors”, i.e. priors reﬂecting a state of indiﬀerence with respect to symmetries
of the problem considered. “Reference priors” exploit the idea of using characteristics of what the experiment is expected to provide to construct priors representing the least informative state of knowledge. In
order to be probability distributions, priors must be proper, i.e. normalizable to unity probability content.
“Flat priors” are often a standard choice, in which the prior is taken to be constant within some minimum
and maximum value of the parameters, i.e. for a 1–dimensional case p(θ) = (θmax −θmin)−1. The rationale
is that we should assign equal probability to equal states of knowledge. However, ﬂat priors are not always
as harmless as they appear. One reason is that a ﬂat prior on a parameter θ does not correspond to a ﬂat
prior on a non–linear function of that parameter, ψ(θ). The two priors are related by
p(ψ) = p(θ)
so for a non–linear dependence ψ(θ) the term |dθ/dψ| means that an uninformative (ﬂat) prior on θ might
be strongly informative about ψ (in the multi–dimensional case, the derivative term is replaced by the
determinant of the Jacobian for the transformation). Furthermore, if we are ignorant about the scale of
a quantity θ, it can be shown (see e.g. ) that the appropriate prior is ﬂat on ln θ, which gives equal
weight to all orders of magnitude. This prior corresponds to p(θ) ∝θ−1 and is called “Jeﬀreys’ prior”. It
is appropriate for example for the rate of a Poisson distribution. For further details on prior choice, and
especially so–called “objective priors”, see e.g. chapter 5 in and references therein.
Bayesian parameter inference
The general problem and its solution
The general problem of Bayesian parameter inference can be speciﬁed as follows. We ﬁrst choose a model
containing a set of hypotheses in the form of a vector of parameters, θ. The parameters might describe
Roberto Trotta
any aspect of the model, but usually they will represent some physically meaningful quantity, such as for
example the mass of an extra–solar planet or the abundance of dark matter in the Universe. Together with
the model we must specify the priors for the parameters. Priors should summarize our state of knowledge
about the parameters before we consider the new data, and for the parameter inference step the prior for
a new observation might be taken to be the posterior from a previous measurement (for model comparison
issues the prior is better understood in a diﬀerent way, see section 4). The caveats about priors and prior
speciﬁcations presented in the previous section will apply at this stage.
The central step is to construct the likelihood function for the measurement, which usually reﬂects the
way the data are obtained. For example, a measurement with Gaussian noise will be represented by a
Normal distribution, while γ–ray counts on a detector will have a Poisson distribution for a likelihood.
Nuisance parameters related to the measurement process might be present in the likelihood, e.g. the
variance of the Gaussian might be unknown or the background rate in the absence of the source might be
subject to uncertainty. This is no matter of concern for a Bayesian, as the general strategy is always to
work out the joint posterior for all of the parameters in the problem and then marginalize over the ones
we are not interested in. Assuming that we have a set of physically interesting parameters φ and a set of
nuisance parameters ψ, the joint posterior for θ = (φ, ψ) is obtained through Bayes’ Theorem:
p(θ|d, M) = L(θ)p(θ|M)
where we have made explicit the choice of a model M by writing it on the righ–hand–side of the conditioning symbol. Recall that L(θ) ≡p(d|θ, M) denotes the likelihood and p(θ|M) the prior. The normalizing
constant p(d|M) (“the Bayesian evidence”) is irrelevant for parameter inference (but central to model comparison, see section 4), so we can write the marginal posterior on the parameter of interest as (marginalizing
over the nuisance parameters)
p(φ|d, M) ∝
L(φ, ψ)p(φ, ψ|M)dψ.
The ﬁnal inference on φ from the posterior can then be communicated either by some summary statistics
(such as the mean, the median or the mode of the distribution, its standard deviation and the correlation
matrix among the components) or more usefully (especially for cases where the posterior presents multiple peaks or heavy tails) by plotting one or two dimensional subsets of φ, with the other components
marginalized over.
In real life there are only a few cases of interest for which the above procedure can be carried out
analytically. Quite often, however, the simple case of a Gaussian prior and a Gaussian likelihood can oﬀer
useful guidance regarding the behaviour of more complex problems. An analytical model of a Poisson–
distributed likelihood for estimating source counts in the presence of a background signal is worked out
in . In general, however, actual problems in cosmology and astrophysics are not analytically tractable
and one must resort to numerical techniques to evaluate the likelihood and to draw samples from the
posterior. Fortunately this is not a major hurdle thanks to the recent increase of cheap computational
power. In particular, numerical inference often employs a technique called Markov Chain Monte Carlo,
which allows to map out numerically the posterior distribution of Eq. (12) even in the most complicated
situations, where the likelihood can only be obtained by numerical simulation, the parameter space can
have hundreds of dimensions and the posterior has multiple peaks and a complicated structure.
For further reading about Bayesian parameter inference, see . For more advanced applications to problems in astrophysics and cosmology, see .
Markov Chain Monte Carlo techniques for parameter inference
The general solution to any inference problem has been outlined in the section above: it remains to ﬁnd a
way to evaluate the posterior of Eq. (12) for the usual case where analytical solutions do not exist or are
Bayes in the sky
insuﬃciently accurate. Nowadays, Bayesian inference heavily relies on numerical simulation, in particular
in the form of Markov Chain Monte Carlo (MCMC) techniques, which are discussed in this section.
The purpose of the Markov chain Monte Carlo algorithm is to construct a sequence of points in parameter
space (called “a chain”), whose density is proportional to the posterior pdf of Eq. (12). Developing a full
theory of Markov chains is beyond the scope of the present article (see e.g. instead). For our purposes
it suﬃces to say that a Markov chain is deﬁned as a sequence of random variables {X(0), X(1), . . . , X(M−1)}
such that the probability of the (t + 1)–th element in the chain only depends on the value of the t–
th element. The crucial property of Markov chains is that they can be shown to converge to a stationary
state (i.e., which does not change with t) where successive elements of the chain are samples from the target
distribution, in our case the posterior p(θ|d). The generation of the elements of the chain is probabilistic in
nature, and several algorithms are available to construct Markov chains. The choice of algorithm is highly
dependent on the characteristics of the problem at hand, and “tayloring” the MCMC to the posterior
one wants to explore often takes a lot of eﬀort. Popular and eﬀective algorithms include the Metropolis–
Hastings algorithm , Gibbs sampling (see e.g. ), Hamiltonian Monte Carlo (see e.g. and
importance sampling.
Once a Markov chain has been constructed, obtaining Monte Carlo estimates of expectations for any
function of the parameters becomes a trivial task. For example, the posterior mean is given by (where ⟨·⟩
denotes the expectation value with respect to the posterior)
p(θ|d)θdθ = 1
where the equality with the mean of the samples from the MCMC follows because the samples θ(t) are
generated from the posterior by construction. In general, one can easily obtain the expectation value of
any function of the parameters f(θ) as
It is usually interesting to summarize the results of the inference by giving the 1–dimensional marginal
probability for the j–th element of θ, θj. Taking without loss of generality j = 1 and a parameter space of
dimensionality n, the equivalent expression to Eq. (3) for the case of continuous variables is
p(θ|d)dθ2 . . . dθn,
where p(θ1|d) is the marginal posterior for the parameter θ1. From the Markov chain it is trivial to obtain
and plot the marginal posterior on the left–hand–side of Eq. (16): since the elements of the Markov chains
are samples from the full posterior, p(θ|d), their density reﬂects the value of the full posterior pdf. It is
then suﬃcient to divide the range of θ1 in a series of bins and count the number of samples falling within
each bin, simply ignoring the coordinates values θ2, . . . , θn. A 2–dimensional posterior is deﬁned in an
analogous fashion.
There are several important practical issues in working with MCMC methods (for details see e.g. ).
Especially for high–dimensional parameter spaces with multi–modal posteriors it is important not to use
MCMC techniques as a black box, since poor exploration of the posterior can lead to serious mistakes in
the ﬁnal inference if it remains undetected. Considerable care is required to ensure as much as possible
that the MCMC exploration has covered the relevant parameter space.
Roberto Trotta
Bayesian model comparison
Shaving theories with Occam’s razor
When there are several competing theoretical models, Bayesian model comparison provides a formal way
of evaluating their relative probabilities in light of the data and any prior information available. The “best”
model is then the one which strikes an optimum balance between quality of ﬁt and predictivity. In fact, it
is obvious that a model with more free parameters will always ﬁt the data better (or at least as good as)
a model with less parameters. However, more free parameters also mean a more “complex” model, in a
sense that we will quantify below in section 4.6. Such an added complexity ought to be avoided whenever
a simpler model provides an adequate description of the observations. This guiding principle of simplicity
and economy of an explanation is known as Occam’s razor — the simplest theory compatible with the
available evidence ought to be preferred1. Bayesian model comparison oﬀers a formal way to evaluate
whether the extra complexity of a model is required by the data, thus putting on a ﬁrmer statistical
grounds the evaluation and selection process of scientiﬁc theories that scientists often carry out at a more
intuitive level. For example, a Bayesian model comparison of the Ptolemaic model of epicycles versus the
heliocentric model based on Newtonian gravity would favour the latter because of its simplicity and ability
to explain planetary motions in a more economic fashion than the baroque construction of epicycles.
An important feature is that an alternative model must be speciﬁed against which the comparison
is made. In contrast with frequentist goodness–of–ﬁt tests (such as chi–square tests), Bayesian model
comparison maintains that it is pointless to reject a theory unless an alternative explanation is available
that ﬁts the observed facts better (for more details about the diﬀerence in approach with frequentist
hypothesis testing, see ). In other words, unless the observations are totally impossible within a model,
ﬁnding that the data are improbable given a theory does not say anything about the probability of the
theory itself unless we can compare it with an alternative. A consequence of this is that the probability
of a theory that makes a correct prediction can increase if the prediction is conﬁrmed by observations,
provided competitor theories do not make the same prediction. This agrees with our intuition that a
veriﬁed prediction lends support to the theory that made it, in contrast with the limited concept of
falsiﬁability advocated by Popper (i.e., that scientiﬁc theories can only be tested by proving them wrong).
So for example, perturbations to the motion of Uranus led the French astronomer U.J.J Leverrier and
the English scholar J.C. Adams to formulate the prediction, based on Newtonian theory, that a further
planet ought to exist beyond the orbit of Uranus. The discovery of Neptune in 1846 within 1 degree of the
predicted position thus should strengthen our belief in the correctness of Newtonian gravity. However, as
discussed in detail in chapter 5 of Ref , the change in the plausibility of Newton’s theory following the
discovery of Uranus crucially depends on the alternative we are considering. If the alternative theory is
Einstein gravity, then obviously the two theories make the same predictions as far as the orbit of Uranus is
concerned, hence their relative plausibility is unchanged by the discovery. The alternative “Newton theory
is false” is not useful in Bayesian model comparison, and we are forced to put on the table a more speciﬁc
model than that before we can assess how much the new observation changes our relative degree of belief
between an alternative theory and Newtonian gravity.
In the context of model comparison it is appropriate to think of a model as a speciﬁcation of a set of
parameters θ and of their prior distribution, p(θ|M). As shown below, it is the number of free parameters
and their prior range that control the strength of the Occam’s razor eﬀect in Bayesian model comparison:
models that have many parameters that can take on a wide range of values but that are not needed in
the light of the data are penalized for their unwarranted complexity. Therefore, the prior choice ought
to reﬂect the available parameter space under the model M, independently of experimental constraints we
might already be aware of. This is because we are trying to assess the economy (or simplicity) of the
model itself, and hence the prior should be based on theoretical or physical constraints on the model under
consideration. Often these will take the form of a range of values that are deemed “intuitively” plausible,
or “natural”. Thus the prior speciﬁcation is inherent in the model comparison approach.
1In its formulation by the medieval English philosopher and Franciscan monk William of Ockham (ca. 1285-1349): “Pluralitas non est
ponenda sine neccesitate”.
Bayes in the sky
The prime tool for model selection is the Bayesian evidence, discussed in the next three sections. A
quantitative measure of the eﬀective model complexity is introduced in section 4.6. We then present some
popular approximations to the full Bayesian evidence that attempt to avoid the diﬃculty of priors choice,
the information criteria, and discuss the limits of their applicability in section 4.7.
For reviews on model selection see e.g. and for cosmological applications. Good starting
points on Bayes factors are . A discussion of the spirit of model selection can be found in the ﬁrst
part of .
The Bayesian evidence
The evaluation of a model’s performance in the light of the data is based on the Bayesian evidence, which
in the statistical literature is often called marginal likelihood or model likelihood. Here we follow the practice
of the cosmology and astrophysics community and will use the term “evidence” instead. The evidence is
the normalization integral on the right–hand–side of Bayes’ theorem, Eq. (6), which we rewrite here for a
continuous parameter space ΩM and conditioning explicitly on the model under consideration, M:
p(d|θ, M)p(θ|M)dθ
(Bayesian evidence).
Thus the Bayesian evidence is the average of the likelihood under the prior for a speciﬁc model choice.
From the evidence, the model posterior probability given the data is obtained by using Bayes’ Theorem
to invert the order of conditioning:
p(M|d) ∝p(M)p(d|M),
where we have dropped an irrelevant normalization constant that depends only on the data and p(M) is
the prior probability assigned to the model itself. Usually this is taken to be non–committal and equal to
1/Nm if one considers Nm diﬀerent models. When comparing two models, M0 versus M1, one is interested
in the ratio of the posterior probabilities, or posterior odds, given by
p(M1|d) = B01
and the Bayes factor B01 is the ratio of the models’ evidences:
B01 ≡p(d|M0)
(Bayes factor).
A value B01 > (<) 1 represents an increase (decrease) of the support in favour of model 0 versus model
1 given the observed data. From Eq. (19) it follows that the Bayes factor gives the factor by which the
relative odds between the two models have changed after the arrival of the data, regardless of what we
thought of the relative plausibility of the models before the data, given by the ratio of the prior models’
probabilities. Therefore the relevant quantity to update our state of belief in two competing models is the
Bayes factor.
To gain some intuition about how the Bayes factor works, consider two competing models: M0 predicting
that a quantity θ = 0 with no free parameters, and M1 which assigns θ a Gaussian prior distribution with
0 mean and variance Σ2. Assume we perform a measurement of θ described by a normal likelihood of
standard deviation σ, and with the maximum likelihood value lying λ standard deviations away from 0,
i.e. |θmax/σ| = λ. Then the Bayes factor between the two models is given by, from Eq. (20)
1 + (σ/Σ)−2 exp
2(1 + (σ/Σ)2)
Roberto Trotta
Empirical scale for evaluating the strength of evidence
when comparing two models, M0 versus M1 (so–called “Jeﬀreys’
scale”). Threshold values are empirically set, and they occur for
values of the logarithm of the Bayes factor of | ln B01| = 1.0,
2.5 and 5.0. The right–most column gives our convention for denoting the diﬀerent levels of evidence above these thresholds.
The probability column refers to the posterior probability of the
favoured model, assuming non–committal priors on the two competing models, i.e. p(M0) = p(M1) = 1/2 and that the two models exhaust the model space, p(M0|d) + p(M1|d) = 1.
Probability
Strength of evidence
Inconclusive
Weak evidence
Moderate evidence
Strong evidence
For λ ≫1, corresponding to a detection of the new parameter at many sigma, the exponential term
dominates and B01 ≪1, favouring the more complex model with a non–zero extra parameter, in agreement
with the usual conclusion. But if λ ∼
< 1 and σ/Σ ≪1 (i.e., the likelihood is much more sharply peaked
than the prior and in the vicinity of 0), then the prediction of the simpler model that θ = 0 has been
conﬁrmed. This leads to the Bayes factor being dominated by the Occam’s razor term, and B01 ≈Σ/σ, i.e.
evidence accumulates in favour of the simpler model proportionally to the volume of “wasted” parameter
space. If however σ/Σ ≫1 then the likelihood is less informative than the prior and B01 →1, i.e. the data
have not changed our relative belief in the two models.
Bayes factors are usually interpreted against the Jeﬀreys’ scale for the strength of evidence, given in
Table 1. This is an empirically calibrated scale, with thresholds at values of the odds of about 3 : 1, 12 : 1
and 150 : 1, representing weak, moderate and strong evidence, respectively. A useful way of thinking of
the Jeﬀreys’ scale is in terms of betting odds — many of us would feel that odds of 150 : 1 are a fairly
strong disincentive towards betting a large sum of money on the outcome. Also notice from Table 1 that
the relevant quantity in the scale is the logarithm of the Bayes factor, which tells us that evidence only
accumulates slowly and that indeed moving up a level in the evidence strength scale requires about an
order of magnitude more support than the level before.
Bayesian model comparison does not replace the parameter inference step (which is performed within
each of the models separately). Instead, model comparison extends the assessment of hypotheses in the light
of the available data to the space of theoretical models, as evident from Eq. (19), which is the equivalent
expression for models to Eq. (12), representing inference about the parameters value within each model
(for multi–model inference, merging the two levels, see section 6.2).
Computation and interpretation of the evidence
The computation of the Bayesian evidence (17) is in general a numerically challenging task, as it involves a
multi–dimensional integration over the whole of parameter space. An added diﬃculty is that the likelihood
is often sharply peaked within the prior range, but possibly with long tails that do contribute signiﬁcantly
to the integral and which cannot be neglected. Other problematic situations arise when the likelihood is
multi–modal, or when it has strong degeneracies that conﬁne the posterior to thin sheets in parameter
space. Until recently, the application of Bayesian model comparison has been hampered by the diﬃculty
of reliably estimating the evidence. Fortunately, several methods are now available, each with its own
strengths and domains of applicability.
(i) The numerical method of choice until recently has been thermodynamic integration, also called simulated annealing (see e.g. and references therein for details). Its computational cost can become
fairly large, as it depends heavily on the dimensionality of the parameter space and on the characteristic
of the likelihood function. In typical cosmological applications , thermodynamic integration can
require up to 107 likelihood evaluations, two orders of magnitude more than MCMC–based parameter
estimation.
Bayes in the sky
(ii) Skilling has put forward an elegant algorithm called “nested sampling”, which has been implemented in the cosmological context by (for a theoretical discussion of the algorithmic properties,
see ). The gist of nested sampling is that the multi–dimensional evidence integral is recast into a
one–dimensional integral that is easy to evaluate numerically. This technique allows to reduce the computational burden to about 105 likelihood evaluations1. Recently, the development of what is called
“multi–modal nested sampling” has allowed to increase signiﬁcantly the eﬃciency of the method ,
reducing the number of likelihood evaluations by another order of magnitude.
(iii) Approximations to the Bayes factor, Eq. (20), are available for situations in which the models being
compared are nested into each other, i.e. the more complex model (M1) reduces to the original model
(M0) for speciﬁc values of the new parameters. This is a fairly common scenario in cosmology, where
one wishes to evaluate whether the inclusion of the new parameters is supported by the data. For
example, we might want to assess whether we need isocurvature contributions to the initial conditions
for cosmological perturbations, or whether a curvature term in Einstein’s equation is needed, or whether
a non–scale invariant distribution of the primordial ﬂuctuation is preferred (see Table 4 for actual
results). Writing for the extended model parameters θ = (φ, ψ), where the simpler model M0 is
obtained by setting ψ = 0, and assuming further that the prior is separable (which is usually the case
in cosmology), i.e. that
p(φ, ψ|M1) = p(ψ|M1)p(φ|M0),
the Bayes factor can be written in all generality as
B01 = p(ψ|d, M1)
This expression is known as the Savage–Dickey density ratio (SDDR, see and references therein).
The numerator is simply the marginal posterior under the more complex model evaluated at the
simpler model’s parameter value, while the denominator is the prior density of the more complex
model evaluated at the same point. This technique is particularly useful when testing for one extra
parameter at the time, because then the marginal posterior p(ψ|d, M1) is a 1–dimensional function
and normalizing it to unity probability content only requires a 1–dimensional integral, which is simple
to do using for example the trapezoidal rule.
(iv) An instructive approximation to the Bayesian evidence can be obtained when the likelihood function
is unimodal and approximately Gaussian in the parameters. Expanding the likelihood around its peak
to second order one obtains the Laplace approximation
p(d|θ, M) ≈Lmax exp
2(θ −θmax)tL(θ −θmax)
where θmax is the maximum–likelihood point, Lmax the maximum likelihood value and L the likelihood
Fisher matrix (which is the inverse of the covariance matrix for the parameters). Assuming as a prior
a multinormal Gaussian distribution with zero mean and Fisher information matrix P one obtains for
the evidence, Eq. (17)
p(d|M) = Lmax
|P|−1/2 exp
2 .
Roberto Trotta
Illustration of Bayesian model comparison for two nested models, where the more complex model has one extra parameter.
The outcome of the model comparison depends both on the information content of the data with respect to the a priori available
parameter space, I10 (horizontal axis) and on the quality of ﬁt (vertical axis, λ, which gives the number of sigma signiﬁcance of the
measurement for the extra parameter). The contours are computed from Eq. (23), assuming a Gaussian likelihood and prior (adapted
from ).
From Eq. (25) we can deduce a few qualitatively relevant properties of the evidence. First, the quality of
ﬁt of the model is expressed by Lmax, the best–ﬁt likelihood. Thus a model which ﬁts the data better will
be favoured by this term. The term involving the determinants of P and F is a volume factor, encoding the
Occam’s razor eﬀect. As |P| ≤|F|, it penalizes models with a large volume of wasted parameter space, i.e.
those for which the parameter space volume |F|−1/2 which survives after arrival of the data is much smaller
than the initially available parameter space under the model prior, |P|−1/2. Finally, the exponential term
suppresses the likelihood of models for which the parameters values which maximise the likelihood, θmax,
diﬀer appreciably from the expectation value under the posterior, θ. Therefore when we consider a model
with an increased number of parameters we see that its evidence will be larger only if the quality–of–ﬁt
increases enough to oﬀset the penalizing eﬀect of the Occam’s factor (see also the discussion in ).
On the other hand, it is important to notice that the Bayesian evidence does not penalizes models
with parameters that are unconstrained by the data. It is easy to see that unmeasured parameters (i.e.,
parameters whose posterior is equal to the prior) do not contribute to the evidence integral, and hence
model comparison does not act against them, awaiting better data.
The rough guide to model comparison
The gist of Bayesian model comparison can be summarized by the following, back–of–the–envelope Bayes
factor computation for nested models. The result is surprisingly close to what one would obtain from the
more elaborate, fully–ﬂedged evidence evaluation, and can serve as a rough guide for the Bayes factor
determination.
Returning to the example of Eq. (21), if the data are informative with respect to the prior on the extra
parameter (i.e., for σ/Σ ≪1) the logarithm of the Bayes factor is given approximately by
ln B01 ≈ln (Σ/σ) −λ2/2,
where as before λ gives the number of sigma away from a null result (the “signiﬁcance” of the measurement).
The ﬁrst term on the right–hand–side is approximately the logarithm of the ratio of the prior to posterior
Bayes in the sky
volume. We can interpret it as the information content of the data, as it gives the factor by which the
parameter space has been reduced in going from the prior to the posterior. This term is positive for
informative data, i.e. if the likelihood is more sharply peaked than the prior. The second term is always
negative, and it favours the more complex model if the measurement gives a result many sigma away from
the prediction of the simpler model (i.e., for λ ≫0). We are free to measure the information content in base–
10 logarithm (as this quantity is closer to our intuition, being the order of magnitude of our information
increase), and we deﬁne the quantity I10 ≡log10 (Σ/σ). Figure 3 shows contours of | ln B01| = const for
const = 1.0, 2.5, 5.0 in the (I10, λ) plane, as computed from Eq. (26). The contours delimit signiﬁcative
levels for the strength of evidence, according to the Jeﬀreys’ scale (Table 1). For moderately informative
data (I10 ≈1−2) the measured mean has to lie at least about 4σ away from 0 in order to robustly disfavor
the simpler model (i.e., λ ∼
> 4). Conversely, for λ ∼
< 3 highly informative data (I10 ∼
> 2) do favor the
conclusion that the extra parameter is indeed 0. In general, a large information content favors the simpler
model, because Occam’s razor penalizes the large volume of “wasted” parameter space of the extended
An useful properties of Figure 3 is that the impact of a change of prior can be easily quantiﬁed. A
diﬀerent choice of prior width (i.e., Σ) amounts to a horizontal shift across Figure 3, at least as long as
I10 > 0 (i.e., the posterior is dominated by the likelihood). Picking more restrictive priors (reﬂecting more
predictive theoretical models) corresponds to shifting the result of the model comparison to the left of
Figure 3, returning an inconclusive result (white region) or a prior–dominated outcome (hatched region).
Notice that results in the 2–3 sigma range, which are fairly typical in cosmology, can only support the
more complex model in a very mild way at best (odds of 3 : 1 at best), while actually being most of the
time either inconclusive or in favour of the simpler hypothesis (pink shaded region in the bottom right
Bayesian model comparison is usually conservative when it comes to admitting a new quantity in our
model, even in the case when the prior width is chosen incorrectly. Consider the following two possibilities:
• If the prior range is too small, the model comparison result will be non–committal (white region in
Figure 3), or even prior dominated (hatched region, where the posterior is dominated by the prior).
Hence in this case we have to hold judgement until better data come along.
• Too wide a prior will instead unduly favour the simpler model (pink, shaded regions). However, as
new, better data come along the result will move to the right (for a ﬁxed prior width, as the likelihood
becomes narrower) but eventually also upwards, towards a larger number of sigma signiﬁcance, if the
true model really has a non–zero extra parameter. Eventually, our initial “poor” prior choice will be
overridden as the number of sigma becomes large enough to take the result into the blue, shaded region.
In both cases the result of the model comparison will eventually override the “wrong” prior choice
(although it might take a long time to do so), exactly as it happens for parameter inference.
Getting around the prior – The maximal evidence for a new parameter
For nested models, Eq. (23) shows that the relative probability of the more complex model can be made
arbitrarily small by increasing the broadness of the prior for the extra parameters, p(ψ|M1) (as the prior
is a pdf, it must integrate to unit probability. Hence a broader prior corresponds to a smaller value of
p(ψ|M1)ψ=0 in the denominator). Often, this is not problematical as prior ranges for the new parameters
can (and should) be motivated from the underlying theory. For example, in assessing whether the scalar
spectral index (ns) of the primordial perturbations diﬀers from the scale–invariant value ns = 1, the prior
range of the index can be constrained to be 0.8 ∼
< 1.2 within the theoretical framework of slow roll
inﬂation (more on this in section 6). The sensitivity of the model comparison result can also be investigated
for other plausible, physically motivated choices of prior ranges, see e.g. . If the model comparison
outcome is qualitatively the same for a broad choice of plausible priors, then we can be conﬁdent that the
result is robust.
Although the Bayesian evidence oﬀers a well–deﬁned framework for model comparison, there are cases
where there is not a speciﬁc enough model available to place meaningful limits on the prior ranges of
Roberto Trotta
Translation table (using Eq. (27)) between frequentist
signiﬁcance values (p–values) and the upper bounds on the odds
(B10) in favour of the more complex model. No other choice of
prior (within the family considered in the text) will give higher
evidence in favour of the extra parameters. The “sigma” column
is the corresponding number of standard deviations away from the
mean for a normal distribution. The “category” column gives the
Jeﬀreys’ scale of Table 1 (from ).
‘weak’ at best
‘moderate’ at best
‘strong’ at best
new parameters in a model. This hurdle arises frequently in cases when the new parameters are a phenomenological description of a new eﬀect, only loosely tied to the underlying physics, such as for example
expansion coeﬃcients of some series. An interesting possibility in such a case is to choose the prior on the
new parameters in such a way as to maximise the probability of the new model, given the data. If, even
under this best case scenario, the more complex model is not signiﬁcantly more probable than the simpler
model, then one can conﬁdently say that the data does not support the addition of the new parameters,
without worrying that some other choice of prior will make the new model more probable .
Consider the Bayes factor in favour of the more complex model, B10 ≡1/B01, with B01 given by Eq. (20).
The simpler model, M0, is obtained from M0 by setting θ = θ∗. An absolute upper bound to the evidence
in favour of the more complex model is obtained by choosing p(θ|M1) to be a delta function centered at
the maximum likelihood value under M1, θmax. It can be shown that in this case the upper bound B10
corresponds to the likelihood ratio between θmax and θ∗. However, it can be argued that such a choice for
the prior is unjustiﬁed, as it can only be made ex post facto after one has seen the data and obtained the
maximum likelihood estimate. It is more natural to appeal to a mild principle of indiﬀerence as to the
value of the parameter under the more complex model, and thus to maximize the evidence over priors that
are symmetric about θ∗and unimodal. This can be shown to be equivalent to maximizing over all priors
that are uniform and symmetric about θ∗. This procedure leads to a very simple expression for the lower
bound on the Bayes factor 
B10 ≤B10 =
for ℘≤e−1, where e is the exponential of one. Here, ℘is the p–value, the probability that the the value
of some test statistics be as large as or larger than the observed value assuming the null hypothesis (i.e.,
the simpler model) is true (see for a detailed discussion). A more precise deﬁnition of p–values is
given in any standard statistical textbook, e.g. .
Eq. (27) oﬀers a useful calibration of frequentist signiﬁcance values (p–values) in terms of upper bounds
on the Bayesian evidence in favour of the extra parameters. The advantage is that the quantity on the
left–hand side of Eq. (27) can be straightforwardly interpreted as an upper bound on the odds for the more
complex model, whereas the p–values cannot. This point is illustrated very clearly with an astronomical
example in . In fact, a word of caution is in place regarding the meaning of the p–value, which is
often misinterpreted as an error probability, i.e. as giving the fraction of wrongly rejected nulls in the
long run. For example, when a frequentist test rejects the null hypothesis (in our example, that θ = θ∗)
at the 5% level, this does not mean that one will make a mistake roughly 5% of the time if one were to
repeat the test many times. The actual error probability is much larger than that, and can be shown to
be at least 29% (for unimodal, symmetric priors, see Table 6 in ). This important conceptual point is
discussed in in greater detail in . The fundamental reason for this discrepancy with intuition is
that frequentist signiﬁcance tests give the probability of observing data as extreme or more extreme than
Bayes in the sky
what has actually been measured, assuming the null hypothesis H0 to be true (which in Bayesian terms
amounts to the choice of a model, M0). But the quantity one is interested in is actually the probability
of the model M0 given the observations, which can only be obtained by using Bayes’ Theorem to invert
the order of conditioning1. Indeed, in frequentist statistics, a hypothesis is either true or false (although
we do not know which case it is) and it is meaningless to attach to it a probability statement.
Table 2 lists B10 for some common thresholds for signiﬁcance values and the strength of evidence scale,
thus giving a conversion table between signiﬁcance values and upper bounds on the Bayesian evidence,
independent of the choice of prior for the extra parameter (within the class of unimodal and symmetric
priors). It is apparent that in general the upper bound on the Bayesian evidence is much more conservative
than the p–value, e.g. a 99% result (corresponding to ℘= 0.01) corresponds to odds of 8 : 1 at best in
favour of the extra parameters, which fall short of even the “moderate evidence” threshold. Strong evidence
at best requires at least a 3.6 sigma result. A useful rule of thumb is thus to think of a s sigma result
as a s −1 sigma result, e.g. a 99.7% result (3 sigma) really corresponds to odds of 21 : 1, i.e. about 95%
probability for the more complex model. Thus when considering the detection of a new parameter, instead
of reporting frequentist signiﬁcance values it is more appropriate to present the upper bound on the Bayes
factor, as this represents the maximum probability that the extra parameter is diﬀerent from its value
under the simpler model.
This approach has been applied to the cosmological context in , who analysed the evidence in favour
of a non–scale invariant spectral index and of asymmetry in the cosmic microwave background maps. For
further details on the comparison between frequentist hypothesis testing and Bayesian model selection,
The eﬀective number of parameters – Bayesian model complexity
The usefulness of a Bayesian model selection approach based on the Bayesian evidence is that it tells us
whether the increased “complexity” of a model with more parameters is justiﬁed by the data. However,
it is desirable to have a more reﬁned deﬁnition of “model complexity”, as the number of free parameters
is not always an adequate description of this concept. For example, if we are trying to measure a periodic
signal in a time series, we might have a model of the data that looks like
f(t) = A(1 + θ cos(t + δ)),
where A, θ, δ are free parameters we wish to constrain. But if θ is very small compared to 1 and the noise
is large compared to θ, then the oscillatory term remains unconstrained by the data and eﬀectively we can
only measure the normalization A. Thus the parameters θ, δ should not count as free parameters as they
cannot be constrained given the data we have, and the eﬀective model complexity is closer to 1 than to 3.
From this example it follows that the very notion of “free parameter” is not absolute, but it depends on
both what our expectations are under the model, i.e. on the prior, and on the constraining power of the
data at hand.
In order to deﬁne a more appropriate measure of complexity, in the notion of Bayesian complexity was
introduced, which measures the number of parameters that the data can support. Consider the information
gain obtained when upgrading the prior to the posterior, as measured by the the Kullback–Leibler (KL)
divergence between the posterior, p and the prior, denoted here by π:
DKL(p, π) ≡
p(θ|d, M) ln p(θ|d, M)
π(θ|M) dθ.
In virtue of Bayes’ theorem, p(θ|d, M) = L(θ)π(θ|M)/p(d|M) hence the KL divergence becomes the sum
1To convince oneself of the diﬀerence between the two quantities, consider the following example . Imagine selecting a person at
random — the person can either be male or female (our hypothesis). If the person is female, her probability of being pregnant (our data)
is about 3%, i.e. p(pregnant|female) = 0.03. However, if the person is pregnant, her probability of being female is much larger than that,
i.e. (female|pregnant) ≫0.03. The two conditional probabilities are related by Bayes’ theorem.
Roberto Trotta
of the negative log evidence and the expectation value of the log–likelihood under the posterior:
DKL(p, π) = −ln p(d|M) +
p(θ|d, M) ln L(θ)dθ.
To gain a feeling for what the KL divergence expresses, let us compute it for a 1–dimensional case, with a
Gaussian prior around 0 of variance Σ2 and a Gaussian likelihood centered on θmax and variance σ2. We
obtain after a short calculation
DKL(p, π) = −1
The second term on the right–hand side gives the reduction in parameter space volume in going from the
prior to the posterior. For informative data, σ/Σ ≪1, this terms is positive and grows as the logarithm
of the volume ratio. On the other hand, in the same regime the third term is small unless the maximum
likelihood estimate is many standard deviations away from what we expected under the prior, i.e. for
θmax/σ ≫1. This means that the maximum likelihood value is “surprising”, in that it is far from what
our prior led us to expect. Therefore we can see that the KL divergence is a summary of the amount of
information, or “surprise”, contained in the data.
Let us now deﬁne an eﬀective χ2 through the likelihood as L(θ) = exp(−χ2/2). Then Eq. (30) gives
DKL(p, π) = −1
2χ2(θ) + ln p(d|M),
where the bar indicates a mean taken over the posterior distribution. The posterior average of the eﬀective
chi–square is a quantity can be easily obtained by Markov chain Monte Carlo techniques (see section 3.2).
We then subtract from the “expected surprise” the estimated surprise in the data after we have actually
ﬁtted the model parameters, denoted by
2χ2(ˆθ) + ln p(d|M),
where the ﬁrst term on the right–hand–side is the eﬀective chi–square at the estimated value of the
parameters, indicated by a hat. This will usually be the posterior mean of the parameters, but other
possible estimators are the maximum likelihood point or the posterior median, depending on the details
of the problem. We then deﬁne the quantity
DKL(p, π) −d
= χ2(θ) −χ2(ˆθ)
(Bayesian complexity),
(notice that the evidence term is the same in Eqs. (32) and (33) as it does not depend on the parameters
and therefore it disappears from the complexity). The Bayesian complexity gives the eﬀective number of
parameters as a measure of the constraining power of the data as compared to the predictivity of the model,
i.e. the prior. Hence Cb depends both on the data and on the prior available parameter space. This can be
understood by considering further the toy example of a Gaussian likelihood of variance σ2 around θmax and
a Gaussian prior around 0 of variance Σ2. Then a short calculation shows that the Bayesian complexity is
given by (see for details)
1 + (σ/Σ)2 .
So for σ/Σ ≪1, Cb ≈1 and the model has one eﬀective, well constrained parameter. But if the likelihood
width is large compared to the prior, σ/Σ ≫1, then the experiment is not informative with respect to our
prior beliefs and Cb →0.
Bayes in the sky
The Bayesian complexity can be a useful diagnostic tool in the tricky situation where the evidence
for two competing models is about the same. Since the evidence does not penalize parameters that are
unmeasured, from the evidence alone we cannot know if we are in the situation where the extra parameters
are simply unconstrained and hence irrelevant (θ ≪1 in the example of Eq. (28)) or if they improve the
quality–of–ﬁt just enough to oﬀset the Occam’s razor penalty term, hence giving the same evidence as the
simpler model. The Bayesian complexity breaks this degeneracy in the evidence allowing to distinguish
between the two cases:
(i) p(d|M0) ≈p(d|M1) and Cb(M1) > Cb(M0): the quality of the data is suﬃcient to measure the
additional parameters of the more complicated model (M1), but they do not improve its evidence by
much. We should prefer model M0, with less parameters.
(ii) p(d|M0) ≈p(d|M1) and Cb(M1) ≈Cb(M0): both models have a comparable evidence and the eﬀective
number of parameters is about the same. In this case the data is not good enough to measure the
additional parameters of the more complicated model (given the choice of prior) and we cannot draw
any conclusions as to whether the extra parameter is needed.
The ﬁrst application of this technique in the cosmological context is in , where it is applied to the
number of eﬀective parameters from cosmic microwave background data, while in it was used to
determine the number of eﬀective dark energy parameters.
Information criteria for approximate model comparison
Sometimes it might be useful to employ methods that aim at an approximate model selection under some
simplifying assumptions that give a default penalty term for more complex models, which replaces the
Occam’s razor term coming from the diﬀerent prior volumes in the Bayesian evidence. While this is an
obviously appealing feature, on closer examination it has the drawback of being meaningful only in fairly
speciﬁc cases, which are not always met in astrophysical or cosmological applications. In particular, it can
be argued that the Bayesian evidence (ideally coupled with an analysis of the Bayesian complexity) ought
to be preferred for model building since it is precisely the lack of predictivity of more complicated models,
as embodied in the physically motivated range of the prior, that ought to penalize them.
With this caveat in mind, we list below three types of information criteria that have been widely used
in several astrophysical and cosmological contexts. An introduction to the information criteria geared
for astrophysicists is given by Ref. . A discussion of the diﬀerences between the diﬀerent information
criteria as applied to astrophysics can be found in , which also presents a few other information criteria
not discussed here.
• Akaike Information Criterion (AIC): Introduced by Akaike , the AIC is an essentially frequentist
criterion that sets the penalty term equal to twice the number of free parameters in the model, k:
AIC ≡−2 ln Lmax + 2k
where Lmax ≡p(d|θmax, M) is the maximum likelihood value. The derivation of the AIC follows from an
approximate minimization of the KL divergence between the true model distribution and the distribution
being ﬁtted to the data.
• Bayesian Information Criterion (BIC): Sometimes called “Schwarz Information Criterion” (from
the name of its proposer ), the BIC follows from a Gaussian approximation to the Bayesian evidence
in the limit of large sample size:
BIC ≡−2 ln Lmax + k ln N
where k is the number of ﬁtted parameters as before and N is the number of data points. The best
model is again the one that minimizes the BIC.
Roberto Trotta
• Deviance Information Criterion (DIC): Introduced by , the DIC can be written as
DKL + 2Cb.
In this form, the DIC is reminiscent of the AIC, with the ln Lmax term replaced by the estimated KL
divergence and the number of free parameters by the eﬀective number of parameters, Cb, from Eq. (34).
Indeed, in the limit of well–constrained parameters, the AIC is recovered from (38), but the DIC has
the advantage of accounting for unconstrained directions in parameters space.
The information criteria ought to be interpreted with care when applied to real situations. Comparison
of Eq. (37) with Eq. (36) shows that for N > 7 the BIC penalizes models with more free parameters more
harshly than the AIC. Furthermore, both criteria penalize extra parameters regardless of whether they
are constrained by the data or not, unlike the Bayesian evidence. This comes about because implicitly
both criteria assume a “data dominated” regime, where all free parameters are well constrained. But in
general the number of free parameters might not be a good representation of the actual number of eﬀective
parameters, as discussed in section 4.6. In a Bayesian sense it therefore appears desirable to replace the
number of parameters k by the eﬀective number of parameters as measured by the Bayesian complexity,
as in the DIC.
It is instructive to inspect brieﬂy the derivation of the BIC. The unnormalized posterior g(θ) ≡
L(θ)p(θ|M) can be approximated by a multi–variate Gaussian around its mode θ, i.e. g(θ) ≈g(θ) −
1/2(θ −θ)tF(θ −θ), where F is minus the Hessian of the posterior evaluated at the posterior mode. Then
the evidence integral can be computed analytically, giving
p(d|M) ≈exp(g(θ))(2π)k/2|F|−1/2.
For large samples, N →∞, the posterior mode tends to the maximum likelihood point, θ →θmax, hence
g(θ) →Lmaxp(θmax) and the log–evidence becomes
ln p(d|M) →ln Lmax + ln p(θmax) + k
2 ln(2π) −1
where the error introduced by the various approximations scales to leading order as O(N −1/2). On the
right–hand–side, the ﬁrst term scales as O(N), the second and third terms are constant in N, while the
last term is given by ln |F| ≈k ln N, since the variance scales as the number of data points, N. Dropping
terms of order O(1) or below1, we obtain
ln p(d|M) →ln Lmax −k
Thus maximising the evidence of Eq. (41) is equivalent to minimizing the BIC in Eq. (37). We see that
dropping the term ln p(θmax) in (40) means that eﬀectively we expect to be in a regime where the model
comparison is dominated by the likelihood, and that the prior Occam’s razor eﬀect becomes negligible. This
is often not the case in cosmology. Furthermore, this “weak” prior choice is intrinsic (even though hidden
from the user) in the form of the BIC, and often it is not justiﬁed. In conclusion, it appears that what
makes the information criteria attractive, namely the absence of an explicit prior speciﬁcation, represents
also their intrinsic limitation.
1See for a more careful treatment, where a better approximation is obtained by assuming a weak prior which contains the same
information as a single datum.
Bayes in the sky
Cosmological parameter inference
Driven by the emergence of inexpensive sensors and computing capabilities, the amount of cosmological
data has been increasing exponentially over the last 15 years or so. For example, the ﬁrst map of cosmic
microwave background (CMB) anisotropies obtained in 1992 by COBE contained ∼103 pixels, which
became ∼5×104 by 2002 with CBI . Current state–of–the art maps (from the WMAP satellite )
involve ∼106 pixels, which are set to grow to ∼107 with Planck in the next couple of years. Similarly,
angular galaxy surveys contained ∼106 objects in the 1970’s, while by 2005 the Sloan Digital Sky Survey had measured ∼2×108 objects, which will increase to ∼3×109 by 2012 when the Large Synoptic
Survey Telescope comes online2.
This data explosion drove the adoption of more eﬃcient map making tools, faster component separation
algorithms and parameter inference methods that would scale more favourably with the number of dimensions of the problem. As data sets have become larger and more precise, so has grown the complexity of the
models being used to describe them. For example, if only 2 parameters could meaningfully be extracted
from the COBE measurement of the large–scale CMB temperature power spectrum (namely the normalization and the spectral tilt ), the number of model parameters had grown to 11 by 2002, when
smaller–scale measurements of the acoustic peaks had become available. Nowadays, parameter spaces of
up to 20 dimensions are routinely considered.
This section gives an introduction to the broad problem of cosmological parameter inference and highlights some of the tools that have been introduced to tackle it, with particular emphasis on innovative
techniques. This is a vast ﬁeld and any summary is bound to be only sketchy. We give throughout references
to selected papers covering both historically important milestones and recent major developments.
The “vanilla” ΛCDM cosmological model
Before discussing the quantities we are interested in measuring in cosmology (the “cosmological parameters”) and some of the observational probes available to do so, we brieﬂy sketch the general framework
which goes under the name of “cosmological concordance model”. Because it is a relatively simple scenario
containing both a cosmological constant (Λ) and cold dark matter (CDM) (more about them below), it is
also known as the “vanilla” ΛCDM model.
Our current cosmological picture is based on the scenario of an expanding Universe, as implied by the
observed redshift of the spectra of distant galaxies (Hubble’s law). This in turn means that the Universe
began from a hot and dense state, the initial singularity of the Big Bang. The existence of the cosmic
microwave background lends strong support to this idea, as it is interpreted as the relic radiation from the
hotter and denser primordial era. The expanding spacetime is described by Einstein’s general relativity. The
cosmological principle states that the Universe is isotropic (i.e., the same in all directions) and homogeneous
(the same everywhere). If follows that an isotropically expanding Universe is described by the so–called
Friedmann–Robertson–Walker metric,
ds2 = dt2 −a2(t)
1 + κr2 + r2(dθ2 + sin2 θdφ2)
where κ deﬁnes the geometry of spatial sections (if κ is positive, the geometry is spherical; if it is zero, the
geometry is ﬂat; if it is negative, the geometry is hyperbolic). The scale factor a(t) describes the expansion
of the Universe, and it is related to redshift z by
1 + z = a(t0)
2Alex Szalay, talk at the specialist discussion meeting “Statistical challenges in cosmology and astroparticle physics”, held at the Royal
Astronomical Society, London, Oct 2007.
Roberto Trotta
where a(t0) is the scale factor today and a(t) the scale factor at redshift z. The relation between redshift
and comoving distance r is obtained from the above metric via the Friedmann equation, and is given by
Ωκ(1 + z)2 + ΩΛ + (Ωb + Ωcdm)(1 + z)3 + (Ωγ + Ων)(1 + z)4−1/2 dz,
where H0 is the Hubble constant, and Ωx are the cosmological parameters describing the matter–energy
content of the Universe. Standard parameters included in the vanilla model are neutrinos (Ων, with a mass
< 1 eV), photons (Ωγ), baryons (Ωb), cold dark matter (Ωcdm) and a cosmological constant (ΩΛ). The
curvature term Ωκ is included for completeness but is currently not required by the standard cosmological
model (see section 6). The comoving distance determines the apparent brightness of objects, their apparent
size on the sky and the number density of objects in a comoving volume. Hence measurements of the
brightness of standard candles, of the length of standard rulers or of the number density of objects at a
given redshift leads to the determination of the cosmological parameters in Eq. (44) (see next section).
The currently accepted paradigm for the generation of density ﬂuctuations in the early Universe is
inﬂation. The idea is that quantum ﬂuctuations in the primordial era were stretched to cosmological scales
by an initial period of exponential expansion, called “inﬂation”, possibly driven by a yet unknown scalar
ﬁeld. This increased the scale factor by about 26 orders of magnitude within about 10−32s after the Big
Bang. Although presently we have only indirect evidence for inﬂation, it is commonly accepted that such a
short burst of exponential growth in the scale factor is required to solve the horizon problem, i.e. to explain
why the CMB is so highly homogeneous across the whole sky. The quantum ﬂuctuations also originated
temperature anisotropies in the CMB, whose study has proved to be one of the powerhouses of precision
cosmology. From the initial state with small perturbations imprinted on a broadly uniform background,
gravitational attraction generated the complex structures we see in the modern Universe, as indicated
both by observational evidence and highly sophisticated computer modelling.
Of course it is possible to consider completely diﬀerent models, based for example on alternative theories
of gravity (such as Bekenstein’s theory or Jordan–Brans–Dicke theory ), or on a diﬀerent way
of comparing model predictions with observations . Discriminating among models and determining
which model is in best agreement with the data is a task for model comparison techniques, whose application to cosmology is discussed section 6. Here we will take the vanilla ΛCDM model as our starting point
for the following considerations on cosmological parameters and how they are measured.
Cosmological observations
The discovery of temperature ﬂuctuations in the Cosmic Microwave Background (CMB) in 1992 by the
COBE satellite marked the beginning of the era of precision cosmology. Many other observations have
contributed to the impressive development of the ﬁeld in less than 20 years. For example, around 1990
the picture of ﬂat Universe with both cold dark matter and a positive cosmological constant was only
beginning to emerge, and only thanks to the painstakingly diﬃcult work of gluing together several fairly
indirect clues . At the time of writing , the total density is known with an error of
order 1%, and it is likely that this uncertainty will be reduced by another two orders of magnitude in
the mid–term . The high accuracy of modern precision cosmology rests on the combination of several
diﬀerent probes, that constrain the physical properties of the Universe at many diﬀerent redshifts.
(i) Cosmic microwave background (CMB): CMB anisotropies oﬀer a snapshot of the Universe at the
time of recombination, about 380,000 years after the Big Bang, at a redshift z ≈1100. As described
above, the temperature diﬀerences measured in the CMB arise from quantum ﬂuctuations during the
inﬂationary phase. Their usefulness lies in the fact that they are small (∆T/T ∼10−5) and hence linear
perturbation theory is mostly suﬃcient to predict very accurately their statistical distribution. The 2–
point correlation function of the anisotropies is usually described via its Fourier transform, the angular
power spectrum, which presents a series of characteristic peaks called acoustic oscillations, see e.g. . Their structure depends in a rich way on the cosmological parameters introduced in Eq. (44), as
well as on the initial conditions for the perturbations emerging from the inﬂationary era (see e.g. 
Bayes in the sky
State–of–the-art cosmic microwave background temperature power spectrum measurements along with the best–ﬁt
ΛCDM model (solid line), showing data from WMAP 3–yr , the Boomerang 2003 ﬂight and ACBAR (from ).
for further details). The anisotropies are polarized at the level of 1%, and measuring accurately the
information encoded by the weak polarization signal is the goal of many ongoing observations. State–
of–the art measurements are described in e.g. . An example of recent measurements of the
temperature power spectrum is shown in Figure 4. Later this year, the Planck satellite is expected to
start full–sky, high–resolution observations of both temperature and polarization.
(ii) Large scale structures (LSS): the correlation function among galaxies gives an estimate of the
correlation properties of the underlying dark matter distribution, up to a bias factor relating the dark
matter to the baryon distribution. Current data typically extend out to z ∼0.7. The resulting power
spectrum (recent data are shown in Figure 5) depends mainly on the ratio of the radiation to matter
energy density, on the initial distribution of the perturbations with scale (spectral index) and on the
overall normalization (which can be extracted once a bias model is speciﬁed). Heavy numerical simulation is nowadays used to model accurately small scales, where non–linear eﬀects become dominant. The
tool of choice to measure the power spectrum on small scales is becoming the observations of absorption lines from neutral hydrogen clouds, the so-called Lyman-α forest , although concerns remain
about the reliability of the theoretical modelling of non–linear eﬀects. Recently, both the Sloan Digital
Sky Survey and the 2dF Galaxy Redshift Survey have detected the presence of baryonic
acoustic oscillations, which appear as a bump in the galaxy–galaxy correlation function corresponding
to the scale of the acoustic oscillations in the CMB. The physical meaning is that galaxies tend to form
preferentially at a separation corresponding to the characteristic scale of inhomogeneities in the CMB.
Baryonic oscillations can be used as rulers of known length (measured via the CMB acoustic peaks)
located at a much smaller redshift than the CMB (currently, z ∼0.3), and hence they are powerful
probes of the recent expansion history of the Universe, with particular focus on dark energy properties.
The distribution of clusters with redshift can also be employed to probe the growth of perturbations
and hence to constrain cosmology. Current galaxy redshift surveys have catalogued about half a million
objects, but a new generation of surveys aims at taking this number to a over a billion.
(iii) Supernovae type Ia: the commonly accepted scenario for the formation of supernovae type Ia is
a white dwarf accreting material from a binary companion. The core heats up as the gravitational
pressure increases, eventually leading to carbon fusion ignition, followed by oxygen burning. This
runaway reaction releases within seconds a huge amount of energy, resulting in a violent explosion which
is accompanied by a massive surge in luminosity. Observationally, type Ia supernovae are characterized
by the absence of hydrogen lines in their spectrum and they are considered almost standard candles, in
the sense that there is a strong empirical correlation between their duration and their peak luminosity.
From measurements of their brightness as a function of time (the light curve), their intrinsic luminosity
can be reconstructed. The data are then used to reconstruct the redshift–distance relationship, i.e. the
Hubble diagram, which in turn depends on the cosmological parameters . Current data extend
Roberto Trotta
Current matter power spectrum measurements from the Sloan Digital Sky Survey and best–ﬁt ΛCDM power spectrum
(solid/dashed lines, without/with non–linear corrections), corresponding to the cosmological parameters extracted from WMAP 3–yr
CMB data (the top and bottom curves are for two diﬀerent samples). This shows that even without ﬁtting to matter power spectrum
data, the best–ﬁt CMB model is in good agreement with the galaxy distribution data (from ).
out to about z ∼1.4 and encompass a few hundreds of supernovae (a number which will increase by a
factor of 10 thanks to planned searches). Supernovae data were the ﬁrst line of evidence in 1998 
that the expansion of the Universe is accelerating – an eﬀect attributed to the existence of dark energy
(see e.g. ).
(iv) Weak gravitational lensing: the presence of inhomogeneities in the distribution of matter along
the line of sight distorts the shape of background galaxies due to the deﬂection of light rays. This is
most spectacularly displayed in observations of strong lensing, showing the characteristic arc–shaped
multiple images of background galaxies. However, the same physics aﬀects to a much smaller degree
the shape of any background galaxy, distorting it by about 0.1 to 1%. This is called weak gravitational
lensing. Although it is impossible to measure such small distortions for any single galaxy, the eﬀect can
still be detected statistically, by correlating the shear pattern (distortion due to gravitational lensing)
of several thousand galaxies. The resulting weak lensing power spectrum probes a combination of the
geometrical setup (distance to the background sources and to the lens) and of the parameters controlling
the growth of structures, in particular the amount of matter (both visible and dark) and the strength
of the perturbations (for a recent review, see Some recent observations are reported in ).
By dividing the source galaxies into slices of diﬀerent redshift, it is possible to carry out a sort of
“cosmic tomography”, reconstructing the dark matter distribution between us and the sources .
Although it has not yet reached the same level of precision of the CMB, weak lensing shows great
promise for the future in constraining cosmological parameters and in particular dark energy.
Constraining cosmological parameters
As outlined is section 3.1, our inference problem is fully speciﬁed once we give the model (which parameters
are allowed to vary and their prior distribution) and the likelihood function for the data sets under
consideration. For the cosmological observations described above, relevant cosmological parameters can be
broadly classiﬁed in four categories.
(i) Parameters describing the dynamics of the background evolution: they represent the matter–energy
content of the Universe and its expansion history, relating redshift with comoving distance, see Eq. (44).
The Hubble constant today is written as H0 = 100h km/s/Mpc, and is used to deﬁne the critical
energy density, i.e. the energy density needed to make the Universe spatially ﬂat: ρcrit = 1.88×10−29h2
Bayes in the sky
g/cm3. The remaining density parameters (Ωx in Eq. (44)) are then written in units of the critical
energy density, so that for example the energy density in baryons is given by ρb = Ωbρcrit. Standard
parameters include the energy density in photons (Ωγ), neutrinos (Ων), baryons (Ωb), cold dark matter
(Ωcdm), cosmological constant (ΩΛ) or, more generally, a possibly time–dependent dark energy (Ωde).
(ii) Parameters describing the initial conditions for the ﬂuctuations: they give the type of initial conditions,
adiabatic (where the spatial distribution of ﬂuctuations is the same for all ﬂuids emerging from inﬂation,
up to multiplicative factors) or isocurvature (where there is a mismatch between perturbations among
two components). The most general type of initial conditions is described by a correlation matrix that
contains 10 free parameters representing the excitation amplitude of each mode . The simplest
parameterization of the distribution of perturbations with scale is then given for each mode in terms
of a spectral index.
(iii) Nuisance parameters: these often relate to insuﬃciently constrained aspects of the physics of the observed objects, or to uncertainties in the measuring process. We are not interested in determining them,
but accounting for their uncertainty is important in order to obtain a correct estimate of the error on
the physical parameters we are seeking to determine. If the observable quantity has a strong dependence on poorly determined nuisance parameters, then simply ﬁxing the nuisance parameters instead
of marginalizing over them will lead to serious underestimation of the uncertainty for the remaining
parameters. Example of nuisance parameters are the bias factor in galaxy surveys, residual beam calibration uncertainty for CMB data, supernovae intrinsic evolution parameters, intrinsic ellipticity of
background galaxies in weak lensing and others.
(iv) Parameters describing new physics: this is where the exciting frontier of data analysis lies, and we are
trying to constrain or detect eﬀects arising from new physics in the model, such as time–variation of the
ﬁne structure constant, the presence of cosmic strings, the mass of neutrinos, non–trivial topology of the
Universe, extra dimensions, time–variations of dark energy properties, and much more. Although often
framed as a parameter inference problem, this is actually a model comparison question, and is therefore
best tackled with the methods described in section 4. Therefore, in this case the parameter inference
step is only the ﬁrst level towards working out the outcome of the higher level model comparison step.
The joint likelihood function. When the observations are independent, the log–likelihoods for
each observation simply add1. Deﬁning χ2 ≡−2 ln L, we have that
LSS + . . .
One important advantage of combining diﬀerent observations as in Eq. (45) is that each observable has
diﬀerent degenerate directions, i.e. directions in parameter space that are poorly constrained by the data.
By combining two or more types of observables, it is often the case that the two data sets together
have a much stronger constraining power than each one of them separately, because they mutually break
parameters degeneracies. Combination of data sets should never be carried out blindly, however. The
danger is that the data sets might be mutually inconsistent, in which case combining them singles out
in the posterior a region that is not favoured by any of the two data sets separately, which is obviously
unsatisfactory. Such discrepancies might arise because of undetected systematics, or insuﬃcient modelling
of the observations.
In order to account for possible discrepancies of this kind, Ref. suggested to replace Eq. (45) by
where αi are (unknown) weight factors (“hyperparameters”) for the various data sets, which determine
the relative importance of the observations. A non–informative prior is speciﬁed for the hyperparameters,
1This is of course not the case when one is carrying out correlation studies, where the aim is precisely to exploit correlation among
diﬀerent observables (for example, the late Integrated Sachs–Wolfe eﬀect).
Roberto Trotta
which are then integrated out in a Bayesian way, obtaining an eﬀective chi–square
where Ni is the number of data points in data set i. This method has been applied to combine diﬀerent
CMB observations in the pre–WMAP era . A technique based on the comparison of the Bayesian
evidence for diﬀerent data sets has been employed in , while Ref. uses a technique similar in spirit
to the hyperparameter approach outlined above to perform a binning of mutually inconsistent observations
suﬀering from undetected systematics, as explained in .
After the likelihood has been speciﬁed, it remains to work out the posterior pdf, usually obtained
numerically via MCMC technology, and report posterior constraints on the model parameters, e.g. by
plotting 1 or 2–dimensional posterior contours. We now sketch the way this program has been carried out
as far as cosmological parameter estimation is concerned.
Likelihood–based parameter determination. Up until around 2002, the method of choice for
cosmological parameter estimation was either direct numerical maximum likelihood search or
evaluation of the likelihood on a grid in parameter space. Once the likelihood has been mapped out,
(frequentist) conﬁdence intervals for the parameters are obtained by ﬁnding the maximum–likelihood
point (or, equivalently, the minimum chi–square) and by delimiting the region of parameter space where
the log–likelihood drops by a speciﬁed amount (details can be found in any standard statistics textbook).
If the likelihood is a multi–normal Gaussian, then this procedure leads to the familiar “delta chi–square”
rule–of–thumb, i.e. that e.g. a 95.4% (2σ) conﬁdence interval for 1 parameter is delimited by the region
where the χ2 ≡−2 log L increases by ∆χ2 = 4.00 from its minimum value (see e.g. ). Of course the
value of ∆χ2 depends both on the number of parameters being constrained and on the desired conﬁdence
Approximate conﬁdence intervals for each parameter were then usually obtained from the above procedure, after maximising the likelihood across the hidden dimensions rather than marginalising ,
since the latter procedure required a computationally expensive multi–dimensional integration. The rationale was that maximisation is approximately equivalent to marginalisation for close–to–Gaussian problems
(a simple proof can be found in Appendix A of ), although it was early recognized that this is not always
a good approximation for real–life situations . Marginalisation methods based on multi–dimensional
interpolation were devised and applied in order to improve on this respect . Many studies adopted
this methodology, which could not quite be described as fully Bayesian yet since it was likelihood–based
and the the notion of posterior was not explicitly introduced. Often, the choice of particular theoretical
scenarios (for example, a ﬂat Universe or adiabatic initial conditions) or the inclusion of external constraints (such as bounds on the baryonic density coming from Big Bang nucleosyhntesis) were described as
“priors”. A more rigorous terminology would call the former a model choice, while the latter amounts to
inclusion (in the likelihood) of external information. Since the likelihood could be well approximated by a
simple log–normal distribution, its computation cost was fairly low. With the advent of CMBFAST , the
availability of a fast numerical code for the computation of CMB and matter power spectra meant that
grids of up to 30 million points and parameter spaces of dimensionality up to order 10 could be handled
in this way .
1An important technical point is that frequentist conﬁdence intervals are considered random variables — they give the range within
which our estimate of the parameter value will fall e.g. 95.4% of the time if we repeat our measurement N →∞times. The true
value of the parameter is given (although unknown to us) and has no probability statement attached to it. On the contrary, Bayesian
credible intervals containing for example 95.4% of the posterior probability mass represent our degree of belief about the value of the
parameter itself. Often, Bayesian credible intervals are imprecisely called “conﬁdence intervals” (a term that should be reserved for the
frequentist quantity), thus fostering confusion between the two. Perhaps this happens because for Gaussian cases the two results are
formally identical, although their interpretation is profoundly diﬀerent. This can have important consequences if the true value is near
the boundary of the parameter space, in which case the results from the frequentist and Bayesian procedure may diﬀer substantially –
see for an interesting example involving the determination of neutrino masses.
Bayes in the sky
State–of–the art cosmological parameter inference from WMAP 3–year CMB data and Sloan Digital Sky Survey data . Posterior
median and 68% posterior region, obtained for ﬂat priors on the parameter set in the top section, with the exception of the reionization optical depth
τ, for which a ﬂat prior has been adopted on exp(−2τ) instead (adapted from ).
Matter budget parameters
0.5918+0.0020
CMB acoustic angular scale ﬁt (degrees)
Θs = rs(zrec)/dA(zrec) × 180/π
0.0222+0.0007
Baryon density
ωb = Ωbh2 ≈ρb/(1.88 × 10−26kg/m3)
0.1050+0.0041
Cold dark matter density
ωc = Ωcdmh2 ≈ρc/(1.88 × 10−26kg/m3)
Initial conditions parameters
0.690+0.045
Scalar ﬂuctuation amplitude
Primordial scalar power at k = 0.05/Mpc
0.953+0.016
Scalar spectral index
Primordial spectral index at k = 0.05/Mpc
Reionization history (abrupt reionization)
0.087+0.028
Reionization optical depth
Nuisance parameters (for galaxy power spectrum)
1.896+0.074
Galaxy bias factor
See for details.
Nonlinear correction parameter
See for details.
Derived parameters (functions of those above)
1.00 (ﬂat Universe assumed)
Total density/critical density
Ωtot = Ωm + ΩΛ = 1 −Ωκ
0.730+0.019
Hubble parameter
(ωb + ωc)/(Ωtot −ΩΛ)
0.0416+0.0019
Baryon density/critical density
Ωb = ωb/h2
0.197+0.016
CDM density/critical density
Ωcdm = ωc/h2
0.239+0.018
Matter density/critical density
Ωm = Ωb + Ωcdm
0.761+0.017
Cosmological constant density/critical density
ΩΛ ≈h−2ρΛ(1.88 × 10−26kg/m3)
0.756+0.035
Density ﬂuctuation amplitude
See for details.
Bayes in the sky — The rise of MCMC. The watershed moment after which methods based
on likelihood evaluation on a grid where deﬁnitely overcome by Bayesian MCMC methods can perhaps be
indicated in Ref. , which marked one of the last major studies performed using essentially frequentist
techniques. Pioneering works in using MCMC technology for cosmological parameter extraction include the
application to VSA data , the use of simulated annealing and the study of Ref. . But it
was the release of the CosmoMC code1 in 2002 that made a huge impact on the cosmological community,
as CosmoMC quickly became a standard and user–friendly tool for Bayesian parameter inference from CMB,
large scale structure and other data. The favourable scalability of MCMC methods with dimensionality of
the parameter space and the easiness of marginalization were immediately recognized as major advantages
of the method. CosmoMC employs the CAMB code to compute the matter and CMB power spectra from
the physical model parameters. It then employs various MCMC algorithms to sample from the posterior
distribution given current CMB, matter power spectrum (galaxy power spectrum, baryonic acoustic wiggles
and Lyman–α observations) and supernovae data.
State–of–the–art applications of cosmological parameter inference can be found in papers such as . Table 3 summarizes recent posterior credible intervals on the parameters of the vanilla
ΛCDM model introduced above while Figure 6 shows the full 1–D posterior pdf for 6 relevant parameters
(both from Ref. ). The initial conditions emerging from inﬂation are well described by one adiabatic
degree of freedom and a distribution of ﬂuctuations that deviates slightly from scale invariance, but which
is otherwise fairly featureless.
Addition of extra parameters to this basic description (for example, a curvature term, or a time–evolution
of the cosmological constant, in which case it is generically called “dark energy”) is best discussed in terms
of model comparison rather than parameter inference (see next section). The breath and range of studies
aiming at constraining extra parameters is such that it would be impossible to give even a rough sketch
here. However we can say that the simple model described by the 6 cosmological parameters given in
Table 3 appears at the moment appropriate and suﬃcient to explain most of the presently available data.
MCMC is nowadays almost universally employed in one form or another in the cosmology community.
1Available from: cosmologist.info/cosmomc .
Roberto Trotta
Posterior constraints on key cosmological parameters from recent CMB and large scale structure data, compare Table 3. Top
row, from left to right, posterior pdf (normalized to the peak) for the cosmological constant density in units of the critical density, the
(physical) baryons and cold dark matter densities. Bottom row, from left to right: optical depth to reionization, scalar tilt and scalar
ﬂuctuations amplitude. Yellow using WMAP 1–yr data, orange WMAP 3–yr data and red adding Sloan Digital Sky Survey galaxy
distribution data. Spatial ﬂatness and adiabatic initial conditions have been assumed. This set of only 6 parameters (plus 2 other
nuisance parameters not shown here) appear currently suﬃcient to describe most cosmological observations (adapted from ).
Recent developments in parameter inference. Nowadays, the likelihood evaluation step is becoming the bottleneck of cosmological parameter inference, as the WMAP likelihood code requires
the evaluation of fairly complex correlation terms, while the computational time for the actual model
prediction in terms of power spectra is subdominant (except for spatially curved models or non–standard
scenarios containing active seeds, such as cosmic strings). This trend is likely to become stronger with future CMB datasets, such as Planck. Currently, for relatively straightforward extensions of the concordance
model presented in Table 3, a Markov chain with enough samples to give good inference can be obtained
in a couple of days of computing time on a “oﬀ–the–shelf” machine with 4 CPUs.
Massive savings in computational eﬀort can be obtained by employing neural networks techniques, a
computational methodology consistent of a network of processors called “neurons”. Neural networks learn
in an unsupervised fashion the structure of the computation to be performed (for example, likelihood evaluation across the cosmological parameter space) from a training set provided by the user. Once trained, the
network becomes a fast and eﬃcient interpolation algorithm for new points in the parameter space, or for
classiﬁcation purposes (for example to determine the redshift of galaxies from photometric data ).
When applied to the problem of cosmological parameter inference, neural networks can teach themselves
the structure of the parameter space (for models up to about 10 dimensions) by employing as little as a
few thousands training points, for which the likelihood has to be computed numerically as usual. Once
trained, the network can then interpolate extremely fast between samples to deliver a complete Markov
chain within a few minutes. The speed–up can thus reach a factor of 30 or more . Another promising tool is a machine–learning based algorithm called PICO 1, requiring a training set of order
∼104 samples, which are then used to perform an interpolation of the likelihood across parameter space.
This procedure can achieve a speed–up of over a factor of 1000 with respect to conventional MCMC.
The forefront of Bayesian parameter extraction is quickly moving on to tackle even more ambitious
problems, involving thousands of parameters. This is the case for the Gibbs sampling technique to extract
the full posterior probability distribution for the power spectrum Cℓ’s directly from CMB maps, performing
component separations (i.e., multi–frequency foregrounds removal) at the same time and fully propagating
uncertainties to the level of cosmological parameters . This method has been tested up to ℓ∼
on WMAP temperature data and is expected to perform well up to ℓ∼
< 100−200 for Planck–quality data.
1Available from: .
Bayes in the sky
Equally impressive is the Hamiltonian sampling approach , which returns the Cℓ’s posterior pdf from
a (previously foreground subtracted) CMB map. At WMAP resolution, this involves working with ∼105
parameters, but the eﬃciency is such that the 800 Cℓdistributions (for the temperature signal) can be
obtained in about a day of computation on a high–end desktop computer.
Another frontier of Bayesian methods is represented by high energy particle physics, which for historical
and methodological reasons has been so far mostly dominated by frequentist techniques. However, the
Bayesian approach to parameter inference for supersymmetric theories is rapidly gathering momentum,
due to its superior handling of nuisance parameters, marginalization and inclusion of all sources of uncertainty. The use of Bayesian MCMC for supersymmetry parameter extraction has been ﬁrst advocated
in , and has then been rigorously applied to a detailed analysis of the Constrained Minimal Supersymmetric Standard Model , a problem that involves order 10 free parameters. A public code called
SuperBayeS is available to perform an MCMC Bayesian analysis of supersymmetric models1. Presently,
there are hints that the constraining power of the data is insuﬃcient to override the prior choice in this
context , but future observations, most notably by the Tevatron or LHC and tighter limits
(or a detection) on the neutralino scattering cross section , should considerably improve the situation
in this respect.
Caveats and common pitfalls
Although Bayesian inference is quickly maturing to become an almost automated procedure, we should
not forget that a “black box” approach to the problem always hides dangers and pitfalls. Every real world
problem presents its own peculiarities that demand careful consideration and statistical inference remains
very much a craft as much as a science. While Bayes’ Theorem is never wrong, incorrect speciﬁcation of the
prior (for example, making unwarranted assumptions of failing to specify relevant external information)
or inappropriate construction of the likelihood (e.g., not reﬂecting the experiment or neglecting relevant
sources of uncertainty) will easily lead to wrong inferences. Below we list a few common pitfalls that need
to be considered in the cosmological context.
(i) Hidden prior information. Sometimes the choice of ﬂat priors on the parameters is uncritically taken
to be uninformative. This is often not the case. For instance, a ﬂat prior is indeed non–informative if
we are estimating the mean of a Gaussian, but if we are interested in its standard deviation σ a prior
which is ﬂat in ln σ (“Jeﬀreys’ prior”) is instead the appropriate choice (see e.g. ). When considering
extensions of the ΛCDM model, for example, it is common practice to parameterize the new physics
with a set of quantities about which very little (if anything at all) is known a priori. Assuming ﬂat
priors on those parameters is often an unwarranted choice. Flat priors in “fundamental” parameters will
in general not correspond to a ﬂat distribution for the observable quantities that are derived from those
parameters, nor for other quantities we might be interested in constraining. Hence the apparently non–
informative choice for the fundamental parameters is actually highly informative for other quantities
that appear directly in the likelihood. It is extremely important that this “hidden prior information”
be brought to light, or one could mistake the eﬀect of the prior for constraining power of the data. An
eﬀective way to do that is to plot pdf’s for the quantities of interest without including the data, i.e.
run an MCMC sampling from the prior only (see for an instructive example).
(ii) No well–deﬁned prior measure. Another diﬃculty is that in many cases several physically equally
plausible parameterizations exist, in particular for problems involving unknown amplitudes, such as for
example isocurvature modes in the initial conditions. Since a ﬂat prior on parameterization A is not ﬂat
in parameterization B if the two are related by a non–linear transformation, see Eq. (11), two physically
equivalent setups might lead to widely diﬀerent inferences. In the absence of theoretical or physical
reasons to prefer parameterization A or B this leads to the unsatisfactory dependence of the posterior on
the volume enclosed by the prior, as discussed for the problem of general initial conditions in . For
an example of such a “prior volume eﬀect” applied to inﬂationary models parameters, see . Other
1Code available from: superbayes.org .
Roberto Trotta
obvious domains where this eﬀect might be problematic are dark energy equation of state reconstruction
(e.g., ), initial power spectrum reconstruction and determination of inﬂationary
potential parameters . In some special cases, fundamental principles can be invoked to deﬁne
the appropriate prior measure, which exploits either symmetries or invariance properties of the problem.
An important example is image reconstruction, where the Maximum Entropy principle is employed to
deﬁne an informative prior on the image space (see e.g. for an astronomical application).
(iii) Lindley’s paradox. A methodological issue is the widespread use of inappropriate tools to answer
what is actually a model selection question. Often we want to assess the “signiﬁcance” of an eﬀect, when
a deviation is observed in the posterior from the parameter value that would correspond to the absence
of that eﬀect. This situation is extremely common across very diﬀerent domains, from estimation of
photon number counts in presence of a background (see for a proper Bayesian treatment),
to the assessment of the anomalies in the large–scale CMB power spectrum ( compare frequentist
methods with the Bayesian evidence), the detection of gravitational waves ( introduce a Bayesian
technique that is automatically optimal) or the discovery of extra–solar planets . In general, the
“number of sigma” away from the expected value in the absence of a signal is not a good indicator
of the signiﬁcance of the eﬀect, a result that goes under the name of “Lindley’s paradox” . For
further details, see Appendix A in .
(iv) Using the data twice. Often the data are used to guide in a qualitative fashion the model building
or the choice of priors. If the same data are then used for a quantitative comparison the signiﬁcance
of the eﬀect can be drastically overestimated. This is a well–known problem in frequentist statistics,
which therefore insists that one should design one’s statistical tests before seeing the data at all. In
cosmology this might often be problematic or impossible to achieve, as new observations will uncover
completely unexpected phenomena (for example, the large–scale anomalies in the cosmic microwave
background). It is however important to keep in mind that the signiﬁcance of such eﬀects is diﬃcult
to assess.
Cosmological Bayesian model building
The Bayesian model comparison approach based on the evaluation of the evidence is being increasingly
applied to model building questions such as: are isocurvature contributions to the initial conditions required
by the data ? Is the Universe ﬂat ? What is the best description of the primordial
power spectrum for density perturbations ? Is dark energy best described as a
cosmological constant ? In this section we review the status of the ﬁeld.
Evidence for the cosmological concordance model
Table 4 is a fairly extensive compilation of recent results regarding possible extensions to (or reduction
of) the vanilla ΛCDM concordance cosmological model introduced in section 5.1. We have chosen to
compile only results obtained using the full Bayesian evidence, rather than approximate model comparisons
obtained via the information criteria because the latter are often not adequate approximations, for the
reasons explained in section 4.7. Of course, the outcome depends on the Occam’s razor eﬀect brought
about by the prior volume (and sometimes, by the choice of parameterization). Where applicable, we have
show the sensitivity of the result on the prior assumptions by giving a ballpark range of values for the
Bayes factor, as presented in the original studies. The reader ought to refer to the original works for the
precise prior and parameter choices and for the justiﬁcation of the assumed prior ranges.
As anticipated, the 6 parameters ΛCDM concordance model is currently well supported by the data,
as the inclusion of extra parameters is not required by the Bayesian evidence. This is shown by the
fact that most model comparisons return either an undecided result or they support the ΛCDM model
(negative values for ln B in Table 4). The only exception is the support for a cut–oﬀon large scales in the
power spectrum reported by . This is clearly driven by the anomalies in the large scale CMB power
spectrum, which in this case are interpreted as being a reﬂection of a lack of power in the primordial power
Bayes in the sky
Summary of model comparison results against the ΛCDM
concordance model (see Table 3) using Bayesian model comparison for nested
models. A negative (positive) value for ln B indicates that the competing model is disfavoured (supported) with respect to the ΛCDM model. The
column ∆Npar gives the diﬀerence in the number of free parameters with respect to the ΛCDM concordance model. A negative value means that one
of the parameters has been ﬁxed. See references for full details and in particular for the choice of priors on the model parameters, which control the
strengths of the Occam’s razor eﬀect.
Competing model
Initial conditions
Isocurvature modes
CDM isocurvature
WMAP3+, LSS
Strong evidence for adiabaticity
+ arbitrary correlations
WMAP1+, LSS, SN Ia
Neutrino entropy
[−2.5, −6.5]p
WMAP3+, LSS
Moderate to strong evidence for adiabaticity
+ arbitrary correlations
WMAP1+, LSS, SN Ia
Neutrino velocity
[−2.5, −6.5]p
WMAP3+, LSS
Moderate to strong evidence for adiabaticity
+ arbitrary correlations
WMAP1+, LSS, SN Ia
Primordial power spectrum
No tilt (ns = 1)
WMAP1+, LSS
[−1.1, −0.6]p
WMAP1+, LSS
WMAP1+, LSS
[−0.7, −1.7]p,d
ns = 1 weakly disfavoured
WMAP3+, LSS
ns = 1 weakly disfavoured
ns = 1 moderately disfavoured
WMAP3+, LSS
ns = 1 moderately disfavoured
WMAP3+, LSS
Moderate evidence at best against ns ̸= 1
[−0.6, 1.0]p,d
WMAP3+, LSS
No evidence for running
WMAP3+, LSS
Running not required
Running of running
WMAP3+, LSS
Not required
Large scales cut–oﬀ
[1.3, 2.2]p,d
WMAP3+, LSS
Weak support for a cut–oﬀ
Matter–energy content
Non–ﬂat Universe
WMAP3+, HST
Flat Universe moderately favoured
WMAP3+, LSS, HST
Flat Universe moderately favoured
Coupled neutrinos
WMAP3+, LSS
No evidence for non–SM neutrinos
Dark energy sector
w(z) = weﬀ̸= −1
[−1.3, −2.7]p
Weak to moderate support for Λ
Moderate support for Λ
WMAP1+, LSS, SN Ia
Weak support for Λ
[−0.2, −1]p
SN Ia, BAO, WMAP3
[−1.6, −2.3]d
SN Ia, GRB
Weak support for Λ
w(z) = w0 + w1z
[−1.5, −3.4]p
Weak to moderate support for Λ
Strong support for Λ
SN Ia, BAO, WMAP3
Weak support for Λ
w(z) = w0 + wa(1 −a)
SN Ia, BAO, WMAP3
Weak support for Λ
[−1.2, −2.6]d
SN Ia, GRB
Weak to moderate support for Λ
Reionization history
No reionization (τ = 0)
WMAP3+, HST
τ ̸= 0 moderately favoured
No reionization and no tilt
WMAP3+, HST
Strongly disfavoured
d Depending on the choice of datasets.
p Depending on the choice of priors.
c Upper bound using Bayesian calibrated p–values, see section 4.5.
Data sets: WMAP1+ (WMAP3+): WMAP 1st year (3–yr) data and other CMB measurements. LSS: Large scale structures data. SN Ia:
supernovae type Ia. BAO: baryonic acoustic oscillations. GRB: gamma ray bursts.
spectrum. Whether such anomalies are of cosmological origin remains however an open question .
If extensions of the model are not supported, reduction of ΛCDM to simpler models is not viable, either:
recent studies employing WMAP 3–yr data ﬁnd that a scale invariant spectrum with no spectral tilt is
now weakly to moderately disfavoured . Also, a Universe with no reionization is no longer
a good description of CMB data, and a non–zero optical depth τ is indeed required .
A few further comments about the results reported in Table 4 are in place:
(i) Regarding the type of initial conditions for cosmological perturbations, all parameter extraction studies
to date (with the exception of ) ﬁnd that a purely adiabatic mode is in agreement with observations,
and constrain the isocurvature fraction to be below about 10% for one single isocurvature mode at
Roberto Trotta
the time and below about 50% for a general mixture of modes . From a model selection
perspective, this means that we expect the purely adiabatic model to be preferred over a more complex
model with a mixture of isocurvature modes. This is indeed the case, but the result is strongly dependent
on the parameterization adopted for the isocurvature sector, which determines the strength of the
Occam’s razor eﬀect. This is a consequence of the diﬃculty of coming up with a well motivated
phenomenological parameterization of the isocurvature amplitudes, see the discussion in .
(ii) The shape of the primordial power spectrum has attracted considerable attention from a model comparison perspective . With the exception of the large scale cut–oﬀmentioned
above, the current consensus appears to be that a power–law distribution of ﬂuctuations, with power
spectrum P(k) = P0(k/k0)ns−1 with ns < 1 is currently the best description. This is usually interpreted
as evidence for inﬂation. However, a proper model comparison of inﬂationary predictions involves including the presence of tensor modes generated by gravitational waves, parameterized in terms of their
amplitude parameter r. Including this extra parameter runs into the diﬃculty of specifying its prior
volume, as the two obvious choices of priors ﬂat in r or log r lead to very diﬀerent model comparison
results . Another problem is that the comparison might be ill–deﬁned, as the simpler model with
ns = 1 and r = 0 is presumably some sort of alternative, unspeciﬁed model without inﬂation that
would not solve the horizon problem. On this ground alone, unless an alternative solution to the horizon problem is put on the table, such an alternative model would be immediately thrown out (see the
discussion in ). Finally, higher–order terms in the Taylor expansion of the power spectrum, such
as a running of the spectral index or a running of the running, are currently not required. This appears
a robust result with respect to a wide choice of priors and data sets.
(iii) Present–day constraints on the curvature of spatial sections are of order |Ωκ| ∼
< 0.01 (with Ωκ =
0 corresponding to a ﬂat, Euclidean geometry), stemming from a combination of CMB, large scale
structures and supernovae data. Choosing a phenomenological prior of width ∆Ωκ = 1 around 0
delivers a moderate support for a ﬂat Universe versus curved models . However, adopting an
inﬂation–motivated prior instead, ∆Ωκ ∼10−5, would lead to an undecided result (ln B = 0) for the
model comparison, as the data are not strong enough to discriminate between the two models in this
case. This can be formalized by considering the Bayesian model complexity for the two choices of priors,
Eq. (35). Noticing that σ/Σ is the ratio between the likelihood and prior widths, for a prior on the
curvature parameter of width 1, σ/Σ ∼10−2 and Cb ≈1, hence the parameter has been measured. But
if we take a prior width ∼10−5, σ/Σ ∼103 hence Cb →0. In the latter case, we can see from Eq. (21)
that the Bayes factor between the two models B01 →1 and the evidence is inconclusive, awaiting
better data.
(iv) Model comparisons regarding the dark energy sector suﬀer from considerably uncertainty. Clearly, the
model to beat is the cosmological constant (with equation of state parameter w = −1 at all redshifts),
but alternative dark energy scenarios suﬀer from the fundamental diﬃculty of motivating physically
both the parameterization of the dark energy time dependence and the prior volume for the extra
parameters (see for a review of models and for on overview of recent constraints).
However, the semi–phenomenological studies shown in Table 4 do agree in deeming a cosmological
constant a suﬃcient description of the data. This is again a consequence of the fact that no time
evolution of the equation of state is detected in the data, hence the strength of the support in favour of
the cosmological constant becomes a function of the available parameter space under the more complex,
alternative models. Given this result, it is interesting to ask what level of accuracy is required before
our degree of belief in the cosmological constant is overwhelmingly larger than for an evolving dark
energy, assuming of course that future data will not detect any signiﬁcant departure from w = −1. To
this end, a simple classiﬁcation of models has been given in in terms of their eﬀective equation of
space parameter, weﬀ, representing the time–varying equation of state averaged over redshift with the
appropriate weighting factor for the observable . The three categories considered are “phantom
models” (exhibiting large, negative values for the equation of state, −11 ≤weﬀ≤−1), “ﬂuid–like dark
energy” (−1 ≤weﬀ≤−1/3) and “small–departures from Λ” models (−1.01 ≤weﬀ≤0.99). Assuming
a ﬂat prior on these ranges of values for weﬀ, consideration of the Bayes factor between each of those
models and the cosmological constant shows that gathering strong evidence against each of the models
Bayes in the sky
Summary of model comparison results against the ΛCDM concordance model for some alternative (i.e., non–nested) cosmological
models. A negative (positive) value for ln B indicates that the competing model is disfavoured (supported) with respect to ΛCDM. The
column Npar gives the number of free parameters in the alternative model. See references for full details about the models, priors and data
Competing model
Alternatives to FRW
Bianchi VIIh
[−0.9, 1.2]d,p
WMAP1, WMAP3
Weak support (at best) for Bianchi template
[−0.1, −1.2]p
No evidence after texture correction
LTB models
WMAP3, BAO, SN Ia
Moderate evidence against LTB
Fractal bubble model
Asymmetry in the CMB
Anomalous dipole
Weak evidence for anomalous dipole
Weak evidence at best
d Depending on the choice of datasets.
p Depending on the choice of priors.
c Upper bound using Bayesian calibrated p–values, see section 4.5.
requires an accuracy on weﬀof order σeﬀ= 0.05 for phantom models (which are therefore already under
pressure from current data, which have an accuracy of order ∼0.1), σeﬀ= 3×10−3 for ﬂuid–like models
(about a factor of 5 better than optimistic constraints from future observations) and σeﬀ= 5 × 10−5
for small–departure models. Reﬁnements of this approach that employ more fundamentally–motivated
priors could lead to an analysis of the expected costs/beneﬁts from future dark energy observations in
terms of their likely model selection outcome (we return on this issue in section 6.2).
Let us now turn to models that are not nested within ΛCDM— i.e., alternative theoretical scenarios.
Table 5 gives some examples of the outcome of the Bayesian model comparison with the concordance
model. As above, we restrict our considerations to studies employing the full Bayesian evidence (there
are many other examples in the literature carrying out approximate model comparison using information
criteria instead). The model comparison is often more diﬃcult for non–nested models, as priors must be
speciﬁed for all of the parameters in the alternative model (and in the ΛCDM model, as well), in order
to compute the evidence ratio. The usual caveats on prior choice apply in this case. From Table 5 it
appears that the data do not seem to require fundamental changes in our underlying theoretical model,
either in the form of Bianchi templates representing a violation of cosmic isotropy (see also ), or as
Lemaitre–Tolman–Bondi models or fractal bubble scenarios with dressed cosmological parameters. The
anomalous dipole in the CMB temperature maps is a ﬁne example of Lindley’s paradox. When ﬁtting a
dipolar template to the CMB maps, the eﬀective chi–square improves by 9 to 11 units (depending on the
details of the analysis) for 3 extra parameters , which would be deemed a “signiﬁcant” eﬀect
using a standard goodness–of–ﬁt test. However, the Bayesian evidence analysis shows that the odds in
favour of an anomalous dipole are 9 to 1 at best (corresponding to ln B < 2.2), which does not reach
the “moderate evidence at best” threshold. Hence Bayesian model comparison is conservative, requiring a
stronger evidence before deeming an eﬀect to be favoured.
Other uses of the Bayesian evidence
Beside cosmological model building, the Bayesian evidence can be employed in many other diﬀerent ways.
Here we presents two aspects that are relevant to our topic, namely the applications to the ﬁeld of multi–
model inference and model selection forecasting.
(i) Multi–model inference. Once we realize that there are several possible models for our data, it
becomes interesting to present parameter inferences that take into account the model uncertainty
associated with this plurality of possibilities. In other words, instead of just constraining parameters
within each model, we can take a step further and produce parameter inferences that are averaged
over the models being considered. Let us suppose that we have a minimal model (in our case, ΛCDM)
and a series of augmented models with extra parameters. A typical example from cosmology is dark
Roberto Trotta
energy (ﬁrst discussed in the context of multi–model averaging in ), where the minimal model
has w = −1 ﬁxed and there are several other candidate models with a time–varying equation of
state, parameterized in terms of a number of free parameters and their priors. Let us denote by θ the
cosmological parameters common to all models. For the extended models, the redshift–dependence of
the dark energy equation of state is described by a vector of parameters ωi (under model Mi). The
ΛCDM model has no free parameters for the equation of state, hence the prior on ωΛCDM is a delta
function centered on w(z) = w0 = −1. Then a straightforward application of Bayes’ theorem leads to
the following posterior distribution for the parameters:
p(θ, ω|d) =
p(Mi|d)p(θ, ω|d, Mi),
where p(θ, ω|d, Mi) is the posterior within each model Mi, and it is understood that the posterior
has non–zero support only along the parameter directions ωi ⊂ω that are relevant for the model, and
delta–functions along all other directions. Each term is weighted by the corresponding posterior model
probability,
p(d|Mi)p(Mi)
i p(d|Mi)p(Mi).
The prior model probabilities p(Mi) are usually set equal, but a model preference can be incorporated
here if necessary. The model averaged posterior distribution of Eq. (48) then represents the parameter
constraints obtained independently of the model choice, which has been marginalized over. Unless one
of the models is overwhelmingly more probable than the others (in which case the model averaging
essentially disappears, as all of the weights for the other models go to zero), the model–averaged
posterior distribution can be signiﬁcantly diﬀerent from the model–speciﬁc distribution. A counter–
intuitive consequence is that in the case of dark energy, the model–averaged posterior shows tighter
constraints around w = −1 than any of the evolving dark energy models by itself. This comes about
because ΛCDM is the preferred model and hence much of the weight in the model–averaged posterior
is shifted to the point w = −1 . For further details on multi–model inference, see e.g. .
(ii) Model selection forecasting. When considering the capabilities of future experiments, it is common stance to predict their performance in terms of constraints on relevant parameters, assuming a
ﬁducial point in parameter space as the true model (often, the current best–ﬁt model). While this is
a useful indicator for parameter inference tasks, many questions in cosmology fall rather in the model
comparison category. A notable example is again dark energy, where the science driver for many future
multi–million–dollar probes is to detect possible departures from a cosmological constant, hence to
gather evidence in favour of an evolving dark energy model. It is therefore preferable to assess the
capabilities of future experiments by their ability to answer model selection questions.
The procedure is as follows (see for details and the application to dark energy scenarios). At
every point in parameter space, mock data from the future observation are generated and the Bayes
factor between the competing models is computed, for example between an evolving dark energy and a
cosmological constant. Then one delimits in parameter space the region where the future data would not
be able to deliver a clear model comparison verdict, for example | ln B| < 5 (evidence falling short of the
“strong” threshold). The experiment with the smallest “model–confusion” volume in parameter space
is to be preferred, since it achieves the highest discriminative power between models. An application
of a related technique to the spectral index from the Planck satellite is presented in .
Alternatively, we can investigate the full probability distribution for the Bayes factor from a future
observation. This allows to make probabilistic statements regarding the outcome of a future model
comparison, and in particular to quantify the probability that a new observation will be able to achieve
a certain level of evidence for one of the models, given current knowledge. This technique is based on
the predictive distribution for a future observation, which gives the expected posterior on θ for an
observation with experimental capabilities described by e (this might describe sky coverage, noise
Bayes in the sky
levels, target redshift, etc):
p(θ|e, d) =
i , e, Mi)p(ψ⋆
i |d, Mi).
Here, d are the currently available observations, p(Mi|d) is the current model posterior, p(θ|ψ⋆
i , e, Mi) is
the posterior on θ from a future observation e computed assuming ψ⋆
i are the correct model parameters,
while each term is weighted by the present probability that ψ⋆
i is the true value of the parameters,
i |d, Mi). The sum over i ensures that the prediction averages over models, as well. From Eq. (50)
we can compute the corresponding probability distribution for ln B from experiment e, for example
by employing MCMC techniques (further details are given in ). This method is called PPOD,
for predictive posterior odds distribution and can be useful in the context of experiment design and
optimization, when the aim is to determine which choice of e will lead to the best scientiﬁc return from
the experiment, in this case in terms of model selection capabilities (see for a discussion of
performance optimization for parameter constraints). For further details on Bayes factor forecasts and
experiment design, see .
Conclusions
Bayesian probability theory oﬀers a consistent framework to deal with uncertainty in several diﬀerent
situations, from parameter inference to model comparison, from prediction to optimization. The notion of
probability as a degree of belief is far more general than the restricted view of probability as frequency, and
it can be applied equally well both to repeatable experiments and to one–oﬀsituations. We have seen how
Bayes’ theorem is a unique prescription to update our state of knowledge in the light of the available data,
and how the basic laws of probability can be used to incorporate all sorts of uncertainty in our inferences,
including noise (measurement uncertainty), systematic errors (hyper–parameters), imperfect knowledge of
the system (nuisance parameters) and modelling uncertainty (model comparison and model averaging).
The same laws can equally well be applied to the problem of prediction, and there is considerable potential
for a systematic exploration of experiment optimization and Bayesian decision theory (e.g., given what we
know about the Universe and our theoretical models, what are the best observations to achieve a certain
scientiﬁc goal?).
The exploration of the full potential of Bayesian methods is only just beginning. Thanks to the increasing
availability of cheap computational power, it now becomes possible to handle problems that were of
intractable complexity until a few years ago. Markov Chain Monte Carlo techniques are nowadays a
standard inference tool to derive parameter constraints, and many algorithms are available to explore
the posterior pdf in a variety of settings. We have highlighted how the issue of priors — which has
traditionally been held against Bayesian methods — is a false problem stemming from a misunderstanding
of what Bayes’ theorem says. This is not to deny that it can be diﬃcult in practice to choose a prior that
is a fair representation of one’s degree of belief. But we should not shy away from this task — the fact
is, there is no inference without assumptions and a correct application of Bayes’ theorem forces us to be
absolutely clear about which assumptions we are making. It remains important to quantify as much as
possible the extent by which our priors are inﬂuencing our results, since in many cases when working at
the cutting–edge of research we might not have the luxury of being in a data–dominated regime.
The model comparison approach can formalize in a quantitative manner the intuitive assessment of
scientiﬁc theories, based on the Occam’s razor notion that simpler models ought to be preferred if they
oﬀer a satisfactory explanation for the observations. The Bayesian evidence and complexity tell us which
models are supported by the data, and what is their eﬀective number of parameters. Multi–model inference
delivers model–averaged parameter constraints, thus merging the two levels of parameter inference and
model comparison.
The application of Bayesian tools to cosmology and astrophysics is blossoming. As both data sets and
models become more complex, our inference tools must acquire a corresponding level of sophistication, as
Roberto Trotta
basic statistical analyses that served us well in the past are no longer up to the task. There is little doubt
that the ﬁeld of cosmostatistics will grow in importance in the future, and Bayesian methods will have a
great role to play.
Acknowledgments
I am grateful to Stefano Andreon, Sarah Bridle, Chris Gordon, Andrew Liddle, Nicolai Meinshausen and
Joe Silk for comments on an earlier draft and for stimulating discussions, and to Martin Kunz, Louis Lyons,
Mike Hobson and Steﬀen Lauritzen for many useful conversations. This work is supported by the Royal
Astronomical Society through the Sir Norman Lockyer Fellowship, and by St Anne’s College, Oxford.