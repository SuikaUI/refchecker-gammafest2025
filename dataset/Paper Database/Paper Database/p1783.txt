Practical Block-wise Neural Network Architecture Generation
Zhao Zhong1,3∗, Junjie Yan2,Wei Wu2,Jing Shao2,Cheng-Lin Liu1,3,4
1National Laboratory of Pattern Recognition,Institute of Automation, Chinese Academy of Sciences
2 SenseTime Research
3 University of Chinese Academy of Sciences
4 CAS Center for Excellence of Brain Science and Intelligence Technology
Email: {zhao.zhong, liucl}@nlpr.ia.ac.cn, {yanjunjie, wuwei, shaojing}@sensetime.com
Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a
block-wise network generation pipeline called BlockQNN
which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by
the learning agent which is trained sequentially to choose
component layers. We stack the block to construct the whole
auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive
results in comparison to the hand-crafted state-of-the-art
networks on image classiﬁcation, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error
rate on CIFAR-10 which beats all existing auto-generate
networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which
only spends 3 days with 32 GPUs, and (3) moreover, it has
strong generalizability that the network built on CIFAR also
performs well on a larger-scale ImageNet dataset.
1. Introduction
During the last decades, Convolutional Neural Networks
(CNNs) have shown remarkable potentials almost in every ﬁeld in the computer vision society . For example, thanks to the network evolution from AlexNet ,
VGG , Inception to ResNet , the top-5 performance on ImageNet challenge steadily increases from
83.6% to 96.43%.
However, as the performance gain
usually requires an increasing network capacity, a high-
∗This work was done when Zhao Zhong worked as an intern at Sense-
Time Research.
performance network architecture generally possesses a
tremendous number of possible conﬁgurations about the
number of layers, hyperparameters in each layer and type
of each layer. It is hence infeasible for manually exhaustive search, and the design of successful hand-crafted networks heavily rely on expert knowledge and experience.
Therefore, constructing the network in a smart and automatic manner remains an open problem.
Although some recent works have attempted computeraided or automated network design , there are several challenges still unsolved: (1) Modern neural networks
always consist of hundreds of convolutional layers, each
of which has numerous options in type and hyperparameters. It makes a huge search space and heavy computational
costs for network generation. (2) One typically designed
network is usually limited on a speciﬁc dataset or task, and
thus is hard to transfer to other tasks or generalize to another
dataset with different input data sizes. In this paper, we provide a solution to the aforementioned challenges by a novel
fast Q-learning framework, called BlockQNN, to automatically design the network architecture, as shown in Fig. 1.
Particularly, to make the network generation efﬁcient
and generalizable, we introduce the block-wise network
generation, i.e., we construct the network architecture as a
ﬂexible stack of personalized blocks rather tedious per-layer
network piling. Indeed, most modern CNN architectures
such as Inception and ResNet Series 
are assembled as the stack of basic block structures. For
example, the inception and residual blocks shown in Fig. 1
are repeatedly concatenated to construct the entire network.
With such kind of block-wise network architecture, the
generated network owns a powerful generalization to other
task domains or different datasets.
In comparison to previous methods like NAS and
MetaQNN , as depicted in Fig. 1, we present a more
readily and elegant model generation method that specifically designed for block-wise generation. Motivated by
the unsupervised reinforcement learning paradigm, we employ the well-known Q-learning with experience rearXiv:1708.05552v3 [cs.CV] 14 May 2018
Dropout 1/8
Dropout 1/4
Dropout 3/8
Inception-block
Residue-block
Hand-crafted Network
Auto-generated Network
Hand-crafted Network
Auto-generated Network
11x11 Conv
Linear 4096
Linear 4096
Figure 1. The proposed BlockQNN (right in red box) compared with the hand-crafted networks marked in yellow and the existing autogenerated networks in green. Automatically generating the plain networks marked in blue need large computational costs on
searching optimal layer types and hyperparameters for each single layer, while the block-wise network heavily reduces the cost to search
structures only for one block. The entire network is then constructed by stacking the generated blocks. Similar block concept has been
demonstrated its superiority in hand-crafted networks, such as inception-block and residue-block marked in red.
play and epsilon-greedy strategy to effectively and
efﬁciently search the optimal block structure. The network
block is constructed by the learning agent which is trained
sequentiality to choose component layers. Afterwards we
stack the block to construct the whole auto-generated network. Moreover, we propose an early stop strategy to enable efﬁcient search with fast convergence. A novel reward
function is designed to ensure the accuracy of the early
stopped network having positive correlation with the converged network. We can pick up good blocks in reduced
training time using this property.
With this acceleration
strategy, we can construct a Q-learning agent to learn the
optimal block-wise network structures for a given task with
limited resources (e.g. few GPUs or short time period). The
generated architectures are thus succinct and have powerful
generalization ability compared to the networks generated
by the other automatic network generation methods.
The proposed block-wise network generation brings a
few advantages as follows:
• Effective.
The automatically generated networks
present comparable performances to those of handcrafted networks with human expertise. The proposed
method is also superior to the existing works and
achieves a state-of-the-art performance on CIFAR-10
with 3.54% error rate.
• Efﬁcient.
We are the ﬁrst to consider block-wise
setup in automatic network generation.
with the proposed early stop strategy, the proposed
method results in a fast search process. The network
generation for CIFAR task reaches convergence with
only 32 GPUs in 3 days, which is much more efﬁcient
than that by NAS with 800 GPUs in 28 days.
• Transferable. It offers surprisingly superior transferable ability that the network generated for CIFAR can
be transferred to ImageNet with little modiﬁcation but
still achieve outstanding performance.
2. Related Work
Early works, from 1980s, have made efforts on automating neural network design which often searched good architecture by the genetic algorithm or other evolutionary algorithms . Nevertheless, these works,
to our best knowledge, cannot perform competitively compared with hand-crafted networks. Recent works, i.e. Neural Architecture Search (NAS) and MetaQNN ,
adopted reinforcement learning to automatically search a
good network architecture. Although they can yield good
performance on small datasets such as CIFAR-10, CIFAR-
100, the direct use of MetaQNN or NAS for architecture
design on big datasets like ImageNet is computationally
expensive via searching in a huge space. Besides, the network generated by this kind of methods is task-speciﬁc or
dataset-speciﬁc, that is, it cannot been well transferred to
other tasks nor datasets with different input data sizes. For
example, the network designed for CIFAR-10 cannot been
generalized to ImageNet.
Instead, our approach is aimed to design network block
architecture by an efﬁcient search method with a distributed asynchronous Q-learning framework as well as an
early-stop strategy. The block design conception follows
the modern convolutional neural networks such as Inception and Resnet . The inception-based
networks construct the inception blocks via a handcrafted multi-level feature extractor strategy by computing
1 × 1, 3 × 3, and 5 × 5 convolutions, while the Resnet uses
residue blocks with shortcut connection to make it
easier to represent the identity mapping which allows a very
deep network. The blocks automatically generated by our
Kernel Size
Convolution
Max Pooling
Average Pooling
Elemental Add
Table 1. Network Structure Code Space. The space contains seven
types of commonly used layers. Layer index stands for the position of the current layer in a block, the range of the parameters is
set to be T = {1, 2, 3, ...max layer index}. Three kinds of kernel sizes are considered for convolution layer and two sizes for
pooling layer. Pred1 and Pred2 refer to the predecessor parameters which is used to represent the index of layers predecessor, the
allowed range is K = {1, 2, ..., current layer index −1}
Codes = [(1,4,0,0,0), (2,1,1,1,0), (3,1,3,2,0),
(4,1,1,1,0), (5,1,5,4,0), (6,6,0,3,5),
(7,2,3,1,0), (8,1,1,7,0), (9,6,0,6,8),
(10,7,0,0,0)]
Codes = [(1,4,0,0,0), (2,1,3,1,0),
(3,1,3,2,0), (4,5,0,1,3),
(5,7,0,0,0)]
Figure 2. Representative block exemplars with their Network
structure codes (NSC) respectively: the block with multi-branch
connections (left) and the block with shortcut connections (right).
approach have similar structures such as some blocks contain short cut connections and inception-like multi-branch
combination. We will discuss the details in Section 5.1.
Another bunch of related works include hyper-parameter
optimization , meta-learning and learning to learn
methods . However, the goal of these works is to
use meta-data to improve the performance of the existing
algorithms, such as ﬁnding the optimal learning rate of optimization methods or the optimal number of hidden layers
to construct the network. In this paper, we focus on learning the entire topological architecture of network blocks to
improve the performance.
3. Methodology
3.1. Convolutional Neural Network Blocks
The modern CNNs, e.g. Inception and Resnet, are designed by stacking several blocks each of which shares
Figure 3. Auto-generated networks on CIFAR-10 (left) and ImageNet (right). Each network starts with a few convolution layers to learn low-level features, and followed by multiple repeated
blocks with several pooling layers inserted to downsample.
similar structure but with different weights and ﬁlter numbers to construct the network. With the block-wise design,
the network can not only achieves high performance but
also has powerful generalization ability to different datasets
and tasks. Unlike previous research on automating neural
network design which generate the entire network directly,
we aim at designing the block structure.
As a CNN contains a feed-forward computation procedure, we represent it by a directed acyclic graph (DAG),
where each node corresponds to a layer in the CNN while
directed edges stand for data ﬂow from one layer to another.
To turn such a graph into a uniform representation, we propose a novel layer representation called Network Structure
Code (NSC), as shown in Table 2. Each block is then depicted by a set of 5-D NSC vectors. In NSC, the ﬁrst three
numbers stand for the layer index, operation type and kernel
size. The last two are predecessor parameters which refer
to the position of a layer’s predecessor layer in structure
codes. The second predecessor (Pred2) is set for the layer
owns two predecessors, and for the layer with only one predecessor, Pred2 will be set to zero. This design is motivated
by the current powerful hand-crafted networks like Inception and Resnet which own their special block structures.
This kind of block structure shares similar properties such
as containing more complex connections, e.g. shortcut connections or multi-branch connections, than the simple connections of the plain network like AlexNet. Thus, the proposed NSC can encode complexity architectures as shown
in Fig. 2. In addition, all of the layer without successor in
the block are concatenated together to provide the ﬁnal output. Note that each convolution operation, same as the dec-
Agent samples
structure codes
Stack blocks
to generate a
network on a
validation
accuracy as
(1,1,1,0,0)
(1,x,x,x,x)
(2,1,3,1,0)
(2,x,x,x,x)
(T,x,x,x,x)
(T,x,x,x,x)
(1,7,0,0,0)
(2,7,0,0,0)
(T,7,0,0,0)
(3,1,3,1,0)
(3,x,x,x,x)
(3,7,0,0,0)
Figure 4. Q-learning process illustration. (a) The state transition process by different action choices. The block structure in (b) is generated
by the red solid line in (a). (c) The ﬂow chart of the Q-learning procedure.
laration in Resnet , refers to a Pre-activation Convolutional Cell (PCC) with three components, i.e. ReLU, Convolution and Batch Normalization. This results in a smaller
searching space than that with three components separate
search, and hence with the PCC, we can get better initialization for searching and generating optimal block structure
with a quick training process.
Based on the above deﬁned blocks, we construct
the complete network by stacking these block structures
sequentially which turn a common plain network into
its counterpart block version.
Two representative autogenerated networks on CIFAR and ImageNet tasks are
shown in Fig. 3.
There is no down-sampling operation
within each block.
We perform down-sampling directly
by the pooling layer. If the size of feature map is halved
by pooling operation, the block’s weights will be doubled.
The architecture for ImageNet contains more pooling layers
than that for CIFAR because of their different input sizes,
i.e. 224 × 224 for ImageNet and 32 × 32 for CIFAR. More
importantly, the blocks can be repeated any N times to
fulﬁll different demands, and even place the blocks in other
manner, such as inserting the block into the Network-in-
Network framework or setting short cut connection between different blocks.
3.2. Designing Network Blocks With Q-Learning
Albeit we squeeze the search space of the entire network
design by focusing on constructing network blocks, there
is still a large amount of possible structures to seek. Therefore, we employ reinforcement learning rather than random
sampling for automatic design. Our method is based on Qlearning, a kind of reinforcement learning, which concerns
how an agent ought to take actions so as to maximize the
cumulative reward. The Q-learning model consists of an
agent, states and a set of actions.
In this paper, the state s ∈S represents the status of
the current layer which is deﬁned as a Network Structure
Code (NSC) claimed in Section 3.1, i.e. 5-D vector {layer
index, layer type, kernel size, pred1, pred2}. The action
a ∈A is the decision for the next successive layer. Thanks
to the deﬁned NSC set with a limited number of choices,
both the state and action space are thus ﬁnite and discrete
to ensure a relatively small searching space. The state transition process (st, a(st)) →(st+1) is shown in Fig. 4(a),
where t refers to the current layer. The block example in
Fig. 4(b) is generated by the red solid lines in Fig. 4(a). The
learning agent is given the task of sequentially picking NSC
of a block. The structure of block can be considered as an
action selection trajectory τa1:T , i.e. a sequence of NSCs.
We model the layer selection process as a Markov Decision
Process with the assumption that a well-performing layer in
one block should also perform well in another block . To
ﬁnd the optimal architecture, we ask our agent to maximize
its expected reward over all possible trajectories, denoted
Rτ = EP (τa1:T )[R],
where the R is the cumulative reward. For this maximization problem, it is usually to use recursive Bellman Equation
to optimality. Given a state st ∈S, and subsequent action
a ∈A(st), we deﬁne the maximum total expected reward
to be Q∗(st, a) which is known as Q-value of state-action
pair. The recursive Bellman Equation then can be written as
Q∗(st, a) = Est+1|st,a[Er|st,a,st+1[r|st, a, st+1]
a′∈A(st+1)) Q∗(st+1, a′)].
An empirical assumption to solve the above quantity is
to formulate it as an iterative update:
Q(sT , a) =0
Q(sT −1, aT ) =(1 −α)Q(sT −1, aT ) + αrT
Q(st, a) =(1 −α)Q(st, a)
+α[rt + γ max
a′ Q(st+1, a′)], t ∈{1, 2, ...T −2},
where α is the learning rate which determines how the
newly acquired information overrides the old information,
γ is the discount factor which measures the importance of
future rewards. rt denotes the intermediate reward observed
Accuracy (%)
Iteration (batch)
Q-learning Performance with Different
intermediate reward
shaped reward
Figure 5. Comparison results of Q-learning with and without the
shaped intermediate reward rt. By taking our shaped reward, the
learning process convergent faster than that without shaped reward
start from the same exploration.
for the current state st and sT refers to ﬁnal state, i.e. terminal layers. rT is the validation accuracy of corresponding
network trained convergence on training set for aT , i.e. action to ﬁnal state. Since the reward rt cannot be explicitly
measured in our task, we use reward shaping to speed
up training. The shaped intermediate reward is deﬁned as:
Previous works ignore these rewards in the iterative
process, i.e. set them to zero, which may cause a slow convergence in the beginning. This is known as the temporal
credit assignment problem which makes RL time consuming . In this case, the Q-value of sT is much higher than
others in early stage of training and thus leads the agent prefer to stop searching at the very beginning, i.e. tend to build
small block with fewer layers. We show a comparison result
in Fig. 5, the learning process of the agent with our shaped
reward rt convergent much faster than previous method.
We summarize the learning procedure in Fig. 4(c). The
agent ﬁrst samples a set of structure codes to build the
block architecture, based on which the entire network is
constructed by stacking these blocks sequentially. We then
train the generated network on a certain task, and the validation accuracy is regarded as the reward to update the Qvalue. Afterwards, the agent picks another set of structure
codes to get a better block structure.
3.3. Early Stop Strategy
Introducing block-wise generation indeed increases
the efﬁciency. However, it is still time consuming to complete the search process. To further accelerate the learning process, we introduce an early stop strategy. As we all
know, early stopping training process might result in a poor
accuracy. Fig. 6 shows an example, where the early-stop accuracy in yellow line is much lower than the ﬁnal accuracy
in orange line, which means that some good blocks unfortunately perform worse than bad blocks when stop training
Scalar for FLOPs and Density
Accuracy (%)
Model (block)
Data Analysis of Early Stop Accuracy
Early Stop ACC
Redefined Reward
Figure 6. The performance of early stop training is poorer than the
ﬁnal accuracy of a complete training. With the help of FLOPs and
Density, it squeezes the gap between the redeﬁned reward function
and the ﬁnal accuracy.
early. In the meanwhile, we notice that the FLOPs and density of the corresponding blocks have a negative correlation
with the ﬁnal accuracy. Thus, we redeﬁne the reward function as
reward = ACCEarlyStop −µ log(FLOPs)
−ρ log(Density),
where FLOPs refer to an estimation of computational
complexity of the block, and Density is the edge number
divided by the dot number in DAG of the block. There are
two hyperparameters, µ and ρ, to balance the weights of
FLOPs and Density. With the redeﬁned reward function,
the reward is more relevant to the ﬁnal accuracy.
With this early stop strategy and small searching space of
network blocks, it just costs 3 days to complete the searching process with only 32 GPUs, which is superior to that
of , spends 28 days with 800 GPUs to achieve the same
performance.
4. Framework and Training Details
4.1. Distributed Asynchronous Framework
To speed up the learning of agent, we use a distributed
asynchronous framework as illustrated in Fig. 7. It consists
of three parts: master node, controller node and compute
nodes. The agent ﬁrst samples a batch of block structures in
master node. Afterwards, we store them in a controller node
which uses the block structures to build the entire networks
and allocates these networks to compute nodes. It can be
regarded as a simpliﬁed parameter-server . Specifically, the network is trained in parallel on each of compute nodes and returns the validation accuracy as reward by
controller nodes to update agent. With this framework, we
Controller Node
Master Node
Compute Nodes
Figure 7. The distributed asynchronous framework. It contains
three parts: master node, controller node and compute nodes.
Table 2. Epsilon Schedules. The number of iteration the agent
trains at each epsilon(ϵ) state.
can generate network efﬁciently on multiple machines with
multiple GPUs.
4.2. Training Details
Epsilon-greedy Strategy. The agent is trained using Qlearning with experience replay and epsilon-greedy
strategy . With epsilon-greedy strategy, the random action is taken with probability ϵ and the greedy action is chosen with probability 1 −ϵ. We decrease epsilon from 1.0 to
0.1 following the epsilon schedule as shown in Table 2 such
that the agent can transform smoothly from exploration to
exploitation. We ﬁnd that the result goes better with a longer
exploration, since the searching scope would become larger
and the agent can see more block structures in the random
exploration period.
Experience Replay. Following , we employ a replay
memory to store the validation accuracy and block description after each iteration. Within a given interval, i.e. each
training iteration, the agent samples 64 blocks with their
corresponding validation accuracies from the memory and
updates Q-value 64 times.
BlockQNN Generation.
In the Q-learning update process, the learning rate α is
set to 0.01 and the discount factor γ is 1. We set the hyperparameters µ and ρ in the redeﬁned reward function as 1
and 8, respectively. The agent samples 64 sets of NSC vectors at a time to compose a mini-batch and the maximum
layer index for a block is set to 23. We train the agent with
178 iterations, i.e. sampling 11, 392 blocks in total.
During the block searching phase, the compute nodes
train each generated network for a ﬁxed 12 epochs on
CIFAR-100 using the early top strategy as described in Section 3.3. CIFAR-100 contains 60, 000 samples with 100
classes which are divided into training and test set with the
ratio of 5 : 1. We train the network without any data augmentation procedure.
The batch size is set to 256.
use Adam optimizer with β1 = 0.9, β2 = 0.999,
ε = 10−8. The initial learning rate is set to 0.001 and is
reduced with a factor of 0.2 every 5 epochs. All weights are
initialized as in . If the training result after the ﬁrst epoch
is worse than the random guess, we reduce the learning rate
by a factor of 0.4 and restart training, with a maximum of 3
times for restart-operations.
After obtaining one optimal block structure, we build
the whole network with stacked blocks and train the network until converging to get the validation accuracy as the
criterion to pick the best network. In this phase, we augment data with randomly cropping the images with size of
32 × 32 and horizontal ﬂipping. All models use the SGD
optimizer with momentum rate set to 0.9 and weight decay
set to 0.0005. We start with a learning rate of 0.1 and train
the models for 300 epochs, reducing the learning rate in the
150-th and 225-th epoch. The batch size is set to 128 and
all weights are initialized with MSRA initialization .
Transferable BlockQNN. We also evaluate the transferability of the best auto-generated block structure searched
on CIFAR-100 to a smaller dataset, CIFAR-10, with only
10 classes and a larger dataset, ImageNet, containing 1.2M
images with 1000 classes. All the experimental settings are
the same as those on the CIFAR-100 stated above. The
training is conducted with a mini-batch size of 256 where
each image has data augmentation of randomly cropping
and ﬂipping, and is optimized with SGD strategy. The initial learning rate, weight decay and momentum are set as
0.1, 0.0001 and 0.9, respectively. We divide the learning
rate by 10 twice, at the 30-th and 60-th epochs. The network is trained with a total of 90 epochs. We evaluate the
accuracy on the test images with center crop.
Our framework is implemented under the PyTorch scientiﬁc computing platform. We use the CUDA backend
and cuDNN accelerated library in our implementation for
high-performance GPU acceleration. Our experiments are
carried out on 32 NVIDIA TitanX GPUs and took about 3
days to complete searching.
5. Results
5.1. Block Searching Analysis
Fig. 8(a) provides early stop accuracies over 178 batches
on CIFAR-100, each of which is averaged over 64 autogenerated block-wise network candidates within in each
mini-batch. After random exploration, the early stop accuracy grows steadily till converges. The mean accuracy
within the period of random exploration is 56% while ﬁnally achieves 65% in the last stage with ϵ = 0.1. We
choose top-100 block candidates and train their respective
networks to verify the best block structure. We show top-
(b) Block-QNN-A
(c) Block-QNN-B
Accuracy (%)
Iteration (batch)
Mean Accuracy
Random Exploration
Start Exploitation
Block-QNN-A
Block-QNN-B
(a) Q-learning performance
(d) Block-QNN-S
Figure 8. (a) Q-learning performance on CIFAR-100. The accuracy goes up with the epsilon decrease and the top models are all found in
the ﬁnal stage, show that our agent can learn to generate better block structures instead of random searching. (b-c) Topology of the Top-2
block structures generated by our approach. We call them Block-QNN-A and Block-QNN-B. (d) Topology of the best block structures
generated with limited parameters, named Block-QNN-S.
Accuracy (%)
Iteration (batch)
Q-learning Performance with Different Structure Codes
PCC{ReLU,Conv,BN}
separate ReLU,BN,Conv
Figure 9. Q-learning result with different NSC on CIFAR-100.
The red line refers to searching with PCC, i.e. combination of
ReLU, Conv and BN. The blue stands for separate searching with
ReLU, BN and Conv. The red line is better than blue from the
beginning with a big gap.
2 block structures in Fig. 8(b-c), denoted as Block-QNN-
A and Block-QNN-B. As shown in Fig. 8(a), both top-2
blocks are found in the ﬁnal stage of the Q-learning process, which proves the effectiveness of the proposed method
in searching optimal block structures rather than randomly
searching a large amount of models. Furthermore, we observe that the generated blocks share similar properties with
those state-of-the-art hand-crafted networks. For example,
Block-QNN-A and Block-QNN-B contain short-cut connections and multi-branch structures which have been manually designed in residual-based and inception-based networks. Compared to other auto-generated methods, the networks generated by our approach are more elegant and can
automatically and effectively reveal the beneﬁcial properties for optimal network structure.
To squeeze the searching space, as stated in Section 3.1,
we deﬁne a Pre-activation Convolutional Cell (PCC) consists of three components, i.e. ReLU, convolution and
ResNet 
Wide ResNet 
ResNet (pre-activation) 
DenseNet (k = 12) 
DenseNet (k = 12) 
DenseNet (k = 24) 
DenseNet-BC (k = 40) 
MetaQNN (ensemble) 
MetaQNN (top model) 
NAS v1 
NAS v2 
NAS v3 
NAS v3 more ﬁlters 
Block-QNN-A, N=4
Block-QNN-B, N=4
Block-QNN-S, N=2
Block-QNN-S more ﬁlters
Table 3. Block-QNN’s results (error rate) compare with state-ofthe-art methods on CIFAR-10 (C-10) and CIFAR-100 (C-100)
batch normalization (BN). We show the superiority of the
PCC, searching a combination of three components, in
Fig. 9, compared to the separate search of each component.
Searching the three components separately is more likely to
generate “bad” blocks and also needs more searching space
and time to pursue “good” blocks.
5.2. Results on CIFAR
Due to the small size of images (i.e. 32 × 32) in CIFAR,
we set block stack number as N = 4. We compare our
generated best architectures with the state-of-the-art handcrafted networks or auto-generated networks in Table 3.
Comparison with hand-crafted networks - It shows that our
Block-QNN networks outperform most hand-crafted networks. The DenseNet-BC uses additional 1 × 1 convolutions in each composite function and compressive transition layer to reduce parameters and improve performance,
which is not adopted in our design. Our performance can
be further improved by using this prior knowledge.
Comparison with auto-generated networks - Our approach
achieves a signiﬁcant improvement to the MetaQNN ,
and even better than NAS’s best model (i.e. NASv3 more
ﬁlters) proposed by Google brain which needs an expensive costs on time and GPU resources. As shown in Table 4, NAS trains the whole system on 800 GPUs in 28
days while we only need 32 GPUs in 3 days to get stateof-the-art performance.
Transfer block from CIFAR-100 to CIFAR-10 - We transfer the top blocks learned from CIFAR-100 to CIFAR-10
dataset, all experiment settings are the same. As shown
in Table 3, the blocks can also achieve state-of-the-art results on CIFAR-10 dataset with 3.60% error rate that proved
Block-QNN networks have powerful transferable ability.
Analysis on network parameters - The networks generated
by our method might be complex with a large amount of parameters since we do not add any constraints during training. We further conduct an experiment on searching networks with limited parameters and adaptive block numbers. We set the maximal parameter number as 10M and
obtain an optimal block (i.e. Block-QNN-S) which outperforms NASv3 with less parameters, as shown in Fig. 8(d).
In addition, when involving more ﬁlters in each convolutional layer (e.g. from to ), we can
achieve even better result (3.54%).
5.3. Transfer to ImageNet
To demonstrate the generalizability of our approach, we
transfer the block structure learned from CIFAR to ImageNet dataset.
For the ImageNet task, we set block repeat number
N = 3 and add more down sampling operation before
blocks, the ﬁlters for convolution layers in different level
blocks are . We use the best blocks structure learned from CIFAR-100 directly without any ﬁnetuning, and the generated network initialized with MSRA
initialization as same as above. The experimental results
are shown in Table 5. The network generated by our framework can get competitive result compared with other human
designed models. The recently proposed methods such as
Xception and ResNext use special depth-wise convolution operation to reduce their total number of parameters and to improve performance. In our work, we do not
use this new convolution operation, so it can’t be compared
Best Model on CIFAR10
Time(days)
MetaQNN 
Our approach
Table 4. The required computing resource and time of our approach compare with other automatic designing network methods.
Input Size
Inception V1 
Inception V2 
ResNet-50 
ResNet-152 
Xception(our test) 
ResNext-101(64x4d) 
Block-QNN-B, N=3
Block-QNN-S, N=3
Table 5. Block-QNN’s results (single-crop error rate) compare
with modern methods on ImageNet-1K Dataset.
fairly, and we will consider this in our future work to further
improve the performance.
As far as we known, most previous works of automatic
network generation did not report competitive result on
large scale image classiﬁcation datasets.
With the conception of block learning, we can transfer our architecture
learned in small datasets to big dataset like ImageNet task
easily. In the future experiments, we will try to apply the
generated blocks in other tasks such as object detection and
semantic segmentation.
6. Conclusion
In this paper, we show how to efﬁciently design high performance network blocks with Q-learning. We use a distributed asynchronous Q-learning framework and an early
stop strategy focusing on fast block structures searching.
We applied the framework to automatic block generation
for constructing good convolutional network. Our Block-
QNN networks outperform modern hand-crafted networks
as well as other auto-generated networks in image classi-
ﬁcation tasks. The best block structure which achieves a
state-of-the-art performance on CIFAR can be transfer to
the large-scale dataset ImageNet easily, and also yield a
competitive performance compared with best hand-crafted
networks. We show that searching with the block design
strategy can get more elegant and model explicable network
architectures. In the future, we will continue to improve the
proposed framework from different aspects, such as using
more powerful convolution layers and making the searching
process faster. We will also try to search blocks with limited FLOPs and conduct experiments on other tasks such as
detection or segmentation.
Acknowledgments
This work has been supported by the National Natural
Science Foundation of China (NSFC) Grants 61721004 and
A. Efﬁciency of BlockQNN
We demonstrate the effectiveness of our proposed Block-
QNN on network architecture generation on the CIFAR-100
dataset as compared to random search given an equivalent
amount of training iterations, i.e. number of sampled networks. We deﬁne the effectiveness of a network architecture auto-generation algorithm as the increase in top autogenerated network performance from the initial random exploration to exploitation, since we aim to getting optimal
auto-generated network instead of promoting the average
performance.
Figure 10 shows the performance of BlockQNN and random search (RS) for a complete training process, i.e. sampling 11, 392 blocks in total. We can ﬁnd that the best model
generated by BlockQNN is markedly better than the best
model found by RS by over 1% in the exploitation phase
on CIFAR-100 dataset. We observe this in the mean performance of the top-5 models generated by BlockQNN compares to RS. Note that the compared random search method
start from the same exploration phase as BlockQNN for
Figure 11 shows the performance of BlockQNN
with limited parameters and adaptive block numbers
(BlockQNN-L) and random search with limited parameters
and adaptive block numbers (RS-L) for a complete training
process. We can see the same phenomenon, BlockQNN-
L outperform RS-L by over 1% in the exploitation phase.
These results prove that our BlockQNN can learn to generate better network architectures rather than random search.
B. Evolutionary Process of Auto-Generated
We sample the block structures with median performance generated by our approach in different stage, i.e. at
iteration , to show the evolutionary process.
As illustrated in Figure 12 and Figure 13, i.e. BlockQNN and BlockQNN-L respectively, the
block structures generated in the random exploration stage
is much simpler than the structures generated in the exploitation stage.
In the exploitation stage, the multi-branch structures appear frequently. Note that the connection numbers is gradu-
Accuracy (%)
Iteration (batch)
BlockQNN Top1
BlockQNN Top5
Start Exploitation
Figure 10. Measuring the efﬁciency of BlockQNN to random
search (RS) for learning neural architectures. The x-axis measures
the training iterations (batch size is 64), i.e. total number of architectures sampled, and the y-axis is the early stop performance after
12 epochs on CIFAR-100 training. Each pair of curves measures
the mean accuracy across top ranking models generated by each
algorithm. Best viewed in color.
Accuracy (%)
Iteration (batch)
BlockQNN-L Top1
BlockQNN-L Top5
Start Exploitation
Figure 11. Measuring the efﬁciency of BlockQNN with limited
parameters and adaptive block numbers (BlockQNN-L) to random search with limited parameters and adaptive block numbers
(RS-L) for learning neural architectures. The x-axis measures the
training iterations (batch size is 64), i.e. total number of architectures sampled, and the y-axis is the early stop performance after
12 epochs on CIFAR-100 training. Each pair of curves measures
the mean accuracy across top ranking models generated by each
algorithm. Best viewed in color.
ally increase and the block tend choose ”Concat” as the last
layer. And we can ﬁnd that the short-cut connections and
elemental add layers are common in the exploitation stage.
Additionally, blocks generated by BlockQNN-L have less
”Conv,5” layers, i.e. convolution layer with kernel size of 5,
since the limitation of the parameters.
These prove that our approach can learn the universal design concepts for good network blocks. Compare to other
automatic network architecture design methods, our generated networks are more elegant and model explicable.
Exploitation from epsilon=0.9to epsilon=0.1
Random Exploration
Figure 12. Evolutionary process of blocks generated by BlockQNN. We sample the block structures with median performance at iteration
 to compare the difference between the blocks in the random exploration stage and the blocks in the
exploitation stage.
Exploitation from epsilon=0.9to epsilon=0.1
Random Exploration
Figure 13. Evolutionary process of blocks generated by BlockQNN with limited parameters and adaptive block numbers (BlockQNN-L).
We sample the block structures with median performance at iteration to compare the difference between
the blocks in the random exploration stage and the blocks in the exploitation stage.
C. Additional Experiment
We also use BlockQNN to generate optimal model on
person key-points task. The training process is conducted
on MPII dataset, and then, we transfer the best model found
in MPII to COCO challenge. It costs 5 days to complete
the searching process.
The auto-generated network for
key-points task outperform the state-of-the-art hourglass 2
stacks network, i.e. 70.5 AP compares to 70.1 AP on COCO
validation dataset.