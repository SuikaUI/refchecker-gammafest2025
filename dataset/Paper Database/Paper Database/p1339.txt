The Neural Basis of Human Error Processing: Reinforcement Learning,
Dopamine, and the Error-Related Negativity
Clay B. Holroyd
University of Illinois at Urbana–Champaign
Michael G. H. Coles
University of Illinois at Urbana–Champaign
and F. C. Donders Centre for Cognitive Neuroimaging
The authors present a unified account of 2 neural systems concerned with the development and
expression of adaptive behaviors: a mesencephalic dopamine system for reinforcement learning and a
“generic” error-processing system associated with the anterior cingulate cortex. The existence of the
error-processing system has been inferred from the error-related negativity (ERN), a component of the
event-related brain potential elicited when human participants commit errors in reaction-time tasks. The
authors propose that the ERN is generated when a negative reinforcement learning signal is conveyed to
the anterior cingulate cortex via the mesencephalic dopamine system and that this signal is used by the
anterior cingulate cortex to modify performance on the task at hand. They provide support for this
proposal using both computational modeling and psychophysiological experimentation.
Human beings learn from the consequences of their actions.
Thorndike originally described this phenomenon with
his law of effect, which made explicit the commonsense notion
that actions that are followed by feelings of satisfaction are more
likely to be generated again in the future, whereas actions that are
followed by negative outcomes are less likely to reoccur. This
fundamental reinforcement learning principle has been developed
by the artificial intelligence community into a body of algorithms
used to train autonomous systems to operate independently in
complex and uncertain environments . Research has also evaluated the neural mechanisms underlying reinforcement learning in biological systems,
but these mechanisms are still poorly understood.
In this article, we provide a framework for understanding the
neural basis of reinforcement learning in humans. Our proposal
links together two areas of research that have, until now, been
considered separately. On the one hand, we have previously inferred the existence of a generic, high-level error-processing system in humans from the error-related negativity (ERN), a negative
deflection in the ongoing electroencephalogram (EEG) seen when
human participants commit errors in a wide variety of psychological tasks. The ERN appears to be generated in the anterior
cingulate cortex. On the other hand, other researchers have argued
that the mesencephalic dopamine system conveys reinforcement
learning signals to the basal ganglia and frontal cortex, where they
are used to facilitate the development of adaptive motor programs.
Although the reinforcement learning function attributed to the
mesencephalic dopamine system and the error-processing function
associated with the ERN appear to be concerned with the same
problem—namely, evaluating the appropriateness of ongoing
events, and using that information to facilitate the development
and expression of adaptive behaviors—a possible relationship between these two systems remains to be explored.
In this article, we propose a hypothesis that unifies the two
accounts by explicitly linking the generation of the ERN to the
activity of the mesencephalic dopamine system. Specifically, we
suggest that when human participants commit errors in reactiontime tasks, the mesencephalic dopamine system conveys a negative reinforcement learning signal to the frontal cortex, where it
generates the ERN by disinhibiting the apical dendrites of motor
neurons in the anterior cingulate cortex. Furthermore, we suggest
that the error signals are used to train the anterior cingulate cortex,
ensuring that control over the motor system will be released to a
motor controller that is best suited for the task at hand.
To address this issue, we follow a parallel approach involving
both computational modeling and empirical investigation. On the
one hand, we develop a computational model that we use to
simulate the ERN and human behavior in two psychological tasks.
On the other hand, we describe two event-related brain potential
(ERP) experiments involving the same tasks, and we compare the
predictions derived from the model with the experimental results.
Although the model is compatible with the neurophysiology and
neuroanatomy of the mesencephalic dopamine system, it is not a
detailed neural model. Instead, it depicts in an abstract fashion the
Clay B. Holroyd, Neuroscience Program, University of Illinois at Urbana–Champaign; Michael G. H. Coles, Department of Psychology, University of Illinois at Urbana–Champaign and F. C. Donders Centre for
Cognitive Neuroimaging, Nijmegen, the Netherlands.
Clay B. Holroyd especially thanks Jesse Reichler for several years of
fruitful, and enjoyable, discussion of reinforcement learning and the errorrelated negativity. Both authors are also grateful to Gary Dell for reading
and commenting on parts of a previous version of the article. This research
was supported in part by National Institutes of Mental Health (NIMH)
predoctoral fellowship MH11530 and NIMH Grant MH41445. Preliminary
versions of this study have been presented in poster format .
Correspondence concerning this article should be addressed to Clay B.
Holroyd, who is now at the Department of Psychology, Princeton University, Green Hall, Princeton, New Jersey 08544. E-mail: cholroyd@
princeton.edu
Psychological Review
Copyright 2002 by the American Psychological Association, Inc.
2002, Vol. 109, No. 4, 679–709
0033-295X/02/$5.00
DOI: 10.1037//0033-295X.109.4.679
function we ascribe to the error-processing system underlying the
ERN. Specifically, the model adopts a reinforcement learning
algorithm, the method of temporal differences, that has previously
been used by others to describe the activity of the mesencephalic
dopamine system. Within this framework, we propose that the
ERN is elicited when a neural system first detects that the consequences of an action are worse than expected, and that the associated error signal is used to train the motor system in a way that
is consistent with neural network implementations of reinforcement learning principles.
We begin by reviewing several experiments that have provided
insight into the system that gives rise to the ERN. We then present
a brief overview of the role of the mesencephalic dopamine system
in reinforcement learning, and of recent computational models of
this system. Next, we review the anatomy and function of the
anterior cingulate cortex. Motivated by previous computational
models, we then present our own simulations of the ERN and
human behavior in two tasks, and we compare the results of the
simulations with empirical observations from two separate experiments. We find that the model accounts for much of the experimental data. We conclude that, when it is taken together with
neurophysiological and neuroanatomical considerations, the evidence is consistent with our proposal that the ERN is elicited by a
dopaminergic system for reinforcement learning. These results
suggest promising avenues for future research.
Background
When human beings make errors in reaction-time tasks, a negative deflection appears in the ongoing EEG at the time of error
commission. This phenomenon is best seen by averaging together
several epochs of the EEG, each of which is associated with an
erroneous response, to create an average ERP . The component of the ERP
revealed by this procedure is called the ERN, or Ne . The onset of the ERN
coincides with response initiation, as determined by the onset of
the electromyogram (EMG) associated with the responding hand,
and peaks roughly 80 ms thereafter. Its spatial distribution lies over
frontal–central regions of the scalp, reaching maximum amplitude
in a region over the supplementary motor area.
Over the past 10 years or so, we and others have elucidated the
nature of the system that generates the ERN, both in terms of
structure and function . We believe that the
ERN is generated by a high-level, generic, error-processing system. By error-processing, we mean that the system in involved in
detecting the fact that an error has occurred in a given task and in
using that error information to improve performance at the task. By
generic, we mean that the system is highly flexible, capable of
processing errors in a wide variety of contexts. And by high-level,
we mean that the system is associated with the executive control
processes mediated by frontal areas of the brain.
Much of our research has been devoted to elucidating the nature
of the error-processing system underlying the ERN. Gehring
 showed that the amplitude of the ERN increased as participants were motivated, via a payoff function, to strive for accuracy
over speed in a choice reaction-time task . His work has suggested that the system that produces the
ERN is sensitive to the importance of error commission to the
participant. In a different study, participants performed a fourchoice reaction-time task by pressing buttons using either of two
fingers on either of their left and right hands. Errors could be
committed with the wrong hand, the wrong finger, or both the
wrong hand and the wrong finger. It was found that the magnitude
of the ERN increased with the degree of error, being largest when
the incorrect response was committed with both the wrong hand
and the wrong finger . Thus,
the error-processing system appears to be sensitive to the degree of
In another experiment, participants were required to press a
button when they estimated that 1 s had elapsed following presentation of a warning stimulus. At the end of the trial, a feedback
stimulus was presented indicating whether or not their estimate on
that trial was within a criterion. The stimulus that indicated to the
participants that their response was not within the criterion (negative feedback) elicited an ERN .
It is important to note that, because the feedback was delivered
some time after the response occurred, the ERN elicited by the
feedback was dissociated from the response generation process.
This result demonstrates that the ERN is not elicited by the process
that causes the error in the first place, by the absence of a process
needed to effect the correct response, nor by the execution of a
remedial action made in conjunction with the error. Rather, the
experiment reveals that the system appears to be concerned with an
aspect of error processing that is not directly tied to error commission. This function might include detection of the error itself or
the use of the error information to prevent future error repetition.
ERNs are elicited by incorrect responses in a wide variety of
tasks .
Hence, the error processing system that gives rise to the ERN must
be flexible enough to be programmable with arbitrary goals. Moreover, the system can be made sensitive to various sources of error
information. As we have mentioned, ERNs can be elicited both by
negative feedback and by error commission itself, but ERNs can
also be elicited by errors committed with the feet or eyes as well as with the
hands, indicating that the system is equally sensitive to different
output modalities of error commission. Similarly, ERNs can be
elicited by negative feedback stimuli presented in the auditory,
visual, and somatosensory modalities, so the system must be
indifferent to the input modality of the error information . When speed is emphasized, ERNs are also
elicited by late responses . Taken together, the evidence paints
a picture of a highly flexible error-processing system.
Converging evidence indicates that the ERN may be generated
in the anterior cingulate cortex. Studies using the brain electric
source analysis (BESA) technique for source localization have suggested that the ERN is
generated within that region . The BESA technique
yields an anterior cingulate location even when the ERN is elicited
HOLROYD AND COLES
by errors committed with the feet as well as with the hands
 and by negative feedback presented in the auditory, somatosensory, and visual modalities . When a source localization technique
was applied to the magnetic equivalent of the ERN, an anterior
cingulate location was again suggested .
Neurophysiological evidence is also consistent with the position
that the ERN is generated in the anterior cingulate cortex. Niki and
Watanabe found error-recognition units in the
monkey anterior cingulate sulcus that were activated when the
animals received negative feedback in the form of an absence of an
expected reward. Similarly, Gemba, Sasaki, and Brooks 
found that when monkeys made errors in a simple response task,
error-related potentials were generated in the anterior cingulate
sulcus. This discovery led Brooks to suggest that the
anterior cingulate cortex functions as a comparator, comparing the
outcome of an action against its intent. Recently, several functional
neuroimaging studies found that the anterior cingulate cortex of
human participants engaged in reaction-time tasks was more activated on error trials than on correct trials . When taken together, the evidence argues in favor of an anterior cingulate location for ERN
generation.
Frontal systems of the brain, including the prefrontal cortex
 , the anterior
cingulate cortex , and the basal ganglia , are believed to contribute to
executive control. Systems concerned with executive function are
thought to regulate the most global aspects of human behavior,
such as planning and decision making, and are said to come into
play when a task is novel or difficult . One aspect of executive control
concerns response monitoring, or ensuring that the consequences
of an action are consistent with the intent. We believe that the
highly flexible nature of the error-processing system associated
with the ERN is consistent with what would be expected of an
executive control system concerned with response monitoring. The
frontal–central scalp distribution of the ERN and its putative
generation within the anterior cingulate cortex also point to an
association between the ERN process and executive functions
implemented in frontal regions. Furthermore, the ERN is abnormal
both in individuals with obsessive–compulsive disorder and in individuals with dorsolateral prefrontal damage
 , conditions that are associated with
executive dysfunction. When considered in this context, the system underlying the ERN appears to contribute to the regulation of
the most global aspects of human behavior.
The observations outlined in this section support two critical
assumptions of our hypothesis, namely, that the ERN is generated
by the anterior cingulate cortex and that the ERN is elicited by a
high-level error processing system.
The Mesencephalic Dopamine System
The mesencephalic dopamine system is composed of a small
collection of nuclei that project diffusely to the basal ganglia and
cortex . These nuclei include the substantia nigra
pars compacta and the ventral tegmental area (VTA). In the rat, the
cortical projections tend to synapse on the cortex in the frontal
midline, including the anterior cingulate cortex; in primates, the
projections are spread more widely but still reach their highest
density over the medial regions of the frontal cortex . The terminals of the dopamine neurons in the basal ganglia
 and prefrontal cortex end in “synaptic triads,” in
which the axon of each dopamine neuron makes contact with a
local synapse. In the basal ganglia, the synapses are composed of
the spine of a local spiny cell abutting the axon of a pyramidal cell;
in the frontal cortex, they consist of a pyramidal cell axon synapsing onto the spine of another pyramidal cell.
In the early 1950s, Olds and Milner discovered that rats
will engage in behaviors, like pressing a lever, that result in the
delivery of current via stimulating electrodes into certain regions
of their brains. The consequence of the stimulation was seen to be
reinforcing, in that the rats gradually increased the frequency with
which they engaged in the behavior that led to the stimulation. This
observation precipitated a search for the neural substrates of reinforcement learning, the results of which implicated the mesencephalic dopamine system .
Accumulating evidence suggests that the mesencephalic dopamine system can facilitate both long-term potentiation and longterm depression at its terminal synapses and that this
learning follows stringent timing requirements, in which the reinforcing signal must coincide with the activity of the pre- and
postsynaptic processes . Dopamine also appears important for the basal ganglia to express
learned conditioned responses that might be used for developing
new motor programs .
Originally, the mesencephalic dopamine system was widely
believed to contribute to reinforcement learning by mediating the
feelings of satisfaction experienced by an animal on receiving a
reward. As R. A. Wise and colleagues put it,
In introspective language we would say that neuroleptics [that disrupt
the midbrain dopamine system] appear to take the pleasure out of
normally rewarding brain stimulation, take the euphoria out of normally rewarding amphetamine, and take the “goodness” out of normally rewarding food. .
The work of Schultz and colleagues has been particularly influential in bringing about this change in view . Schultz and colleagues
recorded spike activity from mesencephalic dopamine cells in
monkeys as the monkeys learned to perform various delayed
response tasks. In a typical task, a monkey may be required to
press a lever every time a green light appears and is rewarded
when it does so. Before the monkey has learned how to perform
the task, presentation of the reward to the monkey elicits a phasic
response in the dopamine neurons. This observation is consistent
with the hypothesis that the activity of the mesencephalic dopamine system codes for the hedonic aspects of reward. However,
once the monkey has learned to perform the task correctly, then
presentation of the reward no longer elicits the phasic dopaminergic response . Rather, the conditioned stimulus elicits
the phasic activity. With learning, the dopaminergic signal “propagates back in time” from the time the reward is delivered to the
onset of the trial, when the conditioned stimulus is presented. Thus
the mesencephalic dopamine system can become active in anticipation of a forthcoming reward rather than on delivery of the
reward itself. Because the hedonic aspect of reinforcement is
presumably associated only with reward consumption, the presence of dopaminergic activity prior to reward presentation is
inconsistent with the hypothesis that the mesencephalic dopamine
system actually codes for the pleasure elicited by the reward.
Similar conclusions have been reached by other investigators .
After the monkey has had some practice at the task in the same
experiments, then when a reward is not given, the mesencephalic
dopamine neurons decrease their firing rate at the time the reward
would normally have been delivered . Dopaminergic activity also falls below baseline when the monkey is
presented with a stimulus that predicts punishment .
Schultz and colleagues have proposed that the dopamine neurons are sensitive to changes in the prediction of the “goodness” of
ongoing events: A positive dopamine signal is elicited when an
event is better than predicted, and a negative dopamine signal is
elicited when an event is worse than predicted. Because modern
learning theories specify that learning occurs when an event is
unpredicted , these observations
provide insight into how a reinforcement learning algorithm might
be implemented in natural systems. Accordingly, Schultz and
colleagues have suggested that the phasic responses seen in the
dopamine neurons might serve as error signals, used for adjusting
the associative strength of stimuli and responses in neural areas
that receive input from the mesencephalic dopamine system
 .
Several groups of investigators have noted similarities between the phasic activity of the mesencephalic dopamine system and a particular error
signal, called a temporal difference error (TD error), associated
with a reinforcement learning algorithm called the method of
temporal differences . This algorithm has theoretical roots in both
artificial intelligence and animal learning
theory and is a generalization of the
Rescorla–Wagner learning rule to the
continuous time domain. In neural network models, TD errors are
computed by an “adaptive critic,” which associates a value with
the ongoing events and outputs a TD error when it changes its own
prediction: Positive TD errors indicate that ongoing events are
“better” than expected, and negative TD errors indicate that ongoing events are “worse” than expected. Typically, a response
selection module called an actor or a motor controller uses the
error signal to reinforce behaviors that elicit reward. The method
of temporal differences accounts for a wide range of behavioral
phenomena, including all of the observations predicted by the
Rescorla–Wagner model and several observations that are inconsistent with it .
It is important to note that, like the phasic dopaminergic activity,
TD errors propagate back in time from the reward to the conditioned stimulus with learning. Also, like the phasic dopaminergic
activity, a negative TD error is elicited by the absence of an
expected reward. These observations led naturally to the hypothesis that the mesencephalic dopamine system carries a TD error.
As a TD error, the signal would be sensitive to changes in the value
of ongoing events, when things are suddenly better or worse than
expected. This hypothesis has been formalized in several neural
network models , the
temporal-difference hypothesis of mesencephalic dopaminergic
function has been highly influential. The hypothesis has had a
positive response in part because the idea dovetails with previous
conceptions about mesencephalic dopamine and reinforcement
learning and because the neural network implementation of the
method of temporal differences contains other formal properties
that are consistent with the structure and function of the midbrain
dopamine system .
The observations outlined in this section support two critical
assumptions of our hypothesis, namely, that the mesencephalic
dopamine system carries predictive error signals, and that these
predictive error signals are used by other parts of the brain for
reinforcement learning.
The Anterior Cingulate Cortex
Impressed by the diversity of cortical and subcortical input into
the anterior cingulate cortex, researchers have long regarded this
area as a neural locus where motor intentions are transformed into
action . In primates, the anterior
cingulate cortex contains somatotopically represented motor areas
buried in the depths of the cingulate sulcus . The human cingulate sulcus is a prominent
feature of the medial wall, composing about half of its surface
 ,
HOLROYD AND COLES
and the anterior cingulate sulcus is distinguished by large Layer V
pyramidal cells with extensive dendritic arborizations . These neurons project to other motor areas, including the
basal ganglia, supplementary motor area, primary motor area, and
spinal cord .
Single-cell, intracortical stimulation, and functional neuroimaging
studies have revealed that the cingulate motor areas contribute to
movement generation and execution .
The anterior cingulate cortex receives input from several areas
concerned with directing motor behavior. For example, the anterior cingulate motor areas receive widespread projections from the
limbic lobe, including the orbitofrontal cortex, from other limbic
areas such as the amygdala, and from nocioceptive sources; for this
reason, the anterior cingulate cortex has been said to provide a
critical pathway for emotional and motivational factors to influence motor activity . Likewise, the ventral bank of the cingulate sulcus
is richly interconnected with the dorsolateral prefrontal cortex
 . The dorsolateral prefrontal cortex is
thought to be involved in the generation of contextually appropriate behaviors in the absence of external stimulation and in the
production of new or novel responses , and the anterior cingulate
cortex tends to be concurrently activated with the dorsolateral
prefrontal cortex in functional neuroimaging experiments . As a
result, some researchers have suggested that the anterior cingulate
motor areas provide an important route for the dorsolateral prefrontal cortex to influence motor output . The anterior cingulate cortex is also highly
activated when two or more incompatible responses are simultaneously activated , suggesting that the anterior cingulate
cortex is involved in detecting or resolving
 response conflict.
Involvement of the anterior cingulate cortex in high-level motor
control is supported by neuropsychological research. For example,
frontal medial lesions that include the anterior cingulate cortex can
produce a condition called akinetic mutism, in which the afflicted
person appears to lack the will or motivation to generate behavior,
even though he or she is physically capable of doing so . Conversely, anterior cingulate dysfunction
is associated with obsessive–compulsive disorder, a condition
characterized by an excessive preoccupation with motor output
 . In one
person with a focal lesion of the anterior cingulate motor area for
manual behavior, performance on two executive control tasks was
impaired when the tasks demanded manual, but not verbal, responses . Taken together, the evidence
indicates that anterior cingulate motor areas are in a pivotal position for using their diverse neural inputs to contribute to the
planning, generation, and execution of behavior .
Learning in the Anterior Cingulate Cortex
The involvement of the anterior cingulate cortex in the acquisition of new behaviors appears to be limited to the earliest stages
of learning, when the task is most novel and difficult. In studies by
Gabriel and his colleagues , rabbits are
required to learn to differentiate between one of two cues (typically tones), only one of which (CS) is consistently followed by
an aversive stimulus (e.g., a foot shock). Gabriel and colleagues
have shown that neurons in the anterior cingulate cortex discriminate between the CS and the other cue during the early stages of
this task, and that the discriminative activity “moves” from anterior to posterior areas of the brain with increased learning .
Functional neuroimaging studies in humans have revealed a
comparable sequence of activations. For example, in a trial-anderror learning experiment, the anterior cingulate cortex (and dorsolateral prefrontal cortex) was activated during the earliest stages
of the task, but its contribution diminished as the task became
overlearned whereas other areas of the brain became more involved as the task progressed . In another study, anterior
cingulate activity associated with response conflict in the Stroop
task was seen to diminish with practice .
During the early stages of learning, anterior cingulate motor
areas appear specifically to use reward and error information to
identify and select appropriate behaviors. In monkeys, the error
potentials discovered by Brooks and colleagues in the anterior
cingulate sulcus occurred only during the earliest stages of the task
as the monkeys learned what was required of them . Brooks argued that the anterior
cingulate cortex contributes to learning only “when certainty is
poised against uncertainty” during “the process of distinguishing
between what is appropriate and inappropriate in the task context”
(p. 31). This position is supported by the observation that, as
monkeys learn a sequence of movements by trial and error, one
group of neurons in the anterior cingulate sulcus is most active
during an early “search” phase, before the animal has identified the
sequence of responses that predict the correct solution and reward,
whereas another group of neurons is most activated during a later
“repetition” phase, once the monkey has just learned the response
contingencies . Furthermore,
following a reduction of an expected reward in a different trialand-error learning experiment, but not following control conditions, cells in the monkey anterior cingulate sulcus exhibit a
change in activity when the monkey prepares to switch from the
unsuccessful response to a different response; when this area is
inactivated (by topical application of muscimol, a -aminobutyric
acid agonist), the monkey is unable to switch to the more adaptive
response, evidently because it is unable to use the information
HUMAN ERROR PROCESSING
relating to the reduced reward . Thus, the
anterior cingulate motor areas appear to be involved in selecting a
new response when the outcome of a trial is not satisfactory.
In humans, functional neuroimaging experiments have also suggested that the anterior cingulate cortex uses reward and error
information to identify and select appropriate behaviors. The anterior cingulate cortex is activated by both monetary reward and
monetary punishment, with the right anterior cingulate cortex
relatively more sensitive to monetary punishment than to monetary
reward . In another
study , subjects used intertrial feedback
(the words correct and incorrect) to develop hypotheses about how
best to respond in the task; it was found that a region corresponding to the ventral bank of the anterior cingulate cortex was most activated when subjects
made both a hypothesis and a choice, compared with when they
made a hypothesis only and when they made a choice only. The
authors suggested that this “ventral anterior cingulate activation in
part reflects evaluative processing related to the emotional consequences of making a choice” (p. 25).
In the rat, neurons in the medial prefrontal cortex predict reward
acquisition in a radial-arm maze, with some neurons differentiating
between high- and low-rewarded arms .
The anterior cingulate cortex in the rat appears to be organized
topographically, with rostral and ventral parts representing stimulus attributes that predict reward or no reward, and caudal and
dorsal parts related to the execution of learned instrumental behaviors . Importantly, dopaminergic input
to the medial prefrontal cortex changes whenever presentation of a
response-contingent food reward deviates from what the animal
has come to expect . It has also been
shown that the first pairing of an unconditioned stimulus with a
foot shock elicits a large increase in dopamine in the rat medial
prefrontal cortex and that this level of dopamine decreases significantly over the next two pairings. This result has been interpreted
to mean that the dopamine system may facilitate the medial prefrontal cortex’s coping with novel situations, when new learning
may be required .
Given that mesencephalic dopamine neurons synapse on motor
neurons in the anterior cingulate cortex, one might expect the
anterior cingulate cortex to contribute to reinforcement learning. In
fact, the medial prefrontal cortex has long been thought to be
involved in reinforcement learning because it is one of the few
cortical areas that supports intracranial self-stimulation in the rat
 and because
increased dopamine levels have been found in the region following
self-stimulation . When conditioned stimuli
of very brief duration (200 ms) are used in discriminative avoidance learning, moreover, rabbits that are neonatally exposed to
cocaine exhibit impaired avoidance response acquisition as well as
abnormal anterior cingulate processing of the conditioned stimulus
 . These authors have suggested that mesencephalic dopamine
may facilitate the development of stimulus associations that capture the participant’s attention. Similarly, in a monkey operant
conditioning task, anterior cingulate neurons were shown to respond differentially not only to the delivery of positive and negative rewards (e.g., to food vs. an aversive stimulus), but also to the
presentation of conditioned stimuli that predicted those rewards
 . In principle, the learning of such associations could be
driven by the mesencephalic dopamine system, perhaps fostering
the development of new behaviors that are appropriate to the
conditioned stimuli.
Using a metabolic mapping technique ,
Porrino and colleagues examined brain activation patterns following electrical stimulation of the VTA. They found that when rats
injected current into the VTA by pressing a lever, and when the
experimenter delivered current to that same area in a way that was
not contingent on the animal’s behavior, several areas of the brain
became activated ; however, the difference
between the two conditions was largest in anterior portions of the
limbic cortex . Porrino concluded:
Because the distribution of changes in metabolic activity in selfstimulating animals is specific to the reinforced behavior of these
animals, and not merely the result of the electrical stimulation to the
VTA, the changes in functional activity observed in cortex specifically reflect reinforcement processing and confirm the role of limbic
cortex in this processing. (pp. 453–454)
The observations outlined in this section support three critical
assumptions of our hypothesis, namely, that anterior cingulate
motor areas comprise a neural locus where high-level motor intentions are mapped into actions; that the anterior cingulate cortex
is involved in learning these mappings; and that this learning is
driven in part by reward-related information carried to the anterior
cingulate cortex by the mesencephalic dopamine system.
We have described a neurophysiological phenomenon, the ERN,
that appears to be elicited by activation of a system concerned with
error detection and compensation and that is generated in the
anterior cingulate motor cortex. We have also described the mesencephalic dopamine system, its putative role in reinforcement
learning, and a recent hypothesis that holds that a phasic increase
in dopaminergic activity occurs when the system detects that
ongoing events are better than expected and that a phasic decrease
occurs when the system detects that ongoing events are worse than
expected. We have further discussed the nature of the anterior
cingulate motor areas, including their strategic position for channeling motor commands issued from various command structures
to the motor system for execution and their potential contribution
to dopamine-dependent reinforcement learning.
Although the reinforcement learning functions associated with
the mesencephalic dopamine system and the error-processing
functions attributed to the anterior cingulate cortex appear to be
concerned with the same problem—namely, evaluating the appropriateness of ongoing events, and using that information to facilitate the development of adaptive behaviors—a possible relationship between these two systems remains to be explored. We now
propose a hypothesis that explicitly links the generation of the
ERN to the activity of the mesencephalic dopamine system.
We assume that the human nervous system is composed, in part,
of multiple motor controllers (Figure 1). We envision each of these
HOLROYD AND COLES
controllers acting semi-independently and in parallel, each trying
to exert their influence over the motor system. More specifically,
we consider that the motor controllers correspond to the various
neural command structures that project to the anterior cingulate
motor cortex. For example, one controller might correspond to the
dorsolateral prefrontal cortex, another to the orbitofrontal cortex,
and still others to the basal ganglia and the amygdala. We suggest
that each controller might approach solving high-level motorcontrol problems in its own way. For example, whereas one
controller may impel the motor system to search for immediate
reinforcement, another controller might inhibit the motor system in
favor of delayed reinforcement, and still another might direct the
motor system to avoid pain at all costs. Other controllers might
guide motor output when guessing, or when making decisions
under uncertainty, or even when navigating delicate social
encounters.
We propose that the anterior cingulate cortex, at the confluence
of all this information, decides which motor commands are actually issued to the motor system. In this view, the anterior cingulate
cortex acts as a motor control filter, enabling any one of the motor
controllers to take command of the motor system. Ignorant as to
which controller is best suited to address the task at hand, the
anterior cingulate cortex must learn which controller should be
delegated motor authority. We assume that the anterior cingulate
cortex is trained to recognize the appropriate controller, with
reinforcement learning signals conveyed to it via the mesencephalic dopamine system. We further assume that some of the
motor controllers may themselves use those same reinforcement
learning signals to identify the appropriate response strategy required of them. .
In keeping with previous simulations, we assume that the reinforcement learning signal conveyed by the mesencephalic dopamine system is specifically a TD error. In one account of the neural
A schematic of the model. The corresponding neural substrate is given in parentheses below each
component label. See text for details. ERN  error-related negativity; TD  temporal difference error.
HUMAN ERROR PROCESSING
basis of temporal difference learning, the adaptive critic and the
actor were assumed to be implemented within “striosome” and
“matrisome” modules of the basal ganglia, respectively . Although we
assume that several actors (motor controllers) are implemented
throughout the brain, our model conforms to the previous account
by attributing the role of adaptive critic to the basal ganglia, which
computes the value and the change in value of ongoing events.
We propose that errors induce phasic decreases in mesencephalic dopaminergic activity when the system first determines that
ongoing events are worse than expected. Conversely, correct responses induce phasic increases in dopaminergic activity. The
ERN is generated on error trials, but not on correct trials, when the
reduction of dopaminergic input disinhibits neurons in the anterior
cingulate cortex. In both cases, the anterior cingulate cortex uses
these predictive error signals to select and reinforce the motor
controller that is most successful at carrying out the task at hand.
The fundamental points of our hypothesis are (a) that the ERN
reflects the transmission of a reinforcement learning signal to the
anterior cingulate cortex; (b) that this error signal is carried by the
mesencephalic dopamine system; and (c) that it is used to train the
anterior cingulate motor cortex to optimize performance on the
task at hand.
Computational Simulations and Experimental Data
In this section, we formalize the assumptions described above in
a computational model. We use the model to simulate both human
behavior and the amplitude of the ERN in two experimental tasks.
We also present the results of two experiments that involve human
participants engaged in the same tasks, and the predictions produced by the simulations are compared with the experimental
The ERN in a Probabilistic Learning Task
The first of the two studies examined the behavior of the ERN
in a simple probabilistic learning task. Previous experiments have
demonstrated that both error commission and negative feedback
stimuli elicit ERNs, but none have
investigated both types of error potentials in the same experiment.
This study afforded us the opportunity to examine the relationship
between the two varieties of ERN as learning progressed throughout the course of a block of trials. Moreover, the task lent itself to
simulation using the method of temporal differences, which we
used to make specific predictions of ERN amplitude and human
behavior.1
On each trial in this task, an imperative stimulus appeared in
front of the participant on a computer screen. The participant was
then required to make a two-choice decision by pressing one of
two buttons. At the end of the trial, a feedback stimulus indicated
to the participant that he or she was either rewarded or penalized
one cent of bonus money on that trial. The participants were not
informed of the appropriate stimulus–response mappings and had
to infer the optimal response strategy by trial and error. The
experiment was divided into 10 blocks of 300 trials each. Each
block was characterized by a new set of six imperative stimuli,
each of which were presented 50 times in random order. In every
block, one of the six stimuli was mapped to the left button, so that
participants were rewarded if they pressed the left button and
penalized if they pressed the right button. Conversely, another of
the six stimuli was mapped to the right button, so that participants
were rewarded if they pressed the right button and penalized if
they pressed the left. We call both of these two mappings 100%
mappings. For two other stimuli in each block, feedback was
delivered at random to the participant, independently of how the
participant actually responded. In this case, the participant was
rewarded on 50% of the trials and penalized on the other 50% of
the trials (50% mapping). When the fifth stimulus appeared, the
participant was always rewarded, regardless of how he or she
responded, and when the sixth stimulus appeared, the participant
was always penalized, also independently of how the person actually responded (always correct/always incorrect mapping).
We predicted that the negative feedback stimuli would elicit
ERNs for all three mapping conditions (100%, 50%, and always
correct/always incorrect) at the start of each block. However, we
predicted that, as the blocks progressed, newly learned associations in each of the three conditions would differentially affect the
amplitude of the ERN. In the 50% mapping condition, the system
must wait for the feedback to determine the outcome of the trial.
Therefore, we predicted that negative feedback stimuli in this
condition would continue to elicit the ERN throughout the course
of each block. In contrast, in the 100% mapping condition, the
response itself determines the outcome of the trial. Therefore, we
predicted that as the system learned the associations between
response and feedback, the ERN associated with the response
would increase, whereas the ERN associated with the feedback
would decrease. Finally, in the always correct/always incorrect
mapping condition, the imperative stimulus determines the outcome of the trial. Therefore, we predicted that as the system
learned the associations between the imperative and feedback
stimuli, neither the response nor the feedback would elicit the
Experiment.
Fifteen participants were paid $5 an hour to participate, plus a bonus for good performance. The experiment
consisted of one 3.5-hr session. Participants sat in front of a CRT
in a dimly lit room and performed the probabilistic learning task
described above. Each trial consisted of the presentation of an
imperative stimulus and the execution of a button-press response
by the participant. Imperative stimuli were visual images generated
from a Neuroscan (Neurosoft Inc., Sterling, VA) library, including
pictures of buildings, animals, vegetables, clothing, and parts of
the body. Participants sat 1m from the display such that each image
subtended about 3° of visual angle. Each block was associated with
images from multiple categories. Stimulus onset asynchrony was
2 s. Participants were required to press one of two buttons (Buttons 2 and 3 on a Neuroscan button box) as soon as possible after
stimulus presentation. One second after the stimulus was displayed, a feedback stimulus was presented to the participant. The
feedback stimuli were an image of a carrot and an image of a head
of lettuce, indicating to the participant that they either were rewarded or were penalized one cent on that trial; the mappings
1 Note that Kopp and Wolff have recently examined the effects of
error-driven learning on other ERP components, for example, P300.
HOLROYD AND COLES
between the reward and feedback stimuli were counterbalanced
across participants.
Rewards and penalties were determined on each trial according
to the schedule described above. Participants were not informed of
the stimulus–response mappings, and on each block they had to
determine the optimal response strategy by trial and error. Participants were told that some mappings might be difficult to figure
out, but to “do the best you can.” Additionally, if the participant
failed to respond within 600 ms following stimulus presentation,
then on trials with a 100% or 50% mapping, a cherry feedback
stimulus was presented in lieu of the carrot or lettuce feedback.
The cherry feedback stimulus communicated to participants that
they were penalized two cents on that trial, providing motivation
for them to respond faster. The speed requirement ensured that the
participants made some impulsive errors to stimuli with 100%
mappings even after the mappings had been learned. Trials in
which no response was generated within the 600-ms window were
not included in the analysis.
Participants began the experiment with a bonus of $1. At the end
of each block, participants were provided with information indicating the amount of money they earned or lost on that block and
the total amount of bonus money they earned throughout the
experiment. Bonus money was paid to the participants on their
completion of the experiment.
For each participant and for each condition, response-locked and
stimulus-locked ERP averages were created by averaging the data
recorded at channel Cz with respect to the onset of the response
and to the onset of the feedback stimulus. Difference waveforms
were generated by subtracting the activity elicited on trials with
positive feedback from the activity elicited on trials with negative
feedback. The amplitude of the response-locked ERN was determined by identifying the peak negativity of the difference waveform between 0 ms and 100 ms following the response, and the
amplitude of each feedback-locked ERN was determined by identifying the peak negativity of the difference waveform between
200 ms and 300 ms following feedback onset. In addition, running
averages of difference-wave amplitude were created for each participant and each condition by averaging the data into 10-trial–
wide bins, that is, Bin 1 contained data from Trials 1–10, Bin 2
contained data from Trials 2–11, Bin 3 contained data from Trials
3–12, and so on. Changes in ERN amplitude during the block were
tested as follows: (a) For each participant, the average amplitude of
the response-locked difference-wave over the first 10 trials was
subtracted from the average amplitude of the feedback-locked
difference-wave over the same period; (b) for each participant, the
average amplitude of the response-locked difference-wave over
the last 10 trials was subtracted from the average amplitude of the
feedback-locked difference-wave over the same period; and (c) a
paired t test was conducted on these two measures. (See Appendix
A for a complete description of recording procedures and data
analysis).
Simulation.
A schematic of the network is depicted in Figure 1. An input layer composed of six units, one for each of the six
stimuli appearing in a block of trials, was fully interconnected with
each of five motor control modules. At the start of each trial, the
identity of the stimulus was noted by the input module and communicated to all of the motor controllers. Each of the motor
controllers was composed of two units, one for each possible
response (left- vs. right-button presses). Each controller selected
one of the two response options probabilistically, with the unit
receiving the highest net activation most likely to be selected. To
simulate the differing ability of the controllers to generate appropriate behavior in the task, a noise term associated with response
selection in each controller was manipulated, so that each of the
controllers was capable of performing the task with varying degrees of success (see Appendix B). A control filter probabilistically selected one of the five controllers and communicated the
response option selected by that controller to the output layer for
execution (see Appendix B). An adaptive critic received the stimulus information processed by the input layer, information about
the response executed by the output layer, and feedback information presented to the network at the end of the trial. The adaptive
critic computed the value of ongoing events and outputted a TD
error when it detected a change in value. The TD error was
distributed to three parts of the network: to the adaptive critic
itself, where it was used to refine the ongoing predictions; to the
motor controllers, where it was used to adjust the stimuluscontroller mappings; and to the control filter, where it was used to
train the filter to select the motor controller with the best performance. Together, the adaptive critic, the motor controllers, and the
control filter learned how best to perform the probabilistic learning
In the model, the motor-control filter corresponded to the anterior cingulate cortex, and the motor controllers corresponded to
each of the neural command areas projecting to the anterior cingulate cortex. Thus, the job of the anterior cingulate was to identify
which of its inputs were best suited for carrying out the task. In
keeping with previous simulations , the adaptive critic was implemented within the basal
ganglia, and the TD error was carried by the mesencephalic dopamine system. An ERN was elicited when the anterior cingulate
cortex received a negative TD error from the mesencephalic dopamine system, during or after the response-generation processes,
when an eligibility trace associated with the winning motor controller was nonzero.
For statistical purposes, the simulation was run 15 times. On
each run, the model was trained anew on 10 blocks of 300 trials.
Thus, each simulation corresponded to the data of an individual
“participant.” The ERN was the amplitude of the TD error multiplied by the magnitude of the eligibility trace (see Appendix B).
However, for comparison with the empirical results (see above),
“difference waves” were determined by subtracting the simulated
ERN on trials with positive feedback from the simulated ERN on
trials with negative feedback. The ERN was then taken as the
magnitude of this difference. In addition, running averages of
difference-wave amplitude were created for each participant and
each condition by averaging the data into 10-trial–wide bins, that
is, Bin 1 contained data from Trials 1–10, Bin 2 contained data
from Trials 2–11, Bin 3 contained data from Trials 3–12, and so
on. Changes in ERN amplitude during the block were tested as
follows: (a) For each participant, the average amplitude of the
response-locked difference-wave over the first 10 trials was subtracted from the average amplitude of the feedback-locked
difference-wave over the same period; (b) for each participant, the
average amplitude of the response-locked difference-wave over
the last 10 trials was subtracted from the average amplitude of the
HUMAN ERROR PROCESSING
feedback-locked difference-wave over the same period; and (c) a
paired t test was conducted on these two measures.
Of the three stimulus types (with 100%, 50%, and the
always correct/always incorrect mappings), performance could
improve only on those trials in which the imperative stimuli were
mapped with 100% probability to one of the two response options.
Figure 2 shows the accuracy values associated with the 100%
mapping condition, averaged across the two stimuli, across blocks,
and across participants, for the first and second halves of each
block separately. As can be seen, participants’ accuracy in this
condition improved as the blocks progressed, from 68% during the
first half of the blocks to 79% during the second half, t(14) 
8.75, p  .01. The model’s performance also improved as the
blocks progressed, from 64% to 83%, t(14)  24.4, p  .01.
Figure 3 illustrates typical ERNs elicited in the experiment.
Shown are the ERPs associated with positive and negative feedback, averaged across trials, across stimuli, and across blocks.
Figure 3A shows the ERN elicited by the feedback in the 50%
mapping condition. Confidence intervals (.95) confirmed that the
amplitude of the associated difference wave was less than zero
(M  4.1 V, SD  3.0 V, interval  5.2 V, 2.2 V).
The ERN reached peak amplitude about 250 ms following presentation of the negative feedback stimulus . Figure 3B illustrates the ERN associated with the
response in the 100% mapping condition. Confidence intervals
(.95) also confirmed that the amplitude of the associated difference
wave was less than zero (M  6.4 V, SD  4.4 V, interval 
8.6 V, 4.2 V). The ERN reached maximum amplitude
about 80 ms following the button press.
Figure 4 illustrates the amplitudes of the ERN associated with
the response and with the feedback for each mapping condition.
Shown in each figure are the magnitudes of the difference waves,
as determined by averaging the experimental and the simulated
data across trials, blocks, stimuli, and participants. Figure 4A
illustrates the amplitude of the ERN on trials with 50% mapping
probabilities. In this condition, the experimental ERN was elicited
primarily by the feedback (M  4.1 V) and not by the response
(M  0.4 V), t(14)  4.57, p  .01. Conversely, in the 100%
mapping condition (Figure 4B), the experimental ERN was elicited
primarily by the response (M  6.4 V), and not by the feedback (M  1.5 V), t(14)  3.46, p  .01. Lastly, the
experimental ERN in the always correct/always incorrect condition (Figure 4C) was relatively small for both the response (M 
1.7 V) and the feedback (M  2.6 V), and there was no
significant difference between the two conditions, t(14)  1.35,
p  .05. Confidence intervals indicated that the small ERNs in this
condition associated with the response (M  1.7 V, SD  1.5
Simulated and empirical accuracy data for the probabilistic
learning task. For both the model and human participants, accuracy in the
100% mapping condition improved in the second half of each block
relative to the first half. Exp  Experiment.
The error-related negativity (ERN) in the probabilistic learning
task elicited by feedback stimuli in the 50% mapping condition (A) and the
response in the 100% mapping condition (B). The waveforms are more
negative on error trials than on correct trials. The ERN peaks about 250 ms
after the onset of the feedback stimulus and about 80 ms after the onset of
the incorrect response. Waveforms were recorded from channel Cz.
HOLROYD AND COLES
V, interval  2.3 V, 0.9 V), and with the feedback (M 
2.7 V, SD  2.3 V, interval  3.8 V, 1.4 V), were
statistically reliable.
The simulated data follow a similar pattern. In the 50% mapping
condition (Figure 4A), the simulated ERN was elicited primarily
by the feedback (M  0.84), and not by the response (M 
0.003), t(14)  81.2, p  .01. Conversely, in the 100% mapping
condition (Figure 4B), the simulated ERN was elicited primarily
by the response (M  1.35), and not by the feedback (M 
0.20), t(14)  61.5, p  .01. The simulated ERN in the always
correct/always incorrect condition (Figure 4C) was also relatively
small for both the response (M  0.21) and the feedback (M 
0.16), but the ERN to the response was the larger of the two,
t(14)  5.3, p  .01. Confidence intervals confirmed that both
simulated ERNs were statistically reliable: response (M  0.209,
SD  .030, interval  0.225, 0.194), feedback (M  0.164,
SD  .021, interval  0.175, 0.154).
Figure 5 shows running averages of ERN amplitude, for both the
simulated and empirical data, associated with the response and
with the feedback for the three mapping conditions. The running
averages illustrate the evolution of the error potentials as learning
progressed throughout the course of each block. In the 50% mapping condition (Figure 5A and 5B), the magnitude of the ERN
elicited by the feedback was greater than the magnitude of ERN
elicited by the response during the entire blocks, for both the
simulated (difference during first 10 trials, M  0.81, difference
during last 10 trials, M  0.82), t(14)  0.14, p  .05, and
empirical data (difference during first 10 trials, M  2.31 V,
difference during last 10 trials, M  3.23 V), t(14)  1.91,
p  .05, data. In contrast, in the 100% mapping condition (Figure
5C and 5D) the magnitude of the ERN elicited by the response
increased relative to the magnitude of the ERN elicited by the
feedback as the blocks progressed, for both the simulated (difference during first 10 trials, M  0.16, difference during last 10
trials, M  1.83), t(14)  35.2, p  .01, and empirical (difference during first 10 trials, M  2.08 V, difference during last 10
trials, M  7.76 V), t(14)  4.89, p  .01, data. For the always
correct/always incorrect condition (Figure 5E and 5F), the simulated results deviated from the empirical results. By the end of the
blocks, for the simulated data, the magnitude of the ERN associated with both the response (0.016) and the feedback (0.003)
approached zero, whereas for the empirical data, the magnitude of
the ERN associated with the response (3.11 V) and with the
feedback (1.54 V) remained relatively far from zero. However,
as the blocks progressed, the magnitude of the ERN elicited by the
response increased relative to the magnitude of the ERN elicited
by the feedback, for both the simulated (difference during first 10
trials, M  0.10, difference during last 10 trials, M  0.01),
and trials with negative feedback. A: 50% mapping condition; B: 100%
mapping condition; C: always correct/always incorrect (AC/AI) mapping
conditions. In the 50% mapping condition, the ERN is elicited primarily by
the feedback, and not by the response. In the 100% mapping condition, the
ERN is elicited primarily by the response, and not by the feedback. In the
AC/AI mapping condition, neither the response nor the feedback elicit a
large ERN. Simulated results are given in multiples of reward (r). Empirical results are given in microvolts. Exp  Experiment.
Simulated (Model) and empirical (Exp) error-related negativity
(ERN) amplitudes in the probabilistic learning task. Amplitudes are determined from the difference in magnitudes on trials with positive feedback
HUMAN ERROR PROCESSING
t(14)  3.7, p  .01, and empirical (difference during first 10
trials, M  0.64 V, difference during last 10 trials, M  1.36
V), t(14)  2.61, p  .05, data. In other words, for both the
simulated and empirical data, the feedback-related ERN was larger
than the response-related ERN at the beginnings of the blocks, but
was smaller than the response-related ERN at the ends of the
We chose a running average bin size of 10 to collect enough
trials per condition per bin to generate empirical ERNs with good
signal-to-noise ratios. However, the simulated ERNs were not
bound by the same constraint. For this reason, the simulated data
in Figure 5 were replotted in Figure 6 (bin size  1) to show the
trial-by-trial changes in amplitude. Replotting the data in this
fashion revealed some transient phenomena at the beginning of
each block that were obscured when the data were plotted with the
larger bin size. In particular, the response-locked ERN in the 100%
mapping condition (Figure 6B) started at about zero (M  0.01) on
the first trial and rose thereafter, whereas the feedback-locked
ERN was large at the beginning of the blocks (M  0.84),
t(14)  26.1, p  .01, and decreased as the blocks progressed. In
the always correct/always incorrect condition (Figure 6C), the
feedback-ERN again was larger at the beginning of the blocks
(M  0.81) than the response-locked ERN (M  0.01),
t(14)  13.8, p  .01, and decreased as the blocks progressed, but
the response-locked ERN increased in magnitude until about
Trial 8, whereupon it started to decrease. Finally, in the 50%
Error-related negativity (ERN) amplitude for simulated data
plotted by trial within block. Solid line: response-locked ERN amplitude.
Dashed line: feedback-locked ERN amplitude. A: 50% mapping condition;
B: 100% mapping condition; C: always correct/always incorrect (AC/AI)
mapping condition. See text for details.
Running averages of error-related negativity (ERN) amplitude
for simulated (Model column) and empirical (Experiment column) data.
Running average bin size  10 trials (see text). Solid line: response-locked
ERN amplitude. Dashed line: feedback-locked ERN amplitude. A and B:
50% mapping condition; C and D: 100% mapping condition; E and F:
always correct/always incorrect (AC/AI) mapping condition. See text for
HOLROYD AND COLES
condition (Figure 6A) the feedback-locked ERN started the blocks
at maximum amplitude (M  0.82), the response-locked ERN
began significantly smaller at zero (M  0.00), t(14)  20.2, p 
.01, and both measures remained constant throughout the blocks.
If the amplitude of the ERN was correlated with the learning
process, then we might expect to see the effects of such learning on
individual trials. In particular, the amplitude of the ERN should be
largest when a feedback stimulus disconfirms a prediction induced
by a previous feedback stimulus, when both stimuli are encountered under the same conditions. Figure 7 illustrates the effects of
different sequences of feedback stimuli on the amplitude of the
ERN. For both the simulation and the human participant data, we
determined the amplitudes of the ERN elicited by feedback on
trials with 50% mapping probabilities according to the type of
feedback seen by the system on the immediately previous encounter with the same stimulus, when both encounters were associated
with an identical response. Thus, the data were averaged according
to four conditions: error trials preceded by error trials, error trials
preceded by correct trials, correct trials preceded by correct trials,
and correct trials preceded by error trials. We then determined the
magnitude of the difference between the waveforms on error trials
preceded by error trials and the waveforms on correct trials preceded by correct trials (same condition) and between the waveforms on error trials preceded by correct trials and the waveforms
on correct trials preceded by error trials (opposite condition).
Figure 7 shows the magnitude of the resulting difference associated with the same and opposite conditions, for both the model and
human participants. The amplitude of the observed ERN tended to
be smaller when two successive encounters with the same stimulus
(and response) were followed by feedback with the same valence
(M  3.0 V), compared with when the two successive encounters were followed by feedback with opposite valence (M  4.3
V), t(14)  2.52, p  .05. This difference in amplitude was
reflected in the simulated data as well (same, M  0.68, opposite, M  1.01), t(14)  35.1, p  .01. Note that the consecutive
encounters were often separated by trials associated with a different stimulus or response type.
In the simulated data, these sequential learning effects arise
from weight changes associated with the response. Figure 8 illustrates the change in weight of the value unit associated with the
response in the 50% mapping condition, for both positive feedback
trials and negative feedback trials. As can be seen, the value
associated with the given response increases following reward but
decreases following punishment. Thus, on subsequent encounters
with the same imperative stimulus, the response that was previously rewarded generates a relatively positive prediction, whereas
the response that was previously punished generates a relatively
negative prediction. If the ERN indeed reflects a prediction error,
feedback stimuli that disconfirm these predictions should elicit
relatively large ERNs, whereas feedback stimuli that confirm these
predictions should elicit relatively small ERNs. This was the
pattern of results we obtained.
Discussion.
In general, there was considerable correspondence
between the simulated and experimental data. For both humans
and the model, on trials with 50% mapping probabilities, the ERN
was elicited primarily by feedback presentation (Figures 4A, 5A,
and 5B). In this condition, the system waited until the feedback
was presented to determine the outcome of each trial. In contrast,
on trials with 100% mapping probabilities, the accuracy of the
system increased across the blocks (Figure 2), and the amplitude of
the ERN was larger to the response than it was to the feedback
(Figures 4B, 5C, and 5D). Evidently, as the system learned the
correct response mappings associated with these stimuli, it tended
to rely less on the feedback and more on its own representation of
what the response should be to determine the outcome of each trial.
The simulated data in Figure 6B suggested that most of this
learning occurred during the first 10 or so encounters with the
imperative stimulus, a plausible result given the simplicity of the
task. In the always correct and always incorrect mappings, the
amplitude of the ERN was small to both the response and the
feedback for both the experimental and simulated data (Figure 4C).
However, in this case, the simulated ERNs decreased to zero as the
blocks progressed (Figure 5E), whereas the empirical ERNs appeared constant throughout the blocks (Figure 5F). In the model,
the system ceased to rely on the response and the feedback as it
learned that the imperative stimulus predicted the outcome of the
trial. The fact that neither the response nor feedback ERNs decreased by much in the empirical data suggests that an additional
cognitive process must have been at work in the human system,
continuing the search for an appropriate response strategy even
after the simpler system had given up. On the other hand, the ERN
in this analysis was computed across two different stimulus conditions (always correct and always incorrect), and may reflect
Sequence effects on error-related negativity (ERN) amplitude
induced by feedback presentation in the 50% mapping condition of the
probabilistic learning task. Data are averaged according to the valence of
the feedback stimulus on the most recent trial associated with the same
imperative stimulus and response. When successive encounters with the
same imperative stimulus and response are associated with feedback stimuli of opposite valence, the ERN elicited by the second feedback stimulus
is larger than when the two feedback stimuli have the same valence.
Simulated results are given in multiples of reward (r). Empirical results are
given in microvolts. Same  ERN amplitude determined by subtracting
data associated with trials with positive feedback when preceded by positive feedback from data associated with trials with negative feedback
when preceded with negative feedback; Opposite  ERN amplitude determined by subtracting data associated with trials with positive feedback when preceded by negative feedback from data associated with trials
with negative feedback when preceded by positive feedback. Exp 
Experiment.
HUMAN ERROR PROCESSING
methodological artifacts in the empirical data that did not occur in
the simulated data.
Trial-to-trial variation in the learning process can be seen in
Figure 7, which illustrates that the magnitude of the ERN elicited
by feedback on trials with 50% mapping probabilities tended to be
larger when the previous encounter with the same imperative
stimulus and response was associated with feedback with opposite
valence, compared with when it was associated with feedback with
the same valence. These variations reflect the development of
transient (and incorrect) predictions in the random feedback condition (Figure 8). The ERN tended to be larger when the feedback
stimulus disconfirmed, rather than confirmed, a prediction induced
by a previous feedback stimulus.
The ERN in a Modified Version of the Eriksen Flankers
In the typical version of the Eriksen Flankers Task , participants are exposed to a series of four different stimulus arrays, presented randomly with equal probabilities:
five Hs in a row (HHHHH), five Ss in a row (SSSSS), an H with
two Ss on both sides (SSHSS), and an S with two Hs on both sides
(HHSHH). Participants are directed to focus their attention onto
the letter in the center of each stimulus and to respond with one
hand (typically, by pressing a button or squeezing a dynamometer)
whenever they see a stimulus with an H in the center (HHHHH,
SSHSS), and with the contralateral hand whenever they see a
stimulus with an S in the center (SSSSS, HHSHH). In the present
experiment, we modified the probabilities so that half the participants saw each of the stimuli with a central H on 10% of the trials
and each of the stimuli with a central S on 40% of the trials; the
remaining half of the participants saw the same stimuli with the
converse set of probabilities. Stimuli in which the central and
flanker letters are the same are referred to as compatible, and
stimuli in which the central letter is different from the flankers are
called incompatible. Likewise, we call the highly probable stimuli
frequents and the less probable stimuli infrequents. Thus, there are
four stimulus conditions, which we refer to with the following
shorthand: infrequent compatible, infrequent incompatible, frequent incompatible, and frequent compatible.
When this experiment was originally conducted, we predicted
that the amplitude of the ERN would be smaller on error trials in
the incompatible conditions than in the compatible conditions,
because we thought the error detection process might be compromised when the flanking letters differed from the central target
letter. In fact, we were surprised to find that the empirical results
did not conform to our expectations (see Discussion section below). The task was then modeled using the method of temporal
differences to see if the simulation could account for, and provide
insight into, the observed results.
Experiment.
The experiment consisted of two 3.5-hr sessions.
Fifteen participants were paid $5 an hour for their participation,
plus a $5 bonus for completing both sessions. Participants sat in
front of a CRT in a dimly lit room and performed the modified
version of the Eriksen Flankers Task described above. Each session consisted of a practice block followed by 12 blocks of 200
trials each, with 5–10-min breaks between blocks. Hence, for each
participant, 4,800 trials of data were collected. Of these, 480 trials
were associated with each of the two infrequent stimulus arrays
(10% probability of appearance) and 1,920 with each of the two
frequent stimulus arrays (40% probability of appearance). Stimulus onset asynchrony was 1.5 s. Each stimulus array appeared on
the screen for 50 ms, and participants sat 1 m from the display such
that each letter in the array subtended about .5° of visual angle.
Participants responded by squeezing two zero-displacement dynamometers (Linear Velocity Force Transducers, Model 152A, Daytronic Corp., Dayton, OH) connected to an amplifier system (Conditioner Amplifiers, Model 830A, Daytronic Corp., Dayton, OH).
During the experiment, overt responses were registered when the
participant’s squeeze force exceeded 25% of his or her maximum
squeeze force, which was determined for each participant at the
start of the session.
Following each block, feedback informing the participants of
their accuracy (percent correct) and average speed (in milliseconds) was presented on the CRT. For the purposes of the feedback,
responses generated during the first 50 ms following stimulus
onset were considered errors. Participants were asked to respond as
quickly as possible while maintaining an accuracy of about 85%.
Participants were told that if their accuracy fell to 80%, then on the
following block they should respond slower to improve their
performance. Conversely, they were told that if their accuracy
were to rise to 90%, then on the following block they should
improve their speed. Verbal feedback was also provided, for
example, as encouragement to the participants to break their personal records in speed and accuracy.
The amplitude of the ERN was determined from the responselocked ERP averages for each participant with an algorithm that
identified the peak negativity recorded at channel Cz between 0 ms
and 200 ms following the response. This algorithm also identified
the onset of the component and computed its base-to-peak magnitude. .
Simulation.
Fifteen simulations were run, one for each participant. The model was trained using the data collected in the
experiment from each individual participant. For each trial for a
Weight changes to the value units associated with each response derived from the simulation. Data are for the 50% mapping condition. Positive  change in weight induced by positive feedback; Negative  change in weight induced by negative feedback.
HOLROYD AND COLES
particular participant, the adaptive critic received the stimulus
encountered by the participant on that trial (HHHHH, SSHSS,
HHSHH, or SSSSS), the response produced (left vs. right hand),
and the associated outcome (correct vs. incorrect). As in the
previous simulation, the adaptive critic computed the value of
ongoing events and outputted a TD error when it detected a change
in value. The TD error was used by the adaptive critic itself to
refine its ongoing predictions. ERN amplitude was taken as the
magnitude of the temporal difference error associated with response generation (see Appendix B). To simulate the effect of task
instructions, which specified the appropriate stimulus–response
mapping, the value layer weights associated with the “correct”
conjunctions of stimuli and responses were initialized at 1, and
the value layer weights associated with the “incorrect” conjunctions of stimuli and responses were initialized at 1. Because the
adaptive critic was trained using the participant’s own responses,
we did not model the response-generation process itself (i.e., the
motor controllers, control filter, and output modules; see Figure 1).
However, under these circumstances, the adaptive critic should
learn to generate prediction errors associated with the outcomes of
each of the participant’s responses that mirror the response-locked
ERNs generated by each participant in the experiment (see Appendix B).
In the experimental data, the ERN was larger on
incorrect (M  4.0 V) than on correct (M  1.3 V) trials,
t(14)  4.8, p  .01 . Figure 9 illustrates the amplitude of the ERN
on infrequent compatible, infrequent incompatible, and frequent
incompatible error trials, as generated by both the simulation and
the human participants. (Very few errors were generated on frequent compatible trials, so an analysis of the ERN in this condition
was omitted.) A one-way analysis of variance (ANOVA) on ERN
amplitude as a function of stimulus type revealed that not all the
mean amplitudes were equal, F(2, 28)  16.8, p  .01,   .61;
Duncan’s Multiple Range Test indicated that ERNs elicited on
infrequent compatible (M  4.5 V) and infrequent incompatible (M  3.3 V) error trials were significantly smaller than
ERNs elicited on frequent incompatible (M  9.2 V) error
trials, but not significantly different from each other. To ensure
that the observed difference in ERN amplitude between the frequent incompatible and infrequent incompatible conditions did not
depend on differences in reaction time between the two conditions,
we also matched each of the errors in the frequent incompatible
and infrequent incompatible conditions by reaction time . When so matched, the ERN
elicited by errors on frequent incompatible (M  9.1 V) trials
was still larger than when it was elicited by errors on infrequent
incompatible (M  5.0, V) trials, t(14)  3.8, p  .01.
As can be seen, the output of the model corresponded qualitatively to the amplitude of the ERN observed in the experiment,
with the largest simulated ERN occurring on frequent incompatible error trials (M  1.83), the smallest on infrequent incompatible error trials (M  0.60), and an intermediate magnitude on
infrequent compatible error trials (M  1.16). A one-way
ANOVA on ERN amplitude as a function of stimulus type revealed that not all the mean amplitudes are equal, F(2, 28)  291,
p  .01,   .93, and Duncan’s Multiple Range Test indicated that
all three means were significantly different from one another.
Further analysis revealed that the amplitude of the error signal
depended on the probability of generating the correct response in
each stimulus condition. Table 1 presents accuracy and reaction
times (of the empirical data) for each condition; a one-way
ANOVA on participant accuracy as a function of stimulus type
revealed that not all the mean accuracies were equal, F(2, 28) 
305, p  .01,   .87, and Duncan’s Multiple Range Test indicated that all three means were significantly different from one
another. . Figure 10 illustrates the magnitude of the real and
simulated ERNs plotted against participant accuracy within the
infrequent compatible, infrequent incompatible, and frequent incompatible stimulus conditions. The magnitude of the error signal
increases with the probability of making a correct response.
Figure 11 illustrates the weights of the value units associated
with the imperative stimuli in the simulated data. As can be seen,
the system learned to attribute a positive value to the frequent
incompatible stimulus, a negative value to the infrequent incompatible stimulus, and an intermediate value to the infrequent compatible stimulus. Hence, the amplitudes of the ERN in Figures 9
and 10 reflect the change in prediction precipitated by error commission in each of these conditions.
Discussion.
Originally, we had predicted that the amplitude of
the ERN should be smaller on error trials in the incompatible
conditions than in the compatible conditions because we thought
the error detection process might be compromised when the flanking letters differed from the central target letter. Thus, we were
surprised to find that the amplitude of the ERN was larger on
frequent incompatible error trials than it was on infrequent compatible and infrequent incompatible error trials. In fact, the ERNs
generated by the computational model mirrored those produced by
the human participants (Figure 9). The variation in ERN amplitude
appeared to be associated with the accuracy of the participant in
each stimulus condition (Figure 10). Evidently, because most
Error-related negativity (ERN) amplitude in the modified Eriksen Flankers Task. For both the model and the experimental data, ERN
amplitude is larger on frequent incompatible error trials than on infrequent
compatible and infrequent incompatible error trials. Simulated results are
given in multiples of reward (r). Empirical results are given in microvolts.
III  infrequent compatible condition; FIF  infrequent incompatible
condition; IFI  frequent incompatible condition; Exp  Experiment.
HUMAN ERROR PROCESSING
frequent incompatible trials were correct, the model predicted that
correct responses would be generated on those trials (Figure 11).
However, when an incorrect response was generated instead, the
prediction was revised and a large ERN was produced. In contrast,
on infrequent compatible and infrequent incompatible trials, the
system was less certain that the outcome would be favorable;
hence, errors in these conditions generated much smaller ERNs.
General Discussion
We have presented an account of the ERN that depends on the
reinforcement learning properties of the mesencephalic dopamine
system and the anterior cingulate cortex. We began by reviewing
the literature on the ERN, and we briefly described the role the
mesencephalic dopamine system may play in reinforcement learning. We then discussed the nature of the anterior cingulate cortex,
including the neuroanatomy of its motor areas, its sensitivity to
response conflict, and its involvement in behavioral adaptation.
We then proposed that the ERN is generated when a negative TD
error is carried by the mesencephalic dopamine system to the
anterior cingulate motor areas, during or after response generation.
According to this view, the ERN is produced when the system first
detects that the consequences of an action are worse than expected.
We also suggested that the anterior cingulate cortex uses the error
signals to improve the system’s performance on the task at hand by
shifting responsibility for the task to the motor controller that is
most capable of performing it. We then presented two simulations
that formalized these propositions, and we compared the output of
the simulations with the results of two ERP experiments. The
simulated and empirical results are largely consistent with each
other and support our hypothesis.
We have provided a theoretical framework for understanding
the error-processing system that produces the ERN. First, we have
suggested a specific algorithm for describing the behavior of the
ERN in various psychological tasks. The consistency of the model’s data with those of human participants suggests that the algorithm is appropriate for modeling the ERN, one that can be used to
elucidate the nature of the learning process in a wide variety of
tasks . Just as important, the model makes
explicit the functional nature of the systems underlying ERN
generation. Thus, our vocabulary is no longer limited to the general statement that the system that gives rise to the ERN is
somehow involved in error processing. Rather, we can say explicitly that the ERN represents the first indication that the consequences of an action are worse than expected and that this signal
is used to train the response production system in a manner that is
consistent with neural network implementations of reinforcement
learning. Second, we have presented a general account of how
these systems are implemented in the brain. Although ours is not
a detailed neurophysiological model, it is nonetheless consistent
with current views on the structure and function of the specified
neural systems. It thus provides an avenue for exploring the
dependence of the ERN on the functional integrity of these systems. In this way, it may provide insight, for example, into why
people with Parkinson’s disease (in whom the midbrain dopamine
Accuracy (Percent Correct) and Reaction Times
(in Milliseconds)
Correct RT
FFF  frequent compatible; IFI  frequent incompatible; FIF 
infrequent incompatible; III  infrequent compatible; RT  reaction time.
a Too few errors were generated in this condition to determine error RT.
Figure 10.
Error-related negativity (ERN) amplitude as a function of
within-condition accuracy. For both the model and the experimental (Exp)
data, ERN amplitude increases as the within-condition accuracy increases.
Simulated results are given in multiples of reward (r). Empirical results are
given in microvolts. IFI  frequent incompatible condition; III  infrequent compatible condition; FIF  infrequent incompatible condition.
Figure 11.
Value unit weights associated with the imperative stimuli
derived from the simulation. Shown are the initial weights at the start of the
experiment and the final weights at the end of the experiment. III 
infrequent compatible condition; FIF  infrequent incompatible condition;
IFI  frequent incompatible condition.
HOLROYD AND COLES
nuclei are damaged) are impaired at a reinforcement learning task
 . At the same time, our
model also raises a number of important questions that need to be
addressed in future research. In the following pages we discuss
several of these unresolved issues.
Comparison With Other Theories
Our model should not be construed as a new theory of the
anterior cingulate cortex. Rather, the model holds simply that the
anterior cingulate cortex is trained to improve its performance
using TD error signals carried to it by the mesencephalic dopamine
system and that transfer of negative TD errors in this fashion elicits
the ERN. In this respect, our theory is firmly grounded in previous
computational work associating phasic dopaminergic activity with
the method of temporal differences and, more generally, with
reinforcement learning signals .
Although we have tentatively suggested a particular function for
the anterior cingulate cortex—namely, as a motor control filter—
our theory is potentially compatible with several other accounts.
Further investigation will be needed before any one theory of
anterior cingulate function can be chosen over another.
Nevertheless, the relationship between our model and two prominent theories of the anterior cingulate cortex deserves consideration. The first of these two theories holds that the anterior cingulate cortex is involved in “selection for action,” which Allport
 defined as a cognitive mechanism that “can selectively
designate a specified subset of the available, and potentially relevant, sensory information to have control of a given effector
system, and can selectively decouple the remainder from such
control” (p. 397). The motivation for this position comes from
functional neuroimaging studies. In a positron-emission tomography (PET) study, blood flow in the anterior cingulate cortex
increased with the number of targets in a cognitive task, suggesting
to investigators that the anterior cingulate cortex was concerned
with selecting among the targets . Because blood flow to this area also increased
relative to control conditions in a verb generation task , it was also proposed that this area was involved in
“selecting among several competing choices of an appropriate verb
for each noun” . Thus, the anterior
cingulate cortex was said to be concerned with “selection among
competing, complex contingencies” , whether those contingencies were stimulus or response related. Though never stated explicitly, in this view, the anterior
cingulate cortex is responsible for applying the mappings between
stimuli and responses, becoming more activated as the nature of
the stimuli or the response options make the mappings more
difficult to implement.
Because the control filter in our model comprises a hidden layer
placed between stimulus input and response output, our model is
compatible with this very broad position on anterior cingulate
function. More importantly, our theory of the ERN is consistent
with the view advanced by Paus et al. , who took the
selection-for-action theory one step further. They suggested that
motor areas in the anterior cingulate cortex “funnel” executive
commands originating from the dorsolateral prefrontal cortex to
the effector system, and they speculated that influence of the
mesencephalic dopamine system on the anterior cingulate cortex
would result in “facilitation of the execution of the appropriate
responses and/or suppression of the inappropriate ones” (p. 467).
By using the mesencephalic dopamine system to train the anterior
cingulate cortex to support appropriate behaviors while suppressing inappropriate ones, our model achieves much the same result.
Conflict monitoring , the second theory of anterior cingulate function,
shares several important characteristics with the selection for action theory but is computationally more specific. The theory holds
that the anterior cingulate cortex is responsible for detecting the
simultaneous activation of incompatible processing channels. In
contrast to the selection for action theory, the anterior cingulate
cortex is not given the role of applying the mapping between
stimulus and response. Instead, the anterior cingulate cortex detects when crosstalk between stimulus channels or between response channels impedes smooth application of the stimulus–
response mapping. This information is then sent to the relevant
channels where it is used to reduce the conflict. In this view, the
anterior cingulate cortex performs a monitoring function, initiating
intervention in other systems to adjust their behavior as needed.
Thus, the conflict theory is distinguished from the action selection
theory, and from the hypothesis advanced in this article, by attributing a monitoring role rather than a response-selection role to the
anterior cingulate cortex.
The conflict model makes specific predictions about the ERN,
and so we compare it here with our own model in detail. With
respect to the ERN, the two models appear to be in fundamental
disagreement. Whereas our theory holds that the ERN is elicited by
transmission of training signals to the anterior cingulate cortex, the
Botvinick et al. model maintains that the ERN is generated
by response conflict—that is, the more conflict, the larger the
ERN. These positions lead to several incompatible predictions.
First, the conflict model predicts that ERNs should be elicited on
some correct trials, when conflict is high. This prediction was
supported by the observation of a so-called “correct ERN,”
which—as the name indicates—is an apparent ERN elicited in
conjunction with a correct response . However, we have recently shown that
ERNs can occur on some correct trials when (a) stimulus-locked
activity creates an ERN-like artifact in the response-locked ERP or
(b) the participant’s representation of the correct or actual response
in compromised . Moreover, both the conflict
and reinforcement learning accounts predict that ERNs can occur
on correct trials when the correct response is produced simultaneously with an incorrect response: in the former case, because of
conflict associated with the simultaneous activation of incompatible response channels, and in the latter case, because of error
processing associated with the generation of the incorrect response. To test these predictions, Scheffers obtained a
measure of conflict by multiplying together the EMG activity
associated with both response channels in a two-choice reactiontime task. He showed that, when matched for conflict, the ERN
was larger on incorrect trials (when activation of the incorrect
response was larger than that of the correct response) than on
HUMAN ERROR PROCESSING
correct trials (when activation of the correct response was larger
than that of the incorrect response). Thus, the process that generates the ERN appears to be more sensitive to error commission
than to response conflict. The same conclusion was reached by
Falkenstein and colleagues in a different set of experiments . Finally, our model accounts nicely for the
observation that negative feedback elicits an ERN , and, as shown in this article, that this ERN
propagates with learning to the erroneous response. It is difficult to
see how the conflict-monitoring theory could account for ERNs
elicited by negative feedback, much less demonstrate how the
behavior of the ERN evolves with learning.
Nevertheless, the conflict-monitoring hypothesis accounts for a
wealth of functional magnetic resonance imaging (fMRI) and PET
data that the reinforcement learning–ERN model does not address,
especially data that suggest activation of the anterior cingulate
cortex in conditions of high response conflict. In general, our
position allows for activation of the anterior cingulate cortex in the
absence of ERN generation. Thus, our model is not incompatible
with a conflict model of anterior cingulate function—only with the
position that the ERN is a manifestation of such conflict. Note that
we could have implemented our model as a simple adaptive
critic–actor combination, with the anterior cingulate cortex serving
as the actor. The anterior cingulate cortex in this design might be
thought to mediate selection for action because it would function
only to translate stimulus input into response output. We decided
against adopting this simpler architecture for two reasons. First,
the single motor-controller model does not provide a role for the
convergence of cognitive and limbic input passing through the
anterior cingulate cortex on the way to the motor system. Second,
the single-controller model does not specify how conflict could
arise from the simultaneous activation of incompatible processing
channels. We suggest that the design we implemented, with multiple motor controllers, is more consistent with these observations.
On the one hand, the motor controllers in our model provide an
avenue for cognitive and limbic input to direct motor output via the
anterior cingulate cortex; on the other hand, the anterior cingulate
cortex decides between any conflicting directives issued by the
competing controllers. A measure of such motor controller conflict
could be developed in a future model.
Recently, Braver and Cohen independently developed a
computational model of dopaminergic modulation of the prefrontal
cortex that is similar in some important respects to our model of
the ERN. In their model, as in ours, the mesencephalic dopamine
system carries a TD error to frontal areas of the brain. This phasic
dopamine activity both gates information into and out of working
memory in the prefrontal cortex and subserves a reinforcement
learning function that enables the system to discover what information is relevant in the task context. Braver and Cohen 
also suggested that a reduction of dopaminergic activity on error
trials could “reset” information held in working memory, facilitating the search for a more adaptive response . Braver and Cohen’s model appears to be compatible
with our model, and current work involves unifying them, together
with a third model of the basal ganglia, prefrontal cortex, and
working memory , into a
common framework.
Structure of the Model
As we noted, our conception of the anterior cingulate cortex as
a control filter was motivated by several observations, including
the convergence of cognitive and limbic input passing through it
on the way to the motor system and its contribution to reinforcement learning. Our interpretation is consistent with prevailing
notions of anterior cingulate function. It does, however, require
future elaboration. For example, response selection by the anterior
cingulate cortex need not occur in a “winner-take-all” fashion, in
which a single motor controller is given responsibility over the
motor system. Instead, more sophisticated algorithms exist for
blending the output of several modules . A future simulation could adopt
such an algorithm, with the anterior cingulate cortex combining the
output of several motor controllers to guide the response selection
process. Moreover,2 the “temperature” used by the control filter
for selecting between controllers ( in Equation 3, Appendix B)
could be gradually reduced over the course of learning, thereby
improving the system’s performance once an appropriate motor
controller has been identified.
Our treatment of the controllers themselves is even more abstract. We envision only that control over the motor system occurs
in a modular fashion, that certain controllers are capable of performing a given task better than others, and that some of the output
from all the modules passes through the anterior cingulate cortex.
Substantial neuropsychological, neurophysiological, and functional neuroimaging evidence indicates that motor control does
proceed in such a modular fashion. For example, areas in the
orbitofrontal cortex appear to be involved in guessing and in decision making under uncertainty
 . These phenomena
require sensitivity to the value associated with different possible
response options . Moreover, neurons in the orbitofrontal cortex code for the relative preference between different rewards . The orbitofrontal cortex
appears to apply such information for deciding between large,
unlikely rewards and small, likely rewards 
and for abandoning obsolete response patterns in favor of newly
rewarded behaviors . In
contrast, the dorsolateral prefrontal cortex appears to be more
concerned with generating responses when the external context is
impoverished, both by brainstorming novel response alternatives
when necessary and by maintaining the external
context in working memory to generate a response in the absence
of external stimulation . Our model asserts only that such
different controllers exist, without detailing the specific nature of
every one.
Although all the motor controllers in the model receive a copy
of the TD error, we do not intend to suggest that this is the case for
all the motor controllers in the human brain. Rather, we expect that
2 We thank one of the reviewers for making this suggestion.
HOLROYD AND COLES
some neural controllers can execute their normal function without
depending on reinforcement learning signals. Nevertheless, given
that the mesencephalic dopamine system projects throughout the
basal ganglia and frontal cortex, we do suggest that many of the
controllers in these regions use the reinforcement signals to carry
out their normal function. Disruption of the mesencephalic dopamine system severely upsets the working memory and executive
control functions of the prefrontal cortex . Thus, although the impact of the dopamine
signal may not be limited to reinforcement learning per se, it may
nevertheless facilitate error-processing functions mediated by
these areas.
Clearly, many of these regions do depend on error-processing
functions that could be facilitated by reinforcement learning. For
example, even though individuals with prefrontal damage make
repeated errors when performing certain psychological tasks, they
can still verbally report when their responses are correct and
incorrect . This observation
has motivated the proposition that the prefrontal cortex, although
not involved in error detection, is concerned with applying error
information toward behavioral modification . Error-related activity
has also been identified in several neural areas throughout the
frontal cortex. For example, two functional neuroimaging studies
 found error-related activity
in the human dorsolateral prefrontal cortex, as well as in the
anterior cingulate cortex, when human participants committed
errors in reaction-time tasks. Single-unit studies in monkeys have
also identified error-sensitive units in the dorsolateral prefrontal
cortex and the supplementary eye field . These
results suggest that the dorsolateral prefrontal cortex and the
supplementary eye field, in addition to the anterior cingulate
cortex, contribute to improving performance at some tasks. Similarly, other studies have found error-related units in the orbitofrontal cortex . These neural signals may
reflect contributions by the mesencephalic dopamine system to
orbitofrontal function, perhaps by developing a value system
needed to guide decision making under uncertainty .
Our model does not include many of the connections known to
exist between the neural areas that are represented in it. Of particular importance is the fact that the model does not include
well-known striatocortical loops. These appear to be segregated
into two more or less independent systems .
On the one hand, a set of segregated loops project in parallel
through “matrisome” modules in the basal ganglia via the substantia nigra pars
reticulata and internal segment of the globus pallidus to the thalamus. From there they project in parallel to the frontal cortex and
then return to the basal ganglia .
These loops are thought by many to form a neural substrate for
working memory . On the other hand, “striosome” modules in the basal ganglia compose the primary input
into the mesencephalic dopamine system—which, as we have
seen, conveys a scalar error signal to the basal ganglia and to the
frontal cortex .
The striosome modules primarily convey limbic information to the
mesencephalic dopamine system, and it has been suggested that
they comprise the neural area where TD errors are computed
 .
Importantly, the only cortical regions seen to project strongly to
the striosomes derive from the caudal orbitofrontal–anterior insular cortex and the mediofrontal prelimbic–anterior cingulate cortex
 . Thus, neural activity in the anterior
cingulate cortex could in principle drive the activity of the mesencephalic dopamine system, which would then be reflected back
to the anterior cingulate cortex and other areas. Indeed, stimulation
of the medial prefrontal cortex and the anterior cingulate cortex
has been shown to induce burst firing in midbrain dopamine
neurons . This feedback complicates the picture of
where exactly error detection occurs. Further research will be
needed to elucidate how the existence of this loop bears on the
generation of the ERN.
Neurophysiological Considerations
We suggest that dopaminergic disinhibition of the ventral bank
of the anterior cingulate sulcus can generate the ERN by enabling
large portions of the apical dendrites of Layer V neurons to
become depolarized. The EEG is generated by the synchronous
activity of postsynaptic potentials acting on the dendrites or soma
of cortical pyramidal cells . Because pyramidal cells are aligned
in parallel, postsynaptic potentials acting across several of them
tend to summate by the principle of superposition 
to generate measurable scalp potentials. Their elongated structure
also ensures that negative potentials associated with current sinks
at one end of the neuron suffer minimal cancellation from positive
potentials associated with current sources at the opposite end of the
cell. This geometry fixes the dipole in parallel with the direction of
pyramidal cell orientation, which is invariably perpendicular to the
cortical surface. We suggest that the dopaminergic disinhibition of
the apical dendrites of anterior cingulate motor neurons is consistent with these requirements. Indeed, the mechanism we offer here
for consideration would seem to be a textbook example of ERP
generation.
As we have seen, the mesencephalic dopamine projection terminates diffusely in the human frontal cortex, but its output is
focused over the frontal midline . Dopamine neurons
innervate Layer I of the cortical mantle densely, preferentially
synapsing there on the apical dendrites of pyramidal cells including Layer V motor neurons
 ,
which express high levels of dopamine D1 receptors . Although the issue
has not been resolved, dopamine’s influence on deep cells in rat
medial prefrontal areas appears to be inhibitory . Dopamine reduces the peak amplitude of excitatory
postsynaptic potentials evoked by stimulating the Layer I inputs to
these neurons and reduces their rate of
action potential generation . Furthermore, stimulation of the VTA inhibits deep
cells in the rat medial prefrontal cortex . Conversely, a
reduction of dopaminergic input from the VTA likely disinhibits
deep cingulate cortical neurons, resulting in their increased activity
 . Hence, in trial-and-error learning tasks, it is possible that
the reduction in dopaminergic activity seen on error trials when an
expected reward is not delivered could give rise to the
increased activity of anterior cingulate neurons seen in the same
circumstances .
Layer V neurons in the anterior cingulate motor area are especially capable of contributing to the EEG. These neurons are the
defining feature of the cingulate sulcus, each having a single apical
dendrite that stretches from the cell body for more than 1 mm to
Layer I, where they bifurcate profusely . In area 24cg, these neurons are called “gigantopyramidal”
cells in deference to their enormous girth and extensive dendritic
arborizations . Excitation of Layer
V neurons generates large amplitude (20.5  1.0 mV) excitatory
postsynaptic potentials lasting longer than 50 ms . Thus, their elongated
structure and strong excitability, together with massive synaptic
input onto their apical dendrites , make activity in these neurons plausible candidates for ERP
generation. Furthermore, a reduction of dopaminergic input to the
apical dendrites of Layer V neurons in the anterior cingulate
cortex, in conjunction with strong cortical excitation, activates
voltage-dependent Ca2 channels, precipitating a cascade of highthreshold spikes that depolarizes extended portions of their dendritic arbor . When taken together, the
morphology and physiology of anterior cingulate motor neurons in
the presence of mesencephalic dopamine would seem to be compatible with the physical principles that underlie ERP generation.
We have described a scenario in which the apical dendrites of
the pyramidal cells are depolarized following disinhibition induced
by a reduction from baseline of dopaminergic input. In this case,
the negative side of the potential field associated with the equivalent dipole lies above the apical dendrites of the pyramidal cells.
Because the ERN is a negativity with a frontal central distribution,
the negative pole of the equivalent dipole must point in this
direction, where the scalp activity reaches a maximum. In practice,
because the equivalent dipole is always oriented in parallel to the
pyramidal cells, this constraint implies that the cortical layer that
generates the ERN must run parallel to the area of the scalp where
the ERN is maximal. Within the medial frontal cortex, the only
cortical areas that are oriented in parallel to that area are the sulci.
Thus, if the ERN is generated by a single focal dipole layer in the
frontal midline, then it must be generated within a sulcus there.
Within the frontal midline, only the cingulate sulcus, followed by
the paracingulate sulcus, are consistent across the human population . Moreover, because of the
orientation of the dipole layer, the ERN must be generated within
the ventral bank of the sulcus, where the apical dendrites of the
pyramidal cells point toward the scalp. Thus, we suggest that the
cingulate motor areas, which belong to the ventral bank of the
cingulate sulcus, which itself composes about a quarter of the
surface of the medial wall , generate the ERN
 .
Computational Considerations
Because the method of temporal differences was developed by
researchers in the artificial intelligence community, the algorithm
might seem to learning theorists to be an odd choice for modeling
human behavior. However, the algorithm is appropriate, for several reasons. First, the roots of the algorithm do in fact stem from
animal learning theory. As we have noted, the method of temporal
differences is a generalization of the Rescorla–Wagner learning
rule, an extension of the model from trial-by-trial learning to the
continuous time domain . The algorithm
accounts for all of the behavioral observations predicted by the
Rescorla–Wagner rule, and more . Therefore, any perceived dissociation between the algorithm and animal
learning theory may be more apparent than real. Second, in addition to accounting for overt behavior, the method of temporal
differences simultaneously describes the dynamics of the neural
system that gives rise to that behavior . The
algorithm also belongs to a class of biologically plausible learning
rules . Thus the algorithm provides a compact description of learning phenomena at
both the behavioral and neurophysiological levels. Third, the algorithm is computationally robust. Several studies have shown that
the algorithm converges rapidly under many conditions . Fourth, the computational power of the algorithm
has been amply demonstrated. For example, the method of temporal differences has been used to teach a computer program to
play world-class backgammon . With 30 game
pieces, 26 possible locations for each of those pieces, and on
average 20 different ways of playing each dice roll, the number of
possible states of the game is huge .
Therefore the algorithm should perform well in real-world situations, where the number of possible world states that predict any
particular outcome are presumably at least as great. Lastly, as a
reinforcement learning algorithm, the method of temporal differences was designed to solve the kinds of executive control prob-
HOLROYD AND COLES
lems associated with the frontal system, that is, guiding the organism in complex and uncertain environments. Thus the algorithm is
appropriate for simulating the function of the neural system in our
Our model makes use of eligibility traces. Eligibility traces
function as a kind of memory, activating when an associated state
is encountered and decaying thereafter .
Each of the motor controllers in the model is associated with an
eligibility trace, which all have zero activation at the start of every
trial. When the anterior cingulate cortex selects a motor controller
during the response generation process, the eligibility trace associated with the winning controller reaches maximum value; when
the response is concluded, the trace decreases to a fraction of its
maximum value. Thus, the representation of the selected controller
is strongest during the selection process, when the winning controller is actually used to generate a response, but is less strong
when maintained as a memory. Eligibility traces are typically
thought to represent a synaptic mechanism whereby a given synapse remains eligible for learning after the pre- and postsynaptic
elements have been activated . In principle,
however, eligibility traces can also be sustained in working memory, at once maintaining an abstract representation of the encountered state and keeping activated the neural units that describe that
state. Given the well-known involvement of the prefrontal cortex
and the basal ganglia in working memory , these systems could in fact
contribute to maintaining states eligible for learning.
In our model, an ERN is generated only when (a) the anterior
cingulate receives a TD error from the mesencephalic dopamine
system and (b) an eligibility trace associated with any of the motor
controllers is nonzero. We suggest that the apical dendrites (in
Layer I) of Layer V motor neurons in the anterior cingulate cortex
receive excitatory input from the motor controllers during the
response generation process, and continue to receive input from
the winning motor controller after the response generation process
has been completed, when the representation of the selected controller is maintained in working memory. We further suggest that
the ERN is generated when this excitatory input is disinhibited by
a phasic reduction from baseline activity of the mesencephalic
dopamine system. In practice, this means that although the TD
error propagates back to the imperative stimulus to the degree the
stimulus predicts the outcome of the trial, the ERN is never elicited
before a response is generated. Thus, the ERN, unlike a negative
TD error, specifically indicates when the consequences of a response are worse than expected. This position is consistent with a
fundamental principle of operant conditioning, which holds that
learning should only occur when the reward or punishment is
contingent on the animal’s behavior. It also is supported by the
work of Porrino , who found that reward-induced activity in
the rat anterior limbic cortex is greatest when the reward is
contingent on the rat’s behavior, compared with when it is delivered independently of what the animal is doing.
Another issue concerns the timing of the ERN with respect to
the error. The onset of the ERN coincides with the onset of EMG
activity that gives rise to the erroneous response . If error detection depended on feedback
information generated by the overt response, then ERN onset
might be expected to be delayed by about 100 ms following the
onset of error commission . For this reason, it
has been argued that the process that gives rise to the ERN depends
on efference copy—a copy of the motor command issued to the
motor system, processed by some other part of the brain. Such a
command signal could be evaluated even before an erroneous
movement has begun . For didactic reasons,
however, the system that detects the error in our model depends on
feedback from the overt response (Figure 1). This fact might
suggest that the ERN should be delayed following error commission. To account for this apparent discrepancy, the model could be
modified such that a copy of the control filter output (and/or the
motor controller output) could be sent directly to the adaptive
critic. In this way, the adaptive critic could monitor internal
efference copy to detect task-inappropriate neural activity before it
has developed into an overt behavior.
The timing of the ERN with respect to the response may have
important functional consequences. Unlike reinforcement learning
algorithms that wait until an episode has terminated for learning to
occur, temporal difference learning can modify behavior on the
fly. Thus, the neural signal reflected by the ERN could in principle
be used to modulate the activity of the motor system while the
behavior is still in progress without having to wait until the end of
the trial to observe its outcome. For example, the error signal could
initiate a remedial action even before an erroneous response has
reached completion. Although this possibility is not realized in our
current version of the model, it could be in future simulations. For
both computational and physiological reasons, it has been suggested that
the level of dopaminergic input into the prefrontal cortex could
modulate the gain of synaptic transmission there. Indeed, Yang
and Seamans have suggested that a reduction of dopaminergic input to the apical dendrites of Layer V pyramidal cells—in
the way we have argued generates the ERN—will in fact decrease
the gain of the pyramidal cell, enabling a wider range of external
input to stimulate the neuron. Lowering the gain, together with
noisy input, would increase the variability in the response selection
process . We speculate that a reduction of gain on error trials would enable the anterior cingulate
cortex to search for an optimal motor controller over a wider
variety of controller inputs. In principle, several controllers might
be compared and evaluated before the response has reached the
overt stage of execution . These investigators have suggested that the
Layer I interface is computationally significant because it is electrically
isolated from the soma, enabling the dendrites there to perform simple
logic operations on incoming information . They have further argued that
synaptic triads on the apical dendrites of Layer V cells form logic gates
strategically situated to inhibit cortical–cortical excitation . If this were indeed the case, then the ERN might reflect massive
disinhibition of this computational interface.
HUMAN ERROR PROCESSING
Keefe, DiFrischia, & Zigmond, 1989; Doherty & Gratton, 1992;
Louilot, Le Moal, & Simon, 1986; Young, Joseph, & Gray, 1993).
If negative feedback stimuli are understood to be punishment
signals, these observations may seem in disagreement with our
position that negative feedback should reduce dopaminergic activity. However, we suggest that this potential criticism is not valid,
for two reasons. First, our argument is based on the observation
that phasic dopaminergic activity decreases from baseline when a
predicted reward is not delivered ; to our knowledge, this
important discovery has not yet been challenged. From a functional standpoint, we suggest that the absence of a predicted
reward conveys the same information to a monkey as a negative
feedback stimulus does to a human participant, namely, that an
error has just been committed. This error information need not be
associated with physical pain or discomfort. Thus, although we
might loosely call the penalties in our feedback learning paradigm
“punishments,” these stimuli are not expected to induce the increases in extracellular dopamine that result from physical distress.
Second, although the absence of a predicted reward elicits dopaminergic activity on a short time scale (100 ms), the increase in
extracellular dopamine induced by punishment occurs on a much
longer time scale ( 1 s). This dissociation between fast and slow
dopaminergic activity suggests that different neural mechanisms
underlie the two processes. Because our argument is agnostic
about how such slow activity should affect the ERP, it makes no
predictions about whether or not such increases would disrupt the
ERN. Specifically, it is unclear whether the slow increases in
extracellular dopamine involve D1 receptors located on the apical
dendrites of Layer V pyramidal cells and, even if they did, how
those increases would affect local signal transduction.
A final note concerns the predictions we used to evaluate the
model. Although the simulated ERNs behaved quite similarly to
what we see in humans, the variance of those data was relatively
small, despite variability in the model’s starting weights and a
noise term added to the TD error (see Appendix B). It might be
argued that the simulation should capture the variance of the
observed ERNs in addition to their central tendencies. We disagree
with this criticism for the following reasons. First, the ERN (like
any ERP) is subject to noise, the unwanted effects of which are
typically reduced, but never eliminated, by averaging. Thus the
observations obtained with our measurement technique contain
variance not associated with the underlying functional process,
which we assume conforms to the output of the simulation. Second, as with most biological phenomena, individual variation
between participants can be quite large; for example, considerable
variation exists between participants in the structure and orientation of sulci on the medial wall . Although such factors can clearly influence the amplitude
and distribution of the ERN, it is less clear if they affect the
associated neural function. It might also be argued that the simulation should produce ERNs that are given in microvolts, like real
ERNs, instead of in arbitrary units. However, because ours is a
functional model, not a biophysical model, we are interested
mainly in the ordinal relationship between the predictions and not
their absolute magnitude. Of course, we could introduce a simple
multiplicative factor to bring the predictions in line with the
observations, but we feel such an addition would be gratuitous and
obscure the core properties of the model.
Conclusion
Demonstrating the existence of biological error signals is fundamentally important to computational neuroscience . Our work on the ERN has revealed one
such signal in normal human participants, has highlighted the
important role it appears to play in executive control, and has
suggested a neural area where it might be generated . This article contributes to that account by grounding the
observations in a computational framework that (a) formalizes a
general set of statements about the error signal into a coherent
model with increased explanatory power and (b) makes predictive
statements about the error processing system that can be tested in
future experiments. Furthermore, the model makes specific claims
about the neural systems that implement the computations and thus
provides an avenue for testing the integrity of the theory using
patient populations. Although the model requires future elaboration, our initial simulations and experiments have yielded positive
results that provide motivation for further research. If valid, this
account of the system that gives rise to the ERN may provide
insight into how large-scale neural networks for motor control are
trained and modulated in biological systems.