Deep Learning in Neural Networks: An Overview
Technical Report IDSIA-03-14 / arXiv:1404.7828 v4 [cs.NE] (88 pages, 888 references)
J¨urgen Schmidhuber
The Swiss AI Lab IDSIA
Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale
University of Lugano & SUPSI
Galleria 2, 6928 Manno-Lugano
Switzerland
8 October 2014
In recent years, deep artiﬁcial neural networks (including recurrent ones) have won numerous
contests in pattern recognition and machine learning. This historical survey compactly summarises
relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal
links between actions and effects. I review deep supervised learning (also recapitulating the history
of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation,
and indirect search for short programs encoding deep and large networks.
LATEX source: 
Complete BIBTEX ﬁle (888 kB): 
This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit
to those who contributed to the present state of the art. I acknowledge the limitations of attempting
to achieve this goal. The DL research community itself may be viewed as a continually evolving,
deep network of scientists who have inﬂuenced each other in complex ways. Starting from recent DL
results, I tried to trace back the origins of relevant ideas through the past half century and beyond,
sometimes using “local search” to follow citations of citations backwards in time. Since not all DL
publications properly acknowledge earlier relevant work, additional global search strategies were employed, aided by consulting numerous neural network experts. As a result, the present preprint mostly
consists of references. Nevertheless, through an expert selection bias I may have missed important
work. A related bias was surely introduced by my special familiarity with the work of my own DL
research group in the past quarter-century. For these reasons, this work should be viewed as merely a
snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send
corrections and suggestions to .
 
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Event-Oriented Notation for Activation Spreading in NNs
Depth of Credit Assignment Paths (CAPs) and of Problems
Recurring Themes of Deep Learning
Dynamic Programming for Supervised/Reinforcement Learning (SL/RL)
. . . . . .
Unsupervised Learning (UL) Facilitating SL and RL
. . . . . . . . . . . . . . . . .
Learning Hierarchical Representations Through Deep SL, UL, RL . . . . . . . . . .
Occam’s Razor: Compression and Minimum Description Length (MDL) . . . . . . .
Fast Graphics Processing Units (GPUs) for DL in NNs . . . . . . . . . . . . . . . .
Supervised NNs, Some Helped by Unsupervised NNs
Early NNs Since the 1940s (and the 1800s)
. . . . . . . . . . . . . . . . . . . . . .
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11) . . . . . . .
1965: Deep Networks Based on the Group Method of Data Handling . . . . . . . . .
1979: Convolution + Weight Replication + Subsampling (Neocognitron) . . . . . .
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs . . . . . . .
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Late 1980s-2000 and Beyond: Numerous Improvements of NNs . . . . . . . . . . .
Ideas for Dealing with Long Time Lags and Deep CAPs . . . . . . . . . . .
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24) .
Potential Beneﬁts of UL for SL (Compare Sec. 5.7, 5.10, 5.15) . . . . . . . .
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15) . . . . . . .
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
. . . . . . . . . . . . . . . . . .
1991: Fundamental Deep Learning Problem of Gradient Descent . . . . . . . . . . .
5.10 1991: UL-Based History Compression Through a Deep Stack of RNNs
. . . . . . .
5.11 1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19) . . . . . . .
5.12 1994: Early Contest-Winning NNs . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.13 1995: Supervised Recurrent Very Deep Learner (LSTM RNN) . . . . . . . . . . . .
5.14 2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs . . . . . .
5.15 2006/7: UL For Deep Belief Networks / AE Stacks Fine-Tuned by BP . . . . . . . .
5.16 2006/7: Improved CNNs / GPU-CNNs / BP for MPCNNs / LSTM Stacks . . . . . .
5.17 2009: First Ofﬁcial Competitions Won by RNNs, and with MPCNNs . . . . . . . . .
5.18 2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record . . . . . . . .
5.19 2011: MPCNNs on GPU Achieve Superhuman Vision Performance
. . . . . . . . .
5.20 2011: Hessian-Free Optimization for RNNs . . . . . . . . . . . . . . . . . . . . . .
5.21 2012: First Contests Won on ImageNet, Object Detection, Segmentation . . . . . . .
5.22 2013-: More Contests and Benchmark Records
. . . . . . . . . . . . . . . . . . . .
5.23 Currently Successful Techniques: LSTM RNNs and GPU-MPCNNs . . . . . . . . .
5.24 Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
. . . . . . .
5.25 Consequences for Neuroscience
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.26 DL with Spiking Neurons? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
DL in FNNs and RNNs for Reinforcement Learning (RL)
RL Through NN World Models Yields RNNs With Deep CAPs . . . . . . . . . . . .
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs) . . . . . . .
Deep RL RNNs for Partially Observable MDPs (POMDPs) . . . . . . . . . . . . . .
RL Facilitated by Deep UL in FNNs and RNNs . . . . . . . . . . . . . . . . . . . .
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs . . . . . .
Deep RL by Direct NN Search / Policy Gradients / Evolution . . . . . . . . . . . . .
Deep RL by Indirect Policy Search / Compressed NN Search . . . . . . . . . . . . .
Universal RL
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Conclusion and Outlook
Acknowledgments
Abbreviations in Alphabetical Order
AE: Autoencoder
AI: Artiﬁcial Intelligence
ANN: Artiﬁcial Neural Network
BFGS: Broyden-Fletcher-Goldfarb-Shanno
BNN: Biological Neural Network
BM: Boltzmann Machine
BP: Backpropagation
BRNN: Bi-directional Recurrent Neural Network
CAP: Credit Assignment Path
CEC: Constant Error Carousel
CFL: Context Free Language
CMA-ES: Covariance Matrix Estimation ES
CNN: Convolutional Neural Network
CoSyNE: Co-Synaptic Neuro-Evolution
CSL: Context Senistive Language
CTC : Connectionist Temporal Classiﬁcation
DBN: Deep Belief Network
DCT: Discrete Cosine Transform
DL: Deep Learning
DP: Dynamic Programming
DS: Direct Policy Search
EA: Evolutionary Algorithm
EM: Expectation Maximization
ES: Evolution Strategy
FMS: Flat Minimum Search
FNN: Feedforward Neural Network
FSA: Finite State Automaton
GMDH: Group Method of Data Handling
GOFAI: Good Old-Fashioned AI
GP: Genetic Programming
GPU: Graphics Processing Unit
GPU-MPCNN: GPU-Based MPCNN
HMM: Hidden Markov Model
HRL: Hierarchical Reinforcement Learning
HTM: Hierarchical Temporal Memory
HMAX: Hierarchical Model “and X”
LSTM: Long Short-Term Memory (RNN)
MDL: Minimum Description Length
MDP: Markov Decision Process
MNIST: Mixed National Institute of Standards
and Technology Database
MP: Max-Pooling
MPCNN: Max-Pooling CNN
NE: NeuroEvolution
NEAT: NE of Augmenting Topologies
NES: Natural Evolution Strategies
NFQ: Neural Fitted Q-Learning
NN: Neural Network
OCR: Optical Character Recognition
PCC: Potential Causal Connection
PDCC: Potential Direct Causal Connection
PM: Predictability Minimization
POMDP: Partially Observable MDP
RAAM: Recursive Auto-Associative Memory
RBM: Restricted Boltzmann Machine
ReLU: Rectiﬁed Linear Unit
RL: Reinforcement Learning
RNN: Recurrent Neural Network
R-prop: Resilient Backpropagation
SL: Supervised Learning
SLIM NN: Self-Delimiting Neural Network
SOTA: Self-Organising Tree Algorithm
SVM: Support Vector Machine
TDNN: Time-Delay Neural Network
TIMIT: TI/SRI/MIT Acoustic-Phonetic Continuous Speech Corpus
UL: Unsupervised Learning
WTA: Winner-Take-All
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Which modiﬁable components of a learning system are responsible for its success or failure? What
changes to them improve performance? This has been called the fundamental credit assignment problem . There are general credit assignment methods for universal problem solvers that
are time-optimal in various theoretical senses (Sec. 6.8). The present survey, however, will focus on
the narrower, but now commercially important, subﬁeld of Deep Learning (DL) in Artiﬁcial Neural
Networks (NNs).
A standard neural network (NN) consists of many simple, connected processors called neurons,
each producing a sequence of real-valued activations. Input neurons get activated through sensors perceiving the environment, other neurons get activated through weighted connections from previously
active neurons (details in Sec. 2). Some neurons may inﬂuence the environment by triggering actions.
Learning or credit assignment is about ﬁnding weights that make the NN exhibit desired behavior,
such as driving a car. Depending on the problem and how the neurons are connected, such behavior
may require long causal chains of computational stages (Sec. 3), where each stage transforms (often in a non-linear way) the aggregate activation of the network. Deep Learning is about accurately
assigning credit across many such stages.
Shallow NN-like models with few such stages have been around for many decades if not centuries
(Sec. 5.1). Models with several successive nonlinear layers of neurons date back at least to the 1960s
(Sec. 5.3) and 1970s (Sec. 5.5). An efﬁcient gradient descent method for teacher-based Supervised
Learning (SL) in discrete, differentiable networks of arbitrary depth called backpropagation (BP) was
developed in the 1960s and 1970s, and applied to NNs in 1981 (Sec. 5.5). BP-based training of deep
NNs with many layers, however, had been found to be difﬁcult in practice by the late 1980s (Sec. 5.6),
and had become an explicit research subject by the early 1990s (Sec. 5.9). DL became practically feasible to some extent through the help of Unsupervised Learning (UL), e.g., Sec. 5.10 , Sec. 5.15
 . The 1990s and 2000s also saw many improvements of purely supervised DL (Sec. 5). In the
new millennium, deep NNs have ﬁnally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines 
in numerous important applications. In fact, since 2009, supervised deep NNs have won many ofﬁcial
international pattern recognition competitions (e.g., Sec. 5.17, 5.19, 5.21, 5.22), achieving the ﬁrst
superhuman visual pattern recognition results in limited domains . Deep NNs also
have become relevant for the more general ﬁeld of Reinforcement Learning (RL) where there is no
supervising teacher (Sec. 6).
Both feedforward (acyclic) NNs (FNNs) and recurrent (cyclic) NNs (RNNs) have won contests
(Sec. 5.12, 5.14, 5.17, 5.19, 5.21, 5.22). In a sense, RNNs are the deepest of all NNs (Sec. 3)—they
are general computers more powerful than FNNs, and can in principle create and process memories
of arbitrary sequences of input patterns .
Unlike traditional methods for automatic sequential program synthesis , RNNs can learn programs that mix sequential
and parallel information processing in a natural and efﬁcient way, exploiting the massive parallelism
viewed as crucial for sustaining the rapid decline of computation cost observed over the past 75 years.
The rest of this paper is structured as follows. Sec. 2 introduces a compact, event-oriented notation
that is simple yet general enough to accommodate both FNNs and RNNs. Sec. 3 introduces the
concept of Credit Assignment Paths (CAPs) to measure whether learning in a given NN application is
of the deep or shallow type. Sec. 4 lists recurring themes of DL in SL, UL, and RL. Sec. 5 focuses
on SL and UL, and on how UL can facilitate SL, although pure SL has become dominant in recent
competitions (Sec. 5.17–5.23). Sec. 5 is arranged in a historical timeline format with subsections on
important inspirations and technical contributions. Sec. 6 on deep RL discusses traditional Dynamic
Programming (DP)-based RL combined with gradient-based search techniques for SL or UL in deep
NNs, as well as general methods for direct and indirect search in the weight space of deep FNNs and
RNNs, including successful policy gradient and evolutionary methods.
Event-Oriented Notation for Activation Spreading in NNs
Throughout this paper, let i, j, k, t, p, q, r denote positive integer variables assuming ranges implicit
in the given contexts. Let n, m, T denote positive integer constants.
An NN’s topology may change over time (e.g., Sec. 5.3, 5.6.3). At any given moment, it can
be described as a ﬁnite subset of units (or nodes or neurons) N = {u1, u2, . . . , } and a ﬁnite set
H ⊆N ×N of directed edges or connections between nodes. FNNs are acyclic graphs, RNNs cyclic.
The ﬁrst (input) layer is the set of input units, a subset of N. In FNNs, the k-th layer (k > 1) is the set
of all nodes u ∈N such that there is an edge path of length k −1 (but no longer path) between some
input unit and u. There may be shortcut connections between distant layers. In sequence-processing,
fully connected RNNs, all units have connections to all non-input units.
The NN’s behavior or program is determined by a set of real-valued, possibly modiﬁable, parameters or weights wi (i = 1, . . . , n). We now focus on a single ﬁnite episode or epoch of information
processing and activation spreading, without learning through weight changes. The following slightly
unconventional notation is designed to compactly describe what is happening during the runtime of
the system.
During an episode, there is a partially causal sequence xt(t = 1, . . . , T) of real values that I call
events. Each xt is either an input set by the environment, or the activation of a unit that may directly
depend on other xk(k < t) through a current NN topology-dependent set int of indices k representing
incoming causal connections or links. Let the function v encode topology information and map such
event index pairs (k, t) to weight indices. For example, in the non-input case we may have xt =
ft(nett) with real-valued nett = P
k∈int xkwv(k,t) (additive case) or nett = Q
k∈int xkwv(k,t)
(multiplicative case), where ft is a typically nonlinear real-valued activation function such as tanh.
In many recent competition-winning NNs (Sec. 5.19, 5.21, 5.22) there also are events of the type
xt = maxk∈int(xk); some network types may also use complex polynomial activation functions
(Sec. 5.3). xt may directly affect certain xk(k > t) through outgoing connections or links represented
through a current set outt of indices k with t ∈ink. Some of the non-input events are called output
Note that many of the xt may refer to different, time-varying activations of the same unit in
sequence-processing RNNs , or also in FNNs sequentially
exposed to time-varying input patterns of a large training set encoded as input events. During an
episode, the same weight may get reused over and over again in topology-dependent ways, e.g., in
RNNs, or in convolutional NNs (Sec. 5.4, 5.8). I call this weight sharing across space and/or time.
Weight sharing may greatly reduce the NN’s descriptive complexity, which is the number of bits of
information required to describe the NN (Sec. 4.4).
In Supervised Learning (SL), certain NN output events xt may be associated with teacher-given,
real-valued labels or targets dt yielding errors et, e.g., et = 1/2(xt−dt)2. A typical goal of supervised
NN training is to ﬁnd weights that yield episodes with small total error E, the sum of all such et. The
hope is that the NN will generalize well in later episodes, causing only small errors on previously
unseen sequences of input events. Many alternative error functions for SL and UL are possible.
SL assumes that input events are independent of earlier output events (which may affect the environment through actions causing subsequent perceptions). This assumption does not hold in the
broader ﬁelds of Sequential Decision Making and Reinforcement Learning (RL) (Sec. 6). In RL, some
of the input events may encode real-valued reward signals given by the environment, and a typical
goal is to ﬁnd weights that yield episodes with a high sum of reward signals, through sequences of
appropriate output actions.
Sec. 5.5 will use the notation above to compactly describe a central algorithm of DL, namely,
backpropagation (BP) for supervised weight-sharing FNNs and RNNs. (FNNs may be viewed as
RNNs with certain ﬁxed zero weights.) Sec. 6 will address the more general RL case.
Depth of Credit Assignment Paths (CAPs) and of Problems
To measure whether credit assignment in a given NN application is of the deep or shallow type, I
introduce the concept of Credit Assignment Paths or CAPs, which are chains of possibly causal links
between the events of Sec. 2, e.g., from input through hidden to output layers in FNNs, or through
transformations over time in RNNs.
Let us ﬁrst focus on SL. Consider two events xp and xq (1 ≤p < q ≤T). Depending on the
application, they may have a Potential Direct Causal Connection (PDCC) expressed by the Boolean
predicate pdcc(p, q), which is true if and only if p ∈inq. Then the 2-element list (p, q) is deﬁned to
be a CAP (a minimal one) from p to q. A learning algorithm may be allowed to change wv(p,q) to
improve performance in future episodes.
More general, possibly indirect, Potential Causal Connections (PCC) are expressed by the recursively deﬁned Boolean predicate pcc(p, q), which in the SL case is true only if pdcc(p, q), or if
pcc(p, k) for some k and pdcc(k, q). In the latter case, appending q to any CAP from p to k yields a
CAP from p to q (this is a recursive deﬁnition, too). The set of such CAPs may be large but is ﬁnite.
Note that the same weight may affect many different PDCCs between successive events listed by a
given CAP, e.g., in the case of RNNs, or weight-sharing FNNs.
Suppose a CAP has the form (. . . , k, t, . . . , q), where k and t (possibly t = q) are the ﬁrst successive elements with modiﬁable wv(k,t). Then the length of the sufﬁx list (t, . . . , q) is called the CAP’s
depth (which is 0 if there are no modiﬁable links at all). This depth limits how far backwards credit
assignment can move down the causal chain to ﬁnd a modiﬁable weight.1
Suppose an episode and its event sequence x1, . . . , xT satisfy a computable criterion used to
decide whether a given problem has been solved (e.g., total error E below some threshold). Then
the set of used weights is called a solution to the problem, and the depth of the deepest CAP within
the sequence is called the solution depth. There may be other solutions (yielding different event
sequences) with different depths. Given some ﬁxed NN topology, the smallest depth of any solution
is called the problem depth.
Sometimes we also speak of the depth of an architecture: SL FNNs with ﬁxed topology imply a
problem-independent maximal problem depth bounded by the number of non-input layers. Certain
SL RNNs with ﬁxed weights for all connections except those to output units have a maximal problem depth of 1, because only
the ﬁnal links in the corresponding CAPs are modiﬁable. In general, however, RNNs may learn to
solve problems of potentially unlimited depth.
Note that the deﬁnitions above are solely based on the depths of causal chains, and agnostic to the
temporal distance between events. For example, shallow FNNs perceiving large “time windows” of
input events may correctly classify long input sequences through appropriate output events, and thus
solve shallow problems involving long time lags between relevant events.
At which problem depth does Shallow Learning end, and Deep Learning begin? Discussions with
DL experts have not yet yielded a conclusive response to this question. Instead of committing myself
1An alternative would be to count only modiﬁable links when measuring depth. In many typical NN applications this would
not make a difference, but in some it would, e.g., Sec. 6.1.
to a precise answer, let me just deﬁne for the purposes of this overview: problems of depth > 10
require Very Deep Learning.
The difﬁculty of a problem may have little to do with its depth. Some NNs can quickly learn
to solve certain deep problems, e.g., through random weight guessing (Sec. 5.9) or other types of
direct search (Sec. 6.6) or indirect search (Sec. 6.7) in weight space, or through training an NN ﬁrst
on shallow problems whose solutions may then generalize to deep problems, or through collapsing
sequences of (non)linear operations into a single (non)linear operation . In general, however,
ﬁnding an NN that precisely models a given training set is an NP-complete problem , also in the case of deep NNs ; compare a survey of negative results .
Above we have focused on SL. In the more general case of RL in unknown environments, pcc(p, q)
is also true if xp is an output event and xq any later input event—any action may affect the environment
and thus any later perception. (In the real world, the environment may even inﬂuence non-input events
computed on a physical hardware entangled with the entire universe, but this is ignored here.) It is
possible to model and replace such unmodiﬁable environmental PCCs through a part of the NN that
has already learned to predict (through some of its units) input events (including reward signals) from
former input events and actions (Sec. 6.1). Its weights are frozen, but can help to assign credit to
other, still modiﬁable weights used to compute actions (Sec. 6.1). This approach may lead to very
deep CAPs though.
Some DL research is about automatically rephrasing problems such that their depth is reduced
(Sec. 4). In particular, sometimes UL is used to make SL problems less deep, e.g., Sec. 5.10. Often
Dynamic Programming (Sec. 4.1) is used to facilitate certain traditional RL problems, e.g., Sec. 6.2.
Sec. 5 focuses on CAPs for SL, Sec. 6 on the more complex case of RL.
Recurring Themes of Deep Learning
Dynamic Programming for Supervised/Reinforcement Learning (SL/RL)
One recurring theme of DL is Dynamic Programming (DP) , which can help to facilitate credit assignment under certain assumptions. For example, in SL NNs, backpropagation itself
can be viewed as a DP-derived method (Sec. 5.5). In traditional RL based on strong Markovian assumptions, DP-derived methods can help to greatly reduce problem depth (Sec. 6.2). DP algorithms
are also essential for systems that combine concepts of NNs and graphical models, such as Hidden
Markov Models (HMMs) and Expectation Maximization
(EM) , e.g., .
Unsupervised Learning (UL) Facilitating SL and RL
Another recurring theme is how UL can facilitate both SL (Sec. 5) and RL (Sec. 6). UL (Sec. 5.6.4)
is normally used to encode raw incoming data such as video or speech streams in a form that is more
convenient for subsequent goal-directed learning. In particular, codes that describe the original data in
a less redundant or more compact way can be fed into SL (Sec. 5.10, 5.15) or RL machines (Sec. 6.4),
whose search spaces may thus become smaller (and whose CAPs shallower) than those necessary for
dealing with the raw data. UL is closely connected to the topics of regularization and compression
(Sec. 4.4, 5.6.3).
Learning Hierarchical Representations Through Deep SL, UL, RL
Many methods of Good Old-Fashioned Artiﬁcial Intelligence (GOFAI) as well as
more recent approaches to AI and Machine Learning learn
hierarchies of more and more abstract data representations. For example, certain methods of syntactic pattern recognition such as grammar induction discover hierarchies of formal rules
to model observations. The partially (un)supervised Automated Mathematician / EURISKO continually learns concepts by combining previously learnt concepts.
Such hierarchical representation learning is also
a recurring theme of DL NNs for SL (Sec. 5), UL-aided SL (Sec. 5.7, 5.10, 5.15), and hierarchical RL
(Sec. 6.5). Often, abstract hierarchical representations are natural by-products of data compression
(Sec. 4.4), e.g., Sec. 5.10.
Occam’s Razor: Compression and Minimum Description Length (MDL)
Occam’s razor favors simple solutions over complex ones. Given some programming language, the
principle of Minimum Description Length (MDL) can be used to measure the complexity of a solution candidate by the length of the shortest program that computes it . Some methods
explicitly take into account program runtime ; many consider only programs with constant runtime, written in non-universal programming
languages . In the NN case, the MDL principle suggests that low NN weight complexity corresponds to high NN probability in the Bayesian
view , and to high
generalization performance , without overﬁtting the training data.
Many methods have been proposed for regularizing NNs, that is, searching for solution-computing
but simple, low-complexity SL NNs (Sec. 5.6.3) and RL NNs (Sec. 6.7). This is closely related to
certain UL methods (Sec. 4.2, 5.6.4).
Fast Graphics Processing Units (GPUs) for DL in NNs
While the previous millennium saw several attempts at creating fast NN-speciﬁc hardware , and at exploiting standard hardware , the new millennium brought a DL breakthrough in form of cheap, multiprocessor graphics cards or GPUs. GPUs are widely used for video games, a huge and competitive
market that has driven down hardware prices. GPUs excel at the fast matrix and vector multiplications
required not only for convincing virtual realities but also for NN training, where they can speed up
learning by a factor of 50 and more. Some of the GPU-based FNN implementations (Sec. 5.16–5.19)
have greatly contributed to recent successes in contests for pattern recognition (Sec. 5.19–5.22), image
segmentation (Sec. 5.21), and object detection (Sec. 5.21–5.22).
Supervised NNs, Some Helped by Unsupervised NNs
The main focus of current practical applications is on Supervised Learning (SL), which has dominated recent pattern recognition contests (Sec. 5.17–5.23). Several methods, however, use additional
Unsupervised Learning (UL) to facilitate SL (Sec. 5.7, 5.10, 5.15). It does make sense to treat SL and
UL in the same section: often gradient-based methods, such as BP (Sec. 5.5.1), are used to optimize
objective functions of both UL and SL, and the boundary between SL and UL may blur, for example,
when it comes to time series prediction and sequence classiﬁcation, e.g., Sec. 5.10, 5.12.
A historical timeline format will help to arrange subsections on important inspirations and technical contributions (although such a subsection may span a time interval of many years). Sec. 5.1 brieﬂy
mentions early, shallow NN models since the 1940s (and 1800s), Sec. 5.2 additional early neurobiological inspiration relevant for modern Deep Learning (DL). Sec. 5.3 is about GMDH networks , to my knowledge the ﬁrst (feedforward) DL systems. Sec. 5.4 is about the relatively deep
Neocognitron NN which is very similar to certain modern deep FNN architectures, as it combines convolutional NNs (CNNs), weight pattern replication, and subsampling mechanisms. Sec. 5.5
uses the notation of Sec. 2 to compactly describe a central algorithm of DL, namely, backpropagation
(BP) for supervised weight-sharing FNNs and RNNs. It also summarizes the history of BP 1960-1981
and beyond. Sec. 5.6 describes problems encountered in the late 1980s with BP for deep NNs, and
mentions several ideas from the previous millennium to overcome them. Sec. 5.7 discusses a ﬁrst hierarchical stack of coupled UL-based Autoencoders (AEs)—this concept resurfaced in the new
millennium (Sec. 5.15). Sec. 5.8 is about applying BP to CNNs , which is important for today’s
DL applications. Sec. 5.9 explains BP’s Fundamental DL Problem (of vanishing/exploding gradients)
discovered in 1991. Sec. 5.10 explains how a deep RNN stack of 1991 (the History Compressor) pretrained by UL helped to solve previously unlearnable DL benchmarks requiring Credit Assignment
Paths (CAPs, Sec. 3) of depth 1000 and more. Sec. 5.11 discusses a particular winner-take-all (WTA)
method called Max-Pooling widely used in today’s deep FNNs. Sec. 5.12 mentions a
ﬁrst important contest won by SL NNs in 1994. Sec. 5.13 describes a purely supervised DL RNN
 for problems of depth 1000 and more. Sec. 5.14 mentions
an early contest of 2003 won by an ensemble of shallow FNNs, as well as good pattern recognition
results with CNNs and deep FNNs and LSTM RNNs . Sec. 5.15 is mostly about Deep Belief
Networks and related stacks of Autoencoders (AEs, Sec. 5.7), both pre-trained by UL to
facilitate subsequent BP-based SL (compare Sec. 5.6.1, 5.10). Sec. 5.16 mentions the ﬁrst SL-based
GPU-CNNs , BP-trained MPCNNs , and LSTM stacks . Sec. 5.17–5.22 focus on
ofﬁcial competitions with secret test sets won by (mostly purely supervised) deep NNs since 2009,
in sequence recognition, image classiﬁcation, image segmentation, and object detection. Many RNN
results depended on LSTM (Sec. 5.13); many FNN results depended on GPU-based FNN code developed since 2004 (Sec. 5.16, 5.17, 5.18, 5.19), in particular, GPU-MPCNNs (Sec. 5.19). Sec. 5.24
mentions recent tricks for improving DL in NNs, many of them closely related to earlier tricks from
the previous millennium (e.g., Sec. 5.6.2, 5.6.3). Sec. 5.25 discusses how artiﬁcial NNs can help to
understand biological NNs; Sec. 5.26 addresses the possibility of DL in NNs with spiking neurons.
Early NNs Since the 1940s (and the 1800s)
Early NN architectures did not learn. The ﬁrst ideas about UL were
published a few years later . The following decades brought simple NNs trained by
SL and
UL , as well as closely related associative memories .
In a sense NNs have been around even longer, since early supervised NNs were essentially variants
of linear regression methods going back at least to the early 1800s (e.g., Legendre, 1805; Gauss, 1809,
1821); Gauss also refers to his work of 1795. Early NNs had a maximal CAP depth of 1 (Sec. 3).
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11)
Simple cells and complex cells were found in the cat’s visual cortex . These cells ﬁre in response to certain properties of visual sensory inputs,
such as the orientation of edges. Complex cells exhibit more spatial invariance than simple cells. This
inspired later deep NN architectures (Sec. 5.4, 5.11) used in certain modern award-winning Deep
Learners (Sec. 5.19–5.22).
1965: Deep Networks Based on the Group Method of Data Handling
Networks trained by the Group Method of Data Handling (GMDH) were perhaps the ﬁrst DL systems of the Feedforward Multilayer Perceptron type, although there was earlier work on NNs with a single hidden
layer . The units of GMDH nets may have polynomial activation
functions implementing Kolmogorov-Gabor polynomials (more general than other widely used NN
activation functions, Sec. 2). Given a training set, layers are incrementally grown and trained by regression analysis (e.g., Legendre, 1805; Gauss, 1809, 1821) (Sec. 5.1), then pruned with the help of
a separate validation set (using today’s terminology), where Decision Regularisation is used to weed
out superﬂuous units (compare Sec. 5.6.3). The numbers of layers and units per layer can be learned
in problem-dependent fashion. To my knowledge, this was the ﬁrst example of open-ended, hierarchical representation learning in NNs (Sec. 4.3). A paper of 1971 already described a deep GMDH
network with 8 layers . There have been numerous applications of GMDH-style
nets, e.g. .
1979: Convolution + Weight Replication + Subsampling (Neocognitron)
Apart from deep GMDH networks (Sec. 5.3), the Neocognitron 
was perhaps the ﬁrst artiﬁcial NN that deserved the attribute deep, and the ﬁrst to incorporate the
neurophysiological insights of Sec. 5.2. It introduced convolutional NNs (today often called CNNs or
convnets), where the (typically rectangular) receptive ﬁeld of a convolutional unit with given weight
vector (a ﬁlter) is shifted step by step across a 2-dimensional array of input values, such as the pixels
of an image (usually there are several such ﬁlters). The resulting 2D array of subsequent activation
events of this unit can then provide inputs to higher-level units, and so on. Due to massive weight
replication (Sec. 2), relatively few parameters (Sec. 4.4) may be necessary to describe the behavior of
such a convolutional layer.
Subsampling or downsampling layers consist of units whose ﬁxed-weight connections originate
from physical neighbours in the convolutional layers below. Subsampling units become active if at
least one of their inputs is active; their responses are insensitive to certain small image shifts (compare
Sec. 5.2).
The Neocognitron is very similar to the architecture of modern, contest-winning, purely supervised, feedforward, gradient-based Deep Learners with alternating convolutional and downsampling
layers (e.g., Sec. 5.19–5.22). Fukushima, however, did not set the weights by supervised backpropagation (Sec. 5.5, 5.8), but by local, WTA-based unsupervised learning rules ,
or by pre-wiring. In that sense he did not care for the DL problem (Sec. 5.9), although his architecture
was comparatively deep indeed. For downsampling purposes he used Spatial Averaging instead of Max-Pooling (MP, Sec. 5.11), currently a particularly convenient and popular
WTA mechanism. Today’s DL combinations of CNNs and MP and BP also proﬁt a lot from later
work (e.g., Sec. 5.8, 5.16, 5.16, 5.19).
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs
The minimisation of errors through gradient descent in the parameter space of
complex, nonlinear, differentiable (Leibniz, 1684), multi-stage, NN-related systems has been discussed at least since the early 1960s , initially within the framework of Euler-LaGrange equations in the Calculus of
Variations (e.g., Euler, 1744).
Steepest descent in the weight space of such systems can be performed by iterating the chain rule (Leibniz, 1676; L’Hˆopital, 1696) `a la Dynamic
Programming (DP) . A simpliﬁed derivation of this backpropagation method uses the
chain rule only .
The systems of the 1960s were already efﬁcient in the DP sense. However, they backpropagated
derivative information through standard Jacobian matrix calculations from one “layer” to the previous
one, without explicitly addressing either direct links across several layers or potential additional efﬁciency gains due to network sparsity (but perhaps such enhancements seemed obvious to the authors).
Given all the prior work on learning in multilayer NN-like systems , it seems surprising in hindsight that a book on the
limitations of simple linear perceptrons with a single layer (Sec. 5.1) discouraged some researchers
from further studying NNs.
Explicit, efﬁcient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected,
NN-like networks apparently was ﬁrst described in a 1970 master’s thesis ,
albeit without reference to NNs. BP is also known as the reverse mode of automatic differentiation , where the costs of forward activation spreading essentially equal the costs of
backward derivative calculation. See early FORTRAN code and closely related
work .
Efﬁcient BP was soon explicitly used to minimize cost functions by adapting control parameters
(weights) . Compare some preliminary, NN-speciﬁc discussion , a method for multilayer threshold NNs , and a computer program for
automatically deriving and implementing BP for given differentiable systems .
To my knowledge, the ﬁrst NN-speciﬁc application of efﬁcient BP as above was described in
1981 . Related work was published several years later . A paper of 1986 signiﬁcantly contributed to the popularisation of BP for NNs , experimentally demonstrating the emergence of useful internal representations in hidden
layers. See generalisations for sequence-processing recurrent NNs , also for equilibrium RNNs with stationary inputs.
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Using the notation of Sec. 2 for weight-sharing FNNs or RNNs, after an episode of activation spreading through differentiable ft, a single iteration of gradient descent through BP computes changes of
all wi in proportion to ∂E
∂wi as in Algorithm 5.5.1 (for the additive case), where each
weight wi is associated with a real-valued variable △i initialized by 0.
The computational costs of the backward (BP) pass are essentially those of the forward pass
(Sec. 2). Forward and backward passes are re-iterated until sufﬁcient performance is reached.
Alg. 5.5.1: One iteration of BP for weight-sharing FNNs or RNNs
for t = T, . . . , 1 do
to compute
∂nett , inititalize real-valued error signal variable δt by 0;
if xt is an input event then continue with next iteration;
if there is an error et then δt := xt −dt;
add to δt the value P
k∈outt wv(t,k)δk; (this is the elegant and efﬁcient recursive chain rule
application collecting impacts of nett on future events)
multiply δt by f ′
for all k ∈int add to △wv(k,t) the value xkδt
change each wi in proportion to △i and a small real-valued learning rate
As of 2014, this simple BP method is still the central learning algorithm for FNNs and RNNs. Notably, most contest-winning NNs up to 2014 (Sec. 5.12, 5.14, 5.17, 5.19, 5.21, 5.22) did not augment
supervised BP by some sort of unsupervised learning as discussed in Sec. 5.7, 5.10, 5.15.
Late 1980s-2000 and Beyond: Numerous Improvements of NNs
By the late 1980s it seemed clear that BP by itself (Sec. 5.5) was no panacea. Most FNN applications
focused on FNNs with few hidden layers. Additional hidden layers often did not seem to offer empirical beneﬁts. Many practitioners found solace in a theorem stating that an NN with a single layer of enough hidden units can approximate
any multivariate continous function with arbitrary accuracy.
Likewise, most RNN applications did not require backpropagating errors far. Many researchers
helped their RNNs by ﬁrst training them on shallow problems (Sec. 3) whose solutions then generalized to deeper problems. In fact, some popular RNN algorithms restricted credit assignment to a
single step backwards , also in more recent studies .
Generally speaking, although BP allows for deep problems in principle, it seemed to work only
for shallow problems. The late 1980s and early 1990s saw a few ideas with a potential to overcome
this problem, which was fully understood only in 1991 (Sec. 5.9).
Ideas for Dealing with Long Time Lags and Deep CAPs
To deal with long time lags between relevant events, several sequence processing methods were proposed, including Focused BP based on decay factors for activations of units in RNNs , Time-Delay Neural Networks (TDNNs) and their adaptive extension , Nonlinear AutoRegressive with eXogenous inputs (NARX) RNNs , certain hierarchical RNNs , RL
economies in RNNs with WTA units and local learning rules , and other methods . However, these algorithms either worked for shallow CAPs only, could not generalize
to unseen CAP depths, had problems with greatly varying time lags between relevant events, needed
external ﬁne tuning of delay constants, or suffered from other problems. In fact, it turned out that
certain simple but deep benchmark problems used to evaluate such methods are more quickly solved
by randomly guessing RNN weights until a solution is found .
While the RNN methods above were designed for DL of temporal sequences, the Neural Heat
Exchanger consists of two parallel deep FNNs with opposite ﬂow directions.
Input patterns enter the ﬁrst FNN and are propagated “up”. Desired outputs (targets) enter the “opposite” FNN and are propagated “down”. Using a local learning rule, each layer in each net tries to be
similar (in information content) to the preceding layer and to the adjacent layer of the other net. The
input entering the ﬁrst net slowly “heats up” to become the target. The target entering the opposite net
slowly “cools down” to become the input. The Helmholtz Machine may be viewed as an unsupervised (Sec. 5.6.4) variant thereof .
A hybrid approach initializes a potentially deep FNN through a domain theory in propositional logic, which may be acquired through
explanation-based learning .
The NN is then ﬁne-tuned through BP (Sec. 5.5). The NN’s depth reﬂects the longest chain of
reasoning in the original set of logical rules. An extension of this approach initializes an RNN by domain knowledge expressed as a Finite State Automaton (FSA). BP-based ﬁne-tuning has become important for later DL systems pre-trained by UL, e.g.,
Sec. 5.10, 5.15.
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Numerous improvements of steepest descent through BP (Sec. 5.5) have been proposed.
Leastsquares methods (Gauss-Newton, Levenberg-Marquardt) and quasi-Newton methods (Broyden-Fletcher-
Goldfarb-Shanno, BFGS) are computationally too expensive for large NNs.
Partial BFGS and conjugate gradient as well as other
methods provide sometimes useful fast alternatives. BP can be treated as a linear least-squares problem ,
where second-order gradient information is passed back to preceding layers.
To speed up BP, momentum was introduced , ad-hoc constants were added
to the slope of the linearized activation function , or the nonlinearity of the slope was
exaggerated .
Only the signs of the error derivatives are taken into account by the successful and widely used
BP variant R-prop and the robust variation iRprop+ , which was also successfully applied to RNNs.
The local gradient can be normalized based on the NN architecture , through a diagonalized Hessian approach , or related efﬁcient methods .
Some algorithms for controlling BP step size adapt a global learning rate , while others compute individual learning rates for each weight . In online learning,
where BP is applied after each pattern presentation, the vario-η algorithm sets each weight’s learning rate inversely proportional to the empirical standard deviation of its local gradient, thus normalizing the stochastic weight ﬂuctuations. Compare a local online
step size adaptation method for nonlinear NNs .
Many additional tricks for improving NNs have been described . Compare Sec. 5.6.3 and recent developments mentioned in Sec. 5.24.
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24)
Many researchers used BP-like methods to search for “simple,” low-complexity NNs (Sec. 4.4)
with high generalization capability. Most approaches address the bias/variance dilemma through strong prior assumptions. For example, weight decay encourages near-zero weights, by penalizing large
weights. In a Bayesian framework (Bayes, 1763), weight decay can be derived from Gaussian or Laplacian weight priors (Gauss, 1809; Laplace, 1774); see also . An extension of this approach postulates that a distribution of networks with
many similar weights generated by Gaussian mixtures is “better” a priori .
Often weight priors are implicit in additional penalty terms or in methods based
on validation sets , Akaike’s information criterion and ﬁnal prediction error , or generalized prediction error . See also . Similar priors (or biases towards simplicity)
are implicit in constructive and pruning algorithms, e.g., layer-by-layer sequential network construction (see also Sec. 5.3, 5.11), input pruning , unit pruning , weight pruning, e.g., optimal brain damage , and
optimal brain surgeon .
A very general but not always practical approach for discovering low-complexity SL NNs or
RL NNs searches among weight matrix-computing programs written in a universal programming
language, with a bias towards fast and short programs (Sec. 6.7).
Flat Minimum Search (FMS) searches for a “ﬂat”
minimum of the error function: a large connected region in weight space where error is low and remains approximately constant, that is, few bits of information are required to describe low-precision
weights with high variance. Compare perturbation tolerance conditions . An MDL-based, Bayesian argument suggests that ﬂat
minima correspond to “simple” NNs and low expected overﬁtting. Compare Sec. 5.6.4 and more
recent developments mentioned in Sec. 5.24.
Potential Beneﬁts of UL for SL (Compare Sec. 5.7, 5.10, 5.15)
The notation of Sec. 2 introduced teacher-given labels dt. Many papers of the previous millennium,
however, were about unsupervised learning (UL) without a teacher ; see also post-2000
work .
Many UL methods are designed to maximize entropy-related, information-theoretic objectives .
Many do this to uncover and disentangle hidden underlying sources of signals .
Many UL methods automatically and robustly generate distributed, sparse representations of input patterns through well-known feature detectors , such as off-center-onsurround-like structures, as well as orientation sensitive edge detectors and Gabor ﬁlters . They extract simple features related to those observed in early visual pre-processing stages
of biological systems .
UL can also serve to extract invariant features from different data items 
through coupled NNs observing two different inputs , also called
Siamese NNs of the ensemble , to disentangle the
unknown factors of variation . Such codes may be sparse and can be
advantageous for (1) data compression, (2) speeding up subsequent BP , (3) trivialising
the task of subsequent naive yet optimal Bayes classiﬁers .
Most early UL FNNs had a single layer. Methods for deeper UL FNNs include hierarchical
(Sec. 4.3) self-organizing Kohonen maps , hierarchical Gaussian
potential function networks , layer-wise UL of feature hierarchies fed into SL
classiﬁers , the Self-Organising Tree Algorithm (SOTA) ,
and nonlinear Autoencoders (AEs) with more than 3 (e.g., 5) layers . Such AE NNs can be trained to map input patterns
to themselves, for example, by compactly encoding them through activations of units of a narrow
bottleneck hidden layer. Certain nonlinear AEs suffer from certain limitations .
LOCOCODE uses FMS (Sec. 5.6.3) to ﬁnd low-complexity
AEs with low-precision weights describable by few bits of information, often producing sparse or
factorial codes. Predictability Minimization (PM) searches for factorial codes
through nonlinear feature detectors that ﬁght nonlinear predictors, trying to become both as informative and as unpredictable as possible. PM-based UL was applied not only to FNNs but also to
RNNs . Compare Sec. 5.10 on UL-based RNN stacks
 , as well as later UL RNNs .
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15)
Perhaps the ﬁrst work to study potential beneﬁts of UL-based pre-training was published in 1987. It
proposed unsupervised AE hierarchies , closely related to certain post-2000 feedforward Deep Learners based on UL (Sec. 5.15). The lowest-level AE NN with a single hidden layer is
trained to map input patterns to themselves. Its hidden layer codes are then fed into a higher-level AE
of the same type, and so on. The hope is that the codes in the hidden AE layers have properties that
facilitate subsequent learning. In one experiment, a particular AE-speciﬁc learning algorithm (different from traditional BP of Sec. 5.5.1) was used to learn a mapping in an AE stack pre-trained by
this type of UL . This was faster than learning an equivalent mapping by BP through
a single deeper AE without pre-training. On the other hand, the task did not really require a deep
AE, that is, the beneﬁts of UL were not that obvious from this experiment. Compare an early survey and the somewhat related Recursive Auto-Associative Memory (RAAM) , originally used to encode sequential linguistic structures of arbitrary
size through a ﬁxed number of hidden units. More recently, RAAMs were also used as unsupervised
pre-processors to facilitate deep credit assignment for RL (Sec. 6.4).
In principle, many UL methods (Sec. 5.6.4) could be stacked like the AEs above, the historycompressing RNNs of Sec. 5.10, the Restricted Boltzmann Machines (RBMs) of Sec. 5.15, or hierarchical Kohonen nets (Sec. 5.6.4), to facilitate subsequent SL. Compare Stacked Generalization , and FNNs that proﬁt from pre-training by competitive
UL prior to BP-based ﬁne-tuning . See
also more recent methods using UL to improve subsequent SL .
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
In 1989, backpropagation (Sec. 5.5) was applied to Neocognitronlike, weight-sharing, convolutional neural layers (Sec. 5.4) with adaptive connections. This combination, augmented by Max-Pooling (MP, Sec. 5.11, 5.16), and sped up on graphics cards (Sec. 5.19),
has become an essential ingredient of many modern, competition-winning, feedforward, visual Deep
Learners (Sec. 5.19–5.23). This work also introduced the MNIST data set of handwritten digits , which over time has become perhaps the most famous benchmark of Machine
Learning. CNNs helped to achieve good performance on MNIST (CAP depth
5) and on ﬁngerprint recognition ; similar CNNs were used commercially
in the 1990s.
1991: Fundamental Deep Learning Problem of Gradient Descent
A diploma thesis represented a milestone of explicit DL research. As mentioned
in Sec. 5.6, by the late 1980s, experiments had indicated that traditional deep feedforward or recurrent networks are hard to train by backpropagation (BP) (Sec. 5.5). Hochreiter’s work formally
identiﬁed a major reason: Typical deep NNs suffer from the now famous problem of vanishing or
exploding gradients. With standard activation functions (Sec. 1), cumulative backpropagated error
signals (Sec. 5.5.1) either shrink rapidly, or grow out of bounds. In fact, they decay exponentially in
the number of layers or CAP depth (Sec. 3), or they explode. This is also known as the long time
lag problem. Much subsequent DL research of the 1990s and 2000s was motivated by this insight.
Later work also studied basins of attraction and their stability under noise from a
dynamical systems point of view: either the dynamics are not robust to noise, or the gradients vanish.
See also . Over the years, several ways of partially
overcoming the Fundamental Deep Learning Problem were explored:
I A Very Deep Learner of 1991 (the History Compressor, Sec. 5.10) alleviates the problem
through unsupervised pre-training for a hierarchy of RNNs. This greatly facilitates subsequent
supervised credit assignment through BP (Sec. 5.5). In the FNN case, similar effects can be
achieved through conceptually related AE stacks (Sec. 5.7, 5.15) and Deep Belief Networks
(DBNs, Sec. 5.15).
II LSTM-like networks (Sec. 5.13, 5.16, 5.17, 5.21–5.23) alleviate the problem through a special
architecture unaffected by it.
III Today’s GPU-based computers have a million times the computational power of desktop machines of the early 1990s. This allows for propagating errors a few layers further down within
reasonable time, even in traditional NNs (Sec. 5.18). That is basically what is winning many of
the image recognition competitions now (Sec. 5.19, 5.21, 5.22). (Although this does not really
overcome the problem in a fundamental way.)
IV Hessian-free optimization (Sec. 5.6.2) can alleviate the problem for FNNs (Sec. 5.6.2) and RNNs (Sec. 5.20).
V The space of NN weight matrices can also be searched without relying on error gradients,
thus avoiding the Fundamental Deep Learning Problem altogether. Random weight guessing
sometimes works better than more sophisticated methods .
Certain more complex problems are better solved by using Universal Search 
for weight matrix-computing programs written in a universal programming language . Some are better solved by using linear methods to obtain optimal weights for
connections to output events (Sec. 2), and evolving weights of connections to other events—
this is called Evolino . Compare also related RNNs pre-trained by
certain UL rules , also in the case of spiking neurons (Sec. 5.26). Direct search methods are relevant not only for SL but also for more
general RL, and are discussed in more detail in Sec. 6.6.
1991: UL-Based History Compression Through a Deep Stack of RNNs
A working Very Deep Learner (Sec. 3) of 1991 could perform credit assignment across hundreds of nonlinear operators or neural layers, by using unsupervised pre-training
for a hierarchy of RNNs.
The basic idea is still relevant today. Each RNN is trained for a while in unsupervised fashion to
predict its next input . From then on, only unexpected inputs
(errors) convey new information and get fed to the next higher RNN which thus ticks on a slower, selforganising time scale. It can easily be shown that no information gets lost. It just gets compressed
(much of machine learning is essentially about compression, e.g., Sec. 4.4, 5.6.3, 6.7). For each
individual input sequence, we get a series of less and less redundant encodings in deeper and deeper
levels of this History Compressor or Neural Sequence Chunker, which can compress data in both
space (like feedforward NNs) and time. This is another good example of hierarchical representation
learning (Sec. 4.3). There also is a continuous variant of the history compressor —as
long as there is remaining local learnable predictability in the data representation on the corresponding
level of the hierarchy. Compare a similar observation for feedforward Deep Belief Networks .
The system was able to learn many previously unlearnable DL tasks. One ancient illustrative
DL experiment required CAPs (Sec. 3) of depth 1200. The top level code of
the initially unsupervised RNN stack, however, got so compact that (previously infeasible) sequence
classiﬁcation through additional BP-based SL became possible. Essentially the system used UL to
greatly reduce problem depth. Compare earlier BP-based ﬁne-tuning of NNs initialized by rules of
propositional logic (Sec. 5.6.1).
There is a way of compressing higher levels down into lower levels, thus fully or partially collapsing the RNN stack. The trick is to retrain a lower-level RNN to continually imitate (predict) the
hidden units of an already trained, slower, higher-level RNN (the “conscious” chunker), through additional predictive output neurons . This helps the lower RNN (the automatizer)
to develop appropriate, rarely changing memories that may bridge very long time lags. Again, this
procedure can greatly reduce the required depth of the BP process.
The 1991 system was a working Deep Learner in the modern post-2000 sense, and also a ﬁrst
Neural Hierarchical Temporal Memory (HTM). It is conceptually similar to earlier AE hierarchies
 and later Deep Belief Networks , but more general in the sense
that it uses sequence-processing RNNs instead of FNNs with unchanging inputs. More recently,
well-known entrepreneurs also got interested in HTMs;
compare also hierarchical HMMs , as well as later UL-based recurrent systems .
Clockwork RNNs also consist of interacting RNN modules with different clock
rates, but do not use UL to set those rates. Stacks of RNNs were used in later work on SL with great
success, e.g., Sec. 5.13, 5.16, 5.17, 5.22.
1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19)
The Neocognitron (Sec. 5.4) inspired the Cresceptron , which adapts its topology during training (Sec. 5.6.3); compare the incrementally growing and shrinking GMDH networks
 .
Instead of using alternative local subsampling or WTA methods , the Cresceptron uses Max-Pooling (MP) layers. Here
a 2-dimensional layer or array of unit activations is partitioned into smaller rectangular arrays. Each
is replaced in a downsampling layer by the activation of its maximally active unit. A later, more complex version of the Cresceptron also included “blurring” layers to improve object
location tolerance.
The neurophysiologically plausible topology of the feedforward HMAX model is very similar to the one of the 1992 Cresceptron .
HMAX does not learn though. Its units have hand-crafted weights; biologically plausible learning
rules were later proposed for similar models .
When CNNs or convnets (Sec. 5.4, 5.8) are combined with MP, they become Cresceptron-like
or HMAX-like MPCNNs with alternating convolutional and max-pooling layers. Unlike Cresceptron
and HMAX, however, MPCNNs are trained by BP (Sec. 5.5, 5.16) . Advantages
of doing this were pointed out subsequently . BP-trained MPCNNs have become
central to many modern, competition-winning, feedforward, visual Deep Learners . No very deep
CAPs (Sec. 3) were needed though.
1995: Supervised Recurrent Very Deep Learner (LSTM RNN)
Supervised Long Short-Term Memory (LSTM) RNN could eventually perform similar feats as the deep RNN hierarchy
of 1991 (Sec. 5.10), overcoming the Fundamental Deep Learning Problem (Sec. 5.9) without any
unsupervised pre-training. LSTM could also learn DL tasks without local sequence predictability
 , dealing
with very deep problems (Sec. 3) .
The basic LSTM idea is very simple. Some of the units are called Constant Error Carousels
(CECs). Each CEC uses as an activation function f, the identity function, and has a connection to itself
with ﬁxed weight of 1.0. Due to f’s constant derivative of 1.0, errors backpropagated through a CEC
cannot vanish or explode (Sec. 5.9) but stay as they are (unless they “ﬂow out” of the CEC to other,
typically adaptive parts of the NN). CECs are connected to several nonlinear adaptive units (some
with multiplicative activation functions) needed for learning nonlinear behavior. Weight changes of
these units often proﬁt from error signals propagated far back in time through CECs. CECs are
the main reason why LSTM nets can learn to discover the importance of (and memorize) events that
happened thousands of discrete time steps ago, while previous RNNs already failed in case of minimal
time lags of 10 steps.
Many different LSTM variants and topologies are allowed. It is possible to evolve good problemspeciﬁc topologies . Some LSTM variants also use modiﬁable self-connections of
CECs .
To a certain extent, LSTM is biologically plausible . LSTM learned to solve
many previously unlearnable DL tasks involving: Recognition of the temporal order of widely separated events in noisy input streams; Robust storage of high-precision real numbers across extended
time intervals; Arithmetic operations on continuous input streams; Extraction of information conveyed by the temporal distance between events; Recognition of temporally extended patterns in noisy
input sequences ; Stable generation of precisely timed rhythms, as well as smooth and non-smooth periodic trajectories . LSTM clearly outperformed previous RNNs on tasks that require learning the rules of regular languages describable by deterministic Finite State Automata (FSAs) , both in terms of
reliability and speed.
LSTM also worked on tasks involving context free languages (CFLs) that cannot be represented
by HMMs or similar FSAs discussed in the RNN literature . CFL recognition requires the functional equivalent of a runtime stack. Some previous RNNs failed to learn small CFL training sets .
Those that did not failed to extract the general rules,
and did not generalize well on substantially larger test sets. Similar for context-sensitive languages
(CSLs) . LSTM generalized well though, requiring only the 30 shortest
exemplars (n ≤10) of the CSL anbncn to correctly predict the possible continuations of sequence
preﬁxes for n up to 1000 and more. A combination of a decoupled extended Kalman ﬁlter and an LSTM RNN learned to deal correctly with values
of n up to 10 million and more. That is, after training the network was able to read sequences of
30,000,000 symbols and more, one symbol at a time, and ﬁnally detect the subtle differences between legal strings such as a10,000,000b10,000,000c10,000,000 and very similar but illegal strings such
as a10,000,000b9,999,999c10,000,000. Compare also more recent RNN algorithms able to deal with long
time lags are designed for input sequences whose starts and ends are known in advance, such as spoken sentences to be labeled by
their phonemes; compare . To take both past and future context of each sequence
element into account, one RNN processes the sequence from start to end, the other backwards from
end to start. At each time step their combined outputs predict the corresponding label (if there is
any). BRNNs were successfully applied to secondary protein structure prediction .
DAG-RNNs generalize BRNNs to multiple dimensions. They learned to predict properties of small organic molecules as well as
protein contact maps , also in conjunction with a growing deep FNN (Sec. 5.21). BRNNs and DAG-RNNs unfold their full potential when combined with the
LSTM concept .
Particularly successful in recent competitions are stacks (Sec. 5.10) of LSTM RNNs trained by Connectionist Temporal Classiﬁcation (CTC) , a gradient-based method for ﬁnding RNN weights that maximize the probability of teacher-given label sequences, given (typically much longer and more highdimensional) streams of real-valued input vectors. CTC-LSTM performs simultaneous segmentation
(alignment) and recognition (Sec. 5.22).
In the early 2000s, speech recognition was dominated by HMMs combined with FNNs . Nevertheless, when trained from scratch on utterances from the TIDIG-
ITS speech database, in 2003 LSTM already obtained results comparable to those of HMM-based
systems . In 2007, LSTM outperformed
HMMs in keyword spotting tasks ; compare recent improvements . By 2013, LSTM also achieved best known results on the famous
TIMIT phoneme recognition benchmark (Sec. 5.22). Recently, LSTM RNN /
HMM hybrids obtained best known performance on medium-vocabulary and
large-vocabulary speech recognition .
LSTM is also applicable to robot localization , robot control , online driver distraction detection , and many other tasks. For example,
it helped to improve the state of the art in diverse applications such as protein analysis , handwriting recognition , voice activity detection , optical character recognition , language identiﬁcation , prosody contour
prediction , audio onset detection , text-to-speech synthesis , social signal classiﬁcation , machine translation , and others.
RNNs can also be used for metalearning , because they can in principle learn to run their own weight change algorithm . A successful metalearner used an LSTM
RNN to quickly learn a learning algorithm for quadratic functions (compare Sec. 6.8).
Recently, LSTM RNNs won several international pattern recognition competitions and set numerous benchmark records on large and complex data sets, e.g., Sec. 5.17, 5.21, 5.22. Gradientbased LSTM is no panacea though—other methods sometimes outperformed it at least on certain
tasks ; compare Sec. 5.20.
2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs
In the decade around 2000, many practical and commercial pattern recognition applications were
dominated by non-neural machine learning methods such as Support Vector Machines (SVMs) . Nevertheless, at least in certain domains, NNs outperformed other
techniques.
A Bayes NN based on an ensemble of NNs won the NIPS 2003 Feature
Selection Challenge with secret test set . The NN was not very deep though—
it had two hidden layers and thus rather shallow CAPs (Sec. 3) of depth 3.
Important for many present competition-winning pattern recognisers (Sec. 5.19, 5.21, 5.22) were
developments in the CNN department. A BP-trained CNN (Sec. 5.4, Sec. 5.8) set
a new MNIST record of 0.4% , using training pattern deformations 
but no unsupervised pre-training (Sec. 5.7, 5.10, 5.15). A standard BP net achieved 0.7% . Again, the corresponding CAP depth was low. Compare further improvements in
Sec. 5.16, 5.18, 5.19.
Good image interpretation results were achieved with rather deep NNs trained
by the BP variant R-prop (Sec. 5.6.2); here feedback through recurrent
connections helped to improve image interpretation. FNNs with CAP depth up to 6 were used to
successfully classify high-dimensional data .
Deep LSTM RNNs started to obtain certain ﬁrst speech recognition results comparable to those
of HMM-based systems ; compare Sec. 5.13, 5.16, 5.21, 5.22.
2006/7: UL For Deep Belief Networks / AE Stacks Fine-Tuned by BP
While learning networks with numerous non-linear layers date back at least to 1965 (Sec. 5.3), and explicit DL research results have been published at least since 1991 (Sec. 5.9, 5.10), the expression Deep
Learning was actually coined around 2006, when unsupervised pre-training of deep FNNs helped to
accelerate subsequent SL through BP . Compare
earlier terminology on loading deep networks and learning deep memories . Compare also BP-based (Sec. 5.5) ﬁne-tuning (Sec. 5.6.1) of
(not so deep) FNNs pre-trained by competitive UL .
The Deep Belief Network (DBN) is a stack of Restricted Boltzmann Machines (RBMs) , which in turn are Boltzmann Machines (BMs) with a single
layer of feature-detecting units; compare also Higher-Order BMs .
Each RBM perceives pattern representations from the level below and learns to encode them in unsupervised fashion. At least in theory under certain assumptions, adding more layers improves a
bound on the data’s negative log probability (equivalent to the data’s description
length—compare the corresponding observation for RNN stacks, Sec. 5.10). There are extensions for
Temporal RBMs .
Without any training pattern deformations (Sec. 5.14), a DBN ﬁne-tuned by BP achieved 1.2%
error rate on the MNIST handwritten digits (Sec. 5.8, 5.14). This
result helped to arouse interest in DBNs. DBNs also achieved good results on phoneme recognition,
with an error rate of 26.7% on the TIMIT core test set ; compare further
improvements through FNNs and LSTM RNNs (Sec. 5.22).
A DBN-based technique called Semantic Hashing maps semantically similar documents (of variable size) to nearby addresses in a space of document representations. It outperformed previous searchers for similar documents, such as Locality Sensitive
Hashing . See the RBM/DBN tutorial .
Autoencoder (AE) stacks (Sec. 5.7) became a popular alternative way of pretraining deep FNNs in unsupervised fashion, before ﬁne-tuning (Sec. 5.6.1) them through BP
(Sec. 5.5) . Sparse coding (Sec. 5.6.4)
was formulated as a combination of convex optimization problems . Recent surveys
of stacked RBM and AE methods focus on post-2006 developments .
Unsupervised DBNs and AE stacks are conceptually similar to, but in a certain sense less general
than, the unsupervised RNN stack-based History Compressor of 1991 (Sec. 5.10), which can process
and re-encode not only stationary input patterns, but entire pattern sequences.
2006/7: Improved CNNs / GPU-CNNs / BP for MPCNNs / LSTM Stacks
Also in 2006, a BP-trained CNN (Sec. 5.4, Sec. 5.8) set a new MNIST record
of 0.39% , using training pattern deformations (Sec. 5.14) but no unsupervised
pre-training. Compare further improvements in Sec. 5.18, 5.19. Similar CNNs were used for offroad obstacle avoidance . A combination of CNNs and TDNNs later learned to
map ﬁxed-size representations of variable-size sentences to features relevant for language processing,
using a combination of SL and UL .
2006 also saw an early GPU-based CNN implementation up to 4 times
faster than CPU-CNNs; compare also earlier GPU implementations of standard FNNs with a reported
speed-up factor of 20 . GPUs or graphics cards have become more and more
important for DL in subsequent years (Sec. 5.18–5.22).
In 2007, BP (Sec. 5.5) was applied for the ﬁrst time to Neocognitroninspired (Sec. 5.4), Cresceptron-like (or HMAX-like) MPCNNs (Sec. 5.11) with alternating convolutional and max-pooling layers. BP-trained MPCNNs have become an essential ingredient of many
modern, competition-winning, feedforward, visual Deep Learners (Sec. 5.17, 5.19–5.23).
Also in 2007, hierarchical stacks of LSTM RNNs were introduced . They
can be trained by hierarchical Connectionist Temporal Classiﬁcation (CTC) . For
tasks of sequence labelling, every LSTM RNN level (Sec. 5.13) predicts a sequence of labels fed to
the next level. Error signals at every level are back-propagated through all the lower levels. On spoken
digit recognition, LSTM stacks outperformed HMMs, despite making fewer assumptions about the
domain. LSTM stacks do not necessarily require unsupervised pre-training like the earlier UL-based
RNN stacks of Sec. 5.10.
2009: First Ofﬁcial Competitions Won by RNNs, and with MPCNNs
Stacks of LSTM RNNs trained by CTC (Sec. 5.13, 5.16) became the ﬁrst RNNs to win ofﬁcial international pattern recognition contests (with secret test sets known only to the organisers). More precisely,
three connected handwriting competitions at ICDAR 2009 in three different languages (French, Arab,
Farsi) were won by deep LSTM RNNs without any a priori linguistic knowledge, performing simultaneous segmentation and recognition. Compare (Sec. 5.22).
To detect human actions in surveillance videos, a 3-dimensional CNN , combined with SVMs, was part of a larger system using a bag
of features approach to extract regions of interest. The system won three 2009
TRECVID competitions. These were possibly the ﬁrst ofﬁcial international contests won with the
help of (MP)CNNs (Sec. 5.16). An improved version of the method was published later orders of magnitudes faster than
previous CPU-DBNs (see Sec. 5.15); see also . The Convolutional DBN (with a probabilistic variant of MP, Sec. 5.11) combines ideas from CNNs and DBNs,
and was successfully applied to audio classiﬁcation .
2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record
In 2010, a new MNIST (Sec. 5.8) record of 0.35% error rate was set by good old BP (Sec. 5.5)
in deep but otherwise standard NNs , using neither unsupervised pre-training
(e.g., Sec. 5.7, 5.10, 5.15) nor convolution (e.g., Sec. 5.4, 5.8, 5.14, 5.16). However, training pattern
deformations (e.g., Sec. 5.14) were important to generate a big training set and avoid overﬁtting. This
success was made possible mainly through a GPU implementation of BP that was up to 50 times
faster than standard CPU versions. A good value of 0.95% was obtained without distortions except
for small saccadic eye movement-like translations—compare Sec. 5.15.
Since BP was 3-5 decades old by then (Sec. 5.5), and pattern deformations 2 decades 
(Sec. 5.14), these results seemed to suggest that advances in exploiting modern computing hardware
were more important than advances in algorithms.
2011: MPCNNs on GPU Achieve Superhuman Vision Performance
In 2011, a ﬂexible GPU-implementation of Max-Pooling (MP) CNNs or Convnets was described (a GPU-MPCNN), building on earlier MP work (Sec. 5.11)
CNNs (Sec. 5.4, 5.8, 5.16), and on early GPU-based CNNs
without MP (Sec. 5.16); compare early GPU-NNs and
GPU-DBNs (Sec. 5.17). MPCNNs have alternating convolutional layers (Sec. 5.4)
and max-pooling layers (MP, Sec. 5.11) topped by standard fully connected layers. All weights are
trained by BP (Sec. 5.5, 5.8, 5.16) . GPU-MPCNNs have
become essential for many contest-winning FNNs (Sec. 5.21, Sec. 5.22).
Multi-Column GPU-MPCNNs are committees of GPU-
MPCNNs with simple democratic output averaging. Several MPCNNs see the same input; their output
vectors are used to assign probabilities to the various possible classes. The class with the on average
highest probability is chosen as the system’s classiﬁcation of the present input. Compare earlier, more
sophisticated ensemble methods , the contest-winning ensemble Bayes-NN of Sec. 5.14, and recent related work .
An ensemble of GPU-MPCNNs was the ﬁrst system to achieve superhuman visual pattern recognition in a controlled competition, namely, the IJCNN 2011 trafﬁc
sign recognition contest in San Jose (CA) . This is of interest for fully
autonomous, self-driving cars in trafﬁc . The GPU-MPCNN ensemble obtained 0.56% error rate and was twice better than human test subjects, three times better than
the closest artiﬁcial NN competitor , and six times better than the best
non-neural method.
A few months earlier, the qualifying round was won in a 1st stage online competition, albeit by
a much smaller margin: 1.02% vs 1.03% for second place . After the deadline, the organisers revealed that human performance on the test set
was 1.19%. That is, the best methods already seemed human-competitive. However, during the
qualifying it was possible to incrementally gain information about the test set by probing it through
repeated submissions. This is illustrated by better and better results obtained by various teams over
time (the organisers eventually imposed a limit of 10 resubmissions). In the
ﬁnal competition this was not possible.
This illustrates a general problem with benchmarks whose test sets are public, or at least can be
probed to some extent: competing teams tend to overﬁt on the test set even when it cannot be directly
used for training, only for evaluation.
In 1997 many thought it a big deal that human chess world champion Kasparov was beaten by
an IBM computer. But back then computers could not at all compete with little kids in visual pattern recognition, which seems much harder than chess from a computational perspective. Of course,
the trafﬁc sign domain is highly restricted, and kids are still much better general pattern recognisers. Nevertheless, by 2011, deep NNs could already learn to rival them in important limited visual
An ensemble of GPU-MPCNNs was also the ﬁrst method to achieve human-competitive performance (around 0.2%) on MNIST . This represented a dramatic improvement,
since by then the MNIST record had hovered around 0.4% for almost a decade (Sec. 5.14, 5.16, 5.18).
Given all the prior work on (MP)CNNs (Sec. 5.4, 5.8, 5.11, 5.16) and GPU-CNNs (Sec. 5.16),
GPU-MPCNNs are not a breakthrough in the scientiﬁc sense. But they are a commercially relevant
breakthrough in efﬁcient coding that has made a difference in several contests since 2011. Today, most
feedforward competition-winning deep NNs are (ensembles of) GPU-MPCNNs (Sec. 5.21–5.23).
2011: Hessian-Free Optimization for RNNs
Also in 2011 it was shown that Hessian-free optimization (Sec. 5.6.2) can alleviate the Fundamental Deep Learning Problem (Sec. 5.9) in RNNs, outperforming standard gradient-based LSTM RNNs (Sec. 5.13) on
several tasks. Compare other RNN algorithms that also at least sometimes yield better results than steepest descent for
LSTM RNNs.
2012: First Contests Won on ImageNet, Object Detection, Segmentation
In 2012, an ensemble of GPU-MPCNNs (Sec. 5.19) achieved best results on the ImageNet classiﬁcation benchmark , which is popular in the computer vision community. Here
relatively large image sizes of 256x256 pixels were necessary, as opposed to only 48x48 pixels for
the 2011 trafﬁc sign competition (Sec. 5.19). See further improvements in Sec. 5.22.
Also in 2012, the biggest NN so far (109 free parameters) was trained in unsupervised mode
(Sec. 5.7, 5.15) on unlabeled data , then applied to ImageNet. The codes across its top
layer were used to train a simple supervised classiﬁer, which achieved best results so far on 20,000
classes. Instead of relying on efﬁcient GPU programming, this was done by brute force on 1,000
standard machines with 16,000 cores.
So by 2011/2012, excellent results had been achieved by Deep Learners in image recognition and
classiﬁcation (Sec. 5.19, 5.21). The computer vision community, however, is especially interested in
object detection in large images, for applications such as image-based search engines, or for biomedical diagnosis where the goal may be to automatically detect tumors etc in images of human tissue.
Object detection presents additional challenges. One natural approach is to train a deep NN classiﬁer
on patches of big images, then use it as a feature detector to be shifted across unknown visual scenes,
using various rotations and zoom factors. Image parts that yield highly active output units are likely
to contain objects similar to those the NN was trained on.
2012 ﬁnally saw the ﬁrst DL system (an ensemble of GPU-MPCNNs, Sec. 5.19) to win a contest
on visual object detection in large images of several million pixels . Such
biomedical applications may turn out to be among the most important applications of DL. The world
spends over 10% of GDP on healthcare (> 6 trillion USD per year), much of it on medical diagnosis
through expensive experts. Partial automation of this could not only save lots of money, but also make
expert diagnostics accessible to many who currently cannot afford it. It is gratifying to observe that
today deep NNs may actually help to improve healthcare and perhaps save human lives.
2012 also saw the ﬁrst pure image segmentation contest won by DL , again
through an GPU-MPCNN ensemble .2 EM stacks are relevant for the recently approved huge brain projects in Europe and the
US . Given electron microscopy images of stacks of thin slices of animal
brains, the goal is to build a detailed 3D model of the brain’s neurons and dendrites. But human
experts need many hours and days and weeks to annotate the images: Which parts depict neuronal
membranes? Which parts are irrelevant background? This needs to be automated . Deep Multi-Column GPU-MPCNNs learned to solve this task through experience with many
training images, and won the contest on all three evaluation metrics by a large margin, with superhuman performance in terms of pixel error.
Both object detection and image segmentation proﬁt
from fast MPCNN-based image scans that avoid redundant computations. Recent MPCNN scanners
speed up naive implementations by up to three orders of magnitude ; compare earlier efﬁcient methods for CNNs without MP .
Also in 2012, a system consisting of growing deep FNNs and 2D-BRNNs 
won the CASP 2012 contest on protein contact map prediction. On the IAM-OnDoDB benchmark,
LSTM RNNs (Sec. 5.13) outperformed all other methods (HMMs, SVMs) on online mode detection and keyword spotting . On the
long time lag problem of language modelling, LSTM RNNs outperformed all statistical approaches
on the IAM-DB benchmark ; improved results were later obtained through a
combination of NNs and HMMs . Compare earlier RNNs for object
recognition through iterative image interpretation ;
see also more recent publications extending work on biologically plausible learning rules for RNNs .
2013-: More Contests and Benchmark Records
A stack (Sec. 5.10) of bi-directional LSTM
RNNs trained by CTC (Sec. 5.13, 5.17) broke a famous TIMIT
speech (phoneme) recognition record, achieving 17.7% test set error rate , despite
thousands of man years previously spent on Hidden Markov Model (HMMs)-based speech recognition
research. Compare earlier DBN results (Sec. 5.15).
CTC-LSTM also helped to score ﬁrst at NIST’s OpenHaRT2013 evaluation .
For optical character recognition (OCR), LSTM RNNs outperformed commercial recognizers of historical data . LSTM-based systems also set benchmark records in language identiﬁcation , medium-vocabulary speech recognition , prosody contour prediction , audio onset detection , text-to-speech synthesis , and social signal classiﬁcation .
An LSTM RNN was used to estimate the state posteriors of an HMM; this system beat the previous
state of the art in large vocabulary speech recognition . Another LSTM RNN with
hundreds of millions of connections was used to rerank hypotheses of a statistical machine translation
2It should be mentioned, however, that LSTM RNNs already performed simultaneous segmentation and recognition when
they became the ﬁrst recurrent Deep Learners to win ofﬁcial international pattern recognition contests—see Sec. 5.17.
system; this system beat the previous state of the art in English to French translation (Sutskever et al.,
A new record on the ICDAR Chinese handwriting recognition benchmark (over 3700 classes)
was set on a desktop machine by an ensemble of GPU-MPCNNs (Sec. 5.19) with almost human
performance ; compare .
The MICCAI 2013 Grand Challenge on Mitosis Detection also was won by an
object-detecting GPU-MPCNN ensemble . Its data set was even larger and more
challenging than the one of ICPR 2012 (Sec. 5.21): a real-world dataset including many ambiguous
cases and frequently encountered problems such as imperfect slide staining.
Three 2D-CNNs (with mean-pooling instead of MP, Sec. 5.11) observing three orthogonal projections of 3D images outperformed traditional full 3D methods on the task of segmenting tibial cartilage
in low ﬁeld knee MRI scans .
Deep GPU-MPCNNs (Sec. 5.19) also helped to achieve new best results on important benchmarks of the computer vision community: ImageNet classiﬁcation and—in conjunction with traditional approaches—PASCAL object detection . They also learned to predict bounding box coordinates of objects in the Imagenet
2013 database, and obtained state-of-the-art results on tasks of localization and detection . GPU-MPCNNs also helped to recognise multi-digit numbers in Google Street View
images , where part of the NN was trained to count visible digits; compare
earlier work on detecting “numerosity” through DBNs . This system also
excelled at recognising distorted synthetic text in reCAPTCHA puzzles. Other successful CNN applications include scene parsing , object detection , shadow
detection , video classiﬁcation , and Alzheimers disease
neuroimaging .
Additional contests are mentioned in the web pages of the Swiss AI Lab IDSIA, the University of
Toronto, NY University, and the University of Montreal.
Currently Successful Techniques: LSTM RNNs and GPU-MPCNNs
Most competition-winning or benchmark record-setting Deep Learners actually use one of two supervised techniques: (a) recurrent LSTM trained by CTC (Sec. 5.13, 5.17, 5.21, 5.22), or
(b) feedforward GPU-MPCNNs based on CNNs with
MP trained through BP .
Exceptions include two 2011 contests specialised on Transfer Learning from one dataset to another . However, deep GPU-MPCNNs do allow for pure SL-based
transfer , where pre-training on one training set greatly improves performance
on quite different sets, also in more recent studies . In
fact, deep MPCNNs pre-trained by SL can extract useful features from quite diverse off-training-set
images, yielding better results than traditional, widely used features such as SIFT 
on many vision tasks . To deal with changing datasets, slowly learning deep
NNs were also combined with rapidly adapting “surface” NNs .
Remarkably, in the 1990s a trend went from partially unsupervised RNN stacks (Sec. 5.10) to
purely supervised LSTM RNNs (Sec. 5.13), just like in the 2000s a trend went from partially unsupervised FNN stacks (Sec. 5.15) to purely supervised MPCNNs (Sec. 5.16–5.22). Nevertheless, in many
applications it can still be advantageous to combine the best of both worlds—supervised learning and
unsupervised pre-training (Sec. 5.10, 5.15).
Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
DBN training (Sec. 5.15) can be improved through gradient enhancements and automatic learning rate
adjustments during stochastic gradient descent , and through Tikhonovtype regularization of RBMs . Contractive AEs discourage hidden unit perturbations in response to input perturbations, similar to how FMS
(Sec. 5.6.3) for LOCOCODE AEs (Sec. 5.6.4) discourages output perturbations in response to weight
perturbations.
Hierarchical CNNs in a Neural Abstraction Pyramid were trained to
reconstruct images corrupted by structured noise , thus enforcing increasingly abstract
image representations in deeper and deeper layers. Denoising AEs later used a similar procedure .
Dropout removes units from NNs during training to
improve generalisation. Some view it as an ensemble method that trains multiple data models simultaneously . Under certain circumstances, it could also be viewed as a form
of training set augmentation: effectively, more and more informative complex features are removed
from the training data. Compare dropout for RNNs . A deterministic approximation coined fast dropout 
can lead to faster learning and evaluation and was adapted for RNNs . Dropout is
closely related to older, biologically plausible techniques for adding noise to neurons or synapses during training , which in turn are closely related to ﬁnding perturbation-resistant lowcomplexity NNs, e.g., through FMS (Sec. 5.6.3). MDL-based stochastic variational methods are also related to FMS. They are useful for RNNs, where classic regularizers such as weight
decay (Sec. 5.6.3) represent a bias towards limited memory capacity .
Compare recent work on variational recurrent AEs .
The activation function f of Rectiﬁed Linear Units (ReLUs) is f(x) = x for x > 0, f(x) = 0
otherwise—compare the old concept of half-wave rectiﬁed units . ReLU
NNs are useful for RBMs , outperformed sigmoidal activation functions in deep NNs , and helped to obtain best results on several
benchmark problems across multiple domains .
NNs with competing linear units tend to outperform those with non-competing nonlinear units,
and avoid catastrophic forgetting through BP when training sets change over time . In this context, choosing a learning algorithm may be more important than choosing activation
functions . Maxout NNs combine competitive
interactions and dropout (see above) to achieve excellent results on certain benchmarks. Compare
early RNNs with competing units for SL and RL . To address overﬁtting,
instead of depending on pre-wired regularizers and hyper-parameters , self-delimiting RNNs (SLIM NNs) with competing units can in principle
learn to select their own runtime and their own numbers of effective free parameters, thus learning
their own computable regularisers (Sec. 4.4, 5.6.3), becoming fast and slim when necessary. One may
penalize the task-speciﬁc total length of connections and communication costs of SLIM NNs implemented on the 3dimensional brain-like multi-processor hardware to be expected in the future.
RmsProp can speed up ﬁrst order gradient descent methods (Sec. 5.5, 5.6.2); compare vario-η , Adagrad and Adadelta . DL in NNs can also be improved by transforming hidden
unit activations such that they have zero output and slope on average . Many additional, older tricks (Sec. 5.6.2, 5.6.3) should also be applicable to today’s deep NNs; compare .
Consequences for Neuroscience
It is ironic that artiﬁcial NNs (ANNs) can help to better understand biological NNs (BNNs)—see
the ISBI 2012 results mentioned in Sec. 5.21 .
The feature detectors learned by single-layer visual ANNs are similar to those found in early
visual processing stages of BNNs (e.g., Sec. 5.6.4). Likewise, the feature detectors learned in deep
layers of visual ANNs should be highly predictive of what neuroscientists will ﬁnd in deep layers
of BNNs. While the visual cortex of BNNs may use quite different learning algorithms, its objective
function to be minimised may be quite similar to the one of visual ANNs. In fact, results obtained with
relatively deep artiﬁcial DBNs and CNNs seem compatible
with insights about the visual pathway in the primate cerebral cortex, which has been studied for
many decades ; compare a computer vision-oriented survey .
DL with Spiking Neurons?
Many recent DL results proﬁt from GPU-based traditional deep NNs, e.g., Sec. 5.16–5.19. Current
GPUs, however, are little ovens, much hungrier for energy than biological brains, whose neurons ef-
ﬁciently communicate by brief spikes , and often remain quiet. Many computational models of such spiking neurons have been proposed and analyzed .
Future energy-efﬁcient hardware for DL in NNs may implement aspects of such models . A simulated, event-driven, spiking variant of an RBM
(Sec. 5.15) was trained by a variant of the Contrastive Divergence algorithm . Spiking
nets were evolved to achieve reasonable performance on small face recognition data sets and to control simple robots . A
spiking DBN with about 250,000 neurons achieved 6% error rate on MNIST; compare similar results with a spiking DBN variant of
depth 3 using a neuromorphic event-based sensor . In practical applications,
however, current artiﬁcial networks of spiking neurons cannot yet compete with the best traditional
deep NNs (e.g., compare MNIST results of Sec. 5.19).
DL in FNNs and RNNs for Reinforcement Learning (RL)
So far we have focused on Deep Learning (DL) in supervised or unsupervised NNs. Such NNs learn
to perceive / encode / predict / classify patterns or pattern sequences, but they do not learn to act
in the more general sense of Reinforcement Learning (RL) in unknown environments . Here we add a
discussion of DL FNNs and RNNs for RL. It will be shorter than the discussion of FNNs and RNNs
for SL and UL (Sec. 5), reﬂecting the current size of the various ﬁelds.
Without a teacher, solely from occasional real-valued pain and pleasure signals, RL agents must
discover how to interact with a dynamic, initially unknown environment to maximize their expected
cumulative reward signals (Sec. 2). There may be arbitrary, a priori unknown delays between actions
and perceivable consequences. The problem is as hard as any problem of computer science, since any
task with a computable description can be formulated in the RL framework . For
example, an answer to the famous question of whether P = NP would
also set limits for what is achievable by general RL. Compare more speciﬁc limitations, e.g., . The following subsections mostly focus
on certain obvious intersections between DL and RL—they cannot serve as a general RL survey.
RL Through NN World Models Yields RNNs With Deep CAPs
In the special case of an RL FNN controller C interacting with a deterministic, predictable environment, a separate FNN called M can learn to become C’s world model through system identiﬁcation,
predicting C’s inputs from previous actions and inputs . Assume M has learned to produce accurate predictions. We can use M to substitute the environment. Then M and C form an RNN where M’s outputs become inputs of C, whose
outputs (actions) in turn become inputs of M. Now BP for RNNs (Sec. 5.5.1) can be used to achieve
desired input events such as high real-valued reward signals: While M’s weights remain ﬁxed, gradient information for C’s weights is propagated back through M down into C and back through M etc.
To a certain extent, the approach is also applicable in probabilistic or uncertain environments, as long
as the inner products of M’s C-based gradient estimates and M’s “true” gradients tend to be positive.
In general, this approach implies deep CAPs for C, unlike in DP-based traditional RL (Sec. 6.2).
Decades ago, the method was used to learn to back up a model truck .
An RL active vision system used it to learn sequential shifts (saccades) of a fovea, to detect targets in
visual scenes , thus learning to control selective attention. Compare
RL-based attention learning without NNs .
To allow for memories of previous events in partially observable worlds (Sec. 6.3), the most general variant of this technique uses RNNs instead of FNNs to implement both M and C . This may cause deep CAPs not only for C but also
M can also be used to optimize expected reward by planning future action sequences .
In fact, the winners of the 2004 RoboCup World Championship in the fast
league trained NNs to predict the effects of steering signals on fast robots
with 4 motors for 4 different wheels. During play, such NN models were used to achieve desirable
subgoals, by optimizing action sequences through quickly planning ahead. The approach also was
used to create self-healing robots able to compensate for faulty motors whose effects do not longer
match the predictions of the NN models .
Typically M is not given in advance. Then an essential question is: which experiments should C
conduct to quickly improve M? The Formal Theory of Fun and Creativity formalizes driving forces and value functions behind such curious and exploratory behavior:
A measure of the learning progress of M becomes the intrinsic reward of C ;
compare . This motivates C to create action sequences
(experiments) such that M makes quick progress.
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs)
The classical approach to RL makes the simplifying
assumption of Markov Decision Processes (MDPs): the current input of the RL agent conveys all
information necessary to compute an optimal next output event or decision. This allows for greatly
reducing CAP depth in RL NNs (Sec. 3, 6.1) by using the Dynamic Programming (DP) trick . The latter is often explained in a probabilistic framework , but
its basic idea can already be conveyed in a deterministic setting. For simplicity, using the notation
of Sec. 2, let input events xt encode the entire current state of the environment, including a realvalued reward rt (no need to introduce additional vector-valued notation, since real values can encode
arbitrary vectors of real values). The original RL goal (ﬁnd weights that maximize the sum of all
rewards of an episode) is replaced by an equivalent set of alternative goals set by a real-valued value
function V deﬁned on input events. Consider any two subsequent input events xt, xk. Recursively
deﬁne V (xt) = rt + V (xk), where V (xk) = rk if xk is the last input event. Now search for weights
that maximize the V of all input events, by causing appropriate output events or actions.
Due to the Markov assumption, an FNN sufﬁces to implement the policy that maps input to output events. Relevant CAPs are not deeper than this FNN. V itself is often modeled by a separate
FNN (also yielding typically short CAPs) learning to approximate V (xt) only from local information
rt, V (xk).
Many variants of traditional RL exist . Most are formulated in
a probabilistic framework, and evaluate pairs of input and output (action) events (instead of input
events only). To facilitate certain mathematical derivations, some discount delayed rewards, but such
distortions of the original RL problem are problematic.
Perhaps the most well-known RL NN is the world-class RL backgammon player ,
which achieved the level of human world champions by playing against itself. Its nonlinear, rather
shallow FNN maps a large but ﬁnite number of discrete board states to values. More recently, a
rather deep GPU-CNN was used in a traditional RL framework to play several Atari 2600 computer
games directly from 84x84 pixel 60 Hz video input , using experience replay , extending previous work on Neural Fitted Q-Learning (NFQ) . Even better results are achieved by using (slow) Monte Carlo tree planning to train comparatively fast deep
NNs . Compare RBM-based RL with high-dimensional
inputs , earlier RL Atari players , and an earlier, raw videobased RL NN for computer games trained by Indirect Policy Search (Sec. 6.7).
Deep RL RNNs for Partially Observable MDPs (POMDPs)
The Markov assumption (Sec. 6.2) is often unrealistic. We cannot directly perceive what is behind our
back, let alone the current state of the entire universe. However, memories of previous events can help
to deal with partially observable Markov decision problems (POMDPs) . A naive way of implementing
memories without leaving the MDP framework (Sec. 6.2) would be to simply consider a possibly huge
state space, namely, the set of all possible observation histories and their preﬁxes. A more realistic
way is to use function approximators such as RNNs that produce compact state features as a function
of the entire history seen so far. Generally speaking, POMDP RL often uses DL RNNs to learn which
events to memorize and which to ignore. Three basic alternatives are:
1. Use an RNN as a value function mapping arbitrary event histories to values . For example, deep LSTM RNNs were used in this
way for RL robots .
2. Use an RNN controller in conjunction with a second RNN as predictive world model, to obtain
a combined RNN with deep CAPs—see Sec. 6.1.
3. Use an RNN for RL by Direct Search (Sec. 6.6) or Indirect Search (Sec. 6.7) in weight space.
In general, however, POMDPs may imply greatly increased CAP depth.
RL Facilitated by Deep UL in FNNs and RNNs
RL machines may proﬁt from UL for input preprocessing . In particular, an UL NN can learn to compactly encode environmental inputs such as images or videos, e.g.,
Sec. 5.7, 5.10, 5.15. The compact codes (instead of the high-dimensional raw data) can be fed into an
RL machine, whose job thus may become much easier ,
just like SL may proﬁt from UL, e.g., Sec. 5.7, 5.10, 5.15. For example, NFQ was
applied to real-world control tasks where purely
visual inputs were compactly encoded by deep autoencoders (Sec. 5.7, 5.15). RL combined with UL
based on Slow Feature Analysis enabled a real
humanoid robot to learn skills from raw high-dimensional video streams . To
deal with POMDPs (Sec. 6.3) involving high-dimensional inputs, RBM-based RL was used , and a RAAM (Sec. 5.7) was employed as a deep unsupervised sequence encoder for RL . Certain types of RL and UL also were combined in biologically
plausible RNNs with spiking neurons (Sec. 5.26) .
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs
Multiple learnable levels of abstraction seem as important for RL as for SL. Work on NN-based Hierarchical RL (HRL) has been published since the early 1990s. In particular, gradient-based subgoal
discovery with FNNs or RNNs decomposes RL tasks into subtasks for RL submodules . Numerous alternative HRL techniques have
been proposed . While HRL frameworks such as
Feudal RL and options do not directly address the problem of automatic subgoal discovery, HQ-Learning automatically decomposes POMDPs (Sec. 6.3) into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive sub-agents. Recent HRL organizes potentially deep NN-based RL sub-modules into self-organizing, 2-dimensional motor control
maps inspired by neurophysiological ﬁndings .
Deep RL by Direct NN Search / Policy Gradients / Evolution
Not quite as universal as the methods of Sec. 6.8, yet both practical and more general than most
traditional RL algorithms (Sec. 6.2), are methods for Direct Policy Search (DS). Without a need for
value functions or Markovian assumptions (Sec. 6.2, 6.3), the weights of an FNN or RNN are directly
evaluated on the given RL problem. The results of successive trials inform further search for better
weights. Unlike with RL supported by BP (Sec. 5.5, 6.3, 6.1), CAP depth (Sec. 3, 5.9) is not a crucial
issue. DS may solve the credit assignment problem without backtracking through deep causal chains
of modiﬁable parameters—it neither cares for their existence, nor tries to exploit them.
An important class of DS methods for NNs are Policy Gradient methods . Gradients of the total reward with respect to policies (NN weights)
are estimated (and then exploited) through repeated NN evaluations.
RL NNs can also be evolved through Evolutionary Algorithms (EAs) in a series of trials. Here several policies
are represented by a population of NNs improved through mutations and/or repeated recombinations
of the population’s ﬁttest individuals . Compare Genetic Programming (GP) which can be used to evolve computer programs of variable size , and Cartesian GP for evolving graph-like programs, including NNs and their topology . Related methods include probability distribution-based EAs , Covariance Matrix Estimation Evolution Strategies (CMA-ES) , and NeuroEvolution of Augmenting Topologies (NEAT) . Hybrid methods combine traditional NN-based RL
(Sec. 6.2) and EAs .
Since RNNs are general computers, RNN evolution is like GP in the sense that it can evolve
general programs. Unlike sequential programs learned by traditional GP, however, RNNs can mix
sequential and parallel information processing in a natural and efﬁcient way, as already mentioned in
Sec. 1. Many RNN evolvers have been proposed . One particularly effective
family of methods coevolves neurons, combining them into networks, and selecting those neurons
for reproduction that participated in the best-performing networks . This can help to solve deep POMDPs . Co-Synaptic Neuro-Evolution (CoSyNE) does something similar on the level of
synapses or weights ; beneﬁts of this were shown on difﬁcult nonlinear POMDP
benchmarks.
Natural Evolution Strategies (NES) link policy gradient methods and evolutionary approaches through the concept of Natural
Gradients . RNN evolution may also help to improve SL for deep RNNs through
Evolino (Sec. 5.9).
Deep RL by Indirect Policy Search / Compressed NN Search
Some DS methods (Sec. 6.6) can evolve NNs with hundreds or thousands of weights, but not millions. How to search for large and deep NNs? Most SL and RL methods mentioned so far somehow
search the space of weights wi. Some proﬁt from a reduction of the search space through shared
wi that get reused over and over again, e.g., in CNNs (Sec. 5.4, 5.8, 5.16, 5.21), or in RNNs for SL
(Sec. 5.5, 5.13, 5.17) and RL (Sec. 6.1, 6.3, 6.6).
It may be possible, however, to exploit additional regularities/compressibilities in the space of solutions, through indirect search in weight space. Instead of evolving large NNs directly (Sec. 6.6), one
can sometimes greatly reduce the search space by evolving compact encodings of NNs, e.g., through
Lindenmeyer Systems , graph rewriting , Cellular Encoding , HyperNEAT (extending NEAT; Sec. 6.6), and extensions
thereof . This helps to avoid overﬁtting (compare Sec. 5.6.3, 5.24) and is
closely related to the topics of regularisation and MDL (Sec. 4.4).
A general approach for both SL and RL seeks to compactly encode weights
of large NNs through programs written in a universal programming language . Often it is much more efﬁcient to
systematically search the space of such programs with a bias towards short and fast programs , instead of directly searching the huge space of possible NN weight
matrices. A previous universal language for encoding NNs was assembler-like .
More recent work uses more practical languages based on coefﬁcients of popular transforms (Fourier,
wavelet, etc). In particular, RNN weight matrices may be compressed like images, by encoding them
through the coefﬁcients of a discrete cosine transform (DCT) . Compact
DCT-based descriptions can be evolved through NES or CoSyNE (Sec. 6.6). An RNN with over a
million weights learned (without a teacher) to drive a simulated car in the TORCS driving game , based on a high-dimensional video-like visual input stream . The RNN learned both control and visual processing from scratch, without being aided by
UL. (Of course, UL might help to generate more compact image codes (Sec. 6.4, 4.2) to be fed into a
smaller RNN, to reduce the overall computational effort.)
Universal RL
General purpose learning algorithms may improve themselves in open-ended fashion and
environment-speciﬁc ways in a lifelong learning context . The most general type of RL is constrained only by the
fundamental limitations of computability identiﬁed by the founders of theoretical computer science
 . Remarkably, there exist blueprints of universal problem solvers or universal RL machines for unlimited problem depth that are time-optimal in
various theoretical senses . In particular, the G¨odel
Machine can be implemented on general computers such as RNNs and may improve any part of its
software (including the learning algorithm itself) in a way that is provably time-optimal in a certain
sense . It can be initialized by an asymptotically optimal meta-method (also applicable to RNNs) which will solve any well-deﬁned problem as quickly as the
unknown fastest way of solving it, save for an additive constant overhead that becomes negligible as
problem size grows. Note that most problems are large; only few are small. AI and DL researchers are
still in business because many are interested in problems so small that it is worth trying to reduce the
overhead through less general methods, including heuristics. Here I won’t further discuss universal
RL methods, which go beyond what is usually called DL.
Conclusion and Outlook
Deep Learning (DL) in Neural Networks (NNs) is relevant for Supervised Learning (SL) (Sec. 5),
Unsupervised Learning (UL) (Sec. 5), and Reinforcement Learning (RL) (Sec. 6). By alleviating
problems with deep Credit Assignment Paths (CAPs, Sec. 3, 5.9), UL (Sec. 5.6.4) can not only facilitate SL of sequences (Sec. 5.10) and stationary patterns (Sec. 5.7, 5.15), but also RL (Sec. 6.4, 4.2).
Dynamic Programming (DP, Sec. 4.1) is important for both deep SL (Sec. 5.5) and traditional RL with
deep NNs (Sec. 6.2). A search for solution-computing, perturbation-resistant (Sec. 5.6.3, 5.15, 5.24),
low-complexity NNs describable by few bits of information (Sec. 4.4) can reduce overﬁtting and improve deep SL & UL (Sec. 5.6.3, 5.6.4) as well as RL (Sec. 6.7), also in the case of partially observable
environments (Sec. 6.3). Deep SL, UL, RL often create hierarchies of more and more abstract representations of stationary data (Sec. 5.3, 5.7, 5.15), sequential data (Sec. 5.10), or RL policies (Sec. 6.5).
While UL can facilitate SL, pure SL for feedforward NNs (FNNs) (Sec. 5.5, 5.8, 5.16, 5.18) and recurrent NNs (RNNs) (Sec. 5.5, 5.13) did not only win early contests (Sec. 5.12, 5.14) but also most
of the recent ones (Sec. 5.17–5.22). Especially DL in FNNs proﬁted from GPU implementations
(Sec. 5.16–5.19). In particular, GPU-based (Sec. 5.19) Max-Pooling (Sec. 5.11) Convolutional NNs
(Sec. 5.4, 5.8, 5.16) won competitions not only in pattern recognition (Sec. 5.19–5.22) but also image
segmentation (Sec. 5.21) and object detection (Sec. 5.21, 5.22).
Unlike these systems, humans learn to actively perceive patterns by sequentially directing attention to relevant parts of the available data. Near future deep NNs will do so, too, extending previous
work since 1990 on NNs that learn selective attention through RL of (a) motor actions such as saccade
control (Sec. 6.1) and (b) internal actions controlling spotlights of attention within RNNs, thus closing
the general sensorimotor loop through both external and internal feedback (e.g., Sec. 2, 5.21, 6.6, 6.7).
Many future deep NNs will also take into account that it costs energy to activate neurons, and to
send signals between them. Brains seem to minimize such computational costs during problem solving in at least two ways: (1) At a given time, only a small fraction of all neurons is active because local
competition through winner-take-all mechanisms shuts down many neighbouring neurons, and only
winners can activate other neurons through outgoing connections (compare SLIM NNs; Sec. 5.24).
(2) Numerous neurons are sparsely connected in a compact 3D volume by many short-range and
few long-range connections (much like microchips in traditional supercomputers). Often neighbouring neurons are allocated to solve a single task, thus reducing communication costs. Physics seems
to dictate that any efﬁcient computational hardware will in the future also have to be brain-like in
keeping with these two constraints. The most successful current deep RNNs, however, are not. Unlike certain spiking NNs (Sec. 5.26), they usually activate all units at least slightly, and tend to be
strongly connected, ignoring natural constraints of 3D hardware. It should be possible to improve
them by adopting (1) and (2), and by minimizing non-differentiable energy and communication costs
through direct search in program (weight) space (e.g., Sec. 6.6, 6.7). These more brain-like RNNs
will allocate neighboring RNN parts to related behaviors, and distant RNN parts to less related ones,
thus self-modularizing in a way more general than that of traditional self-organizing maps in FNNs
(Sec. 5.6.4). They will also implement Occam’s razor (Sec. 4.4, 5.6.3) as a by-product of energy min-
imization, by ﬁnding simple (highly generalizing) problem solutions that require few active neurons
and few, mostly short connections.
The more distant future may belong to general purpose learning algorithms that improve themselves in provably optimal ways (Sec. 6.8), but these are not yet practical or commercially relevant.
Acknowledgments
Since 16 April 2014, drafts of this paper have undergone massive open online peer review through
public mailing lists including , , comp-neuro-
@neuroinf.org, genetic , , imageworld-
@diku.dk, Google+ machine learning forum. Thanks to numerous NN / DL experts for valuable
comments. Thanks to SNF, DFG, and the European Commission for partially funding my DL research group in the past quarter-century. The contents of this paper may be used for educational and
non-commercial purposes, including articles for Wikipedia and similar sites.