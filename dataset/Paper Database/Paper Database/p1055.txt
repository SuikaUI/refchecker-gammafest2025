ONLINE DICTIONARY LEARNING FOR SPARSE CODING
Julien Mairal
Francis Bach
Jean Ponce
Guillermo Sapiro
IMA Preprint Series # 2249
 
INSTITUTE FOR MATHEMATICS AND ITS APPLICATIONS
UNIVERSITY OF MINNESOTA
400 Lind Hall
207 Church Street S.E.
Minneapolis, Minnesota 55455–0436
Phone: 612-624-6066
Fax: 612-626-7370
URL: 
Online Dictionary Learning for Sparse Coding
Julien Mairal
 
Francis Bach
 
INRIA, Paris, France1
Jean Ponce
 
Ecole Normale Sup´erieure, Paris, France1
Guillermo Sapiro
 
University of Minnesota - Department of Electrical and Computer Engineering, Minneapolis, USA
Sparse coding—that is, modelling data vectors as
sparse linear combinations of basis elements—is
widely used in machine learning, neuroscience,
signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to speciﬁc data, an approach
that has recently proven to be very effective for
signal reconstruction and classiﬁcation in the audio and image processing domains. This paper
proposes a new online optimization algorithm
for dictionary learning, based on stochastic approximations, which scales up gracefully to large
datasets with millions of training samples.
proof of convergence is presented, along with
experiments with natural images demonstrating
that it leads to faster performance and better dictionaries than classical batch algorithms for both
small and large datasets.
1. Introduction
The linear decomposition of a signal using a few atoms of
a learned dictionary instead of a predeﬁned one—based on
wavelets for example—has recently led to
state-of-the-art results for numerous low-level image processing tasks such as denoising 
as well as higher-level tasks such as classiﬁcation , showing that sparse
learned models are well adapted to natural signals. Un-
1WILLOW Project, Laboratoire d’Informatique de l’Ecole
Normale Sup´erieure, ENS/INRIA/CNRS UMR 8548.
Appearing in Proceedings of the 26 th International Conference
on Machine Learning, Montreal, Canada, 2009. Copyright 2009
by the author(s)/owner(s).
like decompositions based on principal component analysis and its variants, these models do not impose that the
basis vectors be orthogonal, allowing more ﬂexibility to
adapt the representation to the data. While learning the
dictionary has proven to be critical to achieve (or improve
upon) state-of-the-art results, effectively solving the corresponding optimization problem is a signiﬁcant computational challenge, particularly in the context of the largescale datasets involved in image processing tasks, that may
include millions of training samples. Addressing this challenge is the topic of this paper.
Concretely, consider a signal x in Rm. We say that it admits a sparse approximation over a dictionary D in Rm×k,
with k columns referred to as atoms, when one can ﬁnd a
linear combination of a “few” atoms from D that is “close”
to the signal x. Experiments have shown that modelling a
signal with such a sparse decomposition (sparse coding) is
very effective in many signal processing applications . For natural images, predeﬁned dictionaries
based on various types of wavelets have
been used for this task. However, learning the dictionary
instead of using off-the-shelf bases has been shown to dramatically improve signal reconstruction . Although some of the learned dictionary elements
may sometimes “look like” wavelets (or Gabor ﬁlters), they
are tuned to the input images or signals, leading to much
better results in practice.
Most recent algorithms for dictionary learning 
are second-order iterative batch procedures, accessing the
whole training set at each iteration in order to minimize a
cost function under some constraints. Although they have
shown experimentally to be much faster than ﬁrst-order
gradient descent methods , they cannot
effectively handle very large training sets , or dynamic training data changing over time,
Online Dictionary Learning for Sparse Coding
such as video sequences. To address these issues, we propose an online approach that processes one element (or a
small subset) of the training set at a time. This is particularly important in the context of image and video processing , where it is common to learn
dictionaries adapted to small patches, with training data
that may include several millions of these patches (roughly
one per pixel and per frame). In this setting, online techniques based on stochastic approximations are an attractive
alternative to batch methods . For example, ﬁrst-order stochastic gradient descent with projections
on the constraint set is sometimes
used for dictionary learning 
for instance). We show in this paper that it is possible to
go further and exploit the speciﬁc structure of sparse coding in the design of an optimization procedure dedicated
to the problem of dictionary learning, with low memory
consumption and lower computational cost than classical
second-order batch algorithms and without the need of explicit learning rate tuning. As demonstrated by our experiments, the algorithm scales up gracefully to large datasets
with millions of training samples, and it is usually faster
than more standard methods.
1.1. Contributions
This paper makes three main contributions.
• We cast in Section 2 the dictionary learning problem as
the optimization of a smooth nonconvex objective function
over a convex set, minimizing the (desired) expected cost
when the training set size goes to inﬁnity.
• We propose in Section 3 an iterative online algorithm that
solves this problem by efﬁciently minimizing at each step a
quadratic surrogate function of the empirical cost over the
set of constraints. This method is shown in Section 4 to
converge with probability one to a stationary point of the
cost function.
• As shown experimentally in Section 5, our algorithm is
signiﬁcantly faster than previous approaches to dictionary
learning on both small and large datasets of natural images. To demonstrate that it is adapted to difﬁcult, largescale image-processing tasks, we learn a dictionary on a
12-Megapixel photograph and use it for inpainting.
2. Problem Statement
Classical dictionary learning techniques consider a ﬁnite training set of signals
X = [x1, . . . , xn] in Rm×n and optimize the empirical cost
where D in Rm×k is the dictionary, each column representing a basis vector, and l is a loss function such that l(x, D)
should be small if D is “good” at representing the signal x.
The number of samples n is usually large, whereas the signal dimension m is relatively small, for example, m = 100
for 10 × 10 image patches, and n ≥100, 000 for typical
image processing applications. In general, we also have
k ≪n (e.g., k = 200 for n = 100, 000), and each signal
only uses a few elements of D in its representation. Note
that, in this setting, overcomplete dictionaries with k > m
are allowed. As others for example),
we deﬁne l(x, D) as the optimal value of the ℓ1-sparse coding problem:
2||x −Dα||2
2 + λ||α||1,
where λ is a regularization parameter.2 This problem is
also known as basis pursuit , or the
Lasso .
It is well known that the ℓ1
penalty yields a sparse solution for α, but there is no analytic link between the value of λ and the corresponding
effective sparsity ||α||0. To prevent D from being arbitrarily large (which would lead to arbitrarily small values of
α), it is common to constrain its columns (dj)k
j=1 to have
an ℓ2 norm less than or equal to one. We will call C the
convex set of matrices verifying this constraint:
△= {D ∈Rm×k s.t. ∀j = 1, . . . , k, dT
j dj ≤1}. (3)
Note that the problem of minimizing the empirical cost
fn(D) is not convex with respect to D. It can be rewritten as a joint optimization problem with respect to the dictionary D and the coefﬁcients α = [α1, . . . , αn] of the
sparse decomposition, which is not jointly convex, but convex with respect to each of the two variables D and α when
the other one is ﬁxed:
D∈C,α∈Rk×n
2||xi −Dαi||2
2 + λ||αi||1
A natural approach to solving this problem is to alternate between the two variables, minimizing over one while
keeping the other one ﬁxed, as proposed by Lee et al.
 and Aharon et al.
 , who use ℓ0 rather than ℓ1 penalties, for related approaches).3 Since the computation of α dominates the cost
of each iteration, a second-order optimization technique
2The ℓp norm of a vector x in Rm is deﬁned, for p ≥1, by
i=1 |x[i]|p)1/p. Following tradition, we denote by
||x||0 the number of nonzero elements of the vector x. This “ℓ0”
sparsity measure is not a true norm.
3In our setting, as in , we use the convex ℓ1
norm, that has empirically proven to be better behaved in general
than the ℓ0 pseudo-norm for dictionary learning.
Online Dictionary Learning for Sparse Coding
can be used in this case to accurately estimate D at each
step when α is ﬁxed.
As pointed out by Bottou and Bousquet , however,
one is usually not interested in a perfect minimization of
the empirical cost fn(D), but in the minimization of the
expected cost
△= Ex[l(x, D)] = lim
n→∞fn(D) a.s.,
where the expectation (which is assumed ﬁnite) is taken relative to the (unknown) probability distribution p(x) of the
data.4 In particular, given a ﬁnite training set, one should
not spend too much effort on accurately minimizing the
empirical cost, since it is only an approximation of the expected cost.
Bottou and Bousquet have further shown both theoretically and experimentally that stochastic gradient algorithms, whose rate of convergence is not good in conventional optimization terms, may in fact in certain settings be
the fastest in reaching a solution with low expected cost.
With large training sets, classical batch optimization techniques may indeed become impractical in terms of speed or
memory requirements.
In the case of dictionary learning, classical projected ﬁrstorder stochastic gradient descent for instance) consists of a sequence of updates
t ∇Dl(xt, Dt−1)
where ρ is the gradient step, ΠC is the orthogonal projector on C, and the training set x1, x2, . . . are i.i.d. samples
of the (unknown) distribution p(x). As shown in Section
5, we have observed that this method can be competitive
compared to batch methods with large training sets, when
a good learning rate ρ is selected.
The dictionary learning method we present in the next
section falls into the class of online algorithms based
on stochastic approximations, processing one sample at a
time, but exploits the speciﬁc structure of the problem to
efﬁciently solve it. Contrary to classical ﬁrst-order stochastic gradient descent, it does not require explicit learning
rate tuning and minimizes a sequentially quadratic local approximations of the expected cost.
3. Online Dictionary Learning
We present in this section the basic components of our online algorithm for dictionary learning (Sections 3.1–3.3), as
well as two minor variants which speed up our implementation (Section 3.4).
4We use “a.s.” (almost sure) to denote convergence with probability one.
Algorithm 1 Online dictionary learning.
Require: x ∈Rm ∼p(x) (random variable and an algorithm to draw i.i.d samples of p), λ ∈R (regularization
parameter), D0 ∈Rm×k (initial dictionary), T (number of iterations).
1: A0 ←0, B0 ←0 (reset the “past” information).
2: for t = 1 to T do
Draw xt from p(x).
Sparse coding: compute using LARS
△= arg min
2||xt −Dt−1α||2
2 + λ||α||1.
At ←At−1 + 1
Bt ←Bt−1 + xtαT
Compute Dt using Algorithm 2, with Dt−1 as warm
restart, so that
2||xi −Dαi||2
2 + λ||αi||1,
 Tr(DT DAt) −Tr(DT Bt)
8: end for
9: Return DT (learned dictionary).
3.1. Algorithm Outline
Our algorithm is summarized in Algorithm 1. Assuming
the training set composed of i.i.d. samples of a distribution p(x), its inner loop draws one element xt at a time,
as in stochastic gradient descent, and alternates classical
sparse coding steps for computing the decomposition αt of
xt over the dictionary Dt−1 obtained at the previous iteration, with dictionary update steps where the new dictionary
Dt is computed by minimizing over C the function
2||xi −Dαi||2
2 + λ||αi||1,
where the vectors αi are computed during the previous
steps of the algorithm. The motivation behind our approach
is twofold:
• The quadratic function ˆft aggregates the past information computed during the previous steps of the algorithm,
namely the vectors αi, and it is easy to show that it upperbounds the empirical cost ft(Dt) from Eq. (1). One
key aspect of the convergence analysis will be to show that
ˆft(Dt) and ft(Dt) converges almost surely to the same
limit and thus ˆft acts as a surrogate for ft.
• Since ˆft is close to ˆft−1, Dt can be obtained efﬁciently
using Dt−1 as warm restart.
Online Dictionary Learning for Sparse Coding
Algorithm 2 Dictionary Update.
Require: D = [d1, . . . , dk] ∈Rm×k (input dictionary),
A = [a1, . . . , ak] ∈Rk×k = 1
B = [b1, . . . , bk] ∈Rm×k = Pt
for j = 1 to k do
Update the j-th column to optimize for (9):
(bj −Daj) + dj.
max(||uj||2, 1)uj.
5: until convergence
6: Return D (updated dictionary).
3.2. Sparse Coding
The sparse coding problem of Eq. (2) with ﬁxed dictionary is an ℓ1-regularized linear least-squares problem. A
number of recent methods for solving this type of problems are based on coordinate descent with soft thresholding . When the columns
of the dictionary have low correlation, these simple methods have proven to be very efﬁcient. However, the columns
of learned dictionaries are in general highly correlated, and
we have empirically observed that a Cholesky-based implementation of the LARS-Lasso algorithm, an homotopy
method that provides the whole regularization path—that is, the solutions
for all possible values of λ, can be as fast as approaches
based on soft thresholding, while providing the solution
with a higher accuracy.
3.3. Dictionary Update
Our algorithm for updating the dictionary uses blockcoordinate descent with warm restarts, and one of its main
advantages is that it is parameter-free and does not require
any learning rate tuning, which can be difﬁcult in a constrained optimization setting. Concretely, Algorithm 2 sequentially updates each column of D. Using some simple
algebra, it is easy to show that Eq. (10) gives the solution
of the dictionary update (9) with respect to the j-th column
dj, while keeping the other ones ﬁxed under the constraint
j dj ≤1. Since this convex optimization problem admits separable constraints in the updated blocks (columns),
convergence to a global optimum is guaranteed . In practice, since the vectors αi are sparse, the coef-
ﬁcients of the matrix A are in general concentrated on the
diagonal, which makes the block-coordinate descent more
efﬁcient.5 Since our algorithm uses the value of Dt−1 as a
warm restart for computing Dt, a single iteration has empirically been found to be enough. Other approaches have
been proposed to update D, for instance, Lee et al. 
suggest using a Newton method on the dual of Eq. (9), but
this requires inverting a k × k matrix at each Newton iteration, which is impractical for an online algorithm.
3.4. Optimizing the Algorithm
We have presented so far the basic building blocks of our
algorithm.
This section discusses simple improvements
that signiﬁcantly enhance its performance.
Handling Fixed-Size Datasets. In practice, although it
may be very large, the size of the training set is often ﬁnite (of course this may not be the case, when the data
consists of a video stream that must be treated on the ﬂy
for example). In this situation, the same data points may
be examined several times, and it is very common in online algorithms to simulate an i.i.d. sampling of p(x) by
cycling over a randomly permuted training set . This method works experimentally well
in our setting but, when the training set is small enough,
it is possible to further speed up convergence: In Algorithm 1, the matrices At and Bt carry all the information
from the past coefﬁcients α1, . . . , αt. Suppose that at time
t0, a signal x is drawn and the vector αt0 is computed. If
the same signal x is drawn again at time t > t0, one would
like to remove the “old” information concerning x from At
and Bt—that is, write At ←At−1 + αtαT
instance. When dealing with large training sets, it is impossible to store all the past coefﬁcients αt0, but it is still
possible to partially exploit the same idea, by carrying in
At and Bt the information from the current and previous
epochs (cycles through the data) only.
Mini-Batch Extension. In practice, we can improve the
convergence speed of our algorithm by drawing η > 1
signals at each iteration instead of a single one, which is
a classical heuristic in stochastic gradient descent algorithms. Let us denote xt,1, . . . , xt,η the signals drawn at
iteration t. We can then replace the lines 5 and 6 of Algorithm 1 by
←βAt−1 + Pη
←βBt−1 + Pη
where β is chosen so that β =
θ+1 , where θ = tη if
t < η and η2 +t−η if t ≥η, which is compatible with our
convergence analysis.
5Note that this assumption does not exactly hold: To be more
exact, if a group of columns in D are highly correlated, the coefﬁcients of the matrix A can concentrate on the corresponding
principal submatrices of A.
Online Dictionary Learning for Sparse Coding
Purging the Dictionary from Unused Atoms. Every dictionary learning technique sometimes encounters situations
where some of the dictionary atoms are never (or very seldom) used, which happens typically with a very bad intialization. A common practice is to replace them during the
optimization by elements of the training set, which solves
in practice this problem in most cases.
4. Convergence Analysis
Although our algorithm is relatively simple, its stochastic nature and the non-convexity of the objective function
make the proof of its convergence to a stationary point
somewhat involved.
The main tools used in our proofs
are the convergence of empirical processes and, following Bottou , the convergence of
quasi-martingales . Our analysis is limited to
the basic version of the algorithm, although it can in principle be carried over to the optimized version discussed
in Section 3.4. Because of space limitations, we will restrict ourselves to the presentation of our main results and
a sketch of their proofs, which will be presented in details elsewhere, and ﬁrst the (reasonable) assumptions under which our analysis holds.
4.1. Assumptions
(A) The data admits a bounded probability density p
with compact support K. Assuming a compact support
for the data is natural in audio, image, and video processing applications, where it is imposed by the data acquisition
(B) The quadratic surrogate functions ˆft are strictly
convex with lower-bounded Hessians. We assume that
the smallest eigenvalue of the semi-deﬁnite positive matrix 1
t At deﬁned in Algorithm 1 is greater than or equal
to a non-zero constant κ1 (making At invertible and ˆft
strictly convex with Hessian lower-bounded). This hypothesis is in practice veriﬁed experimentally after a few iterations of the algorithm when the initial dictionary is reasonable, consisting for example of a few elements from the
training set, or any one of the “off-the-shelf” dictionaries,
such as DCT (bases of cosines products) or wavelets. Note
that it is easy to enforce this assumption by adding a term
F to the objective function, which is equivalent in
practice to replacing the positive semi-deﬁnite matrix 1
t At + κ1I. We have omitted for simplicity this penalization in our analysis.
(C) A sufﬁcient uniqueness condition of the sparse coding solution is veriﬁed: Given some x ∈K, where K is
the support of p, and D ∈C, let us denote by Λ the set of
indices j such that |dT
j (x −Dα⋆)| = λ, where α⋆is the
solution of Eq. (2). We assume that there exists κ2 > 0
such that, for all x in K and all dictionaries D in the subset
S of C considered by our algorithm, the smallest eigenvalue of DT
ΛDΛ is greater than or equal to κ2. This matrix
is thus invertible and classical results ensure
the uniqueness of the sparse coding solution. It is of course
easy to build a dictionary D for which this assumption fails.
However, having DT
ΛDΛ invertible is a common assumption in linear regression and in methods such as the LARS
algorithm aimed at solving Eq. (2) . It
is also possible to enforce this condition using an elastic
net penalization , replacing ||α||1 by
||α||1 + κ2
2 and thus improving the numerical stability of homotopy algorithms such as LARS. Again, we have
omitted this penalization for simplicity.
4.2. Main Results and Proof Sketches
Given assumptions (A) to (C), let us now show that our
algorithm converges to a stationary point of the objective
Proposition 1 (convergence of f(Dt) and of the surrogate function).
Let ˆft denote the surrogate function
deﬁned in Eq. (7). Under assumptions (A) to (C):
• ˆft(Dt) converges a.s.;
• f(Dt) −ˆft(Dt) converges a.s. to 0; and
• f(Dt) converges a.s.
Proof sktech: The ﬁrst step in the proof is to show that
Dt −Dt−1 = O
which, although it does not ensure
the convergence of Dt, ensures the convergence of the series P∞
t=1 ||Dt −Dt−1||2
F , a classical condition in gradient descent convergence proofs . In turn,
this reduces to showing that Dt minimizes a parametrized
quadratic function over C with parameters 1
t At and 1
then showing that the solution is uniformly Lipschitz with
respect to these parameters, borrowing some ideas from
perturbation theory . At this
point, and following Bottou , proving the convergence of the sequence ˆft(Dt) amounts to showing that the
stochastic positive process
△= ˆft(Dt) ≥0,
is a quasi-martingale. To do so, denoting by Ft the ﬁltration of the past information, a theorem by Fisk states
that if the positive sum P∞
t=1 E[max(E[ut+1 −ut|Ft], 0)]
converges, then ut is a quasi-martingale which converges
with probability one. Using some results on empirical processes , we obtain a bound that ensures the convergence
of this series. It follows from the convergence of ut that
ft(Dt) −ˆft(Dt) converges to zero with probability one.
Then, a classical theorem from perturbation theory shows that l(x, D)
Online Dictionary Learning for Sparse Coding
is C1. This, allows us to use a last result on empirical
processes ensuring that f(Dt) −ˆft(Dt) converges almost
surely to 0. Therefore f(Dt) converges as well with probability one.
Proposition 2 (convergence to a stationary point). Under assumptions (A) to (C), Dt is asymptotically close to
the set of stationary points of the dictionary learning problem with probability one.
Proof sktech: The ﬁrst step in the proof is to show using
classical analysis tools that, given assumptions (A) to (C),
f is C1 with a Lipschitz gradient. Considering ˜A and ˜B
two accumulation points of 1
t At and 1
t Bt respectively, we
can deﬁne the corresponding surrogate function ˆf∞such
that for all D in C, ˆf∞(D) = Tr(DT D˜A) −Tr(DT ˜B),
and its optimum D∞on C. The next step consists of showing that ∇ˆf∞(D∞) = ∇f(D∞) and that −∇f(D∞) is in
the normal cone of the set C—that is, D∞is a stationary
point of the dictionary learning problem .
5. Experimental Validation
In this section, we present experiments on natural images
to demonstrate the efﬁciency of our method.
5.1. Performance evaluation
For our experiments, we have randomly selected 1.25 ×
106 patches from images in the Berkeley segmentation
dataset , which is a standard image
database; 106 of these are kept for training, and the rest for
testing. We used these patches to create three datasets A,
B, and C with increasing patch and dictionary sizes representing various typical settings in image processing applications:
Signal size m
Nb k of atoms
8 × 8 = 64
12 × 12 × 3 = 432
16 × 16 = 256
We have normalized the patches to have unit ℓ2-norm and
used the regularization parameter λ = 1.2/√m in all of
our experiments. The 1/√m term is a classical normalization factor , and the constant 1.2 has
been experimentally shown to yield reasonable sparsities
(about 10 nonzero coefﬁcients) in these experiments. We
have implemented the proposed algorithm in C++ with a
Matlab interface. All the results presented in this section
use the mini-batch reﬁnement from Section 3.4 since this
has shown empirically to improve speed by a factor of 10
or more. This requires to tune the parameter η, the number
of signals drawn at each iteration. Trying different powers
of 2 for this variable has shown that η = 256 was a good
choice (lowest objective function values on the training set
— empirically, this setting also yields the lowest values on
the test set), but values of 128 and and 512 have given very
similar performances.
Our implementation can be used in both the online setting
it is intended for, and in a regular batch mode where it
uses the entire dataset at each iteration (corresponding to
the mini-batch version with η = n). We have also implemented a ﬁrst-order stochastic gradient descent algorithm
that shares most of its code with our algorithm, except
for the dictionary update step. This setting allows us to
draw meaningful comparisons between our algorithm and
its batch and stochastic gradient alternatives, which would
have been difﬁcult otherwise. For example, comparing our
algorithm to the Matlab implementation of the batch approach from developed by its authors
would have been unfair since our C++ program has a builtin speed advantage. Although our implementation is multithreaded, our experiments have been run for simplicity on a
single-CPU, single-core 2.4Ghz machine. To measure and
compare the performances of the three tested methods, we
have plotted the value of the objective function on the test
set, acting as a surrogate of the expected cost, as a function
of the corresponding training time.
Online vs Batch. Figure 1 (top) compares the online and
batch settings of our implementation. The full training set
consists of 106 samples. The online version of our algorithm draws samples from the entire set, and we have run
its batch version on the full dataset as well as subsets of size
104 and 105 (see ﬁgure). The online setting systematically
outperforms its batch counterpart for every training set size
and desired precision. We use a logarithmic scale for the
computation time, which shows that in many situations, the
difference in performance can be dramatic. Similar experiments have given similar results on smaller datasets.
Comparison with Stochastic Gradient Descent. Our experiments have shown that obtaining good performance
with stochastic gradient descent requires using both the
mini-batch heuristic and carefully choosing the learning
rate ρ. To give the fairest comparison possible, we have
thus optimized these parameters, sampling η values among
powers of 2 (as before) and ρ values among powers of 10.
The combination of values ρ = 104, η = 512 gives the
best results on the training and test data for stochastic gradient descent. Figure 1 (bottom) compares our method with
stochastic gradient descent for different ρ values around
104 and a ﬁxed value of η = 512. We observe that the
larger the value of ρ is, the better the eventual value of the
objective function is after many iterations, but the longer it
will take to achieve a good precision. Although our method
performs better at such high-precision settings for dataset
Online Dictionary Learning for Sparse Coding
Evaluation set A
time (in seconds)
Objective function (test set)
Our method
Batch n=104
Batch n=105
Batch n=106
Evaluation set B
time (in seconds)
Objective function (test set)
Our method
Batch n=104
Batch n=105
Batch n=106
Evaluation set C
time (in seconds)
Objective function (test set)
Our method
Batch n=104
Batch n=105
Batch n=106
Evaluation set A
time (in seconds)
Objective function (test set)
Our method
SG ρ=5.103
SG ρ=2.104
Evaluation set B
time (in seconds)
Objective function (test set)
Our method
SG ρ=5.103
SG ρ=2.104
Evaluation set C
time (in seconds)
Objective function (test set)
Our method
SG ρ=5.103
SG ρ=2.104
Figure 1. Top: Comparison between online and batch learning for various training set sizes. Bottom: Comparison between our method
and stochastic gradient (SG) descent with different learning rates ρ. In both cases, the value of the objective function evaluated on the
test set is reported as a function of computation time on a logarithmic scale. Values of the objective function greater than its initial value
are truncated.
C, it appears that, in general, for a desired precision and a
particular dataset, it is possible to tune the stochastic gradient descent algorithm to achieve a performance similar
to that of our algorithm. Note that both stochastic gradient descent and our method only start decreasing the objective function value after a few iterations. Slightly better
results could be obtained by using smaller gradient steps
during the ﬁrst iterations, using a learning rate of the form
ρ/(t + t0) for the stochastic gradient descent, and initializing A0 = t0I and B0 = t0D0 for the matrices At and Bt,
where t0 is a new parameter.
5.2. Application to Inpainting
Our last experiment demonstrates that our algorithm can
be used for a difﬁcult large-scale image processing task,
namely, removing the text (inpainting) from the damaged
12-Megapixel image of Figure 2. Using a multi-threaded
version of our implementation, we have learned a dictionary with 256 elements from the roughly 7 × 106 undamaged 12×12 color patches in the image with two epochs in
about 500 seconds on a 2.4GHz machine with eight cores.
Once the dictionary has been learned, the text is removed
using the sparse coding technique for inpainting of Mairal
et al. . Our intent here is of course not to evaluate
our learning procedure in inpainting tasks, which would require a thorough comparison with state-the-art techniques
on standard datasets. Instead, we just wish to demonstrate
that the proposed method can indeed be applied to a realistic, non-trivial image processing task on a large image. Indeed, to the best of our knowledge, this is the ﬁrst
time that dictionary learning is used for image restoration
on such large-scale data. For comparison, the dictionaries
used for inpainting in the state-of-the-art method of Mairal
et al. are learned (in batch mode) on only 200,000
6. Discussion
We have introduced in this paper a new stochastic online algorithm for learning dictionaries adapted to sparse coding
tasks, and proven its convergence. Preliminary experiments
demonstrate that it is signiﬁcantly faster than batch alternatives on large datasets that may contain millions of training
examples, yet it does not require learning rate tuning like
regular stochastic gradient descent methods. More experiments are of course needed to better assess the promise
of this approach in image restoration tasks such as denoising, deblurring, and inpainting. Beyond this, we plan to
use the proposed learning framework for sparse coding in
computationally demanding video restoration tasks , with dynamic datasets whose size is not
ﬁxed, and also plan to extend this framework to different
loss functions to address discriminative tasks such as image classiﬁcation , which are more
sensitive to overﬁtting than reconstructive ones, and various matrix factorization tasks, such as non-negative matrix
factorization with sparseness constraints.
Online Dictionary Learning for Sparse Coding
Figure 2. Inpainting example on a 12-Megapixel image.
Damaged and restored images. Bottom: Zooming on the damaged and restored images. (Best seen in color)
Acknowledgments
This paper was supported in part by ANR under grant
MGA. The work of Guillermo Sapiro is partially supported
by ONR, NGA, NSF, ARO, and DARPA.