Author’s personal copy
High Accuracy Android Malware Detection Using
Ensemble Learning
Suleiman Y. Yerima, Sakir Sezer, Igor Muttik
Full article information:
S. Y. Yerima, S. Sezer, I. Muttik. “High Accuracy Android Malware Detection Using Ensemble Learning” IET
Information Security, Available online: April 29 2015, Online ISSN 1751-8717. DOI: 10.1049/iet-ifs.2014.0099
Author’s personal copy
High Accuracy Android Malware Detection Using Ensemble Learning
Suleiman Y. Yerima, Sakir Sezer,
Centre for Secure Information Technologies
Queen’s University Belfast,
Belfast, Northern Ireland, United Kingdom
E-mail: {s.yerima, s.sezer, g.mcwilliams}@qub.ac.uk
Igor Muttik
Senior Principal Research Architect
McAfee Labs (Part of Intel Security)
London, United Kingdom
E-mail: 
Abstract— With over 50 billion downloads and more than 1.3
million apps in Google’s official market, Android has
continued to gain popularity amongst smartphone users
worldwide. At the same time there has been a rise in malware
targeting the platform, with more recent strains employing
highly sophisticated detection avoidance techniques. As
traditional signature based methods become less potent in
detecting unknown malware, alternatives are needed for timely
zero-day discovery. Thus this paper proposes an approach that
utilizes ensemble learning for Android malware detection. It
combines advantages of static analysis with the efficiency and
performance of ensemble machine learning to improve
Android malware detection accuracy. The machine learning
models are built using a large repository of malware samples
Experimental results and analysis presented shows that the
proposed method which uses a large feature space to leverage
the power of ensemble learning is capable of 97.3 % to 99%
detection accuracy with very low false positive rates.
Keywords- mobile security; Android; malware detection;
ensemble learning; static analysis; machine learning; data
mining; random forest
INTRODUCTION
There has been a dramatic increase in Android malware
since the first SMS Trojan was discovered in the wild by
Kaspersky in August 2010. Since then, Android malware
have been evolving, becoming more sophisticated in
avoiding detection. As the study in revealed, recent
malware families exhibit polymorphic behavior, malicious
payload encryption, increased code obfuscation, stealthy
command and control communications channels, dynamic
runtime loading of malicious payload, etc. Not only do these
anti-analysis techniques present problems for traditional
signature-based detection, but also significantly increase the
effort in uncovering malicious behavior and code within the
Android applications.
Due to difficulty in spotting malicious behavior, Android
malware could remain unnoticed for up to three months on
average . Moreover, most antivirus detection capabilities
depend on the existence of an updated malware signature
repository, therefore the antivirus users are at risk whenever
previously un-encountered malware is spread. Since the
response time of antivirus vendors may vary between several
hours to several days to identify malware, generate a
signature, and update their client’s signature database,
hackers have a substantial window of opportunity .
Oberheide et al. also observed that it took on average 48 days
for a signature based antivirus engine to become capable of
detecting new threats . Although Google introduced
Bouncer to its app store in order to screen submitted apps for
malicious behavior, it has been shown to be vulnerable to
detection avoidance by well-crafted malicious apps .
Clearly, there is a need for improved detection
approaches given the evolution of Android malware and the
urgency to narrow the window of opportunity for the threats
posed by emergence of previously unseen strains. Hence,
this paper proposes and investigates an effective approach
that exploits the merits of static analysis and ensemble
machine learning in order to enable zero-day Android
malware detection with high accuracy. Different from
existing work on Android malware, this paper proposes,
develops, and investigates an extensive feature based
approach that applies ensemble learning to the problem. The
main contributions of the paper are:
A new high accuracy static analysis driven Android
malware detection scheme is proposed, investigated and
developed based on ensemble machine learning.
A large feature space approach is developed
employing 179 features for classification decisions. The
features are from diverse categories: API calls, commands
and permissions, thus inherently resilient to obfuscation
within app code as majority of the features will still be
accurately extractable. Furthermore, a large malware sample
repository is used, which when combined with an extensive
features set allows the power of ensemble learning to be fully
exploited.
Presents extensive empirical evaluation studies
based on real malware and benign samples from a leading
antivirus vendor’s repositories giving insights on the efficacy
of the proposed scheme.
Our approach is beneficial in several scenarios: from
filtering apps to uncover new malicious apps in the wild;
prioritizing apps for further (more expensive) analysis;
policing app markets; verification of new apps prior to
installation, etc. The rest of the paper is organized as
follows. In section 2, a survey of related work is presented,
followed by the feature extraction process in section 3. Next,
ensemble machine learning is discussed in Section 4. Section
5 presents methodology and experiments undertaken while
empirical results are presented in section 6. The paper is then
concluded in section 7.
Author’s personal copy
2. RELATED WORK
Several works such as - apply static analysis for
detection of Android malware. Grace et al. proposed
RiskRanker for automated risk assessment and app
profiling in order to police Android markets. Wei et al. also
proposed a profiling tool for Android apps called
ProfileDroid . This provides a multi-layer monitoring and
profiling system to characterize Android app behaviour in
several layers. In , Batyuk et al. proposed using static
analysis for identifying security and privacy threats.
AndroidLeaks , SCANDAL , and the approach
presented in are frameworks that detect privacy
information leakage based on static analysis. Furthermore, in
 , the Android Application Sandbox (AAS) is proposed
by Blasing et al. AAS uses both static and dynamic analysis,
where the static analysis part is based on matching 5
different patterns from decompiled code. Static analysis also
provides the basis for the heuristic engine proposed in for
detecting Android malware using 39 different flags. A risk
score is calculated from the heuristics in order to prioritize
the most likely malicious samples for analysis.
Other previous works utilizing static analysis include
Comdroid and DroidChecker . Different from ,
and - , the approach in this paper leverages automated
static analysis for high accuracy and robust learning based
malware detection.
Machine learning approaches to malware detection have
been previously studied on PC platforms - .
Investigation of machine learning based detection for
Android platform is gaining attention recently with the
growing availability of malware samples. Related work that
apply machine learning with static analysis to detect Android
malware can be found in - for instance.
In , Bayesian classification was applied to categorize
apps into ‘benign’ or ‘suspicious’ using 58 code-based
feature attributes. The training and classification employed
1000 Android malware samples from 49 families and 1000
benign applications. The approach in utilized
permissions and call flow graphs for training SVM models to
distinguish between benign and malicious Android apps. The
authors derived one-class SVM models based on the benign
samples alone and use these for identification of both benign
and malicious apps. In , Sanz et al compared various
machine learning schemes trained with permission features
on their malware detection accuracy. Their analysis is based
on 249 malware samples and 347 benign apps. Sarma et al.
 and Peng et al. also apply permissions to train
SVM based and Bayesian based models respectively for risk
ranking of Android apps. D. –J. Wu et al. proposed
DroidMat in , where Android malware is detected using
k-means clustering after computing the required number of
clusters by Singular Value Decomposition (SVD) approach.
They present experimental results based on 238 Android
samples from 34 families together with 1500 benign apps.
Our work differs from - , as our static analysis driven
approach leverages ensemble learning driven by a more
extensive feature set comprising 179 feature attributes (from
API calls, commands, and permissions). Additionally, our
study utilizes a larger malware dataset than the previous
In , the authors also apply machine learning with
static analysis, but utilize Linux malware rather than Android
malware samples. Their approach extracts Linux system
commands within Android and use the readelf command to
output a list of referenced function calls for each system
command. Some proposed Android malware detection
methods are based on dynamic analysis. Shabtai et al. ,
 proposed a host-based solution that continuously
monitors various features and events like CPU consumption,
number of packets sent, number of running processes,
keyboard/touch-screen pressing etc. Machine learning
anomaly detectors are then applied to classify the collected
data into normal or abnormal. M. Zhao et al. propose
AntiMalDroid in . AntiMalDroid is a software behavior
signature based malware detection framework that uses SVM
to detect malware and their variants in runtime and extend
malware characteristics database dynamically. Logged
behaviour sequence is used as the feature for model training
and detection. Crowdroid is proposed as a behaviorbased malware detection system that traces system calls
behavior, converts them into feature vectors and applies kmeans algorithm to detect malware. Enck et al. perform
dynamic taint tracking of data in order to reveal to a user
when an app is trying to exfiltrate sensitive data.
3. APPLICATION FEATURE EXTRACTION
In order to obtain the features used in the machine
learning based detection, an extended version of our Java
based APK analysis tool described in was used. As
shown in Figure 1, the tool is enhanced with database storage
of feature vectors extracted from app corpus to drive the
training phase. The tool, which embeds a Baksmali
disassembler , can also classify unknown apps using
trained models.
The feature extraction is accomplished using feature
detectors that extract 65 features comprising critical
(Android and Java) API calls and commands as described in
 . These make up the ‘applications attributes’ feature set.
A further 130 features are extracted using permissions
detector that mines the ‘Manifest file’ to detect the app
permissions.
3.1 Proposed feature set
For the purpose of our research, we developed 65
features (54 of which were subsequently used) from API
calls and (Linux/Android) command sets. The APIs include
(SMS manager APIs (for sending, receiving, reading SMS
messages etc.); Telephony manager APIs (for accessing
device ID, subscriber ID, network operator, SIM serial
number, line number etc.); Package manager APIs (for
listing installed packages, installing additional packages
Author’s personal copy
Disassemble each
.dex file to .smali files
Decompress the
.apk files into folders
Convert Manifest file
to readable format
Extract manifest files
from the folders
Extract the Dalvik
executable (.dex) files
Define code based
input feature
attributes for
Mine folder files to
extract code based
Database storage for
feature vectors
Mine manifest files to
extract features
Train machine learning
New sample for
classification
Classification outcome
Define manifest based
input feature
attributes for
Figure 1: Custom built APK analyzer for feature extraction and classification to identify malicious Android applications.
The API calls feature extraction also includes detection
(within the disassembled .smali code) of Java API calls for
encryption, reflection, JNI usage, dynamic class loading,
creation of new processes, and runtime execution of
processes.
Features were also derived from specific Linux
commands e.g. shell commands like ‘chmod’, ‘chown’,
‘mount’, and specific strings like ‘/sys/bin/sh’ etc. which
enable malware to escalate privilege, root devices, or execute
malicious shell scripts at run time. Other type of commands
defined as features include Android commands like ‘pm
install’ which can be used for stealthy installation of
additional malicious packages. All APK subfolders are also
extracting
Permissions extracted from the Manifest file provided
additional set of features. Although all of the 130 standard
permissions were included, only 125 were subsequently used
(see section 5). An extensive and diverse set of features
were chosen for the following reasons:
Robustness: each set of features are extracted from
different parts of the APK (API calls from dex executable,
permissions from the Manifest file, and commands are
mostly detected outside of the app’s executable). Should
feature extraction from dex executable fail for example,
the permissions would still be obtainable from the
Manifest file. Furthermore, if for instance, malware
incorporates encryption to prevent detection of commands,
API call features (including the crypto API) will still be
detectable along with permissions to expose the app’s
malicious intent. This measure of resilience is enabled by
the fact that all of the features will be utilized in the
ensemble learning approach unlike with other algorithms
e.g. Naïve Bayes were feature ranking and selection are
necessary for optimum performance. Our approach is
contrary to most existing work where ranking and
selection steps are used to reduce the feature set. In that
regard, such existing approaches will have a less resilient
feature set compared to ours.
Performance: the diversity and extent of the
features employed is actually advantageous to ensemble
learning as it provides greater degree of freedom to
introduce randomness in feature selection. Additionally, by
employing a large malware repository in our work, both
instances provide randomization
opportunities
performance as explained further in section 4.
4. ENSEMBLE MACHINE LEARNING
Ensemble learning builds a prediction model by
combining the strengths of a collection of simpler base
models . For our classification problem the application
of Random Forest, an ensemble learning method, is proposed
to learn the app characteristics based on all of the 179
learning-based
approaches that advocate a pre-training stage of ‘ranking
and feature reduction’ for improved performance) . The
model can then be used in classifying new Android
applications into suspicious or benign. Random Forest
combines random Decision Trees with Bagging to achieve
Author’s personal copy
very high classification accuracy . This paper focuses on
investigating how the power of ensemble learning can be
applied to improve Android malware detection. By means of
datasets composed of extracted features from a large
repository of malware and benign apps, Random Forest
classification
investigated
experimental scenarios. Furthermore, comparative analysis is
made to Naïve Bayes, Decision Trees, Random Trees, and
Simple Logistic (another ensemble learning technique based
on boosting).
Decision Trees (DT) are sequential models, which
logically combine a sequence of tests that compare a numeric
attribute against a threshold value or a nominal attribute
against a set of possible values . DT algorithms select the
next best feature during splits by employing information
entropy or Gini impurity respectively given by (1) and (2):
where fi is the fraction of items labelled with value i from
m categories and GI is known as the Gini Index.
The Random Tree algorithm departs from the
tradition DT method by testing a given number of random
features at each node of the tree and performs no pruning.
Random Forest uses Bagging (bootstrap aggregation) to
produce a diverse ensemble of Random Trees independently
trained on distinct bootstrap samples obtained by random
sampling the given set N’ <= N times with replacement.
4.1 Random forest algorithm
Random Forest applies Bagging to generate a diverse
ensemble of classifiers by introducing randomness into the
learning algorithms input . Diversity is also achieved by
random feature subset selection during node splitting .
Hence, our classification scenario where we apply all 179
features could benefit from these two dimensions of
randomness to improve accuracy. The Random Forest
algorithm is summarized in Figure 2. The training variables
are the number of trees T and the number of features m from
the input feature space to be randomly selected at each split
of the base tree construction.
Random Forest has several advantages that can be
leveraged for improved machine learning based detection: no
special preprocessing of input is required; can deal with
large numbers of training instances, missing values,
irrelevant features, etc. More importantly training and
prediction phases are both fast, and they are more amenable
to parallelization than Boosting-based ensemble learners (e.g.
Simple Logistic).
Random forest algorithm
for b = 1 to B:
Draw a bootstrap sample Z* of size N from the training data.
(b) Grow a random-forest tree Tb to the bootstrapped data, by
recursively repeating the following steps from each terminal
node of the tree, until the minimum node size nmin is reached:
Select m variables at random from the p variables.
ii. Pick the best variable/ split-point among the m. (e.g.
using Gini index)
iii. Split the node into two daughter nodes.
Output the ensemble of trees
To make a prediction at a new point x:
be the class prediction of the bth random-forest tree.
= majority vote
Figure 2: The Random Forest algorithm.
4.2 Simple logistic algorithm
Simple Logistic is an ensemble learning method based on
‘Boosting’. Simple Logistic utilizes additive logistic
regression using simple regression functions as base
learners. The base learner in Simple Logistic is a regression
model based on a single attribute (feature), i.e. the one that
maximizes the log-likelihood:
)log(1 Pr[1|
where m is the number of features.
Logistic regression models are built and fitted using
LogitBoost which performs additive logistic regression.
Cross-validation is used to determine the number of
iterations to perform which also supports automatic
attribute/feature selection . The additive logistic
regression algorithm is summarized in Figure 3.
Author’s personal copy
Additive logistic regression
Model generation
for j = 1 to t iterations do:
for each feature vector f[i] do:
1: set the target value for the regression to
c[i] = (y[i] – P(1| f[i]))/ [P(1|f[i]) * (1 – P(1| f[i])]
2: set the weight w[i] of instance f[i] to :
P(1| f[i]) * (1-P(1| f[i])
3: fit a regression model r[j] to the data with
class values c[i] and weights w[i]
Classification
Predict class 1 if P(1| f) > 0.5, otherwise predict class 0
Figure 3: Algorithm for binary additive logistic regression.
5. METHODOLOGY AND EXPERIMENTS
The custom built APK analyzer shown in Figure 1 was
utilized in extracting the selected features from a collection
of benign apps and recent malware samples. The analyzer
was run across a total of 6,863 applications (obtained from
McAfee’s internal repository); 2925 of these were malware
while 3938 apps were benign. The extracted features were
converted into binary feature vectors with a 0 or 1 indicating
the absence/presence of a feature. These were stored in a
MySQL database for the training and testing phases.
Out of the initial 65 non-permission based features 54 of
these were selected to produce feature vectors for the
training phase. The eliminated features were those with no
occurrences in either class. Out of the 130 permissions
features, 5 had no occurrence in either class i.e.:
ADD_VOICEMAIL, SET_POINTER_SPEED, USE_SIP,
WRITE_PROFILE, WRITE_SOCIAL_STREAM. Hence
the experiments were based on the remaining 125 permission
features and 54 API and command based features yielding a
total of 179 training features.
In order to investigate the effect of feature diversity, three
separate feature sets were created for training models and
comparative analysis: (a) Feature set consisting of vectors
from the 54 application attribute features only (b) Feature set
of vectors from the 125 permissions only (c) Feature set
consisting of vectors from a mix of all the (diverse) 179
property vectors (Table 1).
Table 1: Feature sets for model building
Feature set
Number of features
App attributes features (AF)
Permission features (PF)
attributes
Permission features (CAPF)
The learning algorithms were investigated with the three
feature sets in order to evaluate their classification
performance using the following evaluation metrics.
Accuracy and Error Rate are respectively given by:
The false positive rate (FPR), false negative rate (FNR),
true positive rate (TPR), true negative rate (TNR) and
precision (р) are defined as follows:
6. RESULTS AND DISCUSSIONS
Results of the experiments undertaken are discussed in
this section. Note that all results are obtained using 10-fold
cross validation where training and testing sets are different.
6.1 Naïve Bayes results
Results from the Naïve Bayes algorithm are presented in
Table 2 showing performance of PF (permissions feature set
with 125 features), AF (application attributes feature set with
54 features) and several configurations of the CAPF
(combined feature sets with 179 features): i.e. top 10, 15, 20,
and 25 features ranked using Mutual Information feature
selection .
With Naïve Bayes, the best detection rate of 85.4% was
obtained with the top 10 features from the CAPF set. This is
depicted in the Bayes (10) CAPF row of Table 2. The top 20
mixed features from MI ranking together with their
Author’s personal copy
frequency of appearance over the entire 6, 863 labeled app
instances are shown in Table 3. They include SEND_SMS,
RECEIVE_SMS,
permissions feature set, and also ‘remount’, ‘/system/app’,
‘chown’, ‘createSubprocess’, which are from the app
attributes feature set. The results in Table 2 shows that with
the Naïve Bayes algorithm, for best detection rates, the
CAPF feature set should be utilized. Note that all the results
of the experiments were obtained using 10-fold cross
validation.
6.2 Simple Logistic, Decision Tree and Random Tree results
Simple Logistic results are presented in Table 4, while
those of Decision Tree and Random Trees are given in
Tables 5 and 6 respectively. In terms of comparative
performance with corresponding feature sets, Simple
Logistic performed better than Bayesian. When trained with
the 179 mixed features (CAPF), a detection rate of
approximately 91% is achieved with 4.6% false positive rate.
With Decision Tree classification, TPR of 94.8% and
corresponding FPR of 4% were obtained using the CAPF set.
As with the Simple Logistics and Naïve Bayes, the CAPF
Decision Tree classifier enables better detection rates than
either PF or AF based Decision Tree classifiers. On the other
hand, the false positive rates for CAPF and AF feature sets
were similar, but also better than that of PF. Overall AUC
performance is good for both AF feature set and CAPF
feature set Decision Trees. Therefore, an FPR-TPR trade off
to improve detection rates is feasible (given the low FPRs of
3.9 % and 4 % respectively).
Table 2: Results from Naïve Bayes classifiers
Feature set
Bayes (179)
Bayes (25)
Bayes (20)
Bayes (10)
Bayes (54)
Bayes (125)
Table 3: Top 20 CAPF features ranked using Mutual Information.
Information gain score
RECEIVE_SMS
/system/app
createSubprocess
/system/bin/sh
abortBroadcast
READ_PHONE_STATE
TelephonyManager
TelephonyManager
_getSubscriberId
Ljava_net_URLDecoder
ACCESS_NETWORK_STATE
RESTART_PACKAGES
CHANGE_WIFI_STATE
Ljavax_crypto_spec_SecretKeySpec
Author’s personal copy
Table 4: Results from Simple logistic classifiers
Feature set
0.801 0.924 0.076 0.199 0.863 0.138 0.886 0.925
0.835 0.943 0.057 0.165 0.889 0.111 0.916 0.938
0.909 0.954 0.046 0.091 0.932 0.069 0.937 0.977
Table 5: Results from Decision tree classifiers
Feature set
0.938 0.062
0.904 0.096 0.912 0.934
0.939 0.961 0.039 0.061 0.950 0.050 0.948 0.967
0.948 0.960
0.052 0.954 0.046 0.946 0.964
The results from Random Tree classifier under different
parameter settings are shown in Table 6. The number of
random variables selection at each split during tree
construction (i.e. k), is varied from log2f+1, to 20 and 50
respectively. For each k configuration, the PF, AF and CAPF
feature sets were applied to train and evaluate the Random
Tree classifiers.
Table 6: Results from Random tree classifiers
Feature set
(k=log2f+1)
0.901 0.928 0.072 0.099 0.915 0.085 0.903 0.934
0.946 0.949 0.051 0.054 0.948 0.053 0.933 0.952
0.955 0.952 0.048 0.045 0.954 0.047 0.936 0.954
Feature set
0.902 0.922 0.078 0.098 0.912 0.088 0.895 0.933
0.052 0.949 0.051 0.934 0.955
0.956 0.044
0.958 0.042 0.942 0.960
Feature set
0.898 0.928 0.072 0.102 0.913 0.087 0.902 0.933
0.951 0.951 0.049 0.049 0.951 0.049 0.935 0.958
0.961 0.956 0.044 0.039 0.959 0.041 0.942 0.960
6.3 Random Forest results
For the Random forest algorithms, two parameters
needed for training the models are the number of trees in the
ensemble, T, and the number of random variables selection
at each split during tree construction, k. As with the Random
Tree classifier, k is varied from log2f +1, to 20 and 50
respectively; and for each k configuration, the three feature
sets were compared. The results for T = 10 trees (from 10fold cross-validation) are given in Table 7.
Table 7: Results from Random forest classifiers.
Feature set
(k=log2f+1)
0.901 0.949 0.051 0.099 0.925 0.075 0.929 0.966
0.954 0.971 0.029 0.046 0.963 0.037 0.961 0.987
0.971 0.977 0.023 0.029 0.974 0.026 0.969 0.992
Feature set
0.898 0.944 0.056 0.102 0.921 0.079 0.922 0.969
0.956 0.969 0.031 0.044 0.963 0.037 0.958 0.987
0.972 0.975 0.025 0.028 0.974 0.026 0.967 0.993
Feature set
0.099 0.921 0.079 0.918 0.966
0.955 0.961 0.039 0.045 0.958 0.042 0.948 0.986
0.973 0.977 0.023 0.027 0.975 0.025 0.969 0.993
As with the Random Tree, the results of the Random
Forest classification are not very sensitive to variation in k.
However, the best detection rates, lowest false positive rates
and largest AUC occur simultaneously with the CAPF
feature set. This not only confirms the robustness of Random
Forest to large number of input training features, but also
justifies our large (179) feature set approach which yields
high fidelity malware detection via improved accuracy. Also,
the number of trees T had a negligible impact on
performance when increased from 10 to 50 (for all the
different values of k) and are therefore omitted.
True positive rate
False positive rate
ROC curve for RF 10 trees
99 % detetion rate at 10% FPR
Figure 4: ROC curve for Random Forest.
From Table 7, a very high classification accuracy of
97.6% is observed in the k = 50 configuration. Malware
detection rates (TPR) are 97.2% and 97.3 % with 2.5% and
2.3% false positive rates for k=20 and k=50 respectively.
These results outperform all the previously discussed
algorithms. Also, Figure 4 illustrates the AUC for the
Random Forest classifier of mixed features for k= 50. It can
Author’s personal copy
be seen that with ROC area of 0.993, malware detection rate
can be improved to 98.6% for 6.3% false positive rate and
99% for 10% false positive rate. These higher TPR
operating points will suffice for some application scenarios
e.g. filtering apps to prioritize resource allocation for
further/manual analysis.
Figures 5 and 6 show graphical comparisons of the
different classifier performances (with CAPF features set).
Not only does the Random Forest learner perform very well
with our mixed features dataset, but also model building time
was quite fast. Random Forest (10 trees) learning with k= 8,
20 and 50 respectively take 1.48s, 2.96s and 6.41 seconds to
build. With Decision Tree classifiers, the requirement for
pruning increases the model build time over Random Trees.
Simple Logistic learning took longest to build in 81.9
seconds. This is due to the additive nature of the underlying
Boosting algorithm which incrementally builds the model
based on previous base learners.
Figure 5: Detection rates for different classifiers.
Figure 6: Area under ROC curve for different classifiers.
6.4 Results comparison with existing work
In order to highlight the significance of our results, a
comparison is made with those published in other static
analysis based works where a quantitative comparison is
possible using similar metrics. Table 8 shows how the results
in this paper measures against the best results of , ,
 , , , , and respectively (using the
available metrics). These comparative results show that our
approach in this paper outperforms previous similar efforts.
This is highlighted in the bottom of the table where the result
of our proposed approach that leverages the power of
ensemble learning clearly shows the highest detection
accuracy and the best AUC performance.
Table 8: Result comparison with existing work.
Related work
Yerima et al.
0.906 0.932 0.068
0.082 0.974
Kirin 
Zhou et al.
Peng et. al
Sarma et. al
Yerima et. al.
0.909 0.949 0.051
0.069 0.977
(T=10, k=50)
0.973 0.977 0.023
0.025 0.993
7. CONCLUSION
This paper presented a new ensemble learning based
Android malware detection approach which can effectively
improve detection rates to 97-99% with low false positives
by harnessing large mixed feature sets in ways infeasible
with traditional machine learning. With this approach there is
no requirement for feature selection step to eliminate ‘less
relevant’ features. This use of an extensive mixed feature set
provides robustness and resilience to code obfuscation and
other anti-analysis techniques being employed by malware
authors vastly improving the chances of prompt zero-day
malware detection. Experiments performed with large
malware dataset from a leading AV vendor demonstrate the
effectiveness of the proposed scheme and the higher fidelity
achievable compared to traditional approaches.