Multi-label ensemble based on variable pairwise constraint projection
Ping Li*a, b, Hong Lib, Min Wub
aCollege of Computer Science, Zhejiang University, Hangzhou 310027, China
bSchool of Information Science and Engineering, Central South University, Changsha 410083, China
Multi-label classification has attracted an increasing amount of attention in recent years. To this end,
many algorithms have been developed to classify multi-label data in an effective manner. However,
they usually do not consider the pairwise relations indicated by sample labels, which actually play
important roles in multi-label classification. Inspired by this, we naturally extend the traditional
pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to
improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a
multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a
novel multi-label classification framework named Variable Pairwise Constraint projection for
Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint
projection to learn a lower-dimensional data representation, which preserves the correlations between
samples and labels. Thereafter, the base classifiers are trained in the new data space. For the
boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to
diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in
comparison with other approaches.
Keywords: Multi-label classification; Ensemble learning; Variable pairwise constraints; Boosting;
Constraint projection
1 Introduction
Traditional supervised learning deals with the data sample associated with only single label, which
indicates its class membership. However, in a broad range of real-world applications, one sample is
usually related to multiple class labels simultaneously, and such data has appeared in a large volume of
various domains, e.g., scene or sentiment classification, video annotation and functional genomics . To address this problem, multi-label learning has received a growing interest in the last
decade . Compared to single-label learning, multi-label learning is more challenging as
each sample contains more than one label, e.g., a document concerning health might also be related to
other topics, such as education, entertainment and government . Typical multi-label learning
methods include multi-label neural networks, multi-label transfer learning and multi-label kernel
learning , which are fundamentally derived from the corresponding single-label learning
approaches. For more details, we refer the readers to .
In this paper, we study supervised multi-label classification. During the past few years, many
multi-label classification methods have been proposed in a number of works , whereas
they often neglect the pairwise relations suggested by sample labels. Such relations can indicate
whether a given sample pair has similar label sets or not, which are of vital importance in multi-label
classification. Besides, the pairwise label constraints have been proved to be effective in single-label
* Corresponding author.
E-mail address: (P. Li)
learning . Therefore, we naturally extend the traditional pairwise constraints to the multi-label
scenario via a flexible thresholding strategy. On the other hand, ensemble learning which combines
multiple base learners to jointly accomplish one common task is shown to be very beneficial for
enhancing the generalization ability of a single classifier . Moreover,
ensemble learning is able to handle the imbalanced data well and discourage the over-fitting incurred
by singular problems . Inspired by this, we adopt a boosting-like strategy to construct a
multi-label ensemble, so as to further improve the generalization ability of the classifier. To achieve
these goals, herein we propose a novel multi-label classification framework named Variable Pairwise
Constraint projection for Multi-label Ensemble (VPCME) to deal with multi-label data. As a matter of
fact, our framework can be mildly decomposed into two components, i.e., the variable pairwise
constraint projection and the boosting-like process.
The main contributions of this work are highlighted as follows.
We present a novel multi-label classification framework to construct a multi-label ensemble.
It takes advantage of the variable pairwise constraint projection to obtain a well preserved
low-dimensional data representation, where the base classifiers are learned.
A boosting-like strategy is utilized to obtain a group of diversified base classifiers, whose
diversities are essentially encouraged by both the variable pairwise constraints and the
bootstrap steps. The attained base classifiers are combined to construct a multi-label
ensemble for final classification.
Extensive experiments were conducted on various real-life datasets. Results demonstrate that
our framework enjoys significant advantages compared to other methods.
The remainder of the paper is structured as follows. We briefly review the multi-label
classification and ensemble learning in Section 2. We introduce our multi-label classification
framework as well as some rigorous analysis in Section 3. Experimental results on a broad range of
real-world multi-label datasets are reported in Section 4. Finally, we provide the concluding remarks
and discuss future work in Section 5.
2 Related works
Our work focuses on multi-label classification and the proposed framework is closely related to
ensemble learning. So in this section, we provide a brief description about multi-label classification and
ensemble learning.
2.1 Multi-label classification
Multi-label classification has received much attention in the past few years since it is practically
relevant and interesting from a theoretical view . Traditional supervised classification
concentrates on single-label data associated with only one label. When it is referred to binary
classification, the cardinality of the discrete label set |L| equals 2. When |L| scales to more than two, it is
treated as a multi-class problem. However, in many real-world applications, a large number of
instances are usually correlated with multiple labels, which is called multi-label problem .
Since problems of this type are ubiquitous, a great number of multi-label classification methods are
continuously developed to deal with multi-label data from a broad range of areas.
In general, the existing multi-label classification techniques can be divided into two groups:
problem transformation and algorithm adaptation. The former aims to transform multi-label problem
into several single-label problems, e.g., Binary Relevance (BR) and Label Power-set (LP) . The
latter attempts to generalize existing single-label classification algorithms to multi-label cases, e.g.,
Boostexter . BR learns q binary classifiers and one for each different label in L. LP regards each
unique set of labels as one class for a new single-label classification task. Zhang et al. proposed the
Multi-Label K-Nearest Neighbor (MLKNN) method , which implements the single classifier
through the combination of KNN and Bayesian inference. Actually, it has been applied to many
practical tasks due to its promising results as well as its simplicity.
Since multi-label data contains multiple labels, it is important to explore the interdependencies
among labels and samples. For example, Cheng et al. come up with an approach based on a unified
framework called IBLR-ML, which combines the model-based and similarity-based inference for
multi-label classification . In particular, IBLR regards the information from instances similar to the
query as one additional feature, and treats the instance-based classification as the logistic regression.
FÃ¼rnkranz et al. put forward a Calibrated Label Ranking (CLR) method for multi-label classification
 , which considers the calibrated scenario and extends the common learning using the pairwise
comparison to the multi-label case. Besides, it makes use of an artificial calibration label to
discriminate the relevant labels from the irrelevant ones for each sample. Recently, Chen et al.
attempted to discover the multi-label temporal patterns in the sequence database and they treated events
as multi-label ones with many states . Furthermore, Han et al. presented a general sparse
representation framework for multi-label transfer learning . This approach learns a multi-label
encoded sparse linear embedding space from a relevant dataset and then maps the target data into the
new data space. In addition, to explore the tag account and the correlation information jointly, Lo et al.
modeled the audio tagging task as a cost-sensitive multi-label learning problem .
2.2 Ensemble learning
Ensemble learning has established itself as a powerful learning paradigm in machine learning
community, and received lots of interests in the last decade . In this paradigm, a
group of base learners are combined to construct an ensemble for one target problem. It is widely
accepted that combining multiple classifiers should improve the generalization ability of the system
under the assumption that the base learners are as accurate and diverse as possible . Since identical
base classifiers will not provide any additional information, it is desirable to diversify them in several
alternative manners, e.g., sub-resampling training data, feature subsets selection, adding stochastic
factors to the learning algorithm . Roughly speaking, the mainstream ensemble methods
include bagging , boosting (e.g., AdaBoost) , random forest , random subspace 
and rotation forest .
Bagging generates replicated training sets by sampling with replacement, and combines their
classification results . Boosting is more complex in the sense that the distribution of the training data
is changing with the sequentially constructed classifiers, and it emphasizes more on the misclassified
instances . For random subspace, base classifiers are constructed in random subspaces . In
comparison, bagging can reduce the variation of the learner, which is more suitable for the decision
tree and the neural networks; boosting is able to reduce the variation and bias at the same time, which
is more appropriate for weak learners. In addition, Rotation forest is based on feature extraction, where
features are randomly split into K subsets and then Principle Component Analysis (PCA) is used to
reduce the dimension . Thus, K axis rotations represent new features for base classifiers.
Besides the above classical methods, other ensemble methods such as decision tree ensemble ,
active ensemble , dynamic ensemble selection , feature sets ensemble and artificial neural
network ensemble , all targets on the single-label problem. Compare to them, multi-label ensemble
is more complicated since it takes into account correlations among multiple labels. To this end, there
are only a few works concerning this topic. For instance, Zhang et al. presented an ensemble method
based on twin-SVM for multi-class and multi-label text categorization . Tsoumakas et al. proposed
a RAndom K-labEL sets (RAKEL) algorithm to construct individual ensemble learners , which
draws a random subset of size k from all labels and the member of each ensemble constructs a Label
Power-set classifier using a small random subset. Furthermore, Read et al. put forward a Pruned Sets
method that only focuses on the most important label correlations and proposed the Classifier
Chains (CChains) to model label correlations with the tolerable computational complexity .
3 The VPCME method
In this section, we introduce our proposed VPCME framework. First, we give an explicit definition
about the variable pairwise constraints. Second, the two inherent components of the multi-label
classification framework are described, i.e., the variable pairwise constraint projection and the
boosting-like strategy. Finally, we summarize our algorithm.
3.1 Variable pairwise constraints
Pairwise constraints are often used to characterize the relations among labels in many tasks, such
as semi-supervised clustering . Traditional pairwise constraints are exclusively utilized for
single-label problem and none works have applied this concept to multi-label problem to the best of our
knowledge. In this work, we aim to explore the multi-label pairwise constraints to discover the latent
relations among multiple labels. To achieve this goal, we propose a new concept called variable
pairwise constraints, which is an extension of traditional ones. Details are shown as follows.
Recall that traditional pairwise constraints in single-label problem are generally defined as
follows: If a sample pair shares the identical label, we impose the must-link constraint on them;
otherwise impose the cannot-link constraint on them. Such constraints are called fixed pairwise
constraints. However, when it comes to multi-label problem, fixed constraints become improper in the
sense that too strict constraints will lead to serious unbalances between the must-link and cannot-link
constraints. Worse still, once constraints are fixed, it might be impossible to further diversify the base
classifiers. As a matter of fact, these shortcomings would bring about many inconveniences with regard
to the succeeding tasks, e.g., constraint projection and multi-label classification. Therefore, it is very
natural for us to extend the traditional pairwise constraint to multi-label scenarios, i.e., the
variable pairwise constraints. Since multi-label data are related to multiple labels, we give an
intuitive definition that if the percentage of the same labels in a sample pair is larger than or equal to
some threshold, then this sample pair will be imposed the must-link constraint; otherwise the
cannot-link constraint. In reality, sample pairs with must-link constraints form a must-link set and
sample pairs with cannot-link constraints form a cannot-link set.
Hopefully, the performance of multi-label classification algorithms could be improved by taking
into account the inherent correlations among multiple labels. In our framework, the label correlations
are reflected in terms of variable pairwise constraints. Concretely, the must-link set collects the sample
pairs with more similar labels during re-sampling process. Meanwhile, the cannot-link set collects
sample pairs with more dissimilar labels. These two kinds of label relationships will be well preserved
via the variable pairwise constraint projection as shown in the following part.
3.2 Variable pairwise constraint projection
It is clear that one promising multi-label ensemble requires that the base classifiers should be as
accurate and diverse as possible. This part focuses on obtaining a well preserved lower-dimensional
data representation using the variable pairwise constraint projection. The base classifier is learned in
the new data space to capture better discriminating power. We begin with the mathematical definitions
described as follows.
Assume that a given multi-label dataset consists of n samples with r classes. Each sample has k
features stacked in a row vector. The feature matrix is denoted by X = [x1, x2, â¦, xn]T, and the label
matrix is Y = [y1, y2, â¦, yn] T, where yi = [yi1, yi2, â¦ , yir] (using PT3 ). If the sample xi in X can be
categorized to the rth class, then yir = 1, otherwise yir = -1. Our goal is to precisely estimate the label
sets of test data by using the proposed multi-label classification framework.
As mentioned earlier, it makes sense that we learn a lower-dimensional data representation
through the variable pairwise constraint projection, which can preserve the inherent correlations among
multiple labels. Now, we formulate the two kinds of variable pairwise constraints according to the
definition in Section 3.1 as followsï¼
where M denotes the must-link set, C denotes the cannot-link set, Î is a constant ranging from 0 to 1,
R( . ) is the label set of a sample, and |R( . )| is the cardinality of the label set (i.e., the number of
elements in the set). Besides, |R(xi)â©R(xj)| represents the number of the same labels shared by the data
pair (xi, xj). In principle, the threshold Î is used to tradeoff the balance between the must-link set and
the cannot-link set, which has large influences on the new data representation.
In order to obtain the low-dimensional data representation, we follow and seek a group of
projection vectors W = [w1, w2, â¦, wd] that best preserve the correlated pairwise constraints of M and C
in the new data space. Thus, we can derive the low-dimensional data representation from the
transformation zi = WTxi. Ideally, we expect that the two samples of any data pair in M are as close as
possible and the two samples of any data pair in C are as far apart as possible. Therefore, we want to
maximize the objective function formulated as:
where nC and nM denote the sizes of C and M respectively. Practically, nC and nM can be varied when
necessary. In this work, they are set to the number of instances. The term r is a scaling coefficient,
which is utilized to govern the contribution of data pairs in M to the objective function. Since the
distance between samples in M is typically smaller than that in C, then r can be estimated by:
After some linear algebraic operations, the objective J(W) in Eq.(3) can be simplified as the matrix
trace, i.e., Tr(WT (SC â rSM) W). We assume that a group of vectors {W1, W2, â¦, Wd} are the
eigenvectors corresponding to the d largest eigenvalues (Î»1, Î»2, â¦ , Î»d) of the matrix SCï¼rSM, and â§ is
a diagonal matrix with the eigenvalues as its diagonal elements. Then Tr(WT (SC â r SM) W) achieves the
optimal value when the number of eigenvectors d is set to the number of non-negative eigenvalues ,
which is also the dimension in the new data space. Thus, J(W) can be rewritten as:
where SC is the cannot-link scatter matrix and SM is the must-link scatter matrix, shown by:
Now, we can obtain the new data representation (zi, yi), where the base classifier is learned. Note
that the above formulations are similar to those in , which is only designed for single-label problem.
In contrast, our framework aims to solve multi-label problem and takes into account the correlations
among multiple labels.
3.3 The boosting-like strategy
In section 3.2, we can obtain the new data representation to learn an accurate classifier, but it
suffers from the weak generalization ability. To overcome this drawback, we adopt a boosting-like
strategy to obtain a group of diverse base classifiers in this section.
Different from bagging or boosting which resample the training data directly , we resample
pairwise constraints, which simultaneously considers the features and the label sets. By using the
boosting-like strategy, the selected pairwise constraints are changing with a flexible thresholding
scheme, thus providing diverse information as much as possible for the variable pairwise constraint
projection. This way, the base classifiers learned in the new lower-dimensional data space are expected
to become as diverse as possible.
On the other hand, in our framework we also use a technique similar to boosting , i.e.,
emphasizing more on misclassified samples. Here, the iteration is defined as one bootstrap step, which
provides further diversity for the base classifier. Particularly, assume that each sample is initially
assigned the same weight wi = 1/n, we will endow the misclassified samples with an increased weight
wi (1 + Î¸) in iteration, where Î¸ is a weight scaling factor, e.g., it can be the training error rate in
previous iteration. Note that the correctly classified samples remain their weights during the process.
In this boosting-like strategy, we cooperatively take advantage of the variable pairwise constraints
and the bootstrap steps to diversify the base classifiers, such that we can obtain a collection of base
classifiers as diverse as possible.
3.4 The VPCME framework
In this part, we summarize the proposed multi-label classification framework named Variable
Pairwise Constraint projection for Multi-label Ensemble. The pseudo-codes of the sketch are shown in
Table 1. In the following, we elaborate this framework more clearly.
First, we randomly select the data pairs (xi, xj) from the training data set and put them into the
corresponding constrained set C or M according to the flexible thresholding scheme as described in
Section 3.1. In total, we select nC samples from the cannot-link set C and nM samples from the
must-link set M. Second, we obtain a new data representation with the reduced dimension d by using
the variable pairwise constraint projection, i.e., Z = WTX. Third, the base multi-label classifier is
learned in the new data space. Furthermore, we adopt the boosting-like strategy to repeat the above
process until the desired number of base classifiers is achieved. Ultimately, a collection of base
classifiers are combined together to construct a robust multi-label ensemble. For prediction, we simply
use the popular majority voting to estimate the label sets of the test data, since this method does not
require any prior field knowledge and has low computational cost .
Table 1 The pseudo-code of the proposed VPCME framework
Essentially, our framework consists of two components, i.e., the variable pairwise constraint
projection and the boosting-like strategy. Both of them are cooperatively applied to construct a robust
multi-label ensemble, e.g., both the variable pairwise constraints and the bootstrap steps are used to
diversify the base classifiers. Notice that the constrained projection is closely related to dimensionality
reduction , but they are applied differently. For one thing, our projection is based on the
variable pairwise constraint sets (i.e., M and C) rather than the original data set. For another, the
boosting-like strategy together with the variable pairwise constraint projection jointly contributes to
obtain multiple base classifiers as accurate and diverse as possible, which is not the case for those
feature extraction methods. In addition, this projection could preserve the pairwise relations between
multiple labels and samples, which are beneficial for training desirable base classifiers.
4 Experiments
This section show empirical studies on a broad range of real-world multi-label datasets. First, we
give a brief description about the evaluation metrics and data sets. Second, experimental setup and
performance comparisons are presented. Third, we report the results as well as some analysis.
4.1 Evaluation metrics
In this test, we employ several popular metrics to evaluate the proposed framework .
Suppose that a multi-label dataset D consists of N instances, represented by D = (xi, Yi), i =1, â¦ , N,
where YiÃ L is the true label set and L={Î»j : j =1, â¦ , M} is the total label set. For a given instance xi,
its estimated label set is denoted by Zi, and the estimated rank of the label Î» is denoted by ri(Î»). The
most relevant label takes the top rank (1) and the least one only gets the lowest rank (M). In the
following, we explain these evaluation metrics from a mathematical viewpoint.
Hamming loss was initially proposed by Schapire and Singer , and it enumerates the
misclassified times of the predicted labels based on instances. It can be defined as follows:
HammingLoss = N
where the symbol ââ³â stands for the symmetric difference of the two sets Yi and Zi, and that is the
set-theoretic equivalent of the exclusive disjunction (XOR operation) in Boolean logic.
Ranking loss denotes the number of times that irrelevant labels are ranked higher than relevant
labels, and it takes the form:
RankingLoss =
where the term
Y is the complementary set of Yi with respect to L.
One-error mainly evaluates how many times the top-ranked label is not in the set of relevant
labels of the instance. It can be formulated as follows:
OneError =
Coverage examines how far we need on average to go down the rank list of labels in order to
cover all the relevant labels of the instance. This can be expressed by:
Average precision assesses the average fraction of labels ranked above a particular label Î»â Yi
which are actually in Yi. And this metric virtually reflects the average classification accuracy of the
predicted labels of the instance. It can be denoted by:
AvePrecision = N
Both of the two metrics F1-metric and Recall are elaborated by Godbole & Sarawagi , and
they can be shown respectively as:
In the above criteria, Hamming loss, Ranking loss, One-error and Coverage suffice to âthe smaller
the betterâ while Average precision, F1-metric and Recall characterizes âthe larger the betterâ. These
metrics are employed jointly to investigate performances of multi-label classification methods .
4.2 Datasets
To examine the multi-label algorithms, we compile a variety of multi-label datasets1, including
text categorization (Yahoo data, enron, medical) , image classification (scene) and bioinformatics
(yeast, genbase) . In summary, twelve datasets were used with 6 to 45 labels and from less than 700
examples to over 2, 400 ones. Their statistics are listed in Table 2.
1 The Yahoo! datasets are available for download at 
the remaining datasets can be obtained from 
Table 2 Multi-label datasets and their statistics
cardinality
multimedia
Yahoo text
Yahoo text
Yahoo text
entertainment
Yahoo text
Yahoo text
Yahoo text
Yahoo text
The label cardinality is the average number of labels assigned to each sample, which is used to
quantify the number of distinct labels. The larger the cardinality, the more difficult we obtain the
impressive classification performance. Besides, this inherent property has direct impacts on Coverage.
Specifically, two datasets with the identical cardinality but distinct label numbers might not exhibit the
same property Label density is the percentage of the averaged labels in the total labels. The value of
density associates with Hamming loss and Ranking loss, and it will affect the values of F1-metric and
Recall as well. Label distinct denotes the number of different label combinations, and it is of key
importance for many algorithm adaptation methods that operate on label subsets. The larger the
distinct takes, the more complex the multi-label problem becomes. If some minority label combinations
appear in an extremely small size, i.e., the imbalanced problem, Coverage will reduce.
The yeast and genbase datasets are both from the biological field. The yeast data is associated
with 14 functional classes from the Comprehensive Yeast Genome Database of the Munich Information
Center for Protein Sequences. The total number of genes amounts to 2417 with each gene represented
by a 103-dimensional feature vector. In the genbase dataset, there are 27 primary protein families,
including PDOC00064 and PDOC00154. After preprocessing, the data set consists of 662 proteins and
each protein might belong to one or more of 27 classes.
In the multimedia domain, the scene dataset contains six possible scenes, such as beach, sunset,
field, fall foliage, mountain and urban. It targets recognizing which of the above scenes can be
observed in 2407 pictures. For a scene image, the spatial color moments are regarded as features and
each picture is decomposed into 49 blocks using one 7 by 7 grid.
The enron dataset is collected and prepared by the Cognitive Assistant that Learns and Organizes
(CALO) Project, containing data from about 150 users, mostly senior management of Enron, organized
into folders. A subset of 1702 labeled email messages with 1001 characteristics is used in experiments
and each message is labeled by two persons.
The medical data is compiled for the Computational Medicine Centers to challenge the
international Natural Language Processing (NLP) community. In 978 documents, each involves a brief
clinical free-text summary of the patient symptom history and their prognosis, labeled with insurance
codes. They are associated with one or more labels from a subset of 45 candidate labels.
Seven Yahoo! datasets (i.e., arts, business, education, entertainment, health, science and social)
are collected from the real Web pages linked to the âYahoo.comâ domain, including fourteen top-level
categories and each category is divided into several second-level subcategories . Focused on the
second-level categories, 7 out of the 14 independent text categorization problems are considered here
with each containing 2000 documents.
4.3 Experimental setup and design
In this part, we give descriptions about parameter settings and performance comparisons for our
proposed framework.
4.3.1 Parameter settings
In the experiments, we compared VPCME with several state-of-the-art algorithms, including
IBLR-ML , CLR , CChains , RAKEL and MLKNN . To investigate the individual
components of VPCME, three typical ensemble methods are also examined, i.e., AdaBoost with pruned
decision tree (AdaBoostPDT), Bagging with the variable pairwise constraint projection (BaggingVPCP),
and multi-label dimensionality reduction via dependence maximization (MDDM) . All experiments
were carried out on a PC machine with Intel (R) Core (TM) Duo CPU 3.16 GHz and 2 GB RAM,
Matlab R2009b version. The instance-based learning method MLKNN was used as the base classifier
for VPCME due to its excellent predictive performance, and the number of nearest neighbor k was set
to 10 as in , where it was found to yield the most satisfactory performances.
To eliminate the bias incurred by different base learners, we respectively utilized KNN, C4.5 (i.e.,
J48 in WEKA ) and SMO for the compared algorithms RAKEL, IBLR-ML, CLR and CChains.
Results show that KNN slightly degrades the performance and results of J48 and SMO are similar, so
we only report the results of J48. For RAKEL, k was set to |labels|/2 , and the smaller k leads to the
lower computational cost. Parameters for IBLR-ML, CLR, CChains were all established as described
in original literatures . The remaining parameters were set to their default values in mulan2
 and WEKA with Java JDK 1.6. Note that AdaBoostPDT handles one of the assigned multiple
labels sequentially. The settings for BaggingVPCP were the same as those for VPCME except the
bootstrap steps. The threshold for MDDM was set to 99% as in .
Since the generalization ability is of vital importance for one learning framework, we investigate
the performance of our framework under various parameter settings in light of some refinement
methods , such as the threshold selection and different ensemble sizes. For all the tests,
five-fold cross validations were carried out to estimate the labels. In detail, the original dataset is
randomly divided into five parts with each almost the same size, and in each fold one of them is held
out for testing and the remainder for training. This process was repeated five times so that each part
would be treated as the test data exactly once. Without loss of generality, we repeated each test run for
20 times and recorded the averaged results as well as the standard deviations. Furthermore, the paired
t-tests at the significance level of 0.01 were done to validate the efficacy of our approach.
2 Mulan is available at 
Table 3(a) Performance of different algorithms in terms of Hamming loss. (meanÂ±std. %)
23.15Â±0.76â
19.37Â±1.04â
22.02Â±0.84â
26.71Â±0.59â
19.29Â±1.05â
17.57Â±0.18
10.63Â±0.49â
8.56Â±0.30â
13.36Â±0.49â
14.43Â±0.61â
8.90Â±0.33â
5.61Â±0.17â
5.27Â±0.14â
5.30Â±0.15â
0.15Â±0.03â
0.21Â±0.07â
0.15Â±0.03â
2.07Â±0.03â
7.70Â±0.15â
3.05Â±0.11â
3.24Â±0.12â
4.40Â±0.14â
4.24Â±0.09â
4.44Â±0.15â
5.44Â±0.11â
entertain.
6.11Â±0.16â
6.13Â±0.16â
7.05Â±0.35â
3.83Â±0.12â
4.29Â±0.22â
4.03Â±0.08â
4.28Â±0.21â
4.43Â±0.23â
3.89Â±0.11â
3.83Â±0.04â
4.40Â±0.19â
2.40Â±0.09â
2.89Â±0.14â
2.44Â±0.09â
2.62Â±0.09â
2.69Â±0.09â
4.3.2 Experimental design
In this subsection, we briefly describe four experimental groups of performance comparisons,
which are designed to explore the performance of our VPCME method. Note that the remaining
parameters of Group2, Group3 and Group4 are fixed as the same in Group1. Detailed descriptions are
shown as follows.
Group1: To explore the performances of all compared multi-label classification algorithms,
experiments on twelve datasets were conducted. For VPCME, the variable pairwise constraint
threshold was empirically set to 0.6 and the ensemble size was tuned to 30. Evaluation results were
recorded in terms of five metrics.
Group2: To exhibit the influences of different variable pairwise constraint thresholds on our
framework, we tested an ascending threshold list ranging from 0.1 to 1.0 at the interval of 0.1 on yeast
and business. This empirically provides an intuitive selection way for this parameter.
Group3: To investigate the performance of VPCME under different ensemble sizes, we examined
a sequence of sizes from 10 to 50 with a grid of 10 on medical and entertainment. The larger the
ensemble size, the more time complexity the multi-label classification method will cost. To attain
satisfactory performance, a proper ensemble size is desired.
Group4: To examine the individual components of VPCME, we compared three other approaches,
i.e., AdaBoostPDT, BaggingVPCP, MDDM. To ensure fairness, we tested on several representative
datasets since they come from distinct domains, i.e., yeast, scene and entertainment.
4.4 Results
In this section, we report the results of four experimental groups in Section 4.3.2 respectively as
well as some analysis. Table 3 (a-e) and Table 4 report the results of Group1, Fig. 1 depicts the results
of Group2, the results of Group3 are tabulated in Table 5 (a-b), and the comparison results of Group4
are shown in Fig. 2.
Table 3(b) Performance of different algorithms in terms of Ranking loss. (meanÂ±std. %)
22.36Â±1.10â
16.50Â±0.99â
17.81Â±1.03â
32.61Â±0.79â
16.60Â±0.95â
12.91Â±0.52
10.94Â±0.93â
7.54Â±0.44â
10.00Â±0.64â
25.05Â±2.67â
7.90Â±0.49â
20.35Â±1.11â
10.69Â±0.81â
7.28Â±0.77â
18.44Â±0.56â
9.37Â±0.68â
0.56Â±0.28â
1.25Â±0.69â
0.81Â±0.38â
7.53Â±0.13â
6.87Â±0.75â
2.71Â±0.48â
7.62Â±1.54â
4.17Â±0.52â
27.90Â±0.90â
17.15Â±0.60â
12.91Â±0.90â
24.63Â±1.11â
15.89Â±0.27â
11.68Â±0.98â
4.64Â±0.61â
3.64Â±0.42â
11.19Â±1.04â
3.95Â±0.38â
34.94Â±1.81â
11.07Â±0.58â
8.63Â±0.65â
23.85Â±1.59â
8.97Â±0.72â
entertain.
32.02Â±1.61â
13.34Â±0.45â
10.90Â±0.33â
24.54Â±3.08â
12.43Â±0.46â
20.04Â±1.85â
6.69Â±0.40â
4.62Â±0.39â
14.16Â±2.33â
6.16Â±0.34â
31.79Â±0.81â
17.41Â±0.52â
11.42Â±0.32â
26.36Â±1.71â
13.97Â±0.31â
20.24Â±1.47â
8.97Â±0.93â
5.81Â±0.49â
13.73Â±0.85â
6.91Â±0.84â
Table 3(c) Performance of different algorithms in terms of One-error. (meanÂ±std. %)
29.33Â±2.39â
22.13Â±2.14â
23.83Â±1.66â
34.26Â±1.10â
22.88Â±1.91â
18.56Â±0.76
28.17Â±1.18â
22.52Â±1.43â
29.33Â±1.77â
39.59Â±2.34â
23.10Â±1.30â
17.39Â±0.82
29.14Â±4.81â
37.02Â±3.10â
21.74Â±2.26â
42.36Â±1.21â
31.20Â±2.54â
20.35Â±0.79
18.21Â±3.40â
34.05Â±2.16â
16.47Â±2.40
18.31Â±3.49â
25.46Â±4.29â
16.16Â±1.56
58.75Â±1.44â
62.60Â±2.57â
52.95Â±1.44â
63.90Â±2.08â
62.35Â±2.11â
42.74Â±0.82
14.85Â±1.01â
12.00Â±1.64
12.60Â±1.81â
23.00Â±0.85â
11.65Â±1.25
11.24Â±1.03
59.80Â±1.23â
60.45Â±0.87â
57.35Â±0.94â
66.30Â±0.81â
58.20Â±1.44â
45.30Â±0.35
entertain.
51.00Â±0.89â
54.20Â±0.98â
48.25Â±1.18â
56.55Â±2.58â
53.65Â±1.45â
39.36Â±0.82
32.85Â±1.75â
39.50Â±2.04â
31.60Â±2.15â
40.30Â±2.38â
40.00Â±2.07â
27.10Â±1.14
62.90Â±1.60â
69.35Â±1.59â
59.55Â±1.96â
68.30Â±1.14â
67.00Â±1.90â
49.52Â±1.25
34.85Â±2.15â
43.95Â±0.91â
35.25Â±1.30â
40.20Â±1.85â
41.65Â±1.71â
31.68Â±0.94
4.4.1 Results of Group1
Table 3 shows the results of different multi-label algorithms on several data sets. Records are
tabulated in terms of averaged mean values as well as standard deviations over 20 test runs. To examine
whether the results are statistically significant, paired t-tests were carried out at 1% significance level.
The marker ââ/ââ suggests our approach is statistically superior/inferior to others. Note that the symbol
âââ indicates the smaller the better while âââ indicates the larger the better. Specifically, when the
presented method achieves significantly better/worse performance than the others, a win/loss is counted
and a marker ââ/ââ is aside the record. Otherwise, a tie is counted and no marker is placed. The
obtained win/tie/loss counts for VPCME against the compared algorithms are summarized in Table 4.
From Table 3 and Table 4, a number of interesting points can be observed as follows:
(1) Our VPCME approach systematically and consistently performs better than other algorithms,
since it takes advantage of variable pairwise constraint projection and the boosting-like strategy.
Particularly, the misclassified samples will receive more emphasis in iteration. Multiple diversified
base classifiers are combined to construct a robust multi-label ensemble, which is able to achieve a
small error rate by utilizing the additional information provided by different base classifiers.
Table 3(d) Performance of different algorithms in terms of Coverage. (meanÂ±std. %)
7.67Â±0.20â
6.72Â±0.16â
8.94Â±0.09â
0.64Â±0.05â
0.58Â±0.03â
1.36Â±0.12â
25.39Â±0.59â
14.89Â±0.73â
11.44Â±0.88â
23.99Â±0.47â
13.28Â±0.73â
0.86Â±0.41â
4.29Â±0.51â
3.93Â±0.38â
4.43Â±0.71â
2.70Â±0.30â
9.47Â±0.35â
6.08Â±0.26â
4.95Â±0.22â
8.68Â±0.31â
5.68Â±0.12â
6.13Â±0.52â
2.59Â±0.27â
2.15Â±0.15â
5.71Â±0.39â
2.25Â±0.15â
13.76Â±0.89â
4.68Â±0.22â
3.84Â±0.28â
9.84Â±0.62â
3.94Â±0.32â
entertain.
7.81Â±0.30â
3.58Â±0.16â
3.09Â±0.03â
6.29Â±0.69â
3.39Â±0.15â
9.49Â±0.68â
3.54Â±0.18â
2.77Â±0.12â
6.89Â±0.98â
3.28Â±0.17â
15.23Â±0.43â
8.66Â±0.25â
6.14Â±0.15â
13.22Â±0.79â
7.14Â±0.15â
9.75Â±0.61â
4.54Â±0.37â
3.10Â±0.18â
6.91Â±0.54â
3.60Â±0.39â
Table 3(e) Performance of different algorithms in terms of Average precision. (meanÂ±std. %)
70.82Â±1.54â
76.72Â±1.50â
74.70Â±1.58â
63.06Â±0.90â
76.47Â±1.55â
80.41Â±0.73
82.67Â±0.82â
86.68Â±0.70â
82.64Â±1.01â
71.33Â±1.91â
86.25Â±0.63â
90.32Â±0.89
60.89Â±2.06â
60.95Â±1.94â
70.25Â±1.65â
57.02Â±0.40â
62.51Â±1.22â
72.86Â±0.66
99.09Â±0.55
98.32Â±0.69â
98.60Â±0.71
98.99Â±0.47
98.70Â±0.45
99.32Â±0.35
83.07Â±2.63â
74.18Â±0.95â
87.58Â±1.32â
83.61Â±2.67â
80.57Â±2.50â
89.83Â±1.06
47.72Â±0.54â
49.15Â±1.94â
56.86Â±1.43â
46.55Â±1.68â
50.05Â±1.80â
63.41Â±0.41
81.44Â±1.12â
87.25Â±1.23â
87.56Â±1.26â
78.04Â±0.77â
87.96Â±0.94â
90.65Â±0.59
45.76Â±1.78â
53.29Â±0.83â
56.67Â±0.72â
45.43Â±0.99â
55.74Â±0.97â
65.93Â±0.82
entertain.
54.39Â±1.16â
58.83Â±0.98â
63.05Â±0.58â
53.68Â±2.55â
59.53Â±1.37â
71.54Â±1.14
67.42Â±1.60â
68.96Â±1.37â
74.73Â±1.26â
65.45Â±2.33â
68.56Â±1.45â
78.58Â±0.87
41.97Â±0.72â
43.33Â±1.14â
52.41Â±1.33â
40.74Â±1.04â
45.91Â±1.26â
58.83Â±1.16
66.34Â±1.58â
66.20Â±1.01â
73.25Â±1.21â
67.16Â±1.38â
68.89Â±1.42â
77.40Â±1.08
(2) CChains performs worse than others, which might be due to fact that the order of the classifier
chains in iteration is inappropriate for the datasets. RAKEL strives to learn a label power set classifier
for each k-subset of labels, but the divided subsets are randomly selected from the dataset, which might
degrade its performance.
(3) IBLR-ML outperforms RAKEL and CChains, since it combines logistic regression into one
unified framework. However, the biased estimations of optimal regression coefficients usually lead to
the imbalance between the global and local inference, which has negative effects on the performance.
(4) CLR performs better on text datasets than other algorithms except VPCME. Recall that CLR is
a label ranking method and its artificial calibration label determines the separating boundary between
relevant and irrelevant labels. As a result, if the confidence of the key calibration label is far from the
desired, CLR tends to be outperformed by others.
Table 4 The win/tie/loss results for VPCME against the compared algorithms
Evaluation
The VPCME method against
Hamming loss
Ranking loss
Ave. precision
(5) MLKNN is compared as a baseline here, since it is selected as the base classifier of our
VPCME framework. Typically, it is a binary relevance learner, which implements each individual
classifier through a combination of KNN and Bayesian inference . Moreover, it often exhibits more
simplicity compared to RankSVM and AdaBoost.MH .
(6) The disparity degrees of the six algorithms are somewhat tiny in terms of Hamming loss on the
text datasets. This evidence demonstrates that the smaller cardinality and the lower density lead to the
robustness of this metric, since the hit ratio of predictive label and ground-truth label becomes very
large to some extent.
4.4.2 Results of Group2
As illustrated in Fig.1, the performance of our proposed method with different variable pairwise
constraint thresholds is vividly depicted.
Variable pairwise constraint threshold (yeast)
Evaluation metrics (%)
Hamming loss
Ranking loss
Ave. precision
Variable pairwise constraint threshold (business)
Evaluation metrics (%)
Hamming loss
Ranking loss
Ave. precision
(a) Performance on yeast dataset (b) Performance on business dataset
Fig.1. Performance of VPCME under different thresholds
On the left panel, Fig.1 (a) reflects the comparison performance on the biology data yeast. On the
right panel, Fig.1 (b) displays the behavior of VPCME on the text data business. From the figures, it
can be seen that in terms of Average precision, F1-metric and Recall, the curves of VPCME begin to
ascend at the initial stage and keeps relatively stable during the intermediate period (i.e., 0.4~0.7), but it
declines as the threshold approaches one. Obviously, VPCME behaves differently compared to
Hamming loss, Ranking loss and One-error.
Clearly from the two figures, we find that the tendency on the left is more significant than that on
the right, which indicates our method enjoys robustness over a larger range on the text data. We
attribute this to the fact that the imbalance problem caused by the ratio of cannot-link constraints and
must-link constraints should be neither too large nor too small, since extreme values will seriously
break the balance between them. Herein, the curves reflect that our framework tends to perform well in
a wide range of varied thresholds.
Table 5(a) Performance of VPCME with different ensemble sizes on medical. (meanÂ±std. %)
Evaluation Metrics
Hamming lossâ%
Ranking lossâ%
One-errorâ%
18.39Â±1.44
17.13Â±1.27
16.54Â±1.39
16.16Â±1.56
16.08Â±1.33
15.98Â±1.21
Ave. precisionâ%
86.21Â±1.09
88.54Â±1.23
89.19Â±1.15
89.93Â±1.06
90.04Â±0.95
90.15Â±1.16
Table 5(b) Performance of VPCME with different ensemble sizes on entertainment. (meanÂ±std. %)
Evaluation Metrics
Hamming lossâ%
Ranking lossâ%
One-errorâ%
42.86Â±0.92
40.25Â±1.13
39.79Â±1.02
39.36Â±0.82
39.14Â±0.94
39.08Â±0.85
Ave. precisionâ%
67.24Â±1.45
69.92Â±1.37
70.86Â±1.05
71.54Â±1.14
71.79Â±1.21
71.95Â±1.06
4.4.3 Results of Group3
To further examine the proposed method, we show the performances of VPCME under different
ensemble sizes in Table 5. In these tables, Table 5 (a) summarizes the results on the medical data and
Table 5 (b) makes records of the entertainment data.
The results clearly show that our proposed method consistently outperforms others as ensemble
size increases. In particular, when the single MLKNN is used (i.e., S=1), VPCME still has good
performance in the new data space. However, when S exceeds 30, the performance is improved very
slightly. This implies that the ensemble performance will achieve a stable value after some peak value.
Generally, we should not choose a larger ensemble size than some proper threshold, since training more
base classifiers would cost much more time and memory.
4.4.4 Results of Group4
As vividly depicted in Fig. 2, it is readily to see that the proposed VPCME framework consistently
performs better than other compared approaches in terms of various evaluation criteria. Particularly, we
find that AdaBoostPDT performs worst since it does not consider the correlations among multiple labels.
BaggingVPCP performs better than AdaBoostPDT and MDDM, because it exploits the variable pairwise
constraints of samples. But it is outperformed by VPCME, which is due to the fact that the BaggingVPCP
framework treats every sample equally while the VPCME framework puts more emphasis on the
misclassified samples in iteration. In addition, MDDM is a dimensionality reduction method via
maximizing the dependence between the original features and the associated class labels, and it does
not take into account the pairwise constraints.
Different multi-label data sets
Hamming loss (/%)
AdaBoostPDT
BaggingVPCP
Different multi-label data sets
Ranking loss (/%)
AdaBoostPDT
BaggingVPCP
(a) Hamming loss (b) Ranking loss
Different multi-label data sets
One-error (/%)
AdaBoostPDT
BaggingVPCP
Different multi-label data sets
Average precision (/%)
AdaBoostPDT
BaggingVPCP
(c) One-error (d) Average precision
Fig.2 Comparison of different approaches in terms of four metrics
5 Conclusions
In this paper, we introduce a novel multi-label classification framework called Variable Pairwise
Constraint projection for Multi-label Ensemble (VPCME) to construct a multi-label ensemble for
handling multi-label data. This framework involves two inherent components, i.e., the variable pairwise
constraint projection and the boosting-like strategy. In detail, we employ the variable pairwise
constraint projection to obtain a well preserved lower-dimensional data space, where the base
classifiers are learned. Besides, we make use of a boosting-like strategy to improve the generalization
ability of the classifier. For the boosting-like strategy, both the variable pairwise constraints and the
bootstrap steps are exploited to diversify a group of base classifiers. In this work, the majority voting is
adopted to decide the estimated label set for each test data. We conducted extensive interesting
experiments over a range of multi-label datasets. Results show that our proposed approach performs
better than other competing methods.
Nevertheless, there still remain some problems to be explored in future. For example, it is sensible
to develop one principle way to select the optimal threshold for variable pairwise constraints. Due to
the high dimensions of many real-world data, it is practically important to study the joint learning of
multi-label feature selection and multi-label ensemble so as to select the most informative feature
subsets. Another interesting problem is to explore the way to speed up the multi-label ensemble
approach such that the computational costs can be greatly reduced.
Acknowledgements
We would like to thank the anonymous reviewers and Dr. Zhang Lijun for their insightful
comments and suggestions, which greatly help improve this paper. This work was supported in part by
the National Science Foundation for Outstanding Young Scientists of China (60425310) and the
Fundamental Research Funds for the Central Universities (2012FZA5017).