ERNEST ORLANDO LAWRENCE
BERKELEY NATIONAL
LABORATORY
On Matrices with Low-Rank-Plus-Shift
Structure: Partial SVI) and L-EI
Semantic Indexing
OGT u 9 19%
Hongyuan Zha and Zhenyue Zhang
Computing Sciences Directorate
Scientific Computing Division
August 1998
National Energy Research
DISCLAIMER
This document was prepared as an account of work sponsored by t h e
United States Government. While this document is believed to contain
correct information, neither the United States Government nor any
agency thereof, nor The Regents of the University of California, nor any
of their employees, makes any warranty, express or implied, or assumes
any legal responsibility for the accuracy, completeness, or usefulness of
information,
apparatus,
disclosed,
represents that its use would not infringe privately owned rights.
Reference herein to any specific commercial product,
process, or
service by its trade name, trademark, manufacturer, or otherwise, does
not necessarily constitute or imply its endorsement, recommendation,
or favoring by the United States Government or any agency thereof, or
The Regents of the University of California. The views and opinions of
authors expressed herein do not necessarily state or reflect those of t h e
United States Government or any agency thereof, or The Regents of the
University of California.
This report has been reproduced directly from the best
available copy.
Available to DOE and DOE Contractors
from the Office of Scientific and Technical Information
P.O. Box 62, Oak Ridge, TN 37831
Prices available from (615) 576-8401
Available to the public from the
National Technical Information Service
U.S. Department of Commerce
5285 Port Royal Road, Springfield, VA 22161
Ernest Orlando Lawrence Berkeley National Laboratory
is an equal opportunity employer.
DISCLAIMER
Portions of this document may be illegible
electronic image products. Images are
produced from the best available original
LBNL-42279
On Matrices with Low-Rank-Plus-Shift Structure:
Partial SVD and Latent Semantic Indexing
Hongyuan Zha and Zhenyue Zhang
Computing Sciences Directorate
National Energy Research Scientific Computing Division
Ernest Orlando Lawrence Berkeley National Laboratory
University of California
Berkeley, California 94720
August 1998
This work was supported in part by the Director, Office of Energy Research, Office of Laboratory Policy and
Infrastructure Management, Office of Computational and Technology Research, Division of Mathematical,
Information, and Computational Sciences, of the U.S. Department of Energy under Contract No. DE-AC03-
76SF00098, by National Science Foundation Grant CCR-9619452, and by NSFC Project No. 1977 1073.
Worked performed at the Ernest Orlando Lawrence Berkeley National Laboratory.
ON MATRICES WITH LOW-RANK-PLUS-SHIFT STRUCTURE:
PARTIAL SVD AND LATENT SEMANTIC INDEXING
HONGYUAN ZHA* AND ZHENYUE ZHANGt
Abstract. We present a detailed analysis of matrices satisfying the so-called low-mnk-plus-shift
property in connection with the computation of their partial singular value decomposition. The
application we have in mind is Latent Semantic Indexing for information retrieval where the term-
document matrices generated from a text corpus approximately satisfy this property. The analysis
is motivated by developing more efficient methods for computing and updating partial SVD of large
term-document matrices and gaining deeper understanding of the behavior of the methods in the
presence of noise.
1. Introduction. In many applications such as compression of multiple-spectral
image cubes, regularization methods for ill-posed problems, latent semantic indexing
in information retrieval for large document collections, it is necessary to find a low
rank approximation of a given large and/or sparse matrix A E Rmxn [ll]. The theory
of singular value decomposition (SVD) provides the following characterization of the
best low rank approximation of A in terms of F'robenius norm 11 '
1.1. Let the singular value decomposition of A E RmXn
be A = PCQT
with C = diag(a1, ..., amin(m,n))7
a1 2 ... 2 amin(m,n),
and P and Q orthogonal.
Then for 1 5 j 5 min(m,n),
a: = min{ llA - Bll$l rank(B) 5 j } .
And the minimum is achieved with Aj E Pj diag(c1,. . . , aj)QT, where Pj and Qj are
the matrices formed by the first j columns of P and Q, respectively.
It follows from Theorem 1.1 that once the SVD of A is available, the best rank-
j approximation of A is readily computed. We call Aj = Pj diag(c1,. . . , aj)QT a
partial SVD of A. The state-of-the-art methods for computing the partial SVD of
large and/or sparse matrices are based on variants of Lanczos algorithms and the
core computation at each iterative steps involves matrix-vector multiplications [SI. In
order to effectively deal with large-scale problems, one is required to exploit various
structures of the matrices. Despite its importance, the exploitation of structures so far
has been restricted to 1) using the sparsity of a sparse matrix, 2) using displacement-
rank structures such as Toeplitz or Hankel structure of the matrix, to accelerate the
matrix-vector multiplications used in the Lanczos process. In this paper, however,
we propose to explore an alternative structure that is based on the singular value
* Department of Computer Science and Engineering, The Pennsylvania State University, Univer-
sity Park, PA 16802, zhaacse .psu.edu. The work of this author was supported in part by the Direc-
tor, Office of Energy Research, Office of Laboratory Policy and Infrastructure Management, Office
of Computational and Technology Research, Division of Mathematical, Information, and Computa-
tional Sciences, of the U.S. Department of Energy under Contract No. DEAC03-76SF00098, and by
NSF grant CCR-9619452.
t Center for Mathematical Sciences & Department of Applied Mathematics, Zhejiang Univer-
sity, Hangshou, 310027, P. R. China. .
The work of this author was
supported in part by NSFC (project 19771073), Zhejiang Provincial Natural Science Foundation of
China, and Scientific Research Foundation for Returned Overseas Chinese Scholars, State Education
Commission.
Partial SVD and Latent Semantic Indexing
spectrum of a matrix. Specifically, we investigate matrices possessing the so-called
low-rank-plus-sh$t structure, i.e., those matrices A (approximately) satisfying
ATA = a low rank matrix + a multiple of the identity matrix.
The particular application we have in mind is Latent Semantic Indexing (LSI) for
information retrieval and we will show by way of examples that the term-document
matrices generated from text corpora approximately satisfy Equation (1.1). In large-
scale LSI applications such as the World Wide Web, the term-document matrix gen-
erated is usually very large and can not be kept in RAM or disk. In [15, 161 we have
shown that the low-rank-plus-shift structure of the term-document matrix A allows
us to compute its partial SVD in a block-wise fashion whereby partial SVD of sub-
matrices of A are computed separately and then merged to obtain a partial SVD of
A. The purpose of this paper is to further analyze the properties of matrices with
low-rank-plus-shift structure especially when Equation (1.1) is only approximately
satisfied. We hope our analysis will provide deeper insights into this special class of
matrices which will enable us to develop more efficient methods for computing their
partial SVD.
The rest of the paper is organized as follows: In Section 2, we provide a brief
background on LSI and review some of the results in [14, 15, 161 related to computing
the partial SVD of term-document matrices. In Section 3, we discuss some matrix
approximation problems associated with the low-rank-plus-shift structure, and show
by way of examples that the term-document matrices generated from text corpora ap-
proximately satisfy Equation (1.1). In Section 4 we prove a result on the partial SVD
of a block-colurnn partitioned matrix with low-rank-plus-shift structure. This result
enables us to efficiently compute the partial SVD either with an incremental approach
or divide-and-conquer approach. We will also discuss the ramification of the result in
dealiig with LSI updating problems. In Section 5 we provide a perturbation analysis
of the result when the low-rank-plus-shift property is only satisfied approximately. In
Section 6 we conclude the paper with some remarks on future research.
2. Latent Semantic Indexing. Latent semantic indexing is a concept-based
automatic indexing method that aims at overcoming the two fundamental problems
which plague traditional lexical-matching indexing schemes: synonymy and polysemy
[2, 51. Synonymy refers to the problem that several different words can be used to
express a concept and the keywords in a user's query may not match those in the
relevant documents while polysemy means that words can have multiple meanings
and user's words may match those in irrelevant documents [SI. LSI is an extension of
the vector space model for information retrieval [7, lo]. In the vector space model, the
collection of text documents is represented by a term-document matrix A = [aij] E
Rmx", where aij is the number of times term i appears in document j , and rn is the
number of terms and n is the number of documents in the collection. Consequently, a
document becomes a column vector, and a user's query can also be represented as a
vector of the same dimension. The similarity between a query vector and a document
vector is usually measured by the cosine of the angle between them, and for each query
a list of documents ranked in decreasing order of similarity is returned to the user. LSI
extends this vector space model by modeling the term-document relationship using
the singular value decomposition (SVD) of the term-document matrix A. Specifically,
using the notation in Theorem 1.1, we substitute A by its best rank-k approximation
A k 3 Pk'CkQT, where CI, is the k-th leading principal submatrix of C . Corresponding
Partial SVD and Latent Semantic Indexing
to each of the k reduced dimensions is associated a latent concept which may not have
any explicit semantic content yet helps to discriminate documents [2, 51.
Large text corpora such as those generated from World Wide Web give rise to
very large term-document matrices, and the computation of their partial SVD poses
a very challenging problem. Fortunately the term-document matrices possess certain
useful properties besides sparsity that can be exploited for this matter. In [15, 161,
we developed a theoretical foundation for LSI using the concept of subspaces, and we
showed that the model we proposed imposes a so-called low-rank-plus-ship structure
that is approximately satisfied by the cross-product of the term-document matrices.l
Specifically, we showed that the term-document matrix A E Rmxn satisfies
ATA/m M CWCT + c2 In,
where C E RnXk
is the matrix whose columns represent latent concepts, W E Rkxk
is a symmetric positive definite matrix, and CT is the variance of the noise. In LSI
applications k << min(m, n}, justifying the use of the terminology low-rank-plus-shift
structure.
In , we considered the updating problems for LSI: Let A be the term-document
matrix for the original text collection and D represents a collection of new documents.
The goal is to compute the partial SVD of [A, D]. However, in LSI applications, only
A6 for some chosen k is available and the matrix A has been discarded. Since updating
in this situation is based on a low-rank approximation of A, it has been argued in the
literature that one will not be able to get an accurate partial SVD of [A, D]. In Section
4, we show, however, that this is not the case since [A, D]
has the low-rank-plus-shift
structure . We will show that no retrieval accuracy degradation will occur if
updating is done with a proper implementation. In [15, 161, we also discussed how to
compute the partial SVD of a term-document matrix in a block-column partitioned
form A = [AI, Az] using a divide-and-conquer approach whereby the partial SVDs of
A1 and A2 are first computed and the results are then merged into a partial SVD of A.
This approach is rich in coarse-grain parallelism and can be used to handle very large
term-document matrices. The justification for this divide-and-conquer approach will
be discussed in greater detail in Section 4, and perturbation analysis will be provided
to show that the approach is still valid even if the term-document matrix A only
approximately satisfies the low-rank-plus-shift structure.
3. A Matrix Approximation Problem. From the discussion in Section 2
we know that the term-document matrix A approximately satisfies the low-rank-
plus-shift property and therefore A should have flat trailing singular values. In this
section we use several example text collections to illustrate this issue. In order to
assess whether a given matrix has the low-rank-plus-shift property, we investigate the
following matrix approximation problem: Given a general rectangular matrix, what
is the closest matrix that has the low-rank-plus-shift property. To proceed we first
define a matrix set for a given k > 0,
Ji = {B Rmxn
I ol(B) L -. - 2 amin{m,n>(B), Q+I(B) = * *
omin{m,n}(B)}*
With this notation, the matrix approximation problem reduces to finding the distance
between a general matrix A and the set &. In the following we consider the cases
where distance is defined either by Frobenius norm 11 .
or spectral norm 11
The low-rank-plus-shift structure was first discussed in the context of array signal processing
[12, 13, 171.
Partial SVD and Latent Semantic Indexing
3.1. Let the SVD of A be A = U W T , = diag(a1,. . , gmin{m,n})j
and U and V orthogonal. Then for k 5 min{m,n},
arg min llA - Jll, = UkCkVz + T,U:(V~)~,
where Ck = diag(a1,. . . , q),
U = [Uk, Uk] and V = [Vk, Vk]. Furthermore,
(ak+l + @min{m,n})/2,
Proof. First define
where ak = [ ~ k + ~ ,
. . . , omin{m,n}]. It is readily checked that min, is achieved by the
right-hand side of Equation (3.3). Therefore all we need to prove is llA - Jll, 2 min,
for p = 2, F and for any J E Jk-
To this end we use standard perturbation analysis of singular values which states
lai(A) -ai(J)I 5 IIA- J112,
i = 1 ,.,., min{m,n},
It follows that
(Gi(A) - 4m2 I (44) - dJN2
5 IIA - J112F.
Notice that uk+l(J) =
= arnin{,,,}(J), it can be readily verified that the minima
of the left-hand sides of the above two inequalities, Le., min,, p = 2, F, are achieved
by 7 2 and TF, respectively. 0
In the following we will apply the above theorem to two example text
collections and see how close the associated term-document matrices are to the set
of matrices with low-rank-plus-shift structure. Our first example is the MEDLINE
collection from the Cornel1 SMART system . The term-document matrix is of size
3681 x 1033. The singular value distribution is plotted on the left of Figure 1. Our
second example is from a collection consisting of news articles from 20 newsgroups
 . The term-document matrix is of size 33583 x 1997. Its singular values are plotted
on the right of Figure 1. From Theorem 3.1, the best approximation from Jk to A is
A(k) E UkCkV'
+ T,U;(V,')'
Partial SVD and Latent Semantic Indexing
FIG. 1. Singular value distributions: 3681 x 1033 tern-document matrix of MEDLINE Collection
(left) and 33583 x 1997 NEWSGROUP Collection (right)
For the MEDLINE collection we have / A - A('oo)ll~/llAll~
= 0.2909 and for the
NEWSGROUP collection we have llA - A(100)Ilz/llAl12 = 0.0491. Several other text
collections from the Cornel1 SMART system have also been tested and we observed
similar singular value distributions: initially the singular values decrease rapidly and
then the spectrum curve levels off, but the singular values are never close to zero.
(Unless the sparse term-document matrix is stmcturallgl rank-deficient .) The last
point is very important: we usually should not treat those matrices simply as near
rank-deficient and it is more appropriate that the more general low-rank-plus-shift
structure with a nonzero cr be used (cf. Equation (2.2)).
4. The Low-Rank-Plus-shift Structure. We start with an examination of the
changes of the singular values of a matrix when its elements undergo certain type of
modifications. If some of the elements of a general matrix is set to zero, generally it is
not possible to tell whether the singular values of the matrix will increase or decrease.
However, a result we will show below states that the singular values of a matrix
will always decrease if some submatrices of the matrix are replaced by its low-rank
approximations. To proceed we introduce some notation: for any matrix A E Rmxn,
we will use bestk(A) to denote its best rank4 approximation (cf. Theorem 1.1), and
its singular values are assumed to be arranged in nonincreasing order,
cl(A) 2 02(A) 2 . * . 2 cm(A).
As a convention when we compare the singular values of two matrices with the same
number of rows but different number of columns we will count the singular values
according to the number of rows. With the above preparation we present our first
result. The proof is similar to that of a slightly special case presented in and
therefore is omitted.
4.1. Let A E Rmxn
and write A = [Al,A2]. Then for any kl and k2,
REMARK. It is not true that replacing arbitrary submatrices of a matrix by
their low-rank approximations will result in the decrease of its singular values as is
Partial SVD and Latent Semantic Indexing
illustrated in the following example: Let
A = [ : ; I , A = [ 1
Notice that 2 is obtained from A by replacing its (1,l) and (2,2) elements by zero (a
best rank-zero approximation). Even though the largest singular value decreases, its
smallest singular value increases.
It is rather easy to find examples for which strict inequalities hold in Theorem 4.1.
In the following we show that this will not be the case if A has the low-rank-plus-shift
structure.
4.2. Let A = [AI, A21 E RmXn
with m 2 n. Furthermore assume that
= x + a 2 ~ ,
where X is symmetric and positive semi-definite with rank(X) = k. Then there are
integers kl 5 k and k2 5 k with kl + k2 2 k such that
bestk([bestk, (AI),
(A2111 = bestk([Al, A&.
Proof. The general idea of the proof is to show that what is discarded when A1
is replaced by bestk(A1) and A2 is replaced by bestk(A2) will also be discarded when
bestk ([AI, Az]) is computed from [AI, Az]. To this end write
Since rank(X) = k, it follows that rank(ATA1- a21)- 5 k and rank(AzA2 - 021) 5 IC.
Let the eigendecompositions of
ATAi - a21 = VA, diag(Ci,,O)Vz,
AifA2 -a21 = V ~ ~ d i a g ( C ~ , , 0 ) V ~ ~ ,
where EA, E Rkl x k l , C A ~
are nonsingular with kl 5 k, k2 5 k. We can
write the SVD of AI and A2 as follows:
AI = U A ~
diag(kA,,aIt,)Vz = [Uy), U~)]diag(ka,,aIt,)[V~:),
(4.7) A2 = U A ~
diag(gA2,aItz)V~
= [Uti, U ~ i ] d i a g ( ~ ~ ~ , c r I t ~ ) [ V ~ 1 ) ,
where g ~ ,
= (Xi, +c~~Ik,)l/~
and 9 : ~ ~
and Ut! E ZmXk1,
k2, and tl = nl - kl , t 2 = 122 - k2, respectively, where ni is the column dimension
of Ai, i = 1,2. Now write Vx ATA~VA~
in a partitioned form as
Since X = ATA - a21 is symmetric positive semi-definite and rank(X) = k, it follows
that 512 = O,S21 = 0, S22 = 0 and kl + k2 2 rank(X) = k. Using the SVD of AI and
A2 in (4.6) and (4.7), Equation (4.8) becomes
Partial SVD and Latent Semantic Indexing
Let 6 be an orthonormal basis of R([UY!, Ut:]),
where E(.) to denote the column
space of a matrix. Then we can write
where B E 72kx(k1+k2)
with all its singular values greater than u, and k 5 k 5 k1 +k2.
Therefore]
the first term in the right hand side of the above is easily seen to be the matrix
[bestk,(Al), bestk,(Az)], and the relation in Equation (4.5) therefore holds. 0
REMARK. Generically we will have k1 + k2 = k. In the following we give an
example that shows the possibility of the case kl + k2 > k. Given any two positive
numbers a and b, choose & and 132 such that
Construct two matrices U1 and U2 as follows,
where ci = cos(&) and si = sin(&) for i = 1,2. Now construct the matrix A = [AI, A2]
Ai = U1 diag( I/=,
o), A2 = U2 diag( Jm,
we obtain that
and therefore k = 1. However, we also have
we use S I
T to denote STT = 0.
Partial SVD and Latent Semantic Indexing
and thus kl = k2 = 1. So we have the case kl + k2 > k.
In essence the result in Theorem 4.2 states that if A has the low-rank-
plus-shift structure, then an optimal low-rank approximation of A can be computed
by merging the optimal low-rank approximations of its two submatrices A1 and A2.
The result can be generalized to the case where A is partitioned into several blocks
In general kl and k2 are not available: they exist in the analysis in the
proof of Theorem 4.2 but never explicitly computed. However, since ki 5 k, i = 1,2,
the relation in Equation (4.5) still holds if we replace ki, i = 1,2, by k, i.e.,
A = [Ai, A2,. . . ,A#].
bestk([bestk(Al),bestk(Az)l) = bestk([Al, A2l).
Referring back to our discussion on LSI updating problem in Section 2, we see
that Theorem 4.2 justifies the replacement of A by its best rank-k approximation
bestk[bestk(A), D])
= bestk([A, a]),
assuming [A, D]
has the low-rank-plus-shift structure. That is to say, we will obtain
the same best low-rank approximation even though A is replaced by bestk(A). Nu-
merical results conducted on several text collections show that no retrieval accuracy
degradation occurs when updating is computed using a proper implementation ..
On the other hand, Theorem 4.2 also leads to some novel approaches for com-
puting a low-rank approximation of a large matrix. There are at least two general
approaches to pursue ideas based on Theorem 4.2:
e AN INCREMENTAL
METHOD. One is what we call incremental approach
whereby we can use certain sampling methods ta divide the whole collection
of documents into several groups: Start with one group and compute its
rank-k approximation, and then add the second group using the updating
algorithm to produce a new rank-k approximation, and repeat the whole
process. This incremental process can be very useful when the data collection
is very large and the whole term-document matrix can not reside completely
in main memory. Some computational results of this approach can be found
a A DIVIDE-AND-CONQUER
METHOD. Another approach is what we call a
divide-and-conquer approach, we can again divide the whole collection of
documents into several groups, and compute the rank-k approximation for
each group and then combine the results together into a rank-k approximation
for the whole data collection. Recursively, the rank-k approximation for each
group can also be computed using this divide-and-conquer approach and so
on. The approach has the property that computation can be organized with
high degree of coarse-grain parallelism. A parallel implementation of this
method is currently under investigation.
5. Perturbation Analysis. In this section, we consider the case where A only
approximately satisfies the low-rank-plus-shift property. Our main goal is to see to
what extent the result in Theorem 4.2 still holds in the presence of perturbation. We
first present some lemmas which are of their own interests as well. In the sequel 11
denotes two-norm and / I . l l ~ denotes Frobenius norm. We will use MATLAB notation
for submatrices: A(i: j ,k: 1) denotes rows i to j and columns k to 1 of A.
PartiaI SVD and Latent Semantic Indexing
LEMMA 5.1. Assume that the matrix X defined below is symmetric positive semi-
X = [ . A BT
Proof. Without loss of generality we assume that the matrix B is diagonal. (The
result still holds even if B is rectangular.) Write
B = diag(a1, ..., a,),
Let all and c11 be the (1,l) element of A and C, respectively. Then
a1 2 ... 2 a,.
for some 2-by-2 symmetric positive semi-definite X1 and E1 with 11El11 I
11311. Since
the smallest eigenvalue of X1 + E1 is no smaller than -11E111, it follows that
((a11 + c11)/2 + llJ%11)2 - ((a11 - .11>/q2
(IlAll + II~ll>(llCll
thus completing the proof. 0
LEMMA 5.2: Let the matrix X be pahitioneti as
X = [ B C ] .
Then IlXIl I
m={llAll, IlCllI + llm
Proof. The proof is straightforward and is therefore omitted. 0
5.3. Let A = [Al,Az] E Rmxn,m 2 n sat$&
= x + a 2 ~ +
where X symmetric positive semi-definite with rank(X) = k. If
X k W > 311~11 + 2JII~ll(llxll + 21lEll)?
then for some k1 5 IC, k2 5 1, and kl + k2 2 L, we have
besh(lA1 , 4 1 ) = b=tk([bestk, (AI),
Partial SVD and Latent Semantic Indexing
Proof. The proof is divided into several parts.
1) We first write the eigendecomposition of the following matrices:
- a21 = x + E = vdiag(A1, ..., A,)v~,
A T A ~ - u2 I = xi +
= K diag(Af), ..., A::
where X and Xi axe symmetric positive semi-definite, and V and K are orthogonal
for i = 1,2. The eigenvalues {Ak} and {A;’}
are arranged in nonincreasing order. It
follows that, for i = 1,2, there axe orthogonal matrices Vi such that
diag(dF’, ...,&)QT,
= Vi diag(Di1, Di2)KT,
where dj (4 -
A . + o and
Dil = diag(df), ..., dc)),
Di2 = diag(dki+l, ..., dni 1,
where ki = rank(Xi), i = 1,2. The definition of best low-rank approximation leads to
be&, (Ai) = UiDilvl,,
2) Using the above decompositions we now write the matrix A in sever.$ different’ - .
forms: A = [Ai, A21 = B[W1, W2IT = [BI, B2]WT, where
B1 = [UllDll, U21D211,
B2 = [UlZDlZ, U22D221,
It can be readily verified that
3) Let the eigendecomposition of BTBl - 021 be
BTBl - u21 = Gdiag(a1, ..., a,)GT,
where a1 2 ... >_ a,, s = IC1 + kp. Now partition diag(GT, I)B*B diag(G, I ) as
diag(GT, I)BTB diag(G, I) =
Partial SVD and Latent Semantic Indexing
where C = diag(a1, ..., a h ) , and the matrix E has the form
Furthermore, let the eigendecomposition of
E2 ] =&A&',
A = diag(X1, ..., An).
Now partition conformally,
, A = diag(A,,A2)
with A, = diag(A1,. . . , Xk). It can be verified that
It follows that we can find U orthogonal such that
B = [Bi, B2] = U ( A + c21)1/2QT diag(GT, I),
which leads to
bestk(B1W:)
= U ( : yl:k)(A1 +~21)1/2(Q(:yl:k))T
4) On the other hand, from
it follows that
batk(A) = U(: , 1 :k)(Al + o ~ I ) ~ / ~ ( Q ( :
= bestk(B1 WT) + A,
A = U ( : ,I:k)(Ai +a21)'/2QF1WT
where we have used Equation (5.11). Now we need to bound II[El,Ez]ll, and this will
be done in the following steps.
Partial SVD and Latent Semantic Indexing
5) Applying Lemma 5.1 to the matrix
we obtain the bound
I I ~ Z ~ ~ ~ 2 2 ~ 2 2 l l 2
(ll@2 - u2111 + 11-E11)(11D;2 - a2111 + 1 1 ~ 1 1 ) I 411~112,
llBD2 - a2111 I
IIB,TB1II2 I
411~II(IIB1TB1 - a2111 + 1 1 ~ 1 1 > I 4ll~ll(llXll+
where we have used llD;2 - a2111 5 IIEll,i = 1,2. By Lemma 5.2 we obtain'
6) Now applying Lemma 5.1 to BTB - a21 in Equation (5.9) we obtain
where we have bounded
8) Now we are ready to complete the proof of the theorem by showing that for
I"jl I IlEll + 2dll~ll(ll~ll
+ 211~11).
In fact, by definition, X j = Xj(BTB - a21), and therefore
IXj - Xj( diag(BTB1 - a21, BTB2 - a21))1 5 llBTBIII.
On the other hand, we have for j > k,
IAj(BTB2 - a21)1 I IlBrB2 - a2111 5 311EIl.
The assumption of the theorem implies that
X k > IlBZTBlII + 311-m
and therefore for j 5 k,
Xj( diag(B, BI - u21, BTB2 - a21)) = Xj(BTB1 - a21) = aj.
Now for any j > k, there is ij > k such that Iaj - Aij I 5 llB$BlII, and thus
IajI I IXij I + IIXBlII 5 IIEII + ~JIIEII(IIxII
+ ~IIEII),
completing the proof. 0
In many of our numerical experiments, we observed that llAl1 =
O(llEll) versus the bound 11A11 = O(llE[11/2) given in the above theorem. Here we
Partial SVD and Latent Semantic Indexing
give an example for which we do have IlAll = O(llE111/2). Let E be small, and define
,W = (1 + E + J(i + c), + 4e)/2 = 1 + 2E - 2e2 + o ( € ~ ) ,
a = (111 - l)/t = t(1 - E + O(2)).
/ ~ 2 = (1 + E - J(l
+ 4 ~ ) / 2 = -E + 2c2 +
Then it can be readily verified that
E = (cL2/(1+ a2)) I ia -4 I '
With IC = rank(X) = 2 and IlEll = lp2l = ~ ( 1 -
2~ + O(e2)), it is easy to see that
x + E = Vdiag(Pl,P1,C12,LL2)v,
c1 = d p ,
is orthogonal and symmetric. For a given D > 0, define
Now construct matrix A as A = [AI, A,], where
AI = [el,es,ez,e4]DV(: y1:2), A2 = [el,es,ez,e4]DV(: ,3:4)
with D = diag(cl,cl,c2,cz), and 1 4 = [el,e2,es,e4] is the identity matrix. Then we
bestk(A) = (c1/dS)[e1,e3]V(l:2, :).
Since a < 1, it can be verified that
Partial SVD and Latent Semantic Indexing
It follows that
bestk( [best1 (AI), best1 (Az)]) = (l/JitaZ>
and therefore
A = bestk(A) - bestk([bestl(Al), bestl(A2)l)
which leads to 11A11 = a c I / d m . Then we see that
lim l l A l l / ~ =
J2(1+ c2 + 2~ + 0 ( ~ 2 ) ) / [ ( 1 + 0 ( ~ ~ ) ) ( 1 - ~ + 0 ( ~ ~ ) ) = d
6. Concluding Remarks. In this paper we present a detailed analysis of ma-
trices with low-rank-plus-shift structure. Our emphasis is placed on justifying some
novel methods for partial SVD computation and partial SVD updating problems aris-
ing from LSI in information retrieval. Our perturbation analysis demonstrates that
the results we have derived are still valid even the low-rank-plus-shift structure is
approximately satisfied. The results we have proved provide theoretical justifications
for the novel LSI updating algorithms and the incremental and divide-and-conquer
approaches proposed in [14, 161. Our future research will concentrate on further de-
veloping the numerical algorithms and their parallel implementations. We will also
refine our perturbation analysis, especially we will try to find conditions on the ma-
trix A that will allow us to improve the perturbation bounds in.Theorem 5.3-from
wl~l11’2) to O(llJm.