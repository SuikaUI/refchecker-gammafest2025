Artificial Intelligence: the global landscape of ethics guidelines
Anna Jobin a, Marcello Ienca a, Effy Vayena a*
a Health Ethics & Policy Lab, ETH Zurich, 8092 Zurich, Switzerland
* Corresponding author: 
 
© The authors 2019
In the last five years, private companies, research institutions as well as public sector
organisations have issued principles and guidelines for ethical AI, yet there is debate about
both what constitutes “ethical AI” and which ethical requirements, technical standards and
best practices are needed for its realization. To investigate whether a global agreement on
these questions is emerging, we mapped and analyzed the current corpus of principles and
guidelines on ethical AI. Our results reveal a global convergence emerging around five
ethical principles (transparency, justice and fairness, non-maleficence, responsibility and
privacy), with substantive divergence in relation to how these principles are interpreted;
why they are deemed important; what issue, domain or actors they pertain to; and how they
should be implemented. Our findings highlight the importance of integrating guidelinedevelopment efforts with substantive ethical analysis and adequate implementation
strategies.
MAIN ARTICLE
Introduction
Artificial Intelligence (AI), or the theory and development of computer systems able to
perform tasks normally requiring human intelligence, is widely heralded as an ongoing
“revolution” transforming science and society altogether1,2. While approaches to AI such as
machine learning, deep learning and artificial neural networks are reshaping data processing
and analysis3, autonomous and semi-autonomous systems are being increasingly used in a
variety of sectors including healthcare, transportation and the production chain4. In light of
its powerful transformative force and profound impact across various societal domains, AI
has sparked ample debate about the principles and values that should guide its development
and use5,6. Fears that AI might jeopardize jobs for human workers7, be misused by
malevolent actors8, elude accountability or inadvertently disseminate bias and thereby
undermine fairness9 have been at the forefront of the recent scientific literature and media
coverage. Several studies have discussed the topic of ethical AI10–13, notably in metaassessments14–16 or in relation to systemic risks17,18 and unintended negative consequences
like algorithmic bias or discrimination19–21.
National and international organisations have responded to these societal fears by
developing ad hoc expert committees on AI, often commissioned with the drafting of policy
documents. These include the High-Level Expert Group on Artificial Intelligence appointed
by the European Commission, the expert group on AI in Society of the Organisation for
Economic Co-operation and Development (OECD), the Advisory Council on the Ethical
Use of Artificial Intelligence and Data in Singapore, and the select committee on Artificial
Intelligence of the United Kingdom (UK) House of Lords. As part of their institutional
appointments, these committees have produced or are reportedly producing reports and
guidance documents on AI. Similar efforts are taking place in the private sector, especially
among corporations who rely on AI for their business. In 2018 alone, companies such as
Google and SAP have publicly released AI guidelines and principles. Declarations and
recommendations have also been issued by professional associations and non-profit
organisations such as the Association of Computing Machinery (ACM), Access Now and
Amnesty International. The intense efforts of such a diverse set of stakeholders in issuing
AI principles and policies demonstrate not only the need for ethical guidance, but also the
strong interest of these stakeholders to shape the ethics of AI in ways that meet their
respective priorities16. Notably, the private sector’s involvement in the AI-ethics arena has
been called into question for potentially using such high-level soft-policy as a portmanteau
to either render a social problem technical16 or to eschew regulation altogether22. Beyond
the composition of the groups that have produced ethical guidance on AI, the content of this
guidance itself is of interest. Are these various groups converging on what ethical AI should
be, and the ethical principles that will determine the development of AI? If they diverge,
what are these differences and can they be reconciled?
To answer these questions, we conducted a scoping review of the existing corpus of
guidelines on ethical AI. Our analysis aims at mapping the global landscape of existing
principles and guidelines for ethical AI and thereby determining whether a global
convergence is emerging regarding both the principles for ethical AI and the requirements
for its realization. This analysis will inform scientists, research institutions, funding
agencies, governmental and inter-governmental organisations and other relevant
stakeholders involved in the advancement of ethically responsible innovation in AI.
Our search identified 84 documents containing ethical principles or guidelines for AI (cf.
Table 1). Data reveal a significant increase over time in the number of publications, with
88% having been released after 2016 (cf. SI Table S1). Data breakdown by type and
geographic location of issuing organisation (cf. SI Table S1) shows that most documents
were produced by private companies (n=19; 22.6%) and governmental agencies
respectively (n=18; 21.4%), followed by academic and research institutions (n=9; 10.7%),
inter-governmental or supra-national organisations (n=8; 9.5%), non-profit organisations
and professional associations/scientific societies (n=7 each; 8.3% each), private sector
alliances (n=4; 4.8%), research alliances (n=1; 1.2%), science foundations (n=1; 1.2%),
federations of worker unions (n=1; 1.2%) and political parties (n=1; 1.2%). Four documents
were issued by initiatives belonging to more than one of the above categories and four more
could not be classified at all (4.8% each).
Table 1- Ethical guidelines for AI by country of issuer
Name of Document/Website
Country of
Artificial Intelligence. Australia's Ethics Framework. A
discussion Paper
Department of Industry Innovation and Science
Montréal Declaration: Responsible AI
Université de Montréal
Work in the age of artificial intelligence. Four perspectives on the
economy, employment, skills and ethics
Ministry of Economic Affairs and Employment
Tieto’s AI ethics guidelines
Commitments and principles
How can humans keep the upper hand? Report on the ethical
matters raised by AI algorithms
French Data Protection Authority (CNIL)
For a meaningful Artificial Intelligence. Towards a French and
European strategy
Mission Villani
Ethique de la recherche en robotique
CERNA (Allistene)
AI Guidelines
Deutsche Telekom
SAP’s guiding principles for artificial intelligence
Automated and Connected Driving: Report
Federal Ministry of Transport and Digital Infrastructure, Ethics
Commission
Ethics Policy
Icelandic Institute for Intelligent Machines (IIIM)
Discussion Paper: National Strategy for Artificial Intelligence
National Institution for Transforming India (Niti Aayog)
L'intelligenzia artificiale al servizio del cittadino
Agenzia per l'Italia Digitale (AGID)
The Japanese Society for Artificial Intelligence Ethical
Guidelines
Japanese Society for Artificial Intelligence
Report on Artificial Intelligence and Human Society (Unofficial
translation)
Advisory Board on Artificial Intelligence and Human Society
(initiative of the Minister of State for Science and Technology
Draft AI R&D Guidelines for International Discussions
Institute for Information and Communications Policy (IICP), The
Conference toward AI Network Society
Sony Group AI Ethics Guidelines
Human Rights in the Robot Age Report
The Rathenau Institute
Netherlands
Dutch Artificial Intelligence Manifesto
Special Interest Group on Artificial Intelligence (SIGAI), ICT
Platform Netherlands (IPN)
Netherlands
Artificial intelligence and privacy
The Norwegian Data Protection Authority
Discussion Paper on Artificial Intelligence (AI) and Personal
Data - Fostering Responsible Development and Adoption of AI
Personal Data Protection Commission Singapore
Mid- to Long-Term Master Plan in Preparation for the Intelligent
Information Society
Government of the Republic of Korea
South Korea
AI Principles of Telefónica
Telefonica
AI Principles & Ethics
Smart Dubai
Principles of robotics
Engineering and Physical Sciences Research Council UK (EPSRC)
The Ethics of Code: Developing AI for Business with Five Core
Principles
Big data, artificial intelligence, machine learning and data
protection
Information Commissioner's Office
DeepMind Ethics & Society Principles
DeepMind Ethics & Society
Business Ethics and Artificial Intelligence
Institute of Business Ethics
AI in the UK: ready, willing and able?
UK House of Lords, Select Committee on Artificial Intelligence
Artificial Intelligence (AI) in Health
Royal College of Physicians
Initial code of conduct for data-driven health and care technology
UK Department of Health & Social Care
Ethics Framework - Responsible AI
Machine Intelligence Garage Ethics Committee
The responsible AI framework
PriceWaterhouseCoopers UK
Responsible AI and robotics. An ethical framework.
Accenture UK
Machine learning: the power and promise of computers that learn
by example
The Royal Society
Ethical, social, and political challenges of Artificial Intelligence
Future Advocacy
Unified Ethical Frame for Big Data Analysis. IAF Big Data
Ethics Initiative, Part A
The Information Accountability Foundation
The AI Now Report. The Social and Economic Implications of
Artificial Intelligence Technologies in the Near-Term
AI Now Institute
Statement on Algorithmic Transparency and Accountability
Association for Computing Machinery (ACM)
AI Principles
Future of Life Institute
AI - Our approach
Artificial Intelligence. The Public Policy Opportunity
Intel Corporation
IBM’s Principles for Trust and Transparency
OpenAI Charter
Our principles
Policy Recommendations on Augmented Intelligence in Health
Care H-480.940
American Medical Association (AMA)
Everyday Ethics for Artificial Intelligence. A practical guide for
designers & developers
Governing Artificial Intelligence. Upholding Human Rights &
Data & Society
Intel’s AI Privacy Policy White Paper. Protecting individuals’
privacy and data in the artificial intelligence world
Intel Corporation
Introducing Unity’s Guiding Principles for Ethical AI – Unity
Unity Technologies
Digital Decisions
Center for Democracy & Technology
Science, Law and Society (SLS) Initiative
The Future Society
AI Now 2018 Report
AI Now Institute
Responsible bots: 10 guidelines for developers of conversational
Preparing for the future of Artificial Intelligence
Executive Office of the President; National Science and Technology
Council; Committee on Technology
The National Artificial Intelligence Research and Development
Strategic Plan
National Science and Technology Council; Networking and
Information Technology Research and Development Subcommittee
AI Now 2017 Report
AI Now Institute
Position on Robotics and Artificial Intelligence
The Greens (Green Working Group Robots)
Report with recommendations to the Commission on Civil Law
Rules on Robotics
European Parliament
Ethics Guidelines for Trustworthy AI
High-Level Expert Group on Artificial Intelligence
AI4People—An Ethical Framework for a Good AI Society:
Opportunities, Risks, Principles, and Recommendations
European ethical Charter on the use of Artificial Intelligence in
judicial systems and their environment
Concil of Europe: European Commission for the efficiency of
Justice (CEPEJ)
Statement on Artificial Intelligence, Robotics and 'Autonomous'
European Commission, European Group on Ethics in Science and
New Technologies
Artificial Intelligence and Machine Learning: Policy Paper
Internet Society
international
Report of COMEST on Robotics Ethics
COMEST/UNESCO
international
Ethical Principles for Artificial Intelligence and Data Analytics
Software & Information Industry Association (SIIA), Public Policy
international
ITI AI Policy Principles
Information Technology Industry Council (ITI)
international
Ethically Aligned Design. A Vision for Prioritizing Human Wellbeing with Autonomous and Intelligent Systems, version 2
Institute of Electrical and Electronics Engineers (IEEE), The IEEE
Global Initiative on Ethics of Autonomous and Intelligent Systems
international
Top 10 Principles for Ethical Artificial Intelligence
UNI Global Union
international
The Malicious Use of Artificial Intelligence: Forecasting,
Prevention, and Mitigation
Future of Humanity Institute; University of Oxford; Centre for the
Study of Existential Risk; University of Cambridge; Center for a
New American Security; Electronic Frontier Foundation; OpenAI
international
White Paper: How to Prevent Discriminatory Outcomes in
Machine Learning
WEF, Global Future Council on Human Rights 2016-2018
international
Privacy and Freedom of Expression In the Age of Artificial
Intelligence
Privacy International & Article 19
international
The Toronto Declaration: Protecting the right to equality and nondiscrimination in machine learning systems
Access Now ; Amnesty International
international
Charlevoix Common Vision for the Future of Artificial
Intelligence
Leaders of the G7
international
Artificial Intelligence: open questions about gender inclusion
international
Declaration on ethics and data protection in Artificial Intelligence
international
Universal Guidelines for Artificial Intelligence
The Public Voice
international
Ethics of AI in Radiology: European and North American
Multisociety Statement
American College of Radiology; European Society of Radiology;
Radiology Society of North America; Society for Imaging
Informatics in Medicine; European Society of Medical Imaging
Informatics; Canadian Association of Radiologists; American
Association of Physicists in Medicine
international
Ethically Aligned Design: A Vision for Prioritizing Human Wellbeing with Autonomous and Intelligent Systems, First Edition
Institute of Electrical and Electronics Engineers (IEEE), The IEEE
Global Initiative on Ethics of Autonomous and Intelligent Systems
international
Partnership on AI
Principles for Accountable Algorithms and a Social Impact
Statement for Algorithms
Fairness, Accountability, and Transparency in Machine Learning
10 Principles of responsible AI
Women leading in AI
In terms of geographic distribution, data show a significant representation of more
economically developed countries (MEDC), with the USA (n=20; 23.8%) and the UK
(n=14; 16.7%) together accounting for more than a third of all ethical AI principles,
followed by Japan (n=4; 4.8%), Germany, France, and Finland (each n=3; 3.6% each). The
cumulative number of sources from the European Union, comprising both documents issued
by EU institutions (n=6) and documents issued within each member state (13 in total),
accounts for 19 documents overall. African and South-American countries are not
represented independently from international or supra-national organisations (cf. Figure 1).
Figure 1- Geographic distribution of issuers of ethical AI guidelines by number of documents released
Figure 1: Geographic distribution of issuers of ethical AI guidelines by number of
documents released. Most ethics guidelines are released in the United States (n=20) and
within the European Union (19), followed by the United Kingdom (14) and Japan (4).
Canada, Iceland, Norway, the United Arab Emirates, India, Singapore, South Korea,
Australia are represented with 1 document each. Having endorsed a distinct G7 statement,
member states of the G7 countries are highlighted separately. Map created using
mapchart.net.
Data breakdown by target audience indicates that most principles and guidelines are
addressed to multiple stakeholder groups (n=27; 32.1%). Another significant portion of the
documents is self-directed, as they are addressed to a category of stakeholders within the
sphere of activity of the issuer such as the members of the issuing organisation or the issuing
company’s employees (n=24; 28.6%). Finally, some documents target the public sector
(n=10; 11.9%), the private sector (n=5; 6.0%), or other specific stakeholders beyond
members of the issuing organisation, namely developers or designers (n=3; 3.6%),
‘organisations’ (n=1; 1.2%) and researchers (n=1; 1.2%). 13 sources (15.5%) do not specify
their target audience (cf. SI Table S1).
Eleven overarching ethical values and principles have emerged from our content analysis.
These are, by frequency of the number of sources in which they were featured: transparency,
justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and
autonomy, trust, dignity, sustainability, and solidarity (cf. Table 2).
Table 2 – Ethical principles identified in existing AI guidelines
Ethical principle
Included codes
Transparency
Transparency, explainability, explicability, understandability,
interpretability, communication, disclosure, showing
Justice & fairness
Justice, fairness, consistency, inclusion, equality, equity, (non-)bias,
(non-)discrimination, diversity, plurality, accessibility, reversibility,
remedy, redress, challenge, access and distribution
Non-maleficence
Non-maleficence, security, safety, harm, protection, precaution,
prevention, integrity (bodily or mental), non-subversion
Responsibility
Responsibility, accountability, liability, acting with integrity
Privacy, personal or private information
Beneficence
Benefits, beneficence, well-being, peace, social good, common good
Freedom, autonomy, consent, choice, self-determination, liberty,
empowerment
Sustainability
Sustainability, environment (nature), energy, resources (energy)
Solidarity
Solidarity, social security, cohesion
No single ethical principle appeared to be common to the entire corpus of documents,
although there is an emerging convergence around the following principles: transparency,
justice and fairness, non-maleficence, responsibility, and privacy. These principles are
referenced in more than half of all the sources. Nonetheless, further thematic analysis
reveals significant semantic and conceptual divergences in both how the eleven ethical
principles are interpreted and the specific recommendations or areas of concern derived
from each. A detailed thematic evaluation is presented in the following.
Transparency
Featured in 73/84 sources, transparency is the most prevalent principle in the current
literature. Thematic analysis reveals significant variation in relation to the interpretation,
justification, domain of application, and mode of achievement. References to transparency
comprise efforts to increase explainability, interpretability or other acts of communication
and disclosure (cf. Table 2). Principal domains of application include data use23–26, human-
AI interaction23,27–35, automated decisions26,36–46, and the purpose of data use or application
of AI systems24,27,47–51. Primarily, transparency is presented as a way to minimize harm and
improve AI36–38,44,45,49,52–55, though some sources underline its benefit for legal
reasons37,45,46,49,50,52 or to foster trust23,24,29,33,36,37,48,51,52,56–58. A few sources also link
transparency to dialogue, participation, and the principles of democracy30,41,49,50,52,59.
To achieve greater transparency, many sources suggest increased disclosure of information
by those developing or deploying AI systems36,51,60,61, although specifications regarding
what should be communicated vary greatly: use of AI45, source code31,52,62, data use35,47,50,58,
evidence base for AI use57, limitations25,33,47,51,58,60,63, laws62,64, responsibility for AI40,
investments in AI44,65 and possible impact66. The provision of explanations ‘in non-technical
humans37,60
encouraged.
auditability28,39,44,45,50,59,61,62,67,68 are mainly proposed by data protection offices and NPOs,
it is mostly the private sector that suggests technical solutions27,30,52,59,69,70. Alternative
measures focus on oversight45,47,48,55,62, interaction and mediation with stakeholders and the
public24,32,36,51,61,71 and the facilitation of whistleblowing36,60.
Justice, fairness, and equity
Justice is mainly expressed in terms of fairness23,25,27–29,48,50,58,60,66,72–77, and of prevention,
monitoring
mitigation
bias23,28,33,40,47,52,54,58,64,69,73,74,78–80
discrimination28,33,36,38,44,45,50,55,56,60,68,81–84, the latter being significantly less referenced than
the first two by the private sector. Whereas some sources focus on justice as respect for
diversity31,38,56,59,65,66,70,72,78,80,85,86, inclusion31,45,47,51,72,80 and equality41,45,51,59,60,72,78, others
call for a possibility to appeal or challenge decisions28,35–37,74,79, or the right to
redress33,42,45,46,50,68,85 and remedy45,48. Sources also emphasize the importance of fair access
to AI59,70,87, to data33,37,44,67,83,88–90, and to the benefits of AI37,38,80,91. Issuers from the public
sector place particular emphasis on AI’s impact on the labor market37,38,55,84,92, and the need
to address democratic33,38,59,73 or societal31,48,55,65 issues. Sources focusing on the risk of
biases within datasets underline the importance of acquiring and processing accurate,
complete and diverse data23,28,52,70,93, especially training data27,33,35,38,52,58.
If specified, the preservation and promotion of justice are proposed to be pursued through:
(a) technical solutions such as standards50,68,89 or explicit normative encoding28,37,43,67; (b)
transparency54,62, notably by providing information36,38,79 and raising public awareness of
existing rights and regulation28,59; (c) testing52,58,67,69, monitoring54,56 and auditing39,46,50,67,
the preferred solution of notably data protection offices; (d) developing or strengthening the
rule of law and the right to appeal, recourse, redress, or remedy37,38,42,45,46,48,68,74,79; (e) via
systemic changes and processes such as governmental action42,45,87,92 and oversight94, a
more interdisciplinary47,65,85,93 or otherwise diverse58,59,70,85,87,95 workforce, as well as better
inclusion of civil society or other relevant stakeholders in an interactive
manner28,33,41,46,55,57,58,65,68,69,79,80,86 and increased attention to the distribution of
benefits25,33,38,48,63,76.
Non-maleficence
References to non-maleficence outweigh those to beneficence by a factor of 1.5 and
encompass general calls for safety and security80,90,96,97 or state that AI should never cause
foreseeable or unintentional harm23,30,33,56,60,79. More granular considerations entail the
avoidance of specific risks or potential harms, e.g. intentional misuse via cyberwarfare and
malicious hacking51,53,54,78,81,89, and suggest risk-management strategies. Harm is primarily
interpreted as discrimination38,44,47,48,50,95,98, violation of privacy23,35,44,64,78,98,99, or bodily
harm25,30,31,33,56,92,96,100. Less frequent characterizations include loss of trust30 or skills44,
‘radical individualism’38, the risk that technological progress might outpace regulatory
measures57, negative impacts on long-term social well-being44, on infrastructure44, or on
psychological35,56, emotional56 or economic aspects44,56.
Harm-prevention guidelines focus primarily on technical measures and governance
strategies, ranging from interventions at the level of AI research27,47,64,79,85,101,
design23,25,27,32,39,56,58, technology development and/or deployment54 to lateral and continuous
approaches33,55,63. Technical solutions include in-built data quality evaluations25 or
security23 and privacy by design23,27,39, though notable exceptions also advocate for
establishing industry standards30,64,102. Proposed governance strategies include active
cooperation across disciplines and stakeholders33,47,53,62, compliance with existing or new
legislation27,31,35,81,95,99, and the need to establish oversight processes and practices, notably
tests36,38,47,74,79, monitoring36,58, audits and assessments by internal units, customers, users,
independent third parties, or governmental entities40,48,51,58,81,94,95,98, often geared towards
standards for AI implementation and outcome assessment. Most sources explicitly mention
potential ‘dual-use’8,32,33,38,60,79 or imply that damages may be unavoidable, in which case
risks should be assessed40,48,51, reduced40,69,72–74, and mitigated34,35,38,53,63,68, and the
attribution of liability should be clearly defined31,37,38,44,82.
Responsibility and accountability
Despite widespread references to ‘responsible AI’43,51,78,83, responsibility and accountability
are rarely defined. Nonetheless, specific recommendations include acting with
‘integrity’47,52,60 and clarifying the attribution of responsibility and legal liability23,58,78,103,
if possible upfront36, in contracts52 or, alternatively, by centering on remedy26. In contrast,
other sources suggest focusing on the underlying reasons and processes that may lead to
potential harm74,83. Yet others underline the responsibility of whistleblowing in case of
potential harm36,55,60, and aim at promoting diversity49,92 or introducing ethics into STEM
education59. Very different actors are named as being responsible and accountable for AI’s
actions and decisions: AI developers58,60,73,96, designers36,44, ‘institutions’40,42 or
‘industry’69. Further disagreement emerged on whether AI should be held accountable in a
human-like manner70 or whether humans should always be the only actors who are
ultimately responsible for technological artifacts31,32,35,37,52,92.
Ethical AI sees privacy both as a value to uphold44,64,75,99 and as a right to be
protected27,28,37,38,53. While often undefined, privacy is often presented in relation to data
protection23,27,36,53,58,66,71,79,83,98 and data security27,35,64,66,88,98. A few sources link privacy to
freedom38,53 or trust74,92. Suggested modes of achievement fall into three categories:
technical solutions64,80 such as differential privacy74,89, privacy by design25,27,28,79,98, data
minimization36,58, and access control36,58, calls for more research47,64,74,98 and awareness64,74,
and regulatory approaches25,52,71, with sources referring to legal compliance more
broadly27,32,36,58,60,81, or suggesting certificates104 or the creation or adaptation of laws and
regulations to accommodate the specificities of AI64,74,88,105.
Beneficence
While promoting good (beneficence in ethical terms) is often mentioned, it is rarely defined,
though notable exceptions mention the augmentation of human senses86, the promotion of
human well-being and flourishing34,90, peace and happiness60, the creation of socioeconomic opportunities36, and economic prosperity37,53. Similar uncertainty concerns the
actors that should benefit from AI: private sector issuers tend to highlight the benefit of AI
for customers23,48, though many sources require AI to be shared49,52,76 and to benefit
‘everyone’36,59,65,84, ‘humanity’27,37,44,60,100,102, both of the above48,66, ‘society’34,87, ‘as many
people as possible’37,53,99, ‘all sentient creatures’83, the ‘planet’37,72 and the environment38,90.
Strategies for the promotion of good include aligning AI with human values34,44, advancing
‘scientific understanding of the world’100, minimizing power concentration102 or,
conversely, using power ‘for the benefit of human rights’82; working more closely with
‘affected’ people65, minimizing conflicts of interests102; proving beneficence through
customer demand48 and feedback58, and developing new metrics and measurements for
human well-being44,90.
Freedom and autonomy
Whereas some sources specifically refer to the freedom of expression28,73,82,105 or
informational self-determination28,90 and ‘privacy-protecting user controls’58, others
generally promote freedom31,69,72, empowerment28,52,99 or autonomy31,33,62,77,81,96. Some
documents refer to autonomy as a positive freedom, specifically the freedom to flourish36,
to self-determination through democratic means38, the right to establish and develop
relationships with other human beings38,92, the freedom to withdraw consent67, or the
freedom to use a preferred platform or technology73,80. Other documents focus on negative
freedom, for example freedom from technological experimentation82, manipulation33 or
surveillance38. Freedom and autonomy are believed to be promoted through transparency
and predictable AI38, by not ‘reducing options for and knowledge of citizens’38, by actively
increasing people’s knowledge about AI36,52,62, giving notice and consent79 or, conversely,
by actively refraining from collecting and spreading data in absence of informed
consent30,38,44,55,74.
References to trust include calls for trustworthy AI research and technology50,97,99,
trustworthy AI developers and organisations51,60,66, trustworthy ‘design principles’91, or
underline the importance of customers’ trust23,52,58,66,74,80. Calls for trust are proposed
because a culture of trust among scientists and engineers is believed to support the
achievement of other organisational goals99, or because overall trust in the
recommendations, judgments and uses of AI is indispensable for AI to ‘fulfill its world
changing potential’24. This last point is contradicted by one guideline explicitly warning
against excessive trust in AI81. Suggestions for building or sustaining trust include
education33, reliability50,51, accountability56, processes to monitor and evaluate the integrity
of AI systems over time51 and tools and techniques ensuring compliance with norms and
standards43,63. Whereas some guidelines require AI to be transparent37,43,57,58,
understandable36,37, or explainable52 in order to build trust, another one explicitly suggests
that, instead of demanding understandability, it should be ensured that AI fulfills public
expectations50. Other reported facilitators of trust include ‘a Certificate of Fairness’104,
multi-stakeholder dialogue64, awareness about the value of using personal data74, and
avoiding harm30,56.
Sustainability
To the extent that is referenced, sustainability calls for development and deployment of AI
to consider protecting the environment33,38,46, improving the planet’s ecosystem and
biodiversity37, contributing to fairer and more equal societies65 and promoting peace66.
Ideally, AI creates sustainable systems44,76,90 that process data sustainably43 and whose
insights remain valid over time48. To achieve this aim, AI should be designed, deployed and
managed with care38 to increase its energy efficiency and minimize its ecological footprint31.
To make future developments sustainable, corporations are asked to create policies ensuring
accountability in the domain of potential job losses37 and to use challenges as an opportunity
for innovation38.
While dignity remains undefined in existing guidelines, safe the specification that it is a
prerogative of humans but not robots92, there is frequent reference to what it entails: dignity
is intertwined with human rights101 or otherwise means avoiding harm31, forced
acceptance31, automated classification38, and unknown human-AI interaction38. It is argued
that AI should not diminish33 or destroy80 but respect82, preserve69 or even increase human
dignity36,37. Dignity is believed to be preserved if it is respected by AI developers in the first
place96 and promoted through new legislation38, through governance initiatives36, or through
government-issued technical and methodological guidelines82.
Solidarity
Solidarity is mostly referenced in relation to the implications of AI for the labor market104.
Sources call for a strong social safety net37,84. They underline the need for redistributing the
benefits of AI in order not to threaten social cohesion49 and respecting potentially vulnerable
persons and groups33. Lastly, there is a warning of data collection and practices focused on
individuals which may undermine solidarity in favour of ‘radical individualism’38.
Discussion
We found a rapid increase in the number and variety of guidance documents for ethical AI,
demonstrating the increasing active involvement of the international community.
Organisations publishing AI guidelines come from a wide range of sectors. In particular the
nearly equivalent proportion of documents issued by the public sector (i.e. governmental
and inter-governmental organisations) and the private sector (companies and private sector
alliances) indicate that the ethical challenges of AI concern both public entities and private
enterprises. However, there is significant divergence in the solutions proposed to meet the
ethical challenges of AI. Further, the relative underrepresentation of geographic areas such
as Africa, South and Central America and Central Asia indicates that the international
debate over ethical AI may not be happening globally in equal measures. MEDC countries
are shaping this debate more than others, which raises concerns about neglecting local
knowledge, cultural pluralism and global fairness.
The proliferation of soft-law efforts can be interpreted as a governance response to advanced
research into AI, whose research output and market size have drastically increased106 in
recent years. Our analysis shows the emergence of an apparent cross-stakeholder
convergence on promoting the ethical principles of transparency, justice, non-maleficence,
responsibility, and privacy. Nonetheless, our thematic analysis reveals substantive
divergences in relation to four major factors: (i) how ethical principles are interpreted, (ii)
why they are deemed important, (iii) what issue, domain or actors they pertain to, and (iv)
how they should be implemented. Furthermore, unclarity remains as to which ethical
principles should be prioritized, how conflicts between ethical principles should be resolved,
who should enforce ethical oversight on AI and how researchers and institutions can comply
with the resulting guidelines. These findings suggest the existence of a gap at the crosssection of principles formulation and their implementation into practice which can hardly
be solved through technical expertise or top-down approaches.
Although no single ethical principle is explicitly endorsed by all existing guidelines,
transparency, justice and fairness, non-maleficence, responsibility and privacy are each
referenced in more than half of all guidelines. This focus could be indicating a developing
convergence on ethical AI around these principles in the global policy landscape. In
particular, the prevalence of calls for transparency, justice and fairness points to an emerging
moral priority to require transparent processes throughout the entire AI continuum (from
transparency in the development and design of algorithms to transparent practices for AI
use), and to caution the global community against the risk that AI might increase inequality
if justice and fairness considerations are not adequately addressed. Both these themes appear
to be intertwined with the theme of responsibility, as the promotion of both transparency
and justice seems to postulate increased responsibility and accountability on the side of AI
makers and deployers.
It has been argued that transparency is not an ethical principle per se, but rather “a proethical
condition for enabling or impairing other ethical practices or principles”107. The proethical
nature of transparency might partly explain its higher prevalence compared to other ethical
principles. It is notable that current guidelines place significant value in the promotion of
responsibility and accountability, yet few of them emphasize the duty of all stakeholders
involved in the development and deployment of AI to act with integrity. This mismatch is
probably associated with the observation that existing guidelines fail to establish a full
correspondence between principles and actionable requirements, with several principles
remaining uncharacterized or disconnected from the requirements necessary for their
realization.
As codes related to non-maleficence outnumber those related to beneficence, it appears that,
for the current AI community, the moral obligation to preventing harm takes precedence
over the promotion of good. This fact can be partly interpreted as an instance of the socalled negativity bias, i.e. a general cognitive bias to give greater weight to negative
entities108,109. This negative characterization of ethical values is further emphasized by the
fact that existing guidelines focus primarily on how to preserve privacy, dignity, autonomy
and individual freedom in spite of advances in AI, while largely neglecting whether these
principles could be promoted through responsible innovation in AI110.
The issue of trust in AI, while being addressed by less than one third of all sources, tackles
a critical ethical dilemma in AI governance: determining whether it is morally desirable to
foster public trust in AI. While several sources, especially those produced within the private
sector, highlight the importance of fostering trust in AI through educational and awarenessraising activities, a smaller number of sources contend that trust in AI may actually diminish
scrutiny and undermine some societal obligations of AI producers111. This possibility would
challenge the dominant view in AI ethics that building public trust in AI is a fundamental
requirement for ethical governance112.
The relative thematic underrepresentation of sustainability and solidarity suggests that these
topics might be currently flying under the radar of the mainstream ethical discourse on AI.
The underrepresentation of sustainability-related principles is particularly problematic in
light of the fact that the deployment of AI requires massive computational resources which,
in turn, require high energy consumption. The environmental impact of AI, however, does
not only involve the negative effects of high-footprint digital infrastructures, but also the
possibility of harnessing AI for the benefit of ecosystems and the entire biosphere. This
latter point, highlighted in a report by the World Economic Forum113 though not in the AI
guidelines by the same institution, requires wider endorsement to become entrenched in the
ethical AI narrative. The ethical principle of solidarity is sparsely referenced, typically in
association with the development of inclusive strategies for the prevention of job losses and
unfair sharing of burdens. Little attention is devoted to promoting solidarity through the
emerging possibility of using AI expertise for solving humanitarian challenges, a mission
that is currently being pursued, among others, by intergovernmental organisations such as
the United Nations Office for Project Services (UNOPS)114 or the World Health
Organization (WHO) and private companies such as Microsoft115. As the humanitarian cost
of anthropogenic climate change is rapidly increasing116, the principles of sustainability and
solidarity appear strictly intertwined though poorly represented compared to other
principles.
While numerical data indicate an emerging convergence around the promotion of some
ethical principles, in-depth thematic analysis paints a more complicated picture, as there are
critical differences in how these principles are interpreted as well as what requirements are
considered to be necessary for their realization. Results show that different and often
conflicting measures are proposed for the practical achievement of ethical AI. For example,
the need for ever larger, more diverse datasets to “unbias” AI appears difficult to conciliate
with the requirement to give individuals increased control over their data and its use in order
to respect their privacy and autonomy. Similar contrasts emerge between the requirement
of avoiding harm at all costs and that of balancing risks and benefits. Furthermore, it should
be noted that risk-benefit evaluations will lead to different results depending on whose wellbeing it will be optimized for by which actors. If not resolved, such divergences and tensions
may undermine attempts to develop a global agenda for ethical AI.
Despite a general agreement that AI should be ethical, significant divergences emerge
within and between guidelines for ethical AI. Furthermore, uncertainty remains regarding
how ethical principles and guidelines should be implemented. These challenges have
implications for science policy, technology governance and research ethics. At the policy
level, they urge increased cooperative efforts among governmental organisations to
harmonize and prioritize their AI agendas, an effort that can be mediated and facilitated by
inter-governmental organisations. While harmonization is desirable, however, it should not
come at the costs of obliterating cultural and moral pluralism over AI. Therefore, a
fundamental challenge for developing a global agenda for AI is balancing the need for crossnational harmonization over the respect for cultural diversity and moral pluralism. This
challenge will require the development of deliberative mechanisms to adjudicate
disagreement concerning the values and implications of AI advances among different
stakeholders from different global regions. At the level of technology governance,
harmonization is typically implemented in terms of standardizations. Efforts in this direction
have been made, among others, by the Institute of Electrical and Electronics Engineers
(IEEE) through the “Ethically Aligned Designed” initiative117. Finally, soft governance
mechanisms such as Independent Review Boards (IRBs) will be increasingly required to
assess the ethical validity of AI applications in scientific research, especially those in the
academic domain. However, AI applications by governments or private corporations will
unlikely fall under their oversight, unless significant expansions to the IRBs’ purview are
The international community seems to converge on the importance of transparency, nonmaleficence, responsibility, and privacy for the development and deployment of ethical AI.
However, enriching the current ethical AI discourse through a better appraisal of critical yet
underrepresented ethical principles such as human dignity, solidarity and sustainability is
likely to result into a better articulated ethical landscape for artificial intelligence.
Furthermore, shifting the focus from principle-formulation to translation into practice must
be the next step. A global agenda for ethical AI should balance the need for cross-national
and cross-domain harmonization over the respect for cultural diversity and moral pluralism.
Overall, our review provides a useful starting point for understanding the inherent diversity
of current principles and guidelines for ethical AI and outlines the challenges ahead for the
global community.
Limitations
This study has several limitations. First, guidelines and soft-law documents are an instance
of gray literature, hence not indexed in conventional scholarly databases. Therefore, their
retrieval is inevitably less replicable and unbiased compared to systematic database search
of peer-reviewed literature. Following best practices for gray literature review, this
limitation has been mitigated by developing a discovery and eligibility protocol which was
pilot-tested prior to data collection. Although search results from search engines are
personalized, the risk of personalization influencing discovery has been mitigated through
the broadness of both the keyword search and the inclusion of results. A language bias may
have skewed our corpus towards English results. Our content analysis presents the typical
limitations of qualitative analytic methods. Following best practices for content analysis,
this limitation has been mitigated by developing an inductive coding strategy which was
conducted independently by two reviewers to minimize subjective bias. Finally, given the
rapid pace of publication of AI guidance documents, there is a possibility that new policy
documents were published after our search was completed. To minimize this risk,
continuous monitoring of the literature was conducted in parallel with the data analysis and
until April 23, 2019.
We conducted a scoping review of the gray literature reporting principles and guidelines for
ethical AI. A scoping review is a method aimed at synthesizing and mapping the existing
literature118, which is considered particularly suitable for complex or heterogeneous areas
of research118,119. Given the absence of a unified database for AI-specific ethics guidelines,
we developed a protocol for discovery and eligibility, adapted from the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) framework120. The protocol
was pilot-tested and calibrated prior to data collection. Following best practices for gray
literature retrieval, a multi-stage screening strategy involving both inductive screening via
search engine and deductive identification of relevant entities with associated websites and
online collections was conducted. To achieve comprehensiveness and systematicity,
relevant documents were retrieved by relying on three sequential search strategies (cf.
Figure 2): First, a manual search of four link hub webpages (“linkhubs”)121–124 was
performed. 68 sources were retrieved, out of which 30 were eligible (27 after removing
duplicates). Second, a keyword-based web search of the Google.com search engine was
performed in private browsing modus, after log-out from personal accounts and erasure of
all web cookies and history.125,126 Search was performed using the following keywords: [AI
principles], [artificial intelligence principles], [AI guidelines], [artificial intelligence
guidelines], [ethical AI] and [ethical artificial intelligence]. Every link in the first thirty
search results was followed and screened (i) for AI principles, resulting in 10 more sources
after removing duplicates, and (ii) for articles mentioning AI principles, leading to the
identification of 3 additional non-duplicate sources. The remaining Google results up to the
200th listings for each Google search were followed and screened for AI principles only.
Within these additional 1020 link listings we identified 15 non-duplicate documents. After
identifying relevant documents through the two processes above, we used citation-chaining
to manually screen the full-texts and, if applicable, reference lists of all eligible sources in
order to identify other relevant documents. 17 additional sources were identified. We
continued to monitor the literature in parallel with the data analysis and until April 23, 2019,
to retrieve eligible documents that were released after our search was completed. Twelve
new sources were included within this extended time frame. To ensure theoretical
saturation, we exhausted the citation chaining within all identified sources until no
additional relevant document could be identified.
Figure 2- PRISMA-based flowchart of retrieval process
Flowchart of our retrieval process based on the PRISMA template for systematic
reviews127. We relied on three search strategies (linkhubs, web search and citation
chaining) and added the most recent records manually, identifying a total of 84 eligible,
non-duplicate documents containing ethical principles for AI.
Based on our inclusion/exclusion criteria, policy documents (including principles,
guidelines and institutional reports) included in the final synthesis were (i) written in
English, German, French, Italian, Greek; (ii) issued by institutional entities from both the
public and the public sectors; (iii) referred explicitly in their title/description to AI or
ancillary notions, (iv) expressed a normative ethical stance defined as a moral preference
for a defined course of action (cf. SI Table S2). Following full-text screening, 84 sources or
parts thereof were included in the final synthesis (cf. SI Table S1).
Content analysis of the 84 sources was independently conducted by two researchers in two
cycles of manual coding and one cycle of code mapping within the qualitative data analysis
software Nvivo for Mac v.11.4. During the first cycle of coding, one researcher exhaustively
tagged all relevant text through inductive coding128 attributing a total of 3457 codes, out of
which 1180 were subsequently discovered to pertain to ethical principles. Subsequently,
two researchers conducted the code mapping process in order to reduce subjective bias. The
process of code mapping, a method for qualitative metasynthesis129, consisted of two
iterations of themeing128, whereby categories were first attributed to each code, then
categorized in turn (cf. SI Table S3). For the theming of ethical principles, we relied
deductively on normative ethical literature. Ethical categories were inspected and assessed
for consistency by two researchers with primary expertise in ethics. Thirteen ethical
categories emerging from code mapping, two of which were merged with others due to
independently assessed semantic and thematic proximity. Finally, we extracted significance
and frequency by applying focused coding, a second cycle coding methodology used for
interpretive analysis128, to the data categorized in ethical categories. Consistency check was
performed both by reference to the relevant ethical literature and a process of deliberative
mutual adjustment among the general principles and the particular judgments contained in
the policy documents, an analytic strategy known as ‘reflective equilibrium’130.