HAL Id: hal-00990008
 
Submitted on 7 Aug 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Consistency of random forests
Erwan Scornet, Gérard Biau, Jean-Philippe Vert
To cite this version:
Erwan Scornet, Gérard Biau, Jean-Philippe Vert. Consistency of random forests. Annals of Statistics,
2015, 43 (4), pp.1716-1741. ￿10.1214/15-AOS1321￿. ￿hal-00990008v4￿
The Annals of Statistics
2015, Vol. 43, No. 4, 1716–1741
DOI: 10.1214/15-AOS1321
⃝Institute of Mathematical Statistics, 2015
CONSISTENCY OF RANDOM FORESTS1
By Erwan Scornet∗, G´erard Biau∗and Jean-Philippe Vert†
Sorbonne Universit´es∗and MINES ParisTech, PSL-Research University†
Random forests are a learning algorithm proposed by Breiman
[Mach. Learn. 45 5–32] that combines several randomized decision trees and aggregates their predictions by averaging. Despite
its wide usage and outstanding practical performance, little is known
about the mathematical properties of the procedure. This disparity
between theory and practice originates in the diﬃculty to simultaneously analyze both the randomization process and the highly datadependent tree structure. In the present paper, we take a step forward
in forest exploration by proving a consistency result for Breiman’s
[Mach. Learn. 45 5–32] original algorithm in the context of
additive regression models. Our analysis also sheds an interesting
light on how random forests can nicely adapt to sparsity.
1. Introduction.
Random forests are an ensemble learning method for
classiﬁcation and regression that constructs a number of randomized decision trees during the training phase and predicts by averaging the results.
Since its publication in the seminal paper of Breiman , the procedure has become a major data analysis tool, that performs well in practice
in comparison with many standard methods. What has greatly contributed
to the popularity of forests is the fact that they can be applied to a wide
range of prediction problems and have few parameters to tune. Aside from
being simple to use, the method is generally recognized for its accuracy and
its ability to deal with small sample sizes, high-dimensional feature spaces
and complex data structures. The random forest methodology has been successfully involved in many practical problems, including air quality prediction , chemoinformatics [Svetnik et al.
 ], ecology [Prasad, Iverson and Liaw , Cutler et al. ], 3D
Received May 2014; revised February 2015.
1Supported by the European Research Council [SMAC-ERC-280032].
AMS 2000 subject classiﬁcations. Primary 62G05; secondary 62G20.
Key words and phrases. Random forests, randomization, consistency, additive model,
sparsity, dimension reduction.
This is an electronic reprint of the original article published by the
Institute of Mathematical Statistics in The Annals of Statistics,
2015, Vol. 43, No. 4, 1716–1741. This reprint diﬀers from the original in
pagination and typographic detail.
E. SCORNET, G. BIAU AND J.-P. VERT
object recognition [Shotton et al. ] and bioinformatics [D´ıaz-Uriarte
and Alvarez de Andr´es ], just to name a few. In addition, many variations on the original algorithm have been proposed to improve the calculation time while maintaining good prediction accuracy; see, for example,
Geurts, Ernst and Wehenkel , Amaratunga, Cabrera and Lee .
Breiman’s forests have also been extended to quantile estimation [Meinshausen ], survival analysis [Ishwaran et al. ] and ranking prediction [Cl´emen¸con, Depecker and Vayatis ].
On the theoretical side, the story is less conclusive, and regardless of their
extensive use in practical settings, little is known about the mathematical
properties of random forests. To date, most studies have concentrated on
isolated parts or simpliﬁed versions of the procedure. The most celebrated
theoretical result is that of Breiman , which oﬀers an upper bound on
the generalization error of forests in terms of correlation and strength of the
individual trees. This was followed by a technical note [Breiman ] that
focuses on a stylized version of the original algorithm. A critical step was
subsequently taken by Lin and Jeon , who established lower bounds
for nonadaptive forests (i.e., independent of the training set). They also highlighted an interesting connection between random forests and a particular
class of nearest neighbor predictors that was further worked out by Biau and
Devroye . In recent years, various theoretical studies [e.g., Biau, Devroye and Lugosi , Ishwaran and Kogalur , Biau , Genuer
 , Zhu, Zeng and Kosorok ] have been performed, analyzing consistency of simpliﬁed models, and moving ever closer to practice. Recent
attempts toward narrowing the gap between theory and practice are by Denil, Matheson and Freitas , who proves the ﬁrst consistency result for
online random forests, and by Wager and Mentch and Hooker 
who study the asymptotic sampling distribution of forests.
The diﬃculty in properly analyzing random forests can be explained by
the black-box nature of the procedure, which is actually a subtle combination of diﬀerent components. Among the forest essential ingredients, both
bagging [Breiman ] and the classiﬁcation and regression trees (CART)split criterion [Breiman et al. ] play a critical role. Bagging (a contraction of bootstrap-aggregating) is a general aggregation scheme which
proceeds by generating subsamples from the original data set, constructing a predictor from each resample and deciding by averaging. It is one
of the most eﬀective computationally intensive procedures to improve on
unstable estimates, especially for large, high-dimensional data sets where
ﬁnding a good model in one step is impossible because of the complexity
and scale of the problem [B¨uhlmann and Yu , Kleiner et al. ,
Wager, Hastie and Efron ]. The CART-split selection originated from
the most inﬂuential CART algorithm of Breiman et al. , and is used
CONSISTENCY OF RANDOM FORESTS
in the construction of the individual trees to choose the best cuts perpendicular to the axes. At each node of each tree, the best cut is selected by
optimizing the CART-split criterion, based on the notion of Gini impurity
(classiﬁcation) and prediction squared error (regression).
Yet, while bagging and the CART-splitting scheme play a key role in the
random forest mechanism, both are diﬃcult to analyze, thereby explaining
why theoretical studies have, thus far, considered simpliﬁed versions of the
original procedure. This is often done by simply ignoring the bagging step
and by replacing the CART-split selection with a more elementary cut protocol. Besides, in Breiman’s forests, each leaf (i.e., a terminal node) of the
individual trees contains a ﬁxed pre-speciﬁed number of observations (this
parameter, called nodesize in the R package randomForests, is usually
chosen between 1 and 5). There is also an extra parameter in the algorithm
which allows one to control the total number of leaves (this parameter is
called maxnode in the R package and has, by default, no eﬀect on the procedure). The combination of these various components makes the algorithm
diﬃcult to analyze with rigorous mathematics. As a matter of fact, most
authors focus on simpliﬁed, data-independent procedures, thus creating a
gap between theory and practice.
Motivated by the above discussion, we study in the present paper some
asymptotic properties of Breiman’s algorithm in the context of additive regression models. We prove the L2 consistency of random forests, which
gives a ﬁrst basic theoretical guarantee of eﬃciency for this algorithm. To our
knowledge, this is the ﬁrst consistency result for Breiman’s original
procedure. Our approach rests upon a detailed analysis of the behavior of
the cells generated by CART-split selection as the sample size grows. It turns
out that a good control of the regression function variation inside each cell,
together with a proper choice of the total number of leaves (Theorem 1) or
a proper choice of the subsampling rate (Theorem 2) are suﬃcient to ensure
the forest consistency in a L2 sense. Also, our analysis shows that random
forests can adapt to a sparse framework, when the ambient dimension p is
large (independent of n), but only a smaller number of coordinates carry
out information.
The paper is organized as follows. In Section 2, we introduce some notation
and describe the random forest method. The main asymptotic results are
presented in Section 3 and further discussed in Section 4. Section 5 is devoted
to the main proofs, and technical results are gathered in the supplemental
article [Scornet, Biau and Vert ].
2. Random forests.
The general framework is L2 regression estimation,
in which an input random vector X ∈ p is observed, and the goal is
to predict the square integrable random response Y ∈R by estimating the
regression function m(x) = E[Y |X = x]. To this end, we assume given a
E. SCORNET, G. BIAU AND J.-P. VERT
training sample Dn = (X1,Y1),...,(Xn,Yn) of p ×R-valued independent
random variables distributed as the independent prototype pair (X,Y ). The
objective is to use the data set Dn to construct an estimate mn : p →R of
the function m. In this respect, we say that a regression function estimate mn
is L2 consistent if E[mn(X) −m(X)]2 →0 as n →∞(where the expectation
is over X and Dn).
A random forest is a predictor consisting of a collection of M randomized regression trees. For the jth tree in the family, the predicted value at
the query point x is denoted by mn(x;Θj,Dn), where Θ1,...,ΘM are independent random variables, distributed as a generic random variable Θ and
independent of Dn. In practice, this variable is used to resample the training
set prior to the growing of individual trees and to select the successive candidate directions for splitting. The trees are combined to form the (ﬁnite)
forest estimate
mM,n(x;Θ1,...,ΘM,Dn) = 1
mn(x;Θj,Dn).
Since in practice we can choose M as large as possible, we study in this
paper the property of the inﬁnite forest estimate obtained as the limit of (1)
when the number of trees M grows to inﬁnity as follows:
mn(x;Dn) = EΘ[mn(x;Θ,Dn)],
where EΘ denotes expectation with respect to the random parameter Θ,
conditional on Dn. This operation is justiﬁed by the law of large numbers,
which asserts that, almost surely, conditional on Dn,
M→∞mn,M(x;Θ1,...,ΘM,Dn) = mn(x;Dn);
see, for example, Scornet , Breiman for details. In the sequel,
to lighten notation, we will simply write mn(x) instead of mn(x; Dn).
In Breiman’s original forests, each node of a single tree is associated
with a hyper-rectangular cell. At each step of the tree construction, the
collection of cells forms a partition of p. The root of the tree is p
itself, and each tree is grown as explained in Algorithm 1.
This algorithm has three parameters:
(1) mtry ∈{1,... ,p}, which is the number of pre-selected directions for splitting;
(2) an ∈{1,... ,n}, which is the number of sampled data points in each tree;
(3) tn ∈{1,...,an}, which is the number of leaves in each tree.
By default, in the original procedure, the parameter mtry is set to p/3, an
is set to n (resampling is done with replacement) and tn = an. However, in
CONSISTENCY OF RANDOM FORESTS
Algorithm 1: Breiman’s random forest predicted value at x
Input: Training set Dn, number of trees M > 0, mtry ∈{1,...,p},
an ∈{1,... ,n}, tn ∈{1,... ,an}, and x ∈ p.
Output: Prediction of the random forest at x.
1 for j = 1,...,M do
Select an points, without replacement, uniformly in Dn.
Set P0 = { p} the partition associated with the root of the tree.
For all 1 ≤ℓ≤an, set Pℓ= ∅.
Set nnodes = 1 and level = 0.
while nnodes < tn do
if Plevel = ∅then
level = level + 1
Let A be the ﬁrst element in Plevel.
if A contains exactly one point then
Plevel ←Plevel \ {A}
Plevel+1 ←Plevel+1 ∪{A}
Select uniformly, without replacement, a subset
Mtry ⊂{1,...,p} of cardinality mtry.
Select the best split in A by optimizing the CART-split
criterion along the coordinates in Mtry (see details
Cut the cell A according to the best split. Call AL and
AR the two resulting cell.
Plevel ←Plevel \ {A}
Plevel+1 ←Plevel+1 ∪{AL} ∪{AR}
nnodes = nnodes + 1
Compute the predicted value mn(x;Θj,Dn) at x equal to the
average of the Yi’s falling in the cell of x in partition
Plevel ∪Plevel+1.
26 Compute the random forest estimate mM,n(x;Θ1,...,ΘM,Dn) at the
query point x according to (1).
E. SCORNET, G. BIAU AND J.-P. VERT
our approach, resampling is done without replacement and the parameters
an, and tn can be diﬀerent from their default values.
In words, the algorithm works by growing M diﬀerent trees as follows.
For each tree, an data points are drawn at random without replacement
from the original data set; then, at each cell of every tree, a split is chosen
by maximizing the CART-criterion (see below); ﬁnally, the construction of
every tree is stopped when the total number of cells in the tree reaches the
value tn (therefore, each cell contains exactly one point in the case tn = an).
We note that the resampling step in Algorithm 1 (line 2) is done by
choosing an out of n points (with an ≤n) without replacement. This is
slightly diﬀerent from the original algorithm, where resampling is done by
bootstrapping, that is, by choosing n out of n data points with replacement.
Selecting the points “without replacement” instead of “with replacement”
is harmless—in fact, it is just a means to avoid mathematical diﬃculties
induced by the bootstrap; see, for example, Efron , Politis, Romano
and Wolf .
On the other hand, letting the parameters an and tn depend upon n oﬀers
several degrees of freedom which opens the route for establishing consistency
of the method. To be precise, we will study in Section 3 the random forest
algorithm in two diﬀerent regimes. The ﬁrst regime is when tn < an, which
means that trees are not fully developed. In this case, a proper tuning of
tn ensures the forest’s consistency (Theorem 1). The second regime occurs
when tn = an, that is, when trees are fully grown. In this case, consistency
results from an appropriate choice of the subsample rate an/n (Theorem 2).
So far, we have not made explicit the CART-split criterion used in Algorithm 1. To properly deﬁne it, we let A be a generic cell and Nn(A) be
the number of data points falling in A. A cut in A is a pair (j,z), where j
is a dimension in {1,... ,p} and z is the position of the cut along the jth
coordinate, within the limits of A. We let CA be the set of all such possible
cuts in A. Then, with the notation Xi = (X(1)
i ,...,X(p)
i ), for any (j,z) ∈CA,
the CART-split criterion [Breiman et al. ] takes the form
(Yi −¯YA)21Xi∈A
(Yi −¯YAL1X(j)
<z −¯YAR1X(j)
≥z)21Xi∈A,
where AL = {x ∈A:x(j) < z}, AR = {x ∈A:x(j) ≥z}, and ¯YA (resp., ¯YAL,
¯YAR) is the average of the Yi’s belonging to A (resp., AL, AR), with the
convention 0/0 = 0. At each cell A, the best cut (j⋆
n) is ﬁnally selected
CONSISTENCY OF RANDOM FORESTS
by maximizing Ln(j,z) over Mtry and CA, that is,
n) ∈argmax
To remove ties in the argmax, the best cut is always performed along the
best cut direction j⋆
n, at the middle of two consecutive data points.
3. Main results.
We consider an additive regression model satisfying the
following properties:
(H1) The response Y follows
mj(X(j)) + ε,
where X = (X(1),...,X(p)) is uniformly distributed over p, ε is an independent centered Gaussian noise with ﬁnite variance σ2 > 0 and each component mj is continuous.
Additive regression models, which extend linear models, were popularized
by Stone and Hastie and Tibshirani . These models, which
decompose the regression function as a sum of univariate functions, are
ﬂexible and easy to interpret. They are acknowledged for providing a good
trade-oﬀbetween model complexity and calculation time, and accordingly,
have been extensively studied for the last thirty years. Additive models also
play an important role in the context of high-dimensional data analysis and
sparse modeling, where they are successfully involved in procedures such as
the Lasso and various aggregation schemes; for an overview, see, for example,
Hastie, Tibshirani and Friedman . Although random forests fall into
the family of nonparametric procedures, it turns out that the analysis of
their properties is facilitated within the framework of additive models.
Our ﬁrst result assumes that the total number of leaves tn in each tree
tends to inﬁnity more slowly than the number of selected data points an.
Theorem 1.
Assume that (H1) is satisﬁed. Then, provided an →∞,
tn →∞and tn(log an)9/an →0, random forests are consistent, that is,
n→∞E[mn(X) −m(X)]2 = 0.
It is noteworthy that Theorem 1 still holds with an = n. In this case, the
subsampling step plays no role in the consistency of the method. Indeed,
controlling the depth of the trees via the parameter tn is suﬃcient to bound
the forest error. We note in passing that an easy adaptation of Theorem 1
shows that the CART algorithm is consistent under the same assumptions.
E. SCORNET, G. BIAU AND J.-P. VERT
The term (log an)9 originates from the Gaussian noise and allows us to
control the noise tail. In the easier situation where the Gaussian noise is
replaced by a bounded random variable, it is easy to see that the term
(log an)9 turns into log an, a term which accounts for the complexity of the
tree partition.
Let us now examine the forest behavior in the second regime, where tn =
an (i.e., trees are fully grown), and as before, subsampling is done at the
rate an/n. The analysis of this regime turns out to be more complicated, and
rests upon assumption (H2) below. We denote by Zi = 1X Θ
↔Xi the indicator
that Xi falls into the same cell as X in the random tree designed with Dn
and the random parameter Θ. Similarly, we let Z′
↔Xj, where Θ′ is an
independent copy of Θ. Accordingly, we deﬁne
ψi,j(Yi,Yj) = E[ZiZ′
j|X,Θ,Θ′,X1,...,Xn,Yi,Yj]
ψi,j = E[ZiZ′
j|X,Θ,Θ′,X1,...,Xn].
Finally, for any random variables W1, W2, Z, we denote by Corr(W1, W2|Z)
the conditional correlation coeﬃcient (whenever it exists).
(H2) Let Zi,j = (Zi,Z′
j). Then one of the following two conditions holds:
(H2.1) One has
n→∞(log an)2p−2(log n)2E
|ψi,j(Yi,Yj) −ψi,j|
(H2.2) There exist a constant C > 0 and a sequence (γn)n →0 such that,
almost surely,
|Corr(Yi −m(Xi),1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj)|
P1/2[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj]
|Corr((Yi −m(Xi))2,1Zi=ℓ1|Xi)|
P1/2[Zi = ℓ1|Xi]
Despite their technical aspect, statements (H2.1) and (H2.2) have simple interpretations. To understand the meaning of (H2.1), let us replace
the Gaussian noise by a bounded random variable. A close inspection of
Lemma 4 shows that (H2.1) may be simply replaced by
|ψi,j(Yi,Yj) −ψi,j|
CONSISTENCY OF RANDOM FORESTS
Therefore, (H2.1) means that the inﬂuence of two Y -values on the probability of connection of two couples of random points tends to zero as n →∞.
As for assumption (H2.2), it holds whenever the correlation between the
noise and the probability of connection of two couples of random points
vanishes quickly enough, as n →∞. Note that, in the simple case where the
partition is independent of the Yi’s, the correlations in (H2.2) are zero, so
that (H2) is trivially satisﬁed. This is also veriﬁed in the noiseless case, that
is, when Y = m(X). However, in the most general context, the partitions
strongly depend on the whole sample Dn, and unfortunately, we do not
know whether or not (H2) is satisﬁed.
Theorem 2.
Assume that (H1) and (H2) are satisﬁed, and let tn = an.
Then, provided an →∞, tn →∞and an log n/n →0, random forests are
consistent, that is,
n→∞E[mn(X) −m(X)]2 = 0.
To our knowledge, apart from the fact that bootstrapping is replaced by
subsampling, Theorems 1 and 2 are the ﬁrst consistency results for Breiman’s
 forests. Indeed, most models studied so far are designed independently of Dn and are, consequently, an unrealistic representation of the true
procedure. In fact, understanding Breiman’s random forest behavior deserves a more involved mathematical treatment. Section 4 below oﬀers a
thorough description of the various mathematical forces in action.
Our study also sheds some interesting light on the behavior of forests
when the ambient dimension p is large but the true underlying dimension
of the model is small. To see how, assume that the additive model (H1)
satisﬁes a sparsity constraint of the form
mj(X(j)) + ε,
where S < p represents the true, but unknown, dimension of the model.
Thus, among the p original features, it is assumed that only the ﬁrst (without
loss of generality) S variables are informative. Put diﬀerently, Y is assumed
to be independent of the last (p −S) variables. In this dimension reduction
context, the ambient dimension p can be very large, but we believe that the
representation is sparse, that is, that few components of m are nonzero. As
such, the value S characterizes the sparsity of the model: the smaller S, the
sparser m.
Proposition 1 below shows that random forests nicely adapt to the sparsity
setting by asymptotically performing, with high probability, splits along the
S informative variables.
E. SCORNET, G. BIAU AND J.-P. VERT
In this proposition, we set mtry = p and, for all k, we denote by j1,n(X),...,
jk,n(X) the ﬁrst k cut directions used to construct the cell containing X,
with the convention that jq,n(X) = ∞if the cell has been cut strictly less
than q times.
Proposition 1.
Assume that (H1) is satisﬁed. Let k ∈N⋆and ξ > 0.
Assume that there is no interval [a,b] and no j ∈{1,...,S} such that mj is
constant on [a,b]. Then, with probability 1 −ξ, for all n large enough, we
have, for all 1 ≤q ≤k,
jq,n(X) ∈{1,...,S}.
This proposition provides an interesting perspective on why random forests
are still able to do a good job in a sparse framework. Since the algorithm
selects splits mostly along informative variables, everything happens as if
data were projected onto the vector space generated by the S informative
variables. Therefore, forests are likely to only depend upon these S variables, which supports the fact that they have good performance in sparse
framework.
It remains that a substantial research eﬀort is still needed to understand
the properties of forests in a high-dimensional setting, when p = pn may
be substantially larger than the sample size. Unfortunately, our analysis
does not carry over to this context. In particular, if high-dimensionality is
modeled by letting pn →∞, then assumption (H2.1) may be too restrictive
since the term (log an)2p−2 will diverge at a fast rate.
4. Discussion.
One of the main diﬃculties in assessing the mathematical properties of Breiman’s forests is that the construction process
of the individual trees strongly depends on both the Xi’s and the Yi’s. For
partitions that are independent of the Yi’s, consistency can be shown by
relatively simple means via Stone’s theorem for local averaging estimates; see also Gy¨orﬁet al. , Chapter 6. However, our partitions and
trees depend upon the Y -values in the data. This makes things complicated,
but mathematically interesting too. Thus, logically, the proof of Theorem 2
starts with an adaptation of Stone’s theorem tailored for random
forests, whereas the proof of Theorem 1 is based on consistency results of
data-dependent partitions developed by Nobel .
Both theorems rely on Proposition 2 below, which stresses an important
feature of the random forest mechanism. It states that the variation of the
regression function m within a cell of a random tree is small provided n
is large enough. To this end, we deﬁne, for any cell A, the variation of m
within A as
∆(m,A) = sup
|m(x) −m(x′)|.
CONSISTENCY OF RANDOM FORESTS
Furthermore, we denote by An(X,Θ) the cell of a tree built with random
parameter Θ that contains the point X.
Proposition 2.
Assume that (H1) holds. Then, for all ρ,ξ > 0, there
exists N ∈N⋆such that, for all n > N,
P[∆(m,An(X,Θ)) ≤ξ] ≥1 −ρ.
It should be noted that in the standard, Y -independent analysis of partitioning regression function estimates, the variance is controlled by letting
the diameters of the tree cells tend to zero in probability. Instead of such a
geometrical assumption, Proposition 2 ensures that the variation of m inside a cell is small, thereby forcing the approximation error of the forest to
asymptotically approach zero.
While Proposition 2 oﬀers a good control of the approximation error of the
forest in both regimes, a separate analysis is required for the estimation error.
In regime 1 (Theorem 1), the parameter tn allows us to control the structure
of the tree. This is in line with standard tree consistency approaches; see,
for example, Devroye, Gy¨orﬁand Lugosi , Chapter 20. Things are
diﬀerent for the second regime (Theorem 2), in which individual trees are
fully grown. In this case, the estimation error is controlled by forcing the
subsampling rate an/n to be o(1/log n), which is a more unusual requirement
and deserves some remarks.
At ﬁrst, we note that the log n term in Theorem 2 is used to control
the Gaussian noise ε. Thus if the noise is assumed to be a bounded random variable, then the log n term disappears, and the condition reduces
to an/n →0. The requirement an log n/n →0 guarantees that every single
observation (Xi,Yi) is used in the tree construction with a probability that
becomes small with n. It also implies that the query point x is not connected
to the same data point in a high proportion of trees. If not, the predicted
value at x would be inﬂuenced too much by one single pair (Xi,Yi), making
the forest inconsistent. In fact, the proof of Theorem 2 reveals that the estimation error of a forest estimate is small as soon as the maximum probability
of connection between the query point and all observations is small. Thus
the assumption on the subsampling rate is just a convenient way to control
these probabilities, by ensuring that partitions are dissimilar enough (i.e.,
by ensuring that x is connected with many data points through the forest).
This idea of diversity among trees was introduced by Breiman , but
is generally diﬃcult to analyze. In our approach, the subsampling is the key
component for imposing tree diversity.
Theorem 2 comes at the price of assumption (H2), for which we do not
know if it is valid in all generality. On the other hand, Theorem 2, which
mimics almost perfectly the algorithm used in practice, is an important step
E. SCORNET, G. BIAU AND J.-P. VERT
toward understanding Breiman’s random forests. Contrary to most previous
works, Theorem 2 assumes that there is only one observation per leaf of each
individual tree. This implies that the single trees are eventually not consistent, since standard conditions for tree consistency require that the number
of observations in the terminal nodes tends to inﬁnity as n grows; see, for
example, Devroye, Gy¨orﬁand Lugosi , Gy¨orﬁet al. . Thus the
random forest algorithm aggregates rough individual tree predictors to build
a provably consistent general architecture.
It is also interesting to note that our results (in particular Lemma 3)
cannot be directly extended to establish the pointwise consistency of random
forests; that is, for almost all x ∈ d,
n→∞E[mn(x) −m(x)]2 = 0.
Fixing x ∈ d, the diﬃculty results from the fact that we do not have a
control on the diameter of the cell An(x,Θ), whereas, since the cells form
a partition of d, we have a global control on their diameters. Thus,
as highlighted by Wager , random forests can be inconsistent at some
ﬁxed point x ∈ d, particularly near the edges, while being L2 consistent.
Let us ﬁnally mention that all results can be extended to the case where ε
is a heteroscedastic and sub-Gaussian noise, with for all x ∈ d, V[ε|X =
x] ≤σ′2, for some constant σ′2. All proofs can be readily extended to match
this context, at the price of easy technical adaptations.
5. Proof of Theorems 1 and 2.
For the sake of clarity, proofs of the
intermediary results are gathered in the supplemental article [Scornet, Biau
and Vert ]. We start with some notation.
5.1. Notation.
In the sequel, to clarify the notation, we will sometimes
write d = (d(1),d(2)) to represent a cut (j,z).
Recall that, for any cell A, CA is the set of all possible cuts in A. Thus,
with this notation, C p is just the set of all possible cuts at the root of
the tree, that is, all possible choices d = (d(1),d(2)) with d(1) ∈{1,...,p} and
d(2) ∈ .
More generally, for any x ∈ p, we call Ak(x) the collection of all
possible k ≥1 consecutive cuts used to build the cell containing x. Such a cell
is obtained after a sequence of cuts dk = (d1,...,dk), where the dependency
of dk upon x is understood. Accordingly, for any dk ∈Ak(x), we let A(x,dk)
be the cell containing x built with the particular k-tuple of cuts dk. The
proximity between two elements dk and d′
k in Ak(x) will be measured via
CONSISTENCY OF RANDOM FORESTS
Accordingly, the distance d∞between dk ∈Ak(x) and any A ⊂Ak(x) is
d∞(dk,A) = inf
z∈A∥dk −z∥∞.
Remember that An(X,Θ) denotes the cell of a tree containing X and
designed with random parameter Θ. Similarly, Ak,n(X,Θ) is the same cell
but where only the ﬁrst k cuts are performed (k ∈N⋆is a parameter to be
chosen later). We also denote by ˆdk,n(X,Θ) = ( ˆd1,n(X,Θ),..., ˆdk,n(X,Θ))
the k cuts used to construct the cell Ak,n(X,Θ).
Recall that, for any cell A, the empirical criterion used to split A in the
random forest algorithm is deﬁned in (2). For any cut (j,z) ∈CA, we denote
the following theoretical version of Ln(·,·) by
L⋆(j,z) = V[Y |X ∈A] −P[X(j) < z|X ∈A]V[Y |X(j) < z,X ∈A]
−P[X(j) ≥z|X ∈A]V[Y |X(j) ≥z,X ∈A].
Observe that L⋆(·,·) does not depend upon the training set and that, by
the strong law of large numbers, Ln(j,z) →L⋆(j,z) almost surely as n →∞
for all cuts (j,z) ∈CA. Therefore, it is natural to deﬁne the best theoretical
split (j⋆,z⋆) of the cell A as
(j⋆,z⋆) ∈arg min
In view of this criterion, we deﬁne the theoretical random forest as before,
but with consecutive cuts performed by optimizing L⋆(·,·) instead of Ln(·,·).
We note that this new forest does depend on Θ through Mtry, but not
on the sample Dn. In particular, the stopping criterion for dividing cells
has to be changed in the theoretical random forest; instead of stopping
when a cell has a single training point, we impose that each tree of the
theoretical forest is stopped at a ﬁxed level k ∈N⋆. We also let A⋆
be a cell of the theoretical random tree at level k, containing X, designed
with randomness Θ, and resulting from the k theoretical cuts d⋆
1(X,Θ),...,d⋆
k(X,Θ)). Since there can exist multiple best cuts at, at least,
one node, we call A⋆
k(X,Θ) the set of all k-tuples d⋆
k(X,Θ) of best theoretical
cuts used to build A⋆
We are now equipped to prove Proposition 2. For reasons of clarity, the
proof has been divided in three steps. First, we study in Lemma 1 the
theoretical random forest. Then we prove in Lemma 3 (via Lemma 2) that
theoretical and empirical cuts are close to each other. Proposition 2 is ﬁnally
established as a consequence of Lemma 1 and Lemma 3. Proofs of these
lemmas are to be found in the supplemental article [Scornet, Biau and Vert
E. SCORNET, G. BIAU AND J.-P. VERT
5.2. Proof of Proposition 2.
We ﬁrst need a lemma which states that the
variation of m(X) within the cell A⋆
k(X,Θ) where X falls, as measured by
k(X,Θ)), tends to zero.
Assume that (H1) is satisﬁed. Then, for all x ∈ p,
k(x,Θ)) →0
almost surely, as k →∞.
The next step is to show that cuts in theoretical and original forests are
close to each other. To this end, for any x ∈ p and any k-tuple of cuts
dk ∈Ak(x), we deﬁne
Ln,k(x,dk) =
Nn(A(x,dk−1))
(Yi −¯YA(x,dk−1))21Xi∈A(x,dk−1)
Nn(A(x,dk−1))
(Yi −¯YAL(x,dk−1)1
−¯YAR(x,dk−1)1
)21Xi∈A(x,dk−1),
where AL(x,dk−1) = A(x,dk−1) ∩{z:z(d(1)
k ) < d(2)
k } and AR(x,dk−1) =
A(x,dk−1) ∩{z:z(d(1)
k }, and where we use the convention 0/0 = 0
when A(x,dk−1) is empty. Besides, we let A(x,d0) = p in the previous
equation. The quantity Ln,k(x,dk) is nothing but the criterion to maximize
in dk to ﬁnd the best kth cut in the cell A(x,dk−1). Lemma 2 below ensures
that Ln,k(x,·) is stochastically equicontinuous, for all x ∈ p. To this end,
for all ξ > 0, and for all x ∈ p, we denote by Aξ
k−1(x) ⊂Ak−1(x) the set
of all (k −1)-tuples dk−1 such that the cell A(x,dk−1) contains a hypercube
of edge length ξ. Moreover, we let ¯
k(x) = {dk :dk−1 ∈Aξ
k−1(x)} equipped
with the norm ∥dk∥∞.
Assume that (H1) is satisﬁed. Fix x ∈ p, k ∈N⋆, and let
ξ > 0. Then Ln,k(x,·) is stochastically equicontinuous on ¯
k(x); that is, for
all α,ρ > 0, there exists δ > 0 such that
|Ln,k(x,dk) −Ln,k(x,d′
Lemma 2 is then used in Lemma 3 to assess the distance between theoretical and empirical cuts.
CONSISTENCY OF RANDOM FORESTS
Assume that (H1) is satisﬁed. Fix ξ,ρ > 0 and k ∈N⋆. Then
there exists N ∈N⋆such that, for all n ≥N,
P[d∞(ˆdk,n(X,Θ),A⋆
k(X,Θ)) ≤ξ] ≥1 −ρ.
We are now ready to prove Proposition 2. Fix ρ,ξ > 0. Since almost sure
convergence implies convergence in probability, according to Lemma 1, there
exists k0 ∈N⋆such that
k0(X,Θ)) ≤ξ] ≥1 −ρ.
By Lemma 3, for all ξ1 > 0, there exists N ∈N⋆such that, for all n ≥N,
P[d∞(ˆdk0,n(X,Θ),A⋆
k0(X,Θ)) ≤ξ1] ≥1 −ρ.
Since m is uniformly continuous, we can choose ξ1 suﬃciently small such
that, for all x ∈ p, for all dk0,d′
k0 satisfying d∞(dk0,d′
k0) ≤ξ1, we have
|∆(m,A(x,dk0)) −∆(m,A(x,d′
Thus, combining inequalities (4) and (5), we obtain
P[|∆(m,Ak0,n(X,Θ)) −∆(m,A⋆
k0(X,Θ))| ≤ξ] ≥1 −ρ.
Using the fact that ∆(m,A) ≤∆(m,A′) whenever A ⊂A′, we deduce from
(3) and (6) that, for all n ≥N,
P[∆(m,An(X,Θ)) ≤2ξ] ≥1 −2ρ.
This completes the proof of Proposition 2.
5.3. Proof of Theorem 1.
We still need some additional notation. The
partition obtained with the random variable Θ and the data set Dn is denoted by Pn(Dn,Θ), which we abbreviate as Pn(Θ). We let
Πn(Θ) = {P((x1,y1),...,(xn,yn),Θ):(xi,yi) ∈ d × R}
be the family of all achievable partitions with random parameter Θ. Accordingly, we let
M(Πn(Θ)) = max{Card(P):P ∈Πn(Θ)}
be the maximal number of terminal nodes among all partitions in Πn(Θ).
Given a set zn
1 = {z1,...,zn} ⊂ d, Γ(zn
1,Πn(Θ)) denotes the number of
distinct partitions of zn
1 induced by elements of Πn(Θ), that is, the number
of diﬀerent partitions {zn
1 ∩A:A ∈P} of zn
1 , for P ∈Πn(Θ). Consequently,
the partitioning number Γn(Πn(Θ)) is deﬁned by
Γn(Πn(Θ)) = max{Γ(zn
1,Πn(Θ)):z1,...,zn ∈ d}.
E. SCORNET, G. BIAU AND J.-P. VERT
Let (βn)n be a positive sequence, and deﬁne the truncated operator Tβn by
Tβnu = u,
if |u| < βn,
Tβnu = sign(u)βn,
if |u| ≥βn.
Hence Tβnmn(X,Θ), YL = TLY and Yi,L = TLYi are deﬁned unambiguously.
We let Fn(Θ) be the set of all functions f : d →R piecewise constant on
each cell of the partition Pn(Θ). [Notice that Fn(Θ) depends on the whole
data set.] Finally, we denote by In,Θ the set of indices of the data points that
are selected during the subsampling step. Thus the tree estimate mn(x,Θ)
mn(·,Θ) ∈arg min
|f(Xi) −Yi|2.
The proof of Theorem 1 is based on ideas developed by Nobel , and
worked out in Theorem 10.2 in Gy¨orﬁet al. . This theorem, tailored
for our context, is recalled below for the sake of completeness.
Theorem 3 [Gy¨orﬁet al. ].
Let mn and Fn(Θ) be as above. Assume that:
(i) limn→∞βn = ∞;
(ii) limn→∞E[inff∈Fn(Θ),∥f∥∞≤βn EX[f(X) −m(X)]2] = 0;
(iii) for all L > 0,
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
n→∞E[Tβnmn(X,Θ) −m(X)]2 = 0.
Statement (ii) [resp., statement (iii)] allows us to control the approximation error (resp., the estimation error) of the truncated estimate. Since the
truncated estimate Tβnmn is piecewise constant on each cell of the partition
Pn(Θ), Tβnmn belongs to the set Fn(Θ). Thus the term in (ii) is the classical
approximation error.
We are now equipped to prove Theorem 1. Fix ξ > 0, and note that we just
have to check statements (i)–(iii) of Theorem 3 to prove that the truncated
estimate of the random forest is consistent. Throughout the proof, we let
βn = ∥m∥∞+ σ
2(log an)2. Clearly, statement (i) is true.
CONSISTENCY OF RANDOM FORESTS
Approximation error.
To prove (ii), let
where zA ∈A is an arbitrary point picked in cell A. Since, according to (H1),
∥m∥∞< ∞, for all n large enough such that βn > ∥m∥∞, we have
EX[f(X) −m(X)]2 ≤E
EX[f(X) −m(X)]2
≤E[fΘ,n(X) −m(X)]2
(since fΘ,n ∈Fn(Θ))
≤E[m(zAn(X,Θ)) −m(X)]2
≤E[∆(m,An(X,Θ))]2
≤ξ2 + 4∥m∥2
∞P[∆(m,An(X,Θ)) > ξ].
Thus, using Proposition 2, we see that for all n large enough,
EX[f(X) −m(X)]2 ≤2ξ2.
This establishes (ii).
Estimation error.
To prove statement (iii), ﬁx L > 0. Then, for all n
large enough such that L < βn,
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
log Γn(Πn(Θ)) + 2M(Πn(Θ)) log
[according to Theorem 9.1 in Gy¨orﬁet al. ]
n log Γn(Πn)
Since each tree has exactly tn terminal nodes, we have M(Πn(Θ)) = tn, and
simple calculations show that
Γn(Πn(Θ)) ≤(dan)tn.
E. SCORNET, G. BIAU AND J.-P. VERT
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
2048 −4σ4 tn(log(dan))9
−8σ4 tn(log an)8
666eσ2(log an)4
by our assumption. Finally, observe that
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
≤2(βn + L)2,
which yields, for all n large enough,
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
≤ξ + 2(βn + L)2P
[f(Xi) −Yi,L]2 −E[f(X) −YL]2
≤ξ + 16(βn + L)2 exp
Thus, according to Theorem 3,
E[Tβnmn(X,Θ) −m(X)]2 →0.
Untruncated estimate.
It remains to show the consistency of the nontruncated random forest estimate, and the proof will be complete. For this
purpose, note that, for all n large enough,
E[mn(X) −m(X)]2 = E[EΘ[mn(X,Θ)] −m(X)]2
≤E[mn(X,Θ) −m(X)]2
(by Jensen’s inequality)
CONSISTENCY OF RANDOM FORESTS
≤E[mn(X,Θ) −Tβnmn(X,Θ)]2
+ E[Tβnmn(X,Θ) −m(X)]2
≤E[[mn(X,Θ) −Tβnmn(X,Θ)]21mn(X,Θ)≥βn] + ξ
n(X,Θ)1mn(X,Θ)≥βn] + ξ
n(X,Θ)1mn(X,Θ)≥βn|Θ]] + ξ.
Since |mn(X,Θ)| ≤∥m∥∞+ max1≤i≤n |εi|, we have
n(X,Θ)1mn(X,Θ)≥βn|Θ]
1max1≤i≤an εi≥σ
2(log an)2
1≤i≤an εi ≥σ
2(log an)2i
1≤i≤an εi ≥σ
2(log an)2i1/2
It is easy to see that
1≤i≤an εi ≥σ
2(log an)2i
2√π(log an)2 .
Finally, since the εi’s are centered i.i.d. Gaussian random variables, we have,
for all n large enough,
E[mn(X) −m(X)]2
∞a1−log an
2√π(log an)2
2√π(log an)2
This completes the proof of Theorem 1.
5.4. Proof of Theorem 2.
Recall that each cell contains exactly one data
point. Thus, letting
Wni(X) = EΘ[1Xi∈An(X,Θ)],
the random forest estimate mn may be rewritten as
E. SCORNET, G. BIAU AND J.-P. VERT
We have in particular that Pn
i=1 Wni(X) = 1. Thus
E[mn(X) −m(X)]2 ≤2E
Wni(X)(Yi −m(Xi))
Wni(X)(m(Xi) −m(X))
= 2In + 2Jn.
Approximation error.
Fix α > 0. To upper bound Jn, note that by Jensen’s
inequality,
1Xi∈An(X,Θ)(m(Xi) −m(X))2
1Xi∈An(X,Θ)∆2(m,An(X,Θ))
≤E[∆2(m,An(X,Θ))].
So, by deﬁnition of ∆(m,An(X,Θ))2,
∞E[1∆2(m,An(X,Θ))≥α] + α
for all n large enough, according to Proposition 2.
Estimation error.
To bound In from above, we note that
Wni(X)Wnj(X)(Yi −m(Xi))(Yj −m(Xj))
ni(X)(Yi −m(Xi))2
↔Xj(Yi −m(Xi))(Yj −m(Xj))
The term I′
n, which involves the double products, is handled separately in
Lemma 4 below. According to this lemma, and by assumption (H2), for all
n large enough,
CONSISTENCY OF RANDOM FORESTS
Consequently, recalling that εi = Yi −m(Xi), we have, for all n large enough,
|In| ≤α + E
ni(X)(Yi −m(Xi))2
1≤ℓ≤nWnℓ(X)
1≤ℓ≤nWnℓ(X) max
Now, observe that in the subsampling step, there are exactly
to pick a ﬁxed observation Xi. Since x and Xi belong to the same cell only
if Xi is selected in the subsampling step, we see that
where PΘ denotes the probability with respect to Θ, conditional on X and
1≤i≤nWni(X) ≤max
1≤i≤nPΘ[X Θ
Thus, combining inequalities (7) and (8), for all n large enough,
|In| ≤α + an
The term inside the brackets is the maximum of n χ2-squared distributed
random variables. Thus, for some positive constant C,
see, for example, Boucheron, Lugosi and Massart , Chapter 1. We
conclude that for all n large enough,
In ≤α + C an log n
Since α was arbitrary, the proof is complete.
Assume that (H2) is satisﬁed. Then, for all ε > 0, and all n
large enough, |I′
First, assume that (H2.2) is veriﬁed. Thus we have for all ℓ1,ℓ2 ∈
Corr(Yi −m(Xi),1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj)
E. SCORNET, G. BIAU AND J.-P. VERT
E[(Yi −m(Xi))1Zi,j=(ℓ1,ℓ2)]
V1/2[Yi −m(Xi)|Xi,Xj,Yj]V1/2[1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj]
E[(Yi −m(Xi))1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj]
σ(P[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj] −P[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj]2)1/2
E[(Yi −m(Xi))1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj]
σP1/2[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj]
where the ﬁrst equality comes from the fact that, for all ℓ1,ℓ2 ∈{0,1},
Cov(Yi −m(Xi),1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj)
= E[(Yi −m(Xi))1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj],
since E[Yi −m(Xi)|Xi,Xj,Yj] = 0. Thus, noticing that, almost surely,
E[Yi −m(Xi)|Zi,j,Xi,Xj,Yj]
E[(Yi −m(Xi))1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj]
P[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj]
1Zi,j=(ℓ1,ℓ2)
|Corr(Yi −m(Xi),1Zi,j=(ℓ1,ℓ2)|Xi,Xj,Yj)|
P1/2[Zi,j = (ℓ1,ℓ2)|Xi,Xj,Yj]
we conclude that the ﬁrst statement in (H2.2) implies that, almost surely,
E[Yi −m(Xi)|Zi,j,Xi,Xj,Yj] ≤4σγn.
Similarly, one can prove that the second statement in assumption (H2.2)
implies that, almost surely,
E[|Yi −m(Xi)|2|Xi,1X Θ
↔Xi] ≤4Cσ2.
Returning to the term I′
n, and recalling that Wni(X) = EΘ[1X Θ
↔Xi], we obtain
↔Xj(Yi −m(Xi))(Yj −m(Xj))
↔Xj(Yi −m(Xi))
× (Yj −m(Xj))|Xi,Xj,Yi,1X Θ
CONSISTENCY OF RANDOM FORESTS
↔Xj(Yi −m(Xi))
× E[Yj −m(Xj)|Xi,Xj,Yi,1X Θ
Therefore, by assumption (H2.2),
↔Xj|Yi −m(Xi)|]
↔Xi|Yi −m(Xi)|]
↔XiE[|Yi −m(Xi)||Xi,1X Θ
↔XiE1/2[|Yi −m(Xi)|2|Xi,1X Θ
≤2σC1/2γn.
This proves the result, provided (H2.2) is true. Let us now assume that
(H2.1) is veriﬁed. The key argument is to note that a data point Xi can be
connected with a random point X if (Xi,Yi) is selected via the subsampling
procedure and if there are no other data points in the hyperrectangle deﬁned
by Xi and X. Data points Xi satisfying the latter geometrical property are
called layered nearest neighbors (LNN); see, for example, Barndorﬀ-Nielsen
and Sobel . The connection between LNN and random forests was ﬁrst
observed by Lin and Jeon , and later worked out by Biau and Devroye
 . It is known, in particular, that the number of LNN Lan(X) among
an data points uniformly distributed on d satisﬁes, for some constant
C1 > 0 and for all n large enough,
an(X)] ≤anP[X
LNNXj] + 16a2
≤C1(log an)2d−2;
see, for example, Barndorﬀ-Nielsen and Sobel , Bai et al. . Thus
LNNX(Yi −m(Xi))(Yj −m(Xj))
E. SCORNET, G. BIAU AND J.-P. VERT
Consequently,
(Yi −m(Xi))(Yj −m(Xj))1Xi
↔Xj|X,Θ,Θ′,X1,...,Xn,Yi,Yj]
LNNX is the event where Xi is selected by the subsampling and is
also a LNN of X. Next, with the notation of assumption (H2),
(Yi −m(Xi))(Yj −m(Xj))1Xi
LNNXψi,j(Yi,Yj)
(Yi −m(Xi))(Yj −m(Xj))1Xi
(Yi −m(Xi))(Yj −m(Xj))1Xi
LNNX(ψi,j(Yi,Yj) −ψi,j)
The ﬁrst term is easily seen to be zero since
(Yi −m(Xi))(Yj −m(Xj))1Xi
LNNXψ(X,Θ,Θ′,X1,...,Xn)
× E[(Yi −m(Xi))(Yj −m(Xj))|X,X1,...,Xn,Θ,Θ′]]
Therefore,
|Yi −m(Xi)||Yj −m(Xj)|1Xi
LNNX|ψi,j(Yi,Yj) −ψi,j|
1≤ℓ≤n|Yi −m(Xi)|2 max
|ψi,j(Yi,Yj) −ψi,j|
CONSISTENCY OF RANDOM FORESTS
Now, observe that
Consequently,
1≤ℓ≤n|Yi −m(Xi)|4i
|ψi,j(Yi,Yj) −ψi,j|
Simple calculations reveal that there exists C1 > 0 such that, for all n,
1≤ℓ≤n|Yi −m(Xi)|4i
≤C1(log n)2.
Thus, by inequalities (9) and (11), the ﬁrst term in (10) can be upper
bounded as follows:
1≤ℓ≤n|Yi −m(Xi)|4i
1≤ℓ≤n|Yi −m(Xi)|4|X,X1,...,Xn
≤C′(log n)(log an)d−1.
n| ≤C′(log an)d−1(log n)α/2E1/2h
|ψi,j(Yi,Yj) −ψi,j|
which tends to zero by assumption.
Acknowledgments.
We greatly thank two referees for valuable comments
and insightful suggestions.
SUPPLEMENTARY MATERIAL
Supplement to “Consistency of random forests”
(DOI: 10.1214/15-AOS1321SUPP; .pdf). Proofs of technical results.