A Unifying Review of Deep
and Shallow Anomaly
This article deals with application of deep learning techniques to anomaly detection.
Furthermore, connections between classic “shallow” and novel deep approaches are
established, and it is shown how this relation might cross-fertilize or extend
both directions.
By LUKAS RUFF
, JACOB R. KAUFFMANN
, ROBERT A. VANDERMEULEN
, GRÉGOIRE MONTAVON
WOJCIECH SAMEK
, Member IEEE, MARIUS KLOFT
, Senior Member IEEE,
THOMAS G. DIETTERICH
, Member IEEE, AND KLAUS-ROBERT MÜLLER
, Member IEEE
| Deep learning approaches to anomaly detection (AD) have recently improved the state of the art in
detection performance on complex data sets, such as large
Manuscript received September 12, 2020; revised December 13, 2020;
accepted January 7, 2021. Date of publication February 4, 2021; date of current
version April 30, 2021. The work of Lukas Ruff was supported by the German
Federal Ministry of Education and Research (BMBF) through the project ALICE III
under Grant 01IS18049B. The work of Robert A. Vandermeulen, Grégoire
Montavon, Wojciech Samek, and Klaus-Robert Müller was supported in part by
the Berlin Institute for the Foundations of Learning and Data (BIFOLD) sponsored
by the BMBF. The work of Marius Kloft was supported in part by the German
Research Foundation (DFG) under Award KL 2698/2-1 and in part by the BMBF
under Award 01IS18051A and Award 031B0770E. The work of Grégoire
Montavon, Wojciech Samek, and Klaus-Robert Müller was supported in part by
the BMBF for the Berlin Center for Machine Learning (01IS18037A-I) under Grant
01IS14013A-E and Grant 031L0207A-D. The work of Klaus-Robert Müller was
supported in part by BMBF under Grant 01GQ1115 and Grant 01GQ0850, in part
by DFG under Grant Math+, EXC 2046/1, Project ID 390685689, in part by the
Institute of Information & Communications Technology Planning &
Evaluation (IITP) grants funded by the Korea Government under Grant
2017-0-00451 (Development of BCI Based Brain and Cognitive Computing
Technology for Recognizing User’s Intentions using Deep Learning), and in part
by the Korea Government through the Artiﬁcial Intelligence Graduate School
Program, Korea University, under Grant 2019-0-00079. The work of Thomas G.
Dietterich was supported by the U.S. Defense Advanced Research Projects
Agency (DARPA) under Contract HR001119C0112, Contract FA8750-19-C-0092,
and Contract HR001120C0022. (Corresponding authors: Marius Kloft; Thomas G.
Dietterich; Klaus-Robert Müller.)
Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, and Grégoire
Montavon are with the ML Group, Technische Universität Berlin, 10587 Berlin,
Wojciech Samek is with the Department of Artiﬁcial Intelligence, Fraunhofer
Heinrich Hertz Institute, 10587 Berlin, Germany.
Marius Kloft is with the Department of Computer Science, Technische
Universität Kaiserslautern, 67653 Kaiserslautern, Germany (e-mail:
 ).
Thomas G. Dietterich is with the School of Electrical Engineering and
Computer Science, Oregon State University, Corvallis, OR 97331 USA (e-mail:
 ).
Klaus-Robert Müller is with the Brain Team, Google Research, 10117 Berlin,
Germany, with the ML Group, Technische Universität Berlin, 10587 Berlin,
Germany, with the Department of Artiﬁcial Intelligence, Korea University, Seoul
136-713, South Korea, and also with the Max Planck Institute for Informatics,
66123 Saarbrücken, Germany (e-mail: ).
Digital Object Identiﬁer 10.1109/JPROC.2021.3052449
collections of images or text. These results have sparked a
renewed interest in the AD problem and led to the introduction
of a great variety of new methods. With the emergence of
numerous such methods, including approaches based on generative models, one-class classiﬁcation, and reconstruction,
there is a growing need to bring methods of this ﬁeld into a
systematic and uniﬁed perspective. In this review, we aim to
identify the common underlying principles and the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic “shallow” and
novel deep approaches and show how this relation might
cross-fertilize or extend both directions. We further provide
an empirical assessment of major existing methods that are
enriched by the use of recent explainability techniques and
present speciﬁc worked-through examples together with practical advice. Finally, we outline critical open challenges and
identify speciﬁc paths for future research in AD.
KEYWORDS | Anomaly detection (AD); deep learning; explainable artiﬁcial intelligence; interpretability; kernel methods;
detection;
classiﬁcation; outlier detection; out-of-distribution (OOD) detection;
unsupervised learning.
NOMENCLATURE
Anomaly detection.
Autoencoder.
Average precision.
Adversarial AE.
Area under the precision–recall curve.
Area under the ROC curve.
Contrastive AE.
Denoising AE.
Deep generative model.
Deep support vector data description.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Deep semisupervised AD.
Energy-based model.
Evidence lower bound.
Generative adversarial network.
Gaussian mixture model.
Geometric transformation.
Isolation forest.
Kernel density estimation.
k-nearest neighbors.
Kernel principal component analysis.
Local outlier factor.
Learning from positive and unlabeled
Long short-term memory.
Markov chain Monte Carlo.
Minimum covariance determinant.
Minimum volume ellipsoid.
Out-of-distribution.
Outlier exposure.
One-class neural network.
One-class support vector machine.
Probabilistic principal component analysis.
Principal component analysis.
Probability density function.
Positive semideﬁnite.
Radial basis function.
Reproducing kernel Hilbert space.
Robust PCA.
Stochastic gradient descent.
Stochastic gradient Langevin dynamics.
Semisupervised AD.
Support vector data description.
Variational AE.
Vector quantization.
Explainable AI.
I. I N T R O D U C T I O N
An anomaly is an observation that deviates considerably
from some concept of normality. Also known as outlier
or novelty, such an observation may be termed unusual,
irregular, atypical, inconsistent, unexpected, rare, erroneous, faulty, fraudulent, malicious, unnatural, or simply
strange—depending on the situation. AD (or outlier detection or novelty detection) is the research area that studies
the detection of such anomalous observations through
methods, models, and algorithms based on data. Classic
approaches to AD include PCA – , the OC-SVM ,
SVDD , nearest neighbor algorithms – , and
KDE , .
What the above methods have in common is that they
are all unsupervised, which constitutes the predominant
approach to AD. This is because, in standard AD settings, labeled anomalous data are often nonexistent. When
available, it is usually insufﬁcient to fully characterize all
notions of anomalousness. This typically makes a supervised approach ineffective. Instead, a central idea in AD
is to learn a model of normality from normal data in an
unsupervised manner so that anomalies become detectable
through deviations from the model.
The study of AD has a long history and spans multiple
disciplines, including engineering, machine learning, data
mining, and statistics. While the ﬁrst formal deﬁnitions of
so-called “discordant observations” date back to the 19th
century , the problem of AD has likely been studied
informally even earlier since anomalies are phenomena
that naturally occur in diverse academic disciplines, such
as medicine and the natural sciences. Anomalous data may
be useless, for example, when caused by measurement
errors, or maybe extremely informative and hold the key to
new insights, such as very long-surviving cancer patients.
Kuhn claimed that persistent anomalies drive scientiﬁc revolutions (see [14, Section VI]).
AD today has numerous applications across a variety of
domains. Examples include intrusion detection in cybersecurity – , fraud detection in ﬁnance, insurance,
healthcare, and telecommunication – , industrial
fault and damage detection – , the monitoring of
infrastructure , and stock markets , ,
acoustic novelty detection – , medical diagnosis
 – and disease outbreak detection , , event
detection in the earth sciences – , and scientiﬁc
discovery in chemistry , , bioinformatics ,
genetics , , physics , , and astronomy
 – . The data available in these domains is continually growing in size. It is also expanding to include complex
data types, such as images, videos, audios, text, graphs,
multivariate time series, and biological sequences, among
others. For applications to be successful in such complex
and high-dimensional data, a meaningful representation of
the data is crucial .
Deep learning – follows the idea of learning
effective representations from the data itself by training
multilayered
has greatly improved the state of the art in many
applications
neural networks provide the most successful solutions
for many tasks in domains, such as computer vision
 – , speech recognition – , or natural
language processing – and have contributed
to the sciences – . Methods based on deep
neural networks are able to exploit the hierarchical or
latent structure that is often inherent to data through
their multilayered, distributed feature representations.
Advances in parallel computation, SGD optimization, and
automated differentiation make it possible to apply deep
learning at scale using large data sets.
developing deep learning approaches for AD. This is
motivated by a lack of effective methods for AD tasks
that involve complex data, for instance, cancer detection
from multigigapixel whole-slide images in histopathology
 . As in other adoptions of deep learning, the goal
of deep AD is to mitigate the burden of manual feature
engineering and to enable effective, scalable solutions.
However, unlike supervised deep learning, it is less clear
what useful representation learning objectives for deep
AD are, due to the mostly unsupervised nature of the
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
AD approaches arranged in the plane spanned by two
major components (model and feature map) of our unifying view.
Based on shared principles, we distinguish one-class classiﬁcation,
probabilistic models, and reconstruction models as the three main
groups of approaches that all formulate shallow and deep models
(see Nomenclature for a list of abbreviations). These three groups
are complemented by purely distance-based methods. Besides
model and feature map, we identify loss, regularization, and
inference mode as other important modeling components of the
AD problem.
The major approaches to deep AD include deep AE
variants , , , – , deep one-class
classiﬁcation – ,
methods based on DGMs,
such as GANs , ,
 – ,
and recent
self-supervised methods – . In comparison to
traditional AD methods, where a feature representation
is ﬁxed a priori (e.g., via a kernel feature map), these
approaches aim to learn a feature map of the data
φω : x →φω(x), a deep neural network parameterized
with weights ω, as part of their learning objective.
research, there exists a wealth of review and survey
literature – and books – on the
topic. Some very recent surveys focus speciﬁcally on
deep AD – . However, an integrated treatment
of deep learning methods in the overall context of AD
research—in particular, its kernel-based learning part ,
 , —is still missing.
In this review article, we aim to ﬁll this gap by presenting a unifying view that connects traditional shallow
and novel deep learning approaches. We will summarize
recent exciting developments, present different classes of
AD methods, provide theoretical insights, and highlight
the current best practices when applying AD. Fig. 1 gives
an overview of the categorization of AD methods within
our unifying view. Note, ﬁnally, that we do not attempt
an encyclopedic treatment of all available AD literature;
rather, we present a slightly biased point of view (drawing
from our own work on the subject), illustrating the main
topics, and provide ample reference to related work for
further reading.
II. A N I N T R O D U C T I O N T O A N O M A L Y
D E T E C T I O N
A. Why Should We Care About Anomaly
Detection?
Though we may not realize it, AD is part of our
daily life. Operating mostly unnoticed, AD algorithms are
continuously monitoring our credit card payments, our
login behaviors, and companies’ communication networks.
If these algorithms detect an abnormally expensive purchase made on our credit card, several unsuccessful login
attempts made from an alien device in a distant country,
or unusual FTP requests made to our computer, they will
issue an alarm. While warnings, such as “someone is trying
to login to your account,” can be annoying when you are
on a business trip abroad and just want to check your
e-mails from the hotel computer, the ability to detect such
anomalous patterns is vital for a large number of today’s
applications and services, and even small improvements in
AD can lead to immense monetary savings.1
In addition, the ability to detect anomalies is also
an important ingredient in ensuring fail-safe and robust
design of deep learning-based systems, for instance,
in medical applications or autonomous driving. Various international standardization initiatives have been
launched toward this goal (e.g., ITU/WHO FG-AI4H,
ISO/IEC CD TR 24029-1, or IEEE P7009).
Despite its importance, discovering a reliable distinction
between “normal” and “anomalous” events is a challenging task. First, the variability within normal data can be
very large, resulting in misclassifying normal samples as
being anomalous (type I error) or not identifying the
anomalous ones (type II error). Especially in biological or
biomedical data sets, the variability between the normal
data (e.g., person-to-person variability) is often as large
or even larger than the distance to anomalous samples
(e.g., patients). Preprocessing, normalization, and feature
selection are potential means to reduce this variability and
improve detectability , , . If such steps
are neglected, the features with wide value ranges, noise,
or irrelevant features can dominate distance computations
and “mask” anomalies (see VIII-A). Second, anomalous events are often very rare, which results in highly
imbalanced training data sets. Even worse, in most cases,
the data set is unlabeled so that it remains unclear which
data points are anomalies and why. Hence, AD reduces to
an unsupervised learning task with the goal to learn a valid
model of the majority of data points. Finally, anomalies
themselves can be very diverse so that it becomes difﬁcult
to learn a complete model for them. Likewise, the solution is again to learn a model for the normal samples
and treat deviations from it as anomalies. However, this
approach can be problematic if the distribution of the
normal data changes (nonstationarity), either intrinsically
or due to environmental changes (e.g., lighting conditions
and recording devices from different manufacturers).
As exempliﬁed and discussed above, we note that AD
has broad practical relevance and impact. Moreover, (accidentally) detecting the unknown unknowns is a
strong driving force in the sciences. If applied in the sciences, AD can help us to identify new, previously unknown
1In 2019, U.K.’s online banking fraud has been estimated to be
111.8 million GBP (source: 
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
patterns in data, which can lead to novel scientiﬁc insights
and hypotheses.
B. Formal Deﬁnition of Anomaly Detection
In the following, we formally introduce the AD problem.
We ﬁrst deﬁne in probabilistic terms what an anomaly is,
explain what types of anomalies there are, and delineate
the subtle differences between an anomaly, an outlier, and
a novelty. Finally, we present a fundamental principle in
AD—the so-called concentration assumption—and give a
theoretical problem formulation that corresponds to density level set estimation.
1) What Is an Anomaly?: We opened this review with
the following deﬁnition:
An anomaly is an observation that deviates considerably from some concept of normality.
To formalize this deﬁnition, we here specify two aspects
more precisely: a “concept of normality” and what
“deviates considerably” signiﬁes. Following many previous
authors , , – , we rely on probability
⊆RD be the data space given by some task
or application. We deﬁne a concept of normality as the
distribution P+ on X that is the ground-truth law of normal
behavior in a given task or application. An observation that
deviates considerably from such a law of normality—an
anomaly—is, then, a data point x ∈X (or set of points)
that lies in a low probability region under P+. Assuming
that P+ has a corresponding pdf p+(x), we can deﬁne a set
of anomalies as
A = {x ∈X | p+(x) ≤τ},
where τ is some threshold such that the probability of A
under P+ is “sufﬁciently small” that we will specify further
in the following.
2) Types of Anomalies: Various types of anomalies have
been identiﬁed in the literature , . These
include point anomalies, conditional or contextual anomalies , , – , and group or collective
anomalies , , – . We extend these
three established types by further adding low-level sensory
anomalies and high-level semantic anomalies , a distinction that is particularly relevant for choosing between
deep and shallow feature maps.
A point anomaly is an individual anomalous data point
x ∈A, for example, an illegal transaction in fraud detection or an image of a damaged product in manufacturing.
This is arguably the most commonly studied type in AD
A conditional or contextual anomaly is a data instance
that is anomalous in a speciﬁc context, such as time, space,
or the connections in a graph. A price of $1 per Apple Inc.
stock might have been normal before 1997 but, as of today
 , would be an anomaly. A mean daily temperature
below freezing point would be an anomaly in the Amazon
rainforest but not in the Antarctic desert. For this anomaly
type, the normal law P+ is more precisely a conditional
distribution P+ ≡P+
X|T with conditional pdf p+(x | t) that
depends on some contextual variable T. Time-series anomalies , , – are the most prominent
example of contextual anomalies. Other examples include
spatial , , spatiotemporal , or graph-based
 , , anomalies.
A group or collective anomaly is a set of related or
dependent points {xj ∈X | j ∈J} that are anomalous,
where J ⊆N is an index set that captures some relation
or dependence. A cluster of anomalies, such as similar or
related network attacks in cybersecurity, forms a collective
anomaly, for instance , , . Often, collective anomalies are also contextual, such as anomalous
time (sub)series or biological (sub)sequences, for example,
some series or sequence {xt, . . . , xt+s−1} of length s ∈N.
It is important to note that although each individual point
xj in such a series or sequence might be normal under
the time-integrated marginal p+(x)
 p+(x, t) dt or
under the sequence-integrated, time-conditional marginal
p+(x | t) given by
p+(xt, . . ., xt+s−1 | t) dxt··· dxj−1 dxj+1··· dxt+s−1
{xt, . . . , xt+s−1}
conditional
p+(xt, . . . , xt+s−1 | t),
distribution of the collective series or sequences.
In the wake of deep learning, a distinction between lowlevel sensory anomalies and high-level semantic anomalies
 has become important. Low and high here refer
to the level in the feature hierarchy of some hierarchical
distribution, for instance, the hierarchy from pixel-level
features, such as edges and textures to high-level objects
and scenes in images or the hierarchy from individual
characters and words to semantic concepts and topics
in text. It is commonly assumed that data with such a
hierarchical structure is generated from some semantic
latent variables Z and Y that describe higher level factors
of variation Z (e.g., the shape, size, or orientation of an
object) and concepts Y (e.g., the object class identity)
 , . We can express this via a law of normality with
conditional pdf p+(x | z, y), where we usually assume Z to
be continuous and Y to be discrete. Low-level anomalies
could be texture defects or artifacts in images, or character
typos in words. In comparison, semantic anomalies could
be images of objects from nonnormal classes , for
instance, or misposted reviews and news articles .
Note that semantic anomalies can be very close to normal
instances in the raw feature space X. For example, a dog
with a fur texture and color similar to that of some cat
can be more similar in raw pixel space than various cat
breeds among themselves (see Fig. 2). Similarly, low-level
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Illustration of the types of anomalies: a point anomaly is a
single anomalous point. A contextual point anomaly occurs if a point
deviates in its local context, here a spike in an otherwise normal
time series. A group anomaly can be a cluster of anomalies or some
series of related points that are anomalous under the joint series
distribution (contextual group anomaly). Note that both contextual
anomalies have values that fall into the global (time-integrated)
range of normal values. A low-level sensory anomaly deviates from
the low-level features, here a cut in the fabric texture of a carpet
 . A semantic anomaly deviates in high-level factors of variation
or semantic concepts, here a dog among the normal class of cats.
Note that the white cat is more similar to the dog than to the other
cats in low-level pixel space.
background statistics can also result in a high similarity
in raw pixel space even when objects in the foreground
are completely different . Detecting semantic anomalies is, thus, innately tied to ﬁnding a semantic feature
representation (e.g., extracting the semantic features of
cats, such as whiskers, slit pupils, and triangular snout),
which is an inherently difﬁcult task in an unsupervised
setting .
3) Anomaly, Outlier, or Novelty?: Some studies make a
concrete (albeit subtle) distinction between what is an
anomaly, an outlier, or a novelty. While all three refer
to instances from low probability regions under P+ (i.e.,
are elements of A), an anomaly is often characterized as
being an instance from a distinct distribution other than P+
(e.g., when anomalies are generated by a different process
than the normal points), an outlier as being a rare or
low-probability instance from P+, and a novelty as being
an instance from some new region or mode of an evolving,
nonstationary P+. Under the distribution P+ of cats, for
instance, a dog would be an anomaly, a rare breed of cats,
such as the LaPerm, would be an outlier, and a new breed
of cats would be a novelty. Such a distinction between
anomaly, outlier, and novelty may reﬂect slightly different
objectives in an application: while anomalies are often
the data points of interest (e.g., a long-term survivor of
a disease), outliers are frequently regarded as “noise” or
“measurement error” that should be removed in a data
preprocessing step (“outlier removal”), and novelties are
new observations that require models to be updated to the
“new normal.” The methods for detecting points from low
probability regions, whether termed “anomaly,” “outlier,”
or “novelty,” are essentially the same, however. For this
reason, we make no distinction between these terms and
call any instance x ∈A an “anomaly.”
4) Concentration Assumption: While, in most situations,
the data space X ⊆RD is unbounded, a fundamental
assumption in AD is that the region where the normal data
lives can be bounded. That is, there exists some threshold
τ ≥0 such that
X \ A = {x ∈X | p+(x) > τ}
is nonempty and small (typically, in the Lebesgue-measure
D-dimensional space). This is known as the so-called concentration or cluster assumption – . Note that
the concentration assumption does not imply that the full
support supp(p+) = {x ∈X | p+(x) > 0} of the normal law
P+ must be bounded; only that some high-density subset of
the support is bounded. A standard univariate Gaussian’s
support is the full real axis, for example, but approximately
95% of its probability mass is contained in the interval
[−1.96, 1.96]. In contrast, the set of anomalies A need not
be concentrated and can be unbounded.
5) Density Level Set Estimation: A law of normality P+
is only known in a few application settings, such as for
certain laws of physics. Sometimes, a concept of normality
might also be user-speciﬁed (as in juridical laws). In most
cases, however, the ground-truth law of normality P+ is
unknown because the underlying process is too complex.
For this reason, we must estimate P+ from data.
Let P be the ground-truth data-generating distribution
on data space X ⊆RD with corresponding density p(x),
that is, the distribution that generates the observed data.
For now, we assume that this data-generating distribution exactly matches the normal data distribution, that is,
P ≡P+ and p ≡p+. This assumption is often invalid in
practice, of course, as the data-generating process might
be subject to noise or contamination, as we will discuss in
Section II-C.
Given data points x1, . . . , xn
X generated by P
(usually assumed to be drawn from i.i.d. random variables
following P), the goal of AD is to learn a model that
allows us to predict whether a new test instance ˜x ∈X
is an anomaly or not, that is, whether ˜x ∈A. Thus,
the AD objective is to (explicitly or implicitly) estimate the
low-density regions (or equivalently high-density regions)
in data space X under the normal law P+. We can formally
express this objective as the problem of density level set
estimation – , which is equivalent to minimum
volume set estimation – for the special case
of density-based sets. The density level set of P for some
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Illustration of the α-density level sets Cα with threshold
τ α for a univariate (left) and bivariate (right) standard Gaussian
distribution.
threshold τ ≥0 is given by C = {x ∈X | p(x) > τ}. For
some ﬁxed level α ∈ , the α-density level set Cα of
distribution P is then deﬁned as the smallest density level
set C that has a probability of at least 1 −α under P, that
Cα = arginf
{λ(C) | P(C) ≥1 −α}
= {x ∈X | p(x) > τα}
where τα ≥0 denotes the corresponding threshold and
λ is typically the Lebesgue measure. The extreme cases
of α = 0 and α →1 result in the full support C0 =
{x ∈X | p(x) > 0} = supp(p) and the most likely modes
argmaxx p(x) of P, respectively. If the aforementioned
concentration assumption holds, there always exists some
level α such that a corresponding level set Cα exists and
can be bounded. Fig. 3 illustrates some density level sets
for the case that P is the familiar standard Gaussian distribution. Given a level set Cα, we can deﬁne a corresponding
threshold anomaly detector cα : X →{±1} as
if x ̸∈Cα.
6) Density Estimation for Level Set Estimation: An obvious approach to density level set estimation is through
density estimation. Given some estimated density model
ˆp(x) = ˆp(x; x1, . . . , xn) ≈p(x) and some target level
α ∈ , one can estimate a corresponding threshold ˆτα
via the empirical p-value function
1[0,ˆp(xi))(τ) ≥1 −α
where 1A(·) denotes the indicator function for some set A.
Using ˆτα and ˆp(x) in (3) yields the plug-in density level
set estimator ˆCα, which can be used in (4) to obtain
the plug-in threshold detector ˆcα(x). Note, however, that
density estimation is generally the most costly approach to
density level set estimation (in terms of samples required)
since estimating the full density is equivalent to ﬁrst estimating the entire family of level sets {Cα | α ∈ } from
which the desired level set for some ﬁxed α ∈ is then
selected , . If there are insufﬁcient samples, this
density estimate can be biased. This has also motivated the
development of one-class classiﬁcation methods that aim
to estimate a collection or single-level sets , ,
 , directly, which we will explain in Section IV
in more detail.
7) Threshold Versus Score: The previous approach to
level set estimation through density estimation is relatively costly, yet results in a more informative model
that can rank inliers and anomalies according to their
comparison,
detector as in (4) only
yields a binary
prediction.
Menon and Williamson proposed a compromise by
learning a density outside the level set boundary. Many AD
methods also target some strictly increasing transformation T : [0, ∞) →R of the density for estimating a model
(e.g., log-likelihood instead of likelihood). The resulting
target T(p(x)) is usually no longer a proper density but still
preserves the density ranking , . An anomaly
score s : X →R can then be deﬁned by using an additional order-reversing transformation, for example, s(x) =
−T(p(x)) (e.g., negative log-likelihood) so that high scores
reﬂect low-density values, and vice versa. Having such
a score that indicates the “degree of anomalousness” is
important in many AD applications. As for the density
in (5), of course, we can always derive a threshold from
the empirical distribution of anomaly scores if needed.
8) Selecting a Level α: As we will show, there are many
degrees of freedom when attacking the AD problem, which
inevitably requires making various modeling assumptions
and choices. Setting the level α is one of these choices
and depends on the speciﬁc application. When the value
of α increases, the anomaly detector focuses only on the
most likely regions of P. Such a detector can be desirable in applications where missed anomalies are costly
(e.g., in medical diagnosis or fraud detection). On the
other hand, a large α will result in high false alarm rates,
which can be undesirable in online settings where lots
of data is generated (e.g., in monitoring tasks). We provide a practical example for selecting α in Section VIII.
Choosing α also involves further assumptions about the
data-generating process P, which we have assumed here
to match the normal data distribution P+. In Section II-C,
we discuss the data settings that can occur in AD that may
alter this assumption.
C. Data Set Settings and Data Properties
The data set settings (e.g., unsupervised or semisupervised) and data properties (e.g., type or dimensionality)
that occur in real-world AD problems can be diverse.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
We here characterize these settings, which may range
from the standard unsupervised to semisupervised and
supervised settings, and list further data properties that are
relevant for modeling an AD problem. However, before we
elaborate on these, we ﬁrst observe that the assumptions
made about the distribution of anomalies (often implicitly)
are also crucial to the problem.
1) Distribution of Anomalies?: Let P- denote the groundtruth anomaly distribution and assume that it exists on
X ⊆RD. As mentioned above, the common concentration assumption implies that some high-density regions of
the normal data distribution are concentrated, whereas
anomalies are assumed to be not concentrated ,
 . This assumption may be modeled by an anomaly
distribution P- that is a uniform distribution over the
(bounded2) data space X . Some well-known unsupervised methods, such as KDE or the OC-SVM ,
implicitly make this assumption that P- follows a uniform
distribution that can be interpreted as a default uninformative prior on the anomalous distribution . This
prior assumes that there are no anomalous modes and that
anomalies are equally likely to occur over the valid data
space X. Semisupervised or supervised AD approaches
often depart from this uninformed prior and try to make
a more informed a priori assumption about the anomalous
distribution P- . If faithful to P-, such a model based
on a more informed anomaly prior can achieve better
detection performance. Modeling anomalous modes can
also be beneﬁcial in certain applications, for example, for
typical failure modes in industrial machines or known
disorders in medical diagnosis. We remark that these prior
assumptions about the anomaly distribution P- are often
expressed only implicitly in the literature though such
assumptions are critical to an AD model.
2) Unsupervised Setting: The unsupervised AD setting is
the case in which only unlabeled data
x1, . . . , xn ∈X
are available for training a model. This setting is arguably
the most common setting in AD , , ,
 . We will usually assume that the data points have
been drawn in an i.i.d. fashion from the data-generating
distribution P. For simplicity, we have so far assumed that
the data-generating distribution is the same as the normal
data distribution P ≡P+. This is often expressed by the
statement that the training data is “clean.” In practice,
however, the data-generating distribution P may contain
noise or contamination.
Noise, in the classical sense, is some inherent source
of randomness ε that is added to the signal in the
data-generating process, that is, samples from P have the
2Strictly speaking, we are assuming that there always exists some
data-enclosing hypercube of numerically meaningful values such that the
data space X is bounded and the uniform distribution is well-deﬁned.
form x + ε, where x ∼P+. Noise might be present due to
irreducible measurement uncertainties in an application,
for example. The greater the noise, the harder it is to
accurately estimate the ground-truth level sets of P+ since
informative normal features get obfuscated . This is
because added noise expands the regions covered by the
observed data in input space X. A standard assumption
about noise is that it is unbiased (E[ε] = 0) and spherically
symmetric.
In addition to noise, the contamination (or pollution) of
the unlabeled data with undetected anomalies is another
important source of the disturbance. For instance, some
unnoticed anomalous degradation in an industrial machine
might have already occurred during the data collection
process. In this case, the data-generating distribution P is
a mixture of the normal data and the anomaly distribution, that is, P ≡(1 −η) P+ + η P- with contamination
(or pollution) rate η ∈(0, 1). The greater the contamination, the more the normal data decision boundary will be
distorted by including the anomalous points.
In summary, a more general and realistic assumption is
that samples from the data-generating distribution P have
the form of x + ε, where x ∼(1 −η) P+ + η P- and ε is the
random noise. Assumptions on both the noise distribution
ε and contamination rate η are crucial for modeling a
speciﬁc AD problem. Robust methods , , 
speciﬁcally aim to account for these sources of disturbance.
Note also that, by increasing the level α in the density
level set deﬁnition above, a corresponding model generally
becomes more robust (often at the cost of a higher false
alarm rate) since the target decision boundary becomes
tighter and excludes the contamination.
3) Semisupervised Setting: The SSAD setting is the case
in which both unlabeled and labeled data
x1, . . . , xn ∈X and (˜x1, ˜y1), . . . , (˜xm, ˜ym) ∈X × Y
are available for training a model with Y = {±1}, where
we denote ˜y = +1 for normal and ˜y = −1 for anomalous points, respectively. Usually, we have m ≪n in
the semisupervised setting, that is, most of the data are
unlabeled and only a few labeled instances are available,
since labels are often costly to obtain in terms of resources
(time, money, and so on). Labeling might, for instance,
require domain experts, such as medical professionals
(e.g., pathologists) or technical experts (e.g., aerospace
engineers). Anomalous instances, in particular, are also
infrequent by nature (e.g., rare medical conditions) or very
costly (e.g., the failure of some industrial machine). The
deliberate generation of anomalies is mostly not an option.
However, including known anomalous examples, if available, can signiﬁcantly improve the detection performance
of a model , , – . Labels are also,
sometimes, available in the online setting where alarms
raised by the anomaly detector have been investigated to
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
determine whether they were correct. Some unsupervised
AD methods can be incrementally updated when such
labels become available . A recent approach called
Outlier Exposure (OE) follows the idea of using
large quantities of unlabeled data that are available in
some domains as auxiliary anomalies (e.g., online stock
photos for computer vision or the English Wikipedia for
NLP), thereby effectively labeling this data with ˜y = −1.
In this setting, we frequently have that m ≫n, but this
labeled data have increased uncertainty in the labels as
the auxiliary data may not only contain anomalies and
may not be representative of test time anomalies. We will
discuss this speciﬁc setting in Sections IV-E and IX-E in
more detail. Verifying unlabeled samples as indeed being
normal can often be easier due to the more frequent nature
of normal data. This is one of the reasons why the special
semisupervised case of LPUE – , that is, labeled
normal and unlabeled examples, is also studied speciﬁcally
in the AD literature , , – .
Previous work has also referred to the special
exclusively from
as the “SSAD” setting, which is confusing terminology. Although meticulously curated normal data can,
sometimes, be available (e.g., in open-category detection ), such a setting in which entirely (and conﬁdently) labeled normal examples are available is rather
rare in practice. The analysis of this setting is rather
again justiﬁed by the assumption that most of the given
(unlabeled) training data are normal but not the absolute
certainty thereof. This makes this setting effectively equivalent to the unsupervised setting from a modeling perspective, apart from maybe weakened assumptions on the
level of noise or contamination, which previous works also
point out . We, therefore, refer to the more general
setting as presented in (7) as the SSAD setting, which
incorporates both labeled normal and anomalous examples
in addition to unlabeled instances, since this setting is
reasonably common in practice. If some labeled anomalies are available, the modeling assumptions about the
anomalous distribution P-, as mentioned in Section II-C1,
become critical for effectively incorporating anomalies into
training. These include, for instance, whether modes or
clusters are expected among the anomalies (e.g., group
anomalies).
4) Supervised Setting: The supervised AD setting is the
case in which completely labeled data
(˜x1, ˜y1), . . . , (˜xm, ˜ym) ∈X × Y
are available for training a model, where, again, Y =
{±1} with ˜y = +1 denoting normal instances and ˜y =
−1 denoting anomalies, respectively. If both the normal
and anomalous data points are assumed to be representative for the normal data distribution P+ and anomaly
distribution P-, respectively, this learning problem is equivalent to supervised binary classiﬁcation. Such a setting
would, thus, not be an AD problem in the strict sense
but rather a classiﬁcation task. Although anomalous modes
or clusters might exist, that is, some anomalies might be
more likely to occur than others, anything not normal is,
by deﬁnition, an anomaly. Labeled anomalies are therefore
rarely fully representative of some “anomaly class.” This
distinction is also reﬂected in modeling: in classiﬁcation,
the objective is to learn a (well-generalizing) decision
boundary that best separates the data according to some
(closed set of) class labels, but the objective in AD remains
the estimation of the normal density level set boundaries. Hence, we should interpret supervised AD problems
as label-informed density level set estimation in which
conﬁdent normal (in-distribution) and anomalous out-ofdistribution (OOD) training examples are available. Due
to the above and also the high costs often involved with
labeling, the supervised AD setting is the most uncommon
setting in practice.
Finally, we note that labels may also carry more granular
information beyond simply indicating whether some point
˜x is normal (˜y = +1) or anomalous (˜y = −1). In OOD
detection or open-category detection problems, for example, the goal is to train a classiﬁer while also
detecting examples that are not from any of the known
training set classes. In these problems, the labeled data
(˜x1, ˜y1), . . . , (˜xm, ˜ym) with ˜y ∈{1, . . . , k} also hold information about the k (sub)classes of the in-distribution P+.
Such information about the structure of the normal data
distribution has been shown to be beneﬁcial for semantic
detection tasks , . We will discuss such speciﬁc
and related detection problems later in Section IX-B.
5) Further
Properties:
described above, the intrinsic properties of the data itself
are also crucial for modeling a speciﬁc AD problem.
We give a list of relevant data properties in Table 1 and
present a toy data set with a speciﬁc realization of these
properties in Fig. 4, which will serve us as a running
example. The assumptions about these properties should
be reﬂected in the modeling choices, such as adding
context or deciding among suitable deep or shallow
feature maps, which can be challenging. We outline these
and further challenges in AD in the following.
D. Challenges in Anomaly Detection
We conclude our introduction by brieﬂy highlighting
some notable challenges in AD, some of which directly
arise from the deﬁnition and data characteristics detailed
above. Certainly, the fundamental challenge in AD is
the mostly unsupervised nature of the problem, which
necessarily requires assumptions to be made about the
speciﬁc application, the domain, and the given data. These
include assumptions about the relevant types of anomalies
(see Section II-B2), possible prior assumptions about the
anomaly distribution (see Section II-C1) and, if available,
the challenge of how to incorporate labeled data instances
in a generalizing way (see Sections II-C3 and II-C4).
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Table 1 Data Properties Relevant in AD
Further questions include how to derive an anomaly score
or threshold in a speciﬁc task (see Section II-B7)? What
level α (see Section II-B8) strikes a balance between false
alarms and missed anomalies that is reasonable for the
task? Is the data-generating process subject to noise or
contamination (see Section II-C2), that is, is robustness
a critical aspect? Moreover, identifying and including the
data properties given in Table 1 into a method and model
can pose challenges as well. The computational complexity
in both the data set size n + m and dimensionality D,
as well as the memory cost of a model at training time,
but also at test time, can be a limiting factor (e.g., for
data streams or in real-time monitoring ). Is the
data-generating process assumed to be nonstationary
 – and are there distributional shifts expected at
test time? For (truly) high-dimensional data, the curse of
dimensionality and the resulting concentration of distances
can be a major issue . Here, ﬁnding a representation
that captures the features that are relevant for the task and
meaningful for the data and domain becomes vital. Deep
AD methods further entail new challenges, such as an
increased number of hyperparameters and the selection of
suitable network architecture and optimization parameters
(learning rate, batch sizes, and so on). In addition,
the more complex the data or a model is, the greater the
challenges of model interpretability (e.g., – )
and decision transparency become. We illustrate some
of these practical challenges and provide guidelines with
worked-through examples in Section VIII.
Considering the various facets of the AD problem that
we have covered in this introduction, it is not surprising
that there is a wealth of literature and approaches on the
topic. We outline these approaches in the following, where
Two-dimensional Big Moon, Small Moon toy example with
real-valued ground-truth normal law P+ that is composed of two 1-D
manifolds (bimodal, two-scale, and nonconvex). The unlabeled
training data (n
  1000 and m
  0) are generated from P
which is subject to Gaussian noise ε. These toy data are
nonhierarchical, context-free, and stationary. Anomalies are
off-manifold points that may occur uniformly over the displayed
we ﬁrst examine density estimation and probabilistic models (see Section III), followed by one-class classiﬁcation
methods (see Section IV), and, ﬁnally, reconstruction models (see Section V). In these sections, we will point out
the connections between deep and shallow methods. Fig. 5
gives an overview and intuition of the approaches. Afterward, in Section VI, we present our unifying view, which
will enable us to systematically identify open challenges
and paths for future research.
III. D E N S I T Y E S T I M AT I O N A N D
P R O B A B I L I S T I C M O D E L S
The ﬁrst category of methods that we introduce predicts
anomalies through estimation of the normal data probability distribution. The wealth of existing probability models
is, therefore, a clear candidate for the task of AD. This
includes classic density estimation methods and
deep statistical models. In the following, we describe the
adaptation of these techniques to AD.
A. Classic Density Estimation
One of the most basic approaches to multivariate AD is
to compute the Mahalanobis distance from a test point to
the training data mean . This is equivalent to ﬁtting a
multivariate Gaussian distribution to the training data and
evaluating the log-likelihood of a test point according to
that model . Compared to modeling each dimension
of the data independently, ﬁtting a multivariate Gaussian
captures linear interactions between pairs of dimensions.
To model more complex distributions, nonparametric density estimators have been introduced, such as KDE ,
 , histogram estimators, and GMMs , .
The KDE is arguably the most widely used nonparametric density estimator due to theoretical advantages over
histograms and the practical issues with ﬁtting and
parameter selection for GMMs . The standard KDE,
along with a more recent adaptation that can deal with
modest levels of outliers in the training data , ,
is, therefore, a popular approach to AD. A GMM with
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Overview of the different approaches to AD. Top: typical decision functions learned by the different AD approaches, where white
corresponds to normal and red to anomalous decision regions. One-class classiﬁcation models typically learn a discriminative decision
boundary, probabilistic models a density, and reconstruction models some underlying geometric structure of the data (e.g., manifold or
prototypes). Right: deep feature maps enable to learn more ﬂexible, nonlinear decision functions suitable for more complex data. Bottom:
diagrams of architectures for a selection of different methods with deep and shallow feature maps. Points (i)–(v): locations in input space,
where we highlight some model-speciﬁc phenomena. (i) Too loose, the biased one-class boundary may leave anomalies undetected.
(ii) Probabilistic models may underﬁt (or overﬁt) the tails of a distribution. (iii) Manifold or prototype structure artifacts may result in a good
reconstruction of anomalies. (iv) Simple shallow models may fail to ﬁt complex, nonlinear distributions. (v) Compression artifacts of deep
feature maps may create “blind spots” in input space.
a ﬁnite number of K mixtures can also be viewed as
a soft (probabilistic) clustering method that assumes K
prototypical modes (see Section V-A2). This has been used,
for example, to represent typical states of a machine in
predictive maintenance .
While classic nonparametric density estimators perform
fairly well for low-dimensional problems, they suffer notoriously from the curse of dimensionality: the sample size
required to attain a ﬁxed level of accuracy grows exponentially in the dimension of the feature space. One goal of
deep statistical models is to overcome this challenge.
B. Energy-Based Models
Some of the earliest deep statistical models are EBMs
 – . An EBM is a model whose density is characterized by an energy function Eθ(x) with
Z(θ) exp(−Eθ(x))
where Z(θ) =
 exp(−Eθ(x)) dx is the so-called partition
function that ensures that pθ integrates to 1. These models
are typically trained via gradient descent, approximating
the log-likelihood gradient ∇θ log pθ(x) via MCMC 
or SGLD , . While one typically cannot evaluate
the density pθ directly due to the intractability of the
partition function Z(θ), the function Eθ can be used as an
anomaly score since it is monotonically decreasing as the
density pθ increases.
Early deep EBMs, such as deep belief networks 
and deep Boltzmann machines , are graphical models consisting of layers of latent states followed by an
observed output layer that models the training data. Here,
the energy function depends not only on the input x, but
also on a latent state z, so the energy function has the
form Eθ(x, z). While including latent states allows these
approaches to richly model latent probabilistic dependencies in data distributions, these approaches are not
particularly amenable to AD since one must marginalize
out the latent variables to recover some value related
to the likelihood. Later studies replaced the probabilistic
latent layers with deterministic ones allowing for
the practical evaluation of Eθ(x) for use as an anomaly
score. This sort of model has been successfully used for
deep AD . Recently, EBMs have also been suggested
as a framework to reinterpret deep classiﬁers where the
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
energy-based training has shown to improve robustness
and OOD detection performance .
C. Neural Generative Models (VAEs and GANs)
Neural generative models aim to learn a neural network
that maps vectors sampled from a simple predeﬁned source
distribution Q, usually a Gaussian or uniform distribution, to the actual input distribution P+. More formally,
the objective is to train the network so that φω(Q) ≈P+,
where φω(Q) is the distribution that results from pushing
the source distribution Q through neural network φω. The
two most established neural generative models are VAEs
 – and GANs .
1) VAEs: VAEs learn deep latent-variable models where
the inputs x are parameterized on latent samples z ∼Q
via some neural network, so as to learn a distribution
pθ(x | z) such that pθ(x) ≈p+(x). A common instantiation of this is to let Q be an isotropic multivariate
Gaussian distribution and let the neural network φd,ω =
(μω, σω) (the decoder) with weights ω parameterize the
mean and variance of an isotropic Gaussian distribution,
so pθ(x | z)
N(x; μω(z), σ2
ω(z)I). Performing maximum likelihood estimation on θ is typically intractable.
To remedy this, an additional network φe,ω′ (the encoder)
is introduced to parameterize a variational distribution
qθ′(z | x), with θ′ encapsulated by the output of φe,ω′,
to approximate the latent posterior p(z | x). The full model
is then optimized via the ELBO in a variational Bayes
θ,θ′ −DKL(qθ′(z|x)∥p(z)) + Eqθ′ (z|x)[log pθ(x|z)]. (10)
Optimization
stochastic
variational
trained VAE,
can estimate pθ(x) via Monte Carlo sampling from the
prior p(z) and computing Ez∼p(z)[pθ(x | z)]. Using this
score directly for AD has a nice theoretical interpretation,
but experiments have shown that it tends to perform worse
 , than alternatively using the reconstruction
probability , which conditions on x to estimate
Eqθ′ (z|x)[log pθ(x|z)]. The latter can also be seen as a
probabilistic reconstruction model
a stochastic
encoding and decoding process (see Section V-C).
2) GANs: GANs pose the problem of learning the target
distribution as a zero-sum-game: a generative model is
trained in competition with an adversary that challenges
it to generate samples whose distribution is similar to
the training distribution. A GAN consists of two neural
networks, a generator network φω
discriminator network ψω′ : X →(0, 1) that are pitted
against each other so that the discriminator is trained to
discriminate between φω(z) and x ∼P+, where z ∼Q.
The generator is trained to fool the discriminator network,
thereby encouraging the generator to produce samples
more similar to the target distribution. This is done using
the following adversarial objective:
Ex∼P+[log ψω′(x)]
+ Ez∼Q[log(1 −ψω′(φω(z)))].
Training is typically carried out via an alternating optimization scheme, which is notoriously ﬁnicky . There exist
many GAN variants, for example, the Wasserstein GAN
 , , which is frequently used for AD methods
using GANs, and StyleGAN, which has produced impressive high-resolution photorealistic images .
Due to their construction, GAN models offer no way to
assign a likelihood to points in the input space. Using the
discriminator directly has been suggested as one approach
to use GANs for AD , which is conceptually close to
one-class classiﬁcation (see Section IV). Other approaches
apply optimization to ﬁnd a point ˜z in latent space Z
such that ˜x ≈φω(˜z) for the test point ˜x. The authors of
AnoGAN recommend using an intermediate layer of
the discriminator, fω′, and setting the anomaly score to
be a convex combination of the reconstruction loss ∥˜x −
φω(˜z)∥and the discrimination loss ∥fω′(˜x) −fω′(φω(˜z))∥.
In AD-GAN , the authors recommend initializing the
search for latent points multiple times to ﬁnd a collection of m latent points ˜z1, . . . , ˜zm while simultaneously
adapting the network parameters ωi individually for each
˜zi to improve the reconstruction and using the mean
reconstruction loss as an anomaly score
∥˜x −φωi(˜zi)∥.
Viewing the generator as a stochastic decoder and the
search for an optimal latent point ˜z as an (implicit)
encoding of a test point ˜x, utilizing a GAN this way with
the reconstruction error for AD is similar to reconstruction
methods, particularly AEs (see Section V-C). Later GAN
adaptations have added explicit encoding networks that
are trained to ﬁnd the latent point ˜z. This has been
used in a variety of ways, usually again incorporating the
reconstruction error , , .
D. Normalizing Flows
generative
normalizing
 – attempt to map data points from a source
distribution z ∼Q (usually called base distribution for normalizing ﬂows) so that x ≈φω(z) is distributed according
to p+. The crucial distinguishing characteristic of normalizing ﬂows is that the latent samples are D-dimensional,
so they have the same dimensionality as the input space,
and the network consists of L layers φi,ωi : RD →RD,
so φω = φL,ωL ◦· · · ◦φ1,ω1, where each φi,ωi is designed
to be invertible for all ωi, thereby making the entire
network invertible. The beneﬁt of this formulation is that
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Density estimation models on the Big Moon, Small Moon
toy example (see Fig. 4). The parametric Gaussian model is limited
to an ellipsoidal (convex, unimodal) density. KDE with an RBF kernel
is more ﬂexible, yet tends to underﬁt the (multiscale) distribution
due to a uniform kernel scale. RealNVP is the most ﬂexible model,
yet ﬂow architectures induce biases as well, here a connected
support caused by afﬁne coupling layers in RealNVP.
the probability density of x can be calculated exactly via a
change of variables
px(x) = pz(φ−1
 det Jφ−1
where xL = x and xi
i+1 ◦· · · ◦φ−1
L (x) otherwise. Normalizing ﬂow models are typically optimized to
maximize the likelihood of the training data. Evaluating
each layer’s Jacobian and its determinant can be very
expensive. Consequently, the layers of ﬂow models are
usually designed so that the Jacobian is guaranteed to
be upper (or lower) triangular or have some other nice
structure such that one does not need to compute the full
Jacobian to evaluate its determinant , , 
(see for an application in physics).
An advantage of these models over other methods is that
one can calculate the likelihood of a point directly without
any approximation while also being able to sample from
it reasonably efﬁciently. Because the density px(x) can be
computed exactly, normalizing ﬂow models can be applied
directly for AD , .
A drawback of these models is that they do not perform any dimensionality reduction, which argues against
applying them to images where the true (effective) dimensionality is much smaller than the image dimensionality.
Furthermore, it has been observed that these models often
assign a high likelihood to anomalous instances .
Recent work suggests that one reason for this seems to
be that the likelihood in current ﬂow models is dominated
by low-level features due to speciﬁc network architecture
inductive biases , . Despite present limitations,
we have included normalizing ﬂows here because we
believe that they may provide an elegant and promising
direction for future AD methods. We will come back to this
in our outlook in Section IX.
E. Discussion
Above, we have focused on the case of density estimation on i.i.d. samples of low-dimensional data and
images. For comparison, we show in Fig. 6 three canonical
density estimation models (Gaussian, KDE, and RealNVP)
trained on the Big Moon, Small Moon toy data set, each of
which makes use of a different feature representation (raw
input, kernel, and neural network). It is worth noting that
there exist many deep statistical models for other settings.
When performing conditional AD, for example, one can
use GAN , VAE , and normalizing ﬂow 
variants that perform conditional density estimation. Likewise, there exist many DGMs for virtually all data types,
including time-series data , , text , ,
and graphs – , all of which may potentially be
used for AD.
It has been argued that full density estimation is not
needed for solving the AD problem since one learns all
density level sets simultaneously when one really only
needs a single density level set , , . This violates
Vapnik’s principle: “[W]hen limited amount of data is
available, one should avoid solving a more general problem as an intermediate step to solve the original problem”
 . The methods in Section IV seek to compute only
a single density level set, that is, they perform one-class
classiﬁcation.
IV. O N E - C L A S S C L A S S I F I C AT I O N
One-class classiﬁcation , , – , occasionally also called single-class classiﬁcation , ,
adopts a discriminative approach to AD. Methods based
on one-class classiﬁcation try to avoid a full estimation
of the density as an intermediate step to AD. Instead,
these methods aim to directly learn a decision boundary
that corresponds to a desired density level set of the
normal data distribution P+, or more generally, to produce
a decision boundary that yields a low error when applied
to unseen data.
A. One-Class Classiﬁcation Objective
We can see one-class classiﬁcation as a particularly
tricky classiﬁcation problem, namely as binary classiﬁcation where we only have (or almost only have) access
to data from one class—the normal class. Given this
imbalanced setting, the one-class classiﬁcation objective
is to learn a one-class decision boundary that minimizes:
1) falsely raised alarms for true normal instances (i.e.,
the false alarm rate or type I error) and 2) undetected or
missed true anomalies (i.e., the miss rate or type II error).
Achieving a low (or zero) false alarm rate is conceptually
simple: given enough normal data points, one could just
draw some boundary that encloses all the points, for
example, a sufﬁciently large ball that contains all data
instances. The crux here is, of course, to simultaneously
keep the miss rate low, that is, to not draw this boundary
too loosely. For this reason, one usually a priori speciﬁes
some target false alarm rate α ∈ for which the miss
rate is then sought to be minimized. Note that this precisely
corresponds to the idea of estimating an α-density level set
for some a priori ﬁxed level α ∈ . The key question in
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
one-class classiﬁcation, thus, is how to minimize the miss
rate for some given target false alarm rate with access to
no (or only a few) anomalies.
We can express the rationale above in terms of the
binary classiﬁcation risk , . Let Y ∈{±1} be
the class random variable, where again Y = +1 denotes
normal and Y
= −1 denotes anomalous points, so we
can then identify the normal data distribution as P+ ≡
PX|Y =+1 and the anomaly distribution as P- ≡PX|Y =−1,
respectively. Furthermore, let ℓ: R×{±1} →R be a binary
classiﬁcation loss and f : X →R be some real-valued score
function. The classiﬁcation risk of f under loss ℓis then
R(f) = EX∼P+[ℓ(f(X), +1)] + EX∼P-[ℓ(f(X), −1)].
Minimizing the second term—the expected loss of classifying true anomalies as normal—corresponds to minimizing the (expected) miss rate. Given some unlabeled data
x1, . . . , xn ∈X and, potentially, some additional labeled
data (˜x1, ˜y1), . . . , (˜xm, ˜ym), we can apply the principle of
empirical risk minimization to obtain
ℓ(f(xi), +1) + 1
ℓ(f(˜xj), ˜yj) + R. (15)
This solidiﬁes the empirical one-class classiﬁcation objective. Note that the second term is an empty sum in the
unsupervised setting. Without any additional constraints
or regularization, the empirical objective (15) would then
be trivial. We add R as an additional term to denote
and capture regularization, which may take various forms
depending on the assumptions about f but critically also
about P-. Generally, the regularization R = R(f) aims to
minimize the miss rate (e.g., via volume minimization and
assumptions about P-) and improve generalization (e.g.,
via smoothing of f). Furthermore, note that the pseudolabeling of y = +1 in the ﬁrst term incorporates the assumption that the n unlabeled training data points are normal.
This assumption can be adjusted, however, through speciﬁc
choices of the loss (e.g., hinge) and regularization, for
example, requiring some fraction of the unlabeled data to
get misclassiﬁed to include an assumption about the contamination rate η or achieve some target false alarm rate α.
B. One-Class Classiﬁcation in Input Space
As an illustrative example that conveys useful intuition,
ﬁtting a data-enclosing ball as a one-class model. Given
x1, . . . , xn ∈X, we can deﬁne the following objective:
s.t. ∥xi −c∥2 ≤R2 + ξi,
In other words, we aim to ﬁnd a hypersphere with radius
0 and center c
that encloses the data
(∥xi −c∥2 ≤R2). To control the miss rate, we minimize the volume of this hypersphere by minimizing R2 to
achieve a tight spherical boundary. Slack variables ξi ≥0
allow some points to fall outside the sphere, thus making
the boundary soft, where hyperparameter ν ∈(0, 1] balances this tradeoff.
corresponds
Equivalently, we
classiﬁcation
cost-weighted)
(1/(1 + ν)) max(0, s)
and ℓ(s, −1) = (ν/(1 + ν)) max(0, −s) . Then, for a
hypersphere model fθ(x) = ∥x−c∥2 −R2 with parameters
θ = (R, c), the corresponding classiﬁcation risk (14) is
EX∼P+[max(0, ∥X −c∥2 −R2)]
+ν EX∼P-[max(0, R2 −∥X −c∥2)].
We can estimate the ﬁrst term in (17) empirically from
x1, . . . , xn, again assuming that (most of) these points
have been drawn from P+. If labeled anomalies are
absent, we can still make an assumption about their
distribution P-. Following the basic, uninformed prior
assumption that anomalies may occur uniformly on X
(i.e., P- ≡U(X )), we can examine the expected value in
the second term analytically:
EX∼U(X)[max(0, R2 −∥X −c∥2)]
max(0, R2 −∥x −c∥2) dλ(x)
≤R2 λ(BR(c))
where BR(c) ⊆X denotes the ball centered at c with
radius R and λ is again the standard (Lebesgue) measure
of volume.3 This shows that the minimum volume principle
 , naturally arises in one-class classiﬁcation
through seeking to minimize the risk of missing anomalies,
here illustrated for an assumption that the anomaly
distribution P- follows a uniform distribution. Overall,
from (17), we, thus, can derive the empirical objective
R,c R2 + 1
max(0, ∥xi −c∥2 −R2)
which corresponds to (16) with the constraints directly
incorporated into the objective function. We remark
that the cost-weighting hyperparameter ν
3Again note that we assume λ(X) < ∞here, that is, the data space
X can be bounded to numerically meaningful values.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
purposefully chosen here since it is an upper bound on the
ratio of points outside and a lower bound on the ratio of
points inside or on the boundary of the sphere , .
We can, therefore, see ν as an approximation of the false
alarm rate, that is, ν ≈α.
A sphere in the input space X is, of course, a very limited
model and only matches a limited class of distributions
P+ (e.g., an isotropic Gaussian distribution). MVEs ,
 and the MCD estimator are a generalization
to nonisotropic distributions with elliptical support. Nonparametric methods, such as one-class neighbor machines
 , provide additional freedom to model multimodal
distributions having nonconvex support. Extending the
objective and principles above to general feature spaces
(e.g., , , and ) further increases the ﬂexibility of one-class models and enables decision boundaries
for more complex distributions.
C. Kernel-Based One-Class Classiﬁcation
kernel-based
 , are perhaps the most well-known one-class
classiﬁcation methods. Let k : X × X
→R be some
PSD kernel with associated RKHS Fk and corresponding
feature map φk : X →Fk, so k(x, ˜x) = ⟨φk(x), φk(˜x)⟩
for all x, ˜x ∈X. The objective of (kernel) SVDD is again
to ﬁnd a data-enclosing hypersphere of minimum volume.
The SVDD primal problem is the one given in (16) but with
the hypersphere model fθ(x) = ∥φk(x) −c∥2 −R2 deﬁned
in feature space Fk instead. In comparison, the OC-SVM
objective is to ﬁnd a hyperplane w ∈Fk that separates
the data in feature space Fk with maximum margin from
the origin
2∥w∥2 −ρ + 1
s.t. ρ −⟨φk(xi), w⟩≤ξi,
ρ −⟨φk(x), w⟩in feature space Fk with model parameters
θ = (w, ρ). The margin to the origin is given by (ρ/∥w∥),
which is maximized via maximizing ρ, where ∥w∥acts as
a normalizer.
Both the OC-SVM and SVDD can be solved in their
respective dual formulations that are quadratic programs
that only involve dot products (the feature map φk is
implicit). For the standard Gaussian kernel (or any kernel
with constant norm k(x, x) = c > 0), the OC-SVM and
SVDD are equivalent . In this case, the corresponding
density level set estimator deﬁned by
ˆCν = {x ∈X | fθ(x) < 0}
is, in fact, an asymptotically consistent ν-density level set
estimator . The solution paths of hyperparameter ν
have been analyzed for both the OC-SVM and
SVDD .
Kernel-induced feature spaces considerably improve the
expressive power of one-class methods and allow learning
well-performing models in multimodal, nonconvex, and
nonlinear data settings. Many variants of kernel one-class
classiﬁcation have been proposed and studied over the
years, such as hierarchical formulations for nested density
level set estimation , , multisphere SVDD ,
OC-SVM for group AD , boosting via L1-norm regularized OC-SVM , one-class kernel Fisher discriminants – , Bayesian data description , and
distributed , incremental learning , or robust
 variants.
D. Deep One-Class Classiﬁcation
Selecting kernels and handcrafting relevant features
can be challenging and quickly become impractical for
Deep one-class
classiﬁcation
aim to overcome these challenges by learning useful
neural network feature maps φω
→Z from the
data or transferring such networks from related tasks.
OC-SVM variants , employ a hypersphere
fθ(x) = ∥φω(x) −c∥2 −R2
ρ −⟨φω(x), w⟩with explicit neural feature
maps φω(·) in (16) and (20), respectively. These methods
are typically optimized with SGD variants – ,
which, together with GPU parallelization, makes them
scale to large data sets.
The one-class Deep SVDD , has been introduced as a simpler variant compared to using a neural
hypersphere model in (16), which poses the following
objective:
∥φω(xi) −c∥2 + R.
Here, the neural network transformation φω(·) is learned
to minimize the mean squared distance over all data points
to center c ∈Z. Optimizing this simpliﬁed objective has
been found to converge faster and be effective in many
situations , , . In light of our unifying
view, we will see that we may interpret one-class Deep
SVDD also as a single-prototype deep clustering method
(see Sections V-A2 and V-D).
A recurring question in deep one-class classiﬁcation is
how to meaningfully regularize against a feature map
collapse φω ≡c. Without regularization, minimum volume or maximum margin objectives, such as (16), (20),
or (22), could be trivially solved with a constant mapping
 , . Possible solutions for this include adding
a reconstruction term or architectural constraints ,
 , freezing the embedding , , , ,
 , inversely penalizing the embedding variance ,
using true , , auxiliary , , ,
 , or artiﬁcial negative examples in training,
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
One-class classiﬁcation models on the Big Moon, Small
Moon toy example (see Fig. 4). An MVE in input space is limited to
enclose an ellipsoidal, convex region. By (implicitly) ﬁtting a
hypersphere in kernel feature space, SVDD enables nonconvex
support estimation. Deep SVDD learns an (explicit) neural feature
map (here with smooth ELU activations) that extracts multiple data
scales to ﬁt a hypersphere model in feature space for support
description.
pseudolabeling , , , , or integrating some manifold assumption . Further variants of
deep one-class classiﬁcation include multimodal or
time-series extensions and methods that employ
adversarial learning , , or transfer
learning , .
Deep one-class classiﬁcation methods generally offer
greater modeling ﬂexibility and enable the learning or
transfer of task-relevant features for complex data. They
usually require more data to be effective though or must
rely on some informative domain prior (e.g., some pretrained network). However, the underlying principle of
one-class classiﬁcation methods—targeting a discriminative one-class boundary in learning—remains unaltered,
regardless of whether a deep or shallow feature map
is used. We show three canonical one-class classiﬁcation
models (MVE, SVDD, and DSVDD) trained on the Big
Moon, Small Moon toy data set, each using a different
feature representation (raw input, kernel, and neural network), in Fig. 7 for comparison.
E. Negative Examples
One-class classiﬁers can usually incorporate labeled
negative examples (y = −1) in a direct manner due to
their close connection to binary classiﬁcation, as explained
above. Such negative examples can facilitate an empirical
estimation of the miss rate [see (14) and (15)]. We here
recognize three qualitative types of negative examples that
have been studied in the literature, which we distinguish
as artiﬁcial, auxiliary, and true negative examples that
increase in their informativeness in this order.
The idea to approach unsupervised learning problems
through generating artiﬁcial data points has been around
for some time (see [340, Section 14.2.4]). If we assume
that the anomaly distribution P- has some form that we
can generate data from, one idea would be to simply train
a binary classiﬁer to discern between the normal and the
artiﬁcial negative examples. For the uniform prior P- ≡
U(X ), this approach yields an asymptotically consistent
density level set estimator . However, classiﬁcation
against uniformly drawn points from a hypercube quickly
becomes ineffective in higher dimensions. To improve
over artiﬁcial uniform sampling, more informed sampling
strategies have been proposed , such as resampling
schemes , manifold sampling , and sampling
based on local density estimation , , as well
as active learning strategies – . Another recent
idea is to treat the enormous quantities of data that are
publicly available in some domains as auxiliary negative
examples , for example, images from photo-sharing
sites for computer vision tasks and the English Wikipedia
for NLP tasks. Such auxiliary examples provide more
informative domain knowledge, for instance, about the
distribution of natural images or the English language,
in general, as opposed to sampling random pixels or words.
This approach, called OE , which trains on known
anomalies, can signiﬁcantly improve deep AD performance
in some domains , . OE has also been used with
density-based methods by employing a margin loss 
or temperature annealing on the log-likelihood
ratio between positive and negative examples. The most
informative labeled negative examples are ultimately
true anomalies, for example, veriﬁed by some domain
expert. Access to even a few labeled anomalies has been
shown to improve detection performance signiﬁcantly
 , , . There also have been active learning
algorithms
subjective
feedback (e.g., from an expert) to learn about the
user-speciﬁc informativeness of particular anomalies in
an application . Finally, we remark that negative
examples have also been incorporated heuristically into
reconstruction models via using a bounded reconstruction
error since maximizing the unbounded error for
negative examples can quickly become unstable. We will
turn to reconstruction models next.
V. R E C O N S T R U C T I O N M O D E L S
Models that are trained on a reconstruction objective are
among the earliest , and most common
approaches
Reconstruction-based methods learn a model that is
optimized to well-reconstruct normal data instances,
thereby aiming to detect anomalies by failing to accurately
reconstruct them under the learned model. Most of these
methods have a purely geometric motivation (e.g., PCA or
deterministic AEs), yet some probabilistic variants reveal
a connection to density (level set) estimation. In this
section, we deﬁne the general reconstruction learning
objective, highlight
underlying
assumptions,
reconstruction-based
discuss their variants.
A. Reconstruction Objective
φθ(x) be a feature map
from the data space X onto itself that is composed of
an encoding function φe : X →Z (the encoder) and a
decoding function φd : Z →X (the decoder), that is,
φθ ≡(φd ◦φe)θ, where θ holds the parameters of both
the encoder and the decoder. We call Z the latent space
and φe(x) = z the latent representation of x. The reconstruction objective then is to learn
φθ such that φθ(x) = φd(φe(x)) = ˆx ≈x, that is, to ﬁnd
some encoding and decoding transformation so that x is
reconstructed with minimal error, usually measured in the
Euclidean distance. Given unlabeled data x1, . . . , xn ∈X,
the reconstruction objective is given by
∥xi −(φd ◦φe)θ(xi)∥2 + R
where R again denotes the different forms of regularization that various methods introduce, for example, on the
parameters θ, the structure of the encoding and decoding transformations, or the geometry of latent space Z.
Without any restrictions, the reconstruction objective (23)
would be optimally solved by the identity map φθ ≡id, but
then, of course, nothing would be learned from the data.
In order to learn something useful, structural assumptions about the data-generating process are, therefore,
necessary. We here identify two principal assumptions: the
manifold and the prototype assumptions.
1) Manifold
Assumption:
assumption
asserts that the data lives (approximately) on some lower
dimensional (possibly nonlinear and nonconvex) manifold M that is embedded within the data space X, that
is, M ⊂X with dim(M) < dim(X ). In this case, X is,
sometimes, also called the ambient or observation space.
For natural images observed in pixel space, for instance,
the manifold captures the structure of scenes, variation due
to rotation and translation, and changes in color, shape,
size, texture, and so on. For human voices observed in
audio-signal space, the manifold captures variation due
to the words being spoken and person-to-person variation in the anatomy and physiology of the vocal folds.
The (approximate) manifold assumption implies that there
exists a lower dimensional latent space Z and functions
φe : X →Z and φd : Z →X such that, for all x ∈X,
x ≈φd(φe(x)). Consequently, the generating distribution
P can be represented as the push-forward through φd of a
latent distribution PZ. Equivalently, the latent distribution
PZ is the push-forward of P through φe.
The goal of learning is, therefore, to learn the pair
of functions φe and φd so that φd(φe(X )) ≈M ⊂X.
Methods that incorporate the manifold assumption usually restrict the latent space Z
Rd to have much
lower dimensionality d than the data space X
(i.e., d ≪D). The manifold assumption is also widespread
in related unsupervised learning tasks, such as manifold
learning itself , , dimensionality reduction ,
 – , disentanglement , , and representation learning, in general , .
2) Prototype Assumption:
The prototype assumption
asserts that there exists a ﬁnite number of prototypical elements in the data space X that characterize the
data well. We can model this assumption in terms of a
data-generating distribution that depends on a discrete
latent categorical variable Z ∈Z = {1, . . . , K} that captures some K prototypes or modes of the data distribution.
This prototype assumption is also common in clustering
and classiﬁcation when we assume a collection of prototypical instances represent clusters or classes well. With the
reconstruction objective under the prototype assumption,
we aim to learn an encoding function that, for x ∈X,
identiﬁes a φe(x) = k ∈{1, . . . , K} and a decoding
function k →φd(k) = ck that maps to some kth prototype
(or some prototypical distribution or mixture of prototypes
more generally) such that the reconstruction error ∥x−ck∥
becomes minimal. In contrast to the manifold assumption
where we aim to describe the data by some continuous
mapping, under the (most basic) prototype assumption,
we characterize the data by a discrete set of vectors
{c1, . . . , cK} ⊆X. The method of representing a data
distribution by a set of prototype vectors is also known as
VQ , .
3) Reconstruction Anomaly Score:
model that is
trained on the reconstruction objective must extract salient
features and characteristic patterns from the data in
its encoding—subject to imposed model assumptions—
so that its decoding from the compressed latent representation achieves low reconstruction error (e.g., feature
correlations and dependencies, recurring patterns, cluster structure, and statistical redundancy). Assuming that
the training data x1, . . . , xn ∈X include mostly normal
points, we, therefore, expect a reconstruction-based model
to produce a low reconstruction error for normal instances
and a high reconstruction error for anomalies. For this
reason, the anomaly score is usually also directly deﬁned
by the reconstruction error
s(x) = ∥x −(φd ◦φe)θ(x)∥2.
For models that have learned some truthful manifold structure or prototypical representation, a high reconstruction
error would then detect off-manifold or nonprototypical
instances.
reconstruction
probabilistic motivation, and a point x gets ﬂagged anomalous simply because it does not conform to its ‘idealized’ representation φd(φe(x)) = ˆx under the encoding
and decoding processes. However, some reconstruction
methods also have probabilistic interpretations, such as
PCA , or even are derived from probabilistic objectives, such as Bayesian PCA or VAEs . These
methods are again related to density (level set) estimation
(under speciﬁc assumptions about some latent structure),
usually in the sense that a high reconstruction error indicates low-density regions, and vice versa.
B. Principal Component Analysis
A common way to formulate the PCA objective is to
seek an orthogonal basis W in data space X
that maximizes the empirical variance of the (centered)
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
data x1, . . . , xn ∈X
s.t. W W ⊤= I.
Solving this objective results in a well-known eigenvalue
problem since the optimal basis is given by the eigenvectors of the empirical covariance matrix where the
respective eigenvalues correspond to the componentwise
variances . The d ≤D components that explain most
of the variance—the principal components—are then given
by the d eigenvectors that have the largest eigenvalues.
 – , which can be considered the default reconstruction baseline. From a reconstruction perspective,
the objective to ﬁnd an orthogonal projection W ⊤W to
a d-dimensional linear subspace (which is the case for
W ∈Rd×D with W W ⊤= I) such that the mean squared
reconstruction error is minimized
∥xi −W ⊤W xi∥2
s.t. W W ⊤= I
yields exactly the same PCA solution. Thus, PCA optimally solves the reconstruction objective (23) for a linear
encoder φe(x) = W x = z and transposed linear decoder
φd(z) = W ⊤z with constraint W W ⊤= I. For linear PCA,
we can also readily identify its probabilistic interpretation , namely that the data distribution follows from
the linear transformation X = W ⊤Z+ε of a d-dimensional
latent Gaussian distribution Z ∼N(0, I), possibly with
added noise ε ∼N(0, σ2 I) so that P ≡N(0, W ⊤W +
σ2 I). Maximizing the likelihood of this Gaussian over the
encoding and decoding parameter W again yields PCA
as the optimal solution . Hence, PCA assumes that
the data live on a d-dimensional ellipsoid embedded in
data space X ⊆RD. Standard PCA, therefore, provides an
illustrative example for the connections between density
estimation and reconstruction.
Linear PCA, of course, is limited to data encodings that
can only exploit linear feature correlations. kPCA introduced a nonlinear generalization of component analysis
by extending the PCA objective to nonlinear kernel feature
maps and taking advantage of the “kernel trick.” For a PSD
kernel k(x, ˜x) with feature map φk : X →Fk, kPCA solves
the reconstruction objective (26) in feature space Fk :
∥φk(xi) −W ⊤W φk(xi)∥2
s.t. W W ⊤= I
which results in an eigenvalue problem of the kernel
matrix . For kPCA, the reconstruction error can again
serve as an anomaly score. It can be computed implicitly
via the dual . This reconstruction from linear principal components in feature space Fk corresponds to a
Reconstruction models on the Big Moon, Small Moon toy
example (see Fig. 4). PCA ﬁnds the linear subspace with the lowest
reconstruction error under an orthogonal projection of the data.
kPCA solves (linear) component analysis in kernel feature space,
which enables an optimal reconstruction from (kernel-induced)
nonlinear components in input space. An AE with 1-D latent code
learns a 1-D, nonlinear manifold in input space having minimal
reconstruction error.
reconstruction from some nonlinear subspace or manifold
in input space X . Replacing the reconstruction
W ⊤W φk(x) in (27) with a prototype c ∈Fk yields a
reconstruction model that considers the squared error to
the kernel mean since the prototype is optimally solved by
i=1 φ(xi) for the L2-distance. For RBF kernels,
this prototype model is (up to a multiplicative constant)
equivalent to KDE , which provides a link between kernel reconstruction and nonparametric density estimation
methods. Finally, rPCA variants have been introduced as
well – , which account for data contamination
or noise (see Section II-C2).
C. Autoencoders
AEs are reconstruction models that use neural networks
for the encoding and decoding of data. They were
originally
introduced
 – 
primarily as methods to perform nonlinear dimensionality
reduction , , yet they have also been studied
early on for AD , . Today, deep AEs are among
the most widely adopted methods for deep AD in the
literature , , , – likely due to
their long history and easy-to-use standard variants. The
standard AE objective is given by
∥xi −(φd ◦φe)ω(xi)∥2 + R
which is a realization of the general reconstruction
objective (23) with θ = ω, that is, the optimization is
carried out over the weights ω of the neural network
encoder and decoder. A common way to regularize
AEs is by mapping to a lower dimensional “bottleneck”
representation φe(x) =
Z through the encoder
network, which enforces data compression and effectively
limits the dimensionality of the manifold or subspace to be
learned. If linear networks are used, such an AE, in fact,
recovers the same optimal subspace as spanned by the PCA
eigenvectors , . In Fig. 8, we show a comparison
of three canonical reconstruction models (PCA, kPCA, and
AE) trained on the Big Moon, Small Moon toy data set, each
using a different feature representation (raw input, kernel,
and neural network), resulting in different manifolds.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Apart from a “bottleneck,” a number of different ways
to regularize AEs have been introduced in the literature.
Following ideas of sparse coding – , sparse
regularize
dimensional, overcomplete) latent code toward sparsity,
for example, via L1 Lasso penalization . DAEs ,
 explicitly feed noise-corrupted inputs ˜x = x + ε into
the network, which is then trained to reconstruct the original inputs x. DAEs, thus, provide a way to specify a noise
model for ε (see Section II-C2), which has been applied for
noise-robust acoustic novelty detection , for instance.
In situations in which the training data are already
corrupted with noise or unknown anomalies, robust deep
AEs , which splits the data into well-represented and
corrupted parts similar to rPCA , have been proposed.
Contractive AEs (CAEs) propose to penalize the
Frobenius norm of the Jacobian of the encoder activations
with respect to the inputs to obtain a smoother and more
robust latent representation. Such ways of regularization
inﬂuence the geometry and shape of the subspace or manifold that is learned by the AE, for example, by imposing
some degree of smoothness or introducing invariances
toward certain types of input corruptions or transformations . Hence, these regularization choices should
again reﬂect the speciﬁc assumptions of a given AD task.
Besides the above deterministic variants, probabilistic
AEs have also been proposed, which again establish a
connection to density estimation. The most explored class
of probabilistic AEs are VAEs – , as introduced in
Section III-C1, through the lens of neural generative models, which approximately maximizes the data likelihood (or
evidence) by maximizing the ELBO. From a reconstruction
perspective, VAEs adopt a stochastic autoencoding process,
which is realized by encoding and decoding the parameters
of distributions (e.g., Gaussians) through the encoder and
decoder networks, from which the latent code and reconstruction then can be sampled. For a standard Gaussian
VAE, for example, where q(z|x)
N(μx, diag(σ2
p(z) ∼N(0, I), and p(x|z) ∼N(μz, I) with encoder
φe,ω′(x) = (μx, σx) and decoder φd,ω(z) = μz, the empirical ELBO objective (10) becomes
2∥xi −μzij ∥2
+ DKL(N(zij; μxi, diag(σ2
xi))∥N(zij; 0, I))
where zi1, . . . , ziM are M Monte Carlo samples drawn
from the encoding distribution z ∼q(z|xi) of xi. Hence,
such a VAE is trained to minimize the mean reconstruction
error over samples from an encoded latent Gaussian that
is regularized to be close to a standard isotropic Gaussian.
VAEs have been used in various forms for AD ,
 , , for instance, on multimodal sequential data
with LSTMs in robot-assisted feeding and for new
physics mining at the Large Hadron Collider . Another
class of probabilistic AEs that has been applied to AD are
AAEs , , . By adopting an adversarial loss
to regularize and match the latent encoding distribution,
AAEs can employ any arbitrary prior p(z), as long as
sampling is feasible.
Finally, other AE variants that have been applied to
AD include RNN-based AEs , , , ,
convolutional AEs , AE ensembles , , and
variants that constrain the gradients or actively
control the latent code topology of an AE. AEs also
have been utilized in two-step approaches that use AEs for
dimensionality reduction and apply traditional methods on
the learned embeddings , , .
D. Prototypical Clustering
Clustering methods that make the prototype assumption provide another approach to reconstruction-based
AD. As mentioned above, the reconstruction error here is
usually given by the distance of a point to its nearest prototype, which ideally has been learned to represent a distinct
mode of the normal data distribution. Prototypical clustering methods include the well-known VQ algorithms
k-means, k-medians, and k-medoids that deﬁne a Voronoi
partitioning , over the metric space where they
are applied—typically the input space X. Kernel variants
of k-means have also been studied and considered
for AD . GMMs with a ﬁnite number of k mixtures
(see Section III-A) have been used for (soft) prototypical
clustering as well. Here, the distance to each cluster (or
mixture component) is given by the Mahalanobis distance
that is deﬁned by the covariance matrix of the respective
Gaussian mixture component .
More recently, deep learning approaches to clustering
introduced
 – ,
based on k-means , and adopted for AD ,
classiﬁcation
(see Section IV-D), a persistent question in deep clustering
is how to effectively regularize against a feature map collapse . Note that, while, for deep clustering methods,
the reconstruction error is measured in latent space Z,
for deep AEs, it is measured in the input space X after
decoding. Thus, a latent feature collapse (i.e., a constant
encoder φe ≡c ∈Z) would result in a constant decoding
(the data mean at optimum) for an AE, which, generally,
is a suboptimal solution of (28). For this reason, AEs seem
less susceptible to a feature collapse though they have also
been observed to converge to bad local optima under SGD
optimization, speciﬁcally if they employ bias terms .
VI. U N I F Y I N G V I E W O F A N O M A L Y
D E T E C T I O N
In this section, we present a unifying view of the AD
problem. We identify speciﬁc AD modeling components
that allow us to characterize the many methods discussed
above in a systematic way. Importantly, this view reveals
connections that enable the transfer of algorithmic ideas
between existing AD methods. Thus, it uncovers promising
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Table 2 AD Methods Identiﬁed With Our Unifying View (Last Column Contains Representative References)
directions for future research, such as transferring concepts
and ideas from kernel-based AD to deep methods, and
vice versa.
A. Modeling Dimensions of the AD Problem
We identify the following ﬁve components or modeling
dimensions for AD:
Dimension D1 Loss is the (scalar) loss function that is
applied to the output of some model fθ(x). Semisupervised
or supervised methods use loss functions that incorporate labels, but, for the many unsupervised AD methods,
we have ℓ(s, y) = ℓ(s). D2 Model deﬁnes the speciﬁc
model fθ that maps an input x ∈X to some scalar value
that is evaluated by the loss. We have arranged our previous three sections along this modeling dimension where
we covered certain groups of methods that formulate
models based on common principles, namely probabilistic
modeling, one-class classiﬁcation, and reconstruction. Due
to the close link between AD and density estimation (see
Section II-B5), many of the methods formulate a likelihood model fθ(x) = pθ(x | Dn) with negative log-loss
ℓ(s) = −log(s), that is, they have a negative log-likelihood
objective, where Dn = {x1, . . . , xn} denotes the training
data. Dimension D3 captures the Feature Map x →φ(x)
that is used in a model fθ. This could be an (implicit)
feature map φk(x) deﬁned by some given kernel k in kernel
methods, for example, or an (explicit) neural network
feature map φω(x) that is learned and parameterized
with network weights ω in deep learning methods. With
dimension D4 Regularization, we capture various forms of
regularization R(f, φ, θ) of the model fθ, the feature map
φ, and their parameters θ in a broader sense. Note that θ
here may include both model parameters and feature map
parameters, that is, θ = (θf, θφ), in general. θf could be the
distributional parameters of a parametric density model,
for instance, and θφ the weights of a neural network.
Our last modeling dimension D5 describes the Inference
Mode, speciﬁcally whether a method performs Bayesian
inference .
The identiﬁcation of the above modeling dimensions
enables us to formulate a general AD learning objective
that encompasses a broad range of AD methods:
Denoting the minimum of (∗) by θ∗, the anomaly score
of a test input ˜x is computed via the model fθ∗(˜x).
In the Bayesian case, where the objective in (∗) is the
negative log-likelihood of a posterior p(θ | Dn) induced
by a prior distribution p(θ), we can predict in a fully
Bayesian fashion via the expected model Eθ∼p(θ | Dn)fθ(x).
In Table 2, we describe many well-known AD methods
using our unifying view.
B. Comparative Discussion
In the following, we compare the various approaches
in light of our unifying view and discuss how this view
enables the transfer of concepts between existing AD
methods. Table 2 shows that the probabilistic methods
are largely based on the negative log-likelihood objective. The resulting negative log-likelihood anomaly scores
provide a (usually continuous) ranking that is generally
more informative than a binary density level set detector (see Section II-B7). Reconstruction methods provide
such a ranking as well, with the anomaly score given by
the difference of a data instance and its reconstruction
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
under the model. Besides ranking and detecting anomalies,
such scores make it possible to also rank inliers, which
can be used, for example, to judge cluster memberships
or determine prototypes (see Section V-D). Reconstruction is particularly well suited when the data follow
some manifold or prototypical structure (see Section V-A).
In comparison, standard one-class classiﬁcation methods,
which aim to estimate a discriminative level set boundary
(see Section IV), usually do not rank inliers. This is typically incorporated into the learning objective via a hinge
loss, as can be seen in Table 2. One-class classiﬁcation
is generally more sample-efﬁcient and more robust to
a nonrepresentative sampling of the normal data (e.g.,
a sampling bias toward speciﬁc normal modes) but
is consequentially also less informative. However, an inlier
ranking for one-class classiﬁcation can still be obtained
via the distance of a point to the decision boundary, but
such an approximate ranking may not faithfully represent in-distribution modes and so on. In addition to the
theoretical comparison and discussion of AD methods in
regard to our unifying view, we will present an empirical
evaluation that includes methods from all three groups
(probabilistic, one-class classiﬁcation, and reconstruction)
and three types of feature maps (raw input, kernel, and
neural network) in Section VII-C, where we ﬁnd that the
detection performance in different data scenarios is very
heterogeneous among the methods (with an advantage for
deep methods on the more complex, semantic detection
tasks). This exempliﬁes the fact that there is no simple
“silver bullet” solution to the AD problem.
In addition to providing a framework for comparing
methods, our unifying view also allows us to identify
concepts that may be transferred between shallow and
deep AD methods in a systematic manner. We discuss a
few explicit examples to illustrate this point here. Table 2
shows that both the (kernel) SVDD and Deep SVDD employ
a hypersphere model. This connection can be used to
transfer adaptations of the hypersphere model from one
world to another (from shallow to deep, or vice versa).
The adoption of semisupervised , , or
multisphere , , model extensions give
successful examples for such a transfer. Next, note in
Table 2, that deep AEs usually consider the reconstruction
error in the original data space X after a neural network
encoding and decoding. In comparison, kPCA deﬁnes the
error in kernel feature space Fk. One might ask whether
using the reconstruction error in some neural feature
space may also be useful for AEs, for instance, to shift
detection toward higher level feature spaces. Recent work
that includes the reconstruction error over the hidden
layers of an AE , indeed, suggests that this concept
can improve detection performance. Another question one
might ask when comparing the reconstruction models
in Table 2 is whether including the prototype assumption
(see Section V-A2) could also be useful in deep autoencoding and how this can be achieved practically. The VQ-VAE
model, which introduces a discrete codebook between the
neural encoder and decoder, presents a way to incorporate
this concept that has shown to result in reconstructions
with improved quality and coherence in some settings
 , . Besides these existing proofs of concept for
transferring ideas, which we have motivated here from our
unifying view, we outline further potential combinations to
explore in future research in Section IX-A.
C. Distance-Based Anomaly Detection
Our unifying view focuses on AD methods that formulate some loss-based learning objectives. Apart from
these methods, there also exists a rich literature on purely
“distance-based” AD methods and algorithms that have
been studied extensively in the data mining community,
in particular. Many of these algorithms follow a lazy
learning paradigm, in which there is no a priori training
phase of learning a model, but, instead, new test points
are evaluated with respect to the training instances only
as they occur. We here group these methods as “distancebased” without further granularity but remark that various
taxonomies for these types of methods have been proposed
 , . Examples of such methods include nearestneighbor-based methods , , – , such as
LOF and partitioning tree-based methods , such
as iForest , . These methods usually also aim to
capture the high-density regions of the data in some manner, for instance, by scaling distances in relation to local
neighborhoods , and, thus, are most consistent with
the formal AD problem deﬁnition presented in Section II.
The majority of these algorithms have been studied and
applied in the original input space X. Few of them have
been considered in the context of deep learning, but some
hybrid AD approaches exist, which apply distance-based
algorithms on top of deep neural feature maps from pretrained networks (e.g., ).
VII. E V A L U AT I O N A N D E X P L A N AT I O N
The theoretical considerations and unifying view above
provide useful insights about the characteristics and
underlying modeling assumptions of the different AD
methods. What matters the most to the practitioner,
however, is to evaluate how well an AD method performs
on real data. In this section, we ﬁrst present different
aspects of evaluation, in particular, the problem of building
a data set that includes meaningful anomalies, and the
problem of robustly evaluating an AD model on the
collected data. In the second step, we will look at the
limitations of classical evaluation techniques, speciﬁcally
their inability to directly inspect, and verify the exact
strategy employed by some model for detection, for
instance, which input variables that a model uses for
prediction. We then present “XAI” approaches for enabling
such deeper inspection of a model.
A. Building Anomaly Detection Benchmarks
Unlike standard supervised data sets, there is an intrinsic difﬁculty in building AD benchmarks: anomalies are
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Table 3 Existing AD Benchmarks
rare, and some of them may have never been observed
before they manifest themselves in practice. Existing
anomaly benchmarks typically rely on one of the following
strategies.
1) k-classes-out: Start from a binary or multiclass data
set and declare one or more classes to be normal
and the rest to be anomalous. Due to the semantic
homogeneity of the resulting “anomalies,” such a
benchmark may not be a good simulacrum of real
anomalies. For example, simple low-level anomalies
(e.g., additive noise) may not be tested for.
2) Synthetic: Start from an existing supervised or unsupervised data set and generate synthetic anomalies
(e.g., – ). Having full control over anomalies is desirable from a statistical viewpoint, to get
robust error estimates. However, the characteristics
of real anomalies may be unknown or difﬁcult to
3) Real-world: Consider a data set that contains anomalies and have them labeled by a human expert. This
is the ideal case. In addition to the anomaly label,
the human can augment a sample with an annotation
of which exact features are responsible for the anomaly (e.g., a segmentation mask in the context of image
We provide examples of AD benchmarks and data sets
falling into these three categories in Table 3.
Although all three approaches are capable of producing
anomalous data, we note that real anomalies may exhibit
much wider and ﬁner variations compared to those in the
data set. In adversarial cases, anomalies may be designed
maliciously to avoid detection (e.g., in fraud and cybersecurity scenarios , , – ).
B. Evaluating Anomaly Detectors
Most applications come with different costs for false
alarms (type I error) and missed anomalies (type II error).
Hence, it is common to consider the decision function
if s(x) ≥τ
if s(x) < τ
where s denotes the anomaly score, and adjust the decision
threshold τ in a way that 1) minimizes the costs associated
with the type I and type II errors on the collected validation
data or 2) accommodates the hard constraints of the
environment in which the AD system will be deployed.
To illustrate this, consider an example in ﬁnancial fraud
detection: anomaly alarms are typically sent to a fraud
analyst who must decide whether to open an investigation into the potentially fraudulent activity. There are,
typically, a ﬁxed number of analysts. Suppose they can
only handle k alarms per day, that is, the k examples
with the highest predicted anomaly score. In this scenario,
the measure to optimize is the precision@k since we want
to maximize the number of anomalies contained in those
In contrast, consider a credit card company that places
an automatic hold on a credit card when an anomaly
alarm is reported. False alarms result in angry customers
and reduced revenue, so the goal is to maximize the
number of true alarms subject to a constraint on the
percentage of false alarms. The corresponding measure
is to maximize recall@k, where k is the number of false
However, it is often the case that application-related
costs and constraints are not fully speciﬁed or vary over
time. With such restrictions, it is desirable to have a measure that evaluates the performance of AD models under
a broad range of possible application scenarios, or analogously, a broad range of decision thresholds τ. The AUROC
(or simply AUC) provides an evaluation measure that considers the full range of decision thresholds on a given test
set , . The ROC curve plots all the (false alarm
rate, recall)-pairs that result from iterating over all thresholds that cover every possible test set decision split, and
the area under this curve is the AUC measure. A convenient
property of the AUC is that the random guessing baseline
always achieves an AUC of 0.5, regardless of whether there
is an imbalance between anomalies and normal instances
in the test set. This makes AUC easy to interpret and
comparable over different application scenarios, which is
one of the reasons why AUC is the most commonly used
performance measure in AD , . One caveat of
AUC is that it can produce overly optimistic scores in the
case of highly imbalanced test sets , . In such
cases, the AUPRC is more informative and appropriate to
use , . The PR curve plots all the (precision,
recall)-pairs that result from iterating over all possible test
set decision thresholds. AUPRC, therefore, is preferable to
AUROC when precision is more relevant than the false
alarm rate. A common robust way to compute AUPRC is
via AP . One downside of AUPRC (or AP) is that
the random guessing baseline is given by the fraction
of anomalies in the test set and, thus, varies between
applications. This makes AUPRC (or AP) generally harder
to interpret and less comparable over different application
scenarios. In scenarios where there is no clear preference
for precision or the false alarm rate, we recommend to
ideally report both threshold-independent measures for a
comprehensive evaluation.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Table 4 AUC Detection Performance on MNIST-C
C. Comparison on MNIST-C and MVTec-AD
In the following, we apply the AUC measure to compare a selection of AD methods from the three major
approaches (probabilistic, one-class, and reconstruction)
and three types of feature representation (raw input,
kernel, and neural network). We perform the comparison
on the synthetic MNIST-C and real-world MVTec-AD data
sets. MNIST-C is MNIST extended with a set of 15 types
of corruptions (e.g., blurring, added stripes, and impulse
noise). MVTec-AD consists of 15 image sets from industrial
production, where anomalies correspond to manufacturing
defects. These images sometimes take the form of textures
(e.g., wood and grid) or objects (e.g., toothbrush and
screw). For MNIST-C, models are trained on the standard
MNIST training set and then tested on each corruption
separately. We measure the AUC separating the corrupted
from the uncorrupted test set. For MVTec-AD, we train
distinct models on each of the 15 image sets and measure
the AUC on the corresponding test set. Results for each
model are shown in Tables 4 and 5. We provide the training
details of each model in Appendix B-B.
The ﬁrst striking observation is the heterogeneity in
the performance of the various methods on the different corruptions and defect classes. For example, AGAN
performs generally well on MNIST-C but is systematically
outperformed by the deep one-class classiﬁcation (DOCC)
model on MVTec-AD. Also, the more powerful nonlinear
models are not better in every class, and simple “shallow” models occasionally outperform their deeper counterparts. For instance, the simple Gaussian model reaches
top performance on MNIST-C:spatter, linear PCA ranks
highest on MVTec-AD:toothbrush, and KDE ranks highest
on MVTec-AD:wood. The fact that some of the simplest
models, sometimes, perform well highlights the strong
differences in the modeling structure of each AD model.
Table 5 AUC Detection Performance on MVTec-AD
Since the MNIST-C and MVTec-AD test sets are not highly
imbalanced, we see the same trends when using AP as an
evaluation measure as to be expected . We provide
the detection performance results in AP in Appendix B-A.
However, what is still unclear is whether the measured
model performance faithfully reﬂects the performance on
a broader set of anomalies (i.e., the generalization performance) or whether some methods only beneﬁt from the
speciﬁc (possibly nonrepresentative) types of anomalies
that have been collected in the test set. In other words,
assuming that all models achieve 100% test accuracy (e.g.,
MNIST-C:stripe), can we conclude that all models will
perform well on a broad range of anomalies? This problem
has been already highlighted in the context of supervised
learning, and explanation methods can be applied to
uncover such potential hidden weaknesses of models, also
known as “Clever Hanses” .
D. Explaining Anomalies
In the following, we consider techniques that augment
anomaly predictions with explanations. These techniques
enable us to better understand the generalization properties and detection strategies used by different anomaly models and, in turn, to also address some of the
limitations of classical validation procedures. Producing
explanations of model predictions is already common in
supervised learning, and this ﬁeld is often referred to
as XAI . Popular XAI methods include LIME ,
(guided) Grad-CAM , integrated gradients ,
 , and layerwise relevance propagation (LRP) .
Grad-CAM and LRP rely on the structure of the network to
produce robust explanations.
XAI has recently also been brought to unsupervised
learning and, in particular, AD ,
 – . Unlike supervised learning, which is largely
dominated by neural networks , , , stateof-the-art methods for unsupervised learning are much
more heterogeneous, including neural networks but also
kernel-, centroid-, or probability-based models. In such a
heterogeneous setting, it is difﬁcult to build explanation
methods that allow for a consistent comparison of detection strategies of the multiple AD models. Two directions
to achieve such consistent explanations are particularly
promising:
1) model-agnostic
explanation
techniques
sampling-based) that apply transparently to any
model, whether it is a neural network or something
different (e.g., );
2) a conversion of non-neural network models into functionally equivalent neural networks, or neuralization,
so that existing approaches for explaining neural networks (e.g., LRP ) can be applied , .
In the following, we demonstrate a neuralization
approach. It has been shown that numerous AD models, in particular, kernel-based models, such as KDE or
one-class SVMs, can be rewritten as strictly equivalent
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Illustration of the neuralization concept that reformulates
models as strictly equivalent neural networks. Here, KDE, deep
one-class classiﬁcation (DOCC), and AE are expressed as a
three-layer architecture : 1)
feature extraction
distance computation
pooling . The “neuralized”
formulation enables to apply LRP for explaining anomalies.
A bag of models (here KDE and DOCC) can also be expressed in this
neural networks , . The neuralized equivalents
of a model may not be unique, and explanations obtained
with LRP consequently depend on the chosen network
structure . Here, we aim to ﬁnd a single structure
that ﬁts many models. We show examples of neuralized
models in Fig. 9. They typically organize into a three-layer
architecture; from left to right: feature extraction, distance
computation, and pooling.
For example, the KDE model, usually expressed as
f(x) = (1/n)
i=1 exp(−γ ∥x −xi∥2), can have its negative log-likelihood s(x)
−log f(x) rewritten as a
two-layer network
hj = γ ∥x −xj∥2 + log n
s(x) = sminj{hj}
where smin is a soft min-pooling of the type log-sum-exp
(see for further details).
Once the model has been converted into a neural network, we can apply explanation techniques, such as LRP
 , to produce an explanation of the anomaly prediction. In this case, the LRP algorithm will take the score
at the output of the model, propagate to “winners” in the
pool, then assign the score to directions in the input or
feature space that contribute the most to the distance, and,
if necessary, propagate the signal further down the feature
hierarchy (see the Supplement of for how this is
done exactly).
Fig. 10 shows, from left to right, an anomaly from
the MNIST-C data set, the ground-truth explanation (the
squared difference between the digit before and after
corruption), and LRP explanations for three AD models
(KDE, DOCC, and AE).
From these observations, it is clear that each model,
although predicting with 100% accuracy on the current
data, will have different generalization properties and
vulnerabilities when encountering subsequent anomalies.
We will work through an example in Section VIII-B to
show how explanations can help to diagnose and improve
a detection model.
To conclude, we emphasize that a standard quantitative
evaluation can be imprecise or even misleading when the
available data are not fully representative, and in that case,
explanations can be produced to more comprehensively
assess the quality of an AD model.
VIII. W O R K E D - T H R O U G H E X A M P L E S
In this section, we work through two speciﬁc, real-world
examples to exemplify the modeling and evaluation
process and provide some best practices.
A. Example 1: Thyroid Disease Detection
In the ﬁrst example, our goal is to learn a model to detect
thyroid gland dysfunctions, such as hyperthyroidism. The
Thyroid data set4 includes n
3772 data instances
and has D = 6 real-valued features. It contains a total
of 93 (∼2.5%) anomalies. For a quantitative evaluation,
we consider a data set split of 60:10:30 corresponding to
the training, validation, and test sets, respectively, while
preserving the ratio of ∼2.5% anomalies in each of the sets.
We choose the OC-SVM with standard RBF kernel
k(x, ˜x) = exp(−γ∥x −˜x∥2) as a method for this task
since the data is real-valued and low-dimensional, and
the OC-SVM scales sufﬁciently well for this comparatively
small data set. In addition, the ν-parameter formulation
[see (20)] enables us to use our prior knowledge and,
thus, approximately control the false alarm rate α and,
with it, implicitly also the miss rate, which leads to our
ﬁrst recommendation:
Assess the risks of false alarms and missed anomalies
Calibrating the false alarm rate and miss rate of a
detection model can make the difference between life and
death in a medical context, such as disease detection.
Though the consequences must not always be as dramatic
4Available
 
Example of anomaly explanations. The input is an
anomalous digit 1 from MNIST-C:stripe that has been corrupted by
inverting the pixels in the left and right vertical stripes. The
ground-truth explanation highlights the anomalous pixels in red.
The KDE, DOCC, and AE detect the stripe anomalies accurately, but
the LRP explanations show that the strategies are very different:
KDE highlights the anomaly but also some regions of the digit itself.
DOCC strongly emphasizes vertical edges. The AE produces a result
similar to KDE but with decision artifacts in the corners of the image
and on the digit itself.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
as in a medical setting, it is important to carefully consider
the risks and costs involved with type I and type II errors
in advance. In our example, a false alarm would suggest
a thyroid dysfunction although the patient is healthy.
On the other hand, a missed alarm would occur if the
model recognizes a patient with dysfunction as healthy.
Such asymmetric risks, with a greater expected loss for
anomalies that go undetected, are very common in medical
diagnosis – . Given only D = 6 measurements
per data record, we, therefore, seek to learn a detector
with a miss rate ideally close to zero, at the cost of an
increased false alarm rate. Patients falsely ascribed with
dysfunction by such a detector could then undergo further, more elaborate clinical testing to verify the disease.
Assuming that our data are representative and ∼12%5 of
the population is at risk of thyroid dysfunction, we choose
a slightly higher ν = 0.15 to further increase the robustness
against potential data contamination (here, the training set
contains ∼2.5% contamination in the form of unlabeled
anomalies). We then train the model and choose the kernel
scale γ according to the best AUC we observe on the small,
labeled validation set that includes nine labeled anomalies.
We select γ from γ ∈{(2iD)−1 | i
= −5, . . . , 5}, that is,
from a log2 span that accounts for the dimensionality D.
Following the above, we observe a rather poor best
validation set AUC of 83.9% at γ = (2−5D)−1, which is
the largest value from the hyperparameter range. This is
an indication that we forgot an important preprocessing
step, namely
Apply feature scaling to normalize value ranges
Any method that relies on computing distances, including kernel methods, requires the features to be scaled to
similar ranges to prevent features with wider value ranges
from dominating the distances. If this is not done, it can
cause anomalies that deviate on smaller scale features to
be undetected. Similar reasoning also holds for clustering
and classiﬁcation (e.g., see discussions in or ).
Min–max normalization or standardization is a common
choice, but, since we assume there might be some contamination, we apply a robust feature scaling via the median
and interquartile range. Remember that scaling parameters should be computed using only information from the
training data and then applied to all of the data. After we
have scaled the features, we observe a much improved
best validation set AUC of 98.6% at γ = (22D)−1. The
so-trained and selected model ﬁnally achieves a test set
AUC of 99.2%, a false alarm rate of 14.8% (i.e., close to
our a priori speciﬁed ν = 0.15), and a miss rate of zero.
B. Example 2: MVTec Industrial Inspection
In our second example, we consider the task of detecting
anomalies in wood images from the MVTec-AD data set.
Unlike the ﬁrst worked-through example, the MVTec data
are high-dimensional and correspond to arrays of pixel
5 
Table 6 AUC Detection Performance on the MVTec-AD “Wood” Class
values. Hence, all input features are already on a similar
scale (between −1 and +1), and thus, we do not need to
apply feature rescaling.
Following the standard model training/validation procedure, we train a set of models on the training data,
select their hyperparameters on hold out data (e.g., a few
inliers and anomalies extracted from the test set), and then
evaluate their performance on the remainder of the test
set. Table 6 shows the AUC performance of the nine models
in our benchmark.
We observe that the best-performing model is KDE.
This is particularly surprising because this model does not
compute the kinds of higher level image features that deep
models, such as DOCC, learn, and apply. An examination of
the data set shows that the anomalies involve properties,
such as small perforations and stains that do not require
high-level semantic information to be detected. But is that
the only reason why the performance of KDE is so high?
In order to get insights into the strategy used by KDE to
arrive at its prediction, we employ the neuralization/LRP
approach presented in Section VII-D.
Apply XAI to analyze model predictions
Fig. 11 shows an example of an image along with
its ground-truth pixel-level anomaly and the computed
pixelwise explanation for KDE.
Ideally, we would like the model to make its decision
based on the actual anomaly (here, the liquid stain), and
therefore, we would expect the ground-truth annotation
and the KDE explanation to coincide. However, it is clear
from an inspection of the explanation that KDE is not
looking at the true cause of the anomaly and is looking
instead at the vertical stripes present everywhere in the
input image. This discrepancy between the explanation
and the ground truth can be observed on other images of
the “wood” class. The high AUC score of KDE, thus, must
Input image, ground-truth source of the anomaly (here,
a stain of liquid), and the explanation of the KDE anomaly
prediction. The KDE model assigns high relevance to the wood grain
instead of the liquid stain. This discrepancy between the ground
truth and model explanation reveals a “Clever Hans” strategy used
by the KDE model.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
be due to a spurious correlation in the test set between
the reaction of the model to these stripes and the presence
of anomalies. We call this a “Clever Hans” effect ,
because, just like the horse Hans, who could correctly
answer arithmetic problems by reading unintended reactions of his master,6 the model appears to work because
of a spurious correlation. It is obvious that the KDE model
is unlikely to generalize well when the anomalies and the
stripes become decoupled (e.g., as we observe more data
or under some adversarial manipulation). This illustrates
the importance of generating explanations to identify these
kinds of failures. Once we have identiﬁed the problem,
how can we change our AD strategy so that it is more
robust and generalizes better?
Improve the model based on explanations
In practice, there are various approaches to improve the
model based on explanation feedback.
1) Data extension: We can extend the data with missing training cases, for instance, anomalous wood
examples that lack stripes or normal wood examples
that have stripes to break to a spurious correlation
between stripes and anomalies. When further data
collection is not possible, synthetic data extension
schemes, such as blurring or sharpening, can also be
considered.
2) Model extension: If the ﬁrst approach is not sufﬁcient,
or if the model is simply not capable of implementing
the necessary prediction structure, the model itself
can be changed (e.g., using a more ﬂexible deep
model). In other cases, the model may have enough
representation power but is statistically inefﬁcient
(e.g., subject to the curse of dimensionality). In that
case, adding structure (e.g., convolutions) or regularization can also help to learn a model with an
appropriate prediction strategy.
3) Ensembles: If all considered models have their own
strengths and weaknesses, ensemble approaches can
be considered. Ensembles have a conceptual justiﬁcation in the context of AD , and they have been
shown to work well empirically , .
Once the model has been improved using these strategies, explanations can be recomputed and examined to
verify that the decision strategy has been corrected. If that
is not the case, the process can be iterated until we reach
a satisfactory model.
In our wood example, we have observed that KDE reacts
strongly to the vertical strains. Based on this observation,
we replace the Gaussian kernel with a Mahalanobis kernel that effectively applies a horizontal Gaussian blur to
the images before computing the distance. This has the
effect of reducing the strain patterns, but keeping the
anomalies intact. This increases the explanation accuracy
of the model from an average cosine similarity of 0.34
to 0.38 on the ground-truth explanations. Fig. 12 shows
6 
Explanations of the original (left) and improved (right)
KDE model. The Gaussian kernel strongly reacts to the vertical wood
stripes (left). After replacing it with a Mahalanobis kernel (right)
that is less sensitive to high horizontal frequencies, the model
focuses on the true source of anomaly considerably better.
the explanation of the improved model. Implementation
details can be found in Appendix B-C. The AUC drops to
87%, which corresponds to a more realistic estimate of the
generalization abilities of the KDE model, which previously
was biased by the spurious correlation.
IX. C O N C L U S I O N A N D O U T L O O K
AD is a blossoming ﬁeld of broad theoretical and practical
interest across the disciplines. In this work, we have given
a review of the past and present state of AD research,
established a systematic unifying view, and discussed many
practical aspects. While we have included some of our
own contributions, we hope that we have fulﬁlled our aim
of providing a balanced and comprehensive snapshot of
this exciting research ﬁeld. The focus was given to a solid
theoretical basis, which then allowed us to put today’s
two main lines of development into perspective: the more
classical kernel world and the more recent world of deep
learning and representation learning for AD.
We will conclude our review by turning to what lies
ahead. In the following, we highlight some critical open
challenges—of which there are many—and identify a number of potential avenues for future research that we hope
will provide useful guidance.
A. Unexplored Combinations of Modeling
Dimensions
As can be seen in Fig. 1 and Table 2, there is a zoo
of different AD algorithms that have historically been
explored along various dimensions. This review has shown
conceptual similarities between AD members from kernel methods and deep learning. Note, however, that the
exploration of novel algorithms has been substantially
different in both domains, which offers unique possibilities
to explore new methodology: steps that have been pursued
in kernel learning but not in deep AD could be transferred
(or vice versa) and powerful new directions could emerge.
In other words, ideas could be readily transferred from
kernels to deep learning and back, and novel combinations
in our unifying view would emerge.
Let us now discuss some speciﬁc opportunities to clarify
this point. Consider the problem of robustness to noise and
contamination or signal-to-noise ratio. For shallow methods, the problem is well studied, and we have many effective methods , , , , , , .
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
In deep AD, very little work has addressed this problem.
The second example is the application of Bayesian methods. The Bayesian inference has been mostly considered for
shallow methods , , due to the prohibitive cost
or intractability of exact Bayesian inference in deep neural
networks. Recent progress in approximate Bayesian inference and Bayesian neural networks , – 
raise the possibility of developing methods that complement anomaly scores with uncertainty estimates or uncertainty estimates of their respective explanations .
In the area of SSAD, ideas have already been successfully transferred from kernel learning , to
deep methods for one-class classiﬁcation. However,
probabilistic and reconstruction methods that can make
use of labeled anomalies are less explored. For time-series
AD , , – , where forecasting (i.e.,
conditional density estimation) models are practical and
widely deployed, semisupervised extensions of such methods could lead to signiﬁcant improvements in applications in which some labeled examples are available (e.g.,
learning from failure cases in monitoring tasks). Concepts
from density ratio estimation , noise contrast estimation , or coding theory could lead to novel
semisupervised methods in principled ways. Finally, active
learning strategies for AD – , which identify
informative instances for labeling, have primarily only
been explored for shallow detectors and could be extended
to deep learning approaches.
This is a partial list of opportunities that we have
noticed. Further analysis of our framework will likely
expose additional directions for innovation.
B. Bridging Related Lines of Research on
Robustness
Other recent lines of research on robust deep learning
are closely related to AD or may even be interpreted
as special instances of the problem. These include OOD
detection, model calibration, uncertainty estimation, and
adversarial examples or attacks. Bridging these lines of
research by working out the nuances of the speciﬁc problem formulations can be insightful for connecting concepts
and transferring ideas to jointly advance research.
A basic approach to creating robust classiﬁers is to
endow them with the ability to reject input objects that are
likely to be misclassiﬁed. This is known as the problem of
classiﬁcation with a reject option, and it has been studied
extensively – . However, this work focuses on
objects that fall near the decision boundary where the
classiﬁer is uncertain.
One approach to making the rejection decision is to
calibrate the classiﬁcation probabilities and then reject
objects for which no class is predicted to have high probability following Chow’s optimal rejection rule . Consequently, many researchers have developed techniques
calibrating
probabilities
classiﬁers
 – or Bayesian uncertainty quantiﬁcation ,
 , , , , .
Recent work has begun to address other reasons for
rejecting an input object. OOD detection considers cases
where the object is drawn from a distribution different
from the training distribution P+ , , ,
 – . From a formal standpoint, it is impossible to
determine whether an input x is drawn from one of two
distributions P1 and P2 if both distributions have support at
x. Consequently, the OOD problem reduces to determining
whether x lies outside regions of high density in P+, which
is exactly the AD problem that we have described in this
The second reason to reject an input object is that it
belongs to a class that was not part of the training data.
This is the problem of open set recognition. Such objects
can also be regarded as being generated by a distribution
P−, so this problem also ﬁts within our framework and
can be addressed with the algorithms described here.
Nonetheless, researchers have developed a separate set
of methods for open set recognition , – ,
and an important goal for future research is to evaluate
these methods from the AD perspective and to evaluate
AD algorithms from the open set perspective.
In rejection, OOD, and open set recognition problems,
there is an additional source of information that is not
available in standard AD problems: the class labels of the
objects. Hence, the learning task combines classiﬁcation
with AD. Formally, the goal is to train a classiﬁer on labeled
data (x1, y1), . . . , (xn, yn) with class labels y ∈{1, . . . , k}
while also developing some measure to decide whether
an unlabeled test point ˜x should be rejected (for any of
the reasons listed above). The class label information tells
us about the structure of P+ and allows us to model it
as a joint distribution P+ ≡PX,Y . Methods for rejection,
OOD, and open set recognition all take advantage of this
additional structure. Note that the labels y are different
from the labels that mark normal or anomalous points in
supervised or SSAD (see Section II-C).
Research on the unresolved and fundamental issue of
adversarial examples and attacks – is related
to AD as well. We may interpret adversarial attacks as
extremely hard-to-detect OOD samples , as they are
speciﬁcally crafted to target the decision boundary and
conﬁdence of a learned classiﬁer. Standard adversarial
attacks ﬁnd a small perturbation δ for an input x so that
˜x = x + δ yields some class prediction desired by the
attacker. For instance, a perturbed image of a dog may be
indistinguishable from the original to the human’s eye, yet
the predicted label changes from “dog” to “cat.” Note that
such an adversarial example ˜x still likely is (and probably
should) be normal under the data marginal PX (an imperceptibly perturbed image of a dog shows a dog after all!)
but the pair (˜x, “cat”) should be anomalous under the joint
PX,Y . Methods for OOD detection have been found
to also increase adversarial robustness , , ,
 , , some of which model the class conditional
distributions for detection , , for the reason just
described.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
The above highlights the connection of these lines of
research toward the general goal of robust deep models.
Thus, we believe that connecting ideas and concepts in
these lines (e.g., the use of spherical models in both AD
 , and OOD , ) may help them to
advance together. Finally, the assessment of the robustness
of neural networks and their fail-safe design and integration are topics of high practical relevance that have
recently found their way in international standardization
initiatives (see Section II-A). Beyond doubt, understanding
the brittleness of deep networks (also in the context of
their explanations ) will be critical for their adoption
in AD applications that involve malicious attackers, such as
fraudsters or network intruders.
C. Interpretability and Trustworthiness
Much of AD research has been devoted to developing
new methods that improve detection accuracy. In most
applications, however, accuracy alone is not sufﬁcient
 , , and further criteria, such as interpretability
 , and trustworthiness , , , are
equally critical, as demonstrated in Sections VII and VIII.
For researchers and practitioners alike , it is vital
to understand the underlying reasons for how a speciﬁc
AD model reaches a particular prediction. Interpretable,
explanatory feedback enhances model transparency, which
is indispensable for accountable decision-making ,
uncovering model failures, such as Clever Hans behavior
 , , and understanding model vulnerabilities
that can be insightful for improving a model or system. This is especially relevant in safety-critical environments , . Existing work on interpretable AD
has considered ﬁnding subspaces of anomaly discriminative features , – , deducing sequential feature explanations , using featurewise reconstruction
errors , , employing fully convolutional architectures , and explaining anomalies via integrated
gradients or LRP , . In relation to the
vast body of literature though, research on interpretability
and trustworthiness in AD has seen comparatively little
attention. The fact that anomalies may not share similar
patterns (i.e., their heterogeneity) poses a challenge for
their explanation, which also distinguishes this setting
from interpreting supervised classiﬁcation models. Furthermore, anomalies might arise due to the presence of
abnormal patterns but conversely also due to a lack of
normal patterns. While, for the former case, an explanation
that highlights the abnormal features is satisfactory, how
should an explanation for missing features be conceptualized? For example, given the MNIST data set of digits, what
should an explanation of an anomalous all-black image
be? The matters of interpretability and trustworthiness get
more pressing as the task and data become more complex. Effective solutions for complex tasks will necessarily
require more powerful methods, for which explanations
become generally harder to interpret. We, thus, believe
that future research in this direction will be imperative.
D. Need for Challenging and Open Data Sets
Challenging problems with clearly deﬁned evaluation
criteria on publicly available benchmark data sets are
invaluable for measuring progress and moving a ﬁeld
forward. The signiﬁcance of the ImageNet database ,
together with corresponding competitions and challenges
 , for progressing computer vision and supervised
deep learning in the last decade give a prime example of this. Currently, the standard evaluation practices
in deep AD , , , , , ,
 – , , , OOD detection , ,
 , – , , , and open set recognition , – still extensively repurpose classiﬁcation data sets by deeming some data set classes
to be anomalous or considering in-distribution versus
OOD data set combinations (e.g., training a model on
Fashion-MNIST clothing items and regarding MNIST digits to be anomalous). Although these synthetic protocols
have some value, it has been questioned how well they
reﬂect real progress on challenging AD tasks , .
Moreover, we think the tendency that only a few methods
seem to dominate most of the benchmark data sets in
the work cited above is alarming since it suggests a bias
toward evaluating only the upsides of newly proposed
methods, yet often critically leaving out an analysis of
their downsides and limitations. This situation suggests a
lack of diversity in the current evaluation practices and
the benchmarks being used. In the spirit of all models are
wrong , we stress that more research effort should
go into studying when and how certain models are wrong
and behave like Clever Hanses. We need to understand
the tradeoffs that different methods make. For example,
some methods are likely making a tradeoff between detecting low-level versus high-level semantic anomalies (see
Section II-B2 and ). The availability of more diverse
and challenging data sets would be of great beneﬁt in
this regard. Recent data sets, such as MVTec-AD ,
and competitions, such as the Medical Out-of-Distribution
Analysis Challenge , provide excellent examples, but
the ﬁeld needs many more challenging open data sets to
foster progress.
E. Weak Supervision and Self-Supervised Learning
The bulk of AD research has been studying the problem in the absence of any kind of supervision, that is,
in an unsupervised setting (see Section II-C2). Recent work
suggests, however, that signiﬁcant performance improvements on complex detection tasks seem achievable through
various forms of weak supervision and self-supervised
supervision
supervised
describes learning from imperfectly or scarcely labeled
data – . Labels might be inaccurate (e.g., due
to labeling errors or uncertainty) or incomplete (e.g.,
covering only a few normal modes or speciﬁc anomalies).
Current work on SSAD indicates that including even
only few labeled anomalies can already yield remarkable
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
performance improvements on complex data , ,
 , , , . A key challenge here is to formulate and optimize such methods so that they generalize
well to novel anomalies. Combining these semisupervised
methods with active learning techniques helps us to
identify informative candidates for labeling – .
It is an effective strategy for designing AD systems that
continuously improve via expert feedback loops ,
 . This approach has not yet been explored for
deep detectors, though. OE , that is, using massive
amounts of data that is publicly available in some domains
(e.g., stock photos for computer vision or the English
Wikipedia for NLP) as auxiliary negative samples (see
Section IV-E), can also be viewed as a form of weak
supervision (imperfectly labeled anomalies). Although
such negative samples may not coincide with ground-truth
anomalies, we believe such contrast can be beneﬁcial for
learning characteristic representations of normal concepts
in many domains (e.g., using auxiliary log data to well
characterize the normal logs of a speciﬁc computer system
 ). So far, this has been little explored in applications.
Transfer learning approaches to AD also follow the idea
of distilling more domain knowledge into a model, for
example, through using and possibly ﬁne-tuning pretrained (supervised) models , , , ,
 . Overall, weak forms of supervision or domain priors
may be essential for achieving effective solutions in semantic AD tasks that involve high-dimensional data, as has
also been found in other unsupervised learning tasks, such
as disentanglement , , . Hence, we think
that developing effective methods for weakly supervised
AD will contribute to advancing the state of the art.
Self-supervised learning describes the learning of representations through solving auxiliary tasks, for example, next sentence and masked words prediction ,
future frame prediction in videos , or the prediction of transformations applied to images , such
as colorization , cropping , , or rotation
 . These auxiliary prediction tasks do not require
(ground-truth) labels for learning and can, thus, be applied
to unlabeled data, which makes self-supervised learning particularly appealing for AD. Self-supervised methods that have been introduced for visual AD train
multiclass classiﬁcation models based on pseudolabels
that correspond to various geometric transformations
(e.g., ﬂips, translations, and rotations) – .
An anomaly score can then be derived from the softmax
activation statistics of a so-trained classiﬁer, assuming
that a high prediction uncertainty (close to a uniform
distribution) indicates anomalies. These methods have
shown signiﬁcant performance improvements on the
common k-classes-out image benchmarks (see Table 3).
Bergman and Hoshen have recently proposed a
generalization of this idea to nonimage data, called
GOAD, which is based on random afﬁne transformations.
We can identify GOAD and self-supervised methods based
on geometric transformations (GT) as classiﬁcation-based
approaches within our unifying view (see Table 2). Other
recent and promising self-supervised approaches are based
on contrastive learning , , . In a broader
context, the interesting question will be to what extent
self-supervision can facilitate the learning of semantic representations. There is some evidence that self-supervised
learning helps improve the detection of semantic anomalies and, thus, exhibits inductive biases toward semantic
representations . On the other hand, there also exists
evidence showing that self-supervision mainly improves
learning of effective feature representations for low-level
statistics . Hence, this research question remains to
be answered but bears great potential for many domains
where large amounts of unlabeled data are available.
F. Foundation and Theory
The recent progress in AD research has also raised more
fundamental questions. These include open questions
about the OOD generalization properties of various methods presented in this review, the deﬁnition of anomalies in
high-dimensional spaces, and information-theoretic interpretations of the problem.
Nalisnick et al. have recently observed that
DGMs (see Section III), such as normalizing ﬂows, VAEs,
or autoregressive models, can often assign a higher
likelihood to anomalies than to in-distribution samples.
For example, models trained on Fashion-MNIST clothing
items can systematically assign a higher likelihood to
MNIST digits . This counterintuitive ﬁnding, which
has been replicated in subsequent work , ,
 , , , , revealed that there is a
critical lack of theoretical understanding of these models.
Solidifying evidence , , , indicates
that one reason seems to be that the likelihood in current
DGMs is still largely biased toward low-level background
statistics. Consequently, simpler data points attain higher
likelihood (e.g., MNIST digits under models trained
on Fashion-MNIST, but not vice versa). Another critical
remark in this context is that, for (truly) high-dimensional
data, the region with the highest density must not
necessarily coincide with the region of highest probability
mass (called the typical set), that is, the region where
data points most likely occur . For instance, while
the highest density of a D-dimensional standard Gaussian
distribution is given at the origin, points sampled from the
distribution concentrate around an annulus with radius
D for large D . Therefore, points close to the
origin have high density but are unlikely to occur. This
mismatch questions the standard theoretical density (level
set) problem formulation (see Section II-B) and use of
likelihood-based anomaly detectors for some settings.
Hence, theoretical research aimed at understanding the
above phenomenon and DGMs, themselves, presents an
exciting research opportunity.
Similar observations suggest that reconstruction-based
models can systematically well reconstruct simpler OOD
points that sit within the convex hull of the data.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Table 7 Notation Conventions
For example, an anomalous all-black image can be well
reconstructed by an AE trained on MNIST digits .
An even simpler example is the perfect reconstruction of
points that lie within the linear subspace spanned by the
principal components of a PCA model, even in regions
far away from the normal training data (e.g., along with
the principal component in Fig. 8). While such OOD
generalization properties might be desirable for general
representation learning , such behavior critically can
be undesirable for AD. Therefore, we stress that more theoretical research on understanding such OOD generalization
properties or biases, especially for more complex models,
will be necessary.
Finally, the push toward deep learning also presents new
opportunities to interpret and analyze the AD problem
from different theoretical angles. AEs, for example, can
be understood from an information-theoretic perspective
 as adhering to the Infomax principle – 
by implicitly maximizing the mutual information between
the input and latent code—subject to structural constraints or regularization of the code (e.g., “bottleneck,”
latent prior, and sparsity)—via the reconstruction objective . Similarly, information-theoretic perspectives of
VAEs have been formulated showing that these models
can be viewed as making a rate-distortion tradeoff 
when balancing the latent compression (negative rate)
and reconstruction accuracy (distortion) , .
This view has recently been used to draw a connection
between VAEs and Deep SVDD, where the latter can be
seen as a special case that only seeks to minimize the
rate (maximize compression) . Overall, AD has been
studied comparatively less from an information-theoretic
Table 8 AP Detection Performance on MNIST-C
Table 9 AP Detection Performance on MVTec-AD
perspective , , yet we think this could be fertile
ground for building a better theoretical understanding of
representation learning for AD.
In conclusion, we ﬁrmly believe that AD in all its exciting
variants will also in the future remain an indispensable
practical tool in the quest to obtain robust learning models
that perform well on complex data.
A P P E N D I X A
N O TAT I O N A N D A B B R E V I AT I O N S
For reference, we provide the notation and abbreviations used in this work in Table 7 and Nomenclature,
respectively.
A P P E N D I X B
A D D I T I O N A L D E TA I L S O N
E X P E R I M E N TA L E V A L U AT I O N
A. Average Precision on MNIST-C and MVTec-AD
We provide the detection performance measured in AP
of the experimental evaluation on MNIST-C and MVTec-AD
from Section VII-C in Tables 8 and 9, respectively. As can
be seen (and as to be expected ), the performance in
AP here shows the same trends as AUC (see Tables 4 and 5)
since the MNIST-C and MVTec-AD test sets are not highly
imbalanced.
B. Training Details
For PCA, we compute the reconstruction error while
maintaining 90% of the variance of the training data.
We do the same for kPCA and additionally choose the
kernel width such that 50% neighbors capture 50% of
total similarity scores. For MVE, we use the fast MCD
estimator with a default support fraction of 0.9 and
a contamination rate parameter of 0.01. To facilitate MVE
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
computation on MVTec-AD, we ﬁrst reduce the dimensionality via PCA retaining 90% of variance. For KDE,
we choose the bandwidth parameter to maximize the
likelihood of a small hold-out set from the training data.
For SVDD, we consider ν ∈{0.01, 0.05, 0.1, 0.2} and select
the kernel scale using a small labeled hold-out set. The
deep one-class classiﬁer applies a whitening transform on
the representations after the ﬁrst fully connected layer of
a pretrained VGG16 model (on MVTec-AD) or a CNN classiﬁer trained on the EMNIST letter subset (on MNIST-C).
For the AE on MNIST-C, we use a LeNet-type encoder that
has two convolutional layers with max-pooling followed
by two fully connected layers that map to an encoding
of 64 dimensions and construct the decoder symmetrically. On MVTec-AD, we use an encoder–decoder architecture, as presented in , which maps to a bottleneck
of 512 dimensions. Both the encoder and decoder here
consist of four blocks having two
convolutional
layers followed by max-pooling or upsampling, respectively. We train the AE such that the reconstruction error
of a small training hold-out set is minimized. For AGAN,
we use the AE encoder and decoder architecture for the
discriminator and generator networks, respectively, where
we train the GAN until convergence to a stable equilibrium.
C. Explaining KDE
neutralized
Section VII-D, replacing the squared Euclidean distance in
the ﬁrst layer with a squared Mahalanobis distance. The
heatmaps of both models (KDE and Mahalanobis KDE) are
computed as
(xj −x) ⊙∇xjs(x)
where ⊙denotes elementwise multiplication. This implements a Taylor-type decomposition, as described in .
D. Open-Source Software, Tutorials, and Demos
For the implementation of the shallow MVE and SVDD
models, we have used the scikit-learn library 
available at For the implementation of the shallow Gauss, PCA, KDE, and kPCA models as well as the deep AGAN, DOCC, RealNVP, and
AE models, we have used the PyTorch library 
available at Implementations of the
Deep SVDD and Deep SAD methods are available at
 Tutorials, demos, and code
for XAI techniques, in particular, LRP, can be found at
 In the spirit of the need for
open-source software in machine learning , a similar
collection of tutorials, demos, and code on AD methods is in the making and will be made available at
 
A c k n o w l e d g m e n t
The authors kindly thank the reviewers and the editor for
their constructive feedback that helped to improve this
R E F E R E N C E S
K. Pearson, “On lines and planes of closest ﬁt to
systems of points in space,” Phil. Mag. J. Sci.,
vol. 2, no. 11, pp. 559–572, 1901.
H. Hotelling, “Analysis of a complex of statistical
variables into principal components,” J. Educ.
Psychol., vol. 24, no. 6, pp. 417–441, 1933.
B. Schölkopf, A. Smola, and K.-R. Müller,
“Nonlinear component analysis as a kernel
eigenvalue problem,” Neural Comput., vol. 10,
no. 5, pp. 1299–1319, Jul. 1998.
H. Hoffmann, “Kernel PCA for novelty detection,”
Pattern Recognit., vol. 40, no. 3, pp. 863–874,
Mar. 2007.
P. J. Huber and E. M. Ronchetti, Robust Statistics,
2nd ed. Hoboken, NJ, USA: Wiley, 2009.
B. Schölkopf, J. C. Platt, J. Shawe-Taylor,
A. J. Smola, and R. C. Williamson, “Estimating the
support of a high-dimensional distribution,”
Neural Comput., vol. 13, no. 7, pp. 1443–1471,
Jul. 2001.
D. M. J. Tax and R. P. W. Duin, “Support vector
data description,” Mach. Learn., vol. 54, no. 1,
pp. 45–66, 2004.
E. M. Knorr, R. T. Ng, and V. Tucakov,
“Distance-based outliers: Algorithms and
applications,” VLDB J., vol. 8, no. 3, pp. 237–253,
S. Ramaswamy, R. Rastogi, and K. Shim, “Efﬁcient
algorithms for mining outliers from large data
sets,” in Proc. ACM SIGMOD Int. Conf. Manage.
Data, 2000, pp. 427–438.
M. M. Breunig, H.-P. Kriegel, R. T. Ng, and
J. Sander, “LOF: Identifying density-based local
outliers,” in Proc. ACM SIGMOD Int. Conf. Manage.
Data, 2000, pp. 93–104.
M. Rosenblatt, “Remarks on some nonparametric
estimates of a density function,” Ann. Math.
Statist., vol. 27, no. 3, pp. 832–837, Sep. 1956.
E. Parzen, “On estimation of a probability density
function and mode,” Ann. Math. Statist., vol. 33,
no. 3, pp. 1065–1076, Sep. 1962.
F. Y. Edgeworth, “On discordant observations,”
Phil. Mag. J. Sci., vol. 23, no. 5, pp. 364–375,
T. S. Kuhn, The Structure of Scientiﬁc Revolutions.
Chicago, IL, USA: Univ. of Chicago Press, 1970.
A. Patcha and J.-M. Park, “An overview of anomaly
detection techniques: Existing solutions and latest
technological trends,” Comput. Netw., vol. 51,
no. 12, pp. 3448–3470, Aug. 2007.
H.-J. Liao, C.-H. Richard Lin, Y.-C. Lin, and
K.-Y. Tung, “Intrusion detection system: A
comprehensive review,” J. Netw. Comput. Appl.,
vol. 36, no. 1, pp. 16–24, Jan. 2013.
M. Ahmed, A. N. Mahmood, and J. Hu, “A survey
of network anomaly detection techniques,”
J. Netw. Comput. Appl., vol. 60, pp. 19–31,
Jan. 2016.
D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and
K. J. Kim, “A survey of deep learning-based
network anomaly detection,” Cluster Comput.,
vol. 10, pp. 1–13, Jan. 2017.
Y. Xin et al., “Machine learning and deep learning
methods for cybersecurity,” IEEE Access, vol. 6,
pp. 35365–35381, 2018.
R. K. Malaiya, D. Kwon, J. Kim, S. C. Suh, H. Kim,
and I. Kim, “An empirical evaluation of deep
learning for network anomaly detection,” in Proc.
Int. Conf. Comput., Netw. Commun., Mar. 2018,
pp. 893–898.
R. J. Bolton and D. J. Hand, “Statistical fraud
detection: A review,” Stat. Sci., vol. 17, no. 3,
pp. 235–255, 2002.
S. Bhattacharyya, S. Jha, K. Tharakunnel, and
J. C. Westland, “Data mining for credit card fraud:
A comparative study,” Decis. Support Syst., vol. 50,
no. 3, pp. 602–613, Feb. 2011.
H. Joudaki et al., “Using data mining to detect
health care fraud and abuse: A review of
literature,” Global J. Health Sci., vol. 7, no. 1,
pp. 194–202, Aug. 2014.
M. Ahmed, A. N. Mahmood, and M. R. Islam,
“A survey of anomaly detection techniques in
ﬁnancial domain,” Future Gener. Comput. Syst.,
vol. 55, pp. 278–288, Feb. 2016.
A. Abdallah, M. A. Maarof, and A. Zainal, “Fraud
detection system: A survey,” J. Netw. Comput.
Appl., vol. 68, pp. 90–113, Jun. 2016.
G. van Capelleveen, M. Poel, R. M. Mueller,
D. Thornton, and J. van Hillegersberg, “Outlier
detection in healthcare fraud: A case study in the
medicaid dental domain,” Int. J. Accounting Inf.
Syst., vol. 21, pp. 18–31, Jun. 2016.
Y.-J. Zheng, X.-H. Zhou, W.-G. Sheng, Y. Xue, and
S.-Y. Chen, “Generative adversarial network based
telecom fraud detection at the receiving bank,”
Neural Netw., vol. 102, pp. 78–86, Jun. 2018.
J. Rabatel, S. Bringay, and P. Poncelet, “Anomaly
detection in monitoring sensor data for preventive
maintenance,” Expert Syst. Appl., vol. 38, no. 6,
pp. 7003–7015, Jun. 2011.
J. Marzat, H. Piet-Lahanier, F. Damongeot, and
E. Walter, “Model-based fault diagnosis for
aerospace systems: A survey,” Proc. Inst. Mech.
Eng., G, J. Aerosp. Eng., vol. 226, no. 10,
pp. 1329–1360, Oct. 2012.
L. Martí, N. Sanchez-Pi, J. Molina, and A. Garcia,
“Anomaly detection based on sensor data in
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
petroleum industry applications,” Sensors, vol. 15,
no. 2, pp. 2774–2797, Jan. 2015.
W. Yan and L. Yu, “On accurate and reliable
anomaly detection for gas turbine combustors:
A deep learning approach,” in Proc. Annu. Conf.
Prognostics Health Manage. Soc., vol. 6, 2015.
[Online]. Available: 
F. Lopez et al., “Categorization of anomalies in
smart manufacturing systems to support the
selection of detection mechanisms,” IEEE Robot.
Autom. Lett., vol. 2, no. 4, pp. 1885–1892,
Oct. 2017.
K. Hundman, V. Constantinou, C. Laporte,
I. Colwell, and T. Soderstrom, “Detecting
spacecraft anomalies using LSTMs and
nonparametric dynamic thresholding,” in Proc.
Int. Conf. Knowl. Discovery Data Mining, 2018,
pp. 387–395.
D. J. Atha and M. R. Jahanshahi, “Evaluation of
deep learning approaches based on convolutional
neural networks for corrosion detection,”
Structural Health Monitor., vol. 17, no. 5,
pp. 1110–1128, Sep. 2018.
D. Ramotsoela, A. Abu-Mahfouz, and G. Hancke,
“A survey of anomaly detection in industrial
wireless sensor networks with critical water
system infrastructure as a case study,” Sensors,
vol. 18, no. 8, p. 2491, Aug. 2018.
R. Zhao, R. Yan, Z. Chen, K. Mao, P. Wang, and
R. X. Gao, “Deep learning and its applications to
machine health monitoring,” Mech. Syst. Signal
Process., vol. 115, pp. 213–237, Jan. 2019.
A. Borghesi, A. Bartolini, M. Lombardi, M. Milano,
and L. Benini, “Anomaly detection using
autoencoders in high performance computing
systems,” in Proc. AAAI Conf. Artif. Intell., vol. 33,
2019, pp. 9428–9433.
J. Sipple, “Interpretable, multidimensional,
multimodal anomaly detection with negative
sampling for detection of device failure,” in Proc.
Int. Conf. Mach. Learn., 2020, pp. 4368–4377.
K. Golmohammadi and O. R. Zaiane, “Time series
contextual anomaly detection for detecting market
manipulation in stock market,” in Proc. IEEE Int.
Conf. Data Sci. Adv. Anal., Oct. 2015, pp. 1–10.
K. Golmohammadi and O. R. Zaiane, “Sentiment
analysis on twitter to improve time series
contextual anomaly detection for detecting stock
market manipulation,” in Proc. Int. Conf. Big Data
Anal. Knowl. Discovery, 2017, pp. 327–342.
A. Rabaoui, M. Davy, S. Rossignol, and N. Ellouze,
“Using one-class SVMs and wavelets for audio
surveillance,” IEEE Trans. Inf. Forensics Security,
vol. 3, no. 4, pp. 763–775, Dec. 2008.
E. Marchi, F. Vesperini, F. Eyben, S. Squartini, and
B. Schuller, “A novel approach for automatic
acoustic novelty detection using a denoising
autoencoder with bidirectional LSTM neural
networks,” in Proc. IEEE Int. Conf. Acoust., Speech
Signal Process., Apr. 2015, pp. 1996–2000.
H. Lim, J. Park, and Y. Han, “Rare sound event
detection using 1d convolutional recurrent neural
networks,” in Proc. Workshop Detection
Classiﬁcation Acoustic Scenes Events, 2017,
pp. 80–84.
E. Principi, F. Vesperini, S. Squartini, and F. Piazza,
“Acoustic novelty detection with adversarial
autoencoders,” in Proc. Int. Joint Conf. Neural
Netw., 2017, pp. 3324–3330.
Y. Koizumi, S. Saito, H. Uematsu, Y. Kawachi, and
N. Harada, “Unsupervised detection of anomalous
sound based on deep learning and the
Neyman–Pearson lemma,” IEEE/ACM Trans.
Audio, Speech, Language Process., vol. 27, no. 1,
pp. 212–224, Jan. 2019.
L. Tarassenko, P. Hayton, N. Cerneaz, and
M. Brady, “Novelty detection for the identiﬁcation
of masses in mammograms,” in Proc. 4th Int. Conf.
Artif. Neural Netw., 1995, pp. 442–447.
S. Chauhan and L. Vig, “Anomaly detection in ECG
time signals via deep long short-term memory
networks,” in Proc. IEEE Int. Conf. Data Sci. Adv.
Anal., Oct. 2015, pp. 1–7.
C. Leibig, V. Allken, M. S. Ayhan, P. Berens, and
S. Wahl, “Leveraging uncertainty information
from deep neural networks for disease detection,”
Sci. Rep., vol. 7, no. 1, pp. 1–14, Dec. 2017.
G. Litjens et al., “A survey on deep learning in
medical image analysis,” Med. Image Anal.,
vol. 42, pp. 60–88, Dec. 2017.
T. Schlegl, P. Seeböck, S. M. Waldstein,
U. Schmidt-Erfurth, and G. Langs, “Unsupervised
anomaly detection with generative adversarial
networks to guide marker discovery,” in Proc. Int.
Conf. Inf. Process. Med. Imag., 2017, pp. 146–157.
X. Chen and E. Konukoglu, “Unsupervised
detection of lesions in brain MRI using
constrained adversarial auto-encoders,” in Proc.
Med. Imag. Deep Learn., 2018. [Online]. Available:
 
D. K. Iakovidis, S. V. Georgakopoulos,
M. Vasilakakis, A. Koulaouzidis, and
V. P. Plagianakos, “Detecting and locating
gastrointestinal anomalies using deep learning
and iterative cluster uniﬁcation,” IEEE Trans. Med.
Imag., vol. 37, no. 10, pp. 2196–2210, Oct. 2018.
S. Latif, M. Usman, R. Rana, and J. Qadir,
“Phonocardiographic sensing using deep learning
for abnormal heartbeat detection,” IEEE Sensors
J., vol. 18, no. 22, pp. 9393–9400, Nov. 2018.
N. Pawlowski et al., “Unsupervised lesion
detection in brain CT using Bayesian
convolutional autoencoders,” in Proc. Med. Imag.
Deep Learn., 2018. [Online]. Available:
 
C. Baur, B. Wiestler, S. Albarqouni, and N. Navab,
“Fusing unsupervised and supervised deep
learning for white matter lesion segmentation,” in
Proc. Med. Imag. Deep Learn., 2019, pp. 63–72.
T. Schlegl, P. Seeböck, S. M. Waldstein, G. Langs,
and U. Schmidt-Erfurth, “F-AnoGAN: Fast
unsupervised anomaly detection with generative
adversarial networks,” Med. Image Anal., vol. 54,
pp. 30–44, May 2019.
P. Seeböck et al., “Exploiting epistemic uncertainty
of anatomy segmentation for anomaly detection
in retinal OCT,” IEEE Trans. Med. Imag., vol. 39,
no. 1, pp. 87–98, Jan. 2019.
P. Guo et al., “Ensemble deep learning for cervix
image selection toward improving reliability in
automated cervical precancer screening,”
Diagnostics, vol. 10, no. 7, p. 451, 2020.
L. Naud and A. Lavin, “Manifolds for unsupervised
visual anomaly detection,” 2020,
 
 
N. Tuluptceva, B. Bakker, I. Fedulova, H. Schulz,
and D. V. Dylov, “Anomaly detection with deep
perceptual autoencoders,” 2020,
 
 
W.-K. Wong, A. W. Moore, G. F. Cooper, and
M. M. Wagner, “Bayesian network anomaly
pattern detection for disease outbreaks,” in Proc.
Int. Conf. Mach. Learn., 2003, pp. 808–815.
W.-K. Wong, A. Moore, G. Cooper, and M. Wagner,
“What’s strange about recent events (WSARE): An
algorithm for the early detection of disease
outbreaks,” J. Mach. Learn. Res., vol. 6,
pp. 1961–1998, Dec. 2005.
R. Blender, K. Fraedrich, and F. Lunkeit,
“Identiﬁcation of cyclone-track regimes in the
North Atlantic,” Quart. J. Roy. Meteorological Soc.,
vol. 123, no. 539, pp. 727–741, Apr. 1997.
J. Verbesselt, A. Zeileis, and M. Herold, “Near
real-time disturbance detection using satellite
image time series,” Remote Sens. Environ.,
vol. 123, pp. 98–108, Aug. 2012.
W. D. Fisher, T. K. Camp, and
V. V. Krzhizhanovskaya, “Anomaly detection in
Earth dam and levee passive seismic data using
support vector machines and automatic feature
selection,” J. Comput. Sci., vol. 20, pp. 143–153,
M. Flach et al., “Multivariate anomaly detection
for Earth observations: A comparison of
algorithms and feature extraction techniques,”
Earth Syst. Dyn., vol. 8, no. 3, pp. 677–696,
Aug. 2017.
Y. Wu, Y. Lin, Z. Zhou, D. C. Bolton, J. Liu, and
P. Johnson, “DeepDetect: A cascaded region-based
densely connected network for seismic event
detection,” IEEE Trans. Geosci. Remote Sens.,
vol. 57, no. 1, pp. 62–75, Jan. 2019.
T. Jiang, Y. Li, W. Xie, and Q. Du, “Discriminative
reconstruction constrained generative adversarial
network for hyperspectral anomaly detection,”
IEEE Trans. Geosci. Remote Sens., vol. 58, no. 7,
pp. 4666–4679, Jul. 2020.
T. I. Oprea, “Chemical space navigation in lead
discovery,” Current Opinion Chem. Biol., vol. 6,
no. 3, pp. 384–389, Jun. 2002.
P. S. Gromski, A. B. Henson, J. M. Granda, and
L. Cronin, “How to explore chemical space using
algorithms and automation,” Nature Rev. Chem.,
vol. 3, no. 2, pp. 119–128, Feb. 2019.
S. Min, B. Lee, and S. Yoon, “Deep learning in
bioinformatics,” Brieﬁngs Bioinf., vol. 18, no. 5,
pp. 851–869, 2017.
S. A. Tomlins, “Recurrent fusion of TMPRSS2 and
ETS transcription factor genes in prostate cancer,”
Science, vol. 310, no. 5748, pp. 644–648,
Oct. 2005.
R. Tibshirani and T. Hastie, “Outlier sums for
differential gene expression analysis,” Biostatistics,
vol. 8, no. 1, pp. 2–8, Jan. 2007.
O. Cerri, T. Q. Nguyen, M. Pierini, M. Spiropulu,
and J.-R. Vlimant, “Variational autoencoders for
new physics mining at the large hadron collider,”
J. High Energy Phys., vol. 2019, no. 5, p. 36,
Y. A. Kharkov, V. E. Sotskov, A. A. Karazeev,
E. O. Kiktenko, and A. K. Fedorov, “Revealing
quantum chaos with machine learning,” Phys. Rev.
B, Condens. Matter, vol. 101, no. 6, Feb. 2020,
Art. no. 064406.
P. Protopapas, J. M. Giammarco, L. Faccioli,
M. F. Struble, R. Dave, and C. Alcock, “Finding
outlier light curves in catalogues of periodic
variable stars,” Monthly Notices Roy. Astronomical
Soc., vol. 369, no. 2, pp. 677–696, Jun. 2006.
H. Dutta, C. Giannella, K. Borne, and H. Kargupta,
“Distributed top-K outlier detection from
astronomy catalogs using the DEMAC system,” in
Proc. SIAM Int. Conf. Data Mining, Apr. 2007,
pp. 473–478.
M. Henrion, D. J. Mortlock, D. J. Hand, and
A. Gandy, “Classiﬁcation and anomaly detection
for astronomical survey data,” in Astrostatistical
Challenges for the New Astronomy. New York, NY,
USA: Springer, 2013, pp. 149–184.
E. Reyes and P. A. Estévez, “Transformation based
deep anomaly detection in astronomical images,”
2020, arXiv:2005.07779. [Online]. Available:
 
Y. Bengio, A. Courville, and P. Vincent,
“Representation learning: A review and new
perspectives,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 35, no. 8, pp. 1798–1828, Aug. 2013.
Y. LeCun, Y. Bengio, and G. Hinton, “Deep
learning,” Nature, vol. 521, no. 7553,
pp. 436–444, 2015.
J. Schmidhuber, “Deep learning in neural
networks: An overview,” Neural Netw., vol. 61,
pp. 85–117, Jan. 2015.
I. Goodfellow, Y. Bengio, and A. Courville, Deep
Learning. Cambridge, MA, USA: MIT Press, 2016.
A. Krizhevsky, I. Sutskever, and G. E. Hinton,
“ImageNet classiﬁcation with deep convolutional
neural networks,” in Proc. Adv. Neural Inf. Process.
Syst., 2012, pp. 1097–1105.
K. Simonyan and A. Zisserman, “Very deep
convolutional networks for large-scale image
recognition,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2015.
C. Szegedy et al., “Going deeper with
convolutions,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., Jun. 2015, pp. 1–9.
J. Long, E. Shelhamer, and T. Darrell, “Fully
convolutional networks for semantic
segmentation,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR), Jun. 2015,
pp. 3431–3440.
S. Ren, K. He, R. Girshick, and J. Sun, “Faster
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
R-CNN: Towards real-time object detection with
region proposal networks,” in Proc. Adv. Neural
Inf. Process. Syst., 2015, pp. 91–99.
L. A. Gatys, A. S. Ecker, and M. Bethge, “Image
style transfer using convolutional neural
networks,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jun. 2016, pp. 2414–2423.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep
residual learning for image recognition,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),
Jun. 2016, pp. 770–778.
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi,
“You only look once: Uniﬁed, real-time object
detection,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit. (CVPR), Jun. 2016, pp. 779–788.
T. Karras, S. Laine, and T. Aila, “A style-based
generator architecture for generative adversarial
networks,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., Jun. 2019, pp. 4401–4410.
Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le,
“Self-training with noisy student improves
ImageNet classiﬁcation,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., Jun. 2020,
pp. 10687–10698.
H. Lee, P. Pham, Y. Largman, and A. Y. Ng,
“Unsupervised feature learning for audio
classiﬁcation using convolutional deep belief
networks,” in Proc. Adv. Neural Inf. Process. Syst.,
2009, pp. 1096–1104.
G. E. Dahl, D. Yu, L. Deng, and A. Acero,
“Context-dependent pre-trained deep neural
networks for large-vocabulary speech
recognition,” IEEE/ACM Trans. Audio, Speech,
Language Process., vol. 20, no. 1, pp. 30–42,
Jan. 2012.
A.-R. Mohamed, G. E. Dahl, and G. Hinton,
“Acoustic modeling using deep belief networks,”
IEEE/ACM Trans. Audio, Speech, Language Process.,
vol. 20, no. 1, pp. 14–22, Jan. 2012.
G. Hinton et al., “Deep neural networks for
acoustic modeling in speech recognition: The
shared views of four research groups,” IEEE Signal
Process. Mag., vol. 29, no. 6, pp. 82–97, Nov. 2012.
A. Graves, A.-R. Mohamed, and G. Hinton,
“Speech recognition with deep recurrent neural
networks,” in Proc. IEEE Int. Conf. Acoust., Speech
Signal Process., May 2013, pp. 6645–6649.
A. Hannun et al., “Deep speech: Scaling up
end-to-end speech recognition,” 2014,
 
 
D. Amodei et al., “Deep speech 2: End-to-end
speech recognition in English and mandarin,” in
Proc. Int. Conf. Mach. Learn., vol. 48, 2016,
pp. 173–182.
W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen,
attend and spell: A neural network for large
vocabulary conversational speech recognition,” in
Proc. IEEE Int. Conf. Acoust., Speech Signal Process.,
Mar. 2016, pp. 4960–4964.
J. Chorowski, R. J. Weiss, S. Bengio, and
A. van den Oord, “Unsupervised speech
representation learning using WaveNet
autoencoders,” IEEE/ACM Trans. Audio, Speech,
Language Process., vol. 27, no. 12, pp. 2041–2053,
Dec. 2019.
S. Schneider, A. Baevski, R. Collobert, and
M. Auli, “wav2vec: Unsupervised pre-training for
speech recognition,” in Proc. Interspeech,
Sep. 2019, pp. 3465–3469.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin,
“A neural probabilistic language model,” J. Mach.
Learn. Res., vol. 3, pp. 1137–1155, Feb. 2003.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,
and J. Dean, “Distributed representations of
words and phrases and their compositionality,” in
Proc. Adv. Neural Inf. Process. Syst., 2013,
pp. 3111–3119.
J. Pennington, R. Socher, and C. Manning, “Glove:
Global vectors for word representation,” in Proc.
Conf. Empirical Methods Natural Lang. Process.,
2014, pp. 1532–1543.
K. Cho et al., “Learning phrase representations
using RNN encoder–decoder for statistical
machine translation,” in Proc. Conf. Empirical
Methods Natural Lang. Process., 2014,
pp. 1724–1734.
P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov,
“Enriching word vectors with subword
information,” Trans. Assoc. Comput. Linguistics,
vol. 5, pp. 135–146, Dec. 2017.
A. Joulin, E. Grave, P. Bojanowski, and T. Mikolov,
“Bag of tricks for efﬁcient text classiﬁcation,” in
Proc. Conf. Eur. Chapter Assoc. Comput. Linguistics,
2017, pp. 427–431.
M. Peters et al., “Deep contextualized word
representations,” in Proc. North Amer. Chapter
Assoc. Comput. Linguistics, 2018, pp. 2227–2237.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“BERT: Pre-training of deep bidirectional
transformers for language understanding,” in
Proc. North Amer. Chapter Assoc. Comput.
Linguistics, 2019, pp. 4171–4186.
C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong,
R. Socher, and P. Fung, “Transferable
multi-domain state generator for task-oriented
dialogue systems,” in Proc. 57th Annu. Meeting
Assoc. Comput. Linguistics, 2019, pp. 808–819.
T. B. Brown et al., “Language models are few-shot
learners,” in Proc. Adv. Neural Inf. Process. Syst.
(NeurIPS), 2020.
T. Lengauer, O. Sander, S. Sierra, A. Thielen, and
R. Kaiser, “Bioinformatics prediction of HIV
coreceptor usage,” Nature Biotechnol., vol. 25,
no. 12, p. 1407, 2007.
P. Baldi, P. Sadowski, and D. Whiteson, “Searching
for exotic particles in high-energy physics with
deep learning,” Nature Commun., vol. 5, no. 1,
p. 4308, Sep. 2014.
K. T. Schütt, F. Arbabzadah, S. Chmiela,
K.-R. Müller, and A. Tkatchenko,
“Quantum-chemical insights from deep tensor
neural networks,” Nature Commun., vol. 8, no. 1,
pp. 1–8, Apr. 2017.
G. Carleo and M. Troyer, “Solving the quantum
many-body problem with artiﬁcial neural
networks,” Science, vol. 355, no. 6325,
pp. 602–606, Feb. 2017.
K. T. Schütt, M. Gastegger, A. Tkatchenko,
K.-R. Müller, and R. J. Maurer, “Unifying machine
learning and quantum chemistry with a deep
neural network for molecular wavefunctions,”
Nature Commun., vol. 10, no. 1, p. 5024,
Dec. 2019.
P. Jurmeister et al., “Machine learning analysis of
DNA methylation proﬁles distinguishes primary
lung squamous cell carcinomas from head and
neck metastases,” Sci. Transl. Med., vol. 11,
no. 509, Sep. 2019, Art. no. eaaw8513.
F. Klauschen et al., “Scoring of tumor-inﬁltrating
lymphocytes: From visual estimation to machine
learning,” Seminars Cancer Biol., vol. 52, no. 2,
p. 151, 2018.
F. Arcadu, F. Benmansour, A. Maunz, J. Willis,
Z. Haskova, and M. Prunotto, “Deep learning
algorithm predicts diabetic retinopathy
progression in individual patients,” NPJ Digit.
Med., vol. 2, no. 1, pp. 1–9, Dec. 2019.
D. Ardila et al., “End-to-end lung cancer screening
with three-dimensional deep learning on low-dose
chest computed tomography,” Nature Med.,
vol. 25, no. 6, pp. 954–961, Jun. 2019.
A. Esteva et al., “A guide to deep learning in
healthcare,” Nature Med., vol. 25, no. 1,
pp. 24–29, 2019.
K. Faust et al., “Visualizing histopathologic deep
learning classiﬁcation and anomaly detection
using nonlinear feature space dimensionality
reduction,” BMC Bioinf., vol. 19, no. 1, p. 173,
Dec. 2018.
R. Chalapathy, A. K. Menon, and S. Chawla,
“Robust, deep and inductive anomaly detection,”
in Proc. Joint Eur. Conf. Mach. Learn. Knowl.
Discovery Databases, 2017, pp. 36–51.
J. Chen, S. Sathe, C. C. Aggarwal, and
D. S. Turaga, “Outlier detection with autoencoder
ensembles,” in Proc. SIAM Int. Conf. Data Mining,
2017, pp. 90–98.
C. Zhou and R. C. Paffenroth, “Anomaly detection
with robust deep autoencoders,” in Proc. 23rd
ACM SIGKDD Int. Conf. Knowl. Discovery Data
Mining, Aug. 2017, pp. 665–674.
B. Zong et al., “Deep autoencoding Gaussian
mixture model for unsupervised anomaly
detection,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2018.
C. Aytekin, X. Ni, F. Cricri, and E. Aksu,
“Clustering and unsupervised anomaly detection
with l2 normalized deep auto-encoder
representations,” in Proc. Int. Joint Conf. Neural
Netw., 2018, pp. 1–6.
D. Abati, A. Porrello, S. Calderara, and
R. Cucchiara, “Latent space autoregression for
novelty detection,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR), Jun. 2019, pp. 481–490.
C. Huang, F. Ye, J. Cao, M. Li, Y. Zhang, and C. Lu,
“Attribute restoration framework for anomaly
detection,” 2019, arXiv:1911.10676. [Online].
Available: 
D. Gong et al., “Memorizing normality to detect
anomaly: Memory-augmented deep autoencoder
for unsupervised anomaly detection,” in Proc. Int.
Conf. Comput. Vis., Oct. 2019, pp. 1705–1714.
P. Oza and V. M. Patel, “C2AE: Class conditioned
auto-encoder for open-set recognition,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recognit.,
Jun. 2019, pp. 2307–2316.
D. T. Nguyen, Z. Lou, M. Klar, and T. Brox,
“Anomaly detection with multiple-hypotheses
predictions,” in Proc. Int. Conf. Mach. Learn.,
vol. 97, 2019, pp. 4800–4809.
K. H. Kim et al., “RaPP: Novelty detection with
reconstruction along projection pathway,” in Proc.
Int. Conf. Learn. Represent. (ICLR), 2020.
S. M. Erfani, S. Rajasegarar, S. Karunasekera, and
C. Leckie, “High-dimensional and large-scale
anomaly detection using a linear one-class SVM
with deep learning,” Pattern Recognit., vol. 58,
pp. 121–134, Oct. 2016.
L. Ruff et al., “Deep one-class classiﬁcation,” in
Proc. Int. Conf. Mach. Learn., vol. 80, 2018,
pp. 4390–4399.
M. Sabokrou, M. Khalooei, M. Fathy, and E. Adeli,
“Adversarially learned one-class classiﬁer for
novelty detection,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., Jun. 2018, pp. 3379–3388.
P. Oza and V. M. Patel, “One-class convolutional
neural network,” IEEE Signal Process. Lett., vol. 26,
no. 2, pp. 277–281, Feb. 2019.
L. Ruff, Y. Zemlyanskiy, R. Vandermeulen,
T. Schnake, and M. Kloft, “Self-attentive,
multi-context one-class classiﬁcation for
unsupervised anomaly detection on text,” in Proc.
57th Annu. Meeting Assoc. Comput. Linguistics,
2019, pp. 4061–4071.
P. Perera, R. Nallapati, and B. Xiang, “OCGAN:
One-class novelty detection using GANs with
constrained latent representations,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., Jun. 2019,
pp. 2898–2906.
P. Perera and V. M. Patel, “Learning deep features
for one-class classiﬁcation,” IEEE Trans. Image
Process., vol. 28, no. 11, pp. 5450–5463,
Nov. 2019.
J. Wang and A. Cherian, “GODS: Generalized
one-class discriminative subspaces for anomaly
detection,” in Proc. Int. Conf. Comput. Vis., 2019,
pp. 8201–8211.
L. Ruff et al., “Deep semi-supervised anomaly
detection,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2020.
Z. Ghafoori and C. Leckie, “Deep multi-sphere
support vector data description,” in Proc. SIAM
Int. Conf. Data Mining, 2020, pp. 109–117.
R. Chalapathy, E. Toth, and S. Chawla, “Group
anomaly detection using deep generative models,”
in Proc. Eur. Conf. Mach. Learn. Princ. Pract.
Knowl. Discovery Databases, 2018, pp. 173–189.
L. Deecke, R. A. Vandermeulen, L. Ruff, S. Mandt,
and M. Kloft, “Image anomaly detection with
generative adversarial networks,” in Proc. Eur.
Conf. Mach. Learn. Princ. Pract. Knowl. Discovery
Databases, 2018, pp. 3–17.
S. Akcay, A. Atapour-Abarghouei, and
T. P. Breckon, “Ganomaly: Semi-supervised
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
anomaly detection via adversarial training,” in
Computer Vision—ACCV, C. V. Jawahar, H. Li,
G. Mori, and K. Schindler, Eds. Cham,
Switzerland: Springer, 2019, pp. 622–637.
H. Choi, E. Jang, and A. A. Alemi, “WAIC, but
why? Generative ensembles for robust anomaly
detection,” 2018, arXiv:1810.01392. [Online].
Available: 
S. Pidhorskyi, R. Almohsen, and G. Doretto,
“Generative probabilistic novelty detection with
adversarial autoencoders,” in Proc. Adv. Neural Inf.
Process. Syst., 2018, pp. 6822–6833.
H. Zenati, M. Romain, C.-S. Foo, B. Lecouat, and
V. Chandrasekhar, “Adversarially learned anomaly
detection,” in Proc. IEEE Int. Conf. Data Mining,
Nov. 2018, pp. 727–736.
I. Golan and R. El-Yaniv, “Deep anomaly detection
using geometric transformations,” in Proc. Adv.
Neural Inf. Process. Syst., 2018, pp. 9758–9769.
D. Hendrycks, M. Mazeika, S. Kadavath, and
D. Song, “Using self-supervised learning can
improve model robustness and uncertainty,” in
Proc. Adv. Neural Inf. Process. Syst., 2019,
pp. 15637–15648.
S. Wang et al., “Effective end-to-end unsupervised
outlier detection via inlier priority of
discriminative network,” in Proc. Adv. Neural Inf.
Process. Syst., 2019, pp. 5960–5973.
L. Bergman and Y. Hoshen, “Classiﬁcation-based
anomaly detection for general data,” in Proc. Int.
Conf. Learn. Represent. (ICLR), 2020.
J. Tack, S. Mo, J. Jeong, and J. Shin, “CSI: Novelty
detection via contrastive learning on
distributionally shifted instances,” in Proc. Adv.
Neural Inf. Process. Syst. (NeurIPS), 2020.
M. Markou and S. Singh, “Novelty detection:
A review—Part 1: Statistical approaches,” Signal
Process., vol. 83, no. 12, pp. 2481–2497,
Dec. 2003.
M. Markou and S. Singh, “Novelty detection:
A review—Part 2: Neural network based
approaches,” Signal Process., vol. 83, no. 12,
pp. 2499–2521, Dec. 2003.
V. Hodge and J. Austin, “A survey of outlier
detection methodologies,” Artif. Intell. Rev.,
vol. 22, no. 2, pp. 85–126, Oct. 2004.
S. Walﬁsh, “A review of statistical outlier
methods,” Pharmaceutical Technol., vol. 30,
no. 11, pp. 1–5, 2006.
V. Chandola, A. Banerjee, and V. Kumar, “Anomaly
detection: A survey,” ACM Comput. Surv., vol. 41,
no. 3, pp. 1–58, 2009.
A. S. Hadi, R. Imon, and M. Werner, “Detection of
outliers,” Wiley Interdiscipl. Rev. Comput. Statist.,
vol. 1, no. 1, pp. 57–70, 2009.
P. Gogoi, D. K. Bhattacharyya, B. Borah, and
J. K. Kalita, “A survey of outlier detection methods
in network anomaly identiﬁcation,” Comput. J.,
vol. 54, no. 4, pp. 570–588, Apr. 2011.
K. Singh and S. Upadhyaya, “Outlier detection:
Applications and techniques,” Int. J. Comput. Sci.
Issues, vol. 9, no. 1, p. 307, 2012.
A. Zimek, E. Schubert, and H.-P. Kriegel, “A survey
on unsupervised outlier detection in
high-dimensional numerical data,” Stat. Anal.
Data Mining, vol. 5, no. 5, pp. 363–387,
Oct. 2012.
H. Aguinis, R. K. Gottfredson, and H. Joo,
“Best-practice recommendations for deﬁning,
identifying, and handling outliers,” Organizational
Res. Methods, vol. 16, no. 2, pp. 270–301,
Apr. 2013.
J. Zhang, “Advancements of outlier detection:
A survey,” ICST Trans. Scalable Inf. Syst., vol. 13,
no. 1, pp. 1–26, 2013.
M. A. F. Pimentel, D. A. Clifton, L. Clifton, and
L. Tarassenko, “A review of novelty detection,”
Signal Process., vol. 99, pp. 215–249, Jun. 2014.
M. Gupta, J. Gao, C. C. Aggarwal, and J. Han,
“Outlier detection for temporal data: A survey,”
IEEE Trans. Knowl. Data Eng., vol. 26, no. 9,
pp. 2250–2267, Sep. 2014.
S. Agrawal and J. Agrawal, “Survey on anomaly
detection using data mining techniques,” Procedia
Comput. Sci., vol. 60, pp. 708–713, 2015.
L. Akoglu, H. Tong, and D. Koutra, “Graph based
anomaly detection and description: A survey,”
Data Mining Knowl. Discovery, vol. 29, no. 3,
pp. 626–688, May 2015.
S. Ranshous, S. Shen, D. Koutra, S. Harenberg,
C. Faloutsos, and N. F. Samatova, “Anomaly
detection in dynamic networks: A survey,” Wiley
Interdiscipl. Rev. Comput. Statist., vol. 7, no. 3,
pp. 223–247, May 2015.
J. Tamboli and M. Shukla, “A survey of outlier
detection algorithms for data streams,” in Proc.
3rd Int. Conf. Comput. Sustain. Global Develop.,
2016, pp. 3535–3540.
M. Goldstein and S. Uchida, “A comparative
evaluation of unsupervised anomaly detection
algorithms for multivariate data,” PLoS ONE,
vol. 11, no. 4, Apr. 2016, Art. no. e0152173.
X. Xu, H. Liu, and M. Yao, “Recent progress of
anomaly detection,” Complexity, vol. 2019,
pp. 1–11, Jan. 2019.
H. Wang, M. J. Bah, and M. Hammad, “Progress in
outlier detection techniques: A survey,” IEEE
Access, vol. 7, pp. 107964–108000, 2019.
V. Barnett and T. Lewis, Outliers in Statistical Data,
3rd ed. Hoboken, NJ, USA: Wiley, 1994.
P. J. Rousseeuw and A. M. Leroy, Robust Regression
Outlier Detection. Hoboken, NJ, USA: Wiley, 2005.
C. C. Aggarwal, Outlier Analysis, 2nd ed. Springer,
R. Chalapathy and S. Chawla, “Deep learning for
anomaly detection: A survey,” 2019,
 
 
F. Di Mattia, P. Galeone, M. De Simoni, and
E. Ghelﬁ, “A survey on GANs for anomaly
detection,” 2019, arXiv:1906.11632. [Online].
Available: 
G. Pang, C. Shen, L. Cao, and A. van den Hengel,
“Deep learning for anomaly detection: A review,”
2020, arXiv:2007.02500. [Online]. Available:
 
K.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, and
B. Schölkopf, “An introduction to kernel-based
learning algorithms,” IEEE Trans. Neural Netw.,
vol. 12, no. 2, pp. 181–201, Mar. 2001.
I. Guyon and A. Elisseeff, “An introduction to
variable and feature selection,” J. Mach. Learn.
Res., vol. 3, no. Mar, pp. 1157–1182, 2003.
S. García, J. Luengo, and F. Herrera, Data
Preprocessing Data Mining, 1st ed. Cham,
Switzerland: Springer, 2015.
D. Rumsfeld, Known Unknown: A Memoir.
Baltimore, MD, USA: Penguin, 2011.
F. J. Anscombe, “Rejection of outliers,”
Technometrics, vol. 2, no. 2, pp. 123–146,
F. E. Grubbs, “Procedures for detecting outlying
observations in samples,” Technometrics, vol. 11,
no. 1, pp. 1–21, Feb. 1969.
D. M. Hawkins, Identiﬁcation of Outliers, vol. 11.
Dordrecht, The Netherlands: Springer, 1980.
P. Bergmann, M. Fauser, D. Sattlegger, and
C. Steger, “MVTec AD—A comprehensive
real-world dataset for unsupervised anomaly
detection,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., Jun. 2019, pp. 9592–9600.
X. Song, M. Wu, C. Jermaine, and S. Ranka,
“Conditional anomaly detection,” IEEE Trans.
Knowl. Data Eng., vol. 19, no. 5, pp. 631–645,
K. Smets, B. Verdonk, and E. M. Jordaan,
“Discovering novelty in spatio/temporal data
using one-class support vector machines,” in Proc.
Int. Joint Conf. Neural Netw., Jun. 2009,
pp. 2956–2963.
V. Chandola, A. Banerjee, and V. Kumar, “Anomaly
detection for discrete sequences: A survey,” IEEE
Trans. Knowl. Data Eng., vol. 24, no. 5,
pp. 823–839, May 2012.
W. Lu et al., “Unsupervised sequential outlier
detection with deep architectures,” IEEE Trans.
Image Process., vol. 26, no. 9, pp. 4321–4330,
Sep. 2017.
W. Samek, S. Nakajima, M. Kawanabe, and
K.-R. Müller, “On robust parameter estimation in
brain–computer interfacing,” J. Neural Eng.,
vol. 14, no. 6, Dec. 2017, Art. no. 061001.
L. Xiong, B. Póczos, and J. G. Schneider, “Group
anomaly detection using ﬂexible genre models,”
in Proc. Adv. Neural Inf. Process. Syst., 2011,
pp. 1071–1079.
K. Muandet and B. Schölkopf, “One-class support
measure machines for group anomaly detection,”
in Proc. Conf. Uncertainty Artif. Intell., 2013,
pp. 449–458.
R. Yu, X. He, and Y. Liu, “GLAD: Group anomaly
detection in social media analysis,” ACM Trans.
Knowl. Discovery Data, vol. 10, no. 2, pp. 1–22,
L. Bontemps et al., “Collective anomaly detection
based on long short-term memory recurrent
neural networks,” in Proc. Int. Conf. Future Data
Secur. Eng. Springer, 2016, pp. 141–152.
F. Ahmed and A. Courville, “Detecting semantic
anomalies,” in Proc. AAAI Conf. Artif. Intell., 2020,
pp. 3154–3162.
A. J. Fox, “Outliers in time series,” J. Roy. Stat.
Soc. Ser. B, Methodol., vol. 34, no. 3, pp. 350–363,
R. S. Tsay, “Outliers, level shifts, and variance
changes in time series,” J. Forecasting, vol. 7,
no. 1, pp. 1–20, Jan. 1988.
R. S. Tsay, D. Peña, and A. E. Pankratz, “Outliers
in multivariate time series,” Biometrika, vol. 87,
no. 4, pp. 789–804, Dec. 2000.
A. Lavin and S. Ahmad, “Evaluating real-time
anomaly detection algorithms—The numenta
anomaly benchmark,” in Proc. Int. Conf. Mach.
Learn. Appl., 2015, pp. 38–44.
S. Chawla and P. Sun, “SLOM: A new measure for
local spatial outliers,” Knowl. Inf. Syst., vol. 9,
no. 4, pp. 412–429, Apr. 2006.
E. Schubert, A. Zimek, and H.-P. Kriegel, “Local
outlier detection reconsidered: A generalized view
on locality with applications to spatial, video, and
network outlier detection,” Data Mining Knowl.
Discovery, vol. 28, no. 1, pp. 190–237, 2014.
C. C. Noble and D. J. Cook, “Graph-based
anomaly detection,” in Proc. Int. Conf. Knowl.
Discovery Data Mining, 2003, pp. 631–636.
J. Höner, S. Nakajima, A. Bauer, K.-R. Müller, and
N. Görnitz, “Minimizing trust leaks for robust sybil
detection,” in Proc. Int. Conf. Mach. Learn., vol. 70,
2017, pp. 1520–1528.
M. Ahmed, “Collective anomaly detection
techniques for network trafﬁc analysis,” Ann. Data
Sci., vol. 5, no. 4, pp. 497–512, Dec. 2018.
F. Locatello et al., “Challenging common
assumptions in the unsupervised learning of
disentangled representations,” in Proc. Int. Conf.
Mach. Learn., vol. 97, 2019, pp. 4114–4124.
B. Schölkopf and A. J. Smola, Learning With
Kernels. Cambridge, MA, USA: MIT Press, 2002.
I. Steinwart, D. Hush, and C. Scovel,
“A classiﬁcation framework for anomaly
detection,” J. Mach. Learn. Res., vol. 6,
pp. 211–232, Feb. 2005.
O. Chapelle, B. Schölkopf, and A. Zien,
Semi-Supervised Learning. Cambridge, MA, USA:
MIT Press, 2006.
W. Polonik, “Measuring mass concentrations and
estimating density contour clusters—An excess
mass approach,” Ann. Statist., vol. 23, no. 3,
pp. 855–881, Jun. 1995.
A. B. Tsybakov, “On nonparametric estimation of
density level sets,” Ann. Statist., vol. 25, no. 3,
pp. 948–969, Jun. 1997.
S. Ben-David and M. Lindenbaum, “Learning
distributions by their density levels: A paradigm
for learning without a teacher,” J. Comput. Syst.
Sci., vol. 55, no. 1, pp. 171–182, Aug. 1997.
P. Rigollet and R. Vert, “Optimal rates for plug-in
estimators of density level sets,” Bernoulli, vol. 15,
no. 4, pp. 1154–1178, Nov. 2009.
W. Polonik, “Minimum vol. sets, and generalized
quantile processes,” Stochastic Processes Their
Appl., vol. 69, no. 1, pp. 1–24, 1997.
J. N. Garcia, Z. Kutalik, K.-H. Cho, and
O. Wolkenhauer, “Level sets and minimum
volume sets of probability density functions,” Int.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
J. Approx. Reasoning, vol. 34, no. 1, pp. 25–47,
C. D. Scott and R. D. Nowak, “Learning minimum
volume sets,” J. Mach. Learn. Res., vol. 7,
pp. 665–704, Apr. 2006.
L. E. Ghaoui, M. I. Jordan, and G. R. Lanckriet,
“Robust novelty detection with single-class MPM,”
in Proc. Adv. Neural Inf. Process. Syst., 2003,
pp. 929–936.
A. K. Menon and R. C. Williamson, “A loss
framework for calibrated anomaly detection,” in
Proc. Adv. Neural Inf. Process. Syst., 2018,
pp. 1494–1504.
D. M. J. Tax and R. P. W. Duin, “Support vector
domain description,” Pattern Recognit. Lett.,
vol. 20, nos. 11–13, pp. 1191–1199, Nov. 1999.
D. M. J. Tax, “One-class classiﬁcation,” Ph.D.
dissertation, Delft Univ. Technol., Delft, The
Netherlands, 2001. [Online]. Available:
 
uuid%3Ae588fc3e-7503-4013-9b6a-
73c7b7f6b173
S. Clémençon and J. Jakubowicz, “Scoring
anomalies: A M-estimation formulation,” in Proc.
Int. Conf. Artif. Intell. Statist., 2013, pp. 659–667.
N. Goix, A. Sabourin, and S. Clémençon, “On
anomaly ranking and excess-mass curves,” in Proc.
Int. Conf. Artif. Intell. Statist., 2015, pp. 287–295.
F. R. Hampel, E. M. Ronchetti, P. J. Rousseeuw,
and W. A. Stahel, Robust Statistics: The Approach
Based Inﬂuence Functions. Hoboken, NJ, USA:
Wiley, 2005.
Y. Liu and Y. F. Zheng, “Minimum enclosing and
maximum excluding machine for pattern
description and discrimination,” in Proc. Int. Conf.
Pattern Recognit., 2006, pp. 129–132.
N. Goernitz, M. Kloft, K. Rieck, and U. Brefeld,
“Toward supervised anomaly detection,” J. Artif.
Intell. Res., vol. 46, pp. 235–262, Feb. 2013.
E. Min, J. Long, Q. Liu, J. Cui, Z. Cai, and J. Ma,
“SU-IDS: A semi-supervised and unsupervised
framework for network intrusion detection,” in
Proc. Int. Conf. Cloud Comput. Secur., 2018,
pp. 322–334.
B. Kiran, D. Thomas, and R. Parakkal, “An
overview of deep learning based methods for
unsupervised and semi-supervised anomaly
detection in videos,” J. Imag., vol. 4, no. 2, p. 36,
Feb. 2018.
M. A. Siddiqui, A. Fern, T. G. Dietterich, R. Wright,
A. Theriault, and D. W. Archer, “Feedback-guided
anomaly discovery via online optimization,” in
Proc. 24th ACM SIGKDD Int. Conf. Knowl.
Discovery Data Mining, Jul. 2018, pp. 2200–2209.
D. Hendrycks, M. Mazeika, and T. G. Dietterich,
“Deep anomaly detection with outlier exposure,”
in Proc. Int. Conf. Learn. Represent., 2019.
F. Denis, “PAC learning from positive statistical
queries,” in Proc. Int. Conf. Algorithmic Learn.
Theory, 1998, pp. 112–126.
B. Zhang and W. Zuo, “Learning from positive and
unlabeled examples: A survey,” in Proc. IEEE Int.
Symp. Inf. Process., May 2008, pp. 650–654.
M. C. Du Plessis, G. Niu, and M. Sugiyama,
“Analysis of learning from positive and unlabeled
data,” in Proc. Adv. Neural Inf. Process. Syst., 2014,
pp. 703–711.
J. Muñoz-Marí, F. Bovolo, L. Gómez-Chova,
L. Bruzzone, and G. Camp-Valls, “Semi-supervised
one-class support vector machines for
classiﬁcation of remote sensing Sata,” IEEE Trans.
Geosci. Remote Sens., vol. 48, no. 8,
pp. 3188–3197, May 2010.
G. Blanchard, G. Lee, and C. Scott,
“Semi-supervised novelty detection,” J. Mach.
Learn. Res., vol. 11, pp. 2973–3009, Nov. 2010.
H. Song, Z. Jiang, A. Men, and B. Yang, “A hybrid
semi-supervised anomaly detection model for
high-dimensional data,” Comput. Intell. Neurosci.,
vol. 2017, pp. 1–9, Nov. 2017.
S. Liu, R. Garrepalli, T. Dietterich, A. Fern, and
D. Hendrycks, “Open category detection with PAC
guarantees,” in Proc. Int. Conf. Mach. Learn.,
vol. 80, 2018, pp. 3169–3178.
D. Hendrycks and K. Gimpel, “A baseline for
detecting misclassiﬁed and out-of-distribution
examples in neural networks,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2017.
T. Che et al., “Deep veriﬁer networks: Veriﬁcation
of deep discriminative models with deep
generative models,” 2019, arXiv:1911.07421.
[Online]. Available:
 
R. T. Schirrmeister, Y. Zhou, T. Ball, and D. Zhang,
“Understanding anomaly detection with deep
invertible networks through hierarchies of
distributions and features,” in Proc. Adv. Neural
Inf. Process. Syst. (NeurIPS), 2020.
G. Boracchi, D. Carrera, C. Cervellera, and
D. Maccio, “QuantTree: Histograms for change
detection in multivariate data streams,” in Proc.
Int. Conf. Mach. Learn., vol. 80, 2018,
pp. 639–648.
M. Sugiyama, M. Krauledat, and K.-R. Müller,
“Covariate shift adaptation by importance
weighted cross validation,” J. Mach. Learn. Res.,
vol. 8, no. May, pp. 985–1005, 2007.
J. Quionero-Candela, M. Sugiyama,
A. Schwaighofer, and N. D. Lawrence, Dataset
Shift in Machine Learning. Cambridge, MA, USA:
MIT Press, 2009.
M. Sugiyama and M. Kawanabe, Mach. Learn.
Non-Stationary Environments: Introduction to
Covariate Shift Adaptation. Cambridge, MA, USA:
MIT Press, 2012.
D. Baehrens, T. Schroeter, S. Harmeling,
M. Kawanabe, K. Hansen, and K.-R. Müller, “How
to explain individual classiﬁcation decisions,”
J. Mach. Learn. Res., vol. 11, pp. 1803–1831,
Jun. 2010.
G. Montavon, W. Samek, and K.-R. Müller,
“Methods for interpreting and understanding deep
neural networks,” Digit. Signal Process., vol. 73,
pp. 1–15, Feb. 2018.
S. Lapuschkin, S. Wäldchen, A. Binder,
G. Montavon, W. Samek, and K.-R. Müller,
“Unmasking Clever Hans predictors and assessing
what machines really learn,” Nature Commun.,
vol. 10, no. 1, p. 1096, Dec. 2019.
W. Samek, G. Montavon, A. Vedaldi, L. K. Hansen,
and K.-R. Müller, Eds., Explainable AI: Interpreting,
Explaining and Visualizing Deep Learning (Lecture
Notes in Computer Science), vol. 11700. Springer,
W. Härdle, Applied Nonparametric Regression,
no. 19. Cambridge, U.K.: Cambridge Univ. Press,
J. Laurikkala, M. Juhola, E. Kentala, N. Lavrac,
S. Miksch, and B. Kavsek, “Informal identiﬁcation
of outliers in medical data,” in Proc. 5th Int.
Workshop Intell. Data Anal. Med. Pharmacol.,
vol. 1, 2000, pp. 20–24.
A. K. Jain and R. C. Dubes, Algorithms for
Clustering Data. Upper Saddle River, NJ, USA:
Prentice-Hall, 1988.
S. Roberts and L. Tarassenko, “A probabilistic
resource allocating network for novelty
detection,” Neural Comput., vol. 6, no. 2,
pp. 270–284, Mar. 1994.
C. M. Bishop, “Novelty detection and neural
network validation,” IEE Proc.-Vis., Image Signal
Process., vol. 141, no. 4, pp. 217–222, 1994.
L. Devroye and L. Györﬁ, Nonparametric Density
Estimation: The L1 View. New York, NY, USA:
Wiley, 1985.
S. Frühwirth-Schnatter, Finite Mixture Markov
Switching Models. New York, NY, USA: Springer,
J. Kim and C. D. Scott, “Robust kernel density
estimation,” J. Mach. Learn. Res., vol. 13, no. 82,
pp. 2529–2565, 2012.
R. Vandermeulen and C. Scott, “Consistency of
robust kernel density estimators,” in Proc. Conf.
Learn. Theory, 2013, pp. 568–591.
N. Amruthnath and T. Gupta, “A research study on
unsupervised machine learning algorithms for
early fault detection in predictive maintenance,”
in Proc. 5th Int. Conf. Ind. Eng. Appl. (ICIEA),
Apr. 2018, pp. 355–361.
S. E. Fahlman, G. E. Hinton, and T. J. Sejnowski,
“Massively parallel architectures for AI: NETL,
Thistle, and Boltzmann machines,” in Proc. AAAI
Conf. Artif. Intell., 1983, pp. 109–113.
J. J. Hopﬁeld, “Neural networks and physical
systems with emergent collective computational
abilities,” Proc. Nat. Acad. Sci. USA, vol. 79, no. 8,
pp. 2554–2558, 1982.
Y. Lecun, S. Chopra, R. Hadsell, M. Ranzato, and
F. Huang, A Tutorial on Energy-Based Learning.
Cambridge, MA, USA: MIT Press, 2006.
G. E. Hinton, “Training products of experts by
minimizing contrastive divergence,” Neural
Comput., vol. 14, no. 8, pp. 1771–1800,
Aug. 2002.
M. Welling and Y. W. Teh, “Bayesian learning via
stochastic gradient langevin dynamics,” in Proc.
Int. Conf. Mach. Learn., 2011, pp. 681–688.
W. Grathwohl, K.-C. Wang, J.-H. Jacobsen,
D. Duvenaud, M. Norouzi, and K. Swersky, “Your
classiﬁer is secretly an energy based model and
you should treat it like one,” in Proc. Int. Conf.
Learn. Represent., 2020.
G. E. Hinton, S. Osindero, and Y.-W. Teh, “A fast
learning algorithm for deep belief nets,” Neural
Comput., vol. 18, no. 7, pp. 1527–1554,
Jul. 2006.
R. Salakhutdinov and G. Hinton, “Deep
Boltzmann machines,” in Proc. Int. Conf. Artif.
Intell. Statist., 2009, pp. 448–455.
J. Ngiam, Z. Chen, P. W. Koh, and A. Ng, “Learning
deep energy models,” in Proc. Int. Conf. Mach.
Learn., 2011, pp. 1105–1112.
S. Zhai, Y. Cheng, W. Lu, and Z. Zhang, “Deep
structured energy based models for anomaly
detection,” in Proc. Int. Conf. Learn. Represent.
(ICLR), vol. 48, 2016, pp. 1100–1109.
D. P. Kingma and M. Welling, “Auto-encoding
variational Bayes,” in Proc. Int. Conf. Learn.
Represent., 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra,
“Stochastic backpropagation and approximate
inference in deep generative models,” in Proc. Int.
Conf. Mach. Learn., vol. 32, 2014, pp. 1278–1286.
D. P. Kingma and M. Welling, “An introduction to
variational autoencoders,” Found. Trends Mach.
Learn., vol. 12, no. 4, pp. 307–392, 2019.
I. Goodfellow et al., “Generative adversarial nets,”
in Proc. Adv. Neural Inf. Process. Syst., 2014,
pp. 2672–2680.
H. Xu et al., “Unsupervised anomaly detection via
variational auto-encoder for seasonal KPIs in Web
applications,” in Proc. World Wide Web Conf.,
2018, pp. 187–196.
E. Nalisnick, A. Matsukawa, Y. W. Teh, D. Gorur,
and B. Lakshminarayanan, “Do deep generative
models know what they don’t know?” in Proc. Int.
Conf. Learn. Represent., 2019.
J. An and S. Cho, “Variational autoencoder based
anomaly detection using reconstruction
probability,” Special Lect. IE, vol. 2, no. 1,
pp. 1–18, 2015.
T. Salimans et al., “Improved techniques for
training gans,” in Proc. Adv. Neural Inf. Process.
Syst., 2016, pp. 2234–2242.
M. Arjovsky, S. Chintala, and L. Bottou,
“Wasserstein generative adversarial networks,” in
Proc. Int. Conf. Mach. Learn., vol. 70, 2017,
pp. 214–223.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin,
and A. C. Courville, “Improved training of
wasserstein gans,” in Proc. Adv. Neural Inf. Process.
Syst., 2017, pp. 5767–5777.
L. Dinh, D. Krueger, and Y. Bengio, “NICE:
Non-linear independent components estimation,”
in Proc. Int. Conf. Learn. Represent., 2015.
G. Papamakarios, E. Nalisnick, D. Jimenez
Rezende, S. Mohamed, and B. Lakshminarayanan,
“Normalizing ﬂows for probabilistic modeling and
inference,” 2019, arXiv:1912.02762. [Online].
Available: 
I. Kobyzev, S. Prince, and M. Brubaker,
“Normalizing ﬂows: An introduction and review
of current methods,” IEEE Trans. Pattern Anal.
Mach. Intell., early access, May 7, 2020, doi:
10.1109/TPAMI.2020.2992934.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
L. Dinh, J. Sohl-Dickstein, and S. Bengio, “Density
estimation using real NVP,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2017.
C.-W. Huang, D. Krueger, A. Lacoste, and
A. Courville, “Neural autoregressive ﬂows,” in
Proc. Int. Conf. Mach. Learn., vol. 80, 2018,
pp. 2078–2087.
F. Noé, S. Olsson, J. Köhler, and H. Wu,
“Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning,”
Science, vol. 365, no. 6457, Sep. 2019,
Art. no. eaaw1147.
B. Nachman and D. Shih, “Anomaly detection with
density estimation,” Phys. Rev. D, Part. Fields,
vol. 101, no. 7, Apr. 2020, Art. no. 075042.
L. Wellhausen, R. Ranftl, and M. Hutter, “Safe
robot navigation via multi-modal anomaly
detection,” IEEE Robot. Autom. Lett., vol. 5, no. 2,
pp. 1326–1333, Apr. 2020.
P. Kirichenko, P. Izmailov, and A. G. Wilson, “Why
normalizing ﬂows fail to detect out-of-distribution
data,” in Proc. Adv. Neural Inf. Process. Syst.
(NeurIPS), 2020.
M. Mirza and S. Osindero, “Conditional
generative adversarial nets,” 2014,
 
 
S. Suh, D. H. Chae, H.-G. Kang, and S. Choi,
“Echo-state conditional variational autoencoder
for anomaly detection,” in Proc. Int. Joint Conf.
Neural Netw., Jul. 2016, pp. 1015–1022.
A. Abdelhamed, M. A. Brubaker, and M. S. Brown,
“Noise ﬂow: Noise modeling with conditional
normalizing ﬂows,” in Proc. Int. Conf. Comput.
Vis., 2019, pp. 3165–3173.
D. Li, D. Chen, B. Jin, L. Shi, J. Goh, and S.-K. Ng,
“MAD-GAN: Multivariate anomaly detection for
time series data with generative adversarial
networks,” in Proc. Int. Conf. Artif. Neural Netw.,
vol. 2019, pp. 703–716.
S. R. Bowman, L. Vilnis, O. Vinyals, A. Dai,
R. Jozefowicz, and S. Bengio, “Generating
sentences from a continuous space,” in Proc. 20th
SIGNLL Conf. Comput. Natural Lang. Learn., 2016,
pp. 10–21.
L. Chen et al., “Adversarial text generation via
feature-mover’s distance,” in Proc. Adv. Neural Inf.
Process. Syst., 2018, pp. 4666–4677.
W. Jin, R. Barzilay, and T. Jaakkola, “Junction tree
variational autoencoder for molecular graph
generation,” in Proc. Int. Conf. Mach. Learn.,
vol. 80, 2018, pp. 2323–2332.
A. Bojchevski, O. Shchur, D. Zügner, and
S. Günnemann, “NetGAN: Generating graphs via
random walks,” in Proc. Int. Conf. Mach. Learn.,
vol. 80, 2018, pp. 610–619.
R. Liao, Y. Li, Y. Song, S. Wang, W. Hamilton,
D. K. Duvenaud, R. Urtasun, and R. Zemel,
“Efﬁcient graph generation with graph recurrent
attention networks,” in Proc. Adv. Neural Inf.
Process. Syst., 2019, pp. 4255–4265.
V. Vapnik, Statistical Learning Theory. Hoboken,
NJ, USA: Wiley, 1998.
M. M. Moya, M. W. Koch, and L. D. Hostetler,
“One-class classiﬁer networks for target
recognition applications,” in Proc. World Congr.
Neural Netw., 1993, pp. 797–801.
M. M. Moya and D. R. Hush, “Network constraints
and multi-objective optimization for one-class
classiﬁcation,” Neural Netw., vol. 9, no. 3,
pp. 463–474, 1996.
S. S. Khan and M. G. Madden, “One-class
classiﬁcation: Taxonomy of study and review of
techniques,” Knowl. Eng. Rev., vol. 29, no. 3,
pp. 345–374, Jun. 2014.
T. Minter, “Single-class classiﬁcation,” in Proc.
LARS Symposia, 1975, p. 54.
R. El-Yaniv and M. Nisenson, “Optimal single-class
classiﬁcation strategies,” in Proc. Adv. Neural Inf.
Process. Syst., 2007, pp. 377–384.
P. J. Rousseeuw, “Multivariate estimation with
high breakdown point,” Math. Statist. Appl.,
vol. 8, pp. 283–297, 1985.
P. J. Rousseeuw and K. V. Driessen, “A fast
algorithm for the minimum covariance
determinant estimator,” Technometrics, vol. 41,
no. 3, pp. 212–223, Aug. 1999.
A. Muñoz and J. M. Moguerza, “Estimation of
high-density regions using one-class neighbor
machines,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 28, no. 3, pp. 476–480, Mar. 2006.
B. Schölkopf et al., “Input space versus feature
space in kernel-based methods,” IEEE Trans.
Neural Netw., vol. 10, no. 5, pp. 1000–1017,
Sep. 1999.
L. M. Manevitz and M. Yousef, “One-class SVMs
for document classiﬁcation,” J. Mach. Learn. Res.,
vol. 2, no. Dec., pp. 139–154, 2001.
R. Vert and J.-P. Vert, “Consistency and
convergence rates of one-class SVMs and related
algorithms,” J. Mach. Learn. Res., vol. 7,
pp. 817–854, May 2006.
G. Lee and C. D. Scott, “The one class support
vector machine solution path,” in Proc. Int. Conf.
Acoust., Speech, Signal Process., vol. 2, 2007,
pp. 521–524.
K. Sjöstrand and R. Larsen, “The entire
regularization path for the support vector domain
description,” in Proc. Int. Conf. Med. Image
Comput.-Assist. Intervent, 2006, pp. 241–248.
G. Lee and C. Scott, “Nested support vector
machines,” IEEE Trans. Signal Process., vol. 58,
no. 3, pp. 1648–1660, Mar. 2010.
A. Glazer, M. Lindenbaum, and S. Markovitch,
“q-OCSVM: A q-quantile estimator for
high-dimensional distributions,” in Proc. Adv.
Neural Inf. Process. Syst., 2013, pp. 503–511.
N. Görnitz, L. A. Lima, K.-R. Müller, M. Kloft, and
S. Nakajima, “Support vector data descriptions
and k-means clustering: One class?” IEEE Trans.
Neural Netw. Learn. Syst., vol. 29, no. 9,
pp. 3994–4006, Sep. 2018.
S. Das, B. L. Matthews, A. N. Srivastava, and
N. C. Oza, “Multiple kernel learning for
heterogeneous anomaly detection: Algorithm and
aviation safety case study,” in Proc. Int. Conf.
Knowl. Discovery Data Mining, 2010, pp. 47–56.
C. Gautam, R. Balaji, K. Sudharsan, A. Tiwari, and
K. Ahuja, “Localized multiple kernel learning for
anomaly detection: One-class classiﬁcation,”
Knowl.-Based Syst., vol. 165, pp. 241–252,
Feb. 2019.
G. Ratsch, S. Mika, B. Scholkopf, and K.-R. Muller,
“Constructing boosting algorithms from SVMs: An
application to one-class classiﬁcation,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 24, no. 9,
pp. 1184–1199, Sep. 2002.
V. Roth, “Outlier detection with one-class kernel
Fisher discriminants,” in Proc. Adv. Neural Inf.
Process. Syst., 2005, pp. 1169–1176.
V. Roth, “Kernel Fisher discriminants for outlier
detection,” Neural Comput., vol. 18, no. 4,
pp. 942–960, Apr. 2006.
F. Dufrenois, “A one-class kernel Fisher criterion
for outlier detection,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 26, no. 5, pp. 982–994,
A. Ghasemi, H. R. Rabiee, M. T. Manzuri, and
M. H. Rohban, “A Bayesian approach to the data
description problem,” in Proc. AAAI Conf. Artif.
Intell., 2012, pp. 907–913.
M. Stolpe, K. Bhaduri, K. Das, and K. Morik,
“Anomaly detection in vertically partitioned data
by distributed Core vector machines,” in Proc. Eur.
Conf. Mach. Learn. Knowl. Discovery Databases,
2013, pp. 321–336.
H. Jiang, H. Wang, W. Hu, D. Kakde, and
A. Chaudhuri, “Fast incremental SVDD learning
algorithm with the Gaussian kernel,” in Proc. AAAI
Conf. Artif. Intell., vol. 33, 2019, pp. 3991–3998.
W. Liu, G. Hua, and J. R. Smith, “Unsupervised
one-class learning for automatic outlier removal,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
Jun. 2014, pp. 3826–3833.
P. Wu, J. Liu, and F. Shen, “A deep one-class neural
network for anomalous event detection in
complex scenes,” IEEE Trans. Neural Netw. Learn.
Syst., vol. 31, no. 7, pp. 2609–2622, Jul. 2020.
R. Chalapathy, A. Krishna Menon, and S. Chawla,
“Anomaly detection using one-class neural
networks,” 2018, arXiv:1802.06360. [Online].
Available: 
Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller,
“Efﬁcient BackProp,” in Neural Networks: Tricks
Trade (Lecture Notes in Computer Science),
vol. 7700, G. Montavon and G. B. Orr, Eds. Berlin,
Germany: Springer, 2012, pp. 9–48.
D. P. Kingma and J. Ba, “Adam: A method for
stochastic optimization,” in Proc. Int. Conf. Learn.
Represent. (ICLR), 2015.
G. Goh, “Why momentum really works,” Distill,
2017. [Online]. Available:
 
doi: 10.23915/distill.00006.
L. Ruff, R. A. Vandermeulen, B. Joe Franks,
K.-R. Müller, and M. Kloft, “Rethinking
assumptions in deep anomaly detection,” 2020,
 
 
S. Goyal, A. Raghunathan, M. Jain, H. V. Simhadri,
and P. Jain, “DROCC: Deep robust one-class
classiﬁcation,” in Proc. Int. Conf. Mach. Learn.,
2020, pp. 11335–11345.
J. Kauffmann, L. Ruff, G. Montavon, and
K.-R. Müller, “The clever Hans effect in anomaly
detection,” 2020, arXiv:2006.10609. [Online].
Available: 
P. Chong, L. Ruff, M. Kloft, and A. Binder, “Simple
and effective prevention of mode collapse in deep
one-class classiﬁcation,” in Proc. Int. Joint Conf.
Neural Netw., 2020, pp. 1–9.
G. Pang, C. Shen, and A. van den Hengel, “Deep
anomaly detection with deviation networks,” in
Proc. Int. Conf. Knowl. Discovery Data Mining,
2019, pp. 353–362.
P. Liznerski, L. Ruff, R. A. Vandermeulen,
B. J. Franks, M. Kloft, and K.-R. Müller,
“Explainable deep one-class classiﬁcation,” in
Proc. Int. Conf. Learn. Represent., 2021.
L. Shen, Z. Li, and J. Kwok, “Timeseries anomaly
detection using temporal hierarchical one-class
network,” in Proc. Adv. Neural Inf. Process. Syst.
(NeurIPS), 2020.
M. Sabokrou, M. Fathy, G. Zhao, and E. Adeli,
“Deep end-to-end one-class classiﬁer,” IEEE Trans.
Neural Netw. Learn. Syst., early access, 2020, doi:
10.1109/TNNLS.2020.2979049.
T. Hastie, R. Tibshirani, and J. Friedman, The
Elements of Statistical Learning: Data Mining,
Inference, and Prediction, 2nd ed. New York, NY,
USA: Springer, 2009.
G. Steinbuss and K. Böhm, “Generating artiﬁcial
outliers in the absence of genuine ones—A
survey,” 2020, arXiv:2006.03646. [Online].
Available: 
J. P. Theiler and D. M. Cai, “Resampling approach
for anomaly detection in multispectral images,”
Proc. SPIE, vol. 5093, pp. 230–240, Sep. 2003.
M. A. Davenport, R. G. Baraniuk, and C. D. Scott,
“Learning minimum volume sets with support
vector machines,” in Proc. IEEE Signal Process. Soc.
Workshop Mach. Learn. Signal Process., Sep. 2006,
pp. 301–306.
W. Fan, M. Miller, S. Stolfo, W. Lee, and P. Chan,
“Using artiﬁcial anomalies to detect unknown and
known network intrusions,” Knowl. Inf. Syst.,
vol. 6, no. 5, pp. 507–527, Sep. 2004.
P. Cheema et al., “On structural health monitoring
using tensor analysis and support vector machine
with artiﬁcial negative data,” in Proc. 25th ACM
Int. Conf. Inf. Knowl. Manage., Oct. 2016,
pp. 1813–1822.
N. Abe, B. Zadrozny, and J. Langford, “Outlier
detection by active learning,” in Proc. Int. Conf.
Knowl. Discovery Data Mining, 2006, pp. 504–509.
J. W. Stokes, J. C. Platt, J. Kravis, and M. Shilman,
“ALADIN: Active learning of anomalies to detect
intrusions,” Microsoft Res., New York, NY, USA,
Tech. Rep. MSR-TR-2008-24, 2008.
N. Görnitz, M. Kloft, and U. Brefeld, “Active and
semi-supervised data domain description,” in Proc.
Eur. Conf. Mach. Learn. Princ. Pract. Knowl.
Discovery Databases, 2009, pp. 407–422.
D. Pelleg and A. W. Moore, “Active learning for
anomaly and rare-category detection,” in Proc.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Adv. Neural Inf. Process. Syst., 2005,
pp. 1073–1080.
M. Du, Z. Chen, C. Liu, R. Oak, and D. Song,
“Lifelong anomaly detection through unlearning,”
in Proc. ACM SIGSAC Conf. Comput. Commun.
Secur., 2019, pp. 1283–1297.
N. Japkowicz, C. Myers, and M. Gluck, “A novelty
detection approach to classiﬁcation,” in Proc. Int.
Joint Conf. Artif. Intell., vol. 1, 1995, pp. 518–523.
S. Hawkins, H. He, G. Williams, and R. Baxter,
“Outlier detection using replicator neural
networks,” in Proc. Int. Conf. Data Warehousing
Knowl. Discovery, vol. 2454, 2002, pp. 170–180.
J. A. Lee and M. Verleysen, Nonlinear
Dimensionality Reduction. New York, NY, USA:
Springer, 2007.
R. Pless and R. Souvenir, “A survey of manifold
learning for images,” IPSJ Trans. Comput. Vis.
Appl., vol. 1, pp. 83–94, 2009.
T. Kohonen, “The self-organizing map,” Proc. IEEE,
vol. 78, no. 9, pp. 1464–1480, Sep. 1990.
T. Kohonen, Self-Organizing Maps, 3rd ed. Berlin,
Germany: Springer, 2001.
L. van der Maaten, E. Postma, and J. van den
Herik, “Dimensionality reduction: A comparative
review,” Tilburg Centre Creative Comput., Tilburg
Univ., Tilburg, The Netherlands,
Tech. Rep. TiCC-TR 2009-005, 2009.
J. Schmidhuber, “Learning factorial codes by
predictability minimization,” Neural Comput.,
vol. 4, no. 6, pp. 863–879, Nov. 1992.
M. Tschannen, O. Bachem, and M. Lucic, “Recent
advances in autoencoder-based representation
learning,” in Proc. Workshop Bayesian Deep Learn.
(NeurIPS), 2018.
Y. Linde, A. Buzo, and R. Gray, “An algorithm for
vector quantizer design,” IEEE Trans. Commun.,
vol. 28, no. 1, pp. 84–95, Jan. 1980.
A. Gersho and R. M. Gray, Vector Quantization and
Signal Compression (The Springer International
Series in Engineering and Computer Science),
vol. 159. Boston, MA, USA: Springer, 1992.
M. E. Tipping and C. M. Bishop, “Probabilistic
principal component analysis,” J. Roy. Stat. Soc. B,
Stat. Methodol., vol. 61, no. 3, pp. 611–622, 1999.
C. M. Bishop, “Bayesian PCA,” in Proc. Adv. Neural
Inf. Process. Syst., 1999, pp. 382–388.
I. T. Jolliffe, Principal Component Analysis
(Springer Series in Statistics), 2nd ed. New York,
NY, USA: Springer, 2002.
D. M. Hawkins, “The detection of errors in
multivariate data using principal components,”
J. Amer. Stat. Assoc., vol. 69, no. 346,
pp. 340–344, Jun. 1974.
J. E. Jackson and G. S. Mudholkar, “Control
procedures for residuals associated with principal
component analysis,” Technometrics, vol. 21,
no. 3, pp. 341–349, Aug. 1979.
L. Parra, G. Deco, and S. Miesbach, “Statistical
independence and novelty detection with
information preserving nonlinear maps,” Neural
Comput., vol. 8, no. 2, pp. 260–269, Feb. 1996.
M.-L. Shyu, S.-C. Chen, K. Sarinnapakorn, and
L. Chang, “A novel anomaly detection scheme
based on principal component classiﬁer,” in Proc.
IEEE Int. Conf. Data Mining, Jan. 2003,
pp. 353–365.
L. Huang, X. Nguyen, M. Garofalakis,
M. I. Jordan, A. Joseph, and N. Taft, “In-network
PCA and anomaly detection,” in Proc. Adv. Neural
Inf. Process. Syst., 2007, pp. 617–624.
V. Sharan, P. Gopalan, and U. Wieder, “Efﬁcient
anomaly detection via matrix sketching,” in Proc.
Adv. Neural Inf. Process. Syst., 2018,
pp. 8069–8080.
J. Ham, D. D. Lee, S. Mika, and B. Schölkopf,
“A kernel view of the dimensionality reduction of
manifolds,” in Proc. 21st Int. Conf. Mach. Learn.
(ICML), 2004, p. 47.
N. Kwak, “Principal component analysis based on
L1-norm maximization,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 30, no. 9, pp. 1672–1680,
Sep. 2008.
M. H. Nguyen and F. Torre, “Robust kernel
principal component analysis,” in Proc. Adv.
Neural Inf. Process. Syst., 2009, pp. 1185–1192.
E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust
principal component analysis?” J. ACM, vol. 58,
no. 3, pp. 1–37, 2011.
Y. Xiao, H. Wang, W. Xu, and J. Zhou, “L1 norm
based KPCA for novelty detection,” Pattern
Recognit., vol. 46, no. 1, pp. 389–396, Jan. 2013.
E. Oja, “A simpliﬁed neuron model as a principal
component analyzer,” J. Math. Biol., vol. 15, no. 3,
pp. 267–273, 1982.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams,
“Learning internal representations by error
propagation,” in Parallel Distributed Processing
Explorations in the Microstructure of Cognition.
Cambridge, MA, USA: MIT Press, 1986, ch. 8,
pp. 318–362.
D. H. Ballard, “Modular learning in neural
networks,” in Proc. AAAI Conf. Artif. Intell., 1987,
pp. 279–284.
G. E. Hinton, “Connectionist learning procedures,”
Artif. Intell., vol. 40, nos. 1–3, pp. 185–234,
Sep. 1989.
M. A. Kramer, “Nonlinear principal component
analysis using autoassociative neural networks,”
AIChE J., vol. 37, no. 2, pp. 233–243, Feb. 1991.
G. E. Hinton, “Reducing the dimensionality of
data with neural networks,” Science, vol. 313,
no. 5786, pp. 504–507, Jul. 2006.
P. Baldi and K. Hornik, “Neural networks and
principal component analysis: Learning from
examples without local minima,” Neural Netw.,
vol. 2, no. 1, pp. 53–58, Jan. 1989.
E. Oja, “Principal components, minor components,
and linear neural networks,” Neural Netw., vol. 5,
no. 6, pp. 927–935, Nov. 1992.
B. A. Olshausen and D. J. Field, “Emergence of
simple-cell receptive ﬁeld properties by learning a
sparse code for natural images,” Nature, vol. 381,
no. 6583, pp. 607–609, Jun. 1996.
B. A. Olshausen and D. J. Field, “Sparse coding
with an overcomplete basis set: A strategy
employed by v1?” Vis. Res., vol. 37, no. 23,
pp. 3311–3325, Dec. 1997.
M. S. Lewicki and T. J. Sejnowski, “Learning
overcomplete representations,” Neural Comput.,
vol. 12, no. 2, pp. 337–365, Feb. 2000.
H. Lee, A. Battle, R. Raina, and A. Y. Ng, “Efﬁcient
sparse coding algorithms,” in Proc. Adv. Neural Inf.
Process. Syst., 2007, pp. 801–808.
A. Makhzani and B. Frey, “k-sparse autoencoders,”
in Proc. Int. Conf. Learn. Represent. (ICLR), 2014.
N. Zeng, H. Zhang, B. Song, W. Liu, Y. Li, and
A. M. Dobaie, “Facial expression recognition via
learning deep sparse autoencoders,”
Neurocomputing, vol. 273, pp. 643–649,
Jan. 2018.
D. Arpit, Y. Zhou, H. Ngo, and V. Govindaraju,
“Why regularized auto-encoders learn sparse
representation?” in Proc. Int. Conf. Mach. Learn.,
vol. 48, 2016, pp. 136–144.
P. Vincent, H. Larochelle, Y. Bengio, and
P.-A. Manzagol, “Extracting and composing robust
features with denoising autoencoders,” in Proc.
Int. Conf. Mach. Learn., 2008, pp. 1096–1103.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio,
P.-A. Manzagol, and L. Bottou, “Stacked denoising
autoencoders: Learning useful representations in
a deep network with a local denoising criterion,”
J. Mach. Learn. Res., vol. 11, pp. 3371–3408,
Dec. 2010.
S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y.
Bengio, “Contractive auto-encoders: Explicit
invariance during feature extraction,” in Proc. Int.
Conf. Mach. Learn., 2011, pp. 833–840.
S. You, K. C. Tezcan, X. Chen, and E. Konukoglu,
“Unsupervised lesion detection via image
restoration with a normative prior,” in Proc. Int.
Conf. Med. Imag. Deep Learn., 2019, pp. 540–556.
D. Park, Y. Hoshi, and C. C. Kemp, “A multimodal
anomaly detector for robot-assisted feeding using
an LSTM-based variational autoencoder,” IEEE
Robot. Autom. Lett., vol. 3, no. 3, pp. 1544–1551,
Jul. 2018.
A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow,
and B. Frey, “Adversarial autoencoders,” 2015,
 
 
P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig,
P. Agarwal, and G. Shroff, “LSTM-based
encoder-decoder for multi-sensor anomaly
detection,” 2016, arXiv:1607.00148. [Online].
Available: 
T. Kieu, B. Yang, C. Guo, and C. S. Jensen, “Outlier
detection for time series with recurrent
autoencoder ensembles,” in Proc. 28th Int. Joint
Conf. Artif. Intell., Aug. 2019, pp. 2725–2732.
G. Kwon, M. Prabhushankar, D. Temel, and
G. AlRegib, “Backpropagated gradient
representations for anomaly detection,” in Proc.
Eur. Conf. Comput. Vis., 2020, pp. 206–226.
C. D. Hofer, R. Kwitt, M. Dixit, and
M. Niethammer, “Connectivity-optimized
representation learning via persistent homology,”
in Proc. Int. Conf. Mach. Learn., vol. 97, 2019,
pp. 2751–2760.
T. Amarbayasgalan, B. Jargalsaikhan, and K. Ryu,
“Unsupervised novelty detection using deep
autoencoders with density based clustering,” Appl.
Sci., vol. 8, no. 9, p. 1468, Aug. 2018.
N. Saraﬁjanovic-Djukic and J. Davis, “Fast
distance-based anomaly detection in images using
an inception-like autoencoder,” in Proc. Int. Conf.
Discovery Sci. Cham, Switzerland: Springer, 2019,
pp. 493–508.
A. K. Jain, “Data clustering: 50 years beyond
K-means,” Pattern Recognit. Lett., vol. 31, no. 8,
pp. 651–666, Jun. 2010.
G. Voronoi, “Nouvelles applications des
paramètres continus à la théorie des formes
quadratiques. Premier mémoire. Sur quelques
propriétés des formes quadratiques positives
parfaites,” J. für die Reine und Angewandte
Mathematik, vol. 1908, no. 133, pp. 97–178,
G. Voronoi, “Nouvelles applications des
paramètres continus à la théorie des formes
quadratiques. Deuxième mémoire. Recherches sur
les parallélloèdres primitifs,” J. für die Reine und
Angewandte Mathematik, vol. 1908, no. 134,
pp. 198–287, 1908.
I. S. Dhillon, Y. Guan, and B. Kulis, “Kernel
k-means, spectral clustering and normalized cuts,”
in Proc. Int. Conf. Knowl. Discovery Data Mining,
2004, pp. 551–556.
J. Xie, R. Girshick, and A. Farhadi, “Unsupervised
deep embedding for clustering analysis,” in Proc.
Int. Conf. Mach. Learn., vol. 48, 2016,
pp. 478–487.
A. Van Den Oord and O. Vinyals, “Neural discrete
representation learning,” in Proc. Adv. Neural Inf.
Process. Syst., 2017, pp. 6306–6315.
A. Razavi, A. van den Oord, and O. Vinyals,
“Generating diverse high-ﬁdelity images with
VQ-VAE-2,” in Proc. Adv. Neural Inf. Process. Syst.,
2019, pp. 14866–14876.
M. Kampffmeyer, S. Løkse, F. M. Bianchi, L. Livi,
A.-B. Salberg, and R. Jenssen, “Deep
divergence-based approach to clustering,” Neural
Netw., vol. 113, pp. 91–101, May 2019
B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong,
“Towards k-means-friendly spaces: Simultaneous
deep learning and clustering,” in Proc. Int. Conf.
Mach. Learn., vol. 70, 2017, pp. 3861–3870.
M. Caron, P. Bojanowski, A. Joulin, and M. Douze,
“Deep clustering for unsupervised learning of
visual features,” in Proc. Eur. Conf. Comput. Vis.,
2018, pp. 132–149.
P. Bojanowski and A. Joulin, “Unsupervised
learning by predicting noise,” in Proc. Int. Conf.
Mach. Learn., vol. 70, 2017, pp. 517–526.
C. M. Bishop, Pattern Recognition and Machine
Learning. New York, NY, USA: Springer, 2006.
K. P. Murphy, Machine Learning: A Probabilistic
Perspective. Cambridge, MA, USA: MIT Press,
S. Theodoridis, Machine Learning: A Bayesian and
Optimization Perspective, 2nd ed. New York, NY,
USA: Academic, 2020.
D. J. C. MacKay, “A practical Bayesian framework
for backpropagation networks,” Neural Comput.,
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
vol. 4, no. 3, pp. 448–472, May 1992.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and
D. Wierstra, “Weight uncertainty in neural
networks,” in Proc. Int. Conf. Mach. Learn., vol. 37,
2015, pp. 1613–1622.
L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder,
E. Müller, and M. Kloft, “Deep support vector data
description for unsupervised and semi-supervised
anomaly detection,” in Proc. ICML Workshop
Uncertainty Robustness Deep Learn., 2019,
S. Harmeling, G. Dornhege, D. Tax, F. Meinecke,
and K.-R. Müller, “From outliers to prototypes:
Ordering data,” Neurocomputing, vol. 69,
nos. 13–15, pp. 1608–1618, Aug. 2006.
M. Zhao and V. Saligrama, “Anomaly detection
with score functions based on nearest neighbor
graphs,” in Proc. Adv. Neural Inf. Process. Syst.,
2009, pp. 2250–2258.
X. Gu, L. Akoglu, and A. Rinaldo, “Statistical
analysis of nearest neighbor methods for anomaly
detection,” in Proc. Adv. Neural Inf. Process. Syst.,
2019, pp. 10923–10933.
P. Juszczak, D. M. J. Tax, E. Pe¸kalska, and
R. P. W. Duin, “Minimum spanning tree based
one-class classiﬁer,” Neurocomputing, vol. 72,
nos. 7–9, pp. 1859–1869, Mar. 2009.
F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation
forest,” in Proc. IEEE Int. Conf. Data Mining,
Dec. 2008, pp. 413–422.
S. Guha, N. Mishra, G. Roy, and O. Schrijvers,
“Robust random cut forest based anomaly
detection on streams,” in Proc. Int. Conf. Mach.
Learn., vol. 48, 2016, pp. 2712–2721.
L. Bergman, N. Cohen, and Y. Hoshen, “Deep
nearest neighbor anomaly detection,” 2020,
 
 
J. Glasser and B. Lindauer, “Bridging the gap:
A pragmatic approach to generating insider threat
data,” in Proc. IEEE Secur. Privacy Workshops,
May 2013, pp. 98–104.
N. Mu and J. Gilmer, “MNIST-C: A robustness
benchmark for computer vision,” 2019,
 
 
D. Hendrycks and T. Dietterich, “Benchmarking
neural network robustness to common
corruptions and perturbations,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2019.
P. Laskov, C. Schäfer, I. Kotenko, and K.-R. Müller,
“Intrusion detection in unlabeled data with
quarter-sphere support vector machines,”
PIK-Praxis der Informationsverarbeitung und
Kommunikation, vol. 27, no. 4, pp. 228–236,
Dec. 2004.
M. Kloft and P. Laskov, “Security analysis of online
centroid anomaly detection,” J. Mach. Learn. Res.,
vol. 13, no. 118, pp. 3681–3724, 2012.
A. F. Emmott, S. Das, T. Dietterich, A. Fern, and
W.-K. Wong, “Systematic construction of anomaly
detection benchmarks from real data,” in Proc.
Workshop Outlier Detection Description (KDD),
2013, pp. 16–21.
A. Emmott, S. Das, T. Dietterich, A. Fern, and
W.-K. Wong, “A meta-analysis of the anomaly
detection problem,” no. v2, pp. 1–35, 2016,
 
 
D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt,
and D. Song, “Natural adversarial examples,”
2019, arXiv:1907.07174. [Online]. Available:
 
W. Huang and P. Wei, “A PCB dataset for defects
detection and classiﬁcation,” 2019,
 
 
B. E. Bejnordi et al., “Diagnostic assessment of
deep learning algorithms for detection of lymph
node metastases in women with breast cancer,”
J. Amer. Med. Assoc., vol. 318, no. 22,
pp. 2199–2210, 2017.
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and
R. M. Summers, “ChestX-ray8: Hospital-scale
chest X-ray database and benchmarks on
weakly-supervised classiﬁcation and localization
of common thorax diseases,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., Jul. 2017,
pp. 2097–2106.
D. Zimmerer et al., Medical Out-of-Distribution
Analysis Challenge. Mar. 2020. [Online]. Available:
 
wiki/599515, doi: 10.5281/zenodo.3784230.
A. Dal Pozzolo, G. Boracchi, O. Caelen, C. Alippi,
and G. Bontempi, “Credit card fraud detection:
A realistic modeling and a novel learning
strategy,” IEEE Trans. Neural Netw. Learn. Syst.,
vol. 29, no. 8, pp. 3784–3797, Aug. 2018.
J. Ma, L. K. Saul, S. Savage, and G. M. Voelker,
“Identifying suspicious URLs: An application of
large-scale online learning,” in Proc. Int. Conf.
Mach. Learn., 2009, pp. 681–688.
N. Moustafa and J. Slay, “UNSW-NB15:
A comprehensive data set for network intrusion
detection systems,” in Proc. Mil. Commun. Inf.
Syst. Conf., 2015, pp. 1–6.
S. Ahmad, A. Lavin, S. Purdy, and Z. Agha,
“Unsupervised real-time anomaly detection for
streaming data,” Neurocomputing, vol. 262,
pp. 134–147, Nov. 2017.
N. Laptev, S. Amizadeh, and I. Flint, “Generic and
scalable framework for automated time-series
anomaly detection,” in Proc. 21th ACM SIGKDD
Int. Conf. Knowl. Discovery Data Mining,
Aug. 2015, pp. 1939–1947.
G. O. Campos et al., “On the evaluation of
unsupervised outlier detection: Measures,
datasets, and an empirical study,” Data Mining
Knowl. Discovery, vol. 30, no. 4, pp. 891–927,
Jul. 2016.
S. Rayana. . ODDS Library. [Online].
Available: 
R. Domingues, M. Filippone, P. Michiardi, and
J. Zouaoui, “A comparative evaluation of outlier
detection algorithms: Experiments and analyses,”
Pattern Recognit., vol. 74, pp. 406–421,
Feb. 2018.
D. Dua and C. Graff. . UCI Machine
Learning Repository. [Online]. Available:
 
A. P. Bradley, “The use of the area under the ROC
curve in the evaluation of machine learning
algorithms,” Pattern Recognit., vol. 30, no. 7,
pp. 1145–1159, Jul. 1997.
T. Fawcett, “An introduction to ROC analysis,”
Pattern Recognit. Lett., vol. 27, no. 8, pp. 861–874,
Jun. 2006.
X. Ding, Y. Li, A. Belatreche, and L. P. Maguire, “An
experimental evaluation of novelty detection
methods,” Neurocomputing, vol. 135,
pp. 313–327, Jul. 2014.
J. Davis and M. Goadrich, “The relationship
between precision-recall and ROC curves,” in
Proc. Int. Conf. Mach. Learn., 2006, pp. 233–240.
K. Boyd, K. H. Eng, and C. D. Page, “Area under
the precision-recall curve: Point estimates and
conﬁdence intervals,” in Proc. Eur. Conf. Mach.
Learn. Princ. Pract. Knowl. Discovery Databases,
2013, pp. 451–466.
M. T. Ribeiro, S. Singh, and C. Guestrin, “‘Why
should I trust you?’: Explaining the predictions of
any classiﬁer,” in Proc. Int. Conf. Knowl. Discovery
Data Mining, 2016, pp. 1135–1144.
R. R. Selvaraju, M. Cogswell, A. Das,
R. Vedantam, D. Parikh, and D. Batra, “Grad-CAM:
Visual explanations from deep networks via
gradient-based localization,” in Proc. IEEE Int.
Conf. Comput. Vis., Oct. 2017, pp. 618–626.
M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic
attribution for deep networks,” in Proc. Int. Conf.
Mach. Learn., vol. 70, 2017, pp. 3319–3328.
Z. Qi, S. Khorram, and F. Li, “Visualizing deep
networks by optimizing with integrated
gradients,” in Proc. CVPR Workshops, vol. 2, 2019,
pp. 11890–11898.
S. Bach, A. Binder, G. Montavon, F. Klauschen,
K.-R. Müller, and W. Samek, “On pixel-wise
explanations for non-linear classiﬁer decisions by
layer-wise relevance propagation,” PLoS ONE,
vol. 10, no. 7, Jul. 2015, Art. no. e0130140.
B. Micenková, R. T. Ng, X.-H. Dang, and I. Assent,
“Explaining outliers by subspace separability,” in
Proc. IEEE 13th Int. Conf. Data Mining, Dec. 2013,
pp. 518–527.
M. A. Siddiqui, A. Fern, T. G. Dietterich, and
W.-K. Wong, “Sequential feature explanations for
anomaly detection,” ACM Trans. Knowl. Discovery
Data, vol. 13, no. 1, pp. 1–22, Jan. 2019.
J. Kauffmann, K.-R. Müller, and G. Montavon,
“Towards explaining anomalies: A deep Taylor
decomposition of one-class models,” Pattern
Recognit., vol. 101, May 2020,
Art. no. 107198.
G. Litjens et al., “A survey on deep learning in
medical image analysis,” Med. Image Anal.,
vol. 42, pp. 60–88, Dec. 2017.
J. Kauffmann, M. Esders, G. Montavon, W. Samek,
and K.-R. Müller, “From clustering to cluster
explanations via neural networks,” 2019,
 
 
P. T. Huynh, A. M. Jarolimek, and S. Daye, “The
false-negative mammogram,” Radiographics,
vol. 18, no. 5, pp. 1137–1154, 1998.
M. Petticrew, A. Sowden, D. Lister-Sharp, and
K. Wright, “False-negative results in screening
programmes: Systematic review of impact and
implications,” Health Technol. Assess., vol. 4, no. 5,
pp. 1–120, 2000.
M. S. Pepe, The Statistical Evaluation of Medical
Tests for Classiﬁcation and Prediction. London,
U.K.: Oxford Univ. Press, 2003.
X.-H. Zhou, N. A. Obuchowski, and D. K. McClish,
Statistical Methods in Diagnostic Medicine, 2nd ed.
Hoboken, NJ, USA: Wiley, 2011.
R. O. Duda and P. E. Hart, Pattern Classiﬁcation
and Scene Analysis. Hoboken, NJ, USA: Wiley,
S. Theodoridis and K. Koutroumbas, Pattern
Recognition, 4th ed. New York, NY, USA:
Academic, 2009.
A. Lazarevic and V. Kumar, “Feature bagging for
outlier detection,” in Proc. Int. Conf. Knowl.
Discovery Data Mining, 2005, pp. 157–166.
H. V. Nguyen, H. H. Ang, and V. Gopalkrishnan,
“Mining outliers with ensemble of heterogeneous
detectors on random subspaces,” in DASFAA (1)
(Lecture Notes in Computer Science), vol. 5981.
Berlin, Germany: Springer, 2010, pp. 368–383.
M. L. Braun, J. M. Buhmann, and K.-R. Müller,
“On relevant dimensions in kernel feature spaces,”
J. Mach. Learn. Res., vol. 9, pp. 1875–1908,
Aug. 2008.
Y. Gal and Z. Ghahramani, “Dropout as a Bayesian
approximation: Representing model uncertainty
in deep learning,” in Proc. Int. Conf. Mach. Learn.,
vol. 48, 2016, pp. 1050–1059.
B. Lakshminarayanan, A. Pritzel, and C. Blundell,
“Simple and scalable predictive uncertainty
estimation using deep ensembles,” in Proc. Adv.
Neural Inf. Process. Syst., 2017, pp. 6402–6413.
A. Kendall and Y. Gal, “What uncertainties do we
need in Bayesian deep learning for computer
vision?” in Proc. Adv. Neural Inf. Process. Syst.,
2017, pp. 5574–5584.
Y. Ovadia et al., “Can you trust your model’s
uncertainty? Evaluating predictive uncertainty
under dataset shift,” in Proc. Adv. Neural Inf.
Process. Syst., 2019, pp. 13991–14002.
K. Bykov, M. M.-C. Höhne, K.-R. Müller,
S. Nakajima, and M. Kloft, “How much can i trust
you?—Quantifying uncertainties in explaining
neural networks,” 2020, arXiv:2006.09000.
[Online]. Available: 
2006.09000
S. Hido, Y. Tsuboi, H. Kashima, M. Sugiyama, and
T. Kanamori, “Statistical outlier detection using
direct density ratio estimation,” Knowl. Inf. Syst.,
vol. 26, no. 2, pp. 309–336, Feb. 2011.
M. Gutmann and A. Hyvärinen, “Noise-contrastive
estimation: A new estimation principle for
unnormalized statistical models,” in Proc. Int.
Conf. Artif. Intell. Statist., 2010, pp. 297–304.
R. A. Vandermeulen, R. Saitenmacher, and
A. Ritchie, “A proposal for supervised density
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
estimation,” in Proc. NeurIPS Pre-Registration
Workshop, 2020.
C. K. Chow, “An optimum character recognition
system using decision functions,” IRE Trans.
Electron. Comput., vol. EC-6, no. 4, pp. 247–254,
Dec. 1957.
C. Chow, “On optimum recognition error and
reject tradeoff,” IEEE Trans. Inf. Theory, vol. 16,
no. 1, pp. 41–46, Jan. 1970.
P. L. Bartlett and M. H. Wegkamp, “Classiﬁcation
with a reject option using a hinge loss,” J. Mach.
Learn. Res., vol. 9, no. Aug, pp. 1823–1840, 2008.
D. M. J. Tax and R. P. W. Duin, “Growing a
multi-class classiﬁer with a reject option,” Pattern
Recognit. Lett., vol. 29, no. 10, pp. 1565–1570,
Jul. 2008.
Y. Grandvalet, A. Rakotomamonjy, J. Keshet, and
S. Canu, “Support vector machines with a reject
option,” in Proc. Adv. Neural Inf. Process. Syst.,
2009, pp. 537–544.
C. Cortes, G. DeSalvo, and M. Mohri, “Learning
with rejection,” in Proc. Int. Conf. Algorithmic
Learn. Theory, 2016, pp. 67–82.
Y. Geifman and R. El-Yaniv, “Selective classiﬁcation
for deep neural networks,” in Proc. Adv. Neural Inf.
Process. Syst., 2017, pp. 4878–4887.
J. C. Platt, “Probabilistic outputs for support
vector machines and comparisons to regularized
likelihood methods,” Adv. Large Margin Classiﬁers,
vol. 10, no. 3, pp. 61–74, 1999.
C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger,
“On calibration of modern neural networks,” in
Proc. Int. Conf. Mach. Learn., vol. 70, 2017,
pp. 1321–1330.
T. DeVries and G. W. Taylor, “Learning conﬁdence
for out-of-distribution detection in neural
networks,” 2018, arXiv:1802.04865. [Online].
Available: 
K. Lee, H. Lee, K. Lee, and J. Shin, “Training
conﬁdence-calibrated classiﬁers for detecting
out-of-distribution samples,” in Proc. Int. Conf.
Learn. Represent., 2018.
C. Ni, N. Charoenphakdee, J. Honda, and
M. Sugiyama, “On the calibration of multiclass
classiﬁcation with rejection,” in Proc. Adv. Neural
Inf. Process. Syst., 2019, pp. 2586–2596.
A. Meinke and M. Hein, “Towards neural networks
that provably know when they don’t know,” in
Proc. Int. Conf. Learn. Represent. (ICLR), 2020.
D. J. C. MacKay and M. N. Gibbs, “Density
networks,” in Statistics and Neural Networks:
Advances at the Interface. USA: Oxford Univ. Press,
1998, pp. 129-146.
S. Liang, Y. Li, and R. Srikant, “Enhancing the
reliability of out-of-distribution image detection in
neural networks,” in Proc. Int. Conf. Learn.
Represent. (ICLR), 2018.
K. Lee, K. Lee, H. Lee, and J. Shin, “A simple
uniﬁed framework for detecting
out-of-distribution samples and adversarial
attacks,” in Proc. Adv. Neural Inf. Process. Syst.,
2018, pp. 7167–7177.
S. Choi and S.-Y. Chung, “Novelty detection via
blurring,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2020.
W. J. Scheirer, A. de Rezende Rocha, A. Sapkota,
and T. E. Boult, “Toward open set recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 35,
no. 7, pp. 1757–1772, Jul. 2013.
W. J. Scheirer, L. P. Jain, and T. E. Boult,
“Probability models for open set recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36,
no. 11, pp. 2317–2324, Nov. 2014.
A. Bendale and T. E. Boult, “Towards open set
deep networks,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., Jun. 2016, pp. 1563–1572.
L. Shu, H. Xu, and B. Liu, “DOC: Deep open
classiﬁcation of text documents,” in Proc. Conf.
Empirical Methods Natural Lang. Process., 2017,
pp. 2911–2916.
H. Zhang, A. Li, J. Guo, and Y. Guo, “Hybrid
models for open set recognition,” 2020,
 
 
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,
D. Erhan, I. Goodfellow, and R. Fergus, “Intriguing
properties of neural networks,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2014.
I. J. Goodfellow, J. Shlens, and C. Szegedy,
“Explaining and harnessing adversarial examples,”
in Proc. Int. Conf. Learn. Represent. (ICLR), 2015.
N. Carlini and D. Wagner, “Towards evaluating the
robustness of neural networks,” in Proc. IEEE
Symp. Secur. Privacy, May 2017, pp. 39–57.
F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow,
D. Boneh, and P. McDaniel, “Ensemble adversarial
training: Attacks and defenses,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2018.
B. Biggio and F. Roli, “Wild patterns: Ten years
after the rise of adversarial machine learning,”
Pattern Recognit., vol. 84, pp. 317–331, Dec. 2018.
A. Athalye, N. Carlini, and D. Wagner, “Obfuscated
gradients give a false sense of security:
Circumventing defenses to adversarial examples,”
in Proc. Int. Conf. Mach. Learn., vol. 80, 2018,
pp. 274–283.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu, “Towards deep learning models resistant
to adversarial attacks,” in Proc. Int. Conf. Learn.
Represent. (ICLR), 2018.
N. Carlini et al., “On evaluating adversarial
robustness,” 2019, arXiv:1902.06705. [Online].
Available: 
H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and
M. Jordan, “Theoretically principled trade-off
between robustness and accuracy,” in Proc. Int.
Conf. Mach. Learn., vol. 97, 2019, pp. 7472–7482.
A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom,
B. Tran, and A. Madry, “Adversarial examples are
not bugs, they are features,” in Proc. Adv. Neural
Inf. Process. Syst., 2019, pp. 125–136.
G. Shalev, Y. Adi, and J. Keshet,
“Out-of-distribution detection using multiple
semantic label representations,” in Proc. Adv.
Neural Inf. Process. Syst., 2018, pp. 7375–7385.
D. Hendrycks, K. Lee, and M. Mazeika, “Using
pre-training can improve model robustness and
uncertainty,” in Proc. Int. Conf. Mach. Learn.,
vol. 97, 2019, pp. 2712–2721.
A. R. Dhamija, M. Günther, and T. Boult,
“Reducing network agnostophobia,” in Proc. Adv.
Neural Inf. Process. Syst., 2018, pp. 9157–9168.
A.-K. Dombrowski, M. Alber, C. Anders,
M. Ackermann, K.-R. Müller, and P. Kessel,
“Explanations can be manipulated and geometry
is to blame,” in Proc. Adv. Neural Inf. Process. Syst.,
2019, pp. 13589–13600.
R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm,
and N. Elhadad, “Intelligible models for
healthcare: Predicting pneumonia risk and
hospital 30-day readmission,” in Proc. Int. Conf.
Knowl. Discovery Data Mining, 2015,
pp. 1721–1730.
W. Samek, G. Montavon, S. Lapuschkin,
C. J. Anders, and K.-R. Müller, “Toward
interpretable machine learning: Transparent deep
neural networks and beyond,” 2020,
 
 
J. D. Lee and K. A. See, “Trust in automation:
Designing for appropriate reliance,” Hum. Factors,
vol. 46, no. 1, pp. 50–80, 2004.
H. Jiang, B. Kim, M. Guan, and M. Gupta, “To
trust or not to trust a classiﬁer,” in Proc. Adv.
Neural Inf. Process. Syst., 2018, pp. 5541–5552.
Z. C. Lipton, “The doctor just won’t accept that!”
in Proc. NIPS Interpretable ML Symp., 2017.
B. Goodman and S. Flaxman, “European union
regulations on algorithmic decision-making and a
‘right to explanation,”’ AI Mag., vol. 38, no. 3,
pp. 50–57, 2017.
D. Amodei, C. Olah, J. Steinhardt, P. Christiano,
J. Schulman, and D. Mané, “Concrete problems in
AI safety,” 2016, arXiv:1606.06565. [Online].
Available: 
C. Richter and N. Roy, “Safe visual navigation via
deep learning and novelty detection,” in Proc.
Robot. Sci. Syst., 2017. [Online]. Available:
 
X. H. Dang, B. Micenková, I. Assent, and R. T. Ng,
“Local outlier detection with interpretation,” in
Proc. Eur. Conf. Mach. Learn. Princ. Pract. Knowl.
Discovery Databases, 2013, pp. 304–320.
X. Hong Dang, I. Assent, R. T. Ng, A. Zimek, and
E. Schubert, “Discriminative features for
identifying and interpreting outliers,” in Proc. Int.
Conf. Data Eng., Mar. 2014, pp. 88–99.
L. Duan, G. Tang, J. Pei, J. Bailey, A. Campbell,
and C. Tang, “Mining outlying aspects on numeric
data,” Data Mining Knowl. Discovery, vol. 29,
no. 5, pp. 1116–1151, Sep. 2015.
N. X. Vinh et al., “Discovering outlying aspects in
large datasets,” Data Mining Knowl. Discovery,
vol. 30, no. 6, pp. 1520–1555, Nov. 2016.
M. Macha and L. Akoglu, “Explaining anomalies
in groups with characterizing subspace rules,”
Data Mining Knowl. Discovery, vol. 32, no. 5,
pp. 1444–1480, Sep. 2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei, “ImageNet: A large-scale hierarchical
image database,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., Jun. 2009, pp. 248–255.
O. Russakovsky et al., “ImageNet large scale visual
recognition challenge,” Int. J. Comput. Vis.,
vol. 115, no. 3, pp. 211–252, Dec. 2015.
J. Wang, S. Sun, and Y. Yu, “Multivariate
triangular quantile maps for novelty detection,” in
Proc. Adv. Neural Inf. Process. Syst., 2019,
pp. 5061–5072.
J. Ren, P. J. Liu, E. Fertig, J. Snoek, R. Poplin,
M. Depristo, J. Dillon, and B. Lakshminarayanan,
“Likelihood ratios for out-of-distribution
detection,” in Proc. Adv. Neural Inf. Process. Syst.,
2019, pp. 14680–14691.
J. Serrà, D. Álvarez, V. Gómez, O. Slizovskaia,
J. F. Núñez, and J. Luque, “Input complexity and
out-of-distribution detection with likelihood-based
generative models,” in Proc. Int. Conf. Learn.
Represent. (ICLR), 2020.
G. E. Box, “Science and statistics,” J. Amer. Stat.
Assoc., vol. 71, no. 356, pp. 791–799,
A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries,
S. Wu, and C. Ré, “Snorkel: Rapid training data
creation with weak supervision,” Proc. VLDB
Endowment, vol. 11, no. 3, pp. 269–282, 2017.
Z.-H. Zhou, “A brief introduction to weakly
supervised learning,” Nat. Sci. Rev., vol. 5, no. 1,
pp. 44–53, Jan. 2018.
Y. Roh, G. Heo, and S. E. Whang, “A survey on
data collection for machine learning: A big
data—AI integration perspective,” IEEE Trans.
Knowl. Data Eng., early access, Oct. 8, 2019, doi:
10.1109/TKDE.2019.2946162.
T. Daniel, T. Kurutach, and A. Tamar, “Deep
variational semi-supervised novelty detection,”
2019, arXiv:1911.04971. [Online]. Available:
 
S. Das, W.-K. Wong, T. Dietterich, A. Fern, and
A. Emmott, “Discovering anomalies by
incorporating feedback from an expert,” ACM
Trans. Knowl. Discovery Data, vol. 14, no. 4,
pp. 1–32, Jul. 2020.
S. Nedelkoski, J. Bogatinovski, A. Acker,
J. Cardoso, and O. Kao, “Self-attentive
classiﬁcation-based anomaly detection in
unstructured logs,” 2020, arXiv:2008.09340.
[Online]. Available: 
2008.09340
K. Ouardini et al., “Towards practical
unsupervised anomaly detection on retinal
images,” in Domain Adaptation and Representation
Transfer and Medical Image Learning with Less
Labels and Imperfect Data. Cham, Switzerland:
Springer, 2019, pp. 225–234.
R. Shu, Y. Chen, A. Kumar, S. Ermon, and B. Poole,
“Weakly supervised disentanglement with
guarantees,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2020.
F. Locatello, B. Poole, G. Rätsch, B. Schölkopf,
O. Bachem, and M. Tschannen,
“Weakly-supervised disentanglement without
compromises,” in Proc. Int. Conf. Mach. Learn.,
2020, pp. 7753–7764.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
M. Mathieu, C. Couprie, and Y. LeCun, “Deep
multi-scale video prediction beyond mean square
error,” in Proc. Int. Conf. Learn. Represent. (ICLR),
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton,
“A simple framework for contrastive learning of
visual representations,” in Proc. Int. Conf. Mach.
Learn., 2020, pp. 10709–10719.
R. Zhang, P. Isola, and A. A. Efros, “Colorful image
colorization,” in Proc. Eur. Conf. Comput. Vis.,
2016, pp. 649–666.
C. Doersch, A. Gupta, and A. A. Efros,
“Unsupervised visual representation learning by
context prediction,” in Proc. Int. Conf. Comput.
Vis., Dec. 2015, pp. 1422–1430.
M. Noroozi and P. Favaro, “Unsupervised learning
of visual representations by solving jigsaw
puzzles,” in Proc. Eur. Conf. Comput. Vis., 2016,
pp. 69–84.
S. Gidaris, P. Singh, and N. Komodakis,
“Unsupervised representation learning by
predicting image rotations,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2018.
J. Winkens et al., “Contrastive training for
improved out-of-distribution detection,” 2020,
 
 
Y. M. Asano, C. Rupprecht, and A. Vedaldi,
“A critical analysis of self-supervision, or what we
can learn from a single image,” in Proc. Int. Conf.
Learn. Represent. (ICLR), 2020.
E. Nalisnick, A. Matsukawa, Y. W. Teh, and
B. Lakshminarayanan, “Detecting
out-of-distribution inputs to deep generative
models using typicality,” in Proc. Workshop
Bayesian Deep Learn. (NeurIPS), 2019.
R. Vershynin, High-Dimensional Probability: An
Introduction With Applications in Data Science,
vol. 47. Cambridge, U.K.: Cambridge Univ. Press,
A. Tong, G. Wolf, and S. Krishnaswamyt, “Fixing
bias in reconstruction-based anomaly detection
with Lipschitz discriminators,” in Proc. IEEE 30th
Int. Workshop Mach. Learn. for Signal Process.
(MLSP), Sep. 2020, pp. 1–6.
D. Krueger et al., “Out-of-distribution
generalization via risk extrapolation (REx),” 2020,
 
 
C. E. Shannon, “A mathematical theory of
communication,” Bell Syst. Tech. J., vol. 27, no. 3,
pp. 379–423, July, Oct. 1948.
R. Linsker, “Self-organization in a perceptual
network,” Computer, vol. 21, no. 3, pp. 105–117,
Mar. 1988.
A. J. Bell and T. J. Sejnowski, “An
information-maximization approach to blind
separation and blind deconvolution,” Neural
Comput., vol. 7, no. 6, pp. 1129–1159,
Nov. 1995.
R. D. Hjelm et al., “Learning deep representations
by mutual information estimation and
maximization,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2019.
T. Berger, “Rate-distortion theory,” in Wiley
Encyclopedia of Telecommunications. New York, NY,
USA: Wiley, 2003.
I. Higgins et al., “β-VAE: Learning basic visual
concepts with a constrained variational
framework,” in Proc. Int. Conf. Learn. Represent.
(ICLR), 2017.
A. Alemi, B. Poole, I. Fischer, J. Dillon,
R. A. Saurous, and K. Murphy, “Fixing a broken
ELBO,” in Proc. Int. Conf. Mach. Learn., vol. 80,
2018, pp. 159–168.
S. Park, G. Adosoglou, and P. M. Pardalos,
“Interpreting rate-distortion of variational
autoencoder and using model uncertainty for
anomaly detection,” 2020, arXiv:2005.01889.
[Online]. Available: 
2005.01889
W. Lee and D. Xiang, “Information-theoretic
measures for anomaly detection,” in Proc. IEEE
Symp. Secur. Privacy, 2001, pp. 130–143.
A. Høst-Madsen, E. Sabeti, and C. Walton, “Data
discovery and anomaly detection using
atypicality: Theory,” IEEE Trans. Inf. Theory,
vol. 65, no. 9, pp. 5302–5322,
Sep. 2019.
F. Pedregosa et al., “Scikit-learn: Machine learning
in Python,” J. Mach. Learn. Res., vol. 12,
pp. 2825–2830, Oct. 2011.
A. Paszke et al., “PyTorch: An imperative style,
high-performance deep learning library,” in Proc.
Adv. Neural Inf. Process. Syst., 2019,
pp. 8026–8037.
S. Sonnenburg et al., “The need for open source
software in machine learning,” J. Mach. Learn.
Res., vol. 8, pp. 2443–2466, Oct. 2007.
A B O U T T H E A U T H O R S
Lukas Ruff received the bachelor’s degree
in mathematical ﬁnance from the University
of Konstanz, Konstanz, Germany, in 2015,
and the joint master’s degree in statistics
from the Humboldt University of Berlin (HU
Berlin), Berlin, Germany, the Technische Universität Berlin (TU Berlin), Berlin, and Freie
Universität Berlin (FU Berlin), Berlin, in 2017.
He is currently working toward the Ph.D.
degree at the Machine Learning Group, TU Berlin.
Jacob R. Kauffmann received the bachelor’s and master’s degrees in computer
science from Technische Universität Berlin,
Berlin, Germany, in 2014 and 2017, respectively, where he is currently working toward
the Ph.D. degree with the Machine Learning
Robert A. Vandermeulen received the
master’s degree in electrical engineering,
the master’s degree in mathematics, and
electrical
engineering from the University of Michigan, Ann
Arbor, MI, USA, in 2012, 2015, and 2016,
respectively.
He is currently a Senior Researcher with
the Machine Learning Group, Technische
Universität Berlin, Berlin, Germany, and the Berlin Institute for the
Foundations of Learning and Data (BIFOLD), Berlin.
received the master’s degree in communication systems from
the École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, in 2009, and
the Ph.D. degree in machine learning from
the Technische Universität Berlin, Berlin,
Germany, in 2013.
He is currently a Senior Researcher with
the Machine Learning Group, Technische
Universität Berlin, and the Berlin Institute for the Foundations of
Learning and Data (BIFOLD), Berlin. His research interests include
explainable machine learning, deep neural networks, and unsupervised learning.
Dr. Montavon is a member of the ELLIS Unit Berlin and an Editorial
Board Member of Pattern Recognition. He was a recipient of the
2020 Pattern Recognition Best Paper Award.
Wojciech Samek (Member, IEEE) studied
computer science at the Humboldt University of Berlin (HU Berlin), Berlin, Germany,
from 2004 to 2010. He received the Ph.D.
degree (honors) from the Technische Universität Berlin (TU Berlin), Berlin, in 2014.
He is currently the Head of the Department of Artiﬁcial Intelligence, Fraunhofer
Heinrich Hertz Institute, Berlin. He is also an
Associate Faculty with the Berlin Institute for the Foundation of
Learning and Data (BIFOLD), Berlin, the ELLIS Unit Berlin, Berlin,
and the DFG Graduate School BIOQIC, Berlin.
Dr. Samek is an Editorial Board Member of Digital Signal Processing, PLoS ONE, Pattern Recognition, and the IEEE TRANSACTIONS ON
NEURAL NETWORKS AND LEARNING SYSTEMS (TNNLS) and an Elected
Member of the IEEE MLSP Technical Committee. He was a recipient
of multiple best paper awards, including the 2020 Pattern Recognition Best Paper Award, and a part of the MPEG-7 Part 17 Standardization. He was an organizer of various deep learning workshops.
He has been serving as an AC for the Annual Conference of the
North American Chapter of the Association for Computational Linguistics (NAACL) 2021.
PROCEEDINGS OF THE IEEE | Vol. 109, No. 5, May 2021
Ruff et al.: Unifying Review of Deep and Shallow Anomaly Detection
Technische Universität Berlin (TU Berlin),
Berlin, Germany, in 2011, and the University
California
(UC Berkeley),
Berkeley, CA, USA.
Humboldt University of Berlin (HU Berlin),
Berlin, from 2014 to 2017, and a Joint
Postdoctoral Fellow with the Courant Institute of Mathematical
Sciences, New York University, New York, NY, USA, and the
Memorial Sloan Kettering Cancer Center, New York. He has been
a Professor of computer science and machine learning with the
Technische Universität Kaiserslautern, Kaiserslautern, Germany,
since 2017. He is interested in the theory and algorithms of
statistical machine learning and its applications. His research
broad range of
topics and applications, where he
tries to unify theoretically proven approaches (e.g., based on
learning theory) with recent advances (e.g., in deep learning and
reinforcement learning). He has been working on, for example,
multimodal learning, anomaly detection, extreme classiﬁcation,
adversarial learning for computer security, and explainable AI.
Dr. Kloft was awarded the Google Most Inﬂuential Papers Award
in 2014. He has been serving as a Senior AC for the Association
for the Advancement of Artiﬁcial Intelligence (AAAI) Conference on
Artiﬁcial Intelligence since 2020 and the International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS) since 2020. He is
also an Associate Editor of the IEEE TRANSACTIONS ON NEURAL
NETWORKS AND LEARNING SYSTEMS (TNNLS).
Dietterich
(Member, IEEE)
received the B.A. degree from the Oberlin
College, Oberlin, OH, USA, in 1977, the M.S.
degree from the University of Illinois at
Urbana–Champaign, Champaign, IL, USA,
in 1979, and the Ph.D. degree from Stanford
University, Stanford, CA, USA, in 1984.
He is currently a Distinguished Professor
Emeritus with the School of Electrical Engineering and Computer Science, Oregon State University, Corvallis,
OR, USA. He is one of the pioneers of the ﬁeld of machine learning.
He has authored more than 200 refereed publications and two
books. His current research topics include robust artiﬁcial intelligence, robust human–AI systems, and applications in sustainability.
He has devoted many years of service to the research community.
Dr. Dietterich is a former President of the Association for the
Advancement of Artiﬁcial Intelligence and the Founding President
of the International Machine Learning Society. His other major roles
include an Executive Editor of the Machine Learning journal, a Co-
Founder of the Journal for Machine Learning Research, and the
Program Chair of the Association for the Advancement of Artiﬁcial
Intelligence (AAAI) Conference on Artiﬁcial Intelligence 1990 and
the Conference on Neural Information Processing Systems (NIPS)
2000. He also serves as one of the moderators for the cs.LG
category on arXiv.
Klaus-Robert
studied physics in Technische Universität
Karlsruhe,
Karlsruhe,
1984 to 1989. He received the Ph.D. degree
Technische
Universität Karlsruhe in 1992.
He has been a Professor of computer
science with Technische Universität Berlin
(TU Berlin), Berlin, Germany, since 2006.
In 2020 and 2021, he is on a sabbatical leave from TU Berlin and
with the Brain Team, Google Research, Berlin. He is also directing
and co-directing the Berlin Machine Learning Center, Berlin, and
the Berlin Big Data Center, Berlin, respectively. After completing
a postdoctoral position at GMD FIRST, Berlin, he was a Research
Fellow with The University of Tokyo, Tokyo, Japan, from 1994 to
1995. In 1995, he founded the Intelligent Data Analysis Group at
GMD-FIRST (later Fraunhofer FIRST) and directed it until 2008.
From 1999 to 2006, he was a Professor with the University of
Potsdam, Potsdam, Germany. His research interests are intelligent
data analysis and machine learning in the sciences (neuroscience
(speciﬁcally brain–computer interfaces), physics, and chemistry)
and in industrial applications.
Dr. Müller was an Elected Member of the German National
Leopoldina,
Brandenburg Academy of Sciences in 2017 and an External
Scientiﬁc Member of the Max Planck Society in 2017. In 2019 and
2020, he became a Highly Cited Researcher in the cross-disciplinary
area. Among others, he was awarded the Olympus Prize for Pattern
Recognition
Communication Award
in 2006, the Science Prize of Berlin by the Governing Mayor of
Berlin in 2014, the Vodafone Innovations Award in 2017, and the
2020 Best Paper Award in the Pattern Recognition journal.
Vol. 109, No. 5, May 2021 | PROCEEDINGS OF THE IEEE