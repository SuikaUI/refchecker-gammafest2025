HAL Id: hal-03753170
 
Submitted on 17 Aug 2022
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution - NonCommercial 4.0 International License
Approximation by superpositions of a sigmoidal function
G. Cybenko
To cite this version:
G. Cybenko.
Approximation by superpositions of a sigmoidal function.
Mathematics of Control,
Signals, and Systems, 1989, 2 (4), pp.303-314. ￿10.1007/BF02551274￿. ￿hal-03753170￿
Approximation by Superpositions of a Sigmoidal Function*
G. Cybenkot
Abstract. In this paper we demonstrate that finite linear combinations of com­
positions of a fixed, univariate function and a set of affine functionals can uniformly
approximate any continuous function of n real variables with support in the unit
hypercube; only mild conditions are imposed on the univariate function. Our
results settle an open question about representability in the class of single hidden
layer neural networks. In particular, we show that arbitrary decision regions can
be arbitrarily well approximated by continuous feedforward neural networks with
only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The
paper discusses approximation properties of other possible types of nonlinearities
that might be implemented by artificial neural networks.
Key words. Neural networks, Approximation, Completeness.
1. Introduction
A number of diverse application areas are concerned with the representation of
general functions of an n-dimensional real variable, x e IR", by finite linear combina­
tions of the form
L ap(yjx + 0),
where Yi E IR" and aj, e E IR are fixed. (yT is the transpose of y so that yT x is the inner
product of y and x.) Here the univariate function <J depends heavily on the context
of the application. Our major concern is with so-called sigmoidal a's:
as t-> +co,
as t-> -co.
Such.functions arise naturally in neural network theory as the activation function
of a neural node (or unit as is becoming the preferred term) [Ll], [RHM]. The main
result of this paper is a demonstration of the fact that sums of the form (1) are dense
in the space of continuous functions on the unit cube if <J is any continuous sigmoidal
t Center for Supercomputing Research and Development and Department of Electrical and Computer
Engineering, University of Illinois, Urbana, Illinois 61801, U.S.A.
This research was supported in part by NSF Grant DCR-619103, ONR Contract N000-86-G-0202
and DOE Grant DE-FG02-85ER25001.
function. This case is discussed in the most detail, but we state general conditions
on other possible u's that guarantee similar results.
The possible use of artificial neural networks in signal processing and control
applications has generated considerable attention recently [BJ, [G]. Loosely speak­
ing, an artificial neural network is formed from compositions and superpositions
of a single, simple nonlinear activation or response function. Accordingly, the
output of the network is the value of the function that results from that particular
composition and superposition of the nonlinearities. In particular, the simplest
nontrivial class of networks are those with one internal layer and they implement
the class of functions given by (1). In applications such as pattern classification [Ll]
and nonlinear prediction of time series [LF], for example, the goal is to select the
compositions and superpositions appropriately so that desired network responses
(meant to implement a classifying function or nonlinear predictor, respectively) are
This leads to the problem of identifying the classes of functions that can be
effectively realized by artificial neural networks. Similar problems are quite familiar
and well studied in circuit theory and filter design where simple nonlinear devices
are used to synthesize or approximate desired transfer functions. Thus, for example,
a fundamental result in digital signal processing is the fact that digital filters made
from unit delays and constant multipliers can approximate any continuous transfer
function arbitrarily well. In this sense, the main result of this paper demonstrates
that networks with only one internal layer and an arbitrary continuous sigmoidal
nonlinearity enjoy the same kind of universality.
Requiring that finite linear combinations such as (1) exactly represent a given
continuous function is asking for too much. In a well-known resolution of Hilbert's
13th problem, Kolmogorov showed that all continuous functions of n variables have
an exact representation in terms of finite superpositions and compositions of a small
number of functions of one variable [K], [L2]. However, the Kolmogorov represen­
tation involves different nonlinear functions. The issue of exact representability has
been further explored in [DS] in the context of projection pursuit methods for
statistical data analysis [H].
Our interest is in finite linear combinations involving the same univariate func­
tion. Moreover, we settle for approximations as opposed to exact representations.
It is easy to see that in this light, ( 1) merely generalizes approximations by finite
Fourier series. The mathematical tools for demonstrating such completeness prop­
erties typically fall into two categories: those that involve algebras of functions
(leading to Stone-Weierstrass arguments [A]) and those that involve translation
invariant subspaces (leading to Tauberian theorems [R2]). We give examples of
each of these cases in this paper.
Our main result settles a long-standing question about the exact class of decision
regions that continuous valued, single hidden layer neural networks can implement.
Some recent discussions of this question are in [HLl], [HL2], [MSJ], and [WL]
while [N] contains one of the early rigorous analyses. In [N] Nilsson showed that
any set of M points can be partitioned into two arbitrary subsets by a network with
one internal layer. There has been growing evidence through examples and special
cases that such networks can implement more general decision regions but a general
theory has been missing. In [MSJ] Makhoul et al. have made a detailed geometric
analysis of èome of the decisions regions that can be constructed exactly with a
single layer. By contrast, our work here shows that any collection of compact,
disjoint subsets of !Rn can be discriminated with arbitrary precision. That result is
contained in Theorem 3 and the subsequent discussion below.
A number of other current works are devoted to the same kinds of questions
addressed in this paper. In [HSW] Hornik et al. show that monotonic sigmoidal
functions in networks with single layers are complete in the space of continuous
functions. Carroll and Dickinson [CD] show that the completeness property can
be demonstrated constructively by using Radon transform ideas. Jones [J] out­
lines a simple constructive demonstration of completeness for arbitrary bounded
sigmoidal functions. Funahashi [F] has given a demonstration involving Fourier
analysis and Paley-Wiener theory. In earlier work [CJ, we gave a constructive
mathematical proof of the fact that continuous neural networks with two hidden
layers can approximate arbitrary continuous functions.
The main techniques that we use are drawn from standard functional analysis.
The proof of the main theorem goes as follows. We start by noting that finite
summations of the form (1) determine a subspace in the space of all continuous
functions on the unit hypercube of !Rn. Using the Hahn-Banach and Riesz Represen­
tation Theorems, we show that the subspace is annihilated by a finite measure. The
measure must also annihilate every term in (1) and this leads to the necessary
conditions ori u. All the basic functional analysis that we use can be found in [A],
[R2] for example.
The organization of this paper is as follows. In Section 2 we deal with prelimi­
naries, state, and prove the major result of the paper. Most of the technical details
of this paper are in Section 2. In Section 3 we specialize to the case of interest in
neural network theory and develop the consequences. Section 4 is a discussion of
other types of functions, u, that lead to similar results while Section 5 is a discussion
and summary.
2. Main Results
Let In denote then-dimensional unit cube, [O, l]n. The space of continuous functions
on In is denoted by C(ln) and we use II! II to denote the supremum (or uniform)
norm of an f E C(Jn). In general we use II · II to denote the maximum of a function
on its domain. The space of finite, signed regular Borel measures on In is denoted
by M(ln)· See [R2] for a presentation of these and other functional analysis construc­
tions that we use.
The main goal of this paper is to investigate conditions under which sums of the
G(x) = L a.iu(yJx + 8)
are dense in C(ln) with respect to the supremum norm.
Definition.
We say that u is discriminatory if for a measureµ e M(Jn)
r u(yT X + 0) dµ(x)
for all y e IR" and 0 e IR implies that µ = 0.
Definition.
We say that u is sigmoidal if
as t--. +co,
as t- -co.
Theorem 1. Let u be any continuous discriminatory function. Then finite sums of
G(x) = L ap(yJx + 0)
are dense in C(In). In other words, given any f e C(Jn) and e > 0, there is a sum, G(x),
of the above form, for which
I G(x) - f(x)I < e
for all x e In.
Let S c C(J") be the set of functions of the form G(x) as in (2). Clearly S is
a linear subspace of C(J"). We claim that the closure of Sis all of C(J").
Aséume that the closure of S is not all of C(J"). Then the closure of S, say R, is a
closed proper subspace of C(Jn). By the Hahn-Banach theorem, there is a bounded
linear functional on C(J"), call it L, with the property that L :F 0 but L(R) = L(S) = 0.
By the Riesz Representation Theorem, this bounded linear functional, L, is of the
L(h) = 1" h(x) dµ(x)
for someµ e M(J"), for all he C(J"). In particular, since u(yT x + 0) is in R for ally
and 0, we must have that
for ally and 0.
r u(yT X + 0) dµ(x)
However, we assumed that u was discriminatory so that this condition implies
that µ = 0 contradicting our assumption. Hence, the subspace S must be dense in
This demonstrates that sums of the form
G(x) = L ap(yJx + 01)
are dense in C(Jn) providing that u is continuous and discriminatory. The argument
used was quite general and can be applied in other cases as discussed in Section 4.
Now, we specialize this result to show that any continuous sigmoidal a of the form
discussed before, namely
as t-+ +co,
as t-+ -co,
is discriminatory. It is worth noting that, in neural network applications, continuous
sigmoidal activation functions are typically taken to be monotonically increasing,
but no monotonicity is required in our results.
Any bounded, measurable sigmoidal function, a, is discriminatory. In
particular, any continuous sigmoidal function is discriminatory.
To demonstrate this, note that for any x, y, (}, cp we have
a(A.(yTx + 8) + cp) -+ 0
for yTx + (} < 0
for yTx + (} > 0
for yTx+O=O
for allA..
Thus, the functions aA(x) = a(A.(yT x + 0) + cp) converge pointwise and boundedly
to the function
as A.-+ +co.
for yT x + (} > 0,
for yTx+8<0,
for yTx + (} = 0
Let Il,,.6 be the hyperplane defined by {xJyTx + (} = 0} and let H,,,6 be the open
half-space defined by {xJyT x + (} > O}. Then by the Lesbegue Bounded Convergence
Theorem, we have that
for all cp, (}, y.
0 = r <TA(x) dµ(x)
= r y(x) dµ(x)
= a(cp)µ(Il,,,6) + µ(H,,.6)
We now show that the measure of all half-planes being 0 implies that the measure
µ itself must be 0. This would be trivial ifµ were a positive measure but here it is not.
Fix y. For a bounded measurable functon, h, define the linear functional, F,
according to
F(h) = r h(yT x) dµ(x)
and note that F is a bounded functional on L 00(1R) since µ is a finite signed measure.
Leth be the indicator funcion of the interval (0, co) (that is, h(u) = 1 if u ê(} and
h(u) = 0 if u < lJ) so that
F(h) = r h(y T x) dµ(x)
= µ(Il,, -e> + µ(H,, -e) = 0.
Similarly, F(h) = 0 if h is the indicator function of the open interval (lJ, oo). By
linearity, F(h) = 0 for the indicator function of any interval and hence for any simple
function ( that is, sum of indicator functons of intervals). Since simple functons are
dense in L '°(IR) (see p. 90 of [A]) F = 0.
In particular, the bounded measurable functions s(u) = sin(m · u) and c(u) =
cos(m · u) give
F(s + ic) = r cos(mTx) + i sin(mTx) dµ(x)
= r exp(imTx) dµ(x)
for all m. Thus, the Fourier transform ofµ is 0 and soµ must be zero as well [R2,
p. 176]. Hence, q is discriminatory.
3. Application to Artificial Neural Networks
In this section we apply the previous results to the case of most interest in neural
network theory. A straightforward combination of Theorem 1 and Lemma 1 shows
that networks with one internal layer and an arbitrary continuous sigmoidal func­
tion can approximate continuous functions wtih arbitrary precision providing that
no constraints are placed on the number of nodes or the size of the weights. This is
Theorem 2 below. The consequences of that result for the approximation of decision
functions for general decision regions is made afterwards.
Theorem 2. Let q be any continuous sigmoidal function. Then finite sums of the
G(x) = L aiq(yJx + (Ji)
are dense in C(In). In other words, given any f E C(Jn) and e > 0, there is a sum, G(x),
of the above form, for which
I G(x) - f(x)I < e
for all x E In.
Combine Theorem 1 and Lemma 1, noting that continuous sigmoidals
satisfy the conditions of that lemma.
We now demonstrate the implications of these results in the context of decision
regions. Let m denote Lesbegue measure in In. Let P1, P2, ••• , Pk be a partition of
In into k disjoint, measurable subsets of In. Define the decision function, f, according
if and only if x e P1•
This function f can be viewed as a decision function for classification: if f(x) = j,
then we know that x E P1 and we can classify x accordingly. The issue is whether
such a decision function can be implemented by a network with a single internal
We have the following fundamental result.
Theorem 3. Let u be a continuous sigmoidal function. Let f be the decision f unc­
tion for any finite measurable partition of In. For any e > 0, there is a finite sum of the
G(x) = L a.1u(yJx + 8)
and a set D c Jn, so that m(D) ê 1 - e and
I G(x) - f(x)I < e
By Lusin's theorem [Rl], there is a continuous function, h, and a set D with
m(D) n I - e so that h(x) = f(x) for x E D. Now h is continuous and so, by Theorem
2, we can find a summation of the form of G above to satisfy IG(x) - h(x)I < e for
all x E In. Then for x ED, we have
I G(x) - f(x)I = I G(x) - h(x)I < e.
Because of continuity, we are always in the position of having to make some
incorrect decisions about some points. This result states that the total measure of
the incorrectly classified points can be made arbitrarily small. In light of this,
Thoerem 2 appears to be the strongest possible result of its kind.
We can develop this approximation idea a bit more by considering the decision
problem for a single closed set D c In. Then f(x) = 1 if x E D and f(x) = 0 otherwise;
f is the indicator function of the set D c In. Suppose we wish to find a summation
of the form (1) to approximate this decision function. Let
.1(x, D) = min{lx - yl , y ED}
so that .1(x, D) is a continuous function of x. Now set
{o e - .1(x, D)}
so that f.(x) = 0 for points x farther than e away from D while J.(x)
= 1 for x ED.
Moreover, f.(x) is continuous in x.
By Theorem 2, find a G(x) as in (1) so that IG(x) - J.(x)I < t and use this Gas an
approximate decision function: G(x) < t guesses that x E De while G(x) ê t guesses
that x ED. This decision procedure is correct for all x e D and for all x at a distance
at least e away from D. If x is within e distance of D, its classification depends on
the particular choice of G(x).
These observations say that points sufficiently far away from and points inside
the closed decision region can be classified correctly. In contrast, Theorem 3 says
that there is a network that makes the measure of points incorrectly classified as
smatt as desired but does not guarantee their location.
4. Results for Other Activation Functions
In this section we discuss other classes of activation functions that have approxima­
tion properties similar to the ones enjoyed by continuous sigmoidals. Since these
other examples are of somewhat less practical interest, we only sketch the corresponding proofs.
There is considerable interest in discontinuous sigmoidal functions such as hard
limiters (u(x) = 1 for x ê 0 and u(x) = 0 for x < 0). Discontinuous sigmoidal func­
tions are not used as often as continuous ones (because of the lack of good training
algorithms) but they are of theoretical interest because of their close relationship to
classical perceptrons and Gamba networks [MP].
Assume that u is a bounded, measurable sigmoidal function. We have an analog
of Theorem 2 that goes as follows:
Theorem 4. Let u be bounded measurable sigmoidal function. Then finite sums of
G(x) = L ap(yJx + 01)
are dense in L1(/n). In other words, given any f e L1(Jn) and e > 0, there is a sum,
G(x), of the above form for which
llG - fllL' = f IG(x) - f(x)I dx < e.
The proof follows the proof of Theorems 1 and 2 with obvious changes such as
replacing continuous functions by integrable functions and using the fact that L ""(Jn)
is the dual .of L 1 Un). The notion of being discriminatory accordingly changes to the
following: for h e L "'(Jn) the condition that
f u(yTx + O)h(x) dx =0
for ally and 0 implies that h(x) = 0 almost everywhere. General sigmoidal functions
are discriminatory in this sense as already seen in Lemma 1 because measures of
the form h(x) dx belong to M(Jn).
Since convergence in L1 implies convergence in measure [A], we have an analog
of Theorem 3 that goes as follows:
Theorem 5. Let u be a general sigmoidal function. Let f be the decision function
for any finite measurable partition of In. For any e > 0, there is a finite sum of the
G(x) = L aiu(yJx + Oi)
and a set D c: In, so that m(D) ê 1 - e and
I G(x) - f(x)I < e
A number of other possible activation functions can be shown to have approxima­
tion properties similar to those in Theorem 1 by simple use of the Stone-Weierstrass
theorem [A]. Those include the sine and cosine functions since linear combinations
of sin(mt) and cos(mt) generate all finite trigonometric polynomials which are
classically known to be complete in C(ln). Interestingly, the completeness of trigono­
metric polynomials was implicitly used in Lemma 1 when the Fourier transform's
one-to-one mapping property (on distributions) was used. Another classical example
is that of exponential functions, exp(mt), and the proof again follows from direct
application of the Stone-Weierstrass theorem. Exponential activation functions
were studied by Palm in [P] where their completeness was shown.
A whole other class of possible activation functions have completeness properties
in L1(ln) as a result of the Wiener Tauberian theorem [R2]. For example, suppose
that u is any L1(1R) function with nonzero integral. Then summations of the form
(1) are dense in L1(1Rn) as the following outline shows.
The analog of Theorem 1 carries through but we change C(ln) to L1(ln) and M(ln)
to the corresponding dual space L 00(ln). The analog of Theorem 3 holds if we can
show that an integrable u with nonzero integral is discriminatory in the sense that
u(yTx + O)h(x)dx =0
for all y and e implies that h = 0.
To do this we proceed as follows. As in Lemma 1, define the bounded linear
functional, F, on L 1 (IR) by
F(9) = r 9(yT x)h(x) dx.
(Note that the integral exists since it is over In and h is bounded. Specifically, if
9 E L1 (IR), then 9(yT x) E L1 (Jn) for any y.)
Letting 98 •• (t) = u(st + 0), we see that
F(9e .• ) = f a((sy)T x + O)h(x) dx = 0
so that F annihilates every translation and scaling of 9o, 1. Letf be the Fourier trans­
form off By standard Fourier transform arguments, 08,,(z) = exp(izO/s)O(z/s)/s.
Because of the scaling by s, the only z for which the Fourier transforms of all the
98,, can vanish is z = 0 but we are assuming that Jn u(t) dt = Oo. 1 (0) =F= 0. By the
Wiener Tauberian theorem [R2], the subspace generated by the functions 98,s is
dense in L 1 (IR). Since F(98,,)
= 0 we must have that F
= 0. Again, this implies that
F(exp(imt)) = f
exp(imt)h(t) dt = 0
for all m and so the Fourier transform of h is 0. Thus h itself is 0. (Note that although
the exponential function is not integrable over all of IR, it is integrable over bounded
regions and since h has support in In, that is sufficient.)
The use of the Wiener Tauberian theorem leads to some other rather curious
activation functions that have the completeness property in L1(In). Consider the
following activation function of n variables: a(x) = 1 if x lies inside a finite fixed
rectangle with sides parallel to the axes in !Rn and zero otherwise. Let U be an n x n
orthogonal matrix and ye !Rn. Now a(Ux + y) is the indicator funciton of an
arbitrarily oriented rectangle. Notice that no scaling of the rectangle is allowed­
only rigid-body motions in Euclidean space! We then have that summations of the
L a.ia(Uix + Yi)
are dense in L1(1R"). This follows from direct application of the Wiener Tauberian
theorem [R2] and the observation that the Fourier transform of a vanishes on a
mesh in IR" that does not include the origin. The intersection of all possible rotations
of those meshes is empty and so a together with its rotations and translations
generates a space dense in L 1 (IR").
This last result is closely related to the classical Pompeiu Problem [BST] and
using the results of [BST] we speculate that the rectangle in the above paragraph
can be replaced by any convex set with a corner as defined in [BST].
5. Summary
We have demonstrated that finite superpositions of a fixed, univariate function that
is discriminatory can uniformly approximate any continuous function of n real
variables with support in the unit hypercube. Continuous sigmoidal functions of
the type commonly used in real-valued neural network theory are discriminatory.
This combination of results demonstrates that any continuous function can be
uniformly approximated by a continuous neural network having only one internal,
hidden layer and with an arbitrary continuous sigmoidal nonlinearity (Theorem 2).
Theorem 3 and the subsequent discussion show in a precise way that arbitrary
decision functions can be arbitrarily well approximated by a neural network with
one internal layer and a continuous sigmoidal nonlinearity.
Table 1 summarizes the various contributions of which we are aware.
Function type and
transformations
Function space
References
u(yT x + 9), u continuous
This paper
sigmoidal, ye IR", 9 e IR
u(yTx + 9), u monotonic
[F], [HSW]
sigmoidal, ye IR", 9 e IR
u(yTx + 9), u
sigmoidal, ye IR", 9 e IR
u(yTx + 9), q E L'(IR)
This paper
J u(t) dt ,;. 0, ye IR", 9 e IR
u(yT x + 9), u continuous
sigmoidal, ye IR", 9 e IR
u(Ux + y), U e IR""",
This paper
y e IR", u indicator of a rectangle
u(tx + y), t e IR, u e L1(1R")
Wiener Tauberian
ye IR", J u(x) dx i' 0
theorem [R2]
While the approximating properties we have described are quite powerful, we
have focused only on existence. The important questions that remain to be answered
deal with fe;isibility, namely how many terms in the summation (or equivalently,
how many neural nodes) are required to yield an approximation of a given quality?
What properties of the function being approximated play a role in determining the
number of terms? At this point, we can only say that we suspect quite strongly that
the overwhelming majority of approximation problems will require astronomical
numbers of terms. This feeling is based on the curse of dimensionality that plagues
multidimensional approximation theory and statistics. Some recent progress con­
cerned with the relationship between a function being approximated and the number
of terms needed for a suitable approximation can be found in [MSJ] and [BH],
[BEHW], and [VJ for related problems. Given the conciseness of the results of this
paper, we believe that these avenues of research deserve more attention.
Acknowledgments.
The author thanks Brad Dickinson, Christopher Chase, Lee
Jones, Todd Quinto, Lee Rubel, John Makhoul, Alex Samarov, Richard Lippmann,
and the anonymous referees for comments, additional references, and improvements
in the presentation of this material.