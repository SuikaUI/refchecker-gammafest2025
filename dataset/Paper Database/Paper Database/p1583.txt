Machine Learning, 46, 291–314, 2002
c⃝2002 Kluwer Academic Publishers. Manufactured in The Netherlands.
A Simple Decomposition Method for Support
Vector Machines
CHIH-WEI HSU
 
CHIH-JEN LIN
 
Department of Computer Science and Information Engineering, National Taiwan University, Taipei 106,
Taiwan, Republic of China
Editor: Nello Cristianini
The decomposition method is currently one of the major methods for solving support vector machines.
An important issue of this method is the selection of working sets. In this paper through the design of decomposition
methods for bound-constrained SVM formulations we demonstrate that the working set selection is not a trivial
task. Then from the experimental analysis we propose a simple selection of the working set which leads to
faster convergences for difﬁcult cases. Numerical experiments on different types of problems are conducted to
demonstrate the viability of the proposed method.
support vector machines, decomposition methods, classiﬁcation
Introduction
The support vector machine (SVM) is a new and promising technique for classiﬁcation.
Surveys of SVM are, for example, Vapnik and Sch¨olkopf, Burges, and Smola
 . Given training vectors xi ∈Rn, i = 1, . . . ,l, in two classes, and a vector y ∈Rl
such that yi ∈{1, −1}, the support vector technique requires the solution of the following
optimization problem:
yi(wT φ(xi) + b) ≥1 −ξi,
i = 1, . . . ,l.
Training vectors xi are mapped into a higher (maybe inﬁnite) dimensional space by the
function φ. The existing common method to solve (1.1) is through its dual, a ﬁnite quadratic
programming problem:
2αT Qα −eT α
i = 1, . . . ,l
C.-W. HSU AND C.-J. LIN
where e is the vector of all ones, C is the upper bound of all variables, Q is an l by l positive
semideﬁnite matrix, Qi j ≡yi y j K(xi, x j), and K(xi, x j) ≡φ(xi)T φ(x j) is the kernel.
The difﬁculty of solving (1.2) is the density of Q because Qi j is in general not zero. In this
case, Q becomes a fully dense matrix so a prohibitive amount of memory is required to store
the matrix. Thus traditional optimization algorithms such as Newton, Quasi Newton, etc.,
cannot be directly applied. Several authors have proposed decomposition methods
to conquer this difﬁculty and reported good numerical results. Basically they separate the
index {1, . . . ,l} of the training set to two sets B and N, where B is the working set and
N = {1, . . . ,l}\B. If we denote αB and αN as vectors containing corresponding elements,
the objective value is equal to 1
B QBBαB −(eB −QBNαN)T αB + 1
N QNNαN −eT
At each iteration, αN is ﬁxed and the following sub-problem with the variable αB is solved:
B QBBαB −(eB −QBNαN)T αB
0 ≤(αB)i ≤C,
i = 1, . . . , q,
where [ QBB
QNN ] is a permutation of the matrix Q and q is the size of B. The strict decrease
of the objective function holds and under some conditions this method converges to an
optimal solution.
Usually a special storage using the idea of a cache is used to store recently used Qi j.
Hence the computational cost of later iterations can be reduced. However, the computational
time is still strongly related to the number of iterations. As the main thing which affects the
number of iterations is the selection of working sets, a careful choice of B can dramatically
reduce the computational time. This will be the main topic of this paper.
Instead of (1.1), in this paper, we decide to work on a different SVM formulation:
yi(wT φ(xi) + b) ≥1 −ξi,
i = 1, . . . ,l.
Its dual becomes a simpler bound-constrained problem:
2αT (Q + yyT )α −eT α
i = 1, . . . ,l.
This formulation was proposed and studied by Friess, Cristianini, and Campbell , and
Mangasarian and Musicant . We think it is easier to handle a problem without general
linear constraints. Later on we will demonstrate that (1.5) is an acceptable formulation in
terms of generalization errors though an additional term b2/2 is added to the objective
SIMPLE DECOMPOSITION METHOD
As (1.5) is only a little different from (1.2), in Section 2, naturally we adopt existing
working set selections for (1.5). Surprisingly we fail on such an extension. Then through
different experiments we demonstrate that ﬁnding a good strategy is not a trivial task.
That is, experiments and analysis must be conducted before judging whether a working set
selection is useful for one optimization formulation or not. Based on our observations, we
propose a simple selection which leads to faster convergences in difﬁcult cases. In Section
3, we implement the proposed algorithm as the software BSVM and compare it with SVMlight
 on problems with different size. After obtaining classiﬁers from training
data, we also compare error rates for classifying test data by using (1.2) and (1.5). We then
apply the proposed working set selection to the standard SVM formulation (1.2). Results
in Section 4 indicate that this selection strategy also performs very well. Finally in Section
5, we present discussions and conclusions.
The software BSVM is available at the authors’ homepage.1
Selection of the working set
Among existing methods, Osuna, Freund, and Girosi , and Saunders et al. ﬁnd
the working set by choosing elements which violate the KKT condition. Platt has a
special heuristic but his algorithm is mainly on the case when q = 2. A systematic way is
proposed by Joachims . In his software SVMlight, the following problem is solved:
∇f (αk)T d
yT d = 0, −1 ≤d ≤1,
if (αk)i = 0,
if (αk)i = C,
|{di | di ̸= 0}| ≤q,
where we represent f (α) ≡1
2αT Qα −eT α, αk is the solution at the kth iteration, ∇f (αk)
is the gradient of f (α) at αk. Note that |{di | di ̸= 0}| means the number of components
of d which are not zero. The constraint (2.1b) implies that a descent direction involving
only q variables is obtained. Then components of αk with non-zero di are included in the
working set B which is used to construct the sub-problem (2.7). Note that d is only used for
identifying B but not as a search direction. Joachims showed that the computational
time for solving (2.1) is mainly on ﬁnding the q/2 largest and q/2 smallest elements of
yi∇f (αk)i, i = 1, . . . ,l with yidi = 1 and yidi = −1, respectively. Hence the cost is at most
O(ql) which is affordable in his implementation.
Therefore, following SVMlight, for solving (1.5), a natural method to choose the working
set B is by a similar problem of (2.1):
∇f (αk)T d
if (αk)i = 0,
if (αk)i = C,
|{di | di ̸= 0}| ≤q,
C.-W. HSU AND C.-J. LIN
where f (α) becomes 1
2αT (Q + yyT )α −eT α. Note that SVMlight’s working set selection
violates the feasibility condition 0 ≤αk + d ≤C. Therefore, instead of (2.2a)–(2.2c), we
can consider other types of constraints such as
0 ≤αk + d ≤C, |{di | di ̸= 0}| = q.
The convergence of using (2.3) is guaranteed under the framework of Chang, Hsu, and Lin
 . For SVMlight’s selection (2.2), the convergence is more complicated and has just
been proved by Lin very recently.
Note that solving (2.2) is very easy by calculating the following vector:
min(∇f (αk)i, 0)
if (αk)i = 0,
−|∇f (αk)i|
if 0 < (αk)i < C,
−max(∇f (αk)i, 0)
if (αk)i = C.
Then B contains indices of the smallest q elements of v. Interestingly, for the boundconstrained formulation, solving (2.2) is the same as ﬁnding maximal violated elements of
the KKT condition. Note that elements which violate the KKT condition are
∇f (αk)i < 0
if (αk)i = 0,
∇f (αk)i > 0
if (αk)i = C,
∇f (αk)i ̸= 0
if 0 < (αk)i < C.
Hence, the q maximal violated elements are the smallest q elements of v. A similar relation
between the KKT condition of (1.2) and (2.1b) is not very easy to see as the KKT condition
of (1.2) involves the number b which does not appear in (2.1b). An interpretation is given
by Laskov where he considers possible intervals of b and select the most violated
points through end points of these intervals. These approaches are reasonable as intuitively
we think that ﬁnding the most violated elements is a natural choice.
However, unlike SVMlight, this selection of the working set does not perform well. In the
rest of this paper we name our implementation BSVM. In Table 1, by solving the problem
heart from the Statlog collection , we can see that
BSVM performs much worse than SVMlight.
SVMlight takes only 63 iterations but BSVM takes 590 iterations. In this experiment, we
use K(xi, x j) = e−∥xi−x j∥2/n, q = 10, and C = 1. Both methods use similar stopping criteria
and the initial solution is zero. The column “#C” presents the number of elements of αk
which are at the upper bound C. Columns yi = ± 1 report number of elements in B in two
different classes. Note that here for easy experiments, we use simple implementation of
SVMlight and BSVM written in MATLAB.
We observe that in each of the early iterations, all components of the working set are
in the same class. The decrease of objective values of BSVM is much slower than that
of SVMlight. In addition, the number of variables at the upper bound after seven SVMlight
iterations is more than that of BSVM while BSVM produces iterations with more free variables. We explain this observation as follows. First we make some assumptions for easy
description:
SIMPLE DECOMPOSITION METHOD
Problem heart: Comparison in early iterations.
1. α and α + d are solutions of the current and next iterations, respectively.
2. B is the current working set, where all elements are from the same class with yi = 1,
that is, yB = eB. In other words, we assume that at one iteration, the situation where all
elements of the working are in the same class happens.
3. αB = 0.
We will show that it is more possible that components with yi = −1 will be selected in
the next working set. Note that dN = 0 and since αB = 0, dB ≥0. Since d ≥0, yB = eB,
(Q + yyT )i j = yi y j(e−∥xi−x j∥2/n + 1) and (e−∥xi−x j∥2/n + 1) > 0,
((Q + yyT )d)i =
e−∥xi−x j∥2/n + 1
if yi = 1,
if yi = −1.
We also know that
∇f (α + d) = ∇f (α) + (Q + yyT )d.
In early iterations, most elements of α are at zero. If for those nonzero elements, vi of (2.4)
are not too small, (2.2) is essentially ﬁnding the smallest elements of ∇f (α + d). For the
next iteration, since vi = 0, i ∈B, elements in B will not be included the working set again.
For elements in N, from (2.5) and (2.6), we have
∇f (α + d)i ≥∇f (α)i,
if yi = 1,
∇f (α + d)i ≤∇f (α)i,
if yi = −1.
Therefore, if ((Q + yyT )d)i is not small, in the next iteration, elements with yi = −1 tend
to be selected because ∇f (α + d)i becomes smaller. For the example in Table 1, we really
C.-W. HSU AND C.-J. LIN
observe that the sign of most ∇f (α)i is changed in every early iteration. The explanation
is similar if yB = −1.
We note that (2.5) and (2.6) hold because of the RBF kernel. For another popular kernel:
the polynomial kernel (x T
i x j/n)d, if all attributes of data are scaled to [−1, 1], (x T
i x j/n)d +
1 > 0 still holds. Hence (2.5) and (2.6) remain valid.
Next we explain that when the above situation happens, in early iterations, very few
variables can reach the upper bound. Since we are solving the following sub-problem:
QBB + yB yT
QBN + yB yT
it is like that the following primal problem is solved:
yi(wT φ(xi) + b) ≥1 −
Qi jα j −ξi,
If components in B are in the same class, (2.8) is a problem with separable data.
Hence ξi = 0, i ∈B implies that αB are in general not equal to C. Thus the algorithm
has difﬁculties to identify correct bounded variables. In addition, the decrease of the
objective function becomes slow. On the other hand, the constraint yT d = 0 in (2.1) of
SVMlight provides an excellent channel for selecting the working set from two different classes. Remember that SVMlight selects the largest q/2 and smallest q/2 elements
of yi∇f (αk)i, i = 1, . . . ,l with yidi = −1 and yidi = 1, respectively. Thus when most
(αk)i are zero, di ≥0 so the largest elements of yi∇f (αk)i are mainly from indices with
yi = −1 and ∇f (αk)i < 0. Conversely, the smallest elements of yi∇f (αk)i are from data
with yi = 1 and ∇f (αk)i < 0. This can be seen in the columns of the number of yi = ± 1
in Table 1.
Another explanation is from the ﬁrst and second order approximations of the optimization
problem. If [αB, αN] is the current solution which is considered as a ﬁxed vector and d is
the variable, problem (1.3) is equivalent to solving
2dT QBBd −(eB −QBNαN −QBBαB)T d
0 ≤αB + d ≤C,
−(eB −QBNαN −QBBαB) = ∇f (α)B,
SIMPLE DECOMPOSITION METHOD
(2.1) is like to select the best q elements such that the linear part of (2.9) is minimized.
Similarly, (2.7) is equivalent to solving
QBB + yB yT
QBN + yB yT
QBB + yB yT
0 ≤αB + d ≤C.
Clearly a main difference between (2.9) and (2.10) is that yB involves in a term dT (yB yT
Bd)2 in the objective value of (2.10) but for (2.9), yB appears in one of its constraints:
Bd = 0. Therefore, since we are now using a linear approximation for selecting the working
set, for (2.10), dT (yB yT
B)d is a quadratic term which is not considered in (2.2). Thus (2.2)
does not know that (yT
Bd)2 should not be too large. On the other hand, for (2.1), yT
remains as a constraint so it is like that (yT
Bd)2 is implicitly minimized. In one word, (2.1)
and (2.2) both come from the linear approximation but one contains more information than
the other.
Based on this observation, we try to modify (2.2) by selecting a d which contains the
best q/2 elements with yi = 1 and the best q/2 elements with yi = −1. The new result is
in Table 2 where a substantial improvement is obtained. Now after seven iterations, both
algorithms reach solutions which have the same number of components at the upper bound.
Objective values are also similar.
However, in Table 2, BSVM still takes more iterations than SVMlight. We observe very
slow convergence in ﬁnal iterations. To improve the performance, we analyze the algorithm
in more detail. In ﬁgure 1, we present the number of free variables in each iteration. It can
be clearly seen that BSVM goes through points which have more free variables. Since the
weakest part of the decomposition method is that it cannot consider all variables together in
each iteration (only q are selected), a larger number of free variables causes more difﬁculty.
In other words, if a component is correctly identiﬁed at C or 0, there is no problem of
numerical accuracy. However, it is in general not easy to decide the value of a free variable
Problem heart: A new comparison in early iterations.
C.-W. HSU AND C.-J. LIN
Number of free variables (line: BSVM, dashed: SVMlight).
if all free variables are not considered together. Comparing to the working set selection
(2.1) of SVMlight, our strategy of selecting q/2 elements with yi = 1 and q/2 elements with
yi = −1 is not very natural. Therefore, in the middle of the iterative process more variables
are not correctly identiﬁed at the upper bound so the number of free variables becomes
larger. This leads us to conjecture that we should keep the number of free variables as
small as possible. A possible strategy to achieve this is by adding some free variables in the
previous iteration to the current working set.
The second observation is on elements of the working set. When we use (2.2) to select
B, in ﬁnal iterations, components of the working set are shown in Table 3. In Table 4,
working sets of running SVMlight are presented. From the last column of Table 3, it can
be seen that the working set of the kth iteration is very close to that of the (k + 2)nd
iteration. However, in Table 4, this situation is not that serious. For this example, at
Working sets of ﬁnal iterations: BSVM.
# in iter.+2
[16 30 53 23 5 228 130 200 7 90]
[168 24 245 197 119 51 134 108 7 90]
[16 23 30 53 266 200 130 228 51 90]
[24 195 25 119 197 108 134 7 51 90]
[16 30 53 5 23 200 228 130 51 7]
[197 245 24 119 195 108 90 134 51 228]
[16 30 23 53 5 200 130 7 134 108]
SIMPLE DECOMPOSITION METHOD
Working set in ﬁnal iterations: SVMlight.
# in iter.+2
[245 16 108 90 119 230 7 195 51 49]
[266 130 51 230 195 25 5 30 53 197]
[245 16 108 119 90 195 30 266 49 24]
[168 7 130 228 19 200 230 73 5 197]
[245 16 108 30 119 230 19 7 228 5]
[25 30 19 119 7 168 53 73 195 197]
[245 108 90 16 24 7 30 25 195 168]
ﬁnal solutions, there are 24 free variables by using (1.5) and 25 by (1.2). For BSVM, in
the second half iterations, the number of free variables is less than 30. Note that in ﬁnal
iterations, the algorithm concentrates on deciding the value of free variables. Since in each
iteration we select 10 variables in the working set, after the sub-problem (2.7) is solved,
gradient at these 10 elements become zero. Hence for the next iteration the solution of
(2.2) mainly comes from the other free variables. This explains why working sets of the
kth and (k + 2)nd iterations are so similar. Apparently a selection like that in Table 3 is
not an appropriate one. We mentioned earlier that the weakest part of the decomposition
method is that it cannot consider all variables together. Now the situation is like two groups
of variables are never considered together so the convergence is extremely slow.
Based on these observations, we propose Algorithm 2.1 for the selection of the working
set. We have q/2 elements from a problem of (2.2) but the other q/2 elements are from free
components with the largest −|∇f (αk)i|. Since free variables in the previous working set
satisfy ∇f (αk)i = 0, if there are not too many such elements, most of them are again included in the next working set. There are exceptional situations where all (αk) are at bounds.
When this happens, we choose q/2 best elements with yi = 1 and q/2 best elements with
yi = −1 following the discussion for results in Table 2.
Algorithm 2.1: Selection of the working set
Let r be the number of free variables at αk
If r > 0, then
Select indices of the largest min(q/2,r) elements in v, where (αk)i is free, into B
Select the (q −min(q/2,r)) smallest elements in v into B.
Select the q/2 smallest elements with yi = 1 and q/2 smallest elements with
yi = −1 in v into B.
The motivation of this selection is described as follows: consider minimizing f (α) =
2αT Aα −eT α, where A is a positive semideﬁnite matrix and there are no constraints.
This problem is equivalent to solving ∇f (α) = Aα −e = 0. If the decomposition method
is used, Bk−1 is the working set at the (k −1)st iteration, and A is written as [ ABk−1
C.-W. HSU AND C.-J. LIN
Number of free variables (line: BSVM, dashed: SVMlight).
we have ABk−1αk = e. Therefore, similar to what we did in (2.2), we can let Bk, the next
working set, contain the smallest q elements of ∇f (αk) = Aαk −e. In other words, elements
violate KKT condition are selected. Thus Bk will not include any elements in Bk−1 where
∇f (αk)i = 0, for all i ∈Bk−1. However, ABk−1αk −e = 0 only holds at the kth iteration.
When α is updated to αk+1, the equality fails again. Hence this is like a zigzaging process.
Fromthepointofviewofsolvingalinearsystem,wethinkthatconsideringsomeinequalities
and equalities together is a better method to avoid the zigzaging process. In addition, our
previous obervations suggest the reduction of the number of free variables. Therefore,
basically we select the q/2 most violated elements from the KKT condition and the q/2
most satisﬁed elements at which αi is free.
Using Algorithm 2.1, BSVM takes about 50 iterations which is fewer than that of SVMlight.
Comparing to the 388 iterations presented in Table 2, the improvement is dramatic. In
ﬁgure 2, the number of free variables in both methods are presented. It can be clearly seen
that in BSVM, the number of free variables is kept small. In early iterations, each time q
elements are considered and some of them move to the upper bound. For free variables,
Algorithm 2.1 tends to consider them again in subsequent iterations so BSVM has more
opportunities to push them to the upper bound. Since now the feasible region is like a box,
we can say that BSVM walks closer to walls of this box. We think that in general this is a good
property as the decomposition method faces more difﬁculties on handling free variables.
Computational experiments
In this section, we describe the implementation of BSVM and present the comparison between BSVM (Version 1.1) and SVMlight (Version 3.2). Results show that BSVM converges
SIMPLE DECOMPOSITION METHOD
faster than SVMlight for difﬁcult cases. The computational experiments for this section were
done on a Pentium III-500 using the gcc compiler.
SVMlight uses the following conditions as the termination criteria:
(Qα)i −1 + byi ≥−ϵ,
if αi < ϵa,
(Qα)i −1 + byi ≤ϵ,
if αi > C −ϵa,
−ϵ ≤(Qα)i −1 + byi ≤ϵ,
otherwise,
where ϵa = 10−12. To have a fair comparison, we use similar criteria in BSVM:
−ϵ ≤((Q + yyT )α)i −1 ≤ϵ,
if 0 < αi < C,
((Q + yyT )α)i −1 ≥−ϵ,
if αi = 0,
((Q + yyT )α)i −1 ≤ϵ,
if αi = C.
Note that now there is no b in the above conditions. For both SVMlight and BSVM, we set
We solve the sub-problem (2.7) by modifying the software TRON by Lin and Mor´e
 .2 TRON is designed for large sparse bound-constrained problems. Here the subproblem is a very small fully dense problem so we cannot directly use it.
As pointed out in existing work of decomposition methods, the most expensive step
in each iteration is the evaluation of the q columns of the matrix Q. In other words, we
maintain the vector Qα so in each iteration, we have to calculate Q(αk+1 −αk) which
involves q columns of Q. To avoid the recomputation of these columns, existing methods
use the idea of a cache where recently used columns are stored. In BSVM, we now have
a very simple implementation of the least-recently-used caching strategy. In the future,
we plan to optimize its performance using more advanced implementation techniques. For
experiments in this section, we use 160 MB as the cache size for both BSVM and SVMlight.
We test problems from different collections. Problems australian to segment are from
the Statlog collection . Problem fourclass is from Ho and Kleinberg
 . Problems adult1 and adult4 are compiled by Platt from the UCI “adult”
data set . Problems web1 to web7 are also from Platt. Note that all
problems from Statlog (except dna) and fourclass are with real numbers so we scale them to
[−1, 1]. Some of these problems have more than 2 classes so we treat all data not in the ﬁrst
class as in the second class. Problems dna, adult, and web are with binary representation so
we do not conduct any scaling.
We test problems by using RBF and polynomial kernels. For the RBF kernel, we use
K(xi, x j) = e−∥xi−x j∥2/n forStatlogproblemsand fourclass.Weuse K(xi, x j) = e−0.05∥xi−x j∥2
for adult and web problems following the setting in Joachims . For the polynomial
kernel, similarly we have K(xi, x j) = (x T
i x j/n)3 and K(xi, x j) = (0.05x T
i x j)3. For each
kernel, we test C = 1 and C = 1000. Usually C = 1 is a good initial guess. As it is difﬁcult
to ﬁnd out the optimal C, a procedure is to try different Cs and compare error rates obtained
by cross validation. In Saunders et al. , they point out that plotting a graph of error
rate on different Cs will typically give a bowl shape, where the best value of C is somewhere
C.-W. HSU AND C.-J. LIN
RBF kernel and C = 1.
SVMlight (without shrinking)
australian
43500 6188(6184) 1117 −5289.17 3036 484.36 6164(6152) 1059 −5241.41 1120 449.27
4781 1888(1655)
700 −1637.58
22.07 1888(1651)
700 −1637.56
24692 2021(927)
−939.70 2045 171.58 2084(909)
−939.71 2384 210.90
in the middle. Therefore, we think it may be necessary to solve problems with large Cs so
C = 1000 is tested here. In addition, the default C of SVMlight is 1000.
Numerical results using q = 10 are presented in Tables 5 to 8. The column “SV(BSV)”
represents the number of support vectors and bounded support vectors. The column “Mis.” is
the number of misclassiﬁed training data while the “Obj.” and “Iter.” columns are objective
values and the number of iterations, respectively. Note that here we present the objective
value of the dual (that is, (1.2) and (1.5)). We also present the computational time (in
seconds) in the last column. SVMlight implements a technique called “shrinking” which
drops out some variables at the upper bound during the iterative process. Therefore, it can
work on a smaller problem in most iterations. Right now we have not implemented similar
techniques in BSVM so in Tables 5–8 we present results by SVMlight without using this
shrinking technique. Except this option, we use all default options of SVMlight. Note that here
we do not use the default optimizer of SVMlight (version 3.2) for solving (1.2). Following the
suggestion by Joachims , we link SVMlight with LOQO to achieve
better stability. To give an idea of effects on using shrinking, in Table 9 we present results
of SVMlight using this technique. It can be seen that shrinking is a very useful technique
for large problems. How to effectively incorporate shrinking in BSVM is an issue for future
investigation.
From Tables 5 to 8, we can see that results obtained by BSVM, no matter number of
support vectors, number of misclassiﬁed data, and objective values, are very similar to
SIMPLE DECOMPOSITION METHOD
RBF kernel and C = 1000.
SVMlight (without shrinking)
australian
−302471.03
−302463.71
−179603.67
−179593.77
1487(1470)
−1188526.85
1491(1468)
−1188414.63
−279114.93
−279116.00
−260147.98
−260147.88
Polynomial kernel and C = 1.
SVMlight (without shrinking)
australian
2126(2109)
2131(2104)
1038(1019)
1040(1015)
17699(17698)
17700(17694)
2007(1716)
2011(1716)
2647(1027)
2755(1005)
Polynomial kernel and C = 1000.
SVMlight (without shrinking)
australian
−332791.89
−332791.88
−247281.87
−247279.97
−200995.69
−200995.34
−142509.32
−142508.03
4626(4613)
−3789837.97
4630(4606)
−3789835.86
−360720.39
−360720.34
−288125.96
−288125.21
−102103.18
−102102.62
−343024.14
−343023.74
C.-W. HSU AND C.-J. LIN
RBF kernel: Using SVMlight with shrinking.
australian
−302463.70
−179593.77
6164(6152)
1488(1467)
−1188414.66
1888(1651)
−279116.00
−260147.88
SIMPLE DECOMPOSITION METHOD
those by SVMlight. This suggests that using BSVM, a formula with an additional term b2/2
in the objective function, does not affect the training results much. Another interesting
property is that the objective value of BSVM is always smaller than that of SVMlight. This
is due to the properties that yT α = 0 in (1.2) and the feasible region of (1.2) is a subset
of that of (1.5). To further check the effectiveness of using (1.5), in Tables 10 and 11, we
present error rates by 10-fold cross validation or classifying original test data. Among all
problems, test data of dna, satimage, letter, and shuttle are available so we present error
rates by classifying them. Results suggest that for these problems, using (1.5) produces a
classiﬁer which is as good as that of using (1.2). In addition, we can see that the best rate
may happen at different values of C.
When C = 1, BSVM and SVMlight take about the same number of iterations. However,
it can be clearly seen that when C = 1000, both decomposition methods take many more
iterations. For most problems we have tested, not only those presented here, we observe
slow convergence of decomposition methods when C is large. There are several possible
reasons which cause this difﬁculty. We think that one of them is that when C is increased,
the number of free variables in the iterative process is increased. In addition, the number of
free variables at the ﬁnal solution is also increased. Though both the numbers of support
and bounded support vectors are decreased when C is increased, in many cases, bounded
variables when C = 1 become free variables when C = 1000. When C is increased, the
separating hyperplane tries to to ﬁt as many training data as possible. Hence more points
(i.e. more free αi) tend to be at two planes wT φ(x) + b = ±1. Since the decomposition
SVMlight: Accuracy rate by 10-fold cross validation or classifying test data.
australian
C.-W. HSU AND C.-J. LIN
BSVM: Accuracy rate by 10-fold cross validation or classifying test data.
australian
method has more difﬁculties on handling free variables, if the problem is ill-conditioned,
more iterations are required. As our selection of the working set always try to push free
variables to be bounded variables, the number of free variables is kept small. Therefore, the
convergence seems faster. It can be clearly seen that for almost all cases in Tables 6 and 8,
BSVM takes fewer iterations than SV Mlight.
Problem fourclass in Table 6 is the best example to show the characteristic of BSVM.
For this problem, at the ﬁnal solution, the number of free variables is small. In the iterative
process of decomposition methods, many free variables of iterates are in fact bounded
variables at the ﬁnal solution. BSVM considers free variables in subsequent iterations so
all bounded variables are quickly identiﬁed. The number of free variables is kept small so
the slow local convergence does not happen. However, SVMlight goes through an iterative
process with more free variables so it takes a lot more iterations. We use ﬁgure 3 to illustrate
this observation in more detail. It can be seen in ﬁgure 3(a) that the number of free variables
in SVMlight is increased to about 70 in the beginning. Then it is difﬁcult do identify whether
they should be at the bounds or not. Especially in ﬁnal iterations, putting a free variable on
a bound can take thousands of iterations. On the other hand, the number of free variables
of BSVM is always small (less than 50).
We also note that sometimes many free variables in the iterative process are still free in
the ﬁnal solution. Hence BSVM may pay too much attention on them or wrongly put them
as bounded variables. Therefore, some iterations are wasted so the gap between BSVM and
SVMlight is smaller. An example of this is adult problems.
SIMPLE DECOMPOSITION METHOD
Problem fourclass: number of SV(line) and free SV(dashed). (a) SVMlight; (b) BSVM.
Next we study the relation between number of iterations and q, the size of the working
set. Using the RBF kernel and C = 1000, results are in Tables 12 and 13. We ﬁnd out
that by using BSVM, number of iterations is dramatically decreased as q becomes larger.
On the other hand, using SVMlight, the number of iterations does not decrease much. Since
optimization solvers costs a certain amount of computational time in each iteration, this
result shows that SVMlight is only suitable for using small q. On the other hand, Algorithm
2.1 provides the potential of using different q in different situations.
Iterations and q: BSVM, C = 1000.
australian
Iterations and q: SVMlight, C = 1000.
australian
C.-W. HSU AND C.-J. LIN
Using algorithm 2.1 for standard SVM formulation
We can modify Algorithm 2.1 for standard SVM formulation (1.2).
Algorithm 4.1: Selection of the working set
Let r be the number of free variables at αk
Let q = q1 + q2, where q1 ≤r and q2 is an even number
For those 0 < (αk)i < C, select q1 elements with the smallest |∇f (αk)i + byi|
Select q2 elements from the rest of the index set by SVMlight’s working set selection.
Here we describe Algorithm 4.1 in a more general way. If under a similar setting of
Algorithm 2.1, we choose q1 ≈q2 ≈q/2. Now if (αk)i is a free variable and i is in the
previous working set, we have
∇f (αk)i + byi = 0.
This is different from the bounded case where we have ∇f (αk)i = 0. Therefore, unlike
Algorithm 2.1 where q/2 free elements with the smallest |∇f (αk)i| are chosen, here we
select the smallest |∇f (αk)i + byi|. Note that b is not a constant and must be recalculated
in each iteration.
As in previous sections we focus on using Algorithm 2.1 for bounded formulation (1.5),
we wonder whether the concept of reducing the number of free variables could also work
for the regular formulation (1.2). Note that the experience in Section 2 tells us that without experiments and analysis it is difﬁcult to judge whether a working set selection is
useful for one formulation or not. Thus in this section we will conduct some preliminary
experiments.
We implement Algorithm 4.1 with q = 10, q1 = 6, and q2 = 4. All other settings such as
stopping criteria are the same as in the previous section. Results using MATLAB for small
problems are in Tables 14 and 15. The RBF kernel is used so we compare them with Tables
5 and 6. It can be clearly seen that the number of iterations is smaller than that of SVMlight
and is competitive with BSVM.
BSVM: RBF kernel and C = 1.
australian
SIMPLE DECOMPOSITION METHOD
BSVM: RBF kernel and C = 1000.
australian
−302470.21
−179597.77
This experiment conﬁrms the importance of choosing free variables into the working set.
A full implementation using Algorithm 4.1 for (1.2) will be an important research topic. A
good linear-constrained optimization software must be chosen in order to solve (1.3).
Discussions and conclusions
From an optimization point of view, decomposition methods are like “coordinate search”
methods or “alternating variables method” . They have slow
convergences as the ﬁrst and second order information is not used. In addition, if the working
set selection is not appropriate, though the strict decrease of the objective value holds, the
algorithm may not converge . However, even with such
disadvantages, the decomposition method has become one of the major methods for SVM.
We think the main reason is that the decomposition method is efﬁcient for SVM in the
following situations:
1. C is small and most support vectors are at the upper bound. That is, there are not many
free variables in the iterative process.
2. The problem is well-conditioned even though there are many free variables.
For example, we do not think that adult problems with C = 1000 belong to the above cases.
They are difﬁcult problems for decomposition methods.
If for most applications we only need solutions of problems which belong to the above
situations, current decomposition methods may be good enough. Especially a SMO type
 algorithm has the advantage of not requiring any optimization solver. However,
if in many cases we need to solve difﬁcult problems (for example, C is large), more optimization knowledge and techniques should be considered. We hope that practical applications
will provide a better understanding on this issue.
Regarding the SVM formulation, we think (1.5) is simpler than (1.1) but with similar
quality for our test problems. In addition, in this paper we experiment with different implementation of the working set selection. The cost is always the same: at most O(ql) by
selecting some of the largest and smallest ∇f (αk). This may not be the case for regular SVM
formulation (1.1) due to the linear constraint yT α = 0. In SVMlight, the implementation is
C.-W. HSU AND C.-J. LIN
simple because (2.1) is very special. If we change constraints of (2.1) to 0 ≤αk + d ≤C,
the solution procedure may be more complicated. Currently we add b2/2 into the objective
function. This is the same as ﬁnding a hyperplane passing through the origin for separating
data [φ(xi), 1], i = 1, . . . ,l. It was pointed out by Cristianini and Shawe-Taylor that
the number 1 added may not be the best choice. Experimenting with different numbers can
be a future issue for improving the performance of BSVM.
From a numerical point of view, there are also possible advantages of using (1.5). When
solving (1.2), a numerical difﬁculty is on deciding whether a variable is at the bound or not
because it is generally not recommended to compare a ﬂoating-point number with another
one. For example, to calculate b of (1.2), we use the following KKT condition
yi(Qα)i + b −1 = 0
if 0 < αi < C.
Therefore, we can calculate b by (5.1) where i is any element in B. However, when implementing (5.1), we cannot directly compare αi to 0 or C. In SVMlight, a small ϵa = 10−12 > 0
is introduced. They consider αi to be free if αi ≥ϵa and αi ≤C −ϵa. Otherwise, if a
wrong αi is considered, the obtained b can be erroneous. On the other hand, if the bounded
formulation is used and appropriate solvers for sub-problem (2.7) are used, it is possible
to directly compare αi with 0 or C without needing an ϵa. For example, in Lin and Mor´e
 , they used a method called “project gradient” and in their implementation all values
at bounds are done by direct assignments. Hence it is safe to compare αi with 0 or C. To be
more precise, for ﬂoating-point computation, if αi ←C is assigned somewhere, a future
ﬂoating-point comparison between C and C returns true as they both have the same internal
representation.
In Platt , a situation was considered where the bias b of the standard SVM formulation (1.2) is a constant instead of a variable. Then the dual problem is a bound-constrained
formulation so BSVM can be easily modiﬁed for it.
In Section 2, we demonstrate that ﬁnding a good working set is not an easy task. Sometimes a natural method turns out to be a bad choice. It is also interesting to note that for
different formulations (1.2) and (1.5), similar selection strategies (2.1) and (2.2) give totally different performance. On the other hand, both Algorithms 2.1 and 4.1 are very useful
for (1.2) and (1.5), respectively. Therefore, for any new SVM formulations, we should be
careful when applying existing selections to them.
Finally we summarize some possible advantages of Algorithm 2.1:
1. It keeps the number of free variables as low as possible. This in general leads to faster
convergences for difﬁcult problems.
2. It tends to consider free variables in the current iteration again in subsequent iterations.
Therefore, corresponding columns of these elements are naturally cached.
Acknowledgments
ThisworkwassupportedinpartbytheNationalScienceCouncilofTaiwanviathegrantNSC
89-2213-E-002-013. The authors thank Chih-Chung Chang for many helpful discussions
SIMPLE DECOMPOSITION METHOD
and comments. Part of the software implementation beneﬁted from his help. They also
thank Thorsten Joachims, Pavel Laskov, John Platt, and anonymous referees for helpful
1. BSVM is available at 
2. TRON is available at