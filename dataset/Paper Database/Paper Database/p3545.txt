Transfer Re-identiﬁcation: From Person to Set-based Veriﬁcation
Wei-Shi Zheng1, Shaogang Gong2, and Tao Xiang2
1School of Information Science and Technology, Sun Yat-sen University, China
2School of Electronic Engineering and Computer Science, Queen Mary University of London, UK
 , , 
Solving the person re-identiﬁcation problem has become
important for understanding people’s behaviours in a multicamera network of non-overlapping views. In this work,
we address the problem of re-identiﬁcation from a set-based
veriﬁcation perspective. More speciﬁcally, we have a small
set of target people on a watch list (a set) and we aim to verify whether a query image of a person is on this watch list.
This differs from the existing person re-identiﬁcation problem in that the probe is veriﬁed against a small set of known
people but requires much higher degree of veriﬁcation accuracy with very limited sampling data for each candidate
in the set. That is, rather than recognising everybody in
the scene, we consider identifying a small set of target people against non-target people when there is only a limited
number of target training samples and a large number of
unlabelled (unknown) non-target samples available. To this
end, we formulate a transfer learning framework for mining discriminant information from non-target people data
to solve the watch list set veriﬁcation problem. Based on
the proposed approach, we introduce the concepts of multishot and one-shot veriﬁcations. We also design new criteria for evaluating the performance of the proposed transfer
learning method against the i-LIDS and ETHZ data sets.
1. Introduction
In recent years, matching people across disjoint camera
views in a multi-camera system, known as the person reidentiﬁcation problem , has gained
increasing interest. If a target disappears from one view
in a large area of public space covered by multiple nooverlapping cameras, person re-identiﬁcation aims to ﬁnd
the same person in another view at a different location/time.
Existing work on person re-identiﬁcation focuses on
ﬁnding distinctive feature representation and learning discriminant models. Popular feature representations of people’s appearance include color histogram , principal axis histogram , rectangle region histogram ,
Figure 1. On the left hand side is a small set of target person (watch
list) images, and on the right hand side is a large sampling set of
non-target person images whose labelled are not available.
graph representation , spatial co-occurrence representation , multiple feature based representation . Due
to the large intra-class and inter-class variations of people’s
appearance , these features may not be reliable under
changes in all conditions (e.g. view angle, lighting, occlusion). To address this problem, a number of feature quantiﬁcation methods have been proposed in the literature including Adaboost , Primal RankSVM and Probabilistic
Relative Distance Comparison (PRDC) .
However, there are two issues that remain unsolved:
• First, person re-identiﬁcation in existing literature is
always treated as a traditional image retrieval or recognition problem. It is assumed that both the gallery set
and probe/query set contain the same subjects. However, in a more realistic public environment, the gallery
set is much bigger than the probe set and it becomes
very difﬁcult to match against everybody in the gallery
set exhaustively. As the number of people increases,
their re-identiﬁcation accuracy decreases signiﬁcantly,
especially in unconstrained public spaces .
• Second, any person’s appearance is mostly only relatively stable for a short period of time in a real world
environment, e.g. a few hours in a day, as people can
dress differently on different days. This is signiﬁcantly
different from other retrieval and classiﬁcation problems in machine learning such as face recognition. As
the current person re-identiﬁcation problem is deﬁned
as a short-period-of-time recognition problem, there
are only limited samples available for learning a model capable of re-identifying every person reliably. A
learned statistical models for matching two images of
a person captured at two different camera views can be
easily overﬁtted.
Set-based Veriﬁcation. We propose a different approach
to solving the problem of person re-identiﬁcation in order
to overcome the above hurdles. In particular, we consider
a more realistic setting as opposed to the close-world setting used in previous works. Under this setting, person reidentiﬁcation becomes the problem of verifying a small set
of target people, which we call a watch list, rather than any
person individually in a scene, whilst the available model
training samples for the set is very small. More importantly,
we also consider the effect of non-target people on diversing
such a match. This transforms the person re-identiﬁcation
problem to a problem of verifying each query against the
watch list. As the appearance of different people can be
rather similar whilst the intra-class variations can be large
due to changes in camera view and lighting condition, it
is generally hard (if not impossible) for a fully automated
system to identify reliably the true targets without the help
of human operators. In this context, it is both more realistic and relevant to consider the re-identiﬁcation problem as
matching a small candidate set such that the true target candidates are mostly included there, despite the lack of exact
labels of each candidate in the set. To that end, we introduce
two set-based person veriﬁcation problems: one-shot veri-
ﬁcation and multi-shot veriﬁcation. One-shot veriﬁcation
is about association to a target person that aims to verify
whether a query image is associated with that target person.
Multi-shot veriﬁcation considers the problem of verifying
whether a query is within the watch list (set), therefore performing a joint one-shot veriﬁcation over all target people
in the set.
Our Approach. For performing veriﬁcation against nontarget people, sampling images of non-target people is important. While there are only limited samples for each target
person, one is more likely to have access to a large set of unlabelled person images that are not sourced from the target
people. For example, as shown in Figure 1, the left hand
side is a set of target people (watch list) and the right side
is a sampling set of non-target ones. The use of unlabelled
non-target person images aims to extract transferrable discriminant information between the images of target people
and those images sampled from the non-targets. To that end,
we formulate a set-based transfer learning framework for
veriﬁcation of each target person and their groupings (inthe-set vs. outside-the-set). Our proposed transfer learning approach is based on the latest bipartite ranking models
 . The proposed method further explores the useful relative comparison between target and non-target data and makes use of such information to enhance the bipartite ranking analysis between target data, resulting in a
more robust statistical model. Our approach is inﬂuenced
by the general concept of transfer learning which aims to
construct more robust statistical learning models that can
beneﬁt from shared knowledge between related domains especially when training data is sparse and imbalanced
 .
Contributions. The main contributions of this work are:
(1) We propose a new and more applicable approach to
solve the person re-identiﬁcation problem by introducing a
veriﬁcation process. The veriﬁcation process aims to verify a query against a watch list when the visual appearance
of the targets are relatively stable. (2) We formulate a novel transfer ranking approach for two types of veriﬁcations: multi-shot veriﬁcation and one-shot veriﬁcation. (3) We
propose new criteria for measuring the veriﬁcation performance of set-based person re-identiﬁcation.
2. The Veriﬁcation Learning Problem
In the following, we ﬁrst present the problem before develop the necessary transfer learning techniques.
2.1. Problem Formulation
Suppose given Nt limited target training data from mt different target people Ct
1, · · · , Ct
mt denoted by {zt
1, · · · , Ct
mt} and zt
i denotes the ith target
sample. Also, we are given a big sampling set of unlabelled
data from other people denoted by {zs
1, · · · , zs
Ns}, where
Ns >> Nt. The problem is how to learn a more robust
matching model by using these unlabelled non-target data
for performing person re-identiﬁcation on the small set of
target people against non-target people.
In some aspect, the above modelling can also be partially translated into separating a target set of people
1, · · · , Ct
mt from another group of people which are unlabelled. From the multi-task learning point of view, the new
modelling of person re-identiﬁcation here is how to perform
the task of group separation learning such that it beneﬁts for
the task of re-identiﬁcation on target people. By combining
these two tasks, the veriﬁcation process for target people
against non-target ones is therefore realised.
2.2. Mining Transferable Information
Veriﬁcation is different from recognition. For veriﬁcation, we are not only aiming to realise the separation between target data of different classes and the separation be-
Figure 2. There are three types of variations among a target set
and non-target data: 1) the target intra-class variations (red lines);
2) the target inter-class variations (green lines); 3) the inter-class
variation between target and non-target images (grey lines).
tween target data and non-target data, but also aiming to
infer the difference information between any pair of target data coming from different classes against the one between any pair of target sample and non-target sample, so as
to achieve better veriﬁcation performance. All these three
types of information will contribute to our bipartite ranking
based veriﬁcation approach.
More speciﬁcally, we wish to learn a score function denoted by f(x) which is a function of a difference vector x,
e.g. f(x) = wT x or the distance model in Sec. 2.4. Note
that the difference vector is computed as an absolute difference vector . For bipartite ranking, the score function
for the difference vector xp computed from a relevant pair
of images should be larger than the difference vector xn
computed from a related irrelevant pair of images, where
only one sample for computing xn is one of the two relevant samples for computing xp. The bipartite ranking is
then constrained by:
f(xp) ≥ρ + f(xn),
where ρ is a non-negative margin variable.
On target person image data, we wish to infer the intraclass and inter-class variations. To that end, we explore the
following target bipartite ranking:
(1) A score comparison between a pair of relevant target
person images (a red line in Figure 2) and a related pair
of the irrelevant target person images (a related green
line in Figure 2): Ot
), where xt,p
difference vector computed between a pair of relevant samples of the same target person and xt,n
difference vector from a related pair of irrelevant samples, i.e. only one sample for computing xt,n
the two relevant samples for computing xt,p
other is a sample from another target person. Denote
the set of all these comparison pairs by Ot = {Ot
Then, we wish to realise the following comparison:
i ) ≥ρ + f(xt,n
), for all Ot
Besides, although the non-target person image data are
not labelled, we know they are not from target people. Thus,
we can explore the inter-class variations between any pair
of target person image and non-target person image, and
then the following new comparison can be incorporated for
bipartite ranking learning:
(2) A score comparison between a pair of relevant target
person images (a red line in Figure 2) and a related
pair of the irrelevant person images between the target
person image and any non-target person image (any related grey line in Figure 2): Ots
is deﬁned in the point (1) and xts,n
is the difference vector between any sample for computing xt,p
and any un-target person image sample. Denote the set
of all these comparison pairs by Ots = {Ots
i }. Then,
we wish to realise the following comparison:
i ) ≥ρ + f(xts,n
), for all Ots
Furthermore, as the appearance of a person can change
dramatically across non-overlapping camera views, the objective of this process is to predict a candidate set for assisting human decision making. It is expected that the candidate set contains the target people, illustrated by the query
image being recognised as one of the targets on the watch
list shown by the solid red line in Figure 3. To that end, we
further incorporate the following comparison information.
(3) A score comparison between a pair of different target images (a green line in Figure 2) and a related
pair of irrelevant person images between the target
and any non-target (a related grey line in Figure 2):
), where xts,n
is the difference
vector between one of the target images for computing xt,n
and any non-target person image. Denote the
set of all these comparison pairs by Otsn = {Otsn
Then, we wish to realise the following comparison:
) ≥ρ + f(xts,n
), for all Otsn
Minimising the error (risk) function (Eqn. 4) of this comparison ensures that the score of the difference vector computed between a pair of different target person images
should be higher than the one computed between a pair of
a related target person image and any non-target person image. Such a learning can make target people in the feature
space more compact than the non-target ones.
By integrating the above three score comparisons, we
develop two transfer bipartite ranking methods, transfer RankSVM and transfer Probabilistic Relative Distance
Comparison (PRDC), as follows. They are signiﬁcantly different from RankSVM and PRDC, because the non-transfer
Figure 3. In traditional person re-identiﬁcation, if the query image
is matched to a wrong target person (as shown by the dash blue
line), then the output is incorrect. In a generalised set-based person
veriﬁcation by transfer learning, if the query image is matched to
one of the target person (as shown by the solid red line), then the
output is correct.
models only take the comparison information among target
data for bipartite ranking, namely just the point (1) in the
above analysis.
2.3. Transfer RankSVM
We model the score function f(x) by f(x) = wT x and
let ρ = 1 in Sec. 2.2. Noting that by using this score function in Eqns. (2), (3) and (4), we would ﬁnd that those score
comparison will aim to maximise the difference of two scores using a normalised vector
||w|| if we divide ||w|| on
both sides, where the difference is characterised by
Therefore, our transfer RankSVM aims to maximise such a
difference via minimising ||w|| by formulating the following optimisation problem:
s.t. wT xt,p
≥1 + wT xt,n
, for all Ot
≥1 + wT xts,n
, for all Ots
≥1 + wT xts,n
, for all Otsn
Since the feature dimension of person appearance is very
high (it is larger than 2700 in this work) and there are a
large amount of bipartite ranking, we also develop the corresponding risk based criterion of Eqn.(5) as follows:
2||w||2 + λ
max{0, 1 −wT (xt,p
max{0, 1 −wT (xt,p
max{0, 1 −wT (xt,n
where λ, β, η ≥0. In the above criterion, we actually integrate two more parameters β, η to balance the impact of
knowledge transfer from unlabelled data.
2.4. Transfer PRDC
Different from transfer RankSVM which aims to maximise the score difference (i.e. the margin
||w||), transfer
PRDC is to further quantify the second-order mutual information between different features and aims to maximise the
probability of the realisation of all the comparisons detailed
in Sec. 2.2.
Let f(x) = −d(x) and ρ = 0 in Sec. 2.2, where d(x) is
a distance function deﬁned by d(x) = xT WWT x for some
matrix W = [w1, · · · , wℓ]. Then transfer PRDC learns
this score function by maximising the following conﬁdence
i ) < d(xt,n
i ) < d(xts,n
) < d(xts,n
P(d(x) < d(x′)) =
d(x) −d(x′)
Maximising this conﬁdence model is equivalent to the following optimisation problem:
W = arg min
W r(W), s.t. wT
i wj = 0, ∀i ̸= j
i∈Ot log(1 + exp
i ||2 −||WT xt,n
i ∈Ots log(1 + exp
i ||2 −||WT xts,n
log(1 + exp
||2 −||WT xts,n
We call the above model as Transfer PRDC.
2.5. Optimisation Algorithm
Due to the limit of space, the optimisation algorithms
of the proposed transfer RankSVM and transfer PRDC are
not detailed here. Gradient based algorithms for transfer
RankSVM and transfer PRDC, which are able to handle the
large-scale computation, can be developed similarly as suggested in , respectively.
3. Multi-shot and One-shot Veriﬁcations
After learning the transfer models in Sec. 2.3 and 2.4
where the effect of non-target people has been considered,
we can perform veriﬁcation for person re-identiﬁcation.
The two following types of veriﬁcation are considered.
Multi-shot Veriﬁcation. Given a query person image I,
the multi-shot veriﬁcation aims to verify whether this image
comes from one of the target person on the watch list. That
is multi-shot veriﬁcation performs a one-to-set verifying.
One-shot Veriﬁcation.
Given a query person image I,
the one-shot veriﬁcation aims to verify whether this image
comes from target person Ct
k and does not come from the
others (including the other target people).
The difference between these two veriﬁcations is that
multi-shot veriﬁcation tells whether the detected person is
within our interest but does not perform veriﬁcation on the
person identity of any query image. The one-shot veriﬁcation performs the latter but would not be able to measure
explicitly the probability that the person of the query image is on the watch list. Their relation is similar to the relation between joint probability density function and marginal
probability density function.
4. Experiments
4.1. Settings
Datasets. We use both the i-LIDS Multiple-Camera Tracking Scenario (MCTS) dataset and the ETHZ
dataset for evaluation. The i-LIDS MCTS dataset
consists of 119 people with a total 476 person images with
an average of 4 images, which are captured by multiple nonoverlapping cameras indoor at a busy airport arrival hall.
Many of these images undergo large illumination change
and are subject to occlusions. The ETHZ dataset consists of 146 people and 8555 image in total, where images of
person were taken from a moving camera in a busy street
scene. Please note that, the labels of all non-target data used
in our experiment are assumed to be unknown. The popular
VIPeR dataset was not used here because there is only two samples for each person. In our experiments, only
one training image for each person (where the other one is
for testing) is not enough for implementing RankSVM and
PRDC for comparison.
Competitors & Parameter Settings.
We compare our
proposed transfer RankSVM and transfer PRDC with
RankSVM and PRDC which have been shown
to be more effective than any other existing person reidentiﬁcation methods. Through all experiments, we ﬁxed
the parameters β, η to be 0.5 in Eqns. (9) and (6) for the
two proposed transfer learning techniques. We also ﬁx the
λ in both RankSVM and Transfer RankSVM to be 0.005.
This can in general be learned through cross-validation .
However, the value in our experiment is more difﬁcult to be
determined by cross-validation as we are considering person re-identiﬁcation on a small set of target people with a
handful of training samples, against other unlabelled nontarget people.
Feature Representation. The popular histogram based feature representation for person re-identiﬁcation is adopted,
which is a mixture of colour features (including RGB, Y-
CbCr, HSV color) and texture feature pattern (extracted by
Schmid and Gabor ﬁlters) . Each person image is represented by a feature vector in a 2784 dimensional
feature space.
4.2. Veriﬁcation Task and Performance Metrics
The objective here is to verify whether a query person
image comes from the people on the watch list with the presence of non-target person images during the veriﬁcation.
More speciﬁcally, for each dataset, we randomly selected
all images of p people (classes) to set up the target data set
and the rest to constitute the non-target data set. Then, for
the target data set, we randomly divided it into a training
set and a testing set, where q images of each person were
randomly selected for training. We also randomly divided
the non-target data set into training and testing sets. Such
a random division is done by person; that is the images of
half of the non-target people in the data set were used as
training non-target person images and all the rest images
were selected as testing non-target images so that there is no
overlap of people between training non-target images and
testing non-target images. It is important to note that we assume the labels of these non-target data are unknown. The
experiment was conducted 10 times and the average veriﬁcation rates were then computed. Through the experiment, we tested two scenarios by setting the number of target
people to 6 and 10 (i.e. p = 6 or p = 10) respectively, and
randomly select two images (i.e. q = 2) as training samples
for each target person.
Since a lot images of non-target people were mixed with
the target ones during the veriﬁcation, we need to quantify
the performance how well a true target has been veriﬁed,
how bad a false target has passed through the veriﬁcation
and their relations. Therefore, we introduce the true target
rate (TTR) and false target rate (FTR) as follows:
True Target Recognition(TTR)
False Target Recognition(FTR)
T Q = {query target images from target people};
NT Q = {query non-target images from non-target people};
T T Q = {query target images that
are veriﬁed as one of the target people};
FNT Q = {query non-target images that
are veriﬁed as one of the target people}.
Note that for performing one-shot veriﬁcation for each target person (see Sec. 3), the above metrics can still be used,
and in this case the non-target people mean any other person
FTR = 30% FTR = 50% FTR = 30% FTR = 50%
Transfer RankSVM
Transfer PRDC
Table 1. Multi-shot Veriﬁcation on 6 People (p = 6): True target rate (%) against False target rate for one-shot learning
FTR = 30% FTR = 50% FTR = 30% FTR = 50%
Transfer RankSVM
Transfer PRDC
Table 2. Multi-shot Veriﬁcation on 10 People (p = 10): True target rate (%) against False target rate on i-LIDS and ETHZ
FTR = 30% FTR = 50% FTR = 30% FTR = 50%
Transfer RankSVM
Transfer PRDC
Table 3. One-shot Veriﬁcation on 6 People (p = 6): True target rate (%) against False target rate on i-LIDS and ETHZ
except that target person.
In our experiments, the score function f(x) which is
speciﬁed in Sec. 2 is used to determine the rank of matching. A value s is used to threshold these scores and therefore a curve depicting the TTR value against FTR value is
reported for each method by changing the threshold value
s. We will also report the TTR when the FTR is ﬁxed.
This is similar to the ROC curve in face veriﬁcation, but
it differs in that we are also caring about the veriﬁcation
on whether the query image is belonging to one of the target people (i.e. multi-shot veriﬁcation), because person reidentiﬁcation could rely much more on the processing of
human operators in real scenario than face recognition due
to large intra- and inter- class variations.
4.3. Veriﬁcation Results
Transfer vs. Non-Transfer. We compare non-transfer person re-identiﬁcation with transfer person re-identiﬁcation
on multi-shot veriﬁcation and one-shot veriﬁcation, i.e.
RankSVM vs. Transfer RankSVM and PRDC vs. Transfer PRDC.
For multi-shot veriﬁcation as shown in Tables 1 and 2
and Figure 4, both transfer PRDC and transfer RankSVM
have achieved signiﬁcant better performance than their nontransfer versions PRDC and RankSVM, respectively. For
example, at least about 10 true mating rate higher for transfer PRDC against PRDC when FTR = 0.3 on i-LIDS and
ETHZ, and more than about 25 true mating rate higher for
Transfer RankSVM against RankSVM when FTR = 0.3 on
i-LIDS and ETHZ. For one-shot veriﬁcation as shown in Tables 3 and 4 and Figure 5, Transfer RankSVM consistently
outperforms RankSVM and Transfer PRDC still performs
better than PRDC on i-LIDS, while slightly negative transfer has been observed for Transfer PRDC on ETHZ (i.e.
slightly lower performance).
Nevertheless, from the multi-shot veriﬁcation to oneshot veriﬁcation, the transfer learning models perform
much more stably and reliably than the non-transfer ones.
Our results, especially the comparison between Transfer
RankSVM and RankSVM, also show that without learning
from unlabelled data, the bipartite ranking may largely fail
for person re-identiﬁcation where un-target instances are involved during the veriﬁcation.
Transfer PRDC vs. Transfer RankSVM. We compare
the two proposed transfer learning algorithms here for veriﬁcation. As shown in Tables 1 and 2 and Figure 4, for
multi-shot veriﬁcation transfer PRDC consistently outperform transfer RankSVM with a large margin (e.g. more
than 15% increase in true matching rate and 12% in true
matching rate at FTR=0.3 on i-LIDS and ETHZ respectively). However, for one-shot veriﬁcation as shown in Tables
3 and 4 and Figure 5, transfer RankSVM performs better than transfer PRDC. This may suggest transfer PRD-
C is more suitable for a joint veriﬁcation, whilst transfer
RankSVM, which explicitly maximises the marginal information, is more suitable for a one-to-one veriﬁcation.
FTR = 30% FTR = 50% FTR = 30% FTR = 50%
Transfer RankSVM
Transfer PRDC
Table 4. One-shot Veriﬁcation on 10 People (p = 10): True target rate (%) against False target rate
False Target Rate (%)
True Target Rate (%)
Transfer PRDC
Transfer RankSVM
False Target Rate (%)
True Target Rate (%)
Transfer PRDC
Transfer RankSVM
Figure 4. Multi-shot Veriﬁcation on 10 people: True Target Rate vs. False Target Rate on i-LIDS and ETHZ
Multi-shot veriﬁcation
Transfer RankSVM 44.50 49.67 53.25 56.69 57.53
Transfer PRDC
69.31 68.25 68.89 68.02 68.32
One-shot veriﬁcation
Transfer RankSVM 84.40 84.23 84.33 84.05 83.50
Transfer PRDC
76.06 75.32 74.65 75.02 74.87
Table 5. True target rate (%) at FTR = 0.3: Parameter Validation
on i-LIDS (p = 6).
Multi-shot Veriﬁcation vs. One-shot Veriﬁcation. It is
clear that the one-shot veriﬁcation always achieved much
higher true target rate as compared to the one for multi-shot
veriﬁcation. However, as analysed in Sec. 3, the one-shot
veriﬁcation is a one-to-one veriﬁcation and cannot make
a joint veriﬁcation explicitly to say whether the person of
a query image is one of the several people needed to be
watched. It is because one-shot veriﬁcation still remains
the uncertainty whether the query is also any of the other
target people on the watch list. The multi-shot veriﬁcation
is therefore to make a prediction by joining all these uncertainty. From another point of view, non-target people may
also have better chances to get access in the multi-shot veri-
ﬁcation case. Thus the relative lower true target rate will be
observed for multi-shot veriﬁcation. It is like the case that
the value of a joint probability density function is lower than
the one of each marginal probability density function. In addition, as shown above, different transfer learning methods
would be suitable for different types of veriﬁcation.
Stability of Parameters. Note that in preceding experiments, we have ﬁxed the parameters β, η to 0.5 in Eqns. (9)
and (6) for both proposed models. We ﬁnally evaluate the
stability of this setting in Table 5. Due to the limit of space,
this validation was only conducted on i-LIDS over 6 target
people (i.e. p = 6). As shown, both transfer PRDC and
transfer RankSVM perform stably from β = η = 0.1 to
β = η = 0.9, especially around 0.5.
5. Conclusion
We have redeﬁned the person-reidentiﬁcation problem
as a set-based veriﬁcation problem to reﬂect a more realistic real-world application requirement. To the best of our
knowledge, it is the ﬁrst attempt on addressing the person
re-identiﬁcation problem from this new perspective. Two
transfer person re-identiﬁcation techniques namely transfer PRDC and transfer RankSVM have been developed to
solve this problem in the context of transfer learning. Based
on these two models, two types of veriﬁcation for person
re-identiﬁcation have been introduced, namely multi-shot
veriﬁcation and one-shot veriﬁcation. Our results indicate
that mining useful discriminant knowledge from unlabelled
non-target people is very useful for performing veriﬁcation
on a small set of target people who are on the watch list.
Acknowledgment
W.-S. Zheng is supported by Natural Science Foundation Of China (No.
61102111), Specialized Research
Fund for the Doctoral Program of Higher Education
(20110171120051), the 985 project in Sun Yat-sen Uni-
False Target Rate (%)
True Target Rate (%)
Transfer PRDC
Transfer RankSVM
False Target Rate (%)
True Target Rate (%)
Transfer PRDC
Transfer RankSVM
Figure 5. One-shot Veriﬁcation on 10 people: True Target Rate vs. False Target Rate on i-LIDS and ETHZ
versity with grant no.
35000-3181305 and the NSFC-
Guangdong (U0835005). S. Gong and T. Xiang are supported by the UK Home Ofﬁce CONTEST Programme and
Vision Semantics Limited.