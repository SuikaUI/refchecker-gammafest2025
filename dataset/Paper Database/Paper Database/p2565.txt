HAL Id: hal-00830491
 
Submitted on 12 Jun 2013
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Image Classification with the Fisher Vector: Theory and
Jorge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek
To cite this version:
Jorge Sanchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek. Image Classification with the
Fisher Vector: Theory and Practice. International Journal of Computer Vision, 2013, 105 (3), pp.222-
245. ￿10.1007/s11263-013-0636-x￿. ￿hal-00830491v2￿
International Journal of Computer Vision manuscript No.
(will be inserted by the editor)
Image Classiﬁcation with the Fisher Vector: Theory and Practice
Jorge S´anchez · Florent Perronnin · Thomas Mensink · Jakob Verbeek
Received: date / Accepted: date
Abstract A standard approach to describe an image for classiﬁcation and retrieval purposes is to extract a set of local
patch descriptors, encode them into a high dimensional vector and pool them into an image-level signature. The most
common patch encoding strategy consists in quantizing the
local descriptors into a ﬁnite set of prototypical elements.
This leads to the popular Bag-of-Visual words (BoV) representation. In this work, we propose to use the Fisher Kernel framework as an alternative patch encoding strategy: we
describe patches by their deviation from an “universal” generative Gaussian mixture model. This representation, which
we call Fisher Vector (FV) has many advantages: it is efﬁcient to compute, it leads to excellent results even with efﬁcient linear classiﬁers, and it can be compressed with a minimal loss of accuracy using product quantization. We report
experimental results on ﬁve standard datasets – PASCAL
VOC 2007, Caltech 256, SUN 397, ILSVRC 2010 and ImageNet10K – with up to 9M images and 10K classes, showing
that the FV framework is a state-of-the-art patch encoding
technique.
Jorge S´anchez
CIEM-CONICET, FaMAF, Universidad Nacional de C´ordoba,
X5000HUA, C´ordoba, Argentine
E-mail: 
Florent Perronnin
Xerox Research Centre Europe,
6 chemin de Maupertuis, 38240 Meylan, France
E-mail: 
Thomas Mensink
Inteligent Systems Lab Amsterdam, University of Amsterdam,
Science Park 904, 1098 XH, Amsterdam, The Netherlands
E-mail: 
Jakob Verbeek
LEAR Team, INRIA Grenoble,
655 Avenue de l’Europe, 38330 Montbonnot, France
E-mail: 
1 Introduction
This article considers the image classiﬁcation problem: given
an image, we wish to annotate it with one or multiple keywords corresponding to different semantic classes. We are
especially interested in the large-scale setting where one has
to deal with a large number of images and classes. Largescale image classiﬁcation is a problem which has received
an increasing amount of attention over the past few years as
larger labeled images datasets have become available to the
research community. For instance, as of today, ImageNet1
consists of more than 14M images of 22K concepts and Flickr contains thousands of groups2 – some
of which with hundreds of thousands of pictures – which
can be exploited to learn object classiﬁers .
In this work, we describe an image representation which
yields high classiﬁcation accuracy and, yet, is sufﬁciently ef-
ﬁcient for large-scale processing. Here, the term “efﬁcient”
includes the cost of computing the representations, the cost
of learning the classiﬁers on these representations as well as
the cost of classifying a new image.
By far, the most popular image representation for classiﬁcation has been the Bag-of-Visual words (BoV) . In a nutshell, the BoV consists in extracting a set
of local descriptors, such as SIFT descriptors ,
in an image and in assigning each descriptor to the closest
entry in a “visual vocabulary”: a codebook learned ofﬂine by
clustering a large set of descriptors with k-means. Averaging the occurrence counts – an operation which is generally
referred to as average pooling – leads to a histogram of “visual word” occurrences. There have been several extensions
of this popular framework including the use of better coding
1 
2 
Jorge S´anchez et al.
techniques based on soft assignment or sparse coding and the use of spatial pyramids to
take into account some aspects of the spatial layout of the
image .
The focus in the image classiﬁcation community was
initially on developing classiﬁcation systems which would
yield the best possible accuracy fairly independently of their
cost as exampliﬁed in the PASCAL VOC competitions . The winners of the 2007 and 2008
competitions used a similar paradigm: many types of lowlevel local features are extracted (referred to as “channels”),
one BoV histogram is computed for each channel and nonlinear kernel classiﬁers such as χ2-kernel SVMs are used to
perform classiﬁcation . The use of many channels and non-linear SVMs –
whose training cost scales somewhere between quadratically
and cubically in the number of training samples – was made
possible by the modest size of the available databases.
In recent years only the computational cost has become
a central issue in image classiﬁcation and object detection.
Maji et al showed that the runtime cost of an intersection kernel (IK) SVM could be made independent of the
number of support vectors with a negligible performance
degradation. Maji and Berg and Wang et al 
then proposed efﬁcient algorithms to learn IKSVMs in a
time linear in the number of training samples. Vedaldi and
Zisserman and Perronnin et al subsequently
generalized this principle to any additive classiﬁer. Attempts
have been made also to go beyond additive classiﬁers . Another line of
research consists in computing BoV representations which
are directly amenable to costless linear classiﬁcation. Boureau
et al , Wang et al , Yang et al showed
that replacing the average pooling stage in the BoV computation by a max-pooling yielded excellent results.
We underline that all the previously mentioned methods
are inherently limited by the shortcomings of the BoV. First,
it is unclear why such a histogram representation should be
optimal for our classiﬁcation problem. Second, the descriptor quantization is a lossy process as underlined in the work
of Boiman et al .
In this work, we propose an alternative patch aggregation mechanism based on the Fisher Kernel (FK) principle
of Jaakkola and Haussler . The FK combines the beneﬁts of generative and discriminative approaches to pattern
classiﬁcation by deriving a kernel from a generative model
of the data. In a nutshell, it consists in characterizing a sample by its deviation from the generative model. The deviation is measured by computing the gradient of the sample
log-likelihood with respect to the model parameters. This
leads to a vectorial representation which we call Fisher Vector (FV). In the image classiﬁcation case, the samples correspond to the local patch descriptors and we choose as generative model a Gaussian Mixture Model (GMM) which can
be understood as a “probabilistic visual vocabulary”.
The FV representation has many advantages with respect
to the BoV. First, it provides a more general way to deﬁne
a kernel from a generative process of the data: we show that
the BoV is a particular case of the FV where the gradient
computation is restricted to the mixture weight parameters
of the GMM. We show experimentally that the additional
gradients incorporated in the FV bring large improvements
in terms of accuracy. A second advantage of the FV is that it
can be computed from much smaller vocabularies and therefore at a lower computational cost. A third advantage of the
FV is that it performs well even with simple linear classi-
ﬁers. A signiﬁcant beneﬁt of linear classiﬁers is that they
are very efﬁcient to evaluate and efﬁcient to learn (linear in
the number of training samples) using techniques such as
Stochastic Gradient Descent (SGD) .
However, the FV suffers from a signiﬁcant disadvantage with respect to the BoV: while the latter is typically
quite sparse, the FV is almost dense. This leads to storage
as well as input/output issues which make it impractical for
large-scale applications as is. We address this problem using
Product Quantization (PQ) which
has been popularized in the computer vision ﬁeld by J´egou
et al for large-scale nearest neighbor search. We show
theoretically why such a compression scheme makes sense
when learning linear classiﬁers. We also show experimentally that FVs can be compressed by a factor of at least 32
with only very limited impact on the classiﬁcation accuracy.
The remainder of this article is organized as follows. In
Section 2, we introduce the FK principle and describe its
application to images. We also introduce a set of normalization steps which greatly improve the classiﬁcation performance of the FV. Finally, we relate the FV to several recent
patch encoding methods and kernels on sets. In Section 3,
we provide a ﬁrst set of experimental results on three smalland medium-scale datasets – PASCAL VOC 2007 , Caltech 256 and
SUN 397 – showing that the FV outperforms signiﬁcantly the BoV. In Section 4, we present PQ
compression, explain how it can be combined with largescale SGD learning and provide a theoretical analysis of
why such a compression algorithm makes sense when learning a linear classiﬁer. In Section 5, we present results on two
large datasets, namely ILSVRC 2010 (1K
classes and approx. 1.4M images) and ImageNet10K (approx. 10K classes and 9M images). Finally,
we present our conclusions in Section 6.
This paper extends our previous work with: (1) a more detailed description of the FK framework and especially of the computation of the Fisher information matrix, (2) a more detailed analysis of the recent
related work, (3) a detailed experimental validation of the
proposed normalizations of the FV, (4) more experiments
on several small- medium-scale datasets with state-of-theart results, (5) a theoretical analysis of PQ compression for
linear classiﬁer learning and (6) more detailed experiments
on large-scale image classiﬁcation with, especially, a comparison to k-NN classiﬁcation.
2 The Fisher Vector
In this section we introduce the Fisher Vector (FV). We ﬁrst
describe the underlying principle of the Fisher Kernel (FK)
followed by the adaption of the FK to image classiﬁcation.
We then relate the FV to several recent patch encoding techniques and kernels on sets.
2.1 The Fisher Kernel
Let X = {xt,t = 1...T} be a sample of T observations xt ∈
X . Let uλ be a probability density function which models the generative process of elements in X where λ =
[λ1,...,λM]′ ∈RM denotes the vector of M parameters of
uλ. In statistics, the score function is given by the gradient
of the log-likelihood of the data on the model:
λ = ∇λ loguλ(X).
This gradient describes the contribution of the individual
parameters to the generative process. In other words, it describes how the parameters of the generative model uλ should
be modiﬁed to better ﬁt the data X. We note that GX
and thus that the dimensionality of GX
λ only depends on the
number of parameters M in λ and not on the sample size T.
From the theory of information geometry , a parametric family of distributions U =
{uλ,λ ∈Λ} can be regarded as a Riemanninan manifold MΛ
with a local metric given by the Fisher Information Matrix
(FIM) Fλ ∈RM×M:
Fλ = Ex∼uλ
Following this observation, Jaakkola and Haussler 
proposed to measure the similarity between two samples X
and Y using the Fisher Kernel (FK) which is deﬁned as:
KFK(X,Y) = GX
Since Fλ is positive semi-deﬁnite, so is its inverse. Using
the Cholesky decomposition F−1
= Lλ ′Lλ, the FK in (3) can
be re-written explicitly as a dot-product:
KFK(X,Y) = G X
λ = Lλ∇λ loguλ(X).
We call this normalized gradient vector the Fisher Vector
(FV) of X. The dimensionality of the FV G X
λ is equal to that
of the gradient vector GX
λ . A non-linear kernel machine using KFK as a kernel is equivalent to a linear kernel machine
λ as feature vector. A clear beneﬁt of the explicit formulation is that, as explained earlier, linear classiﬁers can be
learned very efﬁciently.
2.2 Application to images
Model. Let X = {xt,t = 1,...,T} be the set of D-dimensional
local descriptors extracted from an image, e.g. a set of SIFT
descriptors . Assuming that the samples are independent, we can rewrite Equation (5) as follows:
Lλ∇λ loguλ(xt).
Therefore, under this independence assumption, the FV is a
sum of normalized gradient statistics Lλ∇λ loguλ(xt) computed for each descriptor. The operation:
xt →ϕFK(xt) = Lλ∇λ loguλ(xt)
can be understood as an embedding of the local descriptors
xt in a higher-dimensional space which is more amenable to
linear classiﬁcation. We note that the independence assumption of patches in an image is generally incorrect, especially
when patches overlap. We will return to this issue in Section
2.3 as well as in our small-scale experiments in Section 3.
In what follows, we choose uλ to be a Gaussian mixture model (GMM) as one can approximate with arbitrary
precision any continuous distribution with a GMM . In the computer vision literature, a GMM
which models the generation process of local descriptors in
any image has been referred to as a universal (probabilistic) visual vocabulary . We denote the parameters of the K-component GMM
by λ = {wk,µk,Σk,k = 1,...,K}, where wk, µk and Σk are
respectively the mixture weight, mean vector and covariance
matrix of Gaussian k. We write:
where uk denotes Gaussian k:
(2π)D/2|Σk|1/2 exp
2(x−µk)′Σ −1
Jorge S´anchez et al.
and we require:
∀k : wk ≥0,
to ensure that uλ(x) is a valid distribution. In what follows,
we assume diagonal covariance matrices which is a standard assumption and denote by σ2
k the variance vector, i.e.
the diagonal of Σk. We estimate the GMM parameters on a
large training set of local descriptors using the Expectation-
Maximization (EM) algorithm to optimize a Maximum Likelihood (ML) criterion. For more details about the GMM implementation, the reader can refer to Appendix B.
Gradient formulas. For the weight parameters, we adopt
the soft-max formalism of Krapac et al and deﬁne
j=1 exp(αj).
The re-parametrization using the αk avoids enforcing explicitly the constraints in Eq. (10). The gradients of a single descriptor xt w.r.t. the parameters of the GMM model,
λ = {αk,µk,Σk,k = 1,...,K}, are:
∇αk loguλ(xt) = γt(k)−wk,
∇µk loguλ(xt) = γt(k)
∇σk loguλ(xt) = γt(k)
(xt −µk)2
where γt(k) is the soft assignment of xt to Gaussian k, which
is also known as the posterior probability or responsibility:
j=1 w juj(xt),
and where the division and exponentiation of vectors should
be understood as term-by-term operations.
Having an expression for the gradients, the remaining
question is how to compute Lλ, which is the square-root of
the inverse of the FIM. In Appendix A we show that under
the assumption that the soft assignment distribution γt(i) is
sharply peaked on a single value of i for any patch descriptor
xt (i.e. the assignment is almost hard), the FIM is diagonal.
In section 3.2 we show a measure of the sharpness of γt on
real data to validate this assumption. The diagonal FIM can
be taken into account by a coordinate-wise normalization of
the gradient vectors, which yields the following normalized
gradients:
(xt −µk)2
Note that G X
αk is a scalar while G X
µk and G X
σk are D-dimensional
vectors. The ﬁnal FV is the concatenation of the gradients
µk and G X
σk for k = 1,...,K and is therefore of dimension E = (2D+1)K.
To avoid the dependence on the sample size ), we normalize the resulting FV by the sample size
T, i.e. we perform the following operation:
In practice, T is almost constant in our experiments since
we resize all images to approximately the same number of
pixels (see the experimental setup in Section 3.1). Also note
that Eq. (16)–(18) can be computed in terms of the following
0-order, 1st-order and 2nd-order statistics (see Algorithm 1):
k ∈RD and S2
k ∈RD. As before, the square
of a vector must be understood as a term-by-term operation.
Spatial pyramids. The Spatial Pyramid (SP) was introduced in Lazebnik et al to take into account the rough
geometry of a scene. It was shown to be effective both for
scene recognition and loosely structured object recognition as demonstrated during the PAS-
CAL VOC evaluations . The
SP consists in subdividing an image into a set of regions
and pooling descriptor-level statistics over these regions. Although the SP was introduced in the framework of the BoV,
it can also be applied to the FV. In such a case, one computes one FV per image region and concatenates the resulting FVs. If R is the number of regions per image, then the
FV representation becomes E = (2D + 1)KR dimensional.
In this work, we use a very coarse SP and extract 4 FVs per
image: one FV for the whole image and one FV in three horizontal stripes corresponding to the top, middle and bottom
regions of the image.
We note that more sophisticated models have been proposed to take into account the scene geometry in the FV
framework but we
will not consider such extensions in this work.
2.3 FV normalization
We now describe two normalization steps which were introduced in Perronnin et al and which were shown to
Image Classiﬁcation with the Fisher Vector: Theory and Practice
be necessary to obtain competitive results when the FV is
combined with a linear classiﬁer.
ℓℓℓ222-normalization. Perronnin et al proposed to
ℓ2-normalize FVs. We provide two complementary interpretations to explain why such a normalization can lead to improved results. The ﬁrst interpretation is speciﬁc to the FV
and was ﬁrst proposed in Perronnin et al . The second interpretation is valid for any high-dimensional vector.
In Perronnin et al , the ℓ2-normalization is justi-
ﬁed as a way to cancel-out the fact that different images contain different amounts of background information. Assuming that the descriptors X = {xt,t = 1,...,T} of a given image follow a distribution p and using the i.i.d. image model
deﬁned above, we can write according to the law of large
numbers (convergence of the sample average to the expected
value when T increases):
λ ≈∇λEx∼p loguλ(x) = ∇λ
x p(x)loguλ(x)dx.
Now let us assume that we can decompose p into a mixture
of two parts: a background image-independent part which
follows uλ and an image-speciﬁc part which follows an imagespeciﬁc distribution q. Let 0 ≤ω ≤1 be the proportion of
image-speciﬁc information contained in the image:
p(x) = ωq(x)+(1−ω)uλ(x).
We can rewrite:
x q(x)loguλ(x)dx
x uλ(x)loguλ(x)dx.
If the values of the parameters λ were estimated with a ML
process – i.e. to maximize at least locally and approximately
Ex∼uλ loguλ(x) – then we have:
x uλ(x)loguλ(x)dx = ∇λEx∼uλ loguλ(x) ≈0.
Consequently, we have:
x q(x)loguλ(x)dx = ω∇λEx∼q loguλ(x).
This shows that the image-independent information is approximately discarded from the FV, a desirable property.
However, the FV still depends on the proportion of imagespeciﬁc information ω. Consequently, two images containing the same object but different amounts of background information (e.g. the same object at different scales) will have
different signatures. Especially, small objects with a small ω
value will be difﬁcult to detect. To remove the dependence
on ω, we can ℓ2-normalize3 the vector GX
3 Normalizing by any ℓp-norm would cancel-out the effect of ω.
Perronnin et al chose the ℓ2-norm because it is the natural norm
associated with the dot-product. In Section 3.2 we experiment with
different ℓp-norms.
We now propose a second interpretation which is valid
for any high-dimensional vector (including the FV). LetUp,E
denote the uniform distribution on the ℓp unit sphere in an
E-dim space. If u ∼Up,E, then a closed form solution for the
marginals over the ℓp-normalized coordinates ui = ui/∥u∥p,
is given in Song and Gupta :
gp,E(ui) =
2Γ (1/p)Γ ((E −1)/p)(1−|ui|p)(E−1)/p−1 (28)
ui ∈[−1,1]
For p = 2, as the dimensionality E grows, this distribution
converges to a Gaussian . Moreover, Burrascano suggested that the ℓp metric is a good measure
between data points if they are distributed according to a
generalized Gaussian:
fp(x) = p(1−1/p)
2Γ (1/p) exp
To support this claim Burrascano showed that, for a given
value of the dispersion as measured with the ℓp-norm, fp is
the distribution which maximizes the entropy and therefore
the amount of information. Note that for p = 2, equation (29)
corresponds to a Gaussian distribution. From the above and
after noting that: a) FVs are high dimensional signatures,
b) we rely on linear SVMs, where the similarity between
samples is measured using simple dot-products, and that c)
the dot-product between ℓ2-normalized vectors relates to the
ℓ2-distance as ∥x−y∥2
2 = 2(1−x′y), for ∥x∥2 = ∥y∥2 = 1, it
follows that choosing p = 2 for the normalization of the FV
is natural.
Power normalization. In Perronnin et al , it was
proposed to perform a power normalization of the form:
z ←sign(z)|z|ρ with 0 < ρ ≤1
to each dimension of the FV. In all our experiments the power
coefﬁcient is set to ρ = 1
2, which is why we also refer to
this transformation as “signed square-rooting” or more simply “square-rooting”. The square-rooting operation can be
viewed as an explicit data representation of the Hellinger
or Bhattacharyya kernel, which has also been found effective for BoV image representations, see e.g. Perronnin et al
 or Vedaldi and Zisserman .
Several explanations have been proposed to justify such
a transform. Perronnin et al argued that, as the number of Gaussian components of the GMM increases, the FV
becomes sparser which negatively impacts the dot-product.
In the case where FVs are extracted from sub-regions, the
“peakiness” effect is even more prominent as fewer descriptorlevel statistics are pooled at a region-level compared to the
image-level. The power normalization “unsparsiﬁes” the FV
and therefore makes it more suitable for comparison with the
dot-product. Another interpretation proposed in Perronnin
et al is that the power normalization downplays the
Jorge S´anchez et al.
inﬂuence of descriptors which happen frequently within a
given image (bursty visual features) in a manner similar to
J´egou et al . In other words, the square-rooting corrects for the incorrect independence assumption. A more
formal justiﬁcation was provided in J´egou et al as
it was shown that FVs can be viewed as emissions of a compound distribution whose variance depends on the mean.
However, when using metrics such as the dot-product or the
Euclidean distance, the implicit assumption is that the variance is stabilized, i.e. that it does not depend on the mean. It
was shown in J´egou et al that the square-rooting had
such a stabilization effect.
All of the above papers acknowledge the incorrect patch
independence assumption and try to correct a posteriori for
the negative effects of this assumption. In contrast, Cinbis
et al proposed to go beyond this independence assumption by introducing an exchangeable model which ties
all local descriptors together by means of latent variables
that represent the GMM parameters. It was shown that such
a model leads to discounting transformations in the Fisher
vector similar to the simpler square-root transform, and with
a comparable positive impact on performance.
We ﬁnally note that the use of the square-root transform
is not speciﬁc to the FV and is also beneﬁcial to the BoV as
shown for instance by Perronnin et al , Vedaldi and
Zisserman , Winn et al .
2.4 Summary
To summarize the computation of the FV image representation, we provide an algorithmic description in Algorithm 1.
In practice we use SIFT or Local Color Statistics as descriptors computed on a
dense multi-scale grid. To simplify the presentation in Algorithm 1, we have assumed that Spatial Pyramids (SPs) are
not used. When using SPs, we follow the same algorithm
for each region separately and then concatenate the FVs obtained for each cell in the SP.
2.5 Relationship with other patch-based approaches
The FV is related to a number of patch-based classiﬁcation
approaches as we describe below.
Relationship with the Bag-of-Visual words (BoV). First,
the FV can be viewed as a generalization of the BoV framework . Indeed, in the soft-BoV , the average
number of assignments to Gaussian k can be computed as:
γt(k) = S0
Algorithm 1 Compute Fisher vector from local descriptors
– Local image descriptors X = {xt ∈RD,t = 1,...,T},
– Gaussian mixture model parameters λ = {wk,µk,σk,k = 1,...,K}
– normalized Fisher Vector representation G X
λ ∈RK(2D+1)
1. Compute statistics
– For k = 1,...,K initialize accumulators
– For t = 1,...T
– Compute γt(k) using equation (15)
– For k = 1,...,K:
k +γt(k)xt,
k +γt(k)x2
2. Compute the Fisher vector signature
– For k = 1,...,K:
– Concatenate all Fisher vector components into one vector
α1,...,G X
3. Apply normalizations
– For i = 1,...,K(2D+1) apply power normalization
– Apply ℓ2-normalization:
This is closely related to the gradient with respect to the mixture weight G X
ak in th FV framework, see Equation (16). The
difference is that G X
ak is mean-centered and normalized by
the coefﬁcient √wk. Hence, for the same visual vocabulary
size K, the FV contains signiﬁcantly more information by
including the gradients with respect to the means and standard deviations. Especially, the BoV is only K dimensional
while the dimension of the FV is (2D + 1)K. Conversely,
we will show experimentally that, for a given feature dimensionality, the FV usually leads to results which are as good
– and sometimes signiﬁcantly better – than the BoV. However, in such a case the FV is much faster to compute than
the BoV since it relies on signiﬁcantly smaller visual vocabularies. An additional advantage is that the FV is a more
principled approach than the BoV to combine the generative and discriminative worlds. For instance, it was shown
in (see Theorem 1) that if the
classiﬁcation label is included as a latent variable of the generative model uλ, then the FK derived from this model is,
asymptotically, never inferior to the MAP decision rule for
this model.
Relationship with GMM-based representations. Several works proposed to model an image as a GMM adapted
Image Classiﬁcation with the Fisher Vector: Theory and Practice
from a universal (i.e. image-independent) distribution uλ . Initializing the parameters of the GMM to λ and performing one EM iteration
leads to the following estimates ˆλ for the image GMM parameters:
t=1 γt(k)+τ
t=1 γt(k)xt +τµk
t=1 γt(k)+τ
t=1 γt(k)x2
t=1 γt(k)+τ
where τ is a parameter which strikes a balance between the
prior “universal” information contained in λ and the imagespeciﬁc information contained in X. It is interesting to note
that the FV and the adapted GMM encode essentially the
same information since they both include statistics of order
0, 1 and 2: compare equations (35-37) with (31-33) in Algorithm 1, respectively. A major difference is that the FV
provides a vectorial representation which is more amenable
to large-scale processing than the GMM representation.
Relationship with the Vector of Locally Aggregated
Descriptors (VLAD). The VLAD was proposed in J´egou
et al . Given a visual codebook learned with k-means,
and a set of descriptors X = {xt,t = 1,...,T} the VLAD
consists in assigning each descriptor xt to its closest codebook entry and in summing for each codebook entry the
mean-centered descriptors. It was shown in J´egou et al 
that the VLAD is a simpliﬁed version of the FV under the
following approximations: 1) the soft assignment is replaced
by a hard assignment and 2) only the gradient with respect to
the mean is considered. As mentioned in J´egou et al ,
the same normalization steps which were introduced for the
FV – the square-root and ℓ2-normalization – can also be applied to the VLAD with signiﬁcant improvements.
Relationship with the Super Vector (SV). The SV was
proposed in Zhou et al and consists in concatenating in a weighted fashion a BoV and a VLAD (see equation
(2) in their paper). To motivate the SV representation, Zhou
et al. used an argument based on the Taylor expansion of
non-linear functions which is similar to the one offered by
Jaakkola and Haussler to justify the FK4. A major
difference between the FV and the SV is that the latter one
does not include any second-order statistics while the FV
does in the gradient with respect to the variance. We will
show in Section 3 that this additional term can bring substantial improvements.
Relationship with the Match Kernel (MK). The MK
measures the similarity between two images as a sum of similarities between the individual descriptors .
See appendix A.2 in the extended version of Jaakkola and Haussler which is available at: 
tommi/papers/gendisc.ps
If X = {xt,t = 1,...,T} and Y = {yu,u = 1,...,U} are two
sets of descriptors and if k(·,·) is a “base” kernel between
local descriptors, then the MK between the sets X and Y is
deﬁned as:
KMK(X,Y) =
The original FK without ℓ2- or power-normalization is a MK
if one chooses the following base kernel:
kFK(xt,yu) = ϕFK(xt)′ϕFK(yu),
A disadvantage of the MK is that by summing the contributions of all pairs of descriptors, it tends to overcount multiple matches and therefore it cannot cope with the burstiness effect. We believe this is one of the reasons for the poor
performance of the MK (see the third entry in Table 4 in
the next section). To cope with this effect, alternatives have
been proposed such as the “sum-max” MK of :
KSM(X,Y) = 1
u=1 k(xt,yu)
t=1 k(xt,yu).
or the “power” MK of :
KPOW(X,Y) = 1
k(xt,yu)ρ.
In the FK case, we addressed the burstiness effect using the
square-root normalization (see Section 2.3).
Another issue with the MK is its high computational cost
since, in the general case, the comparison of two images requires comparing every pair of descriptors. While efﬁcient
approximation exists for the original (poorly performing)
MK of equation (38) when there exists an explicit embedding of the kernel k(·,·) , such
approximations do not exist for kernels such as the one de-
ﬁned in .
3 Small-scale experiments
The purpose of this section is to establish the FV as a stateof-the-art image representation before moving to larger scale
scenarios. We ﬁrst describe the experimental setup. We then
provide detailed experiments on PASCAL VOC 2007. We
also report results on Caltech256 and SUN397.
Jorge S´anchez et al.
3.1 Experimental setup
Images are resized to 100K pixels if larger. We extract approximately 10K descriptors per image from 24×24 patches
on a regular grid every 4 pixels at 5 scales. We consider two
types of patch descriptors in this work: the 128-dim SIFT descriptors of Lowe and the 96-dim Local Color Statistic (LCS) descriptors of Clinchant et al . In both cases,
unless speciﬁed otherwise, they are reduced down to 64-dim
using PCA, so as to better ﬁt the diagonal covariance matrix
assumption. We will see that the PCA dimensionality reduction is key to make the FV work. We typically use in the
order of 106 descriptors to learn the PCA projection.
To learn the parameters of the GMM, we optimize a
Maximum Likelihood criterion with the EM algorithm, using in the order of 106 (PCA-reduced) descriptors. In Appendix B we provide some details concerning the implementation of the training GMM.
By default, for the FV computation, we compute the gradients with respect to the mean and standard deviation parameters only (but not the mixture weight parameters). In
what follows, we will compare the FV with the soft-BoV
histogram. For both experiments, we use the exact same
GMM package which makes the comparison completely fair.
For the soft-BoV, we perform a square-rooting of the BoV
(which is identical to the power-normalization of the FV)
as this leads to large improvements at negligible additional
computational cost . For both the soft-BoV and the FV we use the
same spatial pyramids with R = 4 regions (the entire images
and three horizontal stripes) and we ℓ2-normalized the perregion sub-vectors.
As for learning, we employ linear SVMs and train them
using Stochastic Gradient Descent (SGD) .
3.2 PASCAL VOC 2007
We ﬁrst report a set of detailed experiments on PASCAL
VOC 2007 . Indeed, VOC 2007 is
small enough (20 classes and approximately 10K images) to
enable running a large number of experiments in a reasonable amount of time but challenging enough ) so that the conclusions we draw
from our experiments extrapolate to other (equally challenging) datasets. We use the standard protocol which consists in
training and validating on the “train” and “val” sets and testing on the “test” set. We measure accuracy using the standard measure on this dataset which is the interpolated Average Precision (AP). We report the average over 20 categories
(mean AP or mAP) in %. In the following experiments, we
use a GMM with 256 Gaussians, which results in 128K-dim
FVs, unless otherwise speciﬁed.
Local feature dimensionality
Mean AP (in %)
Fig. 1 Inﬂuence of the dimensionality reduction of the SIFT descriptors on the FV on PASCAL VOC 2007.
Table 1 Impact of the proposed modiﬁcations to the FK on PASCAL
VOC 2007. “PN” = power normalization. “ℓ2” = ℓ2-normalization,
“SP” = Spatial Pyramid. The ﬁrst line (no modiﬁcation applied) corresponds to the baseline FK of Perronnin and Dance . Between
parentheses: the absolute improvement with respect to the baseline FK.
Accuracy is measured in terms of AP (in %).
Impact of PCA on local descriptors. We start by studying the inﬂuence of the PCA dimensionality reduction of
the local descriptors. We report the results in Figure 1. We
ﬁrst note that PCA dimensionality reduction is key to obtain
good results: without dimensionality reduction, the accuracy
is 54.5% while it is above 60% for 48 PCA dimensions and
more. Second, we note that the accuracy does not seem to
be overly sensitive no the exact number of PCA components. Indeed, between 64 and 128 dimensions, the accuracy
varies by less than 0.3% showing that the FV combined with
a linear SVM is robust to noisy PCA dimensions. In all the
following experiments, the PCA dimensionality is ﬁxed to
Impact of improvements. The goal of the next set of
experiments is to evaluate the impact of the improvements
over the original FK work of Perronnin and Dance .
This includes the use of the power-normalization, the ℓ2normalization, and SPs. We evaluate the impact of each of
these three improvements considered separately, in pairs or
all three together. Results are shown in Table 1 for SIFT
Image Classiﬁcation with the Fisher Vector: Theory and Practice
and LCS descriptors separately. The improved performance
compared to the results in Perronnin et al , is probably due to denser sampling and a different layout of the
spatial pyramids.
From the results we conclude the following. The single
most important improvement is the power-normalization: +8.3
absolute for SIFT and +11.8 for LCS. On the other hand,
the SP has little impact in itself: +1.9 on SIFT and +0.7
on LCS. Combinations of two improvements generally increase accuracy over a single one and combining all three
improvements leads to an additional increment. Overall, the
improvement is substantial: +12.2 for SIFT and +17.4 for
Approximate FIM vs. empirical FIM. We now compare the impact of using the proposed diagonal closed-form
approximation of the Fisher Information Matrix (FIM) (see
equations (16), (17) and (18) as well as Appendix A) as
opposed to its empirical approximation as estimated on a
training set. We ﬁrst note that our approximation is based
on the assumption that the distribution of posterior probabilities γt(k) is sharply peaked. To verify this hypothesis,
we computed on the “train” set of PASCAL VOC 2007 the
t = maxk γt(k) for each observation xt and plotted its
cumulated distribution. We can deduce from Figure 2 that
the distribution of the posterior probabilities is quite sharply
peaked. For instance, more than 70% of the local descriptors
t ≥0.5, i.e. the majority of the posterior is concentrated in a single Gaussian. However this is still far from
t = 1 assumption we made for the approximated FIM.
Nevertheless, in practice, this seems to have little impact on
the accuracy: using the diagonal approximation of the FIM
we get 61.8% accuracy while we get 60.6% with the empirical diagonal estimation. Note that we do not claim that this
difference is signiﬁcant nor that the closed-form approximation is superior to the empirical one in general. Finally, the
FIM could be approximated by the identity matrix, as originally proposed in Jaakkola and Haussler . Using the
identity matrix, we observe a decrease of the performance to
Impact of patch density. In Section 2.3, it was hypothesized that the power-norm counterbalanced the effect of the
incorrect patch independence assumption. The goal of the
following experiment is to validate this claim by studying
the inﬂuence of the patch density on the classiﬁcation accuracy. Indeed, patches which are extracted more densely
overlap more and are therefore more correlated. Conversely,
if patches are extracted less densely, then the patch independence assumption is more correct. We vary the patch extraction step size from 4 pixels to 24 pixels. Since the size of
our patches is 24 × 24, this means that we vary the overlap
between two neighboring patches between more than 80%
down to 0%. Results are shown in Table 2 for SIFT and LCS
descriptors separately. As the step-size decreases, i.e. as the
max posterior probability
cumulative density
Fig. 2 Cumulative distribution of the max of the posterior probability
t = maxk γt(k) on PASCAL VOC 2007 for SIFT descriptors.
Table 2 Impact of the patch extraction step-size on PASCAL VOC
2007. The patch size is 24×24. Hence, when the step-size is 24, there is
no overlap between patches. We also indicate the approximate number
of patches per image for each step size. “PN” stands for Power Normalization. ∆abs. and ∆rel. are respectively the absolute and relative
differences between using PN and not using PN. Accuracy is measured
in terms of mAP (in %).
Patches per image
independence assumption gets more and more violated, the
impact of the power-norm increases. We believe that this observation validates our hypothesis: the power-norm is a simple way to correct for the independence assumption
Impact of cropping. In Section 2.3, we proposed to ℓ2normalize FVs and we provided two possible arguments.
The ﬁrst one hypothesized that the ℓ2-normization is a way
to counterbalance the inﬂuence of variable amounts of “informative patches” in an image where a patch is considered
non-informative if it appears frequently (in any image). The
second argument hypothesized that the ℓ2 normalization of
high-dimensional vectors is always beneﬁcial when used in
combination with linear classiﬁers.
The goal of the following experiment is to validate (or
invalidate) the ﬁrst hypothesis: we study the inﬂuence of the
ℓ2-norm when focusing on informative patches. One practical difﬁculty is the choice of informative patches. As shown
Jorge S´anchez et al.
p parameter of the Lp norm
Mean AP (in %)
with Lp norm
no Lp norm
Fig. 3 Inﬂuence of the parameter p of the ℓp-norm on the FV on PAS-
CAL VOC 2007.
in Uijlings et al , foreground patches (i.e. object patches)
are more informative than background object patches. Therefore, we carried-out experiments on cropped object images
as a proxy to informative patches. We cropped the PASCAL
VOC images to a single object (drawn randomly from the
ground-truth bounding box annotations) to avoid the bias
toward images which contain many objects. When using all
improvements of the FV, we obtain an accuracy of 64.4%
which is somewhat better than the 61.8% we report on full
images. If we do not use the ℓ2-normalization of the FVs,
then we obtain an accuracy of 57.2%. This shows that the
ℓ2-normalization still has a signiﬁcant impact on cropped
objects which seems to go against our ﬁrst argument and to
favor the second one.
Impact of p in ℓp-norm. In Section 2.3, we proposed
to use the ℓ2-norm as opposed to any ℓp-norm because it
was more consistent with our choice of a linear classiﬁer.
We now study the inﬂuence of this parameter p. Results are
shown in Figure 3. We see that the ℓp-normalization improves over no normalization over a wide range of values
of p and that the highest accuracy is achieved with a p close
to 2. In all the following experiments, we set p = 2.
Impact of different Fisher vector components. We now
evaluate the impact of the different components when computing the FV. We recall that the gradient with respect to the
mixture weights, mean and standard deviation correspond
respectively to 0-order, 1st-order and 2nd-order statistics and
that the gradient with respect to the mixture weight corresponds to the soft-BoV. We see in Figure 4 that there is an
increase in performance from 0-order (BoV) to the combination of 0-order and 1st-order statistics ), and even further when
the 1st-order and 2nd-order statistics are combined. We also
observe that the 0-order statistics add little discriminative
MAP (in %)
Number of Gaussians
Mean AP (in %)
Fig. 4 Accuracy of the FV as a function of the gradient components
on PASCAL VOC 2007 with SIFT descriptors only. w = gradient with
respect to mixture weights, µ = gradient with respect to means and σ
= gradient with respect to standard deviations. Top: accuracy for 256
Gaussians. Bottom: accuracy as a function of the number of Gaussians
(we do not show wµ, wσ and wµσ for clarity as there is little difference
respectively with µ, σ and µσ).
information on top of the 1st-order and 2nd-order statistics.
We also can see that the 2nd-order statistics seem to bring
more information than the 1st-order statistics for a small
number of Gaussians but that both seem to carry similar information for a larger number of Gaussians.
Comparison with the soft-BoV. We now compare the
FV to the soft-BoV. We believe this comparison to be completely fair, since we use the same low-level SIFT features
and the same GMM implementation for both encoding methods. We show the results in Figure 5 both as a function
of the number of Gaussians of the GMM and as a function of the feature dimensionality (note that the SP increases
the dimensionality for both FV and BoV by a factor 4).
The conclusions are the following ones. For a given number of Gaussians, the FV always signiﬁcantly outperforms
the BoV. This is not surprising since, for a given number
of Gaussians, the dimensionality of the FV is much higher
than that of the BoV. The difference is particularly impressive for a small number of Gaussians. For instance for 16
Gaussians, the BoV obtains 31.8 while the FV gets 56.5. For
a given number of dimensions, the BoV performs slightly
better for a small number of dimensions (512) but the FV
performs better for a large number of dimensions. Our best
Image Classiﬁcation with the Fisher Vector: Theory and Practice
64 128 256 512 1K
8K 16K 32K
Number of Gaussians
Mean AP (in %)
Feature dimensionality
Mean AP (in %)
Fig. 5 Accuracy of the soft-BoV and the FV as a function of the number of Gaussians (left) and feature dimensionality (right) on PASCAL VOC
2007 with SIFT descriptors only.
Table 3 Comparison with the state-of-the-art on PASCAL VOC 2007.
MAP (in %)
challenge winners
 
 
 
 
 
 
FV (SIFT + LCS)
results with the BoV is 56.7% with 32K Gaussians while
the FV gets 61.8% with 256 Gaussians. With these parameters, the FV is approximately 128 times faster to compute
since, by far, the most computationally intensive step for
both the BoV and the GMM is the cost of computing the assignments γk(x). We note that our soft-BoV baseline is quite
strong since it outperforms the soft-BoV results in the recent benchmark of Chatﬁeld et al , and performs on
par with the best sparse coding results in this benchmark. Indeed, Chatﬁeld et al. report 56.3% for soft-BoV and 57.6%
for sparse coding with a slightly different setting.
Comparison with the state-of-the-art. We now compare our results to some of the best published results on PAS-
CAL VOC 2007. The comparison is provided in Table 3.
For the FV, we considered results with SIFT only and with
a late fusion of SIFT+LCS. In the latter case, we trained
two separate classiﬁers, one using SIFT FVs and one using
LCS FVs. Given an image, we compute the two scores and
average them in a weighted fashion. The weight was crossvalidated on the validation data and the optimal combination
was found to be 0.6×SIFT+0.4×LCS. The late fusion of
the SIFT and LCS FVs yields a performance of 63.9%, using
only the SIFT features obtains 61.8%. We now provide more
details on the performance of the other published methods.
The challenge winners obtained 59.4% accuracy by combining many different channels corresponding to different
feature detectors and descriptors. The idea of combining multiple channels on PASCAL VOC 2007 has been extensively
used by others. For instance, VanGemert et al reports
60.5% with a soft-BoV representation and several color descriptors and Yang et al reports 62.2% using a group
sensitive form of Multiple Kernel Learning (MKL). Uijlings
et al reports 59.4% using a BoV representation and
a single channel but assuming that one has access to the
ground-truth object bounding box annotations at both training and test time (which they use to crop the image to the
rectangles that contain the objects, and thus suppress the
background to a large extent). This is a restrictive setting
that cannot be followed in most practical image classiﬁcation problems. Harzallah et al reports 63.5% using a
standard classiﬁcation pipeline in combination with an image detector. We note that the cost of running one detector per category is quite high: from several seconds to several tens of seconds per image. Zhou et al reports
64.0% with SV representations. However, with our own reimplementation, we obtained only 58.1% . Finally, Guillaumin et al
 reports 66.7% but assuming that one has access to
the image tags. Without access to such information, their
BoV results dropped to 53.1%.
Computational cost. We now provide an analysis of the
computational cost of our pipeline on PASCAL VOC 2007.
We focus on our “default” system with SIFT descriptors
Jorge S´anchez et al.
Unsupervised learning
SIFT + PCA
Supervised learning
Fig. 6 Breakdown of the computational cost of our pipeline on PAS-
CAL VOC 2007. The whole pipeline takes approx. 2h on a single processor, and is divided into: 1) Learning the PCA on the SIFT descriptors and the GMM with 256 Gaussians (Unsupervised learning). 2)
Computing the dense SIFT descriptors for the 10K images and projecting them to 64 dimensions (SIFT + PCA). 3) Encoding and aggregating the low-level descriptors into FVs for the 10K images (FV). 4)
Learning the 20 SVM classiﬁers using SGD (Supervised Learning).
The testing time – i.e. the time to classify the 5K test FVs – is not
shown as it represents only 0.1% of the total computational cost.
only and 256 Gaussians (128K-dim FVs). Training and testing the whole pipeline from scratch on a Linux server with
an Intel Xeon E5-2470 Processor @2.30GHz and 128GBs
of RAM takes approx. 2h using a single processor. The repartition of the cost is shown in Figure 6. From this breakdown
we observe that 2
3 of the time is spent on computing the lowlevel descriptors for the train, val and test sets. Encoding
the low-level descriptors into image signatures costs about
25% of the time, while learning the PCA and the parameters of the GMM takes about 8%. Finally learning the 20
SVM classiﬁers using the SGD training takes about 2% of
the time and classiﬁcation of the test images is in the order
of seconds (0.1% of the total computational cost).
3.3 Caltech 256
We now report results on Caltech 256 which contains approx. 30K images of 256 categories. As is standard practice,
we run experiments with different numbers of training images per category: 5, 10, 15, ..., 60. The remainder of the
images is used for testing. To cross-validate the parameters,
we use half of the training data for training, the other half for
validation and then we retrain with the optimal parameters
on the full training data. We repeat the experiments 10 times.
We measure top-1 accuracy for each class and report the average as well as the standard deviation. In Figure 7(a), we
compare a soft-BoV baseline with the FV (using only SIFT
descriptors) as a function of the number of training samples.
For the soft-BoV, we use 32K Gaussians and for the FV 256
Gaussians. Hence both the BoV and FV representations are
128K-dimensional. We can see that the FV always outperforms the BoV.
We also report results in Table 4 and compare with the
state-of-the-art. We consider both the case where we use
only SIFT descriptors and the case where we use both SIFT
and LCS descriptors (again with a simple weighted linear
combination). We now provide more details about the different techniques. The baseline of Grifﬁn et al is a
reimplementation of the spatial pyramid BoV of Lazebnik
et al . Several systems are based on the combination of multiple channels corresponding to many different
features including . Other works, considered a single type of descriptors,
typically SIFT descriptors . Bo and Sminchisescu make use of the Efﬁcient Match Kernel (EMK)
framework which embeds patches in a higher-dimensional
space in a non-linear fashion (see also Section 2.5). Wang
et al , Yang et al considered different variants of sparse coding and Boureau et al , Feng et al
 different spatial pooling strategies. Kulkarni and Li
 extracts on the order of a million patches per image
by computing SIFT descriptors from several afﬁne transforms of the original image and uses sparse coding in combination with Adaboost. Finally, the best results we are aware
of are those of Bo et al which uses a deep architecture which stacks three layers, each one consisting of three
steps: coding, pooling and contrast normalization. Note that
the deep architecture of Bo et al makes use of color
information. Our FV which combines the SIFT and LCS descriptors, outperform all other methods using any number of
training samples. Also the SIFT only FV is among the best
performing descriptors.
3.4 SUN 397
We now report results on the SUN 397 dataset which contains approx. 100K images of 397 categories. Following the protocol of Xiao et al , we used
5, 10, 20 or 50 training samples per class and 50 samples per
class for testing. To cross-validate the classiﬁer parameters,
we use half of the training data for training, the other half
for validation and then we retrain with the optimal parameters on the full training data5. We repeat the experiments
10 times using the partitions provided at the website of the
5 Xiao et al also report results with 1 training sample per
class. However, a single sample does not provide any way to perform
cross-validation which is the reason why we do not report results in
this setting.
Image Classiﬁcation with the Fisher Vector: Theory and Practice
Training samples per class
Top−1 accuracy (in %)
Training samples per class
Top−1 accuracy (in %)
Fig. 7 Comparison of the soft-BoV and the FV on Caltech256 (left) and SUN 397 (right) as a function of the number of training samples. We only
use SIFT descriptors and report the mean and 3 times the average deviation.
Table 4 Comparison of the FV with the state-of-the-art on Caltech 256.
Grifﬁn et al 
34.1 (0.2)
Boiman et al 
Bo and Sminchisescu 
23.2 (0.6)
30.5 (0.4)
34.4 (0.4)
37.6 (0.5)
Yang et al 
27.7 (0.5)
34.0 (0.4)
37.5 (0.6)
40.1 (0.9)
Gehler and Nowozin 
VanGemert et al 
27.2 (0.4)
Wang et al 
Boureau et al 
41.7 (0.8)
Feng et al 
Kulkarni and Li 
Bergamo and Torresani 
Bo et al 
40.5 (0.4)
48.0 (0.2)
51.9 (0.2)
55.2 (0.3)
38.5 (0.2)
47.4 (0.1)
52.1 (0.4)
54.8 (0.4)
FV (SIFT+LCS)
41.0 (0.3)
49.4 (0.2)
54.3 (0.3)
57.3 (0.2)
Table 5 Comparison of the FV with the state-of-the-art on SUN 397.
Xiao et al 
19.2 (0.4)
26.6 (0.4)
34.2 (0.3)
43.3 (0.2)
FV (SIFT+LCS)
21.1 (0.3)
29.1 (0.3)
37.4 (0.3)
47.2 (0.2)
dataset.6 We measure top-1 accuracy for each class and report the average as well as the standard deviation. As was
the case for Caltech 256, we ﬁrst compare in Figure 7(b),
a soft-BoV baseline with 32K Gaussians and the FV with
256 Gaussians using only SIFT descriptors. Hence both the
BoV and FV representations have the same dimensionality: 128K-dim. As was the case on the PASCAL VOC and
Caltech datasets, the FV consistently outperforms the BoV
and the performance difference increases when more training samples are available.
6 See 
The only other results we are aware of on this dataset
are those of its authors whose system combined 12 feature
types . The comparison is reported in Table 5. We observe that the proposed FV performs signiﬁcantly better than the baseline of Xiao et al , even
when using only SIFT descriptors.
Jorge S´anchez et al.
4 Fisher vector compression with PQ codes
Having now established that the FV is a competitive image
representation, at least for small- to medium-scale problems,
we now turn to the large-scale challenge.
One of the major issues to address when scaling the FV
to large amounts of data is the memory usage. As an example, in S´anchez and Perronnin we used FV representations with up to 512K dimensions. Using a 4 byte ﬂoating point representation, a single signature requires 2MB
of storage. Storing the signatures for the approx. 1.4M images of the ILSVRC 2010 dataset would
take almost 3TBs, and storing the signatures for the approx.
14M of the full ImageNet dataset around
27TBs. We underline that this is not purely a storage problem. Handling TBs of data makes experimentation very dif-
ﬁcult if not impractical. Indeed, much more time can be
spent writing / reading data on disk than performing any useful calculation.
In what follows, we ﬁrst introduce Product Quantization (PQ) as an efﬁcient and effective approach to perform
lossy compression of FVs. We then describe a complementary lossless compression scheme based on sparsity encoding. Subsequently, we explain how PQ encoding / decoding
can be combined with Stochastic Gradient Descent (SGD)
learning for large-scale optimization. Finally, we provide a
theoretical analysis of the effect of lossy quantization on the
learning objective function.
4.1 Vector quantization and product quantization
Vector Quantization (VQ). A vector quantizer q : ℜE →C
maps a vector v ∈ℜE to a codeword ck ∈ℜE in the codebook C = {ck,k = 1,...,K} . The
cardinality K of the set C , known as the codebook size, de-
ﬁnes the compression level of the VQ as ⌈log2 K⌉bits are
needed to identify the K codeword indices. If one considers
the Mean-Squared Error (MSE) as the distortion measure
then the Lloyd optimality conditions lead to k-means training of the VQ. The MSE for a quantizer q is given as the
expected squared error between v ∈ℜE and its reproduction
value q(v) ∈C :
p(v)∥q(v)−v∥2dv,
where p is a density function deﬁned over the input vector
If we use on average b bits per dimension to encode a
given image signature (b might be a fractional value), then
the cardinality of the codebook is 2bE. However, for E =
O(105), even for a small number of bits (e.g. our target in
this work is typically b = 1), the cost of learning and storing
such a codebook – in O(E2bE) – would be incommensurable.
Product Quantization (PQ). A solution is to use product quantizers which were introduced as a principled way
to deal with high dimensional input spaces for an excellent introduction to the topic). A
PQ q : ℜE →C splits a vector v into a set of M distinct
sub-vectors of size G = E/M, i.e. v = [v1,...,vM]. M subquantizers {qm,m = 1...M} operate independently on each
of the sub-vectors. If Cm is the codebook associated with qm,
then C is the Cartesian product C = C1 ×...×CM and q(v)
is the concatenation of the qm(vm)’s.
The vm’s being the orthogonal projections of v onto disjoint groups of dimensions, the MSE for PQ takes the form:
MSEpq(q) = ∑
pm(vm)∥q(vm)−vm∥2dvm,
which can be equivalently rewritten as:
MSEpq(q) =
∥q(vm)−vm∥2dv.
The sum within the integral corresponds to the squared distortion for q. The term between parentheses can be seen as
an approximation to the underlying distribution:
When M = E, i.e. G = 1, the above approximation corresponds to a naive Bayes model where all dimensions are
assumed to be independent, leading to a simple scalar quantizer. When M = 1, i.e. G = E, we are back to (42), i.e. to the
original VQ problem on the full vector. Choosing different
values for M impose different independence assumptions on
p. Particularly, for groups m and m′ we have:
Cov(vm,vm′) = 0G×G,
where 0G×G denotes the G × G matrix of zeros. Using a
PQ with M groups can be seen as restricting the covariance
structure of the original space to a block diagonal form.
In the FV case, we would expect this structure to be
diagonal since the FIM is just the covariance of the score.
However: i) the normalization by the inverse of the FIM is
only approximate; ii) the ℓ2-normalization (Sec. 2.3) induces
dependencies between dimensions, and iii) the diagonal covariance matrix assumption in the model is probably incorrect. All these factors introduce dependencies among the FV
dimensions. Allowing the quantizer to model some correlations between groups of dimensions, in particular those that
correspond to the same Gaussian, can at least partially account for the dependencies in the FV.
Image Classiﬁcation with the Fisher Vector: Theory and Practice
Let b be the average number of bits per dimension (assuming that the bits are equally distributed across the codebooks Cm) . The codebook size of C is K = (2bG)M = 2bE
which is unchanged with respect to the standard VQ. However the costs of learning and storing the codebook are now
in O(E2bG).
The choice of the parameters b and G should be motivated by the balance we wish to strike between three con-
ﬂicting factors: 1) the quantization loss, 2) the quantization
speed and 3) the memory/storage usage. We use the following approach to make this choice in a principled way.
Given a memory/storage target, we choose the highest possible number of bits per dimension b we can afford (constraint
3). To keep the quantization cost reasonable we have to cap
the value bG. In practice we choose G such that bG ≤8
which ensures that (at least in our implementation) the cost
of encoding a FV is not higher than the cost of extracting
the FV itself (constraint 2). Obviously, different applications
might have different constraints.
4.2 FV sparsity encoding
We mentioned earlier that the FV is dense: on average, only
approximately 50% of the dimensions are zero (see also the
paragraph “posterior thresholding” in Appendix B). Generally speaking, this does not lead to any gain in storage as
encoding the index and the value for each dimension would
take as much space (or close to). However, we can leverage
the fact that the zeros are not randomly distributed in the FV
but appear in a structure. Indeed, if no patch was assigned to
Gaussian i (i.e. ∀t, γt(i) = 0), then in equations (17) and (18)
all the gradients are zero. Hence, we can encode the sparsity
on a per-Gaussian level instead of doing so per dimension.
The sparsity encoding works as follows. We add one bit
per Gaussian. This bit is set to 0 if no low-level feature is
assigned to the Gaussian, and 1 if at least one low-level feature is assigned to the Gaussian (with non-negligible probability). If this bit is zero for a given Gaussian, then we know
that all the gradients for this Gaussian are exactly zero and
therefore we do not need to encode the codewords for the
sub-vectors of this Gaussian. If the bit is 1, then we encode
the 2D mean and standard-deviation gradient values of this
Gaussian using PQ.
Note that adding this per Gaussian bit can be viewed
as a ﬁrst step towards gain/shape coding , i.e. encoding separately the norm and direction of the
gradient vectors. We experimented with a more principled
approach to gain/shape coding but did not observe any substantial improvement in terms of storage reduction.
4.3 SGD Learning with quantization
We propose to learn the linear classiﬁers directly in the uncompressed high-dimensional space rather than in the space
of codebook indices. We therefore integrate the decompression algorithm in the SGD training code. All compressed
signatures are kept in RAM if possible. When a signature
is passed to the SGD algorithm, it is decompressed on the
ﬂy. This is an efﬁcient operation since it only requires lookup table accesses. Once it has been processed, the decompressed version of the sample is discarded. Hence, only one
decompressed sample at a time is kept in RAM. This makes
our learning scheme both efﬁcient and scalable.
While the proposed approach combines on-the-ﬂy decompression with SGD learning, an alternative has been recently proposed by Vedaldi and Zisserman which
avoids the decompression step and which leverages bundle
methods with a non-isotropic regularizer. The latter method,
however, is a batch solver that accesses all data for every
update of the weight vector, and is therefore less suitable
for large scale problems. The major advantage of our SGDbased approach is that we decompress only one sample at a
time, and typically do not even need to access the complete
dataset to obtain good results. Especially, we can sample
only a fraction of the negatives and still converge to a reasonably accurate solution. This proves to be a crucial property
when handling very large datasets such as ImageNet10K,
see Section 5.
4.4 Analysis of the effect of quantization on learning
We now analyze the inﬂuence of the quantization on the
classiﬁer learning. We will ﬁrst focus on the case of Vector Quantization and then turn to PQ.
Let f(x;w) : RD →R be the prediction function. In what
follows, we will focus on the linear case, i.e. f(x;w) = w′x.
We assume that, given a sample (x,y) with x ∈RD and y ∈
{−1,+1}, we incur a loss:
ℓ(yf(x;w)) = ℓ(yw′x).
We assume that the training data is generated from a distribution p. In the case of an unregularized formulation, we
typically seek w that minimizes the following expected loss:
x,y ℓ(yw′x)p(x,y)dxdy.
Underlying the k-means algorithm used in VQ (and PQ)
is the assumption that the data was generated by a Gaussian Mixture Model (GMM) with equal mixture weights and
isotropic covariance matrices, i.e. covariance matrices which
can be written as σ2I where I is the identity matrix.7 If
Actually, any continuous distribution can be approximated with
arbitrary precision by a GMM with isotropic covariance matrices.
Jorge S´anchez et al.
we make this assumption, we can write (approximately) a
random variable x ∼p as the sum of two independent random variables: x ≈q + ε where q draws values in the ﬁnite set of codebook entries C with equal probabilities and
ε ∼N (0,σ2I) is a white Gaussian noise. We can therefore
approximate the objective function (48) as8:
q,ε,y ℓ(yw′(q+ε))p(q,ε,y)dqdεdy
We further assume that the loss function ℓ(u) is twice differentiable. While this is not true of the hinge loss in the SVM
case, this assumption is veriﬁed for other popular losses such
as the quadratic loss or the log loss. If σ2 is small, we can
approximate ℓ(yw′(q+ε)) by its second order Taylor expansion around q:
ℓ(yw′(q+ε))
≈ℓ(yw′q)+ε′∇qℓ(yw′q)+ 1
= ℓ(yw′q)+ε′ywℓ′(yw′q)+ 1
2ε′ww′εℓ′′(yw′q).
where ℓ′(u) = ∂ℓ(u)/∂u and ℓ′′(u) = ∂2ℓ(u)/(∂u)2 and we
have used the fact that y2 = 1. Note that this expansion is
exact for the quadratic loss.
In what follows, we further make the assumption that
the label y is independent of the noise ε knowing q, i.e.
p(y|q,ε) = p(y|q). This means that the label y of a sample
x is fully determined by its quantization q and that ε can be
viewed as a noise. For instance, in the case where σ →0 –
i.e. the soft assignment becomes hard and each codeword is
associated with a Voronoi region – this conditional independence means that (the distribution on) the label is constant
over each Voronoi region. In such a case, using also the independence of q and ε, i.e. the fact that p(q,ε) = p(q)p(ε),
it is easily shown that:
p(q,ε,y) = p(q,y)p(ε).
If we inject (50) and (51) in (49), we obtain:
q,ε,y ℓ(yw′q)p(q,y)dqdy
ε ε′p(ε)dε
q,y ywℓ′(yw′q)p(q,y)dqdy
ε εε′p(ε)dε
q ℓ′′(yw′q)p(q)dq.
Since ε ∼N (0,σ2I), we have:
ε ε′p(ε)dε = 0
ε εε′p(ε)dε = σ2I.
Note that since q draws values in a ﬁnite set, we could replace
q by ∑q in the following equations but we will keep the integral
notation for simplicity.
Therefore, we can rewrite:
q,y ℓ(yw′q)p(q,y)dqdy
q ℓ′′(yw′q)p(q)dq
The ﬁrst term corresponds to the expected loss in the
case where we replace each training sample by its quantized
version. Hence, the previous approximation tells us that, up
to the ﬁrst order, the expected losses in the quantized and
unquantized cases are approximately equal. This provides
a strong justiﬁcation for using k-means quantization when
training linear classiﬁers. If we go to the second order, a
second term appears. We now study its inﬂuence for two
standard twice-differentiable losses: the quadratic and log
losses respectively.
– In the case of the quadratic loss, we have ℓ(u) = (1 −
u)2 and ℓ′′(u) = 2. and the regularization simpliﬁes to
σ2||w2||, i.e. a standard regularizer. This result is in line
with Bishop which shows that adding Gaussian
noise can be a way to perform regularization for the
quadratic loss. Here, we show that quantization actually
has an “unregularization” effect since the loss in quantized case can be written approximately as the loss in the
unquantized case minus a regularization term. Note that
this unregularization effect could be counter-balanced in
theory by cross-validating the regularization parameter
– In the case of the log loss, we have ℓ(u) = −logσ(u) =
log(1 + e−u), where σ(·) is the sigmoid function, and
ℓ′′(z) = σ(u)σ(−u) which only depends on the absolute
value of u. Therefore, the second term of (55) can be
written as:
q σ(w′q)σ(−w′q)p(q)dq
which depends on the data distribution p(q) but does
not depend on the label distribution p(y|q). We can observe two conﬂicting effects in (56). Indeed, as the norm
||w|| increases, the value of the term σ(w′q)σ(−w′q) decreases. Hence, it is unclear whether this term acts as a
regularizer or an “unregularizer”. Again, this might depend on the data distribution. We will study empirically
its effect in section 5.1.
To summarize, we have made three approximations: 1) p
can be approximated by a mixture of isotropic Gaussians, 2)
ℓcan be approximated by its second order Taylor expansion
and 3) y is independent of ε knowing q. We note that these
three approximations become more and more exact as the
number of codebook entries K increases, i.e. as the variance
σ2 of the noise decreases.
We underline that the previous analysis remains valid
in the PQ case since the codebook is a Cartesian product
Image Classiﬁcation with the Fisher Vector: Theory and Practice
of codebooks. Actually, PQ is an efﬁcient way to increase
the codebook size (and therefore reduce σ2) at an affordable cost. Also, the previous analysis remains valid beyond
Gaussian noise, as long as ε is independent of q and has zero
mean. Finally, although we typically train SVM classiﬁers,
i.e. we use a hinge loss, we believe that the intuitions gained
from the twice differentiable losses are still valid, especially
those drawn from the log-loss whose shape is similar.
5 Large-scale experiments
We now report results on the large-scale ILSVRC 2010 and
ImageNet10K datasets. The FV computation settings are almost identical to those of the small-scale experiments. The
only two differences are the following ones. First, we do not
make use of Spatial Pyramids and extract the FVs on the
whole images to reduce the signature dimensionality and
therefore speed-up the processing. Second, because of implementation issues, we found it easier to extract one SIFT
FV and one LCS FV per image and to concatenate them using an early fusion strategy before feeding them to the SVM
classiﬁers (while in our previous experiments, we trained
two classiﬁers separately and peformed late fusion of the
classiﬁer scores).
As for the SVM training, we also use SGD to train onevs-rest linear SVM classiﬁers. Given the size of these datasets,
at each pass of the SGD routine we sample all positives but
only a random subset of negatives .
5.1 ILSVRC 2010
ILSVRC 2010 contains approx. 1.4M images of 1K classes. We use the standard protocol which consists in training on the “train” set (1.2M images), validating
on the “val” set (50K images) and testing on the “test” set
(150K) images. We report the top-5 classiﬁcation accuracy
(in %) as is standard practice on this dataset.
Impact of compression parameters. We ﬁrst study the
impact of the compression parameters on the accuracy. We
can vary the average number of bits per dimension b and the
group size G. We show results on 4K-dim and 64K-dim FV
features in Figure 8 (using respectively a GMM with 16 and
256 Gaussians). Only the training samples are compressed,
not the test samples. In the case of the 4K FVs, we were able
to run the uncompressed baseline as the uncompressed training set (approx. 19 GBs) could ﬁt in the RAM of our servers.
However, this was not possible in the case of the 64K-dim
FVs (approx. 310 GBs). As expected, the accuracy increases
with b: more bits per dimension lead to a better preservation
of the information for a given G. Also, as expected, the accuracy increases with G: taking into account the correlation
Table 6 Memory required to store the ILSVRC 2010 training set using
4K-dim or 64K-dim FVs. For PQ, we used b = 1 and G = 8.
Uncompressed
PQ + sparsity
64K-dim FV
between the dimensions also leads to less loss of information for a given b. Note that by using b = 1 and G = 8, we
reduce the storage by a factor 32 with a very limited loss of
accuracy. For instance, for 4K-dim FVs, the drop with respect to the uncompressed baseline is only 2.1% (from 64.2
We underline that the previous results only make use of
PQ compression but do not include the sparsity compression
described in section 4.2. We report in Table 6 the compression gains with and without sparsity compression for b = 1
and G = 8. We note that the impact of the sparsity compression is limited for 4K-dim FVs: around 10% savings. This is
to be expected given the very tiny number of visual words in
the GMM vocabulary in such a case (only 16). In the case of
64K-dim FVs, the gain from the sparsity encoding is more
substantial: around 30%. This gain increases with the GMM
vocabulary size and it would be also larger if we made use of
spatial pyramids (this was veriﬁed experimentally through
preliminary experiments). In what follows, except where the
contrary is speciﬁed, we use as default compression parameters b =1 and G =8.
Compression and regularization. We now evaluate how
compression impacts regularization, i.e. whether the optimal
regularization parameter changes with compression. Since
we want to compare systems with compressed and uncompressed features, we focus on the 4K-dim FVs. Results are
shown in Figure 9 for three losses: the quadratic and log
losses which are twice differentiable and which are discussed
in section 4.4 and the hinge loss which corresponds to the
SVM classiﬁer but to which our theoretical analysis is not
applicable. We also test two compression settings: our default setting with G =8 and b =1 and a lower-performing
scalar quantization setting with G =1 and b =1. We can see
that for all three losses the optimal regularization parameter
is the same or very similar with and without compression.
Especially, in the quadratic case, as opposed to what is suggested by our analysis, we do not manage to improve the
accuracy with compressed features by cross-validating the
regularization parameter. This might indicate that our analysis is too simple to represent real-world datasets and that we
need to take into account more complex phenomena, e.g. by
considering a more elaborate noise model or by including
higher orders in the Taylor expansion.
K-nearest neighbors search with PQ compression. We
now compare the effect of PQ compression on the linear
SVM classiﬁer to its effect on a k-Nearest Neighbors (k-
Jorge S´anchez et al.
Bits per dimension (compression factor)
Top−5 Accuracy (in %)
Bits per dimension (compression factor)
Top−5 Accuracy (in %)
Fig. 8 Compression results on ILSVRC 2010 with 4K-dim FVs (left) and 64K-dim FVs (right) when varying the number of bits per dimension b
and the sub-vector dimensionality G.
Regularization parameter λ
Top−5 Accuracy (in %)
PQ: G=8, b=1
PQ: G=1, b=1
Regularization parameter λ
Top−5 Accuracy (in %)
PQ: G=8, b=1
PQ: G=1, b=1
Regularization parameter λ
Top−5 Accuracy (in %)
PQ: G=8, b=1
PQ: G=1, b=1
Fig. 9 Impact of the regularization on uncompressed and compressed features for the quadratic loss (left), the log loss (middle) and the hinge loss
(right). Results on ILSVRC 2010 with 4K-dim FVs. We experimented with two compression settings: our default setting (G = 8 and b = 1) and a
degraded setting corresponding to scalar quantization (G = 1 and b = 1).
NN) classiﬁer. For this evaluation we use the 4K- and 64Kdim FVs. We experiment with two compression settings:
a “strong” compression with b = 1 bit per dimension and
G = 8 (compression factor 32, our default setting) and and
a “weak” compression with b = 4 bits per dimension and
G = 2 (compression factor 8).
We follow the asymmetric testing strategy, where only
the training images are PQ encoded and the test images are
uncompressed. This has also been used for nearest neighbor
image retrieval using an PQ encoded dataset and uncompressed query images . We compare three
versions of the k-NN classiﬁer. The ﬁrst directly uses the
PQ-encoded Fisher vectors. Note that since the PQ compression is lossy, the norm of the reconstructed vectors is generally not preserved. Since we use ℓ2 normalized signatures,
however, we can correct the norms after decompression. In
the second version we re-normalize the decompressed vectors to have unit ℓ2 norm on both parts corresponding to the
SIFT and the LCS color descriptors. Finally, we consider a
third variant that simply re-normalizes the complete vector,
without taking into account that it consists of two subvectors
that should have unit norm each.
Because of the cost of running k-NN classiﬁcation, we
used only a subset of 5K test images to evaluate accuracy. In
preliminary experiments on the 4K features, the difference
in performance between running the test on the full 150K
images and the subset of 5K was small: we observed differences in the order of 0.5%. Hence, we believe this subset
of 5K images to be representative of the full set. Note that
we could not run uncompressed experiments in a reasonable
amount of time with the 64K-dim features, even on this reduced test set. For the SVM, although running experiments
on the full test set is possible, we report here results on the
same subset of 5K test images for a fair comparison.
For all k-NN experiments we select the best parameter k
on the test set for simplicity. Typically for the 4K-dim features and Euclidean distance around 100 neighbors are used,
while for the 64K-dim features around 200 neighbors are
used. When using the re-normalization the optimal number
of neighbors is reduced by a factor two, for both 4K and
64K dimensional features. In either case, the performance is
stable over a reasonably large range of values for k.
In Table 7 we show the results of our comparison, using the ﬁrst and second variants. We make the following ob-
Image Classiﬁcation with the Fisher Vector: Theory and Practice
Table 7 Comparison of top-5 accuracy of SVM and k-NN classiﬁcation without compression (“exact”), with “weak” PQ compression
(b = 4) and “strong” compression (b = 1).
64K-dim FV
weak strong
k-NN, direct
k-NN, re-normalization
Images sorted on frequency
4k re−norm
64k re−norm
Fig. 10 Impact of the re-normalization on the frequencies of how often
an image is selected as neighbor. We analyze the 200 nearest neighbors
from the 5K queries, for the 4K and 64K dimensional FV with strong
compression and the exact non-compressed 4K FVs.
servations. First the performance of the k-NN classiﬁer is
always signiﬁcantly inferior to that of the SVM classiﬁer.
Second, when no re-normalization is used, we observe a decrease in accuracy with the k-NN classiﬁer when going from
4K to 64K dimensional FVs. When using re-normalization,
however, the performance does improve from 4K to 64K
features. The normalization per FV (SIFT and LCS) is also
important; when only a single re-normalization is applied to
the complete vector an accuracy of 45.9 is obtained for the
64K-dim feature in the weak compression setting, compared
to 47.0 when both FVs are used separately. Third, the modest improvement of around 3 absolute points when going
from 4K to 64K-dim features for the k-NN classiﬁer might
be explained because the k-NN classiﬁer is not appropriate
for high-dimensional features since all points are approximately at the same distance. In such a case, it can be beneﬁcial to employ metric learning techniques, see e.g. Mensink
et al . However, this is outside the scope of the current
Finally, to obtain a better insight on the inﬂuence of the
re-normalization, we analyze its impact on which images
are selected as nearest neighbors. In Figure 10, we show
for different settings the distribution of how often images
are selected as one of the 200 nearest neighbors for the 5K
queries. In particular, for each train image we count how often it is referenced as a neighbor, then sort these counts in
decreasing order, and plot these curves on log-log axes. We
compare the distributions for strongly compressed 4K and
64K FVs, with and without re-normalization. Moreover, we
also include the distribution for the exact (non-compressed)
4K features. Since there are 5K test images, each of which
has 200 neighbors, and about 1M training images, a uniform distribution over the neighbor selection would roughly
select each training image once. We observe that without renormalization, however, a few images are selected very often. For example, for the 64K features there are images that
are referenced as a neighbor for more than 4500 queries.
When using the re-normalization the neighbor frequencies
are more well behaved, resulting in better classiﬁcation performance, see Table 7. In this case, for the 64K features the
most frequent neighbor is referenced less that 150 times: a
reduction by a factor 30 as compared to the maximum frequency without re-normalization. For the 4K features the
correction of the neighbor frequencies is even more striking: with re-normalization the neighbor frequencies essentially match those of the exact (non-compressed) features.
Comparison with the state-of-the-art. We now compare our results with the state-of-the-art. Using 64K-dim
FVs and the “weak” compression setting described earlier,
we achieve 73.1% top-5 accuracy. Using our default compression setting, we achieve 72.0%. This is to be compared
with the winning NEC-UIUC-Rutgers system which obtained
71.8% accuracy during the challenge , see
also . Their system combined 6 sub-systems
with different patch descriptors, patch encoding schemes and
spatial pyramids. reported
slightly better results – 74.5% top-5 accuracy – using 1Mdim FVs (with more Gaussians and spatial pyramids) but
such high-dimensional features are signiﬁcantly more costly
than the 64K-dim features we used in the present paper. In
a very recent work, Krizhevsky et al reported significantly better results using a deep learning network. They
achieved an 83.0% top-5 accuracy using a network with 8
layers. We note that to increase the size of the training set –
which was found to be necessary to train their large network
– they used different data augmentation strategies (random
cropping of sub-images and random perturbations of the illumination) which we did not use.
We can also compare our results to those of Bergamo
and Torresani whose purpose was not to obtain the
best possible results at any cost but the best possible results
for a given storage budget. Using a 15,458-dim binary representation based on meta-class features, Bergamo and Torresani manage to compress the training set to 2.16 GBs
and report a top-1 classiﬁcation accuracy of 36.0%. Using
4K-dim FVs, our top-1 accuracy is 39.7% while our storage
requirements are four times smaller, see Table 6. We underline that the FV representation and the metaclass features of
Jorge S´anchez et al.
Bergamo and Torresani are not exclusive but complementary. Indeed, the metaclass features could be computed
over FV representations, thus leading to potential improvements.
5.2 ImageNet10K
ImageNet10K Deng et al is a subset of ImageNet
which contains approximately 9M images corresponding to
roughly 10K classes.
For these experiments, we used the exact same setting
as for our ILSVRC 2010 experiments: we do not use spatial
pyramids and the SIFT- and LCS-FVs are concatenated to
obtain either a 4K-dim of a 64K-dim FV. For the compression, we use the default setting (b = 1 bit per dimension and
G = 8). To train the one-vs-rest linear SVMs, we also follow
Perronnin et al and subsample the negatives. At test
time, we also compress FVs because of the large of number
of test images to store on disk (4.5M).
In our experiments, we follow the protocol of S´anchez
and Perronnin and use half of the images for training,
50K for validation and the rest for testing. We compute the
top-1 classiﬁcation accuracy for each class and report the average per-class acuracy as is standard on this dataset . Using the 4Kdim FVs, we achieve a top-1 accuracy of 14.0% and using
the 64K-dim FVs 21.9%.
Comparison with the state-of-the-art. Deng et al 
achive 6.4% accuracy using a BoV representation and the
fast Intesection kernel SVM (IKSVM) technique of Maji
and Berg . Our compressed FV results are more than
3 times higher. S´anchez and Perronnin reported a
16.7% accuracy using FVs but without color information.
Using similar features Perronnin et al improved these
results to 19.1% by carefully cross-validating the balance
between the positive and negative samples, a good practice
we also used in the current work. obtained 13.9% using the same features and PQ compression
as we used in this paper, but with a nearest mean classiﬁer
which only requires a fraction of the training time.
Le et al and Krizhevsky et al also report
results on the same subset of 10K classes using deep architectures. Both networks have 9 layers but they are quite
different. In , the features are learned using a deep autoencoder which is constructed by replicating
three times the same three layers – made of local ﬁltering,
local pooling and contrast normalization. Classiﬁcation is
then performed using linear classiﬁers (trained with a logistic loss). In the network consists
of 6 convolutional layers plus three fully connected layers.
The output of the last fully connected layer is fed to a softmax which produces a distribution over the class labels. Le
et al reports a top-1 per-image accuracy of 19.2% and
Krizhevsky et al of 32.6%.9
6 Conclusion
In this work, we proposed the Fisher Vector representation
as an alternative to the popular Bag-of-Visual words (BoV)
encoding technique commonly adopted in the image classiﬁcation and retrieval literature. Within the Fisher Vector
framework, images are characterized by ﬁrst extracting a set
of low-level patch descriptors and then computing their deviations from a “universal” generative model, i.e. a probabilistic visual vocabulary learned ofﬂine from a large set of
samples. This characterization is given as a gradient vector
w.r.t. the parameters of the model, which we choose to be a
Gaussian Mixture with diagonal covariances.
Compared to the BoV, the Fisher Vector offers a more
complete representation of the sample set, as it encodes not
only the (probabilistic) count of occurrences but also higher
order statistics related to its distribution w.r.t. the words in
the vocabulary. The better use of the information provided
by the model translates also into a more efﬁcient representation, since much smaller vocabularies are required in order to achieve a given performance. We showed experimentally on three challenging small and medium-scale datasets
that this additional information brings large improvements
in terms of classiﬁcation accuracy and, importantly, that stateof-the-art performances can be achieved with efﬁcient linear classiﬁers. This makes the Fisher Vector well suited for
large-scale classiﬁcation.
However, being very high-dimensional and dense, the
Fisher Vector becomes impractical for large-scale applications due to storage limitations. We addressed this problem
by using Product Quantization, which enables balancing accuracy, CPU cost, and memory usage. We provided a theoretical analysis of the inﬂuence of Product Quantization on
the classiﬁer learning. For the linear case, we showed that
compression using quantization has an “unregularization”
effect when learning the classiﬁers.
Finally, we reported results on two large-scale datasets –
including up to 9M images and 10K classes– and performed
a detailed comparison with k-NN classiﬁcation. We showed
that Fisher Vectors can be compressed by a factor of 32 with
a very little impact on classiﬁcation accuracy.
While it is standard practice to report per-class accuracy on
this dataset , S´anchez and Perronnin ),
Krizhevsky et al , Le et al report a per-image accuracy.
This results in a more optimistic number since those classes which are
over-represented in the test data also have more training samples and
therefore have (on average) a higher accuracy than those classes which
are under-represented. This was clariﬁed through a personal correspondence with the ﬁrst authors of Krizhevsky et al , Le et al .
Image Classiﬁcation with the Fisher Vector: Theory and Practice
A An approximation of the Fisher information matrix
In this appendix we show that, under the assumption that the posterior
distribution γx(k) = wkuk(x)/uλ(x) is sharply peaked, the normalization with the Fisher Information Matrix (FIM) takes a diagonal form.
Throughout this appendix we assume the data x to be one dimensional.
The extension to the multidimensional data case is immediate for the
mixtures of Gaussians with diagonal covariance matrices that we are
interested in.
Under some mild regularity conditions on uλ(x), the entries of the
FIM can be expressed as:
[Fλ]i,j = E
−∂2 loguλ(x)
First, let us consider the partial derivatives of the posteriors w.r.t.
the mean and variance parameters. If we use θs to denote one such parameter associated with us(x), i.e. mixture component number s, then:
= γx(k)∂logγx(k)
logwk +loguk(x)−loguλ(x)
[[k = s]]∂logus(x)
−∂loguλ(x)
[[k = s]]∂logus(x)
−γx(s)∂logus(x)
[[k = s]]−γx(s)
∂logus(x)
where [[·]] is the Iverson bracket notation which equals one if the argument is true, and zero otherwise. It is easy to verify that the assumption
that the posterior is sharply peaked implies that the partial derivative is
approximately zero, since the assumption implies that (i) γx(k)γx(s) ≈0
if k ̸= s and (ii) γx(k) ≈γx(k)γx(s) if k = s.
From this result and equations (12), (13), and (14), it is then easy
to see that second order derivatives are zero if (i) they involve mean
or variance parameters corresponding to different mixture components
(k ̸= s), or if (ii) they involve a mixing weight parameter and a mean or
variance parameter (possibly from the same component).
To see that the cross terms for mean and variance of the same
mixture component are zero, we again rely on the observation that
∂γx(k)/∂θs ≈0 to obtain:
∂2 loguλ(x)
≈γx(k)(x−µk)∂σ−2
k γx(k)(x−µk)
Then by integration we obtain:
[Fλ]σk,µk = −
x uλ(x)∂2 loguλ(x)
x uλ(x)γx(k)(x−µk)dx
x uk(x)(x−µk)dx = 0
We now compute the second order derivatives w.r.t. the means:
∂2 loguλ(x)
k γx(k)∂(x−µk)
Integration then gives:
[Fλ]µk,µk = −
x uλ(x)∂2 loguλ(x)
x uλ(x)γx(k)dx
x uk(x)dx = σ−2
and the corresponding entry in Lλ equals σk/√wk. This leads to the
normalized gradients as presented in (17).
Similarly, for the variance parameters we obtain:
∂2 loguλ(x)
1−3(x−µk)2/σ2
Integration then gives:
[Fλ]σk,σk ≈σ−2
x uλ(x)γx(k)
3(x−µk)2/σ2
3(x−µk)2/σ2
which leads to a corresponding entry in Lλ of σk/√2wk. This leads to
the normalized gradients as presented in (18).
Finally, the computation of the normalization coefﬁcients for the
mixing weights is somewhat more involved. To compute the second order derivatives involving mixing weight parameters only, we will make
use of the partial derivative of the posterior probabilities γx(k):
[[k = s]]−γx(s)
where the approximation follows from the same observations as used
in (62). Using this approximation, the second order derivatives w.r.t.
mixing weights are:
∂2 loguλ(x)
 [[k = s]]−ws
Since this result is independent of x, the corresponding block of the
FIM is simply obtained by collecting the negative second order gradients in matrix form:
[Fλ]α,α = ww′ −diag(w),
where we used w and α to denote the vector of all mixing weights, and
mixing weight parameters respectively.
Since the mixing weights sum to one, it is easy to show that this
matrix is non-invertible by verifying that the constant vector is an eigenvector of this matrix with associated eigenvalue zero. In fact, since
there are only K −1 degrees of freedom in the mixing weights, we can
ﬁx αK = 0 without loss of generality, and work with a reduced set of
K−1 mixing weight parameters. Now, let us make the following deﬁnitions: let ˜α = (α1,...,αK−1)T denote the vector of the ﬁrst K −1 mixing weight parameters, let Gx
˜α denote the gradient vector with respect
to these, and F˜α the corresponding matrix of second order derivatives.
Using this deﬁnition F˜α is invertible, and using Woodburry’s matrix
inversion lemma, we can show that
(γx(k)−wk)(γy(k)−wk)/wk.
The last form shows that the inner product, normalized by the inverse
of the non-diagonal K −1 dimensional square matrix F˜α, can in fact be
obtained as a simple inner product between the normalized version of
the K dimensional gradient vectors as deﬁned in (12), i.e. with entries
/√wk. This leads to the normalized gradients as presented
Note also that, if we consider the complete data likelihood:
p(x,z|λ) = uλ(x)p(z|x,λ)
the Fisher information decomposes as:
Fc = Fλ +Fr,
where Fc, Fλ and Fr denote the FIM of the complete, marginal (observed) and conditional terms. Using the 1-of-K formulation for z, it
Jorge S´anchez et al.
can be shown that Fc has a diagonal form with entries given by (69),
(72) and (76), respectively. Therefore, Fr can be seen as the amount of
“information” lost by not knowing the true mixture component generating each of the x’s Titterington et al . By requiring the distribution of the γx(k) to be “sharply peaked” we are making the approximation zk ≈γx(k).
From this derivation we conclude that the assumption of sharply
peaked posteriors leads to a diagonal approximation of the FIM, which
can therefore be taken into account by a coordinate-wise normalization
of the gradient vectors.
B Good Practices for Gaussian Mixture Modeling
We now provide some good practices for Gaussian Mixture Modeling.
For a public GMM implementation and for more details on how to train
and test GMMs, we refer the reader to the excellent HMM ToolKit
(HTK) Young et al 10.
Computation in the log domain. We ﬁrst describe how to compute in practice the likelihood (8) and the soft-assignment (15). Since
the low-level descriptors are quite high-dimensional (typically D = 64
in our experiments), the likelihood values uk(x) for each Gaussian can
be extremely small (and even fall below machine precision if using
ﬂoating point values) because of the exp of equation (9). Hence, for a
stable implementation, it is of utmost importance to perform all computations in the log domain. In practice, for descriptor x, one never
computes uk(x) but
loguk(x) = −1
log(2π)+log(σ2
kd)+ (xd −µkd)2)
where the subscript d denotes the d-th dimension of a vector. To compute the log-likelihood loguλ(x) = log∑K
k=1 wkuk(x), one does so incrementally by writing loguλ(x) = log
 w1u1(x)+∑K
k=2 wkuk(x)
by using the fact that log(a+b) = log(a)+log(1+exp(log(b)−log(a))
to remain in the log domain.
Similarly, to compute the posterior probability (15), one writes
γk = exp[log(wkuk(x))−log(uλ(x))] to operate in the log domain.
Variance ﬂooring. Because the variance σ2
k appears in a log and
as a denominator in equation (79), too small values of the variance
can lead to instabilities in the Gaussian computations. In our case, this
is even more likely to happen since we extract patches densely and we
do not discard uniform patches. In our experience, such patches tend to
cluster in a Gaussian mixture component with a tiny variance. To avoid
this issue, we use variance ﬂooring: we compute the global covariance
matrix over all our training set and we enforce the variance of each
Gaussian to be no smaller than a constant α times the global variance.
Such an operation is referred to as variance ﬂooring. HTK suggest a
value α = 0.01.
Posterior thresholding. To reduce the cost of training GMMs as
well as the cost of computing FVs, we assume that all the posteriors
γ(k) which are below a given threshold θ are equal to exactly zero.
In practice, we use a quite conservative threshold θ = 10−4 and for a
GMM with 256 Gaussians, 5 to 10 Gaussians maximum exceed this
threshold. After discarding some of the γ(k) values, we renormalize
the γ′s to ensure that we still have ∑K
k=1 γ(k) = 1.
Note that this operation does not only reduce the computational
cost, it also sparsiﬁes the FV (see section 4.2). Without such a posterior
thresholding, the FV (or the soft-BoV) would be completely dense.
Incremental training. It is well known that the Maximum Likelihood (ML) estimation of a GMM is a non-convex optimization problem for more than one Gaussian. Hence, different initializations might
lead to different solutions. While in our experience, we have never observed a drastic inﬂuence of the initialization on the end result, we
10 Available at: 
strongly advise the use of an iterative process as suggested for instance
in . This iterative procedure consists in starting with
a single Gaussian (for which a closed-form formula exists), splitting all
Gaussians by slightly perturbing the mean and then re-estimating the
GMM parameters with EM. This iterative splitting-training strategy enables cross-validating and monitoring the inﬂuence of the number of
Gaussians K in a consistent way.