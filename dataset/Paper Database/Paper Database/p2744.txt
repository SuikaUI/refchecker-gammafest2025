Are pre-trained CNNs good feature extractors for
anomaly detection in surveillance videos?
Tiago S. Nazare∗‡, Rodrigo F. de Mello∗, Moacir A. Ponti∗
∗ICMC, University of São Paulo, São Carlos, Brazil.
‡Data Science Team, Itaú-Unibanco, São Paulo, Brazil.
Email: , , 
Abstract—Recently, several techniques have been explored to
detect unusual behaviour in surveillance videos. Nevertheless,
few studies leverage features from pre-trained CNNs and none
of then present a comparison of features generate by different
models. Motivated by this gap, we compare features extracted
by four state-of-the-art image classiﬁcation networks as a way
of describing patches from security video frames. We carry out
experiments on the Ped1 and Ped2 datasets and analyze the usage
of different feature normalization techniques. Our results indicate
that choosing the appropriate normalization is crucial to improve
the anomaly detection performance when working with CNN
features. Also, in the Ped2 dataset our approach was able to
obtain results comparable to the ones of several state-of-the-art
methods. Lastly, as our method only considers the appearance of
each frame, we believe that it can be combined with approaches
that focus on motion patterns to further improve performance.
I. INTRODUCTION
Nowadays security cameras are widely employed to monitor
public spaces – such as malls, squares and universities. Yet,
those surveillance cameras may be ineffective, mainly because
each video feed needs a person constantly watching it. Keeping
track of the events in security videos is very hard for humans,
especially for two reasons: 1) a single person is responsible for
monitoring several cameras simultaneously ; 2) it is hard
for people to maintain an acceptable level of attention when
watching this kind of video . Due to the aforementioned
issues, security footage is of little help with regards to preventing dangerous situations and end up being used mostly for
investigation purposes, after something already happened.
The ineffectuality of surveillance systems motivated the
machine vision community to work on automated systems to
detect unusual behaviour in security videos, consequently, we
have seen outstanding advances in this area. Over the last
few years a great deal of methods have been proposed to
detect anomalies in videos. Among the approaches employed
to tackle this issue we have: time series decomposition of
optical-ﬂow , optical-ﬂow features , , dictionary learning , auto-encoders and GANs (generative adversarial
networks) . Despite all progress obtained over the last
few years, automatically detecting unusual events in videos
remains an open research area and several approaches are yet
to be explored.
Any opinions, ﬁndings, and conclusions expressed in this manuscript are
those of the authors and do not necessarily reﬂect the views, ofﬁcial policy
or position of the Itaú-Unibanco, FAPESP and CNPq.
One kind of approach that – to the best of our knowledge – have not yet been broadly investigated in anomaly
detection scenarios is transfer-learning. This technique consists
of leveraging knowledge obtained from solving a particular
problem to solve a different one (on a related domain). With
regards to CNNs most transfer learning applications use pretrained networks as feature extractors or as starting point for
training. In recent years, such usage of pre-trained networks
has been considered to be very effective by various studies ,
even when the original and target domains are considerably
different . Still, such performance is not guaranteed for
every scenario, as pointed out by some other studies ,
Motivated by the aforementioned results and the lack of
investigation regarding the usage of pre-trained CNNs to detect
unusual behaviour in security videos, in this paper, we devote
our efforts to evaluate the application of several state-of-the-art
image classiﬁcation CNNs as features extractor machines for
surveillance footage. We compared features generated by four
networks (VGG-16 , RestNet-50 , Xception and
DenseNet-121 ) as a way to describe appearance for frame
regions of security videos. Despite neglecting the motion part
of anomaly detection in videos, our experiments (conducted
using the Ped1 and Ped2 UCSD datasets , ) show that
those features are able to achieve competitive results.
II. RELATED WORK
Recently, the usage of CNN features has been explored
to detect unusual activities in surveillance footage. In ,
the authors describe image regions using AlexNet and,
then, track variations in such description in order to detect
anomalies. With this tracking, they are able to use a image
based CNN to ﬁnd both motion and appearance anomalies.
In a pre-trained C3D model was employed for
feature extraction. Such model was originally designed for
action recognition in videos, thus it learns spatio-temporal
representations, which is very useful to deal with both motion
and appearance abnormalities. Nevertheless, in the pretrained model is used in a classiﬁcation setup, therefore it has
access to instances of both normal and anomalous behaviour
during training.
In spite of the fact that are some studies where pre-trained
networks were employed to detect anomalies in security
videos, they do not compare several models in order to
 
Training videos
Test videos
step 1: feature extraction
step 2: data preparation
step 3: model training and evaluation
Fig. 1: Experimental setup diagram.
determine which one is best suited for the application. For
this reason, in this paper we design a experimental setup and
present several experimental results — employing different
network architectures and data normalization procedures — in
order to shed some light on this issue. Instead of attempting
to outperform state-of-the-art methods, we aim at a better
understanding of how off-the-shelf CNN features behave for
the application of video surveillance, providing a guideline for
future research on how to choose the appropriate model for
this task.
III. EXPERIMENTAL SETUP
In order to measure how well the features extracted from
pre-trained image classiﬁcation CNNs perform, when detecting anomalies in surveillance videos, we employed the
experimental setup depicted in Figure 1. This experimental
setup has three steps.
In the ﬁrst step of our setup we start by converting
each video frame to 384 × 256 pixels. Next, we take image
regions of 32 × 32 pixels using a stride of 16 pixels and
perform a forward pass of each of these patches through the
convolutional part of a CNN pre-trained on the ImageNet
dataset . This image patch end up being represented by
d features, where the value of d depends on the network
– it is equal to the number of ﬁlters (neurons) at the last
convolutional layer of the network. At this point one can notice
that using the convolutional part of the network – instead of
the entire network – is rather convenient in our framework.
This is due to the fact that when using the entire networks
we have to provide input images of the same size of the
original dataset (e.g. 224 × 224), while with the convolutional
part of the network we do not have such constraint. In this
part of our setup, we investigated features generated by the
following state-of-the-art image classiﬁcation models (trained
on ImageNet): VGG-16 , ResNet-50 , Xception 
and DenseNet-121 . Please refer to Section III-B for more
details on each model and also for the number of features
generated by each one of them.
In the second step we prepared the data obtained on the
ﬁrst step in order to use it with an anomaly detection method.
To do so, we started by normalizing the data, in this part
we tested the z-score, 0-1, L1 and L2 normalization methods
(see Section III-A for a description of these techniques).
Given that we employed the nearest neighbour technique to
detect anomalies, this normalization step is fundamental to
the performance of the system. After normalizing the data we
used the Incremental PCA (IPCA) algorithm to reduce
data dimensionality to 50 and 100 dimensions. This particular
method was chosen for two reasons: 1) it can be used to
reduce the number of features and, consequently, speed up
the computation of nearest neighbours; 2) it does not need
to load the entire dataset into RAM in order to compute its
transformation, so it can deal with large datasets such as those
used in this paper.
Lastly, in the third step of our experimental setup, we train
an 1-NN model using the datasets obtained in step two (with
50 or 100 features depending on the experiment). Then, we
ﬁnd the distance of all the test set instances to their nearest
neighbour in the training set. Those distances are used as
an anomaly score. Hence, the higher the distance, the more
a instance (image region) is considered to be anomalous.
It is important to notice that we train only one model to
work with descriptions from the entire image. Also, aiming
to speed up our anomaly detection framework, we used the
approximate nearest neighbour method from . Next, we
obtain the anomaly score of a frame simply by taking the
max score among its patches. Based on those frame scores
we can compute the AUC (Area Under ROC Curve) and EER
(Equal Error Ratio) on a frame-level classiﬁcation.
Our experiments were carried out on the UCSD video
anomaly detection datasets: Ped1 and Ped2 , . More
information about the two datasets are presented Section III-C.
A. Data normalization
In our experiments we used the following four data normalization methods:
• z-score: outputs features centered at the origin (i.e. with
zero mean) and with unit standard deviation by computing:
fi = fi −mean(f)
where fi is the value of feature f for instance i, and the
mean and standard deviation of f is obtained using the
entire training set;
• 0-1: outputs features in the interval. To do so, it
uses the following formula:
fi −min(f)
max(f) −min(f),
where fi is the value of feature f for instance i and the
max and min of f are obtained from the training set;
• L1: in this method all instances (dataset rows) are scaled
so their L1 norm are equal to one, as follows:
where x is an instance and |xf| is the absolute value of
the f-th feature of x.
• L2: similar to the previous one, but using the L2 norm
instead, as in the equation bellow:
B. CNN models
Bellow, we quickly present some of the main characteristics
of the four CNNs used as feature extractors in our experiments.
VGG-16 : is composed of 16 layers, 13 of them convolutional and the remaining 3 dense layers. All convolutional
layers use 3×3 kernels and ReLU as activation function. Also,
those 13 layers are divided into 6 groups and each group has
a max-pooling layer at its end.
ResNet-50 : uses an alternative layer conﬁguration
method, called residual units. Residual units – such as the
one depicted in Figure 2 – allow deeper models to be learned
by using shortcut connections to avoid vanishing/exploding
gradients during training. This network has 49 convolutional
layers and only one fully connected layer. All but one of its
convolutional layers are organized into 16 residual units like
the one of Figure 2.
Xception : is inspired by Inception V3 , and built
by replacing the inception modules with depthwise separable
convolutions. This type of convolution is performed by, ﬁrst,
applying convolutions (e.g. using 3×3 kernels) to each tensor
channel separately and, then, applying an 1 × 1 convolution
to all channels, this process is illustrated in Figure 3. By
using this type of operation the Xception model was able to
outperform Inception V3 for image classiﬁcation using the
same number of trainable parameters.
Conv2D: 1✕1 filters
Conv2D: 3✕3 filters
Conv2D: 1✕1 filters
Fig. 2: Residual unit employed by ResNet-50 .
Fig. 3: Depthwise separable convolution layer used by Xception .
DenseNet-121 : is an architecture similar to ResNets
because they also use shortcut/skipping connections. Nevertheless, instead of employing this type of connection in some
speciﬁc parts of the network, they use shortcut connections to
connect every layer to all of its subsequent layers. The authors
claim this approach avoids vanishing/exploding gradients and
allows the training of even deeper networks when compared
to ResNets.
As previously mentioned, those models have different number of trainable parameters and generate descriptors of different sizes. In Table I, we present the number of trainable
parameters in the convolutional part (feature extractor part)
and the number of generated features of each CNN used in
our experiments.
TABLE I: Number of trainable parameters and output features
for each one of the network architectures used in our experiments.
feature extractor
# of trainable parameters
# of features
VGG-16 
14,714,688
ResNet-50 
23,534,592
Xception 
20,806,952
DenseNet-121 
C. Datasets
Both datasets employed in our experiments were obtained
for stationary surveillance cameras at the UCSD (University of
California, San Diego) campus , . This means that all
of their videos are real (they were not staged). Each dataset
contains videos from a single camera and only the test set
has anomalous events. Some speciﬁcations and sample images
from both datasets are presented in Table II and Figure 4,
respectively.
Fig. 4: Frames from Ped1 (ﬁrst row) and Ped2 (second row), with anomalies manually highlighted by red boxes.
TABLE II: Video speciﬁcations for Ped1 and Ped2.
characteristic
training videos
test videos
frame resolution
frames per video
D. Reproducibility remarks
In our experiments we used the Keras library, and
its pre-trained models, for feature extraction. In order to
normalize our features and compute IPCA, we used the
Scikit-learn library. Lastly, for the approximate
nearest neighbour distance computation we have used the
FLANN library. To facilitate the reproduction of our
results our source code is publicly available1.
IV. RESULTS AND DISCUSSION
As stated in Section III, we trained nearest neighbour
models using features extracted from four pre-trained CNNs.
In those tests, before training our nearest neighbour models,
we ﬁrst normalized our features and, then, used IPCA to
reduce the number of dimensions. Aiming at obtaining the
best results out of our features we experimented with several
conﬁguration regarding both the normalization technique and
the number of dimensions used after the IPCA transformation.
The results of such experiments are presented in Table III.
By looking at those results it is possible to notice that the
normalization method can greatly impact anomaly detection
1 Our source code is available at 
anomaly_detection.
results. Also, it is possible to realize that not all networks
have the same behaviour with regards to normalization. With
ResNet-50 and Xcpetion features we usually obtained the
best results by using the z-score normalization, while with
DenseNet-121 the best results were obtained by using the 0-1
normalization. Considering the number of dimensions used to
train the model, in most cases, going from 50 to 100 features
has shown to be beneﬁcial. Nevertheless, it is important to
keep in mind that increasing the number of features makes
the nearest neighbour inference slower.
Now, we compare the best results obtained in our test
against classic and state-of-the-art results methods on the
UCSD (Ped1 and Ped2) datasets. Such comparison is presented in Table IV. Although the CNN features are not better
when compared to state-of-the-art results, the performance was
reasonable. Specially, considering that feature extraction was
not trained with any data from the target task (i.e. Ped1 and
Ped2 frames) and motion information was ignored when we
only described individual frames. In particular for Ped2, the
results are comparable to the ones obtained by some state-ofthe-art methods, therefore they can be considered as a good
baseline for this dataset. In the case of Ped1, the results are
only comparable to some classic methods. We believe that
one of the main reasons for these lower performance is the
fact that Ped1 has changes in perspective – objects change
size (regarding number of pixels) according to the image
region that they are in. Such characteristic can hamper the
performance in a setup like ours, where a single model is
trained to deal with image patches from the entire frame.
V. CONCLUSION
In this paper we investigated the usage of pre-trained image
classiﬁcation CNNs as feature extractors to tackle the detection
TABLE III: Frame level detection results on Ped1 and Ped2 datasets using several pre-trained CNNs as feature extractors,
various feature normalization techniques and approximate nearest neighbour. Please keep in mind that the lower the EER, the
better and the higher the AUC, the better.
feature extractor
IPCA dimensions
normalization method
VGG-16 
ResNet-50 
Xception 
DenseNet-121 
TABLE IV: Comparison of frame-level anomaly detection on
Ped1 and Ped2 datasets. Please note that the lower the EER,
the better and the higher the AUC, the better.
MPPCA 
Social force 
Sparse reconstruction 
Sparse combination 
Motion inﬂuence map 
Composition pattern 
Flow decomposition 
Adversarial discriminator 
Plug-and-play CNN 
CNN features (best)
of unusual events on videos. According to our experiments
such networks – when paired with suitable data normalization
techniques – can be very useful to detect abnormal events in
security videos. This can be noticed in the Ped2 experiments,
where they were able to achieve results comparable to the
ones of the state-of-the-art techniques. Another important fact
regarding our experiments is that we only explored those
features to model appearance anomalies; hence, we neglected
the motion part of the anomalies. That being said, we strongly
believe that the method proposed in this work can be combined
with methods that focus on motion anomalies (e.g. , )
to obtain better results.
VI. FUTURE WORK
The main points that we intent to investigate in feature
research are:
• The performance of such features in RGB surveillance
videos like the ones of the Avenue dataset . Such
analysis can help to better understand if gray scale image
can hamper the description capabilities of CNNs trained
in RGB domains;
• Combine our method with some methods that only tackle
motion related anomalies, such as the ones presented
in , , to see if their fusion can further improve
performance;
• Test the usage of other anomaly detection techniques, for
instance Isolation Forest with those CNN features.
ACKNOWLEDGMENT
This work was supported by FAPESP ,
#307973/2017-4),
Itaú-Unibanco and partially supported by CEPID-CeMEAI
 .