An Overview of Background Modeling for
Detection of Targets and Anomalies in
Hyperspectral Remotely Sensed Imagery
Stefania Matteoli, Member, IEEE, Marco Diani, Member, IEEE, and James Theiler
Abstract—This paper reviews well-known classic algorithms
and more recent experimental approaches for distinguishing the
weak signal of a target (either known or anomalous) from the
cluttered background of a hyperspectral image. Making this
distinction requires characterization of the targets and characterization of the backgrounds, and our emphasis in this review is
on the backgrounds. We describe a variety of background modeling strategies—Gaussian and non-Gaussian, global and local,
generative and discriminative, parametric and nonparametric,
spectral and spatio-spectral—in the context of how they relate to
the target and anomaly detection problems. We discuss the major
issues addressed by these algorithms, and some of the tradeoffs
made in choosing an effective algorithm for a given detection
application. We identify connections among these algorithms and
point out directions where innovative modeling strategies may be
developed into detection algorithms that are more sensitive and
Terms—Anomaly
detection,
background
estimation,
background modeling, hyperspectral imagery, target detection.
I. INTRODUCTION
OMBINING imaging and high spectral resolution
spectroscopy in a unique system, hyperspectral sensors
provide a powerful means to discriminate the materials in a
scene on the basis of their spectral signature. With the advances
in hyperspectral sensing, the last two decades have seen a
growing employment of hyperspectral target and anomaly
detection in several different ﬁelds of real-life applications
 , , , , , , . Methods to search the
image for small rare objects/materials that have a speciﬁc
spectral signature (target detection) or that are spectrally different from the surrounding background (anomaly detection)
can be useful in environmental monitoring, to locate and
identify hazardous materials; in defense applications, for mine
detection and surveillance; and in public safety, for airborne
search and rescue operations.
To detect small targets or anomalies in hyperspectral imagery, one of the key challenges is to characterize the background.
Indeed, many detection algorithms proceed by estimating what
the target-free background signal would be at a given pixel, then
subtracting this background estimate from what is observed at
that pixel, and ﬁnally performing further processing on the
residual , , , , , . Background data in
hyperspectral imagery may be extremely cluttered, highly
spatially variable, and intrinsically high-dimensional; and
modeling this background is a rich and interesting task, in
which more accurate models are rewarded with better detection
performance. Classical approaches to background modeling,
such as the multivariate normal (MVN) and the subspace
models, have led to a variety of detection algorithms widely
employed in the hyperspectral detection literature. The considerable mismatch that often occurs between classical, simple,
easy to handle models and the complicated nature of real
hyperspectral backgrounds has driven research efforts toward
the development of more complex models capable of strengthening the weak links in the background modeling chain. On the
one hand, many attempts have been pursued to make classical
models more robust to speciﬁc limitations, while preserving
their desirable properties. On the other hand, more recent
research trends have involved tailored multimodal distributions, geometrical approaches, and nonlinear high dimensional
spaces with the aim of providing increased adaptability to
complex hyperspectral backgrounds.
In this paper, an overview of the target detection problem from
the background characterization perspective is provided. In
contrast to reviews that speciﬁcally focus on the detection
algorithms , , , here the emphasis is on characterizing
the background. Conventional background models are reviewed
together with their main limitations and more complex background models speciﬁcally adopted to handle cluttered, spatially
variable, nonhomogeneous backgrounds are surveyed.
The outline of the paper is as follows. First, conventional
widely employed background models and their associated detection algorithms are reviewed in Section II, along with their
major limitations. Section III provides an extensive overview of
both well-known and more recent background models speciﬁcally adopted to handle some of these limitations. Finally,
Section IV summarizes the conclusions of the work.
1939-1404 © 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution
requires IEEE permission. See for more information.
Manuscript received February 28, 2013; revised March 21, 2014; accepted
March 28, 2014. Date of publication May 04, 2014; date of current version
August 01, 2014. The work of S. Matteoli and M. Diani was supported by the
Italian Ministry of University and Research (MIUR) under the framework of the
PRIN project “Very High Spatial and Spectral Resolution Remote Sensing: A
NovelIntegrated Data AnalysisSystem.” The work of J. Theiler was supportedby
the United States Department of Energy’s Laboratory Directed Research and
Development (LDRD) program at Los Alamos National Laboratory.
S. Matteoli and M. Diani are with the Information Engineering Department,
University of Pisa, Pisa 56126, Italy (e-mail: ;
 ).
J. Theiler is with Los Alamos National Laboratory, Los Alamos, NM 87545
USA (e-mail: ).
Color versions of one or more of the ﬁgures in this paper are available online at
 
Digital Object Identiﬁer 10.1109/JSTARS.2014.2315772
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
II. CLASSICAL BACKGROUND MODELS AND RELATED
ANOMALY AND TARGET DETECTION ALGORITHMS
In this section, we review classical background models that
underlie detection algorithms widely employed in the literature.
To this end, some basics of target detection and detector design are
brieﬂy reviewed in Section II-A. Then, Section II-B describes two
speciﬁc background models: 1) the MVN model; and 2) the
subspace model. Finally, some of the limitations of these background models are highlighted and discussed in Section II-C.
A. Anomaly and Target Detection Basics
As an organizing principle, target detection is treated as a
binary hypothesis testing problem that is performed at every
pixel in the image. Let
represent the null hypothesis that there
is not a target at the pixel under test, and
the alternative
hypothesis that a target is present. Further, let
denote the
-dimensional vector corresponding to the hyperspectral measure of the pixel under test, where
is the number of spectral
channels. We write
to indicate that the detection statistic
is compared to a
threshold , with
indicating the null hypothesis of no
target, and
indicating the alternative hypothesis that a
target is present. The detection statistic
is often derived by
exploiting a number
of background pixels, which are assumed
to be free of the target signal. These background pixels may be
characterized by means of a spectral variability model, for
instance, by computing their second-order statistics (e.g., mean
vector μ, correlation matrix
, covariance matrix
evaluating the background probability density function (PDF)
The background data can include all image pixels (i.e., global
background) or just the pixels in the spatial neighborhood of the
pixel under test (local background), thus leading to global and
local detection algorithms, respectively. In the latter case, an
annulus of pixels surrounding the pixel under test (see Fig. 1) is
taken to characterize local background. For the local detector,
choosing annulus size is a compromise between statistical
precision in the estimates of the background properties (e.g.,
mean vector and correlation/covariance matrix), which favors a
large annulus; and spatial nonstationarity of the background,
which favors a small annulus. It can be effective to estimate the
mean vector with a smaller annulus using the pixels physically
close to the tested one, whereas the correlation/covariance matrix
is estimated by using a larger annulus, because more pixels are
required to provide a reliable estimate , . This is consistent
with the nonstationary model suggested in , which models an
optical image as a nonstationary multivariate random process
with a quickly spatially varying mean vector and a more slowly
varying covariance matrix. Also shown in Fig. 1 is a guard
window between the annulus and the pixel under test; this
excludes from local background characterization the pixels that
may contain the target signal, due to their closest proximity to the
pixel under test , .
For the anomaly detection problem , , , we seek
pixels in the image that are unusual compared to the rest of the
pixels (i.e., the background). These pixels generally correspond
to small rare objects that occupy a very small fraction of the area
of the background in which they are embedded.
When a reference spectral signature of a given target of interest
is available, the target detection problem can be addressed by
searching the image for all pixels that contain the given target
 , . This is generally performed by seeking the pixels
whose spectrum shows a high degree of correlation with the
target spectrum , , , , , .
In general, a model that provides a background PDF
lead to an anomaly detector that is of the form of
is a monotonic function of its argument.
One widely employed choice for
is the logarithmic function,
thus obtaining the so-called background log-likelihood
To derive a target detector based on
, one needs a model
for the target. One particularly simple but widely used example is
the additive model, which posits that a target pixel will be of the
is the known target signature and
unknown scalar value (corresponding to the target strength or
“abundance”) . One may then combine this background and
target model by invoking the Generalized Likelihood Ratio Test
(GLRT) to obtain the detector
It is also possible to derive this kind of target detector with a
more general approach than the GLRT .
B. Conventional Approaches to Background Modeling
As is evident from the general equations (2) and (3), both for
the detection of anomalies and for speciﬁc targets, it crucially
matters what model is adopted to characterize the background,
and how that model is estimated. Different background models
lead to different anomaly and target detection algorithms. In the
following, we brieﬂy outline two of the simplest and most widely
used models for the background, namely the MVN model and the
subspace model.
Fig. 1. Graphical representation of the annulus employed by local algorithms.
The guard window is employed to avoid target pixels being included in local
background characterization. Generally (see also Section II-B), a smaller annulus
is used to estimate local background mean vector, whereas a larger one is adopted
to estimate local background correlation/covariance matrix.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
1) The MVN Model: The assumption that the background can
be modeled with an MVN distribution has led to many useful
detection algorithms , , and provides a starting point
for many others (see Section III-A). Indeed, the mathematical
tractability of the MVN has allowed straightforward derivation
of detection algorithms as well as simple analytical evaluation of
theoretical detection performance. Even for data that are not
Gaussian, the MVN model can be effective for high dimensional
data because the central limit theorem implies that many
projections of the data will be approximately Gaussian. Also,
the number of parameters in the MVN model scales only
quadratically (not exponentially, as the curse of dimensionality
would suggest, for more arbitrary distributional models) with the
dimension, and this can provide a good compromise between
under-ﬁtting and over-ﬁtting.
In the MVN background modeling approach, a multivariate
Gaussian parametric model, depending on the mean vector μ and
the covariance matrix
, is assumed for
Among the various detection algorithms derived from the
MVN model, the RX anomaly detection algorithm is
probably the most widely known, having become a standard
against which other anomaly detectors are compared , ,
 , . The RX algorithm, as originally conceived by Reed
and Yu , was born as a local detector. Based on the
aforementioned nonstationary model for optical images , a
local mean removal process , is ﬁrst performed by
sliding an annulus (such as that shown in Fig. 1) over the image.
After local mean removal, the covariance matrix (presumed to
spatially vary more slowly than the mean vector) is locally
estimated over the pixels enclosed within a larger annulus. Such
pixels can reasonably be assumed to follow a (local) MVN
distribution. The RX detector basically evaluates the Mahalanobis magnitude of the mean-subtracted pixel
denotes that the local mean has been removed from
is the local background correlation matrix estimated over
the mean-removed background data in the surrounding annulus.
The RX detection statistic in (5) has constant false alarm rate
(CFAR) even when the correlation matrix is estimated on the
background data .
Although the name RX should strictly refer to the algorithm
described above, this designation has been widely employed in
the literature to indicate the Mahalanobis distance detector ,
 , 
which evaluates the Mahalanobis distance of the pixel
estimated mean vector μ of background data on the basis of the
background covariance matrix estimate
. The Mahalanobis
distance detector has been applied both in global and local
For the additive target detection problem referred to in (3), the
best detector is a matched ﬁlter (MF) 
As with RX, it is possible to compute μ and
locally, from an
annulus surrounding the pixel of interest , .
Other well-known detectors have been derived following the
MVN assumption and adopting the GLRT, including different
variants of the MF , , and the adaptive coherence
estimator (ACE) detector .
2) Background Subspace Model: Subspace models of the
background take a different point of view. Here it is assumed
that the background is well modeled as lying in a low-dimensional
subspace, but the background distribution within that subspace
does not ultimately matter. For the target-free pixels, which are
consistent with the null hypothesis, we can write
deﬁnes the background subspace, β speciﬁes the
coefﬁcients of the linear combination of vectors in
is a “lack-of-ﬁt” noise term that should be of small magnitude if
the model is accurate . Although this is less prescriptive than
models (such as MVN) that provide a speciﬁc
, that lack of
speciﬁcity provides a robustness advantage because the subspace
model does not depend on details of that distribution within the
subspace spanned by
Within this subspace modeling perspective, also the target
signal may be seen as lying in a subspace identiﬁed by
the columns of the matrix
span the subspace and γ is the
unknown vector of coefﬁcients identifying the direction of the
target signature
within that subspace. The aforementioned
additive model is a particular case in which the matrix
a single column
and the direction of the target signature within
the subspace is known. Therefore, the subspace modeling approach relies upon the following model for the hyperspectral
pixel in the presence of the target
In general, the target is presumed to at least have some
components that are orthogonal to the background subspace.
By projecting out the background subspace and looking at the
residuals onto the orthogonal subspace, the target signals may be
enhanced with respect to the background. This basic idea was
espoused in , and called orthogonal subspace projection
(OSP). By building the projection matrix
is the pseudo-inverse of
has the property that
, and by considering the simpli-
ﬁed case of a target with known direction
, from (9) we have
, and now the problem is separating the
projected target signature from the projected noise term. In ,
where it is further assumed that
is a zero-mean Gaussian
random vector with diagonal covariance matrix
identity matrix), this is best done with a ﬁlter that
. The resulting OSP detector, derived also in , is
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
This detector simultaneously projects out the background and
maximally enhances the target signal with respect to the residual
background , .
OSP does not specify how
is to be chosen, and a number of
approaches have been taken; an early, but simple and effective
idea is to use the ﬁrst few principal components of the background covariance matrix , .
The extension of this approach to targets that comprise a
subspace (i.e., of possibly more than just one dimension) goes by
the name of matched subspace detection , . The matched
subspace detectors may be derived by applying the GLRT with
the models in (8) and (9).
The use of subspace modeling in anomaly detection is straightforward. On the basis of the anomaly-free model of (8), anomalousness is given by the magnitude of the projected vector in the
residual subspace
In comparing the OSP detector and subspace anomaly detector
with the MVN-based MF and RX anomaly detector, we see that
plays the role of
. Where the eigenvectors of
(i.e., in the directions where the background data have high
variance), the corresponding eigenvectors of
are small, and
MVN-based detectors suppress the background in those directions. But in those directions, which are speciﬁed by
. Thus, where covariance-based detectors suppress,
subspace methods project out entirely. The relationship between
covariance-based and subspace-based detectors is also pointed
out in , where it is further noted that if
is a matrix of the
dominant (largest) eigenvectors of the covariance matrix
is the “principal component inversion approximation” of
An example of a hybrid detector that employs both schemes is
the sub-space RX (SSRX) anomaly detector . SSRX projects
out the ﬁrst few principal components, computes the covariance
matrix on the remaining components, and employs RX on those
components. A number of hybrid subspace/MVN algorithms are
described in ; among these, the target constrained interference minimized ﬁlter (TCIMF) projects out directions deﬁned by
, similar to the OSP algorithm described above, but
then employs an adaptive MF on the residuals.
Finally, it is worth mentioning that if non-negativity and sumto-one physics-based constraints are imposed over the elements
of the vector β in (8) and (9), the subspace model is called linear
mixing model (LMM) and the columns of the matrix
assume the physical meaning of endmembers.
C. Mismatch of Simple Models and Complex Backgrounds
Although there are many issues that affect the detection
outcome (such as the spectral variability of real-world target
spectral signatures, mismatch between library and in-scene
spectral signatures, inaccuracies in the calibration and atmospheric compensation procedures, and inaccurate noise modeling , , ), one of the main causes that limit detection
performance
inadequate
background
characterization,
especially as regards detection of anomalies. A complex (and
inadequately characterized) background provides a rich and
unpredictable source of false alarms and affects target detectability as well. Background modeling is nontrivial. A good
model needs to manage both background spatial variability
(e.g., local, global, and mixture-based models) and spectral
variability (e.g., probabilistic covariance-based models, or subspace-based models). Although conventional background models may seem effective on theoretical grounds, they cannot adapt
to all the diverse situations that can occur in reality. This section
will provide a brief snapshot of the major aspects that keep
bringing further challenges in background modeling.
1) Non-Gaussian Behavior of the Background: Many
conventional
algorithms
background data arise from an MVN distribution. However,
in real hyperspectral data, truly Gaussian behavior rarely occurs
 , , , . This is especially the case for global
detection algorithms, where the MVN model is adopted to
explain the spectral variability of complex backgrounds
encompassing several different materials and objects. A
graphical example is provided in Fig. 2(a) and (b). Fig. 2(a)
displays a true-color image of an urban area. The scene includes
different types of terrain, water, roads, and buildings. Fig. 2(b)
shows a scatterplot of the data in a simpliﬁed two-dimensional
domain along two spectral channels. It is visually evident that the
data distribution is far from Gaussian.
In fact, the MVN model has more effectively been employed
to characterize background pixels in a (possibly) homogeneous
local neighborhood around the test pixel within the framework of
local algorithms. However, if the local neighborhood includes
pixels belonging to different objects and materials, the local
background is likely to follow a multi-modal non-Gaussian
distribution. This situation is exempliﬁed in Fig. 2(c) and (d).
Attempts to enforce the Gaussian assumption may be obtained
by performing a local mean removal procedure (as in the RX
algorithm as proposed by Reed and Yu ). In this case, local
background pixels after local mean removal tend to follow a
distribution that is closer to Gaussian and (mostly) mono-modal,
as exempliﬁed in Fig. 2(e) and (f).
Although MVN model often provides an incomplete representation of hyperspectral backgrounds, there are a number of
algorithms that still use the Gaussian assumption and try to
improve MVN model effectiveness. These algorithms are described in Section III-A. More recent research trends have
deﬁnitely dropped the Gaussian assumption in favor of more
complex multivariate models. These algorithms based on non-
MVN models are described in Section III-B.
2) Contamination Due to the Target Signal and Outliers: Most
detection algorithms require background second-order statistics
(i.e., mean vector and covariance matrix) to be estimated by a set
of data having the same distribution as the background. In
operational situations, it may happen that this set of data is
“contaminated” by the presence of pixels that strongly deviate
from the background distribution such as outliers or pixels
containing the target signal. This contamination leads to the
corruption
second-order
statistics.
covariance matrix is particularly susceptible to corruption due
to contamination and this is generally coupled with a general
degradation of the detection performance , , , ,
Contamination
particularly
deleterious if the contaminating pixels contain the target
signal, because the background characterization procedure is
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
performed violating the key assumption under which adaptive
detectors are developed, i.e., the availability of target-free
training data.
Fig. 3 shows a graphical example of the impact of background
contamination by the target signal on local RX detection. Fig. 3
(a) displays the situation of local RX applied to a target pixel,
while the local background is homogeneous except for the
presence of one contamination source bearing the target signal.
Fig. 3(b) shows a scatterplot of the data in a simpliﬁed twodimensional domain along the two principal axes. Although only
a low percentage (about 3%) of background data is contaminated
by the target signal, RX contours are strongly biased by the
contaminated data and RX outcome at the target test pixel is
expected to be much lower than the one that would be obtained in
a contamination-free case.
Algorithms speciﬁcally developed to deal with covariance
corruption due to contamination are described in Section III-A2.
3) Edges and Spatial Structures: In contrast with global
background characterization models, local models characterize
the background in a small neighborhood of each pixel, thus
adapting to the spatially varying statistics of the background. The
result is that whereas global background characterization
methods would provide the same output even if the positions
of all the pixels were scrambled, local methods implicitly
incorporate into the model that the pixels are laid out
according to speciﬁc spatial patterns, textures, and structures.
In Fig. 4, a graphical example highlights this distinction by
considering the background mean vector estimator. The residual
images obtained from the original image after mean vector
removal may represent the input of a given detection
algorithm, such as the Mahalanobis distance detector. As it is
evident in the ﬁgure, local mean removal [Fig. 4(c)–(f)] provides
stronger background suppression than global mean removal
[Fig. 4(a), (b)]. Furthermore, the smaller the neighborhood
Fig. 2. Graphical example of a situation in which the background data are not well explained by the MVN model. (a) Spatial domain featuring a SpecTIR hyperspectral
image acquired over the city of Reno . A true-color representation is provided. (b) Spectral domain with respect to two spectral channels. The image pixels are
represented as green circles. MVN contours are plotted in blue. (c) Spatial domain pertaining to a
subset of the image. (d) Spectral domain with respect to two
spectral channels. The pixels belonging to the
image subset are represented as green circles. MVN contours are plotted in blue. (e) Spatial domain pertaining to
subset of the image after local mean removal. (f) Spectral domain with respect to two spectral channels. The pixels belonging to the
subset after local mean removal are represented as cyan circles. MVN contours for the data after local mean removal are plotted in blue.
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
used for evaluating the mean vector, the higher the capability to
adapt to background variations. Nonetheless, Fig. 4(e) and (f)
clearly shows that, even though a small
annulus is
background
structures
themselves in the residual image. These residual structures,
transitions
background objects, are likely to be source of false alarms
after the detection step. Therefore, methods to explicitly
Fig. 3. Graphical example of a situation in which the background data are contaminated by the target signal. (a) Spatial domain featuring the test pixel
belonging to a
target (in red), a homogeneous local background (within the green annulus), and one contamination source (in yellow) bearing the same signal as the target. (b) Spectral
domainwith respect to the two principalaxes. The pixelsbelongingto the localhomogenousbackgroundare representedas green circles, the target pixel is denotedwith
a red diamond, and the pixels belonging to the contamination source are identiﬁed as yellow squares. RX contours for the contaminated background are plotted in blue.
RX contours for the contamination-free case are plotted in magenta as reference. Contamination clearly biases RX contours and RX score in
is much lower than the
contamination-free case.
Fig. 4. Graphical example highlighting the impact of edges and structures on a simple background estimation model, using the image shown in Fig. 2(a). (a) Residual
image (true-color representation) after removal of the global mean vector and (b) corresponding energyimage. The major background structures are still clearly evident.
(c) Residual image (true-color representation) after removal of the local mean vector using a
annulus and (d) corresponding energy image. The gross background
structures are no longer evident. Edges and transitions between different background objects emerge, though, with respect to most of pixels where background is
suppressed. (e) Residual image (true-color representation) after removal of the local mean vector using a
annulus and (f) corresponding energy image. Using a
smaller annulus results in stronger background suppression. However, edges and transitions still are present in the residual image, though at a lesser extent.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
account for background structures, either during or after the
performance. Some possible approaches will be described in
Section III-C.
III. IMPROVED BACKGROUND MODELING FOR IMPROVED
DETECTION PERFORMANCE
This section will provide an overview of established as well
as more recent attempts to improve background modeling in
hyperspectral target detection applications. Section III-A will
review robust MVN models, that is background models that still
adopt the Gaussian assumption, although improving some of its
limitations. Models that considerably depart from the Gaussian
assumption in favor of an increased adaptability to complex
nonhomogeneous backgrounds are presented in Section III-B.
Section III-C describes approaches expressly designed to account
for spatial structures and edges. All of these models are summarized in Fig. 5, which also provides a graphical outline for this
A. Robust MVN Background Modeling
Although the MVN model is clearly incomplete—as seen, for
instance, in Fig. 2(b), (d), and (f)—it is an approximation that can
still be useful. In Section III-B, we will describe various non-
MVN models for the background, but here we will discuss
approaches for making MVN more effective. In particular,
although the MVN is a relatively simple model, estimation of
the covariance matrix is not trivial, particularly for local models
where the number of samples per estimate is small. Furthermore,
the presence of outliers in the background data brings further
challenges for reliable covariance matrix estimation.
Covariance
Estimation: Covariance matrix estimation is often a challenge.
Although one can estimate the covariance matrix in a
straightforward way, most detection algorithms actually use
the inverse covariance matrix estimate, and the best estimate
of the inverse covariance is not the same thing as the inverse of
the best estimate of the covariance. The difﬁculty associated with
covariance matrix estimation is increased for local algorithms,
due to the small sample size. In , it is shown that the MF
exhibits an average performance loss that scales like
is the number of channels and
is the number of independent
samples used to compute the sample covariance matrix. It has
been pointed out in and , however, that this performance
loss estimate is optimistic in the case that the covariance matrix
arises from a distribution that is not a single Gaussian of the given
covariance matrix. Similar performance scaling with
been reported for RX-based anomaly detection .
a) Shrinkage: One of the most straightforward ways to
improve the estimate of the inverse covariance matrix is by
shrinkage , , , , . If we write
as the sample covariance matrix, then
is the shrinkage estimator. Here,
is an underﬁt (or prior)
estimate of the covariance matrix; typical choices include an
appropriately scaled identity matrix, or the diagonal elements of
. Another possible choice for
, when it is the local covariance
matrix that is being estimated, is to use the global sample
covariance matrix. The shrinkage parameter
is typically
much less than one, so the effect of shrinkage is a small
perturbation
covariance
especially when
is near-singular, the effect on the inverse
matrix can be considerable.
b) Quasi Local Estimation: The idea of combining global with
local estimators of covariance matrix also drives the “quasilocal” (QL) model . The global sample covariance matrix
may be decomposed as
orthogonal matrix and
is diagonal. If
is the local
Fig. 5. Synoptic graphical summary table. The three main blocks identify the three main subsections of Section III, namely robust MVN modeling, non-MVN
modeling, and exploitation of spatial structures. All algorithm names and acronyms in the table will be deﬁned in the text.
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
sample covariance matrix, then
is the covariance
matrix in the rotated coordinates that diagonalize
is not diagonal, but we can write
operator keeps the diagonal elements of a
matrix and sets the off-diagonal values to zero. This estimator is
readily invertible, since the diagonal elements of
generically be strictly positive, and
In addition to providing a more stable estimator for the inverse
covariance matrix, this scheme also provides an inexpensive way
to compute the inverse; this can be important when local
algorithms require a separate inverse covariance matrix at
every pixel in an image.
One inﬂexibility with the QL estimator is that the “amount” of
regularization is not adjustable; i.e., there is nothing like the
that is used for shrinkage schemes.
c) Sparse Matrix Transform: An extension of the QL estimator
is given by the sparse matrix transform (SMT), ﬁrst introduced in
 , and applied to remote sensing problems in . The SMT
scheme expresses a sample covariance matrix as a product of
Givens rotations
where each Givens rotation is an orthogonal matrix that
corresponds to a rotation about a single pair of coordinates
The Givens rotations
are chosen sequentially,
using an algorithm described in , so that
are increasingly closer to diagonal. The
approximation that SMT makes is to replace
. In particular, if we write
then our estimate for the covariance becomes
This formulation of SMT shares with the QL covariance matrix
the notion of replacing an approximately diagonal matrix with its
diagonal, but it has the advantage that there is a parameter
can be used to adjust the quality of the approximation. It has been
found in practice that better performance is obtained if SMT is
combined with shrinkage; e.g.,
but it bears remarking that this estimator is not efﬁciently
invertible as the straight
estimator.
This formulation of SMT, however, does not take advantage of
a prior covariance estimator
; an example of such a prior is the
global covariance matrix
. A suggestion for doing this ,
 follows the QL idea of working in the eigenspace deﬁned by
. Instead of the approximation
in (13), we write
is the product of K givens rotations that approximately diagonalize
is the orthogonal matrix
that diagonalizes
). This leads to the
approximation
which has the same ﬂavor as QL covariance estimation, but uses
in place of
provides an adjustable parameter; as
increases,
more accurately approximates
less robustly estimates the true underlying covariance matrix.
d) Gaussian Markov Random Field Model: In contrast to the
approaches that employ sample covariance matrix as a starting
point, a method suggested in makes a direct parametric
model of the inverse covariance matrix. This is obtained by
characterizing both spectral and spatial covariance in terms of a
local Gaussian Markov Random Field (GMRF). Even though the
covariance matrix for a
-channel system will have
degrees of freedom, the GMRF model includes only four
parameters, two of which relate to spatial variation. The
authors point out that this simplicity leads to an anomaly
detection algorithm faster than RX and allows for a smaller
annulus size. We remark that the GMRF is one of the few models
(another is described in ) for which the order of the spectral
channels matters.
2) MVN Models Robust to Outliers: As noted in Section II-C2,
the background data may be contaminated by the presence of
pixels strongly deviating from the background distribution.
These outlying pixels may be due to anomalies, to scarcely
background
considerably statistically different from the background signal
of most pixels, or to the target signal.
In the case of target signal contamination, the “background”
covariance matrix estimate inadvertently includes some target
pixels. Thus, the estimated covariance matrix looks like
is the contamination-free covariance
matrix estimate, is the target signature, and
is associated with
the contaminating signal energy and depends on the fraction of
contaminated data.
The resulting contaminated MF is then
instead of the correct
, but it was noted in that
are parallel, and that although the magnitude of
therefore the appropriate threshold
for detectors of the form
) is different for the two MFs, the performance is
identical. Reference did identify a more subtle kind of
covariance contamination, based on a residual correlation of
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
plume and background in a given scene, and showed how
the effect of contamination is more pronounced when the target is
not linearly additive.
When applied to local RX detection, the effect was found to be
more pronounced , . The RX detector in the presence of
contamination
is a positive semi-deﬁnite matrix
depending on
(which includes the contaminating
signal energy and the fraction of contaminated data). The study in
 revealed that both detection and false alarm probabilities
decrease monotonically with increasing contamination, with
the average reduction of the detection probability being much
larger than the average reduction of the false alarm probability.
Contaminated RX performance degradation was quantitatively
examined with respect to several parameters, such as the Contaminating Signal-to-Interference-plus-Noise Ratio
, which depends on the fraction of contaminated data
and the strength of the contaminating signal with respect to
background interference plus noise .
The covariance matrix estimate may also be corrupted by the
presence of other outliers, i.e., pixel containing signals not
necessarily associated with the target signal but that signiﬁcantly
deviate from the background distribution. This problem was
examined in for two speciﬁc detection algorithms. In ,
the effect of a single source of signal contamination was investigated with respect to the performance of Kelly’s detector
and the Mean Level Adaptive Detector when the contaminating signal was statistically independent of the target signal.
In the following, we brieﬂy outline approaches speciﬁcally
developed to mitigate covariance matrix corruption due to outliers. These approaches have turned out to be particularly useful
to reduce the effect of signal contamination in anomaly detection
applications.
a) Minimum Covariance Determinant-Based Covariance
Matrix Estimation: Background covariance matrix susceptibility
to corruption due to signal contamination has led to the development of robust-to-outlier detection methodologies , ,
 , aimed at making detection performance less sensitive
to signal contamination although at the cost of increased architectural complexity.
In , the minimum covariance determinant (MCD) estimation technique was embedded in the local RX detector, as a
solution to mitigate local covariance matrix corruption. The
resulting MCD-RX scheme estimates the covariance matrix over
-dimensional subset (
) of neighboring pixels
characterized by the lowest covariance determinant (proportional
to the squared hyperellipsoid volume), which is likely not to
contain outliers , . In and , the MCD-RX
detector was shown to considerably outperform RX for the
detection of targets placed in close proximity. By recalling the
graphical example depicted in Fig. 3, MCD-RX behavior is
represented in Fig. 6 in the same simpliﬁed two-dimensional
domain spanned by the two principal axes. Whereas RX contours
are strongly biased due to the target pixels contaminating the
background, MCD-RX provides contours that tightly follow the
background data, thanks to the MCD robust estimation that
allows the most outlying pixels to be recognized and neglected
from background statistic evaluation.
Although promising, the use of MCD within the hyperspectral
AD context has been generally restricted to perform global
outlier detection on simpliﬁed low-dimensional data sets ,
 , . This was due to the high computational burden
inherent in the MCD iterative structure , which unavoidably
hinders a computationally efﬁcient local application. A local
algorithm that takes advantage of the MCD-RX approach and is
also much more computationally efﬁcient was developed in .
This algorithm, called Kurtosis-driven MCD-RX (K-MCD-RX),
applies at each pixel a binary hypothesis test based on a statistic
derived from the local sample kurtosis to decide if the pixel
neighborhood contains signiﬁcant outliers and, thus, deserves
application of the more complex MCD-RX in lieu of RX.
Theoretical criteria, based on the sample kurtosis distribution,
are provided to automatically set the threshold for the kurtosisbased binary hypothesis test at each pixel . In and ,
experimental evidence was given of the capability of the kurtosis-based test of indicating the image regions most susceptible to
covariance matrix corruption due to outliers. Also, K-MCD-RX
was found to provide detection performance comparable to that
of MCD-RX, even though robust MCD covariance estimation
was applied to a low percentage of image pixels, with a considerable computational complexity reduction , .
b) Other Algorithms Featuring Robust Covariance Matrix
Estimation: Other algorithms have been applied to robustly
estimate background second-order statistics in hyperspectral
A standard approach for computing a robust covariance matrix
is to de-weight samples based on their Mahalanobis distance
from the mean vector . Furthermore, simply removing the
most anomalous data, and then re-computing the mean vector
and covariance matrix from the remaining points, can lead to
better hyperspectral target detection .
Another method is the blocked adaptive computationally
efﬁcient outlier nominator (BACON) developed in and
applied to robustly detect global anomalies in hyperspectral
Fig. 6. Spectral domain with respect to the two principal axes of the situation
depicted in Fig. 3(a). The pixels belonging to the local homogenous background
arerepresentedas green circles,the target pixelisdenotedwitha reddiamond,and
the pixels belonging to the contamination source are identiﬁed as yellow squares.
RX contours for the contaminated background are plotted in blue. MCD-RX
contours are plotted in red. By excluding the most outlying pixels (i.e., those
belonging to the contamination source) from background statistics computation,
MCD-RX provides more robust contours for the detection of the target.
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
images in . It is worth mentioning that in the same work, a
cluster-based version of BACON was implemented, following
the idea in and, at the same time, making the per-cluster
statistic estimates robust to the presence of outliers.
It should be pointed out that the emphasis on “robust”
covariance estimation may treat the periphery of the distribution
as the source of contamination; but for anomaly detection (or for
low false-alarm rate target detection), it is the periphery of the
distribution that most requires accurate characterization. In
 , a variety of approaches were suggested, including an
“anti-robust” estimator, for estimating a covariance matrix that
incorporates robustness to the most outlying data while not deemphasizing the data on the periphery. One of the approaches
used a support vector machine in a manner similar to support
vector data description (SVDD) , which is described below,
but with a quadratic kernel. The emphasis on periphery makes
these estimators more discriminative than generative.
Just as outliers can corrupt the covariance matrix estimate, they
also corrupt the estimated principal components of the covariance
matrix. This matters for subspace methods, and hybrid subspace
methods,thatemploytheﬁrstfewprincipalcomponentstoidentify
the large variance directions. There has been a lot of interest
recently in the development of efﬁcient algorithms for robustly
estimating those principal components directly , .
B. Non-MVN Background Modeling
Although the MVN model can lead to useful algorithms for
target and anomaly detection, it is ultimately an inadequate
model because hyperspectral data are clearly (and by some
measures, dramatically) non-Gaussian. Thus, there is an opportunity for algorithms to exploit this beyond-Gaussian structure to
produce more sensitive target and anomaly detectors.
In the following, we will review more general background
models that reject the Gaussian approximation entirely and
employ more complicated multivariate distributions.
1) Near-MVN Models: A property of MVN models is that
contours of the Gaussian distribution are ellipsoids, and the size,
aspect, and orientation of those ellipsoids are given by a
covariance matrix. Another property of MVN models is that
the tails of the distribution have very low density, that scale
as a function of radius
from the centroid. A variant
of MVN models called elliptically-contoured (EC) models
preserve the contour shape, and the parameterization with a
covariance matrix, but permit distributions that have heavier
tails than their Gaussian counterparts. Although these models
have been used to model radar clutter for many decades, they were
introduced to hyperspectral modeling by Manolakis et al. ,
 , , . A popular choice is to use the multivariate
t-distribution for modeling the background PDF
is a constant (that depends on the parameter
). Smaller values of the parameter
correspond to
heavier tailed distributions. The
limit produces the
Gaussian distribution, and as
, the limit is a Laplacian
distribution, for which the second and all higher moments diverge.
Because the contours of an EC distribution match those of the
Gaussian distribution, standard anomaly detection algorithms
(e.g., RX) based on EC models give exactly the same results as
those based on the corresponding Gaussian model, but certain
kinds of anomaly detection , and anomalous change
detection have found productive uses for these fatterthan-Gaussian distributions.
But using EC in place of a Gaussian does lead to new target
detection algorithms , , . In particular, using
GLRT for the multivariate-t with the additive target model leads
to the EC-GLRT
which approaches the MF in the
limit, and looks like the
ACE detector , as
Instead of elliptical contours, the single band anomaly detection (SBAD) algorithm effectively employs rectangular
contours, since the detector is the maximum over detectors on
each single band.
2) Semi-Parametric Models: Parametric models typically
assume that the data are drawn from one speciﬁc parametric
distribution (e.g., MVN, EC). Generally, parametric models can
be reliably employed only to characterize background pixels in a
homogeneous local neighborhood around the test pixel and have
been more effectively used within local algorithms after local
mean removal. When highly inhomogeneous backgrounds (e.g.,
global backgrounds, urban environments) have to be characterized, parametric models are no longer able to capture the
complexity of the data. The presence of multiple materials within
the scene suggests that semi-parametric distributions, such as the
ﬁnite mixture models (FMMs), may provide a more accurate
background characterization , , , , .
An FMM models the unknown background PDF
linear combination of PDFs of the same kind, thus accommodating multimodality
denotes the multivariate PDF of
to the th mixture component, controlled by a given parameter
vector θ , whereas
are the mixing proportions (or
weights), which are non-negative and sum to one.
The most widely employed FMM is the Gaussian mixture
model (GMM), which has often been adopted to model global
heterogeneous backgrounds in hyperspectral images , ,
 , , , , . Using the GMM means that each
in (23) is an MVN PDF, where θ
vector specifying the mean vector and covariance matrix of the
th mixture component. With a sufﬁciently high number of
mixture components
, any given PDF may be approximated,
provided that the mixture parameters are adequately estimated
 , , .
A straightforward physical interpretation of hyperspectral
background GMM modeling is that hyperspectral pixels of each
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
background class follow the MVN distribution. However,
several experimental studies performed with real hyperspectral
images , , , have shown that mixtures of EC
distributions may more adequately characterize the statistical
behavior of multi-modal hyperspectral backgrounds with respect
to a GMM. Speciﬁcally, by comparing the empirical and the
theoretical
cumulative
distribution
the Mahalanobis distances computed between each pixel and
the corresponding mixture component , , , , many
background classes have been experimentally shown to exhibit
heavy tails that are more accurately followed by an EC
distribution.
In general, modeling
with an FMM means assuming that
each pixel of the hyperspectral image originates from one
mixture component, according to a given a priori probability.
Thus, this is associated with segmenting the image into homogeneous clusters, each one corresponding to a subset of background pixels modeled by a single mixture PDF of the
. Whereas the segmentation procedure may be
performed with simple clustering methods, such as the -means
 , the expectation maximization (EM) method takes into
account the underlying distribution of the individual mixture
components. Given the number of components
and an initial
set of parameters, the EM algorithm iteratively seeks the maximum likelihood (ML) estimates of the FMM parameters .
Recently, a variational Bayesian method , originally employed with low-dimensional data sets, has been introduced in
the context of FMM parameter estimation for hyperspectral
images , . The EM algorithm can be seen as a special
case of this more general method . By modeling some of the
parameters with hidden variables characterized by proper prior
distributions, the corresponding estimates assume physically
meaningful values dictated by the corresponding prior distributions (e.g., the inverse covariance matrices of the components of
a GMM should follow a Wishart distribution and the mixing
proportions may be modeled as following a Dirichlet distribution, thus enforcing the non-negative and sum-to-one constraints). Casting the FMM parameter estimation in terms of
Bayesian inference makes it possible to overcome some shortcomings of canonical EM, such as its intrinsic inability to
automatically solve the model-order selection and the possibility
of incurring in singular solutions associated with an unlimited
likelihood function , .
Regardless of the speciﬁc clustering method used, detection
may be performed following two different approaches. According to the well-known cluster-based approach , , ,
 , a “cluster-conditional” test is applied to each pixel, for
instance by computing a cluster-conditional Mahalanobis distance for FMM-based anomaly detection , , , 
or a cluster-conditional MF for FMM-based target detection
 , 
where μ and
are the mean vector and covariance matrix of the
MVN distribution modeling the th mixture component, which
may be chosen as the mixture component spectrally closest
(in terms of Mahalanobis distance) to
 .The approach of
(24) has also been applied accounting for local information
extracted from the spatial neighborhood of
 , , ,
 , , e.g., by selecting the th mixture component as the
spectrally closest one among those represented in the neighborhood of
In another approach, not only the parameters of the single
mixture components but also the whole information associated to
the FMM-based estimate of
is used. This is the case of the
anomaly detector based on the FMM background log-likelihood
 , , 
By recalling the graphical example depicted in Fig. 2,
Fig. 7(a)–(d) displays the results of applying the GMM to model
the images shown in Fig. 2(a) and (c). Speciﬁcally, Fig. 7(a) and
(c) shows the images partitioned into a set of clusters by the EM
algorithm. Fig. 7(b) and (d) represents the corresponding data
scattered in simpliﬁed two-dimensional spectral domains together with GMM contours. In both global and local cases, the GMM
is more effective than the MVN at following the inherently multimodal nature of the data.
3) Nonparametric Kernel-Based Models: For nonparametric
models, the background data are not assumed to follow any
speciﬁc distribution; instead, the background characterization is
performed in a data-driven fashion. Nonparametric approaches
employed in hyperspectral data processing include graph-based
methods , , manifold methods , , and the kernelbased methods that we describe in this section.
a) Kernel Density Estimate of the Background Distribution:
Kernel density estimation (KDE) is the most widely known
approach to estimate an unknown PDF without assuming any
ﬁxed functional form for it . Multivariate KDE has been used
to estimate the background PDF of hyperspectral images within
theframework ofthebackgroundlog-likelihoodanomaly detector
 , , , , , . Speciﬁcally, the general
expression for multivariate KDE is as follows :
is the kernel function that is centered at each of the
sample data
is the bandwidth matrix, a
matrix containing the kernel function widths, also referred to as
bandwidths. The kernel function is a smooth function that
decreases in intensity with the distance from the data sample in
which it is centered. According to (27), the background PDF
estimate is constructed by performing a weighted average of the
values assumed in the test pixel
by the kernel function centered
at each of the sample data. Regardless of the speciﬁc kernel
function employed (which is generally taken to be a multivariate
Gaussian function), the degree of smoothing of the PDF estimate
is determined by the bandwidths.
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
The bandwidth matrix may be parameterized to be a diagonal
, thus featuring a bandwidth
value for each spectral dimension. It is more often the case that
the matrix
is taken to be a scaled
identity matrix
, which constrains the contours of the kernel function
to be spherically symmetric. This latter simpliﬁcation has been
shown to be as effective as more complicated forms of
data are pre-scaled to prevent severe differences of spread in the
various spectral directions . Indeed, Fukunaga suggested
that whitening the data (i.e., linearly transforming the data to have
an identity covariance matrix) and adopting
to estimate
the PDF of the transformed data is equivalent to perform estimation in the original domain byusing a bandwidth matrix with same
structure of the data covariance matrix. Thus, in the following we
simplify our analysis to the case
The detector corresponding to employment of (28) within the
background log-likelihood function results in and 
The most widely known form of the KDE is the so-called ﬁxed
KDE (FKDE) , also known as Parzen windowing . In
FKDE, the bandwidth is constant:
. This makes the
modelingabilityofFKDEstronglydependonthespeciﬁcvalueof
, which should be appropriately chosen. Although many
techniques have been proposed and tested to properly choose the
bandwidth , , , a unique bandwidth value might not
exist that avoids over-smoothing the PDF body and, at the same
time, under-smoothing the PDF on the tails and in low-density
regions. This problem has been found to be more and more
signiﬁcant as the data dimension increases . This aspect can
be exempliﬁed in the toy example represented in Fig. 8, featuring
data generated following a mixture of two Gaussian distributions
in a simpliﬁed bivariate domain. The true bivariate PDF is
represented in Fig. 8(a), whereas Fig. 8(b) and (c) represents the
outcomes of FKDE estimation obtained with differentvalues of .
Speciﬁcally, the marginal PDFs, obtained by numerical integration of the joint PDF estimate, are reported for convenience. As it
is evident, variation of
has a major impact on FKDE outcome,
which, for most
values, does not properly respond to the
variations of the true PDF. Too small
values lead to spurious
structures in the estimate at some data sample locations, whereas
values tend to obscure the bimodal nature of the PDF
due to over-smoothing.
In order to reduce KDE performance sensitivity to a ﬁxed
bandwidth value and, at the same time, increase the KDE
capability of following the local data peculiarities across the
data domain, bandwidths that vary on a per-pixel basis may be
employed so as to adapt the amount of smoothing to the local
density of samples in the data space. This approach has led to the
variable-bandwidth KDE (VKDE) , , which has been
shown to bring an increased capability of following complex
backgrounds in hyperspectral images , , . Within
Fig. 7. Outcome of a GMM semi-parametric modeling as concerns the graphical example of Fig. 2. (a) Spatial domain with the image of Reno segmented into clusters
by the EM algorithm. (b) Spectral domain with respect to two spectral channels. The image pixels belonging to a given cluster are represented as circles colored with the
same color employed in the cluster map. GMM contours are plotted in red. (c) Spatial domain with the
image subset segmented into clusters by the EM
algorithm. (d) Spectral domain with respect to two spectral channels. The pixels belonging to a given cluster are represented as circles colored with the same color
employed in the cluster map. GMM contours are plotted in red.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
the VKDE framework, two distinct approaches of varying the
bandwidth may be found. The Balloon Estimator (BE) varies the
bandwidth at each test pixel
keeping it constant for all sample data. The sample point
estimator (SPE) varies the bandwidths at each of the sample
regardless
estimated,
 . Both approaches may be coupled with
-nearest neighbor method ( -NN) as a strategy to let the
bandwidth vary according to the local density of samples in the
data space . The
-NN-based BE results in taking
is the Euclidean distance of
th nearest neighbor within the sample data
implementation
equivalent
being the distance of
th nearest neighbor within
The VKDE—implemented with the
-NN strategy—has
shown to provide better background characterization capabilities
 , than the FKDE, avoiding over-smoothing and undersmoothing effects. Also, the dependence on the
parameter has
beenshown to bevery weak , . Fig. 8(d) and (e) provides a
graphical illustration by displaying the marginal PDFs obtained
by integrating the joint PDF estimate of the bivariate PDF in Fig. 8
(a), performed with VKDE applied in conjunction with the -NN
strategy for different values of . Choosing the bandwidth according to the local data density makes VKDE much more dataresponsive than FKDE. For most
values, VKDE estimates
closely follow the true PDF both in the body and in the tails.
Furthermore, the
parameter weakly affects the ﬁnal outcome
because it simply rules the cardinality of the sample data neighborhood considered for evaluating the bandwidth, which is
computed as the distance to the th neighbor and, thus, expressly
tailored to the density of data samples in that neighborhood.
By recalling the graphical example depicted in Fig. 2, Fig. 9
represents the outcome of KDE (and, speciﬁcally, VKDE) as
Fig. 8. (a) Bivariate PDF of a mixture of two Gaussians. Marginal PDFs estimates along the two axes, obtained by numerical integration of the bivariate PDF estimate.
(b) and (c) FKDE estimates for different
values. (d) and (e) VKDE estimates for different
values. The True marginal PDFs are plotted in dashed black.
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
concerns the two images considered in Fig. 2(a) and (c). VKDE
contours in the simpliﬁed two-dimensional spectral domains
show that this approach is capable of adapting to peculiar data
distribution without assuming any speciﬁc statistical model for
the data themselves.
Finally, it is interesting to observe that the FKDE may be seen
as a simple Euclidean Distance detector applied in a higherdimensional kernel-induced space obtained though a nonlinear
, as shown in and 
is the estimate of the mean vector
computed with the mapped data. By expressing the dot products
in the kernel-induced space in terms of kernel functions
a relationship with FKDE is obtained
is the PDF estimate given by (28) considering
. This holds for any stationary (translational invariant) kernel
b) Kernel-RX: A MVN Model in the Kernel-Induced
Space: The kernel-RX (KRX) algorithm was derived by
assuming an MVN model in the higher-dimensional space
induced by the nonlinear mapping
, and applying a
Mahalanobis-distance-based detector 
covariance matrix computed with the mapped data. Note that
is the pseudo-inverse of
since the covariance matrix is of
lower rank than the dimension of the kernel-induced space. As
with (30), the expression is not evaluated in the high-dimensional
kernel-induced space, but employs kernel functions for dot
products, leading to the main expression for Kernel-RX
algorithm, introduced in , and corrected in 
μ are vectors deﬁned in and
centered Gram matrix , .
Following , we can write
the matrix of eigenvectors and
is the diagonal matrix with
positive eigenvalues
, of the covariance matrix
where the eigenvectors
lie in the
span of the mapped centered data
can be derived from the eigenvectors and eigenvalues of
is the centered kernel function,
Fig. 9. Outcome of a VKDE parametric model as concerns the graphical example
of Fig. 2. (a) Spectral domain with respect to two spectral channels. The image
pixels are represented as green circles. VKDE contours are plotted in magenta.
(b) Spectral domain with respect to two spectral channels. Only the pixels
belonging to a
subset of the image are shown, again represented as
green circles. VKDE contours are plotted in magenta.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
expressed as follows :
Equations (34) and (35) show that, although entailing a more
complex algebraic expression than a simple sum of kernel
functions (as in FKDE), kernel-RX still involves computation
of kernel functions over pairs of sample data and may, thus, be
seen as a generalization of FKDE.
c) Estimating the Support of the Background Distribution: In
 , hyperspectral background characterization was performed
by exploiting the SVDD , which has its foundations in the
support vector approach . The idea behind this approach is to
estimate the data support region where most of the data lie, which
should be easier than estimating the data PDF . Within this
framework, the SVDD aims at identifying the data support region
by means of the minimum hypersphere enclosing (most of) the
sample data in a higher-dimensional kernel-induced space
obtained though a nonlinear mapping
is the hypersphere center, obtained by
solving a constrained optimization problem, and the
scalar values (Lagrange multipliers) subject to non-negativity
and sum-to-one constraints that identify the sample data
lying on the boundary of the support region
(i.e., on the hypersphere surface) . These sample data are
called support vectors because they are the only sample data
needed to describe the hypersphere. By kernelizing (36)
Equation (37) is very similar to (31) with the difference that the
kernel functions that contribute tothe sumare centeredonly onthe
support vectors.
d) Summary of Kernel-Based Methods: Each of the three
kernel-based algorithms described above (FKDE, KRX, and
SVDD) can be derived by mapping the data into a highdimensional kernel-induced feature space
, expressing
the algorithm in terms of dot products, and employing the kernel
trick. Although derived following different approaches, the
three algorithms turn out to be deeply interrelated. Both FKDE
and KRX compute a centroid in the kernel-induced space μ
, whereas SVDD employs an adaptive center
. Both FKDE and SVDD compute anomalousness in terms of Euclidean distance to their respective centers,
whereas KRX employs the Mahalanobis distance.
As noted in , “SVDD is based on a discriminative model,
which does not assume any distribution for the input data. On the
other hand, KRX is a generative model that represents the data as
a Gaussian distribution in the kernel induced high-dimensional
feature space.” In this terminology, FKDE is also generative; it
represents data as Gaussian in the kernel-induced space with a
covariance matrix that is the identity matrix. A further distinction
between KRX and FKDE is that the FKDE Gaussian is of full
rank in the feature space, whereas KRX projects the data in the
feature space to a lower dimensional subspace spanned by
before computing the Mahalanobis
distance. The variant of KRX originally introduced by 
introduces a regularization term in the covariance matrix before
taking its inverse, and effectively computes a distance in the full
rank of the kernel-induced space.
It is worth mentioning that other detectors have been developed following the idea behind KRX and assuming a simple
model in the kernel-induced space so as to obtain models that
more tightly ﬁt the background data in the original data space
 , , , , . More recently, other nonparametric
background models, such as manifold-based models, widely
used in classiﬁcation applications , have been gaining popularity also in detection applications , . As with the
kernel density estimators described in Section III-B3a, a perennial challenge for all kernel methods is choosing a good value for
the kernel bandwidth , , .
4) Subspace Separation: Although it is widely observed that
hyperspectral data are inadequately represented by MVN
models, a number of authors have observed that the data can
be more Gaussian in some directions than in others , , .
That is, the projection of pixel vectors
onto scalar values by a
dot product with some vector
, yields a distribution of scalars
, which may be more or less Gaussian depending on the
direction . This suggests the use of models based on a separation
of hyperspectral data into two subspaces: 1) mostly Gaussian;
and 2) mostly non-Gaussian. With a Gaussian/non-Gaussian
(G/NG)-separation algorithm, one can explicitly produce a
probability density that is a product of the MVN density in
the Gaussian directions and a more tailored distribution in the
non-Gaussian directions. A simpliﬁed graphical illustration
of G/NG subspaces for the image shown in Fig. 2(a) is
reported in Fig. 10(a) and (b), where the data projected onto
two 2-dimensional subspaces exhibit strongly non-Gaussian and
mostly Gaussian behaviors.
In , the non-Gaussian directions were modeled with a
simplex, producing an ellipsoid-simplex hybrid. In , a
nonparametric histogram-based model of the non-Gaussian directions is employed. The approach in treated each dimension
MATTEOLI et al.: OVERVIEW OF BACKGROUND MODELING FOR DETECTION OF TARGETS AND ANOMALIES
of a principal components analysis as a separate subspace, and
modeled those dimensions as individual EC distributions, with
each direction getting its own measure of non-Gaussian-ness.
Another kind of subspace separation was described in ,
and referred to as a “folded subspace” in . Here, the highdimensional hyperspectral space is compacted into two dimensions
corresponding to: 1) an MF projection of the data; and 2) the
magnitude of the residual after MF subtraction. Although of
reduced dimension, this space is rich enough to support a number
of standard detectors (RX, MF, ACE, EC-GLRT, etc.) and furthermore permits new detectors, based on parametric and nonparametric modeling of the background in that lower-dimensional space.
C. Exploitation of Background Spatial Structure
As noted previously, there are both local and global versions
for many of the background characterization algorithms described so far, with the local versions computing statistics from
an annulus around the pixel of interest and global versions
computing those statistics over the whole image. The advantage
of the local versions is that they adapt to the changing statistics
exhibited by spatially inhomogeneous backgrounds (and there is
a multiple window variant that also adapts to different target sizes
 ), and as such implicitly incorporate the spatial structure of
the background into the model. In this section, we will describe
several more direct approaches for explicitly incorporating the
background structure into the model.
1) Exemplar-Based Background Estimation: In general, local
algorithms attempt to estimate the target-free value of a pixel in
terms of pixels that surround it. For an algorithm such as RX, that
estimate is based on the mean of pixels in an annulus. As also
shown in the graphical example in Fig. 4,such local estimates can
be effective, but they are challenged by edges and textures for the
same reason that denoising by low-pass ﬁltering (i.e., smoothing)
is challenged by edges and textures.
Recent developments of image restoration techniques in
traditional image processing suggest an alternative approach
that is more adaptive to the local image structure, and has the
potential for substantially improving the estimate of the targetfree (i.e., background) value at a pixel, and therefore enhancing
the ability to detect whether a target is present. These exemplarbased methods identify patches in the image that are similar to
each other (even though they may be well-separated on the
image) and use them to collectively make inferences about each
other. Problems such as denoising , , inpainting ,
 , and super-resolution are then treated as independent
regression problems on these patches.
While the background estimation problem has not been
directly addressed in mainstream image processing, this problem
is closely related to denoising or inpainting, and is also amenable
to attack via most of the standard exemplar-based techniques.
Inspired by exemplar-based image processing, a regression
framework was suggested in , in which the background
for a pixel under test is estimated as a more general function of the
annular pixels, as exempliﬁed in Fig. 11. Speciﬁcally, this
generic function is learned from the entire image and nonlocal
algorithms are employed that exploit pixels from the
similar annular patches in the image. This was shown to improve
estimates of the central pixel for single-band, multispectral,
and hyperspectral imagery. Adapting these exemplar-based
approaches to the high-dimensional spaces encountered in
hyperspectral imagery is an important challenge, but also a
potential opportunity to more fully exploit both the spatial and
spectral structure that is exhibited by hyperspectral backgrounds.
2) Standard Deviation MF: Pixels belonging to background
structures such as edges between background classes have
generally been found to be particularly susceptible to become
false alarms , . Simple detectors such as the MF may be
slightly modiﬁed to mitigate false alarms induced by edges and
background structures, using the Standard Deviation Matched
Filter (SDMF) 
Fig. 10. Graphical example of G/NG subspace separation for the Reno urban
image shown in Fig. 2(a). (a) Spectral domain with respect to two principal
components where the data are strongly non-Gaussian. Univariate histrograms of
the twoprincipal componentsare plotted for convenience. (b) Spectral domain with
respecttotwoprincipalcomponentswherethedataaremostlyGaussian.Univariate
histrograms of the two principal components are plotted for convenience.
Fig. 11. Graphical representation of the exemplar-based background estimation
concept. Background for the pixel under test is estimated as a general function of
the pixels enclosed in the annulus. For many standard algorithms, the function
is just the mean of the values in the annulus.
IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 7, NO. 6, JUNE 2014
is the background covariance matrix estimate
computed globally over the whole image (other ways of
locally and quasi-locally estimating the covariance matrix have
been already discussed in Section III-A1), μ is the local mean
vector estimated over a spatial neighborhood of
diagonal matrix containing, for each spectral dimension, the
standard deviation computed over a spatial neighborhood of
is a diagonal matrix that contains, for each spectral dimension,
the average standard deviation computed over the whole image.
The multiplication by the matrix
has been expressly
designed to lower the detector output value in pixels characterized by high local standard deviations, i.e., in close proximity of
background transitions. The term
has been introduced to
avoid inﬂation of detector output value in areas of very low
background variability (i.e., very low local standard deviations).
In , it is speciﬁed that
should be chosen as a tradeoff
between a small value, which allows to considerably reduce edge
induced false alarms, and a large value, which makes targets on
edges and background structures easier to detect. As highlighted
in , the SDMF may be coupled with a more robust estimation
of the local mean vector performed in a “directional” fashion. In
fact, conventional local mean vector estimation would lead to
misleading estimates for those pixels lying where statistically
different background classes meet. By averaging a pair of pixels
oriented symmetrically with respect to
and aligned along the
direction characterized by the smallest variance, a more robust
estimate should be obtained, at least in the case of transitions
between two background classes .
3) Post-detection Spatial Analysis: False alarm pixels caused
by background structures generally tend to be found in physical
proximity to each other (i.e., tend to be grouped in clusters).
There are a number of techniques that exploit this tendency, and
by suitably ﬁltering the gray-level image of detection results, can
reduce false alarms. One approach is to use morphological
operators (such as combination of erosion and dilation operators)
to reduce false alarms by eliminating, from the detector outcome,
those groups of pixels characterized by detector high scores that
cannot be associated with target-sized clusters. Another example
is the nonmaximum suppression ﬁlter used in that decreases
the detector output based on the presence of higher scores in the
local spatial neighborhood on a given pixel. Regardless the
speciﬁc method used, post-detection spatial ﬁlters generally
make use of a priori information about the expected target size.
IV. CONCLUSION
Given the huge variety of actual hyperspectral imagery, it is
perhaps ﬁtting that there is such a huge variety of models that
have been proposed for hyperspectral image data. To characterize hyperspectral clutter—which is a high-dimensional and
inhomogeneous
assortment
man-made structures, and the various transitions between
them—researchers have invoked kernels, manifolds, graphs,
histograms, and exemplars; but they have also successfully
employed low-dimensional subspaces and simple distributions.
We encourage the development of complex models for hyperspectral backgrounds, because those backgrounds are indeed
complex. But we remind those researchers that complicated
models may all-too-easily overﬁt the data, and what worked
like a charm on Indian Pines might not fare so well on Jasper
Ridge. We did not say much in this review about validation of
hyperspectral models, but as models grow in complexity, reliable
validation will grow in difﬁculty. As data acquisition rates
continue to rise, so too will the raw size of hyperspectral datasets.
Larger datasets are always welcomed by researchers, because
they support better and more accurate models. But there has to be
a practical tradeoff between sophisticated modeling and operational applicability. To be useful in real-world situations, the
models should strive toward computational efﬁciency, ease of
use by nonexperts, and robustness to the selection of free
parameters.
The emphasis on background characterization over the more
intuitively natural focus on the detection algorithms themselves
represents a perspective that is becoming more prominent in the
hyperspectral data analysis community. Although our ultimate
interest is in effective detection of targets and anomalies, many
detection algorithms can be understood in terms of how they
model the background. In an attempt to keep the scope of this
review bounded, we did not discuss change detection. But it is
our conviction that the background modeling approaches described here can be productively applied in this problem domain
ACKNOWLEDGMENT
The authors are enormously grateful to Prof. S. R. Rotman for
his role in providing an impetus for this work and for intellectually stimulating conversations about background modeling in
hyperspectral target detection applications.