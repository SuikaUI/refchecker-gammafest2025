Learning Loss for Active Learning
Donggeun Yoo1,2 and In So Kweon2
1Lunit Inc., Seoul, South Korea.
2KAIST, Daejeon, South Korea.
 
 
The performance of deep neural networks improves with
more annotated data. The problem is that the budget for
annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it
perceived as uncertain. A variety of recent methods have
been proposed to apply active learning to deep networks
but most of them are either designed speciÔ¨Åc for their target tasks or computationally inefÔ¨Åcient for large networks.
In this paper, we propose a novel active learning method
that is simple but task-agnostic, and works efÔ¨Åciently with
the deep networks. We attach a small parametric module,
named ‚Äúloss prediction module,‚Äù to a target network, and
learn it to predict target losses of unlabeled inputs. Then,
this module can suggest data that the target model is likely
to produce a wrong prediction. This method is task-agnostic
as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image
classiÔ¨Åcation, object detection, and human pose estimation,
with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.
1. Introduction
Data is Ô¨Çooding in, but deep neural networks are still
data-hungry. The empirical analysis of suggests
that the performance of recent deep networks is not yet
saturated with respect to the size of training data.
this reason, learning methods from semi-supervised learning to unsupervised learning 
are attracting attention along with weakly-labeled or unlabeled large-scale data.
However, given a Ô¨Åxed amount of data, the performance
of the semi-supervised or unsupervised learning is still
bound to that of fully-supervised learning. The experimen-
Loss prediction module
Target prediction
Loss prediction
(a) A model with a loss prediction module
training set
Human oracles
annotate top-ùêæ
data points
(b) Active learning with a loss prediction module
Figure 1. A novel active learning method with a loss prediction
module. (a) A loss prediction module attached to a target model
predicts the loss value from an input without its label. (b) All data
points in an unlabeled pool are evaluated by the loss prediction
module. The data points with the top-K predicted losses are labeled and added to a labeled training set.
tal results of semi-supervised learning in demonstrate that the higher portion of annotated data ensures superior performance. This is why we are suffering from annotation labor and cost of time.
The cost of annotation varies widely depending on target tasks. In the natural image domain, it is relatively cheap
to annotate class labels for classiÔ¨Åcation, but detection requires expensive bounding boxes. For segmentation, it is
more expensive to draw pixel-level masks. The situation
gets much worse when we consider the bio-medical image
domain. It requires board-citiÔ¨Åed specialists trained for several years (radiologists for radiography images , pathologists for slide images ) to obtain annotations.
The budget for annotation is limited. What then is the
most efÔ¨Åcient use of the budget? Ô¨Årst proposed ac-
 
tive learning where a model actively selects data points that
the model is uncertain of. For an example of binary classiÔ¨Åcation , the data point whose posterior probability closest to 0.5 is selected, annotated, and added to a training set.
The core idea of active learning is that the most informative
data point would be more beneÔ¨Åcial to model improvement
than a randomly chosen data point.
Given a pool of unlabeled data, there have been three
major approaches according to the selection criteria: an
uncertainty-based approach, a diversity-based approach,
and expected model change.
The uncertainty approach
 deÔ¨Ånes and measures the quantity
of uncertainty to select uncertain data points, while the diversity approach selects diverse data points
that represent the whole distribution of the unlabeled pool.
Expected model change selects data points that
would cause the greatest change to the current model parameters or outputs if we knew their labels. Readers can review most of classical studies for these approaches in .
The simplest method of the uncertainty approach is to
utilize class posterior probabilities to deÔ¨Åne uncertainty.
The probability of a predicted class or an entropy of
class posterior probabilities deÔ¨Ånes uncertainty of
a data point. Despite its simplicity, this approach has performed remarkably well in various scenarios.
complex recognition tasks, it is required to re-deÔ¨Åne taskspeciÔ¨Åc uncertainty such as object detection , semantic
segmentation , and human pose estimation .
As a task-agnostic uncertainty approach, train
multiple models to construct a committee, and measure the
consensus between the multiple predictions from the committee. However, constructing a committee is too expensive for current deep networks learned with large data. Recently, Gal et al. obtains uncertainty estimates from
deep networks through multiple forward passes by Monte
Carlo Dropout . It was shown to be effective for classiÔ¨Åcation with small datasets, but according to , it does
not scale to larger datasets.
The distribution approach could be task-agnostic as it
depends on a feature space, not on predictions. However,
extra engineering would be necessary to design a locationinvariant feature space for localization tasks such as object
detection and segmentation. The method of expected model
change has been successful for small models but it is computationally impractical for recent deep networks.
The majority of empirical results from previous researches suggest that active learning is actually reducing
the annotation cost. The problem is that most of methods
require task-speciÔ¨Åc design or are not efÔ¨Åcient in the recent
deep networks, resulting in another engineering cost. In this
paper, we aim to propose a novel active learning method
that is simple but task-agnostic, and performs well on deep
A deep network is learned by minimizing a single loss,
regardless of what a task is, how many tasks there are, and
how complex an architecture is. This fact motivates our
task-agnostic design for active learning. If we can predict
the loss of a data point, it becomes possible to select data
points that are expected to have high losses. The selected
data points would be more informative to the current model.
To realize this scenario, we attach a ‚Äúloss prediction
module‚Äù to a deep network and learn the module to predict
the loss of an input data point. The module is illustrated in
Figure 1-(a). Once the module is learned, it can be utilized
to active learning as shown in Figure 1-(b). We can apply
this method to any task that uses a deep network.
We validate the proposed method through image classi-
Ô¨Åcation, human pose estimation, and object detection. The
human pose estimation is a typical regression task, and the
object detection is a more complex problem combined with
both regression and classiÔ¨Åcation. The experimental results
demonstrate that the proposed method consistently outperforms previous methods with a current network architecture
for each recognition task. To the best of our knowledge,
this is the Ô¨Årst work veriÔ¨Åed with three different recognition
tasks using the state-of-the-art deep network models.
1.1. Contributions
In summary, our major contributions are
1. Proposing a simple but efÔ¨Åcient active learning method
with the loss prediction module, which is directly applicable to any tasks with recent deep networks.
2. Evaluating the proposed method with three learning
tasks including classiÔ¨Åcation, regression, and a hybrid
of them, by using current network architectures.
2. Related Research
Active learning has advanced for more than a couple of
decades. First, we introduce classical active learning methods that use small-scale models . In the uncertainty approach, a naive way to deÔ¨Åne uncertainty is to use the posterior probability of a predicted class , or the margin
between posterior probabilities of a predicted class and the
secondly predicted class . The entropy 
of class posterior probabilities generalizes the former definitions. For SVMs, distances to the decision
boundaries can be used to deÔ¨Åne uncertainty. Another approach is the query-by-committee . This method
constructs a committee comprising multiple independent
models, and measures disagreement among them to deÔ¨Åne
uncertainty.
The distribution approach chooses data points that represent the distribution of an unlabeled pool. The intuition is
that learning over a representative subset would be competitive over the whole pool. To do so, applies a clustering
algorithm to the pool, and formulate the subset selection as a discrete optimization problem. 
consider how close a data point is to surrounding data points
to choose one that could well propagate the knowledge. The
method of expected model change is a more sophisticated
and decision-theoretic approach for model improvement.
It utilizes the current model to estimate expected gradient
length , expected future errors , or expected output
changes , to all possible labels.
Do these methods, advanced with small models and data,
well scale to large deep networks and data? Fortunately, the uncertainty approach for classiÔ¨Åcation
tasks still performs well despite its simplicity. However, a
task-speciÔ¨Åc design is necessary for other tasks since it utilizes network outputs. As a more generalized uncertainty
approach, obtains uncertainty estimates through multiple forward passes with Monte Carlo Dropout, but it is
computationally inefÔ¨Åcient for recent large-scale learning as
it requires dense dropout layers that drastically slow down
the convergence speed. This method has been veriÔ¨Åed only
with small-scale classiÔ¨Åcation tasks. constructs a committee comprising 5 deep networks to measure disagreement as uncertainty. It has shown the state-of-the-art classiÔ¨Åcation performance, but it is also inefÔ¨Åcient in terms of
memory and computation for large-scale problems.
Sener et al. propose a distribution approach on an
intermediate feature space of a deep network. This method
is directly applicable to any task and network architecture since it depends on intermediate features rather than
the task-speciÔ¨Åc outputs. However, it is still questionable
whether the intermediate feature representation is effective
for localization tasks such as detection and segmentation.
This method has also been veriÔ¨Åed only with classiÔ¨Åcation
tasks. As the two approaches based on uncertainty and distribution are differently motivated, they are complementary
to each other. Thus, a variety of hybrid strategies have been
proposed for their speciÔ¨Åc tasks.
Our method can be categorized into the uncertainty approach but differs in that it predicts ‚Äúloss‚Äù based on the input contents, rather than statistically estimating uncertainty
from outputs. It is similar to a variety of hard example mining since they regard training data points with high
losses as being signiÔ¨Åcant for model improvement. However, ours is distinct from theirs in that we do not have annotations of data.
In this section, we introduce the proposed active learning method. We start with an overview of the whole active learning system in Section 3.1, and provide in-depth
descriptions of the loss prediction module in Section 3.2,
and the method to learn this module in Section 3.3.
3.1. Overview
In this section, we formally deÔ¨Åne the active learning
scenario with the proposed loss prediction module. In this
scenario, we have a set of models composed of a target
model Œòtarget and a loss prediction module Œòloss. The loss
prediction module is attached to the target model as illustrated in Figure 1-(a). The target model conducts the target
task as ÀÜy = Œòtarget(x), while the loss prediction module
predicts the loss ÀÜl = Œòloss(h). Here, h is a feature set of x
extracted from several hidden layers of Œòtarget.
In most real-world learning problems, we can gather a
large pool of unlabeled data UN at once. The subscript N
denotes the number of data points. Then, we uniformly
sample K data points at random from the unlabeled pool,
and ask human oracles to annotate them to construct an initial labeled dataset L0
K. The subscript 0 means it is the initial stage. This process reduces the size of the unlabeled
pool as U0
Once the initially labeled dataset L0
K is obtained, we
jointly learn an initial target model Œò0
target and an initial loss
prediction module Œò0
loss. After initial training, we evaluate
all the data points in the unlabeled pool by the loss prediction module to obtain data-loss pairs {(x, ÀÜl)|x ‚ààU0
Then, human oracles annotate the data points of the Khighest losses. The labeled dataset L0
K is updated with them
and becomes L1
2K. After that, we learn the model set over
2K to obtain {Œò1
target, Œò1
loss}. This cycle, illustrated in Figure 1-(b), repeats until we meet a satisfactory performance
or until we have exhausted the budget for annotation.
3.2. Loss Prediction Module
The loss prediction module is core to our task-agnostic
active learning since it learns to imitate the loss deÔ¨Åned in
the target model. This section describes how we design it.
The loss prediction module aims to minimize the engineering cost of deÔ¨Åning task-speciÔ¨Åc uncertainty for active
learning. Moreover, we also want to minimize the computational cost of learning the loss prediction module, as we are
already suffering from the computational cost of learning
very deep networks. To this end, we design a loss prediction module that is (1) much smaller than the target model,
and (2) jointly learned with the target model. There is no
separated stage to learn this module.
Figure 2 illustrates the architecture of our loss prediction module. It takes multi-layer feature maps h as inputs
that are extracted between the mid-level blocks of the target model. These multiple connections let the loss prediction module to choose necessary information between layers useful for loss prediction. Each feature map is reduced
to a Ô¨Åxed dimensional feature vector through a global average pooling (GAP) layer and a fully-connected layer. Then,
all features are concatenated and pass through another fully-
Target model
prediction
prediction
Figure 2. The architecture of the loss prediction module. This
module is connected to several layers of the target model to take
multi-level knowledge into consideration for loss prediction. The
multi-level features are fused and map to a scalar value as the loss
prediction.
connected layer, resulting in a scalar value ÀÜl as a predicted
loss. Learning this two-story module requires much less
memory and computation than the target model. We have
tried to make this module deeper and wider, but the performance does not change much.
3.3. Learning Loss
In this section, we provide an in-detail description of
how to learn the loss prediction module deÔ¨Åned before. Let
us suppose we start the s-th active learning stage. We have
a labeled dataset Ls
K¬∑(s+1) and a model set composed of a
target model Œòtarget and a loss prediction module Œòloss. Our
objective is to learn the model set for this stage s to obtain
target, Œòs
Given a training data point x, we obtain a target prediction through the target model as ÀÜy = Œòtarget(x), and
also a predicted loss through the loss prediction module as
ÀÜl = Œòloss(h). With the target annotation y of x, the target
loss can be computed as l = Ltarget(ÀÜy, y) to learn the target model. Since this loss l is a ground-truth target of h for
the loss prediction module, we can also compute the loss
for the loss prediction module as Lloss(ÀÜl, l). Then, the Ô¨Ånal
loss function to jointly learn both of the target model and
the loss prediction module is deÔ¨Åned as
Ltarget(ÀÜy, y) + Œª ¬∑ Lloss(ÀÜl, l)
where Œª is a scaling constant. This procedure to deÔ¨Åne the
Ô¨Ånal loss is illustrated in Figure 3.
Perhaps the simplest way to deÔ¨Åne the loss-prediction
loss function is the mean square error (MSE) Lloss(ÀÜl, l) =
(ÀÜl ‚àíl)2. However, MSE is not a suitable choice for this
problem since the scale of the real loss l changes (decreases
in overall) as learning of the target model progresses. Minimizing MSE would let the loss prediction module adapt
roughly to the scale changes of the loss l, rather than Ô¨Åtting to the exact value. We have tried to minimize MSE
but failed to learn a good loss prediction module, and active learning with this module actually demonstrates performance worse than previous methods.
Loss prediction module
prediction
prediction
Loss-prediction
Figure 3. Method to learn the loss. Given an input, the target model
outputs a target prediction, and the loss prediction module outputs
a predicted loss. The target prediction and the target annotation are
used to compute a target loss to learn the target model. Then, the
target loss is regarded as a ground-truth loss for the loss prediction
module, and used to compute the loss-prediction loss.
It is necessary for the loss-prediction loss function to discard the overall scale of l. Our solution is to compare a
pair of samples. Let us consider a training iteration with a
mini-batch Bs ‚äÇLs
K¬∑(s+1). In the mini-batch whose size is
B, we can make B/2 data pairs such as {xp = (xi, xj)}.
The subscript p represents that it is a pair, and the minibatch size B should be an even number. Then, we can learn
the loss prediction module by considering the difference between a pair of loss predictions, which completely make the
loss prediction module discard the overall scale changes. To
this end, the loss function for the loss prediction module is
Lloss(ÀÜlp, lp) = max
0, ‚àí1(li, lj) ¬∑ (ÀÜli ‚àíÀÜlj) + Œæ
1(li, lj) =
where Œæ is a pre-deÔ¨Åned positive margin and the subscript p
also represents the pair of (i, j). For instance when li > lj,
this function states that no loss is given to the module only
if ÀÜli is larger than ÀÜlj + Œæ, but otherwise a loss is given to the
module to force it to increase ÀÜli and decrease ÀÜlj.
Given a mini-batch Bs in the active learning stage s, our
Ô¨Ånal loss function to jointly learn the target model and the
loss prediction module is
Ltarget(ÀÜy, y) + Œª 2
(xp,yp)‚ààBs
Lloss(ÀÜlp, lp)
ÀÜy = Œòtarget(x)
ÀÜlp = Œòloss(hp)
lp = Ltarget( ÀÜyp, yp).
Minimizing this Ô¨Ånal loss give us Œòs
loss as well as Œòs
without any separated learning procedure nor any taskspeciÔ¨Åc assumption. The learning process is efÔ¨Åcient as the
loss prediction module Œòs
loss has been designed to contain
a small number of parameters but to utilize rich mid-level
representations h of the target model. This loss prediction
module will pick the most informative data points and ask
human oracles to annotate them for the next active learning
stage s + 1.
4. Evaluation
In this section, we rigorously evaluate our method
through three visual recognition tasks. To verify whether
our method works efÔ¨Åciently regardless of tasks, we choose
diverse target tasks including image classiÔ¨Åcation as a classiÔ¨Åcation task, object detection as a hybrid task of classiÔ¨Åcation and regression, and human pose estimation as a typical
regression problem. These three tasks are indeed important
research topics for visual recognition in computer vision,
and are very useful for many real-world applications.
We have implemented our method and all the recognition tasks with PyTorch . For all tasks, we initialize a
labeled dataset L0
K by randomly sampling K=1,000 data
points from the entire dataset UN. In each active learning cycle, we continue to train the current model by adding
K=1,000 labeled data points. The margin Œæ deÔ¨Åned in the
loss function (Equation 2) is set to 1. We design the fullyconnected layers (FCs) in Figure 2 except for the last one to
produce a 128-dimensional feature. For each active learning
method, we repeat the same experiment multiple times with
different initial labeled datasets, and report the performance
mean and standard deviation. For each trial, our method
and compared methods share the same random seed for a
fair comparison.
Other implementation details, datasets,
and experimental results for each task are described in the
following Sections 4.1, 4.2, 4.3.
4.1. Image ClassiÔ¨Åcation
Image classiÔ¨Åcation is a common problem that has been
veriÔ¨Åed by most of the previous active learning methods. In
this problem, a target model recognizes the category of a
major object from an input image, so object category labels
are required for supervised learning.
We choose CIFAR-10 dataset as it has been
used for recent active learning methods . CIFAR-10
consists of 60,000 images of 32√ó32√ó3 size, assigned with
one of 10 object categories. The training and test sets contain 50,000 and 10,000 images respectively. We regard the
training set as the initial unlabeled pool U50,000. As studied
in , selecting K-most uncertain samples from such
a large pool U50,000 often does not work well, because image
contents among the K samples are overlapped. To address
this, obtains a random subset SM ‚äÇUN for each active
learning stage and choose K-most uncertain samples from
SM. We adopt this simple yet efÔ¨Åcient scheme and set the
subset size to M=10,000. As an evaluation metric, we use
the classiÔ¨Åcation accuracy.
Target model
We employ the 18-layer residual network
(ResNet-18) as we aim to verify our method with current deep architectures. We have utilized an open source1
in which this model speciÔ¨Åed for CIFAR showing 93.02%
accuracy is implemented. ResNet-18 for CIFAR is identical to the original ResNet-18 except for the Ô¨Årst convolution
and pooling layers. The Ô¨Årst convolution layer is changed
to contain 3√ó3 kernels with the stride of 1 and the padding
of 1, and the max pooling layer is dropped, to adapt to the
small size images of CIFAR.
Loss prediction module
ResNet-18 is composed of 4 basic blocks {convi 1, convi 2 | i=2, 3, 4, 5} following the
Ô¨Årst convolution layer. Each block comprises two convolution layers. We simply connect the loss prediction module
to each of the basic blocks to utilize the 4 rich features from
the blocks for estimating the loss.
For training, we apply a standard augmentation
scheme including 32√ó32 size random crop from 36√ó36
zero-padded images and random horizontal Ô¨Çip, and normalize images using the channel mean and standard deviation vectors estimated over the training set. For each of
active learning cycle, we learn the model set {Œòs
target, Œòs
for 200 epochs with the mini-batch size of 128 and the initial learning rate of 0.1. After 160 epochs, we decrease the
learning rate to 0.01. The momentum and the weight decay
are 0.9 and 0.0005 respectively. After 120 epochs, we stop
the gradient from the loss prediction module propagated to
the target model. We set Œª that scales the loss-prediction
loss in Equation 3 to 1.
Comparison targets
We compare our method with random sampling, entropy-based sampling , and coreset sampling , which is a recent distribution approach.
For the entropy-based method, we compute the entropy
from a softmax output vector. For core-set, we have implemented K-Center-Greedy algorithm in since it is
simple to implement yet marginally worse than the mixed
integer program. We also run the algorithm over the last
feature space right before the classiÔ¨Åcation layer as do.
Note that we use exactly the same hyper-parameters to train
target models for all methods including ours.
The results are shown in Figure 4. Each point is an average of 5 trials with different initial labeled datasets. Our
implementations show that both entropy-based and coreset methods have better results than the random baseline.
In the last active learning cycle, the entropy and core-set
1 
Number of labeled images
Accuracy (mean of 5 trials)
random mean
random mean¬±std
entropy mean
entropy mean¬±std
core-set mean
core-set mean¬±std
learn loss mse mean
learn loss mse mean¬±std
learn loss mean
learn loss mean¬±std
Figure 4. Active learning results of image classiÔ¨Åcation over
methods show 0.9059 and 0.9010 respectively, while the
random baseline shows 0.8764. The performance gaps between these methods are similar to those of . In particular, the simple entropy-based method works very effectively
with the classiÔ¨Åcation which is typically learned to minimize cross-entropy between predictions and target labels.
Our method noted as ‚Äúlearn loss‚Äù shows the highest performance for all active learning cycles.
In the last cycle, our method achieves an accuracy of 0.9101. This is
0.42% higher than the entropy method and 0.91% higher
than the core-set method. Although the performance gap
to the entropy-based method is marginal in classiÔ¨Åcation,
our method can be effectively applied to more complex and
diverse target tasks.
We deÔ¨Åne an evaluation metric to measure the performance of the loss prediction module. For a pair of data
points, we give a score 1 if the predicted ranking is true,
and 0 for otherwise. These binary scores from every pair
of test sets are averaged to a value named ‚Äúranking accuracy‚Äù. Figure 5 shows the ranking accuracy of the loss prediction module over the test set. As we add more labeled
data, loss prediction module becomes more accurate and Ô¨Ånally reaches 0.9074. The use of MSE for learning the loss
prediction module with Œª=0.1, noted by ‚Äúlearn loss mse‚Äù,
yields lower loss-prediction performance (Figure 5) that results in less-efÔ¨Åcient active learning (Figure 4).
4.2. Object Detection
Object detection localizes bounding boxes of semantic
objects and recognizes the categories of the objects. It is
a typical hybrid task as it combines a regression problem
for bounding box estimation and a classiÔ¨Åcation problem
Number of labeled images or poses
Ranking accuracy (mean)
CIFAR-10 (mse)
PASCAL VOC 2007+2012
Figure 5. Loss-prediction accuracy of the loss prediction module.
for category recognition. It requires both object bounding
boxes and category labels for supervised learning.
We evaluate our method on PASCAL VOC 2007
and 2012 that provide full bounding boxes of 20
object categories.
VOC 2007 comprises trainval‚Äô07
and test‚Äô07 which contain 5,011 images and 4,952 images respectively. VOC 2012 provides 11,540 images as
trainval‚Äô12. Following the recent use of VOC for object detection, we make a super-set trainval‚Äô07+12 by
combining the two, and use it as the initial unlabeled
pool U16,551. The active learning method is evaluated over
test‚Äô07 with mean average precision (mAP), which is a
standard metric for object detection. We do not create a
random subset SM since the size of the pool U16,551 is not
very large in contrast to CIFAR-10.
Target model
We employ Single Shot Multibox Detector (SSD) as it is one of the popular models for recent object detection. It is a large network with a backbone
of VGG-16 . We have utilized an open source2 which
shows 0.7743 (mAP) slightly higher than the original paper.
Loss prediction module
SSD estimates bounding-boxes
and their classes from 6-level feature maps extracted from
{convi | i=4 3, 7, 8 2, 9 2, 10 2, 11 2} . Accordingly,
we also connect the loss prediction module to each of them
to utilize the 6 rich features for estimating the loss.
We use exactly the same hyper-parameter values and the data augmentation scheme described in ,
except for the number of iterations since we use a smaller
training set for each active learning cycle. We learn the
model set for 300 epochs with the mini-batch size of 32.
After 240 epochs, we reduce the learning rate from 0.001 to
0.0001. We set the scaling constant Œª in Equation 3 to 1.
2 
Number of labeled images
mAP (mean of 3 trials)
random mean
random mean¬±std
entropy mean
entropy mean¬±std
core-set mean
core-set mean¬±std
learn loss mean
learn loss mean¬±std
Figure 6. Active learning results of object detection over PASCAL
VOC 2007+2012.
Comparison targets
For the entropy-based method, we
compute the entropy of an image by averaging all entropy
values from softmax outputs corresponding to detection
For core-set, we also run K-Center-Greedy over
conv7 (i.e., FC7 in VGG-16) features after applying the spatial average pooling. Note, we use exactly the same hyperparameters to train SSDs for all methods including ours.
Figure 6 shows the results. Each point is an average of
3 trials with different initial labeled datasets. In the last active learning cycle, our method achieves 0.7338 mAP which
is 2.21% higher than 0.7117 of the random baseline. The
entropy and core-set methods, showing 0.7222 and 0.7171
respectively, also perform better than the random baseline.
However, our method outperforms these methods by margins of 1.15% and 1.63%. The entropy method cannot capture the uncertainty about bounding box regression, which
is an important element of object detection, so need to
design another uncertainty metric speciÔ¨Åed for regression.
The core-set method also needs to design a feature space
that well encodes object-centric information while being invariant to object locations. In contrast, our learning-based
approach does not need speciÔ¨Åc designs since it predicts
the Ô¨Ånal loss value, regardless of tasks. Even if it is much
difÔ¨Åcult to predict the Ô¨Ånal loss come from regression and
classiÔ¨Åcation, our loss prediction module yields about 70%
ranking accuracy as shown in Figure 5.
4.3. Human Pose Estimation
Human pose estimation is to localize all the body parts
from an image. The point annotations of all the body parts
are required for supervised learning. It is often approached
by a regression problem as the target is a set of points.
We choose MPII dataset which is commonly
used for the majority of recent works. We follow the same
splits used in where a training set consists of 22,246
poses from 14,679 images and a test set consists of 2,958
poses from 2,729 images. We use the training set as the
initial unlabeled pool U22,246. For each cycle, we obtain a
random sub-pool S5,000 from U22,246, following the similar
portion of the sub-pool to the entire pool in CIFAR-10. The
standard evaluation metric for this problem is Percentage of
Correct Key-points (PCK) which measures the percentage
of predicted key-points falling within a distance threshold
to the ground truth. Following , we use in
which the distance is normalized by a fraction of the head
size and the threshold is 0.5.
Networks , in which an hourglass network consists of
down-scale pooling and subsequent up-sampling processes
to allow bottom-up, top-down inference across scales. The
network produces heatmaps corresponding to the body
parts and they are compared to ground-truth heatmaps by
applying an MSE loss. We have utilized an open source3
yielding 88.78% ( ), which is similar to with
8 hourglass networks. Since learning 8 hourglass networks
on a single GPU with the original mini-batch size of 6
is too slow for our active learning experiments, we have
tried multi-GPU learning with larger mini-batch sizes.
However, the performance has signiÔ¨Åcantly decreased
as the mini-batch size increases, even without the loss
prediction module. Thus, we have inevitably stacked two
hourglass networks which show 86.95%.
Loss prediction module
For each hourglass network, the
body part heatmaps are driven from the last feature map of
(H,W,C)=(64,64,256). We choose this feature map to estimate the loss. As we stack two hourglass networks, the two
feature maps are given to our loss prediction module.
We use exactly the same hyper-parameter values and data augmentation scheme described in , except
the number of training iterations. We learn the model set for
125 epochs with the mini-batch size of 6. After 100 epochs,
we reduce the learning rate from 0.00025 to 0.000025. After 75 epochs, the gradient from the loss prediction module
is not propagated to the target model. We set the scaling
constant Œª in Equation 3 to 0.0001 since the scale of MSE
is very small (around 0.001 after several epochs).
Comparison targets
Stacked Hourglass Networks do not
produce softmax outputs but body part heatmaps. Thus, we
apply the softmax to each heatmap and estimate an entropy
3 
Number of labeled poses
 (mean of 3 trials)
random mean
random mean¬±std
entropy mean
entropy mean¬±std
core-set mean
core-set mean¬±std
learn loss mean
learn loss mean¬±std
Figure 7. Active learning results of human pose estimation over
for each body part.
We then average all of the entropy
For core-set, we run K-Center-Greedy over the
last feature maps after applying the spatial average pooling.
Note, we use exactly the same hyper-parameters to train the
target models for all methods including ours.
Experiment results are given in Figure 7. Each point
is also an average of 3 trials with different initial labeled
The results show that our method outperforms
other methods as the active learning cycle progresses. At
the end of the cycles, our method attains 0.8046 
while the entropy and core-set methods reach 0.7899 and
0.7985, respectively. The performance gaps to these methods are 1.47% and 0.61%.
The random baseline shows
the lowest of 0.7862. In human pose estimation, the entropy method is not as effective as the classiÔ¨Åcation problem. While this method is advantageous to classiÔ¨Åcation in
which a cross-entropy loss is directly minimized, this task
minimizes an MSE to estimate body part heatmaps. The
core-set method also requires a novel feature space that is
invariant to the body part location while preserving the local
body part features.
Our loss prediction module predicts the regression loss
with about 75% of ranking accuracy (Figure 5), which enables efÔ¨Åcient active learning in this problem. We visualize
how the predicted loss correlates with the real loss in Figure 8. At the top of the Ô¨Ågure, the data points of the MPII
test set are scattered to the axes of predicted loss and real
loss. Overall, the two values are correlated, and the correlation coefÔ¨Åcient (0 for no relation, 1 for strong relation)
is 0.68. At the bottom of the Ô¨Ågure, the data points are scattered to the axes of entropy and real loss. The correlation
coefÔ¨Åcient is 0.45, which is much lower than our predicted
Real loss (log-scale)
Predicted loss
Correlation coeff. = 0.68
line fitting
not picked
Real loss (log-scale)
+8.317 Correlation coeff. = 0.45
line fitting
not picked
Figure 8. Data visualization of (top) our method and (bottom)
entropy-based method. We use the model set from the last active learning cycle to obtain the loss, predicted loss and entropy of
a human pose. 2,000 poses randomly chosen from the MPII test
set are shown.
loss. The blue color means 20% data points selected from
the population according to the predicted loss or entropy.
The points chosen by our method actually have high loss
values, while the entropy method chooses many points with
low loss values. This visualization demonstrates that our
method is effective for selecting informative data points.
5. Limitations and Future Work
We have introduced a novel active learning method that
is applicable to current deep networks with a wide range of
tasks. The method has been veriÔ¨Åed with three major visual
recognition tasks with popular network architectures. Although the uncertainty score provided by this method has
been effective, the diversity or density of data was not considered. Also, the loss prediction accuracy was relatively
low in complex tasks such as object detection and human
pose estimation. We will continue this research to take data
distribution into consideration and design a better architecture and objective function to increase the accuracy of the
loss prediction module.