Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890–2896
Brussels, Belgium, October 31 - November 4, 2018. c⃝2018 Association for Computational Linguistics
Generating Natural Language Adversarial Examples
Moustafa Alzantot1∗, Yash Sharma2∗, Ahmed Elgohary3,
Bo-Jhang Ho1, Mani B. Srivastava1, Kai-Wei Chang1
1Department of Computer Science, University of California, Los Angeles (UCLA)
{malzantot, bojhang, mbs, kwchang}@ucla.edu
2Cooper Union 
3Computer Science Department, University of Maryland 
Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to
correctly classiﬁed examples which can cause
the model to misclassify.
In the image domain, these perturbations are often virtually
indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word
can drastically alter the semantics of the document.
Given these challenges, we use a
black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool
well-trained sentiment analysis and textual entailment models with success rates of 97% and
70%, respectively.
We additionally demonstrate that 92.3% of the successful sentiment
analysis adversarial examples are classiﬁed to
their original label by 20 human annotators,
and that the examples are perceptibly quite
similar. Finally, we discuss an attempt to use
adversarial training as a defense, but fail to
yield improvement, demonstrating the strength
and diversity of our adversarial examples. We
hope our ﬁndings encourage researchers to
pursue improving the robustness of DNNs in
the natural language domain.
Introduction
Recent research has found that deep neural networks (DNNs) are vulnerable to adversarial examples . The existence of adversarial examples has
been shown in image classiﬁcation and speech recognition .
In this work, we demonstrate that
adversarial examples can be constructed in the
context of natural language. Using a black-box
∗Moustafa Alzantot and Yash Sharma contribute equally
to this work.
population-based optimization algorithm, we successfully generate both semantically and syntactically similar adversarial examples against models trained on both the IMDB 
sentiment analysis task and the Stanford Natural
Language Inference (SNLI) 
textual entailment task. In addition, we validate
that the examples are both correctly classiﬁed by
human evaluators and similar to the original via
a human study.
Finally, we attempt to defend
against said adversarial attack using adversarial
training, but fail to yield any robustness, demonstrating the strength and diversity of the generated
adversarial examples.
Our results show that by minimizing the semantic and syntactic dissimilarity, an attacker can perturb examples such that humans correctly classify,
but high-performing models misclassify. We are
open-sourcing our attack1 to encourage research
in training DNNs robust to adversarial attacks in
the natural language domain.
Natural Language Adversarial
Adversarial examples have been explored primarily in the image recognition domain.
Examples have been generated through solving an optimization problem, attempting to induce misclassiﬁcation while minimizing the perceptual distortion .
Due to the computational cost of such approaches,
fast methods were introduced which, either in onestep or iteratively, shift all pixels simultaneously
until a distortion constraint is reached . Nearly all popular methods are gradientbased.
1 
adversarial_examples
Such methods, however, rely on the fact that
adding small perturbations to many pixels in the
image will not have a noticeable effect on a human
viewer. This approach obviously does not transfer
to the natural language domain, as all changes are
perceptible. Furthermore, unlike continuous image pixel values, words in a sentence are discrete
tokens. Therefore, it is not possible to compute the
gradient of the network loss function with respect
to the input words. A straightforward workaround
is to project input sentences into a continuous
space (e.g. word embeddings) and consider this as
the model input. However, this approach also fails
because it still assumes that replacing every word
with words nearby in the embedding space will not
be noticeable. Replacing words without accounting for syntactic coherence will certainly lead to
improperly constructed sentences which will look
odd to the reader.
Relative to the image domain, little work has
been pursued for generating natural language adversarial examples. Given the difﬁculty in generating semantics-preserving perturbations, distracting sentences have been added to the input document in order to induce misclassiﬁcation . In our work, we attempt to generate semantically and syntactically similar adversarial examples, via word replacements, resolving the aforementioned issues.
Minimizing the
number of word replacements necessary to induce misclassiﬁcation has been studied in previous work , however without consideration given to semantics or syntactics,
yielding incoherent generated examples. In recent
work, there have been a few attempts at generating adversarial examples for language tasks by using back-translation , exploiting machine-generated rules ,
and searching in underlying semantic space . In addition, while preparing our submission, we became aware of recent work which
target a similar contribution . We treat these contributions as parallel work.
Attack Design
Threat model
We assume the attacker has black-box access to
the target model; the attacker is not aware of the
model architecture, parameters, or training data,
and is only capable of querying the target model
with supplied inputs and obtaining the output predictions and their conﬁdence scores.
This setting has been extensively studied in the image domain , but has yet to be explored in
the context of natural language.
To avoid the limitations of gradient-based attack
methods, we design an algorithm for constructing
adversarial examples with the following goals in
mind. We aim to minimize the number of modiﬁed
words between the original and adversarial examples, but only perform modiﬁcations which retain
semantic similarity with the original and syntactic
coherence. To achieve these goals, instead of relying on gradient-based optimization, we developed
an attack algorithm that exploits population-based
gradient-free optimization via genetic algorithms.
An added beneﬁt of using gradient-free optimization is enabling use in the black-box case;
gradient-reliant algorithms are inapplicable in this
case, as they are dependent on the model being differentiable and the internals being accessible .
Genetic algorithms are inspired by the process
of natural selection, iteratively evolving a population of candidate solutions towards better solutions. The population of each iteration is a called a
generation. In each generation, the quality of population members is evaluated using a ﬁtness function. “Fitter” solutions are more likely to be selected for breeding the next generation. The next
generation is generated through a combination of
crossover and mutation.
Crossover is the process of taking more than one parent solution and
producing a child solution from them; it is analogous to reproduction and biological crossover.
Mutation is done in order to increase the diversity of population members and provide better exploration of the search space. Genetic algorithms
are known to perform well in solving combinatorial optimization problems , and due to employing a population of candidate solutions, these algorithms can ﬁnd successful adversarial examples
with fewer modiﬁcations.
Perturb Subroutine:
In order to explain our
algorithm,
we ﬁrst introduce the subroutine
Perturb. This subroutine accepts an input sentence xcur which can be either a modiﬁed sentence
or the same as xorig. It randomly selects a word w
in the sentence xcur and then selects a suitable replacement word that has similar semantic meaning, ﬁts within the surrounding context, and increases the target label prediction score.
In order to select the best replacement word,
Perturb applies the following steps:
• Computes the N nearest neighbors of the selected word according to the distance in the
GloVe embedding space to post-process the
adversary’s GloVe vectors to ensure that the
nearest neighbors are synonyms. The resulting embedding is independent of the embeddings used by victim models.
• Second, we use the Google 1 billion words
language model to ﬁlter out words that do not ﬁt within the context
surrounding the word w in xcur. We do so by
ranking the candidate words based on their
language model scores when ﬁt within the replacement context, and keeping only the top
K words with the highest scores.
• From the remaining set of words, we pick the
one that will maximize the target label prediction probability when it replaces the word
w in xcur.
• Finally, the selected word is inserted in place
of w, and Perturb returns the resulting sentence.
The selection of which word to replace in the
input sentence is done by random sampling with
probabilities proportional to the number of neighbors each word has within Euclidean distance δ in
the counter-ﬁtted embedding space, encouraging
the solution set to be large enough for the algorithm to make appropriate modiﬁcations. We exclude common articles and prepositions (e.g. a, to)
from being selected for replacement.
Optimization Procedure:
The optimization algorithm can be seen in Algorithm 1. The algorithm starts by creating the initial generation P0 of
size S by calling the Perturb subroutine S times
to create a set of distinct modiﬁcations to the original sentence. Then, the ﬁtness of each population member in the current generation is computed
as the target label prediction probability, found by
Algorithm 1 Finding adversarial examples
for i = 1, ..., S in population do
i ←Perturb(xorig, target)
for g = 1, 2...G generations do
for i = 1, ..., S in population do
xadv = Pg−1
arg maxj F g−1
if arg maxc f(xadv)c == t then
return xadv ▷{Found successful attack}
1 = {xadv}
p = Normalize(F g−1)
for i = 2, ..., S in population do
Sample parent1 from Pg−1 with probs p
Sample parent2 from Pg−1 with probs p
child = Crossover(parent1, parent2)
childmut = Perturb(child, target)
i = {childmut}
querying the victim model function f. If a population member’s predicted label is equal to the
target label, the optimization is complete. Otherwise, pairs of population members from the current generation are randomly sampled with probability proportional to their ﬁtness values. A new
child sentence is then synthesized from a pair of
parent sentences by independently sampling from
the two using a uniform distribution. Finally, the
Perturb subroutine is applied to the resulting
Experiments
To evaluate our attack method, we trained models
for the sentiment analysis and textual entailment
classiﬁcation tasks. For both models, each word
in the input sentence is ﬁrst projected into a ﬁxed
300-dimensional vector space using GloVe . Each of the models used
are based on popular open-source benchmarks,
and can be found in the following repositories23.
Model descriptions are given below.
Sentiment Analysis: We trained a sentiment
analysis model using the IMDB dataset of movie
reviews . The IMDB dataset
consists of 25,000 training examples and 25,000
test examples. The LSTM model is composed of
128 units, and the outputs across all time steps are
2 
blob/master/examples/imdb_lstm.py
3 
snli/blob/master/snli_rnn.py
Original Text Prediction = Negative. (Conﬁdence = 78.0%)
This movie had terrible acting, terrible plot, and terrible choice of actors. (Leslie Nielsen ...come on!!!)
the one part I considered slightly funny was the battling FBI/CIA agents, but because the audience was
mainly kids they didn’t understand that theme.
Adversarial Text Prediction = Positive. (Conﬁdence = 59.8%)
This movie had horriﬁc acting, horriﬁc plot, and horrifying choice of actors. (Leslie Nielsen ...come
on!!!) the one part I regarded slightly funny was the battling FBI/CIA agents, but because the audience
was mainly youngsters they didn’t understand that theme.
Table 1: Example of attack results for the sentiment analysis task. Modiﬁed words are highlighted in green and
red for the original and adversarial texts, respectively.
Original Text Prediction: Entailment (Conﬁdence = 86%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A runner wants to head for the ﬁnish line.
Adversarial Text Prediction: Contradiction (Conﬁdence = 43%)
Premise: A runner wearing purple strives for the ﬁnish line.
Hypothesis: A racer wants to head for the ﬁnish line.
Table 2: Example of attack results for the textual entailment task. Modiﬁed words are highlighted in green and red
for the original and adversarial texts, respectively.
Sentiment Analysis
Textual Entailment
Perturb baseline
Genetic attack
Table 3: Comparison between the attack success rate and mean percentage of modiﬁcations required by the genetic
attack and perturb baseline for the two tasks.
averaged and fed to the output layer. The test accuracy of the model is 90%, which is relatively close
to the state-of-the-art results on this dataset.
Textual Entailment: We trained a textual entailment model using the Stanford Natural Language Inference (SNLI) corpus ,
which encodes the premise and hypothesis sentences by performing a summation over the word
embeddings, concatenates the two sentence embeddings, and ﬁnally passes the output through 3
600-dimensional ReLU layers before feeding it to
a 3-way softmax. The model predicts whether the
premise sentence entails, contradicts or is neutral
to the hypothesis sentence. The test accuracy of
the model is 83% which is also relatively close to
the state-of-the-art .
Attack Evaluation Results
We randomly sampled 1000, and 500 correctly
classiﬁed examples from the test sets of the two
tasks to evaluate our algorithm. Correctly classi-
ﬁed examples were chosen to limit the accuracy
levels of the victim models from confounding our
results. For the sentiment analysis task, the attacker aims to divert the prediction result from
positive to negative, and vice versa. For the textual entailment task, the attacker is only allowed
to modify the hypothesis, and aims to divert the
prediction result from ‘entailment’ to ‘contradiction’, and vice versa.
We limit the attacker to
maximum G = 20 iterations, and ﬁx the hyperparameter values to S = 60, N = 8, K = 4, and
δ = 0.5. We also ﬁxed the maximum percentage
of allowed changes to the document to be 20% and
25% for the two tasks, respectively. If increased,
the success rate would increase but the mean quality would decrease. If the attack does not succeed
within the iterations limit or exceeds the speciﬁed
threshold, it is counted as a failure.
Sample outputs produced by our attack are
shown in Tables 1 and 2. Additional outputs can
be found in the supplementary material. Table 3
shows the attack success rate and mean percentage of modiﬁed words on each task. We compare
to the Perturb baseline, which greedily applies
the Perturb subroutine, to validate the use of
population-based optimization.
As can be seen
from our results, we are able to achieve high success rate with a limited number of modiﬁcations
on both tasks. In addition, the genetic algorithm
signiﬁcantly outperformed the Perturb baseline
in both success rate and percentage of words modiﬁed, demonstrating the additional beneﬁt yielded
by using population-based optimization. Testing
using a single TitanX GPU, for sentiment analysis and textual entailment, we measured average
runtimes on success to be 43.5 and 5 seconds per
example, respectively. The high success rate and
reasonable runtimes demonstrate the practicality
of our approach, even when scaling to long sentences, such as those found in the IMDB dataset.
Speaking of which, our success rate on textual
entailment is lower due to the large disparity in
sentence length. On average, hypothesis sentences
in the SNLI corpus are 9 words long, which is
very short compared to IMDB (229 words, limited to 100 for experiments). With sentences that
short, applying successful perturbations becomes
much harder, however we were still able to achieve
a success rate of 70%. For the same reason, we
didn’t apply the Perturb baseline on the textual
entailment task, as the Perturb baseline fails to
achieve any success under the limits of the maximum allowed changes constraint.
User study
We performed a user study on the sentiment analysis task with 20 volunteers to evaluate how perceptible our adversarial perturbations are. Note
that the number of participating volunteers is signiﬁcantly larger than used in previous studies . The user
study was composed of two parts. First, we presented 100 adversarial examples to the participants
and asked them to label the sentiment of the text
(i.e., positive or negative.) 92.3% of the responses
matched the original text sentiment, indicating that
our modiﬁcation did not signiﬁcantly affect human
judgment on the text sentiment. Second, we prepared 100 questions, each question includes the
original example and the corresponding adversarial example in a pair. Participants were asked to
judge the similarity of each pair on a scale from
1 (very similar) to 4 (very different). The average
rating is 2.23 ± 0.25, which shows the perceived
difference is also small.
Adversarial Training
The results demonstrated in section 4.1 raise the
following question: How can we defend against
these attacks? We performed a preliminary experiment to see if adversarial training , the only effective defense in the image domain, can be used to lower the attack success rate.
We generated 1000 adversarial examples on the
cleanly trained sentiment analysis model using the
IMDB training set, appended them to the existing
training set, and used the updated dataset to adversarially train a model from scratch. We found
that adversarial training provided no additional robustness beneﬁt in our experiments using the test
set, despite the fact that the model achieves near
100% accuracy classifying adversarial examples
included in the training set. These results demonstrate the diversity in the perturbations generated
by our attack algorithm, and illustrates the difﬁculty in defending against adversarial attacks. We
hope these results inspire further work in increasing the robustness of natural language models.
Conclusion
We demonstrate that despite the difﬁculties in generating imperceptible adversarial examples in the
natural language domain, semantically and syntactically similar adversarial examples can be crafted
using a black-box population-based optimization
algorithm, yielding success on both the sentiment
analysis and textual entailment tasks. Our human
study validated that the generated examples were
indeed adversarial and perceptibly quite similar.
We hope our work encourages researchers to pursue improving the robustness of DNNs in the natural language domain.
Acknowledgement
This research was supported in part by the U.S.
Army Research Laboratory and the UK Ministry
of Defence under Agreement Number W911NF-
16-3-0001, the National Science Foundation under
award # CNS-1705135, OAC-1640813, and IIS-
1760523, and the NIH Center of Excellence for
Mobile Sensor Data-to-Knowledge (MD2K) under award 1-U54EB020404-01. Ahmed Elgohary
is funded by an IBM PhD Fellowship. Any ﬁndings in this material are those of the author(s) and
do not reﬂect the views of any of the above funding agencies. The U.S. and U.K. Governments are
authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation hereon.