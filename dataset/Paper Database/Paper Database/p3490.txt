Few-Shot Adversarial Learning of Realistic Neural Talking Head Models
Egor Zakharov1,2
Aliaksandra Shysheya1,2
Egor Burkov1,2
Victor Lempitsky1,2
1Samsung AI Center, Moscow
2Skolkovo Institute of Science and Technology
Target →Landmarks →Result
Target →Landmarks →Result
Figure 1: The results of talking head image synthesis using face landmark tracks extracted from a different video sequence
of the same person (on the left), and using face landmarks of a different person (on the right). The results are conditioned on
the landmarks taken from the target frame, while the source frame is an example from the training set. The talking head
models on the left were trained using eight frames, while the models on the right were trained in a one-shot manner.
Several recent works have shown how highly realistic
human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require
training on a large dataset of images of a single person.
However, in many practical scenarios, such personalized
talking head models need to be learned from a few image
views of a person, potentially even a single image. Here, we
present a system with such few-shot capability. It performs
lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural
talking head models of previously unseen people as adversarial training problems with high capacity generators and
discriminators. Crucially, the system is able to initialize the
parameters of both the generator and the discriminator in a
person-speciﬁc way, so that training can be based on just a
few images and done quickly, despite the need to tune tens
of millions of parameters. We show that such an approach is
able to learn highly realistic and personalized talking head
models of new people and even portrait paintings.
1. Introduction
In this work, we consider the task of creating personalized photorealistic talking head models, i.e. systems that
can synthesize plausible video-sequences of speech expressions and mimics of a particular individual. More specifically, we consider the problem of synthesizing photorealistic personalized head images given a set of face landmarks, which drive the animation of the model. Such ability
has practical applications for telepresence, including videoconferencing and multi-player games, as well as special effects industry. Synthesizing realistic talking head sequences
is known to be hard for two reasons. First, human heads
have high photometric, geometric and kinematic complexity. This complexity stems not only from modeling faces
(for which a large number of modeling approaches exist)
but also from modeling mouth cavity, hair, and garments.
The second complicating factor is the acuteness of the human visual system towards even minor mistakes in the appearance modeling of human heads (the so-called uncanny
valley effect ). Such low tolerance to modeling mistakes explains the current prevalence of non-photorealistic
cartoon-like avatars in many practically-deployed teleconferencing systems.
To overcome the challenges, several works have proposed to synthesize articulated head sequences by warping
a single or multiple static frames. Both classical warping
algorithms and warping ﬁelds synthesized using machine learning (including deep learning) can be
used for such purposes. While warping-based systems can
create talking head sequences from as little as a single image, the amount of motion, head rotation, and disocclusion
 
that they can handle without noticeable artifacts is limited.
Direct (warping-free) synthesis of video frames using
adversarially-trained deep convolutional networks (ConvNets) presents the new hope for photorealistic talking
heads. Very recently, some remarkably realistic results have
been demonstrated by such systems .
However, to succeed, such methods have to train large networks,
where both generator and discriminator have tens of millions of parameters for each talking head. These systems,
therefore, require a several-minutes-long video or
a large dataset of photographs as well as hours of GPU
training in order to create a new personalized talking head
model. While this effort is lower than the one required by
systems that construct photo-realistic head models using sophisticated physical and optical modeling , it is still excessive for most practical telepresence scenarios, where we
want to enable users to create their personalized head models with as little effort as possible.
In this work, we present a system for creating talking
head models from a handful of photographs (so-called fewshot learning) and with limited training time. In fact, our
system can generate a reasonable result based on a single
photograph (one-shot learning), while adding a few more
photographs increases the ﬁdelity of personalization. Similarly to , the talking heads created by our model
are deep ConvNets that synthesize video frames in a direct
manner by a sequence of convolutional operations rather
than by warping. The talking heads created by our system
can, therefore, handle a large variety of poses that goes beyond the abilities of warping-based systems.
The few-shot learning ability is obtained through extensive pre-training (meta-learning) on a large corpus of talking head videos corresponding to different speakers with diverse appearance. In the course of meta-learning, our system simulates few-shot learning tasks and learns to transform landmark positions into realistically-looking personalized photographs, given a small training set of images
with this person. After that, a handful of photographs of
a new person sets up a new adversarial learning problem
with high-capacity generator and discriminator pre-trained
via meta-learning. The new adversarial problem converges
to the state that generates realistic and personalized images
after a few training steps.
In the experiments, we provide comparisons of talking
heads created by our system with alternative neural talking
head models via quantitative measurements and a
user study, where our approach generates images of suf-
ﬁcient realism and personalization ﬁdelity to deceive the
study participants. We demonstrate several uses of our talking head models, including video synthesis using landmark
tracks extracted from video sequences of the same person,
as well as puppeteering (video synthesis of a certain person
based on the face landmark tracks of a different person).
2. Related work
A huge body of works is devoted to statistical modeling of the appearance of human faces , with remarkably
good results obtained both with classical techniques 
and, more recently, with deep learning (to name
just a few). While modeling faces is a highly related task
to talking head modeling, the two tasks are not identical,
as the latter also involves modeling non-face parts such as
hair, neck, mouth cavity and often shoulders/upper garment.
These non-face parts cannot be handled by some trivial extension of the face modeling methods since they are much
less amenable for registration and often have higher variability and higher complexity than the face part. In principle, the results of face modeling or lips modeling 
can be stitched into an existing head video. Such design,
however, does not allow full control over the head rotation
in the resulting video and therefore does not result in a fully-
ﬂedged talking head system.
The design of our system borrows a lot from the recent
progress in generative modeling of images. Thus, our architecture uses adversarial training and, more speciﬁcally,
the ideas behind conditional discriminators , including projection discriminators . Our meta-learning stage
uses the adaptive instance normalization mechanism ,
which was shown to be useful in large-scale conditional
generation tasks . We also ﬁnd an idea of contentstyle decomposition to be extremely useful for separating the texture from the body pose.
The model-agnostic meta-learner (MAML) uses
meta-learning to obtain the initial state of an image classiﬁer, from which it can quickly converge to image classi-
ﬁers of unseen classes, given few training samples. This
high-level idea is also utilized by our method, though our
implementation of it is rather different.
Several works
have further proposed to combine adversarial training with
meta-learning. Thus, data-augmentation GAN , Meta-
GAN , adversarial meta-learning use adversariallytrained networks to generate additional examples for classes
unseen at the meta-learning stage. While these methods
are focused on boosting the few-shot classiﬁcation performance, our method deals with the training of image generation models using similar adversarial objectives. To summarize, we bring the adversarial ﬁne-tuning into the metalearning framework. The former is applied after we obtain
initial state of the generator and the discriminator networks
via the meta-learning stage.
Finally, very related to ours are the two recent works on
text-to-speech generation . Their setting (few-shot
learning of generative models) and some of the components
(standalone embedder network, generator ﬁne-tuning) are
are also used in our case. Our work differs in the application
domain, the use of adversarial learning, its adaptation to the
meta-learning process and implementation details.
Realism score
Synthesized
Match loss
Content loss
Ground truth
RGB & landmarks
Discriminator
AdaIN parameters
Figure 2: Our meta-learning architecture involves the embedder network that maps head images (with estimated face landmarks) to the embedding vectors, which contain pose-independent information. The generator network maps input face
landmarks into output frames through the set of convolutional layers, which are modulated by the embedding vectors via
adaptive instance normalization. During meta-learning, we pass sets of frames from the same video through the embedder,
average the resulting embeddings and use them to predict adaptive parameters of the generator. Then, we pass the landmarks
of a different frame through the generator, comparing the resulting image with the ground truth. Our objective function
includes perceptual and adversarial losses, with the latter being implemented via a conditional projection discriminator.
3. Methods
3.1. Architecture and notation
The meta-learning stage of our approach assumes the
availability of M video sequences, containing talking heads
of different people. We denote with xi the i-th video sequence and with xi(t) its t-th frame. During the learning
process, as well as during test time, we assume the availability of the face landmarks’ locations for all frames (we use an
off-the-shelf face alignment code to obtain them). The
landmarks are rasterized into three-channel images using a
predeﬁned set of colors to connect certain landmarks with
line segments. We denote with yi(t) the resulting landmark
image computed for xi(t).
In the meta-learning stage of our approach, the following
three networks are trained (Figure 2):
• The embedder E(xi(s), yi(s); φ) takes a video frame
xi(s), an associated landmark image yi(s) and maps
these inputs into an N-dimensional vector ˆei(s). Here,
φ denotes network parameters that are learned in the
meta-learning stage. In general, during meta-learning,
we aim to learn φ such that the vector ˆei(s) contains
video-speciﬁc information (such as the person’s identity)
that is invariant to the pose and mimics in a particular
frame s. We denote embedding vectors computed by the
embedder as ˆei.
• The generator G(yi(t), ˆei; ψ, P) takes the landmark image yi(t) for the video frame not seen by the embedder,
the predicted video embedding ˆei and outputs a synthesized video frame ˆxi(t). The generator is trained to maximize the similarity between its outputs and the ground
truth frames. All parameters of the generator are split
into two sets: the person-generic parameters ψ, and the
person-speciﬁc parameters ˆψi.
During meta-learning,
only ψ are trained directly, while ˆψi are predicted from
the embedding vector ˆei using a trainable projection matrix P: ˆψi = Pˆei.
• The discriminator D(xi(t), yi(t), i; θ, W, w0, b) takes a
video frame xi(t), an associated landmark image yi(t)
and the index of the training sequence i. Here, θ, W, w0
and b denote the learnable parameters associated with
the discriminator. The discriminator contains a ConvNet
part V (xi(t), yi(t); θ) that maps the input frame and the
landmark image into an N-dimensional vector. The discriminator predicts a single scalar (realism score) r, that
indicates whether the input frame xi(t) is a real frame of
the i-th video sequence and whether it matches the input
pose yi(t), based on the output of its ConvNet part and
the parameters W, w0, b.
3.2. Meta-learning stage
During the meta-learning stage of our approach, the parameters of all three networks are trained in an adversarial
fashion. It is done by simulating episodes of K-shot learning (K = 8 in our experiments). In each episode, we randomly draw a training video sequence i and a single frame t
from that sequence. In addition to t, we randomly draw additional K frames s1, s2, . . . , sK from the same sequence.
We then compute the estimate ˆei of the i-th video embedding by simply averaging the embeddings ˆei(sk) predicted
for these additional frames:
E (xi(sk), yi(sk); φ) .
A reconstruction ˆxi(t) of the t-th frame, based on the
estimated embedding ˆei, is then computed:
ˆxi(t) = G (yi(t), ˆei; ψ, P) .
The parameters of the embedder and the generator are
then optimized to minimize the following objective that
comprises the content term, the adversarial term, and the
embedding match term:
L(φ, ψ,P, θ, W, w0, b) = LCNT(φ, ψ, P)+
LADV(φ, ψ, P, θ, W, w0, b) + LMCH(φ, W) .
In (3), the content loss term LCNT measures the distance
between the ground truth image xi(t) and the reconstruction ˆxi(t) using the perceptual similarity measure , corresponding to VGG19 network trained for ILSVRC
classiﬁcation and VGGFace network trained for face
veriﬁcation. The loss is calculated as the weighted sum of
L1 losses between the features of these networks.
The adversarial term in (3) corresponds to the realism
score computed by the discriminator, which needs to be
maximized, and a feature matching term , which essentially is a perceptual similarity measure, computed using
discriminator (it helps with the stability of the training):
LADV(φ, ψ, P, θ, W, w0, b) =
−D(ˆxi(t), yi(t), i; θ, W, w0, b) + LFM .
Following the projection discriminator idea , the
columns of the matrix W contain the embeddings that correspond to individual videos. The discriminator ﬁrst maps
its inputs to an N-dimensional vector V (xi(t), yi(t); θ) and
then computes the realism score as:
D(ˆxi(t), yi(t), i; θ, W, w0, b) =
V (ˆxi(t), yi(t); θ)T (Wi + w0) + b ,
where Wi denotes the i-th column of the matrix W. At the
same time, w0 and b do not depend on the video index, so
these terms correspond to the general realism of ˆxi(t) and
its compatibility with the landmark image yi(t).
Thus, there are two kinds of video embeddings in our
system: the ones computed by the embedder, and the ones
that correspond to the columns of the matrix W in the discriminator. The match term LMCH(φ, W) in (3) encourages
the similarity of the two types of embeddings by penalizing
the L1-difference between E (xi(sk), yi(sk); φ) and Wi.
As we update the parameters φ of the embedder and the
parameters ψ of the generator, we also update the parameters θ, W, w0, b of the discriminator. The update is driven
by the minimization of the following hinge loss, which encourages the increase of the realism score on real images
xi(t) and its decrease on synthesized images ˆxi(t):
LDSC(φ, ψ, P, θ, W, w0, b) =
max(0, 1 + D(ˆxi(t), yi(t), i; φ, ψ, θ, W, w0, b))+
max(0, 1 −D(xi(t), yi(t), i; θ, W, w0, b)) .
The objective (6) thus compares the realism of the fake example ˆxi(t) and the real example xi(t) and then updates
the discriminator parameters to push these scores below −1
and above +1 respectively. The training proceeds by alternating updates of the embedder and the generator that minimize the losses LCNT, LADV and LMCH with the updates of
the discriminator that minimize the loss LDSC.
3.3. Few-shot learning by ﬁne-tuning
Once the meta-learning has converged, our system can
learn to synthesize talking head sequences for a new person,
unseen during meta-learning stage. As before, the synthesis is conditioned on the landmark images. The system is
learned in a few-shot way, assuming that T training images
x(1), x(2), . . . , x(T) (e.g. T frames of the same video) are
given and that y(1), y(2), . . . , y(T) are the corresponding
landmark images. Note that the number of frames T needs
not to be equal to K used in the meta-learning stage.
Naturally, we can use the meta-learned embedder to estimate the embedding for the new talking head sequence:
E(x(t), y(t); φ) ,
reusing the parameters φ estimated in the meta-learning
stage. A straightforward way to generate new frames, corresponding to new landmark images, is then to apply the generator using the estimated embedding ˆeNEW and the metalearned parameters ψ, as well as projection matrix P. By
doing so, we have found out that the generated images are
plausible and realistic, however, there often is a considerable identity gap that is not acceptable for most applications
aiming for high personalization degree.
This identity gap can often be bridged via the ﬁne-tuning
stage. The ﬁne-tuning process can be seen as a simpliﬁed
version of meta-learning with a single video sequence and a
smaller number of frames. The ﬁne-tuning process involves
the following components:
• The generator G(y(t), ˆeNEW; ψ, P) is now replaced with
G′(y(t); ψ, ψ′). As before, it takes the landmark image
y(t) and outputs the synthesized frame ˆx(t). Importantly,
the person-speciﬁc generator parameters, which we now
denote with ψ′, are now directly optimized alongside the
person-generic parameters ψ. We still use the computed
embeddings ˆeNEW and the projection matrix P estimated
at the meta-learning stage to initialize ψ′, i.e. we start
with ψ′ = PˆeNEW.
• The discriminator D′(x(t), y(t); θ, w′, b), as before,
computes the realism score. Parameters θ of its ConvNet
part V (x(t), y(t); θ) and bias b are initialized to the result of the meta-learning stage. The initialization of w′ is
discussed below.
During ﬁne-tuning, the realism score of the discriminator is
obtained in a similar way to the meta-learning stage:
D′(ˆx(t), y(t); θ, w′, b) =
V (ˆx(t), y(t); θ)T w′ + b .
As can be seen from the comparison of expressions (5) and
(8), the role of the vector w′ in the ﬁne-tuning stage is the
same as the role of the vector Wi +w0 in the meta-learning
stage. For the intiailization, we do not have access to the
analog of Wi for the new personality (since this person is
not in the meta-learning dataset). However, the match term
LMCH in the meta-learning process ensures the similarity
between the discriminator video-embeddings and the vectors computed by the embedder. Hence, we can initialize
w′ to the sum of w0 and ˆeNEW.
Once the new learning problem is set up, the loss functions of the ﬁne-tuning stage follow directly from the metalearning variants. Thus, the generator parameters ψ and ψ′
are optimized to minimize the simpliﬁed objective:
L′(ψ, ψ′, θ, w′, b) =
CNT(ψ, ψ′) + L′
ADV(ψ, ψ′, θ, w′, b) ,
where t ∈{1 . . . T} is the number of the training example.
The discriminator parameters θ, wNEW, b are optimized by
minimizing the same hinge loss as in (6):
DSC(ψ, ψ′, θ, w′, b) =
max(0, 1 + D(ˆx(t), y(t); ψ, ψ′, θ, w′, b))+
max(0, 1 −D(x(t), y(t); θ, w′, b)) .
In most situations, the ﬁne-tuned generator provides a
much better ﬁt of the training sequence. The initialization
of all parameters via the meta-learning stage is also crucial.
As we show in the experiments, such initialization injects a
strong realistic talking head prior, which allows our model
to extrapolate and predict realistic images for poses with
varying head poses and facial expressions.
3.4. Implementation details
We base our generator network G(yi(t), ˆei; ψ, P) on the
image-to-image translation architecture proposed by Johnson et. al. , but replace downsampling and upsampling
layers with residual blocks similarly to (with batch normalization replaced by instance normalization ).
The person-speciﬁc parameters ˆψi serve as the afﬁne coefﬁcients of instance normalization layers, following the
adaptive instance normalization technique proposed in ,
though we still use regular (non-adaptive) instance normalization layers in the downsampling blocks that encode landmark images yi(t).
For the embedder E(xi(s), yi(s); φ) and the convolutional part of the discriminator V (xi(t), yi(t); θ), we use
similar networks, which consist of residual downsampling
blocks (same as the ones used in the generator, but without normalization layers). The discriminator network, compared to the embedder, has an additional residual block at
the end, which operates at 4×4 spatial resolution. To obtain
the vectorized outputs in both networks, we perform global
sum pooling over spatial dimensions followed by ReLU.
We use spectral normalization for all convolutional
and fully connected layers in all the networks. We also use
self-attention blocks, following and . They are inserted at 32×32 spatial resolution in all downsampling parts
of the networks and at 64 × 64 resolution in the upsampling
part of the generator.
For the calculation of LCNT, we evaluate L1 loss between activations of Conv1,6,11,20,29 VGG19 layers
and Conv1,6,11,18,25 VGGFace layers for real and
fake images. We sum these losses with the weights equal to
1.5·10−1 for VGG19 and 2.5·10−2 for VGGFace terms. We
use Caffe trained versions for both of these networks.
For LFM, we use activations after each residual block of the
discriminator network and the weights equal to 10. Finally,
for LMCH we also set the weight to 10.
We set the minimum number of channels in convolutional layers to 64 and the maximum number of channels
as well as the size N of the embedding vectors to 512. In
total, the embedder has 15 million parameters, the generator has 38 million parameters. The convolutional part of the
discriminator has 20 million parameters. The networks are
optimized using Adam . We set the learning rate of the
embedder and the generator networks to 5 × 10−5 and to
2 × 10−4 for the discriminator, doing two update steps for
the latter per one of the former, following .
4. Experiments
Two datasets with talking head videos are used for quantitative and qualitative evaluation: VoxCeleb1 (256p
videos at 1 fps) and VoxCeleb2 (224p videos at 25 fps),
with the latter having approximately 10 times more videos
Method (T)
X2Face (1)
Pix2pixHD (1)
X2Face (8)
Pix2pixHD (8)
X2Face (32)
Pix2pixHD (32)
Ours-FF (1)
Ours-FT (1)
Ours-FF (8)
Ours-FT (8)
Ours-FF (32)
Ours-FT (32)
Table 1: Quantitative comparison of methods on different
datasets with multiple few-shot learning settings. Please refer to the text for more details and discussion.
than the former. VoxCeleb1 is used for comparison with
baselines and ablation studies, while by using VoxCeleb2
we show the full potential of our approach.
For the quantitative comparisons, we ﬁne-tune
all models on few-shot learning sets of size T for a person not seen during meta-learning (or pretraining) stage.
After the few-shot learning, the evaluation is performed
on the hold-out part of the same sequence (so-called selfreenactment scenario). For the evaluation, we uniformly
sampled 50 videos from VoxCeleb test sets and 32 holdout frames for each of these videos (the ﬁne-tuning and the
hold-out parts do not overlap).
We use multiple comparison metrics to evaluate photorealism and identity preservation of generated images.
Namely, we use Frechet-inception distance (FID) ,
mostly measuring perceptual realism, structured similarity
(SSIM) , measuring low-level similarity to the ground
truth images, and cosine similarity (CSIM) between embedding vectors of the state-of-the-art face recognition network for measuring identity mismatch (note that this
network has quite different architecture from VGGFace
used within content loss calculation during training).
We also perform a user study in order to evaluate perceptual similarity and realism of the results as seen by the human respondents. We show people the triplets of images of
the same person taken from three different video sequences.
Two of these images are real and one is fake, produced by
one of the methods, which are being compared. We ask the
user to ﬁnd the fake image given that all of these images are
of the same person. This evaluates both photo-realism and
identity preservation because the user can infer the identity
from the two real images (and spot an identity mismatch
even if the generated image is perfectly realistic). We use
the user accuracy (success rate) as our metric. The lower
bound here is the accuracy of one third (when users cannot spot fakes based on non-realism or identity mismatch
and have to guess randomly). Generally, we believe that
this user-driven metric (USER) provides a much better idea
of the quality of the methods compared to FID, SSIM, or
On the VoxCeleb1 dataset we compare our
model against two other systems:
X2Face and
Pix2pixHD . For X2Face, we have used the model, as
well as pretrained weights, provided by the authors (in the
original paper it was also trained and evaluated on the Vox-
Celeb1 dataset). For Pix2pixHD, we pretrained the model
from scratch on the whole dataset for the same amount of
iterations as our system without any changes to thetraining
pipeline proposed by the authors. We picked X2Face as a
strong baseline for warping-based methods and Pix2pixHD
for direct synthesis methods.
In our comparison, we evaluate the models in several
scenarios by varying the number of frames T used in fewshot learning. X2Face, as a feed-forward method, is simply
initialized via the training frames, while Pix2pixHD and
our model are being additionally ﬁne-tuned for 40 epochs
on the few-shot set. Notably, in the comparison, X2Face
uses dense correspondence ﬁeld, computed on the ground
truth image, to synthesize the generated one, while our
method and Pix2pixHD use very sparse landmark information, which arguably gives X2Face an unfair advantage.
Comparison results.
We perform comparison with baselines in three different setups, with 1, 8 and 32 frames in the
ﬁne-tuning set. Test set, as mentioned before, consists of
32 hold-out frames for each of the 50 test video sequences.
Moreover, for each test frame we sample two frames at random from the other video sequences with the same person.
These frames are used in triplets alongside with fake frames
for user-study.
As we can see in Table 1-Top, baselines consistently outperform our method on the two of our similarity metrics.
We argue that this is intrinsic to the methods themselves:
X2Face uses L2 loss during optimization , which leads
to a good SSIM score. On the other hand, Pix2pixHD maximizes only perceptual metric, without identity preservation
loss, leading to minimization of FID, but has bigger identity
mismatch, as seen from the CSIM column. Moreover, these
metrics do not correlate well with human perception, since
both of these methods produce uncanny valley artifacts, as
can be seen from qualitative comparison Figure 3 and the
Ground truth
Figure 3: Comparison on the VoxCeleb1 dataset. For each of the compared methods, we perform one- and few-shot learning
on a video of a person not seen during meta-learning or pretraining. We set the number of training frames equal to T (the
leftmost column). One of the training frames is shown in the source column. Next columns show ground truth image, taken
from the test part of the video sequence, and the generated results of the compared methods.
user study results. Cosine similarity, on the other hand, better correlates with visual quality, but still favours blurry, less
realistic images, and that can also be seen by comparing Table 1-Top with the results presented in Figure 3.
While the comparison in terms of the objective metrics
is inconclusive, the user study (that included 4800 triplets,
each shown to 5 users) clearly reveals the much higher realism and personalization degree achieved by our method.
We have also carried out the ablation study of our system
and the comparison of the few-shot learning timings. Both
are provided in the Supplementary material.
Large-scale results.
We then scale up the available data
and train our method on a larger VoxCeleb2 dataset. Here,
we train two variants of our method. FF (feed-forward)
variant is trained for 150 epochs without the embedding
matching loss LMCH and, therefore, we only use it without ﬁne-tuning (by simply predicting adaptive parameters
ψ′ via the projection of the embedding ˆeNEW). The FT variant is trained for half as much (75 epochs) but with LMCH,
which allows ﬁne-tuning. We run the evaluation for both of
these models since they allow to trade off few-shot learning
speed versus the results quality. Both of them achieve considerably higher scores, compared to smaller-scale models
trained on VoxCeleb1. Notably, the FT model reaches the
lower bound of 0.33 for the user study accuracy in T = 32
setting, which is a perfect score. We present results for both
of these models in Figure 4 and more results (including results, where animation is driven by landmarks from a different video of the same person) are given in the supplementary material and in Figure 1.
Generally, judging by the results of comparisons (Table 1-Bottom) and the visual assessment, the FF model performs better for low-shot learning (e.g. one-shot), while the
FT model achieves higher quality for bigger T via adversarial ﬁne-tuning.
Puppeteering results.
Finally, we show the results for the
puppeteering of photographs and paintings. For that, we
evaluate the model, trained in one-shot setting, on poses
from test videos of the VoxCeleb2 dataset. We rank these
videos using CSIM metric, calculated between the original
image and the generated one. This allows us to ﬁnd persons with similar geometry of the landmarks and use them
for the puppeteering. The results can be seen in Figure 5 as
well as in Figure 1.
Ground truth
before ﬁne-tuning
after ﬁne-tuning
Figure 4: Results for our best models on the VoxCeleb2 dataset. The number of training frames is, again, equal to T (the
leftmost column) and the example training frame in shown in the source column. Next columns show ground truth image
and the results for Ours-FF feed-forward model, Ours-FT model before and after ﬁne-tuning. While the feed-forward
variant allows fast (real-time) few-shot learning of new avatars, ﬁne-tuning ultimately provides better realism and ﬁdelity.
Generated images
Figure 5: Bringing still photographs to life. We show the
puppeteering results for one-shot models learned from photographs in the source column. Driving poses were taken
from the VoxCeleb2 dataset. Digital zoom recommended.
5. Conclusion
We have presented a framework for meta-learning of adversarial generative models, which is able to train highlyrealistic virtual talking heads in the form of deep generator
networks. Crucially, only a handful of photographs (as little
as one) is needed to create a new model, whereas the model
trained on 32 images achieves perfect realism and personalization score in our user study (for 224p static images).
Currently, the key limitations of our method are the mimics representation (in particular, the current set of landmarks
does not represent the gaze in any way) and the lack of
landmark adaptation. Using landmarks from a different person leads to a noticeable personality mismatch. So, if one
wants to create “fake” puppeteering videos without such
mismatch, some landmark adaptation is needed. We note,
however, that many applications do not require puppeteering a different person and instead only need the ability to
drive one’s own talking head. For such scenario, our approach already provides a high-realism solution.