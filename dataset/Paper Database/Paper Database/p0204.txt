Relational Knowledge Distillation
Wonpyo Park*
Dongju Kim
Microsoft Research
 
Knowledge distillation aims at transferring knowledge
acquired in one model (a teacher) to another model (a student) that is typically smaller. Previous approaches can be
expressed as a form of training the student to mimic output
activations of individual data examples represented by the
teacher. We introduce a novel approach, dubbed relational
knowledge distillation (RKD), that transfers mutual relations of data examples instead. For concrete realizations
of RKD, we propose distance-wise and angle-wise distillation losses that penalize structural differences in relations.
Experiments conducted on different tasks show that the proposed method improves educated student models with a signiÔ¨Åcant margin. In particular for metric learning, it allows
students to outperform their teachers‚Äô performance, achieving the state of the arts on standard benchmark datasets.
1. Introduction
Recent advances in computer vision and artiÔ¨Åcial intelligence have largely been driven by deep neural networks
with many layers, and thus current state-of-the-art models
typically require a high cost of computation and memory
in inference. One promising direction for mitigating this
computational burden is to transfer knowledge in the cumbersome model (a teacher) into a small model (a student).
To this end, there exist two main questions: (1) ‚Äòwhat constitutes the knowledge in a learned model?‚Äô and (2) ‚Äòhow
to transfer the knowledge into another model?‚Äô. Knowledge
distillation (or transfer) (KD) methods assume the
knowledge as a learned mapping from inputs to outputs, and
transfer the knowledge by training the student model with
the teacher‚Äôs outputs (of the last or a hidden layer) as targets.
Recently, KD has turned out to be very effective not only in
training a student model but also in improving a teacher model itself by self-distillation .
In this work, we revisit KD from a perspective of the lin-
*The work was done when Wonpyo Park was an intern at MSR.
Figure 1: Relational Knowledge Distillation. While conventional KD transfers individual outputs from a teacher
model (fT ) to a student model (fS) point-wise, our approach transfers relations of the outputs structure-wise. It
can be viewed as a generalization of conventional KD.
guistic structuralism , which focuses on structural relations in a semiological system. Saussure‚Äôs concept of the
relational identity of signs is at the heart of structuralist theory; ‚ÄúIn a language, as in every other semiological system,
what distinguishes a sign is what constitutes it‚Äù . In this
perspective, the meaning of a sign depends on its relations
with other signs within the system; a sign has no absolute
meaning independent of the context.
The central tenet of our work is that what constitutes the
knowledge is better presented by relations of the learned
representations than individuals of those; an individual data
example, e.g., an image, obtains a meaning in relation to
or in contrast with other data examples in a system of representation, and thus primary information lies in the structure in the data embedding space. On this basis, we introduce a novel approach to KD, dubbed Relational Knowledge Distillation (RKD), that transfers structural relations
of outputs rather than individual outputs themselves (Figure 1). For its concrete realizations, we propose two RKD
losses: distance-wise (second-order) and angle-wise (third-
 
order) distillation losses. RKD can be viewed as a generalization of conventional KD, and also be combined with
other methods to boost the performance due to its complementarity with conventional KD. In experiments on metric
learning, image classiÔ¨Åcation, and few-shot learning, our
approach signiÔ¨Åcantly improves the performance of student
models. Extensive experiments on the three different tasks
show that knowledge lives in the relations indeed, and RKD
is effective in transferring the knowledge.
2. Related Work
There has been a long line of research and development on transferring knowledge from one model to another. Breiman and Shang Ô¨Årst proposed to learn singletree models that approximate the performance of multipletree models and provide better interpretability. Similar approaches for neural networks have been emerged in the
work of Bucilua et al. , Ba and Caruana , and Hinton et al. , mainly for the purpose of model compression. Bucilua et al. compress an ensemble of neural networks into a single neural network. Ba and Caruana 
increase the accuracy of a shallow neural network by training it to mimic a deep neural network with penalizing the
difference of logits between the two networks. Hinton et
al. revive this idea under the name of KD that trains a
student model with the objective of matching the softmax
distribution of a teacher model. Recently, many subsequent
papers have proposed different approaches to KD. Romero
et al. distill a teacher using additional linear projection
layers to train a relatively narrower students. Instead of imitating output activations of the teacher, Zagoruyko and Komodakis and Huang and Wang transfer an attention map of a teacher network into a student, and Tarvainen
and Valpola introduce a similar approach using mean
weights. Sau et al. propose a noise-based regularizer
for KD while Lopes et al. introduce data-free KD that
utilizes metadata of a teacher model. Xu et al. propose
a conditional adversarial network to learn a loss function
for KD. Crowley et al. compress a model by grouping
convolution channels of the model and training it with an
attention transfer. Polino et al. and Mishra and Marr
 combine KD with network quantization, which aims to
reduce bit precision of weights and activations.
A few recent papers have shown that distilling
a teacher model into a student model of identical architecture, i.e., self-distillation, can improve the student over the
teacher. Furlanello et al. and Bagherinezhad et al. 
demonstrate it by training the student using softmax outputs of the teacher as ground truth over generations. Yim
et al. transfers output activations using Gramian matrices and then Ô¨Åne-tune the student. We also demonstrate that
RKD strongly beneÔ¨Åts from self-distillation.
KD has also been investigated beyond supervised learning. Lopez-Paz et al. unify two frameworks 
and extend it to unsupervised, semi-supervised, and multitask learning scenarios. Radosavovic et al. generate
multiple predictions from an example by applying multiple
data transformations on it, then use an ensemble of the predictions as annotations for omni-supervised learning.
With growing interests in KD, task-speciÔ¨Åc KD methods have been proposed for object detection , face
model compression , and image retrieval and Re-ID .
Notably, the work of Chen et al. proposes a KD technique for metric learning that transfers similarities between
images using a rank loss. In the sense that they transfer
relational information of ranks, it has some similarity with
ours. Their work, however, is only limited to metric learning whereas we introduce a general framework for RKD and
demonstrate its applicability to various tasks. Furthermore,
our experiments on metric learning show that the proposed
method outperforms with a signiÔ¨Åcant margin.
3. Our Approach
In this section we Ô¨Årst revisit conventional KD and introduce a general form of RKD. Then, two simple yet effective
distillation losses will be proposed as instances of RKD.
Notation. Given a teacher model T and a student model
S, we let fT and fS be functions of the teacher and the
student, respectively. Typically the models are deep neural
networks and in principle the function f can be deÔ¨Åned using output of any layer of the network (e.g., a hidden or
softmax layer). We denote by X N a set of N-tuples of
distinct data examples, e.g., X 2 = {(xi, xj)|i Ã∏= j} and
X 3 = {(xi, xj, xk)|i Ã∏= j Ã∏= k}.
3.1. Conventional knowledge distillation
In general, conventional KD methods can commonly be expressed as minimizing the
objective function:
 fT (xi), fS(xi)
where l is a loss function that penalizes the difference between the teacher and the student.
For example, the popular work of Hinton et al. uses
pre-softmax outputs for fT and fS, and puts softmax (with
temperature œÑ) and Kullback-Leibler divergence for l:
The work of Romero et al. propagates knowledge of
hidden activations by setting fT and fS to be outputs of
hidden layers, and l to be squared Euclidean distance. As
Individual Knowledge Distillation
Relational Knowledge Distillation
Relational Information
Model‚Äôs Representation
Model‚Äôs Representation
ùúì(ùë°1, ‚Ä¶ , ùë°ùëõ)
ùúì(ùë†1, ‚Ä¶ , ùë†ùëõ)
Figure 2: Individual knowledge distillation (IKD) vs. relational knowledge distillation (RKD). While conventional KD (IKD)
transfers individual outputs of the teacher directly to the student, RKD extracts relational information using a relational
potential function œà(¬∑), and transfers the information from the teacher to the student.
the hidden layer output of the student usually has a smaller
dimension than that of the teacher, a linear mapping Œ≤ is
introduced to bridge the different dimensions:
fT (xi) ‚àíŒ≤
Likewise, many other methods 
can also be formulated as a form of Eq. (1). Essentially,
conventional KD transfers individual outputs of the teacher
to the student. We thus call this category of KD methods as
Individual KD (IKD).
3.2. Relational knowledge distillation
RKD aims at transferring structural knowledge using
mutual relations of data examples in the teacher‚Äôs output
presentation. Unlike conventional approaches, it computes
a relational potential œà for each n-tuple of data examples
and transfers information through the potential from the
teacher to the student.
For notational simplicity, let us deÔ¨Åne ti = fT (xi) and
si = fS(xi). The objective for RKD is expressed as
(x1,..,xn)‚ààX N
 œà(t1, .., tn), œà(s1, .., sn)
where (x1, x2, ..., xn) is a n-tuple drawn from X N, œà is a
relational potential function that measures a relational energy of the given n-tuple, and l is a loss that penalizes difference between the teacher and the student. RKD trains the
student model to form the same relational structure with that
of the teacher in terms of the relational potential function
used. Thanks to the potential, it is able to transfer knowledge of high-order properties, which is invariant to lowerorder properties, even regardless of difference in output dimensions between the teacher and the student. RKD can be
viewed as a generalization of IKD in the sense that Eq. (4)
above reduces to Eq. (1) when the relation is unary (N = 1)
and the potential function œà is identity. Figure 2 illustrates
comparison between IKD and RKD.
As expected, the relational potential function œà plays
a crucial role in RKD; the effectiveness and efÔ¨Åciency of
RKD relies on the choice of the potential function. For example, a higher-order potential may be powerful in capturing a higher-level structure but be more expensive in computation. In this work, we propose two simple yet effective potential functions and corresponding losses for RKD,
which exploit pairwise and ternary relations of examples,
respectively: distance-wise and angle-wise losses.
Distance-wise distillation loss
Given a pair of training examples, distance-wise potential
function œàD measures the Euclidean distance between the
two examples in the output representation space:
œàD(ti, tj) = 1
¬µ ‚à•ti ‚àítj‚à•2 ,
where ¬µ is a normalization factor for distance. To focus
on relative distances among other pairs, we set ¬µ to be the
average distance between pairs from X 2 in the mini-batch:
(xi,xj)‚ààX 2
‚à•ti ‚àítj‚à•2 .
Since distillation attempts to match the distance-wise potentials between the teacher and the student, this mini-batch
distance normalization is useful particularly when there is
a signiÔ¨Åcant difference in scales between teacher distances
‚à•ti ‚àítj‚à•2 and student distances ‚à•si ‚àísj‚à•2, e.g., due to the
difference in output dimensions. In our experiments, we
observed that the normalization provides more stable and
faster convergence in training.
Using the distance-wise potentials measured in both the
teacher and the student, a distance-wise distillation loss is
(xi,xj)‚ààX 2
 œàD(ti, tj), œàD(si, sj)
where lŒ¥ is Huber loss, which is deÔ¨Åned as
lŒ¥(x, y) =
for |x ‚àíy| ‚â§1,
otherwise.
The distance-wise distillation loss transfers the relationship of examples by penalizing distance differences
between their output representation spaces.
Unlike conventional KD, it does not force the student to match the
teacher‚Äôs output directly, but encourages the student to focus on distance structures of the outputs.
Angle-wise distillation loss
Given a triplet of examples, an angle-wise relational potential measures the angle formed by the three examples in the
output representation space:
œàA(ti, tj, tk) = cos ‚à†titjtk = ‚ü®eij, ekj‚ü©
Using the angle-wise potentials measured in both the
teacher and the student, an angle-wise distillation loss is de-
(xi,xj,xk)‚ààX 3
 œàA(ti, tj, tk), œàA(si, sj, sk)
where lŒ¥ is the Huber loss.
The angle-wise distillation
loss transfers the relationship of training example embeddings by penalizing angular differences.
Since an angle
is a higher-order property than a distance, it may be able
to transfer relational information more effectively, giving
more Ô¨Çexibility to the student in training. In our experiments, we observed that the angle-wise loss often allows
for faster convergence and better performance.
Training with RKD
During training, multiple distillation loss functions, including the proposed RKD losses, can be used either alone or together with task-speciÔ¨Åc loss functions, e.g., cross-entropy
for classiÔ¨Åcation.
Therefore, the overall objective has a
Ltask + ŒªKD ¬∑ LKD,
where Ltask is a task-speciÔ¨Åc loss for the task at hand, LKD
is a knowledge distillation loss, and ŒªKD is a tunable hyperparameter to balance the loss terms. When multiple KD
losses are used during training, each loss is weighted with
a corresponding balancing factor. In sampling tuples of examples for the proposed distillation losses, we simply use
all possible tuples (i.e., pairs or triplets) from examples in a
given mini-batch.
Distillation target layer
For RKD, the distillation target function f can be chosen as
output of any layer of teacher/student networks in principle.
However, since the distance/angle-wise losses do not transfer individual outputs of the teacher, it is not adequate to
use them alone to where the individual output values themselves are crucial, e.g., softmax layer for classiÔ¨Åcation. In
that case, it needs to be used together with IKD losses or
task-speciÔ¨Åc losses. In most of the other cases, RKD is applicable and effective in our experience. We demonstrate its
efÔ¨Åcacy in the following section.
4. Experiments
We evaluate RKD on three different tasks: metric learning, classiÔ¨Åcation, and few-shot learning. Throughout this
section, we refer to RKD with the distance-wise loss as
RKD-D, that with angle-wise loss as RKD-A, and that with
two losses together as RKD-DA. When the proposed losses
are combined with other losses during training, we assign
respective balancing factors to the loss terms.
We compare RKD with other KD methods, e.g., FitNet 1, Attention and HKD (Hinton‚Äôs KD) .
For metric
learning, we conduct an additional comparison with Dark-
Rank which is a KD method speciÔ¨Åcally designed for
metric learning. For fair comparisons, we tune hyperparameters of the competing methods using grid search.
Our code used for experiments is available online:
 
4.1. Metric learning
We Ô¨Årst evaluate the proposed method on metric learning where relational knowledge between data examples appears to be most relevant among other tasks. Metric learning aims to train an embedding model that projects data examples onto a manifold where two examples are close to
each other if they are semantically similar and otherwise far
apart. As embedding models are commonly evaluated on
image retrieval, we validate our approach using image retrieval benchmarks of CUB-200-2011 , Cars 196 ,
and Stanford Online Products datasets and we follow
the train/test splits suggested in . For the details of the
datasets, we refer the readers to the corresponding papers.
For an evaluation metric, recall@K is used. Once all
test images are embedded using a model, each test image
is used as a query and top K nearest neighbor images are
retrieved from the test set excluding the query. Recall for
the query is considered 1 if the retrieved images contain the
same category with the query. Recall@K are computed by
taking the average recall over the whole test set.
1When FitNet is used, following the original paper, we train the model
with two stages: (1) train the model with FitNet loss, and (2) Ô¨Åne-tune the
model with the task-speciÔ¨Åc loss at hand.
Table 1: Recall@1 on CUB-200-2011 and Cars 196. The teacher is based on ResNet50-512. Model-d refers to a network
with d dimensional embedding. ‚ÄòO‚Äô indicates models trained with ‚Ñì2 normalization, while ‚ÄòX‚Äô represents ones without it.
(a) Results on CUB-200-2011 
(Triplet )
FitNet 
Attention 
DarkRank 
‚Ñì2 normalization
ResNet18-16
46.34 / 48.09
45.59 / 48.60
45.76 / 48.14
ResNet18-32
52.68 / 55.72
53.43 / 55.15
53.58 / 54.88
ResNet18-64
56.92 / 58.27
56.77 / 58.44
57.01 / 58.68
ResNet18-128
58.31 / 60.31
58.41 / 60.92
59.69 / 60.67
ResNet50-512
(b) Results on Cars 196 
(Triplet )
FitNet 
Attention 
DarkRank 
‚Ñì2 normalization
ResNet18-16
63.23 / 66.02
61.39 / 66.25
61.78 / 66.04
ResNet18-32
73.50 / 76.15
73.23 / 75.89
73.12 / 74.80
ResNet18-64
78.64 / 80.57
77.92 / 80.32
78.48 / 80.17
ResNet18-128
79.72 / 81.70
80.54 / 82.27
80.00 / 82.50
ResNet50-512
For training, we follows the protocol of . We obtain training samples by randomly cropping 224 √ó 224 images from resized 256 √ó 256 images and applying random
horizontal Ô¨Çipping for data augmentation. During evaluation, we use a single center crop. All models are trained using Adam optimizer with batch size of 128 for all datasets.
For effective pairing, we follow batch construction from
FaceNet , and sample 5 positive images per category
in a mini-batch.
For a teacher model, ResNet50 , which is pre-trained
on ImageNet ILSVRC dataset , is used. We take layers
of the network upto avgpool and append a single fullyconnected layer with embedding size of 512 followed by ‚Ñì2
normalization. For a student model, ResNet18 , which
is also ImageNet-pretrained, is used in a similar manner
but with different embedding sizes. The teacher models are
trained with the triplet loss , which is the most common
and also effective in metric learning.
Triplet . When an anchor xa, positive xp and negative
xn are given, the triplet loss enforces the squared euclidean
distance between anchor and negative to be larger than that
between anchor and positive by margin m:
Ltriplet =
‚à•f(xa) ‚àíf(xp)‚à•2
2 ‚àí‚à•f(xa) ‚àíf(xn)‚à•2
We set the margin m to be 0.2 and use distance-weighted
sampling for triplets. We apply ‚Ñì2 normalization at
the Ô¨Ånal embedding layer such that the embedding vectors
have a unit length, i.e., ‚à•f(x)‚à•=1. Using ‚Ñì2 normalization
is known to stabilize training of the triplet loss by restricting
the range of distances between embedding points to .
Note that ‚Ñì2 normalization for embedding is widely used in
deep metric learning .
RKD. We apply RKD-D and RKD-A on the Ô¨Ånal embedding outputs of the teacher and the student.
Unlike the
triplet loss, the proposed RKD losses are not affected by
range of distance between embedding points, and do not
have sensitive hyperparameters to optimize such as margin m and triplet sampling parameters. To show the robustness of RKD, we compare RKD without ‚Ñì2 normalization to RKD with ‚Ñì2 normalization. For RKD-DA, we
set ŒªRKD-D = 1 and ŒªRKD-A = 2. Note that for metric
learning with RKD losses, we do not use the task loss, i.e.,
the triplet loss, so that the model is thus trained purely by
teacher‚Äôs guidance without original ground-truth labels; using the task loss does not give additional gains in our experiments.
Attention . Following the original paper, we apply the
method on the output of the second, the third, and the fourth
blocks of ResNet. We set ŒªTriplet = 1 and ŒªAttention = 50.
FitNet . Following the original paper, we train a model
in two stages; we Ô¨Årst initialize a model with FitNet loss,
and then Ô¨Åne-tune the model, in our case, with Triplet. We
apply the method on outputs of the second, the third, and
the fourth blocks of ResNet, as well as the Ô¨Ånal embedding.
DarkRank is a KD method for metric learning that
transfers similarity ranks between data examples. Among
two losses proposed in , we use the HardRank loss as it is
computationally efÔ¨Åcient and also comparable to the other
in performance. The DarkRank loss is applied on Ô¨Ånal outputs of the teacher and the student. In training, we use the
same objective with the triplet loss as suggested in the paper. We carefully tune hyperparameters of DarkRank to be
optimal: Œ± = 3, Œ≤ = 3, and ŒªDarkRank = 1, and ŒªTriplet = 1;
we conduct a grid search on Œ± (1 to 3), Œ≤ (2 to 4), ŒªDarkRank
(1 to 4). In our experiment, our hyperparameters give better
results than those used in .
Distillation to smaller networks
Table 1 shows image retrieval performance of student models with different embedding dimensions on CUB-200-2011
 and Cars 196 . RKD signiÔ¨Åcantly improves the
performance of student networks compared to the baseline
model, which is directly trained with Triplet, also outperforms DarkRank by a large margin. Recall@1 of Triplet
decreases dramatically with smaller embedding dimensions
while that of RKD is less affected by embedding dimensions; the relative gain of recall@1 by RKD-DA increases
from 12.5, 13.8, 23.0, to 27.7 on CUB-200-2011, and from
20.0, 24.2, 33.5 to 45.5 on Cars 196. The results also show
that RKD beneÔ¨Åts from training without ‚Ñì2 normalization
by exploiting a larger embedding space. Note that the absence of ‚Ñì2 normalization has degraded all the other methods in our experiments.
Surprisingly, by RKD on Cars
196, students with the smaller backbone and less embedding dimension even outperform their teacher, e.g., 77.17 of
ResNet50-512 teacher vs. 82.50 of ResNet18-128 student.
Self-distillation
As we observe that RKD is able to improve smaller student
models over its teacher, we now conduct self-distillation experiments where the student architecture is identical to the
teacher architecture. Here, we do not apply ‚Ñì2 normalization on students to beneÔ¨Åt from the effect as we observe
in the previous experiment. The students are trained with
RKD-DA over generations by using the student from the
previous generation as a new teacher. Table 2 shows the
result of self-distillation where ‚ÄòCUB‚Äô, ‚ÄòCars‚Äô, and ‚ÄòSOP‚Äô
refer to CUB-200-2011 , Cars 196 , and Stanford
Online Products , respectively. All models consistently
outperform initial teacher models, which are trained with
the triplet loss. In particular, student models of CUB-200-
2011 and Cars 196 outperform initial teachers with a significant gain. However, the performances do not improve from
the second generation in our experiments.
Comparison with state-of-the art methods
We compare the result of RKD with state-of-the art methods for metric learning.
Most of recent methods adopt
GoogLeNet as a backbone while the work of uses
a variant of ResNet50 with a modiÔ¨Åed number of channels. For fair comparisons, we train student models on both
Table 2: Recall@1 of self-distilled models. Student and
teacher models have the same architecture. The model at
Gen(n) is guided by the model at Gen(n-1).
ResNet50-512-Triplet
ResNet50-512@Gen1
ResNet50-512@Gen2
ResNet50-512@Gen3
GoogLeNet and ResNet50 and set the embedding size as the
same as other methods. RKD-DA is used for training student models. The results are summarized in Table 3 where
our method outperforms all the other methods on CUB-200-
2011 regardless of backbone networks. Among those using
ResNet50, our method achieves the best performance on all
the benchmark datasets. Among those using GoogLeNet,
our method achieves the second-best performance on Car
196 and Stanford Online Products, which is right below
ABE8 . Note that ABE8 requires additional multiple attention modules for each branches whereas ours is
GoogLeNet with a single embedding layer.
Discussion
RKD performing better without ‚Ñì2 normalization. One
beneÔ¨Åt of RKD over Triplet is that the student model is stably trained without ‚Ñì2 normalization. ‚Ñì2 norm forces output points of an embedding model to lie on the surface of
unit-hypersphere, and thus a student model without ‚Ñì2 norm
is able to fully utilize the embedding space. This allows
RKD to better perform as shown in Table 1. Note that Dark-
Rank contains the triplet loss that is well known to be fragile
without ‚Ñì2 norm. For example, ResNet18-128 trained with
DarkRank achieves recall@1 of 52.92 without ‚Ñì2 norm (vs.
77.00 with ‚Ñì2 norm) on Cars 196.
Students excelling teachers. The similar effect has also
been reported in classiÔ¨Åcation . The work of 
explains that the soft output of class distribution from
the teacher may carry additional information, e.g., crosscategory relationships, which cannot be encoded in onehot vectors of ground-truth labels. Continuous target labels of RKD (e.g., distance or angle) may also carry useful
information, which cannot properly be encoded in binary
(positive/negative) ground-truth labels used in conventional
losses, i.e., the triplet loss.
RKD as a training domain adaptation. Both Cars 196 and
CUB-200-2011 datasets are originally designed for Ô¨Ånegrained classiÔ¨Åcation, which is challenging due to severe
intra-class variations and inter-class similarity.
datasets, effective adaptation to speciÔ¨Åc characteristics of
the domain may be crucial; recent methods for Ô¨Åne-grained
classiÔ¨Åcation focus on localizing discriminative parts of
target-domain objects . To measure the degree
Table 3: Recall@K comparison with state of the arts on CUB-200-2011, Car 196, and Stanford Online Products. We divide
methods into two groups according to backbone networks used. A model-d refers to model with d-dimensional embedding.
Boldfaces represent the best performing model for each backbone while underlines denote the best among all the models.
CUB-200-2011 
Cars 196 
Stanford Online Products 
GoogLeNet 
LiftedStruct -128
N-pairs -64
Angular -512
A-BIER -512
ABE8 -512
RKD-DA-128
RKD-DA-512
ResNet50 
Margin -128
RKD-DA-128
CUB-200-2011
Stanford Dog
Pretrained
Figure 3: Recall@1 on the test split of Cars 196, CUB-200-
2011, Stanford Dog and CIFAR-100. Both Triplet (teacher)
and RKD-DA (student) are trained on Cars 196. The left
side of the dashed line shows results on the training domain,
while the right side presents results on other domains.
of adaptation of a model trained with RKD losses, we compare recall@1 on a training data domain with those on different data domains. Figure 3 shows the recall@1 results
on different datasets using a student model trained on Cars
196. The student (RKD) has much lower recall@1 on different domains while the recall@1 of the teacher (Triplet)
remains similarly to pretrained feature (an initial model).
These results reveal an interesting effect of RKD that it
strongly adapts models on the training domain at the cost
of sacriÔ¨Åcing generalization to other domains.
4.2. Image classiÔ¨Åcation
We also validate the proposed method on the task of image classiÔ¨Åcation by comparing RKD with IKD methods,
e.g., HKD , FitNet and Attention . We conduct
experiments on CIFAR-100 and Tiny ImageNet datasets.
CIFAR-100 contains 32 √ó 32 sized images with 100 object
categories, and Tiny ImageNet contains 64 √ó 64 sized images with 200 classes. For both datasets, we apply FitNet
and Attention on the output of the second, the third, and
the fourth blocks of CNN, and set ŒªAttention = 50. HKD is
applied on the Ô¨Ånal classiÔ¨Åcation layer on the teacher and
the student, and we set temperature œÑ of HKD to be 4 and
Table 4: Accuracy (%) on CIFAR-100 and Tiny ImageNet.
CIFAR-100 
Tiny ImageNet 
HKD+RKD-DA
FitNet 
FitNet+RKD-DA
Attention 
Attention+RKD-DA
ŒªHKD to be 16 as in . RKD-D and RKD-A are applied
on the last pooling layer of the teacher and the student, as
they produce the Ô¨Ånal embedding before classiÔ¨Åcation. We
set ŒªRKD-D = 25 and ŒªRKD-A = 50. For all the settings, we
use the cross-entropy loss at the Ô¨Ånal loss in addition. For
both the teacher and the student, we remove fully-connected
layer(s) after the Ô¨Ånal pooling layer and append a single
fully-connected layer as a classiÔ¨Åer.
For CIFAR-100, we randomly crop 32√ó32 images from
zero-padded 40 √ó 40 images, and apply random horizontal Ô¨Çipping for data augmentation. We optimize the model
using SGD with mini-batch size 128, momentum 0.9 and
weight decay 5 √ó 10‚àí4.
We train the network for 200
epochs, and the learning rate starts from 0.1 and is multiplied by 0.2 at 60, 120, 160 epochs. We adopt ResNet50 for
a teacher model, and VGG11 with batch normalization
for a student model.
For Tiny ImageNet, we apply random rotation, color jittering, and horizontal Ô¨Çipping for data augmentation. We
optimize the model using SGD with mini-batch 128 and
momentum 0.9. We train the network for 300 epochs, and
the learning rate starts from 0.1, and is multiplied by 0.2 at
60, 120, 160, 200, 250 epochs. We adopt ResNet101 for a
teacher model and ResNet18 as a student model.
Table 4 shows the results on CIFAR-100 and Tiny Im-
Figure 4: Retrieval results on CUB-200-2011 and Cars 196 datasets. The top eight images are placed from left to right. Green
and red bounding boxes represent positive and negative images, respectively. T denotes the teacher trained with the triplet
loss while S is the student trained with RKD-DA. For these examples, the student gives better results than the teacher.
ageNet. On both datasets, RKD-DA combined with HKD
outperforms all conÔ¨Ågurations. The overall results reveal
that the proposed RKD method is complementary to other
KD methods; the model further improves in most cases
when RKD is combined with another KD method.
4.3. Few-shot learning
Finally, we validate the proposed method on the task
of few-shot learning, which aims to learn a classiÔ¨Åer that
generalizes to new unseen classes with only a few examples for each new class. We conduct experiments on standard benchmarks for few-shot classiÔ¨Åcation, which are Omniglot and miniImageNet . We evaluate RKD using the prototypical networks that learn an embedding
network such that classiÔ¨Åcation is performed based on distance from given examples of new classes. We follow the
data augmentation and training procedure of the work of
Snell et al. and the splits suggested by Vinyals et al.
 . As the prototypical networks build on shallow networks that consist of only 4 convolutional layers, we use
the same architecture for the student model and the teacher,
i.e., self-distillation, rather than using a smaller student network. We apply RKD, FitNet, and Attention on the Ô¨Ånal
embedding output of the teacher and the student. We set
ŒªRKD-D = 50 and ŒªRKD-A = 100. When RKD-D and RKD-
A are combined together, we divide the Ô¨Ånal loss by 2. We
set ŒªAttention = 10. For all the settings, we add the prototypical loss at the Ô¨Ånal loss. As the common evaluation protocol of for few-shot classiÔ¨Åcation, we compute accuracy by averaging over 1000 randomly generated episodes
for Omniglot, and 600 randomly generated episodes for
miniImageNet. The Omniglot results are summarized in Table 5 while the miniImageNet results are reported with 95%
conÔ¨Ådence intervals in Table 6. They show that our method
consistently improves the student over the teacher.
Table 5: Accuracy (%) on Omniglot .
5-Way Acc.
20-Way Acc.
Table 6: Accuracy (%) on miniImageNet .
1-Shot 5-Way
5-Shot 5-Way
49.66 ¬± 0.84
67.07 ¬± 0.67
50.02 ¬± 0.83
68.16 ¬± 0.67
50.38 ¬± 0.81
68.08 ¬± 0.65
34.67 ¬± 0.65
46.21 ¬± 0.70
49.1 ¬± 0.82
66.87 ¬± 0.66
5. Conclusion
We have demonstrated on different tasks and benchmarks that the proposed RKD effectively transfers knowledge using mutual relations of data examples. In particular
for metric learning, RKD enables smaller students to even
outperform their larger teachers. While the distance-wise
and angle-wise distillation losses used in this work turn out
to be simple yet effective, the RKD framework allows us
to explore a variety of task-speciÔ¨Åc RKD losses with highorder potentials beyond the two instances. We believe that
the RKD framework opens a door to a promising area of
effective knowledge transfer with high-order relations.
Acknowledgement:
This work was supported by MSRA
Collaborative Research Program and also by Basic Science Research Program and Next-Generation Information
Computing Development Program through the National
Research Foundation of Korea funded by the Ministry
of Science, ICT (NRF-2017R1E1A1A01077999, NRF-
2017M3C4A7069369).