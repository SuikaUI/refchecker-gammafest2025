Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing, pages 6382–6388,
Hong Kong, China, November 3–7, 2019. c⃝2019 Association for Computational Linguistics
EDA: Easy Data Augmentation Techniques for Boosting Performance on
Text Classiﬁcation Tasks
Jason Wei1,2
1Protago Labs Research, Tysons Corner, Virginia, USA
2Department of Computer Science, Dartmouth College
3Department of Mathematics and Statistics, Georgetown University
 
 
We present EDA: easy data augmentation
techniques for boosting performance on text
classiﬁcation tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and
random deletion.
On ﬁve text classiﬁcation
tasks, we show that EDA improves performance for both convolutional and recurrent
neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across ﬁve datasets, training with EDA
while using only 50% of the available training set achieved the same accuracy as normal
training with all available data. We also performed extensive ablation studies and suggest
parameters for practical use.
Introduction
Text classiﬁcation is a fundamental task in natural language processing (NLP). Machine learning
and deep learning have achieved high accuracy on
tasks ranging from sentiment analysis to topic classiﬁcation , but high performance often depends on the
size and quality of training data, which is often tedious to collect. Automatic data augmentation is
commonly used in computer vision and speech 
and can help train more robust models, particularly when using smaller datasets. However, because it is challenging to come up with generalized
rules for language transformation, universal data
augmentation techniques in NLP have not been
thoroughly explored.
Previous work has proposed some techniques
for data augmentation in NLP. One popular study
generated new data by translating sentences into
French and back into English .
Other work has used data noising as smoothing
A sad, superior human comedy played out
on the back roads of life.
A lamentable, superior human comedy
played out on the backward road of life.
A sad, superior human comedy played out
on funniness the back roads of life.
A sad, superior human comedy played out
on roads back the of life.
A sad, superior human out on the roads of
Table 1: Sentences generated using EDA. SR: synonym
replacement. RI: random insertion. RS: random swap.
RD: random deletion.
 and predictive language models
for synonym replacement . Although these techniques are valid, they are not often used in practice because they have a high cost
of implementation relative to performance gain.
In this paper, we present a simple set of universal data augmentation techniques for NLP called
EDA (easy data augmentation). To the best of our
knowledge, we are the ﬁrst to comprehensively
explore text editing techniques for data augmentation. We systematically evaluate EDA on ﬁve
benchmark classiﬁcation tasks, showing that EDA
provides substantial improvements on all ﬁve tasks
and is particularly helpful for smaller datasets.
Code is publicly available at 
com/jasonwei20/eda_nlp.
Frustrated by the measly performance of text classiﬁers trained on small datasets, we tested a number of augmentation operations loosely inspired by
those used in computer vision and found that they
helped train more robust models. Here, we present
the full details of EDA. For a given sentence in the
training set, we randomly choose and perform one
of the following operations:
1. Synonym
Replacement
choose n words from the sentence that are not
stop words. Replace each of these words with
one of its synonyms chosen at random.
2. Random Insertion (RI): Find a random synonym of a random word in the sentence that is
not a stop word. Insert that synonym into a random position in the sentence. Do this n times.
3. Random Swap (RS): Randomly choose two
words in the sentence and swap their positions.
Do this n times.
4. Random Deletion (RD): Randomly remove
each word in the sentence with probability p.
Since long sentences have more words than short
ones, they can absorb more noise while maintaining their original class label. To compensate, we
vary the number of words changed, n, for SR, RI,
and RS based on the sentence length l with the formula n=αl, where α is a parameter that indicates
the percent of the words in a sentence are changed
(we use p=α for RD). Furthermore, for each original sentence, we generate naug augmented sentences.
Examples of augmented sentences are
shown in Table 1. We note that synonym replacement has been used previously ,
but to our knowledge, random insertions, swaps,
and deletions have not been extensively studied.
Experimental Setup
We choose ﬁve benchmark text classiﬁcation tasks
and two network architectures to evaluate EDA.
Benchmark Datasets
We conduct experiments on ﬁve benchmark text
classiﬁcation tasks: (1) SST-2: Stanford Sentiment Treebank , (2) CR: customer reviews ,
(3) SUBJ: subjectivity/objectivity dataset , (4) TREC: question type dataset
 , and (5) PC: Pro-Con dataset
 . Summary statistics are shown in Table 5 in Supplemental Materials. Furthermore, we hypothesize that EDA is
more helpful for smaller datasets, so we delegate
the following sized datasets by selecting a random
subset of the full training set with Ntrain={500,
2,000, 5,000, all available data}.
Text Classiﬁcation Models
We run experiments for two popular models in
text classiﬁcation. (1) Recurrent neural networks
(RNNs) are suitable for sequential data. We use a
LSTM-RNN . (2) Convolutional
neural networks (CNNs) have also achieved high
performance for text classiﬁcation. We implement
them as described in . Details are in
Section 9.1 in Supplementary Materials.
In this section, we test EDA on ﬁve NLP tasks with
CNNs and RNNs. For all experiments, we average
results from ﬁve different random seeds.
EDA Makes Gains
We run both CNN and RNN models with and
without EDA across all ﬁve datasets for varying
training set sizes. Average performances (%) are
shown in Table 2.
Of note, average improvement was 0.8% for full datasets and 3.0% for
Ntrain=500.
Training Set Size
Table 2: Average performances (%) across ﬁve text
classiﬁcation tasks for models with and without EDA
on different training set sizes.
Training Set Sizing
Overﬁtting tends to be more severe when training
on smaller datasets. By conducting experiments
using a restricted fraction of the available training data, we show that EDA has more signiﬁcant
improvements for smaller training sets. We run
both normal training and EDA training for the following training set fractions (%): {1, 5, 10, 20,
30, 40, 50, 60, 70, 80, 90, 100}.
Figure 1(a)-
(e) shows performance with and without EDA for
each dataset, and 1(f) shows the averaged performance across all datasets. The best average accuracy without augmentation, 88.3%, was achieved
using 100% of the training data. Models trained
using EDA surpassed this number by achieving an
Percent of Dataset (%)
SST-2 (N=7,447)
Percent of Dataset (%)
CR (N=4,082)
Percent of Dataset (%)
SUBJ (N=9,000)
Percent of Dataset (%)
TREC (N=5,452)
Percent of Dataset (%)
PC (N=39,418)
Percent of Dataset (%)
Average Accuracy
All Datasets
Figure 1: Performance on benchmark text classiﬁcation tasks with and without EDA, for various dataset sizes
used for training. For reference, the dotted grey line indicates best performances from Kim for SST-2, CR,
SUBJ, and TREC, and Ganapathibhotla for PC.
average accuracy of 88.6% while only using 50%
of the available training data.
Does EDA conserve true labels?
In data augmentation, input data is altered while
class labels are maintained. If sentences are signiﬁcantly changed, however, then original class
labels may no longer be valid. We take a visualization approach to examine whether EDA operations signiﬁcantly change the meanings of augmented sentences.
First, we train an RNN on
the pro-con classiﬁcation task (PC) without augmentation. Then, we apply EDA to the test set
by generating nine augmented sentences per original sentence. These are fed into the RNN along
with the original sentences, and we extract the outputs from the last dense layer. We apply t-SNE
 to these vectors and plot
their 2-D representations (Figure 2).
that the resulting latent space representations for
augmented sentences closely surrounded those of
the original sentences, which suggests that for the
most part, sentences augmented with EDA conserved the labels of their original sentences.
Ablation Study: EDA Decomposed
So far, we have seen encouraging empirical results. In this section, we perform an ablation study
Pro (original)
Con (original)
Figure 2: Latent space visualization of original and
augmented sentences in the Pro-Con dataset.
Augmented sentences (small triangles and circles) closely
surround original sentences (big triangles and circles)
of the same color, suggesting that augmented sentences
maintianed their true class labels.
to explore the effects of each operation in EDA.
Synonym replacement has been previously used
 , but the other three EDA operations have not yet been explored. One could hypothesize that the bulk of EDA’s performance gain
is from synonym replacement, so we isolate each
of the EDA operations to determine their individual ability to boost performance. For all four
operations, we ran models using a single oper-
0.1 0.2 0.3 0.4 0.5
α parameter
Performance Gain (%)
0.1 0.2 0.3 0.4 0.5
α parameter
0.1 0.2 0.3 0.4 0.5
α parameter
0.1 0.2 0.3 0.4 0.5
α parameter
Figure 3: Average performance gain of EDA operations over ﬁve text classiﬁcation tasks for different training
set sizes. The α parameter roughly means “percent of words in sentence changed by each augmentation.” SR:
synonym replacement. RI: random insertion. RS: random swap. RD: random deletion.
ation while varying the augmentation parameter
α={0.05, 0.1, 0.2, 0.3, 0.4, 0.5} (Figure 3).
It turns out that all four EDA operations contribute to performance gain. For SR, improvement
was good for small α, but high α hurt performance, likely because replacing too many words
in a sentence changed the identity of the sentence.
For RI, performance gains were more stable for
different α values, possibly because the original
words in the sentence and their relative order were
maintained in this operation. RS yielded high performance gains at α≤0.2, but declined at α≥0.3
since performing too many swaps is equivalent to
shufﬂing the entire order of the sentence. RD had
the highest gains for low α but severely hurt performance at high α, as sentences are likely unintelligible if up to half the words are removed.
Improvements were more substantial on smaller
datasets for all operations, and α=0.1 appeared to
be a “sweet spot” across the board.
How much augmentation?
The natural next step is to determine how the number of generated augmented sentences per original
sentence, naug, affects performance. In Figure 4,
we show average performances over all datasets
for naug={1, 2, 4, 8, 16, 32}.
For smaller train-
Performance Gain (%)
Figure 4: Average performance gain of EDA across ﬁve
text classiﬁcation tasks for various training set sizes.
naug is the number of generated augmented sentences
per original sentence.
ing sets, overﬁtting was more likely, so generating many augmented sentences yielded large performance boosts. For larger training sets, adding
more than four augmented sentences per original
sentence was unhelpful since models tend to generalize properly when large quantities of real data
are available. Based on these results, we recommend usage parameters in Table 3.
Table 3: Recommended usage parameters.
Comparison with Related Work
Related work is creative but often complex. Backtranslation , translational
data augmentation , and noising have shown improvements in
BLEU measure for machine translation. For other
tasks, previous approaches include task-speciﬁc
heuristics and back-translation
 . Regarding synonym replacement (SR), one study showed
a 1.4% F1-score boost for tweet classiﬁcation by
ﬁnding synonyms with k-nearest neighbors using word embeddings .
Another study found no improvement in temporal analysis when replacing headwords with synonyms , and mixed results were reported for using SR in character-level
text classiﬁcation ; however,
neither work conducted extensive ablation studies.
Most studies explore data augmentation as a
complementary result for translation or in a taskspeciﬁc context, so it is hard to directly compare
EDA with previous literature. But there are two
studies similar to ours that evaluate augmentation
techniques on multiple datasets. Hu proposed a generative model that combines a variational auto-encoder (VAE) and attribute discriminator to generate fake data, demonstrating a 3%
gain in accuracy on two datasets.
 showed that replacing words with other
words that were predicted from the sentence context using a bi-directional language model yielded
a 0.5% gain on ﬁve datasets. However, training
a variational auto-encoder or bidirectional LSTM
language model is a lot of work. EDA yields results on the same order of magnitude but is much
easier to use because it does not require training a
language model and does not use external datasets.
In Table 4, we show EDA’s ease of use compared
with other techniques.
Technique (#datasets)
Trans. data aug.1 (1)
Back-translation2 (1)
VAE + discrim.3 (2)
Noising4 (1)
Back-translation5 (2)
LM + SR6 (2)
Contextual aug.7 (5)
SR - kNN8 (1)
Table 4: Related work in data augmentation. #datasets:
number of datasets used for evaluation. Gain: reported
performance gain on all evaluation datasets. LM: requires training a language model or deep learning. Ex
Dat: requires an external dataset.9
Discussion and Limitations
Our paper aimed to address the lack of standardized data augmentation in NLP (compared to vision) by introducing a set of simple operations that
might serve as a baseline for future investigation.
With the rate that NLP research has progressed in
1 for translation
2 for comprehension
3 for text classiﬁcation
4 for translation
5 for translation
6 for temporal analysis
7 for text classiﬁcation
8 for tweet classiﬁcation
9EDA does use a synonym dictionary, WordNet, but the
cost of downloading it is far less than training a model on an
external dataset, so we don’t count it as an “external dataset.”
recent years, we suspect that researchers will soon
ﬁnd higher-performing augmentation techniques
that will also be easy to use.
Notably, much of the recent work in NLP focuses on making neural models larger or more
complex. Our work, however, takes the opposite
approach. We introduce simple operations, the result of asking the fundamental question, how can
we generate sentences for augmentation without
changing their true labels? We do not expect EDA
to be the go-to augmentation method for NLP, either now or in the future. Rather, we hope that our
line of thought might inspire new approaches for
universal or task-speciﬁc data augmentation.
Now, let’s note many of EDA’s limitations.
Foremost, performance gain can be marginal when
data is sufﬁcient; for our ﬁve classiﬁcation tasks,
the average performance gain for was less than 1%
when training with full datasets. And while performance gains seem clear for small datasets, EDA
might not yield substantial improvements when
using pre-trained models. One study found that
EDA’s improvement was negligible when using
ULMFit , and we expect similar
results for ELMo and BERT
 . Finally, although we evaluate on ﬁve benchmark datasets, other studies on
data augmentation in NLP use different models
and datasets, and so fair comparison with related
work is highly non-trivial.
Conclusions
We have shown that simple data augmentation operations can boost performance on text classiﬁcation tasks. Although improvement is at times
marginal, EDA substantially boosts performance
and reduces overﬁtting when training on smaller
datasets. Continued work on this topic could explore the theoretical underpinning of the EDA operations. We hope that EDA’s simplicity makes a
compelling case for further thought.
Acknowledgements
We thank Chengyu Huang, Fei Xing, and Yifang
Wei for help with study design and paper revisions, and Chunxiao Zhou for insightful feedback.
Jason Wei thanks Eugene Santos for inspiration.