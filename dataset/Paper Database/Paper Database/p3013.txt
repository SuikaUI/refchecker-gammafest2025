DADA: Deep Adversarial Data Augmentation for
Extremely Low Data Regime Classiﬁcation
Xiaofeng Zhang
University of Science and Technology of China
Hefei, Anhui, China
 
Zhangyang Wang
Texas A&M University
College Station, TA, USA
 
University of Science and Technology of China
Hefei, Anhui, China
 
Sun Yat-Sen University
Guangzhou, Guangdong, China
 
Deep learning has revolutionized the performance of classiﬁcation, but meanwhile
demands sufﬁcient labeled data for training. Given insufﬁcient data, while many
techniques have been developed to help combat overﬁtting, the challenge remains
if one tries to train deep networks, especially in the ill-posed extremely low data
regimes: only a small set of labeled data are available, and nothing – including
unlabeled data – else. Such regimes arise from practical situations where not
only data labeling but also data collection itself is expensive. We propose a deep
adversarial data augmentation (DADA) technique to address the problem, in which
we elaborately formulate data augmentation as a problem of training a classconditional and supervised generative adversarial network (GAN). Speciﬁcally, a
new discriminator loss is proposed to ﬁt the goal of data augmentation, through
which both real and augmented samples are enforced to contribute to and be
consistent in ﬁnding the decision boundaries. Tailored training techniques are
developed accordingly. To quantitatively validate its effectiveness, we ﬁrst perform
extensive simulations to show that DADA substantially outperforms both traditional
data augmentation and a few GAN-based options. We then extend experiments to
three real-world small labeled datasets where existing data augmentation and/or
transfer learning strategies are either less effective or infeasible. All results endorse
the superior capability of DADA in enhancing the generalization ability of deep
networks trained in practical extremely low data regimes. Source code is available
at 
Introduction
The performance of classiﬁcation and recognition has been tremendously revolutionized by the
prosperity of deep learning . Deep learning-based classiﬁers can reach unprecedented accuracy
given that there are sufﬁcient labeled data for training. Meanwhile, such a blessing can turn into a
curse: in many realistic settings where either massively annotating labels is a labor-intensive task, or
only limited datasets are available, a deep learning model will easily overﬁt and generalizes poorly.
Many techniques have been developed to help combat overﬁtting with insufﬁcient data, ranging from
classical data augmentation , to dropout and other structural regularizations , to pre-training
 , transfer learning and semi-supervised learning . However in low data regimes, even these
techniques will fall short, and the resulting models usually cannot capture all possible input data
 
 
variances and distinguish them from nuisance variances. The high-variance gradients also cause
popular training algorithms, e.g., stochastic gradient descent, to be extremely unstable.
In this paper, we place ourself in front of an even more ill-posed and challenging problem: how to
learn a deep network classiﬁer, where the labeled training set is high-dimensional but small in sample
size? Most existing methods in the low data regimes deal with the scarcity of labeled data; however,
they often assume the help from abundant unlabeled samples in the same set, or (labeled or unlabeled)
samples from other similar datasets, enabling various semi-supervised or transfer learning solutions.
Different from them, we investigate a less-explored and much more daunting task setting of extremely
low data regimes: besides the given small amount of labeled samples, neither unlabeled data from
the same distribution nor data from similar distributions are assumed to be available throughout
training. In other words, we aim to train a deep network classiﬁer from scratch, using only the
given small number of labeled data and nothing else. Our only hope lies in maximizing the usage
of the given small training set, by ﬁnding nontrivial and semantically meaningful re-composition
of sample information that helps us characterize the underlying distribution. The extremely low
data regimes for classiﬁcation are ubiquitous in and have blocked many practical or scientiﬁc ﬁelds,
where not only data labeling, but data collection itself is also expensive to scale up. For example,
image subjects from military and medical imagery are usually expensive to collect, and often admit
quite different distributions from easily accessible natural images. While we mostly focus on image
classiﬁcation/visual recognition in this paper, our methodology can be readily extended to classifying
non-image data in extremely low data regimes; we will intentionally present one such example of
electroencephalographic (EEG) signal classiﬁcation in Section 5. To resolve the challenges, we have
made multi-fold technical contributions in this paper:
• For learning deep classiﬁers in extremely low data regimes, we focus on boosting the
effectiveness of data augmentation, and introduce learning-based data augmentation, that
can be optimized for classifying general data without relying on any domain-speciﬁc prior or
unlabeled data. The data augmentation module and the classiﬁer are formulated and learned
together as a fully-supervised generative adversarial network (GAN). We call the proposed
framework Deep Adversarial Data Augmentation (DADA).
• We propose a new loss function for the GAN discriminator, that not only learns to classify
real images, but also enforces ﬁne-grained classiﬁcation over multiple “fake classes”. That
is referred to as the 2k loss, in contrast to the k+1 loss used by several existing GANs (to
be compared in the context later). The novel loss function is motivated by our need of
data augmentation: the generated augmented (“fake”) samples need to be discriminative
among classes too, and the decision boundaries learned on augmented samples shall align
consistently with those learned on real samples. We show in experiments that the 2k loss is
critical to boost the overall classiﬁcation performance.
• We conduct extensive simulations on CIFAR-10, CIFAR-100, and SVNH, to train deep
classiﬁers in the extremely low data regimes, demonstrating signiﬁcant performance improvements through DADA compared to using traditional data augmentation. To further
validate the practical effectiveness of DADA, we train deep classiﬁers on three real-world
small datasets: the Karolinska Directed Emotional Faces (KDEF) dataset for the facial
expression recognition task, a Brain-Computer Interface (BCI) Competition dataset for the
EEG brain signal classiﬁcation task, and the Curated Breast Imaging Subset of the Digital
Database for Screening Mammography (CBIS-DDSM) dataset for the tumor classiﬁcation
task. For all of them, DADA leads to highly competitive generalization performance.
Related Work
Generative Adversarial Networks
Generative Adversarial Networks (GANs) have gathered a signiﬁcant amount of attention due to
their ability to learn generative models of multiple natural image datasets. The original GAN model
and its many successors are unsupervised: their discriminators have a single probabilistic realness
output attempting to decipher whether an input image is real or generated (a.k.a. fake). Conditional
GAN generates data conditioned on class labels via label embeddings in both discriminator
and generator. Conditioning generated samples on labels sheds light the option of semi-supervised
classiﬁcation using GANs. In , the semi-supervised GAN has the discriminator network to output
class labels, leading to a k + 1 class loss function consisting of k class labels if the sample is decided
to be real, and a single extra class if the sample is decided to be fake. Such a structured k + 1 loss
has been re-emphasized in to provide more informed training that leads to generated samples
capturing class-speciﬁc variances better. Even with the proven success of GANs for producing
realistic-looking images, tailoring GANs for classiﬁcation is not as straightforward as it looks like
 . The ﬁrst question would naturally be: is GAN really creating semantically novel compositions,
or simply memorizing its training samples (or adding trivial nuisances)? Fortunately, there seems
to be empirical evidence that GANs perform at least some non-trivial modeling of the unknown
distribution and are able to interpolate in the latent space . However, previous examinations
also reported that the diversity of generated samples is far poorer than the true training dataset. In
 , the authors tried several unconditional GANs to synthesize samples, on which they trained
image classiﬁers. They reported that the accuracies achieved by such classiﬁers were comparable
only to the accuracy of a classiﬁer trained on a 100 (or more) subsampled version of the true dataset,
and the gap cannot be reduced by drawing more samples from the GANs. Despite many insights
revealed, the authors did not consider low data regimes. More importantly, they focused on a different
goal on using classiﬁcation performance to measure the diversity of generated data. As a result,
they neither considered class-conditional GANs, nor customized any GAN structure for the goal
of classiﬁcation-driven data augmentation. Besides, GANs also have hardly been focused towards
non-image subjects.
Deep Learning on Small Samples
Training a deep learning classiﬁer on small datasets is a topic of wide interests in the ﬁelds of ﬁnegrained visual recognition , few-shot learning , and life-long learning in new environments
 . Hereby we review and categorize several mainstream approaches.
Dimensionality Reduction and Feature Selection. A traditional solution to overﬁtting caused by
high dimension, low sample size data is to perform dimensionality reduction or feature selection as
pre-processing, and to train (deep) models on the new feature space. Such pre-processing has become
less popular in deep learning because the latter often emphasizes end-to-end trainable pipelines. A
recent work performed the joint training of greedy feature selection and a deep classiﬁer; but
their model was designed for bioinformatics data (attributed vectors) and it was unclear how a similar
model can be applied to raw images.
Pre-training and Semi-Supervised Learning. Both pre-training and semi-supervised learning focus on
improving classiﬁcation with smalled labeled samples, by utilizing extra data from the same training
distribution but is unlabeled. Greedy pre-training with larger unlabeled data, e.g., via auto-encoders,
could help learn (unsupervised) feature extractors and converge to a better generalizing minimum
 . In practice, pre-training is often accompanied with data augmentation . Semi-supervised
learning also utilizes extra unlabeled data, while unlabeled data contribute to depicting data density
and thus locating decision boundaries within low-density regions; see . However, note
that both pre-training and semi-supervised learning rely heavily on the abundance of unlabeled data:
they are motivated by the same hypothesis that while labeling data is difﬁcult, collecting unlabeled
data remains to be a cheap task. While the hypothesis is valid in many computer vision tasks, it may
not always stand true and differs from our target – extremely low data regimes.
Transfer Learning. Compared to the above two, transfer learning admits a more relaxed setting: using
unlabeled data from a similar or overlapped distribution (a.k.a. source domain), rather than from the
same target distribution as labeled samples (a.k.a. target domain). For standard visual recognition,
common visual patterns like edges are often shared between different natural image datasets. This
makes a knowledge transfer between such datasets promising , even though their semantics are not
strictly tied. Empirical study showed that, the weight transfer from deep networks trained on a
source domain with abundant (labeled) data can boost visual recognition on a target domain where
labeled samples are scarce. It is, however, unclear whether transfer or how much learning will help,
if the source and target domains possess notable discrepancy.
Data Augmentation. Data augmentation is an alternative strategy to bypass the unavailability of
labeled training data, by artiﬁcially synthesizing new labeled samples from existing ones. Traditional
data augmentation techniques rely on a very limited set of known invariances that are easy to invoke,
and adopt ad-hoc, minor perturbations that will not change labels. For instance, in the case of
image classiﬁcation, typical augmentations include image rotation, lighting/color tone modiﬁcations,
rescaling, cropping, or as simple as adding random noise . However, such empirical labelpreserving transformations are often unavailable in non-image domains. A latest work presented
a novel direction to select and compose pre-speciﬁed base data transformations (such as rotations,
shears, central swirls for images) into a more sophisticated “tool chain” for data augmentation, using
generative adversarial training. They achieve highly promising results on both image and text datasets,
but need the aid of unlabeled data in training (the same setting as in ). We experimentally compare
the method and DADA and analyze their more differences in Section 5.3.
Few efforts went beyond encoding priori known invariances to explore more sophisticated, learningbased augmentation strategies. Several semi-supervised GANs, e.g., , could also be viewed
as augmented unlabeled samples from labeled ones. A Bayesian Monte Carlo algorithm for data
augmentation was proposed in , and was evaluated on standard label-rich image classiﬁcation
datasets. The authors of learned class-conditional distributions by a diffeomorphism assumption.
A concurrent preprint explored a Data Augmentation Generative Adversarial Network (DAGAN):
the authors developed a completely different GAN model from the proposed DADA, whose generator
does not depend on the classes and the discriminator is a vanilla real/fake one. Hence, it enables
DAGAN to be applicable to unseen new classes for few-shot learning scenarios, different from our
goal of improving fully-supervised classiﬁcation. As we will see in experiments, deriving a stronger
discriminator is critical in our target task.
We also noticed an interesting benchmark study conducted in to compare among various data
augmentation techniques, including very sophisticated generative models such as CycleGAN .
Somewhat surprisingly, they found traditional ad-hoc augmentation techniques to be still able to
outperform existing learning-based choices. Overall, enhancing small sample classiﬁcation via
learning-based data augmentation remains as an open and under-investigated problem.
Domain-speciﬁc Data Synthesis. A number of works explored the “free” generation of
labeled synthetics examples to assist training. However, they either relied on extra information, e.g.,
3D models of the subject, or were tailored for one special object class such as face or license plates.
The synthesis could also be viewed as a special type of data augmentation that hinges on stronger
forms of priori invariance knowledge.
Training Regularization. A ﬁnal option to ﬁght against small datasets is to exploit variance reduction
techniques for network design and training. Examples include dropout , dropconnect , and
enforcing compact structures on weights or connection patterns (e.g., sparsity) . Those techniques
are for the general purposes of alleviating overﬁtting, and they alone are unlikely to resolve the
challenge of extremely low data regimes.
Technical Approach
Problem Formulation and Solution Overview
Consider a general k-class classiﬁcation problem. Suppose that we have a training set D =
{(x1, y1), (x2, y2), ..., (x|D|, y|D|)}, where xi denotes a sample and yi a corresponding label,
yi ∈{1, 2, ..., k}. Our task is to learn a good classiﬁer C to predict the label ˆyi = C(xi), by
minimizing the empirical risk objective
i=1 L(yi, ˆyi) over D, L being some loss function such
as K-L divergence. As our goal, a good C should generalize well on an unseen test set T . In classical
deep learning-based classiﬁcation settings, D is large enough to ensure that goal. However in our
extremely low data regimes, |D| can be too small to support robust learning of any complicated
decision boundary, causing severe overﬁtting.
Data augmentation approaches seek an augmenter A, to synthesize a new set D′ of augmented
labeled data (¯xi, yi) from (xi, yi), constituting the new augmented training set of size |D| + |D′|.
Traditional choices of A, being mostly ad-hoc minor perturbations, are usually class-independent, i.e.,
constructing a sample-wise mapping from xi to ¯xi without taking into account the class distribution.
Such mappings are usually limited to a small number of priori known, hand-crafted perturbations.
They are not learned from data, and are not optimized towards ﬁnding classiﬁcation boundaries.
To further improve A, one may consider the inter-sample relationships , as well as inter-class
relationships in D, where training a generative model A over (xi, yi) becomes a viable option.
Classifier
Figure 1: An illustration of DADA.
The conceptual framework of DADA is depicted in Figure 1. If taking a GAN point of view towards
this, A naturally resembles a generator: its inputs can be latent variables zi conditioned on yi, and
outputs ¯xi belonging to the same class yi but being sufﬁciently diverse from xi. C can act as the
discriminator, if it will incorporate typical GAN’s real-fake classiﬁcation in addition to the target
k-class classiﬁcation. Ideally, the classiﬁer C should: (1) be able to correctly classify both real
samples xi and augmented samples ¯xi into the correct class yi; (2) be unable to distinguish xi and
¯xi. The entire DADA framework of A and C can be jointly trained on (xi, yi), whose procedure will
bear similarities to training a class-conditional GAN. However, existing GANs may not ﬁt the task
well, due to the often low diversity of generated samples. We are hence motivated to introduce a
novel loss function towards generating more diverse and class-speciﬁc samples.
Going More Discriminative: From k + 1 Loss to 2k Loss
The discriminator of a vanilla, unsupervised GAN has only one output to indicate the probability
of its input being a real sample. In , the discriminator is extended with a semi-supervised
fashion k + 1 loss, whose output is a (k + 1)-dimensional probabilistic vector: the ﬁrst k elements
denote the probabilities of the input coming from the class 1, 2, ..., k of real data; the (k + 1)-th
denotes its probability of belonging to the generated fake data. In that way, the generator simply
has the semi-supervised classiﬁer learned on additional unlabeled examples and supplied as a new
“generated” class. In contrast, when in extremely low data regimes, we tend to be more “economical”
on consuming data. We recognize that the unlabeled data provides weaker guidance than labeled data
to learn the classiﬁcation decision boundary. Therefore, if there is no real unlabeled data available and
we can only generate from given limited labeled data, generating labeled data (if with quality) should
beneﬁt classiﬁer learning more, compared to generating the same amount of unlabeled data. Further,
the generated labeled samples should join force with the real labeled samples, and their decisions on
the classiﬁcation boundary should be well aligned. Motivated by the above design philosophy, we
build a new 2k loss function, whose ﬁrst group of k outputs represent the probabilities of the input
data from the class 1, 2, ..., k of real data; its second group of k outputs represent the probabilities of
the input data from the class 1, 2, ..., k of fake data.
Since we use a class-conditional augmenter (generator), the label used to synthesize the augmented
(fake) sample could be viewed to supply the “ground truth” for the second group of k outputs. For
example, for k = 2, if the input datum is real and belongs to class 1, then its ground truth label is
 ; otherwise if the input data is augmented conditionally on label of class 1, then its ground
truth label is . During training, the K-L divergence is computed between the 2k-length
output and its ground truth label. For testing, we add the i-th and (k + i)-th elements of the 2k output
to denote the probability of the input belonging to class i, i = 1, 2, ..., k. A comparison among loss
functions for GANs including DADA is listed in Table 1.
Table 1: The comparison of loss functions among GAN discriminators
Class Number
Training Data
Vanilla GAN 
real, fake
unlabeled only
Improved GAN 
real1, ..., realk; fake
labeled + unlabeled
real1, ..., realk; fake1, ..., fakek
labeled only
The detailed training algorithm for DADA is outlined in supplementary.
Simulations
To evaluate our approach, we ﬁrst conduct a series of simulations on three widely adopted image
classiﬁcation benchmarks: CIFAR-10, CIFAR-100, and SVHN. We intentionally sample the given
training data to simulate the extremely low data regimes, and compare the following training options.
1) C: directly train a classiﬁer using the limited training data; 2) C_augmented: perform traditional
data augmentation (including rotation, translation and ﬂipping), and then train a classiﬁer; 3) DADA:
the proposed data augmentation; 4) DADA_augmented: ﬁrst apply the same traditional augmentation
as C_augmented on the real samples, then perform DADA. We use absolutely no unlabeled data or
any pre-trained initialization in training, different from the setting of most previous works. We use
the original full test sets for evaluation. The network architectures that we used have been exhaustively
tuned to ensure the best possible performance of all baselines in those unusually small training sets.
Detailed conﬁgurations and hyperparameters, as well as visualized examples of augmented samples,
are given in the supplementary.
CIFAR-10 and CIFAR-100
The CIFAR-10 dataset consists of 60,000 color images at the resolution of 32×32 in k = 10 classes, with 5,000 images per class for training and 1,000 for testing. We
sample the training data so that the amount of training images varies from 50 to 1,000 per class.
To illustrate the advantage of our proposed 2k loss, we also use the vanilla GAN (which adopt
the 2-class loss), as well as the Improved GAN (which adopt the (k + 1)-class loss), as two
additional baselines to augment samples. For the vanilla GAN, we train a separate generator for each
class. For Improved GAN, we provide only the labeled training data without using any unlabeled
data: a different and more challenging setting than evaluated in . They work with traditional
data augmentation too, similarly to the DADA_augmented pipeline. For all compared methods, we
generate samples so that the augmented dataset has 10 times the size of the given real labeled dataset.
Number of Samples per Class
C_augmented
DADA_augmented
Improved-GAN
Vanilla GAN
Figure 2: Results on CIFAR-10, the test accuracy
in different training settings with respect to the
number of training images per class.
Figure 2 summarizes the performance of the
compared methods.
The vanilla GAN augmentation performs slightly better than the noaugmentation baseline, but the worst in all other
data augmentation settings. It concurs with 
that, though GAN can generate visually pleasing images, it does not naturally come with increased data diversity from a classiﬁcation viewpoint. While improved GAN achieves superior
performance, DADA (without using traditional
augmentation) is able to outperform it at the
smaller end of sample numbers (less than 400
per class). Comparing with vanilla GAN, Improved GAN and DADA_augmented reveal that
as the discriminator loss goes “more discriminative”, the data augmentation becomes more
effective along the way.
Furthermore, DADA_augmented is the best performer among all, and consistently surpass all other
methods for the full range of samples per class. It leads to around 8 percent top-1 accuracy
improvement in the 500 labeled sample, 10 class subset, without relying on any unlabeled data. It
also raises the top-1 performance to nearly 80%, using only 10% of the original training set (i.e. 1000
samples per class), again with neither pre-training nor unlabeled data.
It is worth pointing out that the traditional data augmentation C_augmented presents a very competitive baseline here: it is next to DADA_augmented, and becomes slightly inferior to DADA when
the labeled samples are less than 300 per class, but is constantly better than all others. Further,
integrating traditional data augmentation contributes to the consistent performance boost from DADA
to DADA_augmented. That testiﬁes the value of empirical domain knowledge of invariance: they
help considerably even learning-based augmentation is in place.
Finally, the comparison experiment is repeated on CIFAR-100. The results (see supplementary)
are consistent with CIFAR-10, where DADA_augmented achieves the best results and outperforms
traditional data augmentation for at least 6%, for all sample sizes. We also study the effects of DADA
and traditional augmentation for deeper classiﬁers, such as ResNet-56 (see supplementary).
Table 2: Results on SVHN, the test accuracy in different training settings
# Samples per class n = 50 n = 80 n = 100 n = 200 n = 500
Improved-GAN
SVHN is a digit recognition dataset, whose major challenge lies in that many images contain
“outlier” digits but only the central digit is regarded as the target of recognition. As such, traditional
data augmentation approaches such as translation or ﬂipping may degrade training, and thus are
excluded in this experiment. Table 2 summarizes the results of using the proposed DADA (without
combining traditional augmentation) in comparison with Improved GAN and the naive baseline of no
data augmentation. It can be observed that, at extremely low data regimes, DADA again performs the
best among the three. However, when a relatively large number of labeled samples are available (500
per class), DADA witnesses a slight negative impact on the accuracy compared to the naive baseline,
but is still better than Improved GAN. We conjecture that this failure case is attributed to the “outlier”
digits occurring frequently in SVNH that might hamper class-conditional generative modeling. We
plan to explore more robust generators as future work to alleviate this problem.
We notice the larger margin of DADA (without augmentation) over Improved GAN on SVNH,
compared to CIFAR-10. We conjecture the reason to be that SVNH has complicated perturbations
(e.g., distracting digits), while CIFAR-10 is much “cleaner” in that sense (objects always lie in central
foregrounds without other distractions). Thus on SVNH, the class information used by DADA could
become more important in supervising the generation of high quality augmented samples, without
being affected by perturbations.
Experiments with Real-World Small Data
In this section, we discuss three real-data experiments which fall into extremely low data regimes.
The data, not just labels, are difﬁcult to collect and subject to high variability. We show that in these
cases, the effects of transfer learning are limited, and/or even no ad-hoc data augmentation approach
might be available to alleviate the difﬁculty to train deep networks. In comparison, DADA can be
easily plugged in and boost the classiﬁcation performance in all experiments.
Emotion Recognition from Facial Expressions: Comparison with Transfer Learning
Background and Challenge. Recognizing facial expressions is a topic of growing interests in the
ﬁeld of human-computer interaction. Among several public datasets in this ﬁeld, the Karolinska
Directed Emotional Faces (KDEF) dataset is a challenging benchmark consisting of rich facial
variations (e.g., orientations, ethnicity, age, and gender), as well as relatively uniform distribution of
the emotion classes. It has a total of 4,900 facial images collected from 70 individuals, displaying
seven different facial expressions (happiness, fear, anger, disgust, surprise, sadness, neutrality). For
each individual, the same expression is displayed twice and captured from 5 different angles. We
choose images from the straight upfront angle in the ﬁrst-time display only, forming a subset of 490
images for a 7-class classiﬁcation problem. That certainly places us in an extremely low data regime.
Results and Analyses. We use a random 5:2 split for the training and testing sets and pre-process the
images by cropping and resizing the face regions to the resolution at 224×224. We choose a VGG-16
model pre-trained on ImageNet as a baseline, which is re-trained and then tested on KDEF. We
do not perform any traditional data augmentation, since each image is taken in a strictly-controlled
setting. The baseline could be viewed as a transfer learning solution with ImageNet as the source
domain. We then treat the pre-trained VGG-16 model as our classiﬁer in DADA, and append it with
an augmenter network (whose conﬁguration is detailed in the supplementary). While the pre-trained
VGG baseline gives rise to an accuracy of 82.86%, DADA obtains a higher accuracy of 85.71%.
We also train vanilla GAN and Improved-GAN on this dataset, and have them compare with DADA
in the similar fair setting as in CIFAR-10. The vanilla GAN augmentation ends up with 83.27% and
Improved-GAN gets 84.03%: both outperform transfer learning but stay inferior to DADA.
Transfer learning is often an effective choice for problems short of training data. But their effectiveness is limited when there are domain mismatches, even it is widely believed that ImageNet
pre-trained models are highly transferable for most tasks. In this case, the gap between the source
Table 3: The accuracy (%) comparison on BCI Competition IV dataset 2b among SVM, CNN,
CNN-SAE, and DADA, on subjects 1–9 and their average.
Method Sub. 1 Sub. 2 Sub. 3 Sub. 4 Sub. 5 Sub. 6 Sub. 7 Sub. 8 Sub. 9 Average
CNN-SAE 76.0
domain (ImageNet, general natural images) and the target domain (KDEF, facial images taken in lab
environments) cannot be neglected. We advocate that learning-based data augmentation could boost
the performance further on top of transfer learning, and their combination is more compelling.
Brain Signal Classiﬁcation: No Domain Knowledge Can be Speciﬁed for Augmentation
Background. The classiﬁcation of brain signals has found extensive applications in brain-computer
interface, entertainment and rehabilitation engineering . Among various tasks, the electroencephalographic (EEG) signal classiﬁcation problem has been widely explored. Existing approaches
include band power method , multivariate adaptive autoregressive (MVAAR) method , and independent component analysis (ICA) . Recent works have explored CNNs in classifying
EEG signals. However, the performance boost has been largely limited by the availability of labeled
data. For example, the commonly used benchmark dataset 2b, a subset from the BCI Competition
IV training set , includes only 400 trials. After several domain-speciﬁc pre-processing steps,
each sample could be re-arranged into a 31 × 32 × 3 image, where 3 comes from the three EEG
channels recorded (C3, Cz, and C4). They are collected from three sessions of motor imagery task
experiments, and are to be classiﬁed into two classes of motions: right and left hand movements. We
thus have a practical binary classiﬁcation problem in extremely low data regimes.
Challenge. Unlike image classiﬁcation problems discussed above, no straightforward knowledgebased, label-preserving augmentation has been proposed for EEG signals, nor has any data augmentation been applied in previous EEG classiﬁcation works to our best knowledge. Also, the
noisy nature of brain signals discourages to manually add more perturbations. The major bottleneck
for collecting EEG classiﬁcation datasets lies in the expensive controlled data collection process itself,
rather than the labeling (since subjects are required to perform designated motions in a monitored lab
environment, the collected EEG signals are naturally labeled). Besides, the high variability of human
subjects also limit the scope of transfer learning in EEG classiﬁcation. The multi-fold challenges
make EEG classiﬁcation an appropriate user case and testbed for our proposed DADA approach.
Results and Analyses. We follow to adopt the benchmark dataset 2b from BCI Competition IV
training set . We train and test classiﬁcation models, as well as DADA models separately for each
of the nine subjects. We randomly select 90% of 400 trials for training and the remaining 10% for
testing, and report the average accuracy of 10 runs. We treat each EEG input as a “color image” and
adopt a mostly similar DADA model architecture as used for CIFAR-10 (except for changing class
number)1. We include three baselines reported in for comparison: directly classifying the inputs
by SVM; a shallow CNN with one convolutional and one fully-connected layers (CNN); and a deeper
CNN with one convolutional layer, concatenated with seven fully-connected layers pre-trained using
stacked auto-encoder (CNN + SAE). Table 3 shows the performance advantage of DADA over the
competitive CNN-SAE method in all nine subjects, with an average accuracy margin of 1.7 percent.
Tumor Classiﬁcation: Comparison with Other Learning-based Augmentation
In the existing learning-based data augmentation work Tanda , most training comes with
the help of unlabeled data.
One exception we noticed is their experiment on the Curated
Breast Imaging Subset of the Digital Database for Screening Mammography (CBIS-DDSM)
 , a medical image classiﬁcation task whose data is expensive to collect besides labeling.
Since both Tanda and DADA use the only available labeled dataset to learn data augmentation, we are able to perform a fair comparison on CBIS-DDSM between the two.
1Note that in the same channel of an EEG input, differently from a natural image, the signal coherence between
vertical neighborhood (i.e., among different frequencies) is less than that between horizontal neighborhood (i.e.,
among different time stamps). The standard 2-D CNN is an oversimpliﬁed model here and could be improved
by considering such anisotropy, which is the theme of our future work.
Table 4: Comparison between
DADA and Tanda (in different
training settings)
Tanda (MF)
Tanda (LSTM)
DADA_augmented 0.6549
We follow the same conﬁguration of the classiﬁer used for CBIS-
DDSM by Tanda: a four-layer all-convolution CNN with leaky
ReLUs and batch normalization. We resize all medical images to 224
× 224. Note that Tanda heavily relies on hand-crafted augmentations:
on DDMS, it uses many basic heuristics (crop, rotate, zoom, etc.) and
several domain-speciﬁc transplantations. For DADA_augmented,
we apply only rotation, zooming, and contrast as the traditional
augmentation pre-processing, to be consistent with the user-speciﬁed
traditional augmentation modules in Tanda. We compare DADA
and DADA_augmented with two versions of Tanda using mean ﬁeld
(MF) and LSTM generators , with Table 4 showing the clear advantage of our approaches.
What differentiates DADA and Tanda? Tanda trains a generative sequence model over user-speciﬁed,
knowledge-based transformation functions, while DADA is purely trained in a data-driven discriminative way. Unlike Tanda whose augmented samples always look like the naturalistic samples of each
class, DADA may sometimes lead to augmented samples which are not visually close, but are optimized towards depicting the boundary between different classes. We display some “un-naturalistic”
augmented samples found in the SVHN experiments in supplementary. Tanda also seems to beneﬁt
from the unlabeled data used in training, which ensures the transformed data points to be within the
data distribution, while DADA can work robustly without unlabeled data (such as CBIS-DDSMF).
Conclusion
We present DADA, a learning-based data augmentation solution for training deep classiﬁers in
extremely low data regimes. We leverage the power of GAN to generate new training data that both
bear class labels and enhance diversity. A new 2k loss is elaborated for DADA and veriﬁed to boost
the performance. We perform extensive simulations as well as three real-data experiments, where
results all endorse the practical advantage of DADA. We anticipate that DADA can be applied into
many real-world tasks, including satellite, military, and biomedical image/data classiﬁcation.