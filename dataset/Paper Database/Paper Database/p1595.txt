Statistics and Computing 14: 199–222, 2004
C⃝2004 Kluwer Academic Publishers. Manufactured in The Netherlands.
A tutorial on support vector regression∗
ALEX J. SMOLA and BERNHARD SCH ¨OLKOPF
RSISE, Australian National University, Canberra 0200, Australia
 
Max-Planck-Institut f¨ur biologische Kybernetik, 72076 T¨ubingen, Germany
 
Received July 2002 and accepted November 2003
In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for
function estimation. Furthermore, we include a summary of currently used algorithms for training
SV machines, covering both the quadratic (or convex) programming part and advanced methods for
dealing with large datasets. Finally, we mention some modiﬁcations and extensions that have been
applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.
machine learning, support vector machines, regression estimation
1. Introduction
The purpose of this paper is twofold. It should serve as a selfcontained introduction to Support Vector regression for readers
new to this rapidly developing ﬁeld of research.1 On the other
hand, it attempts to give an overview of recent developments in
To this end, we decided to organize the essay as follows.
We start by giving a brief overview of the basic techniques in
Sections 1, 2 and 3, plus a short summary with a number of
ﬁgures and diagrams in Section 4. Section 5 reviews current
algorithmic techniques used for actually implementing SV
machines. This may be of most interest for practitioners.
The following section covers more advanced topics such as
extensions of the basic SV algorithm, connections between SV
machines and regularization and brieﬂy mentions methods for
carrying out model selection. We conclude with a discussion
of open questions and problems and current directions of SV
research. Most of the results presented in this review paper
already have been published elsewhere, but the comprehensive
presentations and some details are new.
1.1. Historic background
The SV algorithm is a nonlinear generalization of the Generalized Portrait algorithm developed in Russia in the sixties2
∗An extended version of this paper is available as NeuroCOLT Technical Report
TR-98-030.
 . As
such, it is ﬁrmly grounded in the framework of statistical learning theory, or VC theory, which has been developed over the last
three decades by Vapnik and Chervonenkis and Vapnik
 . In a nutshell, VC theory characterizes properties
of learning machines which enable them to generalize well to
unseen data.
In its present form, the SV machine was largely developed
at AT&T Bell Laboratories by Vapnik and co-workers . Due to this industrial context, SV research has up to date had a sound orientation towards
real-world applications. Initial work focused on OCR (optical
character recognition). Within a short period of time, SV classiﬁers became competitive with the best available systems for
both OCR and object recognition tasks . A
comprehensive tutorial on SV classiﬁers has been published by
Burges . But also in regression and time series prediction applications, excellent performances were soon obtained
 . A snapshot of the state of the art
in SV learning was recently taken at the annual Neural Information Processing Systems conference . SV learning has now evolved into an active
area of research. Moreover, it is in the process of entering the
standard methods toolbox of machine learning . Sch¨olkopf and
C⃝2004 Kluwer Academic Publishers
Smola and Sch¨olkopf
Smola contains a more in-depth overview of SVM regression.Additionally,CristianiniandShawe-Taylor andHerbrich provide further details on kernels in the context of
classiﬁcation.
1.2. The basic idea
Suppose we are given training data {(x1, y1), . . . , (xℓ, yℓ)} ⊂
X × R, where X denotes the space of the input patterns (e.g.
X = Rd). These might be, for instance, exchange rates for some
currency measured at subsequent days together with corresponding econometric indicators. In ε-SV regression ,
our goal is to ﬁnd a function f (x) that has at most ε deviation
from the actually obtained targets yi for all the training data, and
at the same time is as ﬂat as possible. In other words, we do not
care about errors as long as they are less than ε, but will not
accept any deviation larger than this. This may be important if
you want to be sure not to lose more than ε money when dealing
with exchange rates, for instance.
For pedagogical reasons, we begin by describing the case of
linear functions f , taking the form
f (x) = ⟨w, x⟩+ b with w ∈X, b ∈R
where ⟨· , · ⟩denotes the dot product in X. Flatness in the case
of (1) means that one seeks a small w. One way to ensure this is
to minimize the norm,3 i.e. ∥w∥2 = ⟨w, w⟩. We can write this
problem as a convex optimization problem:
subject to
yi −⟨w, xi⟩−b ≤ε
⟨w, xi⟩+ b −yi ≤ε
The tacit assumption in (2) was that such a function f actually
exists that approximates all pairs (xi, yi) with ε precision, or in
other words, that the convex optimization problem is feasible.
Sometimes, however, this may not be the case, or we also may
want to allow for some errors. Analogously to the “soft margin” loss function which was
used in SV machines by Cortes and Vapnik , one can introduce slack variables ξi, ξ ∗
i to cope with otherwise infeasible
constraints of the optimization problem (2). Hence we arrive at
the formulation stated in Vapnik .
subject to
yi −⟨w, xi⟩−b ≤ε + ξi
⟨w, xi⟩+ b −yi ≤ε + ξ ∗
The constant C > 0 determines the trade-off between the ﬂatness of f and the amount up to which deviations larger than
ε are tolerated. This corresponds to dealing with a so called
ε-insensitive loss function |ξ|ε described by
otherwise.
Fig. 1. The soft margin loss setting for a linear SVM 
Figure 1 depicts the situation graphically. Only the points outside
the shaded region contribute to the cost insofar, as the deviations
are penalized in a linear fashion. It turns out that in most cases
the optimization problem (3) can be solved more easily in its dual
formulation.4 Moreover, as we will see in Section 2, the dual formulationprovidesthekeyforextendingSVmachinetononlinear
functions. Hence we will use a standard dualization method utilizing Lagrange multipliers, as described in e.g. Fletcher .
1.3. Dual problem and quadratic programs
The key idea is to construct a Lagrange function from the objective function (it will be called the primal objective function
in the rest of this article) and the corresponding constraints, by
introducing a dual set of variables. It can be shown that this
function has a saddle point with respect to the primal and dual
variables at the solution. For details see e.g. Mangasarian ,
McCormick , and Vanderbei and the explanations
in Section 5.2. We proceed as follows:
(ηiξi + η∗
αi(ε + ξi −yi + ⟨w, xi⟩+ b)
i (ε + ξ ∗
i + yi −⟨w, xi⟩−b)
Here L is the Lagrangian and ηi, η∗
i , αi, α∗
i are Lagrange multipliers. Hence the dual variables in (5) have to satisfy positivity
constraints, i.e.
Note that by α(∗)
i , we refer to αi and α∗
It follows from the saddle point condition that the partial
derivatives of L with respect to the primal variables (w, b, ξi, ξ ∗
have to vanish for optimality.
i −αi) = 0
i L = C −α(∗)
A tutorial on support vector regression
Substituting (7), (8), and (9) into (5) yields the dual optimization
i )(α j −α∗
j)⟨xi, x j⟩
subject to
In deriving (10) we already eliminated the dual variables ηi, η∗
through condition (9) which can be reformulated as η(∗)
i . Equation (8) can be rewritten as follows
i )xi, thus f (x) =
i )⟨xi, x⟩+ b.
This is the so-called Support Vector expansion, i.e. w can be
completely described as a linear combination of the training
patterns xi. In a sense, the complexity of a function’s representation by SVs is independent of the dimensionality of the input
space X, and depends only on the number of SVs.
Moreover, note that the complete algorithm can be described
in terms of dot products between the data. Even when evaluating f (x) we need not compute w explicitly. These observations will come in handy for the formulation of a nonlinear
extension.
1.4. Computing b
So far we neglected the issue of computing b. The latter can be
done by exploiting the so called Karush–Kuhn–Tucker (KKT)
conditions . These state
thatatthepointofthesolutiontheproductbetweendualvariables
and constraints has to vanish.
αi(ε + ξi −yi + ⟨w, xi⟩+ b) = 0
i (ε + ξ ∗
i + yi −⟨w, xi⟩−b) = 0
(C −αi)ξi = 0
This allows us to make several useful conclusions. Firstly only
samples (xi, yi) with corresponding α(∗)
= C lie outside the εinsensitive tube. Secondly αiα∗
i = 0, i.e. there can never be a set
of dual variables αi, α∗
i which are both simultaneously nonzero.
This allows us to conclude that
ε −yi + ⟨w, xi⟩+ b ≥0
if αi < C (14)
ε −yi + ⟨w, xi⟩+ b ≤0
In conjunction with an analogous analysis on α∗
max{−ε + yi −⟨w, xi⟩| αi < C or α∗
i > 0} ≤b ≤
min{−ε + yi −⟨w, xi⟩| αi > 0 or α∗
If some α(∗)
∈(0, C) the inequalities become equalities. See
also Keerthi et al. for further means of choosing b.
Another way of computing b will be discussed in the context
of interior point optimization (cf. Section 5). There b turns out
to be a by-product of the optimization process. Further considerations shall be deferred to the corresponding section. See also
Keerthi et al. for further methods to compute the constant
A ﬁnal note has to be made regarding the sparsity of the SV
expansion. From (12) it follows that only for | f (xi) −yi| ≥ε
the Lagrange multipliers may be nonzero, or in other words, for
all samples inside the ε–tube (i.e. the shaded region in Fig. 1)
the αi, α∗
i vanish: for | f (xi) −yi| < ε the second factor in
(12) is nonzero, hence αi, α∗
i has to be zero such that the KKT
conditions are satisﬁed. Therefore we have a sparse expansion
of w in terms of xi (i.e. we do not need all xi to describe w). The
examples that come with nonvanishing coefﬁcients are called
Support Vectors.
2. Kernels
2.1. Nonlinearity by preprocessing
The next step is to make the SV algorithm nonlinear. This, for
instance, could be achieved by simply preprocessing the training
patterns xi by a map  : X →F into some feature space F,
as described in Aizerman, Braverman and Rozono´er and
Nilsson and then applying the standard SV regression
algorithm. Let us have a brief look at an example given in Vapnik
Example 1 (Quadratic features in R2).
Consider the map  :
R2 →R3 with (x1, x2) = (x2
2). It is understood
that the subscripts in this case refer to the components of x ∈R2.
TrainingalinearSVmachineonthepreprocessedfeatureswould
yield a quadratic function.
While this approach seems reasonable in the particular example above, it can easily become computationally infeasible
for both polynomial features of higher order and higher dimensionality, as the number of different monomial features
of degree p is (d+p−1
), where d = dim(X). Typical values
for OCR tasks (with good performance) are
p = 7, d = 28 · 28 = 784, corresponding to approximately
3.7 · 1016 features.
2.2. Implicit mapping via kernels
Clearly this approach is not feasible and we have to ﬁnd a computationally cheaper way. The key observation is that for the feature map of example 2.1 we
= ⟨x, x′⟩2.
As noted in the previous section, the SV algorithm only depends
on dot products between patterns xi. Hence it sufﬁces to know
k(x, x′) := ⟨(x), (x′)⟩rather than  explicitly which allows
us to restate the SV optimization problem:
i )(α j −α∗
j)k(xi, x j)
subject to
Likewise the expansion of f (11) may be written as
i )k(xi, x) + b.
The difference to the linear case is that w is no longer given explicitly. Also note that in the nonlinear setting, the optimization
problem corresponds to ﬁnding the ﬂattest function in feature
space, not in input space.
2.3. Conditions for kernels
The question that arises now is, which functions k(x, x′) correspond to a dot product in some feature space F. The following
theorem characterizes these functions (deﬁned on X).
Theorem 2 .
Suppose k ∈L∞(X 2) such that
the integral operator Tk : L2(X) →L2(X),
Tk f (·) :=
k(·, x) f (x)dµ(x)
is positive (here µ denotes a measure on X with µ(X) ﬁnite
and supp(µ) = X). Let ψ j ∈L2(X) be the eigenfunction of Tk
associated with the eigenvalue λ j ̸= 0 and normalized such that
∥ψ j∥L2 = 1 and let ψ j denote its complex conjugate. Then
1. (λ j(T )) j ∈ℓ1.
2. k(x, x′) =
j∈N λ jψ j(x)ψ j(x′) holds for almost all (x, x′),
where the series converges absolutely and uniformly for almost all (x, x′).
Less formally speaking this theorem means that if
k(x, x′) f (x) f (x′) dxdx′ ≥0 for all f ∈L2(X)
holds we can write k(x, x′) as a dot product in some feature
space. From this condition we can conclude some simple rules
for compositions of kernels, which then also satisfy Mercer’s
condition . In the following we will call such functions k admissible SV kernels.
Corollary 3 (Positive linear combinations of kernels).
by k1, k2 admissible SV kernels and c1, c2 ≥0 then
k(x, x′) := c1k1(x, x′) + c2k2(x, x′)
is an admissible kernel. This follows directly from (21) by virtue
of the linearity of integrals.
More generally, one can show that the set of admissible kernels forms a convex cone, closed in the topology of pointwise
convergence .
Corollary 4 (Integrals of kernels).
Let s(x, x′) be a function
on X × X such that
k(x, x′) :=
s(x, z)s(x′, z) dz
exists. Then k is an admissible SV kernel.
This can be shown directly from (21) and (23) by rearranging the
order of integration. We now state a necessary and sufﬁcient condition for translation invariant kernels, i.e. k(x, x′) := k(x −x′)
as derived in Smola, Sch¨olkopf and M¨uller .
Theorem 5 (Products of kernels).
Denote by k1 and k2 admissible SV kernels then
k(x, x′) := k1(x, x′)k2(x, x′)
is an admissible kernel.
This can be seen by an application of the “expansion part” of
Mercer’s theorem to the kernels k1 and k2 and observing that
each term in the double sum
gives rise to a positive coefﬁcient when checking (21).
Theorem 6 .
A translation invariant kernel k(x, x′) = k(x −x′) is an admissible SV
kernels if and only if the Fourier transform
F[k](ω) = (2π)−d
e−i⟨ω,x⟩k(x)dx
is nonnegative.
We will give a proof and some additional explanations to this
theorem in Section 7. It follows from interpolation theory
 and the theory of regularization networks
 . For kernels of the dot-product
type, i.e. k(x, x′) = k(⟨x, x′⟩), there exist sufﬁcient conditions
for being admissible.
Theorem 7 .
Any kernel of dot-product type
k(x, x′) = k(⟨x, x′⟩) has to satisfy
k(ξ) ≥0, ∂ξk(ξ) ≥0
∂ξk(ξ) + ξ∂2
ξ k(ξ) ≥0 (26)
for any ξ ≥0 in order to be an admissible SV kernel.
A tutorial on support vector regression
Note that the conditions in Theorem 7 are only necessary but
not sufﬁcient. The rules stated above can be useful tools for
practitioners both for checking whether a kernel is an admissible
SV kernel and for actually constructing new kernels. The general
case is given by the following theorem.
Theorem 8 .
A kernel of dot-product type
k(x, x′) = k(⟨x, x′⟩) deﬁned on an inﬁnite dimensional Hilbert
space, with a power series expansion
is admissible if and only if all an ≥0.
A slightly weaker condition applies for ﬁnite dimensional
spaces. For further details see Berg, Christensen and Ressel
 and Smola, ´Ov´ari and Williamson .
2.4. Examples
In Sch¨olkopf, Smola and M¨uller it has been shown, by
explicitly computing the mapping, that homogeneous polynomial kernels k with p ∈N and
k(x, x′) = ⟨x, x′⟩p
are suitable SV kernels . From this observation
one can conclude immediately that kernels of the type
k(x, x′) = (⟨x, x′⟩+ c)p
i.e. inhomogeneous polynomial kernels with p ∈N, c ≥0 are
admissible, too: rewrite k as a sum of homogeneous kernels and
apply Corollary 3. Another kernel, that might seem appealing
due to its resemblance to Neural Networks is the hyperbolic
tangent kernel
k(x, x′) = tanh(ϑ + κ⟨x, x′⟩).
By applying Theorem 8 one can check that this kernel does
not actually satisfy Mercer’s condition . Curiously,
the kernel has been successfully used in practice; cf. Scholkopf
 for a discussion of the reasons.
Translation invariant kernels k(x, x′)
k(x −x′) are
quite widespread. It was shown in Aizerman, Braverman and
Rozono´er , Micchelli and Boser, Guyon and Vapnik that
k(x, x′) = e−∥x−x′∥2
is an admissible SV kernel. Moreover one can show that (1X denotes the
indicator function on the set X and ⊗the convolution operation)
k(x, x′) = B2n+1(∥x −x′∥) with Bk :=
B-splines of order 2n + 1, deﬁned by the 2n + 1 convolution of
the unit inverval, are also admissible. We shall postpone further
considerations to Section 7 where the connection to regularization operators will be pointed out in more detail.
3. Cost functions
So far the SV algorithm for regression may seem rather strange
and hardly related to other existing methods of function estimation . However, once cast into a more
standard mathematical notation, we will observe the connections to previous work. For the sake of simplicity we will, again,
only consider the linear case, as extensions to the nonlinear one
are straightforward by using the kernel method described in the
previous chapter.
3.1. The risk functional
Let us for a moment go back to the case of Section 1.2. There, we
had some training data X := {(x1, y1), . . . , (xℓ, yℓ)} ⊂X × R.
We will assume now, that this training set has been drawn iid
(independent and identically distributed) from some probability distribution P(x, y). Our goal will be to ﬁnd a function f
minimizing the expected risk 
c(x, y, f (x))d P(x, y)
(c(x, y, f (x)) denotes a cost function determining how we will
penalize estimation errors) based on the empirical data X. Given
that we do not know the distribution P(x, y) we can only use
X for estimating a function f that minimizes R[ f ]. A possible approximation consists in replacing the integration by the
empirical estimate, to get the so called empirical risk functional
Remp[ f ] := 1
c(xi, yi, f (xi)).
A ﬁrst attempt would be to ﬁnd the empirical risk minimizer
f0 := argmin f ∈H Remp[ f ] for some function class H. However,
if H is very rich, i.e. its “capacity” is very high, as for instance
when dealing with few data in very high-dimensional spaces,
this may not be a good idea, as it will lead to overﬁtting and thus
bad generalization properties. Hence one should add a capacity
control term, in the SV case ∥w∥2, which leads to the regularized
risk functional 
Rreg[ f ] := Remp[ f ] + λ
where λ > 0 is a so called regularization constant. Many
algorithms like regularization networks or neural networks with weight decay networks
 minimize an expression similar to (35).
Smola and Sch¨olkopf
3.2. Maximum likelihood and density models
The standard setting in the SV case is, as already mentioned in
Section 1.2, the ε-insensitive loss
c(x, y, f (x)) = |y −f (x)|ε.
It is straightforward to show that minimizing (35) with the particular loss function of (36) is equivalent to minimizing (3), the
only difference being that C = 1/(λℓ).
Loss functions such like |y −f (x)|p
ε with p > 1 may not
be desirable, as the superlinear increase leads to a loss of the
robustness properties of the estimator : in those
cases the derivative of the cost function grows without bound.
For p < 1, on the other hand, c becomes nonconvex.
For the case of c(x, y, f (x)) = (y −f (x))2 we recover the
least mean squares ﬁt approach, which, unlike the standard SV
loss function, leads to a matrix inversion instead of a quadratic
programming problem.
The question is which cost function should be used in (35). On
the one hand we will want to avoid a very complicated function c
as this may lead to difﬁcult optimization problems. On the other
hand one should use that particular cost function that suits the
problem best. Moreover, under the assumption that the samples
were generated by an underlying functional dependency plus
additive noise, i.e. yi = ftrue(xi) + ξi with density p(ξ), then the
optimal cost function in a maximum likelihood sense is
c(x, y, f (x)) = −log p(y −f (x)).
This can be seen as follows. The likelihood of an estimate
X f := {(x1, f (x1)), . . . , (xℓ, f (xℓ))}
for additive noise and iid data is
p(X f | X) =
p( f (xi) | (xi, yi)) =
p(yi −f (xi)).
Maximizing P(X f | X) is equivalent to minimizing −log P
(X f | X). By using (37) we get
−log P(X f | X) =
c(xi, yi, f (xi)).
Table 1. Common loss functions and corresponding density models
Loss function
Density model
ε-insensitive
c(ξ) = |ξ|ε
2(1+ε)exp(−|ξ|ε)
c(ξ) = |ξ|
2exp(−|ξ|)
Huber’s robust loss
Polynomial
2(1/p)exp(−|ξ|p)
Piecewise polynomial
pσ p−1 (ξ)p
|ξ| −σ p−1
However, the cost function resulting from this reasoning might
be nonconvex. In this case one would have to ﬁnd a convex
proxy in order to deal with the situation efﬁciently (i.e. to ﬁnd
an efﬁcient implementation of the corresponding optimization
If, on the other hand, we are given a speciﬁc cost function from
a real world problem, one should try to ﬁnd as close a proxy to
this cost function as possible, as it is the performance wrt. this
particular cost function that matters ultimately.
Table 1 contains an overview over some common density
models and the corresponding loss functions as deﬁned by
The only requirement we will impose on c(x, y, f (x)) in the
following is that for ﬁxed x and y we have convexity in f (x).
This requirement is made, as we want to ensure the existence and
uniqueness (for strict convexity) of a minimum of optimization
problems .
3.3. Solving the equations
For the sake of simplicity we will additionally assume c to
be symmetric and to have (at most) two (for symmetry) discontinuities at ±ε, ε ≥0 in the ﬁrst derivative, and to be
zero in the interval [−ε, ε]. All loss functions from Table 1
belong to this class. Hence c will take on the following
c(x, y, f (x)) = ˜c(|y −f (x)|ε)
Note the similarity to Vapnik’s ε-insensitive loss. It is rather
straightforward to extend this special choice to more general
convex cost functions. For nonzero cost functions in the interval [−ε, ε] use an additional pair of slack variables. Moreover
we might choose different cost functions ˜ci, ˜c∗
i and different
values of εi, ε∗
i for each sample. At the expense of additional
Lagrange multipliers in the dual formulation additional discontinuities also can be taken care of. Analogously to (3) we arrive at
a convex minimization problem .
To simplify notation we will stick to the one of (3) and use C
A tutorial on support vector regression
instead of normalizing by λ and ℓ.
( ˜c(ξi) + ˜c(ξ ∗
subject to
yi −⟨w, xi⟩−b ≤ε + ξi
⟨w, xi⟩+ b −yi ≤ε + ξ ∗
Again, by standard Lagrange multiplier techniques, exactly in
the same manner as in the |·|ε case, one can compute the dual optimization problem (the main difference is that the slack variable
terms ˜c(ξ (∗)
) now have nonvanishing derivatives). We will omit
the indices i and ∗, where applicable to avoid tedious notation.
This yields


i )(α j −α∗
j)⟨xi, x j⟩
i ) −ε(αi + α∗
T (ξi) + T (ξ ∗
T (ξ) := ˜c(ξ) −ξ∂ξ ˜c(ξ)
subject to
α ≤C∂ξ ˜c(ξ)
ξ = inf{ξ | C∂ξ ˜c ≥α}
3.4. Examples
Let us consider the examples of Table 1. We will show explicitly
for two examples how (43) can be further simpliﬁed to bring it
into a form that is practically useful. In the ε-insensitive case,
i.e. ˜c(ξ) = |ξ| we get
T (ξ) = ξ −ξ · 1 = 0.
Morover one can conclude from ∂ξ ˜c(ξ) = 1 that
ξ = inf{ξ | C ≥α} = 0
α ∈[0, C].
For the case of piecewise polynomial loss we have to distinguish
two different cases: ξ ≤σ and ξ > σ. In the ﬁrst case we get
pσ p−1 ξ p −
σ p−1 ξ p = −p −1
and ξ = inf{ξ | Cσ 1−pξ p−1 ≥α} = σC−
p−1 and thus
T (ξ) = −p −1
Table 2. Terms of the convex optimization problem depending on the
choice of the loss function
ε-insensitive
robust loss
Polynomial
polynomial
In the second case (ξ ≥σ) we have
T (ξ) = ξ −σ p −1
−ξ = −σ p −1
and ξ = inf{ξ | C ≥α} = σ, which, in turn yields α ∈[0, C].
Combining both cases we have
α ∈[0, C] and T (α) = −p −1
Table 2 contains a summary of the various conditions on α and
formulas for T (α) (strictly speaking T (ξ(α))) for different cost
functions.5 Note that the maximum slope of ˜c determines the
region of feasibility of α, i.e. s := supξ∈R+ ∂ξ ˜c(ξ) < ∞leads to
compact intervals [0, Cs] for α. This means that the inﬂuence
of a single pattern is bounded, leading to robust estimators
 . One can also observe experimentally that the
performance of a SV machine depends signiﬁcantly on the cost
function used , Burges and Sch¨olkopf and Sch¨olkopf
et al. or sparse decomposition techniques could be applied to address this issue. In a
Bayesian setting, Tipping has recently shown how an L2
cost function can be used without sacriﬁcing sparsity.
4. The bigger picture
Before delving into algorithmic details of the implementation
let us brieﬂy review the basic properties of the SV algorithm
for regression as described so far. Figure 2 contains a graphical
overview over the different steps in the regression stage.
The input pattern (for which a prediction is to be made) is
mapped into feature space by a map . Then dot products
are computed with the images of the training patterns under
Smola and Sch¨olkopf
Fig. 2. Architecture of a regression machine constructed by the SV
the map . This corresponds to evaluating kernel functions
k(xi, x). Finally the dot products are added up using the weights
νi = αi −α∗
i . This, plus the constant term b yields the ﬁnal
prediction output. The process described here is very similar to
regression in a neural network, with the difference, that in the
SV case the weights in the input layer are a subset of the training
Figure 3 demonstrates how the SV algorithm chooses the
ﬂattest function among those approximating the original data
with a given precision. Although requiring ﬂatness only in
feature space, one can observe that the functions also are
very ﬂat in input space. This is due to the fact, that kernels can be associated with ﬂatness properties via regular-
Fig. 3. Left to right: approximation of the function sinc x with precisions ε = 0.1, 0.2, and 0.5. The solid top and the bottom lines indicate the size
of the ε-tube, the dotted line in between is the regression
Fig. 4. Left to right: regression (solid line), datapoints (small dots) and SVs (big dots) for an approximation with ε = 0.1, 0.2, and 0.5. Note the
decrease in the number of SVs
ization operators. This will be explained in more detail in
Section 7.
Finally Fig. 4 shows the relation between approximation quality and sparsity of representation in the SV case. The lower the
precision required for approximating the original data, the fewer
SVs are needed to encode that. The non-SVs are redundant, i.e.
even without these patterns in the training set, the SV machine
would have constructed exactly the same function f . One might
think that this could be an efﬁcient way of data compression,
namely by storing only the support patterns, from which the estimate can be reconstructed completely. However, this simple
analogy turns out to fail in the case of high-dimensional data,
and even more drastically in the presence of noise. In Vapnik,
Golowich and Smola one can see that even for moderate
approximation quality, the number of SVs can be considerably
high, yielding rates worse than the Nyquist rate .
5. Optimization algorithms
While there has been a large number of implementations of SV
algorithms in the past years, we focus on a few algorithms which
will be presented in greater detail. This selection is somewhat
biased, as it contains these algorithms the authors are most familiar with. However, we think that this overview contains some
of the most effective ones and will be useful for practitioners
who would like to actually code a SV machine by themselves.
But before doing so we will brieﬂy cover major optimization
packages and strategies.
A tutorial on support vector regression
5.1. Implementations
Most commercially available packages for quadratic programming can also be used to train SV machines. These are usually
numerically very stable general purpose codes, with special enhancements for large sparse systems. While the latter is a feature
that is not needed at all in SV problems (there the dot product
matrix is dense and huge) they still can be used with good success.6
OSL: This package was written by IBM-Corporation . It
uses a two phase algorithm. The ﬁrst step consists of solving
a linear approximation of the QP problem by the simplex algorithm . Next a related very simple QP problem is dealt with. When successive approximations are close
enough together, the second subalgorithm, which permits a
quadratic objective and converges very rapidly from a good
starting value, is used. Recently an interior point algorithm
was added to the software suite.
CPLEX byCPLEX-Optimization-Inc. usesaprimal-dual
logarithmic barrier algorithm instead with
predictor-corrector step .
MINOS by the Stanford Optimization Laboratory uses a reduced gradient algorithm in conjunction with a quasi-Newton algorithm. The constraints are
handled by an active set strategy. Feasibility is maintained
throughout the process. On the active constraint manifold, a
quasi-Newton approximation is used.
MATLAB: Until recently the matlab QP optimizer delivered only
agreeable, although below average performance on classiﬁcation tasks and was not all too useful for regression tasks
(for problems much larger than 100 samples) due to the fact
that one is effectively dealing with an optimization problem of size 2ℓwhere at least half of the eigenvalues of the
Hessian vanish. These problems seem to have been addressed
in version 5.3 / R11. Matlab now uses interior point codes.
LOQO by Vanderbei is another example of an interior
point code. Section 5.3 discusses the underlying strategies in
detail and shows how they can be adapted to SV algorithms.
Maximum margin perceptron by Kowalczyk is an algorithm speciﬁcally tailored to SVs. Unlike most other techniques it works directly in primal space and thus does not
have to take the equality constraint on the Lagrange multipliers into account explicitly.
Iterative free set methods The algorithm by Kaufman , uses such a technique
starting with all variables on the boundary and adding them as
the Karush Kuhn Tucker conditions become more violated.
This approach has the advantage of not having to compute
the full dot product matrix from the beginning. Instead it is
evaluated on the ﬂy, yielding a performance improvement
in comparison to tackling the whole optimization problem
at once. However, also other algorithms can be modiﬁed by
subset selection techniques (see Section 5.5) to address this
5.2. Basic notions
Most algorithms rely on results from the duality theory in convex
optimization. Although we already happened to mention some
basic ideas in Section 1.2 we will, for the sake of convenience,
brieﬂy review without proof the core results. These are needed
in particular to derive an interior point algorithm. For details and
proofs .
Uniqueness: Every convex constrained optimization problem
has a unique minimum. If the problem is strictly convex then
the solution is unique. This means that SVs are not plagued
with the problem of local minima as Neural Networks are.7
Lagrange function: The Lagrange function is given by the primal objective function minus the sum of all products between
constraints and corresponding Lagrange multipliers . Optimization can be seen
as minimzation of the Lagrangian wrt. the primal variables
and simultaneous maximization wrt. the Lagrange multipliers, i.e. dual variables. It has a saddle point at the solution.
Usually the Lagrange function is only a theoretical device to
derive the dual objective function (cf. Section 1.2).
Dual objective function: It is derived by minimizing the
Lagrange function with respect to the primal variables and
subsequent elimination of the latter. Hence it can be written
solely in terms of the dual variables.
Duality gap: For both feasible primal and dual variables the primal objective function (of a convex minimization problem)
is always greater or equal than the dual objective function.
Since SVMs have only linear constraints the constraint qualiﬁcations of the strong duality theorem are satisﬁed and it follows that
gap vanishes at optimality. Thus the duality gap is a measure
how close (in terms of the objective function) the current set
of variables is to the solution.
Karush–Kuhn–Tucker (KKT) conditions: A set of primal and
dual variables that is both feasible and satisﬁes the KKT
conditions is the solution (i.e. constraint · dual variable = 0).
The sum of the violated KKT terms determines exactly the
size of the duality gap (that is, we simply compute the
constraint · Lagrangemultiplier part as done in (55)). This
allows us to compute the latter quite easily.
A simple intuition is that for violated constraints the dual
variable could be increased arbitrarily, thus rendering the
Lagrange function arbitrarily large. This, however, is in contradition to the saddlepoint property.
5.3. Interior point algorithms
In a nutshell the idea of an interior point algorithm is to compute the dual of the optimization problem (in our case the dual
dual of Rreg[ f ]) and solve both primal and dual simultaneously.
This is done by only gradually enforcing the KKT conditions
Smola and Sch¨olkopf
to iteratively ﬁnd a feasible solution and to use the duality
gap between primal and dual objective function to determine
the quality of the current set of variables. The special ﬂavour
of algorithm we will describe is primal-dual path-following
 .
In order to avoid tedious notation we will consider the slightly
more general problem and specialize the result to the SVM later.
It is understood that unless stated otherwise, variables like α
denote vectors and αi denotes its i-th component.
2q(α) + ⟨c, α⟩
subject to
with c, α,l, u ∈Rn, A ∈Rn·m, b ∈Rm, the inequalities between vectors holding componentwise and q(α) being a convex
function of α. Now we will add slack variables to get rid of all
inequalities but the positivity constraints. This yields:
2q(α) + ⟨c, α⟩
subject to
Aα = b, α −g = l, α + t = u,
g, t ≥0, α free
The dual of (51) is
2(q(α) −⟨⃗∂q(α), α)⟩+ ⟨b, y⟩+ ⟨l, z⟩−⟨u, s⟩
subject to
⃗∂q(α) + c −(Ay)⊤+ s = z, s, z ≥0, y free
Moreover we get the KKT conditions, namely
siti = 0 for all i ∈[1 . . . n].
A necessary and sufﬁcient condition for the optimal solution is
that the primal/dual variables satisfy both the feasibility conditions of (51) and (52) and the KKT conditions (53). We proceed to solve (51)–(53) iteratively. The details can be found in
Appendix A.
5.4. Useful tricks
Before proceeding to further algorithms for quadratic optimization let us brieﬂy mention some useful tricks that can be applied
to all algorithms described subsequently and may have significant impact despite their simplicity. They are in part derived
from ideas of the interior-point approach.
Training with different regularization parameters: For several
reasons (model selection, controlling the number of support
vectors, etc.) it may happen that one has to train a SV machine with different regularization parameters C, but otherwise rather identical settings. If the parameters Cnew = τCold
is not too different it is advantageous to use the rescaled values of the Lagrange multipliers (i.e. αi, α∗
i ) as a starting point
for the new optimization problem. Rescaling is necessary to
satisfy the modiﬁed constraints. One gets
αnew = ταold
likewise bnew = τbold.
Assuming that the (dominant) convex part q(α) of the primal objective is quadratic, the q scales with τ 2 where as the
linear part scales with τ. However, since the linear term dominates the objective function, the rescaled values are still a
better starting point than α = 0. In practice a speedup of
approximately 95% of the overall training time can be observed when using the sequential minimization algorithm,
cf. . A similar reasoning can be applied when
retraining with the same regularization parameter but different (yet similar) width parameters of the kernel function. See
Cristianini, Campbell and Shawe-Taylor for details
thereon in a different context.
Monitoring convergence via the feasibility gap: In the case of
both primal and dual feasible variables the following connection between primal and dual objective function holds:
Dual Obj. = Primal Obj. −
(gizi + siti)
This can be seen immediately by the construction of the
Lagrange function. In Regression Estimation (with the εinsensitive loss function) one obtains for
i gizi + siti
+ max(0, f (xi) −(yi + εi))(C −α∗
−min(0, f (xi) −(yi + εi))α∗
+ max(0, (yi −ε∗
i ) −f (xi))(C −αi)
−min(0, (yi −ε∗
i ) −f (xi))αi
Thus convergence with respect to the point of the solution
can be expressed in terms of the duality gap. An effective
stopping rule is to require
i gizi + siti
|Primal Objective| + 1 ≤εtol
for some precision εtol. This condition is much in the spirit of
primal dual interior point path following algorithms, where
convergenceismeasuredintermsofthenumberofsigniﬁcant
ﬁgures (which would be the decimal logarithm of (57)), a
convention that will also be adopted in the subsequent parts
of this exposition.
5.5. Subset selection algorithms
The convex programming algorithms described so far can be
used directly on moderately sized (up to 3000) samples datasets
without any further modiﬁcations. On large datasets, however, it
is difﬁcult, due to memory and cpu limitations, to compute the
dot product matrix k(xi, x j) and keep it in memory. A simple
calculation shows that for instance storing the dot product matrix
of the NIST OCR database (60.000 samples) at single precision
would consume 0.7 GBytes. A Cholesky decomposition thereof,
which would additionally require roughly the same amount of
memory and 64 Teraﬂops (counting multiplies and adds separately), seems unrealistic, at least at current processor speeds.
A ﬁrst solution, which was introduced in Vapnik relies
on the observation that the solution can be reconstructed from
the SVs alone. Hence, if we knew the SV set beforehand, and
A tutorial on support vector regression
it ﬁtted into memory, then we could directly solve the reduced
problem. The catch is that we do not know the SV set before
solving the problem. The solution is to start with an arbitrary
subset, a ﬁrst chunk that ﬁts into memory, train the SV algorithm
on it, keep the SVs and ﬁll the chunk up with data the current
estimator would make errors on (i.e. data lying outside the εtube of the current regression). Then retrain the system and keep
on iterating until after training all KKT-conditions are satisﬁed.
The basic chunking algorithm just postponed the underlying
problem of dealing with large datasets whose dot-product matrix
cannot be kept in memory: it will occur for larger training set
sizes than originally, but it is not completely avoided. Hence
the solution is Osuna, Freund and Girosi to use only a
subset of the variables as a working set and optimize the problem
with respect to them while freezing the other variables. This
methodisdescribedindetailinOsuna,FreundandGirosi ,
Joachims andSaundersetal. forthecaseofpattern
recognition.8
An adaptation of these techniques to the case of regression
with convex cost functions can be found in Appendix B. The
basic structure of the method is described by Algorithm 1.
Algorithm 1.: Basic structure of a working set algorithm
Initialize αi, α∗
Choose arbitrary working set Sw
Compute coupling terms (linear and constant) for Sw (see
Appendix A.3)
Solve reduced optimization problem
Choose new Sw from variables αi, α∗
i not satisfying the
KKT conditions
until working set Sw = ∅
5.6. Sequential minimal optimization
Recently an algorithm—Sequential Minimal Optimization
(SMO)—was proposed that puts chunking to the
extreme by iteratively selecting subsets only of size 2 and optimizing the target function with respect to them. It has been
reported to have good convergence properties and it is easily
implemented. The key point is that for a working set of 2 the
optimization subproblem can be solved analytically without explicitly invoking a quadratic optimizer.
While readily derived for pattern recognition by Platt ,
one simply has to mimick the original reasoning to obtain an
extension to Regression Estimation. This is what will be done
in Appendix C ). The modiﬁcations consist of a pattern dependent regularization, convergence control via the number of
signiﬁcant ﬁgures, and a modiﬁed system of equations to solve
the optimization problem in two variables for regression analytically.
Note that the reasoning only applies to SV regression with
the ε insensitive loss function—for most other convex cost functionsanexplicitsolutionoftherestrictedquadraticprogramming
problem is impossible. Yet, one could derive an analogous nonquadratic convex optimization problem for general cost functions but at the expense of having to solve it numerically.
The exposition proceeds as follows: ﬁrst one has to derive
the (modiﬁed) boundary conditions for the constrained 2 indices
(i, j) subproblem in regression, next one can proceed to solve the
optimization problem analytically, and ﬁnally one has to check,
which part of the selection rules have to be modiﬁed to make
the approach work for regression. Since most of the content is
fairly technical it has been relegated to Appendix C.
The main difference in implementations of SMO for regression can be found in the way the constant offset b is determined
 and which criterion is used to select a new
set of variables. We present one such strategy in Appendix C.3.
However, since selection strategies are the focus of current research we recommend that readers interested in implementing
the algorithm make sure they are aware of the most recent developments in this area.
Finally, we note that just as we presently describe a generalization of SMO to regression estimation, other learning problems
can also beneﬁt from the underlying ideas. Recently, a SMO
algorithm for training novelty detection systems (i.e. one-class
classiﬁcation) has been proposed .
6. Variations on a theme
There exists a large number of algorithmic modiﬁcations of the
SV algorithm, to make it suitable for speciﬁc settings (inverse
problems, semiparametric settings), different ways of measuring
capacity and reductions to linear programming (convex combinations) and different ways of controlling capacity. We will
mention some of the more popular ones.
6.1. Convex combinations and ℓ1-norms
All the algorithms presented so far involved convex, and at
best, quadratic programming. Yet one might think of reducing
the problem to a case where linear programming techniques
can be applied. This can be done in a straightforward fashion
 forbothSVpatternrecognitionandregression.
The key is to replace (35) by
Rreg[ f ] := Remp[ f ] + λ∥α∥1
where ∥α∥1 denotes the ℓ1 norm in coefﬁcient space. Hence one
uses the SV kernel expansion (11)
αik(xi, x) + b
with a different way of controlling capacity by minimizing
Rreg[ f ] = 1
c(xi, yi, f (xi)) + λ
Smola and Sch¨olkopf
For the ε-insensitive loss function this leads to a linear programming problem. In the other cases, however, the problem still stays
a quadratic or general convex one, and therefore may not yield
the desired computational advantage. Therefore we will limit
ourselves to the derivation of the linear programming problem
in the case of | · |ε cost function. Reformulating (59) yields
subject to
j)k(x j, xi) −b ≤ε + ξi
j)k(x j, xi) + b −yi ≤ε + ξ ∗
i , ξi, ξ ∗
Unlike in the classical SV case, the transformation into its dual
does not give any improvement in the structure of the optimization problem. Hence it is best to minimize Rreg[ f ] directly, which
canbeachievedbyalinearoptimizer, .
In a similar variant of the linear SV approach is used to estimate densities on a line. One can show
 that one may obtain bounds on the generalization error which exhibit even better rates (in terms of the
entropy numbers) than the classical SV case .
6.2. Automatic tuning of the insensitivity tube
Besides standard model selection issues, i.e. how to specify the
trade-off between empirical error and model capacity there also
exists the problem of an optimal choice of a cost function. In
particular, for the ε-insensitive cost function we still have the
problem of choosing an adequate parameter ε in order to achieve
good performance with the SV machine.
Smola et al. show the existence of a linear dependency between the noise level and the optimal ε-parameter for
SV regression. However, this would require that we know something about the noise model. This knowledge is not available in
general. Therefore, albeit providing theoretical insight, this ﬁnding by itself is not particularly useful in practice. Moreover, if we
really knew the noise model, we most likely would not choose
the ε-insensitive cost function but the corresponding maximum
likelihood loss function instead.
There exists, however, a method to construct SV machines
that automatically adjust ε and moreover also, at least asymptotically, have a predetermined fraction of sampling points as SVs
 . We modify (35) such that ε becomes a
variable of the optimization problem, including an extra term in
the primal objective function which attempts to minimize ε. In
other words
minimize Rν[ f ] := Remp[ f ] + λ
2∥w∥2 + νε
for some ν > 0. Hence (42) becomes (again carrying out the
usual transformation between λ, ℓand C)
( ˜c(ξi) + ˜c(ξ ∗
i )) + ℓνε
subject to
yi −⟨w, xi⟩−b ≤ε + ξi
⟨w, xi⟩+ b −yi ≤ε + ξ ∗
We consider the standard |·|ε loss function. Computing the dual
of (62) yields
i )(α j −α∗
j)k(xi, x j)
subject to
Note that the optimization problem is thus very similar to the ε-
SV one: the target function is even simpler (it is homogeneous),
but there is an additional constraint. For information on how this
affects the implementation .
Besides having the advantage of being able to automatically
determine ε (63) also has another advantage. It can be used to
pre–specify the number of SVs:
Theorem 9 .
1. ν is an upper bound on the fraction of errors.
2. ν is a lower bound on the fraction of SVs.
3. Suppose the data has been generated iid from a distribution
p(x, y) = p(x)p(y | x) with a continuous conditional distribution p(y | x). With probability 1, asymptotically, ν equals
the fraction of SVs and the fraction of errors.
Essentially, ν-SV regression improves upon ε-SV regression by
allowing the tube width to adapt automatically to the data. What
is kept ﬁxed up to this point, however, is the shape of the tube.
One can, however, go one step further and use parametric tube
models with non-constant width, leading to almost identical optimization problems .
Combining ν-SV regression with results on the asymptotical
optimal choice of ε for a given noise model 
leads to a guideline how to adjust ν provided the class of noise
models (e.g. Gaussian or Laplacian) is known.
Remark 10 (Optimal choice of ν).
Denote by p a probability
density with unit variance, and by P a famliy of noise models
generated from p by P := {p|p = 1
σ )}. Moreover assume
A tutorial on support vector regression
Fig. 5. Optimal ν and ε for various degrees of polynomial additive
that the data were drawn iid from p(x, y) = p(x)p(y −f (x))
with p(y −f (x)) continuous. Then under the assumption of
uniform convergence, the asymptotically optimal value of ν is
ε := argmin
(p(−τ) + p(τ))−2
For polynomial noise models, i.e. densities of type exp(−|ξ|p)
one may compute the corresponding (asymptotically) optimal
values of ν. They are given in Fig. 5. For further details see
 ; an experimental validation
has been given by Chalimourda, Sch¨olkopf and Smola .
We conclude this section by noting that ν-SV regression is
related to the idea of trimmed estimators. One can show that the
regression is not inﬂuenced if we perturb points lying outside the
tube. Thus, the regression is essentially computed by discarding
a certain fraction of outliers, speciﬁed by ν, and computing the
regression estimate from the remaining points ). Finally the feature
map seems to defy the curse of dimensionality 
by making problems seemingly easier yet reliable via a map into
some even higher dimensional space.
In this section we focus on the connections between SV
methods and previous techniques like Regularization Networks
 .9 In particular we will show
that SV machines are essentially Regularization Networks (RN)
with a clever choice of cost functions and that the kernels are
Green’s function of the corresponding regularization operators.
For a full exposition of the subject the reader is referred to Smola,
Sch¨olkopf and M¨uller .
7.1. Regularization networks
Let us brieﬂy review the basic concepts of RNs. As in (35)
we minimize a regularized risk functional. However, rather than
enforcing ﬂatness in feature space we try to optimize some
smoothness criterion for the function in input space. Thus we
Rreg[ f ] := Remp[ f ] + λ
Here P denotes a regularization operator in the sense of
Tikhonov and Arsenin , i.e. P is a positive semideﬁnite
operator mapping from the Hilbert space H of functions f under
consideration to a dot product space D such that the expression
⟨P f · Pg⟩is well deﬁned for f, g ∈H. For instance by choosing a suitable operator that penalizes large variations of f one
can reduce the well–known overﬁtting effect. Another possible
setting also might be an operator P mapping from L2(Rn) into
some Reproducing Kernel Hilbert Space (RKHS) .
Using an expansion of f in terms of some symmetric function
k(xi, x j) (note here, that k need not fulﬁll Mercer’s condition
and can be chosen arbitrarily since it is not used to deﬁne a
regularization term),
αik(xi, x) + b,
and the ε-insensitive cost function, this leads to a quadratic programming problem similar to the one for SVs. Using
Di j := ⟨(Pk)(xi, .) · (Pk)(x j, .)⟩
we get α = D−1K(β −β∗), with β, β∗being the solution of
2(β∗−β)⊤KD−1K(β∗−β)
−(β∗−β)⊤y −ε
subject to
i ∈[0, C].
Smola and Sch¨olkopf
Unfortunately, this setting of the problem does not preserve sparsity in terms of the coefﬁcients, as a potentially sparse decomposition in terms of βi and β∗
i is spoiled by D−1K, which is not
in general diagonal.
7.2. Green’s functions
Comparing (10) with (67) leads to the question whether and under which condition the two methods might be equivalent and
therefore also under which conditions regularization networks
might lead to sparse decompositions, i.e. only a few of the expansion coefﬁcients αi in f would differ from zero. A sufﬁcient
condition is D = K and thus KD−1K = K (if K does not have
full rank we only need that KD−1K = K holds on the image of
k(xi, x j) = ⟨(Pk)(xi, .) · (Pk)(x j, .)⟩
Our goal now is to solve the following two problems:
1. Given a regularization operator P, ﬁnd a kernel k such that a
SV machine using k will not only enforce ﬂatness in feature
space, but also correspond to minimizing a regularized risk
functional with P as regularizer.
2. Given an SV kernel k, ﬁnd a regularization operator P such
that a SV machine using this kernel can be viewed as a Regularization Network using P.
These two problems can be solved by employing the concept
of Green’s functions as described in Girosi, Jones and Poggio
 . These functions were introduced for the purpose of solving differential equations. In our context it is sufﬁcient to know
that the Green’s functions Gxi(x) of P∗P satisfy
(P∗PGxi)(x) = δxi(x).
Here,δxi(x)istheδ-distribution(nottobeconfusedwiththeKronecker symbol δi j) which has the property that ⟨f ·δxi⟩= f (xi).
The relationship between kernels and regularization operators is
formalized in the following proposition:
Proposition 1 .
be a regularization operator, and G be the Green’s function of
P∗P. Then G is a Mercer Kernel such that D = K. SV machines
using G minimize risk functional (64) with P as regularization
In the following we will exploit this relationship in both ways:
to compute Green’s functions for a given regularization operator
P and to infer the regularizer, given a kernel k.
7.3. Translation invariant kernels
Let us now more speciﬁcally consider regularization operators
ˆP that may be written as multiplications in Fourier space
⟨P f · Pg⟩=
˜f (ω) ˜g(ω)
with ˜f (ω) denoting the Fourier transform of f (x), and P(ω) =
P(−ω) real valued, nonnegative and converging to 0 for |ω| →
∞and  := supp[P(ω)]. Small values of P(ω) correspond to
a strong attenuation of the corresponding frequencies. Hence
small values of P(ω) for large ω are desirable since high frequency components of
˜f correspond to rapid changes in f .
P(ω) describes the ﬁlter properties of P∗P. Note that no attenuation takes place for P(ω) = 0 as these frequencies have been
excluded from the integration domain.
For regularization operators deﬁned in Fourier Space by (70)
one can show by exploiting P(ω) = P(−ω) = P(ω) that
G(xi, x) =
Rn eiω(xi−x)P(ω) dω
is a corresponding Green’s function satisfying translational invariance, i.e.
G(xi, x j) = G(xi −x j)
˜G(ω) = P(ω).
This provides us with an efﬁcient tool for analyzing SV kernels
and the types of capacity control they exhibit. In fact the above
is a special case of Bochner’s theorem stating
that the Fourier transform of a positive measure constitutes a
positive Hilbert Schmidt kernel.
Example 2 (Gaussian kernels).
Following the exposition of
Yuille and Grzywacz as described in Girosi, Jones and
Poggio , one can see that for
m!2m ( ˆOm f (x))2
with ˆO2m = m and ˆO2m+1 = ∇m,  being the Laplacian
and ∇the Gradient operator, we get Gaussians kernels (31).
Moreover, we can provide an equivalent representation of P
in terms of its Fourier properties, i.e. P(ω) = e−σ2∥ω∥2
multiplicative constant.
Training an SV machine with Gaussian RBF kernels corresponds to minimizing the speciﬁc cost function with a regularization operator of type (73). Recall that (73)
means that all derivatives of f are penalized (we have a pseudodifferential operator) to obtain a very smooth estimate. This also
explains the good performance of SV machines in this case, as it
is by no means obvious that choosing a ﬂat function in some high
dimensional space will correspond to a simple function in low
dimensional space, as shown in Smola, Sch¨olkopf and M¨uller
 for Dirichlet kernels.
The question that arises now is which kernel to choose. Let
us think about two extreme situations.
1. Suppose we already knew the shape of the power spectrum
Pow(ω) of the function we would like to estimate. In this case
we choose k such that ˜k matches the power spectrum (Smola
2. If we happen to know very little about the given data a general smoothness assumption is a reasonable choice. Hence
A tutorial on support vector regression
we might want to choose a Gaussian kernel. If computing
time is important one might moreover consider kernels with
compact support, e.g. using the Bq–spline kernels (cf. (32)).
This choice will cause many matrix elements ki j = k(xi −x j)
to vanish.
The usual scenario will be in between the two extreme cases and
we will have some limited prior knowledge available. For more
information on using prior knowledge for choosing kernels .
7.4. Capacity control
All the reasoning so far was based on the assumption that there
exist ways to determine model parameters like the regularization
constant λ or length scales σ of rbf–kernels. The model selection issue itself would easily double the length of this review
and moreover it is an area of active and rapidly moving research.
Therefore we limit ourselves to a presentation of the basic concepts and refer the interested reader to the original publications.
It is important to keep in mind that there exist several fundamentally different approaches such as Minimum Description
Length which is
based on the idea that the simplicity of an estimate, and therefore
also its plausibility is based on the information (number of bits)
needed to encode it such that it can be reconstructed.
Bayesian estimation, on the other hand, considers the posterior probability of an estimate, given the observations X =
{(x1, y1), . . . (xℓ, yℓ)}, an observation noise model, and a prior
probability distribution p( f ) over the space of estimates
(parameters). It is given by Bayes Rule p( f | X)p(X)
p(X | f )p( f ). Since p(X) does not depend on f , one can maximize p(X | f )p( f ) to obtain the so-called MAP estimate.10 As
a rule of thumb, to translate regularized risk functionals into
Bayesian MAP estimation schemes, all one has to do is to consider exp(−Rreg[ f ]) = p( f | X). For a more detailed discussion
 .
A simple yet powerful way of model selection is cross validation. This is based on the idea that the expectation of the error
on a subset of the training sample not used during training is
identical to the expected error itself. There exist several strategies such as 10-fold crossvalidation, leave-one out error (ℓ-fold
crossvalidation), bootstrap and derived algorithms to estimate
the crossvalidation error itself for further details.
Finally, one may also use uniform convergence bounds such
as the ones introduced by Vapnik and Chervonenkis . The
basic idea is that one may bound with probability 1 −η (with
η > 0) the expected risk R[ f ] by Remp[ f ] + (F, η), where
 is a conﬁdence term depending on the class of functions F.
Several criteria for measuring the capacity of F exist, such as the
VC-Dimension which, in pattern recognition problems, is given
by the maximum number of points that can be separated by the
function class in all possible ways, the Covering Number which
is the number of elements from F that are needed to cover F with
accuracy of at least ε, Entropy Numbers which are the functional
inverse of Covering Numbers, and many more variants thereof
 for a
comprehensive overview, Sch¨olkopf, Burges and Smola 
for a snapshot of the current state of the art, Vapnik for an
overview on statistical learning theory, or Cristianini and Shawe-
Taylor for an introductory textbook). Still the authors
hope that this work provides a not overly biased view of the state
of the art in SV regression research. We deliberately omitted
(among others) the following topics.
8.1. Missing topics
Mathematical programming: Starting from a completely different perspective algorithms have been developed that are similar in their ideas to SV machines. A good primer might
be . . A
comprehensive discussion of connections between mathematical programming and SV machines has been given by
 .
Density estimation: with SV machines . There one makes use of the fact that the cumulative distribution function is monotonically increasing,
and that its values can be predicted with variable conﬁdence
which is adjusted by selecting different values of ε in the loss
Dictionaries: were originally introduced in the context of
wavelets by to allow
for a large class of basis functions to be considered simultaneously, e.g. kernels with different widths. In the standard SV
case this is hardly possible except by deﬁning new kernels as
linear combinations of differently scaled ones: choosing the
regularization operator already determines the kernel completely . Hence one has to resort to linear programming .
Applications: The focus of this review was on methods and
theory rather than on applications. This was done to limit
the size of the exposition. State of the art, or even record
performance was reported in M¨uller et al. , Drucker
et al. , Stitson et al. and Mattera and Haykin
Smola and Sch¨olkopf
In many cases, it may be possible to achieve similar performance with neural network methods, however, only if
many parameters are optimally tuned by hand, thus depending largely on the skill of the experimenter. Certainly, SV
machines are not a “silver bullet.” However, as they have
only few critical parameters (e.g. regularization and kernel
width), state-of-the-art results can be achieved with relatively
little effort.
8.2. Open issues
Being a very active ﬁeld there exist still a number of open issues that have to be addressed by future research. After that
the algorithmic development seems to have found a more stable stage, one of the most important ones seems to be to ﬁnd
tight error bounds derived from the speciﬁc properties of kernel functions. It will be of interest in this context, whether
SV machines, or similar approaches stemming from a linear programming regularizer, will lead to more satisfactory
Moreover some sort of “luckiness framework” for multiple model selection parameters, similar to
multiple hyperparameters and automatic relevance detection in
Bayesian statistics , will have to
be devised to make SV machines less dependent on the skill of
the experimenter.
It is also worth while to exploit the bridge between regularization operators, Gaussian processes and priors ) to state Bayesian risk bounds for SV machines in order
to compare the predictions with the ones from VC theory. Optimization techniques developed in the context of SV machines
also could be used to deal with large datasets in the Gaussian
process settings.
Prior knowledge appears to be another important question in
SV regression. Whilst invariances could be included in pattern
recognition in a principled way via the virtual SV mechanism
and restriction of the feature space , it is still not clear how (probably) more
subtle properties, as required for regression, could be dealt with
efﬁciently.
Reduced set methods also should be considered for speeding
up prediction (and possibly also training) phase for large datasets
 . This topic is of great
importance as data mining applications require algorithms that
are able to deal with databases that are often at least one order of
magnitude larger (1 million samples) than the current practical
size for SV regression.
Many more aspects such as more data dependent generalization bounds, efﬁcient training algorithms, automatic kernel selection procedures, and many techniques that already have made
their way into the standard neural networks toolkit, will have to
be considered in the future.
Readers who are tempted to embark upon a more detailed
exploration of these topics, and to contribute their own ideas to
this exciting ﬁeld, may ﬁnd it useful to consult the web page
www.kernel-machines.org.
Appendix A: Solving the interior-point
A.1. Path following
Ratherthantryingtosatisfy(53)directlywewillsolveamodiﬁed
version thereof for some µ > 0 substituted on the rhs in the ﬁrst
place and decrease µ while iterating.
for all i ∈[1 . . . n].
Still it is rather difﬁcult to solve the nonlinear system of equations (51), (52), and (74) exactly. However we are not interested
in obtaining the exact solution to the approximation (74). Instead, we seek a somewhat more feasible solution for a given µ,
then decrease µ and repeat. This can be done by linearizing the
above system and solving the resulting equations by a predictor–
corrector approach until the duality gap is small enough. The
advantage is that we will get approximately equal performance
as by trying to solve the quadratic system directly, provided that
the terms in 2 are small enough.
A(α + α) = b
α + α −g −g = l
α + α + t + t = u
2∂αq(α) + 1
αq(α)α −(A(y + y))⊤
+ s + s = z + z
(gi + gi)(zi + zi) = µ
(si + si)(ti + ti) = µ
Solving for the variables in  we get
Aα = b −Aα =: ρ
α −g = l −α + g =: ν
α + t = u −α −t =: τ
(Ay)⊤+ z −s −1
= c −(Ay)⊤+ s −z + 1
2∂αq(α) =: σ
g−1zg + z = µg−1 −z −g−1gz =: γz
t−1st + s = µt−1 −s −t−1ts =: γs
where g−1 denotes the vector (1/g1, . . . , 1/gn), and t analogously. Moreover denote g−1z and t−1s the vector generated
by the componentwise product of the two vectors. Solving for
A tutorial on support vector regression
g, t, z, s we get
g = z−1g(γz −z)
z = g−1z(ˆν −α)
t = s−1t(γs −s)
s = t−1s(α −ˆτ)
ˆν := ν −z−1gγz
ˆτ := τ −s−1tγs
Now we can formulate the reduced KKT–system for the quadratic case):
 σ −g−1zˆν −t−1s ˆτ
where H := ( 1
αq(α) + g−1z + t−1s).
A.2. Iteration strategies
For the predictor-corrector method we proceed as follows. In
the predictor step solve the system of (75) and (76) with µ = 0
and all -terms on the rhs set to 0, i.e. γz = z, γs = s. The
values in  are substituted back into the deﬁnitions for γz and
γs and (75) and (76) are solved again in the corrector step. As the
quadratic part in (76) is not affected by the predictor–corrector
steps, we only need to invert the quadratic matrix once. This is
done best by manually pivoting for the H part, as it is positive
Nextthevaluesinobtainedbysuchaniterationstepareused
to update the corresponding values in α, s, t, z, . . . . To ensure
that the variables meet the positivity constraints, the steplength
ξ is chosen such that the variables move at most 1 −ε of their
initial distance to the boundaries of the positive orthant. Usually
 one sets ε = 0.05.
Another heuristic is used for computing µ, the parameter determining how much the KKT-conditions should be enforced.
Obviously it is our aim to reduce µ as fast as possible, however
if we happen to choose it too small, the condition of the equations will worsen drastically. A setting that has proven to work
robustly is
µ = ⟨g, z⟩+ ⟨s, t⟩
The rationale behind (77) is to use the average of the satisfaction of the KKT conditions (74) as point of reference and then
decrease µ rapidly if we are far enough away from the boundaries of the positive orthant, to which all variables (except y) are
constrained to.
Finally one has to come up with good initial values. Analogously to Vanderbei we choose a regularized version of
(76) in order to determine the initial conditions. One solves
and subsequently restricts the solution to a feasible set
g = min(α −l, u)
t = min(u −α, u)
2∂αq(α) + c −(Ay)⊤
2∂αq(α) −c + (Ay)⊤
(·) denotes the Heavyside function, i.e. (x) = 1 for x > 0
and (x) = 0 otherwise.
A.3. Special considerations for SV regression
The algorithm described so far can be applied to both SV pattern
recognition and regression estimation. For the standard setting
in pattern recognition we have
αiα j yi y jk(xi, x j)
and consequently ∂αiq(α) = 0, ∂2
αiα jq(α) = yi y jk(xi, x j), i.e.
the Hessian is dense and the only thing we can do is compute
its Cholesky factorization to compute (76). In the case of SV regression, however we have (with α := (α1, . . . , αℓ, α∗
1, . . . , α∗
i )(α j −α∗
j)k(xi, x j)
T (αi) + T (α∗
and therefore
αiα jq(α) = k(xi, x j) + δi j
j q(α) = −k(xi, x j)
j q(α), ∂2
i α jq(α) analogously. Hence we are dealing with
a matrix of type M := [ K+D −K
K+D′ ] where D, D′ are diagonal
matrices. By applying an orthogonal transformation M can be
inverted essentially by inverting an ℓ× ℓmatrix instead of a
2ℓ× 2ℓsystem. This is exactly the additional advantage one
can gain from implementing the optimization algorithm directly
instead of using a general purpose optimizer. One can show that
for practical implementations one can solve optimization problems using nearly arbitrary convex cost functions as efﬁciently as the special case of
ε-insensitive loss functions.
Finally note that due to the fact that we are solving the primal and dual optimization problem simultaneously we are also
Smola and Sch¨olkopf
computing parameters corresponding to the initial SV optimization problem. This observation is useful as it allows us to obtain
the constant term b directly, namely by setting b = y. for details).
Appendix B: Solving the subset selection
B.1. Subset optimization problem
We will adapt the exposition of Joachims to the case of
regression with convex cost functions. Without loss of generality we will assume ε ̸= 0 and α ∈[0, C] (the other situations
can be treated as a special case). First we will extract a reduced
optimization problem for the working set when all other variables are kept ﬁxed. Denote Sw ⊂{1, . . . , ℓ} the working set
and S f := {1, . . . , ℓ}\Sw the ﬁxed set. Writing (43) as an optimization problem only in terms of Sw yields
i )(α j −α∗
j)⟨xi, x j⟩
j)⟨xi, x j⟩
(−ε(αi + α∗
i ) + C(T (αi) + T (α∗
subject to
αi ∈[0, C]
Hence we only have to update the linear term by the coupling
with the ﬁxed set −
i∈Sw(αi −α∗
j∈S f (α j −α∗
j)⟨xi, x j⟩and
the equality constraint by −
i∈S f (αi −α∗
i ). It is easy to see
that maximizing (83) also decreases (43) by exactly the same
amount. If we choose variables for which the KKT–conditions
are not satisﬁed the overall objective function tends to decrease
whilst still keeping all variables feasible. Finally it is bounded
from below.
Even though this does not prove convergence ) this algorithm
proves very useful in practice. It is one of the few methods ) that can deal with problems
whose quadratic part does not completely ﬁt into memory. Still
in practice one has to take special precautions to avoid stalling
of convergence 
indicate that under certain conditions a proof of convergence is
possible). The crucial part is the one of Sw.
B.2. A note on optimality
For convenience the KKT conditions are repeated in a slightly
modiﬁed form. Denote ϕi the error made by the current estimate
at sample xi, i.e.
ϕi := yi −f (xi) = yi −
k(xi, x j)(αi −α∗
Rewriting the feasibility conditions (52) in terms of α yields
2∂αi T (αi) + ε −ϕi + si −zi = 0
i ) + ε + ϕi + s∗
for all i ∈{1, . . . , m} with zi, z∗
i , si, s∗
≥0. A set of dual
feasible variables z, s is given by
2∂αi T (αi) + ε −ϕi, 0
2∂αi T (αi) + ε −ϕi, 0
i ) + ε + ϕi, 0
i ) + ε + ϕi, 0
Consequently the KKT conditions (53) can be translated into
(C −αi)si = 0
All variables αi, α∗
i violating some of the conditions of (87) may
be selected for further optimization. In most cases, especially in
the initial stage of the optimization algorithm, this set of patterns is much larger than any practical size of Sw. Unfortunately
Osuna, Freund and Girosi contains little information on
how to select Sw. The heuristics presented here are an adaptation
of Joachims to regression. See also Lin for details
on optimization for SVR.
B.3. Selection rules
Similarly to a merit function approach the
idea is to select those variables that violate (85) and (87) most,
thus contribute most to the feasibility gap. Hence one deﬁnes a
score variable ζi by
ζi := gizi + siti
= αizi + α∗
i + (C −αi)si + (C −α∗
By construction,
i ζi is the size of the feasibility gap (cf. (56)
for the case of ε-insensitive loss). By decreasing this gap, one
approaches the the solution (upper bounded by the primal objective and lower bounded by the dual objective function). Hence,
the selection rule is to choose those patterns for which ζi is
A tutorial on support vector regression
largest. Some algorithms use
i := αi(zi) + α∗
+ (C −αi)(si) + (C −α∗
i := (αi)zi + (α∗
+ (C −αi)si +  to variables at the boundaries, thus effectively solving smaller subproblems, or completely removing
the corresponding patterns from the training set while accounting for their couplings can signiﬁcantly decrease the size of the problem one has to solve and
thus result in a noticeable speedup. Also caching of already computed entries of the
dot product matrix may have a signiﬁcant impact on the
performance.
Appendix C: Solving the SMO equations
C.1. Pattern dependent regularization
Consider the constrained optimization problem (83) for two indices, say (i, j). Pattern dependent regularization means that Ci
may be different for every pattern (possibly even different for
i ). Since at most two variables may become nonzero
at the same time and moreover we are dealing with a constrained optimization problem we may express everything in
terms of just one variable. From the summation constraint we
i ) + (α j −α∗
for regression. Exploiting α(∗)
j ] yields α(∗)
This is taking account of the fact that there may be only four
different pairs of nonzero variables: (αi, α j), (α∗
i , α j), (αi, α∗
j). For convenience deﬁne an auxiliary variables s
such that s = 1 in the ﬁrst and the last case and s = −1 otherwise.
max(0, γ −C j)
max(0, γ )
min(Ci, γ )
min(Ci, C∗
max(0, −γ )
max(0, −γ −C∗
i , −γ + C j)
C.2. Analytic solution for regression
Next one has to solve the optimization problem analytically. We
make use of (84) and substitute the values of φi into the reduced
optimization problem (83). In particular we use
i )Ki j = ϕi + b +
Moreover with the auxiliary variables γ = αi −α∗
i +α j −α∗
η := (Kii + K j j −2Ki j) one obtains the following constrained
optimization problem in i (after eliminating j, ignoring terms
independent of α j, α∗
j and noting that this only holds for αiα∗
i )2η −ε(αi + α∗
φi −φ j + η
subject to
∈[L(∗), H (∗)].
The unconstrained maximum of (92) with respect to αi or α∗
can be found below.
+ η−1(ϕi −ϕ j)
+ η−1(ϕi −ϕ j −2ε)
old −η−1(ϕi −ϕ j + 2ε)
old −η−1(ϕi −ϕ j)
The problem is that we do not know beforehand which of the
four quadrants (I)–(IV) contains the solution. However, by considering the sign of γ we can distinguish two cases: for γ > 0
only (I)–(III) are possible, for γ < 0 the coefﬁcients satisfy one
of the cases (II)–(IV). In case of γ = 0 only (II) and (III) have
to be considered. See also the diagram below.
For γ > 0 it is best to start with quadrant (I), test whether the
unconstrained solution hits one of the boundaries L, H and if so,
probe the corresponding adjacent quadrant (II) or (III). γ < 0
can be dealt with analogously.
Due to numerical instabilities, it may happen that η < 0. In
that case η should be set to 0 and one has to solve (92) in a linear
fashion directly.11
Smola and Sch¨olkopf
C.3. Selection rule for regression
Finally, one has to pick indices (i, j) such that the objective
functionismaximized.Again,thereasoningofSMO(Platt1999,
Section 12.2.2) for classiﬁcation will be mimicked. This means
that a two loop approach is chosen to maximize the objective
function. The outer loop iterates over all patterns violating the
KKT conditions, ﬁrst only over those with Lagrange multipliers
neither on the upper nor lower boundary, and once all of them
are satisﬁed, over all patterns violating the KKT conditions, to
ensure self consistency on the complete dataset.12 This solves
the problem of choosing i.
Now for j: To make a large step towards the minimum, one
looks for large steps in αi. As it is computationally expensive to
compute η for all possible pairs (i, j) one chooses the heuristic to
maximize the absolute value of the numerator in the expressions
for αi and α∗
i , i.e. |ϕi −ϕ j| and |ϕi −ϕ j ± 2ε|. The index j
corresponding to the maximum absolute value is chosen for this
If this heuristic happens to fail, in other words if little progress
is made by this choice, all other indices j are looked at in the
following way:
1. All indices j corresponding to non–bound examples are
looked at, searching for an example to make progress on.
2. In the case that the ﬁrst heuristic was unsuccessful, all
other samples are analyzed until an example is found where
progress can be made.
3. If both previous steps fail proceed to the next i.
For a more detailed discussion . Unlike interior
point algorithms SMO does not automatically provide a value
for b. However this can be chosen like in Section 1.4 by having
a close look at the Lagrange multipliers α(∗)
C.4. Stopping criteria
By essentially minimizing a constrained primal optimization
problem one cannot ensure that the dual objective function increases with every iteration step.13 Nevertheless one knows that
the minimum value of the objective function lies in the interval
[dual objectivei, primal objectivei] for all steps i, hence also in
the interval [(max j≤i dual objective j), primal objectivei]. One
uses the latter to determine the quality of the current solution.
The calculation of the primal objective function from the prediction errors is straightforward. One uses
i )(α j −α∗
j)ki j = −
i )(ϕi + yi −b),
i.e. the deﬁnition of ϕi to avoid the matrix–vector multiplication
with the dot product matrix.
Acknowledgments
This work has been supported in part by a grant of the DFG
(Ja 379/71, Sm 62/1). The authors thank Peter Bartlett, Chris
Burges, Stefan Harmeling, Olvi Mangasarian, Klaus-Robert
M¨uller, Vladimir Vapnik, Jason Weston, Robert Williamson, and
Andreas Ziehe for helpful discussions and comments.
1. Our use of the term ‘regression’ is somewhat lose in that it also includes
cases of function estimation where one minimizes errors other than the mean
square loss. This is done mainly for historical reasons .
2. A similar approach, however using linear instead of quadratic programming,
was taken at the same time in the USA, mainly by Mangasarian for an overview over other ways of specifying ﬂatness of
such functions.
4. This is true as long as the dimensionality of w is much higher than the
number of observations. If this is not the case, specialized methods can
offer considerable computational savings .
5. The table displays CT(α) instead of T (α) since the former can be plugged
directly into the corresponding optimization equations.
6. Thehighpricetagusuallyisthemajordeterrentfornotusingthem.Moreover
one has to bear in mind that in SV regression, one may speed up the solution
considerably by exploiting the fact that the quadratic form has a special
structure or that there may exist rank degeneracies in the kernel matrix
7. For large and noisy problems (e.g. 100.000 patterns and more with a substantial fraction of nonbound Lagrange multipliers) it is impossible to solve the
problem exactly: due to the size one has to use subset selection algorithms,
hence joint optimization over the training set is impossible. However, unlike
in Neural Networks, we can determine the closeness to the optimum. Note
that this reasoning only holds for convex cost functions.
8. A similar technique was employed by Bradley and Mangasarian in
the context of linear programming in order to deal with large datasets.
9. Due to length constraints we will not deal with the connection between
Gaussian Processes and SVMs. See Williams for an excellent
10. Strictly speaking, in Bayesian estimation one is not so much concerned about
the maximizer ˆf of p( f | X) but rather about the posterior distribution of
11. Negative values of η are theoretically impossible since k satisﬁes Mercer’s
condition: 0 ≤∥(xi) −(x j)∥2 = Kii + K j j −2Ki j = η.
12. It is sometimes useful, especially when dealing with noisy data, to iterate
over the complete KKT violating dataset already before complete self consistency on the subset has been achieved. Otherwise much computational
resources are spent on making subsets self consistent that are not globally
self consistent. This is the reason why in the pseudo code a global loop
is initiated already when only less than 10% of the non bound variables
13. It is still an open question how a subset selection optimization algorithm
could be devised that decreases both primal and dual objective function
at the same time. The problem is that this usually involves a number of
dual variables of the order of the sample size, which makes this attempt
unpractical.