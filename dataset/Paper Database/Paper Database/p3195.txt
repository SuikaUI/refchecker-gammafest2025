Received January 7, 2020, accepted January 20, 2020, date of publication February 6, 2020, date of current version February 14, 2020.
Digital Object Identifier 10.1109/ACCESS.2020.2972009
A Multiple Classifiers System for Anomaly
Detection in Credit Card Data With
Unbalanced and Overlapped Classes
SURAYA NURAIN KALID
1, KENG-HOONG NG1, GEE-KOK TONG1, AND KOK-CHIN KHOR
1Faculty of Computing and Informatics, Multimedia University, Cyberjaya 63100, Malaysia
2Lee Kong Chian Faculty of Engineering Science, Universiti Tunku Abdul Rahman, Kajang 43000, Malaysia
Corresponding author: Kok-Chin Khor ( )
This work was supported by the Ministry of Higher Education Malaysia under Grant FRGS/1/2019/SS01/MMU/03/11.
ABSTRACT Frauds and default payments are two major anomalies in credit card transactions. Researchers
have been vigorously ﬁnding solutions to tackle them and one of the solutions is to use data mining
approaches. However, the collected credit card data can be quite a challenge for researchers. This is because
of the data characteristics that contain: (i) unbalanced class distribution, and (ii) overlapping of class samples.
Both characteristics generally cause low detection rates for the anomalies that are minorities in the data.
On top of that, the weakness of general learning algorithms contributes to the difﬁculties of classifying
the anomalies as the algorithms generally bias towards the majority class samples. In this study, we used
a Multiple Classiﬁers System (MCS) on these two data sets: (i) credit card frauds (CCF), and (ii) credit
card default payments (CCDP). The MCS employs a sequential decision combination strategy to produce
accurate anomaly detection. Our empirical studies show that the MCS outperforms the existing research,
particularly in detecting the anomalies that are minorities in these two credit card data sets.
INDEX TERMS Anomaly detection, credit card, multiple classiﬁers.
I. INTRODUCTION
Credit cards are widely used because they ease our daily
transactions in many ways. However, banks need to take
note of these issues seriously, i.e., (i) the intervention of
unauthorised third parties – frauds, and (ii) the negligence of
repayment by cardholders – default payments.
According to , the global credit card fraud losses have
shown an uptrend, from USD 9.84 billion in the year 2011 to
USD 27.69 billion in the year 2017. It is also reported that
the worldwide credit card fraud is expected to reach a total of
USD 31.67 million by the year 2020. The Malaysian banking
sector also reported a total loss of RM 51.3 million in the
credit card fraud in the year 2016 . It was reported that in
the year 2016, the outstanding balance of credit card holders
in Malaysia is RM36.9 million and 12.8% of them failed to
pay the minimum payment of the balance . The Central
Bank of Malaysia (Bank Negara Malaysia) also reported that
The associate editor coordinating the review of this manuscript and
approving it for publication was Zhe Xiao
the high outstanding balance by the credit card holders, has
triggered an alarm to the Malaysian government .
Researchers have been vigorously ﬁnding ways to tackle
both issues, including data mining. Data mining is not an
option or a trend, but more of a necessity that the banking
sector should invest in , . However, banking data such as
credit card fraud and default payment are quite of a challenge
to data mining researchers. This is because the data usually
exhibited characteristics: (i) unbalanced class distribution,
and (ii) overlapping of class samples.
The size of the important classes in the data, i.e., fraud
and default payment, are usually the minorities. Generally,
it is easy for learning algorithms to ﬁnd their regularities
if they have sufﬁcient records. But when their numbers are
very small, ﬁnding their regularities becomes difﬁcult and so
as generalising their actual decision regions using learning
algorithms – . It adds difﬁculty if their attribute values
are overlapped by a large amount of normal transactions.
In general, the performance of learning algorithms will be less
affected if the minority classes are linearly separable, even
though the data involved are highly unbalanced – .
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
FIGURE 1. The decision boundary that separates the majority class and
minority class samples.
Another aspect to take note is the weakness of learning algorithms in assessing its own classiﬁcation capability , – . A commonly used metric to evaluate data
mining results, which computes the number of correctly classiﬁed records, is the classiﬁcation accuracy. The learning
algorithms generally assume that positive and negative samples are roughly equal in data. Therefore, many learning algorithms aim to maximize accuracy, which lean more towards
majority classes and against minorities. Subsequently, they
are unlikely to produce satisfactory results when dealing with
unbalanced data sets, especially the minority classes.
In a nutshell, the characteristics of the credit card data
and learning algorithms have caused the problem of low
anomaly detection rates. Therefore, single classiﬁers may not
give good classiﬁcation results. Hence, this research aims to
design a Multiple Classiﬁer System (MCS) for mitigating the
low anomaly detection rate problem on the credit card data
sets utilised in this study.
II. LITERATURE REVIEW
A. UNBALANCED CLASS DISTRIBUTION
A data set is unbalanced in its class distribution when one or
more classes have a much greater number of samples than the
other classes , . In reality, the number of anomalies
are very much fewer than normal transactions in the credit
card data.
The unbalanced class distribution problem has been the
focus of many researchers – . This is due to the high
probability of producing errors in classiﬁcation. The unbalanced class distribution is as illustrated in Fig. 1. The curved
line that separates between two classes is called the decision
boundary, which separates the region of different classes .
Classifying an unbalanced data set may lead to the low
classiﬁcation rate problem – . According to , general learning algorithms are able to give high accuracies on
balanced data sets, but not on unbalanced data sets. Many
researchers did comparative studies using some popular
learning algorithms on unbalanced data sets , , .
In the next section, some selected popular single learning
algorithms shall be discussed. A brief explanation on why
they are weak towards unbalanced data sets shall be given.
FIGURE 2. The illustration of finding the class label for a new sample.
B. LEARNING ALGORITHM
1) NAÏVE BAYES
NB is a simple, yet powerful learning algorithm that uses
the probabilistic method to classify data samples. It assumes
that every attribute is conditionally independent of the other
attributes . It will predict whether a data sample belongs
to one class or another based on the Bayesian Theorem as
P (C | X) = P (X | C) P(C)
Let X be a data sample (evidence) that is described by
multiple attributes. The probability of X belongs to a class C
is calculated as P(C|X). P(C) is the initial or prior probability
while P(X|C) is the likelihood or the probability that the
sample data is observed. P(X) is the evidence with a constant
value and therefore can be omitted.
According to , NB is weak against unbalanced data sets
as it biases towards the majority class. As illustrated in Fig. 2,
given two classes of different class distribution (class A 20:
class B 10) and a new sample that needs to be classiﬁed.
Firstly, NB will ﬁnd the prior probability P(C) of each
class; it is assumed that the new sample will belong to Class A
as it has more samples as compared with Class B. Secondly,
the likelihood is calculated based on the number of samples
of each class that is within the vicinity (within the circle) of
the new sample. Lastly, the posterior probability is calculated
by combining the result of the prior probability and the
likelihood of the new sample (refer to (2) and (3)).
Posterior prob. : Class A = 20
20 = 0.100
Posterior prob. : Class B = 10
10 = 0.067
The new sample is classiﬁed as Class A because of the
largest posterior probability. Using this example, it shows that
NB is likely bias towards the majority class.
C4.5 is a popular learning algorithm that uses a divideand-conquer method to build a decision tree from a training
set – . It is popular due to its ability to produce good
classiﬁcation results in a much shorter time. To improve
classiﬁcation performance, it prunes small and deep nodes
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
FIGURE 3. The illustration of finding the class label for a new sample
using KNN.
in the preliminary tree caused by the ‘noises’ contained in
training samples. The advantage of pruning is that it will
decrease the risk of ‘over-ﬁtting’ , , . Over-ﬁtting
refers to a classiﬁer that learns a training data, the details as
well as noises, too well. Due to the inability of such classiﬁer
in generalising the training data well, the classiﬁer is weak in
classifying new or unknown data.
Avoiding over-ﬁtting gives a more precise classiﬁcation
for unknown data , . Nevertheless, the pruning process can also be a disadvantage to unbalanced data sets.
Removing ‘noises’ from such data set may also remove
small and deep nodes of the preliminary tree that belong to
a minority class, thus reducing the coverage for a precise
classiﬁcation , – .
3) K-NEAREST NEIGHBOUR (KNN)
KNN builds the classiﬁer’s function by majority vote of
its local neighbouring data points , . Fig. 3 shows
how KNN identiﬁes the class of a new sample. Suppose the
number of neighbours, k = 5, and the Euclidean distance is
the distance measure. The KNN classiﬁer will ﬁnd the nearest
ﬁve samples to the new sample. The Euclidean distance
between the target sample (x) and the new sample (y) in
an n-dimensional space is calculated using the measure as
per (4), where p is 2.
|xi −yi|p)1/p
Out of ﬁve neighbours, the samples of class A is more
than the samples of class B. Therefore, the new sample is
classiﬁed as class A. This example shows that KNN is likely
bias towards the majority class. The chance of the new sample
to be classiﬁed as class B is relatively low as compared with
4) ARTIFICIAL NEURAL NETWORK (ANN)
ANNs are made up of simple and highly interconnected
nodes that respond to inputs by the dynamic state of the
nodes , . It is made up of layers, i.e., input layer, hidden layer, and output layer, as shown in Fig. 4. These layers
FIGURE 4. The illustration of an ANN model that contains an input layer,
hidden layers, and an output layer.
FIGURE 5. SVM transforms an original feature space into a higher
dimensional feature space for finding a better decision boundary.
are formed using interconnected ‘nodes’ that are associated
with activation functions.
ANNs contain some forms of learning rules that modify
the weights of the connections according to the input patterns – learning by examples , . Similar to other
single classiﬁers, ANNs are also biased towards majority
classes when they involve unbalanced data sets , .
Due to overwhelming samples of majority classes, samples
of minority classes will be imperceptible to ANNs.
5) SUPPORT VECTOR MACHINE (SVM)
classiﬁcation
regression
As shown in Fig. 5, SVM plots each sample as a point in
n-dimensional space, where n is the number of attributes in
a data set. The value of each attribute will be the value of
a particular coordinate. Then, SVM will identify the best
hyper-plane that is able to differentiate classes .
SVM is powerful in getting the best decision boundary
between classes. However, SVM does not work well with
data sets that contain unbalanced class distribution, noises
and overlapping class samples. The parameters in SVM can
be altered to make the classiﬁer more immune to noises and
to work well for balanced data sets. But when it involves
unbalanced data sets, minority class samples may consider
as noises. Therefore, minority class samples will be ignored
completely by SVM .
C. OVERLAPPING OF CLASSES
The other factor that contributes to the low classiﬁcation rate
is the overlapping class samples. Overlapping happens when
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
FIGURE 6. The unbalanced classes and overlapping samples that may
lead to a low classification rate.
the samples are located too close to the decision boundary of
classes and overlapped with each other , .
Based on a systemic study using a set of artiﬁcially generated data sets prepared by , the study result showed
that the degree of overlapping class samples had a strong
correlation with unbalanced class distribution. Fig. 6 shows
the relationship between unbalanced class distribution and
overlapping class samples. When the samples of two classes
are not well distributed, some samples may overlap with each
other. The grey area in Fig. 6 indicates the samples that are
overlapped.
Another reason of overlapping is because the samples in
both classes share almost that same value of attributes. Such
overlapping causes difﬁculties for a classiﬁer to classify
the samples and may eventually lead to a low classiﬁcation
rate , . In the next section, solutions to the unbalanced
class distribution and the overlapping class samples shall be
discussed based on the previous work conducted by other
researchers.
D. MULTIPLE CLASSIFIERS SYSTEM (MCS)
Many researchers attempted different solutions to address the
low classiﬁcation rate problem caused by the two problems,
as discussed in the earlier section – , , . One
of the solutions is MCS. MCS is the combination of a set
of classiﬁers to produce a better prediction. MCS employs
various decision combination strategies that are able to produce a more robust, reliable, efﬁcient recognition and accurate classiﬁcation – . There are three combination
strategies when employing MCS: (i) sequential combination,
(ii) parallel combination, and (iii) hybrid combination.
1) SEQUENTIAL COMBINATION
Using the sequential combination, two or more single classi-
ﬁers process the input data in a sequential manner. As shown
in Fig. 7, the output resulted from a single classiﬁer will then
be used as the input data for the subsequent single classiﬁers.
Usually, simple classiﬁers are utilised ﬁrst, followed by
more accurate and complex classiﬁers . However, this
order can be reversed depending on the needs of the design.
When the prior classiﬁer is unable to accurately classify
one of the class samples, then the sample is given to the
next classiﬁer for further classiﬁcation . An example
FIGURE 7. The sequential combination of MCS. The MCS passes data from
one classifier to another.
of learning algorithm that uses sequential combination is
Boosting .
Reference used AdaBoost on a credit card fraud data
set. The data set is highly unbalanced with only 0.173%
fraud transaction. The authors used Naïve Bayes as the base
classiﬁer. Upon completing the experiment, they obtained
0.999, 0.825 and 0.981 for accuracy, true positive rate and true
negative rate, respectively. However, accuracy is not a good
performance measure as compared to the other two measures
when involving unbalanced data sets. This is because the
accuracy measure favours the majority class.
Reference used a ﬁne-tuned boosting ensemble
approach, which is known as XGBoost, to solve a classiﬁcation problem. The problem was to decide on the granting of
loan application. The data sets used in the experiments were
German credit, Australian credit, Taiwan credit, P2P landing
data set A, and P2P landing data set B. Three of them are
unbalanced and the other two are approximately balanced in
class distribution. The classiﬁer’s performance was measured
using accuracy and the highest accuracy obtained was 0.879.
Using accuracy in their work showed that the authors did not
explicitly address the issues of unbalanced data and overlapping class samples. However, the authors suggested to integrate XGBoost into MCS to further improve the classiﬁcation
performance. MCS is one of the recommended approaches to
consider in data mining research when involving unbalanced
data sets.
Reference also used boosting algorithm, AdaBoost.
M1. The authors boosted three different classiﬁers, which
were Multilayer Perceptron (MLP), Radial Basis Function (RBF), and Naïve Bayes (NB), on the Taiwan credit card
default payment data. To handle the unbalanced data set issue,
the authors reduced the size of majority class samples by
using random under sampling.
2) PARALLEL COMBINATION
The same data are processed by multiple single classi-
ﬁers using the parallel combination, where each classiﬁer is
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
FIGURE 8. The parallel combination of the MCS. All the single classifiers
involved are independent from each other.
independent from the others. The output from all the single
classiﬁers will be combined to get a ﬁnal decision, as shown
in Fig. 8. An example of learning algorithm that uses parallel
combination is Bagging .
Reference used Bagging with Random Forest as the
base classiﬁer. The data set used was about credit card default
payment from a Taiwan bank. The data set is unbalanced
with 28% of it categorised as default-payment. The author
obtained 0.816 for accuracy, 0.371 for TPR, and 0.764 for
Reference used the bagged decision trees to classify
new transactions as fraudulent or legitimate type. The data
set used in the experiment was the UCSD-FICO competition
credit card data set. Out of 97,707 instances, the data set
contains only 2.9% of fraudulent instances. The authors did
some comparison with other single classiﬁers and the result
showed that bagging approach outperformed other single
classiﬁers.
The authors of used Random Forest, an ensemble
method. Random Forest uses the same combination strategy
as Bagging. Similar to , credit card default payment
data set from a Taiwan bank was used. In the experiment,
the authors applied the Correlation Based Feature Selection (CFS) technique to reduce the data set dimension with the
purpose of improving classiﬁcation accuracy. Based on the
experiment result, the authors obtained a good TPR f 0.816.
In the study by , the credit card fraud data set used was
obtained from a European bank with an unbalanced class distribution and overlapping class samples. The data set is highly
unbalanced with only 492 fraud transactions (0.173%). The
authors ﬁrst partitioned and clustered the data set. Then,
a Random Forest framework, with C4.5 as the base classiﬁer,
was trained using the resulted clusters. The ﬁnal result was
determined based on the majority vote. The authors used
AUC as the performance measure. Based on the experiment
result, the author obtained an AUC value of 0.965.
The study by used the similar data set used as .
The data set was bootstrapped such that each resulted data
set had a balanced class distribution. Then, the authors used
an ensemble of Deep Belief Network and applied to each of
the bootstrapped samples. The author used the performance
measures, i.e., accuracy, TPR, TNR, and AUC and obtained
0.906, 0.818, 0.995, and 0.978, respectively.
FIGURE 9. The MCS that utilises the hybrid combination. The sequential
and parallel combinations are put into one architecture.
3) HYBRID COMBINATION
The hybrid combination puts the sequential and parallel combinations into one architecture. Fig. 9 illustrates the hybrid
combination.
In this case, the input data is fed to the ﬁrst classiﬁer. The
output from the ﬁrst classiﬁer will be the input to several
parallel classiﬁers. Then, a single combination function or
classiﬁer will merge the output of the individual parallel
classiﬁers.
Reference used the hybrid combination to classify
credit or loan applicants into good and bad applicants. The
data set used in the experiment is a German credit data
set. The data set has 20 attributes with 700 Good and
300 Bad Applicant data. The hybrid combinations used are
a two-level voting scheme: level I – AdaBoost approach,
and level II – single classiﬁer approach. The authors used
accuracy as the performance measure. Both level I and level II
had achieved an average accuracy of 76.33% and 78.33%,
respectively. However, the evaluation measure is not suitable
as the study involved the unbalanced data set.
Reference also employed a hybrid model using a
combination of AdaBoost and majority voting. ANN and NB
were used as the base classiﬁers for AdaBoost that employed
sequential combination. Then, the ﬁnal result was obtained
using majority voting, which was done in parallel. Similar
to , the credit card fraud data set obtained from the
European bank was used in this experiment. Upon completing
the experiment, the authors were able to obtain an accuracy
of 0.999, a TPR of 0.789 and a TNR 0.999.
E. EVALUATION MEASURES
Given a binary class problem, general learning algorithms
assume that the classes involved are approximately balanced and attempt to maximise its accuracy regardless of
classes , . With a balanced data set, accuracy is the
suitable measure to evaluate the performance of a classiﬁer.
However, when it involves an unbalanced data set, learning
algorithms may bias towards the majority class – .
This may lead to a high accuracy in overall, but a poor
classiﬁcation rate for the minority class.
In this study, the True Positive Rate (TPR) was used to
evaluate the performance of the proposed MCS. By using
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
TABLE 1. The attributes of the Credit card fraud (CCF) data set.
TPR as the evaluation metric, we were able to identify the
classiﬁcation rate for both majority and minority classes.
In this study, we focused on the minority classes: (i) the credit
card frauds and (ii) the credit card default payments.
III. METHODOLOGY
A. DATA SETS OVERVIEW AND
THEIR INHERITED PROBLEMS
Two credit card data sets were utilised in this study to
demonstrate the challenges: (i) overlapping class samples
and (ii) unbalanced class distribution. The general learning
algorithms have difﬁculty in handling these two issues and
caused low detection rates for minority classes.
The ﬁrst data set is the Credit card fraud (CCF) data set
released by . Credit card fraud is an act of gaining unlawful advantage such as performing a variety of unauthorised
transactions using the victim’s credit card account , .
This data set was formed during a big data mining and
fraud detection research between Worldline and the Machine
Learning Group of Université Libre de Bruxelles (ULB).
The data set contains transactions made by European credit
card holders. The data set has a total of 284,807 transactions and it is highly unbalanced with only 492 fraud transactions (0.173%). Further, the data set has a total number
of 31 attributes, as shown in Table 1. Unfortunately, due to
conﬁdentiality, the details of certain attribute are not disclosed. Most attributes of the CCF data set exhibit the scenarios as follows. Fig. 10 (a) shows the pairwise relationship
of attributes V20 and V14. The red crosses are samples of
Class 1 (frauds), while the blue crosses represent samples of
Class 0 (normal transactions). The class distribution is clearly
unbalanced and the samples of both classes are overlapped.
Fig. 10 (b) also displays the same scenario where the plot
involves attributes V24 and V12.
The second data set is the credit card default payment (CCDP) data set , . Default credit card payment
refers to the failure of a credit card holder in performing
the minimum amount of credit card repayment within the
agreed period , . This data set contains the payment
data of credit card holders of a Taiwan bank from April
2005 to September 2005. The CCDP data set is also slightly
unbalanced with a ratio of approximately 3:1 (non-default
payment: default payment). It has a total of 30,000 payment
instances. 23,364 instances belong to ‘no’ class (non-default
payment next month), and 6,636 of them belong to the ‘yes’
class (default payment next month). This means that there
are only 28% of default payment instances out of the whole
FIGURE 10. The visualisation of the CCF data set showing the unbalanced
class distribution and the overlapping class problem between attributes
(a) V20 vs. V14, and (b) V24 vs. V12.
payment data. This data set has a total of 25 attributes. The
detail of each attribute is described in Table 2.
Fig. 11 (a) shows the scatter plot of attributes ‘‘Repayment status in April’’ and ‘‘Amount of previous statement in April’’. The red crosses are samples of class ‘yes’
(non-default payment), while the blue crosses are samples of
class ‘no’ (default payment). The distribution of both classes
is clearly unbalanced and we can see that the blue crosses are
overwhelmed by the red crosses. Fig. 11 (b) shows the scatter
plot of attributes ‘‘Bill statement in August’’ and ‘‘Repayment
status in August’’.
Fig. 11 (a) and (b) show the overlapping samples of majority and minority classes. It is expected to be difﬁcult for classiﬁers to accurately detect the minority classes (class 1 and
class ‘yes’).
B. TACKLING THE PROBLEM USING MCS
MCS is the combination of predictions from a set of
classiﬁers to produce a better prediction. MCS employs
decision combination strategies that are able to produce
a more robust, reliable, efﬁcient recognition and accurate
classiﬁcation – , .
Basically, there are three different combination strategies when employing MCS: (i) sequential, (ii) parallel, and
(iii) hybrid. Among the three combination strategies of MCS,
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
TABLE 2. The attributes of the CCDP data set.
we expect the sequential combination to perform well. Using
the sequential combination, the output of the ﬁrst classiﬁer
will be an input of the subsequent classiﬁer. This means that
the same piece of sample will be classiﬁed more than once.
Taking credit card fraud data set (CCFD) as the example,
our primary concern will be the frauds that are misclassi-
ﬁed as normal transactions. Therefore, the sample classiﬁed
as normal by the ﬁrst classiﬁer will be fed to the subsequent classiﬁer for re-classiﬁcation purpose. Further, having
one classiﬁer for each class is an advantage. Since each of
the classes shall be handled by one classiﬁer, then it will
mitigate the effect of the unbalanced class distribution and
FIGURE 11. The visualisation of the CCDP data set showing the
unbalanced class distribution and overlapping class problem between
attributes (a) ‘‘Amount of previous statement in April’’ vs. ‘‘Repayment
status in April’’, and (b) ‘‘Repayment status in August’’ vs. ‘‘Amount of bill
statement in August’’.
overlapping classes. Employing the MCS with sequential
combination will therefore reduce the overall misclassiﬁcation rate and improve the detection rate, particularly for
the minority class. Algorithm 1 shows the pseudocode of
the proposed MCS that utilises the sequential combination
The credit card data set shall be classiﬁed by classiﬁer
C1 (Line 1, Algo. 1). Classiﬁer C1 is the expert classiﬁer
to classify majority class samples. During the classiﬁcation,
if the sample is classiﬁed as 1 or ‘yes’, then the sample will
be stored in the data set F_ds. Conversely, if the sample is
classiﬁed as 0 or ‘no’, then the sample will be stored in a
different data set called N_ds (Line 2 – 8, Algo. 1). Our primary concern for this project is the samples of class 1 or ‘yes’
that are misclassiﬁed as 0 or ‘no’. To address the concern,
we will re-classify the data by feeding N_ds into classiﬁer
C2 (Line 9, Algo. 1). Classiﬁer C2 is the expert classiﬁer
to classify minority class samples. During the classiﬁcation,
if the sample is classiﬁed as 1 or ‘yes’, then the sample
will be stored in data set F_ds. If the sample is classiﬁed as
0 or ‘no’, then the sample will remain in the data set N_ds
(Line 10 – 14, Algo. 1). Subsequently, we combine F_ds and
N_ds into one data set called C_ds (Line 15, Algo. 1).Finally,
a confusion matrix as per Table 3 shall be generated based
on C_ds (Line 16 – 19, Algo. 1). Once the confusion matrix
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
Algorithm 1 Detect_Anomaly (CC_db)
Input: The credit card data set, CC_db that comprises of x features for
each credit card transaction
Output: A confusion matrix of classiﬁed data
1. classify CC_db with classiﬁer C1 // C1 classiﬁer to classify
// majority class samples
2. for each transaction, s in the CC_db do
if C1. class(s) is equal to 1 or ‘yes’
// it is a fraud transaction
assign s to F_ds
// F_ds is a data set to keep 1 or ‘yes’ data
else {C1. Class(s) is or ‘no’}
// it is a normal transaction
assign s to N_ds
// N_ds is a data set to keep or ‘no’ data
8. end for
9. classify N_ds with classiﬁer, C2 // C2 classiﬁer to classify minority
// class samples
10. for each transaction, sn in the N_ds do
if C2. class(sn) is equal to 1 or ‘yes’
assign sn to F_ds
14. end for
15. combine F_ds and N_ds assign to C_ds // C_ds are combination of
// dataset F_ds and N_ds
16. actual = C_ds //actual class
17. predicted = C_ds //predicted class
18. result ←confusion matrix (actual,predicted)
19. return result
20. TPRmin←Calculate TPRmin(result) // calculate the TPR for
// minority class samples
21. TPRmaj←Calculate TPRmaj(result) // calculate the TPR for
// majority class samples
22. return TPRmin, TPRmaj
TABLE 3. The Confusion matrix for the classified data.
is obtained, the True Positive Rate (TPR) for both majority
and minority classes shall be calculated using (5) and (6)
(Line 20 – 22, Algo. 1).
To implement Algorithm 1, we need to identify the expert
classiﬁers, C1 and C2. An experiment had been conducted
to identify them. A few popular single classiﬁers, namely,
Naïve Bayes (NB), C4.5, Random Forest, Random Tree,
Logistic Regression (LR), Multilayer Perceptron (MLP), and
IBk were tested on both CCF and CCDP data sets. The
experiments were evaluated using TPR. The TPR for both
majority and minority classes can be calculated using (5)
TPR (minority) =
Num. of detected frauds or
default payments
Total frauds or default payments
TPR (majority) =
Num. of detected non frauds or
non default payments
Total non frauds or
non default payments
Apart from TPR, Area under the ROC Curve (AUC) were
also used as the performance metric. In general, AUC tells the
TABLE 4. The experimental results of using single classifiers. on both CCF
and CCDP data sets.
capability of a classiﬁer in differentiating classes. The closer
the AUC value to 1, the better a classiﬁer is in distinguishing
between classes.
As shown in Table 4, most classiﬁers including C4.5 were
able to achieve the perfect TPR for the majority class (class
0) of the CCF data set. C4.5 also scored the highest TPR,
0.955, for the majority class (class ‘no’) of the CCDP data
set. Therefore, we identiﬁed C4.5 as the C1 as it is able to
produce high TPRs for the majority class of both data sets.
On the other hand, NB obtained the highest TPR, 0.829,
for the minority class (class 1) of the CCF data set. NB also
obtained the highest TPR for the minority class (class yes),
0.719, of the CCDP data set. Therefore, we identiﬁed NB as
C2 as it is able to produce high TPR for the minority class of
both data sets.
We employed sequential combination in our proposed
MCS, with C4.5 and NB as the ﬁrst and second expert classiﬁers. The results shall be discussed in the next section.
IV. RESULTS & DISCUSSION
A. DETECTION RESULTS USING SINGLE CLASSIFIERS
Based on our studies in the literature review, it was found
that single classiﬁers are weak against classifying data sets
that contain unbalanced class distribution and overlapping
classes. This is proven by our experiment using some popular
single classiﬁers, as shown in Table 4.
The majority class of the CCF data set was classiﬁed perfectly, with a TPR of 1.000, by most of the single classiﬁers.
As for the minority class, the TPRs were mostly just average.
The highest TPR for class 1 is 0.829, which was achieved by
On the other hand, the TPRs for the majority class of the
CCDP data set were quite promising, except for NB which
VOLUME 8, 2020
S. N. Kalid et al.: MCS for Anomaly Detection in Credit Card Data with Unbalanced and Overlapped Classes
TABLE 5. The comparison between the proposed MCS and the other researchers’ work on the CCF data set.
TABLE 6. The comparison between the proposed MCS and the other researchers’ work on the CCDP data set.
achieved only 0.634. As for the minority class, the TPRs were
generally low, except for NB that scored an average TPR of
In general, the single classiﬁers did not perform well
in detecting the minority class in both CCF and CCDP
data sets.
B. DETECTION RESULTS USING THE PROPOSED MCS
Table 5 shows the comparison between our approach and
the other researcher’ approach in classifying CCF data set.
All the other researchers’ work listed above used ensemble approaches in tackling the unbalanced class distribution.
By using our proposed MCS, we managed to achieve the
highest TPR of 0.872 for the minority class and outperformed
the other researchers’’ work. Our proposed approach also
gave a good accuracy of 0.999 and a TNR of 1.000.
Our MCS was also tested on CCDP data set. Table 6 shows
the comparison between our approach and the other
researchers’ work. We outperformed their work by obtaining the highest TPR for the minority class, which is 0.840.
Our proposed approach also achieved an accuracy of 0.930
and a TNR of 0.955, which are better than their work.
In summary, we can conclude that our proposed approach
is able to tackle the unbalanced class distribution and the
overlapping class samples that exists in both credit card data
V. CONCLUSION
Credit card is one alternative of cash payment. Some card
holders may abuse their responsibility in credit card usage
and repayment. Apart from that, credit card transaction is
also prone to fraudulent where unauthorized parties perform
illegal transactions using credit cards. Therefore, it is the
responsibility of card issuers or the banks to ﬁnd an effective
way to reduce the cost that may incur when the issues above
happen. One way to address these issues is via data mining.
Due to the characteristics such as overlapping class samples
and unbalanced class distribution that exist in credit card data
sets, it gives challenges to data mining researchers. On top
of that, the weakness of general learning algorithms also
contributes to the difﬁculties of classifying the minority class,
which is usually the important class, of the data sets.
This study proposed a MCS to tackle the issues as discussed above. Based on our analysis using single classiﬁers,
we found that C4.5 is the expert in classifying the majority
class samples and NB is the expert in classifying the minority class samples. Therefore, they were arranged sequentially
in our proposed MCS to detect credit card anomalies. Our
proposed MCS was evaluated using two different credit card
data sets: CCF and CCDP. We have compared our work
with the other researchers’ work. The experimental results
showed that the proposed MCS outperformed their work.
In general, our proposed MCS demonstrates its superiority
in handling the credit data sets that inherit the characteristics of overlapping classes and unbalanced class distribution.
However, there are rooms to improve the TPR for the minority classes. We are looking into other MCS combination
strategies for our future work, particularly the hybrid combination. Currently, researchers had attempted deep learning algorithms such as Long Short-term Memory (LSTM)
and Deep Belief Networks for detecting anomalies in credit
card transactions . We are also considering combining
the deep learning algorithms, as in the study of , for
promising detection results.