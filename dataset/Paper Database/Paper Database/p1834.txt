Enriching Word Vectors with Subword Information
Piotr Bojanowski∗and Edouard Grave∗and Armand Joulin and Tomas Mikolov
Facebook AI Research
{bojanowski,egrave,ajoulin,tmikolov}@fb.com
Continuous word representations, trained on
large unlabeled corpora are useful for many
natural language processing tasks.
models that learn such representations ignore
the morphology of words, by assigning a distinct vector to each word. This is a limitation,
especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram
model, where each word is represented as a
bag of character n-grams. A vector representation is associated to each character n-gram;
words being represented as the sum of these
representations.
Our method is fast, allowing to train models on large corpora quickly
and allows us to compute word representations
for words that did not appear in the training
data. We evaluate our word representations on
nine different languages, both on word similarity and analogy tasks.
By comparing to
recently proposed morphological word representations, we show that our vectors achieve
state-of-the-art performance on these tasks.
Introduction
Learning continuous representations of words has a
long history in natural language processing .
These representations are typically derived from large unlabeled corpora using
co-occurrence statistics . A large
body of work, known as distributional semantics,
has studied the properties of these methods . In the neural
network community, Collobert and Weston 
proposed to learn word embeddings using a feedforward neural network, by predicting a word based
on the two words on the left and two words on the
right. More recently, Mikolov et al. proposed simple log-bilinear models to learn continuous representations of words on very large corpora
efﬁciently.
Most of these techniques represent each word of
the vocabulary by a distinct vector, without parameter sharing. In particular, they ignore the internal
structure of words, which is an important limitation
for morphologically rich languages, such as Turkish or Finnish. For example, in French or Spanish,
most verbs have more than forty different inﬂected
forms, while the Finnish language has ﬁfteen cases
for nouns.
These languages contain many word
forms that occur rarely (or not at all) in the training
corpus, making it difﬁcult to learn good word representations. Because many word formations follow
rules, it is possible to improve vector representations
for morphologically rich languages by using character level information.
In this paper, we propose to learn representations
for character n-grams, and to represent words as the
sum of the n-gram vectors. Our main contribution
is to introduce an extension of the continuous skipgram model , which takes
into account subword information. We evaluate this
model on nine languages exhibiting different morphologies, showing the beneﬁt of our approach.
Transactions of the Association for Computational Linguistics, vol. 5, pp. 135–146, 2017. Action Editor: Hinrich Sch¨utze.
Submission batch: 9/2016; Revision batch: 12/2016; Published 6/2017.
c⃝2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.
Downloaded from by guest on 26 March 2025
Related work
Morphological word representations.
years, many methods have been proposed to incorporate morphological information into word representations.
To model rare words better, Alexandrescu and Kirchhoff introduced factored
neural language models, where words are represented as sets of features. These features might include morphological information, and this technique
was succesfully applied to morphologically rich languages, such as Turkish .
Recently, several works have proposed different composition functions to derive representations of words
from morphemes .
These different approaches rely on a
morphological decomposition of words, while ours
does not. Similarly, Chen et al. introduced
a method to jointly learn embeddings for Chinese
words and characters. Cui et al. proposed
to constrain morphologically similar words to have
similar representations.
Soricut and Och 
described a method to learn vector representations
of morphological transformations, allowing to obtain representations for unseen words by applying
these rules. Word representations trained on morphologically annotated data were introduced by Cotterell and Schütze . Closest to our approach,
Schütze learned representations of character
four-grams through singular value decomposition,
and derived representations for words by summing
the four-grams representations. Very recently, Wieting et al. also proposed to represent words
using character n-gram count vectors. However, the
objective function used to learn these representations is based on paraphrase pairs, while our model
can be trained on any text corpus.
Character level features for NLP.
Another area
of research closely related to our work are characterlevel models for natural language processing. These
models discard the segmentation into words and aim
at learning language representations directly from
characters. A ﬁrst class of such models are recurrent neural networks, applied to language modeling , text normalization , part-of-speech tagging and parsing . Another family of models are convolutional neural networks trained on characters, which
were applied to part-of-speech tagging , sentiment analysis , text classiﬁcation and language modeling introduced a language
model based on restricted Boltzmann machines, in
which words are encoded as a set of character ngrams. Finally, recent works in machine translation
have proposed using subword units to obtain representations of rare words .
In this section, we propose our model to learn word
representations while taking into account morphology. We model morphology by considering subword
units, and representing words by a sum of its character n-grams. We will begin by presenting the general
framework that we use to train word vectors, then
present our subword model and eventually describe
how we handle the dictionary of character n-grams.
General model
We start by brieﬂy reviewing the continuous skipgram model introduced by Mikolov et al. ,
from which our model is derived. Given a word vocabulary of size W, where a word is identiﬁed by
its index w
{1, ..., W}, the goal is to learn a
vectorial representation for each word w. Inspired
by the distributional hypothesis , word
representations are trained to predict well words that
appear in its context. More formally, given a large
training corpus represented as a sequence of words
w1, ..., wT , the objective of the skipgram model is to
maximize the following log-likelihood:
log p(wc | wt),
where the context Ct is the set of indices of words
surrounding word wt. The probability of observing
a context word wc given wt will be parameterized
using the aforementioned word vectors. For now, let
us consider that we are given a scoring function s
which maps pairs of (word, context) to scores in R.
Downloaded from by guest on 26 March 2025
One possible choice to deﬁne the probability of a
context word is the softmax:
p(wc | wt) =
es(wt, wc)
j=1 es(wt, j) .
However, such a model is not adapted to our case as
it implies that, given a word wt, we only predict one
context word wc.
The problem of predicting context words can instead be framed as a set of independent binary classiﬁcation tasks. Then the goal is to independently
predict the presence (or absence) of context words.
For the word at position t we consider all context
words as positive examples and sample negatives at
random from the dictionary. For a chosen context
position c, using the binary logistic loss, we obtain
the following negative log-likelihood:
1 + e−s(wt, wc)
1 + es(wt, n)
where Nt,c is a set of negative examples sampled
from the vocabulary. By denoting the logistic loss
function ℓ: x 7→log(1 + e−x), we can re-write the
objective as:
ℓ(s(wt, wc)) +
ℓ(−s(wt, n))
A natural parameterization for the scoring function
s between a word wt and a context word wc is to use
word vectors. Let us deﬁne for each word w in the
vocabulary two vectors uw and vw in Rd. These two
vectors are sometimes referred to as input and output vectors in the literature. In particular, we have
vectors uwt and vwc, corresponding, respectively, to
words wt and wc. Then the score can be computed
as the scalar product between word and context vectors as s(wt, wc) = u⊤
wtvwc. The model described
in this section is the skipgram model with negative
sampling, introduced by Mikolov et al. .
Subword model
By using a distinct vector representation for each
word, the skipgram model ignores the internal structure of words. In this section, we propose a different
scoring function s, in order to take into account this
information.
Each word w is represented as a bag of character
n-gram. We add special boundary symbols < and >
at the beginning and end of words, allowing to distinguish preﬁxes and sufﬁxes from other character
sequences. We also include the word w itself in the
set of its n-grams, to learn a representation for each
word (in addition to character n-grams). Taking the
word where and n = 3 as an example, it will be
represented by the character n-grams:
<wh, whe, her, ere, re>
and the special sequence
Note that the sequence <her>, corresponding to the
word her is different from the tri-gram her from the
word where. In practice, we extract all the n-grams
for n greater or equal to 3 and smaller or equal to 6.
This is a very simple approach, and different sets of
n-grams could be considered, for example taking all
preﬁxes and sufﬁxes.
Suppose that you are given a dictionary of ngrams of size G. Given a word w, let us denote by
Gw ⊂{1, . . . , G} the set of n-grams appearing in
w. We associate a vector representation zg to each
n-gram g. We represent a word by the sum of the
vector representations of its n-grams. We thus obtain the scoring function:
This simple model allows sharing the representations across words, thus allowing to learn reliable
representation for rare words.
In order to bound the memory requirements of our
model, we use a hashing function that maps n-grams
to integers in 1 to K. We hash character sequences
using the Fowler-Noll-Vo hashing function (speciﬁcally the FNV-1a variant).1 We set K = 2.106 below. Ultimately, a word is represented by its index
in the word dictionary and the set of hashed n-grams
it contains.
Experimental setup
In most experiments (except in Sec. 5.3), we
compare our model to the C implementation
1 
Downloaded from by guest on 26 March 2025
of the skipgram and cbow models from the
word2vec2 package.
Optimization
We solve our optimization problem by performing stochastic gradient descent on the negative log
likelihood presented before.
As in the baseline
skipgram model, we use a linear decay of the step
size. Given a training set containing T words and
a number of passes over the data equal to P, the
step size at time t is equal to γ0(1 −
TP ), where
γ0 is a ﬁxed parameter. We carry out the optimization in parallel, by resorting to Hogwild . All threads share parameters and update
vectors in an asynchronous manner.
Implementation details
For both our model and the baseline experiments, we
use the following parameters: the word vectors have
dimension 300. For each positive example, we sample 5 negatives at random, with probability proportional to the square root of the uni-gram frequency.
We use a context window of size c, and uniformly
sample the size c between 1 and 5. In order to subsample the most frequent words, we use a rejection
threshold of 10−4 ). When building the word dictionary, we
keep the words that appear at least 5 times in the
training set. The step size γ0 is set to 0.025 for the
skipgram baseline and to 0.05 for both our model
and the cbow baseline. These are the default values
in the word2vec package and work well for our
model too.
Using this setting on English data, our model with
character n-grams is approximately 1.5× slower
to train than the skipgram baseline.
we process 105k words/second/thread versus 145k
words/second/thread for the baseline. Our model is
implemented in C++, and is publicly available.3
comparison
work (Sec. 5.3), we train our models on Wikipedia
We downloaded Wikipedia dumps in nine
languages:
2 
3 
4 
Spanish, French, Italian, Romanian and Russian.
We normalize the raw Wikipedia data using Matt
Mahoney’s pre-processing perl script.5
datasets are shufﬂed, and we train our models by
doing ﬁve passes over them.
We evaluate our model in ﬁve experiments: an evaluation of word similarity and word analogies, a comparison to state-of-the-art methods, an analysis of
the effect of the size of training data and of the size
of character n-grams that we consider. We will describe these experiments in detail in the following
Human similarity judgement
We ﬁrst evaluate the quality of our representations
on the task of word similarity / relatedness. We do
so by computing Spearman’s rank correlation coefﬁcient between human judgement and the cosine similarity between the vector
representations. For German, we compare the different models on three datasets: GUR65, GUR350
and ZG222 . For English, we use the WS353 dataset introduced by Finkelstein et al. and the rare
word dataset (RW), introduced by Luong et al.
We evaluate the French word vectors on
the translated dataset RG65 . Spanish, Arabic and Romanian word vectors
are evaluated using the datasets described in . Russian word vectors are evaluated using the HJ dataset introduced by Panchenko
et al. .
We report results for our method and baselines
for all datasets in Table 1. Some words from these
datasets do not appear in our training data, and
thus, we cannot obtain word representation for these
words using the cbow and skipgram baselines. In
order to provide comparable results, we propose by
default to use null vectors for these words. Since our
model exploits subword information, we can also
compute valid representations for out-of-vocabulary
words. We do so by taking the sum of its n-gram
vectors. When OOV words are represented using
5 
Downloaded from by guest on 26 March 2025
Table 1: Correlation between human judgement and
similarity scores on word similarity datasets. We
train both our model and the word2vec baseline on
normalized Wikipedia dumps. Evaluation datasets
contain words that are not part of the training set,
so we represent them using null vectors (sisg-).
With our model, we also compute vectors for unseen
words by summing the n-gram vectors (sisg).
null vectors we refer to our method as sisg- and
sisg otherwise (Subword Information Skip Gram).
First, by looking at Table 1, we notice that the proposed model (sisg), which uses subword information, outperforms the baselines on all datasets except
the English WS353 dataset. Moreover, computing
vectors for out-of-vocabulary words (sisg) is always at least as good as not doing so (sisg-). This
proves the advantage of using subword information
in the form of character n-grams.
Second, we observe that the effect of using character n-grams is more important for Arabic, German and Russian than for English, French or Spanish. German and Russian exhibit grammatical declensions with four cases for German and six for
Russian. Also, many German words are compound
words; for instance the nominal phrase “table tennis” is written in a single word as “Tischtennis”. By
exploiting the character-level similarities between
“Tischtennis” and “Tennis”, our model does not represent the two words as completely different words.
Finally, we observe that on the English Rare
Words dataset (RW), our approach outperforms the
Table 2: Accuracy of our model and baselines on
word analogy tasks for Czech, German, English and
Italian. We report results for semantic and syntactic
analogies separately.
baselines while it does not on the English WS353
dataset. This is due to the fact that words in the English WS353 dataset are common words for which
good vectors can be obtained without exploiting
subword information. When evaluating on less frequent words, we see that using similarities at the
character level between words can help learning
good word vectors.
Word analogy tasks
We now evaluate our approach on word analogy
questions, of the form A is to B as C is to D,
where D must be predicted by the models. We use
the datasets introduced by Mikolov et al. 
for English, by Svoboda and Brychcin for
Czech, by Köper et al. for German and by
Berardi et al. for Italian. Some questions contain words that do not appear in our training corpus,
and we thus excluded these questions from the evaluation.
We report accuracy for the different models in
Table 2. We observe that morphological information signiﬁcantly improves the syntactic tasks; our
approach outperforms the baselines.
In contrast,
it does not help for semantic questions, and even
degrades the performance for German and Italian.
Note that this is tightly related to the choice of the
length of character n-grams that we consider. We
show in Sec. 5.5 that when the size of the n-grams
is chosen optimally, the semantic analogies degrade
Downloaded from by guest on 26 March 2025
Luong et al. 
Qiu et al. 
Soricut and Och 
Botha and Blunsom 
Table 3: Spearman’s rank correlation coefﬁcient between human judgement and model scores for different
methods using morphology to learn word representations. We keep all the word pairs of the evaluation set
and obtain representations for out-of-vocabulary words with our model by summing the vectors of character
n-grams. Our model was trained on the same datasets as the methods we are comparing to (hence the two
lines of results for our approach).
less. Another interesting observation is that, as expected, the improvement over the baselines is more
important for morphologically rich languages, such
as Czech and German.
Comparison with morphological
representations
We also compare our approach to previous work on
word vectors incorporating subword information on
word similarity tasks. The methods used are: the
recursive neural network of Luong et al. ,
the morpheme cbow of Qiu et al. and the
morphological transformations of Soricut and Och
 . In order to make the results comparable, we
trained our model on the same datasets as the methods we are comparing to: the English Wikipedia
data released by Shaoul and Westbury , and
the news crawl data from the 2013 WMT shared
task for German, Spanish and French.
compare our approach to the log-bilinear language
model introduced by Botha and Blunsom ,
which was trained on the Europarl and news commentary corpora. Again, we trained our model on
the same data to make the results comparable. Using our model, we obtain representations of out-ofvocabulary words by summing the representations
of character n-grams. We report results in Table 3.
We observe that our simple approach performs well
relative to techniques based on subword information
obtained from morphological segmentors. We also
observe that our approach outperforms the Soricut
and Och method, which is based on preﬁx
and sufﬁx analysis. The large improvement for German is due to the fact that their approach does not
model noun compounding, contrary to ours.
Effect of the size of the training data
Since we exploit character-level similarities between
words, we are able to better model infrequent words.
Therefore, we should also be more robust to the size
of the training data that we use.
In order to assess that, we propose to evaluate the performance
of our word vectors on the similarity task as a function of the training data size. To this end, we train
our model and the cbow baseline on portions of
Wikipedia of increasing size. We use the Wikipedia
corpus described above and isolate the ﬁrst 1, 2, 5,
10, 20, and 50 percent of the data. Since we don’t
reshufﬂe the dataset, they are all subsets of each
other. We report results in Fig. 1.
As in the experiment presented in Sec. 5.1, not
all words from the evaluation set are present in the
Wikipedia data. Again, by default, we use a null
vector for these words (sisg-) or compute a vector by summing the n-gram representations (sisg).
The out-of-vocabulary rate is growing as the dataset
shrinks, and therefore the performance of sisgand cbow necessarily degrades. However, the proposed model (sisg) assigns non-trivial vectors to
previously unseen words.
First, we notice that for all datasets, and all sizes,
the proposed approach (sisg) performs better than
Downloaded from by guest on 26 March 2025
percentage of data
spearman rank
(a) DE-GUR350
percentage of data
spearman rank
Figure 1: Inﬂuence of size of the training data on performance. We compute word vectors following the
proposed model using datasets of increasing size. In this experiment, we train models on a fraction of the
full Wikipedia dump.
the baseline. However, the performance of the baseline cbow model gets better as more and more data
is available. Our model, on the other hand, seems
to quickly saturate and adding more data does not
always lead to improved results.
Second, and most importantly, we notice that the
proposed approach provides very good word vectors
even when using very small training datasets. For instance, on the German GUR350 dataset, our model
(sisg) trained on 5% of the data achieves better
performance (66) than the cbow baseline trained on
the full dataset (62). On the other hand, on the English RW dataset, using 1% of the Wikipedia corpus
we achieve a correlation coefﬁcient of 45 which is
better than the performance of cbow trained on the
full dataset (43). This has a very important practical implication: well performing word vectors can
be computed on datasets of a restricted size and still
work well on previously unseen words.
In general, when using vectorial word representations in
speciﬁc applications, it is recommended to retrain
the model on textual data relevant for the application.
However, this kind of relevant task-speciﬁc
data is often very scarce and learning from a reduced
amount of training data is a great advantage.
Effect of the size of n-grams
The proposed model relies on the use of character ngrams to represent words as vectors. As mentioned
in Sec. 3.2, we decided to use n-grams ranging from
3 to 6 characters. This choice was arbitrary, motivated by the fact that n-grams of these lengths will
cover a wide range of information. They would include short sufﬁxes (corresponding to conjugations
and declensions for instance) as well as longer roots.
In this experiment, we empirically check for the in-
ﬂuence of the range of n-grams that we use on performance. We report our results in Table 4 for English and German on word similarity and analogy
We observe that for both English and German,
our arbitrary choice of 3-6 was a reasonable decision, as it provides satisfactory performance across
languages.
The optimal choice of length ranges
depends on the considered task and language and
should be tuned appropriately.
However, due to
the scarcity of test data, we did not implement any
proper validation procedure to automatically select
the best parameters.
Nonetheless, taking a large
range such as 3 −6 provides a reasonable amount
of subword information.
This experiment also shows that it is important to
include long n-grams, as columns corresponding to
n ≤5 and n ≤6 work best. This is especially true
for German, as many nouns are compounds made
up from several units that can only be captured by
longer character sequences. On analogy tasks, we
observe that using larger n-grams helps for semantic analogies. However, results are always improved
by taking n ≥3 rather than n ≥2, which shows
that character 2-grams are not informative for that
task. As described in Sec. 3.2, before computing
Downloaded from by guest on 26 March 2025
(a) DE-GUR350
(b) DE Semantic
(c) DE Syntactic
(e) EN Semantic
(f) EN Syntactic
Table 4: Study of the effect of sizes of n-grams considered on performance. We compute word vectors by
using character n-grams with n in {i, . . . , j} and report performance for various values of i and j. We evaluate this effect on German and English, and represent out-of-vocabulary words using subword information.
character n-grams, we prepend and append special
positional characters to represent the beginning and
end of word. Therefore, 2-grams will not be enough
to properly capture sufﬁxes that correspond to conjugations or declensions, since they are composed of
a single proper character and a positional one.
Language modeling
In this section, we describe an evaluation of the word
vectors obtained with our method on a language
modeling task.
We evaluate our language model
on ﬁve languages (CS, DE, ES, FR, RU) using the
datasets introduced by Botha and Blunsom .
Each dataset contains roughly one million training
tokens, and we use the same preprocessing and data
splits as Botha and Blunsom .
Our model is a recurrent neural network with 650
LSTM units, regularized with dropout (with probability of 0.5) and weight decay (regularization parameter of 10−5). We learn the parameters using
the Adagrad algorithm with a learning rate of 0.1,
clipping the gradients which have a norm larger
than 1.0. We initialize the weight of the network in
the range [−0.05, 0.05], and use a batch size of 20.
Two baselines are considered: we compare our approach to the log-bilinear language model of Botha
and Blunsom and the character aware language model of Kim et al. . We trained word
vectors with character n-grams on the training set
of the language modeling task and use them to initialize the lookup table of our language model. We
report the test perplexity of our model without using
pre-trained word vectors (LSTM), with word vectors
pre-trained without subword information (sg) and
with our vectors (sisg). The results are presented
in Table 5.
We observe that initializing the lookup table of
the language model with pre-trained word representations improves the test perplexity over the baseline LSTM. The most important observation is that
using word representations trained with subword information outperforms the plain skipgram model.
We observe that this improvement is most signiﬁcant for morphologically rich Slavic languages such
as Czech (8% reduction of perplexity over sg) and
Russian (13% reduction). The improvement is less
signiﬁcant for Roman languages such as Spanish
(3% reduction) or French (2% reduction).
shows the importance of subword information on the
language modeling task and exhibits the usefulness
Downloaded from by guest on 26 March 2025
Vocab. size
Table 5: Test perplexity on the language modeling
task, for 5 different languages. We compare to two
state of the art approaches: CLBL refers to the work
of Botha and Blunsom and CANLM refers
to the work of Kim et al. .
of the vectors that we propose for morphologically
rich languages.
Qualitative analysis
Nearest neighbors.
We report sample qualitative results in Table 7. For
selected words, we show nearest neighbors according to cosine similarity for vectors trained using the
proposed approach and for the skipgram baseline. As expected, the nearest neighbors for complex, technical and infrequent words using our approach are better than the ones obtained using the
baseline model.
Character n-grams and morphemes
We want to qualitatively evaluate whether or not
the most important n-grams in a word correspond
to morphemes. To this end, we take a word vector
that we construct as the sum of n-grams. As described in Sec. 3.2, each word w is represented as
the sum of its n-grams: uw = P
g∈Gw zg. For each
n-gram g, we propose to compute the restricted representation uw\g obtained by omitting g:
We then rank n-grams by increasing value of cosine
between uw and uw\g. We show ranked n-grams for
selected words in three languages in Table 6.
For German, which has a lot of compound nouns,
we observe that the most important n-grams corword
autofahrer
freundeskreis
sprachschule
tageslicht
politeness
Table 6: Illustration of most important character ngrams for selected words in three languages. For
each word, we show the n-grams that, when removed, result in the most different representation.
respond to valid morphemes. Good examples include Autofahrer (car driver) whose most important
n-grams are Auto (car) and Fahrer (driver). We also
observe the separation of compound nouns into morphemes in English, with words such as lifetime or
starﬁsh. However, for English, we also observe that
n-grams can correspond to afﬁxes in words such as
kindness or unlucky. Interestingly, for French we observe the inﬂections of verbs with endings such as
ais>, ent> or ions>.
Word similarity for OOV words
As described in Sec. 3.2, our model is capable of
building word vectors for words that do not appear
in the training set. For such words, we simply average the vector representation of its n-grams. In order to assess the quality of these representations, we
analyze which of the n-grams match best for OOV
words by selecting a few word pairs from the English RW similarity dataset. We select pairs such
that one of the two words is not in the training vocabulary and is hence only represented by its ngrams. For each pair of words, we display the cosine
similarity between each pair of n-grams that appear
Downloaded from by guest on 26 March 2025
english-born
micromanaging
tech-dominated
british-born
micromanage
restaurants
tech-heavy
polish-born
micromanaged
technology-heavy
most-capped
restaurants
epithelial
ex-scotland
internalise
Table 7: Nearest neighbors of rare words using our representations and skipgram. These hand picked
examples are for illustration.
Figure 2: Illustration of the similarity between character n-grams in out-of-vocabulary words. For each pair,
only one word is OOV, and is shown on the x axis. Red indicates positive cosine, while blue negative.
Downloaded from by guest on 26 March 2025
in the words. In order to simulate a setup with a
larger number of OOV words, we use models trained
on 1% of the Wikipedia data as in Sec. 5.4. The results are presented in Fig. 2.
We observe interesting patterns, showing that subwords match correctly. Indeed, for the word chip,
we clearly see that there are two groups of n-grams
in microcircuit that match well. These roughly correspond to micro and circuit, and n-grams in between don’t match well.
Another interesting example is the pair rarity and scarceness.
scarce roughly matches rarity while the sufﬁx -ness
matches -ity very well. Finally, the word preadolescent matches young well thanks to the -adolescsubword. This shows that we build robust word representations where preﬁxes and sufﬁxes can be ignored if the grammatical form is not found in the
dictionary.
Conclusion
In this paper, we investigate a simple method to
learn word representations by taking into account
subword information. Our approach, which incorporates character n-grams into the skipgram model,
is related to an idea that was introduced by Schütze
 . Because of its simplicity, our model trains
fast and does not require any preprocessing or supervision. We show that our model outperforms baselines that do not take into account subword information, as well as methods relying on morphological
analysis. We will open source the implementation
of our model, in order to facilitate comparison of future work on learning subword representations.
Acknowledgements
We thank Marco Baroni, Hinrich Schütze and the
anonymous reviewers for their insightful comments.