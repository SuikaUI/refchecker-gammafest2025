Received July 29, 2019, accepted August 12, 2019, date of publication August 19, 2019, date of current version September 3, 2019.
Digital Object Identifier 10.1109/ACCESS.2019.2936346
Mini-Batch Normalized Mutual Information:
A Hybrid Feature Selection Method
G. S. THEJAS
1, (Member, IEEE), SAJAL RAJ JOSHI2, S. S. IYENGAR
1, (Life Fellow, IEEE),
N. R. SUNITHA2, (Member, IEEE), AND PRAJWAL BADRINATH1
1School of Computing and Information Sciences, Florida International University, Miami, FL 33199, USA
2Department of Computer Science Engineering, Siddaganga Institute of Technology, Tumakuru 572103, India
Corresponding authors: G. S. Thejas ( ) and Sajal Raj Joshi ( )
This work was supported by the Florida International University Graduate School Dissertation Year Fellowship Award received by the
author G. S. Thejas.
ABSTRACT Feature Selection has been a signiﬁcant preprocessing procedure for classiﬁcation in the area
of Supervised Machine Learning. It is mostly applied when the attribute set is very large. The large set of
attributes often tend to misguide the classiﬁer. Extensive research has been performed to increase the efﬁcacy
of the predictor by ﬁnding the optimal set of features. The feature subset should be such that it enhances the
classiﬁcation accuracy by the removal of redundant features. We propose a new feature selection mechanism,
an amalgamation of the ﬁlter and the wrapper techniques by taking into consideration the beneﬁts of both the
methods. Our hybrid model is based on a two phase process where we rank the features and then choose the
best subset of features based on the ranking. We validated our model with various datasets, using multiple
evaluation metrics. Furthermore, we have also compared and analyzed our results with previous works. The
proposed model outperformed many existent algorithms and has given us good results.
INDEX TERMS Feature selection, ﬁlter method, hybrid feature selection, normalized mutual information,
mini batch K-means, random forest, wrapper method.
I. INTRODUCTION
One of the essential phases in classiﬁcation is to determine
the useful set of features for the classiﬁer. In supervised as
well as in unsupervised learning, the large volume of data
has become a signiﬁcant problem and is becoming more
prominent with the increase in data samples and the number
of features in each sample. The main intention of reducing the
dimension by keeping a minimum number of features is to
decrease the computation time, obtain greater accuracy, and
reduce overﬁtting.
Dimensionality reduction is divided into 2 categories:
Feature Extraction (FE) and Feature Selection(FS). In FE,
we transform the existing features into new features with
lesser dimensionality employing a linear or a nonlinear
combination of features. In this method, the actual data is
manipulated and hence not immune from distortion under
transformation. In the FS process, we select the feature’s
subset based on some criteria. Many of the attributes in the
dataset may be utterly irrelevant to the class or redundant
The associate editor coordinating the review of this manuscript and
approving it for publication was Nizam Uddin Ahamed.
when considered along with other features. The accuracy
of the induced classiﬁer is decreased by the presence of
irrelevant or redundant features . Identifying such features
and removing them reduces the dimensionality which inturn
reduces the computation time while improving the accuracy.
In , they state that the overabundance of features rendered
the nearest neighbor approach on Internet Advertisement
FS has many applications in various ﬁelds like image
processing, natural language processing, bioinformatics, data
mining, and machine learning(ML) . The selection method
is divided into two standard categories based on their working
modules, classiﬁer independent ’ﬁlter’ technique, and classi-
ﬁer dependent ’wrapper’ and ’embedded’ technique.
The ﬁlter technique, a classiﬁer independent process, performs the selection of the features based on statistical metrics
like distance, correlation, consistency measure, and mutual
information (MI). It either ranks the features or provides a
relevant subset of features associated with the class label.
It improves the computational efﬁciency and scales down
the data dimensionality by entirely being independent of
the classiﬁer . The drawback of this process is the lack
VOLUME 7, 2019
This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see 
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
of knowledge regarding the relationship between feature
attributes and target class.
The classiﬁer dependent systems rely upon the classiﬁer
for the selection process. The wrapper method uses the outcome of the classiﬁer to obtain the subset of features, making
it biased to the classiﬁer. Also, it is vulnerable to overﬁtting,
mostly when the quantity of data is very few . The embedded method makes use of the classiﬁer in the training phase
and selects the optimal features like the learning procedure.
When compared to the wrapper method, embedded methods
are less vulnerable to overﬁtting and computation is much
faster .
We propose a combination of ﬁlter and wrapper method,
which has the advantage of both the techniques. It is fast and
general like the ﬁlter method. At the same time, it accounts to
learning algorithm obtaining the best set of features without
the need for the user to input the feature number unlike
most of other established algorithms like Recursive Feature
Elimination (RFE).
In this paper, we cluster the data using mini-batch Kmeans
clustering and rank them using normalized mutual information(NMI), a measure to calculate the relevance and the
redundancy between the candidate attribute and the class.
We apply a greedy search method by using Random Forest
to get the optimal set of features. However, our method is
ﬂexible in terms of the learning algorithm that can be used
in our process.
Organization of the paper: Section II discusses the related
work regarding various standard techniques as well as different hybrid approaches. In section III, we discuss the preliminary concepts behind this work and propose our techniques.
In section IV, we elaborate each component of our work
in detail. In section V, we show experimental results and
compare them with the previous works, and in Section VI,
we conclude our work and give light to the future work.
A. ABBREVIATIONS AND ACRONYMS
All Features(AF), Feature Selection (FS), Feature Extraction
(FE), Mini Batch K-Means Normalized Mutual Information Feature Inclusion(KNFI), Mini batch K-Means Normalized Mutual Information Feature Elimination (KNFE),
Normalized Mutual Information (NMI), Random Forest
(RF), Recursive Feature Elimination (RFE).
II. RELATED WORK
A. FILTER METHOD
Guyon and Elisseeff give information about all the developments to improve the performance of the model using
statistical analysis. They came up with a simple approach
where less computation is required. Their process did not
consider the dependency between the features but considered each feature as an independent one. Saeys et al. 
show that the various FS methods have given impressive
results in the ﬁeld of bioinformatics. Using Weka tool,
Pushpalatha and Karegowda perform CFS based ﬁlter
approach to rank with ﬁve search techniques. Dash et al. 
choose the best feature subset for clustering by evaluating
the various subsets of features. It considers the effect of the
underlying clusters with no unanimous agreement in evaluating the clusters.
B. WRAPPER METHOD
In , the authors introduced a variant of particle swarm
optimization (PSO) to determine the least number of features which results in ﬁner classiﬁcation. It is a wrapper based technique named as competitive swarm optimizer
(CSO). Also, they proposed an archive technique to reduce
computational cost. Jiang et al. optimized the multi
objective function of a pre-existing wrapper method to a
single objective function to reduce the computation cost by
adding a new evaluation function. Mafarja and Mirjalili 
introduced a new wrapper method which was mainly based
upon Whale Optimization Algorithm (WOA), with slight
changes to make the model work even for binary datasets.
Xue et al. performed feature selection with genetic algorithm and extreme machine learning, which is computationally efﬁcient in comparison with other wrapper methods.
C. HYBRID METHOD
Venkatesh and Anuradha came up with a hybrid
approach of ﬁlter and wrapper method by considering MI and
RFE. Sharmin et al. used MI as a metric for creating
a framework for selecting features and discretization at the
same time based on x2 test. Hoque et al. considered the
information between the attributes and the classes. They considered the MI of the candidate attribute with all the attributes
in the selected set of feature attributes. Genetic algorithm
was used to select attributes that increased the MI with the
label class and decreased the MI with other feature attributes.
Battiti introduced Mutual Information Feature Selection
(MIFS), an incremental greedy search method to select the
most likely ’k’ features among n features. Instead of calculating MI between the attributes and the classes, they calculated MI between the attributes i.e., the previously selected
attributes and the set of candidate attributes. The performance
tends to degrade if there are signiﬁcant errors in estimating
MI. Kwak and Choi proposed an improvised method of
MIFS to improve the estimations of MI between the class
labels and the input attributes called MIFS-U. Peng et al. 
proposed a method mRMR which avoids expansion of subset
where the redundancy divides over the cardinality ∥C∥of
the selected subset. Brown et al. justiﬁed that this alteration allows mRMR to outperform the established MIFS &
MIFS-U techniques. Estevez et al. normalized the value
of MI to curb down the value between zero and unity, which
removed the bias towards multivalued features. The proposed
approach of normalizing the value of mutual information in
the FS process, namely NIMFS, which is as an upgraded
model of mRMR, MIFS-U, and MIFS to ﬁnd the irrelevant
and redundant features. They also proposed genetic algorithm
based feature selection process. Haury et al. give the
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
Runtime analysis of k-means and minibatch k-means. .
comparative analysis of FS method based upon stability and
interpretability of the classes. This comparative analysis suggested that a simple ﬁlter method outperformed more complex embedded and wrapper method. Faker and Dogdu 
considered homogeneity metric as a measure to rank and
remove the least ranked features. Zhang et al. , proposed
a hybrid ﬁlter and wrapper method where they created a subset of features with bootstrapping strategy. For each subset,
classiﬁcation accuracy is calculated to ﬁnd the optimized
III. PROPOSED APPROACH
A. PRELIMINARIES
1) MINI BATCH K-MEANS METHOD
K-means is one of the popular clustering algorithms. With
the increase in dataset size, the computation time increases as
the entire data needs to be present in the main memory .
Because of this, we prefer Mini Batch K-means for large
datasets. We intend to apply a ﬁxed size of small random
batches of data for easy storage in the memory. In every
iteration, the cluster is updated taking new random samples
from the dataset. For a given dataset D = x1, x2, x3, ....., xp,
xi ∈Rm∗n, xi represents the records in an n-dimensional real
vector. The number of records in dataset D is ’m’. We obtain
a set S of cluster center s ∈Rn to decrease over the dataset D
of records s ∈Rm∗n as shown in the following function.
∥f (S, x) −x∥2
where, f(S,x) yields the nearest cluster center s ∈S to
record x. If K is the number of clusters, it is given by
k = |S|. We randomly select K records by using Kmeans++
to initialize the centers and we set the cluster centers S to be
equal to the values of these. In our case, we have considered
the number of clusters equal to the number of class. When
the data is huge, the convergence rate of the original Kmeans
signiﬁcantly drops. In this case, an improved K-means called
Mini Batch Kmeans is introduced .
2) NORMALIZED MUTUAL INFORMATION (NMI)
NMI is one of the ways for measuring the criteria of cluster
quality, which is information-theoretic interpretation. This
measure calculates the cluster quality with cluster number.
Mathematically:
NMI(, S) =
[G() + G(S))]/2
where  is the set of clusters and S is the set of classes. Here
MI is given by the formula:
MI(; S) =
P(dk ∩sj)log P(dk ∩sj)
P(dk)P(sj)
where P(dk) =probability of document in cluster dk,*
P(sj) = probability of document in cluster sj, *
P(dk ∩sj) =probability of document being in the convergence of dk and sj. *
NMI increases the knowledge of the class by evaluating the
amount of information obtained from the clusters. The value
is 0 when the clustering is random concerning the class and
gives no knowledge about the class. MI reaches maximum
value if it perfectly recreates the classes. G is the entropy.
Mathematically:
P(dk)logP(dk)
This gives the entropy of cluster levels. The normalization in
Eqn. 2 by the denominator solves the problem of purity. It also
formalizes that fewer clusters are better since the entropy
usually increases with the increase in cluster number. 
The value of NMI is always between 0 and 1.
B. OUR APPROACH
ﬁlter-wrapper
approach for the FS. There are two objective functions in
our FS. First, the feature ranking function based on the
ﬁlter approach and second, the selection of optimal features
based upon the rankings. This optimal selection is a wrapper
based method that depends upon the outcome of the learning
algorithm. Our approach is independent of any number of a
class labels and is suitable to use with any classiﬁer. In our
experiments, we have considered Random Forest as the classiﬁer. However, we can use any classiﬁer. The implemented
code can be accessed from the project repository 1. Our
approach has 2 phases;
1) FEATURE RANKING
In the ﬁrst phase, the main idea is to separately cluster the
features one by one based upon the total classes in the dataset.
Our objective is to have a selection algorithm which takes less
computation time in comparison to the existing algorithms.
Since the data are large these days, we have considered
mini-batch K-means, which takes into account a batch of
data and performs clustering. The computation time, in this
case, is much lesser than the normal K-means clustering.
The cluster’s quality is the metric to ﬁnd the relation of
that feature with the class. As the cluster quality increases,
1 
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
the feature tends to be more relevant and is considered to be
more important. The use of NMI gives a cluster score between
0 to 1. The high ranking score indicates better classiﬁcation
using the candidate feature. The cluster score for all the
features is evaluated separately. Comparing the score of each
feature, we obtain the ranking list. This ranking obtained is
based upon the individual relationship between the candidate
attribute and the class label.
2) FEATURE SELECTION
In the FS problem, a feature variable may have a dependency
on other variables. The dependent features tend to produce
imbalanced results when acted upon together and hence,
is considered a redundant feature. The redundant feature
tends to deteriorate the classiﬁcation process so we remove
those features. We considered the ranking obtained from the
ﬁrst phase as the base for the selection of features. We consider this to have a linear approach of selecting the features to
get the optimal features in minimum time. When the feature
size in the dataset increases, comparison with all the possible
subsets is an impractical approach and seems to be computationally very expensive. We present two approaches for the
selection of features:
1) Feature Inclusion: This is almost a linear selection
approach where the ranked features from phase one
are added one by one into the subset. If the addition
of the features enhances the classiﬁcation accuracy,
we consider the feature or else we discard the feature.
Here, the highest ranked feature is initially included in
the list as shown in step one of algorithm 1. We add
the next ranked feature and obtain its performance.
If the performance increases, we add the feature into the
list or else discard the feature. The feature is removed
if it does not perform well with the selected subset,
considering that it is redundant as it degrades the classiﬁcation model. This process loops for all the features, as shown in algorithm 1. This process is named
MiniBatch K-Means Normalized Mutual Information
Feature Inclusion (KNFI)
2) Least Ranked Feature Exclusion: This is a linear elimination approach where the least ranked features are
eliminated one by one from the entire set of features.
Initially, the list consists of all the features and the
classiﬁcation accuracy is calculated for the entire list.
Then, in every loop, one least ranked feature is removed
from the list. This process is carried out until the
list becomes empty. The highest performance among
all the iterations is considered as the outcome of our
approach, as shown in algorithm 2. This process is
named Mini-Batch K-Means Normalized Mutual Information least ranked Feature Exclusion (KNFE)
IV. EXPERIMENT
A. EXPERIMENTAL SETUP
The conduction of all the experiments is performed in Python
Language using the python libraries. Florida International
Algorithm 1 Ranking Based Feature Inclusion for Optimal
Feature Subset(KNFI)
Input: Set of ranked features S = {f0, f1, f2, .......fm}, where
m = total number of features, obtained from the feature
ranking phase, f0 is the highest ranked feature and fm is
the least ranked feature.
Output: prints the selected set of features
Initialisation :
1: Lst = S prev=0
LOOP Process
2: for k = 0 to m-1 do
x_tst = x_tst [ Lst ]
x_tr =x_tr [ Lst ]
train the model based on any classiﬁer and store the
accuracy on acc
if acc > prev then
if (k ̸= m −1) then
Add S[ k + 1 ] into the Lst
Remove S [ k ] object from the Lst
if (k ̸= m −1) then
Add S[ k + 1 ] to the Lst
21: end for
22: return Lst
University provided us the required hardware. We used an
Intel i7 4 core CPU with 16GB RAM. Also for large datasets,
we used the Flounder Server 
Input: Set of ranked features S = {f0, f1, f2, .......fm}, where
m = total number of features,.f0 is the least ranked feature
and fm is the highest ranked feature.
Output: prints the result for every eliminated feature from
the feature list
Initialization :
1: Lst = S prev=0
LOOP Process
2: for k = 0 to m-1 do
x_tst = x_tst [ Lst ]
x_tr =x_tr [ Lst ]
//train the model based on any classiﬁer and store the
accuracy on acc
//print the result along with the evaluation metrics
if acc > prev then
prev=acc // to store the greatest accuracy
fet=i // to store the no. of feature eliminated
delete Lst //deleting the least ranked feature
12: end for
13: return
single dataset and performed the experiments. We removed
the socket information(i.e., source ip address, source port
number, destination ip address and destination port number)
such that model becomes independent of them. We removed
the white spaces present in some of the multiclass labels. All
the categorical values were converted to the numerical values
as the classiﬁer can only learn numerical values. The different
ranges of numerical data in the features become a challenge
for the classiﬁer to train the model . To compensate this,
we performed normalization on the entire data.
1) TalkingData dataset
It is an AdTracking Fraud Dataset which has records
of 200 million clicks over four days. It has features like
app ID, os, IP address, click time, device type, channel,
attributed time, and target label as is_attributed. In the preprocessing stage, we dropped the attributed time. We separated
Click time into separate columns, i.e., day, hour, minute,
and second. Two variants of the above mentioned dataset were
used. In the ﬁrst version, we considered one million rows of
data in which the ratio of classes match the ratio at 200 million
rows (Talkingdata Version 1) is taken. 913692 data samples
were used for the second variant, where the rows were equally
categorized into two classes (Talkingdata Version 2) .
This dataset is a Click fraud dataset consisting of clicks
recorded over ten days and has features like id, click (Target
Label), device_id, device_ip, an hour of click, and so on.
We do the preprocessing i.e., separation of the ’hour of click’
column into separate columns. We consider 1 million rows of
data in which the ratio of classes match the ratio at 200 million
rows to reduce the data size
It is a Click fraud dataset that consists of 40 features. To clean
the data, we have removed instances with ’NaN’ values.
4) Ionosphere Dataset
In the Ionosphere dataset provided UCI repository, we converted the class labels (good, bad) into numerical values.
5) Breast Cancer, Lung Cance, Heart Disease datasets
In this dataset, there are some missing values represented by
a question mark(?). We removed the instances containing ? as
a cleaning process.
6) Lymphography Dataset, Iris Dataset
These datasets were clean, and no preprocessing step had
to be applied. However, we performed resampling as the
instances with the same classes were together in the actual
7) Abalone Dataset
In this dataset, the ﬁrst feature consists of categorical string
values that we converted into numerical values.
8) Spambase Dataset, Sonar dataset
These datasets are considered to compare our model with
other research approaches. The spambase dataset is taken
from UCI repository, and Sonar dataset is taken from Kaggle
dataset. The datasets were clean with no NaN values, and no
preprocessing was needed.
We normalized the entire data by using MinMaxScalar
function for all the datasets.
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
V. RESULTS AND DISCUSSION
A. BASE CLASSIFIER: RANDOM FOREST
RF is a prevalent supervised ML technique that is ﬂexible and
very easy to use . As the name implies, RF has a large
number of individual decision trees. Each decision tree acts
as an individual classiﬁer. We get a class prediction from each
tree in the RF, and the class that gets the most votes becomes
the model prediction of RF. With the increase in the number
of trees, the classiﬁer has a greater ability to resist noise and
obtain greater accuracy. The RF, being a simple classiﬁer built
on decision trees, can easily adapt to large changes in the data
size, having the beneﬁt of scalability .
B. EVALUATION METRICS
The accuracy of the algorithm needs to be evaluated by certain
standard metrics. For binary classiﬁcation, we have considered the standard metric, Area Under Curve (AUC) and also
the F1 Score, which is computed based upon the Precision and
Recall score. For the multiclass dataset, we have considered
the F1 Score as the evaluation criteria. The F1 Score can also
be obtained from the confusion matrix. This metric can only
be used for the test data whose true values are already known
such that we get a confusion matrix.
We can obtain the following information from the confusion matrix:
• True Positive (TrPos): model correctly predicting Positive cases as Positive.
• False Positive (FlPos): model incorrectly predicting the
Negative cases as Positive.
• False Negative (FlNeg): model incorrectly predicting
positive cases as Negative.
• True Negative (TrNeg): model correctly predicting negative cases as positive.
Precision score(Pr): It measures accuracy based upon correctly predicted cases.
TrPos + FlPos
Recall score(RC): It is the TrPos rate to predict the ofteness
of predicting positive.
TrPos + FlNeg
F1 Score(F1): F1 is the weighted average of recall and precision of each class.
ROC-AUC curve is a standard metric to measure the
performance of the classiﬁcation model. The probability
curve between the true positive rates against false positive
rates is referred to as ROC. AUC represents the degree of separability. The higher the AUC, the more the efﬁciency of the
C. ANALYSIS METHOD
To empirically test the advantages and disadvantages of our
method, we performed several experiments on real-world
datasets with four different approaches. They are:
1) Considering all the features present in the dataset for
classiﬁcation and calculation of its accuracy, AUC (for
binary datasets), precision, recall, F-1 Score. We represent this as AF.
2) Our approach (KNFI), where we perform classiﬁcation based on the ranked features and determine its
evaluation metrics. Without the need of the user to
specify the number of optimal features, our approach
automatically calculates it. This number has been considered as the base number for performing RFE, where
we explicitly have to provide the required number of
optimal features.
3) Using RFE ( Recursive Feature Elimination), a standard process, provided by Scikit_learn , selects
features by recursively considering the small set of
features. The user explicitly has to give the desired
subset number (k), and then it returns the best accuracy
from the best subset with k features. In our experiment,
we have considered the value of K, referring to our
KNFI approach.
4) Our second approach KNFE, where we remove the
least ranked features one after another, performing the
classiﬁcation and calculating its evaluation metrics.
The best accuracy obtained after removing ’k’ features is considered as the comparing value with other
A comparative analysis is performed for the results obtained
from the four methods in terms of various evaluation metrics,
as mentioned above. We can observe that our approach takes
less computation time compared to the existing methods, and
in many datasets, it produced better results.
D. DISCUSSIONS
1) FOR BINARY DATASETS
In the UNSW_NB15 dataset, both our KNFI and KNFE
methods improvised the learning algorithm to obtain greater
accuracy, AUC, and F1-Score, as shown in Table 3. KNFI
selected 17 features and stood superior in terms of all the evaluation metrics. Also, the evaluation metrics greatly increased
in the Ionosphere dataset as in Table 4 for our 6 selected features among the 34 features. Most of the redundant features
were removed, giving us better results.
We have a slight increase in accuracy for the Avazu dataset
as in Table 5 for both of our approaches. However, the AUC
is slightly decreased in both the methods. The decrease in
AUC could be due to the presence of imbalanced data. The
F1-Score is a much better metric of measurement . The
F1-Score remained constant with an increase in accuracy,
giving us a better-trained model with the selected features.
2Ftr is the number of selected features.
3Acc is the accuracy of the model.
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
TABLE 3. Experimental results of UNSW_NB15 binary datasets.
TABLE 4. Experimental results of ionosphere datasets.
TABLE 5. Experimental results of avazu dataset.
TABLE 6. Experimental results of talking dataset version 2.
TABLE 7. Experimental results of spambase dataset.
This is shown in Table 5. Also, in the TalkingData dataset
(version 2), the accuracy increased slightly for KNFI. However, for KNFE, it showed zero elimination of features for
the best classiﬁcation accuracy meaning all the features are
independent and contributing for the classiﬁcation model.
In the Spambase dataset, our KNFE approach enhanced the
classiﬁcation accuracy along with all the evaluation metrics
by removing three redundant features. From KNFI approach,
the accuracy slightly reduced, taking least prediction time
and performed well in comparision to RFE. This is shown
in Table 7. Also, in the Sonar dataset, KNFE method outperformed all other approaches by removing nine redundant
features. Our KNFI approach also gave better results compared to the AF and the RFE methods, as shown in Table 8.
The relevance of the features in Sonar Dataset is shown
in ﬁg. 2. Some features tend to have very high importance in
accordance to the class label and some features tend to have
no importance or very low importance in accordance to the
class label. We obtain the ranking of the features and then
FIGURE 2. Feature ranking for the sonar dataset.
FIGURE 3. Change in accuracy in the KNFE method.
TABLE 8. Experimental results of sonar dataset.
preform KNFI and KNFE. In ﬁg. 3, we show the change in
the accuracy as we eliminate the least ranked features one at a
time. There is a drastic decrease in accuracy as we eliminate
large number of features. For a particular number of features
eliminated, we observed the highest accuracy.
However, in the TalkingData (Version 1), Criteo and Breast
Cancer datasets shown in Table 9, 10 and 11 respectively,
the performance seems to drop when performing KNFI process. However, KNFE gave either better results or the same
results. This case appears when all the features tend to contribute to ﬁtting the model. In such a scenario, either few features are removed or zero features are removed as in case the
of TalkingData dataset (Table 9). The difference in prediction
for AF contribution and zero feature elimination in KNFE is
due to the change in the pattern of features provided during
the training of data. The performance decreased in KNFI
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
TABLE 9. Experimental results of talking dataset version 1.
TABLE 10. Experimental results of criteo dataset.
TABLE 11. Experimental results of breast cancer dataset.
TABLE 12. Experimental results of UNSW_NB15 dataset.
model. Whenever proper information is not extracted from
the FS process, the classiﬁcation accuracy may be negatively
affected. The corealtion of the features also affect the FS
process. Furthermore, when the sample size is big, the classi-
ﬁer predicts values well with the entire attributes. Also some
datasets tend to perform well with other classifers .
2) MULTICLASS DATASETS
In most of the MultiClass datasets, we can observe the
positive impact of our KNFI as well as KNFE techniques.
In UNSW_NB15 dataset (Table 12), the accuracy increased
by 0.781 percent along with the increase in F1 Score. Our
model selected 16 out of 43 features to get the most efﬁcient
results. Our KNFI method enhanced the accuracy and outperformed all other methods giving us good results.
For the Lung_cancer dataset (Table 13), both our methods
doubled the accuracy as well as the F1 Score and took the least
prediction time. Similarly, for the Lymphographic dataset
(Table 14), our KNFE method gave better results when compared to all the methods.
The Iris Dataset (Table 15 ) performed well when selecting two of the best features from all the four features.
The heart disease dataset (Table 17) had a massive ﬁfteen percent increase in accuracy along with a considerable
increase in F1 Score using KNFI. Even KNFE increased the
TABLE 13. Experimental results of lung_cancer dataset dataset.
TABLE 14. Experimental results of lymphography dataset dataset.
TABLE 15. Experimental results of iris dataset dataset.
TABLE 16. Experimental results of heart disease dataset.
TABLE 17. Experimental results of abalone dataset.
For the Abalone dataset, our KNFI did not produce
improved the performance. However, our KNFE increased
the preformance. The dataset contains less number of features and many classes. This makes the prediction of classiﬁcation much tricky. Also, if additional knowlegde is
not obtained from the FS method, it may not increase the
performance. .
3) OTHER COMPARED WORKS
Other than RFE, we also compared our work with other
previous works. In comparison with the previous studies of
the UNSW_NB15 dataset, our approach of KNFI produced
improved results for binary as well as multiclass datasets.
As a preprocessing step, we remove all the instances that
have NaN values, which decreases the number of instances.
This has enhanced the performance of the classiﬁer. When
our model is run on this dataset, the efﬁcacy of the predictor increased signiﬁcantly. These results can be seen in
Tables (18 & 19).
VOLUME 7, 2019
G. S. Thejas et al.: Mini-Batch NMI: Hybrid FS Method
TABLE 18. Comparision of accuracy for binary UNSW_NB15 with previous
TABLE 19. Comparision of accuracy for UNSW_NB15 multiClass with
previous studies.
TABLE 20. Comparision of ionosphere data with previous studies.
We compared the Ionosphere dataset with the previously existing hybrid feature selection methods. We can
observe in Table 20 that both KNFI and KNFE methods produced much better results with greater classiﬁcation
We also compare the Spambase dataset and Sonar dataset
with the previous works performed in – in terms of
classiﬁcation accuracy since other evaluation metrics have
not been provided. They have calculated the rate of classi-
ﬁcation for the different number of selected features. As a
comparison metric, we have taken the instances with the
highest accuracy as presented in their papers. To give comparative analysis, we have also calculated the accuracy using
KNFE for the same number of features as provided in the
previous papers. Also, we have evaluated using KNFI and
KNFE. They are shown in Table 21 and Table 22. Our method
outperformed other methods giving us good results. The
KNFE(MAX) represents our method without any constraint
of number of required features.
TABLE 21. Comparision of Accuracy for Spambase dataset with previous
TABLE 22. Comparision of Accuracy for Sonar dataset with previous
VI. CONCLUSION
This paper presented a new hybrid method taking into consideration the advantages of both ﬁlter and wrapper method
with no constraint for the user to input the number of features required. In our approach, we used the NMI as a metric to rank the features after clustering by Mini-Batch K
Means. Once we obtained the ranked features, we came up
with two methods to select the features; the feature inclusion method (KNFI) and feature exclusion method (KNFE).
We came up with an algorithm for the feature inclusion
method, and in the feature removal method, we removed the
least important features to get the best performance accuracy.
In most of the datasets, our KNFI method performed well
taking least number of features whereas, in datasets with
least relationship among the features, our KNFE method
produced superior results. For future work, optimizing the
time taken to get the selected features would help to reduce
time complexity. Also, we can come up with better metrics to
get the actual relationships among the features such that the
redundant features are eliminated.