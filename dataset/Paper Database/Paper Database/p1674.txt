Low-shot Visual Recognition by Shrinking and Hallucinating Features
Bharath Hariharan and Ross Girshick
Facebook AI Research (FAIR)
Representation learning
Low-shot learning
Base classes (many training examples)
Classifier (base and novel categories)
Novel classes (few training examples)
Figure 1: Our low-shot learning benchmark in two phases: representation learning and low-shot learning. Modern recognition
models use large labeled datasets like ImageNet to build good visual representations and train strong classiﬁers (representation learning).
However, these datasets only contain a ﬁxed set of classes. In many realistic scenarios, once deployed, the model might encounter novel
classes that it also needs to recognize, but with very few training examples available (low-shot learning). We present two ways of signiﬁcantly
improving performance in this scenario: (1) a novel loss function for representation learning that leads to better visual representations that
generalize well, and (2) a method for hallucinating additional examples for the data-starved novel classes.
Low-shot visual learning—the ability to recognize novel
object categories from very few examples—is a hallmark
of human visual intelligence. Existing machine learning
approaches fail to generalize in the same way. To make
progress on this foundational problem, we present a lowshot learning benchmark on complex images that mimics
challenges faced by recognition systems in the wild. We then
propose (1) representation regularization techniques, and
(2) techniques to hallucinate additional training examples
for data-starved classes. Together, our methods improve the
effectiveness of convolutional networks in low-shot learning,
improving the one-shot accuracy on novel classes by 2.3×
on the challenging ImageNet dataset.
1. Introduction
Recently, error rates on benchmarks like ImageNet 
have been halved, and then halved again. These gains come
from deep convolutional networks (ConvNets) that learn
rich feature representations . It is now clear that if an
application has an a priori ﬁxed set of visual concepts and
thousands of examples per concept, an effective way to build
an object recognition system is to train a deep ConvNet. But
what if these assumptions are not satisﬁed and the network
must learn novel categories from very few examples?
The ability to perform low-shot learning—learning novel
concepts from very few examples—is a hallmark of the human visual system. We are able to do this not only for natural
object categories such as different kinds of animals, but also
for synthetic objects that are unlike anything we’ve seen
before . In contrast, in spite of signiﬁcant improvements
in recognition performance, computational recognition approaches fail to generalize well from few examples . Our
goal in this paper is to make progress towards imparting this
human ability to modern recognition systems.
Our ﬁrst contribution is a low-shot learning benchmark
based on the challenging ImageNet1k dataset. As shown
in Figure 1, our benchmark is implemented in two phases.
In the representation learning phase, the learner tunes its
feature representation on a set of base classes that have many
training instances. In the low-shot learning phase, the learner
is exposed to a set of novel classes with only a few examples per class and must learn a classiﬁer over the joint label
space of base and novel classes. This benchmark simulates a
scenario in which the learner is deployed in the wild and has
to quickly learn novel concepts it encounters from very little
training data. Unlike previous low-shot learning tests (e.g.,
 ) we measure the learner’s accuracy on both the base
and novel classes. This provides a sanity check that accuracy
gains on novel classes do not come at the expense of a large
 
loss in performance on the base classes. This evaluation
protocol follows the standard way that image classiﬁers are
evaluated on popular benchmarks like ImageNet, thus easing the comparison of progress on low-shot learning to the
typical data-rich scenario.
Next, we investigate how to improve the learner’s performance on the benchmark. We build on the intuition
that certain modes of intra-class variation generalize across
categories (e.g., pose transformations). We present a way
of “hallucinating” additional examples for novel classes by
transferring modes of variation from the base classes. These
additional examples improve the one-shot top-5 accuracy on
novel classes by 15 points (absolute) while also maintaining
accuracy on the base classes.
Finally, we show that the feature representation learnt
in the ﬁrst phase has a large impact on low-shot generalization ability. Speciﬁcally, we formulate a loss function
that penalizes the difference between classiﬁers learnt on
large and small datasets, and then draw connections between
this loss and regularization of feature activations. We show
that simply regularizing feature activations can increase oneshot, top-5 accuracy on novel classes by 9 points (absolute)
without harming base class performance. Combining this
better representation with the hallucination strategy pushes
our improvement up to 18 points above the baseline.
2. Related work
One-shot and low-shot learning.
One class of approaches to one-shot learning uses generative models of
appearance that tap into a global or a supercategorylevel prior. Generative models based on strokes or
parts have shown promise in restricted domains such
as hand-written characters . They also work well in
datasets without much intra-class variation or clutter, such
as Caltech 101 . Dixit et al. leverage a corpus with
attribute annotations to generate additional examples by varying attributes. We also propose a way to generate additional
examples, but our model does not use supervision. A similar
approach to synthesizing additional examples by transforming existing ones is presented in early work by Miller et
al. . Our approach generalizes this to realistic, generic
image categories and is non-parametric.
Jia et al. present a promising alternative to generation
using Bayesian reasoning to infer an object category from a
few examples; however, in the full, large-scale training
set is available during training.
Among discriminative approaches, early work attempted
to use a single image of the novel class to adapt classiﬁers
from similar base classes using simple hand-crafted
features. Bertinetto et al. regress from single examples
to a classiﬁers, while Wang and Hebert regress from
classiﬁers trained on small datasets to classiﬁers trained on
large datasets. Recent “meta-learning” techniques learn to
directly map training sets and test examples to classiﬁcation outputs . We compare favorably with these
approaches in our experiments.
Amongst representation learning approaches, metric
learning, such as the triplet loss or siamese
networks , has been used to automatically learn feature representations where objects of the same class are
closer together. Such approaches have shown beneﬁts in
face identiﬁcation . On benchmarks involving more general Internet imagery, such as ImageNet , these methods
perform worse than simple classiﬁcation baselines , and
it is unclear if they can beneﬁt low-shot learning.
Zero-shot learning. Zero-shot recognition uses textual
or attribute-level descriptions of object classes to train classiﬁers. While this problem is different than ours, the motivation is the same: to reduce the amount of data required
to learn classiﬁers. One line of work uses hand-designed
attribute descriptions that are provided to the system for the
novel categories . Another class of approaches
embeds images into word embedding spaces learnt using
large text corpora, so that classiﬁers for novel concepts can
be obtained simply from the word embedding of the concept . A ﬁnal class of approaches attempts to
directly regress to image classiﬁers from textual descriptions
 or from prototypical images of the category .
Similar to our benchmark, Chao et al. propose that zeroshot learning evaluation should also include the training
categories that do have examples. We believe this evaluation
style is good for both zero and low-shot learning.
Transfer learning. The ability to learn novel classes
quickly is one of the main motivations for multitask and
transfer learning. Thrun’s classic paper convincingly argues
that “learning the n-th task should be easier than learning
the ﬁrst,” with ease referring to sample complexity .
However, recent transfer learning research has mostly focussed on the scenario where large amounts of training data
are available for novel classes. For that situation, the efﬁcacy of pre-trained ConvNets for extracting features is well
known . There is also some analysis on what
aspects of ImageNet training aid this transfer . For
faces, Taigman et al. ﬁnd that low-dimensional feature
representations transfer better on faces and Galanti et al. 
provide some theoretical justiﬁcation for this ﬁnding. This
work hints at a link between the complexity of the feature
representation and its generalizability, a link which we also
observe in this paper. We ﬁnd that stronger base classiﬁers
generalize better than weaker classiﬁers (e.g. comparing
ResNet-10 to ResNet-50 ). There have also been novel
losses proposed explicitly to aid transfer, such as the multiverse loss of Littwin and Wolf . Our paper also proposes
novel losses designed speciﬁcally for low-shot learning.
3. A low-shot learning benchmark
Our goal is to build a benchmark for low-shot learning
that mimics situations that arise in practice. Current recognition systems require days or even weeks of training on
expensive hardware to develop good feature representations.
The trained recognition systems may then be deployed as
a service to be used by downstream applications. These
downstream applications may need the ability to recognize
novel categories, but they may have neither the training data
required, nor the infrastructure needed to retrain the models.
Thus, there are two natural phases: in the ﬁrst phase, we
have the data and resources to train sophisticated feature
extractors on large labelled datasets, and in the second phase,
we want to add additional categories to our repertoire at
minimal computational and data cost.
Our low-shot learning benchmark implements a similar
setup. It employs a learner, two training phases, and one
testing phase. The learner is assumed to be composed of a
feature extractor and a multi-class classiﬁer. The benchmark
is agnostic to the speciﬁc form of each component.
During representation learning (training phase one), the
learner receives a ﬁxed set of base categories Cbase, and a
dataset D containing a large number of examples for each
category in Cbase. The learner uses D to set the parameters
of its feature extractor.
In the second phase, which we call low-shot learning, the
learner is given a set of categories Cl that it must learn to
distinguish. Cl = Cbase ∪Cnovel is a mix of base categories
Cbase, and unseen novel categories Cnovel. For each novel
category, the learner has access to only n positive examples,
where n ∈{1, 2, 5, 10, 20}. For the base categories, the
learner still has access to D. The learner may then use these
examples and its feature extractor to set the parameters of
its multi-class classiﬁer while also optionally modifying the
feature extractor.
In the testing phase, the learnt model predicts labels from
the combined label space Cbase ∪Cnovel on a set of previously unseen test images. To measure the variability in
low-shot learning accuracy, we repeat the low-shot learning and testing phases for 5 trials, each time with a random
draw of examples for the novel classes. We report the mean
accuracy and the standard deviation over these trials.
The simplest, and commonly used, baseline approach is
to train a ConvNet with label cross-entropy loss in the representation learning phase and then train a new linear classiﬁer
head in the low-shot learning phase. We now show signiﬁcant improvements on this baseline, ﬁrst by a novel strategy
of hallucinating additional training examples (Section 4) and
then by improving the representation itself (Section 5).
4. Better low-shot learning through generation
In the low-shot learning phase, our goal is to train good
classiﬁers for novel categories from only a few examples.
Intuitively, the challenge is that these examples capture very
little of the category’s intra-class variation. For instance, if
the category is a particular bird species, then we may only
have examples of the bird perched on a branch, and none of
it in ﬂight. The classiﬁer might then erroneously conclude
that this novel category only consists of perched birds.
However, this mode of variation is common to many bird
species, including those we have encountered in the base
classes. From the many base class examples we have seen,
we can understand the transformation that relates perched
bird images to the image of the corresponding bird in ﬂight,
and then use this transformation to “hallucinate” additional
examples for our novel bird category. If we were given the
set of all such category-independent transformations, then
we can hallucinate as many new examples for each novel
category example as there are transformations.
However, we do not have a pre-deﬁned set of transformations that we can apply. But we can take a non-parametric
approach. Any two examples z1 and z2 belonging to the
same category represent a plausible transformation. Then,
given a novel category example x, we want to apply to x
the transformation that sent z1 to z2. That is, we want to
complete the transformation “analogy” z1 : z2 :: x : ?.
We do this by training a function G that takes as input the concatenated feature vectors of the three examples
[φ(x), φ(z1), φ(z2)]. It produces as output a “hallucinated”
feature vector (of the same dimensionality as φ), which corresponds to applying the z1 →z2 transformation to x. We
use an MLP with three fully connected layers for G.
We ﬁrst describe how we train G, and then show how we
use the generated examples in the low-shot learning phase.
4.1. Learning to generate new examples
To train G, we ﬁrst collect a dataset of completed analogies from our base classes. To do this we ﬁrst cluster the
feature vectors of the examples in each base category into
a ﬁxed number of clusters (100). This is to keep computational complexity manageable. Next, for each pair of centroids ca
2 in one category a, we search for another pair
of centroids cb
2 from another category b, such that the
cosine distance between ca
2 is minimized.
We collect all such quadruplets (ca
2) with cosine
similarity greater than zero into a dataset DG. See Figure 2
for example transformation analogies.
We now use the dataset DG to train G. For each quadruplet (ca
2), we feed (ca
2) to the generator. Let
2]) be the output of the generator. We then
minimize λLmse( ˆca
2) + Lcls(W, ˆca
2, a), where:
1. Lmse( ˆca
2) is the mean squared error between the
Figure 2: Example mined analogies. Each row shows the four
image clusters that form the four elements in the analogy. Row 1:
birds with a sky backdrop vs birds with greenery in the background.
Row 2: whole fruits vs cut fruit. Row 3: machines (printer, coffee
making) in isolation vs the same machine operated by a human.
generator’s output and the true target of the analogy ca
2. Lcls(W, ˆca
2, a) is the classiﬁcation loss, where W is the
ﬁxed linear classiﬁer on the base classes learnt during
representation learning, and Lcls(W, x, y) is the log
loss of the classiﬁer W on the example (x, y).
4.2. Using generated examples for low-shot learning
Our generated examples are unlikely to be as good as real
examples, but should provide a useful bias to the classiﬁer
when only a few real examples are present. Therefore we
want to rely on generated examples only when the number
of real examples is low.
Concretely, we have a hyperparameter k (set through
cross-validation), which is the minimum number of examples
per novel category that we want to have. If the actual number
of real examples for a novel category, n, is less than k, then
we additionally generate k −n hallucinated examples. To
generate a synthetic example for a novel category l, we
sample the feature vector of a “seed” example φ(x) from one
of the n real examples for this category, and a pair of cluster
centroids ca
2 from a base category a chosen uniformly at
random. We then pass this triplet through G, and add the
hallucinated feature vector G([φ(x), ca
2]) to our training
set with label l. We then train the logistic regression classiﬁer
on this mix of real and generated data in the usual manner.
5. Better representations for low-shot learning
We now turn to the question of improving representation
learning so as to enable better low-shot learning. As described above, the learner consists of a feature extractor φ
and a classiﬁer W. The goal of representation learning is a
good feature extractor: one that enables learning of effective
classiﬁers from few examples. Intuitively, our goal is to
reduce the difference between classiﬁers trained on large
datasets and classiﬁers trained on small datasets so that those
trained on small datasets generalize better.
We ﬁrst describe a proposal that encodes this goal in a
loss that can be minimized during representation learning.
Then, we draw connections to several alternatives.
5.1. Squared gradient magnitude loss (SGM)
We assume that the classiﬁer W is linear, e.g., the last
layer of a ConvNet. Let D denote a large labeled dataset of
base class images. Typically, training the feature extractor φ
and the classiﬁer W on D involves minimizing a classiﬁcation objective with respect to φ and W:
LD(φ, W) = min
Lcls(W, φ(x), y) (1)
where Lcls(W, x, y) is the multiclass logistic loss on an example x with label y for a linear classiﬁer W:
Lcls(W, x, y) = −log py(W, x)
pk(W, x) =
We modify this training procedure as follows. We simulate low-shot learning experiments on the base classes by
considering several tiny training sets S ⊂D, |S| ≪|D|.
We then want to reduce the difference between classiﬁers
trained on the large dataset D (using the feature extractor φ)
and classiﬁers trained on these small datasets S.
The classiﬁer trained on D is just W. Training a classiﬁer
on S involves solving a minimization problem:
LS(φ, V ) = min
Lcls(V, φ(x), y)
We want the minimizer of this objective to match W. In
other words, we want W to minimize LS(φ, V ). LS(φ, V )
is convex in V (Fig. 3), so a necessary and sufﬁcient condition for this is that the gradient of LS(φ, V ) at V = W,
denoted by ∇V LS(φ, V )|V =W , is 0. More generally, the
closer W is to the global minimum of LS(φ, V ), the lower
the magnitude of this gradient. Thus, we want to minimize:
˜LS(φ, W) = ∥∇V LS(φ, V )|V =W ∥2
The gradient ∇V LS(φ, V ) has a simple analytical form
(see supplemental material for details1 ):
∇V LS(φ, V ) = [g1(S, V ), . . . gK(S, V )]
gk(S, V ) = 1
(pk(V, φ(x)) −δyk)φ(x)
1Supplemental material is available at 
info/lowshotsupp.pdf
Figure 3: Motivation for the SGM loss. We want to learn a representation φ such that the arg min of the small set training objective
LS(φ, V ) matches W, the classiﬁer trained on a large dataset D.
where K is the number of classes, δyk is 1 when y = k and
0 otherwise, and pk is as deﬁned in equation (3).
analytical
function ˜LS(φ, W) :
(x,y)∈S(pk(W, φ(x)) −
δyk)φ(x)∥2. We use this analytical function of W and φ as
We consider an extreme version of this loss where S is a
single example (x, y). In this case,
˜LS(φ, W) =
(pk(W, φ(x)) −δyk)2∥φ(x)∥2
= α(W, φ(x), y)∥φ(x)∥2.
where α(W, φ(x), y) = P
k(pk(W, φ(x)) −δyk)2 is a perexample weight that is higher for data points that are misclassiﬁed. Thus the loss becomes a weighted L2 regularization
on the feature activations.
Our ﬁnal loss, which we call SGM for Squared Gradient
Magnitude, averages this over all examples in D.
α(W, φ(x), y)∥φ(x)∥2
We train our feature representation by minimizing a
straightforward linear combination of the SGM loss and
the original classiﬁcation objective.
LD(φ, W) + λLSGM
λ is obtained through cross-validation.
Batch SGM.
Above, we used singleton sets as our tiny
training sets S. An alternative is to consider every minibatch of examples B that we see during SGD as S. Hence,
we penalize the squared gradient magnitude of the average
loss over B, yielding the loss term: λ˜LB(φ, W). In each
SGD iteration, our total loss is thus the sum of this loss term
and the standard classiﬁcation loss. Note that because this
loss is deﬁned on mini-batches the number of examples per
class in each mini-batch is a random variable. Thus this loss,
which we call “batch SGM”, optimizes for an expected loss
over a distribution of possible low-shot values n.
5.2. Feature regularization-based alternatives
In Eq. (9), it can be shown that α(W, φ(x), y) ∈ 
(see supplementary). Thus, in practice, the SGM loss is
dominated by ∥φ(x)∥2, which is much larger. This suggests
a simple squared L2 norm as a loss:
LD(φ, W) + λ 1
While L2 regularization is a common technique, note that
here we are regularizing the feature representation, as opposed to regularizing the weight vector. Regularizing the
feature vector norm has been a staple of unsupervised learning approaches to prevent degenerate solutions , but
to the best of our knowledge it hasn’t been considered in
supervised classiﬁcation.
We can also consider other ways of regularizing the representation, such as an L1 regularization:
LD(φ, W) + λ 1
We also evaluate other forms of feature regularization
that have been proposed in the literature. The ﬁrst of these
is dropout , which was used in earlier ConvNet architectures , but has been eschewed by recent architectures
such as ResNets . Another form of feature regularization involves minimizing the correlation between the features . We also compare to the multiverse loss 
which was shown to improve transfer learning performance.
Why should feature regularization help?
When learning the classiﬁer and feature extractor jointly, the feature
extractor can choose to encode less discriminative information in the feature vector because the classiﬁer can learn to
ignore this information. However, when learning new classi-
ﬁers in the low-shot phase, the learner will not have enough
data to identify discriminative features for the unseen classes
from its representation. Minimizing the norm of the feature
activations might limit what the learner can encode into the
features, and thus force it to only encode useful information.
5.3. Metric-learning based approaches
A common approach to one-shot learning is to learn a
good distance metric that generalizes to unseen classes. We
train a ConvNet with the triplet loss as a representative baseline method. The triplet loss takes as input a triplet of examples (x, x+, x−), where x and x+ belong to the same
category while x−does not:
Ltriplet(φ(x), φ(x+), φ(x−)) =
max(∥φ(x+) −φ(x)∥−∥φ(x−) −φ(x)∥+ γ, 0).
The loss encourages x−to be at least γ farther away from x
than x+ is.
6. Experiments and discussion
6.1. Low-shot learning setup
We use the ImageNet1k challenge dataset for experiments
because it has a wide array of classes with signiﬁcant intraclass variation. We divided the 1000 ImageNet categories
randomly into 389 base categories and 611 novel categories
(listed in the supplementary material).
Many of the methods we evaluate have hyperparameters
that need to be cross-validated. Since we are interested in
generalization to novel classes, we did not want to crossvalidate on the same set of classes that we test on. We
therefore constructed two disjoint sets of classes by dividing
the base categories into two subsets C1
base (193 classes) and
base (196 classes) and the novel categories into C1
(300 classes) and C2
novel (311 classes). Then, for crossvalidating hyperparameters, we provided the learner with
novel in the low-shot learning and testing
phase, and evaluated its top-5 accuracy on the combined
label set Ccv. The hyperparameter setting that gave the
highest top-5 accuracy was then frozen. We then conducted
our ﬁnal experiments using these hyperparameter settings
by providing the learner with Cﬁn = C2
novel. All
reported numbers in this paper are on Cﬁn.
Our test images are a subset of the ImageNet1k validation
set: we simply restricted it to only include examples from the
classes of interest (Ccv or Cﬁn). Performance is measured
by top-1 and top-5 accuracy on the test images for each
value of n (number of novel examples per category). We
report the mean and standard deviation from 5 runs each
using a different random sample of novel examples during
the low-shot training phase.
To break down the ﬁnal performance metrics, we report
separately the average accuracy on the test samples from the
novel classes and on all test samples. While our focus is on
the novel classes, we nevertheless need to ensure that good
performance on novel classes doesn’t come at the cost of
lower accuracy on the base classes.
6.2. Network architecture and training details
For most of our experiments, we use a small ten-layer
ResNet architecture as our feature extractor φ (details
in supplementary material). When trained on all 1000 categories of ImageNet, it gives a validation top-5 error rate of
16.7% (center crop), making it similar to AlexNet . We
use this architecture because it’s relatively fast to train (2
days on 4 GPUs) and resembles state-of-the-art architectures.
Note that ResNet architectures, as described in , do not
use dropout. Later, we show some experiments using the
larger and deeper ResNet-50 architecture.
For all experiments on representation learning, except
the triplet embedding, the networks are trained from scratch
for 90 epochs on the base classes. The learning rate starts
at 0.1 and is divided by 10 every 30 epochs. The weight
decay is ﬁxed at 0.0001. For the triplet embedding, we ﬁrst
pretrain the network using a softmax classiﬁer and log loss
for 90 epochs, and then train the network further using the
triplet loss and starting with a learning rate of 0.001. We
stop training when the loss stops decreasing (55 epochs).
This schedule is used because, as described in , triplet
networks train slowly from scratch.
For methods that introduce a new loss, there is a hyperparameter that controls how much we weigh the new loss.
Dropout also has a similar hyperparameter that governs what
fraction of activations are dropped. We set these hyperparameters by cross-validation.
For our generator G, we use a three layer MLP with ReLU
as the activation function. We also add a ReLU at the end,
since φ is known to be non-negative. All hidden layers have
a dimensionality of 512.
In the low-shot learning phase, we train the linear classi-
ﬁer using SGD for 10000 iterations with a mini-batch size
of 1000. We cross-validate for the learning rate.
6.3. Training with class imbalance
The low-shot benchmark creates a heavily imbalanced
classiﬁcation problem. During low-shot learning the base
classes may have thousands of examples, while each novel
class has only a few examples. We use two simple strategies to mitigate this issue. One, we oversample the novel
classes when training the classiﬁer by sampling uniformly
over classes and then uniformly within each chosen class.
Two, we L2 regularize the multi-class logistic classiﬁer’s
weights by adding weight decay during low-shot learning.
We ﬁnd that the weight of the classiﬁer’s L2 regularization
term has a large impact and needs to be cross-validated.
6.4. Results
Impact of representation learning. We plot a subset of
the methods2 in Figure 4, and show the full set of numbers
in Tables 1 and 2. The plots show the mean top-5 accuracy,
averaged over 5 low-shot learning trials, for the novel classes,
and over the combined set of novel and base classes. The
standard deviations are low (generally less than 0.5%, see
supplementary material) and are too small to display clearly
as error bars. Top-1 accuracy and numerical values are in
the supplementary material. We observe that:
• When tested just on base classes, many methods perform similarly (not shown), but their performance differs drastically in the low-shot scenarios, especially for
2The subset reduces clutter, making the plots more readable. We omit
results for Batch SGM, Dropout and L1 because Batch SGM performs
similarly to SGM and L2, while L1 and Dropout perform worse.
Representation
Lowshot phase
14.1 33.3 56.2 66.2 71.5
Generation∗+ Classiﬁer 29.7 42.2 56.1 64.5 70.0
23.1 42.4 61.7 69.6 73.8
Generation∗+ Classiﬁer 32.8 46.4 61.7 69.7 73.8
Batch SGM∗
23.0 42.4 61.9 69.9 74.5
20.8 40.8 59.8 67.5 71.6
29.1 47.4 62.3 68.0 70.6
24.5 41.8 56.0 61.3 64.2
Dropout 
26.8 43.9 59.6 66.2 69.5
13.0 33.9 59.3 68.9 73.4
Multiverse Classiﬁer
13.7 30.6 52.5 63.8 71.1
Data augmentation
16.0 31.4 52.7 64.4 71.8
Model Regression 
20.7 39.4 59.6 68.5 73.5
Matching Network 
41.3 51.3 62.1 67.8 71.8
Baseline-ft
12.5 29.5 53.1 64.6 70.4
28.2 51.0 71.0 78.4 82.3
Generation∗+ Classiﬁer 44.8 59.0 71.4 77.7 82.3
37.8 57.1 72.8 79.1 82.6
Generation∗+ Classiﬁer 45.1 58.8 72.7 79.1 82.6
Table 1: Top-5 accuracy on only novel classes. Best are bolded and
blue; the second best are italicized and red. ∗Our methods.
small n. Thus, accuracy on base classes does not generalize to novel classes, especially when novel classes
have very few training examples.
• Batch SGM, SGM, and L2 are top performers overall
with L2 being better for small n. They improve novel
class accuracy by more than 10 points for small n (1 or
2) and more than 3 points for n > 10. L1 also improves
low-shot performance, but the gains are much smaller.
• Dropout is on par with SGM for small n, but ends up
being similar or worse than the baseline for n ≥5 in
terms of all class accuracy. Empirically, dropout also reduces feature norm, suggesting that implicit L2 feature
regularization might explain some of these gains.
• Triplet loss improves accuracy for small n but is 5
points worse than the baseline for n = 20 in terms of
all class accuracy. While more sophisticated variants of
the triplet loss may improve performance , feature
regularization is both effective and much simpler.
• The decov loss provides marginal gains for higher
values of n but is outperformed by the feature regularization alternatives.
As an additional experiment, we also attempted to ﬁnetune the baseline representation on all the base class examples and the small set of novel class examples. We found
that this did not improve performance over the frozen representation (see Baseline-ft in Tables 1 and 2). This indicates
that ﬁnetuning the representation is not only expensive, but
also does not help in the low-shot learning scenario.
Impact of generation. Figure 5 shows the top-5 accu-
Representation
Lowshot phase
43.0 54.3 67.2 72.8 75.9
Generation∗+ Classiﬁer 52.4 59.4 67.5 72.6 76.9
49.4 60.5 71.3 75.8 78.1
Generation∗+ Classiﬁer 54.3 62.1 71.3 75.8 78.1
Batch SGM∗
49.3 60.5 71.4 75.8 78.5
47.1 58.5 69.2 73.7 76.1
52.7 63.0 71.5 74.8 76.4
47.6 57.1 65.2 68.4 70.2
Dropout 
50.1 59.7 68.8 72.7 74.7
43.3 55.7 70.1 75.4 77.9
Multiverse Classiﬁer
44.1 54.2 67.0 73.2 76.9
Data Augmentation
44.9 54.0 66.4 73.0 77.2
Model Regression 
46.4 56.7 66.8 70.4 72.0
Matching Network 
55.0 61.5 69.3 73.4 76.2
Baseline-ft
41.7 51.7 65.0 71.2 74.5
54.1 67.7 79.1 83.2 85.4
Generation∗+ Classiﬁer 63.1 71.5 78.8 82.6 85.4
60.0 71.3 80.0 83.3 85.2
Generation∗+ Classiﬁer 63.6 71.5 80.0 83.3 85.2
Table 2: Top-5 accuracy on base and novel classes. Best are bolded
and blue; the second best are italicized and red. ∗Our methods.
racies on novel classes and on base+novel classes for our
generation method applied on top of the baseline representation and the SGM feature representation. The numbers
are in Tables 1 and 2. Note that we only generate examples
when n < k, with k = 20 for baseline representations and 5
for SGM (see Section 4.2). We observe that the generated
examples provide a large gain of over 9 points for n = 1, 2
on the novel classes for the baseline representation. When
using the SGM representation, the gains are smaller, but
signiﬁcant.
We also compared our generation strategy to common
forms of data augmentation (aspect ratio and scale jitter, horizontal ﬂips, and brightness, contrast and saturation changes).
Data augmentation only provides small improvements (about
1 percentage point). This conﬁrms that our generation strategy produces more diverse and useful training examples than
simple data augmentation.
Comparison to other low-shot methods. We also compared to two recently proposed low-shot learning methods:
matching networks and model regression . Model
regression trains a small MLP to regress from the classiﬁer
trained on a small dataset to the classiﬁer trained on the
full dataset. It then uses the output from this regressor to
regularize the classiﬁer learnt in the low-shot learning phase.
Matching networks proposes a nearest-neighbor approach
that trains embeddings end-to-end for the task of low-shot
learning. We apply both these techniques on our baseline
representation.
For both these methods, the respective papers evaluated
on the novel classes only. In contrast, real-world recognition
Examples per novel class (n)
Top-5 accuracy (%)
Novel classes
Examples per novel class (n)
Top-5 accuracy (%)
All classes
Examples per novel class (n)
Top-5 accuracy (%)
All classes (zoom n = 1,2)
Figure 4: Representation learning comparison. Top-5 accuracy on ImageNet1k val. Top-performing feature regularization methods
reduce the training samples needed to match the baseline accuracy by 2x. Note the different Y-axis scales.
Examples per novel class (n)
Top-5 accuracy (%)
Novel classes
baseline+gen
Examples per novel class (n)
Top-5 accuracy (%)
All classes
Examples per novel class (n)
Top-5 accuracy (%)
All classes (zoom n = 1,2)
Figure 5: Comparisons with and without example generation. Top-5 accuracy on ImageNet1k val. Note the different Y-axis scales.
Examples per novel class (n)
Top-5 accuracy (%)
Novel classes
baseline+gen
match. net
Examples per novel class (n)
Top-5 accuracy (%)
All classes
Examples per novel class (n)
Top-5 accuracy (%)
All classes (zoom n = 1,2)
Figure 6: Comparison to recently proposed methods. Top-5 accuracy on ImageNet1k val. Note the different Y-axis scales.
systems will need to discriminate between data-starved novel
concepts, and base classes with lots of data. We adapt these
methods to work with both base and novel classes as follows.
For model regression, we only use the model regressor-based
regularization on the novel classes, with the other classiﬁers
regularized using standard weight decay. We use one-vs-all
classiﬁers to match the original work.
Matching networks require the training dataset to be kept
in memory during test time. To make this tractable, we use
100 examples per class, with the novel classes correspondingly oversampled.
Comparisons between these methods and our approach
are shown in Figure 6. We ﬁnd that model regression improves signiﬁcantly over the baseline, but our generation
strategy works better for low n. Model regression also hurts
overall accuracy for high n.
Matching networks work very well on novel classes. In
terms of overall performance, they perform better than our
generation approach on top of the baseline representation,
but worse than generation combined with the SGM representation, especially for n > 2. Further, matching networks are
based on nearest neighbors and keep the entire training set
in memory, making them much more expensive at test time
than our simple linear classiﬁers.
Deeper networks. We also evaluated our approach on
the ResNet-50 network architecture to test if our conclusions
extend to deeper convnets that are now in use (Tables 1
and 2). First, even with the baseline representation and
without any generation we ﬁnd that the deeper architecture
also leads to improved performance in all low-shot scenarios.
However, our SGM loss and our generation strategy further
improve this performance. Our ﬁnal top-5 accuracy on novel
classes is still more than 8 points higher for n = 1, 2, and
our overall accuracy is about 3 points higher, indicating that
our contributions generalize to deeper and better models.
7. Conclusion
recognition
benchmark of realistic complexity,
(2) the squared
gradient magnitude (SGM) loss that encodes the endgoal of low-shot learning, and (3) a novel way of
transferring modes of variation from base classes to
data-starved ones. Source code and models are available at:
 
low-shot-shrink-hallucinate.