REINFORCEMENT LEARNING
USING QUANTUM BOLTZMANN MACHINES
DANIEL CRAWFORD, ANNA LEVIT, NAVID GHADERMARZY, JASPREET S. OBEROI,
AND POOYA RONAGH
Abstract. We investigate whether quantum annealers with select chip layouts can outperform
classical computers in reinforcement learning tasks.
We associate a transverse ﬁeld Ising spin
Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use
simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system.
We design a reinforcement learning algorithm in which the set of visible nodes representing the
states and actions of an optimal policy are the ﬁrst and last layers of the deep network. In absence
of a transverse ﬁeld, our simulations show that DBMs are trained more eﬀectively than restricted
Boltzmann machines (RBM) with the same number of nodes. We then develop a framework for
training the network as a quantum Boltzmann machine (QBM) in the presence of a signiﬁcant
transverse ﬁeld for reinforcement learning. This method also outperforms the reinforcement learning
method that uses RBMs.
1. Introduction
Recent theoretical extensions of the quantum adiabatic theorem suggest the possibility of using quantum devices with manufactured spins as samplers of the instantaneous
steady states of quantum systems. With this motivation, we consider reinforcement learning as
the computational task of interest, and design a method of reinforcement learning consisting of
sampling from a layout of quantum bits similar to that of a deep Boltzmann machine (DBM) (see
Fig. 1b for a graphical representation). We use simulated quantum annealing (SQA) to demonstrate
the advantage of reinforcement learning using deep Boltzmann machines and quantum Boltzmann
machines over their classical counterpart, for small problem instances.
Reinforcement learning ( , known also as neuro-dynamic programming ) is an area of optimal
control theory at the intersection of approximate dynamic programming and machine learning. It
has been used successfully for many applications, in ﬁelds such as engineering , sociology
 , and economics .
It is important to diﬀerentiate between reinforcement learning and common streams of research
in machine learning. For instance, in supervised learning, the learning is facilitated by training
samples provided by a source external to the agent and the computer. In reinforcement learning,
the training samples are provided only by the interaction of the agent itself with the environment.
For example, in a motion planning problem in an uncharted territory, it is desired that the agent
Date: January 7, 2019.
Key words and phrases. Reinforcement learning, Machine learning, Neuro-dynamic programming, Markov decision
process, Quantum Monte Carlo simulation, Simulated quantum annealing, Restricted Boltzmann machine, Deep
Boltzmann machine, General Boltzmann machine, Quantum Boltzmann machine.
 
 
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
Figure 1. (a) The general RBM layout used in RBM-based reinforcement learning. The visible
layer on the left consists of state and action nodes, and is connected to the hidden layer, forming a
complete bipartite graph. (b) The general DBM layout used in DBM-based reinforcement learning.
The visible nodes on the left represent states and the visible nodes on the right represent actions.
The training procedure captures the correlations between states and actions in the weights of the
edges between the nodes.
learns in the fastest possible way to navigate correctly, with the fewest blind decisions required to be
made. This is known as the dilemma of exploration versus exploitation; that is, neither exploration
nor exploitation can be pursued exclusively without facing a penalty or failing at the task. The
goal is hence not only to design an algorithm that eventually converges to an optimal policy, but
for it to be able to generate good policies early in the learning process. We refer the reader to [8,
Ch. 1.1] for a thorough introduction to use cases and problem scenarios addressed by reinforcement
The core idea in reinforcement learning is deﬁning an operator on the Banach space of realvalued functions on the set of states of a system such that a ﬁxed point of the operator carries
information about an optimal policy of actions for a ﬁnite or inﬁnite number of decision epochs.
A numerical method for computing this ﬁxed point is to explore this function space by travelling
in a direction that minimizes the distance between two consecutive applications of the contraction
mapping operator .
This optimization task, called learning in the context of reinforcement learning, can be performed
by locally parametrizing the above function space using a set of auxiliary variables, and applying
a gradient method to these variables. One approach for such a parametrization, due to , is to
use the weights of a restricted Boltzmann machine (RBM) (see Fig. 1a) as the parameters, and the
free energy of the RBM as an approximator for the elements in the function space. The descent
direction is then calculated in terms of the expected values of the nodes of the RBM.
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
Figure 2. (a) A 3 × 5 maze. W represents a wall, R is a positive real number representing a
reward, and P is a real number representing a penalty. (b) The previous maze with two additional
stochastic rewards. (c) The set of all optimal actions for each cell of the maze in Fig (a). An
optimal traversal policy is a choice of any combination of these actions. (d) A sample conditional
state transition probability for a windy problem with no obstacles (left), and with a wall present
It follows from the universal approximation theorem that RBMs can approximate any joint
distribution over binary variables .
However, in the context of reinforcement learning,
RBMs are not necessarily the best choice for approximating Q-functions relating to Markov decision
processes because RBMs may require an exponential number of hidden variables with respect to
the number of visible variables in order to approximate the desired joint distribution . On
the other hand, DBMs have the potential to model higher-order dependencies than RBMs, and are
more robust than deep belief networks .
One may, therefore, consider replacing the RBM with other graphical models and investigating
the performance of the models in the learning process. Except in the case of RBMs, calculating statistical data from the nodes of a graphical model amounts to sampling from a Boltzmann
distribution, creating a bottleneck in the learning procedure. Therefore, any improvement in the
eﬃciency of Boltzmann distribution sampling is beneﬁcial for reinforcement learning and machine
learning in general.
As we explain in what follows, DBMs are good candidates for reinforcement learning tasks.
Moreover, an important advantage of a DBM layout for a quantum annealing system is that the
proximity and couplings of the qubits in the layout are similar to those of a sequence of bipartite
blocks in D-Wave Systems’ devices , and it is therefore feasible that such layouts could be
manufactured in the near future. In addition, embedding Boltzmann machines in larger quantum
annealer architectures is problematic when excessively large weights and biases are needed to emulate logical nodes of the Boltzmann machine using chains and clusters of physical qubits. These
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
are the reasons why, instead of attempting to embed a Boltzmann machine structure on an existing
quantum annealing system as in , we work under the assumption that the network
itself is the native connectivity graph of a near-future quantum annealer, and, using numerical
simulations, we attempt to understand its applicability to reinforcement learning.
We also refer the reader to current trends in machine learning using quantum circuits, speciﬁcally,
 and for reinforcement learning, and and for training quantum Boltzmann machines
with applications in deep learning and tomography. To the best of our knowledge, the present paper
complements the literature on quantum machine learning as the ﬁrst proposal on reinforcement
learning using adiabatic quantum computation.
Quantum Monte Carlo (QMC) numerical simulations have been found to be useful in simulating time-dependant quantum systems. Simulated quantum annealing (SQA) , one of the
many ﬂavours of QMC methods, is based on the Suzuki–Trotter expansion of the path integral
representation of the Hamiltonian of Ising spin models in the presence of a transverse ﬁeld driver
Hamiltonian. Even though the eﬃciency of SQA for ﬁnding the ground state of an Ising model is
topologically obstructed , we consider the samples generated by SQA to be good approximations of the Boltzmann distribution of the quantum Hamiltonian . Experimental studies have
shown similarities in the behaviour of SQA and that of quantum annealing and its physical
realization by D-Wave Systems .
We expect that when SQA is set such that the ﬁnal strength of the transverse ﬁeld is negligible,
the distribution of the samples approaches the classical limit one expects to observe in absence
of the transverse ﬁeld.
Another classical algorithm which can be used to obtain samples from
the Boltzmann distribution is conventional simulated annealing (SA), which is based on thermal
annealing. Note that this algorithm can be used to create Boltzmann distributions from the Ising
spin model only in the absence of a transverse ﬁeld. It should, therefore, be possible to use SA or
SQA to approximate the Boltzmann distribution of a classical Boltzmann machine. However, unlike
in the case of SA, it is possible to use SQA not only to approximate the Boltzmann distribution
of a classical Boltzmann machine, but also that of a graphical model in which the energy operator
is a quantum Hamiltonian in the presence of a transverse ﬁeld. These graphical models, called
quantum Boltzmann machines (QBM), were ﬁrst introduced in .
We use SQA simulations to provide evidence that a quantum annealing device that approximates
the distribution of a DBM or a QBM may improve the learning process compared to a reinforcement
learning method that uses classical RBM techniques. Other studies have shown that SQA is more
eﬃcient than thermal SA . Therefore, our method, used in conjunction with SQA, can also
be viewed as a quantum-inspired approach for reinforcement learning.
What distinguishes our work from current trends in quantum machine learning is that (i) we
consider the use of quantum annealing in reinforcement learning applications rather than frequently
studied classiﬁcation or recognition problems; (ii) using SQA-based numerical simulations, we assume that the connectivity graph of a DBM directly maps to the native layout of a feasible quantum
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
annealer; and (iii) the results of our experiments using SQA to simulate the sampling of an entangled system of spins suggest that using quantum annealers in reinforcement learning tasks can oﬀer
an advantage over thermal sampling.
2. Preliminaries
2.1. Adiabatic Evolution of Open Quantum Systems. The evolution of a quantum system
under a slowly changing time-dependent Hamiltonian is characterized by the quantum adiabatic
theorem (QAT). QAT has a long history going back to the work of Born and Fock . Colloquially,
QAT states that a system remains close to its instantaneous steady state provided there is a gap
between the eigenenergy of the steady state and the rest of the Hamiltonian’s spectrum at every
point in time if the evolution is suﬃciently slow. This result motivated and to introduce the
closely related paradigms of quantum computing known as quantum annealing (QA) and adiabatic
quantum computation (AQC).
QA and AQC, in turn, inspired eﬀorts in the manufacturing of physical realizations of adiabatic evolution via quantum hardware ( ). In reality, the manufactured chips operate at nonzero
temperature and are not isolated from their environment. Therefore, the existing adiabatic theory
did not describe the behaviour of these machines. A contemporary investigation in quantum adiabatic theory was thus initiated to study adiabaticity in open quantum systems ( ).
These references prove adiabatic theorems to various degrees of generality and under a variety of
assumptions about the system.
In fact, develops an adiabatic theory for equations of the form
ε ˙x(s) = L(s)x(s),
where L is a family of linear operators on a Banach space and L(s) is a generator of a contraction semigroup for every s. This provides a general framework that encompasses many adiabatic
theorems, including that of classical stochastic systems, all the way to quantum evolutions of open
systems generated by Lindbladians. The manifold of instantaneous stationary states is identical
to ker(L(s)), and shows that the dynamics of the system are parallel-transported along this
manifold as ε →0.
An example of (1) is the case in which the Banach space is the space of bounded operators on a
Hilbert space, and in this case we study the evolution of the density matrix ρ of a quantum system.
The Lindbladian is deﬁned via the adjoint action of a Hermitian H on ρ, and couplings to the heat
bath are represented via a family of operators Γα with P
αΓα being bounded:
Lρ = −i[H, ρ] + 1
α] + [Γα, ρΓ∗
In the work of , it was then proven that ρ(s) is parallel-transported along ker(L(s)), and that if
(i) is the generator of a contraction semigroup;
(ii) has closed and complementary range and kernel;
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
(iii) is Ck with respect to s; and
(iv) is constant near the endpoints s = 0 and s = 1;
then the solution to (1) with initial condition in ker(L(0)) deviates only in O(εk) from ker(L(1))
The authors of focuses on estimating the adiabatic error in terms of the physical parameters
of the theory. In particular, they study the case of a quantum system coupled to a thermal bath
satisfying the Kubo–Martin–Schwinger (KMS) condition. Given a distance δ, in order for the norm
of the solution of (1) to stay δ-close to the instantaneous steady state of the system at s = 1, they
show that ε has to decrease at a rate of O(λ2), where λ denotes the smallest nonzero eigenvalue in
L. Note that the KMS condition implies that the Gibbs state exp(−βH(s))/ tr[exp(−βH(s))] is,
in fact, in ker(L(s)).
This stream of research suggests promising opportunities to use quantum annealers to sample
from the Gibbs state of a quantum Hamiltonian using adiabatic evolution.
In this paper, the
transverse ﬁeld Ising model (TFIM) has been the centre of attention. In practice, due to additional
complications in quantum annealing (e.g., level crossings and gap closure), the samples gathered
from the quantum annealer are far from the Gibbs state of the ﬁnal Hamiltonian. In fact, 
suggests that the distribution of the samples would correspond more closely to an instantaneous
Hamiltonian at an intermediate point in time, called the freeze-out point. Therefore, our goal is
to investigate the applicability of sampling from a TFIM with signiﬁcant Γ to free energy–based
reinforcement learning.
2.2. Simulated Quantum Annealing. Simulated quantum annealing (SQA) methods are a class
of quantum-inspired algorithms that perform discrete optimization by classically simulating quantum tunnelling phenomena (see [43, p. 422] for an introduction). The algorithm used in this paper
is a single spin-ﬂip version of quantum Monte Carlo numerical simulation based on the Suzuki–
Trotter formula, and uses the Metropolis acceptance probabilities. The SQA algorithm simulates
the quantum annealing phenomena of an Ising spin model with a transverse ﬁeld, that is,
where σz and σx represent the Pauli z- and x-matrices, respectively, the indices i and j range over
the sites of the system, and the time t ranges from 0 to 1. In this quantum evolution, the strength
of the transverse ﬁeld is slowly reduced to zero at ﬁnite temperature. In our implementations, we
have used a linear transverse ﬁeld schedule for the SQA algorithm as in and . Based on the
Suzuki–Trotter formula, the key idea of this algorithm is to approximate the partition function of
the Ising model with a transverse ﬁeld as a partition function of a classical Hamiltonian denoted
by Heﬀ, corresponding to a classical Ising model of one dimension higher. More precisely,
Heﬀ(σ) = −
r σikσjk −J+ X
σikσi,k+1 −
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
where r is the number of replicas, J+ =
2β log coth
, and σik represent spins of the classical
system of one dimension higher.
In our experiments, the strength Γ of the transverse ﬁeld is scheduled to linearly decrease from
20.00 to one of Γf = 0.01 or 2.00. The inverse temperature β is set to the constant 2.00. The initial
value, 20.00, of the transverse ﬁeld is empirically chosen to be well above the coupling strengths
created during the training. Each spin is replicated 25 times to represent the Trotter slices in the
extra dimension. The simulation is set to iterate over all replications of all spins one time per
sweep, and the number of sweeps is set to 300, which appears to be large enough for the sizes of
Ising models constructed during our experiments. For each instance of input, the SQA algorithm
is run 150 times. After termination, the conﬁguration of each replica, as well as the conﬁguration
of the entire classical Ising model of one dimension higher, is returned.
Although the SQA algorithm does not follow the dynamics of a physical quantum annealer explicitly, it is used to simulate this process, as it captures major quantum phenomena such as tunnelling
and entanglement . In , for example, it is shown that quantum Monte Carlo simulations can
be used to understand the tunnelling behaviour in quantum annealers. As mentioned previously,
it readily follows from the results of that the limiting distribution of SQA is the Boltzmann
distribution of Heﬀ. This makes SQA a candidate classical algorithm for sampling from Boltzmann
distributions of classical and quantum Hamiltonians. The former is achieved by setting Γf ≃0,
and the latter by constructing an eﬀective Hamiltonian of the system of one dimension higher, representing the quantum Hamiltonian with non-negligible Γf. Alternatively, a classical Monte Carlo
simulation used to sample from the Boltzmann distribution of the classical Ising Hamiltonian is the
SA algorithm, based on thermal ﬂuctuations of classical spin systems.
2.3. Markov Decision Process. The stochastic control problem of interest to us is a Markov
decision process (MDP), deﬁned as having:
(i) ﬁnite sets of states S and actions A;
(ii) a controlled Markov chain , deﬁned by a transition kernel P(s′ ∈S|s ∈S, a ∈A);
(iii) a real-valued function r : S × A →R, known as the immediate reward structure; and
(iv) a constant γ ∈[0, 1), known as the discount factor.
A function π : S →A is called a stationary policy; that is, it is a choice of action π(s) for every
state s independent of the point in time that the controlled process reaches s. The application of a
stationary policy π reduces the MDP into a time-homogeneous Markov chain Π, with a transition
probability P(s′|s, π(s)). The random process Π with initial condition Π0 = s we denote by Πs.
Our Markov decision problem is to ﬁnd
π∗(s) = argmax
When both S and A are ﬁnite, the MDP is said to be ﬁnite.
The transition kernel does not need to be time-homogeneous; however, this deﬁnition suﬃces for the purposes of
this work.
For more-general statements, see .
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
V (π, s) = E
2.3.1. Maze Traversal as a Markov Decision Process. Maze traversal is a problem typically used
to develop and benchmark reinforcement learning algorithms . A maze is structured as a twodimensional grid of r rows and c columns in which a decision-making agent is free to move up,
down, left, or right, or to stand still. During the maze traversal, the agent encounters obstacles
(e.g., walls), rewards (e.g., goals), and penalties (negative rewards, e.g., a pit). Each cell of the
maze can contain either a deterministic or stochastic reward, a wall, a pit, or a neutral value.
Fig. 2a and Fig. 2b show examples of two mazes. Fig. 2c shows the corresponding solutions to the
maze in Fig. 2a.
The goal of the reinforcement learning algorithm in the maze traversal problem is for the agent
to learn the optimal action to take in each cell of the maze by maximizing the total reward, that is,
ﬁnding a route across the maze that avoids walls and pits while favouring rewards. This problem
can be modelled as an MDP determined by the following components:
• The state of the system is the agent’s position within the maze. The position state s takes values
in the set of states
S = {1, ..., r} × {1, ..., c}.
• In any state, the agent can decide to take one of the ﬁve actions
a ∈{↑, ↓, ←, →, ⟲}.
These actions will guide the agent through the maze. An action that would lead the agent into
a wall (W) or outside of the maze boundary is treated as an inadmissible action. Each action
can be viewed as an endomorphism on the set of states
If a = ⟲, then a(s) = s; otherwise, a(s) is the state adjacent to S in the direction shown by a.
We do not consider training samples where a is inadmissible.
• The transition kernel determines the probability of the agent moving from one state to another
given a particular choice of action. In the simplest case, the probability of transition from s to
a(s) is one:
P(a(s)|s, a) = 1.
We call the maze clear if the associated transition kernel is as above, as opposed to the windy
maze, in which there is a nonzero probability that if the action a is taken at state s, the next
state will diﬀer from a(s).
• The immediate reward r(s, a) that the agent gains from taking an action a in state s is the value
contained in the destination state. Moving into a cell containing a reward returns the favourable
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
value R, moving into a cell containing a penalty returns the unfavourable value P, and moving
into a cell with no reward returns a neutral value in the interval (P, R).
• A discount factor for future rewards is a non-negative constant γ < 1. In our experiments, this
discount factor is set to γ = 0.8. The discount factor is a feature of the problem rather than
a free parameter of an implementation. For example, in a ﬁnancial application scenario, the
discount factor might be a function of the risk-free interest rate.
The immediate reward for moving into a cell with a stochastic reward is given by a random
variable R. If an agent has prior knowledge of this distribution, then it should be able to treat the
cell as one with a deterministic reward value of E[R]. This allows us to ﬁnd the set of all optimal
policies in each maze instance. This policy information is denoted by α∗: S →2A, associating with
each state s ∈S a set of optimal actions α∗(s) ⊆A.
In our maze model, the neutral value is set to 100, the reward R = 200, and the penalty P = 0.
In our experiments, the stochastic reward R is simulated by drawing a sample from the Bernoulli
distribution 200 Ber(0.5); hence, it has the expected value E[R] = 100, which is identical to the
neutral value. Therefore, the solutions depicted in Fig. 2c are solutions to the maze of Fig. 2b as
2.4. Value Iteration. Bellman writes V (π, s) recursively in the following manner using the
monotone convergence theorem:
V (π, s) = E
0))] + γ E
= E[r(s, π(s))] + γ
P(s′|s, π(s)) V (π, s′) .
In particular, it leads to the Bellman optimality equation:
V ∗(s) = V (π∗, s) = max
E[r(s, a)] + γ
P(s′|s, a) V ∗(s′)
Hence, V ∗is a ﬁxed point for the operator
TV (f) : s 7→max
E[r(s, a)] + γ
on the space L∞(S) of bounded functions S →R endowed with the max norm. Here, the integral
is taken with respect to the probability measure on S, induced by the conditional probability
distribution P(s′|s, a). It is easy to check that TV is a contraction mapping, and thus V ∗is the
unique ﬁxed point of TV and the uniform limit of any sequence of functions {T n
V f}n. Numerical
computation of this limit using (6), called value iteration, is a common method of solving the
Markov decision problem (4). However, even the ε-optimal algorithms for this approach depend
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
heavily on the cardinality of both S and A, and suﬀer from the curse of dimensionality .
Moreover, the value iteration method requires having full knowledge of the transition probabilities,
as well as the distribution of the immediate rewards.
2.5. Q-functions. For a stationary policy π, the Q-function (also known as the action–value function) is deﬁned as a mapping of a pair (s, a) to the expected value of the reward of the Markov
chain that begins with taking action a at initial state s and continuing according to π :
Q(π, s, a) = E[r(s, a)] + E
It is straightforward to check that
V (π∗, s) = max
Q(π∗, s, a),
and for Q∗(s, a) = maxπ Q(π, s, a) = Q(π∗, s, a), the optimal policy for the MDP can be retrieved
via the following:
π∗(s) = argmax
This reduces the Markov decision problem to computing Q∗(s, a). The Bellman optimality equation
for Q∗(s, a) is
Q∗(s, a) = E[r(s, a)] + γ
P(s′|s, a) max
Q∗(s′, a′),
which makes Q∗the ﬁxed point of a diﬀerent operator
TQ(f) : (s, a) 7→E[r(s, a)] + γ
deﬁned on L∞(S × A).
2.6. Temporal-Diﬀerence Gradient Descent. In this section, we derive the Q-learning method
for MDPs. From the previous section, we know that starting from an initial Q0 : S × A →R, the
sequence {Qn = T n
QQ0} converges to Q∗. The diﬀerence
Qn+1(s, a) −Qn(s, a) = E[r(s, a)] + γ
P(s′|s, a) max
is called the temporal diﬀerence of Q-functions, and is denoted by ETD.
Employing a gradient approach to ﬁnd the ﬁxed point of T on L∞(S × A) involves locally
parametrizing the functions in this space by a vector of parameters θ, that is,
Q(s, a) = Q(s, a; θ),
and travelling in the direction that minimizes ∥ETD∥2:
∆θ ∝−ETD∇θETD .
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
The method TD(0) consists of treating the (∗) in (8) as constant with respect to the parameterization θ, in which case we may write
∆θ ∝∼ETD(s, a)∇θQ(s, a; θ).
For an agent agnostic with respect to the transition kernel or the distribution of the reward
r(s, a), or both, this update rule for θ is not possible. The alternative is to substitute, at each
iteration, the expected value
P(s′|s, a) max
Qn(sn+1, a)
by maxa Qn(sn+1, a), where sn+1 is drawn from the probability distribution P(s′|s, a), and substitute
E[r(sn, an)] by a sample of r(sn, an). This leads to a successful Monte Carlo training method called
Q-learning.
In what follows, we explain the case in which θ comprises the weights of a Boltzmann machine.
Let us begin by introducing clamped Boltzmann machines, which are of particular importance in
the case of reinforcement learning.
2.7. Clamped Boltzmann Machines. A classical Boltzmann machine is a type of stochastic
neural network with two sets V and H of visible and hidden nodes, respectively. Both visible and
hidden nodes represent binary random variables. We use the same notation for a node and the
binary random variable it represents. The interactions between the variables represented by their
respective nodes are speciﬁed by real-valued weighted edges of the underlying undirected graph. A
GBM, as opposed to models such as RBMs and DBMs, allows weights between any two nodes.
The energy of the classical Boltzmann machine is
E (v, h) = −
with wvh, wvv′, and whh′ denoting the weights between visible and hidden, visible and visible, and
hidden and hidden nodes of the Boltzmann machine, respectively, deﬁned as a function of binary
vectors v and h corresponding to the visible and hidden variables, respectively.
A clamped GBM is a neural network whose underlying graph is the subgraph obtained by
removing the visible nodes for which the eﬀect of a ﬁxed assignment v of the visible binary variables
contributes as constant coeﬃcients to the associated energy
A clamped quantum Boltzmann machine (QBM) has the same underlying graph as a clamped
GBM, but instead of a binary random variable, a qubit is associated to each node of the network.
The energy function is substituted by the quantum Hamiltonian
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
h represent the Pauli z-matrices and σx
h represent the Pauli x-matrices. Thus, a clamped
QBM with Γ = 0 is equivalent to a clamped classical Boltzmann machine. This is because Hv
is a diagonal matrix in the σz-basis, the spectrum of which is identical to the range of Ev. The
remainder of this section is formulated for the clamped QBMs, acknowledging that it can easily be
specialized to clamped classical Boltzmann machines.
kBT be a ﬁxed thermodynamic beta. For an assignment of visible variables v, F(v)
denotes the equilibrium free energy, and is deﬁned as
F(v) := −1
β ln Zv = ⟨Hv⟩+ 1
β tr(ρv ln ρv) .
Here, Zv = tr(e−βHv) is the partition function of the clamped QBM and ρv is the density matrix
Zv e−βHv. The term −tr(ρv ln ρv) is the entropy of the system. The notation ⟨· · · ⟩is used
for the expected value of any observable with respect to the Gibbs measure, in particular,
tr(Hve−βHv).
2.8. Reinforcement Learning Using Clamped Boltzmann Machines. In this section, we
explain how a general Boltzmann machine (GBM) can be used to provide a Q-function approximator
in a Q-learning method. To the best of our knowledge, this derivation has not been previously given,
although it can be readily derived from the ideas presented in and . Following , the goal
is to use the negative free energy of a Boltzmann machine to approximate the Q-function through
the relationship
Q(s, a) ≈−F(s, a) = −F(s, a; θ)
for each admissible state–action pair (s, a) ∈S × A. Here, s and a are binary vectors encoding the
state s and action a on the state nodes and action nodes, respectively, of the Boltzmann machine.
In reinforcement learning, the visible nodes of the GBM are partitioned into two subsets of state
nodes S and action nodes A.
The parameters θ, to be trained according to a TD(0) update rule (see Sec. 2.6), are the weights
in a Boltzmann machine. For every weight w, the update rule is
∆w = −ε(rn(sn, an) + γ max
Q(sn+1, a) −Q(sn, an))∂F
From (13), we obtain
βe−βHs,a ∂
Therefore, the update rule for TD(0) for the clamped QBM can be rewritten as
∆wvh = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))v⟨σz
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
∆whh′ = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))⟨σz
where the thermodynamic beta is absorbed into the learning rate ε, and
an+1 = argmax
Q(sn+1, a).
Here, h and h′ denote two distinct hidden nodes and (by a slight abuse of notation) the letter v
stands for a visible (state or action) node, and also the value of the variable associated to that
To approximate the right-hand side of each of (14) and (15), we use SQA experiments. By [49,
Theorem 6], we may ﬁnd the expected values of the observables ⟨σz
h′⟩by averaging
the corresponding spins in the classical Ising model of one dimension higher used in SQA. To
approximate the Q-function, we take advantage of [49, Theorem 4] and use (13) applied to this
classical Ising model. More precisely, let Heﬀ
v represent the Hamiltonian of the classical Ising model
of one dimension higher and the associated energy function E eﬀ
v . The free energy of this model can
be written
F(v) = ⟨Heﬀ
P(c|v) log P(c|v) ,
where c ranges over all spin conﬁgurations of the classical Ising model of one dimension higher.
The above argument holds in the absence of the transverse ﬁeld, that is, for the classical Boltzmann machine. In this case, the TD(0) update rule is given by
∆wvh = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))v⟨h⟩
∆whh′ = ε(rn(sn, an) + γQ(sn+1, an+1) −Q(sn, an))⟨hh′⟩,
where ⟨h⟩(referred to as activations of the hidden nodes in machine learning terminology) and
⟨hh′⟩are the expected values of the variables and the product of variables, respectively, in the
binary encoding of the hidden nodes with respect to the Boltzmann distribution given by P(h|v) =
exp(−βEv(h))/P
h′ exp(−βEv(h′)). Therefore, they may be approximated using SA or SQA when
The values of the Q-functions in (17) and (18) can also be approximated empirically, since, in a
classical Boltzmann machine,
P(h|v)Ev(h) + 1
P(h|v) log P(h|v)
uhh′⟨hh′⟩+ 1
P(h|s, a) log P(h|s, a).
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
Algorithm 1
1: initialize weights of RBM
2: for all training samples (s1, a1) do
s2 ←a1(s1), a2 ←argmaxa Q(s2, a)
calculate ⟨hi⟩for (i = 1, 2) using (21)
calculate F(si, ai) for (i = 1, 2) using (20)
Q(si, ai) ←−F(si, ai) for (i = 1, 2)
update RBM weights using (22) and (23)
π(s1) ←argmaxa Q(s1, a)
9: return π
Remark 2.1. In the case of an RBM, Sallans and Hinton show that the free energy is given
−F(s, a) =
[⟨h⟩log⟨h⟩+ (1 −⟨h⟩) log(1 −⟨h⟩)] .
The update rule for the weights of the RBM is (17) alone. Moreover, in the case of RBMs, the
equilibrium free energy F(s, a) and its derivatives with respect to the weights can be calculated
without the need for Boltzmann distribution sampling, according to the closed formula
⟨h⟩= P(σh = 1|s, a) = σ
Here, σ denotes the sigmoid function. Note that, in the general case, since the hidden nodes of a
clamped Boltzmann machine are not independent, the calculation of the free energy is intractable.
3. Algorithms
In this section, we present the details of classical reinforcement learning using RBM, a semiclassical approach based on a DBM (using SA and SQA), and a quantum reinforcement learning
approach (using SQA or quantum annealing). All of the algorithms are based on the Q-learning
TD(0) method presented in the previous section. Pseudo-code for these methods is provided in
Algorithms 1, 2, and 3 below.
3.1. Reinforcement Learning Using RBMs. The RBM reinforcement learning algorithm is
due to Sallans and Hinton . This algorithm uses the update rule (17), with v representing state
or action encoding, to update the weights of an RBM, and (21) to calculate the expected values
of random variables associated with the hidden nodes ⟨h⟩. As explained in Sec. 2.8, the main
advantage of RBM is that it has explicit formulas for the hidden-node activations, given the values
of the visible nodes. Moreover, only for RBMs can the entropy portion of the free energy (19) be
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
Algorithm 2
1: initialize weights of DBM
2: for all training samples (s1, a1) do
s2 ←a1(s1), a2 ←argmaxa Q(s2, a)
approximate ⟨hi⟩, ⟨hih′i⟩, P(h|si, ai)
using SA or SQA for (i = 1, 2)
calculate F(si, ai) using (19) for (i = 1, 2)
Q(si, ai) ←−F(si, ai) for (i = 1, 2)
update DBM weights using (18), (22), and (23)
π(s1) ←argmaxa Q(s1, a)
9: return π
written in terms of the activations of the hidden nodes. More-complicated network architectures
do not possess this property, so there is a need for a Boltzmann distribution sampler.
In Algorithm 1, we recall the steps of the classical reinforcement learning algorithm using an RBM
with a graphical model similar to that shown in Fig. 1a. We set the initial Boltzmann machine
weights using Gaussian zero-mean values with a standard deviation of 1.00, as is common practice
for implementing Boltzmann machines . Consequently, this initializes an approximation of a
Q-function and a policy π given by
π(s) = argmax
In each training iteration, we select a state–action pair (s1, a1) ∈S × A. We associate a classical
spin variable σh to each hidden node h. Then, the activations of the hidden nodes are calculated
via (21). In our experiments, all Boltzmann machines have as many state nodes as |S| and as
many action nodes as |A|. We associate one node for every state s ∈S, and the corresponding
binary encoding is s = (0, 0, . . . , 1, . . . , 0), with zeroes everywhere except at the index of the node
corresponding to s. We use similar encoding for the actions, using the action nodes. A subsequent
state s2 is obtained from the state–action pair (s1, a1) using the transition kernel outlined in Sec.
2, and a corresponding action a2 is chosen via policy π. The free energy of the RBM is calculated
using (20) for both (s1, a1) and (s2, a2).
This results in an approximation of the Q-function (see Sec. 2.5) deﬁned on the state–action
space S × A ,
Q(s, a) ≈−F(s, a) ,
for both state–action pairs. We then use the update rule (17), or, more precisely,
∆wsh = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))s1⟨h⟩
∆wah = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))a1⟨h⟩,
with a learning rate ε to update the weights of the RBM. In view of (7), the best known policy can
be acquired via π(s) = argmaxa Q(s, a) for any state s.
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
3.2. Reinforcement Learning Using DBMs. Since we are interested in the dependencies between states and actions, we consider a DBM architecture that has a layer of states connected to
the ﬁrst layer of hidden nodes, followed by multiple hidden layers, and a layer of actions connected
to the ﬁnal layer of hidden nodes (see Fig. 1). We demonstrate the advantages of this deep architecture trained using SQA and the derivation in Sec. 2.8 of the temporal-diﬀerence gradient method
for reinforcement learning using general Boltzmann machines (GBM).
In Algorithm 2, we summarize the DBM-RL method. Here, the graphical model of the Boltzmann
machine is similar to that shown in Fig. 1b.
The initialization of the weights of the DBM is
performed in a similar fashion to the previous algorithm.
In each training iteration, we select a state–action pair (s1, a1) ∈S×A. Every node corresponding
to a state or an action is removed from the graph and the conﬁgurations of the spins corresponding
to the hidden nodes are sampled using SA or SQA on an Ising spin model constructed as follows:
the state s1 contributes to a bias of ws1h to σh if h is adjacent to s1; and the action a1 contributes
to a bias of wa1h to σh if h is adjacent to a1. The bias on any spin σh for which h is a hidden node
not adjacent to state s1 or action a1 is zero.
A subsequent state s2 is obtained from the state–action pair (s1, a1) using the transition kernel
outlined in Sec. 2, and a corresponding action a2 is chosen via policy π. Another SQA sampling is
performed in a similar fashion to the above for this pair.
According to lines 4 and 5 of Algorithm 2, the samples from the SA or SQA algorithm are used
to approximate the free energy of the classical DBM at points (s1, a1) and (s2, a2) using (19).
If SQA is used, averages are taken over each replica of each run; hence, there are 3750 samples
of conﬁgurations of the hidden nodes for each state–action pair. The strength Γ of the transverse
ﬁeld is scheduled to linearly decrease from 20.00 to Γf = 0.01.
The SA algorithm is used with a linear inverse temperature schedule that increases from 0.01 to
2.00 in 50,000 sweeps, and is run 150 times. So, if SA is used, there are only 150 sample points
used in the above approximation. The results of DBM-RL using SA or SQA have no signiﬁcant
diﬀerences.
The ﬁnal diﬀerence between Algorithm 1 and Algorithm 2 is that the update rule now includes
updates of weights between two hidden nodes given by (18),
∆whh′ = ε(r(s1, a1) + γQ(s2, a2) −Q(s1, a1))⟨hh′⟩,
in addition to the previous rules (22) and (23).
3.3. Reinforcement Learning Using QBMs. The last algorithm is QBM-RL, presented in
Algorithm 3. The initialization is performed as in Algorithms 1 and 2. However, according to lines
4 and 5, the samples from the SQA algorithm are used to approximate the free energy of a QBM
at points (s1, a1) and (s2, a2) by computing the free energy corresponding to an eﬀective classical
Ising spin model of one dimension higher representing the quantum Ising spin model of the QBM,
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
In this case, ⟨Heﬀ
s,a⟩from (16) is approximated by the average energy of the entire system of one
dimension higher and P(c|s, a) is approximated by the normalized frequency of the conﬁguration
c of the entire system of one dimension higher (hence, there are only 150 sample points for each
input instance in this case). The strength Γ of the transverse ﬁeld in SQA is scheduled to linearly
decrease from 20.00 to Γf = 2.00. In this algorithm, the weights are updated as in Algorithm 2.
However, ⟨h⟩and ⟨hh′⟩in this algorithm represent expectations of measurements in the z-basis.
In each training iteration, we select a state–action pair (s1, a1) ∈S×A. Every node corresponding
to a state or an action is removed from this graph and the conﬁgurations of the spins corresponding
to the hidden nodes are sampled using SQA on an Ising spin model constructed as follows: the
state s1 contributes to a bias of ws1h to σh if h is adjacent to s1; and the action a1 contributes to
a bias of wa1h to σh if h is adjacent to a1. The bias on any spin σh for which h is a hidden node
not adjacent to state s1 or action a1 is zero.
A subsequent state s2 is obtained from the state–action pair (s1, a1) using the transition kernel
outlined in Sec. 2, and a corresponding action a2 is chosen via policy π. Another SQA sampling is
performed in a similar fashion to the above for this pair.
In Fig. 3a and Fig. 3b, the selection of (s1, a1) is performed by sweeping across the set of state–
action pairs. In Fig. 3d, the selection of (s1, a1) and s2 is performed by sweeping over S × A × S.
In Fig. 3c, the selection of s1, a1, and s2 are all performed uniformly randomly.
We experiment with a variety of learning-rate schedules, including exponential, harmonic, and
linear; however, we found that for the training of both RBMs and DBMs, an adaptive learningrate schedule performed best (for information on adaptive subgradient methods, see ). In our
experiments, the initial learning rate is set to 0.01.
In all of our studied algorithms, training terminates when a desired number of training samples
have been processed, after which the updated policy is returned.
4. Numerical Results
We study the performance of temporal-diﬀerence reinforcement learning algorithms (explained
in detail in Sec. 3) using Boltzmann machines. We generalize the method introduced in , and
Algorithm 3
1: initialize weights of QBM
2: for all training samples (s1, a1) do
s2 ←a1(s1), a2 ←argmaxa Q(s2, a)
approximate ⟨hi⟩, ⟨hih′i⟩, ⟨Heﬀ
and P(c|si, ai) using SQA for (i = 1, 2)
calculate F(si, ai) using (16) for (i = 1, 2)
Q(si, ai) ←−F(si, ai) for (i = 1, 2)
update QBM weights using (18), (22), and (23)
π(s1) ←argmaxa Q(s1, a)
9: return π
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
compare the policies obtained from these algorithms to the optimal policy using a ﬁdelity measure,
which we deﬁne in (25).
For Tr independent trials of the same reinforcement learning algorithm, Ts training samples are
used for reinforcement learning. The ﬁdelity measure at the i-th training sample is deﬁned by
ﬁd(i) = (Tr × |S|)−1
1A(s,i,l)∈α∗(s),
where A(s, i, l) denotes the action assigned at the l-th run and i-th training sample to the state s.
In our experiments, each algorithm is run 1440 times, and for each run of an algorithm, Ts = 500
training samples are generated.
Fig. 3a and Fig. 3b show the ﬁdelity of the generated policies obtained from various reinforcement
learning experiments on two clear 3 × 5 mazes. In Fig. 3a, the maze includes one reward, one wall,
and one pit, and in Fig. 3b, the maze additionally includes two stochastic rewards.
experiments, the training samples are generated by sweeping over the maze. Each sweep iterates
over the maze elements in the same order. This explains the periodic behaviour of the ﬁdelity
curves (cf. Fig. 3c).
The curves labelled ‘QBM-RL’ represent the ﬁdelity of reinforcement learning using QBMs.
Sampling from the QBM is performed using SQA. All other experiments use classical Boltzmann
machines as their graphical model. In the experiment labelled ‘RBM-RL’, the graphical model
is an RBM, trained classically using formula (21). The remaining curve is labelled ‘DBM-RL’ for
classical reinforcement learning using a DBM. In these experiments, sampling from conﬁgurations of
the DBM is performed with SQA (with Γf = 0.01). The ﬁdelity results of DBM-RL coincide closely
with those of sampling conﬁgurations of the DBM using SA; therefore, we have not included them.
Fig. 3c regenerates the results of Fig. 3a using uniform random sampling (i.e., without sweeping
through the maze).
Our next result, shown in Fig. 3d, compares RBM-RL, DBM-RL, and QBM-RL for a windy
maze of size 3 × 5. The transition kernel for this experiment is chosen such that P(a(s)|s, a) = 0.8,
and P(s′|s, a) has a nonzero value for all s′ ̸= a(s) that are reachable from s by taking some action,
in which case all these values are equal. The transition probability is zero for all other states.
Fig. 2d shows examples of the transition probabilities in the windy problem.
To demonstrate the performance of RBM-RL, DBM-RL, and QBM-RL with respect to scaling,
we deﬁne another measure called average ﬁdelity, avℓ, where we take the average ﬁdelity over the
last ℓtraining samples of the ﬁdelity measure. Given Ts total training samples and ﬁd(i) as deﬁned
above, we write
In Fig. 4, we report the eﬀect of maze size on avℓfor RBM-RL, DBM-RL, and QBM-RL for varying
maze sizes. We plot avℓfor each algorithm with ℓ= 500, 250, and 10 as a function of maze size
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
Training Sample
Training Sample
Training Sample
Training Sample
1R1W1P-windy
Figure 3. Comparison of RBM-RL, DBM-RL, and QBM-RL training results. Every underlying
RBM has 16 hidden nodes and every DBM has two layers of eight hidden nodes. The shaded areas
indicate the standard deviation of each training algorithm. (a) The ﬁdelity curves for the three
algorithms run on the maze in Fig 2a. (b) The ﬁdelity curves for the maze in Fig 2b. (c) The
ﬁdelity curves of the mentioned three algorithms corresponding to the same experiment as that
of (a), except that the training is performed by uniformly generated training samples rather than
sweeping across the maze. (d) The ﬁdelity curves corresponding to a windy maze similar to Fig 2a.
for a family of problems with one deterministic reward, two stochastic rewards, one pit, and n −2
walls. We use nine n × 5 mazes in this experiment, indexed by various values of n. In addition to
the avℓplots, we include a dotted-line plot depicting the ﬁdelity for a completely random policy.
The ﬁdelity of the random policy is given by the average probability of choosing an optimal action
at each state when generating admissible actions uniformly at random, which is given by
Note that the ﬁdelity of the random policy increases as the maze size increases. This is due to the
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
3R(n−2)W1P - QBM/DBM , RBM 
DBM-RL, ℓ= 500
DBM-RL, ℓ= 250
DBM-RL, ℓ= 10
RBM-RL, ℓ= 500
RBM-RL, ℓ= 250
RBM-RL, ℓ= 10
QBM-RL, ℓ= 500
QBM-RL, ℓ= 250
QBM-RL, ℓ= 10
Figure 4. A comparison between the performance of RBM-RL, DBM-RL, and QBM-RL as the
size of the maze grows. All Boltzmann machines have 20 hidden nodes. (a) The schematics of an
n × 5 maze with one deterministic reward, 2 stochastic rewards, one pit, and n −2 walls. (b) The
scaling of the average ﬁdelity of each algorithm run on each instance of the n × 5 maze. The dotted
line is the average ﬁdelity of uniformly randomly generated actions.
fact that maze rows containing a wall have more average admissible optimal actions than the top
and bottom rows of the maze.
5. Discussion
The ﬁdelity curves in Fig. 3 show that DBM-RL outperforms RBM-RL with respect to the
number of training samples. Therefore, we expect that in conjunction with a high-performance
sampler of Boltzmann distributions (e.g., a quantum or a quantum-inspired oracle taken as such),
DBM-RL improves the performance of reinforcement learning. QBM-RL is not only on par with
DBM-RL, but actually slightly improves upon it by taking advantage of sampling in the presence
of a signiﬁcant transverse ﬁeld.
This is a positive result for the potential of sampling from a quantum device in machine learning, as we do not expect quantum annealing to obtain the Boltzmann distribution of a classical
Hamiltonian . However, given the discussion in Sec. 2.1, a quantum annealer viewed
REINFORCEMENT LEARNING USING QUANTUM BOLTZMANN MACHINES
as an open system coupled to a heat bath could be a better choice of sampler from its instantaneous Hamiltonian in earlier stages of the annealing process, compared to a sampler of the problem
Hamiltonian at the end of the evolution. Therefore, these experiments address whether a quantum
Boltzmann machine with a transverse ﬁeld Ising Hamiltonian can perform at least as well as a
classical Boltzmann machine.
In each experiment, the ﬁdelity curves from DBM-RL produced using SQA with Γf = 0.01 match
the ones produced using SA. This is consistent with our expectation that using SQA with Γ →0
produces samples from the same distribution as SA, namely, the Boltzmann distribution of the
classical Ising Hamiltonian with no transverse ﬁeld.
The best algorithm in our experiments is evidently QBM-RL using SQA. Here, the ﬁnal transverse ﬁeld is Γf = 2.00, corresponding to one-third of the anneal for a quantum annealing algorithm
that evolves along the convex linear combination of the initial and ﬁnal Hamiltonians with constant
speed. This is consistent with ideas found in on sampling at freeze-out .
Fig. 3c shows that, whereas the maze can be solved with fewer training samples using ordered
sweeps of the maze, the periodic behaviour of the ﬁdelity curves is due to this periodic choice of
training samples. This eﬀect disappears once the training samples are chosen uniformly randomly.
Fig. 3d shows that the improvement in the learning of the DBM-RL and QBM-RL algorithms
persists in the case of more-complicated transition kernels. The same ordering of ﬁdelity curves
discussed earlier is observed: QBM-RL outperforms DBM-RL, and DBM-RL outperforms RBM-
It is worth mentioning that, even though it may seem that more connectivity between the
hidden nodes may allow a Boltzmann machine to capture more- complicated correlations between
the visible nodes, the training process of the Boltzmann machine becomes more computationally
involved. In our reinforcement learning application, an RBM with m hidden nodes, and n = |S|+|A|
visible nodes, has mn weights to train. A DBM with two hidden layers of equal size has 1
weights to train. Therefore, when m < 2n, the training of the DBM is in a domain of a lower
dimension. Further, a GBM with all of its hidden nodes forming a complete graph requires mn+
weights to train, which is always larger than that of an RBM or a DBM with the same number of
hidden nodes.
One can observe from Fig. 4 that, as the maze size increases and the complexity of the reinforcement learning task increases, avℓdecreases for each algorithm. The RBM algorithm, while
always outperformed by DBM-RL and QBM-RL, shows a much faster decay in average ﬁdelity as
a function of maze size compared to both DBM-RL and QBM-RL. For larger mazes, the RBM
algorithm fails to capture maze traversal knowledge, and approaches avℓof a random action allocation (the dotted line), whereas the DBM-RL and QBM-RL algorithms continue to be trained
well. DBM-RL and QBM-RL are capable of training the agent to traverse larger mazes, whereas
the RBM algorithm, utilizing the same number of hidden nodes and a larger number of weights,
fails to converge to an output that is better than a random policy.
D. CRAWFORD, A. LEVIT, N.GHADERMARZY, J. S. OBEROI, AND P. RONAGH
The runtime and computational resources needed to compare DBM-RL and QBM-RL with RBM-
RL have not been investigated here. We expect that in view of , the size of RBM needed to
solve larger maze problems will grow exponentially. Thus, it would be interesting to research the
extrapolation of the asymptotic complexity and size of the DBM-RL and QBM-RL algorithms
with the aim of attaining a quantum advantage. Applying the algorithms described in this paper
to tasks that have larger state and action spaces, as well as to more-complicated environments, will
allow us to demonstrate the scalability and usefulness of the DBM-RL and QBM-RL approaches.
The experimental results shown in Fig. 4 represent only a rudimentary attempt to investigate
this matter, yet the results are promising. However, this experiment does not provide a practical
characterization of the scaling of our approach, and further investigation is needed.
Acknowledgements
We would like to thank Hamed Karimi, Helmut Katzgraber, Murray Thom, Matthias Troyer,
and Ehsan Zahedinejad, as well as the referees and editorial board of Quantum Information and
Computation, for reviewing this work and providing many helpful suggestions. The idea of using
SQA to run experiments involving measurements with a nonzero transverse ﬁeld was communicated in person by Mohammad Amin. We would also like to thank Marko Bucyk for editing this
manuscript.