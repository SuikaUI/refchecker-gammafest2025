Optimizing Statistical Machine Translation for Text Simpliﬁcation
Wei Xu1, Courtney Napoles2, Ellie Pavlick1, Quanze Chen1 and Chris Callison-Burch1
1 Computer and Information Science Department
University of Pennsylvania
{xwe, epavlick, cquanze, ccb}@seas.upenn.edu
2 Department of Computer Science
Johns Hopkins University
 
Most recent sentence simpliﬁcation systems
use basic machine translation models to learn
lexical and syntactic paraphrases from a manually simpliﬁed parallel corpus. These methods are limited by the quality and quantity of
manually simpliﬁed corpora, which are expensive to build. In this paper, we conduct an indepth adaptation of statistical machine translation to perform text simpliﬁcation, taking
advantage of large-scale paraphrases learned
from bilingual texts and a small amount of
manual simpliﬁcations with multiple references. Our work is the ﬁrst to design automatic metrics that are effective for tuning and
evaluating simpliﬁcation systems, which will
facilitate iterative development for this task.
Introduction
The goal of text simpliﬁcation is to rewrite an input
text so that the output is more readable. Text simpliﬁcation has applications for reducing input complexity for natural language processing 
and providing reading aids for people with limited language skills or language impairments such as dyslexia , autism , and aphasia .
It is widely accepted that sentence simpliﬁcation
can be implemented by three major types of operations: splitting, deletion and paraphrasing . The splitting operation decomposes a long
sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The
paraphrasing operation includes reordering, lexical
substitutions and syntactic transformations. While
sentence splitting and deletion have been intensively studied, there has been
considerably less research on developing new paraphrasing models for text simpliﬁcation — most previous work has used off-the-shelf statistical machine
translation (SMT) technology and achieved reasonable results . However, they have
either treated the judgment technology as a black
 
or they have been limited to modifying only one aspect of it, such as the translation model or the reranking
component .
In this paper, we present a complete adaptation
of a syntax-based machine translation framework to
perform simpliﬁcation. Our methodology poses text
simpliﬁcation as a paraphrasing problem: given an
input text, rewrite it subject to the constraints that
the output should be simpler than the input, while
preserving as much meaning of the input as possible, and maintaining the well-formedness of the
Going beyond previous work, we make di-
Transactions of the Association for Computational Linguistics, vol. 4, pp. 401–415, 2016. Action Editor: Stefan Riezler.
Submission batch: 10/2015; Revision batch: 2/2016; Published 7/2016.
c⃝2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.
Downloaded from by guest on 26 March 2025
rect modiﬁcations to four key components in the
SMT pipeline:1 1) two novel simpliﬁcation-speciﬁc
tunable metrics; 2) large-scale paraphrase rules automatically derived from bilingual parallel corpora,
which are more naturally and abundantly available
than manually simpliﬁed texts; 3) rich rule-level
simpliﬁcation features; and 4) multiple reference
simpliﬁcations collected via crowdsourcing for tuning and evaluation.
In particular, we report the
ﬁrst study that shows promising correlations of automatic metrics with human evaluation. Our work
answers the call made in a recent TACL paper to address problems in current simpliﬁcation research — we amend human evaluation criteria, develop automatic metrics, and generate an
improved multiple reference dataset.
Our work is primarily focused on lexical simpliﬁcation (rewriting words or phrases with simpler versions), and to a lesser extent on syntactic rewrite
rules that simplify the input. It largely ignores the
important subtasks of sentence splitting and deletion. Our focus on lexical simpliﬁcation does not affect the generality of the presented work, since deletion or sentence splitting could be applied as pre- or
post-processing steps.
Background
Xu et al. laid out a series of problems that
are present in current text simpliﬁcation research,
and argued that we should deviate from the previous
state-of-the-art benchmarking setup.
First, the Simple English Wikipedia data has dominated simpliﬁcation research since 2010 , and is used together with
Standard English Wikipedia to create parallel text
to train MT-based simpliﬁcation systems.
However, recent studies showed that the parallel Wikipedia simpliﬁcation corpus contains a large proportion of inadequate (not much simpler) or inaccurate (not aligned
or only partially aligned) simpliﬁcations. It is one of
the leading reasons that existing simpliﬁcation systems struggle to generate simplifying paraphrases
and leave the input sentences unchanged . Previously researchers attempted some
quick ﬁxes by adding phrasal deletion rules or reranking n-best outputs
based on their dissimilarity to the input . In contrast, we exploit data with improved quality and enlarged quantity, namely, largescale paraphrase rules automatically derived from
bilingual corpora and a small amount of manual
simpliﬁcation data with multiple references for tuning parameters. We then systematically design new
tuning metrics and rich simpliﬁcation-speciﬁc features into a syntactic machine translation model to
enforce optimization towards simplicity. This approach achieves better simpliﬁcation performance
without relying on a manually simpliﬁed corpus to
learn paraphrase rules, which is important given the
fact that Simple Wikipedia and the newly released
Newsela simpliﬁcation corpus are
only available for English.
Second, previous evaluation used in the simpliﬁcation literature is uninformative and not comparable across models due to the complications between
the three different operations of paraphrasing, deletion, and splitting. This, combined with the unreliable quality of Simple Wikipedia as a gold reference
for evaluation, has been the bottleneck for developing automatic metrics. There exist only a few studies on automatic simpliﬁcation evaluation using existing MT
metrics which show limited correlation with human
assessments. In this paper, we restrict ourselves to
lexical simpliﬁcation, where we believe MT-derived
evaluation metrics can best be deployed. Our newly
proposed metric is the ﬁrst automatic metric that
shows reasonable correlation with human evaluation on the text simpliﬁcation task. We also introduce multiple references to make automatic evaluation feasible.
The most related work to ours is that of Ganitkevitch et al. on sentence compression, in
which compression of word and sentence lengths
can be more straightforwardly implemented in features and the objective function in the SMT framework. We want to stress that sentence simpliﬁcation is not a simple extension of sentence compression, but is a much more complicated task, primarily
because high-quality data is much harder to obtain
and the solution space is more constrained by word
Downloaded from by guest on 26 March 2025
choice and grammar. Our work is also related to
other tunable metrics designed to be very simple and
light-weight to ensure fast repeated computation for
tuning bilingual translation models . To the best of our knowledge,
no tunable metric has been attempted for simpliﬁcation, except for BLEU. Nor do any evaluation metrics exist for simpliﬁcation, although there are several designed for other text-to-text generation tasks:
grammatical error correction ,
paraphrase generation , and conversation
generation . Another line of related work is lexical simpliﬁcation that focuses on
ﬁnding simpler synonyms of a given complex word
 .
Adapting Machine Translation for
Simpliﬁcation
We adapt the machinery of statistical machine translation to the task of text simpliﬁcation by making
changes in the following four key components:
Simpliﬁcation-speciﬁc Objective Functions
In the statistical machine translation framework, one
crucial element is to design automatic evaluation
metrics to be used as training objectives.
Training algorithms, such as MERT or PRO
 , then directly optimize the
model parameters such that the end-to-end simpliﬁcation quality is optimal. Unfortunately, previous
work on text simpliﬁcation has only used BLEU for
tuning, which is insufﬁcient as we show empirically
in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references.
Unlike machine translation metrics which do not
compare against the (foreign) input sentence, it is
necessary to compare simpliﬁcation system outputs
against the inputs to assess readability changes. It
is also important to keep tunable metrics as simple
as possible, since they are repeatedly computed during the tuning process for hundreds of thousands of
candidate outputs.
Our ﬁrst metric combines a previously proposed
metric for paraphrase generation, iBLEU , and the widely used readability metric,
Flesch-Kincaid Index . iBLEU
is an extension of the BLEU metric to measure diversity as well as adequacy of the generated paraphrase output. Given a candidate sentence O, human
references R and input text I, iBLEU is deﬁned as:
α × BLEU(O, R)
−(1 −α) × BLEU(O, I).
where α is a parameter taking balance between adequacy and dissimilarity, and set to 0.9 empirically as
suggested by Sun and Zhou .
Since the text simpliﬁcation task aims at improving readability, we include the Flesch-Kincaid Index
(FK) which estimates the readability of text using
cognitively motivated features :
#sentences
#syllables
with a lower value indicating higher readability.2 We
adapt FK to score individual sentences and change it
so that it counts punctuation tokens as well as word,
and counts each punctuation token as one syllable.
This prevents it from arbitrarily deleting punctuation.
FK measures readability assuming that the text is
well-formed, and therefore is insufﬁcient alone as
a metric for generating or evaluating automatically
generated sentences.
Combining FK and iBLEU
captures both a measure of readability and adequacy.
The resulting objective function, FKBLEU, is de-
ﬁned as a geometric mean of the iBLEU and the FK
difference between input and output sentences:
FKBLEU = iBLEU(I, R, O) × FKdiff(I, O)
FKdiff = sigmoid(FK(O) −FK(I))
Sentences with higher FKBLEU values are better
simpliﬁcations with higher readability.
2The FK coefﬁcients were derived via multiple regression
applied to the reading compression test scores of 531 Navy personnel reading training manuals.
These values are typically
used unmodiﬁed, as we do here.
Downloaded from by guest on 26 March 2025
We design a second new metric SARI that principally compares system output against references
and against the input sentence. It explicitly measures the goodness of words that are added, deleted
and kept by the systems (Figure 1).
We reward addition operations, where system output O was not in the input I but occurred in any of
the references R, i.e. O ∩I ∩R. We deﬁne n-gram
precision p(n) and recall r(n) for addition operations as follows:3
#g(O ∩I), #g(R)
g∈O #g(O ∩I)
#g(O ∩I), #g(R)
g∈O #g(R ∩I)
where #g(·) is a binary indicator of occurrence of ngrams g in a given set (and is a fractional indicator
in some later formulas) and
#g(O ∩I) = max(#g(O) −#g(I), 0)
#g(R ∩I) = max(#g(R) −#g(I), 0)
Therefore, in the example below, the addition of unigram now is rewarded in both padd(n) and radd(n),
while the addition of you in OUTPUT-1 is penalized
in padd(n):
INPUT: About 95 species are currently accepted .
REF-1: About 95 species are currently known .
REF-2: About 95 species are now accepted .
REF-3: 95 species are now accepted .
OUTPUT-1: About 95 you now get in .
OUTPUT-2: About 95 species are now agreed .
OUTPUT-3: About 95 species are currently agreed.
The corresponding SARI scores of these three toy
outputs are 0.2683, 0.7594, 0.5890, which match
with intuitions about their quality.
To put it in
perspective, the BLEU scores are 0.1562, 0.6435,
0.6435 respectively. BLEU fails to distinguish between OUTPUT-2 and OUTPUT-3 because matching any one of references is credited the same. Not
all the references are necessarily complete simpli-
ﬁcations, e.g.
REF-1 doesn’t simplify the word
3In the rare case when the denominator is 0 in calculating
precision p or recall r, we simply set the value of p and r to 0.
references
Input that is unchanged by
system and which is not in
the reference
Input that is retained
in the references,
but was deleted by
the system
between all 3
Input that was
correctly deleted by
the system, and
replaced by content
from the references
Potentially
system output
Figure 1: Metrics that evaluate the output of monolingual
text-to-text generation systems can compare system output against references and against the input sentence, unlike in MT metrics which do not compare against the (foreign) input sentence. The different regions of this Venn
diagram are treated differently with our SARI metric.
currently, which gives BLEU too much latitude for
matching the input.
Words that are retained in both the system output and references should be rewarded. When multiple references are used, the number of references
in which an n-gram was retained matters. It takes
into account that some words/phrases are considered
simple and are unnecessary (but still encouraged) to
be simpliﬁed. We use R′ to mark the n-gram counts
over R with fractions, e.g. if a unigram (word about
in above example) occurs in 2 out of the total r references, then its count is weighted by 2/r in computation of precision and recall:
pkeep(n) =
#g(I ∩O), #g(I ∩R′)
g∈I #g(I ∩O)
rkeep(n) =
#g(I ∩O), #g(I ∩R′)
g∈I #g(I ∩R′)
#g(I ∩O) = min
 #g(I), #g(O)
#g(I ∩R′) = min
 #g(I), #g(R)/r
For deletion, we only use precision because overdeleting hurts readability much more signiﬁcantly
than not deleting:
#g(I∩O),#g(I∩R′)
g∈I #g(I∩O)
#g(I ∩O) = max
 #g(I) −#g(O), 0
#g(I ∩R′) = max
 #g(I) −#g(R)/r, 0
Downloaded from by guest on 26 March 2025
unnecessary
accomplished
carried out
make a signiﬁcant contribution
contribute greatly
is generally acknowledged that
is widely accepted that
the manner in which NN
the way NN
NNP ’s population
the people of NNP
NNP ’s JJ legislation
the JJ law of NNP
Table 1: Example paraphrase rules in the Paraphrase Database (PPDB) that result in simpliﬁcations of the input. The
rules are synchronous context-free grammar (SCFG) rules where uppercase indicates non-terminal symbols. Nonterminals can be complex symbols like VP/S which indicates that the rule forms a verb phrase (VP) missing a sentence
(S) to its right. The ﬁnal syntactic rule both simpliﬁes and reorders the input phrase.
The precision of what is kept also reﬂects the suf-
ﬁciency of deletions. The n-gram counts are also
weighted in R′ to compensate n-grams, such as the
word currently in the example, that are not considered as required simpliﬁcation by human editors.
Together, in SARI, we use arithmetic average of
n-gram precisions Poperation and recalls Roperation:
SARI = d1Fadd + d2Fkeep + d3Pdel
where d1 = d2 = d3 = 1/3 and
Poperation = 1
n=[1,...,k]
poperation(n)
Roperation = 1
n=[1,...,k]
roperation(n)
Foperation = 2 × Poperation × Roperation
Poperation + Roperation
operation ∈[del, keep, add]
where k is the highest n-gram order and set to 4 in
our experiments.
Incorporating Large-Scale Paraphrase
Another challenge for text simpliﬁcation is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied
on either hand-crafted rules or dictionaries like WordNet
 . This technique does manage to
learn a small number of transformations that simplify. However, we argue that because the size of the
Normal-Simple Wikipedia parallel corpus is quite
small (108k sentence pairs with 2 million words),
the diversity and coverage of patterns that can be
learned is actually quite limited.
In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 as a rich source of lexical,
phrasal and syntactic simpliﬁcation operations. It
is created by extracting English paraphrases from
bilingual parallel corpora using a technique called
“bilingual pivoting” . The PPDB is represented as a synchronous
context-free grammar (SCFG), which is commonly
used as the formalism for syntax-based machine
translation . Table 1 shows some example paraphrase rules in the PPDB.
PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the
Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million
paraphrase rules, consisting of 8 million lexical,
73 million phrasal and 140 million syntactic para-
4 
Downloaded from by guest on 26 March 2025
phrase patterns. The key differences between the
paraphrase rules from PPDB and the transformations learned by the naive application of SMT to
the Normal-Simple Wikipedia parallel corpus, are
that the PPDB paraphrases are much more diverse.
For example, PPDB contains 214 paraphrases for
ancient including antique, ancestral, old, age-old,
archeological, former, antiquated, longstanding, archaic, centuries-old, and so on. However, there is
nothing inherent in the rule extraction process to say
which of the PPDB paraphrases are simpliﬁcations.
In this paper, we model the task by incorporating
rich features into each rule and let SMT advances
in decoding and optimization determine how well a
rule simpliﬁes an input phrase. An alternative way
of using PPDB for simpliﬁcation would be to simply discard any of its rules which did not result in
a simpliﬁed output, possibly using a simple supervised classiﬁer .
Simpliﬁcation-speciﬁc Features for
Paraphrase Rules
Designing good features is an essential aspect of
modeling. For each input sentence i and its candidate output sentence j, a vector of feature functions
⃗ϕ = {ϕ1...ϕN} are combined with a weight vector
⃗w in a linear model to obtain a single score h⃗w:
h⃗w(i, j) = ⃗w · ⃗ϕ(i, j)
In SMT, typical feature functions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which
governs whether the system prefers fewer longer
phrases or a greater number of shorter phrases), and
a language model probability. Together these features are what the model uses to distinguish between
good and bad translations. For monolingual translation tasks, previous research suggests that features
like paraphrase probability and distributional similarity are potentially helpful in picking out good
paraphrases and for text-to-text
generation . While these
two features quantify how good a paraphrase rule is
in general, they do not indicate how good the rule is
for a speciﬁc task, like simpliﬁcation.
For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add
9 new features for simpliﬁcation purposes:5 length
in characters, length in words, number of syllables,
language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word
with the maximum number of syllables on each side
and the difference between the two sides, when it is
applicable. We use language models built from the
Gigaword corpus and the Simple Wikipedia corpus
collected by Kauchak . We also use a list of
3000 most common US English words compiled by
Paul and Bernice Noll.6
Creating Multiple References
Like with machine translation, where there are many
equally good translations, in simpliﬁcation there
may be several ways of simplifying a sentence. Most
previous work on text simpliﬁcation only uses a single reference simpliﬁcation, often from the Simple
Wikipedia.
This is undesirable since the Simple
Wikipedia contains a large proportion of inadequate
or inaccurate simpliﬁcations .
In this study, we collect multiple human reference
simpliﬁcations that focus on simpliﬁcation by paraphrasing rather than deletion or splitting. We ﬁrst
selected the Simple-Normal sentence pairs of similar length (≤20% differences in number of tokens)
from the Parallel Wikipedia Simpliﬁcation (PWKP)
corpus that are more likely to be
paraphrase-only simpliﬁcations. We then asked 8
workers on Amazon Mechanical Turk to rewrite a
selected sentence from Normal Wikipedia (a subset
of PWKP) into a simpler version while preserving
its meaning, without losing any information or splitting sentence. We removed bad workers by manual inspection on the worker’s ﬁrst several submissions on the basis of a recent study 
on crowdsourcing translation that suggests Turkers’
performance stays consistent over time and can be
reliably predicted by their ﬁrst few translations.
In total, we collected 8 reference simpliﬁcations
for 2350 sentences, and randomly split them into
2000 sentences for tuning, 350 for evaluation. Many
crowdsourcing workers were able to provide simpli-
ﬁcations of good quality and diversity . Having multiple references allows us to
develop automatic metrics similar to BLEU to take
advantage of the variation across many people’s simpliﬁcations. We leave more in-depth investigations
on crowdsourcing simpliﬁcation for future work.
Tuning Parameters
Like in statistical machine translation, we set the
weights of the linear model ⃗w in the Equation (8)
so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000
sentence development set.
We use the pairwise
ranking optimization (PRO) algorithm implemented in the open-source Joshua
toolkit 
for tuning.
Speciﬁcally, we train the system to distinguish a
good candidate output j from a bad candidate j′,
measured by an objective function o (Section 3.1),
for an input sentence i:
o(i, j) >o(i, j′) ⇐⇒h⃗w(i, j) > h⃗w(i, j′)
⇐⇒h⃗w(i, j) −h⃗w(i, j′) > 0
⇐⇒⃗w · ⃗ϕ(i, j) −⃗w · ⃗ϕ(i, j′) > 0
⇐⇒⃗w · (⃗ϕ(i, j) −⃗ϕ(i, j′)) > 0
Thus, the optimization reduces to a binary classiﬁcation problem. Each training instance is the difference vector ⃗ϕ(i, j) −⃗ϕ(i, j′)) of a pair of candidates, and its training label is positive or negative
depending on whether the value of o(i, j) −o(i, j′)
is positive or negative. The candidates are generated
according to h⃗w at each iteration, and sampled for
making the training tractable. We use different metrics: BLEU, FKBLEU and SARI as objectives.
Experiments and Analyses
We implemented all the proposed adaptations into
the open source syntactic machine translation decoder Joshua ,7 and conducted
the experiments with PPDB and the dataset of 2350
sentences collected in Section 3.4.
Most recent
7 We augment its latest version to include the text-to-text generation functionality
described in this paper.
Paraphrase Rule
Trans. Model Score
principal →key
principal →main
principal →major
principal →chief
principal →core
principal →principal
principal →top
principal →senior
principal →lead
principal →primary
principal →prime
principal →keynote
able-bodied →valid
able-bodied →sound
able-bodied →healthy
able-bodied →able-bodied
able-bodied →job-ready
able-bodied →employable
able-bodied →non-disabled
Table 3: Qualitative analysis of candidate paraphrases
ranked by the translation model in SBMT (PPDB +
SARI), showing that the model is optimized towards simplicity in addition to the correctness of paraphrases. The
ﬁnal simpliﬁcations (in bold) are chosen in conjunction
with the language model to ﬁt the context and further bias
towards more common n-grams.
end-to-end sentence simpliﬁcation systems use a
basic phrase-based MT model trained on parallel
Wikipedia data using the Moses decoder . One of the best systems
is PBMT-R by Wubben et al. , which reranks
Moses’ n-best outputs based on their dissimilarity to
the input to promote simpliﬁcation. We also build a
baseline by using BLEU as the tuning metric in our
adapted MT framework. We conduct both human
and automatic evaluation to demonstrate the advantage of the proposed simpliﬁcation systems. We also
show the effectiveness of the two new metrics in tuning and automatic evaluation.
Qualitative Analysis
Table 2 shows a representative example of the simpliﬁcation results.
The PBMT-R model failed to
learn any good substitutions for the word ablebodied or the phrase are required to from the manually simpliﬁed corpora of limited size. In contrast,
our proposed method can make use of more para-
Downloaded from by guest on 26 March 2025
Normal Wikipedia
Jeddah is the principal gateway to Mecca, Islam’s holiest city, which able-bodied Muslims
are required to visit at least once in their lifetime.
Simple Wikipedia
Jeddah is the main gateway to Mecca, the holiest city of Islam, where able-bodied Muslims
must go to at least once in a lifetime.
Mechanical Turk #1
Jeddah is the main entrance to Mecca, the holiest city in Islam, which all healthy Muslims
need to visit at least once in their life.
Mechanical Turk #2
Jeddah is the main entrance to Mecca, Islam’s holiest city, which pure Muslims are required to visit at least once in their lifetime.
PBMT-R 
Jeddah is the main gateway to Mecca, Islam’s holiest city, which able-bodied Muslims are
required of Muslims at least once in their lifetime.
SBMT (PPDB + BLEU)
Jeddah is the main door to Mecca, Islam’s holiest city, which sound Muslims are to go to
at least once in their life.
SBMT (PPDB + FKBLEU)
Jeddah is the main gateway to Mecca, Islam’s holiest city, which sound Muslims must
visit at least once in their life.
SBMT (PPDB + SARI)
Jeddah is the main gateway to Mecca, Islam’s holiest city, which sound Muslims have to
visit at least once in their life.
Table 2: Example human reference simpliﬁcations and automatic simpliﬁcation system outputs. The bold font highlights the parts of the sentence that are different from the original version in the Normal Wikipedia, and strikethrough
denotes deletions.
phrases learned from the more abundant bilingual
texts. It improves method applicability to languages
other than English, for which no simpler version of
Wikipedia is available.
Our proposed approach also provides an intuitive way to inspect the ranking of candidate paraphrases in the translation model. This is done by
scoring each rule in PPDB by Equation 8 using the
weights optimized in the tuning process, as in Table 3. It shows that our proposed method is capable
of capturing the notion of simplicity using a small
amount of parallel tuning data. It correctly ranks key
and main as good simpliﬁcations for principal. Its
choices are not always perfect as it prefers sound
over healthy for able-bodied.
The ﬁnal simpliﬁcation outputs are generated according to both the
translation model and the language model trained
on the Gigaword corpus to take into account context
and further bias towards more common n-grams.
Quantitative Evaluation of Simpliﬁcation
For the human evaluation, participants were shown
the original English Wikipedia sentence as a reference and asked to judge a set of simpliﬁcations
that were displayed in random order.
They evaluated a simpliﬁcation from each system, the Simple Wikipedia version, and a Turker simpliﬁcation.
Judges rated each simpliﬁcation on two 5-point
scales of meaning retention and grammaticality (0
is the worst and 4 is the best). We also ask participants to rate Simplicity Gain (Simplicity+) by counting how many successful lexical or syntactic paraphrases occurred in the simpliﬁcation. We found this
makes the judgment easier and that it is more informative than rating the simplicity directly on 5-point
scale, since the original sentences have very different readability levels to start with. More importantly,
using simplicity gain avoids over-punishment of errors, which are already penalized for poor meaning
retention and grammaticality, and thus reduces the
bias towards very conservative models. We collect
judgments on these three criteria from ﬁve different
annotators and report the average scores.
Table 4 shows that our best system, a syntacticbased MT system (SBMT) using PPDB as the
source of paraphrase rules and tuning towards the
SARI metric, achieves better performance in all
three simpliﬁcation measurements than the state-ofthe-art system PBMT-R. The relatively small values of simplicity gain, even for the two human references (Simple Wikipedia and Mechanical Turk),
clearly show the major challenge of simpliﬁcation,
which is the need of not only generating paraphrases
but also ensuring the generated paraphrases are simpler while ﬁtting the contexts. Although many researchers have noticed this difﬁculty, PBMT-R is
one of the few that tried to address it by promoting
Downloaded from by guest on 26 March 2025
Simplicity+
Edit Dist.
Normal Wikipedia
Simple Wikipedia
Mechanical Turk
PBMT-R 
SBMT (PPDB + BLEU)
SBMT (PPDB + FKBLEU)
SBMT (PPDB + SARI)
Table 4: Human evaluation (Grammar, Meaning, Simplicity+) and basic statistics of our proposed systems (SBMTs)
and baselines. PBMT-R is an reimplementation of the state-of-the-art system by Wubben et al. . Newly proposed
metrics FKBLEU and SARI show advantages for tuning.
Normal Wikipedia
Simple Wikipedia
Mechanical Turk
PBMT-R 
SBMT (PPDB + BLEU)
SBMT (PPDB + FKBLEU)
SBMT (PPDB + SARI)
Table 5: Automatic evaluation of different simpliﬁcation systems. Most systems achieve similar FK readability scores
as human. The SARI metric ranks all 5 different systems and 3 human references in the same order as human assessment. Tuning towards BLEU with all 8 references results in identical transformation (same as Normal Wikipedia),
as this can get a near-perfect BLEU score of 99.05 (out of 100).
outputs that are dissimilar to the input. Our best system is able to make more effective paraphrases (better Simplicity+) while introducing less errors (better
Grammar and Meaning).
Table 5 shows the automatic evaluation. An encouraging fact is that SARI metric ranks all 5 different systems and 3 human references in the same
order as human assessment. Most systems achieve
similar FK readability as human editors, using fewer
words or words with fewer syllables.
Tuning towards BLEU with all 8 references results in no transformation (same as input), as this can get a nearperfect BLEU score of 99.05 (out of 100).
Table 6 shows the computation time for different
metrics. SARI is only slightly slower than BLEU
but achieves much better simpliﬁcation quality.
Time (milliseconds)
0.12540908
0.15506646
Table 6: Average computation time of different metrics
per candidate sentence.
Correlation of Automatic Metrics with
Human Judgments
Table 7 shows the correlation of automatic metrics
with human judgment. There are several interesting
observations.
First, simplicity is essential in measuring the
goodness of simpliﬁcation. However, none of the
existing metrics (i.e. FK, BLEU, iBLEU) demonstrate any signiﬁcant correlation with the simplicity
scores rated by humans, same as noted in previous
work . In
contrast, our two new metrics, FKBLEU and SARI,
achieve a much better correlation with humans in
simplicity judgment while still capturing the notion
of grammaticality and meaning preservation. This
explains why they are more suitable than BLEU to
be used in training the simpliﬁcation models.
particular, SARI provides a balanced and integrative
measurement of system performance that can assist
iterative development. To date, developing advanced
simpliﬁcation systems has been a difﬁcult and timeconsuming process, since it is impractical to run new
Downloaded from by guest on 26 March 2025
Spearman’s ρ
Simplicity+
- 0.002 (≈.976)
0.136 (<.010)
0.147 (<.010)
0.366 (<.001)
0.459 (<.001)
0.151 (<.005)
0.589 (<.001)
0.701 (<.001)
0.111 (<.050)
0.313 (<.001)
0.397 (<.001)
0.149 (<.005)
0.492 (<.001)
0.609 (<.001)
0.141 (<.010)
0.349 (<.001)
0.410 (<.001)
0.235 (<.001)
0.342 (<.001)
0.397 (<.001)
0.343 (<.001)
Table 7: Correlations (and two-tailed p-values) of metrics against the human ratings at sentence-level (also see Figure
3). In this work, we propose to use multiple (eight) references and two new metrics: FKBLEU and SARI. For all three
criteria of simpliﬁcation quality, SARI correlates reasonably with human judgments. In contrast, previous works use
only a single reference. Existing metrics BLEU and iBLEU show higher correlations on grammaticality and meaning
preservation using multiple references, but fail to measure the most important aspect of simpliﬁcation – simplicity.
human evaluation every time a new model is built or
parameters are adjusted.
Second, the correlation of automatic metrics with
human judgment of grammaticality and meaning
preservation is higher than any reported before
 . It validates our argument that constraining simpliﬁcation
to only paraphrasing reduces the complication from
deletion and splitting, and thus makes automatic
evaluation more feasible. Using multiple references
further improves the correlations.
Why Does BLEU Correlate Strongly with
Meaning/Grammar, and SARI with
Simplicity?
Here we look more deeply at the correlations of
BLEU and SARI with human judgments. Our SARI
metric has highest correlation with human judgments of simplicity, but BLEU exhibits higher correlations on grammaticality and meaning preservation.
BLEU was designed to evaluate bilingual translation systems. It measures the n-gram precision of
a system’s output against one or more references.
BLEU ignores recall (and compensates for this with
its brevity penalty). BLEU prefers an output that is
not too short and contains only n-grams that appear
in any reference. The role of multiple references
in BLEU is to capture allowable variations in translation quality. When applied to monolingual tasks
like simpliﬁcation, BLEU does not take into account
anything about the differences between the input and
the references. In contrast, SARI takes into account
both precision and recall, by looking at the difference between the references and the input sentence.
Figure 2: A scatter plot of BLEU scores vs. SARI scores
for the individual sentences in our test set. The metrics’
scores for many sentences substantially diverge. Few of
the sentences that scored perfectly in BLEU receive a
high score from SARI.
In this work, we use multiple references to capture
many different ways of simplifying the input.
Unlike bilingual translation, the more references
created for the monolingual simpliﬁcation task the
more n-grams of the original input will be included
in the references. That means, with more references,
outputs that are close or identical to the input will get
high BLEU. Outputs with few changes also receive
high Grammar/Meaning scores from human judges;
but these do not necessarily get high SARI score nor
are they good simpliﬁcations. BLEU therefore tends
to favor conservative systems that do not make many
changes, while SARI penalizes them. This can be
Downloaded from by guest on 26 March 2025
Human Score
SARI vs. Grammar
SARI vs. Meaning
SARI vs. Simplicity
Automatic Score
Human Score
BLEU vs. Grammar
Automatic Score
BLEU vs. Meaning
Automatic Score
Human Rating
BLEU vs. Simplicity
Figure 3: Scatter plots of automatic metrics against human scores for individual sentences.
seen in Figure 2 where sentences with a BLEU score
of 1.0 receive a range of scores from SARI.
The scatter plots in Figure 3 further illustrate the
above analysis. These plots emphasize the correlation of high human scores on meaning/grammar for
systems that make few changes (which BLEU rewards, but SARI does not). The tradeoff is that conservative outputs with few or no changes do not result in increased simplicity. SARI correctly rewards
systems that make changes that simplify the input.
Conclusions and Future Work
In this paper, we presented an effective adaptation
of statistical machine translation techniques.
ﬁnd the approach promising in suggesting two new
directions:
designing tunable metrics that correlate with human judgements and using simplicityenriched paraphrase rules derived from larger data
than the Normal-Simple Wikipedia dataset. For future work, we think it might be possible to design
a universal metric that works for multiple text-totext generation tasks (including sentence simpliﬁcation, compression and error correction), at the same
time using the same idea of comparing system output against multiple references and against the input.
The metric could possibly include tunable parameters or weighted human judgments on references to
accommodate different tasks. Finally, we are also
interested in designing neural translation models for
the simpliﬁcation task.
Acknowledgments
The authors would like to thank Juri Ganitkevitch,
Jonny Weese, Kristina Toutanova, Matt Post, and
Shashi Narayan for valuable discussions. We also
thank action editor Stefan Riezler and three anonymous reviewers for their thoughtful comments. This
material is based on research sponsored by the NSF
under grant IIS-1430651 and the NSF GRFP under
grant 1232825.
The views and conclusions contained in this publication are those of the authors
and should not be interpreted as representing ofﬁcial policies or endorsements of the NSF or the U.S.
Government. This research is also supported by the
Alfred P. Sloan Foundation, and by Facebook via a
student fellowship and a faculty research award.
Downloaded from by guest on 26 March 2025