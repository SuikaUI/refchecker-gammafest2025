Stable Architectures for Deep Neural Networks
Eldad Haber1,3 and Lars Ruthotto2,3
1Department of Earth and Ocean Science, The University of British Columbia, Vancouver,
BC, Canada, ( )
2Emory University, Department of Mathematics and Computer Science, Atlanta, GA, USA
( )
3Xtract Technologies Inc., Vancouver, Canada, ( )
February 19, 2019
Deep neural networks have become invaluable tools for supervised machine learning, e.g.,
classiﬁcation of text or images. While often oﬀering superior results over traditional techniques
and successfully expressing complicated patterns in data, deep architectures are known to be
challenging to design and train such that they generalize well to new data. Critical issues with
deep architectures are numerical instabilities in derivative-based learning algorithms commonly
called exploding or vanishing gradients. In this paper, we propose new forward propagation
techniques inspired by systems of Ordinary Diﬀerential Equations (ODE) that overcome this
challenge and lead to well-posed learning problems for arbitrarily deep networks.
The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability
and well-posedness of deep learning and use this new understanding to develop new network
architectures. We relate the exploding and vanishing gradient phenomenon to the stability of
the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments
show their competitiveness with state-of-the-art networks.
Machine Learning, Deep Neural Networks, Dynamic Inverse Problems, PDE-Constrained
Optimization, Parameter Estimation, Image Classiﬁcation.
Introduction
In this work, we propose new architectures for Deep Neural Networks (DNN) and exemplarily
show their eﬀectiveness for solving supervised Machine Learning (ML) problems; for a general
overview about DNN and ML see, e.g., and reference therein.
We consider the
following classiﬁcation problem: Assume we are given training data consisting of s feature vectors,
y1, . . . , ys ∈Rn, and label vectors, c1, . . . , cs ∈Rm, whose kth components represent the likelihood
of an example belonging to class k. The goal is to learn a function that approximates the datalabel relation on the training data and generalizes well to similar unlabeled data. Our goals in this
work are to highlight the relation of the learning problem to dynamic inverse problems, analyze
 
its stability and ill-posedness for commonly used architectures, and derive new architectures that
alleviate some of these diﬃculties for arbitrarily deep architectures.
We are particularly interested in deep learning, i.e., machine learning using neural networks
with many hidden layers. DNNs have been successful in supervised learning, particularly when
the relationship between the data and the labels is highly nonlinear; see, e.g., and
reference therein. Their depths (i.e., their number of layers) allow DNNs to express complex datalabel relationships since each layer nonlinearly transforms the features and therefore eﬀectively
ﬁlters the information content.
Given the training data (y1, c1), . . . (ys, cs), an inverse problem needs to be solved in order to
train a given network architecture. This problem, also called the learning problem, aims at ﬁnding
a parameterization of the DNN that explains the data-label relation and generalizes well to new
unlabeled data. Clearly, using deeper network architectures increases the capacity of the network
but also the dimensionality, and thus the computational complexity, of the parameter estimation
Additionally, more labeled data is required to calibrate very deep networks reliably.
Therefore, despite the fact that neural networks have been used since the early 70’s, deep learning
has only recently revolutionized many applications fueled by advances in computational hardware
and availability of massive data sets.
Well-known sources of diﬃculty in deep learning are the dimensionality and non-convexity of
the associated optimization problem. Traditionally, stochastic gradient descent methods have been
used . It has been observed that the performance of the DNN can be highly dependent on the
choice of optimization algorithm and sample size . Furthermore, it has been noted that
some optimization algorithms yield DNNs that generalize poorly to new unlabeled data .
Additional diﬃculties in deep learning stem from instabilities of the underlying forward model,
most importantly, the propagation of features through the DNN. As has been shown in , the
output of some networks can be unstable with respect to small perturbations in the original features.
A related problem is the observation of vanishing or exploding gradients . These results are
unsettling since predictions made by networks with unstable forward propagation are very sensitive
to small perturbations of the input features (as is common, e.g., in adversarial attacks), which may
render the network useless in practice.
The main goal of this work is to gain new insight into the stability of the forward propagation
and the well-posedness of the learning problem summarized in the following two questions:
1. Given a network architecture and parameters obtained by some optimization process, is the
forward propagation problem well-posed?
2. Is the learning problem well-posed? In other words, given suﬃcient training are there parameters such that the DNN generalizes well or can generalization be improved by adding
appropriate regularization?
The ﬁrst question is important because, while it may be possible to ﬁt the training data even
for unstable forward propagation models, the trained network is unlikely to generalize. In other
words, small deviations in the data, e.g., due to noise, may be drastically ampliﬁed by the forward
propagation, resulting in incorrect labels. We show that the forward problem can be thought of as
a discretization of an Ordinary Diﬀerential Equation (ODE). Therefore, the stability of the network
corresponds to the stability of its underlying ODE. Based on this observation we develop stability
criteria for a simpliﬁed version of the commonly used Residual Network (ResNet) architecture 
and develop new network architectures that are ensured to be stable and lead to well-posed learning
The paper is organized as follows. In Section 2 we give a brief mathematical derivation of the
deep learning problem illustrated using the ResNet architecture. In Section 3 we analyze the stability of the simpliﬁed ResNet forward propagation and the well-posedness of the resulting learning
problem. Our analysis and examples suggest stability criteria, and as a result, in Section 4, we
propose three new stable architectures. In Section 5 we propose regularization functions favoring smoothness of parameters and multi-level ideas to initialize deep networks. In Section 6 we
demonstrate the eﬀectiveness of our new architectures on a number of small-scale model problems
and explore their eﬀectiveness for solving image classiﬁcation problems. Finally, in Section 7 we
summarize the paper.
Mathematical Formulation of the Deep Learning Problem
In this section we brieﬂy describe three main ingredients of deep learning relevant to our work;
for a comprehensive introduction see, e.g., . First, we outline forward propagation
techniques, which transforms the input features in a nonlinear way to ﬁlter their information.
Second, we describe classiﬁcation, which predicts the class label probabilities using the features at
the output layer (i.e., the output of the forward propagation). Finally, we formulate the learning
problem, which aims at estimating parameters of the forward propagation and classiﬁcation that
approximate the data-label relation.
For notational convenience we stack the training features and labels row-wise into matrices
Y0 = [y1, y2, . . . , ys]⊤∈Rs×n and C = [c1, c2, . . . , cs]⊤∈Rs×m.
To exemplify our discussion of forward propagation we consider a simpliﬁed version of the
Residual Neural Network (ResNet) model that has been very successful in classifying images
using deep network architectures; see for other options. In ResNets, the forward propagation
of the input values, Y0 ∈Rs×n, through a network consisting of N layers is given by
Yj+1 = Yj + hσ(YjKj + bj)
j = 0, . . . , N −1.
The propagation in Eq. 2.1 is parametrized by the nonlinear activation function σ : Rs×n →Rs×n
and aﬃne transformations represented by their weights, K0, K1, . . . , KN−1 ∈Rn×n, and biases,
b0, b1, . . . , bN−1 ∈R. We augmented the original formulation in by the parameter h > 0 in
order to increase the stability of the forward propagation and allow for a continuous interpretation
of the process; see also Section 3. The values Y1, . . . , YN−1 are also called hidden layers and YN is
called the output layer. The activation function is applied element-wise and is typically (piecewise)
smooth and monotonically non-decreasing.
As two commonly used examples, we consider the
hyperbolic tangent and the Rectiﬁed Linear Unit (ReLU) activations
σht(Y) = tanh(Y)
σReLU(Y) = max(0, Y).
The class label probabilities are predicted using the values at the output layers, YN, a hypothesis
function h(YNW + esµ⊤), and its associated weights, W ∈Rn×m, and bias, µ ∈Rm.
ek ∈Rk denotes the k-dimensional vector of all ones. For Bernoulli variables (i.e., C ∈{0, 1}s×m)
it is natural to consider the logistic regression function
h(x) = exp(x)./(1 + exp(x)),
where the exponential and the division are applied element-wise. For Multinomial distributions we
use the softmax function
h(X) = exp(X)./(exp(X)em).
The learning problem aims at estimating the parameters of the forward propagation (i.e.,
K0, K1, . . . , KN−1 and b0, b1, . . . , bN−1) and the classiﬁer (W and µ) so that the DNN accurately
approximates the data-label relation for the training data and generalizes to new unlabeled data.
As we show below, the learning problem can be cast as a dynamic inverse problem, which provides new opportunities for applying theoretical and computational techniques from parameter
estimation to deep learning problems. We phrase learning as an optimization problem
sS(h(YNW + esµ⊤), C) + αR(W, µ, K0,...,N−1, b0,...,N−1)
Yj+1 = Yj + hσ(YjKj + bj),
j = 0, 1, . . . , N −1,
where the loss function S is convex in its ﬁrst argument and measures the quality of the predicted
class label probabilities, the convex regularizer R penalizes undesirable (e.g., highly oscillatory)
parameters, and the parameter α > 0 balances between minimizing the data ﬁt and regularity of
the parameters. A simple example for a loss function is the sum-of-squared diﬀerence function
S(Cpred, C) = 1
2∥Cpred −C∥2
F . Since our numerical experiments deal with classiﬁcation we use
cross entropy loss functions. Choosing an ”optimal” regularizer, R, and regularization parameter,
α, is both crucial and nontrivial. Commonly Tikhonov regularization , also referred to as
weight decay, has been used , although, other possibilities enforcing sparsity or other structure
have been proposed . We introduce novel regularization functions in Section 5. For simplicity,
we assume that a suitable value of α > 0 is chosen by the user or that it is done dynamically as
suggested in .
There are numerous approaches to solving the learning problem. In this work we use a simple
block coordinate descent method to demonstrate the properties of the forward propagation. Our
method alternates between updating the parameters of the classiﬁer (W, µ) ﬁxing the current value
of the propagated features YN and then updating the parameters of the forward propagation while
keeping the updated weights of the classiﬁer ﬁxed. The ﬁrst problem is typically convex and the
latter problem is generally non-convex due to the forward propagation process. Both steps are
based on subsampling the training data. To this end note that most common loss functions can be
written as a sum over all examples, i.e.,
sS(h(YNW + esµ⊤), C)
i=1 S(h(y⊤
i W + µ⊤), c⊤
i∈T S(h(y⊤
i W + µ⊤), c⊤
where T ⊂{1, 2, . . . , s} is a randomly chosen set updated in each iteration of the block coordinate
descent method. The size of the batches is a parameter in our algorithm whose choice depends on
the size and complexity of the problem and resource considerations. In the following, we assume the
sample size is constant; for adaptive selection of sample size see, e.g., . In each iteration of the
block coordinate descent scheme, our algorithm approximately solves the resulting classiﬁcation
problem using an Newton-PCG method, i.e., an inexact Newton method that uses a Preconditioned Conjugate Gradient (PCG) method to determine a search direction (see, e.g., ).
Subsequently, the weights of the forward propagation are updated using a Gauss-Newton-PCG
method. Note that gradient and approximated Hessian computations require matrix-vector products with the derivative matrices of the values of the output layer, YN, with respect to K0,...,N−1
and b0,...,N−1. The matrix-vector products with the derivative matrix can be computed without
its explicit construction through forward and backward propagation, respectively; see also .
However, this requires storing (or recomputing) the values at the output layers; see, for instance,
Section 4.4 for derivative computation. Therefore, as also suggested in , we subsample T further
to reduce the cost of the Hessian matrix-vector products in our PCG scheme.
Our implementation includes computing the validation error in each iteration of the block
coordinate descent method. The ﬁnal output of our algorithm are the parameters that achieve the
lowest validation error.
Stability and well-posedness of the forward propagation
In this section we analyze the stability of the ResNet forward problem 2.1 and illustrate why some
choices of transformation weights may generate instabilities or prohibit eﬀective learning altogether.
It is well-known that any parameter estimation problem requires a well-posed forward problem,
i.e., a problem whose output is continuous with respect to its input. For example, practical image
classiﬁcation algorithms need to be robust against noisy or slightly shifted input images.
Illposedness of the forward problem implies that even if the estimated parameters lead to a small
training error they will probably fail or will do poorly on a perturbation of that data. In other
words a network whose forward propagation is ill-posed will generalize poorly. Thus, well-posedness
of the forward propagation is a necessary condition to obtain DNNs that generalize well.
The following discussion also gives new perspectives on two well-known phenomena in the deep
learning community: Vanishing and exploding gradients; see, e.g., . These phenomena refer to the
gradient of the objective function in Eq. 2.4 and pose a severe challenge for very deep architectures.
Note that the gradient represents the sensitivity of the output with respect to a perturbation in
the input. Thus, an exploding gradient implies that the output is unstable with respect to the
input. Similarly a vanishing gradient implies that the output is insensitive with respect to the
input. Clearly both cases prohibit eﬀective training, but more importantly, may not provide DNNs
that generalize well.
To understand the phenomena we consider a simpliﬁed version of the forward propagation in
ResNets given in Eq. 2.1. As pointed out in the forward propagation can be seen as an explicit
Euler discretization of the nonlinear Ordinary Diﬀerential Equation (ODE)
˙y(t) = σ(K⊤(t)y(t) + b(t)),
y(0) = y0,
over a time interval t = [0, T]. The ﬁnal time T > 0 and the magnitude of K(t) control the depth
of the network. The ODE is stable if K(t) is changing suﬃciently slow and
i=1,2,...,n Re(λi(J(t))) ≤0,
∀t ∈[0, T],
where Re(·) denotes the real part, λi(J(t)) is the ith eigenvalue of the Jacobian of the right hand
side in Eq. 3.5, denoted by J(t) ∈Rn×n. A more accurate statement that uses kinematic eigenvalues
of the Jacobian can be found in . Here, the Jacobian is
 σ(K(t)⊤y + b(t))
 σ′(K(t)⊤y + b(t))
Since the activation function σ is typically monotonically non-decreasing, i.e., σ′(·) ≥0, Eq. 3.6 is
satisﬁed if K changes suﬃciently slowly and
i=1,2,...,n Re(λi(K(t))) ≤0,
∀t ∈[0, T].
Controlling the smoothness of K can be done by regularization as described in Section 5. To ensure
the stability of the overall discrete forward propagation, we also require the discrete version of the
ODE to have a suﬃciently small h as summarized in the following well-known lemma.
Lemma 1 (Stability of Forward Euler Method) The forward propagation in Eq. 2.1 is stable
i=1,2,...,n |1 + hλi(Jj)| ≤1,
∀j = 0, 1, . . . , N −1.
Proof 1 See, e.g., for the proof of stability criteria for the forward Euler method.
The above discussion suggests that the stability of the continuous forward propagation in Eq. 3.8
and its discrete analog Eq. 3.9 need to be added to the optimization problem 2.4 as constraints.
Otherwise, one may obtain some transformation weights, K0, K1, . . . , KN−1, that may ﬁt the training data but generate an unstable process. As discussed above these solutions cannot be expected
to generalize well for other data.
We illustrate the stability issues in ResNet using a simple example.
Example 1 (Stability of ResNet) For s = 3 and n = 2 we consider the forward propagation
through a ResNet as given by Eq. 2.1. We consider three networks consisting of N = 10 identical
layers, i.e., on each layer we use the activation σht = tanh, h = 0.1, b = 0, and a constant weight
matrix. To illustrate the impact of the eigenvalues of the weight matrix on the propagation, we
consider three ResNets parameterized by
where λ1(K+) = λ2(K+) = 2,
λ1(K−) = λ2(K−) = −2 and λ1(K0) = ı,
λ2(K0) = −ı. We
consider the feature vectors y1 = [0.1, 0.1]⊤, y2 = −y1, y3 = [0, 0.5]⊤. After propagating the
features through the layers, we illustrate the diﬀerent propagations in Figure 1. We represent the
values at the hidden layers as colored lines in the 2D plane where each color is associated with one
feature vector. To highlight the diﬀerences in the dynamics in all three cases, we also depict the
force ﬁeld using black arrows in the background. This plot is often referred to as the phase plane
As can be seen in left subplot, the features diverge away from the origin and each other using K+.
Note that y1 and y2, which are close together initially, depart into opposite directions. This clearly
suggests an unstable forward propagation that cannot be expected to generalize well. In contrast to
that, the center subplot shows that K−yields an accumulation point at the origin. While the forward
propagation satisﬁes Eq. 3.8 and is thus stable, the learning problem, which requires inversion of
this process, is ill-posed. Finally, the antisymmetric matrix K0 yields a rotation in the feature
space, which preserves the distances between the features and leads to a stable forward propagation
and a well-posed learning problem.
Clearly the eﬀects depicted here are more pronounced in deeper networks with more layers and/or
larger values for h.
K+, λ(K+) = 2
K−, λ(K−) = −2
K0, λ(K0) = ±ı
Figure 1: Phase plane diagrams for ResNets with N = 10 identical layers parameterized by the
weight matrices in Eq. 3.10 starting from three diﬀerent input features. The values at hidden layers
are indicated by colored lines and arrows depict the force ﬁeld. Left: Due to its positive eigenvalues,
the features diverge using K+ leading to an unstable forward propagation. Center: K−yields a
contraction that annihilates diﬀerences in the features and renders the learning problem ill-posed.
Right: The antisymmetric matrix K0 leads to rotations, preserves distances between the features,
and yields well-posed forward propagation and learning.
While the ﬁrst case in Example 1 indicates that Eq. 3.8 is a necessary condition for successful
learning, the second case suggests that it is not suﬃcient. In general, when Re(λ(K(t))) < 0 for
most times and the network is deep (e.g., long time integration) diﬀerences in the initial feature
vectors decay.
In other words, for all initial conditions y0 we have that y(t) →0 as t →∞;
compare with center plot in Figure 1. Hence, even though the forward problem is stable the inverse
problem is highly ill-posed as it comparable to an inverse heat equation. In these situations, the
gradients of the objective function in Eq. 2.4 will vanish since small changes in the kernels will have
no noticeable impact on the values of the outputs.
If we are inspired from the propagation of signals through neurons, then a ResNet as in Eq. 2.4
with maxi Re (λi(K)) > 0 for all i consists of neurons that amplify the signal with no upper bound
(which is not biological) and a ResNet with maxi Re (λi(K)) ≪0 can be seen as a lossy network. A
moderately lossy network may be advantageous when the input is noisy, since it tends to decay high
order oscillations. However, having too much signal loss is clearly harmful as it also annihilates
relevant diﬀerences in the input features.
In summary, our discussion of ResNets illustrates that stable forward propagation and wellposed learning problems can be obtained for deep networks when
Re (λi(K(t))) ≈0,
∀i = 1, 2, . . . , n, t ∈[0, T].
In this case, the forward propagation causes only moderate ampliﬁcation or loss and thus even deep
networks preserve features in the input data and allow for eﬀective learning.
Stable Forward Propagation for DNNs
Motivated by the discussion in the previous section, we introduce three new forward propagation
methods that are stable for arbitrarily deep neural networks and lead to well-posed learning problems. Our approaches, presented in Section 4.1 and 4.2, use diﬀerent means to enforce Jacobians
whose eigenvalues have very small real part; see Eq. 3.11. The methods in Section 4.2 are inspired
by Hamiltonian systems and we propose leapfrog and Verlet integration techniques for forward
propagation in Section 4.3. Finally, we compute the derivative of the Verlet method using back
propagation and discuss the relation between back propagation and the older and more general
adjoint method in Section 4.4.
Antisymmetric Weight Matrices
Perhaps the simplest way to obtain a stable forward propagation is to construct force ﬁelds whose
Jacobians are antisymmetric. For example, consider the forward propagation
Yj+1 = Yj + hσ
, j = 0, . . . , N −1,
where γ ≥0 is a small constant and I is an identity matrix. The resulting forward propagation can
be seen as a forward Euler discretization of the ODE
2(K(t) −K(t)⊤−γI)y(t) + b(t)
∀t ∈[0, T].
Since K(t)−K(t)⊤is anti-symmetric its eigenvalues are imaginary, which also holds for the Jacobian
in Eq. 3.7. Thus, the continuous ResNet parameterized by the antisymmetric weight matrix is stable
and preserves information given an appropriate integration technique and suﬃciently small time
To stabilize the discrete forward propagation, which is based on the forward Euler method, we
have added diﬀusion to the system in Eq. 4.12. The amount of diﬀusion depends on the parameter
γ ≥0. While small values of γ might improve the robustness against noise in the feature vectors,
too large values can render the learning problem ill-posed.
Alternatively, more advanced time
integration methods can be used with γ = 0, i.e., without adding diﬀusion.
Hamiltonian Inspired Neural Networks
Restricting the parameter space to antisymmetric kernels is only one way to obtain a stable forward
propagation. Alternatively we can recast forward propagation as a Hamiltonian system, which has
the structure
˙y(t) = −∇zH(y, z, t)
˙z(t) = ∇yH(y, z, t),
∀t ∈[0, T].
The function H : Rn ×Rn ×[0, T] →R is the Hamiltonian. In our application Hamiltonian systems
for forward propagation are attractive due to their property to conserve rather than increase or
dissipate the energy of the system; see [2, Ch.6] for a general introduction.
The Hamiltonian function H measures the energy of the system, which in the autonomous case
gets conserved. A simple approach can be derived from the Hamiltonian
H(y, z) = 1
2z⊤z + f(y),
where f : Rn →R is at least twice continuously diﬀerentiable. This leads to the ODE system
˙z(t) = −∇yf(y(t)),
˙y(t) = z(t),
∀t ∈[0, T].
Eliminating z we obtain a second order system
¨y(t) = ∇yf(y(t)),
∀t ∈[0, T].
Inspired by the continuous version of the ResNet forward propagation given in Eq. 3.5 we
propose to use the following second order ODE
¨y(t) = σ(K⊤(t)y(t) + b(t)),
y(0) = y0,
˙y(0) = ˙y0,
where in the following we assume that ˙y0 = 0. The dynamical system in Eq. 4.13 is stable for all
weight matrices with non-positive real eigenvalues, i.e., that satisfy Eq. 3.8. This constraint can
either be added to the optimization problem 2.4 or, similar to the previous section, be enforced by
design. The latter approach can be obtained, e.g., using the parametrization
K(C) = −C⊤C,
Note that in contrast to the antisymmetric model in Eq. 4.12, the negative deﬁnite model is based
on a nonlinear parametrization which might lead to a more challenging optimization problem.
Alternatively Eq. 3.8 can be added as a constraint to the optimization problem 2.4 requiring
expensive eigenvalue computations.
The forward propagations Eqs. 2.1, 4.12, or 4.13 require additional constraints on K and are
limited to square weight matrices.
We therefore propose a network that is intrinsically stable
and supports non-square weight matrices, i.e., a network whose forward propagation is stable
independent of the choice of the weights. To this end, we introduce symmetry into the Hamiltonian
system by deﬁning
˙y(t) = σ(K(t)z(t) + b(t)) and ˙z(t) = −σ(K(t)⊤y(t) + b(t)).
Note that Eq. 4.15 can be seen as a special case of ResNet with an augmented variable z ∈Rm
and the associated ODE
(t) + b(t)
This system is stable regardless of the spectrum or size of the matrices K(t) since the overall matrix
in the linear transformation is antisymmetric. To ensure stability of the discrete version the time
step size needs to be suﬃciently small; see Lemma 3.9.
Symplectic Forward Propagation
In this section we use symplectic integration techniques for solving the discrete versions of the
Hamiltonian-inspired networks in Eq. 4.13 and Eq. 4.15. Symplectic methods have been shown
to capture the long time features of Hamiltonian systems and thus provide a slightly diﬀerent
approach as compared to the forward Euler method used in ResNet 2.4; we refer to [2, Ch.6] for a
leapfrog, K−, N = 500
leapfrog, K−, N = 5, 000
Verlet, Kv, N = 500
Verlet, Kv, N = 5, 000
Figure 2: Phase space diagrams for the forward propagation using the leapfrog and Verlet methods
starting from y1 = [0.1, 0.1]⊤(blue) and y2 = [−0.1, −0.1]⊤(red). For each network we show the
short term (N = 500) and long term (N = 5, 000) behavior for identical layers. In both cases
non-trivial behavior can be observed even for constant weight matrices. (This ﬁgure is optimized
for screen use.)
detailed discussion. For the second-order ODE in Eq. 4.13 we recommend the conservative leapfrog
discretization
 2yj + h2σ(Kjyj + bj),
2yj −yj−1 + h2σ(Kjyj + bj),
j = 1, 2, . . . , N −1 .
To discretize the augmented network in Eq. 4.15 we use a Verlet integration where for j =
0, 1, . . . , N −1 we have
j yj + bj) and yj+1 = yj + hσ(Kjzj+ 1
Since both discretizations are symplectic, the respective Hamiltonians are preserved if the transformation weights are time invariant and the step size is suﬃciently small.
We demonstrate the long time dynamics of the two new Hamiltonian-inspired forward propagation methods using a simple example with two-dimensional features and identical layers.
Example 2 Let s = 2 and n = 2 and consider the features y1 = [0.1, 0.1]⊤and y2 = [−0.1, −0.1]⊤.
We consider networks with identical layers featuring b = 0 and hyperbolic tangent as activation
function. For the leapfrog integration we use the matrix K = K−deﬁned in Eq. 3.10 (recall that
λ1(K) = λ2(K) = −2) and a step size of h = 1. To illustrate the Verlet integration (which can
handle non-square kernels) we use a time step size of h = 0.1 and
To expose the short term behavior we use N = 500 layers and to demonstrate the non-trivial
long term characteristics we use N = 5, 000 layers. We depict the phase plane diagrams for both
networks and both depths in Figure 2. Even though we use identical layers with constant kernels
the features neither explode nor vanish asymptotically.
We proposed three new ways of forward propagation to ensure stability for arbitrary numbers
of layers. Among the three approaches, the Verlet method is the most ﬂexible as it can handle
non-square weighting matrices and does not require additional constraints on K.
Derivatives of the Verlet Method
We now discuss the computation of derivatives for the Verlet method. The derivatives of the ResNet
are standard and can be found, e.g., in . Given the ResNet derivatives the antisymmetric model
can be diﬀerentiated using chain rule and the time stepping. The leapfrog method can be treated
in a similar way to usual ResNets and is therefore omitted.
Our presentation follows the standard approach used in machine learning known as back propagation . However, we note that back propagation is a special case of the adjoint method used
in time-dependent optimal control; see, e.g., . The adjoint method is more general than back
propagation as it can also be used compute the gradient of less “standard” time stepping methods.
Thus, we discuss the back propagation method in the context of the adjoint method as applied to
the Verlet method. We start by the usual sensitivity equation and then discuss how to eﬃciently
compute matrix-vector products.
Exemplarily we show how to diﬀerentiate the values at the output layer in Eq. 4.17 with respect
to the weight matrix Kk at an arbitrary layer k.
For simplicity we assume that K is square.
Derivatives for non-square weight matrices and with respect to the bias, bk, can be computed along
the same lines. Clearly, the values of zj and yj at the hidden layers j ≤k are independent of Kk.
Applying the chain rule to Eq. 4.17 we see that
= hdiag (σ′(Kkyk + bk)) (yk ⊗I) =: C1
where ⊗denotes the Kronecker product and I is the n×n identity matrix. We can now diﬀerentiate
layer by layer, where for k < j < N −1 we obtain
∂Kk + hdiag (σ′(Kjyj + bj)) Kj
∂Kk −hdiag
Combining equations 4.18, 4.19, and 4.20 the derivatives can be written compactly as a block linear
= −hdiag (σ′(Kjyj + bj)) Kj,
The linear system in Eq. 4.22 is block triangular with identity matrices on its diagonal and
thus can be solved explicitly using forward substitution. Such systems commonly arise in optimal
control of time-dependent PDEs and in dynamic inverse problems. For more details involving notes
on implementation see, e.g., .
In the optimal control literature the matrices
∂Kk are often referred to as the sensitivity matrices. While the matrices can be dense and large, computing matrix-vector products
can be done eﬃciently by forward propagation. To this end, the right hand side of Eq. 4.22 is
multiplied by the vector and the linear system is then solved with a vector right hand side. The
multiplication with the transpose, also known as the adjoint method, is done by solving the equation backwards. It is important to note that the back-propagation algorithm is nothing but a
particular implementation of the adjoint method discussed much earlier in .
Regularization
In this section we present derivative-based regularization functions and a multi-level learning approach to ensure smoothness of parameters in deep learning.
By biasing the learning process
towards smooth time dynamics we aim at improving the stability of the forward propagation (e.g.,
to ensure that K(t) changes suﬃciently slow as assumed in Sec. 3) and ultimately the generalization. Intuitively, we cannot expect the network to generalize well in the absence of smoothness.
While the presented regularization techniques are commonly employed in other areas of inverse
problems , e.g., imaging their application to deep learning is, to the best of our knowledge,
rather novel.
Regularizing the Forward Propagation
The perhaps most common regularization strategy used in deep learning is referred to as weight
decay, which is equivalent to standard Tikhonov regularization of the form
where ∥· ∥F denotes the Frobenius norm; see, e.g., . While this regularization reduces the
magnitude of the weights, it is not sensitive to rapidly changing weights between adjacent layers.
To illustrate why this may be problematic, consider removing a single layer from a deep network.
Since the network is deep, we should not expect large changes in the values of the output layer and
thus similar classiﬁcation errors. However, if this layer performs, e.g., a 90 degree rotation of the
features while the adjacent layers keep the features unchanged, the eﬀect will be dramatic.
Our interpretation of forward propagation as a system of non-autonomous ODEs and the stability analysis in Sec. 3 motivate that K should be smooth, or at least piecewise smooth in time.
To this end we propose the following new regularization for the transformation weights
∥Kj −Kj−1∥2
F and R(b) = 1
(bj −bj−1)2.
This regularization favors weights that vary smoothly between adjacent layers. Furthermore, as we
see in our numerical experiments, the regularization adds robustness to the process. We can easily
add or subtract steps without signiﬁcantly changing the ﬁnal result, thus adding more generalizing
power to our network.
Regularizing the Classiﬁcation Weights
We propose using smoothness regularization on the classiﬁcation weights for image classiﬁcation
problems, e.g., in Convolution Neural Networks (CNN); see; e.g., [22, Ch.9]. For motivation, let the
examples in Y0 represent vectorized n1 × n2 × n3 images. The network propagates the images in
time and generates perturbed images of the same size. The hypothesis function predicts the class
label probabilities based on aﬃnely transformed output images, i.e.,
Cpred = h(YNW + esµ⊤).
The number of rows in W is n = n1 · n2 · n3 and the operation YN(j, :)⊤W(:, k) is a dot product
between the jth output image and the classiﬁcation weights for the kth class. Noting that the rows
of YN and the columns of W can be interpreted as discrete images sets the stage for developing
regularization approaches commonly used in image processing; see, e.g., .
To further motivate the importance of regularization, note that if the number of examples
is much larger than the number of features (pixels in the image) and if there is no signiﬁcant
redundancy, ﬁnding the optimal W given YN and C is an over-determined problem. Otherwise
the problem is ill-posed and there may be inﬁnitely many weights that yield the same classiﬁcation
on the observed data. The risk in both cases is overﬁtting since the optimal classiﬁcation weights
can be highly irregular and generalize poorly. Thus, one way to enforce uniqueness and improve
generalization is to regularize the weights.
A standard approach in machine learning also known as pooling aims at achieving this goal; see,
e.g., . The main idea of pooling is to coarsen the output images and thereby to decrease the
dimensionality of the classiﬁcation problem. The simplest approach is skipping, i.e., subsampling
the image YN, e.g., at every other pixel. Other common options are average pooling and maxpooling, where image patches are represented by their average and maximum value, respectively.
From an inverse problems perspective, pooling can be thought of as a subspace regularization .
For example, average pooling is similar to requiring W to be constant over each image patch.
An alternative approach to regularization is to interpret the jth feature, yj ∈Rn, and the kth
classiﬁcation weight, wk ∈Rn, of the CNN as discretizations of image functions yj : Ω→R and
wk : Ω→R, respectively, where Ω⊂R2 is the image domain. Assuming yj is suﬃciently regular
(which is to be enforced by the regularization) we can see that the probability of the jth example
belonging to the kth class can be viewed as
j wk + µk) ≈h
y(x)w(x) dx + µk
where vol(Ω) denotes the volume of the image domain. To obtain weights that are insensitive to
small displacements of the images it is reasonable to favor spatially smooth parameters by using
regularization of the form
where L is a discretized diﬀerential operator. This regularization also embeds the optimal wk into
a suitable function space. For example, using an image gradient ensures that wk is in the Sobolev
space H1(Ω, R) . Most importantly for our application at hand, derivative-based regularization
yields smooth classiﬁcation weights that, as we see next, can be interpreted by visual inspection.
Multi-level Learning
As another means of regularizing the problem, we exploit a multi-level learning strategy that
gradually increases the number of layers in the network.
Our idea is based on the continuous
interpretation of the forward propagation in which the number of layers in the network corresponds
to the number of discretization points. Our idea is closely related to cascadic multigrid methods 
and ideas in image processing where multi-level strategies are commonly used to decrease the risk
of being trapped in local minima; see, e.g., . More details about multi-level methods in learning
can be found in .
The basic idea is to ﬁrst solve the learning problem using a network with only a few layers and
then prolongate the estimated weights of the forward propagation to a network with twice as many
layers. The prolongated weights are then used to initialize the optimization problem on the ﬁner
level. We repeat this process until a user-speciﬁed maximum number of layers is reached.
Besides realizing some obvious computational savings arising from the reduced size of the networks, the main motivation behind our approach is to obtain good starting guesses for the next
level. This is key since, while deeper architectures oﬀer more ﬂexibility to model complicated datalabel relation, they are in our experience more diﬃcult to initialize. Additionally, the Gauss-Newton
method used to estimate the parameters of the forward propagation beneﬁts in general from good
starting guesses.
Numerical Examples
In this section we present numerical examples for classiﬁcation problems of varying level of diﬃculty.
We begin with three examples aiming at learning classiﬁcation functions in two variables, which
allow us to easily assess and illustrate the performance of the DNNs. In Section 6.4 we show results
for the MNIST data set , which is a common benchmark problem in image classiﬁcation.
Concentric Ellipses
As a ﬁrst test we consider a small-scale test problem in two dimensions. The test data consists of
1,200 points that are evenly divided into two groups that form concentric ellipsoids; see left subplot
in Figure 4. The original data is randomly divided into 1,000 training examples and 200 examples
used for validation.
We train the original ResNet, the antisymmetric ResNet, and the Hamiltonian network with
Verlet propagation using the block coordinate descent method and a multi-level strategy with
4, 8, 16, 32, 64, 128, 256, 1024 layers. Each block coordinate descent iteration consists of a classiﬁcation step (≤2 iterations of Newton-PCG with ≤2 PCG iterations) and a Gauss-Newton-PCG step
to update the propagation weights (≤20 PCG iterations preconditioned by regularization operator).
In all examples we use the logistic regression hypothesis function in Eq. 2.2 and tanh activation
function. The ﬁnal time is T = 20 and the width of the network is n = 2. To enforce smoothness
validation accuracy
Swiss Roll
Figure 3: Multi-level convergence for the two-dimensional examples in Sections 6.1–6.3. For each
level (level ℓcorresponds to DNN with 2ℓlayers) we show the best validation accuracy (the optimal
value is 1, which corresponds to a validation error of 0%).
labeled training data
propagated training data
prediction + validation data
Figure 4: Classiﬁcation results for the ellipse problem from Section 6.1 using a Hamiltonian Neural
Network with Verlet propagation with N = 1, 024 layers. Left: Labeled input features representing
two concentric ellipses that are not linearly separable. Center: The output features are linearly
separable and can be accurately classiﬁed. Right: We show the predictions of the network (colors
in the background) superimposed by the validation data. (This ﬁgure is optimized for screen use.)
of the propagation weights in time we employ the regularizer in Eq. 5.23 weighted by a factor of
α = 10−3. No regularization is used in the classiﬁcation.
We show the performance of the multi-level scheme in the left subplot of Figure 3. For this
simple data set, all forward propagation methods achieve an optimal validation accuracy of 100%
at some level. As to be expected, the validation accuracy increases with increasing depth of the
network, which also results in more degrees of freedom. The results for the Verlet method at the
ﬁnal level are shown in Figure 4. The two steps of deep learning (propagation and classiﬁcation)
can be seen in the center plot. The propagation transforms the feature such that they can be
linearly separated. The result of the learning process is a network that predicts the class for all
points in the 2D plane, which is illustrated in the right subplot of Figure 4.
input training data
propagated training data
prediction + validation data
Figure 5: Classiﬁcation results for swiss roll example described in Section 6.2 using a Hamiltonian
network with N = 1, 024 layers and the Verlet forward propagation. Left: We show the training
data (red and blue dots). Center: The Verlet method propagates the features so that they can
be separated by the aﬃne linear logistic regression classiﬁer. Right: We show the interpolation
obtained by our network (colored background) and the validation data. (This ﬁgure is optimized
for screen use.)
Swiss Roll
We consider another small-scale test problem that is inspired by the swiss roll example. The data
is obtained by sampling the vector functions
f1(r, θ) = r
f2(r, θ) = (r + 0.2)
for r ∈ and θ ∈[0, 4π] at 513 points each. Every other point along the curve is removed from
the data set and used for validation; see left subplot in Figure 5.
Given the data we use the same parameterization of the multi-level and block coordinate descent
method as in the previous example. For all networks, except the Hamiltonian network with Verlet
forward propagation, we increase the dimensionality of the input feature space to n = 4. As before
we use the tanh activation, logistic regression function in Eq. 2.2, and choose a ﬁnal time of T = 20.
To enforce smoothness of the propagation weights in time we employ the regularizer in Eq. 5.23
weighted by a factor of α = 5 · 10−3. No regularization is used in the classiﬁcation.
We plot the validation accuracy for each network and the diﬀerent steps of the multi-level
strategy in the center subplot of Figure 3. The standard ResNet and the Hamiltonian NN with
Verlet forward propagation both achieve an optimal validation accuracy for N = 1, 024. However,
the convergence considerably faster for the Hamiltonian network that reaches the optimal accuracy
with N = 32 layers. We visualize the results obtained using the Verlet method in Figure 5.
We propose a new challenging test problem for classiﬁcation into multiple classes using the peaks
function in MATLAB®, which reads,
3(1 −x1)2 exp(−(x2
1) −(x2 + 1)2) −10(x1/5 −x3
2) −1/3 exp(−(x1 + 1)2 −x2
where x ∈[−3, 3]2. The peaks function is smooth but has some nonlinearities and most importantly
non-convex level sets. We discretize the function on a regular 256 × 256 grid and divide the points
into 5 diﬀerent classes based on their function value. The points in each class are then randomly
subsampled such that the training data approximately represents the volumes of the level sets. In
our case the s = 5, 000 sample points are divided evenly into ﬁve classes of size 1,000. We illustrate
the test data in the left subplot of Figure 6.
We randomly choose 20% of the data for validation and train the networks using the remaining
examples. We use tanh as activation, the softmax hypothesis function in Eq. 2.3, and choose a ﬁnal
time of T = 5. To enforce smoothness of the propagation weights in time we employ the regularizer
in Eq. 5.23 weighted by a factor of α = 5 · 10−6. No regularization is used in the classiﬁcation. We
use the same multi-level strategy and identical parameters in the block coordinate descent methods
as in the previous examples.
We ﬁrst train the standard ResNet and the antisymmetric variant where we use a width of
n = 8 by duplicating the features. The performance of both models is approximately the same; see
right subplot in Figure 3. The optimal accuracy, achieved for N = 1, 024, is around 98.8%.
For the Hamiltonian network with Verlet forward propagation we use a narrower network containing only the original features (i.e., n = 2 and no duplication). As in the previous experiments
the accuracy generally improves with increasing depth (with one exception between levels 4 and
5). The optimal accuracy is obtained at the ﬁnal level (N = 1, 024) with a validation error of
99.1%. We illustrate the results for the Verlet network in Figure 6. The center subplot shows how
the forward propagation successfully rearranges the features such that they can be labeled using
a linear classiﬁer. The right subplot shows that the prediction function ﬁts the training data, but
also approximates the true level sets.
We use the MNIST dataset of hand-written digits to illustrate the applicability of our
methods in image classiﬁcation problems. The data set consists of 60,000 labeled digital images of
size 28×28 showing hand written digits from 0 to 9. We randomly divide the data into 50,000 training and 10,000 validation examples. We train the network using the standard and antisymmetric
ResNet and the ﬁrst-order Hamiltonian network using Verlet integration.
We use a three-level multi-level strategy where the number of layers is 4, 8, and 16. In each step
we use the block coordinate descent method to approximately solve the learning problem and prolongate the forward propagation parameters to the next level using piecewise linear interpolation.
The width of the network is 6 (yielding n = 4, 704 features used in the classiﬁcation) and we use
3 × 3 convolution operators that are fully connected within a given layer to model the linear transformation matrices K0, . . . , KN−1. The ﬁnal time is set to T = 6. To compute the Gauss-Newton
step we ﬁrst compute the full gradient over all 50,000 examples and then randomly subsample 5,000
input training data
propagated training data
prediction + training data
Figure 6: Classiﬁcation results for peaks example described in 6.3 using a Hamiltonian network
with n = 1, 024 layers and the Verlet forward propagation. Left: We illustrate the training data by
colored dots that represent the class. Center: We show the propagated features and the predictions
of the softmax classiﬁer. Right: We depict the predictions of our network (colored background)
and the training data. (This ﬁgure is optimized for screen use.)
terms for Hessian computations in the PCG step. The maximum number of iterations is set to 20
at each layer.
Within each step of the block coordinate descent we solve the classiﬁcation problem using at
most 5 iterations of Newton-PCG with up to 10 inner iterations. We use a discretized Laplacian
as a regularization operator in Eq. 5.25 and its shifted symmetric product as a preconditioner to
favor smooth modes; see . The regularization parameter used in the classiﬁcation is 0.01. The
output values for all examples are used at the ﬁnal layer and no pooling is performed.
Smooth time dynamics are enforced by penalizing the time derivatives as outlined in Section 5.1
with a manually calibrated regularization parameter of α = 0.005. The regularization operator is
used to precondition the PCG iterations in the Gauss-Newton method.
To show the performance of the multi-level strategy, we summarize the training and validation
errors at the respective levels in Table 1. The table shows similar performance for all forward
propagation methods.
Note that both the validation and training error are reduced for larger
number of layers, but no overﬁtting is observed.
In this experiment, the multi-level approach
considerably simpliﬁed the initialization of the deep networks. For example, the initial parameters
of the standard ResNet at the ﬁnal level (N = 16 layers) already gave a validation accuracy of
98.41%, which was improved to 98.47%. We illustrate the results of the antisymmetric ResNet,
which yields slightly superior performance in terms of validation error, in Figure 7. The smoothness
of the classiﬁcation weights enforced by our regularizer can be seen in the right column.
Summary and conclusions
In this paper, we expose the relation between deep learning and dynamic inverse problems and lay
the foundation for fruitful research at the interface of inverse problems and data science. Speciﬁcally,
we propose new architectures for deep neural networks (DNN) that improve the stability of the
forward propagation. Using derivative-based regularization, we also improve the well-posedness
of the learning task. We also propose a multi-level strategy that simpliﬁes the choice of initial
anti symmetric ResNet
Hamiltonian Verlet
Table 1: Multi-level training result for MNIST data set (see Section 6.4). We show the Training
Error (TE) and Validation Error (VE) for the standard and antisymmetric ResNet and the Hamiltonian inspired network with Verlet forward propagation for increasing number of layers in the
network. The diﬀerent forward propagation methods yield comparable results with the antisymmetric ResNet giving slightly lower validation errors at each level.
parameters and thereby simpliﬁes the training of very deep networks. Our forward propagation
methods are inspired by Hamiltonian systems and motivated by a stability analysis that is based
on a continuous formulation of the learning problem.
Our stability discussion in Section 3 provides new insights why ResNets , a commonly
used forward propagation scheme in deep learning, can be unstable.
Our result is based on a
continuous interpretation of a simpliﬁed version of ResNets as a system of nonlinear ODEs and
a standard stability argument.
Using intuitive examples, we show the impact of the spectral
properties of the transformation matrices on the stability of the forward propagation and state
conditions under which gradients explode, vanish, or are stable. We also note that stability depends
on the smoothness of the network parameters over time. Based on our ﬁndings and numerical
experience, we argue that it is desirable to restrict ResNets to matrices that lead to Jacobians
whose real parts of the spectrum are close to zero and penalize the time-derivative of network
parameters through regularization.
We also emphasize that the well-posedness of the learning
problem in ResNets relies on suﬃciently small time steps. Extending our analysis to more general
versions of ResNet is straightforward and will be investigated in future work.
Motivated by our theoretical considerations, we propose three new forward propagation methods
in Section 4 that can be used to obtain well-posed learning problems for arbitrarily deep networks.
The ﬁrst one is a simple modiﬁcation of ResNet that applies a linear transformation to the transformation matrices to annihilate the real parts of their eigenvalues. The other two methods are
inspired by Hamiltonian systems and provide alternative ways to preserve in information in the
forward propagation. The Hamiltonian networks require special integration techniques; see for
details on their treatment. The second-order forward propagation can be discretized using the
leapfrog method, and our implementation of the ﬁrst-order propagation uses the Verlet method.
While the stability of the leapfrog method requires matrices with non-positive real eigenvalues, the
Verlet method does not require any restrictions and yields the best performance in our numerical
experiments.
Our approach to stabilizing the learning problem for very deep architectures diﬀers signiﬁcantly
from existing approaches, most importantly batch normalization . Batch normalization scales
values at hidden layers to prevent vanishing or exploding gradients and has been shown to improve
the eﬃciency of training deep networks. In contrast to that our approach does not modify the values
of the features, but alleviates the need of normalization by constructing stable forward propagation
propagated ”0”
propagated ”8”
weights for ”0”
weights for ”8”
Figure 7: Classiﬁcation results for the MNIST data set (see Section 6.4) using an antisymmetric
ResNet with N = 16 layers and 6 neurons. We show two randomly chosen input images, their
propagated versions at the output layer, and the classiﬁcation weights for the two digits shown.
The smoothness enforced by the second-order regularization operator is evident in the weights.
In order to improve the generalization quality, improve stability, and simplify training of deep
networks we also propose new regularization approaches that depend on our continuous formulation
of the problem.
We use derivative-based regularizers to favor smooth time dynamics and, for
image classiﬁcation problems, spatially smooth classiﬁcation weights. To further regularize and
simplify the initialization of the learning algorithm, we employ a multi-level learning strategy
that gradually increases the depth of the network. In our experiments, this approach has been a
simple yet eﬀective way to obtain good initializations for the learning problems. Our regularization
methods are commonly used in imaging science, e.g., image registration , however, to the best
of our knowledge not commonly employed in deep learning. Our numerical examples show that
approximately solving the regularized learning problem yields works that generalize well even when
the number of network parameters exceeds the number of training features.
We illustrate our methods using three academic test problems with available ground-truth and
the MNIST problem , which is a commonly used benchmark problem for image classiﬁcation. Our experiments show that the proposed new architectures yield results that are competitive
with the established ResNet architecture. This is particularly noteworthy for the proposed antisymmetric ResNet, where we restrict the dimensionality of the search space to ensure stability.
By establishing a link between deep learning and dynamic inverse problems, we are positive that
this work will stimulate further research by both communities. An important item of future work is
investigating the impact of the proposed architectures on the performance of learning algorithms.
Currently, stochastic gradient descent is commonly used to train deep neural networks due to
its computational eﬃciency and empirical evidence supporting its generalizations . A speciﬁc
question is if the improvements in the forward propagation and regularization proposed in this paper
will lead to better generalization properties of subsampled second-order methods such as .
Another thrust of future work is the development of automatic parameter selection strategies for
deep learning based on the approaches presented, e.g., in . A particular challenge
in this application is the nontrivial relationship between the regularization parameters chosen for
the classiﬁcation and forward propagation parameters.
Acknowledgements
This work is supported in part by National Science Foundation award DMS 1522599 and by the
NVIDIA Corporation through their donation of a Titan X GPU.