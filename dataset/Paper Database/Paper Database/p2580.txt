Frustum PointNets for 3D Object Detection from RGB-D Data
Charles R. Qi1∗
Chenxia Wu2
Leonidas J. Guibas1
1Stanford University
2Nuro, Inc.
3UC San Diego
In this work, we study 3D object detection from RGB-
D data in both indoor and outdoor scenes. While previous
methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly
operate on raw point clouds by popping up RGB-D scans.
However, a key challenge of this approach is how to efﬁciently localize objects in point clouds of large-scale scenes
(region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization,
achieving efﬁciency as well as high recall for even small objects. Beneﬁted from learning directly in raw point clouds,
our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse
points. Evaluated on KITTI and SUN RGB-D 3D detection
benchmarks, our method outperforms the state of the art by
remarkable margins while having real-time capability.
1. Introduction
Recently, great progress has been made on 2D image understanding tasks, such as object detection and instance
segmentation . However, beyond getting 2D bounding
boxes or pixel masks, 3D understanding is eagerly in demand in many applications such as autonomous driving and
augmented reality (AR). With the popularity of 3D sensors
deployed on mobile devices and autonomous vehicles, more
and more 3D data is captured and processed. In this work,
we study one of the most important 3D perception tasks –
3D object detection, which classiﬁes the object category and
estimates oriented 3D bounding boxes of physical objects
from 3D sensor data.
While 3D sensor data is often in the form of point clouds,
how to represent point cloud and what deep net architectures to use for 3D object detection remains an open problem. Most existing works convert 3D point clouds to images by projection or to volumetric grids by quantization and then apply convolutional networks.
∗Majority of the work done as an intern at Nuro, Inc.
depth to point cloud
2D region (from CNN) to 3D frustum
3D box (from PointNet)
Figure 1. 3D object detection pipeline. Given RGB-D data, we
ﬁrst generate 2D object region proposals in the RGB image using a
CNN. Each 2D region is then extruded to a 3D viewing frustum in
which we get a point cloud from depth data. Finally, our frustum
PointNet predicts a (oriented and amodal) 3D bounding box for
the object from the points in frustum.
This data representation transformation, however, may obscure natural 3D patterns and invariances of the data. Recently, a number of papers have proposed to process point
clouds directly without converting them to other formats.
For example, proposed new types of deep net architectures, called PointNets, which have shown superior performance and efﬁciency in several 3D understanding tasks
such as object classiﬁcation and semantic segmentation.
While PointNets are capable of classifying a whole point
cloud or predicting a semantic class for each point in a point
cloud, it is unclear how this architecture can be used for
instance-level 3D object detection. Towards this goal, we
have to address one key challenge: how to efﬁciently propose possible locations of 3D objects in a 3D space. Imitating the practice in image detection, it is straightforward
to enumerate candidate 3D boxes by sliding windows 
or by 3D region proposal networks such as . However,
the computational complexity of 3D search typically grows
cubically with respect to resolution and becomes too expensive for large scenes or real-time applications such as
autonomous driving.
Instead, in this work, we reduce the search space following the dimension reduction principle: we take the advantage of mature 2D object detectors (Fig. 1). First, we
extract the 3D bounding frustum of an object by extruding
2D bounding boxes from image detectors. Then, within the
3D space trimmed by each of the 3D frustums, we consecutively perform 3D object instance segmentation and amodal
 
3D bounding box regression using two variants of Point-
Net. The segmentation network predicts the 3D mask of
the object of interest (i.e. instance segmentation); and the
regression network estimates the amodal 3D bounding box
(covering the entire object even if only part of it is visible).
In contrast to previous work that treats RGB-D data as
2D maps for CNNs, our method is more 3D-centric as we
lift depth maps to 3D point clouds and process them using 3D tools. This 3D-centric view enables new capabilities
for exploring 3D data in a more effective manner. First,
in our pipeline, a few transformations are applied successively on 3D coordinates, which align point clouds into a
sequence of more constrained and canonical frames. These
alignments factor out pose variations in data, and thus make
3D geometry pattern more evident, leading to an easier job
of 3D learners. Second, learning in 3D space can better exploits the geometric and topological structure of 3D space.
In principle, all objects live in 3D space; therefore, we believe that many geometric structures, such as repetition, planarity, and symmetry, are more naturally parameterized and
captured by learners that directly operate in 3D space. The
usefulness of this 3D-centric network design philosophy has
been supported by much recent experimental evidence.
Our method achieve leading positions on KITTI 3D object detection and bird’s eye view detection benchmarks. Compared with the previous state of the art , our
method is 8.04% better on 3D car AP with high efﬁciency
(running at 5 fps). Our method also ﬁts well to indoor RGB-
D data where we have achieved 8.9% and 6.4% better 3D
mAP than and on SUN-RGBD while running one
to three orders of magnitude faster.
The key contributions of our work are as follows:
• We propose a novel framework for RGB-D data based
3D object detection called Frustum PointNets.
• We show how we can train 3D object detectors under our framework and achieve state-of-the-art performance on standard 3D object detection benchmarks.
• We provide extensive quantitative evaluations to validate our design choices as well as rich qualitative results for understanding the strengths and limitations of
our method.
2. Related Work
3D Object Detection from RGB-D Data
Researchers
have approached the 3D detection problem by taking various ways to represent RGB-D data.
Front view image based methods:
 take
monocular RGB images and shape priors or occlusion patterns to infer 3D bounding boxes. represent depth
data as 2D maps and apply CNNs to localize objects in 2D
image. In comparison we represent depth as a point cloud
and use advanced 3D deep networks (PointNets) that can
exploit 3D geometry more effectively.
Bird’s eye view based methods: MV3D projects Li-
DAR point cloud to bird’s eye view and trains a region proposal network (RPN ) for 3D bounding box proposal.
However, the method lags behind in detecting small objects,
such as pedestrians and cyclists and cannot easily adapt to
scenes with multiple objects in vertical direction.
3D based methods: train 3D object classiﬁers
by SVMs on hand-designed geometry features extracted
from point cloud and then localize objects using slidingwindow search. extends by replacing SVM with
3D CNN on voxelized 3D grids. designs new geometric features for 3D object detection in a point cloud. 
convert a point cloud of the entire scene into a volumetric
grid and use 3D volumetric CNN for object proposal and
classiﬁcation. Computation cost for those method is usually quite high due to the expensive cost of 3D convolutions
and large 3D search space. Recently, proposes a 2Ddriven 3D object detection method that is similar to ours
in spirit. However, they use hand-crafted features (based
on histogram of point coordinates) with simple fully connected networks to regress 3D box location and pose, which
is sub-optimal in both speed and performance. In contrast,
we propose a more ﬂexible and effective solution with deep
3D feature learning (PointNets).
Deep Learning on Point Clouds
Most existing works
convert point clouds to images or volumetric forms before
feature learning. voxelize point clouds into
volumetric grids and generalize image CNNs to 3D CNNs.
 design more efﬁcient 3D CNN or neural network architectures that exploit sparsity in point cloud. However, these CNN based methods still require quantitization
of point clouds with certain voxel resolution. Recently, a
few works propose a novel type of network architectures (PointNets) that directly consumes raw point clouds
without converting them to other formats. While PointNets
have been applied to single object classiﬁcation and semantic segmentation, our work explores how to extend the architecture for the purpose of 3D object detection.
3. Problem Deﬁnition
Given RGB-D data as input, our goal is to classify and
localize objects in 3D space. The depth data, obtained from
LiDAR or indoor depth sensors, is represented as a point
cloud in RGB camera coordinates. The projection matrix
is also known so that we can get a 3D frustum from a 2D
image region. Each object is represented by a class (one
among k predeﬁned classes) and an amodal 3D bounding
box. The amodal box bounds the complete object even if
part of the object is occluded or truncated. The 3D box is
region2frustum
point cloud
in frustum
(n points)
3D Instance
Segmentation
one-hot class vector
Estimation
center residual
translation
Box Parameters
Amodal 3D Box Estimation
3D Instance Segmentation
Frustum Proposal
object points
(m points)
Figure 2. Frustum PointNets for 3D object detection. We ﬁrst leverage a 2D CNN object detector to propose 2D regions and classify
their content. 2D regions are then lifted to 3D and thus become frustum proposals. Given a point cloud in a frustum (n × c with n points
and c channels of XYZ, intensity etc. for each point), the object instance is segmented by binary classiﬁcation of each point. Based on the
segmented object point cloud (m×c), a light-weight regression PointNet (T-Net) tries to align points by translation such that their centroid
is close to amodal box center. At last the box estimation net estimates the amodal 3D bounding box for the object. More illustrations on
coordinate systems involved and network input, output are in Fig. 4 and Fig. 5.
parameterized by its size h, w, l, center cx, cy, cz, and orientation θ, φ, ψ relative to a predeﬁned canonical pose for
each category. In our implementation, we only consider the
heading angle θ around the up-axis for orientation.
4. 3D Detection with Frustum PointNets
As shown in Fig. 2, our system for 3D object detection
consists of three modules: frustum proposal, 3D instance
segmentation, and 3D amodal bounding box estimation. We
will introduce each module in the following subsections.
We will focus on the pipeline and functionality of each module, and refer readers to supplementary for speciﬁc architectures of the deep networks involved.
4.1. Frustum Proposal
The resolution of data produced by most 3D sensors, especially real-time depth sensors, is still lower than RGB
images from commodity cameras. Therefore, we leverage
mature 2D object detector to propose 2D object regions in
RGB images as well as to classify objects.
With a known camera projection matrix, a 2D bounding
box can be lifted to a frustum (with near and far planes speciﬁed by depth sensor range) that deﬁnes a 3D search space
for the object. We then collect all points within the frustum
to form a frustum point cloud. As shown in Fig 4 (a), frustums may orient towards many different directions, which
result in large variation in the placement of point clouds.
We therefore normalize the frustums by rotating them toward a center view such that the center axis of the frustum is
orthogonal to the image plane. This normalization helps improve the rotation-invariance of the algorithm. We call this
entire procedure for extracting frustum point clouds from
RGB-D data frustum proposal generation.
While our 3D detection framework is agnostic to the exact method for 2D region proposal, we adopt a FPN 
based model. We pre-train the model weights on ImageNet
classiﬁcation and COCO object detection datasets and further ﬁne-tune it on a KITTI 2D object detection dataset to
classify and predict amodal 2D boxes. More details of the
2D detector training are provided in the supplementary.
4.2. 3D Instance Segmentation
Given a 2D image region (and its corresponding 3D frustum), several methods might be used to obtain 3D location of the object: One straightforward solution is to directly regress 3D object locations (e.g., by 3D bounding
box) from a depth map using 2D CNNs.
However, this
problem is not easy as occluding objects and background
clutter is common in natural scenes (as in Fig. 3), which
may severely distract the 3D localization task. Because objects are naturally separated in physical space, segmentation
in 3D point cloud is much more natural and easier than that
in images where pixels from distant objects can be near-by
to each other. Having observed this fact, we propose to segcamera
Background
Foreground
Object of Interest
Figure 3. Challenges for 3D detection in frustum point cloud.
Left: RGB image with an image region proposal for a person.
Right: bird’s eye view of the LiDAR points in the extruded frustum from 2D box, where we see a wide spread of points with both
foreground occluder (bikes) and background clutter (building).
(a) camera
coordinate
(b) frustum
coordinate
(c) 3D mask
coordinate
(d) 3D object
coordinate
mask point
Figure 4. Coordinate systems for point cloud. Artiﬁcial points
(black dots) are shown to illustrate (a) default camera coordinate; (b) frustum coordinate after rotating frustums to center view
(Sec. 4.1); (c) mask coordinate with object points’ centroid at origin (Sec. 4.2); (d) object coordinate predicted by T-Net (Sec. 4.3).
ment instances in 3D point cloud instead of in 2D image or
depth map. Similar to Mask-RCNN , which achieves
instance segmentation by binary classiﬁcation of pixels in
image regions, we realize 3D instance segmentation using a
PointNet-based network on point clouds in frustums.
Based on 3D instance segmentation, we are able to
achieve residual based 3D localization. That is, rather than
regressing the absolute 3D location of the object whose offset from the sensor may vary in large ranges (e.g. from 5m
to beyond 50m in KITTI data), we predict the 3D bounding
box center in a local coordinate system – 3D mask coordinates as shown in Fig. 4 (c).
3D Instance Segmentation PointNet.
The network takes
a point cloud in frustum and predicts a probability score for
each point that indicates how likely the point belongs to the
object of interest. Note that each frustum contains exactly
one object of interest. Here those “other” points could be
points of non-relevant areas (such as ground, vegetation) or
other instances that occlude or are behind the object of interest. Similar to the case in 2D instance segmentation, depending on the position of the frustum, object points in one
frustum may become cluttered or occlude points in another.
Therefore, our segmentation PointNet is learning the occlusion and clutter patterns as well as recognizing the geometry
for the object of a certain category.
In a multi-class detection case, we also leverage the semantics from a 2D detector for better instance segmentation.
For example, if we know the object of interest is
a pedestrian, then the segmentation network can use this
prior to ﬁnd geometries that look like a person. Speciﬁcally, in our architecture we encode the semantic category
as a one-hot class vector (k dimensional for the pre-deﬁned
k categories) and concatenate the one-hot vector to the intermediate point cloud features. More details of the speciﬁc
architectures are described in the supplementary.
After 3D instance segmentation, points that are classiﬁed
as the object of interest are extracted (“masking” in Fig. 2).
Amodal 3D Box Estimation PointNet
Abstraction
Propagation
frustum point cloud
(frustum coordinate)
object of interest
probability
Abstraction
object point cloud
(object coordinate)
box parameters
(object coordinate)
3D Instance Segmentation PointNet
Abstraction
center residual
(mask coordinate)
object point cloud
(mask coordinate)
Figure 5. Basic architectures and IO for PointNets. Architecture
is illustrated for PointNet++ (v2) models with set abstraction
layers and feature propagation layers (for segmentation). Coordinate systems involved are visualized in Fig. 4.
Having obtained these segmented object points, we further
normalize its coordinates to boost the translational invariance of the algorithm, following the same rationale as in
the frustum proposal step. In our implementation, we transform the point cloud into a local coordinate by subtracting
XYZ values by its centroid. This is illustrated in Fig. 4 (c).
Note that we intentionally do not scale the point cloud, because the bounding sphere size of a partial point cloud can
be greatly affected by viewpoints and the real size of the
point cloud helps the box size estimation.
In our experiments, we ﬁnd that coordinate transformations such as the one above and the previous frustum rotation are critical for 3D detection result as shown in Tab. 8.
4.3. Amodal 3D Box Estimation
Given the segmented object points (in 3D mask coordinate), this module estimates the object’s amodal oriented
3D bounding box by using a box regression PointNet together with a preprocessing transformer network.
Learning-based 3D Alignment by T-Net
Even though
we have aligned segmented object points according to their
centroid position, we ﬁnd that the origin of the mask coordinate frame (Fig. 4 (c)) may still be quite far from the amodal
box center. We therefore propose to use a light-weight regression PointNet (T-Net) to estimate the true center of the
complete object and then transform the coordinate such that
the predicted center becomes the origin (Fig. 4 (d)).
The architecture and training of our T-Net is similar to
the T-Net in , which can be thought of as a special type
of spatial transformer network (STN) . However, different from the original STN that has no direct supervision on
transformation, we explicitly supervise our translation network to predict center residuals from the mask coordinate
origin to real object center.
Amodal 3D Box Estimation PointNet
The box estimation network predicts amodal bounding boxes (for entire
object even if part of it is unseen) for objects given an object point cloud in 3D object coordinate (Fig. 4 (d)). The
network architecture is similar to that for object classiﬁcation , however the output is no longer object class
scores but parameters for a 3D bounding box.
As stated in Sec. 3, we parameterize a 3D bounding box
by its center (cx, cy, cz), size (h, w, l) and heading angle
θ (along up-axis). We take a “residual” approach for box
center estimation. The center residual predicted by the box
estimation network is combined with the previous center
residual from the T-Net and the masked points’ centroid to
recover an absolute center (Eq. 1). For box size and heading
angle, we follow previous works and use a hybrid
of classiﬁcation and regression formulations. Speciﬁcally
we pre-deﬁne NS size templates and NH equally split angle bins. Our model will both classify size/heading (NS
scores for size, NH scores for heading) to those pre-deﬁned
categories as well as predict residual numbers for each category (3×NS residual dimensions for height, width, length,
NH residual angles for heading). In the end the net outputs
3 + 4 × NS + 2 × NH numbers in total.
Cpred = Cmask + ∆Ct−net + ∆Cbox−net
4.4. Training with Multi-task Losses
We simultaneously optimize the three nets involved (3D
instance segmentation PointNet, T-Net and amodal box estimation PointNet) with multi-task losses (as in Eq. 2).
Lc1−reg is for T-Net and Lc2−reg is for center regression
of box estimation net. Lh−cls and Lh−reg are losses for
heading angle prediction while Ls−cls and Ls−reg are for
box size. Softmax is used for all classiﬁcation tasks and
smooth-l1 (huber) loss is used for all regression cases.
Lmulti−task =Lseg + λ(Lc1−reg + Lc2−reg + Lh−cls+
Lh−reg + Ls−cls + Ls−reg + γLcorner)
Corner Loss for Joint Optimization of Box Parameters
While our 3D bounding box parameterization is compact
and complete, learning is not optimized for ﬁnal 3D box accuracy – center, size and heading have separate loss terms.
Imagine cases where center and size are accurately predicted but heading angle is off – the 3D IoU with ground
truth box will then be dominated by the angle error. Ideally all three terms (center,size,heading) should be jointly
optimized for best 3D box estimation (under IoU metric).
To resolve this problem we propose a novel regularization
loss, the corner loss:
In essence, the corner loss is the sum of the distances
between the eight corners of a predicted box and a ground
truth box. Since corner positions are jointly determined by
center, size and heading, the corner loss is able to regularize
the multi-task training for those parameters.
To compute the corner loss, we ﬁrstly construct NS ×
NH “anchor” boxes from all size templates and heading
angle bins. The anchor boxes are then translated to the estimated box center. We denote the anchor box corners as
k , where i, j, k are indices for the size class, heading
class, and (predeﬁned) corner order, respectively. To avoid
large penalty from ﬂipped heading estimation, we further
compute distances to corners (P ∗∗
k ) from the ﬂipped ground
truth box and use the minimum of the original and ﬂipped
cases. δij, which is one for the ground truth size/heading
class and zero else wise, is a two-dimensional mask used to
select the distance term we care about.
5. Experiments
Experiments are divided into three parts1. First we compare with state-of-the-art methods for 3D object detection
on KITTI and SUN-RGBD (Sec 5.1). Second,
we provide in-depth analysis to validate our design choices
(Sec 5.2). Last, we show qualitative results and discuss the
strengths and limitations of our methods (Sec 5.3).
5.1. Comparing with state-of-the-art Methods
We evaluate our 3D object detector on KITTI and
SUN-RGBD benchmarks for 3D object detection. On
both tasks we have achieved signiﬁcantly better results
compared with state-of-the-art methods.
Tab. 1 shows the performance of our 3D detector
on the KITTI test set. We outperform previous state-of-theart methods by a large margin. While MV3D uses multiview feature aggregation and sophisticated multi-sensor fusion strategy, our method based on the PointNet (v1)
and PointNet++ (v2) backbone is much cleaner in design. While out of the scope for this work, we expect that
sensor fusion (esp. aggregation of image feature for 3D detection) could further improve our results.
We also show our method’s performance on 3D object
localization (bird’s eye view) in Tab. 2. In the 3D localization task bounding boxes are projected to bird’s eye view
plane and IoU is evaluated on oriented 2D boxes. Again,
our method signiﬁcantly outperforms previous works which
include DoBEM and MV3D that use CNNs on projected LiDAR images, as well as 3D FCN that uses 3D
CNNs on voxelized point cloud.
1Details on network architectures, training parameters as well as more
experiments are included in the supplementary material.
Pedestrians
DoBEM 
Table 1. 3D object detection 3D AP on KITTI test set. DoBEM and MV3D (previous state of the art) are based on 2D CNNs with
bird’s eye view LiDAR image. Our method, without sensor fusion or multi-view aggregation, outperforms those methods by large margins
on all categories and data subsets. 3D bounding box IoU threshold is 70% for cars and 50% for pedestrians and cyclists.
Pedestrians
DoBEM 
3D FCN 
Table 2. 3D object localization AP (bird’s eye view) on KITTI test set. 3D FCN uses 3D CNNs on voxelized point cloud and is far
from real-time. MV3D is the previous state of the art. Our method signiﬁcantly outperforms those methods on all categories and data
subsets. Bird’s eye view 2D bounding box IoU threshold is 70% for cars and 50% for pedestrians and cyclists.
Mono3D 
VeloFCN 
MV3D (LiDAR) 
Table 3. 3D object detection AP on KITTI val set (cars only).
Mono3D 
VeloFCN 
MV3D (LiDAR) 
Table 4. 3D object localization AP on KITTI val set (cars only).
The output of our network is visualized in Fig. 6 where
we observe accurate 3D instance segmentation and box prediction even under very challenging cases. We defer more
discussions on success and failure case patterns to Sec. 5.3.
We also report performance on KITTI val set (the same split
as in ) in Tab. 3 and Tab. 4 (for cars) to support comparison with more published works, and in Tab. 5 (for pedestrians and cyclists) for reference.
Most previous 3D detection works specialize either on outdoor LiDAR scans where objects are well
separated in space and the point cloud is sparse (so that
it’s feasible for bird’s eye projection), or on indoor depth
maps that are regular images with dense pixel values such
Pedestrian (3D Detection)
Pedestrian (Bird’s Eye View)
Cyclist (3D Detection)
Cyclist (Bird’s Eye View)
Table 5. Performance on KITTI val set for pedestrians and cyclists.
Model evaluated is Ours (v2).
that image CNNs can be easily applied. However, methods
designed for bird’s eye view may be incapable for indoor
rooms where multiple objects often exist together in vertical space. On the other hand, indoor focused methods could
ﬁnd it hard to apply to sparse and large-scale point cloud
from LiDAR scans.
In contrast, our frustum-based PointNet is a generic
framework for both outdoor and indoor 3D object detection. By applying the same pipeline we used for KITTI data
set, we’ve achieved state-of-the-art performance on SUN-
RGBD benchmark (Tab. 6) with signiﬁcantly higher mAP
as well as much faster (10x-1000x) inference speed.
5.2. Architecture Design Analysis
In this section we provide analysis and ablation experiments to validate our design choices.
Experiment setup.
Unless otherwise noted, all experiments in this section are based on our v1 model on KITTI
data using train/val split as in . To decouple the inﬂuence
of 2D detectors, we use ground truth 2D boxes for region
proposals and use 3D box estimation accuracy (IoU threshold 0.7) as the evaluation metric. We will only focus on the
car category which has the most training examples.
Figure 6. Visualizations of Frustum PointNet results on KITTI val set (best viewed in color with zoom in). These results are based
on PointNet++ models , running at 5 fps and achieving test set 3D AP of 70.39, 44.89 and 56.77 for car, pedestrian and cyclist,
respectively. 3D instance masks on point cloud are shown in color. True positive detection boxes are in green, while false positive boxes
are in red and groundtruth boxes in blue are shown for false positive and false negative cases. Digit and letter beside each box denote
instance id and semantic class, with “v” for cars, “p” for pedestrian and “c” for cyclist. See Sec. 5.3 for more discussion on the results.
nightstand
2D-driven 
Table 6. 3D object detection AP on SUN-RGBD val set. Evaluation metric is average precision with 3D IoU threshold 0.25 as proposed
by . Note that both COG and 2D-driven use room layout context to boost performance while ours and DSS not.
Compared with previous state-of-the-arts our method is 6.4% to 11.9% better in mAP as well as one to three orders of magnitude faster.
Comparing with alternative approaches for 3D detection.
In this part we evaluate a few CNN-based baseline
approaches as well as ablated versions and variants of our
pipelines using 2D masks. In the ﬁrst row of Tab. 7, we
show 3D box estimation results from two CNN-based networks.
The baseline methods trained VGG models
on ground truth boxes of RGB-D images and adopt the
same box parameter and loss functions as our main method.
While the model in the ﬁrst row directly estimates box location and parameters from vanilla RGB-D image patch,
the other one (second row) uses a FCN trained from the
COCO dataset for 2D mask estimation (as that in Mask-
RCNN ) and only uses features from the masked region
for prediction. The depth values are also translated by subtracting the median depth within the 2D mask. However,
both CNN baselines get far worse results compared to our
main method.
To understand why CNN baselines underperform, we visualize a typical 2D mask prediction in Fig. 7. While the
estimated 2D mask appears in high quality on an RGB image, there are still lots of clutter and foreground points in
the 2D mask. In comparison, our 3D instance segmentation gets much cleaner result, which greatly eases the next
module in ﬁner localization and bounding box regression.
In the third row of Tab. 7, we experiment with an ablated
version of frustum PointNet that has no 3D instance segmentation module. Not surprisingly, the model gets much
worse results than our main method, which indicates the
critical effect of our 3D instance segmentation module. In
the fourth row, instead of 3D segmentation we use point
clouds from 2D masked depth maps (Fig. 7) for 3D box estimation. However, since a 2D mask is not able to cleanly
segment the 3D object, the performance is more than 12%
worse than that with the 3D segmentation (our main method
in the ﬁfth row). On the other hand, a combined usage of 2D
and 3D masks – applying 3D segmentation on point cloud
network arch.
depth representation
point cloud
point cloud
point cloud
point cloud
Table 7. Comparing 2D and 3D approaches. 2D mask is from
FCN on RGB image patch. 3D mask is from PointNet on frustum
point cloud. 2D+3D mask is 3D mask generated by PointNet on
point cloud poped up from 2D masked depth map.
frustum rot.
mask centralize
Table 8. Effects of point cloud normalization. Metric is 3D box
estimation accuracy with IoU=0.7.
regularization
regression only
cls-reg (normalized)
cls-reg (normalized)
corner loss
Table 9. Effects of 3D box loss formulations. Metric is 3D box
estimation accuracy with IoU=0.7.
from 2D masked depth map – also shows slightly worse results than our main method probably due to the accumulated
error from inaccurate 2D mask predictions.
Effects of point cloud normalization.
As shown in
Fig. 4, our frustum PointNet takes a few key coordinate
transformations to canonicalize the point cloud for more effective learning. Tab. 8 shows how each normalization step
helps for 3D detection. We see that both frustum rotation
(such that frustum points have more similar XYZ distributions) and mask centroid subtraction (such that object points
have smaller and more canonical XYZ) are critical. In addition, extra alignment of object point cloud to object center
by T-Net also contributes signiﬁcantly to the performance.
Effects of regression loss formulation and corner loss.
In Tab. 9 we compare different loss options and show that a
combination of “cls-reg” loss (the classiﬁcation and residual
regression approach for heading and size regression) and a
regularizing corner loss achieves the best result.
The naive baseline using regression loss only (ﬁrst row)
achieves unsatisfactory result because the regression target
is large in range (object size from 0.2m to 5m). In comparison, the cls-reg loss and a normalized version (residual
normalized by heading bin size or template shape size) of it
achieve much better performance. At last row we show that
a regularizing corner loss further helps optimization.
2d mask by CNN
range: 9m ~ 55m
range: 12m ~ 16m
points from our 3d
instance segmentation
points from masked
2d depth map
(baseline)
range: 8m ~ 55m
Figure 7. Comparisons between 2D and 3D masks. We show a
typical 2D region proposal from KITTI val set with both 2D (on
RGB image) and 3D (on frustum point cloud) instance segmentation results. The red numbers denote depth ranges of points.
5.3. Qualitative Results and Discussion
In Fig. 6 we visualize representative outputs of our frustum PointNet model. We see that for simple cases of nonoccluded objects in reasonable distance (so we get enough
number of points), our model outputs remarkably accurate
3D instance segmentation mask and 3D bounding boxes.
Second, we are surprised to ﬁnd that our model can even
predict correctly posed amodal 3D box from partial data
(e.g. parallel parked cars) with few points. Even humans
ﬁnd it very difﬁcult to annotate such results with point cloud
data only. Third, in some cases that seem very challenging
in images with lots of nearby or even overlapping 2D boxes,
when converted to 3D space, the localization becomes much
easier (e.g. P11 in second row third column).
On the other hand, we do observe several failure patterns, which indicate possible directions for future efforts.
The ﬁrst common mistake is due to inaccurate pose and
size estimation in a sparse point cloud (sometimes less than
5 points). We think image features could greatly help esp.
since we have access to high resolution image patch even
for far-away objects. The second type of challenge is when
there are multiple instances from the same category in a
frustum (like two persons standing by). Since our current
pipeline assumes a single object of interest in each frustum, it may get confused when multiple instances appear
and thus outputs mixed segmentation results. This problem could potentially be mitigated if we are able to propose
multiple 3D bounding boxes within each frustum. Thirdly,
sometimes our 2D detector misses objects due to dark lighting or strong occlusion. Since our frustum proposals are
based on region proposal, no 3D object will be detected
given no 2D detection. However, our 3D instance segmentation and amodal 3D box estimation PointNets are not restricted to RGB view proposals. As shown in the supplementary, the same framework can also be extended to 3D
regions proposed in bird’s eye view.
Acknowledgement
The authors wish to thank the support
of Nuro Inc., ONR MURI grant N00014-13-1-0341, NSF
grants DMS-1546206 and IIS-1528025, a Samsung GRO
award, and gifts from Adobe, Amazon, and Apple.