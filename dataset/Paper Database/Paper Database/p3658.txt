Mach Learn 68: 235–265
DOI 10.1007/s10994-007-5019-5
Active learning for logistic regression: an evaluation
Andrew I. Schein · Lyle H. Ungar
Received: 21 April 2006 / Revised: 4 March 2007 / Accepted: 18 June 2007 /
Published online: 4 August 2007
Springer Science+Business Media, LLC 2007
Abstract Which active learning methods can we expect to yield good performance in learning binary and multi-category logistic regression classiﬁers? Addressing this question is a
natural ﬁrst step in providing robust solutions for active learning across a wide variety of exponential models including maximum entropy, generalized linear, log-linear, and conditional
random ﬁeld models. For the logistic regression model we re-derive the variance reduction
method known in experimental design circles as ‘A-optimality.’ We then run comparisons
against different variations of the most widely used heuristic schemes: query by committee and uncertainty sampling, to discover which methods work best for different classes of
problems and why. We ﬁnd that among the strategies tested, the experimental design methods are most likely to match or beat a random sample baseline. The heuristic alternatives
produced mixed results, with an uncertainty sampling variant called margin sampling and
a derivative method called QBB-MM providing the most promising performance at very
low computational cost. Computational running times of the experimental design methods
were a bottleneck to the evaluations. Meanwhile, evaluation of the heuristic methods lead
to an accumulation of negative results. We explore alternative evaluation design parameters
to test whether these negative results are merely an artifact of settings where experimental
design methods can be applied. The results demonstrate a need for improved active learning
methods that will provide reliable performance at a reasonable computational cost.
Keywords Active learning · Logistic regression · Experimental design · Generalized linear
Editor: David Page.
A.I. Schein () · L.H. Ungar
Department of Computer and Information Science, The University of Pennsylvania, 3330 Walnut Street,
Philadelphia, PA 19104-6389, USA
e-mail: 
L.H. Ungar
e-mail: 
Mach Learn 68: 235–265
1 Introduction
Procurement of labeled training data is the seminal step of training a supervised machine
learning algorithm. A recent trend in machine learning has focused on pool-based settings
where unlabeled data is inexpensive and available in large supply, but the labeling task is
expensive. Pool-based active learning methods attempt to reduce the “cost” of learning in a
pool-based setting by using a learning algorithm trained on the existing data and selecting
the portion of the remaining data with the greatest expected beneﬁt. In classiﬁcation settings
beneﬁt may be measured in terms of the generalization accuracy (or error) of the ﬁnal model.
The last decade has also seen increased use of the logistic regression classiﬁer in machine
learning applications, though under different names: multinomial regression, multi-class logistic regression or the maximum entropy classiﬁer. In this study we address the question of
how to best perform pool-based active learning with the logistic regression model. We view
treatment of this problem as a natural ﬁrst step in developing active learning solutions to
the expansive set of models derived from the exponential family of distributions, of which
logistic regression is a member.
1.1 Active learning: a deﬁnition
Active learning is deﬁned as a setting where a learning agent interacts with its environment
in procuring a training set, rather than passively receiving an i.i.d. sample from some underlying distribution. The term pool-based active learning is used to distinguish sampling
a pre-deﬁned pool of examples from other forms of active learning including methods that
construct examples from Rn or other sets from ﬁrst principles. Henceforth we will often
use the term active learning to refer to pool-based active learning; since the study does not
treat the other forms, no confusion will arise. Furthermore, we focus almost entirely on the
problem of training classiﬁers.
The purpose of developing active learning methods is to achieve the best possible generalization accuracy at the least cost, where cost is usually measured as a function of the
number of examples labeled. Frequently we plot the tradeoff between number of examples
labeled and generalization accuracy through learning curves. It is commonly believed that
there should exist active learning methods that perform at least as well as random sampling
from a pool at worst, and these methods should often outperform random sampling. This
belief is given theoretical justiﬁcation under very speciﬁc assumptions , but is also occasionally contradicted by empirical evaluations of existing
1.2 Background and related work
The earliest research in active learning stressed counterexample requests 
or query construction . Focus soon turned to methods applicable
to pool-based active learning including the query by committee method 
and experimental design methods based on A-optimality . The above methods
are motivated by theory and explicit objective functions. Empirical evaluation of such objective function approaches has been scant due to computational costs associated with these
methods. Of late, there are some signs of renewed interest in objective function approaches
 .
There has been growing interest in application of active learning to real-world data sets.
A trend of the last ten years 68: 235–265
2006; Dagan and Engelson 1995; Hwa 2004; Lewis and Gale 1994; McCallum and Nigam
1998; Melville and Mooney 2004; Roy and McCallum 2001; Tang et al. 2002) has been to
employ heuristic methods of active learning with no explicitly deﬁned objective function.
Uncertainty sampling , query by committee ,1
and variants have proven particularly attractive because of their portability across a wide
spectrum of machine learning algorithms. A subtrend in the ﬁeld has sought to improve
performance of heuristics by combining them with secondary heuristics such as: similarity
weighting , interleaving active learning with EM , interleaving active learning with co-training , and
sampling from clusters , among others.
1.3 Purpose and contributions of study
The goal of this study is to learn which of the methods for active learning work best with
logistic regression, and why methods perform badly when they do. We are interested both
in the binary classiﬁcation as well as the less often explored multiple category settings. The
experiments necessary to make conclusions about active learning help establish a picture
of the ‘state of the art’ that will be useful for practitioners of active learning in addition to
researchers in the ﬁeld.
There are two main categories of methods that we evaluate. First, we re-examine the theory of experimental design in the context of the logistic regression classiﬁer. A technique
for minimizing prediction variance known as A-optimality emerges as a promising technique for active learning. The variance reduction technique is generalized in this work from
a squared loss to a log loss. Second, we use these two loss-motivated methods as a baseline
in evaluating six heuristic methods of active learning. Ultimately, we use the evaluations to
make conclusions about the performance of different active learning methods.
The empirical investigations within this work have several distinguishing features. The
focus of this study is on logistic regression, and methods that perform well (or poorly) with
alternative machine learning algorithms may behave differently with logistic regression. The
majority of our evaluations focus on multi-category problems. Our evaluations of the experimental design methods are the largest scale of any to date in a pool-based active learning
setting. So these evaluations are an opportunity to test the hypothesis that the computational
costs of principled methods come with performance gains. Noting that heuristic methods
occasionally perform worse than random, we also explore the causes of these failures, and
identify conditions that lead the uncertainty sampling heuristics to failure.
2 The logistic regression classiﬁer
2.1 Bernoulli model
Logistic regression can be viewed as arising from a Bernoulli
model. Given a set of predictors, xn, we wish to determine the probability of a binary outcome yn. We deﬁne a probability model:
P(Yn = 1|xn) .= σ(w · xn)
1Query by Committee is a method with strong theoretical properties under limited circumstances , but the overwhelming trend has been to apply the method in circumstances where
the theory does not apply. Often the term Query by Bagging is used to describe such ad hoc applications.
Section 3 contains further discussion.
Mach Learn 68: 235–265
with corresponding likelihood function:
P(y|xn,n = 1...N) =
σ(w · xn)yn(1 −σ(w · xn))(1−yn)
σ(w · xn)ynσ(−w · xn)(1−yn)
where the logistic function
1 + exp[−θ]
is a continuous increasing function mapping any real valued θ into the interval (0,1), and
thus is suitable for representing the probability of a Bernoulli trial outcome.
A useful variant for scientiﬁc and sociology experiments employs a binomial rather than Bernoulli formulation to facilitate repeated trials.
2.2 Multinomial model
When there are more than two outcome categories, the situation is a little more complex; the
outcome variables Yn take on one of three or more discrete outcomes rather than a 0 or a 1.
We deﬁne a probability model as follows:
P(Yn = c|xn) .= π(c,xn,w) =
exp(wc · xn)
c′ exp(wc′ · xn).
The parameter vector w of the binary logistic model subdivides into a set of vectors wc: one
for each category. The resulting likelihood is:
P(y|xn,n = 1...N) =
π(c,xn,w)ync.
The multinomial model is a generalization of the binary case, as can be seen by deﬁning
w0 = 0 and w1 = w in which case:
P(Yn = 1|xn) =
exp(w · xn)
exp(0 · xn) + exp(w · xn)
exp(w · xn)
1 + exp(w · xn)
1 + exp(−w · xn)
= σ(w · xn).
The logistic regression model is closely related to a large collection of well-worn models
such as the exponential family of distributions , generalized linear
models , maximum entropy classiﬁers ,
and conditional Markov random ﬁeld models . Schein reviews
these relationships in greater depth.
Mach Learn 68: 235–265
2.3 Parameter estimation
Analysis of the Hessian of the logistic regression log likelihood function reveals the model
is convex in the parameters. Any number of standard convex optimization procedures including gradient, conjugate gradient, and Broyden, Fletcher, Goldfarb, and Shanno (BFGS)
methods sufﬁce . When
the predictors are all positive (xni ≥0), generalized iterative scaling and variants work as well. Iterative scaling procedures have the advantage that they are extremely simple to implement. Methods that take
second order information into account such as conjugate gradient and BFGS are known
to converge quicker than generalized iterative scaling (GIS) and improved iterative scaling
(IIS) in maximum entropy modeling .
An important characteristic of the parameters of logistic regression are the existence and
consistency of the maximum likelihood parameters. It can be shown for logistic regression
parameters w and estimates ˆw that:
L(√n( ˆw −w)) →N(0,F −1(w)),
ˆwn = wn + Op
F refers to the Fisher information matrix. The L in this notation refers to the distribution
that its argument follows, ˆwn and wn refer to estimate based on a sample and expected
estimate of w respectively. F(w) is the Fisher information matrix of the model, described
in Sect. 4.2. The Op notation refers to a rate of convergence in probability. The requisite
theory for demonstrating (11) and (12) is beyond the scope of this exposition, and we refer
the reader to for an account. We use (11)
and (12) in Sect. 4 in deriving an asymptotically correct estimate of variance.
3 Heuristic active learning for logistic regression
All of the pool-based active learning methods evaluated in this study ﬁt into a common
framework described by Algorithm 1. The key difference distinguishing methods of active
learning is the method for ranking examples, amounting to different assessments of the value
of labeling individual examples. Usually, the ranking rule makes use of the model trained
on the currently labeled data. This is the reason for the requirement of a partial training set
when the algorithm begins.
Algorithm 1 A Generalized Active Learning Loop
Other variants of Algorithm 1 are used frequently. For example, some researchers mix
active learning with random acquisition. Others label the top n examples in addition to the
Mach Learn 68: 235–265
top example in order to decrease the number times a learner is retrained. This evaluation
will focus on labeling one example at a time. In principle this gives a rigorous method the
opportunity to pick only the best examples.
The ﬂexibility of the setting described by Algorithm 1 leaves open the possibility of using
heuristics that are independent of the classiﬁcation algorithm (in the present case, logistic
regression). Research of the last ﬁfteen years has produced many heuristics for ranking
examples, and the most prominent methods are introduced in this section in anticipation of
the evaluation. In the general classiﬁcation setting that this study focuses on, little can be
said that relates these approaches to explicit objective functions. Under a few assumptions,
including at a minimum the assumption that classiﬁcation is a noise free function of the
predictors, it may be possible to establish a relationship between each of these methods and
an objective function.
In our evaluations we look at three types of heuristics for active learning: uncertainty
sampling, query by committee and classiﬁer certainty. We describe these methods along
with their computational complexities, and then brieﬂy review variations of these methods
in the remaining subsections.
3.1 Uncertainty sampling
Uncertainty sampling is a term invented by Lewis and Gale , though the ideas can be
traced back to the query methods of Hwang et al. and Baum . We discuss the
Lewis and Gale variant since it is widely implemented and general to probabilistic classiﬁers
such as logistic regression. The uncertainty sampling heuristic chooses for labeling the example for which the model’s current predictions are least certain. The intuitive justiﬁcation
for this approach is that regions where the model is uncertain indicate a decision boundary,
and clarifying the position of decision boundaries is the goal of learning classiﬁers.
A key question is how to measure uncertainty. Different methods of measuring uncertainty will lead to different variants of uncertainty sampling. We will look at two such measures. As a convenient notation we use q to represent the trained model’s predictions, with
qc equal to the predicted probability of class c. One method is to pick the example whose
prediction vector q displays the greatest Shannon entropy:
Such a rule means ranking candidate examples in Algorithm 1 by (13).
An alternative method picks the example with the smallest margin: the difference between the largest two values in the vector q. In other words, if c,c′ are the two most likely
categories for observation xn (under the current model), the margin is measured as follows:
Mn = |ˆP(c|xn) −ˆP(c′|xn)|.
In this case, Algorithm 1 would rank examples by increasing values of margin, with the
smallest value at the top of the ranking.
The original deﬁnition of uncertainty sampling describes the
method in the binary classiﬁcation setting, where the two deﬁnitions of uncertainty are
equivalent. We are not aware of previous usages of minimum margin sampling active learning in multiple category settings except when motivated as a variant of query by committee
(Sect. 3.2).
Mach Learn 68: 235–265
Using uncertainty sampling, the computational cost of picking an example from T candidates is: O(T DK) where D is the number of predictors, K is the number of categories.
In the evaluations we refer to the different uncertainty methods as entropy and margin sampling.
3.2 Query by committee
Query by committee (QBC) was proposed by Seung et al. , and then rejustiﬁed for
the perceptron case by Freund et al. . The method assumes (a) A noise-free (e.g.
separable) classiﬁcation task and (b) A binary classiﬁer with a Gibbs training procedure.
Under these assumptions and a few others a procedure
can be found that guarantees exponential decay in the generalization error:
Eg ∼e−nI(∞)
where I(∞) denotes a limiting (in committee size) query information gain and n is the size
of the training set.
A description of the query by committee algorithm follows. A committee of k models
Mi are sampled from the version space over the existing training set using a Gibbs training
procedure. The next training example is picked to minimize the entropy of the distribution
over the model parameter posteriors. In the case of perceptron learning, this is achieved
by selecting query points of prediction disagreement. The method is repeated until enough
training examples are found to reduce error to an acceptable level.
Alas, the assumptions of the method are frequently broken, and in particular the noisefree assumption does not apply to logistic regression on the data sets we intend to use in
the evaluations. The noise-free assumption is critical to QBC, since the method depends
on an ability to permanently discard a portion of version space (the volume the parameters
may occupy) with each query. Version space volume in the noisy case is analogous to the
D-optimality score of experimental design, since a determinant is essentially a volume measure. Generally the model variance, as measured through the D-optimality score of linear
and non-linear models, does not decrease exponentially in the training set size even under
optimal conditions.
The use of the query by committee method in situations where the assumptions do not
apply is an increasing trend with the modiﬁcations of Abe and Mamitsuka and Mc-
Callum and Nigam who substitute bagging for the Gibbs training procedure. The
term “query by bagging” (QBB) is becoming a catchphrase for algorithms that take a bagging approach to implementing the query by committee procedure. Query by bagging is
implemented as follows. An ensemble of models ˆfi is formed from the existing training set
using the bagging procedure . An observation is picked from the pool that
maximizes disagreement among the ensemble members. The procedure is repeated until
enough training examples are chosen.
As a modiﬁcation to Algorithm 1, the following pseudocode replaces the original line
producing a ranking:
Use bagging to train B classiﬁers ˆfi.
Rank candidates by disagreement among the ˆfi.
The deﬁnition of disagreement is wide open and several methods have been proposed.
A margin-based disagreement method (14) is to average the predictions of the ˆfi (normalizing to ensure a proper distribution), and using the margin computation of (14). We refer to
Mach Learn 68: 235–265
this method as QBB-MM (query by bagging followed by author’s initials). The method is a variation of using probabilities
from the ensemble members rather than votes to form the margin.
An alternative approach to measuring disagreement is to take the average prediction (as
above) and measure the average KL divergence from the average:
KL( ˆfb|| ˆfavg).
Larger values of average divergence indicate more disagreement, and so ranking occurs
from larger to smaller values in Algorithm 1. Following the convention of using the author’s
initials, we refer to this method as QBB-MN . Under these two
disagreement measures, query by bagging methods take only slightly more computational
time than certainty sampling methods: O(BT DK); the cause of the difference is inclusion
of the bag size B in the formula.
3.3 Classiﬁer certainty
For logistic regression and other probabilistic classiﬁers, several researchers have proposed
minimizing the entropy of the algorithm’s predictions :2
ˆP(c|xp)log ˆP(c|xp)
as a criteria for picking a training set. The sum is over the pool of unlabeled data and the set
of categories. In intuitive terms (17) measures degree of certainty of the individual classi-
ﬁcations over the pool, and so we call the method the Classiﬁer Certainty (CC) method. In
order to rank examples in Algorithm 1, an expected value of CC is computed with respect
to the current model ˆP for each candidate. The expectation is over possible labellings of the
candidate. A more detailed explanation of the expectation procedure is given in Sect. 4.3.
Note however, that CC is not a proper loss function and minimization need not lead to
good accuracy; (17) does not depend on the true probabilities P but only the estimates ˆP.
For example, we often ﬁnd ourselves certain of facts or beliefs that are later found not be
true. Restricting the search for examples to those that makes us more certain of previously
held beliefs could be a bad choice when learning.
Excluding the cost of model ﬁtting, implementation of CC is at worse: O(TNKD), where
N is the number of observations from the pool used to compute the beneﬁt of adding an
observation, D is the number of predictors, T is the number of candidates evaluated for
labeling, and K is the number of categories. An approximation that saves computational
time is Monte Carlo sampling from the pool to assess the beneﬁt of labeling. For example,
in our evaluations, we sample 300 examples from the pool to assess model improvement.
2Some readers familiar with the language modeling literature will be used to “prediction entropy” as a measure of performance. However, in language modeling, it is actually a cross-entropy that is measured, not
prediction entropy for the reasons outlined below.
Mach Learn 68: 235–265
Table 1 Notation used in the decomposition of squared error
Expectation with respect to actual distribution governing (x,y).
Expectation with respect to training sets of size s. The s variable is often left implicit.
π(c,x, ˆw;D)
Model’s probability of c given x. Parameter vector ˆw is determined by a training set D.
The variables ˆw or D are frequently dropped in the notation for this reason.
Model’s probability of c given x using arbitrary weight vector w.
3.4 Heuristic generalizations and variations
Uncertainty sampling and query by committee methods appear so general in their implementation that it is tempting to port the methods to more complex problems than the classiﬁcation setting. Such has happened in the case of part of speech tagging, where the query
by committee methods are generalized to apply to hidden Markov models . In parsing, uncertainty sampling and other heuristic approaches
have been applied .
A recent trend in the pool-based active learning literature has been to take various
approaches, usually uncertainty sampling or query by committee and try to improve performance through additional heuristics. Such schemes include: observation similarity weighting , sampling from clusters , interleaving labeling with EM , interleaving labeling with co-training , increasing diversity of ensembles , among
others. These sorts of variations are so numerous that we are unable to evaluate them here.
Though not necessarily following under the category of heuristic methods, recent efforts
in active learning have combined existing method under a decision theory framework including use of the logistic regression classiﬁer .
4 Loss function active learning for logistic regression
In this section we explore a methodology for active learning of the logistic regression classi-
ﬁer using explicit loss functions. The techniques are motivated by experimental design, but
have not been used in active learning of the logistic regression classiﬁer. What makes these
loss functions appealing is that they deﬁne an explicit criterion for labeling examples, and
can use a large class of loss functions. For that reason, we detail their derivation in depth.
Our derivations are for arbitrary numbers of categories. In the binary classiﬁcation setting,
many of the formula simplify .
4.1 A squared error decomposition for probabilistic classiﬁcation
Squared error is a loss function more often associated with regression rather than classiﬁer
settings. However, the loss is still applicable to classiﬁers and so we exploit its analytical
properties in this setting. Geman et al. provide a detailed account of the bias/variance
decomposition for squared loss. We will use some details of the decomposition to understand
what a variance reduction approach to active learning accomplishes.
Analysis of squared error begins with the decomposition into training set independent
and dependent terms:
E[(1c −π(c,x;D))2|x,D] =
E[(1c −E[c|x])2|x,D]
(π(c,x;D) −E[c|x])2.
Mach Learn 68: 235–265
The left hand side is the squared error for a single observation (x,y); the variable 1c is an
indicator function taking on the value 1 when the observation has label c, and 0 otherwise.
The expectation E is with respect to the true distribution producing (y,x).
A further expectation with respect to the distribution generating (x,y) gives the expected
loss over a test set. However we hold x constant to simplify the notation for the time being.
The variable D represents a training set distribution, for our purposes a multiset of s observations (x,y) sampled from the underlying distribution governing (x,y). The ﬁrst term of
the decomposition (18) named “noise” represents error that is training set independent: the
expectation is conditioned on the training set D. Another interpretation of the ﬁrst term is
the portion of error induced when the actual distribution of categories (conditioned on x) is
used in making predictions.
In contrast, the second term of the decomposition depends on the particular training set
since no conditioning on D occurs. A sensible analysis on the second term is to consider the
expectation with respect to alternative training sets D. Taking such an expectation produces
the mean squared error (MSE) of the model:
ED[(π(c,x;D) −E[c|x])2].
The MSE decomposes as follows:
(ED[π(c,x;D)] −E[c|x])2
“squared bias”
ED[(π(c,x;D) −ED[π(c,x;D)])2]
“variance”.
The bias term captures the difference between EDπ(c,x;D) (the expected model from a
ﬁxed size sample) and the distribution that actually generates y from x. The variance term
captures the variability of the model under resampling data sets of ﬁxed size, represented
The notation can capture training sets of differing size using the variable s thusly: Ds, in
which case it is useful to consider the limiting behavior of variance and squared bias as the
training set size grows. Variance is then:
(π(c,x;Ds) −lim
s→∞EDs[π(c,x;Ds)])2
The variance of the model disappears as the training set grows. This is a consequence of the
consistency of the parameter estimates of the model .
For the squared bias term we have:
s→∞EDs[π(c,x;D)] −E[c|x]
When equality holds for the limiting bias term, we say the model is consistent. In general,
when modeling problems involving real world data, logistic regression is not consistent.
This is true, for instance, when the appropriate predictors are missing. In other situations,
all necessary predictors are available, but the probability model governing y given x is not
in the class of distributions that logistic regression can encode.
Mach Learn 68: 235–265
We deﬁne several terms to denote the limiting error of the model:
Residual Bias =
s→∞EDs[π(c,x;D)] −E[c|x]
Residual Error =
E[(1c −E[c|x])2|x,D] + Residual Bias
= Noise + Residual Bias.
This last term consists of the training set-independent error of (18) and the portion of bias
that is training set size independent. For now, we deﬁne our goal in learning as minimizing
squared error. From the various decompositions we see that this is equivalent to minimizing
MSE, and thus both bias and variance. To achieve our goals, we may focus on decreasing
bias, variance or both simultaneously. While estimation of bias may be possible, for instance
following , we leave this subject for future work, and focus on estimation of
variance and its consequences for active learning. When using ﬂexible models with large
numbers of features, variance is often more of a problem then bias.
4.2 A variance estimating technique
The decomposition (20) suggests that minimization of the variance will decrease MSE. Fortunately, statistical theory governing prediction variance provides a convenient mechanism
for estimating variance over a pool of unlabeled data points. Without this theory, a bootstrap
approach to variance estimation might be the only recourse. Minimization of this variance is known in the ﬁeld of optimal design of experiments
as A-optimality, cf. . We derive the requisite theory for multinomial logistic regression below.
Taking two terms of a Taylor expansion of π(c,x,w;D):
π(c,x, ˆw;D) = π(c,x,w) + g(c)( ˆw −w) + O
where w and ˆw are the expected (with respect to D of ﬁxed size) and current estimates of the
parameters, and s is once again the size of the training set. The D parameter disappears from
the ﬁrst term since w is a free parameter in this setting rather than something determined by
a training set D, in contrast to ˆw in previous equations.
The gradient vector g(c) indexed by category/predictor pairs (c′,i) is deﬁned as follows:
π(c,x,w)(1 −π(c,x,w))xi,
−π(c,x,w)π(c′,x,w)xi,
otherwise.
Computing the variance of the Taylor approximation (26) produces:
Var[π(c,x, ˆw)] ≃Var[gn(c)( ˆwc −wc)]
= g(c)′F −1g(c).
Mach Learn 68: 235–265
The asymptotics in (26) and the variance calculation of (29) follow from normality of the
maximum likelihood estimate:
ˆw ∼N(w,F −1).
F is the Fisher information matrix with dimensions (k · d) × (k · d) deﬁned as follows:
F(ci)(c′j)= E(x,y)∼D
i π(c,x,w)π(¬c,x,w) + 1
c = c′ and i = j,
xixjπ(c,x,w)π(¬c,x,w),
c = c′ and i ̸= j,
xixjπ(c,x,w)π(c′,x,w),
One ﬁnal bit of algebra allows more efﬁcient computation of the variance. Deﬁne
An(c) = gn(c)gn(c)′, An = 
c An(c) and A = 
n An, where n indexes individual observations in the pool. With these few tricks, a compact representation of the variance computation follows:
Var[ ˆπ(c|xn)] ≃
gn(c)′F −1gn(c)
tr{gn(c)gn(c)′F −1}
tr{An(c)F −1}
= tr{AF −1}
.= φ(D,A).
Using the variance estimated over the pool is intended to give an estimate of variance over
the actual distribution of observations. As the pool size increases this is a reasonable assumption.
Equation (36) is the A-optimality objective function for multinomial regression with the
A matrix that gives the method its name. Some choose to denote the A matrix A(w) in order
to make explicit the dependence of the matrix on the parameters. However, the φ(D,A)
notation for variance illustrates the dependence on the training set (D) and validation sets
(A), and will be useful in Sect. 4.3. We refer to the method as variance reduction active
learning, noting that the greedy method we will employ in picking examples will not lead
to optimal solutions.
The technique of A-optimality for logistic regression has been developed previously
 in the context of designing location/scale two parameter logistic regression experiments. Such two-parameter experiments
are useful for determining the dosage of a compound that leads to an outcome (e.g. death in
an animal subject) at some probability, for instance 50% of the time. We are not aware of
any previous use of the method in logistic regression models with more than two parameters
or more than two categories. Nor are we aware of evaluations of the method in pool-based
active learning of logistic regression.
4.3 How to pick the next example
Equation (36) shows how to compute the expected variance of a ﬁtted model using a ﬁxed
training set. We now need to derive a quantity that describes the expected beneﬁt of labeling
Mach Learn 68: 235–265
a new observation. The training set D consists of a sequence of observations: {(xn,yn)}N
Using the current estimated model π(y,x, ˆw), the expected beneﬁt of labeling observation
E[Loss] = π(c0,x, ˆw)φ(D ∪(x,c0),A)
+ π(ck,x, ˆw)φ(D ∪(x,ck),A).
Informally, (38) represents the possible changes in φ weighted by current estimates of the
scenario’s likelihood.
Ignoring model-ﬁtting, the worst-case computational cost associated with picking a new
example is:3 O(T NK2(K +D2)+T K3D3), where N is the number of pool examples used
to create the A matrix, T is the number of candidates evaluated for inclusion in the training
set, K is the number of categories and D are the number of predictors in the model. The N
term may be reduced using Monte Carlo sampling from the pool. The term T NK2(K +D2)
corresponds to creation of the A matrix, while the term T K3D3 corresponds to inversion
and multiplication by the F matrix. Model training can be safely ignored from such analysis
when the training set size is small relative to pool size, as is the case in the evaluations of
this study.
4.4 A generalization to other loss functions
Minimizing variance (20) is equivalent to minimizing squared loss:
(pc −qc)2,
with vectors p and q deﬁned with components pc = ED[π(c,xn, ˆw;D)] and qc =
π(c,xn, ˆw;D). The natural next step is to develop a technique applicable to other loss
functions for these values of p and q. Many common loss functions, including both squared
and log loss, have the convenient property that they are twice differentiable and the second
term of their Taylor approximation disappears. The ﬁrst three terms of a Taylor expansion
of this class of loss functions produces an approximation:
L(p,q) ≃L(p,p) + 0 + (p −q)′
∂q2 L(p,q)|q=p
Now, taking the expectation with respect to the training sets of size D (ED) we have:
ED[L(p,q)] ≃L(p,p) + 1
∂q2 L(p,q)|q=p
In the special case of squared loss L(p,q) = 
c(pc −qc)2, the approximation is exact,
and the variance minimization criteria (36) emerges:
ED[L(p,q)] =
Var[qc] = ED[(qc −ED[qc])2].
3We assume naive implementations for the matrix calculations in this analysis.
Mach Learn 68: 235–265
Unfortunately, not all loss functions are amenable to this analysis. For example, 0/1 loss
is not differentiable. Further discussion of this technique can be found in .
4.5 A log loss method of active learning
Applying the Taylor expansion method to log loss we ﬁnd:
pc logpc + 0 +
The ﬁrst term is a constant with respect to training set inputs. The third term is identical
to the variance reduction criteria 36, but with the A matrix reweighted by a factor of
Furthermore, the computational complexity of implementing the log loss procedure remains
identical to that of variance reduction.
As a reminder, the procedure estimates a log loss based on the expected value over training sets of ﬁxed size ED:
ED[π(c,xn, ˆw;D)]log(π(c,xn, ˆw;D))
rather than the correct probability distribution generating categories c given predictors x:
E[yc|x]log(π(c,xn, ˆw;D)).
4.6 Applicability of the approach to conditional exponential models
The method of estimating variance relied on the ability to perform an approximation by
means of Taylor series, compute the variance of the second term, and showing that the
higher order terms vanish. What of the maximum entropy classiﬁer 
and conditional random ﬁeld models ? We expect that the variance
estimation technique will carry over to these more general forms of conditional exponential
models. Demonstrating this result is beyond the scope of the present work.
5 Evaluation
The evaluations in this study have speciﬁc goals: to discover which methods work in addition to why methods perform badly when they do. Towards this end, we assembled a suite
of machine learning data sets consisting of a diverse number of predictors, categories and
domains. In this section, we describe our evaluation methodology, present the most salient
of our results and interpret their meaning. Necessarily, evaluation of the loss function methods require setting the parameters of evaluation in a way to make loss function strategies
computationally tractable. It follows that the heuristics should be evaluated with the same
parameter settings when/if applicable. Surprisingly, the evaluation of heuristic methods of
this section revealed active learning is often worse than random sampling. There is a possibility that these negative results reﬂect a particularly unfortunate evaluation design decision
rather than reﬂecting an underlying systemic problems with the heuristic methods. We systematically modify the evaluation design for evaluating the heuristics in order to rule out
this possibility in Sect. 5.5.
Mach Learn 68: 235–265
5.1 Active learning methods and method-speciﬁc parameter settings
The evaluations consist of seven different methods of pool-based active learning in addition
to two “straw men:” random sampling from the pool as well as random sampling combined
with the bagging procedure. The active learning methods tested include: variance reduction (see (36)), log loss reduction (see (44)), minimum margin sampling and maximum
entropy sampling (Sect. 3.1), QBB-MN and QBB-MM (Sect. 3.2), and classiﬁer certainty
(CC) (Sect. 3.3).
Several of the active learning methods require method-speciﬁc parameter settings. For
example, the variance reduction, log loss reduction and CC methods require a random sample from the pool of some predetermined size to assess expected beneﬁt of example labeling.
In the case of variance reduction and log loss reduction the random sample composes the A
matrix. All evaluations employ a sample size of 300 for assessing beneﬁt of labeling.
The QBB methods, QBB-MN and QBB-MM rely on bagging, and so the evaluation requires a bag size setting. Following , the bag size is 3. Sect. 5.5
explores sensitivity of the results to the choice of 3.
5.2 Evaluation data sets and data set-speciﬁc evaluation parameters
We tested these seven active learning methods on ten data sets (see Table 2 for summary of
data sets). From the UCI machine learning repository of data sets 
we used LetterDB and OptDigits . We used the TIMIT
database to make predictions in a voice recognition domain. Web
pages from the WebKB database provided a document classiﬁcation
task. For additional document classiﬁcation tasks we took the 20 NewsGroups topic disambiguation task , along with two data sets made from
different subsets of the NewsGroups categories. We used three artiﬁcial data sets to explore
the effects of adding different types of noise to data.
Table 2 Descriptions of the data sets used in the evaluation. Included are counts of: the number of categories
(Classes), the number of observations (Obs), the test set size after splitting the data set into pool/test sets
(Test), the number of predictors (Pred), the number of observations in the majority category (Maj), and the
training set stopping point for the evaluation (Stop)
NewsGroups
Mach Learn 68: 235–265
5.2.1 Data set evaluation parameters
Several parameters of the evaluation are intrinsic to the data sets. For instance, how many
random examples should serve as a “seed” set before active learning begins? This section
presents results for seed size 20. Other starting seed sizes are shown in Sect. 5.5 to have
minimal effect.
Another choice is the stopping point for the evaluation. The evaluation uses 300 as a
stopping point except when there is good reason not to. Smaller stopping points are used
for three (of ten) data sets: ArtConf, Comp2a and Comp2b, and the sections on processing
of the individual data sets present the reasons for these decisions. A summary of the actual
stopping points is included in Table 2.
The test set size for each data set is another tunable parameter. The data set is split into
a pool and test set as part of a 10 fold cross validation. In other words this splitting occurs
10 times with ten results averaged into a ﬁnal accuracy. Table 2 shows test set sizes used
for different data sets. As described below, the results are not sensitive to these exact values;
what is important to the qualitative results of this and subsequent sections is that both the
pool and test set are quite large, facilitating hypothesis testing on the averaged results.
5.2.2 Natural data sets
Seven of the evaluation data sets are “natural,” that is they come from some real world
domain rather than an artiﬁcial stochastic generation engine. The data sets are: LetterDB,
OptDigits, TIMIT, NewsGroups, Comp2a, Comp2b, and WebKB. The paragraphs below
describe the sources and pre-processing steps for each of these natural data sets.
The LetterDB database consists of 20,000 instances of uppercase capital letters in a variety of fonts and distortions. The predictors are 16 numerical attributes computed from
statistical moments and edge counts. LetterDB was the most computationally intensive data
set we attempted loss-based active learning on, and evaluations employing seed size 20 took
approximately three weeks to run to completion using ten machines (each machine ran one
tenth of the ten-fold cross-validation). The OptDigits data set consists of 5,620 examples of
handwritten digits from 43 people. The predictors consist of counts of the number of “on
bits” in each of 64 regions.
We processed the WebKB and NewsGroups data set by running a stop word list and
using a count cutoff of 5 or fewer documents. Numbers were converted to a generic
N token. The Comp2a data set consists of the comp.os.ms-windows.misc and
comp.sys.ibm.pc.hardware subset of NewsGroups used previously in an active learning evaluation . The Comp2b data set consists of
comp.graphics and comp.windows.x categories from the same study. We employed
a count cut-off of 2 or fewer documents to trim down the vocabulary for these two binarycategory data sets.
Of the four document classiﬁcation problems only the two binary classiﬁcation problems
proved feasible to test the experimental design approaches due to computational limitations. Implementation tricks included elimination of non-occurring token counts from the
matrix computations of the loss function methods in addition to application of the Sherman-
Morrison formula. Due to computational time costs of the loss function methods, we stopped
training after 150 examples for these two document data sets.
The TIMIT database was formatted into 10,080 points consisting of the ﬁrst 12 Barkscale PLP coefﬁcients (excluding coefﬁcient 0, which usually hurts performance). The
points represent the male speakers from dialect regions 1 through 3. The goal is to predict
which of 20 different vowel sounds are uttered.
Mach Learn 68: 235–265
5.2.3 Artiﬁcial data sets
We constructed three artiﬁcial data sets to explore the effects of two different types of
noise on the modeling performance. The ﬁrst type of noise is the prediction residual error
(see (25)). As a reminder, this is the portion of squared error that is independent of training
set size. The residual error may be estimated when the training set is sufﬁciently large that
the mean squared error (see (19)) becomes negligible.
We explore the effects of increased residual error using two similar artiﬁcial data sets.
The ﬁrst, named Art, consists of 20 categories and 5 predictors with observations generated
according to: xn ∼N(0,I) and wc ∼N(0,5I). Art serves as a noise-free baseline data set.
The second data set, ArtNoisy, is generated similarly except the probabilities are formed
by adding a noise term to the dot product calculation of (5): wc · xn + Gnc, where Gnc ∼
N(0,10). Thus, ArtNoisy models the presence of unknown features that inﬂuence the true
probabilities of an outcome: a form of noise that will increase residual error.
A second type of noise involves different levels of confusion among the categories. For
instance, when categories are related by clusters, we would expect members of the same
cluster to be more difﬁcult to disambiguate than two categories in different clusters. The
NewsGroups data set is an example of a data set with intrinsic category clusters as can be
seen in the list of topics or by clustering the rows of a confusion matrix (see Fig. 1 for list
of topics and result of clustering).
One hypothesis we would like to explore is that heuristics that sample uncertain regions
should fall prey to intrinsically uncertain regions that have little teaching value. We generate
a third data set, ArtConf, consisting of two regions of predictor space and 20 categories
in order to test our ability to construct intrinsically confusable regions. In the ﬁrst region,
predictor no. 1 is set to 1, all remaining 5 predictors are set to 0 and categories 0 or 1 are
assigned with equal probability. Region 1 is the intrinsically uncertain region, and 33% of
the observations inhabit this space. In region 2, predictor no. 1 is set to 0, and the remaining
18 categories generate the remaining 5 predictors according to a multinomial naive Bayes
model . In other words, categories 1 and 2 are intrinsically
hard to disambiguate, but the remaining categories are relatively easy to disambiguate.
The ArtConf data set has the property that learning the generation function takes relatively few examples. This is a byproduct of the simplistic generation process. As a result,
tangible learning improvement disappears by 150 examples. Hypothesis testing results, box
plots and means are reported at a stopping point of 120 observations for this reason.
5.3 Evaluation design
An average of results over 10 random pool/test set splits formed the core of our evaluation
technique. Table 2 indicates the pool and test set sizes; to compute the pool set size, subtract
the test set size from the number of observations in the entire data set. On each of the 10 runs,
the same random seed examples of size 20, 50, 100 or 200 were given to the learners which
proceeded to use their example selecting function to select new examples. Only results for
the seed size 20 are reported; results from alternative starting points look more and more like
random observation sampling as the seed size increases in the sense that any beneﬁt (or loss)
due to active learning diminishes. Results for the alternative starting points are available on
Results are reported once the learner has reached the data set stopping points given in
Table 4. At each iteration of observation selection, 10 candidates were chosen at random
from the pool and the tested method chose the next example from those 10. The number 10
Mach Learn 68: 235–265
Fig. 1 Clusters of topics based on distance measured on confusion matrix rows. The confusion matrix was
computed in this case after training on the entire pool and averaging over 10 pool/test splits
was used because larger numbers cause variance, log loss and CC methods to slow proportionately (see discussions of asymptotics, Sect. 4.3). On the other hand, ﬁxing the sample
size at 10 allows for fair comparison across all methods. Section 5.5 examines the sensitivity
of the heuristic methods’ performance to this parameter.
All evaluations employed a logistic regression using the regularization σ 2
p = 1 for 100
iterations or convergence for the seed set. Once additional data was added, the model parameters were updated 20 iterations or until convergence. In generating results for straw
men bagging and random sampling, the same seed examples are used, and then followed by
additional random sampling to form training sets of appropriate size.
5.4 Evaluation results
Section 5.4 presents several different views of the evaluation results incorporating various
tables and ﬁgures. A guiding principle to keep in mind is that each of these devices present
the same evaluation, but explore different components. For instance, Figures 2–5 present
learning curves for each of the data set in the right column, while the left column shows Box
Mach Learn 68: 235–265
Table 3 Average accuracy and
squared error ((18), left hand
side) results for the tested data
sets when the entire pool is used
as the training set. The data sets
are sorted by squared error as
detailed in Sect. 5.4
Squared error
NewsGroups
plots of the distribution of accuracies at the stopping point (300 observations in most cases).
Table 3 shows the accuracies attainable by training on the entire pool of unlabeled data.
This information gives an understanding of how much continued labeling of training data
can help. The learning curves in Figures 2–5 convey the same information as a horizontal
line on top of the y-axis. Table 4 contains the result of a hypothesis test on the mean stopping
point accuracy: comparing different alternatives to random sampling, while Table 5 presents
the same experiments in terms of the percent of the number of random examples needed to
generate stopping point accuracy of each method.
Table 6 contains the deﬁciency values of the various methods. Deﬁciency is a statistic developed to compare performance of active learning methods globally
across the learning curve. It is computed as
Defn(ACTIVE) =
t=1 Accn(ALG) −Acct(ACTIVE)
t=1 Accn(ALG) −Acct(ALG)
Here, Acct is the accuracy at t training examples averaged over 10 runs. ALG is the learning
algorithm trained on a random samples, and ACTIVE is the active learning variant of the
learning algorithm ALG. In our presentation of results, n refers to the evaluation stopping
points (see Table 2). Random sampling gives deﬁciency 1, with smaller values indicating
that active learning helps performance. Conversely, a larger value indicates a negative result.
Comparisons between Table 4 and Table 5 show that statistically signiﬁcant performance
changes are associated with large savings or loss in the number observations. By corollary
large portions of the learning curve are affected when stopping point accuracies are signiﬁcantly different.
Variance and log loss reduction gave the best results; they provided above-random performance on four of the data sets while never giving less than random performance. The
results do not support any deﬁnitive reason to draw favorites between variance or log loss.
Though not statistically signiﬁcant, the weak performance on the TIMIT data set by variance
reduction suggests favoring log loss.
Maximum entropy sampling results are the worst of all methods tested. In order to assess
what properties of the data sets cause entropy sampling to fail we report the residual error
(see (25)) of each data set after training on the entire pool in Table 3. The data sets sort neatly
by noise, with entropy sampling failing on more noisy data such as TIMIT and performing
at least as well as random for all data sets less noisy than WebKB.
Mach Learn 68: 235–265
Fig. 2 Box plots and learning curves for Art, ArtNoisy and ArtConf data sets. Box plots show the distribution
of the accuracy at the training set stopping point. Conﬁdence bars indicate the variability of competing active
learning schemes
Mach Learn 68: 235–265
Fig. 3 Box plots and learning curves for Comp2a, Comp2b and LetterDB data sets. Box plots show the
distribution of the accuracy at the training set stopping point. Conﬁdence bars indicate the variability of
competing active learning schemes
Mach Learn 68: 235–265
Fig. 4 Box plots and learning curves for NewsGroups, OptDigits and TIMIT data sets. Box plots show
the distribution of the accuracy at the training set stopping point. Conﬁdence bars indicate the variability of
competing active learning schemes
Mach Learn 68: 235–265
Fig. 5 Box plot and learning curves for the WebKB data set. The box plot shows the distribution of the accuracy at the training set stopping point. Conﬁdence bars indicate the variability of competing active learning
Table 4 Results of hypothesis tests comparing bagging and seven active learning method accuracies to random sampling at the ﬁnal training set size. ‘+’ indicates statistically signiﬁcant improvement and ‘–’ indicates
statistically signiﬁcant deterioration. ‘NA’ indicates ‘not applicable.’ Figures 2–5 display the actual results
used for hypothesis testing as a box plot
NewsGroups
NewsGroups
Mach Learn 68: 235–265
Table 5 Results comparing random sampling, bagging, and seven active learning methods reported as the
percentage of random examples over (or under) the ﬁnal training set size needed to give similar accuracies.
Active learning methods were seeded with 20 random examples, and stopped when training set sizes reached
ﬁnal tested size (300 observations with exceptions; see Sect. 5.3 for details on the rationale for different
stopping points)
NewsGroups
NewsGroups
Margin sampling results are quite good except for two notable failures on the ArtConf
and NewsGroups data set. The ArtConf data set was constructed in such a way as to sucker
uncertainty sampling methods into sampling regions with low utility. One hypothesis we had
was that in the NewsGroups data set we would see similar behavior with increased sampling
among more confusable categories, for instance over sampling of computer-related topics.
This did not occur in practice .
Table 7 gives an alternative explanation for margin sampling’s lackluster performance on
the Newsgroups data set; the ability to identify the two categories forming the margin in the
NewsGroups data set is much harder than any of the data sets we tested. This is a problem
speciﬁc to margin sampling in the multi-category active learning, and has not been reported
before. Still, margin sampling provides performance competitive to the alternative heuristics
at the best computational cost.
Before examining the QBB method results it is useful to analyze bagging since it is a
key ingredient. The results for bagging are almost entirely negative, a possibility anticipated
in the bagging literature . Our own results in measuring variance indicate that variance is usually small in comparison to squared error. In contrast,
bagging is known to work well with highly unstable methods such as decision trees, which
are associated with large amounts of variance. We speculate that it would take very many
Mach Learn 68: 235–265
Table 6 Average deﬁciency (see (47)) achieved by the various methods. For each dataset the winner appears
in boldface and marked with a star. The runner-up appears in boldface
NewsGroups
NewsGroups
Table 7 The average percentage
of matching test set margins
when comparing models trained
on data sets of size 300 to a
model trained on the entire pool.
Margins match if they are formed
from the same pair of categories.
Ten repetitions of the experiment
produce the averages below
Correct Margin Percentage
NewsGroups
bag members to improve the variance of the logistic regression model. MacKay 
gives a parametric solution to the problem of variance reduction of logistic regression that
may prove more expedient. With that said, QBB-MM performs slightly better than margin
sampling while QBB-MN results were mixed.
5.5 Alternative evaluation design decisions
The results in Sect, 5.4 suggest that the experimental design methods more reliably match or
beat random performance than the heuristic methods. There is a possibility that the heuristic
Mach Learn 68: 235–265
Table 8 Results of hypothesis tests comparing four heuristic active learning method accuracies to random
sampling at the ﬁnal training set size. These active learners used the larger candidate size of 300. ‘+’ indicates statistically signiﬁcant improvement and ‘–’ indicates statistically signiﬁcant deterioration compared to
random sampling. ‘NA’ indicates ‘not applicable’
NewsGroups
methods are handicapped under certain experimental parameter settings, and so we examine
the alternative settings. Space restrictions prevent displaying the full set of tables and plots
as shown in Sect. 5.4. Instead, a description of the experimental outcome, summary tables
and a couple sample plots will sufﬁce. provides greater details of these experimental results, including learning curves and box plots displaying the actual means used
for hypothesis tests.
1. Larger Candidate Sample Size. The evaluations reported in the previous sections allow
the algorithms to select a single observation from 10 random candidates at each iteration
of Algorithm 1. A potential objection is that the heuristic methods should not be handicapped by such a small candidate sample since the computational cost of considering
additional candidates is small. Changing the number of candidates to 300 has almost no
effect on the outcomes, as demonstrated by Table 8 which shows hypothesis testing results at the data set stopping points. In total, two additional negative results emerge in
this table, along with one new positive result. Table 9 shows the deﬁciency values for this
experiment. Figure 6 shows box plots of the stopping point distributions for two of these
evaluations. For the NewsGroups data set the entropy method is weakened from average stopping accuracy 0.356 to 0.303, demonstrating that increased candidates can harm
performance. A similar degradation is found comparing margin sampling results for the
ArtConf data set; average performance degrades from 0.795 to 0.788. Margin sampling
performance on the NewsGroups data set using larger candidate sizes increased median
performance, however mean performance produced a negative result in the hypothesis
test of Table 8 due to an outlier stopping accuracy of 0.361. Similar behavior with respect to outliers was observed with candidate sample size 10 in the primary evaluation of
Sect. 5.4.
A notable change in Table 8 is with the QBB-MM, where it seems using bagging
perturbs the observation selection enough to remove the negative results of margin sampling. The deﬁciency values of Table 9 suggest that usually when QBB-MM excels, margin sampling does better, and when margin sampling fails, QBB-MM either performs
well or close to random. An exception is the ArtConf data set which happens to be very
easy to learn under random sampling, and here QBB-MM does very well. The increased
sample size can help for the ArtConf data set by increasing the likelihood that something
Mach Learn 68: 235–265
Table 9 Average deﬁciency (see (47)) achieved by the various methods using a larger candidate size of 300.
For each dataset the winner appears in boldface and marked with a star. The runner-up appears in boldface
NewsGroups
Fig. 6 Box plots of stopping point accuracy distributions for ArtConf and NewsGroups data sets when 300
candidates are considered for active learning. Median values are shown as a horizontal line
in the less noisy region will have the smallest margin, and thus be picked for labeling.
From these observations we conclude that adding bagging to margin sampling may help
by dampening extreme behavior and making sampling more random. However, with the
exception of the ArtConf result, there is little to support favoring the larger candidate
sizes for either margin sampling or QBB-MM.
2. Evaluation Starting Points. The evaluations reported in the previous sections begin with
20 random examples. Experiments employing a starting point of 300 and stopping point
of 600 produced no substantial changes in the outcomes. Table 10 shows hypothesis test
evaluations. We could only perform this analysis on the few data sets where learning occurs (e.g. accuracy rates continue to improve) between 300–600 random samples. There
were no additional positive results. Four negative results disappear, and three positive
results disappear compared to Table 4. This experiment suggests that it is harder to generate positive results on more mature portions of the learning curve. Table 11 contains
deﬁciency values for this experiment.
Mach Learn 68: 235–265
Table 10 Results of hypothesis tests comparing bagging and four active learning method accuracies to random sampling at training set size 600. ‘+’ indicates statistically signiﬁcant improvement and ‘–’ indicates
statistically signiﬁcant deterioration. ‘NA’ indicates ‘not applicable’
NewsGroups
Table 11 Average deﬁciency (see (47)) achieved by the various methods beginning at 300 observations and
ending at 600. For each dataset the winner appears in boldface and marked with a star. The runner-up appears
in boldface
NewsGroups
Table 12 Results of hypothesis tests comparing bagging and two query by bagging methods using a bag size
of 15. ‘+’ indicates statistically signiﬁcant improvement and ‘–’ indicates statistically signiﬁcant deterioration. ‘NA’ indicates ‘not applicable’
NewsGroups
3. Larger Bag Sizes for QBB Methods. The evaluations reported in the previous sections
use a bag size of 3. Increasing the bag size to 15 shifts the outcomes of the hypothesis
tests around a little in Table 12, but the overall result is similar to the original results in
Table 4. Table 13 shows the deﬁciency values for this experiment.
In summary, the experiments under alternative evaluation choices conﬁrm the overall
trend of negative results of the previous section, and suggest that the heuristics are prone to
failure in a wide range of settings.
Mach Learn 68: 235–265
Table 13 Average deﬁciency (see (47)) achieved by bagging and the two query by bagging methods using
bag size 15. For each dataset the winner appears in boldface and marked with a star. The runner-up appears
in boldface
NewsGroups
6 Conclusions
The evaluations indicate that experimental design active learning of logistic regression is one
of the more robust strategies available. The experimental design methods produced attractive
results much of the time without ever performing worse than random sampling. This can be
seen by the hypothesis testing results and the deﬁciency measurements in Table 6. Future
work in active learning using logistic regression will beneﬁt from evaluating against these
gold standard methods. Throughout the active learning literature, we found statements to
the effect that these methods are too computationally expensive to evaluate, but our results
demonstrate that experimental design approaches are tractable for many data sets.
The results also expose the weaknesses of many of the active learning algorithms. The
experimental design methods have the disadvantage of memory and computational complexity, and we were unable to evaluate them on two of the larger document classiﬁcation
tasks. All of the heuristic methods fail to beat random sampling on some portion of the evaluation. The result is so surprising that a separate section (5.5) is included to explore whether
negative heuristic performance is an artifact of an “unlucky” evaluation design.
We ﬁnd that most heuristics perform roughly equally well in comparison to each other,
but it is easier to analyze the cause of failure among the simplest heuristics. In the case of
uncertainty sampling using the Shannon entropy measure of uncertainty, bad performance
goes hand in hand with noise, as deﬁned by the portion of squared error that is training set
size independent. For margin sampling, inability to identify the pairs of categories forming
the margin on multi-category problems is the biggest danger, as seen on the NewsGroups
data set. In spite of this observation, margin sampling competes favorably with the alternative heuristics and is the most computationally efﬁcient method examined. Adding bagging (i.e. QBB-MM) improved worst case behavior of margin sampling, and future work
should examine alternative perturbations of the method such as weighted uncertainty sampling . Improving the performance of this method in
the multi-category setting remains a promising direction for future research.
Acknowledgements
Andrew Schein was supported by NSF grant ITR-0205448. The authors thank Andreas Buja, Mark Liberman, Andrew McCallum, Fernando Pereira, and the reviewers for helpful comments.
Mach Learn 68: 235–265