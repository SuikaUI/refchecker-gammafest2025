Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5059–5069
Florence, Italy, July 28 - August 2, 2019. c⃝2019 Association for Computational Linguistics
HIBERT: Document Level Pre-training of Hierarchical Bidirectional
Transformers for Document Summarization
Xingxing Zhang, Furu Wei and Ming Zhou
Microsoft Research Asia, Beijing, China
{xizhang,fuwei,mingzhou}@microsoft.com
Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created
heuristically using rule-based methods. Training the hierarchical encoder with these inaccurate labels is challenging. Inspired by the
recent work on pre-training transformer sentence encoders , we propose HIBERT (as shorthand for HIerachical
Bidirectional Encoder Representations from
Transformers) for document encoding and a
method to pre-train it using unlabeled data. We
apply the pre-trained HIBERT to our summarization model and it outperforms its randomly
initialized counterpart by 1.25 ROUGE on the
CNN/Dailymail dataset and by 2.0 ROUGE
on a version of New York Times dataset. We
also achieve the state-of-the-art performance
on these two datasets.
Introduction
Automatic document summarization is the task of
rewriting a document into its shorter form while
still retaining its important content.
years, many paradigms for document summarization have been explored for an overview). The most popular
two among them are extractive approaches and abstractive approaches. As the name implies, extractive approaches generate summaries by extracting parts of the original document (usually sentences), while abstractive methods may generate
new words or phrases which are not in the original
Extractive summarization is usually modeled
as a sentence ranking problem with length constraints (e.g., max number of words or sentences).
Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features . Based on these sparse
features, sentence are selected using a classiﬁer or
a regression model. Later, the feature engineering
part in this paradigm is replaced with neural networks. Cheng and Lapata propose a hierarchical long short-term memory network to encode a
document and then use another LSTM to predict
binary labels for each sentence in the document.
This architecture is widely adopted recently . Our model also employs a hierarchical document encoder, but we adopt a hierarchical
transformer rather a hierarchical LSTM. Because recent studies show the transformer model performs better than LSTM in many
Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models , where a document is viewed a sequence and its summary is viewed as another sequence.
Although seq2seq based summarizers
can be equipped with copy mechanism , coverage model and reinforcement learning , there is still no guarantee that the generated
summaries are grammatical and convey the same
meaning as the original document does. It seems
that extractive models are more reliable than their
abstractive counterparts.
However, extractive models require sentence
level labels, which are usually not included in
most summarization datasets (most datasets only
contain document-summary pairs). Sentence labels are usually obtained by rule-based methods
(e.g., maximizing the ROUGE score between a set
of sentences and reference summaries) and may
not be accurate. Extractive models proposed re-
cently employ hierarchical document encoders and
even have neural decoders, which are complex.
Training such complex neural models with inaccurate binary labels is challenging. We observed
in our initial experiments on one of our dataset
that our extractive model (see Section 3.3 for details) overﬁts to the training set quickly after the
second epoch, which indicates the training set
may not be fully utilized. Inspired by the recent
pre-training work in natural language processing
 , our solution to this problem is to
ﬁrst pre-train the “complex”’ part (i.e., the hierarchical encoder) of the extractive model on unlabeled data and then we learn to classify sentences
with our model initialized from the pre-trained encoder. In this paper, we propose HIBERT, which
stands for HIerachical Bidirectional Encoder
Representations from Transformers. We design
an unsupervised method to pre-train HIBERT for
document modeling.
We apply the pre-trained
HIBERT to the task of document summarization
and achieve state-of-the-art performance on both
the CNN/Dailymail and New York Times dataset.
Related Work
In this section, we introduce work on extractive
summarization, abstractive summarization and
pre-trained natural language processing models.
For a more comprehensive review of summarization, we refer the interested readers to Nenkova
and McKeown and Mani .
Extractive Summarization
Extractive summarization aims to select important sentences (sometimes other textual units such as elementary discourse units (EDUs)) from a document as its summary. It is usually modeled as a sentence ranking problem by using the scores from classiﬁers
 , sequential labeling models
 as well as integer linear programmers .
Early work with these models above mostly leverage human engineered features such as sentence
position and length , word frequency and event features
 .
As the very successful applications of neural
networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs
and the sequence labeling (or classiﬁcation) model
is replaced with an LSTM decoder .
The architecture is widely adopted in recent neural extractive models and is extended with reinforcement learning , latent variable models ,
joint scoring and iterative document representation .
Recently, transformer networks achieves good performance in machine
translation and a range of
NLP tasks are based on neural
sequence to sequence learning . However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different
from the original and contents can be repeated).
Therefore, copy mechanism , coverage model and reinforcement
learning model optimizing ROUGE are introduced. These problems are alleviated but not solved. There is also an interesting
line of work combining extractive and abstractive
summarization with reinforcement learning , fused attention and bottom-up attention . Our model, which is a very good extractive
model, can be used as the sentence extraction component in these models and potentially improves
their performance.
Pre-trained NLP Models
Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the
surrounding words within a ﬁxed size window to
predict the word in the middle with a log bilinear model. The resulting word embedding table
can be used in other downstream tasks. There are
other word embedding pre-training methods using
similar techniques . Peters et al. and
Radford et al. ﬁnd even a sentence encoder
Figure 1: The architecture of HIBERT during training.
senti is a sentence in the document above, which has
four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3.
(not just word embeddings) can also be pre-trained
with language model objectives (i.e., predicting
the next or previous word). Language model objective is unidirectional, while many tasks can
leverage the context in both directions. Therefore,
Devlin et al. propose the naturally bidirectional masked language model objective (i.e.,
masking several words with a special token in
a sentence and then predicting them).
methods above aim to pre-train word embeddings
or sentence encoders, while our method aims to
pre-train the hierarchical document encoders (i.e.,
hierarchical transformers), which is important in
summarization.
In this section, we present our model HIBERT. We
ﬁrst introduce how documents are represented in
HIBERT. We then describe our method to pre-train
HIBERT and ﬁnally move on to the application of
HIBERT to summarization.
Document Representation
Let D = (S1, S2, . . . , S|D|) denote a document,
where Si = (wi
2, . . . , wi
|Si|) is a sentence in D
j a word in Si. Note that following common
practice in natural language processing literatures,
|Si| is an artiﬁcial EOS (End Of Sentence) token.
To obtain the representation of D, we use two encoders: a sentence encoder to transform each sentence in D to a vector and a document encoder
to learn sentence representations given their surrounding sentences as context. Both the sentence
encoder and document encoder are based on the
Transformer encoder described in Vaswani et al.
As shown in Figure 1, they are nested
in a hierarchical fashion. A transformer encoder
usually has multiple layers and each layer is composed of a multi-head self attentive sub-layer followed by a feed-forward sub-layer with residual
connections and layer normalizations . For more details of the
Transformer encoder, we refer the interested readers to Vaswani et al. . To learn the representation of Si, Si = (wi
2, . . . , wi
|Si|) is ﬁrst
mapped into continuous space
2, . . . , ei
where e(wi
j) and pj are the word and positional
embeddings of wi
j, respectively. The word embedding matrix is randomly initialized and we adopt
the sine-cosine positional embedding 1. Then the sentence encoder (a Transformer) transforms Ei into a list of hidden representations (hi
2, . . . , hi
|Si|). We take the last
hidden representation hi
|Si| (i.e., the representation
at the EOS token) as the representation of sentence
Si. Similar to the representation of each word in
Si, we also take the sentence position into account.
The ﬁnal representation of Si is
Note that words and sentences share the same positional embedding matrix.
In analogy to the sentence encoder, as shown
in Figure 1, the document encoder is yet another
Transformer but applies on the sentence level. After running the Transformer on a sequence of sentence representations (ˆh1, ˆh2, . . . , ˆh|D|), we obtain the context sensitive sentence representations
(d1, d2, . . . , d|D|). Now we have ﬁnished the encoding of a document with a hierarchical bidirectional transformer encoder HIBERT. Note that in
previous work, document representation are also
1We use the sine-cosine embedding because it works well
and do not introduce additional trainable parameters.
learned with hierarchical models, but each hierarchy is a Recurrent Neural Network or Convolutional
Neural Network . We
choose the Transformer because it outperforms
CNN and RNN in machine translation , semantic role labeling and other NLP tasks .
In the next section we will introduce how we train
HIBERT with an unsupervised training objective.
Pre-training
Most recent encoding neural models used in NLP
(e.g., RNNs, CNNs or Transformers) can be pretrained by predicting a word in a sentence (or a
text span) using other words within the same sentence (or span). For example, ELMo and OpenAI-GPT 
predict a word using all words on its left (or right);
while word2vec predicts
one word with its surrounding words in a ﬁxed
window and BERT predicts
(masked) missing words in a sentence given all the
other words.
All the models above learn the representation
of a sentence, where its basic units are words.
HIBERT aims to learn the representation of a document, where its basic units are sentences. Therefore, a natural way of pre-training a document
level model (e.g., HIBERT) is to predict a sentence
(or sentences) instead of a word (or words). We
could predict a sentence in a document with all the
sentences on its left (or right) as in a (document
level) language model. However, in summarization, context on both directions are available. We
therefore opt to predict a sentence using all sentences on both its left and right.
Document Masking
Speciﬁcally, suppose D =
(S1, S2, . . . , S|D|) is a document, where Si =
2, . . . , wi
|Si|) is a sentence in it. We randomly select 15% of the sentences in D and mask
them. Then, we predict these masked sentences.
The prediction task here is similar with the Cloze
task , but the
missing part is a sentence. However, during test
time the input document is not masked, to make
our model can adapt to documents without masks,
we do not always mask the selected sentences.
Once a sentence is selected (as one of the 15%
selected masked sentences), we transform it with
one of three methods below. We will use an example to demonstrate the transformation. For instance, we have the following document and the
second sentence is selected2:
William Shakespeare is a poet .
He died in 1616 .
He is regarded
as the greatest writer .
In 80% of the cases, we mask the selected
sentence (i.e., we replace each word in the sentence with a mask token [MASK]). The document
above becomes William Shakespeare is
[MASK] [MASK] [MASK]
[MASK] [MASK] He is regarded as
the greatest writer .
died in 1616 .
” is masked).
In 10% of the cases, we keep the selected sentence as it is. This strategy is to simulate the input
document during test time (with no masked sentences).
In the rest 10% cases, we replace the selected
sentence with a random sentence.
In this case,
the document after transformation is William
Shakespeare is a poet .
He is regarded as the
greatest writer .
The second sentence
is replaced with “Birds can fly .”
strategy intends to add some noise during training
and make the model more robust.
Sentence Prediction
After the application of
the above procedures to a document D
(S1, S2, . . . , S|D|), we obtain the masked document eD = ( ˜S1, ˜S2, . . . , ˜
S|D|). Let K denote the
set of indicies of selected sentences in D. Now
we are ready to predict the masked sentences
M = {Sk|k ∈K} using eD.
We ﬁrst apply
the hierarchical encoder HIBERT in Section 3.1 to
eD and obtain its context sensitive sentence representations ( ˜d1, ˜d2, . . . , ˜
d|D|). We will demonstrate how we predict the masked sentence Sk =
2, . . . , wk
|Sk|) one word per step (wk
an artiﬁcially added BOS token). At the jth step,
we predict wk
j given wk
0, . . . , wk
j−1 and eD. ˜
dk already encodes the information of eD with a focus
around its kth sentence ˜Sk. As shown in Figure 1,
we employ a Transformer decoder to predict wk
dk as its additional input.
The transformer decoder we used here is slightly
different from the original one. The original decoder employs two multi-head attention layers to
2There might be multiple sentences selected in a document, but in this example there is only one.
include both the context in encoder and decoder,
while we only need one to learn the decoder context, since the context in encoder is a vector (i.e.,
dk). Speciﬁcally, after applying the word and positional embeddings to (wk
0, . . . , wk
j−1), we obtain
1:j−1 = ( ˜ek
0, . . . ,
j−1) (also see Equation 1).
Then we apply multi-head attention sub-layer to
hj−1 = MultiHead(qj−1, Kj−1, Vj−1)
Kj−1 = WK eEk
Kj−1 = WV eEk
where qj−1, Kj−1, Vj−1 are the input query,
key and value matrices of the multi-head attention
function MultiHead(·, ·, ·),
respectively. WQ ∈Rd×d, WK ∈Rd×d and
WV ∈Rd×d are weight matrices.
Then we include the information of eD by addition:
We also follow a feedforward sub-layer activation function) after
xj−1 as in Vaswani et al.
gj−1 = Wff
2 max(0, Wff
xj−1 + b1) + b2 (5)
Note that the transformer decoder can have multiple layers by applying Equation (3) to (5) multiple
times and we only show the computation of one
layer for simplicity.
The probability of wk
j given wk
0, . . . , wk
0:j−1, eD) = softmax(WO
Finally the probability of all masked sentences M
given eD is
p(M| eD) =
0:j−1, eD)
The model above can be trained by minimizing the
negative log-likelihood of all masked sentences
given their paired documents.
We can in theory have unlimited amount of training data for
HIBERT, since they can be generated automatically from (unlabeled) documents. Therefore, we
can ﬁrst train HIBERT on large amount of data and
then apply it to downstream tasks. In the next section, we will introduce its application to document
summarization.
Figure 2: The architecture of our extractive summarization model. The sentence and document level transformers can be pretrained.
Extractive Summarization
Extractive summarization selects the most important sentences in a document as its summary. In
this section, summarization is modeled as a sequence labeling problem.
Speciﬁcally, a document is viewed as a sequence of sentences and
a summarization model is expected to assign a
True or False label for each sentence, where
True means this sentence should be included in
the summary.
In the following, we will introduce the details of our summarization model based
Let D = (S1, S2, . . . , S|D|) denote a document and Y
= (y1, y2, . . . , y|D|) its sentence
labels (methods for obtaining these labels are
in Section 4.1).
As shown in Figure 2, we
ﬁrst apply the hierarchical bidirectional transformer encoder HIBERT to D and yields the context dependent representations for all sentences
(d1, d2, . . . , d|D|). The probability of the label of
Si can be estimated using an additional linear projection and a softmax:
p(yi|D) = softmax(WS di)
where WS ∈R2×d. The summarization model
can be trained by minimizing the negative loglikelihood of all sentence labels given their paired
documents.
Experiments
In this section we assess the performance of our
model on the document summarization task. We
ﬁrst introduce the dataset we used for pre-training
and the summarization task and give implementation details of our model. We also compare our
model against multiple previous models.
We conducted our summarization experiments
on the non-anonymous version CNN/Dailymail
(CNNDM) dataset , and the New York Times dataset
 . For
the CNNDM dataset, we preprocessed the dataset
using the scripts from the authors of See et al.
 3. The resulting dataset contains 287,226
documents with summaries for training, 13,368
for validation and 11,490 for test. Following , we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words
from New York Times dataset. We used the same
training/validation/test splits as in Xu and Durrett , which contain 137,778 documents for
training, 17,222 for validation and 17,223 for test.
To create sentence level labels for extractive summarization, we used a strategy similar to Nallapati
et al. . We label the subset of sentences in
a document that maximizes ROUGE 
(against the human summary) as True and all
other sentences as False.
unsupervisedly
model HIBERT (see Section 3.2 for details), we
created the GIGA-CM dataset (totally 6,626,842
documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of
the CNNDM dataset. We used the validation set
of CNNDM as the validation set of GIGA-CM
as well. As in See et al. , documents and
summaries in CNNDM, NYT50 and GIGA-CM
are all segmented and tokenized using Stanford
CoreNLP toolkit . To reduce the vocabulary size, we applied byte pair encoding to all of our
datasets. To limit the memory consumption during training, we limit the length of each sentence
to be 50 words (51th word and onwards are removed) and split documents with more than 30
sentences into smaller documents with each containing at most 30 sentences.
3Scripts publicly available at 
abisee/cnn-dailymail
4 
Implementation Details
Our model is trained in three stages, which includes two pre-training stages and one ﬁnetuning
The ﬁrst stage is the open-domain pretraining and in this stage we train HIBERT with the
pre-training objective (Section 3.2) on GIGA-CM
dataset. In the second stage, we perform the indomain pre-training on the CNNDM (or NYT50)
dataset still with the same pre-training objective.
In the ﬁnal stage, we ﬁnetune HIBERT in the summarization model (Section 3.3) to predict extractive sentence labels on CNNDM (or NYT50).
The sizes of the sentence and document level
Transformers as well as the Transformer decoder
in HIBERT are the same. Let L denote the number of layers in Transformer, H the hidden size
and A the number of attention heads.
 , the hidden size of the feedforward sublayer is 4H. We
mainly trained two model sizes: HIBERTS (L = 6,
H = 512 and A = 8) and HIBERTM (L = 6,
H = 768 and A = 12). We trained both HIBERTS
and HIBERTM on a single machine with 8 Nvidia
Tesla V100 GPUs with a batch size of 256 documents. We optimized our models using Adam
with learning rate of 1e-4, β1 = 0.9, β2 = 0.999,
L2 norm of 0.01, learning rate warmup 10,000
steps and learning rate decay afterwards using the
strategies in Vaswani et al. . The dropout
rate in all layers are 0.1. In pre-training stages,
we trained our models until validation perplexities
do not decrease signiﬁcantly (around 45 epochs on
GIGA-CM dataset and 100 to 200 epochs on CN-
NDM and NYT50). Training HIBERTM for one
epoch on GIGA-CM dataset takes approximately
Our models during ﬁne-tuning stage can be
trained on a single GPU. The hyper-parameters are
almost identical to these in the pre-training stages
except that the learning rate is 5e-5, the batch size
is 32, the warmup steps are 4,000 and we train our
models for 5 epochs. During inference, we rank
sentences using p(yi|D) (Equation (8)) and choose
the top K sentences as summary, where K is tuned
on the validation set.
Evaluations
We evaluated the quality of summaries from different systems automatically using ROUGE (Lin,
We reported the full length F1 based
ROUGE-1, ROUGE-2 and ROUGE-L on the
Pointer+Coverage
Abstract-ML+RL
SentRewrite
InconsisLoss
SummaRuNNer
NeuSum-MMR
HierTransformer
HIBERTS (in-domain) 42.10
Table 1: Results of various models on the CNNDM test
set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R-
2), and ROUGE-L (R-L).
CNNDM and NYT50 datasets.
We compute
ROUGE scores using the ROUGE-1.5.5.pl
Additionally, we also evaluated the generated
summaries by eliciting human judgments.
Following , we randomly sampled 20 documents from
the CNNDM test set. Participants were presented
with a document and a list of summaries produced
by different systems. We asked subjects to rank
these summaries (ties allowed) by taking informativeness (is the summary capture the important information from the document?) and ﬂuency (is the
summary grammatical?) into account. Each document is annotated by three different subjects.
Our main results on the CNNDM dataset are
shown in Table 1, with abstractive models in
the top block and extractive models in the bottom block. Pointer+Coverage ,
Abstract-ML+RL and DCA
 are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep
communicating agents extensions.
SentRewrite
 and InconsisLoss all try to decompose the word by
word summary generation into sentence selection
from document and “sentence” level summarization (or compression).
Bottom-Up generates summaries by combines a
word prediction model with the decoder attention
model. The extractive models are usually based
on hierarchical encoders . They have been extended with reinforcement learning , Maximal Marginal
Relevance , latent variable modeling and syntactic compression .
Lead3 is a baseline which simply selects the ﬁrst three sentences. Our model
HIBERTS (in-domain), which only use one pretraining stage on the in-domain CNNDM training
set, outperforms all of them and differences between them are all signiﬁcant with a 0.95 conﬁdence interval (estimated with the ROUGE script).
Note that pre-training HIBERTS (in-domain) is
very fast and it only takes around 30 minutes
for one epoch on the CNNDM training set. Our
models with two pre-training stages (HIBERTS) or
larger size (HIBERTM) perform even better and
HIBERTM outperforms BERT by 0.5 ROUGE5.
We also implemented two baselines.
the hierarchical transformer summarization model
(HeriTransfomer; described in 3.3) without pretraining. Note the setting for HeriTransfomer is
(L = 4,H = 300 and A = 4) 6. We can see
that the pre-training (details in Section 3.2) leads
to a +1.25 ROUGE improvement. Another baseline is based on a pre-trained BERT 7 and ﬁnetuned on the CNNDM dataset. We
used the BERTbase model because our 16G RAM
V100 GPU cannot ﬁt BERTlarge for the summarization task even with batch size of 1. The positional embedding of BERT supports input length
up to 512 words, we therefore split documents
with more than 10 sentences into multiple blocks
5The difference is signiﬁcant according to the ROUGE
6We tried deeper and larger models, but obtained inferior
results, which may indicates training large or deep models on
this dataset without a good initialization is challenging.
7Our BERT baseline is adapted from this implementation
 
pytorch-pretrained-BERT
EXTRACTION
HeriTransformer
HIBERTS (in-domain)
HIBERTM (in-domain) 49.06
Table 2: Results of various models on the NYT50
test set using full-length F1 ROUGE. HIBERTS (indomain) and HIBERTM (in-domain) only uses one pretraining stage on the NYT50 training set.
Pretraining Strategies
Open-Domain
Open+In-Domain
Table 3: Results of summarization model (HIBERTS
setting) with different pre-training strategies on the
CNNDM validation set using full-length F1 ROUGE.
(each block with 10 sentences8). We feed each
block (the BOS and EOS tokens of each sentence
are replaced with [CLS] and [SEP] tokens) into
BERT and use the representation at [CLS] token
to classify each sentence. Our model HIBERTS
outperforms BERT by 0.4 to 0.5 ROUGE despite
with only half the number of model parameters
(HIBERTS 54.6M v.s. BERT 110M).
Results on the NYT50 dataset show the similar
trends (see Table 2). EXTRACTION is a extractive model based hierarchical LSTM and we use
the numbers reported by Xu and Durrett .
The improvement of HIBERTM over the baseline
without pre-training (HeriTransformer) becomes
2.0 ROUGE. HIBERTS (in-domain), HIBERTM
(in-domain), HIBERTS and HIBERTM all outperform BERT signiﬁcantly according to the ROUGE
We also conducted human experiment with 20
randomly sampled documents from the CNNDM
We compared our model HIBERTM
against Lead3, DCA, Latent, BERT and the human
reference (Human)9. We asked the subjects to rank
8We use 10 sentences per block, because maximum sentence length 50 × 10 < 512 (maximum BERT supported
length). The last block of a document may have less than 10
sentences.
9We obtained the outputs of DCA via emails.
0.03 0.18 0.15 0.30 0.30 0.03
0.08 0.15 0.18 0.20 0.15 0.23
0.05 0.33 0.28 0.20 0.13 0.00
0.13 0.37 0.32 0.15 0.03 0.00
0.30 0.35 0.25 0.10 0.00 0.00
0.58 0.15 0.20 0.00 0.03 0.03
Table 4: Human evaluation: proportions of rankings
and mean ranks (MeanR; lower is better) of various
the outputs of these systems from best to worst.
As shown in Table 4, the output of HIBERTM is
selected as the best in 30% of cases and we obtained lower mean rank than all systems except for
Human. We also converted the rank numbers into
ratings (rank i to 7 −i) and applied student t-test
on the ratings. HIBERTM is signiﬁcantly different
from all systems in comparison (p < 0.05), which
indicates our model still lags behind Human, but
is better than all other systems.
Pre-training Strategies
As mentioned earlier,
our pre-training includes two stages.
stage is the open-domain pre-training stage on
the GIGA-CM dataset and the following stage
is the in-domain pre-training on the CNNDM
(or NYT50) dataset.
As shown in Table 3,
we pretrained HIBERTS using only open-domain
stage (Open-Domain), only in-domain stage (In-
Domain) or both stages (Open+In-Domain) and
applied it to the CNNDM summarization task. Results on the validation set of CNNDM indicate the
two-stage pre-training process is necessary.
Conclusions
The core part of a neural extractive summarization model is the hierarchical document encoder.
We proposed a method to pre-train document level
hierarchical bidirectional transformer encoders on
unlabeled data. When we only pre-train hierarchical transformers on the training sets of summarization datasets with our proposed objective, application of the pre-trained hierarchical transformers to extractive summarization models already
leads to wide improvement of summarization performance. Adding the large open-domain dataset
to pre-training leads to even better performance.
In the future, we plan to apply models to other
tasks that also require hierarchical document encodings (e.g., document question answering). We
are also interested in improving the architectures
of hierarchical document encoders and designing
other objectives to train hierarchical transformers.
Acknowledgments
We would like to thank Nan Yang, Houwen Peng,
Li Dong and the ACL reviewers for their valuable feedback. We are grateful to Jiacheng Xu and
Greg Durrett for sharing their splits of the New
York Times dataset with us.