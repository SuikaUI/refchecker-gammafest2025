HAL Id: hal-02370192
 
 
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Dense Classification and Implanting for Few-Shot
Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, Andrei Bursuc
To cite this version:
Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, Andrei Bursuc. Dense Classification and Implanting
for Few-Shot Learning. 2019. ￿hal-02370192￿
Dense Classiﬁcation and Implanting for Few-Shot Learning
Yann Lifchitz1,2
Yannis Avrithis1
Sylvaine Picard2
Andrei Bursuc3
1Univ Rennes, Inria, CNRS, IRISA
Training deep neural networks from few examples is a
highly challenging and key problem for many computer vision tasks.
In this context, we are targeting knowledge
transfer from a set with abundant data to other sets with few
available examples. We propose two simple and effective
solutions: (i) dense classiﬁcation over feature maps, which
for the ﬁrst time studies local activations in the domain of
few-shot learning, and (ii) implanting, that is, attaching new
neurons to a previously trained network to learn new, taskspeciﬁc features. On miniImageNet, we improve the prior
state-of-the-art on few-shot classiﬁcation, i.e., we achieve
62.5%, 79.8% and 83.8% on 5-way 1-shot, 5-shot and 10shot settings respectively.
1. Introduction
Current state of the art on image classiﬁcation , object detection , semantic segmentation , and practically most tasks with some degree
of learning involved, rely on deep neural networks. Those
are powerful high-capacity models with trainable parameters ranging from millions to tens of millions, which require
vast amounts of annotated data to ﬁt. When such data is
plentiful, supervised learning is the solution of choice.
Tasks and classes with limited available data, i.e. from
the long-tail , are highly problematic for this type of approaches. The performance of deep neural networks poses
several challenges in the low-data regime, in particular in
terms of overﬁtting and generalization. The subject of fewshot learning is to learn to recognize previously unseen
classes with very few annotated examples. This is not a
new problem , yet there is a recent resurgence in interest
through meta-learning inspired by early
work in learning-to-learn .
In meta-learning settings, even when there is single large
training set with a ﬁxed number of class, it is treated as a
collection of datasets of different classes, where each class
has a few annotated examples. This is done so that both
meta-learning and meta-testing are performed in a similar
manner . However this choice does not always
come with best performance. We argue that a simple conventional pipeline using all available classes and data with
a parametric classiﬁer is effective and appealing.
Most few-shot learning approaches do not deal explicitly
with spatial information since feature maps are usually ﬂattened or pooled before the classiﬁcation layer. We show that
performing a dense classiﬁcation over feature maps leads to
more precise classiﬁcation and consistently improves performance on standard benchmarks.
While incremental learning touches similar aspects with
few-shot learning by learning to adapt to new tasks using
the same network or extending an existing network
with new layers and parameters for each new task , few
of these ideas have been adopted in few shot learning. The
main impediment is the reduced number of training examples which make it difﬁcult to properly deﬁne a new task.
We propose a solution for leveraging incremental learning
ideas for few-shot learning.
Contributions: We present the following contributions.
First, we propose a simple extension for few-shot learning pipelines consisting of dense classiﬁcation over feature
maps. Through localized supervision, it enables reaping additional knowledge from the limited training data. Second,
we introduce neural implants, which are layers attached to
an already trained network, enabling it to quickly adapt to
new tasks with few examples. Both are easy to implement
and show consistent performance gains.
2. Problem formulation and background
Problem formulation. We are given a collection of training examples X := (x1, . . . , xn) with each xi ∈X, and
corresponding labels Y := (y1, . . . , yn) with each yi ∈C,
where C := [c]1 is a set of base classes. On this training
data we are allowed to learn a representation of the domain
X such that we can solve new tasks. This representation
learning we shall call stage 1.
In few-shot learning, one new task is that we are given a
collection of few support examples X′ := (x′
1, . . . , x′
with each x′
X, and corresponding labels Y ′
1, . . . , y′
n′) with each y′
i ∈C′, where C′ := [c′] is a set
of novel classes disjoint from C and n′ ≪n; with this new
1We use the notation [i] := {1, . . . , i} for i ∈N.
 
data, the objective is to learn a classiﬁer that maps a new
query example from X to a label prediction in C′. The latter classiﬁer learning, which does not exclude continuing
the representation learning, we shall call stage 2.
Classiﬁcation is called c′-way where c′ is the number of
novel classes; in case there is a ﬁxed number k of support
examples per novel class, it is called k-shot. As in standard classiﬁcation, there is typically a collection of queries
for evaluation of each task. Few-shot learning is typically
evaluated on a large number of new tasks, with queries and
support examples randomly sampled from (X′, Y ′).
Network model. We consider a model that is conceptually
composed of two parts: an embedding network and a classi-
ﬁer. The embedding network φθ : X →Rr×d maps the input to an embedding, where θ denotes its parameters. Since
we shall be studying the spatial properties of the input, the
embedding is not a vector but rather a tensor, where r represents the spatial dimensions and d the feature dimensions.
For a 2d input image and a convolutional network for instance, the embedding is a 3d tensor in Rw×h×d taken as the
activation of the last convolutional layer, where r = w × h
is its spatial resolution. The embedding can still be a vector
in the special case r = 1.
The classiﬁer network can be of any form and depends
on the particular model, but it is applied on top of φθ and
its output represents conﬁdence over c (resp. c′) base (resp.
novel) classes. If we denote by fθ : X →Rc (resp. Rc′)
the network function mapping the input to class conﬁdence,
then a prediction for input x ∈X is made by assigning the
label of maximum conﬁdence, arg maxi f i
Prototypical networks. Snell et al. introduce a simple
classiﬁer for novel classes that computes a single prototype
per class and then classiﬁes a query to the nearest prototype.
More formally, given X′, Y ′ and an index set S ⊂N ′ :=
[n′], let the set Sj := {i ∈S : y′
i = j} index the support
examples in S labeled in class j. The prototype of class j is
given by the average of those examples
for j ∈C′. Then, the network function is deﬁned as3
fθ[P](x) := σ
[s(φθ(x), pj)]c′
for x ∈X, where P := (p1, . . . , pc′) and s is a similarity
function that may be cosine similarity or negative squared
Euclidean distance and σ : Rm →Rm is the softmax func-
2Given vector x ∈Rm, xi denotes the i-th element of x. Similarly for
f : A →Rm, fi(a) denotes the i-the element of f(a) for a ∈A.
3We deﬁne [e(i)]n
i=1 := (e(1), . . . , e(n)) for n ∈N and any expression e(i) of variable i ∈N.
tion deﬁned by
i=1 exp(xi)
for x ∈Rm and m ∈N.
Given a new task with support data (X′, Y ′) over novel
classes C′ (stage 2), the full index set N ′ is used and computing class prototypes (1) is the only learning to be done.
When learning from the training data (X, Y ) over base
classes C (stage 1), a number of ﬁctitious tasks called
episodes are generated by randomly sampling a number
classes from C and then a number of examples in each class
from X with their labels from Y ; these collections, denoted
as X′, Y ′ respectively and of length n′, are supposed to be
support examples and queries of novel classes C′, where
labels are now available for the queries and the objective
is that queries are classiﬁed correctly. The set N ′ := [n′]
is partitioned into a support set S ⊂N ′ and a query set
Q := N ′ \ S. Class prototypes P are computed on index
set S according to (1) and the network function fθ is deﬁned
on these prototypes by (2). The network is then trained by
minimizing over θ the cost function
J(X′, Y ′; θ) :=
ℓ(fθ[P](x′
on the query set Q, where ℓis the cross-entropy loss
ℓ(a, y) := −log ay
for a ∈Rm, y ∈[m] and m ∈N.
Learning with imprinted weights. Qi et al. follow a
simpler approach when learning on the training data (X, Y )
over base classes C (stage 1). In particular, they use a fullyconnected layer without bias as a parametric linear classi-
ﬁer on top of the embedding function φθ followed by softmax and they train in a standard supervised classiﬁcation
setting. More formally, let wj ∈Rr×d be the weight parameter of class j for j ∈C. Then, similarly to (2), the
network function is deﬁned by
fθ,W (x) := σ
 [sτ(φθ(x), wj)]c
for x ∈X, where W := (w1, . . . , wc) is the collection of
class weights and sτ is the scaled cosine similarity
sτ(x, y) := τ ⟨ˆx, ˆy⟩
for x, y ∈Rr×d; ˆx := x/ ∥x∥is the ℓ2-normalized counterpart of x for x ∈Rr×d; ⟨·, ·⟩and ∥·∥denote Frobenius
inner product and norm respectively; and τ ∈R+ is a trainable scale parameter. Then, training amounts to minimizing
over θ, W the cost function
J(X, Y ; θ, W) :=
ℓ(fθ,W (xi), yi).
Given a new task with support data (X′, Y ′) over novel
classes C′ (stage 2), class prototypes P are computed on N ′
according to (1) and they are imprinted in the classiﬁer, that
is, W is replaced by W ′ := (W, P). The network can now
make predictions on n + n′ base and novel classes. The
network is then ﬁne-tuned based on (8), which aligns the
class weights W with the prototypes P at the cost of having
to store and re-train on the entire training data (X, Y ).
Few-shot learning without forgetting. Gidaris and Komodakis , concurrently with , develop a similar
model that is able to classify examples of both base and
novel classes. The main difference to is that only the
weight parameters of the base classes are stored and not the
entire training data. They use the same parametric linear
classiﬁer as in both stages, and they also use episodestyle training like in stage 2.
Given training data of base classes (stage 1), we use a
parametric classiﬁer like , which however applies at
all spatial locations rather than following ﬂattening or pooling; a very simple idea that we call dense classiﬁcation and
discuss in §3.1. Given support data of novel classes (stage
2), we learn in episodes as in prototypical networks ,
but on the true task. As discussed in §3.2, the embedding
network learned in stage 1 remains ﬁxed but new layers
called implants are trained to learn task-speciﬁc features.
Finally, §3.3 discusses inference of novel class queries.
3.1. Dense classiﬁcation
As discussed in §2, the embedding network φθ : X →
Rr×d maps the input to an embedding that is a tensor. There
are two common ways of handling this high-dimensional
representation, as illustrated in Figure 1.
The ﬁrst is to apply one or more fully connected layers,
for instance in networks C64F, C128F in few-shot learning . This can be seen as ﬂattening the activation
into a long vector and multiplying with a weight vector of
the same length per class; alternatively, the weight parameter is a tensor of the same dimension as the embedding.
This representation is discriminative, but not invariant.
The second way is to apply global pooling and reduce the
embedding into a smaller vector of length d, for instance in
small ResNet architectures used more recently in few-shot
learning . This reduces dimensionality signiﬁcantly, so it makes sense if d is large enough. It is an invariant representation, but less discriminative.
In this work we follow a different approach that we
call dense classiﬁcation and is illustrated in Figure 2.
We view the embedding φθ(x) as a collection of vectors
[φ(k)(x)]r
k=1, where φ(k)(x) ∈Rd for k ∈[r]4. For a 2d
4Given tensor a ∈Rm×n, denote by a(k) the k-th n-dimensional
feature (d)
spatial (r)
class weights
feature (d)
spatial (r)
class weights
Figure 1. Flattening and pooling. Horizontal (vertical) axis represents feature (spatial) dimensions. Tensors w1, w2, w3 represent
class weights, and φ(x) the embedding of example x. An embedding is compared to class weights by similarity (s) and then softmax (σ) and cross-entropy (ℓ) follow. (a) Flattening is equivalent
to class weights having the same r × d shape as φ(x). (b) Global
pooling. Embedding φ(x) is pooled (Σ) into vector a ∈Rd before
being compared to class weights, which are in Rd too.
image input and a convolutional network, φθ(x) consists of
the activations of the last convolutional layer, that is a tensor
in Rw×h×d where r = w × h is its spatial resolution. Then,
φ(k)(x) is an embedding in Rd that represents a single spatial location k on the tensor.
When learning from the training data (X, Y ) over base
classes C (stage 1), we adopt the simple approach of training a parametric linear classiﬁer on top of the embedding
function φθ, like and the initial training of . The
main difference in our case is that the weight parameters
do not have the same dimensions as φθ(x); they are rather
vectors in Rd and they are shared over all spatial locations.
More formally, let wj ∈Rd be the weight parameter of
class j for j ∈C. Then, similarly to (6), the classiﬁer mapping fθ,W : X →Rr×c is deﬁned by
fθ,W (x) :=
θ (x), wj)]c
for x ∈X, where W := (w1, . . . , wc) is the collection of
class weights and sτ is the scaled cosine similarity deﬁned
by (7), with τ being a learnable parameter as in 5.
slice along the ﬁrst group of dimensions for k ∈[m].
5Temperature scaling is frequently encountered in various formats in
feature (d)
spatial (r)
class weights
Figure 2. Dense classiﬁcation. Notation is the same as in Figure 1. The embedding a := φ(x) ∈Rr×d is seen as a collection of vectors
(a(1), . . . , a(r)) in Rd (here r = 3) with each being a vector in Rd and representing a region of the input image. Each vector is compared
independently to the same class weights and the losses are added, encouraging all regions to be correctly classiﬁed.
Figure 3. Examples overlaid with correct class activation
maps (red is high activation for ground truth) on Resnet-12
(cf. §5) trained with global average pooling or dense classiﬁcation
(cf. (9)). From top to bottom: base classes, classiﬁed correctly by
both (walker hound, tile roof); novel classes, classiﬁed correctly
by both (king crab, ant); novel classes, dense classiﬁcation is better (ferret, electric guitar); novel classes, pooling is better (mixing
bowl, ant). In all cases, dense classiﬁcation results in smoother
activation maps that are more aligned with objects.
Here fθ,W (x) is a r × c tensor: index k ranges over spatial
resolution [r] and j over classes [c].
This operation is a 1 × 1 convolution followed by depthwise softmax. Then, f (k)
θ,W (x) at spatial location k is a vector in Rc representing conﬁdence over the c classes. On the
other hand, f (:,j)
θ,W (x) is a vector in Rr representing conﬁdence of class j for j ∈[c] as a function of spatial location.6
For a 2d image input, f (:,j)
θ,W (x) is like a class activation map
several works to enable soft-labeling or to improve cosine similarity
in the ﬁnal layer .
6Given tensor a ∈Rm×n, denote by a(:,j) the j-th m-dimensional
slice along the second group of dimensions for j ∈[n].
(CAM) for class j, that is a 2d map roughly localizing
the response to class j, but differs in that softmax suppresses
all but the strongest responses at each location.
Given the deﬁnition (9) of fθ,W , training amounts to
minimizing over θ, W the cost function
J(X, Y ; θ, W) :=
θ,W (xi), yi),
where ℓis cross-entropy (5). The loss function applies to all
spatial locations and therefore the classiﬁer is encouraged
to make correct predictions everywhere.
Learning a new task with support data (X′, Y ′) over
novel classes C′ (stage 2) and inference are discussed
in §3.2.2 and §3.3 respectively.
Discussion. The same situation arises in semantic segmentation , where given per-pixel labels, the loss function applies per pixel and the network learns to make localized predictions on upsampled feature maps rather than just
classify. In our case there is just one image-level label and
the low resolution, e.g. 5 × 5, of few-shot learning settings
allows us to assume that the label applies to all locations
due to large receptive ﬁeld.
Dense classiﬁcation improves the spatial distribution of
class activations, as shown in Figure 3. By encouraging all
spatial locations to be classiﬁed correctly, we are encouraging the embedding network to identify all parts of the object
of interest rather than just the most discriminative details.
Since each location on a feature map corresponds to a region in the image where only part of the object may be visible, our model behaves like implicit data augmentation of
exhaustive shifts and crops over a dense grid with a single
forward pass of each example in the network.
3.2. Implanting
From the learning on the training data (X, Y ) of base
classes C (stage 1) we only keep the embedding network
φθ and we discard the classiﬁcation layer. The assumption
is that features learned on base classes are generic enough to
be used for other classes, at least for the bottom layers .
However, given a new few-shot task on novel classes C′
(stage 2), we argue that we can take advantage of the sup-
Forward all classes
Forward base classes
Forward novel classes
Backprop novel classes
Backprop base classes
Figure 4. Neural implants for CNNs. The implants are convolutional ﬁlters operating in a new processing stream parallel to the base
network. The input of an implant is the depth-wise concatenation of hidden states from both streams. When training neural implants,
previously trained parameters are frozen. Purple and black arrows correspond to stage 1 ﬂows; red and black to stage 2.
port data (X′, Y ′) to ﬁnd new features that are discriminative for the task at hand, at least in the top layers.
Architecture
We begin with the embedding network φθ, which we call
base network. We widen this network by adding new convolution kernels in a number of its top convolutional layers.
We call these new neurons implants. While learning the implants, we keep the base network parameters frozen, which
preserves the representation of the base classes.
Let al denote the output activation of the convolutional
layer l in the base network. The implant for this layer, if it
exists, is a distinct convolutional layer with output activation a′
l. Then the input of an implant at the next layer l + 1
is the depth-wise concatenation [al, a′
l exists, and just
al otherwise. If θ′
l are the parameters of the l-th implant,
then we denote by θ′ := (θ′
l0, . . . , θ′
L) the set of all new
parameters, where l0 is the ﬁrst layer with an implant and
L the network depth. The widened embedding network is
denoted by φθ,θ′.
As illustrated in Figure 4, we are creating a new stream
of data in parallel to the base network. The implant stream
is connected to the base stream at multiple top layers and
leverages the previously learned features by learning additional connections for the new tasks.
Why implanting? In several few-shot learning works, in
particular metric learning, it is common to focus on the top
layer of the network and learn or generate a new classiﬁer
for the novel classes. The reason behind this choice underpins a major challenge in few-shot learning: deep neural networks are prone to overﬁtting. With implanting, we
attempt to diminish this risk by adding a limited amount
of new parameters, while preserving the previously trained
ones intact. Useful visual representations and parameters
learned from base classes can be quickly squashed during ﬁne-tuning on the novel classes.
With implants, we
freeze them and train only the new neurons added to the
network, maximizing the contribution of the knowledge acquired from base classes.
To learn the implants only makes sense when a new task
is given with support data (X′, Y ′) over novel classes C′
(stage 2). Here we use an approach similar to prototypical
networks in the sense that we generate a number of
ﬁctitious subtasks of the new task, the main difference being
that we are now working on the novel classes.
We choose the simple approach of using each one of the
given examples alone as a query in one subtask while all the
rest are used as support examples. This involves no sampling and the process is deterministic. Because only one
example is missing from the true support examples, each
subtask approximates the true task very well.
In particular, for each i ∈N ′ := [n′], we deﬁne a query
set Qi := {i} and a support set Si := N ′ \ Qi. We compute class prototypes Pi on index set Si according to (1),
where we replace φθ by φθ,θ′ and θ′ are the implanted parameters. We deﬁne the widened network function fθ,θ′[Pi]
on these prototypes by (2) with a similar replacement. We
then freeze the base network parameters θ and train the implants θ′ by minimizing a cost function like (4). Similarly
to (4) and taking all subtasks into account, the overall cost
function we are minimizing over θ′ is given by
J(X′, Y ′; θ, θ′) :=
ℓ(fθ,θ′[Pi](x′
where ℓis cross-entropy (5).
In (11), activations are assumed ﬂattened or globally
pooled. Alternatively, we can densely classify them and apply the loss function to all spatial locations independently.
Combining with (10), the cost function in this case is
J(X′, Y ′; θ, θ′) :=
θ,θ′[Pi](x′
Prototypes in (11) or (12) are recomputed at each iteration
based on the current version of implants. Note that this
training setup does not apply to the 1-shot scenario as it
requires at least two support samples per class.
3.3. Inference on novel classes
Inference is the same whether the embedding network
has been implanted or not. Here we adopt the prototypical
network model too. What we have found to work best is
to perform global pooling of the embeddings of the support
examples and compute class prototypes P := (p1, . . . , pc′)
by (1). Given a query x ∈X, the standard prediction is then
to assign it to the nearest prototype
j∈C′ s(φθ,θ′(x), pj),
where s is cosine similarity .
Alternatively, we can
densely classify the embedding φθ,θ′(x), soft-assigning independently the embedding φ(k)
θ,θ′(x) of each spatial location, then average over all locations k ∈[r] according to
fθ,θ′[P](x) := 1
θ,θ′(x), pj)]c′
where sτ is the scaled cosine similarity (7), and ﬁnally classify to arg maxj∈C′ f j
θ,θ′[P](x).
4. Related work
Metric learning is common in few-shot learning. Multiple
improvements of the standard softmax and cross-entropy
loss are proposed by to this end. Traditional methods like siamese networks are also considered along with models that learn by comparing
multiple samples at once . Learning to generate
new samples is another direction. Our solution is related
to prototypical networks and matching networks 
but we rather use a parametric classiﬁer.
Meta-learning is the basis of a large portion of the few-shot
learning literature. Recent approaches can be roughly classiﬁed as: optimization-based methods, that learn to initialize the parameters of a learner such that it becomes faster
to ﬁne-tune ; memory-based methods leveraging memory modules to store training samples or to encode
adaptation algorithms ; data generation methods
that learn to generate new samples ; parameter generating methods that learn to generate the weights of a classiﬁer or the parameters of a network with multiple
layers . The motivation behind the latter is that
it should be easier to generate new parameters rather than
to ﬁne-tune a large network or to train a new classiﬁer from
scratch. By generating a single linear layer at the end of the
network , one neglects useful coarse visual information found in intermediate layers. We plug our neural
54.28 ±0.18
71.60 ±0.13
76.92 ±0.12
49.84 ±0.18
69.64 ±0.15
74.61 ±0.13
58.61 ±0.18
76.40 ±0.13
80.76 ±0.11
61.26 ±0.20
79.01 ±0.13
83.04 ±0.12
miniImageNet, stage 1 only.
Pooling refers to stage 1 training. GAP: global average pooling; DC: dense classiﬁcation. At
testing, we use global max-pooling on queries for models trained
with dense classiﬁcation, and global average pooling otherwise.
implants at multiple depth levels, taking advantage of such
features during ﬁne-tuning and learning new ones.
Network adaptation is common when learning a new task
or new domain. One solution is to learn to mask part of the
network, keeping useful neurons and re-training/ﬁne-tuning
the remaining neurons on the new-task . Rusu et
al. rather widen the network by adding new neurons
in parallel to the old ones at every layer.
New neurons
receive data from all hidden states, while previously generated weights are frozen when training for the new task.
Our neural implants are related to as we add new neurons in parallel and freeze the old ones. Unlike , we
focus on low-data regimes, keeping the number of new implanted neurons small to diminish overﬁtting risks and train
faster, and adding them only at top layers, taking advantage
of generic visual features from bottom layers .
5. Experiments
extensively
miniImageNet and FC100 datasets.
We describe the
experimental setup and report results below.
5.1. Experimental setup
Networks. In most experiments we use a ResNet-12 network as our embedding network, composed of four
residual blocks , each having three 3×3 convolutional
layers with batch normalization and swish-1 activation function . Each block is followed by 2×2 maxpooling.
The shortcut connections have a convolutional
layer to adapt to the right number of channels. The ﬁrst
block has 64 channels, which is doubled at each subsequent
block such that the output has depth 512. We also test dense
classiﬁcation on a lighter network C128F composed of
four convolutional layers, the ﬁrst (last) two having 64 (128)
channels, each followed by 2×2 max-pooling.
We use miniImageNet , a subset of ImageNet ILSVRC-12 of 60,000 images of resolution
84 × 84, uniformly distributed over 100 classes. We use
the split proposed in : C = 64 classes for training, 16
for validation and 20 for testing.
Stage 1 training
Support/query pooling at testing
Global average pooling
Base classes
63.55 ±0.20
77.17 ±0.11
79.37 ±0.09
77.15 ±0.11
Novel classes
72.25 ±0.13
70.71 ±0.14
76.40 ±0.13
73.28 ±0.14
Both classes
37.74 ±0.07
38.65 ±0.05
56.25 ±0.10
54.80 ±0.09
Base classes
79.28 ±0.10
80.67 ±0.10
80.61 ±0.10
80.70 ±0.10
Dense classiﬁcation
Novel classes
79.01 ±0.13
77.93 ±0.13
78.55 ±0.13
78.95 ±0.13
Both classes
42.45 ±0.07
57.98 ±0.10
67.53 ±0.10
67.78 ±0.10
Table 2. Average 5-way 5-shot accuracy on base, novel and both classes of miniImageNet with ResNet-12, stage 1 only. GMP: global
max-pooling; GAP: global average pooling; DC: dense classiﬁcation. Bold: accuracies in the conﬁdence interval of the best one.
Stage 2 training
Query pooling at testing
79.03 ± 0.19
78.92 ± 0.19
79.04 ± 0.19
79.06 ± 0.19
79.37 ± 0.18
79.15 ± 0.19
79.62 ± 0.19
74.57 ± 0.22
79.77 ± 0.19
79.56 ± 0.19
74.58 ± 0.22
79.52 ± 0.19
Average 5-way 5-shot accuracy on novel classes of
miniImageNet with ResNet-12 and implanting in stage 2.
testing, we use GAP for support examples. GMP: global maxpooling; GAP: global average pooling; DC: dense classiﬁcation.
We also use FC100, a few-shot version of CIFAR-100
recently proposed by Oreshkin et al. .
Similarily to
miniImageNet, CIFAR-100 has 100 classes of 600 images each, although the resolution is 32 × 32. The split is
C = 60 classes for training, 20 for validation and 20 for
testing. Given that all classes are grouped into 20 superclasses, this split does not separate super-classes: classes
are more similar in each split and the semantic gap between
base and novel classes is larger.
Evaluation protocol. The training set X comprises images
of the base classes C. To generate the support set X′ of
a few-shot task on novel classes, we randomly sample C′
classes from the validation or test set and from each class we
sample k images. We report the average accuracy and the
corresponding 95% conﬁdence interval over a number of
such tasks. More precisely, for all implanting experiments,
we sample 5,000 few-shot tasks with 30 queries per class,
while for all other experiments we sample 10,000 tasks. Using the same task sampling, we also consider few-shot tasks
involving base classes C, following the benchmark of .
We sample a set of extra images from the base classes to
form a test set for this evaluation, which is performed in two
ways: independently of the novel classes C′ and jointly on
the union C ∪C′. In the latter case, base prototypes learned
at stage 1 are concatenated with novel prototypes .
Implementation details.
In stage 1, we train the embedding network for 8,000 (12,500) iterations with minibatch size 200 (512) on miniImageNet (FC100).
miniImageNet, we use stochastic gradient descent with Nesterov momentum. On FC100, we rather use Adam optimizer . We initialize the scale parameter at τ = 10
(100) on miniImageNet (FC100). For a given few-shot task
in stage 2, the implants are learned over 50 epochs with
AdamW optimizer and scale ﬁxed at τ = 10.
5.2. Results
Networks. In Table 1 we compare ResNet-12 to C128F,
with and without dense classiﬁcation.
We observe that
dense classiﬁcation improves classiﬁcation accuracy on
novel classes for ResNet-12, but it is detrimental for the
small network. C128F is only 4 layers deep and the receptive ﬁeld at the last layer is signiﬁcantly smaller than the one
of ResNet-12, which is 12 layers deep. It is thus likely that
units from the last feature map correspond to non-object areas in the image. Regardless of the choice of using dense
classiﬁcation or not, ResNet-12 has a large performance gap
over C128F. For the following experiments, we use exclusively ResNet-12 as our embedding network.
Dense classiﬁcation. To evaluate stage 1, we skip stage 2
and directly perform testing. In Table 2 we evaluate 5-way
5-shot classiﬁcation on miniImageNet with global average
pooling and dense classiﬁcation at stage 1 training, while
exploring different pooling strategies at inference. We also
tried using global max-pooling at stage 1 training and got
similar results as with global average pooling. Dense classi-
ﬁcation in stage 1 training outperforms global average pooling in all cases by a large margin. It also improves the ability of the network to integrate new classes without forgetting the base ones. Using dense classiﬁcation at testing as
well, the accuracy on both classes is 67.78%, outperforming
the best result of 59.35% reported by . At testing, dense
classiﬁcation of the queries with global average pooling of
the support samples is the best overall choice. One exception is global max-pooling on both the support and query
samples, which gives the highest accuracy for new classes
but the difference is insigniﬁcant.
Implanting. In stage 2, we add implants of 16 channels to
58.61 ± 0.18
76.40 ± 0.13
80.76 ± 0.11
62.53 ± 0.19
78.95 ± 0.13
82.66 ± 0.11
61.73 ± 0.19
78.25 ± 0.14
82.03 ± 0.12
DC + IMP (ours)
79.77 ± 0.19
83.83 ± 0.16
48.70 ± 1.8
63.10 ± 0.9
49.42 ± 0.78
68.20 ± 0.66
Gidaris et al. 
55.45 ± 0.7
73.00 ± 0.6
56.50 ± 0.4
74.20 ± 0.2
78.60 ± 0.4
TADAM 
Average 5-way accuracy on novel classes of mini-
The top part is our solutions and baselines, all on
ResNet-12. GAP: global average pooling (stage 1); DC: dense
classiﬁcation (stage 1); WIDE: last residual block widened by 16
channels (stage 1); IMP: implanting (stage 2). In stage 2, we use
GAP on both support and queries. At testing, we use GAP on
support examples and GAP or DC on queries, depending on the
choice of stage 1. The bottom part results are as reported in the literature. PN: Prototypical Network . MAML and PN 
use four-layer networks; while PN and TADAM use the
same ResNet-12 as us. Gidaris et al. use a Residual network of
comparable complexity to ours.
41.02 ± 0.17
56.63 ± 0.16
61.65 ±0.15
42.04 ± 0.17
57.05 ± 0.16
61.91 ± 0.16
DC + IMP (ours)
57.63 ± 0.23
62.91 ± 0.22
37.80 ± 0.40
53.30 ± 0.50
58.70 ± 0.40
TADAM 
40.10 ± 0.40
56.10 ± 0.40
61.60 ± 0.50
Table 5. Average 5-way accuracy on novel classes of FC100 with
ResNet-12. The top part is our solutions and baselines. GAP:
global average pooling (stage 1); DC: dense classiﬁcation (stage
1); IMP: implanting (stage 2). In stage 2, we use GAP on both
support and queries. At testing, we use GAP on support examples and GAP or DC on queries, depending on the choice of stage
1. The bottom part results are as reported in the literature. All
experiments use the same ResNet-12.
all convolutional layers of the last residual block of our embedding network pretrained in stage 1 on the base classes
with dense classiﬁcation. The implants are trained on the
few examples of the novel classes and then used as an integral part of the widened embedding network φθ,θ′ at testing.
In Table 3, we evaluate different pooling strategies for support examples and queries in stage 2. Average pooling on
both is the best choice, which we keep in the following.
Ablation study. In the top part of Table 4 we compare
our best solutions with a number of baselines on 5-way
miniImageNet classiﬁcation. One baseline is the embedding network trained with global average pooling in stage
1. Dense classiﬁcation remains our best training option. In
stage 2, the implants are able to further improve on the results of dense classiﬁcation. To illustrate that our gain does
not come just from having more parameters and greater feature dimensionality, another baseline is to compare it to
widening the last residual block of the network by 16 channels in stage 1. It turns out that such widening does not
bring any improvement on novel classes. Similar conclusions can be drawn from the top part of Table 5, showing
corresponding results on FC100. The difference between
different solutions is less striking here. This may be attributed to the lower resolution of CIFAR-100, allowing
for less gain from either dense classiﬁcation or implanting,
since there may be less features to learn.
Comparison with the state-of-the-art. In the bottom part
of Table 4 we compare our model with previous few-shot
learning methods on the same 5-way miniImageNet classiﬁcation. All our solutions outperform by a large margin
other methods on 1, 5 and 10-shot classiﬁcation. Our implanted network sets a new state-of-the art for 5-way 5-shot
classiﬁcation of miniImageNet. Note that prototypical network on ResNet-12 is already giving very competitive
performance. TADAM builds on top of this baseline
to achieve the previous state of the art. In this work we
rather use a cosine classiﬁer in stage 1. This setting is our
baseline GAP and is already giving similar performance to
TADAM . Dense classiﬁcation and implanting are both
able to improve on this baseline. Our best results are at
least 3% above TADAM in all settings. Finally, in the
bottom part of Table 5 we compare our model on 5-way
FC100 classiﬁcation against prototypical network and
TADAM . Our model outperforms TADAM here too,
though by a smaller margin.
6. Conclusion
In this work we contribute to few-shot learning by building upon a simpliﬁed process for learning on the base
classes using a standard parametric classiﬁer. We investigate for the ﬁrst time in few-shot learning the activation
maps and devise a new way of handling spatial information
by a dense classiﬁcation loss that is applied to each spatial
location independently, improving the spatial distribution of
the activation and the performance on new tasks. It is important that the performance beneﬁt comes with deeper network architectures and high-dimensional embeddings. We
further adapt the network for new tasks by implanting neurons with limited new parameters and without changing the
original embedding. Overall, this yields a simple architecture that outperforms previous methods by a large margin
and sets a new state of the art on standard benchmarks.