Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 498–508,
November 16–20, 2020. c⃝2020 Association for Computational Linguistics
Contrastive Distillation on Intermediate Representations
for Language Model Compression
Siqi Sun, Zhe Gan, Yu Cheng, Yuwei Fang, Shuohang Wang, Jingjing Liu
Microsoft Dynamics 365 AI Research
{siqi.sun,zhe.gan,yu.cheng,yuwfan,shuohang.wang,jingjl}@microsoft.com
Existing language model compression methods mostly use a simple L2 loss to distill
knowledge in the intermediate representations
of a large BERT model to a smaller one. Although widely used, this objective by design
assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To
achieve better distillation efﬁcacy, we propose
Contrastive Distillation on Intermediate Representations (CODIR), a principled knowledge
distillation framework where the student is
trained to distill knowledge through intermediate layers of the teacher via a contrastive
objective.
By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden
layers. CoDIR can be readily applied to compress large-scale language models in both pretraining and ﬁnetuning stages, and achieves
superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.1
Introduction
Large-scale pre-trained language models (LMs),
such as BERT , XLNet and RoBERTa ,
have brought revolutionary advancement to the
NLP ﬁeld . However, as newgeneration LMs grow more and more into behemoth size, it becomes increasingly challenging
to deploy them in resource-deprived environment.
Naturally, there has been a surge of research interest in developing model compression methods 
1Code will be released at 
intersun/CoDIR.
to reduce network size in pre-trained LMs, while
retaining comparable performance and efﬁciency.
PKD was the ﬁrst known effort
in this expedition, an elegant and effective method
that uses knowledge distillation (KD) for BERT
model compression at ﬁnetuning stage. Later on,
DistilBERT , TinyBERT and MobileBERT carried on the torch and extended similar compression
techniques to pre-training stage, allowing efﬁcient
training of task-agnostic compressed models. In
addition to the conventional KL-divergence loss applied to the probabilistic output of the teacher and
student networks, an L2 loss measuring the difference between normalized hidden layers has proven
to be highly effective in these methods. However,
L2 norm follows the assumption that all dimensions of the target representation are independent,
which overlooks important structural information
in the many hidden layers of BERT teacher.
Motivated by this, we propose Contrastive
Distillation
Intermediate
Representations
(CODIR), which uses a contrastive objective to
capture higher-order output dependencies between
intermediate representations of BERT teacher and
the student. Contrastive learning aims to learn representations by
enforcing similar elements to be equal and dissimilar elements further apart. Formulated in either supervised or unsupervised way, it has been successfully applied to diverse applications . To the best of our knowledge, utilizing
contrastive learning to compress large Transformer
models is still an unexplored territory, which is the
main focus of this paper.
A teacher network’s hidden layers usually contain rich semantic and syntactic knowledge that
can be instrumental if successfully passed on to
the student . Thus, instead of directly
applying contrastive loss to the ﬁnal output layer of
the teacher, we apply contrastive learning to its intermediate layers, in addition to the use of KL-loss
between the probabilistic outputs of the teacher and
student. This casts a stronger regularization effect
for student training by capturing more informative
signals from intermediate representations. To maximize the exploitation of intermediate layers of the
teacher, we also propose the use of mean-pooled
representation as the distillation target, which is
empirically more effective than commonly used
special [CLS] token.
To realize constrastive distillation, we deﬁne a
congruent pair (ht
i) as the pair of representations of the same data input from the teacher and
student networks, as illustrated in Figure 1. Incongruent pair (ht
j) is deﬁned as the pair of representations of two different data samples through the
teacher and the student networks, respectively. The
goal is to train the student network to distinguish
the congruent pair from a large set of incongruent
pairs, by minimizing the constrastive objective.
For efﬁcient training, all data samples are stored
in a memory bank . During ﬁnetuning, incongruent pairs can be
selected by choosing sample pairs with different
labels to maximize the distance. For pre-training,
however, it is not straightforward to construct incongruent pairs this way as labels are unavailable.
Thus, we randomly sample data points from the
same mini-batch pair to form incongruent pairs,
and construct a proxy congruent-incongruent sample pool to assimilate what is observed in the downstream tasks during ﬁnetuning stage. This and other
designs in CoDIR make constrative learning possible for LM compression, and have demonstrated
strong performance and high efﬁciency in experiments.
Our contributions are summarized as follows.
(i) We propose CODIR, a principled framework
to distill knowledge in the intermediate representations of large-scale language models via a contrastive objective, instead of a conventional L2 loss.
(ii) We propose effective sampling strategies to
enable CoDIR in both pre-training and ﬁnetuning
stages. (iii) Experimental results demonstrate that
CoDIR can successfully train a half-size Transformer model that achieves competing performance
to BERT-base on the GLUE benchmark , with half training time and GPU demand. Our pre-trained model checkpoint will be
released for public access.
Related Work
Language Model Compression
To reduce computational cost of training large-scale language
models, many model compression techniques have
been developed, such as quantization , pruning , knowledge distillation ,
and direct Transformer block modiﬁcation .
Quantization refers to storing model parameters
from 32- or 16-bit ﬂoating number to 8-bit or even
lower. Directly truncating the parameter values will
cause signiﬁcant accuracy loss, hence quantizationaware training has been developed to maintain similar accuracy to the original model . Michel et al. 
found that even after most attention heads are removed, the model still retains similar accuracy, indicating there is high redundancy in the learned
model weights. Later studies proposed different
pruning-based methods. For example, Gordon et al.
 simply removed the model weights that
are close to zero; while Guo et al. used
re-weighted L1 and proximal algorithm to prune
weights to zero. Note that simple pruning does not
improve inference speed, unless there is structure
change such as removing the whole attention head.
There are also some efforts that try to improve
the Transformer block directly. Typically, language
models such as BERT and
RoBERTa can only handle a sequence of tokens in length up to 512. Kitaev et al.
 proposed to use reversible residual layers
and locality-sensitive hashing to reduce memory usage to deal with extremely long sequences. Besides,
Wu et al. proposed to use convolutional neural networks to capture short-range attention such
that reducing the size of self-attention will not signiﬁcantly hurt performance.
Another line of research on model compression
is based on knowledge transfer, or knowledge distillation (KD) , which is the
main focus of this paper. Note that previously introduced model compression techniques are orthogonal to KD, and can be bundled for further speedup.
Distilled BiLSTM tried to dis-
Teacher Model (𝑓𝑡)
Student Model (𝑓𝑠)
𝑋0 = the greatest musicians [True]
𝑋1 = cold movie [False]
𝑋2 = no apparent joy [False]
𝑋𝐾= the action is stilted [False]
ℒ𝐶𝐸or ℒ𝑀𝐿𝑀
Contrastive Distillation
Intermediate
Representations
𝑋0 = [MASK] music is a [MASK] of ...
𝑋1 = The term “[MASK] music” ...
𝑋2 = “pop” [MASK] “rock” were ...
𝑋𝐾= Identifying [MASK] usually ...
S1: pop music is a genre of popular
music that ...
S2: The term “popular music” and
“pop music” ...
S3: “pop” and “rock” were roughly
synonymous...
SK: Identifying factors usually
include short ...
Pre-Training
Finetuning
Incongruent
Figure 1: Overview of the proposed CoDIR framework for language model compression in both pre-training and
ﬁnetuning stages. “Trm” represents a Transformer block, X are input tokens, f t and f s are teacher and student
models, and X0, {Xi}K
i=1 represent one positive sample and a set of negative samples, respectively. The difference
between CoDIR pre-training and ﬁnetuning mainly lies in the negative example sampling method.
till knowledge from BERT into a simple LSTM.
Though achieving more than 400 times speedup
compared to BERT-large, it suffers from signiﬁcant
performance loss due to the shallow network architecture. DistilBERT proposed to
distill predicted logits from the teacher model into
a student model with 6 Transformer blocks. BERT-
PKD proposed to not only distill
the logits, but also the representation of [CLS]
tokens from the intermediate layers of the teacher
model. TinyBERT , MobileBERT
 and SID further proposed to improve BERT-PKD by distilling
more internal representations to the student, such
as embedding layers and attention weights. These
existing methods can be generally divided into two
categories: (i) task-speciﬁc, and (ii) task-agnostic.
Task-speciﬁc methods, such as Distilled BiLSTM,
BERT-PKD and SID, require the training of individual teacher model for each downstream task;
while task-agnostic methods such as DistilBERT,
TinyBERT and MobileBERT use KD to pre-train a
model that can be applied to all downstream tasks
by standard ﬁnetuning.
Contrastive
Representation
Contrastive learning is a popular research area that
has been successfully applied to density estimation and representation learning, especially in selfsupervised setting . However, it is unclear whether the success
is determined by mutual information or by the speciﬁc form of the contrastive loss . Recently, it has been extended to knowledge
distillation and cross-modal transfer for image classiﬁcation tasks . Different from
prior work, we propose the use of contrastive objective for Transformer-based model compression
and focus on language understanding tasks.
CoDIR for Model Compression
In this section, we ﬁrst provide an overview of
the proposed method in Sec. 3.1, then describe the
details of contrastive distillation in Sec. 3.2. Its
adaptation to pre-training and ﬁnetuning is further
discussed in Sec. 3.3.
Framework Overview
We use RoBERTa-base as the
teacher network, denoted as ft, which has 12 layers with 768-dimension hidden representations. We
aim to transfer the knowledge of ft into a student network fs, where fs is a 6-layer Trans-
former to mimic the behavior of ft . Denote a training sample as (X, y), where X = (x0, . . . , xL−1)
is a sequence of tokens in length L, and y is
the corresponding label (if available). The word
embedding matrix of X is represented as X =
(x0, . . . , xL−1), where xi ∈Rd is a d-dimensional
vector, and X ∈RL×d. In addition, the intermediate representations at each layer for the teacher
and student are denoted as Ht = (Ht
1, . . . , Ht
and Hs = (Hs
1, . . . , Hs
6), respectively, where
i ∈RL×d contains all the hidden states in
one layer. And zt, zs ∈Rk are the logit representations (before the softmax layer) of the teacher
and student, respectively, where k is the number of
As illustrated in Figure 1, our distillation objective consists of three components: (i) original
training loss from the target task; (ii) conventional
KL-divergence-based loss to distill the knowledge
of zt into zs; (iii) proposed contrastive loss to distill the knowledge of Ht into Hs. The ﬁnal training
objective can be written as:
LCoDIR(θ) = LCE(zs, y; θ) + α1LKD(zt, zs; θ)
+ α2LCRD(Ht, Hs; θ) ,
where LCE, LKD and LCRD correspond to the original loss, KD loss and contrastive loss, respectively.
θ denotes all the learnable parameters in the student fs, while the teacher network is pre-trained
and kept ﬁxed. α1, α2 are two hyper-parameters to
balance the loss terms.
LCE is typically implemented as a cross-entropy
loss for classiﬁcation problems, and LKD can be
written as
LKD(zt, zs; θ) = KL
 g(zt/ρ)∥g(zs/ρ)
where g(·) denotes the softmax function, and ρ
is the temperature. LKD encourages the student
network to produce distributionally-similar outputs
to the teacher network.
Only relying on the ﬁnal logit output for distillation discards the rich information hidden in the intermediate layers of BERT. Recent work has found that distilling the
knowledge from intermediate representations with
L2 loss can further enhance the performance. Following the same intuition, our proposed method
also aims to achieve this goal, with a more principled contrastive objective as detailed below.
Contrastive Distillation
First, we describe how to summarize intermediate
representations into a concise feature vector. Based
on this, we detail how to perform contrastive distillation for model compression.
Intermediate Representation
Directly using
Ht and Hs for distillation is infeasible, as the total
feature dimension is |Hs| = 6 × 512 × 768 ≈2.4
million for a sentence in full length (i.e., L =
512). Therefore, we propose to ﬁrst perform meanpooling over Ht and Hs to obtain a layer-wise
sentence embedding. Note that the embedding of
the [CLS] token can also be used directly for this
purpose; however, in practice we found that meanpooling performs better. Speciﬁcally, we conduct
row-wise average over Ht
i = Pool(Ht
i = Pool(Hs
i ∈Rd are the sentence embedding
for layer i of the teacher and student model, respectively.
Therefore, the student’s intermediate representation can be summarized as ¯hs =
1; . . . ; ¯hs
6] ∈R6d, where [; ] denotes vector
concatenation. Similarly, the teacher’s intermediate representation can be summarized as ¯ht =
1; . . . ; ¯ht
12] ∈R12d. Two linear mappings φs :
R6d →Rm and φt : R12d →Rm are further
applied to project ¯ht and ¯hs into the same lowdimensional space, yielding ht, hs ∈Rm, which
are used for calculating the contrastive loss.
Contrastive Objective
Given a training sample
(X0, y0), we ﬁrst randomly select K negative samples with different labels, denoted as {(Xi, yi)}K
Following the above process, we can obtain a summarized intermediate representation ht
by sending X0 to both the teacher and student network. Similarly, for negative samples, we can obtain {hs
Contrastive learning aims to map the student’s
representation hs
0 close to the teacher’s representation ht
0, while the negative samples’ representations {hs
i=1 far apart from ht
0. To achieve this,
we use the following InfoNCE loss for model training:
LCRD(θ) = −log
where ⟨·, ·⟩denotes the cosine similarity between
two feature vectors, and τ is the temperature that
controls the concentration level. As demonstrated,
contrastive distillation is implemented as a (K +
1)-way classiﬁcation task, which is interpreted as
maximizing the lower bound of mutual information
between ht
0 , which renders large-scale contrastive distillation infeasible for practical use. To address this
issue, we follow Wu et al. and use a memory bank M ∈RN×m to store the intermediate
representation of all N training examples, and the
representation is only updated for positive samples
in each forward propagation. Therefore, the training cost is roughly the same as in normal training.
Speciﬁcally, assume the mini-batch size is 1, then
at each training step, M is updated as:
m0 = β · m0 + (1 −β) · hs
where m0 is the retrieved representation from
memory bank M that corresponds to hs
β ∈(0, 1) is a hyper-parameter that controls how
aggressively the memory bank is updated.
Finetuning
Since task-speciﬁc label supervision
is available in ﬁnetuning stage, applying CoDIR
to ﬁnetuning is relatively straightforward. When
selecting negative samples from the memory bank,
we make sure the selected samples have different
labels from the positive sample.
Pre-training
For pre-training, the target task becomes masked language modeling (MLM) . Therefore, we replace the LCE
loss in Eqn. (1) with LMLM. Following Liu et al.
 ; Lan et al. , we did not include the
next-sentence-prediction task for pre-training, as
it does not improve performance on downstream
tasks. Since task-speciﬁc label supervision is unavailable during pre-training, we propose an effective method to select negative samples from the
memory bank. Speciﬁcally, we sample negative
examples randomly from the same mini-batch each
time, as they have closer semantic meaning as some
of them are from the same article, especially for
Bookcorpus . Then, we use the
sampled negative examples to retrieve representations from the memory bank. Intuitively, negative
examples sampled in this way serve as “hard” negatives, compared to randomly sampling from the
whole training corpus; otherwise, the LCRD loss
could easily drop to zero if the task is too easy.
Experiments
In this section, we present comprehensive experiments on a wide range of downstream tasks and
provide detailed ablation studies, to demonstrate
the effectiveness of the proposed approach to largescale LM compression.
We evaluate the proposed approach on sentence
classiﬁcation tasks from the General Language Understanding Evaluate (GLUE) benchmark , as our ﬁnetuning framework is designed for classiﬁcation, and we only exclude the
STS-B dataset . Following other
works , we also do not run experiments on WNLI
dataset , as it is very dif-
ﬁcult and even majority voting outperforms most
benchmarks.2
Linguistic
Acceptability
 contains a collection of 8.5k
sentences drawn from books or journal articles.
The goal is to predict if the given sequence
of words is grammatically correct.
correlation coefﬁcient is used as the evaluation
Stanford Sentiment Treebank consists of 67k human-annotated
movie reviews. The goal is to predict whether each
review is positive or negative. Accuracy is used as
the evaluation metric.
Microsoft Research Paraphrase Corpus
 consists of 3.7k sentence pairs extracted from online news, and the
goal to predict if each pair of sentences is semantically equivalent. F1 score from GLUE server is
reported as the metric.
The Quora Question Pairs3 task consists
of 393k question pairs from Quora webiste. The
2Please refer to 
3 
Question-Pairs
MNLI-m/-mm
RoBERTa-base (Ours)
BERT-base (Google)
DistilBERT
MLM-Pre + Fine
CoDIR-Fine
CoDIR-Pre + CoDIR-Fine
Table 1: Results on GLUE Benchmark. (*) indicates those numbers are unavailable in the original papers and were
obtained by us through submission to the ofﬁcial leaderboard using their codebases. Other results are obtained from
published papers. (⋆) indicates those methods with fewer Transformer blocks, and may not be fair comparison.
task is to predict whether a pair of questions is
semantically equivalent. Accuracy is used as the
evaluation metric.
Multi-Genre Natural Language Inference
Corpus (MNLI) , Questionanswering NLI (QNLI) 
and Recognizing Textual Entailment (RTE)4 are
all natural language inference (NLI) tasks, which
consist of 393k/108k/2.5k pairs of premise and
hypothesis. The goal is to predict if the premise
entails the hypothesis, or contradicts it, or neither.
Accuracy is used as the evaluation metric. Besides,
MNLI test set is further divided into two splits:
matched (MNLI-m, in-domain) and mismatched
(MNLI-mm, cross-domain), accuracy for both are
Implementation Details
We mostly follow the pre-training setting from Liu
et al. , and use the fairseq implementation
 . Speciﬁcally, we truncate raw text
into sentences with maximum length of 512 tokens,
and randomly mask 15% of tokens as [MASK].
For model architecture, we use a randomly initialized 6-layer Transformer model as the student, and
RoBERTa-base with 12-layer Transformer as the
teacher. The student model was ﬁrst trained by
using Adam optimizer with learning rate 0.0007
and batch size 8192 for 35,000 steps. For computational efﬁciency, this model serves as the initialization for the second-stage pre-training with
4Collections of series of annual textual entailment challenges.
the teacher. Then, the student model is further
trained for another 10,000 steps with KD and the
proposed contrastive objective, with learning rate
set to 0.0001. We denote this model as CoDIR-Pre.
For ablation purposes, we also train two baseline
models with only MLM loss or KD loss, using the
same learning rate and number of steps. Similarly,
these two models are denoted as MLM-Pre and
KD-Pre, respectively. For other hyper-parameters,
we use α1 = α2 = 0.1 for both LKD and LCRD.
Due to high computational cost for pre-training,
all the hyper-parameters are set empirically without
tuning. As there exist many combinations of pretraining loss (MLM, KD, and CRD) and ﬁnetuning
strategies (standard ﬁnetuning with cross-entropy
loss, and ﬁnetuning with additional CRD loss), a
grid search of all the hyper-parameters is infeasible.
Thus, for standard ﬁnetuning, we search learning
rate from {1e-5, 2e-5} and batch size from {16,
32}. The combination with the highest score on
dev set is reported for ablation studies, and is kept
ﬁxed for future experiments. We then ﬁx the hyperparameters in KD as ρ = 2, α1 = 0.7, and search
weight of the CRD loss α2 from {0.1, 0.5, 1}, and
the number of negative samples from {100, 500,
1000}. Results with the highest dev scores were
submitted to the ofﬁcial GLUE server to obtain
the ﬁnal results. For fair comparison with other
baseline methods, all the results are based on singlemodel performance.
Experimental Results
Results of different methods from the ofﬁcial
GLUE server are summarized in Table 1. For sim-
Pre-training Loss
Finetuning Method
CoDIR-Fine
LMLM + LKD
Table 2: Ablation study on different combination of pre-trained models and ﬁnetuning approach. The results are
based on GLUE dev set.
#Trm Layers
#Params (Emb Layer)
Inference Time (ms/seq)
RoBERTa-base
DistilBERT
Table 3: Inference speed comparison between teacher and students. Inference time is measured on MNLI dev set.
Speed up is measured against BERT-base, which is the teacher model for other baseline methods.
plicity, we denote our baseline approach without using any teacher supervision as “MLM-Pre + Fine”:
pre-trained by using MLM loss ﬁrst, then ﬁnetuning using standard cross-entropy loss. Our baseline
already achieves high average score across 8 tasks,
and outperforms task-speciﬁc model compression
methods and
BERT-PKD ) as well as Distil-
BERT by a large margin.
After adding contrastive loss at the ﬁnetuning
stage (denoted as CoDIR-Fine), the model outperforms the state-of-the-art compression method,
TinyBERT with 6-layer Transformer, on average
GLUE score. Especially on datasets with fewer
data samples, such as CoLA and MRPC, the improved margin is large (+2.5% and +2.1%, respectively). Compared to our MLM-Pre + Fine baseline,
CoDIR-Fine achieves signiﬁcant performance gain
on almost all tasks (+1.2% absolute improvement
on average score), demonstrating the effectiveness
of the proposed approach. The only exception is
QQP (-0.1%) with more than 360k training examples. In such case, standard ﬁnetuning may already bring in enough performance boost with this
large-scale labeled dataset, and the gap between
the teacher and student networks is already small
(89.6 vs 89.2).
We further test the effectiveness of CoDIR for
pre-training (CoDIR-Pre), by applying standard
ﬁnetuning on model pre-trained with additional
contrastive loss. Again, compared to the MLM-
Pre + Fine baseline, this improves the model performance on almost all the tasks (except QQP),
with a signiﬁcant lift on the average score (+1.4%).
We notice that this model performs similarly to
the contrastive-ﬁnetuning only approach (CoDIR-
Fine) on almost all tasks. However, CoDIR-Pre is
preferred because it utilizes the teacher’s knowledge in the pre-training stage, thus no task-speciﬁc
teacher is needed for ﬁnetuning downstream tasks.
Finally, we experiment with the combination of
CoDIR-Pre and CoDIR-Fine, and our observation
is that adding constrastive loss for ﬁnetuning is not
bringing in much improvement after already using
constrastive loss in pre-training. Our hypothesis is
that the model’s ability to identify negative examples is already well learned during pre-training.
Inference Speed
We compare the inference
speed of the proposed CoDIR with the teacher
network and other baselines. Statistics of Transformer layers and parameters are presented in Table
3. The statistics for BERT6-PKD and TinyBERT6
are omitted as they share the same model architecture as DistilBERT. To test the inference speed, we
ran each algorithm on MNLI dev set for 3 times,
with batch size 32 and maximum sequence length
128 under the same hardware conﬁguration. The
average running time with 3 different random seeds
is reported as the ﬁnal inference speed. Though
our RoBERTa teacher has almost 16 million more
parameters, it shares almost the same inference
MNLI-m/-mm
CoDIR-Fine ([CLS])
CoDIR-Fine (Mean Pool)
CoDIR-Fine (100-neg)
CoDIR-Fine (500-neg)
CoDIR-Fine (1000-neg)
Table 4: Ablation study on the use of [CLS] and Mean-Pooling as sentence embedding (upper part) and effect of
number of negative examples (neg) for CoDIR-Fine (bottom part). The results are based on GLUE dev set.
Standard Deviation
Table 5: Analysis of model variance on GLUE dev set. Statistical results (median, maximum, and standard deviation) are based on 8 runs with the same hyper-parameters.
speed as BERT-base, because its computational
cost mainly comes from the embedding layer with
50k vocabulary size that does not affect inference
speed. By reducing the number of Transformer
layers to 6, our proposed student model achieves
2 times speed up compared to the teacher, and
achieves state-of-the-art performance among all
models with similar inference time.
Ablation Studies
Sentence Embedding
We also conduct experiments to evaluate the effectiveness of using different sentence embedding strategies. More detailed, based on the same model pre-trained on
LMLM alone, we run ﬁnetuning experiments with
contrastive loss on the GLUE dataset by using:
(i) [CLS] as sentence embedding; and (ii) meanpooling as sentence embedding. The results on
GLUE dev set are presented in top rows of Table
4, showing that mean-pooling yields better results
than [CLS] (83.0 vs. 82.5 on average). As a result, we use mean pooling as our chosen sentence
embedding for all our experiments.
Negative Examples
As we mentioned in Section
4.2, the experiments are conducted using 100, 500
and 1000 negative examples. We then evaluate the
effect of number of negative examples by comparing their results on GLUE dev set, and the results
are presented in the bottom part of Table 4. Obviously, for most dataset the accuracy increases
as a larger number of negative examples are used
during training. Similar observations were also reported in Tian et al. , and a theoretical analysis is provided in Arora et al. . The only
two exceptions are QQP and RTE. As discussed
in Section 4.3, our CoDIR method seems also not
work well on QQP due to the small gap between
teacher and student. As for RTE, due to the small
number of training examples, the results are quite
volatile, which may make the results inconsistent.
Besides, the number of negative examples is close
to the number of examples per class (1.25k) for
RTE, which can also result in the contrastive loss
close to 0.
Contrastive Loss
We ﬁrst evaluate the effectiveness of the proposed CRD loss for ﬁnetuning on a
subset of GLUE dev set, using the following settings: (i) ﬁnetuning with cross-entropy loss only;
(ii) ﬁnetuning with additional KD loss; and (iii)
ﬁnetuning with additional KD loss and CRD loss.
Results in Table 2 (upper part) show that using
KD improves over standard ﬁnetuning by 0.9% on
average, and using CRD loss further improves another 1.0%, demonstrating the advantage of using
contrastive learning for ﬁnetuning.
To further validate performance improvement
of using contrastive loss on pre-training, we apply
standard ﬁnetuning to three different pre-trained
models: (i) model pre-trained by LMLM (MLM-
Pre); (ii) model pre-trained by LMLM + LKD (KD-
Pre); and (iii) model pre-trained by LMLM +LKD +
LCRD (CoDIR-Pre). Results are summarized in Ta-
ble 2 (bottom part). Similar trend can be observed
that the model pre-trained with additional CRD
loss performs the best, outperforming MLM-Pre
and KD-Pre by 1.9% and 1.0% on average, respectively.
Model Variance
Since different random seeds
can exhibit different generalization behaviors, especially for tasks with a small training set (e.g.,
CoLA ), we examine the median, maximum and
standard deviation of model performance on the
dev set of each GLUE task, and present the results
in Table 5. As expected, the models are more stable on larger datasets (SST-2, QQP, MNLI, and
QNLI), where all standard deviations are lower
than 0.5. However, the model is sensitive to the
random seeds on smaller datasets (CoLA, MRPC,
and RTE) with the standard deviation around 1.5.
These analysis results provide potential references
for future work on language model compression.
Conclusion
In this paper, we present CoDIR, a novel approach
to large-scale language model compression via the
use of contrastive loss. CoDIR utilizes information
from both teacher’s output layer and its intermediate layers for student model training. Extensive experiments demonstrate that CoDIR is highly effective in both ﬁnetuning and pre-training stages, and
achieves state-of-the-art performance on GLUE
benchmark compared to existing models with a
similar size. All existing work either use BERTbase or RoBERTa-base as teacher. For future work,
we plan to investigate the use of a more powerful
language model, such as Megatron-LM , as the teacher; and different strategies
for choosing hard negatives to further boost the
performance.