On the Statistical Interpretation of the
Piecewise Smooth Mumford-Shah Functional
Thomas Brox and Daniel Cremers
CVPR Group, University of Bonn
R¨omerstr. 164, 53117 Bonn, Germany
{brox,dcremers}@cs.uni-bonn.de
Abstract. In region-based image segmentation, two models dominate
the ﬁeld: the Mumford-Shah functional and statistical approaches based
on Bayesian inference. Whereas the latter allow for numerous ways to
describe the statistics of intensities in regions, the ﬁrst includes spatially smooth approximations. In this paper, we show that the piecewise smooth Mumford-Shah functional is a ﬁrst order approximation of
Bayesian a-posteriori maximization where region statistics are computed
in local windows. This equivalence not only allows for a statistical interpretation of the full Mumford-Shah functional. Inspired by the Bayesian
model, it also oﬀers to formulate an extended Mumford-Shah functional
that takes the variance of the data into account.
Introduction
Since the beginning of image analysis research, there has been enormous interest
in image segmentation. While the topic was handled in a quite heuristic manner
for a long time, a more systematic approach to the problem has been initiated
by three seminal works in the 1980s: the Bayesian formulation of Geman and
Geman , the energy functional of Mumford and Shah , and the snakes
model by Kass, Witkin, and Terzopoulos . In all these works, the formerly
purely algorithmic description of a segmentation method has been replaced by
its formulation as an optimization problem. This systematic description based on
sound mathematical concepts has considerably improved the understanding of
image segmentation and, hence, supported the development of new models and
better algorithms. The initially large gap between sound energy formulations
and eﬃcient ways to ﬁnd solutions of these energies, in particular in case of the
Mumford-Shah functional, was bridged by the works of Ambrosio and Tortorelli
 , Morel and Solimini , as well as the use of level set representations of
contours by Caselles et al. , Chan and Vese , and Paragios and Deriche .
A further type of optimization strategy has emerged in the spatially discrete
case with graph cut methods .
Whereas all three approaches to image segmentation are based on energy
minimization, their motivation is quite diﬀerent. In , Zhu and Yuille outlined
many relations between the methods and algorithmic implementations such as
region merging or region growing. In particular, they established a link between
F. Sgallari, A. Murli, and N. Paragios (Eds.): SSVM 2007, LNCS 4485, pp. 203–213, 2007.
⃝Springer-Verlag Berlin Heidelberg 2007
T. Brox and D. Cremers
a Bayesian approach to image segmentation and the piecewise constant case
of the Mumford-Shah functional, sometimes called the cartoon limit. Zhu and
Yuille also suggested a more general energy functional that replaces the constant
approximation of image regions by arbitrary intensity distributions. This formulation was used particularly in level set based segmentation approaches where
full Gaussian distributions , Laplace distributions , and nonparametric
kernel densities have been suggested.
Zhu and Yuille established relations between Bayesian methods and the cartoon limit of the Mumford-Shah functional, yet in their work, they ignored the
part of the functional that allows also for piecewise smooth approximations. In
the present paper, we complete their work by showing that the Mumford-Shah
functional can be interpreted as a ﬁrst-order approximation of a Bayesian model
with probability densities estimated in local windows. Such types of densities
have been used in in the scope of contour-based pose estimation. Similar to the
work of Zhu and Yuille , this equivalence allows to generalize the Mumford-
Shah functional. We demonstrate this by proposing a functional which allows
to approximate the input intensity by a piecewise smooth Gaussian distribution
including mean and variance.
The Mumford-Shah Functional
The idea of Mumford and Shah was to ﬁnd a piecewise smooth approximation
u : (Ω ⊂R2) →R of the image I : (Ω ⊂R2) →R and an edge set K1 separating
the pieces of u, such that u is close to I and the total length of the edge set is
minimal. This can be expressed as minimizing the functional
(u −I)2dx + λ
|∇u|2dx + ν |K| →min,
where λ ≥0 and ν ≥0 are constant weighting parameters. An interesting special
case arises for λ →∞, where u is required to be piecewise constant. This case,
already discussed by Mumford and Shah in , is also known as the cartoon
limit and can be written in short form
(ui −I)2dx + ν0 |K| →min,
where Ωi denotes the piecewise constant regions separated by K and ν0 is the
rescaled version of the parameter ν in (1). Due to the quadratic error measure,
given Ωi, the solution of ui is the mean of I within Ωi. A related approach was
independently developed by Blake and Zisserman . In the spatially discrete
case, (2) is related to the Potts model .
The model in (2) can be simpliﬁed further by assuming a ﬁxed number of
regions N. In particular, the case N = 2 and its level set formulation by Chan
1 Since our focus lies on image segmentation, we will only consider edge sets which
are sets of closed curves .
Statistical Interpretation of the Mumford-Shah Functional
and Vese has become very popular. A discrete version of the binary case has
been introduced by Lenz and Ising for modeling ferromagnetism already in the
1920s .
Bayesian Model and Local Region Statistics
An alternative approach to image segmentation can be derived using Bayes’ rule
p(K|I) = p(I|K)p(K)
Here one seeks for a partitioning by the edge set K that maximizes the aposteriori probability given the image I. The ﬁrst factor in the nominator is
in general approximated by an intensity distribution in the regions i = 1, ..., N
separated by K. The second factor is the a-priori probability of a certain partitioning K. Usually, the total length of the edge set K is assumed to be small,
p(K) = exp(−νB|K|),
but other more sophisticated shape priors can be integrated here, as well .
Assuming independence of intensities at diﬀerent locations x, one can write
p(I(x)|K, x)dx,
a continuous product with dx being the inﬁnitesimal bin size. With the partitioning of Ω by the edge set K into disjoint regions Ω = 
i Ωi, Ωi ∩Ωj = ∅, ∀i ̸= j,
the product over the whole domain Ω can be separated into products over the
p(I(x)|K, x)dx =
p(I(x)|x, x ∈Ωi)dx.
For convenience we deﬁne the conditional probability density to encounter an
intensity s at position x given that x ∈Ωi as
pi(s, x) := p(s|x, x ∈Ωi).
Note that we have here a family of probability densities pi(s, x) for all x ∈Ω,
pi(s, x) : R →R+
pi(s, x) ≥0
∀s ∈R, ∀x ∈Ω
R pi(s, x)ds = 1∀x
In general, it is preferable to express the maximization of (3) by the minimization of its negative logarithm. With the above assumptions, this leads to
the energy functional
−log pi(I(x), x)dx + νB |K|.
T. Brox and D. Cremers
It obviously resembles the cartoon limit of the Mumford-Shah functional. We
will come back to this issue in the next section.
There are several possibilities how to model the probability densities pi. Typically, one assumes a homogeneous Gaussian distribution in each region Ωi:
where μi and σi denote the mean and standard deviation of I in region Ωi.
Other choices like a Laplace distribution or a nonparametric density 
are possible, as well. All these models apply the same probability density to all
points in a region. Hence, we will call them spatially homogeneous region models.
In contrast, local region models take the spatial position into account, i.e.,
there is in general a diﬀerent probability density at each point x in the region.
For a Gaussian distribution this yields :
pi(s, x) =
2πσi(x) exp
(s −μi(x))2
Estimation of the parameters μi(x) and σi(x) can be achieved using a window
function, e.g. a Gaussian Gρ with standard deviation ρ, and restricting the estimation only to points within this window:
Ωi Gρ(ζ −x)I(ζ) dζ
Ωi Gρ(ζ −x) dζ
Ωi Gρ(ζ −x)(I(ζ) −μi(x))2 dζ
Ωi Gρ(ζ −x) dζ
Obviously, the local region model converges to the corresponding homogeneous
model for ρ →∞.
Bayesian Interpretation of the Mumford-Shah
Functional
The Bayesian model from the last section is quite ﬂexible in the choice of the
probability density function. It further yields a nice statistical interpretation of
the model assumptions and allows for the sound integration of a-priori information. On the other hand, the Mumford-Shah functional combines segmentation
and image restoration by a piecewise smooth function. The reader may have already noticed similarities between the models in Section 2 and Section 3. In this
section, we will investigate the relation between both segmentation approaches
aiming at a statistical interpretation of the full Mumford-Shah functional.
We start with the Bayesian model in (9). A comparison to the cartoon model
in (2) reveals a large similarity. As shown in , for a speciﬁc choice of the probability densities, both formulations turn out to be equivalent. Indeed, equivalence
of (2) and (9) is established by modeling the probability densities as Gaussian
functions with ﬁxed standard deviation
Statistical Interpretation of the Mumford-Shah Functional
Applying the logarithm
log pi(s) = −1
2 log(2πσ2) −(s −μi)2
and plugging this into (9) yields
2 log(2πσ2) + (I(x) −μi)2
dx + νB |K|
(I(x) −μi)2
dx + νB |K| + const.
Due to the same ﬁxed standard deviation in all regions, the logarithm term
containing σ does not depend on K and, hence, is a negligible constant in the
energy functional. Also the denominator 2σ2 is a constant and merely leads to
a rescaling of the parameter νB. Thus, with μi ≡ui, σ =
0.5, and νB = ν0,
(15) states exactly the same energy minimization problem as the cartoon model
With this equivalence in mind, the question arises, whether there exists a
choice of the probability density function that relates the Bayesian model to
the full, piecewise smooth Mumford-Shah functional stated in (1). Since (1)
explicitly allows the approximation u to vary within a region, a homogeneous
region model is obviously not suﬃcient. Local region statistics, on the other
hand, include varying parameters in the region. Hence, having in mind that the
equivalence of the Bayesian model and the cartoon model was established for a
homogeneous Gaussian region model with ﬁxed standard deviation, we take a
closer look at the local Gaussian model, again with ﬁxed standard deviation.
Since the standard deviation is ﬁxed, we can focus on the local mean in (12):
Ωi Gρ(ζ −x)I(ζ) dζ
Ωi Gρ(ζ −x) dζ
The numerator is a convolution of the image I with the Gaussian function Gρ.
The denominator is only for normalization in case the window hits the boundary
of Ωi. It ensures the preservation of the average gray value of μi in the domain
Ωi independent of ρ.
In order to bend the bow to the Mumford-Shah functional, we will relate this
ﬁltering operation to a regularization framework. Yuille and Grzywacz as
well as Nielsen et al. showed that the outcomes of some linear ﬁlters are
exact minimizers of certain energy functionals with an inﬁnite sum of penalizer
terms of arbitrarily high order. More precisely, it was shown in that ﬁltering
an image I with the ﬁlter
T. Brox and D. Cremers
given in the frequency domain, yields the minimizer of the following energy
functional:
In particular, this includes for αk = λk
k! , the Gaussian ﬁlter
ˆh(ω, λ) =
k! ω2k = exp(−λω2).
This ﬁlter corresponds to the Gaussian Gρ with standard deviation ρ =
in the spatial domain. Nielsen et al. further showed in that for Cartesian
invariants, such as the Gaussian, this correspondence can be generalized to higher
dimensions. Therefore, the convolution result in (16) is the exact minimizer of
⎝(μi −I)2 +
with natural boundary conditions.
Based on these ﬁndings, we can proceed to generalize the piecewise constant
case in (15). We plug the local Gaussian probability density from (11) with ﬁxed
standard deviation σ =
0.5 into the Bayesian model in (9):
2 log(2πσ2) + (I(x) −μi(x))2
dx + νB |K|
(I(x) −μi(x))2dx + νB |K| + const.
The means μi have in (16) been deﬁned as the results of local convolutions. As
we have just found, this convolution result is the minimizer of (20). Hence, we
can write the Bayesian energy as:
EB(μ, K) =
⎝(μi −I)2 +
⎠dx + νB |K|.
Neglecting all penalizer terms of order k > 1 yields
EMS(μ, K) =
(μi −I)2 + λ|∇μi|2
dx + νB |K| + const.
which states exactly the Mumford-Shah functional in (1). Consequently, minimizing the full piecewise smooth Mumford-Shah functional is equivalent to a
ﬁrst-order approximation of a Bayesian a-posteriori maximization based on local
region statistics. In particular, it is the approximation of the Bayesian setting
Statistical Interpretation of the Mumford-Shah Functional
with a Gaussian distribution, ﬁxed standard deviation σ =
0.5, and a Gaussian
windowing function where ρ =
2λ and νB = ν.
What is the eﬀect of neglecting the higher order terms, as done by the
Mumford-Shah functional? The main eﬀect is that the minimizers μi of the
functional in (23) are less smooth than those of the functional in (22). Figure 1
depicts a comparison in case of the whole image domain being a single region.
Obviously, the visual diﬀerence is almost negligible, and it can be further reduced by choosing λ in the ﬁrst-order approximation slightly larger than in the
regularizer containing the inﬁnite sum of penalizers.
Fig. 1. Comparison of regularization with and without higher order penalizers. Left:
Original image. Center: Smoothing result with the regularizer in (22) (Gaussian
smoothing) for λ = 20. Right: Smoothing results with the regularizer in (23) for
Extending the Mumford-Shah Functional
In the previous section, we have shown that the full, piecewise smooth version of
the Mumford-Shah functional is a ﬁrst-order approximation of a Bayesian segmentation approach assuming local Gaussian distributions with a ﬁxed standard
deviation. In this section, we will make use of this relation in order to extend
the Mumford-Shah functional in a way that it also takes the variance of the data
into account. In the Bayesian formulation, this is easy to achieve, as shown in
Section 3. Hence, we can take the Bayesian model and express the convolutions
by regularization formulations.
With the full Gaussian model, the probability densities
pi(s, x) =
2πσi(x) exp
(s −μi(x))2
depend on two functions μi(x) and σi(x) given by (12). For ρ →∞they are the
mean and standard deviation of I in Ωi, i.e., the minimizers of
2 log(2πσ2
|∇μi|2 + |∇σi|2
T. Brox and D. Cremers
for λ →∞. This yields a generalized cartoon model. For ρ ≪∞we make use
of the relation between Gaussian convolution and regularization stated in the
previous section and obtain μi(x) and σi(x) as the minimizers of
E(μi, σi) =
2 log(2πσ2
and the Bayesian energy can be written as
EB(μ, σ, K) =
E(μi, σi) + ν|K|.
Based on the observation in Section 4, a qualitatively similar approach is obtained by neglecting the penalizer terms with k > 1
EMS(μ, σ, K) =
2 log(2πσ2)
|∇μ|2 + |∇σ|2
dx + ν |K|,
which we may call an extended version of the Mumford-Shah functional. Main
advantage of this extension over the original Mumford-Shah functional is that
the parameter ν gets invariant with respect to the image contrast. This contrast
invariance becomes even more interesting when dealing with vector-valued input
images and estimating a separate variance for each vector channel. The inﬂuence
of each channel on the segmentation then only depends on its discriminative
properties and not on the magnitude of the channel values. This allows for the
sound integration of diﬀerent input channels with diﬀerent contrast and noise
levels. For a proof in case of a global Gaussian model we refer to . This proof
can be adopted for the local Gaussian model in a straightforward manner.
Another advantage of taking the variance into account is the possibility to
distinguish regions that are equal in their mean value, but diﬀer in the variance.
Figure 2 illustrates a result obtained with the extended Mumford-Shah functional. For the experiment we used a level set implementation and expect two
regions in the image. Our implementation is based on gradient descent and,
hence, can only ensure a local optimum that need not necessarily be the global
one. The initial contour is shown in Figure 2a. The background region of the
input image has been generated by a constant function at 127 and Gaussian
noise with standard deviation 20. The circular foreground region contains a gradient ranging from 0 to 255. Gaussian noise with standard deviation 70 has been
added to this region. The resulting contour and the local mean approximation
Statistical Interpretation of the Mumford-Shah Functional
Fig. 2. Example for two regions. Top left: (a) Original image of size 162 × 171 pixels
with the initial contour. Top right: (b) Contour obtained with the extended Mumford-
Shah functional in (28). Bottom left: (c) Approximated mean μ. Bottom right: (d)
Contour obtained with the original Mumford-Shah functional.
for λ = 32 and ν = 32 are shown in Figure 2b and Figure 2c, respectively.
For comparison, we depict in Figure 2d the contour found with the same implementation but the standard deviation set ﬁxed, i.e., the original Mumford-Shah
functional. For this case, the parameter ν had to be increased to ν = 1000 to
obtain reasonable results. Since the two regions have diﬀerent variances, which
can only be exploited by the extended Mumford-Shah functional, the extension
ﬁnds a more attractive solution than the original version. Larger ν in the original
Mumford-Shah functional cannot improve the quality of the result as they lead
to an over-smoothed contour not capturing the full circle anymore.
T. Brox and D. Cremers
We have provided a statistical interpretation of the Mumford-Shah functional
with piecewise smooth regions by showing its relations to Bayesian image segmentation with local region statistics. The link has been established by means
of a theorem that relates Gaussian convolution to a regularization problem with
an inﬁnite sum of penalizers of arbitrarily high order. Based on this relation, we
showed that the Mumford-Shah functional is equivalent to a ﬁrst-order approximation of a Bayesian approach with Gaussian probability densities estimated
with a Gaussian windowing function and the standard deviation set ﬁxed. By
means of this relation, we derived an extended version of the Mumford-Shah
functional from the Bayesian model which includes the standard deviation as a
spatially varying, dynamic function.