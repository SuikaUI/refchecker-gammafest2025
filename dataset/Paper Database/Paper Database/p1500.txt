HAL Id: hal-01109581
 
Submitted on 29 Jan 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Hybrid Simplification using Deep Semantics and
Machine Translation
Shashi Narayan, Claire Gardent
To cite this version:
Shashi Narayan, Claire Gardent. Hybrid Simplification using Deep Semantics and Machine Translation. the 52nd Annual Meeting of the Association for Computational Linguistics, ACL, Jun 2014,
Baltimore, United States. pp.435 - 445. ￿hal-01109581￿
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435–445,
Baltimore, Maryland, USA, June 23-25 2014. c⃝2014 Association for Computational Linguistics
Hybrid Simpliﬁcation using Deep Semantics and Machine Translation
Shashi Narayan
Universit´e de Lorraine, LORIA
Villers-l`es-Nancy, F-54600, France
 
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-l`es-Nancy, F-54500, France
 
We present a hybrid approach to sentence
simpliﬁcation which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways.
is semantic based in that it takes as input a deep semantic representation rather
than e.g., a sentence or a parse tree. Second, it combines a simpliﬁcation model
for splitting and deletion with a monolingual translation model for phrase substitution and reordering.
When compared
against current state of the art methods,
our model yields signiﬁcantly simpler output that is both grammatical and meaning
preserving.
Introduction
Sentence simpliﬁcation maps a sentence to a simpler, more readable one approximating its content. Typically, a simpliﬁed sentence differs from
a complex one in that it involves simpler, more
usual and often shorter, words (e.g., use instead
of exploit); simpler syntactic constructions (e.g.,
no relative clauses or apposition); and fewer modiﬁers (e.g., He slept vs. He also slept). In practice, simpliﬁcation is thus often modeled using
four main operations: splitting a complex sentence into several simpler sentences; dropping and
reordering phrases or constituents; substituting
words/phrases with simpler ones.
As has been argued in previous work, sentence
simpliﬁcation has many potential applications. It
is useful as a preprocessing step for a variety of
NLP systems such as parsers and machine translation systems , summarisation , sentence
fusion and semantic
role labelling . It also
has wide ranging potential societal application as a
reading aid for people with aphasis , for low literacy readers and for non native speakers .
Machine Translation systems have
been adapted to translate complex sentences into
simple ones .
And handcrafted
rules have been proposed to model the syntactic
transformations involved in simpliﬁcations .
In this paper, we present a hybrid approach to
sentence simpliﬁcation which departs from this
previous work in two main ways.
First, it combines a model encoding probabilities for splitting and deletion with a monolingual machine translation module which handles
reordering and substitution. In this way, we exploit the ability of statistical machine translation
(SMT) systems to capture phrasal/lexical substitution and reordering while relying on a dedicated probabilistic module to capture the splitting
and deletion operations which are less well (deletion) or not at all (splitting) captured by SMT approaches.
Second, our approach is semantic based. While
previous simpliﬁcation approaches starts from either the input sentence or its parse tree, our model
takes as input a deep semantic representation
namely, the Discourse Representation Structure
 ) assigned by Boxer to the input complex sentence. As we
shall see in Section 4, this permits a linguistically
principled account of the splitting operation in that
semantically shared elements are taken to be the
basis for splitting a complex sentence into several simpler ones; this facilitates completion (the
re-creation of the shared element in the split sentences); and this provide a natural means to avoid
deleting obligatory arguments.
When compared against current state of the art
methods , our model yields signiﬁcantly simpler output that is both grammatical
and meaning preserving.
Related Work
Earlier work on sentence simpliﬁcation relied
on handcrafted rules to capture syntactic simpliﬁcation e.g., to split coordinated and subordinated sentences into several, simpler clauses
or to model active/passive transformations .
While these handcrafted approaches can encode precise and linguistically well-informed syntactic transformation (using e.g., detailed morphological and syntactic information), they are limited in scope to purely syntactic rules and do not account for lexical simpli-
ﬁcations and their interaction with the sentential
Using the parallel dataset formed by Simple English Wikipedia (SWKP)1 and traditional English
Wikipedia (EWKP)2, more recent work has focused on developing machine learning approaches
to sentence simpliﬁcation.
Zhu et al. constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple
sentences by aligning sentences from EWKP and
SWKP and used the resulting bitext to train a simpliﬁcation model inspired by syntax-based machine translation .
Their simpliﬁcation model encodes the probabilities for four rewriting operations on the parse
tree of an input sentences namely, substitution, reordering, splitting and deletion.
It is combined
with a language model to improve grammaticality and the decoder translates sentences into sim-
1SWKP ( is a
corpus of simple texts targeting “children and adults who are
learning English Language” and whose authors are requested
to “use easy words and short sentences”.
2 
pler ones by greedily selecting the output sentence
with highest probability.
Using both the PWKP corpus developed by
Zhu et al. and the edit history of Simple
Wikipedia, Woodsend and Lapata learn a
quasi synchronous grammar describing a loose alignment between parse
trees of complex and of simple sentences. Following Dras , they then generate all possible rewrites for a source tree and use integer linear programming to select the most appropriate
simpliﬁcation. They evaluate their model on the
same dataset used by Zhu et al. namely,
an aligned corpus of 100/131 EWKP/SWKP sentences and show that they achieve better BLEU
score. They also conducted a human evaluation
on 64 of the 100 test sentences and showed again
a better performance in terms of simplicity, grammaticality and meaning preservation.
In , simpliﬁcation is viewed as a monolingual
translation task where the complex sentence is the
source and the simpler one is the target. To account for deletions, reordering and substitution,
Coster and Kauchak trained a phrase based
machine translation system on the PWKP corpus
while modifying the word alignment output by
GIZA++ in Moses to allow for null phrasal alignments. In this way, they allow for phrases to be
deleted during translation. No human evaluation
is provided but the approach is shown to result in
statistically signiﬁcant improvements over a traditional phrase based approach. Similarly, Wubben
et al. use Moses and the PWKP data to train
a phrase based machine translation system augmented with a post-hoc reranking procedure designed to rank the output based on their dissimilarity from the source. A human evaluation on
20 sentences randomly selected from the test data
indicates that, in terms of ﬂuency and adequacy,
their system is judged to outperform both Zhu et
al. and Woodsend and Lapata systems.
Simpliﬁcation Framework
We start by motivating our approach and explaining how it relates to previous proposals w.r.t.,
the four main operations involved in simpliﬁcation namely, splitting, deletion, substitution and
reordering. We then introduce our framework.
Sentence Splitting.
Sentence splitting is arguably semantic based in that in many cases, splitting occurs when the same semantic entity participates in two distinct eventualities. For instance, in
example (1) below, the split is on the noun bricks
which is involved in two eventualities namely,
“being resistant to cold” and “enabling the construction of permanent buildings”.
(1) C. Being more resistant to cold, bricks enabled the construction of permanent buildings.
S. Bricks were more resistant to cold. Bricks enabled
the construction of permanent buildings.
While splitting opportunities have a clear counterpart in syntax (i.e., splitting often occurs whenever a relative, a subordinate or an appositive
clause occurs in the complex sentence), completion i.e., the reconstruction of the shared element
in the second simpler clause, is arguably semantically governed in that the reconstructed element
corefers with its matching phrase in the ﬁrst simpler clause. While our semantic based approach
naturally accounts for this by copying the phrase
corresponding to the shared entity in both phrases,
syntax based approach such as Zhu et al. 
and Woodsend and Lapata will often fail to
appropriately reconstruct the shared phrase and introduce agreement mismatches because the alignment or rules they learn are based on syntax alone.
For instance, in example (2), Zhu et al. 
fails to copy the shared argument “The judge” to
the second clause whereas Woodsend and Lapata
 learns a synchronous rule matching (VP
and VP) to (VP. NP(It) VP) thereby failing to produce the correct subject pronoun (“he” or “she”)
for the antecedent “The judge”.
(2) C. The judge ordered that Chapman should receive
psychiatric treatment in prison and sentenced him to
twenty years to life.
S1. The judge ordered that Chapman should get psychiatric treatment. In prison and sentenced him to twenty
years to life. 
S2. The judge ordered that Chapman should receive
psychiatric treatment in prison.
It sentenced him to
twenty years to life. 
By handling deletion using a probabilistic model trained on semantic representations,
we can avoid deleting obligatory arguments. Thus
in our approach, semantic subformulae which are
related to a predicate by a core thematic roles (e.g.,
agent and patient) are never considered for deletion. By contrast, syntax based approaches do not
distinguish between optional and obligatory arguments. For instance Zhu et al. simpliﬁes
(3C) to (3S) thereby incorrectly deleting the obligatory theme (gifts) of the complex sentence and
modifying its meaning to giving knights and warriors (instead of giving gifts to knights and warriors).
(3) C. Women would also often give knights and warriors
gifts that included thyme leaves as it was believed to
bring courage to the bearer.
S. Women also often give knights and warriors. Gifts
included thyme leaves as it was thought to bring
courage to the saint. 
We also depart from Coster and Kauchak 
who rely on null phrasal alignments for deletion
during phrase based machine translation. In their
approach, deletion is constrained by the training
data and the possible alignments, independent of
any linguistic knowledge.
Substitution and Reordering
SMT based approaches to paraphrasing and to
sentence simpliﬁcation have
shown that by utilising knowledge about alignment and translation probabilities, SMT systems
can account for the substitutions and the reorderings occurring in sentence simpliﬁcation.
Following on these approaches, we therefore rely on
phrase based SMT to learn substitutions and reordering. In addition, the language model we integrate in the SMT module helps ensuring better
ﬂuency and grammaticality.
An Example
Figure 1 shows how our approach simpliﬁes (4C)
into (4S).
(4) C. In 1964 Peter Higgs published his second paper in
Physical Review Letters describing Higgs mechanism
which predicted a new massive spin-zero boson for the
ﬁrst time.
S. Peter Higgs wrote his paper explaining Higgs mechanism in 1964. Higgs mechanism predicted a new elementary particle.
The DRS for (4C) produced using Boxer is shown at the top of the Figure
and a graph representation3 of the dependencies
between its variables is shown immediately below.
Each DRS variable labels a node in the graph and
each edge is labelled with the relation holding between the variables labelling its end vertices. The
3The DRS to graph conversion goes through several preprocessing steps: the relation nn is inverted making modi-
ﬁer noun (higgs) dependent of modiﬁed noun (mechanism),
named and timex are converted to unary predicates, e.g.,
named(x,peter) is mapped to peter(x) and timex(x) =
1964 is mapped to 1964(x); and nodes are introduced for
orphan words (e.g., which).
named(X0, higgs, per)
named(X0, peter, per)
second(X2)
of(X2, X1)
publish(X3)
agent(X3, X0)
patient(X3, X2)
named(X4, physical, org)
named(X4, review, org)
named(X4, letters, org)
in(X3, X4)
in(X3, X5)
timex(X5) = 1964
mechanism(X8)
nn(X7, X8)
named(X7, higgs, org)
X9, X10, X11, X12
massive(X9)
spin-zero(X9)
predict(X10)
event(X10)
describe(X11)
event(X11)
agent(X10, X8)
patient(X10, X9)
agent(X11, X6)
patient(X11, X8)
for(X10, X12)
[Discourse Representation Structure produced by BOXER]
[DRS Graph Representation]
X12 24, 25, 26
ﬁrst/a, time/n
describe/v, event
predict/v, event
18, 19, 20
new/a, spin-zero/a
massive/a, boson/n
mechanism/n
thing/n, 1964
X4 10, 11, 12
physical/org
review/org, letters/org
publish/v, event
second/a, paper/a
higgs/per, peter/per
node pos. in S
predicate/type
for, X10 →X12
patient, X10 →X9
agent,X10 →X8
nn, X8 →X7
patient, X11 →X8
agent,X11 →X6
in, X3 →X5
in, X3 →X4
of, X2 →X1
patient, X3 →X2
agent,X3 →X0
wwwwSPLIT
In 1964 Peter Higgs published his
paper describing Higgs mechanism
Higgs mechanism predicted
a new boson
wwwwDELETION
Peter Higgs wrote his paper explaining
Higgs mechanism in 1964 .
Higgs mechanism predicted
a new elementary particle .
wwwwPBMT+LM
Figure 1: Simpliﬁcation of “In 1964 Peter Higgs published his second paper in Physical Review Letters
describing Higgs mechanism which predicted a new massive spin-zero boson for the ﬁrst time .”
two tables to the right of the picture show the predicates (top table) associated with each variable and
the relation label (bottom table) associated with
each edge. Boxer also outputs the associated positions in the complex sentence for each predicate
(not shown in the DRS but in the graph tables). Orphan words (OW) i.e., words which have no corresponding material in the DRS (e.g., which at position 16), are added to the graph (node O1) thus
ensuring that the position set associated with the
graph exactly matches the positions in the input
sentence and thus deriving the input sentence.
Split Candidate
(agent, for, patient) - (agent, in, in,
Table 1: Simpliﬁcation: SPLIT
Given the input DRS shown in Figure 1, simpli-
ﬁcation proceeds as follows.
Splitting. The splitting candidates of a DRS are
event pairs contained in that DRS. More precisely,
the splitting candidates are pairs4 of event variables associated with at least one of the core thematic roles (e.g., agent and patient). The features
conditioning a split are the set of thematic roles associated with each event variable. The DRS shown
in Figure 1 contains three such event variables
X3, X11 and X10 with associated thematic role
sets {agent, in, in, patient}, {agent, patient} and
{agent, for, patient} respectively. Hence, there are
3 splitting candidates (X3-X11, X3-X10 and X10-
X11) and 4 split options: no split or split at one of
the splitting candidates. Here the split with highest
probability (cf. Table 1) is chosen and the DRS is
split into two sub-DRS, one containing X3, and
the other containing X10.
After splitting, dangling subgraphs are attached to the root of the new
subgraph maximizing either proximity or position
overlap. Here the graph rooted in X11 is attached
to the root dominating X3 and the orphan word O1
to the root dominating X10.
Deletion. The deletion model (cf. Table 2) regulates the deletion of relations and their associated
subgraph; of adjectives and adverbs; and of orphan
words. Here, the relations in between X3 and X4
and for between X10 and X12 are deleted resulting
in the deletion of the phrases “in Physical Review
Letters” and “for the ﬁrst time” as well as the ad-
4The splitting candidates could be sets of event variables
depending on the number of splits required. Here, we consider pairs for 2 splits.
jectives second, massive, spin-zero and the orphan
word which.
Substitution and Reordering. Finally the translation and language model ensures that published,
describing and boson are simpliﬁed to wrote, explaining and elementary particle respectively; and
that the phrase “In 1964” is moved from the beginning of the sentence to its end.
The Simpliﬁcation Model
Our simpliﬁcation framework consists of a probabilistic model for splitting and dropping which
we call DRS simpliﬁcation model (DRS-SM); a
phrase based translation model for substitution
and reordering (PBMT); and a language model
learned on Simple English Wikipedia (LM) for
ﬂuency and grammaticality. Given a complex sentence c, we split the simpliﬁcation process into
two steps. First, DRS-SM is applied to Dc (the
DRS representation of the complex sentence c)
to produce one or more (in case of splitting) intermediate simpliﬁed sentence(s) s′. Second, the
simpliﬁed sentence(s) s′ is further simpliﬁed to s
using a phrase based machine translation system
(PBMT+LM). Hence, our model can be formally
deﬁned as:
ˆs = argmax
p(s′|c)p(s|s′)
p(s′|Dc)p(s′|s)p(s)
where the probabilities p(s′|Dc), p(s′|s) and
p(s) are given by the DRS simpliﬁcation model,
the phrase based machine translation model and
the language model respectively.
To get the DRS simpliﬁcation model, we combine the probability of splitting with the probability of deletion:
p(s′|Dc) =
θ:str(θ(Dc))=s′
p(Dsplit|Dc)p(Ddel|Dsplit)
where θ is a sequence of simpliﬁcation operations and str(θ(Dc)) is the sequence of words associated with a DRS resulting from simplifying Dc
The probability of a splitting operation for a
given DRS Dc is:
p(Dsplit|Dc) =
SPLIT(sptrue
split at spcand
SPLIT(spfalse
relation candidate
mod. cand.
OW candidate
isBoundary
Table 2: Simpliﬁcation: DELETION (Relations, modiﬁers and OW respectively)
That is, if the DRS is split on the splitting candidate spcand, the probability of the split is then given
by the SPLIT table (Table 1) for the isSplit value
“true” and the split candidate spcand; else it is the
product of the probability given by the SPLIT table
for the isSplit value “false” for all split candidate
considered for Dc. As mentioned above, the features used for determining the split operation are
the role sets associated with pairs of event variables (cf. Table 1).
The deletion probability is given by three models: a model for relations determining the deletion
of prepositional phrases; a model for modiﬁers
(adjectives and adverbs) and a model for orphan
words (Table 2). All three deletion models use the
associated word itself as a feature. In addition, the
model for relations uses the PP length-range as a
feature while the model for orphan words relies on
boundary information i.e., whether or not, the OW
occurs at the associated sentence boundary.
p(Ddel|Dsplit) =
DELrel(relcand)
DELmod(modcand)
DELow(owcand)
Estimating the parameters
We use the EM algorithm 
to estimate our split and deletion model parameters. For an efﬁcient implementation of EM algorithm, we follow the work of Yamada and Knight
 and Zhu et al. ; and build training
graphs (Figure 2) from the pair of complex and
simple sentence pairs in the training data.
Each training graph represents a complexsimple sentence pair and consists of two types
of nodes: major nodes (M-nodes) and operation
nodes (O-nodes). An M-node contains the DRS
representation Dc of a complex sentence c and the
associated simple sentence(s) si while O-nodes
determine split and deletion operations on their
parent M-node. Only the root M-node is considered for the split operations. For example, given
del-rel∗; del-mod∗; del-ow∗
Figure 2: An example training graph
the root M-node (Dc, (s1, s2)), multiple successful split O-nodes will be created, each one further
creating two M-nodes (Dc1, s1) and (Dc2, s2). For
the training pair (c, s), the root M-node (Dc, s) is
followed by a single split O-node producing an Mnode (Dc, s) and counting all split candidates in Dc
for failed split. The M-nodes created after split operations are then tried for multiple deletion operations of relations, modiﬁers and OW respectively.
Each deletion candidate creates a deletion O-node
marking successful or failed deletion of the candidate and a result M-node. The deletion process
continues on the result M-node until there is no
deletion candidate left to process. The governing
criteria for the construction of the training graph
is that, at each step, it tries to minimize the Levenshtein edit distance between the complex and the
simple sentences. Moreover, for the splitting operation, we introduce a split only if the reference
sentence consists of several sentences (i.e., there
is a split in the training data); and only consider
splits which maximises the overlap between split
and simple reference sentences.
We initialize our probability tables Table 1 and
Table 2 with the uniform distribution, i.e., 0.5 because all our features are binary. The EM algorithm iterates over training graphs counting model
features from O-nodes and updating our probability tables. Because of the space constraints, we
do not describe our algorithm in details. We refer
the reader to for more
Our phrase based translation model is trained
using the Moses toolkit5 with its default command
line options on the PWKP corpus (except the sentences from the test set) considering the complex
sentence as the source and the simpler one as the
target. Our trigram language model is trained using the SRILM toolkit6 on the SWKP corpus7.
Decoding. We explore the decoding graph similar to the training graph but in a greedy approach
always picking the choice with maximal probability. Given a complex input sentence c, a split Onode will be selected corresponding to the decision of whether to split and where to split. Next,
deletion O-nodes are selected indicating whether
or not to drop each of the deletion candidate. The
DRS associated with the ﬁnal M-node Dfin is then
mapped to a simpliﬁed sentence s′
fin which is
further simpliﬁed using the phrase-based machine
translation system to produce the ﬁnal simpliﬁed
sentence ssimple.
Experiments
We trained our simpliﬁcation and translation models on the PWKP corpus.
To evaluate performance, we compare our approach with three other
state of the art systems using the test set provided
by Zhu et al. and relying both on automatic
metrics and on human judgments.
Training and Test Data
The DRS-Based simpliﬁcation model is trained
on PWKP, a bi-text of complex and simple sentences provided by Zhu et al. . To construct
this bi-text, Zhu et al. extracted complex
and simple sentences from EWKP and SWKP respectively and automatically aligned them using
TF*IDF as a similarity measure. PWKP contains
108016/114924 complex/simple sentence pairs.
We tokenize PWKP using Stanford CoreNLP
We then parse all complex sentences
in PWKP using Boxer9 to produce their DRSs.
Finally, our DRS-Based simpliﬁcation model is
trained on 97.75% of PWKP; we drop out 2.25%
of the complex sentences in PWKP which are repeated in the test set or for which Boxer fails to
produce DRSs.
5 
6 
7We downloaded the snapshots of Simple Wikipedia
dated 2013-10-30 available at 
8 
9 Version 1.00
We evaluate our model on the test set used by
Zhu et al. namely, an aligned corpus of
100/131 EWKP/SWKP sentences.
Boxer produces a DRS for 96 of the 100 input sentences.
These input are simpliﬁed using our simpliﬁcation system namely, the DRS-SM model and the
phrase-based machine translation system (Section
3.2). For the remaining four complex sentences,
Boxer fails to produce DRSs.
These four sentences are directly sent to the phrase-based machine translation system to produce simpliﬁed sentences.
Automatic Evaluation Metrics
To assess and compare simpliﬁcation systems, two
main automatic metrics have been used in previous work namely, BLEU and the Flesch-Kincaid
Grade Level Index (FKG).
The FKG index is a readability metric taking
into account the average sentence length in words
and the average word length in syllables. In its
original context (language learning), it was applied to well formed text and thus measured the
simplicity of a well formed sentence. In the context of the simpliﬁcation task however, the automatically generated sentences are not necessarily
well formed so that the FKG index reduces to a
measure of the sentence length (in terms of words
and syllables) approximating the simplicity level
of an output sentence irrespective of the length
of the corresponding input. To assess simpliﬁcation, we instead use metrics that are directly related to the simpliﬁcation task namely, the number
of splits in the overall (test and training) data and
in average per sentences; the number of generated
sentences with no edits i.e., which are identical to
the original, complex one; and the average Levenshtein distance between the system’s output and
both the complex and the simple reference sentences.
BLEU gives a measure of how close a system’s
output is to the gold standard simple sentence. Because there are many possible ways of simplifying
a sentence, BLEU alone fails to correctly assess
the appropriateness of a simpliﬁcation. Moreover
BLEU does not capture the degree to which the
system’s output differs from the complex sentence
input. We therefore use BLEU as a means to evaluate how close the systems output are to the reference corpus but complement it with further manual metrics capturing other important factors when
evaluating simpliﬁcations such as the ﬂuency and
the adequacy of the output sentences and the degree to which the output sentence simpliﬁes the
Results and Discussion
Number of Splits
Table 3 shows the proportion
of input whose simpliﬁcation involved a splitting
operation. While our system splits in proportion
similar to that observed in the training data, the
other systems either split very often (80% of the
time for Zhu and 63% of the time for Woodsend)
or not at all (Wubben). In other words, when compared to the other systems, our system performs
splits in proportion closest to the reference both
in terms of total number of splits and of average
number of splits per sentence.
Total number
of sentences
average split /
Table 3: Proportion of Split Sentences (% split)
in the training/test data and in average per sentence (average split / sentence).
GOLD is the
test data with the gold standard SWKP sentences;
Zhu, Woodsend, Wubben are the best output of the
models of Zhu et al. , Woodsend and Lapata and Wubben et al. respectively;
Hybrid is our model.
Number of Edits
Table 4 indicates the edit distance of the output sentences w.r.t. both the complex and the simple reference sentences as well as
the number of input for which no simpliﬁcation
occur. The right part of the table shows that our
system generate simpliﬁcations which are closest
to the reference sentence (in terms of edits) compared to those output by the other systems.
also produces the highest number of simpliﬁcations which are identical to the reference. Conversely our system only ranks third in terms of dissimilarity with the input complex sentences (6.32
edits away from the input sentence) behind the
Woodsend (8.63 edits) and the Zhu (7.87 edits)
system. This is in part due to the difference in
splitting strategies noted above : the many splits
applied by these latter two systems correlate with
a high number of edits.
Edits (Complex
to System)
to Simple)
Table 4: Automated Metrics for Simpliﬁcation:
average Levenshtein distance (LD) to complex and
simple reference sentences per system ; number of
input sentences for which no simpliﬁcation occur
(No edit).
BLEU score
We used Moses support tools:
multi-bleu10 to calculate BLEU scores.
BLEU scores shown in Table 4 show that our system produces simpliﬁcations that are closest to the
reference.
In sum, the automatic metrics indicate that our
system produces simpliﬁcation that are consistently closest to the reference in terms of edit distance, number of splits and BLEU score.
Human Evaluation
The human evaluation was done online using the
LG-Eval toolkit 11.
evaluators were allocated a trial set using a Latin
Square Experimental Design (LSED) such that
each evaluator sees the same number of output
from each system and for each test set item. During the experiment, the evaluators were presented
with a pair of a complex and a simple sentence(s)
and asked to rate this pair w.r.t. to adequacy (Does
the simpliﬁed sentence(s) preserve the meaning
of the input?) and simpliﬁcation (Does the generated sentence(s) simplify the complex input?).
They were also asked to rate the second (simpliﬁed) sentence(s) of the pair w.r.t.
(Is the simpliﬁed output ﬂuent and grammatical?).
Similar to the Wubben’s human evaluation setup,
we randomly selected 20 complex sentences from
Zhu’s test corpus and included in the evaluation
corpus: the corresponding simple (Gold) sentence
from Zhu’s test corpus, the output of our system
(Hybrid) and the output of the other three systems (Zhu, Woodsend and Wubben) which were
provided to us by the system authors. The evaluation data thus consisted of 100 complex/simple
pairs. We collected ratings from 27 participants.
10 
11 
All were either native speakers or proﬁcient in English, having taken part in a Master taught in English or lived in an English speaking country for
an extended period of time.
Simpliﬁcation
Table 5: Average Human Ratings for simplicity,
ﬂuency and adequacy
Table 5 shows the average ratings of the human
evaluation on a slider scale from 0 to 5. Pairwise
comparisons between all models and their statistical signiﬁcance were carried out using a one-way
ANOVA with post-hoc Tukey HSD tests and are
shown in Table 6.
Table 6: ♦/♦is/not signiﬁcantly different (sig.
diff.) wrt simplicity. □/■is/not sig. diff. wrt
ﬂuency. △/▲is/not sig. diff. wrt adequacy. (signiﬁcance level: p < 0.05)
With regard to simpliﬁcation, our system ranks
ﬁrst and is very close to the manually simpli-
ﬁed input (the difference is not statistically significant). The low rating for Woodsend reﬂects the
high number of unsimpliﬁed sentences (24/100 in
the test data used for the automatic evaluation and
6/20 in the evaluation data used for human judgments). Our system data is not signiﬁcantly different from the manually simpliﬁed data for simplicity whereas all other systems are.
For ﬂuency, our system rates second behind
Wubben and before Woodsend and Zhu.
difference between our system and both Zhu
and Woodsend system is signiﬁcant.
In particular, Zhu’s output is judged less ﬂuent probably because of the many incorrect splits it licenses.
Manual examination of the data shows
that Woodsend’s system also produces incorrect
splits. For this system however, the high proportion of non simpliﬁed sentences probably counterbalances these incorrect splits, allowing for a good
ﬂuency score overall.
Regarding adequacy, our system is against closest to the reference (3.50 for our system vs.
3.66 for manual simpliﬁcation). Our system, the
Wubben system and the manual simpliﬁcations
are in the same group (the differences between
these systems are not signiﬁcant).
The Woodsend system comes second and the Zhu system
third (the difference between the two is signiﬁcant). Wubben’s high ﬂuency, high adequacy but
low simplicity could be explained with their minimal number of edit (3.33 edits) from the source
In sum, if we group together systems for which
there is no signiﬁcant difference, our system ranks
ﬁrst (together with GOLD) for simplicity; ﬁrst
for ﬂuency (together with GOLD and Wubben);
and ﬁrst for adequacy , correctly capturing the interactions between these phenomena is essential to ensuring
text cohesion.
In the future, we would like to
investigate how our framework deals with such
discourse level simpliﬁcations i.e., simpliﬁcations
which involves manipulation of the coreference
and of the discourse structure. In the PWKP data,
the proportion of split sentences is rather low (6.1
%) and many of the split sentences are simple sentence coordination splits.
A more adequate but
small corpus is that used in 
which consists of 95 cases of discourse simpliﬁcation. Using data from the language learning or the
children reading community, it would be interesting to ﬁrst construct a similar, larger scale corpus;
and to then train and test our approach on more
complex cases of sentence splitting.
Acknowledgments
We are grateful to Zhemin Zhu, Kristian Woodsend and Sander Wubben for sharing their data.
We would like to thank our annotators for participating in our human evaluation experiments and
to anonymous reviewers for their insightful comments.