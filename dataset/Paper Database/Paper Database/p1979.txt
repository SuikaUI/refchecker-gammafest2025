RippleNet: Propagating User Preferences on the Knowledge
Graph for Recommender Systems
Hongwei Wang1,2, Fuzheng Zhang3, Jialin Wang4, Miao Zhao4, Wenjie Li4, Xing Xie2, Minyi Guo1∗
1Shanghai Jiao Tong University, , 
2Microsoft Research Asia, , 3Meituan AI Lab, 
4The Hong Kong Polytechnic University, {csjlwang, csmiaozhao, cswjli}@comp.polyu.edu.hk
To address the sparsity and cold start problem of collaborative filtering, researchers usually make use of side information, such as social
networks or item attributes, to improve recommendation performance. This paper considers the knowledge graph as the source of
side information. To address the limitations of existing embeddingbased and path-based methods for knowledge-graph-aware recommendation, we propose RippleNet, an end-to-end framework that
naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the water, RippleNet stimulates the propagation of user preferences over the set
of knowledge entities by automatically and iteratively extending a
user’s potential interests along links in the knowledge graph. The
multiple "ripples" activated by a user’s historically clicked items
are thus superposed to form the preference distribution of the user
with respect to a candidate item, which could be used for predicting the final clicking probability. Through extensive experiments
on real-world datasets, we demonstrate that RippleNet achieves
substantial gains in a variety of scenarios, including movie, book
and news recommendation, over several state-of-the-art baselines.
Recommender systems; knowledge graph; preference propagation
ACM Reference Format:
Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing
Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the
Knowledge Graph for Recommender Systems. In The 27th ACM International
Conference on Information and Knowledge Management (CIKM ’18), October
22–26, 2018, Torino, Italy. ACM, New York, NY, USA, 10 pages. 
org/10.1145/3269206.3271739
INTRODUCTION
The explosive growth of online content and services has provided
overwhelming choices for users, such as news, movies, music,
restaurants, and books. Recommender systems (RS) intend to address the information explosion by finding a small set of items for
∗M. Guo is the corresponding author. This work was partially sponsored by the National
Basic Research 973 Program of China under Grant 2015CB352403.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CIKM ’18, October 22–26, 2018, Torino, Italy
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6014-2/18/10.
 
The Green Mile
the Future
Forrest Gump
Raiders of
the Lost Ark
Interstellar
collaborate
Movies the user
have watched
Knowledge Graph
Movies the user
may also like
Figure 1: Illustration of knowledge graph enhanced movie
recommender systems. The knowledge graph provides fruitful facts and connections among items, which are useful for
improving precision, diversity, and explainability of recommended results.
users to meet their personalized interests. Among recommendation strategies, collaborative filtering (CF), which considers users’
historical interactions and makes recommendations based on their
potential common preferences, has achieved great success .
However, CF-based methods usually suffer from the sparsity of
user-item interactions and the cold start problem. To address these
limitations, researchers have proposed incorporating side information into CF, such as social networks , user/item attributes ,
images and contexts .
Among various types of side information, knowledge graph (KG)
usually contains much more fruitful facts and connections about
items. A KG is a type of directed heterogeneous graph in which
nodes correspond to entities and edges correspond to relations. Recently, researchers have proposed several academic KGs, such as
NELL1, DBpedia2, and commercial KGs, such as Google Knowledge
Graph3 and Microsoft Satori4. These knowledge graphs are successfully applied in many applications such as KG completion ,
question answering , word embedding , and text classification .
Inspired by the success of applying KG in a wide variety of tasks,
researchers also tried to utilize KG to improve the performance of
recommender systems. As shown in Figure 1, KG can benefit the
recommendation from three aspects: (1) KG introduces semantic
relatedness among items, which can help find their latent connections and improve the precision of recommended items; (2) KG
1 
2 
3 
4 
 
consists of relations with various types, which is helpful for extending a user’s interests reasonably and increasing the diversity of
recommended items; (3) KG connects a user’s historical records and
the recommended ones, thereby bringing explainability to recommender systems. In general, existing KG-aware recommendation
can be classified into two categories:
The first category is embedding-based methods , which
pre-process a KG with knowledge graph embedding (KGE) algorithms and incorporates the learned entity embeddings into a
recommendation framework. For example, Deep Knowledge-aware
Network (DKN) treats entity embeddings and word embeddings as different channels, then designs a CNN framework to
combine them together for news recommendation. Collaborative
Knowledge base Embedding (CKE) combines a CF module
with knowledge embedding, text embedding, and image embedding of items in a unified Bayesian framework. Signed Heterogeneous Information Network Embedding (SHINE) designs
deep autoencoders to embed sentiment networks, social networks
and profile (knowledge) networks for celebrity recommendations.
Embedding-based methods show high flexibility in utilizing KG
to assist recommender systems, but the adopted KGE algorithms
in these methods are usually more suitable for in-graph applications such as link prediction than for recommendation , thus
the learned entity embeddings are less intuitive and effective to
characterize inter-item relations.
The second category is path-based methods , which explore the various patterns of connections among items in KG to
provide additional guidance for recommendations. For example,
Personalized Entity Recommendation (PER) and Meta-Graph
Based Recommendation treat KG as a heterogeneous information network (HIN), and extract meta-path/meta-graph based
latent features to represent the connectivity between users and
items along different types of relation paths/graphs. Path-based
methods make use of KG in a more natural and intuitive way, but
they rely heavily on manually designed meta-paths, which is hard
to optimize in practice. Another concern is that it is impossible
to design hand-crafted meta-paths in certain scenarios (e.g., news
recommendation) where entities and relations are not within one
To address the limitations of existing methods, we propose RippleNet, an end-to-end framework for knowledge-graph-aware recommendation. RippleNet is designed for click-through rate (CTR)
prediction, which takes a user-item pair as input and outputs the
probability of the user engaging (e.g., clicking, browsing) the item.
The key idea behind RippleNet is preference propagation: For each
user, RippleNet treats his historical interests as a seed set in the
KG, then extends the user’s interests iteratively along KG links
to discover his hierarchical potential interests with respect to a
candidate item. We analogize preference propagation with actual
ripples created by raindrops propagating on the water, in which
multiple "ripples" superpose to form a resultant preference distribution of the user over the knowledge graph. The major difference
between RippleNet and existing literature is that RippleNet combines the advantages of the above mentioned two types of methods:
(1) RippleNet incorporates the KGE methods into recommendation
naturally by preference propagation; (2) RippleNet can automatically discover possible paths from an item in a user’s history to a
candidate item, without any sort of hand-crafted design.
Empirically, we apply RippleNet to three real-world scenarios of
movie, book, and news recommendations. The experiment results
show that RippleNet achieves AUC gains of 2.0% to 40.6%, 2.5%
to 17.4%, and 2.6% to 22.4% in movie, book, and news recommendations, respectively, compared with state-of-the-art baselines for
recommendation. We also find that RippleNet provides a new perspective of explainability for the recommended results in terms of
the knowledge graph.
In summary, our contributions in this paper are as follows:
• To the best of our knowledge, this is the first work to combine embedding-based and path-based methods in KG-aware
recommendation.
• We propose RippleNet, an end-to-end framework utilizing
KG to assist recommender systems. RippleNet automatically
discovers users’ hierarchical potential interests by iteratively
propagating users’ preferences in the KG.
• We conduct experiments on three real-world recommendation scenarios, and the results prove the efficacy of RippleNet
over several state-of-the-art baselines.
PROBLEM FORMULATION
The knowledge-graph-aware recommendation problem is formulated as follows. In a typical recommender system, let U = {u1,u2, ...}
and V = {v1,v2, ...} denote the sets of users and items, respectively. The user-item interaction matrix Y = {yuv |u ∈U,v ∈V}
is defined according to users’ implicit feedback, where
if interaction (u,v) is observed;
otherwise.
A value of 1 for yuv indicates there is an implicit interaction between user u and item v, such as behaviors of clicking, watching,
browsing, etc. In addition to the interaction matrix Y, we also have
a knowledge graph G available, which consists of massive entityrelation-entity triples (h,r,t). Here h ∈E, r ∈R, and t ∈E denote
the head, relation, and tail of a knowledge triple, respectively, E and
R denote the set of entities and relations in the KG. For example, the
triple (Jurassic Park, film.film.director, Steven Spielberg) states the
fact that Steven Spielberg is the director of the film "Jurassic Park".
In many recommendation scenarios, an item v ∈V may associate
with one or more entities in G. For example, the movie "Jurassic
Park" is linked with its namesake in KG, while news with title
"France’s Baby Panda Makes Public Debut" is linked with entities
"France" and "panda".
Given interaction matrix Y as well as knowledge graph G, we aim
to predict whether useru has potential interest in itemv with which
he has had no interaction before. Our goal is to learn a prediction
function ˆyuv = F (u,v; Θ), where ˆyuv denotes the probability that
user u will click item v, and Θ denotes the model parameters of
function F .
In this section, we discuss the proposed RippleNet in detail. We
also give some discussions on the model and introduce the related
The framework of RippleNet is illustrated in Figure 2. RippleNet
takes a user u and an item v as input, and outputs the predicted
probability that user u will click item v. For the input user u, his
historical set of interests Vu is treated as seeds in the KG, then
extended along links to form multiple ripple sets Sku (k = 1, 2, ...,H).
A ripple set Sku is the set of knowledge triples that are k-hop(s)
away from the seed set Vu. These ripple sets are used to interact
with the item embedding (the yellow block) iteratively for obtaining
the responses of user u with respect to item v (the green blocks),
which are then combined to form the final user embedding (the
grey block). Lastly, we use the embeddings of user u and item v
together to compute the predicted probability ˆyuv.
Ripple Set
A knowledge graph usually contains fruitful facts and connections
among entities. For example, as illustrated in Figure 3, the film
"Forrest Gump" is linked with "Robert Zemeckis" (director), "Tom
Hanks" (star), "U.S." (country) and "Drama" (genre), while "Tom
Hanks" is further linked with films "The Terminal" and "Cast Away"
which he starred in. These complicated connections in KG provide
us a deep and latent perspective to explore user preferences. For example, if a user has ever watched "Forrest Gump", he may possibly
become a fan of Tom Hanks and be interested in "The Terminal" or
"Cast Away". To characterize users’ hierarchically extended preferences in terms of KG, in RippleNet, we recursively define the set of
k-hop relevant entities for user u as follows:
Definition 1 (relevant entity). Given interaction matrix Y
and knowledge graph G, the set of k-hop relevant entities for user u
is defined as
u = {t | (h,r,t) ∈G and h ∈Ek−1
k = 1, 2, ...,H,
where E0u = Vu = {v | yuv = 1} is the set of user’s clicked items in
the past, which can be seen as the seed set of user u in KG.
Relevant entities can be regarded as natural extensions of a user’s
historical interests with respect to the KG. Given the definition of
relevant entities, we then define the k-hop ripple set of user u as
Definition 2 (ripple set). Thek-hop ripple set of useru is defined
as the set of knowledge triples starting from Ek−1
u = {(h,r,t) | (h,r,t) ∈G and h ∈Ek−1
k = 1, 2, ...,H. (3)
The word "ripple" has two meanings: (1) Analogous to real ripples
created by multiple raindrops, a user’s potential interest in entities
is activated by his historical preferences, then propagates along the
links in KG layer by layer, from near to distant. We visualize the
analogy by the concentric circles illustrated in Figure 3. (2) The
strength of a user’s potential preferences in ripple sets weakens
with the increase of the hop number k, which is similar to the
gradually attenuated amplitude of real ripples. The fading blue in
Figure 3 shows the decreasing relatedness between the center and
surrounding entities.
One concern about ripple sets is their sizes may get too large
with the increase of hop number k. To address the concern, note
that: (1) A large number of entities in a real KG are sink entities,
meaning they only have incoming links but no outgoing links, such
as "2004" and "PG-13" in Figure 3. (2) In specific recommendation
scenarios such as movie or book recommendations, relations can
be limited to scenario-related categories to reduce the size of ripple
sets and improve relevance among entities. For example, in Figure 3,
all relations are movie-related and contain the word "film" in their
names. (3) The number of maximal hop H is usually not too large in
practice, since entities that are too distant from a user’s history may
bring more noise than positive signals. We will discuss the choice
of H in the experiments part. (4) In RippleNet, we can sample a
fixed-size set of neighbors instead of using a full ripple set to further
reduce the computation overhead. Designing such samplers is an
important direction of future work, especially the non-uniform
samplers for better capturing user’s hierarchical potential interests.
Preference Propagation
Traditional CF-based methods and their variants learn latent
representations of users and items, then predict unknown ratings
by directly applying a specific function to their representations such
as inner product. In RippleNet, to model the interactions between
users and items in a more fine-grained way, we propose a preference
propagation technique to explore users’ potential interests in his
ripple sets.
As shown in Figure 2, each item v is associated with an item
embedding v ∈Rd, where d is the dimension of embeddings. Item
embedding can incorporate one-hot ID , attributes , bagof-words or context information of an item, based on the
application scenario. Given the item embedding v and the 1-hop
ripple set S1u of user u, each triple (hi,ri,ti) in S1u is assigned a
relevance probability by comparing item v to the head hi and the
relation ri in this triple:
pi = softmax
(h,r,t)∈S1u exp  vTRh ,
where Ri ∈Rd×d and hi ∈Rd are the embeddings of relationri and
head hi, respectively. The relevance probability pi can be regarded
as the similarity of item v and the entity hi measured in the space of
relation Ri. Note that it is necessary to take the embedding matrix
Ri into consideration when calculating the relevance of item v and
entity hi, since an item-entity pair may have different similarities
when measured by different relations. For example, "Forrest Gump"
and "Cast Away" are highly similar when considering their directors
or stars, but have less in common if measured by genre or writer.
After obtaining the relevance probabilities, we take the sum of
tails in S1u weighted by the corresponding relevance probabilities,
and the vector o1u is returned:
(hi,ri,ti)∈S1u piti,
where ti ∈Rd is the embedding of tail ti. Vector o1u can be seen as
the 1-order response of useru’s click history Vu with respect to item
user click
history 𝓥𝒖
propagation
propagation
probability
ripple set 𝓢𝒖𝟐
ripple set 𝓢𝒖𝟏
ripple set 𝓢𝒖𝑯
Figure 2: The overall framework of the RippleNet. It takes one user and one item as input, and outputs the predicted probability
that the user will click the item. The KGs in the upper part illustrate the corresponding ripple sets activated by the user’s click
Forrest Gump
film.director
film.genre
film.country
film.country
actor.film
The Terminal
actor.film
the Future
director.film
genre.film
film.language
Will Go On
film.music
film.rating
film.rating
film.language
film.language
Figure 3: Illustration of ripple sets of "Forrest Gump" in KG
of movies. The concentric circles denotes the ripple sets with
different hops. The fading blue indicates decreasing relatedness between the center and surrounding entities.Note that
the ripple sets of different hops are not necessarily disjoint
in practice.
v. This is similar to item-based CF methods , in which a user
is represented by his related items rather than a independent feature
vector to reduce the size of parameters. Through the operations in
Eq. (4) and Eq. (5), a user’s interests are transferred from his history
set Vu to the set of his 1-hop relevant entities E1u along the links
in S1u, which is called preference propagation in RippleNet.
Note that by replacing v with o1u in Eq. (4), we can repeat the
procedure of preference propagation to obtain user u’s 2-order
response o2u, and the procedure can be performed iteratively on
user u’s ripple sets Siu for i = 1, ...,H. Therefore, a user’s preference is propagated up to H hops away from his click history,
and we observe multiple responses of user u with different orders:
o1u, o2u, ..., oHu . The embedding of user u with respect to item v is
calculated by combining the responses of all orders:
u + ... + oH
Note that though the user response of last hop oHu contains all the
information from previous hops theoretically, it is still necessary
to incorporate oku of small hops k in calculating user embedding
since they may be diluted in oHu . Finally, the user embedding and
item embedding are combined to output the predicted clicking
probability:
ˆyuv = σ(uTv),
where σ(x) =
1+exp(−x) is the sigmoid function.
Learning Algorithm
In RippleNet, we intend to maximize the following posterior probability of model parameters Θ after observing the knowledge graph
G and the matrix of implicit feedback Y:
maxp(Θ|G, Y),
where Θ includes the embeddings of all entities, relations and items.
This is equivalent to maximizing
p(Θ|G, Y) = p(Θ, G, Y)
∝p(Θ) · p(G|Θ) · p(Y|Θ, G)
according to Bayes’ theorem. In Eq. (9), the first term p(Θ) measures
the priori probability of model parameters Θ. Following , we
set p(Θ) as Gaussian distribution with zero mean and a diagonal
covariance matrix:
p(Θ) = N(0, λ−1
The second item in Eq. (9) is the likelihood function of the observed
knowledge graph G given Θ. Recently, researchers have proposed
a great many knowledge graph embedding methods, including
translational distance models and semantic matching models
 (We will continue the discussion on KGE methods in Section
3.6.3). In RippleNet, we use a three-way tensor factorization method
to define the likelihood function for KGE:
(h,r,t)∈E×R×E p (h,r,t)|Θ
(h,r,t)∈E×R×E N  Ih,r,t −hTRt, λ−1
Algorithm 1 Learning algorithm for RippleNet
Input: Interaction matrix Y, knowledge graph G
Output: Prediction function F (u,v|Θ)
1: Initialize all parameters
2: Calculate ripple sets {Sku }H
k=1 for each user u;
3: for number of training iteration do
Sample minibatch of positive and negative interactions from
Sample minibatch of true and false triples from G;
Calculate gradients ∂L/∂V, ∂L/∂E, {∂L/∂R}r ∈R, and
{∂L/∂αi }H
i=1 on the minibatch by back-propagation according to Eq. (4)-(13);
Update V, E, {R}r∈R, and {αi }H
i=1 by gradient descent with
learning rate η;
8: end for
9: return F (u,v|Θ)
where the indicator Ih,r,t equals 1 if (h,r,t) ∈G and is 0 otherwise.
Based on the definition in Eq. (11), the scoring functions of entityentity pairs in KGE and item-entity pairs in preference propagation
can thus be unified under the same calculation model. The last
term in Eq. (9) is the likelihood function of the observed implicit
feedback given Θ and the KG, which is defined as the product of
Bernouli distributions:
p(Y|Θ, G) =
(u,v)∈Y σ(uTv)yuv ·  1 −σ(uTv)1−yuv
based on Eq. (2)–(7).
Taking the negative logarithm of Eq. (9), we have the following
loss function for RippleNet:
min L = −log  p(Y|Θ, G) · p(G|Θ) · p(Θ)
yuv logσ(uTv) + (1 −yuv) log  1 −σ(uTv)
∥Ir −ETRE∥2
where V and E are the embedding matrices for all items and entities,
respectively, Ir is the slice of the indicator tensor I in KG for relation
r, and R is the embedding matrix of relation r. In Eq. (13), The first
term measures the cross-entropy loss between ground truth of
interactions Y and predicted value by RippleNet, the second term
measures the squared error between the ground truth of the KG Ir
and the reconstructed indicator matrix ETRE, and the third term is
the regularizer for preventing over-fitting.
It is intractable to solve the above objection directly, therefore, we
employ a stochastic gradient descent (SGD) algorithm to iteratively
optimize the loss function. The learning algorithm of RippleNet is
presented in Algorithm 1. In each training iteration, to make the
computation more efficient, we randomly sample a minibatch of
positive/negative interactions from Y and true/false triples from G
following the negative sampling strategy in . Then we calculate
the gradients of the loss L with respect to model parameters Θ, and
update all parameters by back-propagation based on the sampled
minibatch. We will discuss the choice of hyper-parameters in the
experiments section.
Discussion
Explainability. Explainable recommender systems aim
to reveal why a user might like a particular item, which helps improve their acceptance or satisfaction of recommendations and
increase trust in RS. The explanations are usually based on community tags , social networks , aspect , and phrase sentiment Since RippleNet explores users’ interests based on the
KG, it provides a new point of view of explainability by tracking
the paths from a user’s history to an item with high relevance
probability (Eq. (4)) in the KG. For example, a user’s interest in film
"Back to the Future" might be explained by the path "user
Forrest Gump
directed by
−−−−−−−−−−−→Robert Zemeckis
−−−−−−−→Back to the
Future", if the item "Back to the Future" is of high relevance probability with "Forrest Gump" and "Robert Zemeckis" in the user’s 1-hop
and 2-hop ripple set, respectively. Note that different from pathbased methods where the patterns of path are manually
designed, RippleNet automatically discovers the possible explanation paths according to relevance probability. We will further
present a visualized example in the experiments section to intuitively demonstrate the explainability of RippleNet.
Ripple Superposition. A common phenomenon in RippleNet
is that a user’s ripple sets may be large in size, which dilutes his
potential interests inevitably in preference propagation. However,
we observe that relevant entities of different items in a user’s click
history often highly overlap. In other words, an entity could be
reached by multiple paths in the KG starting from a user’s click
history. For example, "Saving Private Ryan" is connected to a user
who has watched "The Terminal", "Jurassic Park" and "Braveheart"
through actor "Tom Hanks", director "Steven Spielberg" and genre
"War", respectively. These parallel paths greatly increase a user’s
interests in overlapped entities. We refer to the case as ripple superposition, as it is analogous to the interference phenomenon in
physics in which two waves superpose to form a resultant wave of
greater amplitude in particular areas. The phenomenon of ripple
superposition is illustrated in the second KG in Figure 2, where the
darker red around the two lower middle entities indicates higher
strength of the user’s possible interests. We will also discuss ripple
superposition in the experiments section.
Links to Existing Work
Here we continue our discussion on related work and make comparisons with existing techniques in a greater scope.
Attention Mechanism. The attention mechanism was originally proposed in image classification and machine translation
 , which aims to learn where to find the most relevant part of
the input automatically as it is performing the task. The idea was
soon transplanted to recommender systems . For
example, DADM considers factors of specialty and date when
assigning attention values to articles for recommendation; D-Attn
 proposes an interpretable and dual attention-based CNN model
that combines review text and ratings for product rating prediction; DKN uses an attention network to calculate the weight
between a user’s clicked item and a candidate item to dynamically
aggregate the user’s historical interests. RippleNet can be viewed
as a special case of attention where tails are averaged weighted by
similarities between their associated heads, tails, and certain item.
The difference between our work and literature is that RippleNet
designs a multi-level attention module based on knowledge triples
for preference propagation.
Memory Networks. Memory networks is a recurrent attention model that utilizes an external memory module for
question answering and language modeling. The iterative reading
operations on the external memory enable memory networks to
extract long-distance dependency in texts. Researchers have also
proposed using memory networks in other tasks such as sentiment
classification and recommendation . Note that these
works usually focus on entry-level or sentence-level memories,
while our work addresses entity-level connections in the KG, which
is more fine-grained and intuitive when performing multi-hop iterations. In addition, our work also incorporates a KGE term as a
regularizer for more stable and effective learning.
Knowledge Graph Embedding. RippleNet also connects to a
large body of work in KGE methods .
KGE intends to embed entities and relations in a KG into continuous vector spaces while preserving its inherent structure. Readers
can refer to for a more comprehensive survey. KGE methods
are mainly classified into two categories: (1) Translational distance
models, such as TransE , TransH , TransD , and TransR
 , exploit distance-based scoring functions when learning representations of entities and relations. For example, TransE wants
h+r ≈t when (h,r,t) holds, where h, r and t are the corresponding
representation vector of h, r and t. Therefore, TransE assumes the
score function fr (h,t) = ∥h+r−t∥2
2 is low if (h,r,t) holds, and high
otherwise. (2) Semantic matching models, such as ANALOGY ,
ComplEx , and DisMult , measure plausibility of knowledge triples by matching latent semantics of entities and relations.
For example, DisMult introduces a vector embedding r ∈Rd
and requires Mr = diag(r). The scoring function is hence defined
as fr (h,t) = h⊤diag(r)t = Íd
i=1[r]i · [h]i · [t]i. Researchers also
propose incorporating auxiliary information, such as entity types
 , logic rules , and textual descriptions to assist KGE.
However, these methods are more suitable for in-graph applications
such as link prediction or triple classification, according to their
learning objectives. From this point of view, RippleNet can be seen
as a specially designed KGE method that serves recommendation
EXPERIMENTS
In this section, we evaluate RippleNet on three real-world scenarios:
movie, book, and news recommendations 5. We first introduce
the datasets, baselines, and experiment setup, then present the
experiment results. We will also give a case study of visualization
and discuss the choice of hyper-parameters in this section.
We utilize the following three datasets in our experiments for movie,
book, and news recommendation:
5Experiment code is provided at 
Table 1: Basic statistics of the three datasets.
MovieLens-1M
Book-Crossing
# interactions
# 1-hop triples
# 2-hop triples
# 3-hop triples
# 4-hop triples
Table 2: Hyper-parameter settings for the three datasets.
MovieLens-1M
d = 16, H = 2, λ1 = 10−7, λ2 = 0.01, η = 0.02
Book-Crossing
d = 4, H = 3, λ1 = 10−5, λ2 = 0.01, η = 0.001
d = 32, H = 3, λ1 = 10−5, λ2 = 0.05, η = 0.005
• MovieLens-1M6 is a widely used benchmark dataset in movie
recommendations, which consists of approximately 1 million explicit ratings (ranging from 1 to 5) on the MovieLens
• Book-Crossing dataset7 contains 1,149,780 explicit ratings
(ranging from 0 to 10) of books in the Book-Crossing community.
• Bing-News dataset contains 1,025,192 pieces of implicit feedback collected from the server logs of Bing News8 from
October 16, 2016 to August 11, 2017. Each piece of news has
a title and a snippet.
Since MovieLens-1M and Book-Crossing are explicit feedback
data, we transform them into implicit feedback where each entry
is marked with 1 indicating that the user has rated the item (the
threshold of rating is 4 for MovieLens-1M, while no threshold is set
for Book-Crossing due to its sparsity), and sample an unwatched
set marked as 0 for each user, which is of equal size with the rated
ones. For MovieLens-1M and Book-Crossing, we use the ID embeddings of users and items as raw input, while for Bing-News, we
concatenate the ID embedding of a piece of news and the averaged
word embedding of its title as raw input for the item, since news
titles are typically much longer than names of movies or books,
hence providing more useful information for recommendation.
We use Microsoft Satori to construct the knowledge graph for
each dataset. For MovieLens-1M and Book-Crossing, we first select
a subset of triples from the whole KG whose relation name contains
"movie" or "book" and the confidence level is greater than 0.9. Given
the sub-KG, we collect IDs of all valid movies/books by matching
their names with tail of triples (head, film.film.name, tail) or (head,
book.book.title, tail). For simplicity, items with no matched or multiple matched entities are excluded. We then match the IDs with
the head and tail of all KG triples, select all well-matched triples
from the sub-KG, and extend the set of entities iteratively up to
four hops. The constructing process is similar for Bing-News except
that: (1) we use entity linking tools to extract entities in news titles;
(2) we do not impose restrictions on the names of relations since
the entities in news titles are not within one particular domain. The
basic statistics of the three datasets are presented in Table 1.
6 
7 
8 
# common k-hop neighbors
with common rater(s)
without common rater
(a) MovieLens-1M
# common k-hop neighbors
with common rater(s)
without common rater
(b) Book-Crossing
# common k-hop neighbors
with common rater(s)
without common rater
(c) Bing-News
MovieLens-1M
Book-Crossing
(d) Ratio of two average numbers
Figure 4: The average number of k-hop neighbors that two
items share in the KG w.r.t. whether they have common
raters in (a) MovieLens-1M, (b) Book-Crossing, and (c) Bing-
News datasets. (d) The ratio of the two average numbers with
different hops.
We compare the proposed RippleNet with the following state-ofthe-art baselines:
• CKE combines CF with structural knowledge, textual
knowledge, and visual knowledge in a unified framework for
recommendation. We implement CKE as CF plus structural
knowledge module in this paper.
• SHINE designs deep autoencoders to embed a sentiment network, social network, and profile (knowledge) network for celebrity recommendation. Here we use autoencoders for user-item interaction and item profile to predict
click probability.
• DKN treats entity embedding and word embedding as
multiple channels and combines them together in CNN for
CTR prediction. In this paper, we use movie/book names and
news titles as textual input for DKN.
• PER treats the KG as HIN and extracts meta-path based
features to represent the connectivity between users and
items. In this paper, we use all item-attribute-item features
for PER (e.g., “movie-director-movie").
• LibFM is a widely used feature-based factorization
model in CTR scenarios. We concatenate user ID, item ID,
and the corresponding averaged entity embeddings learned
from TransR as input for LibFM.
• Wide&Deep is a general deep model for recommendation combining a (wide) linear channel with a (deep) nonlinear channel. Similar to LibFM, we use the embeddings of
users, items, and entities to feed Wide&Deep.
Experiment Setup
In RippleNet, we set the hop numberH = 2 for MovieLens-1M/Book-
Crossing and H = 3 for Bing-News. A larger number of hops
hardly improves performance but does incur heavier computational overhead according to experiment results. The complete
hyper-parameter settings are given in Table 2, where d denotes the
dimension of embedding for items and the knowledge graph, and η
denotes the learning rate. The hyper-parameters are determined
by optimizing AUC on a validation set. For fair consideration, the
latent dimensions of all compared baselines are set the same as in
Table 2, while other hyper-parameters of baselines are set based on
grid search.
For each dataset, the ratio of training, evaluation, and test set
is 6 : 2 : 2. Each experiment is repeated 5 times, and the average
performance is reported. We evaluate our method in two experiment scenarios: (1) In click-through rate (CTR) prediction, we apply
the trained model to each piece of interactions in the test set and
output the predicted click probability. We use Accuracy and AUC
to evaluate the performance of CTR prediction. (2) In top-K recommendation, we use the trained model to select K items with highest
predicted click probability for each user in the test set, and choose
Precision@K, Recall@K, F1@K to evaluate the recommended sets.
Empirical Study
We conduct an empirical study to investigate the correlation between the average number of common neighbors of an item pair
in the KG and whether they have common rater(s) in RS. For each
dataset, we first randomly sample one million item pairs, then count
the average number of k-hop neighbors that the two items share in
the KG under the following two circumstances: (1) the two items
have at least one common rater in RS; (2) the two items have no
common rater in RS. The results are presented in Figures 4a, 4b, 4c,
respectively, which clearly show that if two items have common
rater(s) in RS, they likely share more common k-hop neighbors in
the KG for fixed k. The above findings empirically demonstrate that
the similarity of proximity structures of two items in the KG could
assist in measuring their relatedness in RS. In addition, we plot the
ratio of the two average numbers with different hops (i.e., dividing
the higher bar by its immediate lower bar for each hop number)
in Figure 4d, from which we observe that the proximity structures
of two items under the two circumstances become more similar
with the increase of the hop number. This is because any two items
are probable to share a large amount of k-hop neighbors in the KG
for a large k, even if there is no direct similarity between them in
reality. The result motivates us to find a moderate hop number in
RippleNet to explore users’ potential interests as far as possible
while avoiding introducing too much noise.
The results of all methods in CTR prediction and top-K recommendation are presented in Table 3 and Figures 5, 6, 7, respectively.
Several observations stand out:
• CKE performs comparably poorly than other baselines, which
is probably because we only have structural knowledge available, without visual and textual input.
• SHINE performs better in movie and book recommendation
than news. This is because the 1-hop triples for news are too
complicated when taken as profile input.
Precision@K
(a) Precision@K
(b) Recall@K
Figure 5: Precision@K, Recall@K, and F1@K in top-K recommendation for MovieLens-1M.
Precision@K
(a) Precision@K
(b) Recall@K
Figure 6: Precision@K, Recall@K, and F1@K in top-K recommendation for Book-Crossing.
Precision@K
(a) Precision@K
(b) Recall@K
Figure 7: Precision@K, Recall@K, and F1@K in top-K recommendation for Bing-News.
Table 3: The results of AUC and Accuracy in CTR prediction.
MovieLens-1M
Book-Crossing
RippleNet*
* Statistically significant improvements by unpaired two-sample t-test with p = 0.1.
• DKN performs best in news recommendation compared with
other baselines, but performs worst in movie and book recommendation. This is because movie and book names are
too short and ambiguous to provide useful information.
• PER performs unsatisfactorily on movie and book recommendation because the user-defined meta-paths can hardly
be optimal. In addition, it cannot be applied in news recommendation since the types of entities and relations involved
in news are too complicated to pre-define meta-paths.
• As two generic recommendation tools, LibFM and Wide&Deep
achieve satisfactory performance, demonstrating that they
can make well use of knowledge from KG into their algorithms.
• RippleNet performs best among all methods in the three
datasets. Specifically, RippleNet outperforms baselines by
2.0% to 40.6%, 2.5% to 17.4%, and 2.6% to 22.4% on AUC
in movie, book, and news recommendation, respectively.
RippleNet also achieves outstanding performance in top-K
recommendation as shown in Figures 5, 6, and 7. Note that
the performance of top-K recommendation is much lower
for Bing-News because the number of news is significantly
larger than movies and books.
Size of ripple set in each hop. We vary the size of a user’s ripple
set in each hop to further investigate the robustness of RippleNet.
The results of AUC on the three datasets are presented in Table 4,
Table 4: The results of AUC w.r.t. different sizes of a user’s
ripple set.
Size of ripple set
MovieLens-1M
Book-Crossing
Table 5: The results of AUC w.r.t. different hop numbers.
Hop number H
MovieLens-1M
Book-Crossing
Click history:
1. Family of Navy SEAL Trainee Who Died During Pool Exercise Plans to Take Legal Action
2. North Korea Vows to Strengthen Nuclear Weapons
3. North Korea Threatens ‘Toughest Counteraction’ After U.S. Moves Navy Ships
4. Consumer Reports Pulls Recommendation for Microsoft Surface Laptops
North Korea
Candidate news: Trump Announces Gunman Dead, Credits ‘Heroic Actions’ of Police
World War II
Franklin D.
Democratic
Republican
Figure 8: Visualization of relevance probabilities for a randomly sampled user w.r.t. a piece of candidate news with label 1. Links with value lower than −1.0 are omitted.
from which we observe that with the increase of the size of ripple
set, the performance of RippleNet is improved at first because a
larger ripple set can encode more knowledge from the KG. But
notice that the performance drops when the size is too large. In
general, a size of 16 or 32 is enough for most datasets according to
the experiment results.
Hop number. We also vary the maximal hop number H to see
how performance changes in RippleNet. The results are shown in
Table 5, which shows that the best performance is achieved when
H is 2 or 3. We attribute the phenomenon to the trade-off between
the positive signals from long-distance dependency and negative
signals from noises: too small of an H can hardly explore interentity relatedness and dependency of long distance, while too large
of an H brings much more noises than useful signals, as stated in
Section 4.4.
Case Study
To intuitively demonstrate the preference propagation in RippleNet,
we randomly sample a user with 4 clicked pieces of news, and
select one candidate news from his test set with label 1. For each of
the user’s k-hop relevant entities, we calculate the (unnormalized)
relevance probability between the entity and the candidate news or
itsk-order responses. The results are presented in Figure 8, in which
MovieLens-1M
(a) Dimension of embedding
0.001 0.005 0.01
MovieLens-1M
(b) Training weight of KGE term
Figure 9: Parameter sensitivity of RippleNet.
the darker shade of blue indicates larger values, and we omit names
of relations for clearer presentation. From Figure 8 we observe that
RippleNet associates the candidate news with the user’s relevant
entities with different strengths. The candidate news can be reached
via several paths in the KG with high weights from the user’s
click history, such as "Navy SEAL"–"Special Forces"–"Gun"–"Police".
These highlighted paths automatically discovered by preference
propagation can thus be used to explain the recommendation result,
as discussed in Section 3.5.1. Additionally, it is also worth noticing
that several entities in the KG receive more intensive attention
from the user’s history, such as "U.S.", "World War II" and "Donald
Trump". These central entities result from the ripple superposition
discussed in Section 3.5.2, and can serve as the user’s potential
interests for future recommendation.
Parameter Sensitivity
In this section, we investigate the influence of parameters d and
λ2 in RippleNet. We vary d from 2 to 64 and λ2 from 0.0 to 1.0,
respectively, while keeping other parameters fixed. The results
of AUC on MovieLens-1M are presented in Figure 9. We observe
from Figure 9a that, with the increase of d, the performance is
boosted at first since embeddings with a larger dimension can
encode more useful information, but drops after d = 16 due to
possible overfitting. From Figure 9b, we can see that RippleNet
achieves the best performance when λ2 = 0.01. This is because the
KGE term with a small weight cannot provide enough regularization
constraints, while a large weight will mislead the objective function.
CONCLUSION AND FUTURE WORK
In this paper, we propose RippleNet, an end-to-end framework that
naturally incorporates the knowledge graph into recommender systems. RippleNet overcomes the limitations of existing embeddingbased and path-based KG-aware recommendation methods by introducing preference propagation, which automatically propagates
users’ potential preferences and explores their hierarchical interests in the KG. RippleNet unifies the preference propagation with
regularization of KGE in a Bayesian framework for click-through
rate prediction. We conduct extensive experiments in three recommendation scenarios. The results demonstrate the significant
superiority of RippleNet over strong baselines.
For future work, we plan to (1) further investigate the methods of
characterizing entity-relation interactions; (2) design non-uniform
samplers during preference propagation to better explore users’
potential interests and improve the performance.