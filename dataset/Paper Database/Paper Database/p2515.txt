PAQ: Time series forecasting for approximate query
answering in sensor networks
Daniela Tulone1,2 and Samuel Madden1
1 MIT Computer Science and Artiﬁcial Intelligence Laboratory
{tulone,madden}@csail.mit.edu
2 Computer Science Department, University of Pisa
Abstract. In this paper, we present a method for approximating the values
of sensors in a wireless sensor network based on time series forecasting. More
speciﬁcally, our approach relies on autoregressive models built at each sensor
to predict local readings. Nodes transmit these local models to a sink node,
which uses them to predict sensor values without directly communicating with
sensors. When needed, nodes send information about outlier readings and
model updates to the sink. We show that this approach can dramatically
reduce the amount of communication required to monitor the readings of all
sensors in a network, and demonstrate that our approach provides provablycorrect, user-controllable error bounds on the predicted values of each sensor.
Introduction
Wireless sensor networks oﬀer the potential to collect large amounts of high-ﬁdelity
information about a variety of remote locations. Recent deployments have demonstrated their utility in environmental monitoring , agriculture , and industrial
monitoring and process control . Most of these deployments have a similar character – data is collected at a regular rate to some centralized basestation (or sink),
where it is stored on disk and analyzed using conventional data processing tools (e.g.,
databases, mathematical analysis packages, and GIS software.) One major focus of
sensor network research has been on building tools to facilitate this collection of data.
Researchers have proposed a variety of abstractions to enable such applications to be
rapidly built, ranging from database query languages (as in TinyDB ) to parallel
programming systems (e.g., Regions ), to power conserving and failure-masking
network layers (e.g., Directed Diﬀusion ).
In this paper we focus on improving the performance of these data collection
applications using a probabilistic approach. More precisely, we employ a class of statistical techniques broadly known as time series forecasting. These techniques apply
to phenomena evolving over time, and use the recent history of readings to predict
the most likely future values. In this paper, we propose a general framework to ef-
ﬁciently answer queries at the sink based on a particular type of time-series model
called an autoregressive models (AR), the simplest time series model. We chose this
model because it is computationally tractable on modern-generation sensor networks
(unlike the fully general ARMA models, for example ) and because, as we show,
it can oﬀer a substantial reduction in communication and improvement in loss rates
over existing data collection approaches.
We evaluate our AR model both analytically and through simulation results that
show it can properly model physical phenomena and accurately predict future values.
We also show that it has low computational cost and memory usage, suggesting its
suitability for a wide range of hardware. Our system, called PAQ (for Probabilistic
Adaptable Query system) uses a combination of AR models to probabilistically answer
queries. The model is used both globally, at the sink, to predict the readings of
individual sensors, and locally, at each sensor, to detect when the sensor produces
outlier readings or when the model ceases to properly ﬁt the data (allowing the
sensor to re-learn the model and notify the sink of the new model parameters.) Our
approach has the following advantages over previous deterministic query systems:
– It signiﬁcantly reduces the amount of communication required to report the value
of every sensor at the sink.
– It allows the detection sensor readings that are “outliers”, in the sense that they
are not consistent with recent history or have malfunctioned.
– It is adaptive to dynamic changes in the distribution of data produced by sensors,
and tolerant of missing sensor data.
– It does not require a large amount of training data or a priori knowledge of the
probability distribution of sensor values and can work with any of the previously
mentioned abstractions (e.g., TinyDB, Directed Diﬀusion) for data collion.
There has been some recent interest in applying statistical modeling techniques to
sensor network query systems . Our approach is similar to in its general probabilistic approach: the sink answers queries within a user–speciﬁed error bound by
computing a prediction for those values without communicating with the sensors, thus
avoiding message transmissions and sensing operations. However, there are substantial diﬀerences between our approach and previous probabilistic approaches to query
answering. First, these existing approaches typically build a model centrally at the
sink, using an expensive learning phase where each sensor transmits many readings
to the sink. One reason for this is that these previous approaches have used relatively
complex probabilistic models (e.g., multi-variate Gaussians or generalized graphical models) which are too complex to build or maintain on many current classes
of sensor network hardware (e.g., Berkeley/Crossbow Motes ). These approaches
cannot adapt to changes in the underlying distribution of sensor data without re–
running this expensive learning phase. In contrast, our framework relies mostly on
local probabilistic models computed and maintained at each sensor. In order to adapt
the local model to variations in the data distribution, each sensor continuously maintains its local model, and notiﬁes the sink only of signiﬁcant changes. This allows
both the sensors and sink to adapt to changes in the underlying distribution without
the addition of a complex decision process that tries to decide when to invoke an
expensive re-training phase.
We further limit communication from the sink to each of the nodes by exploiting
data similarities between sensors that are geographically nearby. Our PAQ system
relies on geographic clusters of sensors that are similar at a given point in time and
that are computed by sensors that are near each other. Therefore, the sink maintains
only the models (coeﬃcients) of a few designated sensors, called cluster leaders, and
uses them for prediction. Before discussing the details of our algorithm, we brieﬂy
review related work.
Related Work
There has been some work on the use of probabilistic and time series models in sensor
networks. As in PAQ, rely on a combination of local and global probabilistic
models which are kept in synch to reduce communication between sensor nodes and
the network sink. For instance, Jain et al. propose a query framework based on
Kalman ﬁlters: both the sink and sensors activate a Kalman ﬁlter with user-speciﬁed
accuracy when a new query is received. However, this strategy does not support multiple queries with variable precision or clustering, and the local models do not adjust
to non–linear phenomena. Recent work by Chu et al. is similar to ours in that
it also exploits temporal/spatial correlation. However, it has a heavyweight learning
phase that does not work well for non-stationary data. Neither work provides a
provable bound on the maximum error or on the error probability of answers provided
at the sink. The snapshot queries approach proposed by Kotidis is also similar to
ours in that it exploits local models and correlations, but it provides weaker guarantees. Cheng et al. and Deshpande et al. have shown that generative-model based
approaches can signiﬁcantly reduce the communication burden in sensor networks.
However, these approaches require a relatively sophisticated user who can describe
the appropriate model for his or her domain and usually involve a complex centralized learning phase that must be re-run if the data distribution changes. In contrast,
our approach is predicated on lightweight models that can be learned by the individual nodes in the network and rapidly retrained when confronted with non-stationary
distributions.
Other work, such as the work by Olston et al. shows how to approximate
answers to queries in distributed environments with a ﬁxed bound on the error;
these approaches, though simple, have the potential to oﬀer far less reduction in
communication than model-based approaches such as ours and those discussed above.
Han et al. show how similar (non-probabilistic) techniques can be adapted to the
sensor network domain in an energy eﬃcient way.
Our approach is similar to the one proposed by Tulone for using autoregressive
models built at each sensor node to reduce communications in the context of time
syncrhonization. That work did not focus on querying or clustering issues, however.
Lazaridis and Mehrotra use a diﬀerent time-series method to create a piecewise
linear approximation of signals generated by sensors, and send those approximations
out of the network. Their approach diﬀers from ours in that they capture a large time
series and approximate it, rather than building a model that can be used for prediction
outside of the network. Other time series approximation methods, based, for instance,
on wavelets have been proposed; these too strive to reduce communication but
do not oﬀer the same predictive power as our methods. Autoregressive time series
models have been widely used outside the wireless sensor network domain as a way to
approximate and summarize time series with applications in ﬁnance, communication,
weather prediction, and a variety of other domains. Brockwell and Davis provide
an excellent introduction to time series and their applications.
Preliminaries
In this section, we introduce the notation and terminology we will use throughout
the paper, and provide an overview of time series techniques.
System model
Our network consists of a dynamic set S of sensor nodes and one or more sink
nodes. Each node is equipped with some sensing capability, performing readings on
m physical phenomena (metrics) F1, F2, . . . , Fm each of which evolves over time. For
example, we might say that F1 =temperature, and F2 =light. We assume that each
sensor performs a reading on each Fi every Γ time units. We have designed PAQ to
work with the lowest-end of today’s sensor nodes, including Berkeley Motes , with
just a few kilobytes of memory and slow, 8-bit processors without ﬂoating point or
dedicated signal processing hardware.
Queries are submitted at the sink. In this paper, we focus on queries of the form:
SELECT sensorlist WHERE P(F1, . . . , Fm) ERROR x CONFIDENCE k%.
where P(F1, . . . , Fm) is a predicate over F1, . . . , Fm consisting of atoms Fi ∈[a, b],
Fi > a, and Fi < b where a, b are user-speciﬁed. For instance, a valid predicate could
be [(F2 ∈[a, b] ∨F3 ∈[c, d]) ∧Fm > g]. Here ERROR x indicates that the user is
tolerant to a maximum absolute error in the query result, and the CONFIDENCE
clause indicates that at least k% of the readings should be within x of their true value.
For example, the user might issue the query “SELECT nodeid, temp WHERE temp
> 25◦C ERROR .1◦C CONFIDENCE 95%”, which would report the temperature at
each node to within .1◦C, a property which would be satisﬁed by 95% of the readings.
Time series forecasting
In general, a time series is a set of observations xt, each of which is recorded at time
t. An important part of the analysis of time series is the description of a suitable
uncertainty model for the data. To allow for the possibly unpredictable nature of
future observations, it is natural to suppose that each observation xt is a sample of a
random variable Xt (often denoted as X(t)).
Deﬁnition 1. A time series model for the observed data {xt} is a speciﬁcation of
the joint distributions of the random variables {Xt} of which {xt} is a sample.
Clearly, if we wish to make predictions, then we must assume that some part of this
model does not vary with time. Therefore, an important component of time series
modeling is to remove trend and seasonal components to get a (weakly) stationary
time series. Informally, a time series {Xt} is stationary if it has statistical properties
similar to those of the time–shifted series {Xt+h} for each integer h. More precisely,
{Xt} is weakly stationary if its mean function µX(t) and its covariance function
γX(t + h, t) are independent of t for each h.
Linear time series models, which includes the class of autoregressive moving–
average (ARMA) models, provide a general framework for studying stationary processes. The ARMA processes are deﬁned by linear diﬀerence equations with constant
coeﬃcients. One of the key properties is the existence and uniqueness of stationary
solutions of the deﬁning equations .
Deﬁnition 2. {Xt} is an ARMA(p, q) process if {Xt} is stationary and for all t,
Xt −φ1Xt−1 −. . . −φpXt−p = Zt + θ1Zt−1 + . . . + θqZt−q
where {Zt} ∼WN(0, σ2) and the polynomials (1 −φ1z −. . . −φpzp) and (1 −θ1z −
. . . −θqzq) have no common factors.
Here, {Zt} is a series of uncorrelated random variables, each with zero mean and σ2
variance. Such a sequence is referred as white noise and denoted by WN(0, σ2). An
autoregressive model of degree p, denoted by AR(p), is a particular type of ARMA
model with q = 0 (i.e., the right hand side of Deﬁnition 2 contains just a single term.)
Such models are simply referred to as autoregressive or AR models. We will adopt this
model to predict the value of Fi read at time t by a sensor. This choice is motivated
by its simplicity, which leads to lower computational cost and memory requirements,
making such models practical on many current-generation sensor networks. Clearly,
there is an inherent trade–oﬀbetween eﬃciency and precision since there exist more
sophisticated models (e.g., Hidden Markov chains , or recursive neural networks
 ) that are able to capture non-linear data distributions but are much more computationally expensive. We will show that AR models oﬀer a good balance between
simplicity and accuracy in sensor networks.
PAQ system overview
As mentioned above, we em-
Parameter Description
Parameters Used in Basic Model
Coeﬃcients of AR(3) model
Time elapsed between two
consecutive readings
Conﬁdence parameter on predicted
sensor readings
Error bound on predicted sensor readings;
will have error > ε with probability 1/ν2
Parameters Used in Dynamic Model
Error threshold above which model
re-computation is triggered
Fraction of readings that must be wrong
before model re-computation is triggered
Number of readings during the monitor
window in monitor() algorithm
Time that cluster leader waits in cluster
formation algorithm
Data similarity parameter
Fig. 1. Notation used in this paper.
ploy a combination of statistical models with live data acquisition. Each sensor in PAQ maintains a local AR model, and samples its values once every Γ seconds. It uses recent readings to
predict future local readings. When
a reading is not properly predicted
by the model, the node may mark
the reading as an outlier or choose
to re-learn the model, depending
on the extent to which the reading disagrees with the model and
the number of recent outliers that
have been detected. This design
is motivated by the need to (1)
monitor changes in the physical
phenomena and detect outliers,
and (2) reduce communication
between the local sensors and the
sink. Except when a node is a cluster leader (see below), it does not need to communicate while monitoring the model or during its learning phase, but only when
computing and adjusting its cluster, as described in Section 7.2. We discuss local
modeling more in detail in Section 4.1 and the process of marking readings as outliers and deciding when to re-learn in Section 5.
The sink maintains one AR model per geographic cluster. A cluster is a subset of
sensors within communication range of each other whose values diﬀer from each other
by at most by a constant value θ. Intuitively, it should be possible to cluster sensors
in this way, since we expect that nearby sensors will often produce similar readings.
Clearly, clusters are dynamic sets that can vary in number since the local models are
One sensor in each cluster is designated the leader. It is responsible for communicating with the sink on behalf of its cluster. The leader’s AR model, called the
cluster model, is used to predict the values of all sensors in the cluster with an error
of at most θ over the member sensor’s local models, and with the same conﬁdence.
The sink maintains the coeﬃcients associated with each of the leaders’ models and
receives periodic readings from them. It also maintains a list of the current clusters.
The leaders’ models and the cluster sets stored at the sink allow the sink to answer
queries over all sensors using just the cluster models, avoiding a large number of message transmissions. To reduce communications, clusters are computed locally by the
cluster leaders and members of each cluster. Each leader is responsible for notifying
the sink for changes in its model (in the coeﬃcients) or in its cluster members, and
for transmitting periodic readings to the sink. Details of the cluster formation and
query algorithms are given in Section 7.
Local AR model
The probabilistic model maintained by the sensor must be light-weight, in terms of
both the computational and storage requirements because of limitations of the sensor.
Our local models are designed with these energy restrictions in mind.
Trends and seasonal components. Since physical phenomena are typically not
stationary, a time series is usually decomposed into a trend component that grows
very slowly over time, a seasonal component with some periodicity, and a stationary
component. However, maintaining the trend and seasonal components substantially
increases the complexity of both learning and adapting the model. In order to simplify
our model and ignore trend and seasonal components, we consider an autoregressive
model AR(p) with a narrow prediction window, such as p = 3, similarly to . As
discussed in
 , if the time elapsed since the last reading is relatively short, it is
reasonable to neglect those components.
However, an AR(p) model is unlikely to be a good ﬁt for non–linear physical
phenomena. In particular, we have observed that sensor network data is typically
locally linear, but there are periodic non-linearities that are not well-predicted by
AR(p) models. To solve this problem, we enhance our linear models with dynamic
updates that are detected and performed locally – the idea is to detect when the
model is no longer a good ﬁt for the data being sensed, and dynamically re-learn
the model coeﬃcients when this happens. The eﬃciency of this approach comes from
the fact that learning and updating the AR model is cheap compared to the costs of
learning and maintaining a non–linear model.
Multivariate vs. univariate models. In sensor networks, each sensor device typically has multiple sensors. To handle multiple sensors, we can compute a multivariate
AR model with m components, one for each physical measurement or we can create
m univariate models. Clearly, for m > 2 the computational cost of learning the multivariate model is higher than the cost associated with m univariate models. To show
this, we compare the number of unknowns that have to be computed in both cases
during the learning phase: for a multivariate model, the sensor has to compute qm2
coeﬃcients, while in case of m univariate models it computes mq coeﬃcients. In terms
of storage space the multivariate model requires qm2 + (q + 2)m memory compared
to 2m(q + 1) required by m univariate AR models, which is a savings of qm(m −2).
For these reasons, it is more eﬃcient for the sensor to compute m univariate AR
models, although the multivariate model provides additional predictive power as it is
able to capture correlations between measurements. For simplicity of presentation we
consider only one measurement, F, in the rest of this paper. However, in general, our
techniques apply to either multiple univariate models or a single multivariate model.
Sensing model. We assume that each measurement reads F every Γ time units,
and denote the history of these values up to time t as v1, . . . , vi, . . . , vt. Although
our proposal is independent of the size of the parameter vector q, we consider q = 3
because it allows us simplify the model and ignore seasonal and trend components,
and has low computational and storage costs. Therefore, each sensor Sj models F as
a dynamic AR(3) time series with Gaussian white noise of zero mean and standard
deviation b(ω). In case of a time series F(t) with non–zero mean η, we study the time
series X(t) = F(t) −η, as in 
X(t) = α X(t −1) + β X(t −2) + γ X(t −3) + b(ω)N(0, 1)
with α, β, γ ∈R. Therefore, the predictor P(t) of F at time t is given by its mean η
plus a linear combination of the increments or decrements of the last three readings
with respect to η. More precisely, the prediction at time t > ti−1 is given as
P(t) = η + α(vi−1 −η) + β(vi−2 −η) + γ(vi−3 −η)
Function b(ω) represents the standard deviation of the white noise. Here, we assume
that the distribution of the noise does not vary over time. The following lemma
computes the error bound associated with the prediction P(t) of F(t) at time t.
Lemma 1. Let P(t) be the prediction of F at time t associated with model (1), and
let ε = ν b(ω), where ν is a real-valued constant larger than 1. Then, the actual value
at time t is contained in [P(t) −ε, P(t) + ε] with error probability at most
Proof. Since X(t) is driven by a Gaussian white noise with zero mean and standard
deviation b(ω), at any time t the prediction error vt−P(t) is normally distributed with
zero mean and standard deviation b(ω). The proof follows by applying the Chebychev
inequality to obtain the prediction error:
P(|F(t) −P(t)| ≥ε) ≤b(ω)2
ν2b(ω)2 = 1
We choose ν such that the error probability ν−2 can be considered negligible; e.g.,
for a typical value of ν we use 6 or 7. As a result, readings with an error larger than
ε are classiﬁed as anomalies that are handled specially as described in Section 5.2.
Learning phase
In this section we illustrate the steps taken by each sensor to learn its local model as
deﬁned by (1).
Data structures. There are two parameters that principally aﬀect the eﬃciency
and accuracy of the learning phase: the number of readings, N, collected during the
learning phase, and Γ, the time interval between two consecutive readings. During our
experiments, we study diﬀerent values of these parameters. Given these parameters,
each sensor builds the following data structures during the learning phase:
– a (initially empty) queue V , containing the most recent N readings.
– the coeﬃcients α, β, γ, and the mean η;
– the standard deviation b(Γ) of the white noise during Γ time units.
The learning algorithm is illustrated in Figlearn():
2) for k = 1 to N
3) read value vk
4) enqueue vk into V
5) wait for Γ time units
6) ⟨α, β, γ⟩←solveSys(V )
7) b(Γ) ←compVar(V )
Fig. 2. Learning phase.
ure 2. During the learning phase the sensor performs a reading every Γ time units, and inserts it
into V (Figure 2 lines 3–5.) After performing N
readings, it invokes the function solveSys which
computes the mean η from the N readings, and
the coeﬃcients α, β and γ. Notice that we do
not recursively compute α, β, γ as in the Durban-
Levine algorithm ), but only at the end for ef-
ﬁciency reasons, as in . The coeﬃcients are
computed by calculating the minimum squared
error between the readings contained in V and
the predicted values via least-squares regression.
Least-squares regression works as follows: suppose that v1, . . . , vN are the values
read during the learning phase, and that ¯v1, . . . , ¯vN are such that ¯vi = vi −η for
i = 1, . . . , N. Then, α, β, γ correspond to the coeﬃcients of the best linear predictor
and are obtained by minimizing the function Q(α, β, γ)
Q(α, β, γ) =
(¯vi −(α ¯vi−1 + β ¯vi−2 + γ ¯vi−3))2
The coeﬃcients α, β, γ can be computed by setting the partial derivatives of the
minimum squared error to zero and solving a linear system of three equations (we
omit the details of this computation and instead direct the reader towards a standard
linear algebra text, such as ).
solveSys computes the solution of these equations. After computing the mean η
and coeﬃcients α, β, γ, the sensor computes the variance of the white noise during
Γ time units by invoking compVar, Fig 2 line 7. This is done by computing the
prediction error ei = Pi −¯vi for i = 1, . . . , N, and the mean ¯e of e1, . . . , eN. Hence,
the variance of the white noise during Γ time units is:
i=1 (ei −¯e)2
Thus, the parameters η, α, β, γ and b(Γ) uniquely describe the AR model for a
given set of learning data {v1, . . . , vN}.
Costs of the learning phase. The computational cost of the learning phase is
the cost of reading N values, plus the cost of computing matrices A and B of the
linear system A X = B in 3 unknowns, which involves 12(N −3) sum and product
operations, plus the cost of solving the linear system. Therefore, the total cost is
equal to l + (s + 12)N −36, with s cost per reading and l cost for solving a linear
system with 3 unknowns. In terms of memory, it requires a (N + 5)–vector plus a
3 × 4 real–valued matrix.
A dynamic local AR model
As discussed in Section 4.1 the local AR model must be dynamic. Hence, the sensor
periodically monitors its local model and updates it as needed. This design oﬀers
several beneﬁts with respect to having the sink or another specialized node monitor
the validity of the sensor’s model:
1. It allows the sensor to detect data anomalies with respect to previous history,
where a data anomaly is a sensor value that the model does not predict to within
the user-speciﬁed error bound. These anomalies can be classiﬁed into outlier values, which are transient mispredictions that the model simply does not account
for, or distribution changes, which are persistent mispredictions that suggest the
model needs to be re-learned, either because of a faulty sensor or a fundamental
shift in the data being sensed.
2. It requires no communication during learning and updating (in contrast with
the approach taken in BBQ , for example). This is possible because of the
simplicity of our local model, which requires relatively small learning history and
low computational cost and memory storage.
However, the eﬃciency of this approach is related to the eﬃciency of monitoring
and updating the AR model, which we discuss in the remainder of this Section. For
the purpose of this discussion, we assume that each cluster consists of just one sensor
that is responsible for transmitting changes to its model and/or outlier values to the
sink. We discuss forming geographic clusters in Section 7.1; once clusters have been
formed, only cluster leaders transmit their values to the sink.
Choosing to Re-learn the Model
In order to save energy, we believe the model should be updated only when its readings
diverge consistently from its model. We classify data anomalies based on the model’s
prediction error. We consider two thresholds: δ and the maximum error ε = νb(Γ)
(as deﬁned in Lemma 1.) These thresholds are chosen such that if the absolute value
of the prediction error falls in [0, δ], then the model is a good predictor of the data.
If it falls in [δ, ε] the data is still within the user speciﬁed error bound but the model
might need to be updated. Finally, if the error prediction exceeds ε, then the data
is an outlier because of Lemma 1. Since ν is chosen such that fewer than a fraction
ν−2 of the values will be mispredicted (if the stationarity assumption holds) a single
outlier value can be neglected while still satisfying the user speciﬁed conﬁdence bound.
However, though this might be an isolated anomaly that requires no action, it might
correspond to an abrupt change in the data distribution (signifying that the data
was not stationary), in which case the node should update the model and send the
updated model parameters to the sink.
Monitoring algorithm
We can now describe the algorithm that is used to monitor the quality of the model,
based on the considerations in the previous section. The monitoring algorithm is
illustrated in Figure 3. Each sensor starts monitoring its model right after the learning
phase. It takes a reading every Γ time units and updates its queue V , which contains
the most recent N values (see Figure 3 lines 1-3.) If the prediction error exceeds δ it
begins monitoring the next Λ readings, if it is not already doing so. While monitoring,
the sensor keeps track of the number of times the prediction error is in the range
[δ . . . ε] (denoted in Figure 3 by the counter variable upd), as well as the number of
times the error exceeds ε (denoted by the variable out).
This is shown in Figure 3 lines 6-9. The
monitor():
1) every Γ time units do
3) update V
4) if |v −P(t)| ≥δ
mark-if-not()
if δ ≤|v −P(t)| ≤ν
else if |v −P(t)| ≥ν
if notify(out)
if (upd > a) || (upd + out ≥a + 1)
change ←true
send ⟨change⟩
15)if (end-of-monitor) ∧(changes)
solveSys()
send ⟨η, α, β, γ,outlierList⟩
Fig. 3. Monitoring algorithm.
sensor sends a notiﬁcation to the sink as
soon as it detects a variation in the data
distribution (Figure 3 lines 12-14). This
occurs if out + upd exceeds a parameter
a, expressed as a fraction of the number
of readings during the monitor window,
denoted by Λ, (e.g. 50% of the readings
performed). Notice that the sensor node
can employ diﬀerent strategies in reporting outlier values to the sink, depending
on application requirements. For instance,
it could send only the ﬁrst outlier value
occurred during a monitoring window, or
it could avoid transmitting outliers since
it promptly reports variations in its data
distribution to the sink, or it could simply report every outlier to the sink. The
notify() operation implements the strategy used by the sensor, (Figure 3 line 10).
After Λ readings, if the sensor has detected
variations in the data distribution, it recomputes η, α, β, γ based on the values stored
in V (Figure 3 lines 15-16.) Then, it sends the new coeﬃcients to the sink and optionally a list of the outlier values, (Figure 3 lines 17.)
Discussion
Clearly, the accuracy and eﬃciency of the dynamic update depends on the parameters
N, Λ, δ, ε, and a. N represents the length of the history that is used to compute the
model. The computational cost of re–learning increases linearly with N. However,
our experiments suggest that the accuracy does not necessarily improve as N grows.
For instance, if the data distribution is irregular (e.g., not well ﬁt by a linear model),
then a larger value of N will not result in a better ﬁt.
The choice of error bound ε presents a trade–oﬀbetween accuracy and error
probability (ability to meet a user speciﬁed conﬁdence bound) which is inversely
proportional to ν2. Moreover, the choice of ν has an impact also on the number of
readings marked as outliers, as shown in Section 6.
Another trade–oﬀbetween accuracy and eﬃciency is presented by the threshold
δ which deﬁnes, along with ε and a, the conditions under which the model should
be updated. Clearly, it is desirable to keep the number of updates low, since updates
incur additional learning and communication costs. However, making the interval [δ, ε]
too small will not result in a energy improvement since the model will not properly ﬁt
the data and will thus ﬂag more readings as outliers over time. Finally, the duration
of the monitor window, Λ, presents another tradeoﬀ: we want to keep the monitor
window relatively short to update the model as fast as possible, but making it too
short can result in excessively frequent updates.
Simulation results
To study the tradeoﬀs discussed in the previous section, we implemented the PAQ dynamic model and evaluated it on real data. We used a trace of sensor data from the
Intel, Berkeley research lab ( 
which consists of about a month’s worth of light, temperature, humidity, and voltage
readings collected approximately every thirty seconds from 54 sensors. In particular,
we have focused on the temperature read by 5 sensors with diﬀerent characteristics:
some of them with many high spikes, others with irregularity, and some with a relatively smooth distribution (this corresponds to sensors 6, 7, 22, 32, and 45 in the
aforementioned sensor data.) We selected these particular sensors to provide a sense
of the performance of PAQ under diﬀerent conditions. The traces we used from these
sensors include a number of missing readings because the Berkeley Motes used to
capture the readings were communicating over a lossy radio channel. Overall, about
25% of the 30 second intervals do not have readings associated with them.
We ran experiments where each node used its model to predict its next value,
and measured the error, the frequency with which the model needed to be re-learned,
and the number of messages that a node would send to the sink during execution.
In Section 7.1 we discuss how the sink uses these local models; in that Section we
prove that the sink can maintain a centralized model that introduces a user-speciﬁable
constant error over the error bounds shown here.
We varied several parameters throughout our experiments, though we only report
on experiments with a small number of settings here due to space constraints. In
general, we did not notice a large degree of sensitivity to the settings of various
parameters, except in a few cases which we discuss more below.
Figure 4 shows the total number of messages transmitted by the nodes versus the
user speciﬁed error threshold ν (here, we use communication overhead as a proxy
for total energy consumption, as communication tends to be the most expensive
aspect of operation in wireless sensor networks .) This includes transmissions as
a result of learning new model parameters as well as transmission of outliers. In
these experiments, approximately 25% of transmissions are a result of outliers being
transmitted; transmitting the coeﬃcients accounts for the remaining communication
overhead. In our implementation, we transmit all outlier readings; if users are not
concerned with receiving every outlier report, our approach could use somewhat fewer
messages. The number of times the model is re-learned varies from point to point; at
the lowest error rates (.1), the model is re-learned on about 20% of the monitoring
windows. Notice that some sensors have a higher cost; these correspond to sensors
that have “noisier” signals. Figure 5 compares raw data from sensor 6 (low cost) to
sensor 45 (high cost); notice that sensor 45 has more noise than sensor 6.
We also compared our algorithm to an algorithm similar to that proposed in 
which transmits a value to the sink whenever the current sensor value is more than
the user-deﬁned error threshold away from the value last transmitted to the sink. To
make the comparison fair, we use a version of this algorithm that uses a monitoring
window of the same size as in our algorithm, and that transmits new values at most
once per monitoring period. We call this method “approximate caching” and show
the number of transmissions it requires for two of the sensors in Figure 4. Notice that
our approach substantially outperforms the approximate caching approach.
Figure 6 shows how the error varies with the user speciﬁed error threshold. Here,
the X axis is the value of ε; the conﬁdence is set to 95%. The Y axis shows the average
No. of Transmission
User Defined Error
Number of Transmissions vs. User Specified Error
(30s per sample)
Approx. Caching Sensor 6
Approx. Caching Sensor 45
Fig. 4. Figure showing number of messages
sent (either outlier or new model messages)
for varying error thresholds. Parameters are
as in Figure 6.
Temperature
Sample No Vs. Temperature
(30s per sample)
Fig. 5. Figure comparing the raw temperature readings from sensor 6 to sensor 45.
prediction error, which increases slightly with increasing ε but not dramatically since
our error bounds are relatively conservative (notice that the actual error is well below
the user-speciﬁed allowable error line deﬁned by y = x). Because all errors are below
the user speciﬁed threshold, it is not surprising that there is little variation between
sensors; for example, even though sensor 45 requires substantially more re-learning
phases and outlier transmissions, its average error is still low. We do not show errors
for the “approximate caching” method, as it always meets the error bound.
The parameters that most aﬀected
Median Error
User Defined Error
User-Specified Error vs. Median Actual Error
(30s per sample)
Fig. 6. Figure showing the actual error rate for
diﬀerent user-input error thresholds. Parameter settings are as follows: N = 60, Λ = 15,
ν = 6, a = 8, Γ = 30, δ = 1.8
our results were Λ, the size of learning window, and a and δ the thresholds
that aﬀect when readings are ﬂagged as
outliers and when the model is rebuilt.
We noticed that the accuracy does not
grow with the size of the learning sample N. In most cases it seems that the
best prediction error occurs for N =
60, or, for sensor 45 (which tends to
be “spikier” than the other data) with
N = 120, while a learning phase consisting of N = 20 values implies a high
number of model updates. For these reasons we chose to report results where
Clearly, irregular readings (e.g., sensor 45 in Figure 6) aﬀect the accuracy of the
prediction. However, the overall error is not aﬀected noticeably by frequent missing
data as shown by our results which are based on real data with a high number of
irregular readings, though more irregular sensors do incur a higher overall cost as
more outliers must be transmitted out of the network.
With respect to predicting values at the sink, our analysis suggests the maximum
prediction error is equal to ν−2; the graphs above suggest, at least for our data set,
that this bound does in fact hold. We will see in the next section that this error grows
by at most a user-deﬁned constant θ when predicting the values of sensors that are a
member of a cluster from that cluster’s leader.
An Eﬃcient Centralized Model
In the algorithms we have described thus far each sensor sends the sink the coeﬃcients
of its local model after the learning phase. However, to use the AR model, the sink
requires periodic readings from the sensors, which would require energy-consuming
communication on the part of each sensor at every Γ time units. As described above,
we can reduce the extent of these communications by exploiting geographic similarities
between sensors that are within direct radio communication with each other. This
is based on the observation that sensors located near each other are likely to have
similar readings. Once sensors are organized into clusters, only the leader sends its
periodic readings to the sink. Other sensors continue to run the monitor() algorithm
to update their local models, and detect group membership changes.
Sensor clusters
In this section we deﬁne data similarity among sensor readings, and show how we can
group similar sensors into cluster sets. Let us suppose that θ, the similarity constant
is a user-provided positive real value.
Deﬁnition 3. Two sensors are similar at time t if their readings diﬀer by at most
the similarity constant, θ.
Using Deﬁnition 3, we group sensors that are withing communication range of
each other and are similar into clusters. Clusters will change over time since the AR
model is dynamic. Hence, we deﬁne a set of clusters C(t) at time t as follows:
Deﬁnition 4. A cluster set C(t) at time t is a set of subsets of S with the following
properties:
1. any cluster C ∈C(t) contains sensors within radio broadcast range of each other
that are similar at time t;
C∈C(t) C = S.
Each cluster C has a leader, a speciﬁed sensor in C which is elected locally by
choosing, for example, the sensor with the lowest ID (or via any other function that
can be locally evaluated at each node.) At any time the AR model associated with
the cluster is the same as the local AR model of the current leader. The following
Lemma computes the maximum error performed by predicting the value of a sensor
in a cluster using the cluster model.
Lemma 2. The maximum prediction error associated with the value of a sensor in
a cluster is at most ε + θ, with error probability at most ν−2.
Proof. Suppose we have a cluster C at time t such that C = {Si1, . . . , Sik}, and node
Si1 is its leader. Therefore, the model of C at time t is represented by the model of
Si1. For any sensor Sij in C, then |vj
t −P i1(t)| ≤|vj
t −vi1(t)| + |vi1(t) −P i1(t)| and
given Deﬁnitions 3 and 4, then |vt −P i1(t)| ≤ε + θ. The error probability is at most
ν−2 because of Lemma 1.
In the next section, we sketch our approach for forming geographic clusters. This
is a basic protocol whose correctness relies on reliable communication and symmetric
radio links. We also assume that the round–trip communication time between the sink
and any sensor is bounded. Though symmetry and reliability assumptions are not
entirely realistic, recent publications suggest that careful neighborhood management
and retransmissions can provide loss rates as low as 1-2 percent in static sensor
networks , which should be suﬃcient for our purposes.
Building and maintaining clusters
The clustering protocol involves two major steps: ﬁrst, at the end of the learning
phase, sensors compute clusters; second, each sensor monitors and maintains its
cluster-membership dynamically. Figures 7 and 8 present the high-level description
of the clustering protocol. Due to space limitations we omit some details.
Building clusters. At the end of the learning phase each sensor runs a protocol
to compute its cluster. The protocol is illustrated in Figure 7. Here, id denotes the
identiﬁcation number of the sensor running the protocol, and v is its current reading.
The sensor node sets its cluster set CL to its identiﬁcation number, and its leader
L to zero since each id is a positive number (Figure 7 lines 1-2.) Each sensor node
broadcasts its current value v and listens for ∆time units for other sensor broadcasts
in order to detect similarities among its neighbors (Figure 7 lines 3-4.) During these
∆time units it listens for broadcasts, for each, and checks if the received value ¯v
diverges by θ or less from its value v. In this case it inserts the sensor identiﬁcation
into CL. After ∆time units it chooses the sensor with minimum identiﬁcation number
as the cluster leader of CL (Figure 7 line 5.) If the sensor has been chosen as the
cluster leader, it sends a leader notiﬁcation message that contains the cluster set CL
and its model coeﬃcients to its neighbors and to the sink (Figure 7 line 6-7.) Figure
8 illustrates the steps taken by a sensor when receiving a leader notiﬁcation. If the
sensor belongs to cluster cl and its cluster leader has not been set yet, it sets its leader
to l (Figure 8 lines 1-2.) Otherwise, it keeps track of the other leaders within its radio
broadcast by adding them into list OL (Figure 8 lines 3-4.) Notice that since CL is
computed locally based on the messages received from its neighbors, a sensor might
belong to two diﬀerent clusters and might receive diﬀerent leader notiﬁcations. The
sensor follows the ﬁrst leader notiﬁcation. Since data changes over time, the leader
periodically transmits its current value to its neighbors (Figure 7 lines 8-9.) The other
sensors in the cluster verify that their value is θ similar to the leader’s value.
2) CL ←{id}
3) broadcast(v)
wait for ∆time units
upon event receive(⟨¯v, i⟩) do
if |¯v −v| ≤θ
CL ←CL ∪{i}
5) L ←min CL
6) if L = id
send ⟨leader, L, CL, η, α, β, γ⟩
every Γ time units do
send ⟨v, L⟩
Fig. 7. Building clusters.
leader notiﬁcation: receive(l, cl)
1) if (id ∈cl) ∧(L = 0)
OL ←OL ∪{l}
periodic validations: receive(⟨¯v, l⟩)
1) if (l = L) ∧(|¯v −v| > θ)
if (s =nextLeader(OL)> 0)
broadcast(⟨join, s⟩)
send (⟨leader, L, CL, η, α, β, γ⟩)
Fig. 8. Notiﬁcation protocols.
Maintaining clusters. Two factors can trigger changes in the cluster: (1) a sensor
in the cluster can become dissimilar from the leadre, and (2) a leader can fail.
Let us consider the ﬁrst case. Upon receiving the leader value, each sensor checks
if its current value is within θ units from the leader value. If this condition does not
hold, the sensor checks if its value is at most θ units away from the current values
of the other leaders within its radio broadcast (Figure 8 lines 1-2.) If it detects a
leader whose value diverges by at most θ units from its current value, it broadcasts
a join request, Figure 8 line 3. Otherwise, it creates a new cluster and notiﬁes its
neighbors and the sink of this change (Figure 8 lines 4-7). The leader that receives
a join request updates its cluster set CL and notiﬁes the sink, while the previous
leader removes that sensor node from its cluster set.
Leader failures are detected by sensors listening for periodic sensor value messages.
When several broadcasts are missed, a sensor infers that the leader may have failed
and computes the new leader based on the remaining sensors in the group-member
list. If a sensor detects that it should be the current leader (e.g., because it has the
lowest id), it will broadcast a leader-change message as in the initial phase.
Answering queries
The sink locally stores the cluster models and the sensors belonging to each cluster.
It uses this information to answer queries without additional communication. More
precisely, it predicts the sensor values in a cluster using the model of the cluster
leader, and veriﬁes if this prediction satisﬁes the error-bounds associated with the
query. Since the correctness of this query framework is strictly related to the validity
of the cluster models at the sink, our query algorithm has to take into account the
latency occurred in transmitting variations in the cluster to the sink. This is done
by delaying the reporting of answers from the sink for a time equal to the maximum
network latency. The system must also guarantee the error bound in the case of abrupt
changes in the local data distributions at individual sensors. This is ensured by the
outlier-transmission protocol described in the previous section. Such notiﬁcations are
also delayed by at most the maximum latency of the network. Hence, the sink learns
of new models and outliers within bounded time, and is able to answer queries with
error at most ε + θ.
Although this approach conserves energy by reducing the amount of communication between the sensors and sink it works well in relatively high density networks
since it detects data similarity for sensors which are one–hop away from each other.
Second, it can require a many transmissions when data distributions change or nodes
fail frequently. We are currently exploring other clustering techniques that are able
to overcome these limitations.
Conclusions
In this paper, we showed that AR models have the potential to dramatically reduce
the amount of communication required to accurately monitor the values of sensors
in a wireless sensor network. Compared to existing approaches based on centralized
probabilistic models built at the sink, our approach is much lighter weight, allowing
sensors to build models locally and monitor their values for signiﬁcant changes or deviations from the model while still providing substantial communications reductions.
This means that our approach is able to detect outliers or fundamental changes in
model parameters. We also presented a simple clustering algorithm that allows us to
further reduce communication from sensors to the sink while still providing a provable
bound on the maximum predicted error at each sensor. Hence, we are optimistic that
our approach will be important in future sensor network monitoring systems, and we
look forward to extending our work with a complete implementation and evaluation.