Learning to Explain with Complemental Examples
Atsushi Kanehira1 and Tatsuya Harada2,3
1Preferred Networks, 2The University of Tokyo, 3RIKEN
This paper addresses the generation of explanations with
visual examples. Given an input sample, we build a system that not only classiÔ¨Åes it to a speciÔ¨Åc category, but also
outputs linguistic explanations and a set of visual examples
that render the decision interpretable. Focusing especially
on the complementarity of the multimodal information, i.e.,
linguistic and visual examples, we attempt to achieve it by
maximizing the interaction information, which provides a
natural deÔ¨Ånition of complementarity from an information
theoretical viewpoint. We propose a novel framework to
generate complemental explanations, on which the joint distribution of the variables to explain, and those to be explained is parameterized by three different neural networks:
predictor, linguistic explainer, and example selector. Explanation models are trained collaboratively to maximize the
interaction information to ensure the generated explanation
are complemental to each other for the target. The results
of experiments conducted on several datasets demonstrate
the effectiveness of the proposed method.
1. Introduction
When we explain something to others, we often provide
supporting examples. This is primarily because examples
enable a concrete understanding of abstract explanations.
With regard to machines, which are often required to justify
their decision, do examples also help explanations?
This paper addresses the generation of visual explanations with visual examples. More speciÔ¨Åcally, given an input sample, we build a system that not only classiÔ¨Åes it to
a speciÔ¨Åc category but also outputs linguistic explanations
and a set of examples that render the decision interpretable.
An example output is shown in Fig. 1.
The Ô¨Årst question to be raised toward this problem would
be ‚ÄúHow do examples help explanations?‚Äù, or equivalently, ‚ÄúWhy are examples required for explanations?‚Äù
This work is done at the University of Tokyo.
 

 
   
Figure 1: Our system not only classiÔ¨Åes a given sample to
a speciÔ¨Åc category (in the red dotted box), but also outputs
linguistic explanations and a set of examples (in the blue
dotted box).
To answer these questions, we consider the characteristics
of two types of explanations pertaining to this work: linguistic explanation, and example-based explanation.
‚Ä¢ Using language, one can transmit information efÔ¨Åciently by converting an event to a shared concept between humans. Inherently, the conversion process is invertible; thus, the whole event can not necessarily be
represented by language alone.
‚Ä¢ Using examples, one can transmit information more
concretely than language can, as the saying, ‚Äúa picture
is worth a thousand words.‚Äù However, the way of the
interpretation for the given examples is not determined
uniquely. Thus, using examples alone is inappropriate
for the explanation.
These explanations with different characteristics can be expected to complement each other, that is, from a lexicon,
a thing that contributes extra features to something else in
such a way as to improve or emphasize its quality .
The next important questions here are as follows: ‚ÄúHow
can the complementarity be achieved?‚Äù and ‚ÄúWhich explanation is complemental, and which is not?‚Äù.
We answer the former question from the information
 
theoretical viewpoint, that is, interaction-information 
maximization. Interaction-information is one of the generalizations of mutual information deÔ¨Åned on more than three
random variables, and provides the natural deÔ¨Ånition of the
complementarity: The increase of dependency of two variables when the third variable is conditioned.
We propose a novel framework in this work to build a
system that generates complemental explanations.
we introduce a linguistic explainer and an example selector parameterized by different neural networks, in addition
to the predictor that is the target of the explanation. These
two auxiliary models are responsible for generating explanations with linguistic and examples, respectively, and they
are simultaneously trained to maximize the interaction information between variables of explanations and the output
of the predictor in a post-hoc manner. Because the direct
optimization of interaction-information with regard to the
selector is intractable owing to the number of combination
of examples, we maximize the variational lower bound instead. One more additional classiÔ¨Åer, referred to as reasoner, appears in the computation of the lower bound. Taking linguistics and example-based explanations as inputs,
the reasoner attempts to predict the output of the predictor. To enable the optimization of the selector with backpropagation, we utilized a reparameterization trick that replaces the sampling process of the examples with a differentiable function.
Under our framework, where complementarity is deÔ¨Åned
by information theory, we can understand better the complemental explanation related to the latter question. It can be
mentioned that complemental examples for a linguistic explanation are a discriminative set of examples, by which one
can reason to the correct conclusion with the given linguistic explanations, but cannot be achieved with different possible explanations. Complemental linguistic explanations
to examples are also considered to be explanations that can
construct such a set of examples. More details will be discussed in the subsequent section.
We conducted experiments on several datasets and
demonstrated the effectiveness of the proposed method.
The contributions of this work are as follows:
‚Ä¢ Propose a novel visual explanation task using linguistic
and set of examples,
‚Ä¢ Propose a novel framework for achieving complementarity on multimodal explanations.
‚Ä¢ Demonstrate the effectiveness of the proposed method
by quantitative and qualitative experiments.
The remainder of this paper is organized as follows. In
Section 2, we discuss the related work of the visual explanation task. Further, we explain the proposed framework
to achieve complemental explanations in Section 3 and describe and discuss the experiments that we performed on it
in Section 4. Finally, we conclude our paper in Section 5.
2. Related Work
The visual cognitive ability of a machine has improved
signiÔ¨Åcantly primarily because of the recent development in
deep-learning techniques. Owing to its high complexity, the
decision process is inherently a black-box; therefore, many
researchers have attempted to make a machine explain the
reason for the decision to verify its trustability.
The primary stream is visualizing where the classiÔ¨Åer
weighs for its prediction by assigning an importance to each
element in the input space, by propagating the prediction to
the input space , or by learning
the instance-wise importance of elements with an
auxiliary model. As a different stream, some works trained
the generative model that outputs explanations with natural language in a post-hoc manner. Although most
studies are focused on single modality, our work exploits
multimodal information for explanations.
Prototype selection or machine
teaching can be considered as example-based explanations. The essential idea of these methods is to extract
representative and discriminative (parts of) examples. In
other words, they attempt to obtain examples that represent
p(x|c), which is the distribution of sample x conditioned
on the category c. Our work is different in that we attempt
to explain the black-box posterior distribution p(c|x) such
as that represented by deep CNN. Moreover, we utilized
the linguistic information as well because the interpretation toward example-based explanation is not determined
Few works have treated multimodality for explanation
 , which is visual and linguistic. Although they provided visual information by referring to a part of the target
samples, we explore the method to utilize other examples
for explanation.
The goal of this study is to build a model that generates
linguistic and example-based explanations, which are complemental to each other. In this section, we describe the
proposed framework. First, in subsection 3.1, we formulate our novel task with the notation used throughout this
paper. Subsequently, the objective function to be optimized
is illustrated in subsection 3.2. From subsection 3.3 to 3.6,
we explain the details of the actual optimization process.
The proposed method is discussed qualitatively in subsection 3.7. Finally, its relation with other explanation methods
is mentioned in subsection 3.8.
3.1. Problem formulation
We denote by x and y the sample and the category that
are the target for explanations where y is a one-hot vector. s is a vector representing a discrete attribute, whose
reference data
X = {x1, x2, ..., xN}
trainable model
freezed model
Figure 2: The pipeline of our explanation system. It holds
two auxiliary models, which are responsible for generating
explanations with linguistics and examples, respectively. In
addition, it contains a reasoner that predicts the output of
the predictor from the given explanations as described in
subsection 3.3.
every index corresponds to the type of attribute (e.g., dim1
‚Üícolor, dim2 ‚Üíshape..), and the value of the vector corresponds to the value of attributes (e.g., 1 ‚Üíred, 2 ‚Üíblue
..). Attribute values are also treated as one-hot vector on implementation. We assume that the attributes are assigned to
all the samples used for training the explanation model. In
this study, we utilize one attribute as an element of linguistic explanation. More speciÔ¨Åcally, linguistic explanation s
contains only one non-zero value (i.e., ||s||0 = 1), and the
corresponding type-value is outputted (e.g., ‚Äúbecause color
is red.‚Äù). To explicitly distinguish the variable representing
linguistic explanation from one representing attributes assigned to the samples, we denote the former by s and the
latter by ÀÜs. The set of candidate examples used for explanation is represented by X = {(xi,ÀÜsi, yi)}N
i=1, and its subset
D ‚äÇX, |D| = k is used as an element of the examplebased explanation. We assume
, and that the number
of combinations D, is sufÔ¨Åciently large. Our system generates multiple elements (s1, D1), (s2, D2), . . . , (sM, DM),
and construct a explanation by simply applying them to the
template as in Fig. 1.
We built a model not only categorizing the input x to
a speciÔ¨Åc class y, but also providing an explanation with
linguistics and example-based explanations s and D. We
decomposed a joint distribution p(y, s, D|x) to three probabilistic models: predictor, explainer, selector, all of which
were parameterized by different neural networks:
p(y, s, D|x) = p(y|x)
p(D|x, y, s)
predictor p(y|x) is the target model of the explanation,
which categorizes sample x to y. Particularly, we study
the model pretrained for the classiÔ¨Åcation task. Throughout
this paper, the weight of the predictor is frozen, and the
remaining two auxiliary models, namely, explainer and
selector, are trained to explain the output of the predictor.
explainer p(s|x, y) is the probability of linguistic
explanation s being selected given target sample x and
class y. We limit ||s||0 = 1, and the dimension and the
value corresponding to the non-zero element is used as an
explanation.
selector p(D|x, y, s) is the probability of example-based
explanation D being selected out of all candidate examples
given x, y, and s as inputs.
3.2. Objective function
We illustrate the objective function optimized for training the explanation models in this subsection. As stated
earlier, linguistic explanation s and example-based explanation D are expected to be complemental to each other.
Intuitively, one type of explanation should contain the information for the target y, that is different from what the
other explanation contains.
Hence, we leverage the interaction information as
an objective function. Interaction-information is a generalization of the mutual information deÔ¨Åned on more than
three random variables, and it measures how the dependency of two variable increases when the third variable
is conditioned, which provides a natural deÔ¨Ånition toward
complementarity.
From the deÔ¨Ånition, the interaction information of
y, s, D conditioned on the input x is written as the difference of two mutual information:
I(y, s, D|x) = I(y, s|x, D) ‚àíI(y, s|x)
I(y, s|x, D)
p(y, s, D, x)log
p(y, s|x, D)
p(s|x, D)p(y|x, D)dx,
p(s, D|x, y)p(y|x)logp(s|x, y, D)
and similarly,
p(y, s|x)log
y p(s|x, y)p(y|x)
Intuitively, it measures how much linguistic explanation
s becomes useful information to identify a category y when
given a set of example-based explanation D.
The direct estimation of (3) is difÔ¨Åcult, as calculating the
expectation over all possible D is intractable. We handle
j = {1, 2, ..., N}
i = {1, 2, ..., k}
Gij = Gumbel(0, 1)
ÀÜs1,ÀÜs2, ...,ÀÜsk
w1, w2, ..., wk
q(y|x, s, D)
function (with parameter)
function (without parameter)
probability
pre-trained CNN
x1, x2, ..., xk
y1, y2, ..., yk
y1, y2, ..., yk
Figure 3: Structures of three neural networks representing three probabilistic models. As described in subsection 3.4, the network of the selector predict the parameter of categorical distribution unlike the other two models for the ease of optimization.
this problem by (a) introducing the variational lower bound
and (b) leveraging reparameterization trick similar to ,
which are described in subsections 3.3 and 3.4, respectively.
3.3. Maximizing variational bound
In this subsection, we consider the variational lower
bound of (A) in (3). From the deÔ¨Ånition of the KL divergence, p log p ‚â•p log q is applied for any distribution p
and q. Using this relation, (A) inside the expectation in (3)
can be lower-bounded as follows:
p(s, D|x, y)p(y|x)logq(s|x, y, D)
q(s|x, y, D) can be any distribution provided that it is normalized, and the expectation of the KL divergence between
q(s|x, y, D) and true distribution p(s|x, y, D) is the difference between (A) and the lower bound. Similar to the
method in , we used the following
q(s|x, y, D) = q(s, y|x, D)
q(y|x, s, D)p(s|x, D)
s‚Ä≤ q(y|x, s‚Ä≤, D)p(s‚Ä≤|x, D),
and substituted it to (5). When considering parameterizing as in (1), it is computationally difÔ¨Åcult to calculate
p(s|x, D). Considering the sampling order, we approximate
it to p(s|x, y) instead for simplicity. The Ô¨Årst term of the
objective function used for optimization is as follows:
(5) ‚âàEp(y,s,D|x)
q(y|x, s, D)
Ep(s|x,y)[q(y|x, s, D)]
q(y|x, s, D) is hereafter referred to as a reasoner, which
is expected to reason the category of input given a pair of
explanation for it.
3.4. Continuous relaxation of subset sampling
The abovementioned (6) is required to be optimized
stochastically with sampling to avoid calculating the summation over the enormous number of possible combinations
of D. In this situation, the difÔ¨Åculty of optimization with regard to the network parameter still exists. As it involves the
expectation over the distribution to be optimized, sampling
process disables calculating the gradient of parameters, rendering it impossible to apply back-propagation.
We resort on the reparameterization trick to overcome
this issue, which replaces the non-differential sampling process to the deterministic estimation of the distribution parameter, followed by adding random noise. In particular,
the Gumbel-softmax function is utilized similar to
 , which approximates a random variable represented as
a one-hot vector sampled from a categorical distribution to
a vector using continuous values. SpeciÔ¨Åcally, we estimate
the parameter of categorical distribution p ‚ààRN satisfying
i=1 pi = 1 by the network where N = |X| is the candidate set of examples. An N-dimensional vector C, which is
a continuous approximation of the categorical one-hot vector, is sampled by applying softmax to the estimated parameter after taking logarithm and adding a noise G sampled
from the Gumbel distribution as follows:
exp{(log pi + Gi)/œÑ}
j=1 exp{(log pj + Gj)/œÑ}
Gi = ‚àílog(‚àílog ui), ui ‚àºUniform(0, 1),
and œÑ is the temperature of softmax controlling the hardness
of the approximation to the discrete vector. To sample k-hot
vector representing example-based explanation D, concrete
vector C is independently sampled k times, and elementwise maximum is taken to C1, C2, . . . , Ck to construct a
vector corresponding to D.
3.5. Structure of networks
We parameterize three probabilistic distributions, explainer, selector, and reasoner with different neural networks. We elucidate their detailed structures.
Explainer p(s|x, y) is represented by a neural network
that predicts the probability of each type (dimension) of
attribute is selected. The model is constituted using three
fully-connected layers as in left of Fig. 2. Taking the target sample x and the category label y as inputs, the model
projects them to the common-space and element-wise summation is applied. After one more projection, they are normalized by the the softmax function. The output dimension
of the network f(x, y) is the same as that of the attribute
vector, and each dimension indicates the probability that
each type of attribute is selected as an explanation. When
training, the attribute ÀÜs assigned to the sample is used as
the value. Formally, for all i-th dimension of the linguistic
explanation vector,
p(s|x, y) =
f(x, y)[i]
if s[i] = ÀÜs[i]
For inference, the value that maximizes the output of the
reasoner (described later) for the class to be explained is
Selector p(D|x, y, s) takes the linguistic explanation s in
addition to x and y as inputs; their element-wise summation
is calculated after projecting them to the common-space. As
stated in the previous subsection, we leverage reparameterization trick to render the optimization tractable owing to
the enormous number of the combination D. The network
estimates the parameter p of categorical distribution. When
sampling from a distribution, noise variables that are independently generated k times are added to the parameter, and
the element-wise maximum is computed after the Gumbel
softmax is applied.
Reasoner q(y|x, s, D) infers the category to which the
sample x belongs, given a pair of generated explanation (s,
D). We design it by modifying the matching network ,
which is a standard example-based classiÔ¨Åcation model.
The prediction of the reasoner must be based on the given
explanations. Such reasoning process is realized by considering (1) consistency to the linguistic explanation s, and (2)
similarity to the target sample x, for each example in D.
Based on a certain reason, the reasoner decides whether
each example deserves consideration, and predicts the category exploiting only selected examples. The weight of each
referred sample xi is determined by the visual and semantic
similarity to the target x. More formally,
q(y|x, s, D) =
(xi,ÀÜsi,yi)‚ààD
Œ±(s,ÀÜsi) w(x, s, xi,ÀÜsi) yi
q(¬Øy|x, s, D) = 1 ‚àí
(xi,ÀÜsi,yi)‚ààD
Œ±(s,ÀÜsi) w(x, s, xi,ÀÜsi) (10)
w(x, s, xi,ÀÜsi) =
exp(g(x, s)‚ä§g(xi,ÀÜsi))
(xi,ÀÜsi,yi)‚ààD
exp(g(x, s)‚ä§g(xi,ÀÜsi))
Œ± indicates the function to verify the coincidence of the linguistic explanation and the attribute assigned to each sample. In our setting, we set as Œ±(s,ÀÜs) = P
i[[s[i] = ÀÜs[i]]]
where [[¬∑]] is an indicator function of 1 if the condition
inside bracket are satisÔ¨Åed, otherwise 0. Note Œ±(s,ÀÜs) ‚àà
{0, 1} as ||s||0 = 1. w measures the weight of each referred sample used for prediction. The probability of the
sample being assigned to each class is determined to utilize
the samples in D, which match to the linguistic explanation
as in (9). An additional ‚Äúunknown‚Äù category ¬Øy is introduced
for convenience, indicating the inability to predict from the
input explanations. The remaining weight is assigned to
the probability of the ‚Äúunknown‚Äù class, as in (10). In (11),
g(x, s) is the feature embedding implemented by the neural
network as in the right-most in Fig. 3, and the similarity is
computed by the dot product in that space following normalization by the softmax function.
While the reasoner attempts to make a decision based on the
given explanations, the other two models are trained collaboratively to generate explanations such that the reasoner can
reach the appropriate conclusion.
3.6. Training and Inference
We parameterize the joint distribution as in (1), and the
lower bound of the objective (2) calculated by (4) and (6)
is optimized with regard to the parameters of the neural
network models representing p(s|x, y), p(D|x, y, s), and
q(y|x, s, D). Assuming that the calculation of the expectation over s is feasible, although that over D is not, we
optimized the model of the selector by sampling, and that
of the explainer was optimized directly.
The processing Ô¨Çow in each iteration is as follows:
1. x is sampled randomly from the training dataset,
2. y is sampled randomly from the predictor p(y|x),
3. p(s|x, y) is computed for possible s,
4. D is sampled randomly from the selector p(D|x, y, s)
for each s,
5. For each sampled (x, y, s, D), the objective is calculated by (6) and (4), and the gradient of it w.r.t the
weights of all parametric models are computed.
6. All weights are updated by stochastic gradient decent
The inference is performed by sequentially sampling
variables from the distributions given input x. When generating linguistic explanations, M identical attribute type is
selected whose output value of the predictor is the largest,
where M is the number of attribute-examples pairs. used
for explanation. For estimating the attribute value, the one
 
 x
 x
Figure 4: Intuitive understanding of complemental explanations. The reasoner predicts the target sample x (written as gray circles) by referring other samples based on the
similarity space (orange and blue) corresponding to each
linguistic explanation s1, s2. Considering two pairs of possible explanations (s1, D1) and (s2, D2), the expected D1
(written as green circle) is the one by which the reasoner
can reach the correct conclusion with s1; however, this cannot be achieved with s2.
that most explains the prediction the best will be selected.
In other words, s1, s2, ... having the same attribute type, the
value maximizing q(y|x, s, D) is outputted after the corresponding D1, D2, ... are sampled from the selector.
3.7. Which explanation is complemental?
By analyzing the proposed method, it provides an intuitive understanding of complemental explanations, from the
viewpoint of maximizing interaction information.
To understand which set D is preferred, we consider (6)
where D relates. Inside the expectation in this equation, the
numerator is the output of the reasoner, and the denominator is that averaged over s‚Ä≤. Given x, y, and s, the situation where the ratio becomes large is when the reasoner can
reach the correct conclusion y for given linguistic explanation s with D but it can not when D is used with other s‚Ä≤. In
other words, an example-based explanation is complemental to its linguistic counterpart when it is a discriminative
set of examples for not only the target but also the linguistic
explanation.
The concept of ‚Äúa set is discriminative‚Äù is clearly different from ‚Äúsingle example is discriminative‚Äù in our framework. This can be understood intuitively by Fig. 4. A reasoner contains a different similarity space for each linguistic
explanation s. Here, we consider two possible explanations
s1, s2, and D1 which is the counterpart of s1. In this situation, the desired D for linguistic explanation s is that the
correct class is predicted for the given s, but a wrong one
is predicted for different s‚Ä≤. Therefore, the set should contain both of the same/different classes from the predicted
acc (predictor)
acc (reasoner)
consistency
Table 1: The accuracy of identifying the target category of
the predictor (target) and reasoner (explain), and the consistency between them.
one. As shown, a naive method of example selection, such
as one selecting only one nearest sample from the target, is
not appropriate for selecting a complemental explanation.
Considering s, it relates to both terms in (2). For (6), the
same claim as that mentioned above can be applied: a complemental linguistic explanation s for examples D is one
where a speciÔ¨Åc set D can be derived, instead of another see
D‚Ä≤. As for (4), it can be regarded as a regularizer to avoid
weighing excessively on the attribute that can identify the
target class without considering examples for selectings.
3.8. Relationship with other methods
The existing works for visual explanation explanations
(e.g., ) trains the classiÔ¨Åer as well as the explanation
generator to guarantee that the generated explanations are
discriminative for the target class. In this work, we also
train the auxiliary classiÔ¨Åer (i.e., reasoner) similar to the
existing methods; however, it naturally appears in the context of interaction information (mutual information) maximization. Conversely, we found that such an intuitive idea
in these works is justiÔ¨Åed from the information theoretical viewpoint. Similarly, our method shares the idea with
methods for generating referring expression (e.g., ) in
that they utilize auxiliary models.
4. Experiment
We conducted experiments to verify that the proposed
method can generate the appropriate explanation. Given
a target sample x, our system generates a prediction y
from the predictor, and explanations (s1, D1), (s2, D2), ...,
(sM, DM) from explanation models. We evaluated the proposed method by quantifying the properties that the generated explanation should satisfy: (a) Ô¨Ådelity and (b) complementarity. Related to (a), we consider two types of Ô¨Ådelity
as follows. (a1) The target value y to be explained should
be obtained from the explanations. Moreover, (a2) The outputted linguistic explanation s should be correct. In addition, as for (b), we would like to assess whether the output
explanations (s, D) are complemental to each other. In the
following subsections, we describe the evaluation method
as well as discussing the obtained results after elucidating
the setting of the experiments in subsection 4.1.
4.1. Experimental setting
Dataset In our experiments, we utilized Caltech-UCSD
Birds-200-2011 Dataset (CUB) and Aesthetics with
baseline (random)
baseline (predict)
Table 2: The accuracy of identifying the attribute value of
our model and that of baselines: selecting attribute value
randomly (random), and predicting attributes by the perceptron (predict).
Attributes Database (AADB) , both of which hold attributes assigned for all contained images. CUB is a standard dataset for Ô¨Åne-grained image recognition, and it contains 11,788 images in total and 200 categories of bird
species. It contains 27 types of attributes, such as ‚Äúwing
pattern‚Äù or ‚Äúthroat color.‚Äù AADB is a dataset created for
the automatic image aesthetics rating. It contains 10,000
images in total and the aesthetic score in [-1.0, 1.0] is assigned for each image. We treat the binary classiÔ¨Åcation
by regarding images having non-negative scores as samples
of the positive class, and remaining samples as the negative class. Attributes are also assigned as the continuous
values in [-1.0, 1.0], and we discretized them according to
the range that it belongs to: [-1.0, -0.4), [-0.4, -0.2), [-0.2,
0.2), [0.2, 0.4), or [0.4, 1.0]. It contains eleven types of
attributes, including ‚Äúcolor harmony‚Äù and ‚Äúsymmetry.‚Äù Unlike the standard split, we utilized 60% of the test set for
CUB, and 30% of the train set for AADB as the candidates
of examples X.
Although CUB dataset is for the Ô¨Åne-grained task, where
the inner-class variance of the appearance is considered
small, that of AADB is large owing to the subjective nature of the task. We selected these two datasets to assess the
inÔ¨Çuence of the variation of the samples within the class.
Detailed setting To prepare a predictor, we Ô¨Åne-tuned
a deep residual network having 18 layers for each
dataset, which is pre-trained on ImageNet dataset . The
optimization was performed with SGD. The learning rate,
weight decay, momentum, and batch size were set to 0.01,
10‚àí4, 0.9, and 64, respectively. When training explanation models, all networks were optimized with SGD without momentum with learning rate 10‚àí3, weight decay 10‚àí3,
and batch size 64 for AADB and 20 for CUB. We set k, the
number of examples used for explanations, to 10 in all experiments.
Empirically, we found that the linguistic explainer
p(s|x, y) tended to assign a high probability (almost 1) on
only one type of attribute, and small probability (almost
0) to the others. To avoid it, we added an extra entropy
term H(s|x, y) = P
s ‚àíp(s|x, y) log p(s|x, y) to the maximized objective function as our goal is to generate multiple
outputs. The implementation was performed on the Pytorch
framework .
consistency
consistency
Table 3: The ablation study for the accuracy of identifying the target category on AADB dataset (above) and CUB
dataset (below).
4.2. Fidelity
One important factor of the explanation model is the Ô¨Ådelity to the target to be explained. We conducted an experiment to investigate whether the target can be obtained
from its explanation. Interestingly, our framework holds
two types of paths to the decision. One is the target predictor p(y|x) to be explained. The other is the route via
explanations interpretable for humans, i.e., y ‚Üís, D ‚Üíy‚Ä≤
through the explainer p(s|x, y), selector p(D|x, y, s), and
reasoner q(y‚Ä≤|x, s, D).
We evaluated the Ô¨Ådelity to the
model by the consistency between the interpretable decisions from the latter process and that from the target predictor. In the Table. 1, we reports the consistency as well
as the mean accuracy of each models. As shown, the explanation model (written as reasoner) achieved the similar
performance as the target model (written as predictor), and
considerably high consistency on both datasets.
We also conducted the ablation study to clarify the inÔ¨Çuence of three variables x, y, s on the quality of explanations.
We measured the accuracy in the same manner as above except that we dropped one variable by replacing the vector
Ô¨Ålled by 0 when generating explanations. The results in Table 3 exhibits that our models put the most importance on
the category label out of three. These results are reasonable
because it contains information for which the explanation
should be discriminative.
The other important aspect for the Ô¨Ådelity in our task is
the correctness of the linguistic explanation. In particular,
the attribute value (e.g., ‚Äúred‚Äù or ‚Äúblue‚Äù for attribute type
‚Äúcolor‚Äù) is also estimated during the inference. We evaluated the validity by comparing the predicted attributes with
that of grand-truth on the test set. The attribute value that
explains the output of the predictor y the best will be selected as written in subsection 3.6. As baselines, we employed the three layers perceptron with a hidden layer of
512 dimensions (predict). It was separately trained for each
type of attribute with SGD. Moreover, we also report the
performance when the attribute is randomly selected (random). We measured the accuracy and the results are shown
in the Table 2. As shown, our method, which generates linguistic explanations through selecting examples, can predict it as accurate as the direct estimation, and the accuracy
is much better than the random selection.
# of explanations (M)
2 4 6 8 101214161820222426
# of explanations (M)
Figure 5: The mean accuracy of identifying the linguistic
explanation from the examples on AADB (left) and CUB
(right) dataset. The y-axis and x-axis indicates the accuracy
and the number of generated explnations.
4.3. Complementarity
To quantify the complementarity of explanations, we investigate how the example-based explanation D renders the
linguistic explanation s identiÔ¨Åable. SpeciÔ¨Åcally, utilizing
the reasoner q(y|x, s, D), which is trained to reason the target from the explanation, we conÔ¨Årmed whether it can reason to the correct conclusion only from the generated explanation pair as discussed in subsection 3.7. For generated
pairs of explanations (s1, D1), (s2, D2), ..., (sM, DM) of
which attribute type is identical, we computed the output of
the reasoner as qij = q(y|si, Dj) (1 ‚â§i, j ‚â§M) for y
obtained from the predictor. Selecting the index having the
maximum value for all j as i‚ãÜ= arg maxi qij, we veriÔ¨Åed
i‚ãÜ= j. The mean accuracy is compared with a baseline that
outputs the same examples for all si and results are shown
in Fig. 5. The x-axis of the Ô¨Ågure indicates the number of
the generated explanations (i.e., M).
On both datasets, the accuracy of our model is better than
the baseline. Furthermore, as shown in Fig. 6, we observe
that the diagonal element has a high value on the confusion matrix. These results demonstrate the ability of our
method to generate complemental explanations. The difference in the performance between the proposed method and
baseline on AADB is lower than on CUB. We conjecture
that one reason is the difference between appearance and attributes. AADB dataset contains highly-semantic attributes
(e.g., ‚Äúcolor harmony‚Äù) compared with those in CUB (e.g.,
‚Äúcolor‚Äù or ‚Äúshape‚Äù). Such the semantic gap may hinder to
construct the discriminative set which renders the attribute
identiÔ¨Åable.
4.4. Output example
An output of our system on CUB dataset when the number of explanations is two is shown in Fig. 7. In this example, the combination of linguistic and example-based explanation seems compatible, where it will not make sense
Figure 6: The confusion matrix of identifying the attribute
type from the examples on AADB (left) and CUB (right)
throat color
upper tail color
is red cockaded Woodpecker
Figure 7: Example output of our system on CUB dataset.
if these pairs are switched. For instance, the below linguistic explanation ‚Äúthroat color is white‚Äù may not be a good
explanation for the above examples.
Although not the primary scope in this work, the proposed task may be extended to machine teaching task,
where the machine teaches to human by showing examples
iteratively.
5. Conclusion
In this work, we performed a novel task, that is, generating visual explanations with linguistic and visual examples
that are complemental to each other. We proposed to parameterize the joint probability of variables to explain, and
to be explained by the three neural networks. To explicitly treat the complementarity, auxiliary models responsible
for the explanations were trained simultaneously to maximize the approximated lower bound of the interaction information. We empirically demonstrated the effectiveness
of the method by the experiments conducted on the two visual recognition datasets.
6. Acknowledgement
This work was partially supported by JST CREST Grant
Number JPMJCR1403, Japan, and partially supported by
the Ministry of Education, Culture, Sports, Science and
Technology (MEXT) as ‚ÄùSeminal Issue on Post-K Computer.‚Äù Authors would like to thank Hiroharu Kato, Toshihiko Matsuura for helpful discussions.