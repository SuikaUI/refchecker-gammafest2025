The Kernel Polynomial Method
Alexander Weiße
School of Physics, The University of New South Wales, Sydney, NSW 2052, Australia∗
Gerhard Wellein
Regionales Rechenzentrum Erlangen, Universit¨at Erlangen, 91058 Erlangen, Germany
Andreas Alvermann and Holger Fehske
Institut f¨ur Physik, Ernst-Moritz-Arndt-Universit¨at Greifswald, 17487 Greifswald, Germany
 
Eﬃcient and stable algorithms for the calculation of spectral quantities and correlation functions
are some of the key tools in computational condensed matter physics. In this article we review basic properties and recent developments of Chebyshev expansion based algorithms and the Kernel
Polynomial Method. Characterized by a resource consumption that scales linearly with the problem dimension these methods enjoyed growing popularity over the last decade and found broad
application not only in physics. Representative examples from the ﬁelds of disordered systems,
strongly correlated electrons, electron-phonon interaction, and quantum spin systems we discuss
in detail. In addition, we illustrate how the Kernel Polynomial Method is successfully embedded
into other numerical techniques, such as Cluster Perturbation Theory or Monte Carlo simulation.
PACS numbers: 02.70.Hm, 02.30.Mv, 71.15.-m
I. Introduction
II. Chebyshev expansion and the Kernel Polynomial
Method (KPM)
A. Basic features of Chebyshev expansion
1. Chebyshev polynomials
2. Modiﬁed moments
B. Calculation of moments
1. General considerations
2. Stochastic evaluation of traces
C. Kernel polynomials and Gibbs oscillations
1. Expansions of ﬁnite order & simple kernels
2. Fej´er kernel
3. Jackson kernel
4. Lorentz kernel
D. Implementational details and remarks
1. Discrete cosine & Fourier transforms
2. Integrals involving expanded functions
E. Generalization to higher dimension
1. Expansion of multivariate functions
2. Kernels for multidimensional expansions
3. Reconstruction with cosine transforms
III. Applications of KPM
A. Densities of states
1. General considerations
2. Non-interacting systems: Anderson model of
3. Interacting systems: Double exchange
B. Static correlations at ﬁnite temperature
C. Dynamical correlations at zero temperature
1. General considerations
∗New address: Institut f¨ur Physik, Ernst-Moritz-Arndt-Universit¨at
Greifswald, 17487 Greifswald, Germany
2. One-particle spectral function
3. Optical conductivity
4. Spin structure factor
D. Dynamical correlations at ﬁnite temperature
1. General considerations
2. Optical conductivity of the Anderson model
3. Optical conductivity of the Holstein model
IV. KPM as a component of other methods
A. Monte Carlo simulations
B. Cluster Perturbation Theory (CPT)
1. General features of CPT
2. CPT for the Hubbard model
3. CPT for the Holstein model
V. KPM versus other numerical approaches
A. KPM and dedicated many-particle techniques
B. Close relatives of KPM
1. Chebyshev expansion and Maximum Entropy
2. Lanczos recursion
3. Projection methods
VI. Conclusions & Outlook
Acknowledgements
References
I. INTRODUCTION
In most areas of physics the fundamental interactions
and the equations of motion that govern the behavior of
real systems on a microscopic scale are very well known,
but when it comes to solving these equations they turn
out to be exceedingly complicated. This holds, in particular, if a large and realistic number of particles is in-
Inventing and developing suitable approximations and analytical tools has therefore always been a
cornerstone of theoretical physics.
Recently, however,
research continued to focus on systems and materials,
whose properties depend on the interplay of many different degrees of freedom or on interactions that compete on similar energy scales. Analytical and approximate methods quite often fail to describe the properties
of such systems, so that the use of numerical methods remains the only way to proceed. On the other hand, the
available computer power increased tremendously over
the last decades, making direct simulations of the microscopic equations for reasonable system sizes or particle
numbers more and more feasible. The success of such
simulations, though, depends on the development and
improvement of eﬃcient algorithms. Corresponding research therefore plays an increasingly important role.
On a microscopic level the behavior of most physical
systems, like their thermodynamics or response to external probes, depends on the distribution of the eigenvalues
and the properties of the eigenfunctions of a Hamilton operator or dynamical matrix. In numerical approaches the
latter correspond to Hermitian matrices of ﬁnite dimension D, which can become huge already for a moderate
number of particles, lattice sites or grid points. The calculation of all eigenvalues and eigenvectors then easily
turns into an intractable task, since for a D-dimensional
matrix in general it requires memory of the order of D2,
and the number of operations and the computation time
scale as D3. Of course, this large resource consumption
severely restricts the size of the systems that can be studied by such a “naive” approach. For dense matrices the
limit is currently of the order of D ≈105, and for sparse
matrices the situation is only slightly better.
Fortunately, alternatives are at hand: In the present
article we review basic properties and recent developments of numerical Chebyshev expansion and of the Kernel Polynomial Method (KPM). As the most time consuming step these iterative approaches require only multiplications of the considered matrix with a small set of
vectors, and therefore allow for the calculation of spectral properties and dynamical correlation functions with
a resource consumption that scales linearly with D for
sparse matrices, or like D2 otherwise. If the matrix is
not stored but constructed on-the-ﬂy dimensions of the
order of D ≈109 or more are accessible.
The ﬁrst step to achieve this favorable behavior is
setting aside the requirement for a complete and exact
knowledge of the spectrum. A natural approach, which
has been considered from the early days of quantum mechanics, is the characterization of the spectral density
ρ(E) in terms of its moments µl =
iteration these moments can usually be calculated very
eﬃciently, but practical implementations in the context
of Gaussian quadrature showed that the reconstruction
of ρ(E) from ordinary power moments is plagued by substantial numerical instabilities . These
occur mainly because the powers El put too much weight
to the boundaries of the spectrum at the expense of poor
precision for intermediate energies. The observation of
this deﬁciency advanced the development of modiﬁed
moment approaches , where El is replaced by (preferably orthogonal)
polynomials of E.
With studies of the spectral density of harmonic solids and
of autocorrelation functions , which made
use of Chebyshev polynomials of second kind, these ideas
soon found their way into physics application.
similar Chebyshev expansion methods became popular
also in quantum chemistry, where the focus was on
the time evolution of quantum states and on Filter Diagonalization .
The modiﬁed moment approach noticeably improved when kernel polynomials
were introduced to damp the Gibbs oscillations, which
for truncated polynomial series occur near discontinuities of the expanded function . Only recently this range was
extended to cover also dynamical correlation functions at
ﬁnite-temperature , and below we present
some new applications to complex-valued quantities, e.g.
Green functions. Being such a general tool for studying
large matrix problems, KPM can also be used as a core
component of more involved numerical techniques. As recent examples we discuss Monte Carlo (MC) simulations
and Cluster Perturbation Theory (CPT).
In parallel to Chebyshev expansion techniques and
to KPM also the Lanczos Recursion Method was
developed , which is based on a recursive Lanczos tridiagonalization of the
considered matrix and the expression of the spectral density or of correlation functions in terms of continued fractions. The approach, in general, is applicable to the same
problems as KPM and found wide application in solid
state physics .
It suﬀers, however,
from the shortcomings of the Lanczos algorithm, namely
loss of orthogonality and spurious degeneracies if extremal eigenstates start to converge. We will compare the
two methods in Sec. V and explain, why we prefer to use
Lanczos for the calculation of extremal eigenstates and
KPM for the calculation of spectral properties and correlation functions. In addition, we will comment on more
specialized iterative schemes, such as projection methods and Maximum Entropy ap-
proaches . Drawing more attention to KPM
as a potent alternative to all these techniques is one of
the purposes of the present work.
The outline of the article is as follows: In Sec. II we
give a detailed introduction to Chebyshev expansion and
the Kernel Polynomial Method, its mathematical background, convergence properties and practical aspects of
its implementation. In Sec. III we apply KPM to a variety of problems from solid state physics. Thereby, we
focus mainly on illustrating the types of quantities that
can be calculated with KPM, rather than on the physics
of the considered models. In Sec. IV we show how KPM
can be embedded into other numerical approaches that
require knowledge of spectral properties or correlation
functions, namely Monte Carlo simulation and Cluster
Perturbation Theory. In Sec. V we shortly discuss alternatives to KPM and compare their performance and
precision, before summarizing in Sec. VI.
II. CHEBYSHEV EXPANSION AND THE KERNEL
POLYNOMIAL METHOD (KPM)
A. Basic features of Chebyshev expansion
1. Chebyshev polynomials
Let us ﬁrst recall the basic properties of expansions in
orthogonal polynomials and of Chebyshev expansion in
particular. Given a positive weight function w(x) deﬁned
on the interval [a, b] we can introduce a scalar product
w(x)f(x)g(x) dx
between two integrable functions f, g : [a, b] →R. With
respect to each such scalar product there exists a complete set of polynomials pn(x), which fulﬁl the orthogonality relations
⟨pn|pm⟩= δn,m/hn ,
where hn = 1/⟨pn|pn⟩denotes the inverse of the squared
norm of pn(x). These orthogonality relations allow for an
easy expansion of a given function f(x) in terms of the
pn(x), since the expansion coeﬃcients are proportional
to the scalar products of f and pn,
αn = ⟨pn|f⟩hn .
In general, all types of orthogonal polynomials can
be used for such an expansion and for the Kernel Polynomial approach we discuss in this article
 ).
However, as we
frequently observe whenever we work with polynomial expansions , Chebyshev polynomials of ﬁrst
and second kind turn out to be the best choice for most
applications, mainly due to the good convergence properties of the corresponding series and to the close relation
to Fourier transform . The
latter is also an important prerequisite for the derivation
of optimal kernels (see Sec. II.C), which are required for
the regularization of ﬁnite-order expansions, and which
so far have not been derived for other sets of orthogonal
polynomials.
Both sets of Chebyshev polynomials are deﬁned on the
interval [a, b] = [−1, 1], where the weight function w(x) =
1 −x2)−1 yields the polynomials of ﬁrst kind, Tn,
and the weight function w(x) = π
1 −x2 those of second
kind, Un. Based on the scalar products
1 −x2 dx ,
1 −x2 f(x) g(x) dx ,
the orthogonality relations thus read
⟨Tn|Tm⟩1 = 1+δn,0
⟨Un|Um⟩2 = π2
By substituting x = cos(ϕ) one can easily verify that they
correspond to the orthogonality relations of trigonometric functions, and that in terms of those the Chebyshev
polynomials can be expressed in explicit form,
Tn(x) = cos(n arccos(x)) ,
Un(x) = sin((n + 1) arccos(x))
sin(arccos(x))
These expressions can then be used to prove the recursion
relations,
T0(x) = 1 ,
T−1(x) = T1(x) = x ,
Tm+1(x) = 2 x Tm(x) −Tm−1(x) ,
U0(x) = 1 ,
U−1(x) = 0 ,
Um+1(x) = 2 x Um(x) −Um−1(x) ,
which illustrate that Eqs. (8) and (9) indeed describe
polynomials, and which, moreover, are an integral part
of the iterative numerical scheme we develop later on.
Two other useful relations are
2 Tm(x)Tn(x) = Tm+n(x) + Tm−n(x) ,
2 (x2 −1) Um−1(x)Un−1(x) = Tm+n(x) −Tm−n(x) .
When calculating Green functions we also need Hilbert
transforms of the polynomials (Abramowitz and Stegun,
1 −y2 = π Un−1(x) ,
1 −y2 Un−1(y) dy
= −π Tn(x) ,
where P denotes the principal value. Chebyshev polynomials have many more interesting properties, for a detailed discussion we refer the reader to text books such
as .
2. Modiﬁed moments
As sketched above, the standard way of expanding a
function f : [−1, 1] →R in terms of Chebyshev polynomials of ﬁrst kind is given by
Tn(x) = α0 + 2
αn Tn(x) (16)
with coeﬃcients
αn = ⟨f|Tn⟩1 =
1 −x2 dx .
However, the calculation of these coeﬃcients requires integrations over the weight function w(x), which in practical applications to matrix problems prohibits a simple
iterative scheme. The solution to this problem follows
from a slight rearrangement of the expansion, namely
with coeﬃcients
f(x)Tn(x) dx .
More formally this rearrangement of the Chebyshev series
corresponds to using the second scalar product ⟨.|.⟩2 and
expanding in terms of the orthogonal functions
which fulﬁl the orthogonality relations
⟨φn|φm⟩2 = 1+δn,0
The expansion in Eq. (18) is thus equivalent to
with moments
µn = ⟨f|φn⟩2 =
f(x)Tn(x) dx .
The µn now have the form of modiﬁed moments that
we announced in the introduction, and Eqs. (18) and (19)
represent the elementary basis for the numerical method
which we review in this article. In the remaining sections
we will explain how to translate physical quantities into
polynomial expansions of the form of Eq. (18), how to
calculate the moments µn in practice, and, most importantly, how to regularize expansions of ﬁnite order.
Naturally, the moments µn depend on the considered
quantity f(x) and on the underlying model.
specify these details when discussing particular applications in Sec. III. Nevertheless, there are features which
are similar to all types of applications, and we start with
presenting these general aspects in what follows.
B. Calculation of moments
1. General considerations
A common feature of basically all Chebyshev expansions is the requirement for a rescaling of the underlying
matrix or Hamiltonian H. As we described above, the
Chebyshev polynomials of both ﬁrst and second kind are
deﬁned on the real interval [−1, 1], whereas the quantities we are interested in usually depend on the eigenvalues {Ek} of the considered (ﬁnite-dimensional) matrix.
To ﬁt this spectrum into the interval [−1, 1] we apply a
simple linear transformation to the Hamiltonian and all
energy scales,
˜H = (H −b)/a ,
˜E = (E −b)/a ,
and denote all rescaled quantities with a tilde hereafter.
Given the extremal eigenvalues of the Hamiltonian, Emin
and Emax, which can be calculated, e.g. with the Lanczos
algorithm , or for which bounds may be
known analytically, the scaling factors a and b read
a = (Emax −Emin)/(2 −ǫ) ,
b = (Emax + Emin)/2 .
The parameter ǫ is a small cut-oﬀintroduced to avoid
stability problems that arise if the spectrum includes or
exceeds the boundaries of the interval [−1, 1]. It can be
ﬁxed, e.g. to ǫ = 0.01, or adapted to the resolution of
the calculation, which for an expansion of ﬁnite order N
is proportional 1/N (see below).
The next similarity of most Chebyshev expansions is
the form of the moments, namely their dependence on
the matrix or Hamiltonian ˜H. In general, we ﬁnd two
types of moments: Simple expectation values of Chebyshev polynomials in ˜H,
µn = ⟨β|Tn( ˜H)|α⟩,
where |α⟩and |β⟩are certain states of the system, or
traces over such polynomials and a given operator A,
µn = Tr[A Tn( ˜H)] .
Handling the ﬁrst case is rather straightforward. Starting from the state |α⟩we can iteratively construct the
states |αn⟩= Tn( ˜H)|α⟩by using the recursion relations
for the Tn, Eq. (10),
|α0⟩= |α⟩,
|α1⟩= ˜H|α0⟩,
|αn+1⟩= 2 ˜H|αn⟩−|αn−1⟩.
Scalar products with |β⟩then directly yield
µn = ⟨β|αn⟩.
This iterative calculation of the moments, in particular
the application of ˜H to the state |αn⟩, represents the
most time consuming part of the whole expansion approach and determines its performance. If ˜H is a sparse
matrix of dimension D the matrix vector multiplication
is an order O(D) process and the calculation of N moments therefore requires O(ND) operations and time.
The memory consumption depends on the implementation. For moderate problem dimension we can store the
matrix and, in addition, need memory for two vectors
of dimension D. For very large D the matrix certainly
does not ﬁt into the memory and has to be reconstructed
on-the-ﬂy in each iteration or retrieved from disc. The
two vectors then determine the memory consumption of
the calculation.
Overall, the resource consumption of
the moment iteration is similar or even slightly better
than that of the Lanczos algorithm, which requires a few
more vector operations (see our comparison in Sec. V).
In contrast to Lanczos, Chebyshev iteration is completely
stable and can be carried out to arbitrary high order.
The moment iteration can be simpliﬁed even further,
if |β⟩= |α⟩. In this case the product relation (12) allows
for the calculation of two moments from each new |αn⟩,
µ2n = 2⟨αn|αn⟩−µ0 ,
µ2n+1 = 2⟨αn+1|αn⟩−µ1 ,
which is equivalent to two moments per matrix vector
multiplication. The numerical eﬀort for N moments is
thus reduced by a factor of two. In addition, like many
other numerical approaches KPM beneﬁts considerably
from the use of symmetries that reduce the Hilbert space
dimension.
2. Stochastic evaluation of traces
The second case where the moments depend on a trace
over the whole Hilbert space, at ﬁrst glance, looks far
more complicated. Based on the previous considerations
we would estimate the numerical eﬀort to be proportional
to D2, because the iteration needs to be repeated for all
D states of a given basis. It turns out, however, that
extremely good approximations of the moments can be
obtained with a much simpler approach: the stochastic evaluation of the trace , i.e., an estimate
of µn based on the average over only a small number
R ≪D of randomly chosen states |r⟩,
µn = Tr[A Tn( ˜H)] ≈1
⟨r|A Tn( ˜H)|r⟩.
The number of random states, R, does not scale with D.
It can be kept constant or even reduced with increasing
D. To understand this, let us consider the convergence
properties of the above estimate. Given an arbitrary basis {|i⟩} and a set of independent identically distributed
random variables ξri ∈C, which in terms of the statistical average
= δrr′δij ,
a random vector is deﬁned through
We can now calculate the statistical expectation value of
the trace estimate Θ = 1
r=0 ⟨r|B|r⟩for some Hermitian operator B with matrix elements Bij = ⟨i|B|j⟩, and
indeed ﬁnd,
Bii = Tr(B) .
Of course, this only shows that we obtain the correct
result on average. To assess the associated error we also
need to study the ﬂuctuation of Θ, which is characterized
by (δΘ)2 =
2. Evaluating
⟨r|B|r⟩⟨r′|B|r′⟩
i,j,i′,j′=0
i,j,i′,j′=0
δijδi′j′BijBi′j′
i,j,i′,j′=0
(Tr B)2 + 1
= (Tr B)2 + 1
Tr(B2) + (
we get for the ﬂuctuation
Tr(B2) + (
The trace of B2 will usually be of order O(D), and the
relative error of the trace estimate, δΘ/Θ, is thus of order
RD). It is this favorable behavior, which ensures
the convergence of the stochastic approach, and which
was the basis for our initial statement that the number
of random states R ≪D can be kept small or even be
reduced with the problem dimension D.
Note also that the distribution of the elements of
|r⟩, p(ξri), has a slight inﬂuence on the precision of
the estimate, since it determines the expectation value
that enters Eq. (43). For an optimal distribution
should be as close as possible to its lower bound
2 = 1, and indeed, we ﬁnd this result if we ﬁx the
amplitude of the ξri and allow only for a random phase
φ ∈[0, 2π], ξri = eiφ. Moreover, if we were working in the
eigenbasis of B this would cause δΘ to vanish entirely,
which led Iitaka and Ebisuzaki to conclude that
random phase vectors are the optimal choice for stochastic trace estimates. However, all these considerations depend on the basis that we are working in, which in practice will never be the eigenbasis of B (in particular, if B
corresponds to something like A Tn( ˜H), as in Eq. (36)).
A random phase vector in one basis does not necessarily correspond to a random phase vector in another basis, but the other basis may well lead to smaller value
jj, thus compensating for the larger value of
. Presumably, the most natural choice are Gaussian distributed ξri, which lead to
= 2 and thus a
basis-independent ﬂuctuation (δΘ)2. To summarize this
section, we think that the actual choice of the distribution of ξri is not of high practical signiﬁcance, as long as
Eqs. (37)–(39) are fulﬁlled for ξri ∈C, or
= δrr′δij ,
hold for ξri ∈R.
Typically, within this article we
will consider Gaussian or uniformly distributed variables ξri ∈R.
C. Kernel polynomials and Gibbs oscillations
1. Expansions of ﬁnite order & simple kernels
In the preceding sections we introduced the basic ideas
underlying the expansion of a function f(x) in an inﬁnite
series of Chebyshev polynomials, and gave a few hints for
the numerical calculation of the expansion coeﬃcients µn.
As expected for a numerical approach, however, the total
number of these moments will remain ﬁnite, and we thus
arrive at a classical problem of approximation theory.
Namely, we are looking for the best (uniform) approximation to f(x) by a polynomial of given maximal degree,
which in our case is equivalent to ﬁnding the best approximation to f(x) given a ﬁnite number N of moments
µn. To our advantage, such problems have been studied for at least 150 years and we can make use of results
by many renowned mathematicians, such as Chebyshev,
Weierstrass, Dirichlet, Fej´er, Jackson, to name only a
few. We will also introduce the concept of kernels, which
facilitates the study of the convergence properties of the
mapping f(x) →fKPM(x) from the considered function
f(x) to our approximation fKPM(x).
Experience shows that a simple truncation of an inﬁnite series,
leads to poor precision and ﬂuctuations — also known
as Gibbs oscillations — near points where the function
f(x) is not continuously diﬀerentiable. The situation is
even worse for discontinuities or singularities of f(x), as
we illustrate below in Figure 1. A common procedure to
damp these oscillations relies on an appropriate modiﬁcation of the expansion coeﬃcients, µn →gnµn, which
depends on the order of the approximation N,
gnµn Tn(x)
In more abstract terms this truncation of the inﬁnite series to order N together with the corresponding modiﬁcation of the coeﬃcients is equivalent to the convolution
of f(x) with a kernel of the form
KN(x, y) = g0φ0(x)φ0(y) + 2
gnφn(x)φn(y) ,
1 −y2 KN(x, y) f(y) dy
= ⟨KN(x, y)|f(y)⟩2 .
The problem now translates into ﬁnding an optimal kernel KN(x, y), i.e., coeﬃcients gn, where the notion of
“optimal” partially depends on the considered application.
The simplest kernel, which is usually attributed to
Dirichlet, is obtained by setting gD
n = 1 and evaluating
the sum with the help of the Christoﬀel-Darboux identity ,
N (x, y) = φ0(x)φ0(y) + 2
φn(x)φn(y)
= φN(x)φN−1(y) −φN−1(x)φN(y)
Obviously, convolution of KD
N with an integrable function
f yields the above truncated series, Eq. (46), which for
N →∞converges to f within the integral norm deﬁned
by the scalar product Eq. (5), ||f||2 =
⟨f|f⟩2, i.e. we
||f −fKPM||2
This is, of course, not particularly restrictive and leads
to the disadvantages we mentioned earlier.
2. Fej´er kernel
A ﬁrst improvement is due to Fej´er who showed
that for continuous functions an approximation based on
the kernel
N(x, y) = 1
ν (x, y) ,
converges uniformly in any restricted interval [−1+ǫ, 1−
ǫ]. This means that now the absolute diﬀerence between
the function f and the approximation fKPM goes to zero,
||f −fKPM||ǫ
−1+ǫ<x<1−ǫ|f(x) −fKPM(x)|
Owing to the denominator in the expansion (46) convergence is not uniform in the vicinity of the endpoints
x = ±1, which we accounted for by the choice of a small
ǫ in the rescaling of the Hamiltonian H →˜H.
The more favorable uniform convergence is obtained
under very general conditions. Speciﬁcally, it suﬃces to
demand that:
1. The kernel is positive: KN(x, y) > 0 ∀x, y ∈[−1, 1].
2. The kernel is normalized,
−1 K(x, y) dx = φ0(y),
which is equivalent to g0 = 1.
3. The second coeﬃcient g1 approaches 1 as N →∞.
Then, as a corollary to Korovkin’s theorem , an approximation based on KN(x, y) converges
uniformly in the sense explicated for the Fej´er kernel.
The coeﬃcients gn, n ≥2 are restricted only through
the positivity of the kernel, the latter one being equivalent to monotonicity of the mapping f →fKPM, i.e.
KPM. Note also that the conditions 1 and 2 are very useful for practical applications:
The ﬁrst ensures that approximations of positive quantities become positive, the second conserves the integral
of the expanded function,
fKPM(x) dx =
Applying the kernel, for example, to a density of states
thus yields an approximation which is strictly positive
and normalized.
For a proof of the above theorem we refer the reader
to the literature . Let us
here only check that the Fej´er kernel indeed fulﬁls the
conditions 1 to 3: The last two are obvious by inspection
of Eq. (52). To prove the positivity we start from the
positive 2π-periodic function
with arbitrary aν ∈R. Straight-forward calculation then
aνaµ ei(ν−µ)ϕ =
aνaµ cos(ν −µ)ϕ
aνaν+n cos nϕ .
Hence, with
the function
p(ϕ) = g0 + 2
is positive and periodic in ϕ. However, if p(ϕ) is positive,
then the expression 1
2[p(arccosx+arccos y)+p(arccosx−
arccosy)] is positive ∀x, y ∈[−1, 1]. Using Eq. (8) and
cos α cos β = 1
2[cos(α + β) + cos(α −β)], we immediately
observe that the general kernel KN(x, y) from Eq. (48) is
positive ∀x, y ∈[−1, 1], if the coeﬃcients gn depend on
arbitrary coeﬃcients aν ∈R via Eq. (57). Setting aν =
N yields the Fej´er kernel KF
N(x, y), thus immediately
proving its positivity.
In terms of its analytical properties and of the convergence in the limit N →∞the Fej´er kernel is a major improvement over the Dirichlet kernel. However, as yet we
did not quantify the actual error of an order-N approximation: For continuous functions an appropriate scale is
given by the modulus of continuity,
|x−y|≤∆|f(x) −f(y)| ,
in terms of which the Fej´er approximation fulﬁls
||f −fKPM||∞∼wf(1/
For suﬃciently smooth functions this is equivalent to an
error of order O(1/
N). The latter is also an estimate for
the resolution or broadening that we will observe when
expanding less regular functions containing discontinuities or singularities, like the examples in Figure 1.
3. Jackson kernel
With the coeﬃcients gF
n of the Fej´er kernel we have not
fully exhausted the freedom oﬀered by the coeﬃcients aν
and Eq. (57). We can hope to further improve the kernel
by optimizing the aν in some sense, which will lead us to
recover old results by Jackson .
In particular, let us tighten the third of the previously
deﬁned conditions for uniform convergence by demanding
that the kernel has optimal resolution in the sense that
(x −y)2KN(x, y) dx dy
is minimal. Since KN(x, y) will be peaked at x = y, Q is
basically the squared width of this peak. For suﬃciently
smooth functions this more stringent condition will minimize the error ||f −fKPM||∞, and in all other cases lead
to optimal resolution and smallest broadening of “sharp”
To express the variance Q of the kernel in terms of gn
and aν, respectively, note that
(x −y)2 = (T1(x) −T1(y))2
2(T2(x) + T0(x))T0(y) −2T1(x)T1(y)
2T0(x)(T2(y) + T0(y)) .
Using the orthogonality of the Chebyshev polynomials
and inserting Eqs. (48) and (62) into (61), we can thus
rephrase the condition of optimal resolution as
Q = g0 −g1
!= minimal w.r.t. aν .
Hence, compared to the previous section, where we
merely required g0 = 1 and g1 →1 for N →∞, our
new condition tries to optimize the rate at which g1 approaches unity.
Minimizing Q = g0 −g1 under the constraint C =
g0 −1 = 0 yields the condition
where λ is a Lagrange multiplier. Using Eq. (57) and
setting a−1 = aN = 0 we arrive at
2aν −aν−1 −aν+1 = λaν
which the alert reader recognizes as the eigenvalue problem of a harmonic chain with ﬁxed boundary conditions.
Its solution is given by
aν = ¯a sin πk(ν + 1)
λ = 1 −cos
where ν = 0, . . . , (N −1) and k = 1, 2, . . ., N. Given
aν and the abbreviation q = πk/(N + 1) we can easily
calculate the gn:
aνaν+n = ¯a2
sin qν sin q(ν + n)
[cos qn −cos q(2ν + n)]
(N −n) cos qn −Re
ei q(2ν+n)
2 [(N −n + 1) cosqn + sin qn cot q] .
The normalization g0 = 1 is ensured through ¯a2 =
2/(N + 1), and with g1 = cos q we can directly read oﬀ
the optimal value for
Q = g0 −g1 = 1 −cos
which is obtained for k = 1,
Qmin = 1 −cos
The latter result shows that for large N the resolution
√Q of the new kernel is proportional to 1/N. Clearly,
this is an improvement over the Fej´er kernel KF
which gives only √Q = 1/
With the above calculation we reproduced results
by Jackson , who showed that with a similar kernel a continuous function f can be approximated
by a polynomial of degree N −1 such that
||f −fKPM||∞∼wf(1/N) ,
which we may interpret as an error of the order of
O(1/N). Hereafter we are thus referring to the new optimal kernel as the Jackson kernel KJ
N(x, y), with
(N −n + 1) cos
Before proceeding with other kernels let us add a few
more details on the resolution of the Jackson kernel: The
quantity √Qmin obtained in Eq. (69) is mainly a measure
for the spread of the kernel KJ
N(x, y) in the x-y-plane.
However, for practical calculations, which may also involve singular functions, it is often reasonable to ask for
the broadening of a δ-function under convolution with
the kernel,
δKPM(x −a) = ⟨KN(x, y)|δ(y −a)⟩2
= g0φ0(x)T0(a) + 2
gnφn(x)Tn(a) .
It can be characterized by the variance σ2 =
where we use x = T1(x) and x2 = [T2(x) + T0(x)]/2 to
x δKPM(x −a) dx = g1T1(a) ,
x2 δKPM(x −a) dx = g0T0(a) + g2T2(a)
Hence, for KJ
N(x, y) the squared width of δKPM(x −a) is
1 )2) + (gJ
= N −a2(N −1)
1 −a2 + 3a2 −2
δ function
Dirichlet kernel
Jackson kernel
Gaussian (σ=π/64)
Lorentz kernel (λ=4)
Lorentzian (ε=λ/64)
step function
FIG. 1 (Color in online edition) Order N = 64 expansions of
δ(x) (left) and a step function (right) based on diﬀerent kernels. Whereas the truncated series (Dirichlet kernel) strongly
oscillate, the Jackson results smoothly converge to the expanded functions. The Lorentz kernel leads to relatively poor
convergence at the boundaries x = ±1, but otherwise yields
perfect Lorentz-broadened approximations.
Using the Jackson kernel, an order N expansion of a δfunction at x = 0 thus results in a broadened peak of
width σ = π
N , whereas close to the boundaries, a = ±1,
we ﬁnd σ =
N 3/2 . It turns out that this peak is a good
approximation to a Gaussian,
which we illustrate in Figure 1.
4. Lorentz kernel
The Jackson kernel derived in the preceding sections
is the best choice for most of the applications we discuss
In some situations, however, special analytical
properties of the expanded functions become important,
which only other kernels can account for.
functions that appear in the Cluster Perturbation Theory, Sec. IV.B, are an example. Considering the imaginary part of the Plemelj-Dirac formula which frequently
occurs in connexion with Green functions,
x + i ǫ = P
−i πδ(x) ,
the δ-function on the right hand side is approached in
terms of a Lorentz curve,
x + i ǫ = lim
π(x2 + ǫ2) ,
which has a diﬀerent and broader shape compared to
the approximations of δ(x) we get with the Jackson
kernel. There are attempts to approximate Lorentzian
like behavior in the framework of ﬁlter diagonalization , but these solutions do not lead
to a positive kernel. Note that positivity of the kernel
is essential to guarantee basic properties of Green functions, e.g. that poles are located in the lower (upper) half
complex plane for a retarded (advanced) Green function.
Since we know that the Fourier transform of a Lorentz
peak is given by exp(−ǫ|k|), we can try to construct an
appropriate positive kernel assuming aν = e−λν/N in
Eq. (57), and indeed, after normalization, g0 = 1, this
yields what we call the Lorentz kernel KL
N(x, y) hereafter,
n = sinh[λ(1 −n/N)]
The variable λ is a free parameter of the kernel which
as a compromise between good resolution and suﬃcient
damping of the Gibbs oscillations we empirically choose
to be of the order of 3 . . . 5.
It is related to the ǫparameter of the Lorentz curve, i.e. to its resolution, via
ǫ = λ/N. Note also, that in the limit λ →0 we recover
the Fej´er kernel KF
N(x, y) with gF
n = 1 −n/N, suggesting
that both kernels share many of their properties.
In Figure 1 we compare truncated Chebyshev expansions — equivalent to using the Dirichlet kernel —
to the approximations obtained with the Jackson and
Lorentz kernels, which we will later use almost exclusively. Clearly, both kernels yield much better approximations to the expanded functions and, in particular,
the oscillations have disappeared almost completely. The
comparison with a Gaussian or Lorentzian, respectively,
illustrates the nature of the broadening of a δ-function
under convolution with the kernels, which later on will facilitate the interpretation of our numerical results. With
Table I we conclude this section on kernels, and, for the
sake of completeness, also list two other kernels that are
occasionally used in the literature. Both have certain disadvantages, in particular, they are not strictly positive.
D. Implementational details and remarks
1. Discrete cosine & Fourier transforms
Having discussed the theory behind Chebyshev expansion, the calculation of moments, and the various kernel
approximations, let us now come to the practical issues of
the implementation of KPM, namely to the reconstruction of the expanded function f(x) from its moments µn.
Knowing a ﬁnite number N of coeﬃcients µn (see Sec. III
for examples and details), we usually want to reconstruct
f(x) on a ﬁnite set of abscissas xk. Naively we could sum
up Eq. (47) separately for each point, thereby making use
of the recursion relations for Tn, i.e.,
gnµn Tn(xk)
For a set {xk} containing ˜N points these summations
would require of the order of N ˜N operations. We can
do much better, however, remembering the deﬁnition of
the Chebyshev polynomials Tn, Eq. (8), and the close
relation between KPM and Fourier expansion: First, we
may introduce the short-hand notation
˜µn = µngn
for the kernel improved moments. Second and more important, we make a special choice for our data points,
xk = cos π(k + 1/2)
k = 0, . . . , .
number ˜N of points in the set {xk} is not necessarily
the same as the number of moments N. Usually we will
consider ˜N ≥N and a reasonable choice is, e.g. ˜N = 2N.
All values f(xk) can now be obtained through a discrete
cosine transform,
πn(k + 1/2)
which allows for the use of divide-and-conquer type algorithms that require only ˜
N log ˜N operations — a clear
advantage over the above estimate N ˜N.
Routines for fast discrete cosine transform are implemented in many mathematical libraries or Fast
Fourier Transform (FFT) packages, for instance, in
FFTW that ships with
most Linux distributions. If no direct implementation is
at hand we may also use fast discrete Fourier transform.
(2 −δn,0) ˜µn exp
and the standard deﬁnition of discrete Fourier transform,
after some reordering we ﬁnd for an even number of data
γ2j = Re(˜λj) ,
γ2j+1 = Re(˜λ ˜
with j = 0, . . . , ˜N/2−1. If we need only a discrete cosine
transform this setup is not optimal, as it makes no use
of the imaginary part which the complex FFT calculates.
It turns out, however, that the “wasted” imaginary part
is exactly what we need when we later calculate Green
Parameters positive? Remarks
N+1[(N −n + 1) cos
best for most applications
sinh[λ(1 −n/N)]/ sinh(λ)
best for Green functions
mainly of academic interest
= 3 closely matches the Jackson
positive 
Wang and Zunger
found empirically, not optimal 
least favorable choice
TABLE I Summary of diﬀerent integral kernels that can be used to improve the quality of an order N Chebyshev series. The
coeﬃcients gn refer to Eq. (47) or (48), respectively.
functions and other complex quantities, i.e., we can use
γ2j = ˜λj ,
γ2j+1 = ˜λ∗
to evaluate Eq. (140).
2. Integrals involving expanded functions
We have already mentioned that our particular choice
of xk corresponds to the abscissas of numerical Chebyshev integration. Hence, Gauss-type numerical approximations to integrals of the form
−1 f(x)g(x)dx become simple sums,
f(x)g(x) dx =
1 −x2f(x)g(x)
k f(xk)g(xk) = 1
where γk denotes the raw output of the cosine or Fourier
transforms deﬁned in Eq. (83). We can use this feature,
for instance, to calculate partition functions, where f(x)
corresponds to the expansion of the spectral density ρ(E)
and g(x) to the Boltzmann or Fermi weight.
E. Generalization to higher dimension
1. Expansion of multivariate functions
For the calculation of ﬁnite-temperature dynamical
correlation functions we will later need expansions of
functions of two variables.
Let us therefore comment
on the generalization of the previous considerations to ddimensional space, which is easily obtained by extending
the scalar product ⟨.|.⟩2 to functions f, g : [−1, 1]d →R,
f(⃗x)g(⃗x)
dx1 . . . dxd .
Here xj denote the d components of the vector ⃗x. Naturally, this scalar product leads to the expansion
⟨φ⃗n|φ⃗n⟩2
⃗n=⃗0 µ⃗nh⃗n
j=1 Tnj(xj)
where we introduced a vector notation for indices, ⃗n =
{n1, . . . , nd}, and the following functions and coeﬃcients
µ⃗n = ⟨f|φ⃗n⟩2
dx1 . . . dxd ,
⟨φ⃗n|φ⃗n⟩2
2. Kernels for multidimensional expansions
As in the one-dimensional case, a simple truncation of
the inﬁnite series will lead to Gibbs oscillations and poor
convergence. Fortunately, we can easily generalize our
previous results for kernel approximations. In particular,
we ﬁnd that the extended kernel
KN(⃗x, ⃗y) =
KN(xj, yj)
maps an inﬁnite series onto an truncated series,
fKPM(⃗x) = ⟨KN(⃗x, ⃗y)|f(⃗y)⟩2
⃗n=⃗0 µ⃗nh⃗n
j=1 gnjTnj(xj)
where we can take the gn of any of the previously discussed kernels. If we use the gJ
n of the Jackson kernel,
N(⃗x, ⃗y) fulﬁls generalizations of our conditions for an
optimal kernel, namely
N(⃗x, ⃗y) is positive ∀⃗x, ⃗y ∈[−1, 1]d.
N(⃗x, ⃗y) is normalized with
fKPM(⃗x) dx1 . . . dxd =
f(⃗x) dx1 . . . dxd .
N(⃗x, ⃗y) has optimal resolution in the sense that
(⃗x −⃗y)2KN(⃗x, ⃗y) dx1 . . . dxd dy1 . . . dyd
= d(g0 −g1)
is minimal.
Note that for simplicity the order of the expansion, N,
was chosen to be the same for all spatial directions. Of
course, we could also deﬁne more general kernels,
N(⃗x, ⃗y) =
KNj(xj, yj) ,
where the vector ⃗N denotes the orders of expansion for
the diﬀerent spatial directions.
3. Reconstruction with cosine transforms
Similar to the 1D case we may consider the function
f : [−1, 1]d →R on a discrete grid ⃗x⃗k with
x⃗k,j = cos(ϕkj) ,
ϕkj = π(kj + 1/2)
kj = 0, . . . , ( ˜N −1) .
Note again that we could deﬁne individual numbers of
points for each spatial direction, i.e., a vector ⃗˜N with
elements ˜Nj instead of a single ˜N. For all grid points
⃗x⃗k the function f(⃗x⃗k) is obtained through multidimensional discrete cosine transform, i.e., with coeﬃcients
κ⃗n = ˜µ⃗nh⃗n = µ⃗ng⃗nh⃗n we ﬁnd
γ⃗k = f(cos(ϕk1), . . . , cos(ϕkd))
π sin(ϕkj)
cos(njϕkj)
cos(n1ϕk1) . . .
cos(ndϕkd)κ⃗n .
The last line shows that the multidimensional discrete
cosine transform is equivalent to a nesting of onedimensional transforms in every coordinate. With fast
implementations the computational eﬀort is thus proportional to d ˜N d−1 ˜
N log ˜N, which equals the expected
˜N d data points,
˜N d log ˜N d.
If we are not
using libraries like FFTW, which provide ready-to-use
multidimensional routines, we may also resort to onedimensional cosine transform or the above translation
into FFT to obtain high-performance implementations
of general d-dimensional transforms.
III. APPLICATIONS OF KPM
Having described the mathematical background and
many details of the implementation of the Kernel Polynomial Method, we are now in the position to present
practical applications of the approach. Already in the
introduction we have mentioned that KPM can be used
whenever we are interested in the spectral properties of
large matrices or in correlation functions that can be expressed through the eigenstates of such matrices. Apparently, this leads to a vast range of applications. In what
follows, we try to cover all types of accessible quantities
and for each give at least one example. We thereby focus
on lattice models from solid state physics.
A. Densities of states
1. General considerations
The ﬁrst and basic application of Chebyshev expansion and KPM is the calculation of the spectral density of
Hermitian matrices, which could correspond to the densities of states of both interacting or non-interacting quantum models . To be speciﬁc, let us consider a D-dimensional matrix M with eigenvalues Ek,
whose spectral density is deﬁned as
δ(E −Ek) .
As described earlier, the expansion of ρ(E) in terms of
Chebyshev polynomials requires a rescaling of M →˜
such that the spectrum of
M = (M −b)/a lies within
the interval [−1, 1]. Given the eigenvalues ˜Ek of ˜
rescaled density ˜ρ( ˜E) reads
˜ρ( ˜E) = 1
δ( ˜E −˜Ek) ,
and according to Eq. (19) the expansion coeﬃcients become
˜ρ( ˜E) Tn( ˜E) d ˜E = 1
D Tr(Tn( ˜
This is exactly the trace form that we introduced in
Sec. II.B, and we can immediately calculate the µn using
the stochastic techniques described in Sec. II.B.2. Knowing the moments we can use the techniques of Sec. II.D
to reconstruct ˜ρ( ˜E) for the whole range [−1, 1], and a
ﬁnal rescaling yields ρ(E).
2. Non-interacting systems: Anderson model of disorder
Applied to a generalized model of non-interacting
fermions c(†)
the matrix of interest M is formed by the coupling constants Mij. Knowing the spectrum of M, i.e. the singleparticle density of states ρ(E), all thermodynamic quantities of the model can be calculated. For example, the
particle density is given by
1 + eβ(E−µ) dE
and the free energy per site reads
ρ(E) log(1 + e−β(E−µ)) dE ,
where µ is the chemical potential and β = 1/T the inverse
temperature.
As the ﬁrst physical example let us consider the Anderson model of non-interacting fermions moving in a
random potential ,
Here hopping occurs along nearest neighbor bonds
⟨ij⟩on a simple cubic lattice and the local potential ǫi is chosen randomly with uniform distribution
ρ(E), ρtyp(E)
ρ(E), ρtyp(E)
ρ(E), ρtyp(E)
FIG. 2 (Color in online edition) Standard (dashed) and typical density of states (solid line), ρ(E) and ρtyp(E) respectively,
of the 3D Anderson model on a 503 site cluster with periodic
boundary conditions. For ρ(E) we calculated N = 2048 moments with R = 10 start vectors and S = 240 realizations of
disorder, for ρtyp(E) these numbers are N = 8192, R = 32
and S = 200. The lower right panel shows the phase diagram
of the model we obtained from ρtyp(E)/ρ(E) →0 . The disorder averaged density of states ρ(E) of the model can
be obtained as described, but it contains no information
about localization. The KPM method, however, allows
also for the calculation of the local density of states,
|⟨i|k⟩|2 δ(E −Ek) ,
which is a measure for the contribution of a single lattice
site (denoted by the basis state |i⟩) to the complete spectrum. For delocalized states all sites contribute equally,
whereas localized states reside on just a few sites, or,
equivalently, a certain site contributes only to a few eigenstates.
This property has a pronounced eﬀect on the
distribution of ρi(E), which at a ﬁxed energy E characterizes the variation of ρi over diﬀerent realizations of
disorder and sites i. For energies that correspond to localized eigenstates the distribution is highly asymmetric and becomes singular in the thermodynamic limit,
whereas in the delocalized case the distribution is regular and centered near its expectation value ρ(E). Therefore a comparison of the geometric and the arithmetic
average of ρi(E) over a set of realizations of disorder
and over lattice sites reveals the position of the Anderson transition . The expansion of ρi(E) is even
simpler than the expansion of ρ(E), since the moments
have the form of expectation values and do not involve a
˜ρi(E) Tn(E) dE = 1
|⟨i|k⟩|2Tn( ˜Ek)
M)|k⟩⟨k|i⟩= 1
D ⟨i|Tn( ˜
In Figure 2 we show the standard density of states ρ(E),
which coincides with the arithmetic mean of ρi(E), in
comparison to the typical density of states ρtyp(E), which
is deﬁned as the geometric mean of ρi(E),
ρtyp(E) = exp[
log(ρi(E))
With increasing disorder, starting from the boundaries
of the spectrum, ρtyp(E) is suppressed until it vanishes
completely for W/t ≳16.5, which is known as the critical
strength of disorder where the states in the band center
become localized . The calculation yields the phase diagram shown in the lower right
corner of Figure 2, which compares well to other numerical results.
Since the method requires storage only for the sparse
Hamiltonian matrix and for two vectors of the corresponding dimension, quite large systems can be studied on standard desktop computers (of the order of 1003
sites). The recursion is stable for arbitrarily high expansion order. In the present case we calculated as many
as 8192 moments to achieve maximum resolution in the
local density of states. The standard density of states is
usually far less demanding.
3. Interacting systems: Double exchange
Coming to interacting quantum systems, as a second example we study the evolution of the quantum
double-exchange model 
for large spin amplitude S, which in terms of spin-less
fermions c(†)
and Schwinger bosons a(†)
iσ (σ =↑, ↓) is given
by the Hamiltonian
with the local constraint P
iσaiσ = 2S + c†
model describes itinerant electrons on a lattice whose
spin is strongly coupled to local spins of amplitude S,
so that the motion of the electrons mediates an eﬀective ferromagnetic interaction between these localized
spins. In the case of colossal magneto-resistant manganites , for instance, cubic site symmetry
leads to a crystal ﬁeld splitting of the manganese d-shell,
and three electrons in the resulting t2g-shell form the local spins. The remaining electrons occupy the eg-shell
and can become itinerant upon doping, causing these
materials to show ferromagnetic order . If
the ferromagnetic (Hund’s rule) coupling is large, at each
site only the high-spin states are relevant and we can describe the total on-site spin in terms of Schwinger bosons
 .
From the electrons only the
charge degree of freedom remains, which is denoted by
the spin-less fermions c(†)
 
for more details). The full quantum model, Eq. (115),
is rather complicated for analytical or numerical studies,
and we expect major simpliﬁcation by treating the spin
background classically (remember that S is quite large
for the systems of interest). The limit of classical spins,
S →∞, is obtained by averaging Eq. (115) over spin
coherent states,
|Ω(S, θ, φ)⟩=
2) ei φ/2 a†
2) e−i φ/2 a†
where θ and φ are the classical polar angles and |0⟩the
bosonic vacuum. The resulting non-interacting Hamiltonian reads,
icj + H.c. ,
with the matrix element 
2 e−i(φi−φj)/2
ei(φi−φj)/2 
i.e., spin-less fermions move in a background of random
or ordered classical spins which aﬀect their hopping amplitude.
To assess the quality of this classical approximation
we considered four electrons moving on a ring of eight
sites, and compared the densities of states obtained for
a background of S = 3/2 quantum spins and a background of classical spins. For the full quantum Hamiltonian, Eq. (115), the (canonical) density of states was
calculated on the basis of 400 Chebyshev moments. To
reduce the Hilbert space dimension and to save resources
we made use of the SU(2) symmetry of the model: With
the stochastic approach we calculated separate moments
for each Sz-sector,
n = TrSz[Tn( ˜H)] ,
and used the dimensions DSz of the sectors to obtain the
total normalized µn from the average
D Tr[Tn( ˜H)] =
Sz=−Smax µSz
Sz=−Smax DSz
ρqu(E) run. avg.
FIG. 3 (Color in online edition) Density of nonzero eigenvalues of the quantum double-exchange model with S = 3/2
(dashed line) and running average (red dot-dashed), calculated for 4 electrons on a 8-site ring, compared to the classical
result S →∞(green solid). Expansion parameters: N = 400
moments and R = 100 random vectors per Sz sector.
Note, that such a setup can be used whenever the model
under consideration has certain symmetries.
On the other hand, we solved the eﬀective noninteracting model (117) and calculated the distributions
of non-zero energies for a background of fully disordered
classical spins. As Figure 3 illustrates, the spectrum of
the quantum model with S = 3/2 closely matches that
of the system with classical spins, providing good justiﬁcation, e.g. for studies of colossal magneto-resistive
manganites that make use of a classical approximation
for the spin background. Since for the ﬁnite cluster considered the spectrum of the quantum model is discrete,
at the present expansion order KPM starts to resolve
distinct energy levels (dashed line). Therefore a running
average (dot-dashed line) compares better to the classical
spin-averaged data (bold line).
B. Static correlations at ﬁnite temperature
Densities of states provide only the most basic information about a given quantum system, and much more
details can usually be learned from the study of correlations and the response of the system to an external
probe or perturbation. Starting with static correlation
functions, let us now extend the application range of the
expansion techniques to such more involved quantities.
Given the eigenstates |k⟩of an interacting quantum
system the thermodynamic expectation value of an operator A reads
ZD Tr(A e−βH) =
⟨k|A|k⟩e−βEk ,
D Tr(e−βH) = 1
where H is the Hamiltonian of the system, Z the partition function, and Ek the energy of the eigenstate |k⟩.
Using the function
⟨k|A|k⟩δ(E −Ek)
and the (canonical) density of states ρ(E), we can express
the thermal expectation value in terms of integrals over
the Boltzmann weight,
a(E) e−βE dE ,
ρ(E) e−βE dE .
Of course, similar relations hold also for non-interacting
fermion systems, where the Boltzmann weight e−βE has
to be replaced by the Fermi function f(E) = (1 +
eβ(E−µ))−1 and the single-electron wave functions play
the role of |k⟩.
Again, the particular form of a(E) suggests an expansion in Chebyshev polynomials, and after rescaling we
˜a(E) Tn(E) dE = 1
⟨k|A|k⟩Tn( ˜Ek)
D Tr(ATn( ˜H)) ,
which can be evaluated with the stochastic approach,
Sec. II.B.2.
For interacting systems at low temperature the expression in Eq. (124) is a bit problematic, since the Boltzmann factor puts most of the weight on the lower end
of the spectrum and heavily ampliﬁes small numerical
errors in ρ(E) and a(E). We can avoid these problems
by calculating the ground state and some of the lowest
excitations exactly, using standard iterative diagonalization methods like Lanczos or Jacobi-Davidson. Then we
split the expectation value of A and the partition function Z into contributions from the exactly known states
KPM 4x4, C=0
KPM 4x4, C=2 .. 9
KPM 4x6, C=2 .. 8
open circles = ED results
lines = KPM results
FIG. 4 (Color in online edition) Nearest-neighbor Sz-Sz correlations of the XXZ model on a square lattice. Lines represent the KPM results with separation of low-lying eigenstates
(bold solid and bold dashed) and without (thin dashed), open
symbols denote exact results from a complete diagonalization
of a 4 × 4 system.
and contributions from the rest of the spectrum,
⟨k|A|k⟩e−βEk + 1
as(E) e−βE dE ,
ρs(E) e−βE dE
The functions
⟨k|A|k⟩δ(E −Ek) ,
describe the rest of the spectrum and can be expanded
in Chebyshev polynomials easily. Based on the known
states we can introduce the projection operator
and ﬁnd for the expansion coeﬃcients of ˜as(E)
D Tr(PATn( ˜H)) ≈
⟨r|PATn( ˜H)P|r⟩,
and similarly for those of ˜ρs(E)
D Tr(PTn( ˜H)) ≈
⟨r|PTn( ˜H)P|r⟩. (133)
Note, that in addition to the two vectors for the Chebyshev recursion we now need memory also for the eigenstates |k⟩.
Otherwise the resource consumption is the
same as in the standard scheme.
We illustrate the accuracy of this approach in Figure 4
considering the nearest-neighbor Sz-Sz correlations of
the square-lattice spin-1/2 XXZ model as an example,
As a function of temperature and for an anisotropy
−1 < ∆< 0 this model shows a quantum to classical crossover in the sense that the correlations are
anti-ferromagnetic at low temperature (quantum eﬀect)
and ferromagnetic at high temperature (as expected
for the classical model). Comparing
the KPM results with the exact correlations of a 4 × 4
system, which were obtained from a complete diagonalization of the Hamiltonian, the improvement due to the
separation of only a few low-lying eigenstates is obvious.
Whereas for C = 0 the data is more or less random below
T ≈1, the agreement with the exact data is perfect, if the
ground state and one or two excitations are considered
separately. The numerical eﬀort required for these calculations diﬀers largely between complete diagonalization
and the KPM method. For the former, 18 or 20 sites are
practically the limit, whereas the latter can easily handle
30 sites or more.
Note that for non-interacting systems the above separation of the spectrum is not required, since for T →0
the Fermi function converges to a simple step function
without causing any numerical problems.
C. Dynamical correlations at zero temperature
1. General considerations
Having discussed simple expectation values and static
correlations, the calculation of time dependent quantities
is the natural next step in the study of complex quantum
models. This is motivated also by many experimental setups, which probe the response of a physical system to
time dependent external perturbations. Examples are inelastic scattering experiments or measurements of transport coeﬃcients.
In the framework of linear response
theory and the Kubo formalism the system’s response
is expressed in terms of dynamical correlation functions,
which can also be calculated eﬃciently with Chebyshev
expansion and KPM. Technically though, we need to distinguish between two diﬀerent situations: For interacting
many-particle systems at zero temperature only matrix
elements between the ground state and excited states
contribute to a dynamical correlation function, whereas
for interacting systems at ﬁnite temperature or for noninteracting systems with a ﬁnite particle density transi-
tions between all eigenstates — many-particle or singleparticle, respectively — contribute. We therefore split
the discussion of dynamical correlations into two sections,
starting here with interacting many-particle systems at
Given two operators A and B a general dynamical correlation function can be deﬁned through
ω + i ǫ ∓H B|0⟩
⟨0|A|k⟩⟨k|B|0⟩
ω + i ǫ ∓Ek
where Ek is the energy of the many-particle eigenstate |k⟩
of the Hamiltonian H, |0⟩its ground state, and ǫ > 0.
If we assume that the product ⟨0|A|k⟩⟨k|B|0⟩is real
the imaginary part
⟨0|A|k⟩⟨k|B|0⟩δ(ω ∓Ek)
has a similar structure as, e.g., the local density of states
in Eq. (112), and in fact, with ρi(E) we already calculated
a dynamical correlation function. Hence, after rescaling
the Hamiltonian H →˜H and all energies ω →˜ω we can
proceed as usual and expand Im⟨A; B⟩±
ω in Chebyshev
polynomials,
Again, the moments are obtained from expectation values
˜ω Tn(˜ω) d˜ω = ⟨0|ATn(∓˜H)B|0⟩,
and for A ̸= B† we can follow the scheme outlined in
Eqs. (30) to (33). For A = B† the calculation simpliﬁes
to the one in Eqs. (34) and (35), now with B|0⟩as the
starting vector.
In many cases, especially for the spectral functions and
optical conductivities studied below, only the imaginary
part of ⟨A; B⟩±
ω is of interest, and the above setup is all
we need. Sometimes however — e.g., within the Cluster Perturbation Theory discussed in Sec. IV.B — also
the real part of a general correlation function ⟨A; B⟩±
is required.
Fortunately it can be calculated with almost no additional eﬀort: The analytical properties of
ω arising from causality imply that its real part is
fully determined by the imaginary part. Indeed a Hilbert
transform gives
⟨0|A|k⟩⟨k|B|0⟩P
µn Un−1(˜ω) ,
where we used Eq. (14). The full correlation function
Un−1(˜ω) + i Tn(˜ω)
µn exp(−i n arccos ˜ω)
can thus be reconstructed from the same moments µn
that we derived for its imaginary part, Eq. (138). In contrast to the real quantities we considered so far, the reconstruction merely requires complex Fourier transform,
see Eqs. (88) and (89). If only the imaginary or real part
of ⟨A; B⟩±
ω is needed, a cosine or sine transform, respectively, is suﬃcient.
Note again, that the calculation of dynamical correlation functions for non-interacting electron systems is not
possible with the scheme discussed in this section, not
even at zero temperature. At ﬁnite band ﬁlling (ﬁnite
chemical potential) the ground state consists of a sum
over occupied single-electron states, and dynamical correlation functions thus involve a double summation over
matrix elements between all single-particle eigenstates,
weighted by the Fermi function.
Clearly, this is more
complicated than Eq. (135), and we postpone the discussion of this case to Sec. III.D, where we describe methods
for dynamical correlation functions at ﬁnite temperature
and — for the case of non-interacting electrons — ﬁnite
2. One-particle spectral function
An important example of a dynamical correlation function is the (retarded) Green function in momentum space,
Gσ(⃗k, ω) = ⟨c⃗k,σ; c†
⃗k,σ; c⃗k,σ⟩−
and the associated spectral function
Aσ(⃗k, ω) = −1
π Im Gσ(⃗k, ω)
σ (⃗k, ω) + A−
σ (⃗k, ω) ,
which characterizes the electron absorption or emission
of an interacting system. For instance, A−
σ can be measured experimentally in angle resolved photo-emission
spectroscopy (ARPES).
application,
one-dimensional
fermions ,
ici+1 + H.c.)
i + bi)ni + ω0
which is one of the basic models for the study of electronlattice interaction. A single band of electrons is approximated by spinless fermions c(†)
i , the density of which
couples to the local lattice distortion described by dispersionless phonons b(†)
i . At low fermion density, with
increasing electron phonon interaction the charges get
dressed by a surrounding lattice distortion and form new,
heavy quasi-particles known as polarons. Eventually, for
strong coupling the width of the corresponding band is
suppressed exponentially, leading to a process called selftrapping. For a half-ﬁlled band, i.e., 0.5 fermions per site,
the model allows for the study of quantum eﬀects at the
transition from a metal to a band (or Peierls) insulator,
marked by the opening of a gap at the Fermi wave vector
and the development of a matching lattice distortion.
Since the Hamiltonian (143) involves bosonic degrees
of freedom, the Hilbert space of even a ﬁnite system has
inﬁnite dimension.
In practice, nevertheless, the contribution of highly excited phonon states is found to be
negligible at low temperature or for the ground-state, and
the system is well approximated by a truncated phonon
space with P
ibi ≤M .
In addition, the translational symmetry of the model can be
used to reduce the Hilbert space dimension, and, moreover, the symmetric phonon mode with momentum q = 0
can be excluded from the numerics: Since it couples to
the total number of electrons, which is a conserved quantity, its contribution can be handled analytically . Below we present results for
a cluster size of L = 8 or 10, where a cut-oﬀM = 24
or 15, respectively, leads to truncation errors < 10−6
for the ground-state energy.
Alternatively, for one or
two fermionic particles and low temperatures an optimized variational basis can be constructed for inﬁnite
systems , which would also be suitable for our numerical approach.
In Figure 5 we present KPM data for the spectral function of the spinless-fermion Holstein model and assess its
quality by comparing with results from Quantum Monte
Carlo (QMC) and Dynamical Density Matrix Renormalization Group (DDMRG) calculations. Starting with the case of a single electron on a
ten-site ring, Figure 5 (a) illustrates the presence of a
narrow polaron band at the Fermi level and of a broad
range of incoherent contributions to the spectral func-
(a) n = 0.1
(b) n = 0.5
FIG. 5 (Color in online edition) One-particle spectral function and its integral for the Holstein model (a) on a 10-site ring
with one electron, εp = g2ω0 = 2.0t, ω0 = 0.4t, and (b) on a
8-site ring, band ﬁlling n = 0.5, εp = g2ω0 = 1.6t, ω0 = 0.1t.
For comparison, in (a) the blue dashed lines represent Quantum Monte Carlo data at βt = 8 ,
and green stars indicate the position of the polaron band in
the inﬁnite system . In (b) the blue and
green curves denote results of Dynamical DMRG for the same
lattice size and T = 0 .
tion, which in the spinless case reads
A−(k, ω) =
|⟨l, Ne −1| ck |0, Ne⟩|2
× δ[ω + (El,Ne−1 −E0,Ne)]
A+(k, ω) =
|⟨l, Ne + 1| c†
k |0, Ne⟩|2
× δ[ω −(El,Ne+1 −E0,Ne)] .
Here |l, Ne⟩denotes the lth eigenstate with Ne electrons and energy El,Ne.
The photo-emission part A−
reﬂects the Poisson-like phonon distribution of the polaron ground state, whereas A+ has most of its weight in
the vicinity of the original free electron band. In terms of
the overall shape and the integrated weight, both KPM
and QMC agree very well. QMC, however, is not able to
resolve all the narrow features of the spectral function,
and the polaron band is hardly observable. Nevertheless,
QMC has the advantage that larger systems can be studied, in particular at ﬁnite temperature. As a guide to the
eye we also show the position of the polaron band in the
inﬁnite system, which was calculated with the approach
of Bonˇca et al. . In Figure 5 (b) we consider the
case of a half-ﬁlled band and strong electron-phonon coupling, where the system is in an insulating phase with an
excitation gap at the Fermi momentum k = ±π/2. Below and above the gap the spectrum is characterized by
broad multi-phonon absorption. Compared to DDMRG,
again KPM oﬀers the better resolution and unfolds all
the discrete phonon sidebands.
Concerning numerical
performance DDMRG has the advantage of a small optimized Hilbert space, which can be handled with standard
workstations. However, the basis optimization is rather
time consuming and, in addition, each frequency value
ω requires a new simulation. The KPM calculations, on
the other hand, involved matrix dimensions between 108
and 1010, and we therefore used high-performance computers such as Hitachi SR8000-F1 or IBM p690 for the
moment calculation. For the reconstruction of the spectra, of course, a desktop computer is suﬃcient.
3. Optical conductivity
The next example of a dynamical correlation function
is the optical conductivity. Here the imaginary and real
parts of our general correlation functions ⟨A; B⟩ω change
their roles due to an additional frequency integration.
The so-called regular contribution to the real part of the
optical conductivity is thus given by,
σreg(ω) = 1
|⟨k|J|0⟩|2 δ(ω −(Ek −E0)) , (146)
where the operator
i,σci+1,σ −H.c.)
describes the current.
After rescaling the energy and
shifting the frequency, ω = ˜ω + ˜E0, the sum can be expanded as described earlier, now with J|0⟩as the initial
U/2εp= 0.93
U/2εp= 2.14
Mott insulator
Peierls insulator
(b) Ne=L, S
FIG. 6 (Color in online edition) (a) The optical conductivity σreg(ω) and its integral Sreg(ω) for the Holstein Hubbard
model at half-ﬁlling with diﬀerent ratios of the Coulomb interaction U to the electron-lattice coupling εp = g2ω0, ω0 = 0.1t,
and g2 = 7. Black dotted lines denote excitations of the pure
Hubbard model. (b) The one-particle spectral function at the
transition point, i.e., for the same parameters as in the middle
panel of (a). The system size is L = 8.
state for the Chebyshev recursion. Back-scaling and dividing by ω then yields the ﬁnal result.
In Figure 6 we apply this setup to the Holstein Hubbard model, which is the generalization of the Holstein
model to true, spin-carrying electrons that interact via
a screened Coulomb interaction, modelled by a Hubbard
i,σci+1,σ + H.c.) + U
i + bi)niσ + ω0
For a half-ﬁlled band, which now denotes a density of one
S(q,ω) [arb. units], N(q,ω) / N(q)
(π/4,0) (3π/4,0)
Ja/Jb= 0.4
Jx/Jb= 0.425
FIG. 7 Spin structure factor at T = 0 calculated for the
model (149) which aims at describing the magnetic compound
(VO)2P2O7. For more details see .
electron per site, the electronic properties of the model
are governed by a competition of two insulating phases: a
Peierls (or band) insulator caused by the electron-lattice
interaction and a Mott (or correlated) insulator caused
by the electron-electron interaction.
Within the optical conductivity both phases are signalled by an excitation gap, which closes at the transition between the
two phases. We illustrate this behavior in Figure 6 (a),
showing σreg(ω) at strong electron-phonon coupling and
for increasing U. The data for the one-particle spectral
function in Figure 6 (b) proves that simultaneously to the
optical gap also the charge gap vanishes at the quantum
phase transition point .
4. Spin structure factor
Apart from electron systems, of course, the KPM approach works also for other quantum problems such as
pure spin systems.
To describe the excitation spectrum and the magnetic properties of the compound
(VO)2P2O7, some years ago we proposed the 2D spin
Hamiltonian 
(1 + δ(−1)i)⃗Si,j · ⃗Si+1,j + Ja
⃗Si,j · ⃗Si,j+1
(⃗S2i,j · ⃗S2i+1,j+1 + ⃗S2i+1,j · ⃗S2i,j+1) ,
where ⃗Si,j denote spin-1/2 operators on a square lattice.
With this model we aimed at explaining the observation
of two branches of low-lying triplet excitations by neutron
scattering , which was inconsistent
with the then prevailing picture of (VO)2P2O7 being a
spin-ladder or alternating chain compound.
Studying the low-energy physics of the model (149)
the KPM approach can be used to calculate the spin
structure factor and the integrated spectral weight,
S(⃗q, ω) =
|⟨k|⃗Sz(⃗q)|0⟩|2δ(Ek −E0 −ω) ,
N(⃗q, ω) =
dω′S(⃗q, ω′) ,
where ⃗Sz(⃗q) = P
i,j ei ⃗q·⃗ri,j Sz
Figure 7 shows these
quantities for a 4×8 cluster with periodic boundary conditions. The dimension of the sector Sz = 0, which contains the ground state, is rather moderate here being of
the order of D ≈4 · 107 only. The expansion clearly resolves the lowest (massive) triplet excitations T1, a number of singlets and, in particular, a second triplet branch
The shaded region marks the two-particle continuum obtained by exciting two of the elementary triplets
T1, and illustrates that T2 is lower in energy. Since the
system is ﬁnite in size, of course, the continuum appears
only as a set of broad discrete peaks, the density of which
increases with the system size.
D. Dynamical correlations at ﬁnite temperature
1. General considerations
In the preceding section we mentioned brieﬂy that for
non-interacting electron systems or for interacting systems at ﬁnite temperature the calculation of dynamical
correlation functions is more involved, due to the required
double summation over all matrix elements of the measured operators. Chebyshev expansion, nevertheless, offers an eﬃcient way for handling these problems.
be speciﬁc, let us derive all new ideas on the basis of
the optical conductivity σ(ω), which will be our primary
application below. Generalizations to other dynamical
correlations can be derived without much eﬀort.
For an interacting system the extension of Eq. (146) is
|⟨k|J|q⟩|2(e−βEk −e−βEq)
δ(ω −ωqk) ,
with ωqk = Eq −Ek. Compared to Eq. (146) a straightforward expansion of the ﬁnite temperature conductivity is spoiled by the presence of the Boltzmann weighting factors. Some authors 
try to handle this problem by expanding these factors
in Chebyshev polynomials and performing a numerical
time evolution subsequently, which, however, requires a
new simulation for each temperature. A much simpler
approach is based on the function
j(x, y) = 1
|⟨k|J|q⟩|2 δ(x −Ek) δ(y −Eq)
which we may interpret as a matrix element density. Being a function of two variables, j(x, y) can be expanded
with two-dimensional KPM,
˜j(x, y) =
µnmhnmgngmTn(x)Tm(y)
(1 −x2)(1 −y2)
where ˜j(x, y) refers to the rescaled j(x, y), gn are the
usual kernel damping factors (see Eq. (71)), and hnm
account for the correct normalization (see Eq. (95)). The
moments µnm are obtained from
˜j(x, y)Tn(x)Tm(y) dx dy
|⟨k|J|q⟩|2 Tn( ˜Ek) Tm( ˜Eq)
⟨k|Tn( ˜H)J|q⟩⟨q|Tm( ˜H)J|k⟩
 Tn( ˜H)JTm( ˜H)J
and again the trace can be replaced by an average over a
relatively small number R of random vectors |r⟩. The
numerical eﬀort for an expansion of order n, m < N
ranges between 2RDN and RDN 2 operations, depending on whether memory is available for up to N vectors
of the Hilbert space dimension D or not. Given the operator density j(x, y) we ﬁnd the optical conductivity by
integrating over Boltzmann factors,
j(y + ω, y)
 e−βy −e−β(y+ω) 
|⟨k|J|q⟩|2(e−βEk −e−βEq)
δ(ω −ωqk) ,
and, as above, we get the partition function Z from an
integral over the density of states ρ(E). The latter can
be expanded in parallel to j(x, y).
Note that the calculation of the conductivity at diﬀerent temperatures is
based on the same operator density j(x, y), i.e., it needs
to be expanded only once for all temperatures.
Surprisingly,
were suggested already ten years ago , but — probably overlooking
its potential — applied only to the zero-temperature response of non-interacting electrons. A reason for the poor
appreciation of these old ideas may also lie in the use of
non-optimal kernels, which did not ensure the positivity
FIG. 8 (Color in online edition) The matrix element density
j(x, y) for the 3D Anderson model with disorder W/t = 2 and
of j(x, y) and reduced the numerical precision. Only recently, one of the authors generalized the Jackson kernel
and obtained high resolution optical data for the Anderson model . More results, in particular for
interacting quantum systems at ﬁnite temperature, we
present hereafter.
2. Optical conductivity of the Anderson model
Since the Anderson model describes non-interacting
fermions, the eigenstates |k⟩occurring in σ(ω) now denote single-particle wave functions and the Boltzmann
weight has to be replaced by the Fermi function,
σreg(ω) = 1
j(y + ω, y)
 f(y) −f(y + ω)
|⟨k|J|q⟩|2(f(Ek) −f(Eq))
δ(ω −ωqk) .
Clearly, from a computational point of view this expression is of the same complexity for both, zero and ﬁnite
temperature, and indeed, compared to Sec. III.C, we
need the more advanced 2D KPM approach.
Figure 8 shows the matrix element density j(x, y) calculated for the 3D Anderson model on a D = 503 site
cluster. The expansion order is N = 64, and the moment data was averaged over S = 10 disorder samples
and R = 10 random start vectors each. Starting from a
“shark ﬁn” at weak disorder, with increasing W the density j(x, y) spreads in the entire energy plane, simultaneously developing a sharp dip along x = y. A comparison
with Eq. (157) reveals that this dip is responsible for the
decreasing and ﬁnally vanishing DC conductivity of the
model . In Figure 9 we show the resulting
optical conductivity at W/t = 12 for diﬀerent chemical
potentials µ and temperatures β = 1/T . Note that all
curves are derived from the same matrix element density
j(x, y), which is now based on a D = 1003 site cluster,
expansion order N = 2048, an average over S = 440
samples and only R = 1 random start vectors each.
µ = -8.47 ... 0.0 t
β = 0.01 ... 1000 / t
FIG. 9 (Color in online edition) Optical conductivity of the
3D Anderson model at disorder W = 12 and for diﬀerent
chemical potentials µ and temperatures β = 1/T.
3. Optical conductivity of the Holstein model
Having discussed dynamical correlations for noninteracting electrons, let us now come back to the case
of interacting systems. The setup described so far works
well for high temperatures, but as soon as T gets small we
experience the same problems as with thermal expectation values and static correlations. Again, the Boltzmann
factors put most of the weight to the margins of the domain of j(x, y), thus amplifying small numerical errors.
To properly approach the limit T →0 we therefore have
to separate the ground state and a few lowest excitations from the rest of the spectrum in a fashion similar
to the static correlations in Sec. III.B. Since we start
from a 2D expansion, the correlation function (optical
conductivity) now splits into three parts: a contribution
from the transitions (or matrix elements) between the
separated eigenstates, a sum of 1D expansions for the
transitions between the separated states and the rest of
the spectrum (see Sec. III.C), and a 2D expansion for all
transitions within the rest of the spectrum,
(σk,q + σq,k)
σk,q = |⟨k|J|q⟩|2(e−βEk −e−βEq) δ(ω −ωqk)
The expansions required for σreg
1D (ω) are carried out in
analogy to Sec. III.C.3, but the resulting conductivities
are weighted appropriately when all contributions are
combined to σreg(ω). Using the projection operator de-
ﬁned in Eq. (131), the corresponding moments read
n = ⟨k|JPTn( ˜H)PJ|k⟩.
εp/t = 0.4
εp/t = 2.0
εp/t = 3.0
εp/t = 4.0
ω0/t = 0.4
FIG. 10 (Color in online edition) Left: Schematic setup for
the calculation of ﬁnite-temperature dynamical correlations
for interacting quantum systems, which requires a separation
into parts handled by exact diagonalization (ED), 1D Chebyshev expansion and 2D Chebyshev expansion.
Right: The
lowest eigenvalues of the Holstein model on a six site chain
for diﬀerent electron-phonon coupling εp. The shaded region
marks the lowest polaron band, which was handled separately
when calculating the spectra in Figure 11.
FIG. 11 (Color in online edition) Finite temperature optical
conductivity of a single electron coupled to the lattice via a
Holstein type interaction. Diﬀerent colors illustrate how, in
particular, the low-temperature spectra beneﬁt from a separation of C = 0, 1 or 6 low-energy states . The phonon frequency is ω0/t = 0.4.
2D (ω) we follow the scheme outlined in III.D.1, but
use projected moments
µnm = Tr(Tn( ˜H)PJTm( ˜H)PJ)/D .
In Figure 10 we illustrate our setup schematically and
show the lowest forty eigenvalues of the Holstein model,
Eq. (143), with a band ﬁlling of one electron. Separating
up to six states from the rest of the spectrum we obtain
the ﬁnite-temperature optical conductivity of the system,
Figure 11. For high temperatures (T = t, see lower panels) the separation of low-energy states is not necessary,
the conductivity curves for C = 0, 1 and 6 agree very
well. For low temperatures (T = 0.1t, see upper panels),
the separation is crucial. Without any separated states
(C = 0) the conductivity has substantial numerical errors
and can even become negative, if large Boltzmann factors
amplify inﬁnitesimal numerical round-oﬀerrors of negative sign. Splitting oﬀthe ground state (C = 1) or the
entire (narrow) polaron band (C = 6 for the present sixsite cluster), we obtain reliable, high-resolution spectra
down to the lowest temperatures. From a physics point
of view, at strong electron phonon coupling (right panels)
the conductivity shows an interesting transfer of spectral
weight from high to low frequencies, if the temperature is
increased for more details).
With this discussion of optical conductivity as a ﬁnite
temperature dynamical correlation function we conclude
the section on direct applications of KPM. Of course, the
described techniques can be used for the solution of many
other interesting and numerically demanding problems,
but an equally important ﬁeld of applications emerges,
when KPM is embedded into other numerical or analytical techniques, which is the subject of the next section.
IV. KPM AS A COMPONENT OF OTHER METHODS
A. Monte Carlo simulations
In condensed matter physics some of the most intensely
studied materials are aﬀected by a complex interplay of
many degrees of freedom, and when deriving suitable
approximate descriptions we frequently arrive at models, where non-interacting fermions are coupled to classical degrees of freedom. Examples are colossal magnetoresistant manganites or magnetic semiconductors , where the classical variables correspond to localized spin degrees of freedom. We already introduced such a model when we discussed the limit S →∞of the double-exchange model,
Eq. (117). The properties of these systems, e.g. a ferromagnetic ordering as a function of temperature, can be
studied by standard MC procedures. However, in contrast to purely classical systems the energy of a given
spin conﬁguration, which enters the transition probabilities, cannot be calculated directly, but requires the solution of the corresponding non-interacting fermion problem. This is usually the most time consuming part, and
an eﬃcient MC algorithm should therefore evaluate the
fermionic trace as fast and as seldom as possible.
The ﬁrst requirement can be matched by using KPM
for calculating the density of states of the fermion system, which by integration over the Fermi function yields
the energy of the underlying spin conﬁguration. Combined with standard Metropolis single-spin updates this
led to the ﬁrst MC simulations of double-exchange systems on reasonably large clusters (83 sites), which were later improved by replacing full traces by trace estimates and
by increasing the eﬃciency of the matrix vector multiplications , L=6
Alonso et al. , L=12
Motome et al. , L→∞
Cluster MC, L=6
Cluster MC, L=12
classical double exchange, n=0.5
FIG. 12 (Color in online edition) Magnetization as a function of temperature for the classical double-exchange model
at doping n
We compare data obtained from
the eﬀective model Heﬀ(see text), from a hybrid Monte
Carlo approach , the Truncated Polynomial Expansion Method , and from a KPM based Cluster Monte Carlo technique . L denotes the size of the underlying three-dimensional cluster, i.e., D = L3 is the dimension
of the fermionic problem.
tageous to replace the above single-spin updates by updates of the whole spin background.
The ﬁrst implementation of such ideas was given in terms of an hybrid
Monte Carlo algorithm , which combines an approximate time evolution of the spin system
with a diagonalization of the fermionic problem by Legendre expansion, and requires a much smaller number of
MC accept-reject steps. However, this approach has the
drawback of involving a molecular dynamics type simulation of the classical degrees of freedom, which is a bit
complicated and which may bias the system in the direction of the assumed approximate dynamics.
Focussing on the problem of classical double exchange, Eq. (117), we therefore proposed a third approach , which combines the advantages of KPM with the highly eﬃcient Cluster MC algorithms . In
general, for a classical MC algorithm the transition probability from state a to state b can be written as
P(a →b) = A(a →b) ˜P(a →b) ,
where A(a →b) is the probability of considering the move
a →b, and ˜P(a →b) is the probability of accepting the
move a →b. Given the Boltzmann weights of the states
a and b, W(a) and W(b), detailed balance requires that
W(a)P(a →b) = W(b)P(b →a) ,
which can be fulﬁlled with a generalized Metropolis algorithm
˜P(a →b) = min
1, W(b)A(b →a)
W(a)A(a →b)
In the standard MC approach for spin systems only a
single randomly chosen spin is ﬂipped. Hence, A(a →
b) = A(b →a) and the probability ˜P(a →b) is usually
much smaller than 1, since it depends on temperature
via the weights W(a) and W(b). This disadvantage can
be avoided by a clever construction of clusters of spins,
which are ﬂipped simultaneously, such that the a priori
probabilities A(a →b) and A(b →a) soak up any diﬀerence in the weights W(a) and W(b). We then arrive at
the famous rejection-free cluster MC algorithms , which are characterized by ˜P(a →b) = 1.
For the double-exchange model (117) we cannot expect
to ﬁnd an algorithm with ˜P(a →b) = 1, but even a
method with ˜P(a →b) = 0.5 would be highly eﬃcient.
The amplitude of the hopping matrix element (118) is
given by the cosine of half the relative angle between
neighboring spins, or |tij|2 = (1 + ⃗Si · ⃗Sj)/2. Averaging
over the fermionic degrees of freedom, we thus arrive at
an eﬀective classical spin model
1 + ⃗Si · ⃗Sj ,
where the particle density n approximately deﬁnes the
coupling, Jeﬀ≈n(1 −n)/
Similar to a classical
Heisenberg model, the Hamiltonian Heﬀis a sum over
contributions of single bonds, and we can therefore construct a cluster algorithm with ˜P(a →b) = 1. Surprisingly, the simulation of this pure spin model yields magnetization data, which almost perfectly matches the results for the full classical double-exchange model at doping n = 0.5, see Figure 12.
For simulating the coupled spin fermion model (117)
we suggested to apply the single cluster algorithm for
Heﬀuntil approximately every spin in the system has
been ﬂipped once, thereby keeping track of all a priori
probabilities A(a →b) of subsequent cluster ﬂips. Then
for the new spin conﬁguration the energy of the electron
system is evaluated with the help of KPM. Note however, that for a reliable discrimination of Heﬀand the full
fermionic model (117) the energy calculation needs to be
very precise. For the moment calculation we therefore
relied on complete trace summations instead of stochastic estimates.
The KPM step is thus no longer linear
in D, but still much faster than a full diagonalization
of the bilinear fermionic model. Based on the resulting
energy, the new spin conﬁguration is accepted with the
probability (164).
Figure 12 shows the magnetization
of the double-exchange model as a function of temperature for n = 0.5. Except for small deviations near the
critical temperature the data obtained with the new approach compares well with the results of the hybrid MC
approach , and due to the low numerical eﬀort rather large systems can be studied.
Of course, the combination of KPM and classical
Monte Carlo not only works for spin systems. We may
also think of models involving the coupling of electronic
degrees of freedom to adiabatic lattice distortions or
other classical variables , and as yet
the potential of such combined approaches is certainly
not fully exhausted.
The next application, which makes use of KPM as a
component of a more general numerical approach, brings
us back to interacting quantum systems, in particular,
correlated electron systems with strong local interactions.
B. Cluster Perturbation Theory (CPT)
1. General features of CPT
Earlier in this review we have demonstrated the advantages of the Chebyshev approach for the calculation
of spectral functions, optical conductivities and structure factors of complicated interacting quantum systems.
However, owing to the ﬁnite size of the considered systems, quantities like the spectral function A(⃗k, ω) could
only be calculated for a ﬁnite set of independent momenta ⃗k. The interpretation of this “discrete” data may
sometimes be less convenient, e.g. the ⃗k-integrated oneelectron density ρ(ω) =
dkd A(⃗k, ω) does not show
bands but only discrete poles which are grouped to bandlike structures. Although this does not substantially bias
the interpretation it is desirable to restore the translational symmetry of the lattice and reintroduce an inﬁnite
momentum space.
Perturbation
(CPT) a straightforward way to perform
this task approximatively has recently been devised.
To describe it in a nutshell, let us consider a model of
interacting fermions on a one-dimensional chain
i+1,σci,σ + H.c.) +
Here Ui denotes a local interaction, e.g. Ui = Uni↑ni↓
for the Hubbard model. CPT starts by breaking up the
inﬁnite system into short ﬁnite chains of L sites each
(clusters), which all are equivalent due to translational
From the Green function of a ﬁnite chain,
ij(ω) with i, j = 0, . . . , L −1, which is calculated exactly by a suitable numerical method, the Green function
G(k, ω) of the inﬁnite chain is obtained by reintroducing the hopping between the segments. This inter-chain
hopping is treated on the level of a random phase approximation, which neglects correlations between diﬀerent chains. The Green function Gnm
ij (ω) is then given
through a Dyson equation
ij (ω) = δnmGc
ii′(ω)V nm′
where V nm
= −t(δn,m+1δi0δj,L−1 + δn,m−1δi,L−1δj0) describes the inter-chain hopping and upper indices number the diﬀerent clusters.
A partial Fourier transform
of the inter-chain hopping, Vij(Q) = −t(ei Q δi0δj,L−1 +
Lorentz kernel
Jackson kernel
FIG. 13 Spectral function for non-interacting tight-binding
electrons. Based on the Lorentz kernel CPT exactly reproduces the inﬁnite system result (left), the Jackson kernel does
not have the correct analytical properties, therefore CPT cannot close the ﬁnite size gap at k = π/2 (right).
e−i Q δi,L−1δj0), gives the inﬁnite-lattice Green function
in a mixed representation
ˆGij(Q, ω) =
1 −V (Q)Gc(ω)
for a momentum vector Q of the super-lattice of ﬁnite
chains and cluster indices i, j. Finally, from this mixed
representation the inﬁnite lattice Green function in momentum space is recovered in the CPT approximation as
a simple Fourier transform
G(k, ω) = 1
exp(i(i −j)k) ˆGij(Lk, ω) .
The reader should be aware that restoring translational
symmetry in the CPT sense is diﬀerent from performing the thermodynamic limit of the interacting system.
The CPT may be understood as a kind of interpolation
scheme from the discrete momentum space of a ﬁnite
cluster to the continuous ⃗k-values of the inﬁnite lattice.
The amount of information attainable from the solution
of a ﬁnite cluster problem does however not increase. Especially ﬁnite-size eﬀects aﬀecting the interaction properties are by no means reduced, but still determined
through the size of the underlying cluster. Nevertheless,
CPT yields appealing presentations of the ﬁnite-cluster
data, which can ease its interpretation.
At present, all numerical studies within the CPT context use Lanczos recursion for the cluster diagonalization, thus suﬀering from the shortcomings we discussed
earlier. As an alternative, we prefer to use the formalism
introduced in Sec. III.C, which is much better suited for
the calculation of spectral properties in a ﬁnite energy
On applying the CPT crucial attention has to be paid
to the kernel used in the reconstruction of Gc
it turns out, the Jackson kernel is an inadequate choice
here, since already for the non-interacting tight-binding
model it introduces spurious structures into the spectra.
The failure can be attributed to the shape of the Jackson
kernel: Being optimized for high resolution, a pole in the
Green function will give a sharp peak with most of its
weight concentrated at the center, and rapidly decaying
tails. The reconstructed (cluster) Green function therefore does not satisfy the correct analytical properties required in the CPT step. To guarantee these properties,
instead, we use the Lorentz kernel, which we constructed
in Sec. II.C.4 to mimic the eﬀect of a ﬁnite imaginary
part in the energy argument of a Green function. Using this kernel for the reconstruction of Gc
ij(ω) the CPT
works perfectly (cf. Figure 13).
To provide further examples we present results for two
diﬀerent interacting models where the cluster Green function Gc
ij(ω) has been calculated through a Chebyshev expansion as in Eq. (140). Using Gc
ij(ω) = Gc
ji(ω) (no magnetic ﬁeld), for a L-site chain L diagonal and L(L −1)/2
oﬀ-diagonal elements of Gc
ij(ω) have to be calculated.
The latter can be reduced to Chebyshev iterations for
the operators c(†)
j , which allows application of the
“doubling trick” (see the remark after Eq. (138)). However, the numerical eﬀort can be further reduced by a
factor 1/L: If we keep the ground state |0⟩of the system
we can calculate the moments µij
n = ⟨0|ciTn( ˜H)c†
L elements i = 1, . . . , L of Gc
ij(ω) in a single Chebyshev
iteration. To achieve a similar reduction within the Lanczos recursion we had to explicitly construct the eigenstates to the Lanczos eigenvalues. Then the factor 1/L
is exceeded by at least ND additional operations for the
construction of N eigenstates of a D-dimensional sparse
matrix. Hence using KPM for the CPT cluster diagonalization the numerical eﬀort can be reduced by a factor of
1/L in comparison to the Lanczos recursion.
2. CPT for the Hubbard model
As a ﬁrst example we consider the 1D Hubbard model
(Eq. (148) with g = ω0 = 0), which is exactly solvable
by Bethe ansatz and was also extensively studied with DDMRG .
It thus provides the opportunity to assess the precision
of the KPM-based CPT. The top left panel of Figure 14
shows the one-particle spectral function at half-ﬁlling,
calculated on the basis of L = 16 site clusters and an
expansion order of N = 2048. The matrix dimension is
D ≈1.7·108. Remember that the cluster Green function
is calculated for a chain with open boundary conditions.
The reduced symmetry compared to periodic boundary
conditions results in a larger dimension of the Hilbert
space that has to be dealt with numerically. In the top
right panel the dots show the Bethe ansatz results for a
L = 64 site chain, and the lines denote the L →∞spinon
and holon excitations each electron separates into (spincharge separation).
So far the Bethe ansatz does not
k = kh + ks
E(k) / t = [Eholon(kh) + Espinon(ks)] / t
FIG. 14 (Color in online edition) Spectral function of the
1D Hubbard model for half-ﬁlling and U = 4t.
CPT result with cluster size L = 16 and expansion order
For similar data based on Lanczos recursion
see S´en´echal et al. . Top right: Within the exact Bethe
ansatz solution each electron separates into the sum of independent spinon (red dashed) and holon (green) excitations.
The dots mark the energies of a 64-site chain. Bottom: CPT
data compared to selected DDMRG results for a system with
L = 128 sites, open boundary conditions and a broadening of
ǫ = 0.0625t. Note that in DDMRG the momenta are approximate.
allow for a direct calculation of the structure factor, the
data thus represents only the position and density of the
eigenstates, but is not weighted with the matrix elements
of the operators c(†)
kσ . Although for an inﬁnite system we
would expect a continuous response, the CPT data shows
some faint ﬁne-structure. A comparison with the ﬁnitesize Bethe ansatz data suggests that these features are
an artifact of the ﬁnite-cluster Greens function which the
CPT spectral function is based on. The ﬁne-structure is
also evident in the lower panel of Figure 14, where we
compare with DDMRG data for a L = 128 site system.
Otherwise the CPT nicely reproduces all expected features, like the excitation gap, the two pronounced spinon
and holon branches, and the broad continuum. Note also,
that CPT is applicable to all spatial dimensions, whereas
DDMRG works well only for 1D models.
ground-state dispersion
tight binding dispersion
phonon excitation threshold
ω0/t=1.0, εp=0.5, Lc=16
ω0/t=1.0, εp=4, Lc=6
FIG. 15 Spectral function A+(k, ω) of a single electron in the
Holstein model (corresponding to Ne = 0 in Eq. (145)). For
weak electron-phonon coupling the original band is still very
pronounced (left), for intermediate-to-strong coupling many
narrow polaron bands develop (right).
The cluster size is
L = 16 (left) or L = 6 (right) and the expansion order N =
2048. See Hohenadler et al. for similar data based on
Lanczos recursion.
3. CPT for the Holstein model
Our second example is the spectral function of a single
electron in the Holstein model, i.e., Eq. (148) with U = 0.
Here, as a function of the electron-phonon interaction,
polaron formation sets in and the band width of the resulting quasi particles becomes extremely narrow at large
coupling strength. Figure 15 illustrates this behavior for
two values of the electron-phonon coupling εp = g2ω0.
For weak coupling the original one-electron band is still
clearly visible (dot-dashed line), but the dispersion-less
phonon (dashed line) cuts in approximately at an energy
ω0 above the band minimum, causing the formation of
a polaron band ), an avoided-crossing like gap and
a number of ﬁnite-size features. For strong coupling the
spectral weight of the electron is distributed over many
narrow polaron bands separated approximately by the
bare phonon frequency ω0.
In all these cases, KPM works as a reliable highresolution cluster solver, and using the concepts from
Sec. III.D we could also extend these calculations to ﬁnite
temperature. Probably, CPT is not the only approximate
technique that proﬁts from the simplicity and stability of
KPM, and the range of its applications can certainly be
V. KPM VERSUS OTHER NUMERICAL APPROACHES
After we have given a very detailed description of the
Kernel Polynomial Method and presented a wide range
of applications, let us now classify the method in the
context of numerical many-particle techniques and comment on a number of other numerical approaches that
are closely related to KPM.
A. KPM and dedicated many-particle techniques
In the previous sections we already compared KPM
data and results of other numerical many-particle techniques.
Nevertheless, it seems appropriate to add a
few comments about the general concept of such calculations and the role KPM-like methods play in the
ﬁeld of many-particle physics and complex quantum systems. The numerical study of interacting quantum manyparticle systems is complicated by the huge Hilbert space
dimensions involved, which usually grow exponentially
with the number of particles or the system size. There
are diﬀerent strategies to cope with this:
Carlo approaches only part of the Hilbert space is sampled stochastically, thereby trying to capture the essential physics with an appropriate weighting mechanism.
On the other hand, variational methods, like
DMRG or the
specialized approach of Bonˇca et al. , aim at reducing the Hilbert space dimension in an intelligent way
by discarding unimportant states, which, for instance,
contribute only at high temperature. Compared to such
methods KPM is much more basic: It is designed only for
the fast and stable calculation of the spectral properties
of a given matrix and of related correlations. Choosing a
suitable Hilbert space or optimizing the basis is the matter of the user or of external programs. It is thus a more
general approach, which can be used directly or embedded into other methods, as we illustrated in the preceding section. Of course, this simplicity and general applicability come at a certain price: For interacting manyparticle models the system sizes that can be studied by
using KPM directly are usually much smaller, compared
to DMRG and Monte Carlo. Note however, that both of
the latter methods have limitations too: For many interesting models Monte Carlo methods are plagued by the
infamous sign problem, which is not present in KPM.
When it comes to the calculation of dynamical correlation functions Monte Carlo approaches rely on power moments. The reconstruction of correlation functions from
power moments is known to be an ill-conditioned problem, in particular, if the moments are subject to statistical noise. The resolution of Monte Carlo results is
therefore much smaller compared to the data obtained
with KPM. The DMRG method develops its full potential only in one spatial dimension and for short ranged
interactions. In addition, the calculation of dynamical
correlations is limited to zero temperature, with only
a few exceptions . None of
these restrictions apply to KPM.
five δ-peaks
step function
KPM (Jackson kernel)
MEM 
FIG. 16 (Color in online edition) Comparison of a KPM and a
MEM approximation to a spectrum consisting of ﬁve isolated
δ-peaks, and to a step function. The expansion order is N =
512. Clearly, for the δ-peaks MEM yields a higher resolution,
but for the step function the Gibbs oscillations return.
B. Close relatives of KPM
Having compared KPM to specialized many-particle
methods, let us now discuss more direct competitors of
KPM, i.e., methods that share the broad application
range and some of its general concepts.
1. Chebyshev expansion and Maximum Entropy Methods
The ﬁrst of these approaches, the combination of
Chebyshev expansion and Maximum Entropy (MEM), is
basically an alternative procedure to transform moment
data µn into convergent approximations of the considered
function f(x). To achieve this, instead of (or in addition
to) applying kernel polynomials, an entropy
S(f, f0) =
(f(x)−f0(x)−log(f(x)/f0(x))) dx (170)
is maximized under the constraint that the moments of
the estimated f(x) agree with the given data. The function f0(x) describes our initial knowledge about f(x),
and may in the worst case just be a constant.
related to Maximum Entropy approaches to the classical moment problem , for the case of Chebyshev moments different implementations of the method have been suggested . Since for a given set of N moments
µn the approximation to the function f(x) is usually not
restricted to a polynomial of degree N −1, compared
to the KPM with Jackson kernel the Maximum Entropy
approach usually yields estimates of higher resolution.
However, this higher resolution results from adding a priori assumptions and not from a true information gain (see
also Figure 16). The resource consumption of Maximum
Entropy is generally much higher than the N log N behavior we found for KPM. In addition, the approach is
non-linear in the moments and can occasionally become
unstable for large N. Note also that as yet Maximum Entropy methods have been derived only for positive quantities, f(x) > 0, such as densities of states or strictly
positive correlation functions.
Maximum Entropy, nevertheless, is a good alternative
to KPM, if the calculation of the µn is particularly time
consuming. Based on only a moderate number of moments it yields very detailed approximations of f(x), and
we obtained very good results for some computationally
demanding problems .
2. Lanczos recursion
Lanczos Recursion Method
certainly the
most capable competitor of the Kernel Polynomial
Method .
It is based on the Lanczos
algorithm , a method which was initially developed for the tridiagonalization of Hermitian
matrices and later evolved to one of the most powerful methods for the calculation of extremal eigenstates
of sparse matrices . Although ideas like the mapping of the classical moment
problem to tridiagonal matrices and continued fractions
have been suggested earlier , the use of
the Lanczos algorithm for the characterization of spectral densities was ﬁrst proposed at about the same time as the Chebyshev expansion approaches, and in principle Lanczos recursion is
also a kind of modiﬁed moment expansion .
Its generalization
from spectral densities to zero temperature dynamical
correlation functions was ﬁrst given in terms of continued fractions , and later
also an approach based on the eigenstates of the tridiagonal matrix was introduced and termed Spectral Decoding
Method . This technique was then
generalized to ﬁnite temperature , and, in addition, some variants of the
approach for low temperature 
and based on the micro-canonical ensemble have been proposed recently.
To give an impression, in Table II we compare the
setup for the calculation of a zero temperature dynamical
correlation function within the Chebyshev and the Lanczos approach. The most time consuming step for both
methods is the recursive construction of a set of vectors
|φn⟩, which in terms of scalar products yield the moments
µn of the Chebyshev series or the elements αn, βn of the
Lanczos tridiagonal matrix. In terms of the number of
operations the Chebyshev recursion has a small advantage, but, of course, the application of the Hamiltonian
as the dominant factor is the same for both methods. As
a drawback, at high expansion order the Lanczos iteration tends to lose the orthogonality between the vectors
|φn⟩, which it intends to establish by construction. When
the Lanczos algorithm is applied to eigenvalue problems
this loss of orthogonality usually signals the convergence
of extremal eigenstates, and the algorithm then starts to
generate artiﬁcial copies of the converged states. For the
calculation of spectral densities or correlation functions
this means that the information content of the αn and
βn does no longer increase proportionally to the number of iterations. Unfortunately, this deﬁciency can only
be cured with more complex variants of the algorithm,
which also increase the resource consumption. Chebyshev expansion is free from such defects, as there is a
priori no orthogonality between the |φn⟩.
The reconstruction of the considered function from its
moments µn or coeﬃcients αn, βn, respectively, is also
faster and simpler within the KPM, as it makes use of
Fast Fourier Transformation. In addition, the KPM is
a linear transformation of the moments µn, a property
we used extensively above when averaging moment data
instead of the corresponding functions. Continued fractions, in contrast, are non-linear in the coeﬃcients αn,
A further advantage of KPM is our good understanding of its convergence and resolution as a function
of the expansion order N.
For the Lanczos algorithm
these issues have not been worked out with the same
We therefore think that the Lanczos algorithm is an
excellent tool for the calculation of extremal eigenstates
of large sparse matrices, but for spectral densities and
correlation functions the Kernel Polynomial Method is
the better choice. Of course, the advantages of both algorithms can be combined, e.g.
when the Chebyshev
expansion starts from an exact eigenstate that was calculated with the Lanczos algorithm.
3. Projection methods
Projection methods were developed mainly in the context of electronic structure calculations or tight-binding
molecular dynamics, which both require knowledge of
the total energy of a non-interacting electron system or
of related expectation values . The starting point of these methods is the density matrix F = f(H), where f(E) again represents
the Fermi function. Thermal expectation values, total
energies and other quantities of interest are then expressed in terms of traces over F and corresponding operators .
For instance,
the number of electrons and their energy are given by
Nel = Tr(F) and E = Tr(FH), respectively. To obtain
a numerical approach that is linear in the dimension D
of H, F is expanded as a series of polynomials or other
suitable functions in the Hamiltonian H,
1 + eβ(H−µ) =
and the above traces are replaced by averages over random vectors |r⟩. Chebyshev polynomials are a good basis
Chebyshev / KPM
complexity
Lanczos recursion
complexity
Initialization:
˜H = (H −b)/a
|φ0⟩= A|0⟩,
|φ1⟩= ˜H|φ0⟩
µ0 = ⟨φ0|φ0⟩,
µ1 = ⟨φ1|φ0⟩
Initialization:
|φ0⟩= A|0⟩/β0,
Recursion for 2N moments µn:
|φn+1⟩= 2 ˜H|φn⟩−|φn−1⟩
µ2n+2 = 2⟨φn+1|φn+1⟩−µ0
µ2n+1 = 2⟨φn+1|φn⟩−µ1
Recursion for N coeﬃcients αn, βn:
|φ′⟩= H|φn⟩−βn|φn−1⟩,
αn = ⟨φn|φ′⟩
|φ′′⟩= |φ′⟩−αn|φn⟩,
|φn+1⟩= |φ′′⟩/βn+1
→very stable
→tends to lose orthogonality
Reconstruction in three simple steps:
Apply kernel: ˜µn = gnµn
Fourier transform: ˜µn →˜f(˜ωi)
Rescale: f(ωi) =
˜f[(ωi −b)/a]
a2 −(ωi −b)2
O(M log M)
Reconstruction via continued fraction
z −α2 −. . .
where z = ωi + i ǫ
→procedure is linear in µn
→procedure is non-linear in αn, βn
→well deﬁned resolution ∝1/N
→ǫ is somewhat arbitrary
TABLE II Comparison of Chebyshev expansion and Lanczos recursion for the calculation of a zero-temperature dynamical
correlation function f(ω) = P
n |⟨n|A|0⟩|2δ(ω −ωn). We assume N matrix vector multiplications with a D-dimensional sparse
matrix H, and a reconstruction of f(ω) at M points ωi.
for such an expansion of F ,
and the corresponding approaches are thus closely related
to the KPM setup we described in Sec. III.A. Note however, that the expansion in Eq. (171) has to be repeated
whenever the temperature 1/β or the chemical potential µ is modiﬁed. This is particularly inconvenient, if µ
needs to be adjusted to ﬁx the electron density of the system. To compensate for this drawback, at least partially,
we can make use of the fact that in Eq. (171) the expanded function and its expansion coeﬃcients are known
in advance: Using implicit methods the
order N approximation of F can be calculated with only
O(log N) matrix vector operations involving the Hamiltonian H. The total computation time for one expansion
is thus proportional to D log N, compared to DN if the
sum in Eq. (171) is evaluated iteratively, e.g., on the basis
of the recursion relation Eq. (10).
Projection methods can also be used for the calculation of dynamical correlation functions. In this case the
expansion of the density matrix, which accounts for the
thermodynamics, is supplemented by a numerical time
evolution. Hence, a general correlation function is writ-
Iitaka, W=14.9, N=256
KPM, W=15, N=50
3, M=1024, 240 samples
KPM, W=15, N=100
3, M=2048, 280 samples
KPM, W=15, N=200
3, M=2048, 8 samples
FIG. 17 (Color in online edition) The optical conductivity of
the Anderson model, Eq. (111), calculated with KPM and a
projection method . The disorder is W ≈15;
temperature and chemical potential read T = 0 and µ = 0.
⟨A; B⟩ω = lim
ei(ω+i ǫ)t Tr(ei Ht A e−i Ht BF) dt ,
Crank-Nicolson
 .
Of course,
not only the fermionic density matrix F but also its
interacting counterpart, exp(−βH), can be expanded
in polynomials, which leads to similar methods for
interacting
(Iitaka and Ebisuzaki,
To give an impression, in Figure 17 we compare the
optical conductivity of the Anderson model calculated
with KPM (see Sec. III.D.2) and with a projection approach . Over a wide frequency range the
data agrees very well, but at low frequency the projection results deviate from both KPM and the analytically
expected power law σ(ω) −σ0 ∼ωα. Presumably this
discrepancy is due to an insuﬃcient resolution or a too
short time-integration interval. There is no fundamental
reason for the projection approach to fail here.
In summary, the projection methods have a similarly
broad application range as KPM, and can also compete
in terms of numerical eﬀort and computation time. For
ﬁnite-temperature dynamical correlations the projection
methods are characterized by a smaller memory consumption. However, in contrast to KPM they require a
new simulation for each change in temperature or chemical potential, which represents their major disadvantage.
VI. CONCLUSIONS & OUTLOOK
In this review we gave a detailed introduction to the
Kernel Polynomial Method, a numerical approach that
on the basis of Chebyshev expansion allows for an eﬃcient calculation of the spectral properties of large matrices and of the static and dynamic correlation functions,
which depend on them. The method has a wide range
of applications in diﬀerent areas of physics and quantum chemistry, and we illustrated its capability with numerous examples from solid state physics, which covered
such diverse topics as non-interacting electrons in disordered media, quantum spin models, or strongly correlated electron-phonon systems.
Many of the considered quantities are hardly accessible with other methods, or could previously be studied only on smaller systems. Comparing with alternative numerical approaches,
we demonstrated the advantages of KPM measured in
terms of general applicability, speed, resource consumption, algorithmic simplicity and accuracy of the results.
Apart from further direct applications of the KPM outside the ﬁelds of solid state physics and quantum chemistry, we think that the combination of KPM with other
numerical techniques will become one of the major future
research directions. Certainly not only classical MC simulations and CPT, but potentially also other cluster approaches or quantum MC can proﬁt
from the concepts outlined in this review.
Acknowledgements
We thank A. Basermann,
B. B¨auml,
M. Hohenadler, E. Jeckelmann, M. Kinateder, G. Schubert, and in particular R.N. Silver for fruitful discussions and technical support.
Most of the calculations could only be performed with the generous grant
of resources by the John von Neumann-Institute for
Computing (NIC J¨ulich), the Leibniz-Rechenzentrum
M¨unchen (LRZ), the High Performance Computing Center Stuttgart (HLRS), the Norddeutscher Verbund f¨ur
Hoch- und H¨ochstleistungsrechnen (HLRN), the Australian Partnership for Advanced Computing (APAC)
and the Australian Centre for Advanced Computing and
Communications (ac3). In addition, we are grateful for
support by the Australian Research Council, the Gordon
Godfrey Bequest, and the Deutsche Forschungsgemeinschaft through SFB 652.