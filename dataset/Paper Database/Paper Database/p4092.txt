Quantum Circuit Learning
K. Mitarai,1, ∗M. Negoro,1, 2 M. Kitagawa,1, 3 and K. Fujii4, 2, †
1Graduate School of Engineering Science, Osaka University,
1-3 Machikaneyama, Toyonaka, Osaka 560-8531, Japan.
2JST, PRESTO, 4-1-8 Honcho, Kawaguchi, Saitama 332-0012, Japan
3Quantum Information and Quantum Biology Division,
Institute for Open and Transdisciplinary Research Initiatives, Osaka University
4Graduate School of Science, Kyoto University, Yoshida-Ushinomiya-cho, Sakyo-ku, Kyoto 606-8302, Japan.
 
We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum
processors, which we call quantum circuit learning. A quantum circuit driven by our framework
learns a given task by tuning parameters implemented on it.
The iterative optimization of the
parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that
a quantum circuit can approximate nonlinear functions, which is further conﬁrmed by numerical
simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for
quantum machine learning.
INTRODUCTION
In recent years, machine learning has acquired much
attention in a wide range of areas including the ﬁeld of
quantum physics . Since quantum information processing is expected to bring us exponential speedups on
some problems , usual machine learning tasks might
as well be improved when it is carried on a quantum
computer. Also, for the purpose of learning a complex
quantum system, it is natural to utilize a quantum system as our computational resource. A variety of machine
learning algorithms for quantum computers has been proposed , since Harrow-Hassidim-Lloyd (HHL) algorithm enabled us to perform basic matrix operations
on a quantum computer. These HHL-based algorithms
have the quantum phase estimation algorithm at its
heart, which requires a high-depth quantum circuit. To
circumvent a high-depth quantum circuit, which is still a
long-term goal on the hardware side, classical-quantum
hybrid algorithms consisting of a relatively low-depth
quantum circuit such as quantum variational eigensolver
 (QVE) and quantum approximate optimization
algorithm (QAOA) have been suggested. In these
methods, a problem is encoded into a Hermitian matrix
A. Its expectation value ⟨A⟩with respect to an ansatz
state |ψ(θ)⟩is iteratively optimized by tuning the parameter θ. The central idea of hybrid algorithms is dividing
the problem into two parts, each of which can be performed easily on a classical and a quantum computer.
In this paper, we present a new hybrid framework,
which we call quantum circuit learning (QCL), for machine learning with a low-depth quantum circuit.
QCL, we provide input data to a quantum circuit, and iteratively tunes the circuit parameters so that it gives the
∗ 
† 
desired output. Gradient-Based systematic optimization
of parameters is introduced for the tuning just like backpropagation method utilized in feedforward neural
We theoretically show that a quantum circuit driven by the QCL framework can approximate any
analytical function if the circuit has a suﬃcient number
of qubits. The ability of the QCL framework to learn
nonlinear functions and perform a simple classiﬁcation
task is demonstrated by numerical simulations. Also, we
show by simulation that a 6-qubit circuit is capable of
ﬁtting dynamics of 3 spins out of a 10-spin system with
fully connected Ising Hamiltonian. We stress here that
the proposed framework is easily realizable on near-term
QUANTUM CIRCUIT LEARNING
Our QCL framework aims to perform supervised or unsupervised learning tasks . In supervised learning, an
algorithm is provided with a set of input {xi} and corresponding teacher data {f(xi)}. The algorithm learns
to output yi = y(xi, θ) that is close to the teacher f(xi),
by tuning θ. The output and the teacher can be vectorvalued. QCL assigns the calculation of the output yi to a
quantum circuit and the update of the parameter θ to a
classical computer. The objective of learning is to minimize a cost function, which is a measure of how close the
teacher and the output is, by tuning θ. As an example,
the quadratic cost L = P
i ∥f(xi) −yi∥2 is often used in
regression problems. On the other hand, in unsupervised
learning (e.g. clustering), only input data are provided,
and some objective cost function that does not involve
teacher is minimized.
Here we summarize the QCL algorithm on N qubit
 
1. Encode input data {xi} into some quantum state
|ψin(xi)⟩by applying a unitary input gate U(xi) to
initialized qubits |0⟩
2. Apply a θ-parameterized unitary U(θ) to the input
state and generate an output state |ψout(xi, θ)⟩=
U(θ) |ψin(xi)⟩.
3. Measure the expectation values of some chosen observables. Speciﬁcally, we use a subset of Pauli operators {Bj} ⊂{I, X, Y, Z}⊗N. Using some output
function F, output yi = y(xi, θ) is deﬁned to be
y(xi, θ) ≡F ({⟨Bj(xi, θ)⟩}) .
4. Minimize the cost function L (f(xi), y(xi, θ)) of the
teacher f(xi) and the output yi, by tuning the circuit parameters θ iteratively.
5. Evaluate the performance by checking the cost
function with respect to a data set that is taken
independently from the training one.
Relation with existing algortihms
Minimization of the quadratic cost can be performed
using a high-depth quantum circuit with HHL-based algorithms. For example, Ref. shows a detailed procedure. This matrix inversion approach is similar to the
quantum support vector machine .
As opposed to
this, QCL applied to a regression problem minimizes the
cost by iterative optimization, successfully circumventing
a high-depth circuit.
Quantum reservoir computing (QRC) shares a
similar idea, in a sense that it passes the central optimization procedure to a classical computer. There, output is deﬁned to be y(xi) ≡w · ⟨B⟩where B is a set
of observables taken from quantum many-body dynamics driven with a ﬁxed Hamiltonian, and w is the weight
vector, which is tuned on a classical device to minimize a
cost function. The idea stems from a so-called echo-state
network approach . If one views QRC as a quantum
version of the echo-state network, QCL, which tunes the
whole network, can be regarded as a quantum counterpart of a basic neural network. In QVE/QAOA, the famous hybrid quantum algorithms, weighted sum of measured expectation values wﬁxed · ⟨B(θ)⟩is minimized by
tuning the parameter θ. There, an input x of a problem,
such as geometry of a molecule or topology of a graph,
is encoded to the weight vector wﬁxed as wﬁxed(x). This
procedure corresponds to a special case of QCL where
we do not use the input unitary U(x), and a cost function L = wﬁxed · ⟨B⟩is utilized. Fig. 1 summarizes and
shows the comparison of QVE/QAOA, QRC, and presented QCL framework.
࢝୤ixୣୢ⋅��:
to be minimized via �
QVE / QAOA
→��࢞, �࢞, �
: to be minimized via �
: to be minimized via ࢝
Simulated annealing
Echo-State Network
Neural Network
Classical counterpart
Hybrid algorithms
Comparison of QVE/QAOA, QRC, and presented
QCL framework. In QVE, the output of the quantum circuit
is directly minimized. QRC and QCL both optimize the output to the teacher f(x). QRC optimization is done via tuning
the linear weight w, as opposed to QCL approach which tunes
the circuit parameter θ.
Ability to approximate a function
First, we consider the case where input data are one dimension for simplicity. It is straightforward to generalize
the following argument for higher dimensional inputs.
Let x and ρin(x) = |ψin(x)⟩⟨ψin(x)| be an input
data and a corresponding density operator of input
state. ρin(x) can be expanded by a set of Pauli operators {Pk} = {I, X, Y, Z}⊗N with ak(x) as coeﬃcients,
ρin(x) = P
k ak(x)Pk. A parameterized unitary transformation U(θ) acting on ρin(x) creates the output state,
which can also be expanded by {Pk} with {bk(x, θ)}.
Now let uij(θ) be such that bm(x, θ) = P
k umk(θ)ak(x).
bm is an expectation value of a Pauli observable itself,
therefore, the output is linear combination of input coef-
ﬁcient functions ak under unitarity constraints imposed
When the teacher f(x) is an analytical function, we
can show, at least in principle, QCL is able to approximate it by considering a simple case with an input state
created by single-qubit rotations.
The tensor product
structure of quantum system plays an important role in
this analysis. Let us consider a state of N qubits:
ρin(x) = 1
This state can be generated for any x ∈[−1, 1] with
single-qubit rotations, namely, QN
i (sin−1 x), where
i (φ) is the rotation of ith qubit around y axis with
The state given by Eq. (1) has higher order
terms up to the Nth with respect to x. Thus an arbitrary
unitary transformation on this state can provide us with
an arbitrary Nth order polynomial as expectation values
of an observable.
Terms like x
1 −x2 in Eq. (1) can
enhance its ability to approximate a function.
Important notice in the example given above is that
the highest order term xN is hidden in an observable
X⊗N. To extract xN from Eq. (1), one needs to transfer
the nonlocal observable X⊗N to a single-qubit observable
using entangling gate such as the controlled-NOT gate.
Entangling nonlocal operations are the key ingredients of
the nonlinearity of an output.
The above argument can readily be generalized to
multi-dimensional inputs. Assume that we are given with
d-dimensional data x = {x1, x2, .., xd} and want higher
terms up to the nkth (k = 1, · · · , d) for each data, then
encode this data into a N = P
k nk-qubit quantum state
as ρin(x) =
I + xkXi +
These input states automatically has an exponentially
large number of independent functions as coeﬃcient set
to the number of qubits. The tensor product structure
of quantum system readily “calculates” the product such
The unitarity condition of uij may have an eﬀect to
avoid an overﬁtting problem, which is crucial for their
performance in machine learning or in regression methods.
One way to handle it in classical machine learning methods is adding a regularization term to the cost
function. For example, ridge regression adds regularization term ∥w∥2 to the quadratic cost function. Overall
i ∥f(xi) −w · φ(xi)∥2 + ∥w∥2 is minimized. The
weight vector w corresponds to the matrix element uij
in QCL. The norm of a row vector ∥ui∥, however, is
restricted to unity by the unitarity condition, which prevents overﬁtting, from the unitarity of quantum dynamics. Simple examples of this are given in the Appendix.
Possible quantum advantages
We have shown by above discussions that approximation of any analytical functions is possible with the use
of nonlinearity created by the tensor product. In fact,
nonlinear basis functions are crucial for many methods
utilized in classical machine learning.
They require a
large number of basis functions to create a complex model
that predicts with high precision.
However, the computational cost of learning increases with respect to the
increasing number of basis functions. To avoid this problem, the so-called kernel trick method, which circumvents
the direct use of a large number of them, is utilized .
In contrast, QCL directly utilizes the exponential number of functions with respect to the number of qubits to
model the teacher, which is basically intractable on classical computers. This is a possible quantum advantage of
our framework, which was not obvious from the previous
approaches like QVE or QAOA.
Moreover, let us now argue about the potential power
of QCL representing complex functions.
Suppose we
want to learn the output of QCL that is allowed to use
an unlimited resource in the learning process, via classical neural networks. Then it has to learn the relation
between inputs and outputs of a quantum circuit, which,
in general, includes universal quantum cellular automata
This certainly could not be achieved using a
polynomial-size classical computational resource to the
size (qubits and gates) of QCL. This implies that QCL
has a potential power to represent more complex functions than the classical counterpart. Further investigations are needed including the learning costs and which
actual learning problem enjoys such an advantage.
Optimization procedure
In QVE , it has been suggested to use gradientfree methods like Nelder-Mead. However, gradient-based
methods are generally more preferred when the parameter space becomes large. In neural networks, backpropagation method , which is basically gradient descent,
is utilized in the learning procedure.
To calculate a gradient of an expectation value of
an observable with respect to a circuit parameter θ,
suppose the unitary U(θ) consists of a chain of unitary transformations Ql
j=1 Uj(θj) on a state ρin and
we measure an observable B.
For convenience, we
use notation Uj:k = Uj · · · Uk.
Then ⟨B(θ)⟩is given
BUl:1ρinU †
. We assume Uj
generated by a Pauli product Pj, that is, Uj(θ) =
exp(−iθjPj/2). The gradient is calculated to be ∂⟨B⟩
BUl:j[Pj, Uj−1:1ρinU †
. While we cannot
evaluate the commutator directly, the following property
of commutator for an arbitrary operator ρ enables us to
compute the gradient on a quantum circuit:
[Pj, ρ] = i
The gradient can be evaluated by
where ρj = Uj:1ρinU †
j:1. Just by inserting ±π/2 rotation
generated by Pj and measuring the respective expectation values ⟨B⟩±
j , we can evaluate the exact gradient of an
observable ⟨B⟩, via ∂⟨B⟩
. A similar method
is used by Li et al. in their research of control pulse
optimization with target quantum system.
NUMERICAL SIMULATIONS
We demonstrate the performance of QCL framework
for several prototypical machine learning tasks by numerically simulating a quantum circuit in the form of Fig. 2
with N = 6 and D = 6. U(θ(i)
j ) in Fig. 2 is an arbitrary
FIG. 2. Quantum circuit used in numerical simulations. The
parameter θ of single qubit arbitrary unitaries U(θ(i)
optimized to minimize the cost function. D denotes the depth
of the cicuit.
rotation of a single qubit.
We use the decomposition
j3 ). H is Hamiltonian
of a fully connected transverse Ising model:
The coeﬃcients aj and Jjk are taken randomly from uniform distribution on [−1, 1]. Evolution time T is ﬁxed
The results shown throughout this section are
generated by the Hamiltonian with the same coeﬃcients.
Here we note that we have checked a similar result can
be achieved with diﬀerent Hamiltonians. The dynamics
under this form of Hamiltonian can generate a highly
entangled state and is, in general for a large number
of qubits, not eﬃciently simulatable on a classical computer. Eq. (4) is the basic form of interaction in trapped
ions or superconducting qubits, which makes the time
evolution easily implementable experimentally. θ is initialized with random numbers uniformly distributed on
[0, 2π]. In all numerical simulations, outputs are taken
from Z expectation values. To emulate a sampling, we
added small gaussian noise with standard deviation σ determined by σ =
2/Ns(⟨Z⟩2 −1)/4, where Ns and ⟨Z⟩
are the number of samples and a calculated expectation
value, to ⟨Z⟩. 
First, we perform ﬁtting of f(x) = x2, ex, sin x, |x| as
a demonstration of representability of nonlinear functions .
We use the normal quadratic loss for the
cost function.
The number of teacher samples is 100.
The output is taken from Z expectation value of the
ﬁrst qubit as shown in Fig. 2. In this simulation, we
allow output to be multiplied by a constant a which is
initialized to unity.
This constant a and θ are simultaneously optimized. Input state ρin(x) is prepared by
applying Uin(x) = Q
j (cos−1 x2)RY
j (sin−1 x) to initialized qubits |0⟩. This unitary creates a state similar to
Results are shown in Fig. 3. All of the functions are
well approximated by a quantum circuit driven by presented QCL framework. To approximate highly nonlinear functions such as sin x or a nonanalytical function
|x|, QCL has brought out the high order terms which
are initially hidden in nonlocal operators.
The result
of ﬁtting |x| (Fig. 3 (d)) is relatively poor because of
Demonstration of QCL performance to represent
functions. “initial” shows the output of quantum circuit with
randomly chosen θ, and “ﬁnal” is the output from optimized
quantum circuit. Each graph shows ﬁtting of (a) x2, (b) ex,
(c) sin x, (d) |x|.
its nonanalytical characteristics. A possible solution for
this is to employ diﬀerent functions as an input function,
such as Legendre polynomials. Although the choice of
input functions aﬀects the performance of QCL, the result shows that QCL with simple input has an ability to
output a wide variety of functions.
As a second demonstration, the classiﬁcation problem,
which is an important family of tasks in machine learning,
is performed. Fig. 4 (a) shows the training data set, blue
and red points indicate class 0 and 1 respectively. Here
we train the quantum circuit to classify based on each
training input data points xi = (xi,0, xi,1). We deﬁne
the teacher f(xi) for each input xi to be two dimensional
vector (1, 0) for class 0, and (0, 1) for class 1. The number of teacher samples is 200 (100 for class 0, and 100 for
class 1). The output is taken from the expectation value
of the Pauli Z operator of the ﬁrst 2 qubits, and they are
transformed by softmax function F . For d-dimensional
vector q, softmax function returns d-dimensional vector
F (q) with its kth element being Fk(q) = eqk/ P
Thus the output yi = (yi,0, yi,1) is deﬁned by yi =
F (⟨Z1(xi, θ⟩), ⟨Z2(xi, θ⟩)) For the cost function, we
use the cross-entropy L = P
k∈{0,1} (f(xi))k log yik.
The input state is prepared by applying Uin(x)
j (cos−1 x2
i,j mod 2)RY
j (sin−1 xi,j mod 2) to initialized
qubits |0⟩. j mod 2 is the remainder of j devided by 2. In
this task, the multiplication constant a is ﬁxed to unity.
Learned output is shown in Fig. 4 (b). We see that
QCL works as well for the nonlinear classiﬁcation task.
The same task can be classically performed using, for
Demonstration of a simple nonlinear classiﬁcation
task. (a) teacher data. Data points that belong to class 0,
1 is shown as blue and red dot, respectively. (b) Optimized
output from ﬁrst qubit (after softmax transformation). 0.5 is
the threshold for classiﬁcation, less than and greater than 0.5
means that the point is classiﬁed as class 0 and 1, respectively.
initial Z1
initial Z2
initial Z3
teacher Z1
teacher Z2
teacher Z3
Demonstration of ﬁtting quantum many-body dynamics.
Partial dynamics of a 10-spin system can be well
approximated by a 6-qubit circuit.
example, kernel-trick support vector machine.
Kerneltrick approach discards the direct use of a large number
of basis functions with respect to the number of qubits,
as opposed to QCL approach, which utilizes an exponentially large number of basis functions under certain
constraints. In this sense, QCL can beneﬁt from the use
of a quantum computer.
Finally, we demonstrate the ability of QCL to perform
a ﬁtting task of quantum many-body dynamics. Simulation of dynamics of the 10-spin system under the fully
connected transverse Ising Hamiltonian Eq. (4) is performed in advance to generate teacher data. Coeﬃcients
aj and Jjk are taken from a uniform distribution on
[−1, 1] independently of the coeﬃcients of Hamiltonian
in the circuit. The dynamics started from the initialized
state |0⟩⊗10. The transient at the beginning of evolution
is discarded for duration Ttransient = 300.
For practical use, one can employ dynamics obtained experimentally from a quantum system with unknown Hamiltonian as teacher data. Learned dynamics is of Z expectation values of 3 spins during t ∈[Ttransient, Ttransient + 8].
This span of t is mapped on x ∈[−1, 1] uniformly by
t = 4(x + 1) + Ttransient to be properly introduced to
input gate. Output are taken from Z expectation values of the ﬁrst, second, and third qubits of the circuit.
The quadratic cost function is employed. The number
of teacher samples is 100 for each. The multiplication
constant a is ﬁxed to unity.
The result is shown in Fig. 5. It is notable that the
3 observables of a complex 10-spin system can be well
ﬁtted, simultaneously, using the 3 observables of a tuned
6-qubit circuit. Although the task performed here is not
what is commonly referred to as quantum simulation,
we believe that we provide an alternative way to learn a
quantum many-body dynamics with a near-term quantum computer. It may also be possible to extract partial
information of the system Hamiltonian by taking derivative of the output with respect to x, which can readily be
performed using the same method of calculating a gradient.
CONCLUSION
We have presented a machine learning framework on
near-term realizable quantum computers. Our method
fully employs the exponentially large space of the quantum system, in a way that it mixes simply injected nonlinear functions with a low-depth circuit to approximate
a complex nonlinear function.
Numerical results have
shown the ability to represent a function, to classify, and
to ﬁt a relatively large quantum system. Also, the theoretical investigation has shown QCL’s ability to provide
us a means to deal with high dimensional regression or
classiﬁcation tasks, which has been unpractical on classical computers. We have recently become aware of related
works .
Appendix: Unitarity avoids overﬁtting
In this appendix, we demonstrate a simple example
that supports our claim in the main text that states the
unitarity of the transformation has an eﬀect to avoid
overﬁttings. We perform the one-dimensional ﬁtting task
with a small number of training data set to see the avoidance of the overﬁtting. To observe the unitarity eﬀect,
we ﬁx the multiplication constant a to unity. For simplicity, here we use a 3-qubit circuit in the same form of
the main text, with D = 3 and using Uin = Q
as an input gate . In this case, the set of basis function
that QCL utilizes is {x, x2, x3, (1 −x2)1/2, 1 −x2, (1 −
x2)3/2, x(1 −x2)1/2, x2(1 −x2)1/2, x(1 −x2)}. Therefore
for comparison, we run a simple classical linear regression
program using the same basis function set.
Fig. 6 (a) and (b) show the result of the task to ﬁt
data points of 0.5 sin x, with Gaussian noise of standard
deviation 0.05 added, using QCL and classical regression,
respectively. The result shows that, probably due to the
unitarity of the transformation, QCL accepts some errors
in the ﬁnal output, as opposed to the classical one which
does not accept any errors in the ﬁnal output, that is,
it overﬁts. As opposed to ∥w∥= 1 constraint on QCL,
the classical algorithm in this case output a weight vector
with ∥w∥≈134. Fig. 6 (c) and (d) show the result of
the task to ﬁt data points of x2, with Gaussian noise of
standard deviation 0.05 added, using QCL and classical
regression, respectively. Again, the same observation can
The weight vector obtained by the classical
algorithm exhibits ∥w∥≈15800 in this case.
FIG. 6. A simple example of the avoidance of the overﬁtting
resulting from unitarity. (a) and (c): Fitting result of noiseadded sin x and x2 using QCL. (b) and (d): Fitting result of
noise-added sin x and x2 using the classical regression with
same basis functions as used in QCL.
 G. Carleo and M. Troyer, Science 355, 602 .
 M. Rupp, A. Tkatchenko, K.-R. M¨uller, and O. A. von
Lilienfeld, Phys. Rev. Lett. 108, 058301 .
 P. Broecker, J. Carrasquilla, R. G. Melko, and S. Trebst,
Sci. Rep. 7, 8823 .
 R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von
Lilienfeld, J. Chem. Theory Comput. 11, 2087 .
 M. August and X. Ni, Phys. Rev. A 95, 012335 .
 P. W. Shor, SIAM J. Comput. 26, 1484 .
 M. A. Nielsen and I. L. Chuang, Quantum Computation
and Quantum Information .
 I. Kerenidis and A. Prakash, arXiv:1704.04992.
 N. Wiebe, D. Braun, and S. Lloyd, Phys. Rev. Lett. 109,
050505 .
 P. Rebentrost, M. Mohseni,
and S. Lloyd, Phys. Rev.
Lett. 113, 130503 .
 Y. Cao, G. G. Guerreschi,
and A. Aspuru-Guzik,
 
 A. W. Harrow, A. Hassidim,
and S. Lloyd, Phys. Rev.
Lett. 103, 150502 .
 A. Peruzzo, J. McClean, P. Shadbolt, M. Yung, X. Zhou,
P. J. Love, A. Aspuru-Guzik,
and J. L. O’Brien, Nat.
Commun. 5 .
 A. Kandala, A. Mezzacapo, K. Temme, M. Takita,
M. Brink, J. M. Chow,
and J. M. Gambetta, Nature
549, 242 .
Goldstone,
 
 E. Farhi and A. W. Harrow, arXiv:1602.07674.
 J. S. Otterbach, R. Manenti, N. Alidoust, A. Bestwick,
M. Block, B. Bloom, S. Caldwell, N. Didier, E. S. Fried,
S. Hong, P. Karalekas, C. B. Osborn, A. Papageorge,
E. C. Peterson, G. Prawiroatmodjo, N. Rubin, C. A.
Ryan, D. Scarabelli, M. Scheer, E. A. Sete, P. Sivarajah,
R. S. Smith, A. Staley, N. Tezak, W. J. Zeng, A. Hudson, B. R. Johnson, M. Reagor, M. P. da Silva,
C. Rigetti, , arXiv:1712.05771.
 C. M. Bishop, Pattern Recognition and Machine Learning
 .
 M. Schuld, I. Sinayskiy, and F. Petruccione, Phys. Rev.
A 94, 022342 .
 K. Fujii and K. Nakajima, Phys. Rev. Appl. 8, 024030
 H. Jaeger and H. Haas, Science 304, 78 .
 R. Raussendorf, Phys. Rev. A 72, 022301 ,
0412048v1.
 D. Janzing and P. Wocjan, Quantum Inf. Process. 4, 129
 J. Li, X. Yang, X. Peng, and C. Sun, Phys. Rev. Lett.
118, 150503 .
 The simulation is carried on using Python library QuTip
 . We use BFGS method provided in SciPy optimization library for optimization of parameters.
 L. Cincio, Y. Subasi, A. T. Sornborger, and P. J. Coles,
 
 E. Farhi and H. Neven, arXiv:1802.06002.
Benedetti,
Garcia-Pintos,
A. Perdomo-Ortiz, arXiv:1801.07686.
 M. Schuld and N. Killoran, , arXiv:1803.07128.
 W. Huggins, P. Patel, K. B. Whaley, and E. M. Stoudenmire, , arXiv:1803.11537.
 J.-G. Liu and L. Wang, , arXiv:1804.04168.
 M. Schuld, A. Bocharov, K. Svore,
and N. Wiebe,
 , arXiv:1804.00633.
 M. Fanizza, A. Mari,
and V. Giovannetti,
 
 M. Benedetti, E. Grant, L. Wossnig,
and S. Severini,
 , arXiv:1806.00463.
 J. Johansson, P. Nation,
and F. Nori, Comput. Phys.
Commun. 184, 1234 .
 J. Nocedal and S. Wright, Numerical Optimization
 .