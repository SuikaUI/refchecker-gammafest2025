ADEPOS: ANOMALY DETECTION BASED POWER SAVING FOR PREDICTIVE
MAINTENANCE USING EDGE COMPUTING
Sumon Kumar Bose
School of Electrical and Electronic Engineering
Nanyang Technological University
Singapore 639798
 
School of Electrical and Electronic Engineering
Nanyang Technological University
Singapore 639798
 
Mohendra Roy
School of Electrical and Electronic Engineering
Nanyang Technological University
Singapore 639798
 
Pradeep Kumar Gopalakrishnan
School of Electrical and Electronic Engineering
Nanyang Technological University
Singapore 639798
 
Arindam Basu
School of Electrical and Electronic Engineering
Nanyang Technological University
Singapore 639798
 
October 30, 2018
In Industry 4.0, predictive maintenance (PM) is one of the most important
applications pertaining to the Internet of Things (IoT). Machine learning is used to
predict the possible failure of a machine before the actual event occurs. However,
main challenges in PM are: (a) lack of enough data from failing machines, and (b)
paucity of power and bandwidth to transmit sensor data to cloud throughout the
lifetime of the machine. Alternatively, edge computing approaches reduce data
transmission and consume low energy. In this paper, we propose Anomaly
Detection based Power Saving (ADEPOS) scheme using approximate computing
through the lifetime of the machine. In the beginning of the machine’s life, low
accuracy computations are used when machine is healthy. However, on detection
of anomalies as time progresses, system is switched to higher accuracy modes. We
show using the NASA bearing dataset that using ADEPOS, we need 8.8X less neurons
on average and based on post-layout results, the resultant energy savings are 6.4-
A PREPRINT - OCTOBER 30, 2018
1 Introduction
Machine failures can be very expensive for industries as they result in unplanned downtime and loss of productivity.
Maintenance programs typically reduce downtime of machines through routine, scheduled maintenance. Statistical
models and reliability data are used to create maintenance schedules that involve periodically replacing machine
parts, regardless of their state of health. Although it results in fewer breakdowns, periodic maintenance is not very
cost effective. Whenever a non-failing part is replaced during scheduled maintenance, its remaining useful life is
wasted . This is a major motivation for industry to move towards Predictive Maintenance (PM). There have been
several previous attempts to employ machine learning (ML) techniques in PM such as for aero-engine control system
sensor fault detection , and fault diagnostics of rotary machine .
Figure 1: (a) Autoencoder (AE) Architectures: In reconstruction based one class classification (OCC), AE is trained using healthy data to learn input
statistics and to reconstruct the input vector at the output. (b) Boundary based OCC is trained to produce a value of 1 at the output for healthy
data. During testing when either reconstructed output deviates from input (reconstruction based OCC ) or from 1 (boundary based OCC) by a
threshold amount, the machine can be declared as faulty.
Several challenges are associated with Predictive Maintenance systems employing ML techniques that includes
nonavailability of adequate amount of data for constructing the prediction model. Adequate information related to
machine failures are difficult to obtain as failures are relatively rare and failure signatures vary drastically for
different kind of machine’s failures. Therefore, learning from a generic dataset may not be an effective approach. It
is more reasonable to learn the models from sensor’s data attached to the machine. Thus, anomaly detection
approach is more suitable for this kind of health monitoring tasks , by identifying machine failures based on the
deviations from healthy data .
Typically, data from IoT sensors are sent to servers for final processing towards failure prediction. This involves large
amounts of data transmission and thus lead to significant power consumption and bandwidth requirement.
Moreover, unacceptable latencies may be incurred in this approach. Instead, the data processing can be pushed to
the edge where bulk of the processing is done near the IoT sensors and only the decision is transmitted to the servers,
thus realizing low latency systems. However, this requires the machine learner on edge device to consume low energy
to last the lifetime of the machine .
Here we present an edge computing framework that exploits anomaly detection for machine health monitoring
and reduces energy consumption of the machine learner. We outline the major contributions as: (a) use of ELMboundary
(ELM-B) based anomaly detector for predictive maintenance with ≈ 20% less convergence time than traditional AE,
(b) adaptive use of approximate computing techniques along the age of the machine in order to save energy without
compromising on the detection accuracy (ADEPOS), and (c) a hardware architecture to implement the
aforementioned approximate computing scheme. While other work have shown hardware architecture for precision
scaling of CNN at different steps of the algorithm, our proposal enables even higher power savings by running the
entire processor at low precision for most of the life of the machine. The scheme is general and can be used with
other approximation enabled processors.
We revisit the preliminaries on autoencoder (AE) and extreme learning machines (ELM) in section 2. Subsequently,
we discuss Anomaly Detection based Power Saving (ADEPOS) scheme in section 3. In section 4, we present the results
on our experiments on NASA bearing dataset, followed by the concluding remarks.
A PREPRINT - OCTOBER 30, 2018
2 Preliminaries
Autoencoder and Extreme Learning Machines
An Autoencoder (AE) is a single layer feed forward neural network, as depicted in Fig. 1(a), consisting of input layer
X = [x1,x2,···,xd]T, output layer X˜ = [x˜1,x˜2,···,x˜d]T, and hidden layer h = [h1,h2,···,hL]T, using d, d and L neurons
respectively. AE is trained to learn input data distribution and to reconstruct input features at its output. The hidden
neurons encode an input vector X into feature space h using connection weights W and biases b = [b1,b2,···,bL]T.
Likewise, weights (β) between the hidden and output neurons are used to decode feature space h. In boundary based
architecture (Fig. 1(b)), the model is trained to produce 1 (or any non-zero real number) at its output . Unknown
parameters W, β, b of both architectures are learned using healthy data of the machine since healthy data is readily
available. During online monitoring (testing) of machine health when either reconstructed output deviates from
input (reconstruction based OCC) or from 1 (boundary based OCC) by a threshold, an anomaly is detected and the
machine is declared as faulty.
Figure 2: Ensemble Learner containing N-base learners (BL) each having L hidden neurons. The final output is decided based on the basis of
majority voting of all active BLs.
Details of the encoding and decoding process of AE are presented in Eq. 1 and 2, where g() refers to the neuronal
activation function and hj denotes output of the j-th hidden neuron.
While g() is typically chosen as sigmoid, we found Rectified Linear Unit (ReLU) to be equally good for AE with the
benefit of easier hardware implementation.
Training of traditional autoencoder (TAE) requires a sufficiently large amount of data and iterations for computing
optimal W and β values. Backpropagation method, although known to yield very accurate models, incurs high
computational overhead. An alternative framework, known as Extreme Learning Machines (ELM), has been proposed
by choosing W and biases b randomly from a continuous probability distribution .
In the batch approach for computing least square solution of β, hidden neuron outputs H are computed for N
data samples X¯ = [X1,X2,···,XN]. Since the desired targets are same as training samples X¯ for a reconstruction based
AE, the optimal output weight β∗ is computed as shown in Eqn. 3 where H† denotes the Moore-Penrose generalized
inverse .
Evidently, the batch approach for computing β∗ requires a large number of mathematical operations including matrix
inversion and multiplication involving H and X¯ on a large number of data samples. These computations require large
A PREPRINT - OCTOBER 30, 2018
memory and computational energy making them difficult to implement on a sensor node. Alternatively, several
online learning techniques such as OSELM and OPIUM have been presented in which have the advantage of
not requiring to store the entire training data and reducing computational overhead.
OPIUM method has been adopted for our reconstruction and boundary based one class classification methods due
to its smaller computational overhead and reasonable convergence time (see Section 44.1). The equations for
updating β for each training sample are shown in Eqn. 4, 5 and 6 using initial value of θLXL (θ0 = cI, c = constant and I
= LXL Identity Matrix) and an initial estimate of β (β0) which is computed with a small number of samples (N0 ≥ L)
Given the convergence speed of ELM methods and the requirement of lesser output neurons in boundary methods,
we combine the advantages and use the ELM boundary method in this work and refer to it as ELM-B in the rest of
the paper.
Figure 3: Flowchart for ADEPOS: Approximate computing is achieved by adapting the number of hidden neurons in an Ensemble Learning
architecture.
Anomaly Detection and Machine Health Monitoring
In this work, ELM-B based OCC model is trained online during the early phase of machine’s life, when the machine is
supposed to be in good health. When the learning converges with minimal change in β, the OCC engine enters into
the monitoring (testing) mode. During the early phase of monitoring, the sensor data usually indicates good health
of the machine by producing minimal error. If the machine is undergoing any kind of degradation subsequently, the
corresponding error from the OCC engine will deviate from the ideal value 1 and can thus be treated as an indication
of a potential failure of the machine.
3 Our work
In this section, we present an approximate computing strategy–ADEPOS–for machine health monitoring application
based on anomaly detection. Though we use ELM-B as the machine learning algorithm, ADEPOS is applicable to other
algorithms as well. We also present a VLSI architecture for realizing ADEPOS.
ADEPOS: Anomaly Detection based Power Savings
It is expected that in the beginning of the machine’s lifetime, its health condition will have very less degradation.
Hence, output of the OCC will be close to ideal and far from the threshold to flag an anomaly. We hypothesize that Fault?
Calculatetest
BL ? =1 Yes
DeclareFault
TrainedOddNumberofBaselearners
SampleSensorsdata
A PREPRINT - OCTOBER 30, 2018
we can use very coarse/approximate computing at this initial stage such that the injected errors due to approximation
do not cross the anomaly threshold. If an error is observed, we can dynamically increase the accuracy of computation
to verify if this is truly an error or a false alarm. Hence, we propose to save power throughout the machine’s lifetime
based on feedback from the anomaly detector itself justifying the name “ADEPOS". Approximations may be
introduced in the network in many ways such as reducing the number of neurons in the network, reducing precision
or bit-width of the datapath, reducing accuracy in feature extraction and so on. In this paper, we just show a method
to reduce the number of neurons adaptively as a representative example of applying ADEPOS while leaving other
approximations for future work.
One of the main challenges of dynamically varying the number of hidden neurons L in the network is that we cannot
afford to retrain the new network. Hence, it is best if we can train a larger network in the learning phase and then
adaptively shut down parts of this bigger network. However, training a single ELM network with large value of L and
then pruning neurons will give incorrect results since the weights obtained during training for neuron ‘i’ is affected
by neuron ‘j’. One of the ways to circumvent this problem is to train N different mini ELM-B based models from the
input data and create their ensemble as the case with large L. We call each mini ELM-B based model as a base
learner with L hidden neurons (see Fig. 2).
From a hardware viewpoint, to train a larger neural network of NL neurons we need N2L2 memory for storing θk (Eq.
4) whereas for N base learners ( L neurons for each BL) we need only NL2 memory. This helps to reduce memory area
and leakage in the chip implementation. In related work, application of ELM-based adaBoosting method is mentioned
in to enhance system resiliency but it has not been exploited for energy optimization.
The flowchart in Fig. 3 shows our proposed algorithm for dynamic increase and decrease of network complexity using
ensemble method. At any iteration, effective number of hidden neurons in the network, Leff, is given by:
Leff = L × NBL
Figure 4: VLSI Architecture of one base learner (ELM based OCC Engine). A system with multiple such base learners can be tuned by ADEPOS to
gain energy savings.
where NBL is the number of active or selected base learners (BL) in the network. In the training phase, we trained
different BL models to learn the input data statistics. During online testing of the machine health, we start with
maximum odd number of BL models available in the network and take the majority voting to decide the output of
the anomaly detector. If the system does not raise any alarm for the input sample, we can reduce the number of BL
by 2 (since we always need odd number of BLs to get majority voting) at that step. On the other hand, if the system
raises an alarm, we immediately increase the number of BLs by 2 to verify if this is truly a fault or just a false alarm.
When the anomaly detector raises an alarm and all available BLs are active, we declare a valid fault and call for
maintenance. To get power savings, power Supply, Vdd, of inactive BLs is lowered to the minimum value Vdd,retention
where its memory can retain its parameters to reduce leakage power.
TD Hidden Neuron
TD Output Neuron
output layer
input layer
Online Learning Module
A PREPRINT - OCTOBER 30, 2018
VLSI Architecture for ELM-AE
Here, we present an overview of the VLSI implementation of the ELM based OCC (Fig. 4). As mentioned earlier, we
implemented the online learning framework OPIUM in order to reduce the number of computations per sample
as well as to reduce memory area. To reduce logic area, we adopt a minimalist approach with a single neuron each
for both hidden and output layers configured to work in time-division-multiplexing (TDM) fashion. This architecture
also provides the flexibility to configure the system as either a reconstruction or a boundary based (d = 1 and x˜1 = 1)
ELM network for OCC (trade-off is higher processing time for reconstruction mode).
The bit width of the data path is configurable from 8 to 16 bits while the number of input and output neurons can
vary from 1 to 16. In this design, we use pseudo-random binary sequence (PRBS) module for generating [W, b] for
the input layer in order to reduce on-chip memory requirement, while we store the output weights β∗ in on-chip
memory. In order to reduce area and leakage power of the SRAM, we restrict the maximum datapath width to 16bit.
Vdd for the learning module is independent from inference and can be shut off after convergence of online learning.
We used TSMC 65nm Low power (LP) technology library for implementing this design. Power analysis was done at
20MHz on the post-layout netlist.
For validating our work, we use NASA bearing dataset provided by the Center for Intelligent Maintenance
(IMS), University of Cincinnati that is commonly used for testing machine health detection algorithms. Using the
information provided in , we label the corresponding failed bearings as 1, while 0 is used for the non-failing ones
for our cross validation exercise. So far, time, frequency and time–frequency domain analysis of raw bearing data
were used extensively for relevant feature extraction . Statistical feature extraction processes on the time-series
data were shown to be useful for bearing health monitoring . However, frequency domain based feature
extraction techniques were also used for machine health monitoring in the industries to a great extent . In this
work, we conduct our experiments on five time-domain features extracted from the raw vibration data contained in
NASA dataset, such as (a) RMS, (b) Kurtosis, (c) Peak-Peak, (d) Crest factor, and (e) Skewness since we have validated
that this is a minimal set of features that are most informative. Therefore, in our experiments, we set d = 5 for
obtaining all the parameters, both in MATLAB and hardware simulations.
Figure 5: Convergence study of different OCC methods which shows that convergence of OPIUM-B is ≈ 20% faster than TAE-Un.
A comparative study of ELM-based algorithms and traditional AEs
A comparative study of ELM based methods in classifying health conditions of bearings summarized in Fig. 5
shows that ELM based models usually take fewer samples to converge than the traditional AEs . In case of traditional
AEs, we use both tied (TAE-Ti) and untied versions (TAE-Un) . For OSELM and OPIUM, we use both boundary
OSELM-B OSELM-AE OPIUM-B OPIUM-AE TAE-Un
300 No. of Samples
A PREPRINT - OCTOBER 30, 2018
and reconstruction based methods. In the above plot, they are represented as OSELM-B, OSELM-AE, OPIUM-B, and
OPIUM-AE respectively and the values correspond to the number of training samples taken by the respective models
to converge with the same criteria. Though TAE-Ti takes less number of samples, its detection accuracy is poor
compared to TAE-Un. Hence, we do not consider it for comparing with ELM-based algorithms. Clearly, ELM-B
(OSELM-B/OPIUM-B) and OSELM-AE based OCC methods take fewer samples to converge compared to TAE-Un. Even
though OSELM-B converges faster than OPIUM-B, the computational complexity of OSELM-B (O(L3)) is higher than
OPIUM-B (O(L2)) as OSELM-B involves matrix inversion during β update. Since OPIUM-B takes less computation
during training and testing phase, it will consume less power. Hence, we decided to use OPIUM-B algorithm as the
online learning algorithm.
ADEPOS: Accuracy vs. Network size
Threshold Selection
As the number of bearing data is limited and there is no information regarding time instance to failure, we utilize a
leave-one-out strategy in order to estimate the threshold value (Thr) using Eqn. 8. In this threshold calculation
we utilized reconstruction errors (difference of reconstructed and expected output) obtained only for good bearings
out of all 11 training bearings data. This threshold is then used to test the remaining bearing data. We iterate this
approach for each of 12 bearings used as one test dataset.
Thr = Max(Err) + 0.5 × k × σErr
Here, for each good bearing X, we first note the maximum value of testing error, TX across its lifetime. Max(Err) and
σErr are computed as the maximum and standard deviation of these TX values. In each experimental results presented
here, we use k = 1. By changing K, threshold of the anomaly detector can be varied.
Adaptive Hidden Neurons
In Fig. 6, we present the variation of detection accuracy with respect to effective number of hidden neurons (Leff),
using ensemble learning method for specific L values. As expected, for same value of L, accuracy increases if higher
number of base learners ensemble together. To decide on a suitable value of L, we generate NBL = (9,7,5) base
learners for L = (20,30,40) respectively. It can be seen from the figure that for the same value of Leff, creating an
ensemble of larger number of base learners with smaller L yields higher accuracy.
Based on the previous study, we fix L = 20 and NBL = 9 and simulate the ADEPOS algorithm described earlier since
we obtained almost 100% accuracy at this configuration. Based on the adaptive usage of number of ensembles, we
find that the average value of Leff (over 10 trials) throughout the lifetime of all the bearings is only 20.42 without
Figure 6: Mean Accuracy vs. number of hidden neurons (Leff) for detecting health condition of 12 bearings. Higher number of base learners with
small L is preferable.
1 Mean Accuracy
A PREPRINT - OCTOBER 30, 2018
Figure 7: Impact of hidden neurons (Leff) and effective bit precision of datapath on Energy and Accuracy
sacrificing the accuracy obtained by using Leff = 180 for NBL = 9 networks in the ensemble. Thus compared to the case
of using a fixed value of L = 180 neurons, ADEPOS enables a 8.8X reduction in effective number of neurons.
ADEPOS: Energy Savings
In this section, we study the impact of ADEPOS on energy savings through simulation of our VLSI architecture. We
also evaluate the effect of bit precision by running simulations with data width of (16,12,8) bits. In power analysis,
we varied Leff from 180 down to 20 in steps of 40 and effective bit width from 16bits down to 8bits in steps of 4bits.
As expected, we notice that the energy consumption reduces from the highest possible accuracy level (Leff = 180 and
bit width = 16bits), to various lower levels dictated by the values of Leff and the effective bit width of the datapath as
shown in Fig. 7. Since detection accuracy degrades drastically at 8bits and lower, we fix the minimum bit precision to
12bits. The figure provides an idea of how much energy savings is possible by using ADEPOS to vary the number of
hidden neurons using the algorithm depicted in Fig. 3.
Table 1: Energy Consumption of Different algorithms
Energy(nJ)
Normalized Energy
Figure 8: Energy savings by use of ELM-B and ADEPOS mode.
8 Normalized Energy
A PREPRINT - OCTOBER 30, 2018
In particular, ADEPOS enables average 6.4 − 6.65X reduction in energy for 12-bit and 16-bit datapaths respectively
by varying the value of Leff. The combined energy savings obtained by choice of algorithm and use of ADEPOS are
presented in Fig. 8 and table 1 where 16bit datapath and L = 180 are used in ELM-AE and ELM-B.
5 Conclusion
In this paper, we present ELM based OCC for anomaly detection in machine health monitoring. Aimed at low energy
IoT application for Industry 4.0, our proposed methodology helps to reduce the energy consumption in edge
computing devices for anomaly detection. We have shown that ELM based online learning methods require lesser
convergence time than TAE. Further, usage of boundary methods as opposed to traditionally used reconstruction
methods allow reduction in energy. Further, we propose ADEPOS–a method to save power through approximate
computing along the lifetime of the machine by exploiting the fact that it is easy to classify machine health in early
part of its life. We adopt ensemble learning method in boundary based OCC that allows us to use a low number of
neurons and lower bit width, while dynamically changing to high accuracy mode on detection of anomalies. We show
that ADEPOS enables an average energy savings of 6.65X when trained to detect failures on NASA bearing dataset.
Though we have only demonstrated ADEPOS by varying the number of neurons and bit width in the network, it is
equally applicable to other means of approximation such as reduced sampling rate of raw data. In future, we will
explore methods to combine these different approximation modes for a generic solution.