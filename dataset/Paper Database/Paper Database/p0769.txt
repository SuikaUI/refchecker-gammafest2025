Deep Learning for Case-Based Reasoning through Prototypes:
A Neural Network that Explains Its Predictions
Oscar Li,∗1 Hao Liu,∗3 Chaofan Chen,1 Cynthia Rudin1,2
1Department of Computer Science, Duke University, Durham, NC, USA 27708
2Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA 27708
3Kuang Yaming Honors School, Nanjing University, Nanjing, China, 210000
 , , {cfchen, cynthia}@cs.duke.edu
Deep neural networks are widely used for classiﬁcation.
These deep models often suffer from a lack of interpretability – they are particularly difﬁcult to understand because of
their non-linear nature. As a result, neural networks are often treated as “black box” models, and in the past, have been
trained purely to optimize the accuracy of predictions. In this
work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special
prototype layer, where each unit of that layer stores a weight
vector that resembles an encoded training input. The encoder
of the autoencoder allows us to do comparisons within the
latent space, while the decoder allows us to visualize the
learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be
similar to at least one encoded input, a term that encourages
every encoded input to be close to at least one prototype, and
a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used
as part of the classiﬁcation process. Since the prototypes are
learned during training, the learned network naturally comes
with explanations for each prediction, and the explanations
are loyal to what the network actually computes.
Introduction
As machine learning algorithms have gained importance for
important societal questions, interpretability (transparency)
has become a key issue for whether we can trust predictions
coming from these models. There have been cases where
incorrect data fed into black box models have gone unnoticed, leading to unfairly long prison sentences . In radiology, lack of transparency causes challenges to FDA approval for deep learning products. Because of these issues, “opening the black
box” of neural networks has become a debated issue in
the media . Artiﬁcial neural networks are particularly
difﬁcult to understand because their highly nonlinear functions do not naturally lend to an explanation that humans are
able to process.
∗Contributed equally
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
In this work, we create an architecture for deep learning
that explains its own reasoning process. The learned models
naturally come with explanations for each prediction, and
the explanations are loyal to what the network actually computes. As we will discuss shortly, creating the architecture
to encode its own explanations is in contrast with creating
explanations for previously trained black box models, and
aligns more closely with work on prototype classiﬁcation
and case-based reasoning.
In the past, neural networks have often been designed
purely for accuracy, with posthoc interpretability analysis.
In this case, the network architecture was chosen ﬁrst, and
afterwards one aims to interpret the trained model or the
learned high-level features. To do the interpretability analysis requires a separate modeling effort. One problem with
generating explanations posthoc is that the explanations
themselves can change based on the model for the explanation. For instance, it may be easy to create multiple con-
ﬂicting yet convincing explanations for how the network
would classify a single object, none of which are the correct reason for why the object was classiﬁed that way. A
related issue is that posthoc methods often create explanations that do not make sense to humans. This means that
extra modeling is needed to ensure that the explanations are
interpretable. This happens, for instance, in the Activation
Maximization (AM) approach, where one aims to ﬁnd an
input pattern that produces a maximum model response for
a quantity of interest to the user . Since
the images from AM are not generally interpretable (they
tend to be gray), regularized optimization is used to ﬁnd an
interpretable high activation image . When we add regularization, however,
the result is a combination of what the network actually
computes and the extrinsic regularization. Given that the explanations themselves come from a separate modeling process with strong priors that are not part of training, we then
wonder how we can trust the explanations from the posthoc
analysis. In fact there is a growing literature discussing the
issues mentioned above for AM . For images, posthoc analysis often involves
visualization of layers of a neural network. For instance, an
alternative to AM was provided by Zeiler and Fergus ,
who use deconvolution as a technique to visualize what a
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
convolutional neural network (CNN) has learned. Deconvolution is one method for decoding; our method can use any
type of decoder to visualize the prototypes, including deconvolution. In addition, Zeiler and Fergus try to visualize parts of images that most strongly activate a given
feature map, but they do not provide an explanation for how
the network reaches its decision. In contrast, we build a reasoning process into our network and do not consider posthoc
analysis in this work.
There are other works that also build interpretability into
deep neural networks without using posthoc analysis. Pinheiro and Collobert design a network for weakly
supervised image segmentation by training a classiﬁcation
network that extracts important pixels which could potentially belong to an object of some class. Lei, Barzilay, and
Jaakkola propose a network architecture that extracts
parts of an input as a rationale and uses the rationale for
predictions. Both of these works build interpretability into
neural networks by extracting parts of an input and focusing
on those parts for their respective tasks. Our method differs
in that we use case-based reasoning instead of extractive reasoning – our model explains its predictions based on similarity to prototypical cases, rather than highlighting the most
relevant parts of the input; it is possible for their ideas to be
combined with ours. Tan, Sim, and Gales and Wu
et al. aim to improve the interpretability of activation patterns of feature maps in deep neural networks used
for speech recognition. In contrast, our model does not aim
to enforce a particular structure on feature maps – it allows
ﬂexibility in feature learning but introduces a special prototype layer for decision interpretation.
Our network is a form of prototype classiﬁer, where observations are classiﬁed based on their proximity to a prototype observation within the dataset. For instance, in our
handwritten digit example, we can determine that an observation was classiﬁed as a “3” because the network thinks it
looks like a particular prototypical “3” within the training
set. If the prediction is uncertain, it would identify prototypes similar to the observation from different classes, e.g.,
“4” is often hard to distinguish from “9”, so we would expect to see prototypes of classes 4 and 9 identiﬁed when the
network is asked to classify an image of a 9.
Our work is closely aligned with other prototype classi-
ﬁcation techniques in machine learning . Prototype classiﬁcation is a classical
form of case-based reasoning ; however, because our work uses neural networks, the distance measure
between prototypes and observations is measured in a ﬂexible latent space. The fact that the latent space is adaptive is
the driving force behind its high quality performance.
The word “prototype” is overloaded and has various
meanings. For us, a prototype is very close or identical to an
observation in the training set, and the set of prototypes is
representative of the whole data set. In other contexts, a prototype is not required to be close to any one of the training
examples, and could be just a convex combination of several
observations. In few-shot and zero-shot learning, prototypes
are points in the feature space used to represent a single
class, and distance to the protoype determines how an observation is classiﬁed. For example, ProtoNets utilize the mean of several embedded
“support” examples as the prototype for each class in fewshot learning. Li and Wang use a generative probabilistic model to generate prototypes for zero shot learning,
which are points in the feature space. In both cases, prototypes are not optimized to resemble actual observations, and
are not required to be interpretable (meaning that their visualizations will not generally resemble natural images), and
each class can have only one prototype.
Our deep architecture uses an autoencoder to create a latent low-dimensional
space, and distances to prototypes are computed in that latent space. Using a latent space for distance computation enables us to ﬁnd a better dissimilarity measure than L2 on
the pixel space. Other works also use latent spaces, e.g.,
Salakhutdinov and Hinton conduct a soft k-nearest
neighbors classiﬁcation on the latent space of a restricted
Boltzman machine autoencoder, although not for the aim of
interpretability.
Methodology
Network Architecture
Let D = {(xi, yi)}n
i=1 be the training dataset with xi ∈Rp
and yi ∈{1, ..., K} for each i ∈{1, ..., n}. Our model architecture consists of two components: an autoencoder (including an encoder, f : Rp →Rq, and a decoder, g : Rq →
Rp) and a prototype classiﬁcation network h : Rq →RK,
illustrated in Figure 1. The network uses the autoencoder to
reduce the dimensionality of the input and to learn useful
features for prediction; then it uses the encoded input to produce a probability distribution over the K classes through
the prototype classiﬁcation network h. The network h is
made up of three layers: a prototype layer, p : Rq →Rm, a
fully-connected layer w : Rm →RK, and a softmax layer,
s : RK →RK. The network learns m prototype vectors
p1, ..., pm ∈Rq (each corresponds to a prototype unit in
the architecture) in the latent space. The prototype layer p
computes the squared L2 distance between the encoded input z = f(xi) and each of the prototype vectors:
In Figure 1, the prototype unit corresponding to pj executes
the computation ∥z −pj∥2
2. The fully-connected layer w
computes weighted sums of these distances Wp(z), where
W is a K ×m weight matrix. These weighted sums are then
normalized by the softmax layer s to output a probability
distribution over the K classes. The k-th component of the
output of the softmax layer s is deﬁned by
k′=1 exp(vk′)
where vk is the k-th component of the vector v = Wp(z) ∈
During prediction, the model outputs the class that it
thinks is the most probable. In essence, our classiﬁcation algorithm is distance-based on the low-dimensional learned
reconstructed
  x)
transformed
classifier
  x)
prototype classifier network 
fully-connected
Figure 1: Network Architecture
feature space. A special case is when we use one prototype for every class (let m = K) and set the weight matrix
of the fully-connected layer to the negative identity matrix,
W = −IK×K (i.e. W is not learned during training). Then
the data will be predicted to be in the same class as the nearest prototype in the latent space. More realistically, we typically do not know how many prototypes should be assigned
to each class, and we may want a different number of prototypes from the number of classes, i.e., m ̸= K. In this case,
we allow W to be learned by the network, and, as a result,
the distances to all the prototype vectors will contribute to
the probability prediction for each class.
This network architecture has at least three advantages.
First, unlike traditional case-based learning methods, the
new method automatically learns useful features. For image datasets, which have dimensions equal to the number
of pixels, if we perform classiﬁcation using the original input space or use hand-crafted feature spaces, the methods
tend to perform poorly (e.g., k-nearest neighbors). Second,
because the prototype vectors live in the same space as the
encoded inputs, we can feed these vectors into the decoder
and visualize the learned prototypes throughout the training
process. This property, coupled with the case-based reasoning nature of the prototype classiﬁcation network h, gives
users the ability to interpret how the network reaches its predictions and visualize the prototype learning process without
posthoc analysis. Third, when we allow the weight matrix
W to be learnable, we are able to tell from the strengths of
the learned weight connections which prototypes are more
representative of which class.
Cost Function
The network’s cost function reﬂects the needs for both accuracy and interpretability. In addition to the classiﬁcation
error, there is a (standard) term that penalizes the reconstruction error of the autoencoder. There are two new error terms
that encourage the learned prototype vectors to correspond
to meaningful points in the input space; in our case studies,
these points are realistic images. All four terms are described
mathematically below.
We use the standard cross-entropy loss for penalizing the
misclassiﬁcation. The cross-entropy loss on the training data
D is denoted by E, and is given by
E(h◦f, D) = 1
−1[yi = k] log((h◦f)k(xi)) (3)
where (h ◦f)k is the k-th component of (h ◦f). We use the
squared L2 distance between the original and reconstructed
input for penalizing the autoencoder’s reconstruction error.
The reconstruction loss, denoted by R, on the training data
D is given by
R(g ◦f, D) = 1
∥(g ◦f)(xi) −xi∥2
The two interpretability regularization terms are formulated
as follows:
R1(p1, ..., pm, D) = 1
i∈[1,n] ∥pj −f(xi)∥2
R2(p1, ..., pm, D) = 1
j∈[1,m] ∥f(xi) −pj∥2
Here both terms are averages of minimum squared distances.
The minimization of R1 would require each prototype vector to be as close as possible to at least one of the training
examples in the latent space. As long as we choose the decoder network to be a continuous function, we should expect two very close vectors in the latent space to be decoded
to similar-looking images. Thus, R1 will push the prototype
vectors to have meaningful decodings in the pixel space. The
minimization of R2 would require every encoded training
example to be as close as possible to one of the prototype
vectors. This means that R2 will cluster the training examples around prototypes in the latent space. We notice here
that although R1 and R2 involve a minimization function
that is not differentiable everywhere, these terms are differentiable almost everywhere and many modern deep learning libraries support this type of differentiation. Ideally, R1
would take the minimum distance over the entire training
set for every prototype; therefore, the gradient computation
would grow linearly with the size of the training set. However, this would be impractical during optimization for a
large dataset. To address this problem, we relax the minimization to be over only the random minibatch used by the
Stochastic Gradient Descent (SGD) algorithm. For the other
three terms, since each of them is a summation over the entire training set, it is natural to apply SGD to randomly selected batches for gradient computation.
Putting everything together, the cost function, denoted by
L, on the training data D with which we train our network
(f, g, h), is given by
L((f, g, h), D) = E(h ◦f, D) + λR(g ◦f, D)
+ λ1R1(p1, ..., pm, D)
+ λ2R2(p1, ..., pm, D),
where λ, λ1, and λ2 are real-valued hyperparameters that
adjust the ratios between the terms.
Case Study 1: Handwritten Digits
We now begin a detailed walkthrough of applying our model
to the well-known MNIST dataset. The Modiﬁed NIST Set
(MNIST) is a benchmark dataset of gray-scale images of
segmented and centered handwritten digits . We used 55,000 training examples, 5,000 validation
examples, and 10,000 testing examples, where every image
is of size 28 × 28 pixels. We preprocess the images so that
every pixel value is in . This section is organized as
follows: we ﬁrst introduce the architecture and the training
details, then compare the performance of our network model
with other noninterpretible network models (including a regular convolutional neural network), and ﬁnally visualize the
learned prototypes, the weight matrix W, and how a speciﬁc
image is classﬁed.
Architecture Details
Hinton and Salakhutdinov show that a multilayer
fully connected autoencoder network can achieve good reconstruction on MNIST even when using a very low dimensional latent space. We choose a multilayer convolutional autoencoder with a symmetric architecture for the
encoder and decoder to be our model’s autoencoder; these
types of networks tend to reduce spatial feature extraction
redundancy on image data sets and learn useful hierarchical
features for producing state-of-the-art classiﬁcation results.
Each convolutional layer consists of a convolution operation followed by a pointwise nonlinearity. We achieve downsampling in the encoder through strided convolution, and use
strided deconvolution in the corresponding layer of the decoder. After passing the original image through the encoder,
the network ﬂattens the resulted feature maps into a code
vector and feeds it into the prototype layer. The resulting
unﬂattened feature maps are fed into the decoder to reconstruct the original image. To visualize a prototype vector in
the pixel space, we ﬁrst reshape the vector to be in the same
shape as the encoder output and then feed the shaped vector
(now a series of feature maps) into the decoder.
The autoencoder in our network has four convolutional
layers in both the encoder and decoder. All four convolutional layers in the encoder use kernels of size 3 × 3, same
zero padding, and stride of size 2 in the convolution stage.
The ﬁlters in the corresponding layers in the encoder and
decoder are not constrained to be transposes of each other.
Each of the outputs of the ﬁrst three layers has 32 feature
maps, while the last layer has 10. Given an input image of dimension 28×28×1, the shape of the encoder layers are thus:
14×14×32; 7×7×32; 4×4×32; 2×2×10, and therefore the
network compresses every 784-dimensional image input to a
40-dimensional code vector (2×2×10). Every layer uses the
sigmoid function σ(x) =
1+e−x as the nonlinear transformation. We speciﬁcally use the sigmoid function in the last
encoder layer so that the output of the encoder is restricted
to the unit hypercube (0, 1)40. This allows us to initialize
15 prototype vectors uniformly at random in that hypercube.
We do not use the rectiﬁed linear unit in the last encoder layer because using it would make it more difﬁcult to initialize the
prototype vectors, as initial states throughout R⩾0
need to be explored, and the network would take longer to
stabilize. We also speciﬁcally choose the sigmoid function
for the last decoder layer to make the range of pixel values
in the reconstructed output (0, 1), roughly the same as the
preprocessed image’s pixel range.
Training Details
We set all the hyperparameters λ, λ1, λ2 to 0.05 and the
learning rate to 0.0001. We minimize (7) as a whole: we do
not employ a greedy layer-wise optimization for different
layers of the autoencoder nor do we ﬁrst train the autoencoder and then the prototype classiﬁcation network.
Our goal in this work is not just to obtain reasonable accuracy, but also interpretability. We use only a few of the
general techniques for improving performance in neural networks, and it is possible that using more techniques would
improve accuracy. In particular, we use the data augmentation technique elastic deformation to improve prediction accuracy and reduce potential overﬁtting. The set of all elastic deformations is a
superset of afﬁne transformations. For every mini-batch of
size 250 that we randomly sampled from the training set, we
apply a random elastic distortion where a Gaussian ﬁlter of
standard deviation equal to 4 and a scaling factor of 20 are
used for the displacement ﬁeld. Due to the randomness in the
data augmentation process, the network sees a slightly different set of images during every epoch, which signiﬁcantly
reduces overﬁtting.
After training for 1500 epochs, our model achieved a classi-
ﬁcation accuracy of 99.53% on the standard MNIST training
set and 99.22% on the standard MNIST test set.
To examine how the two key elements of our interpretable
network (the autoencoder and prototype layer) affect predictive power, we performed a type of ablation study. In particular, we trained two classiﬁcation networks that are similar to
ours, but removed some key pieces in both of the networks.
The ﬁrst network substitutes the prototype layer with a fullyconnected layer whose output is a 15-dimensional vector, the
same dimension as the output from the prototype layer; the
second network also removes the decoder and changes the
nonlinearity to ReLU. The second network is just a regular
convolutional neural network that has similar architectural
complexity to LeNet 5 . After training
both networks using elastic deformation for 1500 epochs,
we obtained test accuracies of 99.24% and 99.23% respectively. These test accuracies, along with the test accuracy of
99.2% reported by Lecun et al. , are comparable to
the test accuracy of 99.22% obtained using our interpretable
network. This result demonstrates that changing from a traditional convolutional neural network to our interpretable
network architecture does not hinder the predictive ability
of the network (at least not in this case).
In general, it is not always true that accuracy needs to
be sacriﬁced to obtain interpretability; there could be many
models that are almost equally accurate. The extra terms in
the cost function (and changes in architecture) encourage the
model to be more interpretable among the set of approximately equally accurate models.
Visualization
Let us ﬁrst discuss the quality of the autoencoder, because
good performance of the autoencoder will allow us to interpret the prototypes. After training, our network’s autoencoder achieved an average squared L2 reconstruction error
of 4.22 over the undeformed training set, where examples
are shown in Figure 2. This reconstruction result assures us
that the decoder can faithfully map the prototype vectors to
the pixel space.
Figure 2: Some random images from the training set in the
ﬁrst row and their corresponding reconstructions in the second row.
Figure 3: 15 learned MNIST prototypes visualized in pixel
We visualize the learned prototype vectors in Figure 3,
by sending them through the decoder. The decoded prototype images are sharp-looking and mostly resemble real-life
handwritten digits, owing to the interpretability terms R1
and R2 in the cost function. Note that there is not a one-toone correspondence between classes and prototypes. Since
we multiply the output of the prototype layer by a learnable
weight matrix prior to feeding it into the softmax layer, the
distances from an encoded image to each prototype have differing effects on the predicted class probabilities.
We now look at the transposed weight matrix connecting
the prototype layer to the softmax layer, shown in Table 1, to
see the inﬂuence of the distance to each prototype on every
class. We observe that each decoded prototype is visually
similar to an image of a class for which the corresponding
entry in the weight matrix has a signiﬁcantly negative value.
We will call the class to which a decoded prototype is visually similar the visual class of the prototype.
The reason for such a signiﬁcantly negative value can be
understood as follows. The prototype layer is computing
the dissimilarity between an input image and a prototype
through the squared L2 distance between their representations in the latent space. Given an image xi and a prototype
pj, if xi does not belong to the visual class of pj, then the
distance between f(xi) and pj will be large, so that when
∥pj −f(xi)∥2
2 is multiplied by the highly negative weight
connection between the prototype pj and its visual class, the
product will also be highly negative and will therefore signiﬁcantly reduce the activation of the visual class of pj. As
a result, the image xi will likely not be classiﬁed into the visual class of pj. Conversely, if xi belongs to the visual class
of pj, then when the small squared distance ∥pj −f(xi)∥2
is multiplied by the highly negative weight connection between pj and its visual class, the product will not decrease
the activation of pj’s visual class too much. In the end, the
activations of every class that xi does not belong to will
be signiﬁcantly reduced because of some non-similar prototype, leaving only the activation of xi’s actual class comparatively large. Therefore, xi is correctly classiﬁed in general.
An interesting prototype learned by the network is the last
prototype in Table 1. It is visually similar to an image of
class 2; however, it has strong negative weight connections
with class 7 and class 8 as well. Therefore, we can think of
this prototype as being shared by these three classes, which
means that an encoded input image that is far away from this
prototype in latent space would be unlikely to be an image
of 7, 8, or 2. This should not be too surprising: if we look
at this decoded prototype image carefully, we can see that if
we hide the tail of the digit, it would look like an image of
7; if we connect the upper-left endpoint with the lower-right
endpoint, it would look like an image of 8.
Let us now look at the learned prototypes in Figure 3.
The three prototypes for class 6 seem to represent different
writing habits in terms of what the loop and angle of “6”
looks like. The ﬁrst and third 6’s have their loops end at the
bottom while the second 6’s loop ends more on the side.
The 2’s show similar variation. As for the two 3’s, the two
prototypes correspond to different curvatures.
Let us look into the model as it produces a prediction for
Table 1: Transposed weight matrix (every entry rounded off to 2 decimal places) between the prototype layer and the softmax
layer. Each row represents a prototype node whose decoded image is shown in the ﬁrst column. Each column represents a digit
class. The most negative weight is shaded for each prototype. In general, for each prototype, its most negative weight is towards
its visual class except for the prototype in the last row.
a speciﬁc image of digit 6, shown on the left of Table 2. The
distances computed by the prototype layer between the encoded input image and each of the prototypes are shown below the decoded prototypes in Table 2, and the three smallest
distances correspond to the three prototypes that resemble 6
after decoding. We observe here that these three distances
are quite different, and the encoded input image is signiﬁcantly closer to the third “6” prototype than the other two.
This indicates that our model is indeed capturing the subtle
differences within the same class.
After the prototype layer computes the 15-dimensional
vector of distances shown in Table 2, it is multiplied by the
weight matrix in Table 1, and the output is the unnormalized
probability vector used as the logit for the softmax layer.
The predicted probability of class 6 for this speciﬁc image is
Table 2: The (rounded) distances between a test image 6 and
every prototype in the latent space.
Case Study 2: Cars
The second dataset we use consists of rendered color images, each with 64 × 64 × 3 pixels, of 3D car models with
varying azimuth angles at 15◦intervals, from −75◦to 75◦
 . There are 11 views
of each car and every car’s class label is one of the 11 angles (see Figure 4). The dataset is split into a training set
(169 × 11 = 1859 images) and a test set is a
dataset of Zalando’s article images, consisting of a training
set of 60,000 examples and a test set of 10,000 examples.
Each example is a 28×28 grayscale image, associated with a
label from 10 classes, each being a type of clothes item. The
dataset shares the same image size and structure of training
and testing splits as MNIST.
We ran the same model from Case Study 1 on this fashion dataset and achieved a testing accuracy of 89.95%. This
result is comparable to those obtained using standard convolutional neural networks with max pooling reported on the
dataset website . The
learned prototypes are shown in Figure 9. For each class,
there is at least one prototype representing that class. The
learned prototypes have fewer details (such as stripes, precence of a collar, texture) than the original images. This
again shows that the model has recognized what information
is important in this classiﬁcation task – the contour shape of
the input is more useful than its ﬁne-grained details. The
learned weight matrix W is shown in Table 5 in the Supplementary Material.
Figure 9: 15 decoded prototypes for Fashion-MNIST.
Discussion and Conclusion
We combine the strength of deep learning and the interpretability of case-based reasoning to make an interpretable
deep neural network. The prototypes can provide useful insight into the inner workings of the network, the relationship between classes, and the important aspects of the latent
space, as demonstrated here. Although our model does not
provide a full solution to problems with accountability and
transparency of black box decisions, it does allow us to partially trace the path of classiﬁcation for a new observation.
We have noticed in our experiments that the addition of
the two interpretability terms R1 and R2 tend to act as regularizers and help to make the network robust to overﬁtting.
The extent to which interpretability reduces overﬁtting is a
topic that could be explored in future work.
Supplementary Material and Code: Our supplementary
material and code are available at this URL: 
com/OscarcarLi/PrototypeDL.
Acknowledgments: This work was sponsored in part by
MIT Lincoln Laboratory.