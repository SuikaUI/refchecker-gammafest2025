Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 85–96
July 5 - 10, 2020. c⃝2020 Association for Computational Linguistics
PLATO: Pre-trained Dialogue Generation Model with
Discrete Latent Variable
Siqi Bao∗, Huang He∗, Fan Wang, Hua Wu and Haifeng Wang
Baidu Inc., China
{baosiqi, hehuang, wangfan04, wu hua, wanghaifeng}@baidu.com
Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a
novel dialogue generation pre-training framework to support various kinds of conversations,
including chit-chat, knowledge grounded dialogues, and conversational question answering.
In this framework, we adopt ﬂexible
attention mechanisms to fully leverage the
bi-directional context and the uni-directional
characteristic of language generation. We also
introduce discrete latent variables to tackle the
inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition
are designed and carried out simultaneously
within a shared network. Comprehensive experiments on three publicly available datasets
verify the effectiveness and superiority of the
proposed framework.
Introduction
Dialogue generation is a challenging task due to
the limited corpus of human conversations, complex background knowledge, and diverse relationships between utterances. Recently, pre-trained
large-scale language models, such as BERT and XLNet ,
have achieved prominent success in natural language processing. Such models are usually constructed based on a massive scale of general text
corpora, like English Wikipedia or BooksCorpus
 , where distributed representations can be learned automatically from the raw
text. With these representations being ﬁne-tuned,
breakthroughs have been continuously reported for
various downstream tasks, especially those on natural language understanding, such as question answering, natural language inference, and so on.
∗First two authors contributed equally to this work.
This pre-training and ﬁne-tuning paradigm also
sheds light on the tasks of natural language generation, like dialogue generation. However, the
previous study demonstrates that there are some de-
ﬁciencies in performance while directly ﬁne-tuning
BERT on small conversation datasets . Possible reasons are
three-fold: 1) the underlying linguistic patterns in
human conversations can be highly different from
those in general text, which suggests a potentially
large gap in knowledge or data distribution; 2) the
training mode of uni-directional dialogue generation is also distinct from that of bi-directional natural language understating as applied in BERT; 3)
unlike most of the general NLP tasks, there exists
a one-to-many relationship in dialogue generation,
where the dialogue context may correspond to multiple appropriate replies.
In this paper, we propose a new method to tackle
the above challenges, aiming to obtain a highquality pre-training model for dialogue generation.
First of all, to reduce the gap between data distributions, large-scale Reddit and Twitter conversations are utilized to further pre-train the generation model (upon the basis of language models
pre-trained with general text). Secondly, to mitigate
the difference in training mode, a ﬂexible paradigm
integrating uni- and bi-directional processing is
employed in this work, which is inspired by the latest uniﬁed language modeling .
Thirdly, a discrete latent variable is introduced to
model the one-to-many relationship among utterances in conversations. Each value of the latent
variable corresponds to the particular conversational intent of one response, which is referred as
latent speech act.
Distinct with those controllable dialogue generation based on explicit labels (including emotion,
keywords, domain codes, and so on) , our latent variable gets
exempted from the restriction of human annotations and can be learned automatically from the
corpus in an unsupervised way. In the pre-training
of dialogue generation, response generation and latent act recognition are carried out simultaneously
within a shared network. Based on the context and
latent variable, the generation task tries to maximize the likelihood of the target response. Meanwhile, the recognition task aims to estimate the
latent variable w.r.t. the given context and target
response. Apparently, the accurate recognition of
the latent variable is a crucial factor in boosting the
quality of response generation.
We conducted experiments on three different
kinds of conversation tasks: chit-chat, knowledge
grounded conversation, and conversational question answering. Experimental results verify the effectiveness and superiority of our pre-trained model
as compared with the other state-of-the-art methods. Our pre-trained models and source code have
been released at GitHub, hoping to facilitate further
research progress in dialogue generation.1
Dialogue Generation Pre-training
Given a piece of context, there exist multiple appropriate responses, leading to diverse conversation
ﬂows. It is widely recognized that the capability
of modeling one-to-many relationship is crucial
for the dialogue generation system . To this end, we propose to encode discrete latent variables into transformer blocks for one-to-many relationship modeling, where two reciprocal tasks of response generation and latent act recognition are collaboratively
carried out.
Model Architecture
In our model, there are three elements: dialogue
context c, response r and latent variable z. The
dialogue context c consists of several history utterances. .)
The response r is one piece of appropriate reply towards the given context. The latent variable z is one
K-way categorical variable z ∈[1, K], with each
value corresponding to a particular latent speech
act in the response.
1 
Research/tree/master/NLP/Dialogue-PLATO
It is snowing outside.
How about making a snowman?
It’s so cold. I really miss summer.
Figure 1: Graphical illustration of response generation (gray lines) and latent act recognition (dashed blue
The probabilistic relationships among these elements are elaborated with the graphical model
in Figure 1. Given a context c, there are multiple latent speech acts which can be taken as response intents (represented by the latent variable z).
Conditioned on the context and one selected latent
speech act, the response is generated as p(r|c, z)
(gray lines). Given a pair of context and response,
the underlying latent speech act can be estimated
as p(z|c, r) (dashed blue lines). As such, our pretraining of dialogue generation contains the following two tasks – response generation and latent
act recognition.
We propose a uniﬁed infrastructure for the
joint learning of both tasks, shown as Figure 2.
The backbone of our infrastructure is inspired
by the transformer blocks in ,
which supports both bi-directional encoding and
uni-directional decoding ﬂexibly via speciﬁc selfattention masks. Both response generation and
latent act recognition are carried out under the uni-
ﬁed network with shared parameters. Their detailed
implementations are described as follows.
Given the context c and a speciﬁc speech act z,
the response generation can be estimated as
p(r|c, z) = ΠT
t=1 p(rt|c, z, r<t) ,
where T is the length of the target response r and
r<t denotes previously generated words. Since the
response generation is a uni-directional decoding
process, each token in the response only attends
to those before it, shown as dashed orange lines in
The latent act recognition task is included to
identify the corresponding value of z for the given
context and the target response in the training data.
The latent act recognition shares network parameters with response generation, but has a separate
self-attention mask for bi-directional encoding. As
shown in Figure 2, with a special mask symbol [M]
as input, it keeps collecting information from the
context and target response (red lines). In this way,
Figure 2: Architecture of dialogue generation with discrete latent variable. In self-attention visualization, red and
blue lines denote bi-directional attention, and dashed orange lines denote uni-directional attention.
Embeddings
Embeddings
Embeddings
Embeddings
Figure 3: Input representation. The input embedding is the sum of token, role, turn and position embeddings.
the corresponding speech act for the target response
can be recognized as z ∼p(z|c, r), where p(z|c, r)
is the estimated posterior distribution over discrete
latent values.
Input Representation
For multi-turn conversation modeling, elaborate designs have been made on the input representation
in this work. For each token, its input embedding is
the sum of corresponding token, role, turn and position embeddings. One visual example is shown in
Figure 3 and details are described in the following.
• The input is the concatenation of latent variable,
dialogue context and response. Following the
pre-processing of BERT ,
the input text is tokenized with WordPiece . A special end-of-utterance [EOU]
token is appended to the end of each utterance for
separation. Another begin-of-utterance [BOU]
token is added at the beginning of the response,
whose ﬁnal hidden state (i.e., output of the last
transformer block) is used to predict next token
during generation.
• Given that z is one K-way categorical variable,
its token embedding E[z] is mapped from the
latent embedding space Ez ∈RK×D. For the
rest tokens in the vocabulary, they are initialized
using BERT’s WordPiece embeddings.
• Role embeddings are employed to differentiate
the characters evolved in the conversation. The
role embedding EA is added for the response,
as well as dialogue utterances generated by the
same character in the context. And role embedding EB is used for the other character. (For
knowledge grounded conversation, EC is used as
the role embedding of background knowledge.)
• In the interactive conversation, there are multiturn utterances and we employ relative order in
the assignment of turn embeddings. The turn embedding for the response is set to E , and the
turn embedding of its last utterance is E[−1], and
etc. Our utilization of relative turn embeddings
instead of absolute ones enables the model to
assign turn embedding E to the response consistently and makes response generation exempt
from the disturbance of its round number within
the dialogue.
• Position embeddings are added according to the
token position in each utterance. Note that for
the special token of latent variable, its corresponding role, turn and position embeddings are
all set to empty.
Pre-training Objectives
We employ three loss functions in dialogue generation pre-training: negative log-likelihood (NLL)
loss, bag-of-words (BOW) loss and response selection (RS) loss. Brief illustration is shown in the last
column of Figure 2 and detailed descriptions will
be provided in this section.
Response Generation
In our model, the response is generated conditioned
on the latent variable and the context. The widely
adopted NLL loss is embraced in the pre-training:
LNLL = −Ez∼p(z|c,r) log p(r|c, z)
= −Ez∼p(z|c,r)
log p(rt|c, z, r<t) ,
where z is the latent speech act of this training
pair (c, r), sampled from the probability distribution p(z|c, r). The posterior distribution over latent
values is estimated through the task of latent act
recognition:
p(z|c, r) = softmax(W1h[M] + b1) ∈RK , (3)
where h[M] ∈RD is the ﬁnal hidden state of the
special mask, W1 ∈RK×D and b1 ∈RK denote
the weight matrices of one fully-connected layer.
Besides the classical NLL loss, the bag-of-words
loss is also employed to facilitate the training process of latent discrete variables:
LBOW = −Ez∼p(z|c,r)
log p(rt|c, z)
= −Ez∼p(z|c,r)
where V refers to the whole vocabulary. The function f tries to predict the words within the target
response in a non-autoregressive way:
f = softmax(W2hz + b2) ∈R|V | ,
where hz is the ﬁnal hidden state of the latent variable and |V | is the vocabulary size. frt denotes
the estimated probability of word rt. As compared
with NLL loss, the BOW loss discards the order of
words and forces the latent variable to capture the
global information of the target response.
Response Selection
Response selection helps distinguish whether the
response is relevant with the dialogue context and
consistent with the background knowledge. Meanwhile, its score can be regarded as an indicator
of coherence during inference, helping to select
the most coherent one from multiple candidate responses.
Particularly, the training of response selection is
carried out together with the bi-directional encoding of latent act recognition. The positive training
samples come from the dialogue context and corresponding target response (c, r), with label lr = 1.
And the negative samples are created by randomly
selecting responses from the corpus (c, r−), with
label lr−= 0. The binary cross-entropy loss of
response selection is deﬁned as follows:
LRS = −log p(lr = 1|c, r)−log p(lr−= 0|c, r−)
The above probability is estimated through one
fully-connected layer, with the ﬁnal hidden state of
the special mask fed as input:
p(lr = 1|c, r) = sigmoid(W3h[M] + b3)
To sum up, the total objective of our pre-training
model is to minimize the integrated loss:
L = LNLL + LBOW + LRS
Pre-training Procedure
Our pre-training model contains 12 transformer
blocks, with network parameters initialized using
BERTBASE. Large-scale conversation datasets –
Twitter and Reddit are employed for pretraining, which results in 8.3 million training samples in total. For each training sample of context
and target response (c, r), it needs to pass through
the network twice to accomplish the tasks of latent
act recognition and response generation. And the
pre-training steps are summarized as follows:
1) Latent Act Recognition
– Given a pair of context and target response,
estimate the posterior distribution p(z|c, r)
– Randomly select r−and calculate LRS
2) Response Generation
– With the sampled latent value z ∼p(z|c, r),
calculate LNLL and LBOW
3) Optimization
– Sum up to obtain L, and update network parameters with back-propagation
The hyper-parameters used in pre-training are
listed as follows. The maximum sequence length
of context and response is set to 256 and 50, respectively. The number of transformer blocks in our
model L is 12 and the hidden embedding dimension D is 768. The batch size is set to 64 and K
is set to 20 for the discrete latent variable. Adam
optimizer is employed for
optimization with a learning rate of 5e-5. The pretraining of dialogue generation was carried out on 8
Nvidia Telsa V100 32G GPU cards for 3.5M steps,
taking about two weeks to reach convergence.
Fine-tuning and Inference
Our pre-trained model is ﬂexible enough to support various kinds of dialogues, including chit-chat,
knowledge grounded conversation, conversational
question answering, etc. The ﬁne-tuning on small
conversation datasets can be carried out by following the training objectives deﬁned in Equation (8).
As the ﬁne-tuning process reaches convergence, the
response towards the given context can be obtained
through the following inference procedure:
1) Candidate Response Generation
– Conditioned on each latent value z ∈[1, K],
generate corresponding candidate response r.
2) Response Selection
– Calculate the probability for each response
p(lr = 1|c, r) and select the one with highest
coherence value as the ﬁnal response.
It is worth noting that the above ﬁne-tuning and
inference procedures are set up for the dialogue
generation without any speciﬁc objectives. If there
exists a speciﬁc objective within the conversation,
such as letting both participants know more about
each other , the ﬁne-tuning can
proceed to maximize the pre-deﬁned rewards with
reinforcement learning (RL). Under such circumstances, our latent discrete variable can be naturally
treated as action within RL, and thus the response
selection can be straightforwardly solved by selecting the action that results in the maximum reward.
Experiments
To evaluate the performance of our proposed
method, comprehensive experiments have been carried out on three publicly available datasets.
• Persona-Chat is a knowledge grounded conversation dataset. It provides
both manually annotated conversations and corresponding persona proﬁles (background knowledge), where two participants chat naturally and
try to get to know each other.
• Daily Dialog is a chit-chat
dataset, which contains high-quality human conversations about daily life.
• DSTC7-AVSD , short for
Audio Visual Scene-aware Dialog of the DSTC7
challenge, is a conversational question answering dataset. In DSTC7-AVSD, the system need
to generate an answer given dialogue context and
background knowledge. There are two available
options of knowledge utilization: 1) using singlemodal information of text only, including video’s
caption and summary; 2) relying on multi-modal
information, including text, audio and visual features. The single-modal option is adopted by our
method in the experiments.
The descriptions and statistics of these datasets are
summarized in Table 1.
Compared Methods
The following models have been compared in the
experiments.
Baseline. Sequence to sequence with attention
(Seq2Seq) is employed as
the baseline for the experiments on Persona-Chat
and Daily Dialog. DSTC7-AVSD has provided a
baseline system, which is built upon hierarchical
recurrent encoders with multi-modal features.
State of the art. Persona-Chat was also utilized in
the ConvAI2 challenge , where
the team of Lost in Conversation (LIC) obtains the best performance. LIC
is also one transformer based generation method
and ﬁne-tuned upon the pre-trained model of GPT
 . For the dataset of Daily
Dialog, its best results are reported by the recently
developed method – iVAEMI ,
which generates diverse responses with samplebased latent representation. In DSTC7-AVSD, the
team of CMU obtains the
best performance across all the evaluation metrics.
Our method. To better analyze the effects of our
latent discrete variable, we also compare to the
version without latent variable (Our w/o Latent).2
2It shares the same training settings as our method with
latent variables: network parameters are ﬁrst initialized with
BERTBASE, and the pre-training is further carried out on Reddit
and Twitter. The only difference lies in the incorporation of
latent variable.
Persona-Chat
with persona
Persona profiles
8,939 dialogues
131,438 turns
1,000 dialogues
15,602 turns
968 dialogues
15,024 turns
Daily Dialog
11,118 dialogues
87,170 turns
1,000 dialogues
8,069 turns
1,000 dialogues
7,740 turns
DSTC7-AVSD
Conversational QA
Video caption
7,659 dialogues
153,180 turns
1,787 dialogues
35,740 turns
1,710 dialogues
13,490 turns
Table 1: Summary of datasets used in the experiments.
Automatic Evaluation
Human Evaluation
Distinct-1/2
Knowledge R/P/F1
Informativeness
0.448 / 0.353
0.004 / 0.016
0.004 / 0.016 / 0.006
0.405 / 0.320
0.019 / 0.113
0.042 / 0.154 / 0.064
Our w/o Latent
0.458 / 0.357
0.012 / 0.064
0.085 / 0.263 / 0.125
Our Method
0.406 / 0.315
0.021 / 0.121
0.142 / 0.461 / 0.211
0.336 / 0.268
0.030 / 0.128
0.309 / 0.249
0.029 / 0.250
Our w/o Latent
0.405 / 0.322
0.046 / 0.246
Our Method
0.397 / 0.311
0.053 / 0.291
Table 2: Experimental results on Persona-Chat and Daily Dialog with automatic and human evaluations, with
highest value written in bold.
DSTC7-AVSD
Our Method
Our Method
Upper Bound
Table 3: Experimental results on DSTC7-AVSD with automatic evaluation, with highest value written in bold.
breaking news on the presidential race that could change your vote
Generated Responses
i m not voting for hillary i m voting for trump
i can t believe this is happening
it would be interesting to see the results of this election
trump is a role model that the news media mafia hides
i will be voting for hillary
i plan on walking into the appalachian mountains and never coming back .
Generated Responses
i've been to the appalachian mountains a few times . it's a beautiful place .
i've never been to the appalachian mountains , but i've heard it's nice .
i wouldn't want to live there .
don't worry , you'll come back .
that sounds like a good plan .
Table 4: Examples of response generation with our pre-trained model.
Initialization
Training Configurations
# Fine-tuning Dialogues
Context Attention
Bi-direction
Bi-direction
Uni-direction
Uni-direction
Uni-direction
Bi-direction
Uni-direction
Bi-direction
Twitter & Reddit
Bi-direction
Bi-direction
Twitter & Reddit
Bi-direction
Bi-direction
Twitter & Reddit
Table 5: Perplexity of different pre-trained models on Persona-Chat, with best value written in bold.
Evaluation Metrics
Both automatic and human evaluations are employed to assess the performance of compared
methods. In automatic evaluation, the following
metrics are included:
• BLEU measures the
n-gram overlap between generated response and
the target response.
• Distinct-1/2 measures the generation diversity, which is deﬁned as the number
of distinct uni- or bi-grams divided by the total
amount of generated words.
• Knowledge Recall/Precision/F1 measures the degree of informativeness
w.r.t. background knowledge.
• In DSTC7-AVSD, the MSCOCO platform is employed for evaluation. It compares the generated response with six ground
truth responses, using metrics of BLEU, ME-
TEOR, ROUGH-L and CIDEr.
In human evaluation, we randomly select 100
dialogue contexts and generate responses with compared methods. Three crowd-sourcing workers are
asked to score the response quality on a scale of from four aspects – ﬂuency, coherence, informativeness and overall. The higher score, the better.
Details about the criteria are given as follows.
• Fluency measures whether the generated sentence is smooth and grammatically correct.
• Coherence evaluates whether the generated response is relevant with the dialogue context and
consistent with the expressed information or
background knowledge.
• Informativeness assesses whether the response
is informative or not.
• Overall represents the general evaluation, where
0 indicates a bad response, 1 refers to a normal
response and 2 stands for a good response.
After collecting the assessments from annotators,
the response’s ﬁnal score is determined via majority voting. The average Fleiss’s kappa on Persona-Chat and Daily Dialog is
0.515 and 0.480 respectively, indicating annotators
have reached moderate agreement.
Experimental Results
The experimental results on Persona-Chat and
Daily Dialog with automatic and human evaluations are summarized in Table 2. As suggested in
the empirical study , the correlation between automatic metrics and human judgments is weak in open-domain dialogue generation.
In the automatic evaluation, experimental results
demonstrate that no method can consistently outperform the others.
During human evaluations, our method achieves
better performance consistently across all the metrics on Persona-Chat and Daily Dialog. The scores
of ﬂuency almost approach the upper bound, revealing that our generated responses are very ﬂuent.
The informativeness assessments indicate that the
information in our generated responses is signiﬁcantly richer, as compared with the baseline methods. Our responses are coherent with the context
and favored most by crowd-sourcing workers. The
ablation study with our method and our w/o latent
also suggests that through the incorporation of discrete latent variables, remarkable improvements
can be achieved for dialogue generation. In addition, it can be observed that the generation quality of transformed-based approaches (LIC and our
method) is signiﬁcantly better than that of RNNbased methods (Seq2Seq and iVAEMI).3
The experimental results on DSTC7-AVSD with
automatic evaluation are provided in Table 3. In the
3It is a normal phenomenon that the performance of our
w/o latent is close to that of LIC. Both of them initialize
network parameters with pre-trained language models and
continue training with large-scale conversation data as Reddit.
experiments, our response selection is strengthened
with an extra ranking step, which learns to rank the
candidates according to automatic scores and selects the top one as the ﬁnal answer. The results in
Table 3 demonstrate that our method has brought a
new breakthrough for DSTC7-AVSD. Additionally,
the upper bound of our method is also reported,
under the ideal scenario that the optimal candidate
answer can be selected.4 The incredible results
validate the great potential of our approach.
Discussions
Case Analysis
To further dissect the quality of our pre-trained
model, several examples of generated responses
are provided in Table 4. For each piece of context,
our model can produce multiple responses by assigning distinct values to the latent variable and
ﬁve candidate responses are selected for display in
the table. It shows that our pre-trained model is
able to generate diverse and appropriate responses.
More examples on the conversational datasets are
provided in the Appendix.
Comparison of Pre-trained Models
To further analyze the effectiveness of our pretrained model, more ablation studies have been
conducted on Persona-Chat. Distinct pre-trained
models are included for comparison. To be fair,
their transformer layers are all set to 12. There are
three different sizes of training dialogues: 1k, 5k
and 9k (all training data). The training conﬁgurations and experimental results measured with perplexity are summarized in Table 5. There are three
groups of pre-trained models: group 1 applies direct ﬁne-tuning of BERT or GPT-2 on Persona-Chat; group 2 employs Twitter
and Reddit for further training upon the basis of pretrained language models; group 3 carries out the
training process with latent variable.5 (Model 2.2
is our w/o latent one and model 3.1 is our method.)
These results demonstrate that our method outperforms the other pre-trained models consistently
4Given a dialogue context and background knowledge, our
model is able to generate K diverse responses. Each of them
will be evaluated using MSCOCO and the one obtaining the
best score will be treated as the optimal candidate answer.
5Overall, group 1 involves two-stage training: pre-training
of language model with general text and ﬁne-tuning on small
conversation datasets. Whereas, group 2 and group 3 involve
three-stage training: pre-training of language model with general text, further pre-training of dialogue generation with Twitter and Reddit, and ﬁne-tuning on small conversation datasets.
with lower perplexity across different training sets.
Several interesting conclusions can be also drawn
from these results. Firstly, the comparison between
model 1.2 and model 1.3 encourages the adoption
of ﬂexible attention mechanism to fully leverage
the bi-directional context information.6 Secondly,
the superiority of group 2 over group 1 mainly
comes from the employment of Twitter and Reddit,
which are closer to human conversations than general text. Thirdly, the comparison between model
2.2 and model 3.1 reﬂects that the incorporation of
discrete latent variable is able to boost the quality
of response generation, whose effects have also
been veriﬁed in Table 2.
Related Work
Related work involves pre-trained language models
and one-to-many modeling in dialogue generation.
Pre-trained Language Models. Pre-trained language models, which are trained on massive general text, have brought many breakthroughs on various NLP tasks. These models can be roughly divided into two categories according to their attention mechanisms. GPT and
GPT-2 are representative unidirectional language models, where one token is
only allowed to attend its previous tokens and the
objective is to maximize left-to-right generation
likelihood. BERT and XL-
Net are bi-directional language
models, where bi-directional context attention is
enabled for token prediction. The latest uniﬁed
language model UniLM is able
to support both uni- and bi-directional attention
with ﬂexible self-attention mask designs. Recently,
some attempts have been made to adapt
generative language models GPT or GPT-2 for dialogue generation. Whereas the special issues of
conversations, such as impacts from background
knowledge and problems of one-to-many relationship, are not fully considered and tackled in these
adaptations.
One-to-many Modeling. Given one piece of context, there exists multiple appropriate responses,
which is know as the one-to-many mapping problem.
To model this one-to-many relationship,
CVAE employs Gaussian distri-
6The results of model 1.1 demonstrate that there are some
deﬁciencies in performance to apply direct ﬁne-tuning of
BERT on small conversation datasets, as discussed in the
introduction.
bution to capture the discourse-level variations of
responses. To alleviate the issue of posterior collapse in VAE, some extension approaches are further developed, including conditional Wasserstein
auto-encoder of DialogWAE and
implicit feature learning of iVAEMI aims to
jointly optimize diversity and relevance in the latent
space, which are roughly matched by the distance
and direction from the predicted response vector.
Besides the continuous representation in VAE, discrete categorical variables are also utilized for interpretable generation . Additionally, multiple mapping modules as latent mechanisms are introduced for diverse generation , where accurate optimization is carried
out via posterior mapping selection. However, due
to the small scale of annotated conversation data
and limited capacity of generation network, it remains challenging for these methods to balance the
diversity and ﬂuency during response generation.
Conclusion
A novel pre-training model for dialogue generation
is introduced in this paper, incorporated with latent discrete variables for one-to-many relationship
modeling. To pre-train our model, two reciprocal
tasks of response generation and latent recognition
are carried out simultaneously on large-scale conversation datasets. Our pre-trained model is ﬂexible enough to handle various down-stream tasks
of dialogue generation. Extensive and intensive
experiments have been carried out on three different kinds of publicly available datasets. And the
results demonstrate that our model obtains signiﬁcant improvements over the other state-of-the-art
Our work can be potentially improved with more
ﬁne-grained latent variables. In the future, we will
also explore to boost the latent selection policy with
reinforcement learning and extend our pre-training
to support dialogue generation in other languages.
Acknowledgments
We would like to thank the ACL reviewers for
their constructive suggestions and Chaotao Chen,
Junkun Chen, Tong Wu and Wenxia Zheng for their
generous help. This work was supported by the
Natural Key Research and Development Project of
China (No. 2018AAA0101900).